{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c8d1db48-b69a-1e59-f274-ff432fe166b7"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport nltk\nimport seaborn as sns\nfrom nltk.corpus import stopwords\nfrom nltk import word_tokenize, ngrams\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b9993e89-44ec-ed67-2731-dc1f3fe87ec5"},"outputs":[],"source":"test = \"../input/test.csv\"\ntrain = \"../input/train.csv\"\ntestData = pd.read_csv(test,sep=\",\")\ntrainData = pd.read_csv(train,sep=\",\")\nprint(testData.info())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0df2659c-5209-82dc-8d1d-ba5423b33dcf"},"outputs":[],"source":"print(trainData.info())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e6a4457d-b370-5630-3e20-fadc81411ff9"},"outputs":[],"source":"print(trainData.groupby('is_duplicate').size())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c5e19e9c-caac-f28e-4f95-f1a041dbda46"},"outputs":[],"source":"print(trainData.head(10))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f2bd26ba-e2cf-61c0-ab41-023e90e40fd0"},"outputs":[],"source":"\ndef get_feature_mat(train):\n\t#feature engineering in this funciton is applied to both test and train\n\tdf \t= pd.read_csv(\"../input/\"+train)\n\treturn(df)\n\ntrain, test = [get_feature_mat(train) for train in ['train.csv', 'test.csv']]\n\nprint(train.columns)\nprint(test.columns)\n\nv = np.array(train['is_duplicate'])\nmean = np.mean(v)\nstd = np.std(v)\nprint(\"mean\", mean)\nprint(\"std\", std)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dc804f8c-866d-8909-27fc-ee154348376a"},"outputs":[],"source":"trainData['is_duplicate'].hist()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3f9929ca-eb9a-b19a-4e7c-924454118789"},"outputs":[],"source":"import string\nremove_punctuation_map = dict((ord(char), None) for char in string.punctuation)\n\nsw = stopwords.words(\"english\")\nprint(sw.index('what'))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"edba78b4-048d-fb78-9476-77f7b42fe951"},"outputs":[],"source":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(stop_words=\"english\")\nquestion2 = []\nfor s in trainData['question2']: \n    if (type(s)!=float):\n        question2.append(s.lower())\n    else:    \n        print(s)\n        question2.append(\"\")"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e5948caa-4542-81a3-ad91-2f4a9590fe7a"},"outputs":[],"source":"#Stemming\nfrom nltk.stem.snowball import SnowballStemmer\nstemmer = SnowballStemmer(\"english\")\nwords =\"\"\nquestion1_stem =[]\nquestion2_stem =[]\n\nfor q in trainData['question1']:\n    q=q.translate(remove_punctuation_map)\n    stemmed = [stemmer.stem(w) for w in q.split() if w not in sw]\n    if stemmed not in sw:\n        words = \" \".join(stemmed)\n        question1_stem.append(words)    \nfor q in question2:\n    q=q.translate(remove_punctuation_map)\n    stemmed = [stemmer.stem(w) for w in q.split()]\n    if stemmed not in sw:\n        words = \" \".join(stemmed)\n        question2_stem.append(words)    \n    \nprint(len(question1_stem))\nprint(len(question2_stem))        "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6a01a393-7e1a-e42b-9603-5cf137a6a02a"},"outputs":[],"source":"print(question1_stem[:5])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9fa12d2a-dd39-fa1a-9d48-137ac01c3831"},"outputs":[],"source":"print(question2_stem[:5])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9fe8a01b-454d-88cb-4ec3-c027dcc27e27"},"outputs":[],"source":"#for x in range (1,40):\nprint(question1_stem[9545])\nprint(question2_stem[9545])\n\n#print(question1_stem[35641])\n#print(question2_stem[35641])\n\nprint(question1_stem[42947])\nprint(question2_stem[42947])\n\nprint(question1_stem[44344])\nprint(question2_stem[44344])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b09ea6eb-cd62-4935-bf12-70fc34b5ad45"},"outputs":[],"source":"calculated_values = []\ncnt = -1\n#for x in range(0,404290):\nfor q1,q2 in zip (question1_stem, question2_stem):\n    cnt =cnt +1\n    #print(trainData['is_duplicate'][cnt])\n    try:\n        tfidf = vectorizer.fit_transform([q1, q2])\n        calculated_values.append(((tfidf * tfidf.T).A)[0,1])\n    except ValueError:\n        print('Index with empty values or only root values',cnt)\n        calculated_values.append(trainData['is_duplicate'][cnt])\n\nprint(len(calculated_values))    \nprint(calculated_values[:6])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7c5d36d6-a8c0-35cb-45a4-f3032834c18f"},"outputs":[],"source":"# Two columns\nlen(trainData['is_duplicate'])\nnew_train_data = pd.DataFrame({\"calculated\":calculated_values,\"actual\":trainData['is_duplicate']})\n\nprint(type(new_train_data))\nprint(len(new_train_data))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bdbf6bbb-ffd3-ea99-7c1e-85478c4c0bce"},"outputs":[],"source":"from sklearn import model_selection\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\n\nX = new_train_data['calculated'].values.reshape(-1,1)\ny = new_train_data['actual']"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ba2b85d4-3287-5c16-db89-76c339b435bd"},"outputs":[],"source":"print(new_train_data.describe())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ea23974d-da05-6205-701e-2bc750016327"},"outputs":[],"source":"from pandas.tools.plotting import scatter_matrix\nscatter_matrix(new_train_data)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8b085518-2aa1-b3bf-0331-eaac0e42a31a"},"outputs":[],"source":"#split = int(0.75*1000)\n#X_train = X[0:split]\n#X_test  = X[split:]\n#y_train = y[0:split]\n#y_test  = y[split:]\n\nvalidation_size = 0.20\nseed = 7\nX_train, X_validation, Y_train, Y_validation = model_selection.train_test_split(X, y, test_size=validation_size, random_state=seed)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e20cb000-187a-22e9-1eb4-0538e7bec658"},"outputs":[],"source":"#clf = GaussianNB()\n#clf.fit(X_train,y_train)\n\n#Test\nseed = 7\nscoring = 'accuracy'\n\n# Various algos\nmodels = []\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\n#models.append(('SVM', SVC()))\n# evaluate each model in turn\nresults = []\nnames = []\nfor name, model in models:\n\tkfold = model_selection.KFold(n_splits=10, random_state=seed)\n\tcv_results = model_selection.cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n\tresults.append(cv_results)\n\tnames.append(name)\n\tmsg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n\tprint(msg)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3066d72f-a7fe-41db-3684-cb8903bc692d"},"outputs":[],"source":"question1 = []\nfor s in testData['question1'][15:22]: \n    if (type(s)!=float):\n        question1.append(s.lower())\n    else:    \n        print(s)\n        question1.append(\"\")\nquestion2 = []\nfor s in testData['question2'][15:22]: \n    if (type(s)!=float):\n        question2.append(s.lower())\n    else:    \n        print(s)\n        question2.append(\"\")"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a5a7e541-f963-58e1-da53-c74e14bbd896"},"outputs":[],"source":"#print(clf.predict(0.67))\n#print(clf.predict(0.17))\n#print(clf.predict(0.47))\nquestion1_stem =[]\nquestion2_stem =[]\n\nfor q in question1:\n    q=q.translate(remove_punctuation_map)\n    stemmed = [stemmer.stem(w) for w in q.split() if w not in sw]\n    if stemmed not in sw:\n        words = \" \".join(stemmed)\n        question1_stem.append(words)    \nfor q in question2:\n    q=q.translate(remove_punctuation_map)\n    stemmed = [stemmer.stem(w) for w in q.split()]\n    if stemmed not in sw:\n        words = \" \".join(stemmed)\n        question2_stem.append(words)    \n    \nprint(len(question1_stem))\nprint(len(question2_stem))        "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"164a0db8-0746-355d-344a-99a69adcdc52"},"outputs":[],"source":"calculated_values = []\ncnt = -1\n\nfor q1,q2 in zip (question1_stem, question2_stem):\n    cnt =cnt +1\n    try:\n        tfidf = vectorizer.fit_transform([q1, q2])\n        calculated_values.append(((tfidf * tfidf.T).A)[0,1])\n    except ValueError:\n        print('Index with empty values or only root values',cnt)\n        calculated_values.append(1.0)\n\nprint(len(calculated_values))    \nprint(calculated_values[:6])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"971f2b4b-ae91-5b08-e1ae-060b4595d861"},"outputs":[],"source":"print(testData.info())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4c7686dd-0966-726a-516e-5387515e1126"},"outputs":[],"source":"clf = DecisionTreeClassifier()\nclf.fit(X_train, Y_train)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"33e301eb-0236-db1f-5244-ea814131ad04"},"outputs":[],"source":"clf = DecisionTreeClassifier()\nclf.fit(X_train, Y_train)\ntest_id = testData['test_id'][15:22]\ncalc = pd.DataFrame(calculated_values)\npreds = clf.predict(calc.values.reshape(-1,1))\n\nprint(len(preds))\nprint(preds[:10])\nout_df = pd.DataFrame({\"test_id\":test_id, \"is_duplicate\":preds})\nprint(out_df[:15])\nfile = out_df.to_csv(\"isduplicate_predicted_final5.csv\", index=False)"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}