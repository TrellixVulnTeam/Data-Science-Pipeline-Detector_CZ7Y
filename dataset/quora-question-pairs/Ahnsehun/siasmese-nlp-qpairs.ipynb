{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport numpy as np \nimport pandas as pd \n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-25T15:45:34.796009Z","iopub.execute_input":"2021-10-25T15:45:34.796263Z","iopub.status.idle":"2021-10-25T15:45:34.806313Z","shell.execute_reply.started":"2021-10-25T15:45:34.796235Z","shell.execute_reply":"2021-10-25T15:45:34.805354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 데이터 불러오기\ndata =  pd.read_csv(\"../input/quora-question-pairs/train.csv.zip\")","metadata":{"execution":{"iopub.status.busy":"2021-10-25T15:45:34.808417Z","iopub.execute_input":"2021-10-25T15:45:34.808893Z","iopub.status.idle":"2021-10-25T15:45:37.138891Z","shell.execute_reply.started":"2021-10-25T15:45:34.808844Z","shell.execute_reply":"2021-10-25T15:45:37.137994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"execution":{"iopub.status.busy":"2021-10-25T15:45:37.142086Z","iopub.execute_input":"2021-10-25T15:45:37.142316Z","iopub.status.idle":"2021-10-25T15:45:37.172007Z","shell.execute_reply.started":"2021-10-25T15:45:37.14229Z","shell.execute_reply":"2021-10-25T15:45:37.171237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_test =  pd.read_csv(\"../input/quora-question-pairs/train.csv.zip\")","metadata":{"execution":{"iopub.status.busy":"2021-10-25T15:45:37.173469Z","iopub.execute_input":"2021-10-25T15:45:37.173738Z","iopub.status.idle":"2021-10-25T15:45:38.69343Z","shell.execute_reply.started":"2021-10-25T15:45:37.173705Z","shell.execute_reply":"2021-10-25T15:45:38.692672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_test","metadata":{"execution":{"iopub.status.busy":"2021-10-25T15:45:38.695452Z","iopub.execute_input":"2021-10-25T15:45:38.695845Z","iopub.status.idle":"2021-10-25T15:45:38.718568Z","shell.execute_reply.started":"2021-10-25T15:45:38.695805Z","shell.execute_reply":"2021-10-25T15:45:38.717942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 구글 word2vec 다운로드\n!wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"","metadata":{"execution":{"iopub.status.busy":"2021-10-25T15:45:38.719813Z","iopub.execute_input":"2021-10-25T15:45:38.720174Z","iopub.status.idle":"2021-10-25T15:46:14.468165Z","shell.execute_reply.started":"2021-10-25T15:45:38.720141Z","shell.execute_reply":"2021-10-25T15:46:14.467427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# KeyedVectors와 모델사용을 위한 gensim 패키지 인스톨\n!pip install gensim==3.8.0","metadata":{"execution":{"iopub.status.busy":"2021-10-25T15:46:14.469932Z","iopub.execute_input":"2021-10-25T15:46:14.470231Z","iopub.status.idle":"2021-10-25T15:46:26.623799Z","shell.execute_reply.started":"2021-10-25T15:46:14.470194Z","shell.execute_reply":"2021-10-25T15:46:26.623006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 파이썬 정규표현식 사용\n# re.sub를 이용해서 특수문자 제거 및 여러가지 관용구 등을 통일\nimport re\n\nfrom tensorflow.python.keras import backend as K\nfrom tensorflow.python.keras.layers import Layer\nfrom tensorflow.python.keras.preprocessing.sequence import pad_sequences\n\nfrom nltk.corpus import stopwords\nfrom gensim.models import KeyedVectors\n\nimport gensim\n\nimport numpy as np\n\nimport itertools\n\n\ndef text_to_word_list(text):\n    # Pre process and convert texts to a list of words\n    text = str(text)\n    text = text.lower()\n\n    # Clean the text\n    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n    text = re.sub(r\"what's\", \"what is \", text)\n    text = re.sub(r\"\\'s\", \" \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"can't\", \"cannot \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"i'm\", \"i am \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub(r\",\", \" \", text)\n    text = re.sub(r\"\\.\", \" \", text)\n    text = re.sub(r\"!\", \" ! \", text)\n    text = re.sub(r\"\\/\", \" \", text)\n    text = re.sub(r\"\\^\", \" ^ \", text)\n    text = re.sub(r\"\\+\", \" + \", text)\n    text = re.sub(r\"\\-\", \" - \", text)\n    text = re.sub(r\"\\=\", \" = \", text)\n    text = re.sub(r\"'\", \" \", text)\n    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n    text = re.sub(r\":\", \" : \", text)\n    text = re.sub(r\" e g \", \" eg \", text)\n    text = re.sub(r\" b g \", \" bg \", text)\n    text = re.sub(r\" u s \", \" american \", text)\n    text = re.sub(r\"\\0s\", \"0\", text)\n    text = re.sub(r\" 9 11 \", \"911\", text)\n    text = re.sub(r\"e - mail\", \"email\", text)\n    text = re.sub(r\"j k\", \"jk\", text)\n    text = re.sub(r\"\\s{2,}\", \" \", text)\n\n    text = text.split()\n\n    return text\n\n\n# 사전 생성\ndef make_w2v_embeddings(df, embedding_dim=300, empty_w2v=False):\n    vocabs = {}\n    vocabs_cnt = 0\n\n    vocabs_not_w2v = {}\n    vocabs_not_w2v_cnt = 0\n\n    # Stopwords설정\n    stops = set(stopwords.words('english'))\n\n    # Load word2vec\n    print(\"Loading word2vec model(it may takes 2-3 mins) ...\")\n    from gensim import models\n    \n    if empty_w2v:\n        word2vec = EmptyWord2Vec\n    else:\n        word2vec = KeyedVectors.load_word2vec_format(\"./GoogleNews-vectors-negative300.bin.gz\", binary=True)\n        #word2vec = models.Word2Vec.load_word2vec_format(\"./GoogleNews-vectors-negative300.bin.gz\", binary=True)\n\n        # word2vec = gensim.models.word2vec.Word2Vec.load(\"./data/Quora-Question-Pairs.w2v\").wv\n\n    for index, row in df.iterrows():\n        # Print the number of embedded sentences.\n        if index != 0 and index % 1000 == 0:\n            print(\"{:,} sentences embedded.\".format(index), flush=True)\n\n        # Iterate through the text of both questions of the row\n        for question in ['question1', 'question2']:\n\n            q2n = []  # q2n -> question numbers representation\n            for word in text_to_word_list(row[question]):\n                # Check for unwanted words\n                if word in stops:\n                    continue\n\n                # If a word is missing from word2vec model.\n                if word not in word2vec.vocab:\n                    if word not in vocabs_not_w2v:\n                        vocabs_not_w2v_cnt += 1\n                        vocabs_not_w2v[word] = 1\n\n                # If you have never seen a word, append it to vocab dictionary.\n                if word not in vocabs:\n                    vocabs_cnt += 1\n                    vocabs[word] = vocabs_cnt\n                    q2n.append(vocabs_cnt)\n                else:\n                    q2n.append(vocabs[word])\n\n            # Append question as number representation\n            df.at[index, question + '_n'] = q2n\n\n    embeddings = 1 * np.random.randn(len(vocabs) + 1, embedding_dim)  # This will be the embedding matrix\n    embeddings[0] = 0  # So that the padding will be ignored\n\n    # Build the embedding matrix\n    for word, index in vocabs.items():\n        if word in word2vec.vocab:\n            embeddings[index] = word2vec.word_vec(word)\n    del word2vec\n\n    return df, embeddings\n\n# 제로패딩\ndef split_and_zero_padding(df, max_seq_length):\n    # Split to dicts\n    X = {'left': df['question1_n'], 'right': df['question2_n']}\n\n    # Zero padding\n    for dataset, side in itertools.product([X], ['left', 'right']):\n        dataset[side] = pad_sequences(dataset[side], padding='pre', truncating='post', maxlen=max_seq_length)\n\n    return dataset\n\n\n#  --\n# 맨하탄거리, L1 Distance 구하기\nclass ManDist(Layer):\n    \"\"\"\n    Keras Custom Layer that calculates Manhattan Distance.\n    \"\"\"\n\n    # initialize the layer, No need to include inputs parameter!\n    def __init__(self, **kwargs):\n        self.result = None\n        super(ManDist, self).__init__(**kwargs)\n\n    # input_shape will automatic collect input shapes to build layer\n    def build(self, input_shape):\n        super(ManDist, self).build(input_shape)\n\n    # This is where the layer's logic lives.\n    def call(self, x, **kwargs):\n        self.result = K.exp(-K.sum(K.abs(x[0] - x[1]), axis=1, keepdims=True))\n        return self.result\n\n    # return output shape\n    def compute_output_shape(self, input_shape):\n        return K.int_shape(self.result)\n\n\nclass EmptyWord2Vec:\n    \"\"\"\n    Just for test use.\n    \"\"\"\n    vocab = {}\n    word_vec = {}","metadata":{"execution":{"iopub.status.busy":"2021-10-25T15:46:26.626014Z","iopub.execute_input":"2021-10-25T15:46:26.626297Z","iopub.status.idle":"2021-10-25T15:46:34.736556Z","shell.execute_reply.started":"2021-10-25T15:46:26.626257Z","shell.execute_reply":"2021-10-25T15:46:34.735838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from time import time\nimport pandas as pd\n\nimport matplotlib\n\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf\n\nfrom tensorflow.python.keras.models import Model, Sequential\nfrom tensorflow.python.keras.layers import Input, Embedding, LSTM, GRU, Conv1D, Conv2D, GlobalMaxPool1D, Dense, Dropout\n\n#from util import make_w2v_embeddings\n#from util import split_and_zero_padding\n#from util import ManDist\n\n# File paths\nTRAIN_CSV = \"../input/quora-question-pairs/train.csv.zip\"\n\n# training set Load하기\ntrain_df = pd.read_csv(TRAIN_CSV)\nfor q in ['question1', 'question2']:\n    train_df[q + '_n'] = train_df[q]\n\n# Make word2vec embeddings\nembedding_dim = 300\nmax_seq_length = 20\nuse_w2v = True\n\ntrain_df, embeddings = make_w2v_embeddings(train_df, embedding_dim=embedding_dim, empty_w2v=not use_w2v)\n\n# train과 validation split\nvalidation_size = int(len(train_df) * 0.1)\ntraining_size = len(train_df) - validation_size\n\nX = train_df[['question1_n', 'question2_n']]\nY = train_df['is_duplicate']\n\nX_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size=validation_size)\n\nX_train = split_and_zero_padding(X_train, max_seq_length)\nX_validation = split_and_zero_padding(X_validation, max_seq_length)\n\n# Convert labels to their numpy representations\nY_train = Y_train.values\nY_validation = Y_validation.values\n\n# Make sure everything is ok\nassert X_train['left'].shape == X_train['right'].shape\nassert len(X_train['left']) == len(Y_train)\n\n# --\n\n# Model variables\ngpus = 2\nbatch_size = 1024 * gpus\nn_epoch = 2\nn_hidden = 50\n\n# Define the shared model\nx = Sequential()\nx.add(Embedding(len(embeddings), embedding_dim,\n                weights=[embeddings], input_shape=(max_seq_length,), trainable=False))\n# CNN\n# x.add(Conv1D(250, kernel_size=5, activation='relu'))\n# x.add(GlobalMaxPool1D())\n# x.add(Dense(250, activation='relu'))\n# x.add(Dropout(0.3))\n# x.add(Dense(1, activation='sigmoid'))\n# LSTM\nx.add(LSTM(n_hidden))\n\nshared_model = x\n\n# The visible layer\nleft_input = Input(shape=(max_seq_length,), dtype='int32')\nright_input = Input(shape=(max_seq_length,), dtype='int32')\n\n# Pack it all up into a Manhattan Distance model\nmalstm_distance = ManDist()([shared_model(left_input), shared_model(right_input)])\nmodel = Model(inputs=[left_input, right_input], outputs=[malstm_distance])\n\n#if gpus >= 2:\n    # `multi_gpu_model()` is a so quite buggy. it breaks the saved model.\n #   model = tf.keras.utils.multi_gpu_model(model, gpus=gpus)\nmodel.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.Adam(), metrics=['accuracy'])\nmodel.summary()\nshared_model.summary()\n\n# 트레이닝 시작\ntraining_start_time = time()\nmalstm_trained = model.fit([X_train['left'], X_train['right']], Y_train,\n                           batch_size=batch_size, epochs=n_epoch,\n                           validation_data=([X_validation['left'], X_validation['right']], Y_validation))\ntraining_end_time = time()\nprint(\"Training time finished.\\n%d epochs in %12.2f\" % (n_epoch,\n                                                        training_end_time - training_start_time))\n\n","metadata":{"execution":{"iopub.status.busy":"2021-10-25T15:46:34.737885Z","iopub.execute_input":"2021-10-25T15:46:34.738135Z","iopub.status.idle":"2021-10-25T15:52:22.088138Z","shell.execute_reply.started":"2021-10-25T15:46:34.738105Z","shell.execute_reply":"2021-10-25T15:52:22.087055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nmalstm_distance = ManDist()([shared_model(left_input), shared_model(right_input)])\n\n# Added the logits layer followed by Dense to get binary_crossentropy loss\n# becauase our targets are binary not continuous\n\nmalstm_distance_logits = Dense(1)(malstm_distance)\nmodel = Model(inputs=[left_input, right_input], outputs=[malstm_distance_logits])\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2021-10-25T15:52:22.089206Z","iopub.execute_input":"2021-10-25T15:52:22.090728Z","iopub.status.idle":"2021-10-25T15:52:22.096612Z","shell.execute_reply.started":"2021-10-25T15:52:22.090688Z","shell.execute_reply":"2021-10-25T15:52:22.09568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('./data/SiameseLSTM.h5')\n\n# Plot accuracy\nplt.subplot(211)\nplt.plot(malstm_trained.history['accuracy'])\nplt.plot(malstm_trained.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\n\n# Plot loss\nplt.subplot(212)\nplt.plot(malstm_trained.history['loss'])\nplt.plot(malstm_trained.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper right')\n\nplt.tight_layout(h_pad=1.0)\nplt.savefig('./data/history-graph.png')\n\nprint(str(malstm_trained.history['val_accuracy'][-1])[:6] +\n      \"(max: \" + str(max(malstm_trained.history['val_accuracy']))[:6] + \")\")\nprint(\"Done.\")","metadata":{"execution":{"iopub.status.busy":"2021-10-25T17:07:45.488416Z","iopub.execute_input":"2021-10-25T17:07:45.488901Z","iopub.status.idle":"2021-10-25T17:07:45.83328Z","shell.execute_reply.started":"2021-10-25T17:07:45.488773Z","shell.execute_reply":"2021-10-25T17:07:45.830439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\nimport tensorflow as tf\n\n#from util import make_w2v_embeddings\n#from util import split_and_zero_padding\n#from util import ManDist\n\n# File paths\nTEST_CSV = '../input/quora-question-pairs/train.csv.zip'\n\n# Load training set\ntest_df = pd.read_csv(TEST_CSV)\nfor q in ['question1', 'question2']:\n    test_df[q + '_n'] = test_df[q]\n\n# Make word2vec embeddings\nembedding_dim = 300\nmax_seq_length = 20\ntest_df, embeddings = make_w2v_embeddings(test_df, embedding_dim=embedding_dim, empty_w2v=False)\n\n# Split to dicts and append zero padding.\nX_test = split_and_zero_padding(test_df, max_seq_length)\n\n# Make sure everything is ok\nassert X_test['left'].shape == X_test['right'].shape\n\n# --\n\nmodel = tf.keras.models.load_model('./data/SiameseLSTM.h5', custom_objects={'ManDist': ManDist})\nmodel.summary()\n\nprediction = model.predict([X_test['left'], X_test['right']])\nprint(prediction)","metadata":{"execution":{"iopub.status.busy":"2021-10-25T15:52:22.697797Z","iopub.execute_input":"2021-10-25T15:52:22.698059Z","iopub.status.idle":"2021-10-25T16:00:46.514594Z","shell.execute_reply.started":"2021-10-25T15:52:22.698024Z","shell.execute_reply":"2021-10-25T16:00:46.513702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"index = list(range(0,len(prediction)))\npred = []\nfor i in range(0,len(prediction)):\n    pred.append(prediction[i][0])","metadata":{"execution":{"iopub.status.busy":"2021-10-25T16:00:46.552303Z","iopub.execute_input":"2021-10-25T16:00:46.552637Z","iopub.status.idle":"2021-10-25T16:00:46.814413Z","shell.execute_reply.started":"2021-10-25T16:00:46.5526Z","shell.execute_reply":"2021-10-25T16:00:46.813643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pandas import Series\nfrom numpy.random import randn\n\nsubmission = pd.DataFrame({\n    'test_id' : index,\n    'is_duplicate': pred\n})\n# submission.set_index('test_id', inplace=True)\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-10-25T16:00:46.815694Z","iopub.execute_input":"2021-10-25T16:00:46.815997Z","iopub.status.idle":"2021-10-25T16:00:48.375635Z","shell.execute_reply.started":"2021-10-25T16:00:46.81596Z","shell.execute_reply":"2021-10-25T16:00:48.374885Z"},"trusted":true},"execution_count":null,"outputs":[]}]}