{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk import pos_tag\nfrom nltk.corpus import wordnet\nfrom nltk.corpus import stopwords\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\nfrom keras.layers.normalization import BatchNormalization\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.layers import LSTM, Dense, Bidirectional\nfrom tensorflow.keras.layers import Dropout, SpatialDropout1D, TimeDistributed,Flatten, GlobalMaxPool1D\nfrom tensorflow.keras.layers import Embedding\nfrom keras.layers import multiply\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport re\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv('../input/quora-question-pairs/train.csv.zip')\ntest_data = pd.read_csv(\"../input/quora-question-pairs/test.csv.zip\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = train_data.sample(frac=0.1).reset_index(drop=True)\ntest_data = test_data.sample(frac=0.01).reset_index(drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.shape, test_data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = train_data.drop(['id','qid1','qid2'],axis=1)\ntest_data = test_data.drop(['test_id'],axis=1)\ntrain_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.is_duplicate.value_counts().plot(kind='bar')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data[train_data['is_duplicate']==1].shape,train_data[train_data['is_duplicate']==0].shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"is_duplicate = train_data[train_data['is_duplicate']==1]\nnot_duplicate = train_data[train_data['is_duplicate']==0].sample(14798)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = pd.concat([is_duplicate,not_duplicate])\ntrain_data = train_data.sample(frac=1).reset_index(drop=True)\ntrain_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.is_duplicate.value_counts().plot(kind='bar')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Lemmatize Words\n\ndef get_pos_tag(tag):\n    if tag.startswith('J'):\n        return wordnet.ADJ\n    elif tag.startswith('V'):\n        return wordnet.VERB\n    elif tag.startswith('N'):\n        return wordnet.NOUN\n    elif tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        # As default pos in lemmatization is Noun\n        return wordnet.NOUN\n\nlemmatizer = WordNetLemmatizer()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#cleaning the data now \n\nregex = [\n    r'<[^>]+>', #HTML tags\n    r'@(\\w+)', # @-mentions\n    r\"#(\\w+)\", # hashtags\n    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n    r'[^0-9a-z #+_\\\\r\\\\n\\\\t]', #BAD SYMBOLS\n]\n\nREPLACE_URLS = re.compile(r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+')\nREPLACE_HASH = re.compile(r'#(\\w+)')\nREPLACE_AT = re.compile(r'@(\\w+)')\nREPLACE_HTML_TAGS = re.compile(r'<[^>]+>')\n#REPLACE_DIGITS = re.compile(r'\\d+')\n#REPLACE_BY = re.compile(r\"[/(){}\\[\\]\\|,;.:?\\-\\'\\\"$]\")\nREPLACE_BY = re.compile(r\"[^a-z0-9\\-]\")\n\nSTOPWORDS = set(stopwords.words('english'))\n\n#tokens_re = re.compile(r'('+'|'.join(regex)+')', re.VERBOSE | re.IGNORECASE)\n\n# sentences = [] #for Word2Vec model\n\ndef clean_text(text):\n    text = text.lower()\n    text = REPLACE_HTML_TAGS.sub(' ', text)\n    text = REPLACE_URLS.sub('', text)\n    text = REPLACE_HASH.sub('', text)\n    text = REPLACE_AT.sub('', text)\n    #text = REPLACE_DIGITS.sub(' ', text)\n    text = REPLACE_BY.sub(' ', text)\n    \n    \n    text = \" \".join(lemmatizer.lemmatize(word.strip(), get_pos_tag(pos_tag([word.strip()])[0][1])) for word in text.split() if word not in STOPWORDS and len(word)>3)\n    \n    #sentences.append(text.split())\n    return text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_pos_tag(pos_tag(['playing'.strip()])[0][1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pos_tag(['word'.strip()])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lemmatizer.lemmatize('playing'.strip(), get_pos_tag(pos_tag(['playing'.strip()])[0][1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_pos_tag(pos_tag(['playing'.strip()])[0][1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lemmatizer.lemmatize('playable'.strip(),'v' )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data['q1'] = train_data['question1'].apply(clean_text)\ntrain_data['q2'] = train_data['question2'].apply(clean_text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data['q1'] = test_data['question1'].apply(clean_text)\ntest_data['q2'] = test_data['question2'].apply(clean_text)\ntest_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#max len of clean data\nmax_len_q1 = np.max(train_data[\"q1\"].apply(lambda x: len(x.split())))\nmax_len_q2 = np.max(train_data[\"q2\"].apply(lambda x: len(x.split())))\nmax_len_q1, max_len_q2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_NB_WORDS = 200000\n\ntokenizer = Tokenizer(num_words = MAX_NB_WORDS)\n\ntokenizer.fit_on_texts(list(train_data['q1'].values.astype(str))+list(train_data['q2'].values.astype(str)))\n\n\nX_train_q1 = tokenizer.texts_to_sequences(train_data['q1'].values.astype(str))\nX_train_q1 = pad_sequences(X_train_q1, maxlen = 30, padding = 'post')\n\nX_train_q2 = tokenizer.texts_to_sequences(train_data['q2'].values.astype(str))\nX_train_q2 = pad_sequences(X_train_q2, maxlen = 30, padding = 'post')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test_q1 = tokenizer.texts_to_sequences(test_data['q1'].ravel())\nX_test_q1 = pad_sequences(X_test_q1,maxlen = 30, padding='post')\n\nX_test_q2 = tokenizer.texts_to_sequences(test_data['q2'].astype(str).ravel())\nX_test_q2 = pad_sequences(X_test_q2, maxlen = 30, padding='post')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = train_data['is_duplicate']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab_size = len(tokenizer.word_index) + 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Loading Glove word embedding","metadata":{}},{"cell_type":"code","source":"embeddings_index = {}\nglovefile = open('../input/glove6b200d/glove.6B.200d.txt','r',encoding='utf-8')\nfor line in tqdm(glovefile):\n    values = line.split(\" \")\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n#     coefs.shape\n    embeddings_index[word] = coefs\nglovefile.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#creating embedding matrix for the words we have in the dataset\nembedding_matrix = np.zeros((len(tokenizer.word_index)+1, 200))\nfor words, index in tqdm(tokenizer.word_index.items()):\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[index] = embedding_vector","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_q1,X_valid_q1, X_train_q2, X_valid_q2, y_train, y_valid = train_test_split(X_train_q1, X_train_q2, y, test_size=0.20, random_state=12)\nprint(X_train_q1.shape, X_train_q2.shape, y_train.shape)\nprint(X_valid_q1.shape, X_valid_q2.shape, y_valid.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model for Q1\n\nmodel_q1 = Sequential([Embedding(input_dim = vocab_size,output_dim = 200,weights = [embedding_matrix],input_length = 30),\n                      LSTM(128, activation = 'tanh', return_sequences = True),\n                      Dropout(0.2),\n                      LSTM(128, return_sequences = True),\n                      LSTM(128),\n                      Dense(60, activation = 'tanh'),\n                      Dense(2, activation = 'sigmoid')])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model for Q2\n\nmodel_q2 = Sequential([Embedding(input_dim = vocab_size,output_dim = 200,weights = [embedding_matrix],input_length = 30),\n                      LSTM(128, activation = 'tanh', return_sequences = True),\n                      Dropout(0.2),\n                      LSTM(128, return_sequences = True),\n                      LSTM(128),\n                      Dense(60, activation = 'tanh'),\n                      Dense(2, activation = 'sigmoid')])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Merging the output of the two models,i.e, model_q1 and model_q2\nmergedOut = multiply([model_q1.output, model_q2.output])\n\nmergedOut = Flatten()(mergedOut)\nmergedOut = Dense(100, activation = 'relu')(mergedOut)\nmergedOut = Dropout(0.2)(mergedOut)\nmergedOut = Dense(50, activation = 'relu')(mergedOut)\nmergedOut = Dropout(0.2)(mergedOut)\nmergedOut = Dense(2, activation = 'sigmoid')(mergedOut)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_model = tf.keras.Model([model_q1.input, model_q2.input], mergedOut)\nnew_model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy',\n                 metrics = ['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = new_model.fit([X_train_q1,X_train_q2],y_train, batch_size = 2000, epochs = 20, validation_data = ([X_valid_q1,X_valid_q2],y_valid))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy = history.history[\"accuracy\"]\nloss = history.history[\"loss\"]\n\nval_accuracy = history.history[\"val_accuracy\"]\nval_loss = history.history[\"val_loss\"]\n\n#plot \nplt.plot(accuracy,'g',label='training accuracy')\nplt.plot(val_accuracy, 'r', label='validation accuracy')\nplt.legend()\nplt.show()\n\n\nplt.plot(loss,'g',label='training loss')\nplt.plot(val_loss, 'r', label='validation loss')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = new_model.predict([X_test_q1, X_test_q2])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.argmax(y_pred[:10], axis=-1)  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}