{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5ed21ba7-2a99-683e-6f6d-26c60b2c3a2e"},"outputs":[],"source":"import numpy as np\nimport pandas as pd\nimport xgboost as xgb\nimport datetime\nimport operator\nfrom sklearn.cross_validation import train_test_split\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nimport matplotlib.pyplot as plt\nfrom pylab import plot, show, subplot, specgram, imshow, savefig\n\nRS = 12357\nROUNDS = 315\n\nprint(\"Started\")\nnp.random.seed(RS)\ninput_folder = '../input/'"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"485df095-9e83-dab1-a2d7-d3b9a4581515"},"outputs":[],"source":"def train_xgb(X, y, params):\n\tprint(\"Will train XGB for {} rounds, RandomSeed: {}\".format(ROUNDS, RS))\n\tx, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=RS)\n\n\txg_train = xgb.DMatrix(x, label=y_train)\n\txg_val = xgb.DMatrix(X_val, label=y_val)\n\n\twatchlist  = [(xg_train,'train'), (xg_val,'eval')]\n\treturn xgb.train(params, xg_train, ROUNDS, watchlist)\n\ndef predict_xgb(clr, X_test):\n\treturn clr.predict(xgb.DMatrix(X_test))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"005d5c58-5735-475e-54cd-6187d90f4f62"},"outputs":[],"source":"def create_feature_map(features):\n\toutfile = open('xgb.fmap', 'w')\n\ti = 0\n\tfor feat in features:\n\t\toutfile.write('{0}\\t{1}\\tq\\n'.format(i, feat))\n\t\ti = i + 1\n\toutfile.close()\n\ndef add_word_count(x, df, word):\n\tx['q1_' + word] = df['question1'].apply(lambda x: (word in str(x).lower())*1)\n\tx['q2_' + word] = df['question2'].apply(lambda x: (word in str(x).lower())*1)\n\tx[word + '_both'] = x['q1_' + word] * x['q2_' + word]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5711bf16-be6b-ca77-2d02-1c49e51ca493"},"outputs":[],"source":"params = {}\nparams['objective'] = 'binary:logistic'\nparams['eval_metric'] = 'logloss'\nparams['eta'] = 0.11\nparams['max_depth'] = 5\nparams['silent'] = 1\nparams['seed'] = RS\n\ndf_train = pd.read_csv(input_folder + 'train.csv')\ndf_test  = pd.read_csv(input_folder + 'test.csv')\nprint(\"Original data: X_train: {}, X_test: {}\".format(df_train.shape, df_test.shape))\n\nprint(\"Features processing, be patient...\")\n\n# If a word appears only once, we ignore it completely (likely a typo)\n# Epsilon defines a smoothing constant, which makes the effect of extremely rare words smaller\ndef get_weight(count, eps=10000, min_count=2):\n\treturn 0 if count < min_count else 1 / (count + eps)\ntrain_qs = pd.Series(df_train['question1'].tolist() + df_train['question2'].tolist()).astype(str)\nwords = (\" \".join(train_qs)).lower().split()\ncounts = Counter(words)\nweights = {word: get_weight(count) for word, count in counts.items()}\nstops = set(stopwords.words(\"english\"))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bc96a341-0a26-95a4-fefc-77505efd353e"},"outputs":[],"source":"def word_shares(row):\n\tq1_list = str(row['question1']).lower().split()\n\tq1 = set(q1_list)\n\tq1words = q1.difference(stops)\n\tif len(q1words) == 0:\n\t\treturn '0:0:0:0:0:0:0:0'\n        \n\tq2_list = str(row['question2']).lower().split()\n\tq2 = set(q2_list)\n\tq2words = q2.difference(stops)\n\tif len(q2words) == 0:\n\t\treturn '0:0:0:0:0:0:0:0'\n\n\twords_hamming = sum(1 for i in zip(q1_list, q2_list) if i[0]==i[1])/max(len(q1_list), len(q2_list))\n\n\tq1stops = q1.intersection(stops)\n\tq2stops = q2.intersection(stops)\n\tq1_2gram = set([i for i in zip(q1_list, q1_list[1:])])\n\tq2_2gram = set([i for i in zip(q2_list, q2_list[1:])])\n\tshared_2gram = q1_2gram.intersection(q2_2gram)\n\tshared_words = q1words.intersection(q2words)\n\tshared_weights = [weights.get(w, 0) for w in shared_words]\n\tq1_weights = [weights.get(w, 0) for w in q1words]\n\tq2_weights = [weights.get(w, 0) for w in q2words]\n\ttotal_weights = q1_weights + q1_weights\n\t\n\tR1 = np.sum(shared_weights) / np.sum(total_weights) #tfidf share\n\tR2 = len(shared_words) / (len(q1words) + len(q2words) - len(shared_words)) #count share\n\tR31 = len(q1stops) / len(q1words) #stops in q1\n\tR32 = len(q2stops) / len(q2words) #stops in q2\n\tRcosine_denominator = (np.sqrt(np.dot(q1_weights,q1_weights))*np.sqrt(np.dot(q2_weights,q2_weights)))\n\tRcosine = np.dot(shared_weights, shared_weights)/Rcosine_denominator\n\tif len(q1_2gram) + len(q2_2gram) == 0:\n\t\tR2gram = 0\n\telse:\n\t\tR2gram = len(shared_2gram) / (len(q1_2gram) + len(q2_2gram))\n\treturn '{}:{}:{}:{}:{}:{}:{}:{}'.format(R1, R2, len(shared_words), R31, R32, R2gram, Rcosine, words_hamming)\n\ndf = pd.concat([df_train, df_test])\ndf['word_shares'] = df.apply(word_shares, axis=1, raw=True)\nx = pd.DataFrame()\n\nx['word_match']       = df['word_shares'].apply(lambda x: float(x.split(':')[0]))\nx['word_match_2root'] = np.sqrt(x['word_match'])\nx['tfidf_word_match'] = df['word_shares'].apply(lambda x: float(x.split(':')[1]))\nx['shared_count']     = df['word_shares'].apply(lambda x: float(x.split(':')[2]))\n\nx['stops1_ratio']     = df['word_shares'].apply(lambda x: float(x.split(':')[3]))\nx['stops2_ratio']     = df['word_shares'].apply(lambda x: float(x.split(':')[4]))\nx['shared_2gram']     = df['word_shares'].apply(lambda x: float(x.split(':')[5]))\nx['cosine']           = df['word_shares'].apply(lambda x: float(x.split(':')[6]))\nx['words_hamming']    = df['word_shares'].apply(lambda x: float(x.split(':')[7]))\nx['diff_stops_r']     = x['stops1_ratio'] - x['stops2_ratio']\n\nx['len_q1'] = df['question1'].apply(lambda x: len(str(x)))\nx['len_q2'] = df['question2'].apply(lambda x: len(str(x)))\nx['diff_len'] = x['len_q1'] - x['len_q2']\n\t\nx['caps_count_q1'] = df['question1'].apply(lambda x:sum(1 for i in str(x) if i.isupper()))\nx['caps_count_q2'] = df['question2'].apply(lambda x:sum(1 for i in str(x) if i.isupper()))\nx['diff_caps'] = x['caps_count_q1'] - x['caps_count_q2']\n\nx['len_char_q1'] = df['question1'].apply(lambda x: len(str(x).replace(' ', '')))\nx['len_char_q2'] = df['question2'].apply(lambda x: len(str(x).replace(' ', '')))\nx['diff_len_char'] = x['len_char_q1'] - x['len_char_q2']\n\nx['len_word_q1'] = df['question1'].apply(lambda x: len(str(x).split()))\nx['len_word_q2'] = df['question2'].apply(lambda x: len(str(x).split()))\nx['diff_len_word'] = x['len_word_q1'] - x['len_word_q2']\n\nx['avg_world_len1'] = x['len_char_q1'] / x['len_word_q1']\nx['avg_world_len2'] = x['len_char_q2'] / x['len_word_q2']\nx['diff_avg_word'] = x['avg_world_len1'] - x['avg_world_len2']\n\nx['exactly_same'] = (df['question1'] == df['question2']).astype(int)\nx['duplicated'] = df.duplicated(['question1','question2']).astype(int)\nadd_word_count(x, df,'how')\nadd_word_count(x, df,'what')\nadd_word_count(x, df,'which')\nadd_word_count(x, df,'who')\nadd_word_count(x, df,'where')\nadd_word_count(x, df,'when')\nadd_word_count(x, df,'why')\n\nprint(x.columns)\nprint(x.describe())\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"66ef2a44-cf42-8126-cd9f-d9ca06c3acea"},"outputs":[],"source":"feature_names = list(x.columns.values)\ncreate_feature_map(feature_names)\nprint(\"Features: {}\".format(feature_names))\n\nx_train = x[:df_train.shape[0]]\nx_test  = x[df_train.shape[0]:]\ny_train = df_train['is_duplicate'].values\ndel x, df_train\n\nif 1: # Now we oversample the negative class - on your own risk of overfitting!\n\tpos_train = x_train[y_train == 1]\n\tneg_train = x_train[y_train == 0]\n\n\tprint(\"Oversampling started for proportion: {}\".format(len(pos_train) / (len(pos_train) + len(neg_train))))\n\tp = 0.165\n\tscale = ((len(pos_train) / (len(pos_train) + len(neg_train))) / p) - 1\n\twhile scale > 1:\n\t\tneg_train = pd.concat([neg_train, neg_train])\n\t\tscale -=1\n\tneg_train = pd.concat([neg_train, neg_train[:int(scale * len(neg_train))]])\n\tprint(\"Oversampling done, new proportion: {}\".format(len(pos_train) / (len(pos_train) + len(neg_train))))\n\n\tx_train = pd.concat([pos_train, neg_train])\n\ty_train = (np.zeros(len(pos_train)) + 1).tolist() + np.zeros(len(neg_train)).tolist()\n\tdel pos_train, neg_train\n\t\nprint(\"Training data: X_train: {}, Y_train: {}, X_test: {}\".format(x_train.shape, len(y_train), x_test.shape))\nclr = train_xgb(x_train, y_train, params)\npreds = predict_xgb(clr, x_test)\n\nprint(\"Writing output...\")\nsub = pd.DataFrame()\nsub['test_id'] = df_test['test_id']\nsub['is_duplicate'] = preds *.75\nsub.to_csv(\"xgb_seed{}_n{}.csv\".format(RS, ROUNDS), index=False)\n\nprint(\"Features importances...\")\nimportance = clr.get_fscore(fmap='xgb.fmap')\nimportance = sorted(importance.items(), key=operator.itemgetter(1))\nft = pd.DataFrame(importance, columns=['feature', 'fscore'])\nft.plot(kind='barh', x='feature', y='fscore', legend=False, figsize=(10, 25))\nplt.gcf().savefig('features_importance.png')"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}