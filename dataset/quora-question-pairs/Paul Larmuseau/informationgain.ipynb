{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"36befce7-0609-e22c-c90e-221440ab5323"},"source":"\nhttp://streamhacker.com/2010/06/16/text-classification-sentiment-analysis-eliminate-low-information-features/"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bd39b885-0aa8-bc4f-17e8-fb5087a4e251"},"outputs":[],"source":"import time\nstart = time.clock()\n\n#open data\nimport pandas as pd\nimport numpy as np\nimport nltk\nimport codecs\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\n\n\nSampleSize=100000\n\ndatas = pd.read_csv('../input/train.csv')\ndatas = datas[300000:300000+SampleSize]\n\n\n#datas=datas[datas['is_duplicate'] == 1]\n#datas=datas.sample(SampleSize)\n#datas=datas[0:SampleSize]\ndatas = datas.fillna('leeg')\n\n\ndef cleantxt(x):    # aangeven sentence\n    x = x.lower()\n    # Removing non ASCII chars\n    x = x.replace(r'[^\\x00-\\x7f]',r' ')\n    # Pad punctuation with spaces on both sides\n    for char in ['.', '\"', ',', '(', ')', '!', '?', ';', ':']:\n        x = x.replace(char, ' ' + char + ' ')\n    return x\n\ndatas['question1']=datas['question1'].map(cleantxt)\ndatas['question2']=datas['question2'].map(cleantxt)\n\nend = time.clock()\nprint('open:',end-start)\n\ndef difwords(row):\n    q1words = str(row[3]).lower().split()\n    q2words = str(row[4]).lower().split()\n    equq1 = [w for w in q1words if w in q2words]\n    difq1 = [w for w in q1words if w not in q2words]\n    difq2 = [w for w in q2words if w not in q1words]\n    signw = [i for i in difq1+difq2 if i not in stop]\n    return ' '.join(signw)\n\ndef lengte(row):\n    return len(row[6].split())\n\n\n# all questions with only one word different, leading to a 0 or a 1\n\ndatas['diff'] = datas.apply(difwords, axis=1, raw=True)\ndatas['tell']= datas.apply(lengte, axis=1, raw=True)\nuni_diff=datas[datas['tell']>0]\nuni_w1=uni_diff[uni_diff['is_duplicate']==1]\nuni_w0=uni_diff[uni_diff['is_duplicate']==0]\nprint(len(uni_w1))\n#print('non differecings words',' '.join(uni_w1['diff']))\nprint(len(uni_w0))\n#print('differencing words',' '.join(uni_w0['diff']))\n\n\n# all questions with two words different, leading to a 0 or a 1\n\nuni_diff=datas[datas['tell']==2]\nend = time.clock()\nprint('finding relevant and non relevant words:',end-start)\nprint(uni_w1.head())\nprint(uni_w0.head())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"74a137e9-d003-6391-2139-80b12bdf2796"},"outputs":[],"source":"from nltk.tokenize import word_tokenize\nfrom nltk.probability import FreqDist\nfdist = FreqDist(' '.join(uni_w1['diff']))\nprint(fdist)\nfdist.most_common(50)\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"35eaf533-1789-adc9-340a-6fbcfb457b0a"},"outputs":[],"source":"from nltk.tokenize import word_tokenize\nfrom nltk.probability import FreqDist,LaplaceProbDist\nfdist_pos = LaplaceProbDist(word.lower() for word in word_tokenize(' '.join(uni_w1['diff'])))\nfdist_neg = LaplaceProbDist(word.lower() for word in word_tokenize(' '.join(uni_w0['diff'])))\nprint(fdist_pos.most_common(200))\nprint(fdist_neg.most_common(200))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ae7112d9-723c-4fdb-fb6f-08fa4e48de56"},"outputs":[],"source":"def unusual_words(text):\n    text_vocab = set(w.lower() for w in text if w.isalpha())\n    english_vocab = set(w.lower() for w in nltk.corpus.words.words())\n    unusual = text_vocab - english_vocab\n    return sorted(unusual)\n\ntext=' '.join(datas['question1'])\nunusual_words(text.split())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4e2e7764-47ad-ecd1-e454-b041b8b9a9b5"},"outputs":[],"source":"print(datas.head())\nimport collections, itertools\nfrom nltk.metrics import precision,recall\nimport nltk.classify.util\nfrom nltk.classify import NaiveBayesClassifier\nfrom nltk.corpus import stopwords\nfrom nltk.collocations import BigramCollocationFinder\nfrom nltk.metrics import BigramAssocMeasures, scores\nfrom nltk.probability import FreqDist, ConditionalFreqDist\n\nnegx=[]\nposx=[]\ndef evaluate_classifier(featx):\n    negids = uni_w0\n    posids = uni_w1\n\n    for xi in range (len(uni_w0)):\n        words= uni_w0.iloc[xi].question1.split() + uni_w0.iloc[xi].question2.split()\n        negx.append(word_feats(words))\n    negy=[]\n    for yi in range (len(negx)):\n        negy.append('neg')\n    negfeats=list(zip(negx,negy))\n\n    for xi in range (len(uni_w1)):\n        words= uni_w1.iloc[xi].question1.split() + uni_w1.iloc[xi].question2.split()\n        posx.append(word_feats(words))\n    posy=[]\n    for yi in range (len(posx)):\n        posy.append('pos')\n    posfeats=list(zip(posx,posy))\n\n \n    negcutoff = int(len(negfeats)*3/4)\n    poscutoff = int(len(posfeats)*3/4)\n    print(negcutoff)\n    trainfeats = negfeats[:negcutoff] + posfeats[:poscutoff]\n    testfeats = negfeats[negcutoff:] + posfeats[poscutoff:]\n    end = time.clock()\n    print('sampling:',end-start)\n    print(negfeats[:10])\n \n    classifier = NaiveBayesClassifier.train(trainfeats)\n    refsets = collections.defaultdict(set)\n    testsets = collections.defaultdict(set)\n \n    for i, (feats, label) in enumerate(testfeats):\n            refsets[label].add(i)\n            observed = classifier.classify(feats)\n            testsets[observed].add(i)\n \n    print ('accuracy:', nltk.classify.util.accuracy(classifier, testfeats))\n    #print ('pos precision:', nltk.metrics.precision(refsets['pos'], testsets['pos']))\n    #print ('pos recall:', nltk.metrics.recall(refsets['pos'], testsets['pos']))\n    #print ('neg precision:', nltk.metrics.precision(refsets['neg'], testsets['neg']))\n    #print ('neg recall:', nltk.metrics.recall(refsets['neg'], testsets['neg']))\n    classifier.show_most_informative_features()\n \ndef word_feats(words):\n    return dict([(word, True) for word in words])\n \nprint ('evaluating single word features')\nevaluate_classifier(word_feats)\n \nword_fd = []\n#FreqDist(' '.join(datas['question1']) + ' '.join(datas['question2']) )\n\n\n    \nlabel_word_fd = ConditionalFreqDist()\n\nfor word in posx:\n    word_fd[word]+=1\n    label_word_fd['pos'][word]+=1\n\nfor word in negx:\n    word_fd[word]+=1\n    label_word_fd['neg'][word]+=1\n\nend = time.clock()\nprint('freq distr:',end-start)\n\npos_word_count = label_word_fd['pos'].N()\nneg_word_count = label_word_fd['neg'].N()\ntotal_word_count = pos_word_count + neg_word_count\n \nword_scores = {}\n \nfor word, freq in word_fd.iteritems():\n    pos_score = BigramAssocMeasures.chi_sq(label_word_fd['pos'][word],\n        (freq, pos_word_count), total_word_count)\n    neg_score = BigramAssocMeasures.chi_sq(label_word_fd['neg'][word],\n        (freq, neg_word_count), total_word_count)\n    word_scores[word] = pos_score + neg_score\n \nbest = sorted([word_scores(v,k) for (k,v) in d.items()], reverse=True)\nprint(best)\nbestwords = set([w for w, s in best])\n \ndef best_word_feats(words):\n    return dict([(word, True) for word in words if word in bestwords])\n \nprint ('evaluating best word features')\nevaluate_classifier(best_word_feats)\n \ndef best_bigram_word_feats(words, score_fn=BigramAssocMeasures.chi_sq, n=200):\n    bigram_finder = BigramCollocationFinder.from_words(words)\n    bigrams = bigram_finder.nbest(score_fn, n)\n    d = dict([(bigram, True) for bigram in bigrams])\n    d.update(best_word_feats(words))\n    return d\n \nprint ('evaluating best words + bigram chi_sq word features')\nevaluate_classifier(best_bigram_word_feats)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8a298b09-2c9f-ab65-45ef-3c4a0cb82e01"},"outputs":[],"source":"import collections, itertools\nimport nltk.classify.util, nltk.metrics\nfrom nltk.classify import NaiveBayesClassifier\nfrom nltk.corpus import movie_reviews, stopwords\nfrom nltk.collocations import BigramCollocationFinder\nfrom nltk.metrics import BigramAssocMeasures\nfrom nltk.probability import FreqDist, ConditionalFreqDist\n \ndef evaluate_classifier(featx):\n    negids = movie_reviews.fileids('neg')\n    posids = movie_reviews.fileids('pos')\n \n    negfeats = [(featx(movie_reviews.words(fileids=[f])), 'neg') for f in negids]\n    posfeats = [(featx(movie_reviews.words(fileids=[f])), 'pos') for f in posids]\n \n    negcutoff = len(negfeats)*3/4\n    poscutoff = len(posfeats)*3/4\n \n    trainfeats = negfeats[0:10] + posfeats[0:10]\n\n    \n    print(trainfeats)\n\ndef word_feats(words):\n    return dict([(word, True) for word in words])\n \nprint ('evaluating single word features')\nevaluate_classifier(word_feats)\n \nword_fd = FreqDist()\nlabel_word_fd = ConditionalFreqDist()\n \nfor word in movie_reviews.words(categories=['pos']):\n    word_fd.inc(word.lower())\n    label_word_fd['pos'].inc(word.lower())\n \nfor word in movie_reviews.words(categories=['neg']):\n    word_fd.inc(word.lower())\n    label_word_fd['neg'].inc(word.lower())\n \n# n_ii = label_word_fd[label][word]\n# n_ix = word_fd[word]\n# n_xi = label_word_fd[label].N()\n# n_xx = label_word_fd.N()\n \npos_word_count = label_word_fd['pos'].N()\nneg_word_count = label_word_fd['neg'].N()\ntotal_word_count = pos_word_count + neg_word_count\n\nevaluate_classifier(best_word_feats)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"156c3509-daa1-fb11-7fcb-0fb0515efe84"},"outputs":[],"source":"from nltk.corpus import stopwords # Import the stop word list\n\nfrom nltk import word_tokenize, ngrams\n#print stopwords.words(\"english\") simil_d=[]\n\n\nsimil_c=[]\nsimil_i=[]\nsimil_b=[]\nsimil_d=[]\nfor xyz in range(0,1000):\n    q1=datas.iloc[xyz].question1\n    q2=datas.iloc[xyz].question2\n    uni_sent1 = nltk.wordpunct_tokenize(q1) #tokenize sentence\n    uni_sent2 = nltk.wordpunct_tokenize(q2) \n    #sims=pd.DataFrame(data=None, index=uni_sent1, columns=uni_sent2)  #abs(np.random.randn(len(sent1), len(sent2))/1000000)\n    equq1 = [w for w in uni_sent1 if w in uni_sent2]\n    difq1 = [w for w in uni_sent1 if w not in uni_sent2]\n    difq2 = [w for w in uni_sent2 if w not in uni_sent1]\n    diftot = difq1+difq2\n    difton = [w for w in diftot if not w in stopwords.words(\"english\")]\n    Q2no = [w for w in uni_sent2 if not w in stopwords.words(\"english\")]\n\n    canar=len(equq1)/(len(equq1)+len(diftot))\n    simil_c.append(canar)\n    \n    \n\n    if len(difton)==0 and datas.iloc[xyz].is_duplicate==0:\n        simil_d.append(2)\n    elif ('not' in uni_sent1) != ('not' in uni_sent2) :\n        simil_d.append(3)\n    #elif Q2no<3 :\n    #    simil_d.append(4)\n    else:\n        simil_d.append(datas.iloc[xyz].is_duplicate)\n        \nend = time.clock()\nprint('first canary:',end-start)\n  \n "}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}