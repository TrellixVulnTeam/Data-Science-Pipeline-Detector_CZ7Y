{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"f140ced8-9c25-0628-8b78-89e0498bf697"},"source":""},{"cell_type":"markdown","metadata":{"_cell_guid":"60a22c80-73a7-81b9-54fc-0b9e6437284a"},"source":"**Open data as usual**\n========"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"397bb2ef-f186-3bb5-5a38-366d8496d22d"},"outputs":[],"source":"import pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom textblob import TextBlob\nfrom textblob.np_extractors import ConllExtractor\nextractor = ConllExtractor()\n\n#NLTK functioncs\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.corpus import wordnet\n\n# timing function\nimport time   \nstart = time.clock() #_________________ measure efficiency timing\n\n\n# read data\n#train = pd.read_csv('c:/py/testQ.csv',encoding='utf8')#[:1000]  #_______________________ open data files\ntrain = pd.read_csv('../input/train.csv',encoding='utf8')\nprint(train.head(30))\ntrain.fillna(value='leeg',inplace=True)\nend = time.clock()\nprint('open:',end-start)\n\n\nfor xi in range (0,30):\n    q1= TextBlob(train.iloc[xi].question1)\n    q2= TextBlob(train.iloc[xi].question2)\n    blob1 = TextBlob(train.iloc[xi].question1, np_extractor=extractor)\n    blob2 = TextBlob(train.iloc[xi].question2, np_extractor=extractor) \n    print(blob1.noun_phrases)\n    print(blob2.noun_phrases)    \n    print(q1.tags)\n    print(q2.tags)\n    print(q1.correct())\n    print(q2.correct())\n    print(q1.noun_phrases)\n    print(q2.noun_phrases)    \n    print(q1.sentiment)\n    print(q2.sentiment)    \n    print(q1.sentiment.polarity)\n    print(q2.sentiment.polarity)    "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"809f0ad4-49ae-51db-d812-612c75163604"},"outputs":[],"source":"import time\nstart = time.clock()\n\n#open data\nimport pandas as pd\nimport numpy as np\nimport nltk\nimport codecs\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\n\n\nSampleSize=100000\n\ndatas = pd.read_csv('../input/train.csv')\ndatas = datas[300000:300000+SampleSize]\n\n\n#datas=datas[datas['is_duplicate'] == 1]\n#datas=datas.sample(SampleSize)\n#datas=datas[0:SampleSize]\ndatas = datas.fillna('leeg')\n\n\ndef cleantxt(x):    # aangeven sentence\n    x = x.lower()\n    # Removing non ASCII chars\n    x = x.replace(r'[^\\x00-\\x7f]',r' ')\n    # Pad punctuation with spaces on both sides\n    for char in ['.', '\"', ',', '(', ')', '!', '?', ';', ':']:\n        x = x.replace(char, ' ' + char + ' ')\n    return x\n\ndatas['question1']=datas['question1'].map(cleantxt)\ndatas['question2']=datas['question2'].map(cleantxt)\n\nend = time.clock()\nprint('open:',end-start)\n\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"6f1f5755-7045-e5f9-8a36-a47683412315"},"source":"Finding garbage words...\n----------\nso the first thing i could advise our dear Quora website developers is to create a 'spellcheck' function on their question formulation zone... Since people have only like in a twitter message say 200 letters to ask a question, every word seems to be important. Hence every word could better be spelled right..."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"975f7e46-b494-07b2-8e9e-c7477c48c830"},"outputs":[],"source":"def unusual_words(text):\n    text_vocab = set(w.lower() for w in text if w.isalpha())\n    english_vocab = set(w.lower() for w in nltk.corpus.words.words())\n    unusual = text_vocab - english_vocab\n    return sorted(unusual)\n\ntext=' '.join(datas['question1'])\n#unusual_words(text.split())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6c905fce-25ec-47ea-b953-7588db1f6233"},"outputs":[],"source":"import re\nfrom collections import Counter\n\ndef words(text): return re.findall(r'\\w+', text.lower())\n\nWORDS = Counter(words(open('big.txt').read()))\n\ndef P(word, N=sum(WORDS.values())): \n    \"Probability of `word`.\"\n    return WORDS[word] / N\n\ndef correction(word): \n    \"Most probable spelling correction for word.\"\n    return max(candidates(word), key=P)\n\ndef candidates(word): \n    \"Generate possible spelling corrections for word.\"\n    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n\ndef known(words): \n    \"The subset of `words` that appear in the dictionary of WORDS.\"\n    return set(w for w in words if w in WORDS)\n\ndef edits1(word):\n    \"All edits that are one edit away from `word`.\"\n    letters    = 'abcdefghijklmnopqrstuvwxyz'\n    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n    deletes    = [L + R[1:]               for L, R in splits if R]\n    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n    inserts    = [L + c + R               for L, R in splits for c in letters]\n    return set(deletes + transposes + replaces + inserts)\n\ndef edits2(word): \n    \"All edits that are two edits away from `word`.\"\n    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9452134f-f639-bd47-7ebf-400e3d644813"},"outputs":[],"source":"# all questions with only one word different, leading to a 0 or a 1\ndef difwords(row):\n    q1words = str(row[3]).lower().split()\n    q2words = str(row[4]).lower().split()\n    equq1 = [w for w in q1words if w in q2words]\n    difq1 = [w for w in q1words if w not in q2words]\n    difq2 = [w for w in q2words if w not in q1words]\n    signw = [i for i in difq1+difq2 if i not in stop]\n    return ' '.join(signw)\n\ndef lengte(row):\n    return len(row[6].split())\n\n\ndatas['diff'] = datas.apply(difwords, axis=1, raw=True)\ndatas['tell']= datas.apply(lengte, axis=1, raw=True)\nuni_diff=datas[datas['tell']>0]\nuni_w1=uni_diff[uni_diff['is_duplicate']==1]\nuni_w0=uni_diff[uni_diff['is_duplicate']==0]\nprint(len(uni_w1))\n#print('non differecings words',' '.join(uni_w1['diff']))\nprint(len(uni_w0))\n#print('differencing words',' '.join(uni_w0['diff']))\n\n\ndef document_features(document):\n    document_words = set(document)\n    features = {}\n    for word in word_features:\n        features['contains({})'.format(word)] = (word in document_words)\n    return features\n\n\n#poswords=' '.join(uni_w1['diff'])\n#negwords=' '.join(uni_w0['diff'])\nprint(uni_w1.head())\ndocuments = [(list(uni_w1.ix[qnr].question1.split()), 'pos') for qnr in uni_w1['id']]\ndocuments.append([(list(uni_w1.ix[qnr].question2.split()), 'pos') for qnr in uni_w1['id']])\ndocuments.append([(list(uni_w0.ix[qnr].question1.split()), 'neg') for qnr in uni_w0['id']])\ndocuments.append([(list(uni_w0.ix[qnr].question2.split()), 'neg') for qnr in uni_w0['id']])\nfeaturesets = [(document_features(d), c) for (d,c) in documents]\ntrain_set = featuresets[100:]\nclassifier = nltk.NaiveBayesClassifier.train(train_set)\nclassifier.show_most_informative_features(100)\n\n# all questions with two words different, leading to a 0 or a 1\n\nuni_diff=datas[datas['tell']==2]\nend = time.clock()\nprint('finding relevant and non relevant words:',end-start)\nprint(uni_w1.head())\nprint(uni_w0.head())"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}