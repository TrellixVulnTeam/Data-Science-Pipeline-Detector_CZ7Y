{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e7d4aa51-afc3-ef96-11e6-f944b9bdda73"},"outputs":[],"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\nfrom sklearn.model_selection import train_test_split\n\ndef read_data():\n    df = pd.read_csv(\"../input/train.csv\")\n    print (\"Shape of base training File = \", df.shape)\n    # Remove missing values and duplicates from training data\n    df.drop_duplicates(inplace=True)\n    df.dropna(inplace=True)\n    print(\"Shape of base training data after cleaning = \", df.shape)\n    return df\n\ndf = read_data()\ndf_train, df_test = train_test_split(df, test_size = 0.02)\nprint (df_train.head(2))\nprint (df_test.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ad969c65-aac3-f40c-4650-b50ccc143fa6"},"outputs":[],"source":"from collections import Counter\nimport matplotlib.pyplot as plt\nimport operator\n\ndef eda(df):\n    print (\"Duplicate Count = %s , Non Duplicate Count = %s\" \n           %(df.is_duplicate.value_counts()[1],df.is_duplicate.value_counts()[0]))\n    \n    question_ids_combined = df.qid1.tolist() + df.qid2.tolist()\n    \n    print (\"Unique Questions = %s\" %(len(np.unique(question_ids_combined))))\n    \n    question_ids_counter = Counter(question_ids_combined)\n    sorted_question_ids_counter = sorted(question_ids_counter.items(), key=operator.itemgetter(1))\n    question_appearing_more_than_once = [i for i in question_ids_counter.values() if i > 1]\n    print (\"Count of Quesitons appearing more than once = %s\" %(len(question_appearing_more_than_once)))\n    \n    \neda(df_train)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9eb1f3bb-0ba9-bef3-487b-1ad5a440abf9"},"outputs":[],"source":"import re\nimport gensim\nfrom gensim import corpora\nfrom nltk.corpus import stopwords\n\nwords = re.compile(r\"\\w+\",re.I)\nstopword = stopwords.words('english')\n\ndef tokenize_questions(df):\n    question_1_tokenized = []\n    question_2_tokenized = []\n\n    for q in df.question1.tolist():\n        question_1_tokenized.append([i.lower() for i in words.findall(q) if i not in stopword])\n\n    for q in df.question2.tolist():\n        question_2_tokenized.append([i.lower() for i in words.findall(q) if i not in stopword])\n\n    df[\"Question_1_tok\"] = question_1_tokenized\n    df[\"Question_2_tok\"] = question_2_tokenized\n    \n    return df\n\ndef train_dictionary(df):\n    \n    questions_tokenized = df.Question_1_tok.tolist() + df.Question_2_tok.tolist()\n    \n    dictionary = corpora.Dictionary(questions_tokenized)\n    dictionary.filter_extremes(no_below=5, no_above=0.5, keep_n=10000000)\n    dictionary.compactify()\n    \n    return dictionary\n    \ndf_train = tokenize_questions(df_train)\ndictionary = train_dictionary(df_train)\nprint (\"No of words in the dictionary = %s\" %len(dictionary.token2id))\n\ndf_test = tokenize_questions(df_test)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c6c05814-8314-8af4-cabc-b39c51a58293"},"outputs":[],"source":"def get_vectors(df, dictionary):\n    \n    question1_vec = [dictionary.doc2bow(text) for text in df.Question_1_tok.tolist()]\n    question2_vec = [dictionary.doc2bow(text) for text in df.Question_2_tok.tolist()]\n    \n    question1_csc = gensim.matutils.corpus2csc(question1_vec, num_terms=len(dictionary.token2id))\n    question2_csc = gensim.matutils.corpus2csc(question2_vec, num_terms=len(dictionary.token2id))\n    \n    print(question1_csc)\n    return question1_csc.transpose(),question2_csc.transpose()\n\n\nq1_csc, q2_csc = get_vectors(df_train, dictionary)\n\nprint (q1_csc.shape)\nprint (q2_csc.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8b90d4fd-5c98-9f58-3e6e-1ed968b6f869"},"outputs":[],"source":"from sklearn.metrics.pairwise import cosine_similarity as cs\nfrom sklearn.metrics.pairwise import manhattan_distances as md\nfrom sklearn.metrics.pairwise import euclidean_distances as ed\nfrom sklearn.metrics import jaccard_similarity_score as jsc\nfrom sklearn.metrics.pairwise import polynomial_kernel as poke\nfrom sklearn.metrics.pairwise import rbf_kernel as rbfk\nfrom sklearn.neighbors import DistanceMetric\nfrom sklearn.preprocessing import MinMaxScaler\n\nminkowski_dis = DistanceMetric.get_metric('minkowski')\nmms_scale_man = MinMaxScaler()\nmms_scale_euc = MinMaxScaler()\nmms_scale_mink = MinMaxScaler()\n\ndef get_similarity_values(q1_csc, q2_csc):\n    cosine_sim = []\n    manhattan_dis = []\n    eucledian_dis = []\n    jaccard_dis = []\n    minkowsk_dis = []\n    polynomial_kern =[]\n    rbf_kern =[]\n    \n    for i,j in zip(q1_csc, q2_csc):\n        sim = cs(i,j)\n        cosine_sim.append(sim[0][0])\n        sim = md(i,j)\n        manhattan_dis.append(sim[0][0])\n        sim = ed(i,j)\n        eucledian_dis.append(sim[0][0])\n        sim = poke(i,j)*1000-1000\n        polynomial_kern.append(sim[0][0])        \n        sim = 1000-rbfk(i,j)*1000\n        rbf_kern.append(sim[0][0])\n        i_ = i.toarray()\n        j_ = j.toarray()\n        try:\n            sim = jsc(i_,j_)\n            jaccard_dis.append(sim)\n        except:\n            jaccard_dis.append(0)\n            \n        sim = minkowski_dis.pairwise(i_,j_)\n        minkowsk_dis.append(sim[0][0])\n    \n    return cosine_sim, manhattan_dis, eucledian_dis, jaccard_dis, minkowsk_dis,polynomial_kern,rbf_kern   \n\n\n# cosine_sim = get_cosine_similarity(q1_csc, q2_csc)\ncosine_sim, manhattan_dis, eucledian_dis, jaccard_dis, minkowsk_dis,polynomial_kern,rbf_kern = get_similarity_values(q1_csc[0:1000,:], q2_csc[0:1000,:])\nprint (\"cosine_sim sample= \\n\", cosine_sim[0:2])\nprint (\"manhattan_dis sample = \\n\", manhattan_dis[0:2])\nprint (\"eucledian_dis sample = \\n\", eucledian_dis[0:2])\nprint (\"jaccard_dis sample = \\n\", jaccard_dis[0:2])\nprint (\"minkowsk_dis sample = \\n\", minkowsk_dis[0:2])\nprint (\"polynomial kern sample = \\n\", polynomial_kern[0:2])\nprint (\"rbf_kern sample = \\n\", rbf_kern[0:2])\n\neucledian_dis_array = np.array(eucledian_dis).reshape(-1,1)\nmanhattan_dis_array = np.array(manhattan_dis).reshape(-1,1)\nminkowsk_dis_array = np.array(minkowsk_dis).reshape(-1,1)\n    \nmms_scale_man.fit(manhattan_dis_array)\nmms_scale_euc.fit(eucledian_dis_array)\nmms_scale_mink.fit(minkowsk_dis_array)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8c4e9eb2-f52a-950e-8ddb-14110f64ba7b"},"outputs":[],"source":"from sklearn.metrics import log_loss\n\ndef calculate_logloss(y_true, y_pred):\n    loss_cal = log_loss(y_true, y_pred)\n    return loss_cal\n\nq1_csc_test, q2_csc_test = get_vectors(df_test, dictionary)\ny_pred_cos, y_pred_man, y_pred_euc, y_pred_jac, y_pred_mink, y_pred_poly, y_pred_rbf = get_similarity_values(q1_csc_test, q2_csc_test)\ny_true = df_test.is_duplicate.tolist()\n\ny_pred_man_array = mms_scale_man.transform(np.array(y_pred_man).reshape(-1,1))\ny_pred_man = y_pred_man_array.tolist()\n\ny_pred_euc_array = mms_scale_euc.transform(np.array(y_pred_euc).reshape(-1,1))\ny_pred_euc = y_pred_euc_array.tolist()\n\ny_pred_mink_array = mms_scale_mink.transform(np.array(y_pred_mink).reshape(-1,1))\ny_pred_mink = y_pred_mink_array.tolist()\n\nlogloss = calculate_logloss(y_true, y_pred_cos)\nprint (\"The calculated log loss value on the test set for cosine sim is = %f\" %logloss)\n\nlogloss = calculate_logloss(y_true, y_pred_man)\nprint (\"The calculated log loss value on the test set for manhattan sim is = %f\" %logloss)\n\nlogloss = calculate_logloss(y_true, y_pred_euc)\nprint (\"The calculated log loss value on the test set for euclidean sim is = %f\" %logloss)\n\nlogloss = calculate_logloss(y_true, y_pred_jac)\nprint (\"The calculated log loss value on the test set for jaccard sim is = %f\" %logloss)\n\nlogloss = calculate_logloss(y_true, y_pred_mink)\nprint (\"The calculated log loss value on the test set for minkowski sim is = %f\" %logloss)\n\nlogloss = calculate_logloss(y_true, y_pred_poly)\nprint (\"The calculated log loss value on the test set for polynomial kernel sim is = %f\" %logloss)\n\nlogloss = calculate_logloss(y_true, y_pred_rbf)\nprint (\"The calculated log loss value on the test set for rbf kernel sim is = %f\" %logloss)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a828b3a9-228a-7faa-75f6-44116c6fedae"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}