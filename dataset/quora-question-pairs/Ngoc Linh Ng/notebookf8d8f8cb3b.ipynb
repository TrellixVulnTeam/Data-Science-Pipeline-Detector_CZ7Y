{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b119831a-df88-e1a9-4225-e9024fd57190"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom nltk.stem.porter import PorterStemmer # word stemming using Porter stemmer algorithm\nimport re # regex module\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"49062d3b-df6b-8562-4053-bf4a9ba46c60"},"outputs":[],"source":"# Loads data\ndata = pd.read_csv('../input/train.csv')\ndata.head(5)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"335adbe5-1278-332e-9b03-9de569fc97a0"},"outputs":[],"source":"data.tail(5)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ede30e29-4e9f-f737-17a6-3f960b26b654"},"outputs":[],"source":"# Make sure that everything is ok\ndata.head(10)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"48c64b8a-ee98-ce0e-7335-8dbaa2cc9d37"},"outputs":[],"source":"# Drop the first three columns id, qid1, qid2 since they carry no informations\ndata = data.drop(['id', 'qid1', 'qid2'], axis=1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f39e0349-cec1-9837-f049-5fdbe8fc4e67"},"outputs":[],"source":"# Looks good! \n# Now, it's time to check if there are any missing data\ndata.isnull().sum()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f6697c05-1217-fcd9-9ae8-93ea71d669af"},"outputs":[],"source":"# Oops! Looks like we have two missing datas on the column question2\n# Remove rows that contain missing data\nprint(\"Before dropping: \", data.shape)\ndata.dropna(axis=0, how='any',inplace=True)\nprint(\"After dropping: \", data.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5791ddd2-3b81-69bd-45e3-2bab58d7eb42"},"outputs":[],"source":"# Cool!\n# Now it's time to do some text preprocessing\n# We need to:\n#            1. Get rid of all non alphanumeric characters, like: ?, *, ^_^,...\n#            2. Exclude stopwords ??? (shall we?)\n#            3. Transform each word to its root, like: coolest->cool, houses->house, etc...\n#            4. TODO: get rid of all non representative words\n\ndef text_preprocessor(text, stemmed=False, stopwords=set()):\n    \"\"\"\n    - Converts text to lower case\n    - Gets rid of non-words\n    - stems word using Porter Stemmer algorithm\n    \"\"\"\n    text = text.lower()\n    text = re.sub('[^\\w\\s]+','',text)\n    tokens = [w for w in text.split() if w not in stopwords]\n    if stemmed:\n        porter = PorterStemmer()\n        # There is a bug in some nltk versions: porter.stem('oed') ---> crash!\n        # Solution: just ignore it!\n        temp = []\n        for t in tokens:\n            try:\n                temp.append(porter.stem(t))\n            except IndexError:\n                pass  \n        tokens = temp\n                \n        \n    return \" \".join(tokens)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"38be2d53-0809-2f3d-d0fd-aa4f33983881"},"outputs":[],"source":"# Sanity check\nporter = PorterStemmer()\nporter.stem('oed')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2247f324-876c-ef91-b7f3-6beac84a8a14"},"outputs":[],"source":"# But\ntext_preprocessor('oed')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ab5be20d-e2be-0a9e-938e-ca99fa7c250c"},"outputs":[],"source":"# Looks good!"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9e286406-fa43-6960-8e9a-3b0b096742fc"},"outputs":[],"source":"# Before we dive into any further, Let's convert data to numpy matrices\nX,y = data.iloc[:,:2].values, data.iloc[:,2].values\n\n# Make sure that we din't break anything\nprint('X.shape: ', X.shape)\nprint('y.shape: ', y.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1e5f8149-917b-c637-05fb-213a54ed29d2"},"outputs":[],"source":"# It's to to play with the text_preprocessor function a little bit\nquestions = X[:10,0]\nfor q in questions:\n    print(q)\n    print(text_preprocessor(q))\n    print('----------------------------------------------------------------------------')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"429d2f28-91a3-0cb4-6bd8-6b45623f78b5"},"outputs":[],"source":"# What if stemmed is set to True\nquestions = X[:10,0]\nfor q in questions:\n    print(q)\n    print(text_preprocessor(q, stemmed=True))\n    print('----------------------------------------------------------------------------')\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"af27dc51-a8d6-51f8-15ec-9be7d0f7f70d"},"outputs":[],"source":"# Try one more time with the last 10 questions in column question2\nquestions = X[-10:,1]\nfor q in questions:\n    print(q)\n    print(text_preprocessor(q, stemmed=True))\n    print('----------------------------------------------------------------------------')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a0d876d7-f97c-8c48-6348-2a10ab4ff01f"},"outputs":[],"source":"# TODO: \n#      1. Consider to add stopwords, non-representative words\n#      2. ????"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"811d8bac-f1e8-15f2-e9f6-ec414170ca23"},"outputs":[],"source":"# It's time to apply text_preprocessor to our data\ndata['question1'] = data['question1'].apply(text_preprocessor, args=(True,{}))\ndata['question2'] = data['question2'].apply(text_preprocessor, args=(True,{}))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a8be1806-70a0-6f7a-6a68-ed277d8a9663"},"outputs":[],"source":"# Have a look\ndata.tail(10)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e58e88b2-9b1e-a3a4-2ede-efd8d4644ed3"},"outputs":[],"source":"# Save data for later use\n# import csv\n# file_name = 'final_data.csv'\n# data.to_csv(file_name, \n#             header=['question1', 'question1', 'is_duplicate'], \n#             index=False, quoting=csv.QUOTE_NONNUMERIC)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ba4d4a99-b8aa-2c8e-b3b7-2da99491fe53"},"outputs":[],"source":"# Statistic similarity between two short docs\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ndef statistic_sim(doc1, doc2):\n    doc1 = text_preprocessor(doc1, stemmed=True)\n    doc2 = text_preprocessor(doc2, stemmed=True)\n    count = CountVectorizer()\n    bag = count.fit_transform(np.array([doc1, doc2]))\n    v1, v2 = bag.toarray()\n    return np.dot(v1,v2)/(np.linalg.norm(v1) * np.linalg.norm(v2))\n    \n    \n    \n    "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8699eb39-8051-9dab-f78b-daa021f6ecee"},"outputs":[],"source":"# Test statistic_sim func\npairs = X[:10]\nduplicates = y[:10]\n\nfor pair, dup in enumerate((pairs, duplicates)):\n    print(pair, \" \", dup)\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"20842ee0-d259-8308-8d4c-3e8cfbe6aa46"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}