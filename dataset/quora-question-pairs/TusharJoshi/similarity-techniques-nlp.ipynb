{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"19446ef4-8bbb-71af-3dcc-d70f28982b93"},"source":"##Predict Duplicate using basic ML + NLP techniques##\n\nI am trying to predict the duplicate sentences using vector similarity calculations and NLP technique in this module.\n\nMethods to be tried out\n\n - List item\n\n- BOW/TFIDF + Cosine/Euclidean Similarity(other similarity techniques)\n- BOW/TFIDF + POS tagger + Cosine/Euclidean Similarity(other similarity techniques)\n- BOW/TFIDF + POS tagging + Dependency parsing + Cosine/Euclidean Similarity(other similarity techniques)\n- Word2Vec + Cosine/Euclidean Similarity(other similarity techniques)\n- Doc2Vec + Cosine/Euclidean Similarity(other similarity techniques)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ad261ee6-3df1-6320-1648-22a76181d685"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"markdown","metadata":{"_cell_guid":"596d2ea3-3c9d-023a-4d73-35771de60d33"},"source":"**Reading train data, Cleaning**\n\n*Reading Training Data ,\nRemoving duplicates , \nRemoving NULL values*"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4e727c28-1e1d-b461-29a9-dd7549c15f5d"},"outputs":[],"source":"def read_data():\n    df = pd.read_csv(\"../input/train.csv\")\n    print (\"Shape of base training File = \", df.shape)\n    # Remove missing values and duplicates from training data\n    df.drop_duplicates(inplace=True)\n    df.dropna(inplace=True)\n    print(\"Shape of base training data after cleaning = \", df.shape)\n    return df\n\ndf_train = read_data()\nprint (df_train.head(2))"},{"cell_type":"markdown","metadata":{"_cell_guid":"dcb17931-ea76-e990-07a6-212994434bc5"},"source":"**EDA**\n\nSome EDA on the data to get a look and feel about the data."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a77efb8d-7210-fb1e-4db4-eece028f47ec"},"outputs":[],"source":"from collections import Counter\nimport matplotlib.pyplot as plt\nimport operator\nfile = open(\"EDA.txt\",\"wb\")\n\n\ndef eda(df):\n    eda_file = open(\"EDA.text\",\"wb\")\n    print (\"Duplicate Count = %s , Non Duplicate Count = %s\" \n           %(df.is_duplicate.value_counts()[1],df.is_duplicate.value_counts()[0]))\n    \n    question_ids_combined = df.qid1.tolist() + df.qid2.tolist()\n    print (\"Unique Questions = %s\" %(len(np.unique(question_ids_combined))))\n    question_ids_counter = Counter(question_ids_combined)\n    sorted_question_ids_counter = sorted(question_ids_counter.items(), key=operator.itemgetter(1))\n    question_appearing_more_than_once = [i for i in question_ids_counter.values() if i > 1]\n   \n    print (\"Count of Quesitons appearing more than once = %s\" %(len(question_appearing_more_than_once)))\n    \n    \neda(df_train)"},{"cell_type":"markdown","metadata":{"_cell_guid":"68d4fd18-31a2-8073-9f94-37bf71ba6c22"},"source":"## Train Dictionary ##\n\nUsing gensims to train a dictionary of words available in the corpus"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d8868610-4ab1-ac40-d03a-fed46d428149"},"outputs":[],"source":"import re\nimport gensim\nfrom gensim import corpora\nfrom nltk.corpus import stopwords\n\nwords = re.compile(r\"\\w+\",re.I)\nstopword = stopwords.words('english')\n\ndef tokenize_questions(df):\n    question_1_tokenized = []\n    question_2_tokenized = []\n\n    for q in df.question1.tolist():\n        question_1_tokenized.append([i.lower() for i in words.findall(q) if i not in stopword])\n\n    for q in df.question2.tolist():\n        question_2_tokenized.append([i.lower() for i in words.findall(q) if i not in stopword])\n\n    df[\"Question_1_tok\"] = question_1_tokenized\n    df[\"Question_2_tok\"] = question_2_tokenized\n    \n    return df\n\ndef train_dictionary(df):\n    \n    questions_tokenized = df.Question_1_tok.tolist() + df.Question_2_tok.tolist()\n    \n    dictionary = corpora.Dictionary(questions_tokenized)\n    dictionary.filter_extremes(no_below=5, no_above=0.5, keep_n=10000000)\n    dictionary.compactify()\n    \n    return dictionary\n    \ndf_train = tokenize_questions(df_train)\ndictionary = train_dictionary(df_train)\n\nprint (df_train.columns)\nprint (df_train.shape)\nprint (len(dictionary.token2id))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e135d7b9-5f56-ccf1-4451-393b91330b79"},"outputs":[],"source":"def get_vectors(df, dictionary):\n    \n    question1_vec = [dictionary.doc2bow(text) for text in df.Question_1_tok.tolist()]\n    question2_vec = [dictionary.doc2bow(text) for text in df.Question_2_tok.tolist()]\n    \n    question1_csc = gensim.matutils.corpus2csc(question1_vec, num_terms=len(dictionary.token2id))\n    question2_csc = gensim.matutils.corpus2csc(question2_vec, num_terms=len(dictionary.token2id))\n    \n    return question1_csc.transpose(),question2_csc.transpose()\n\n\nq1_csc, q2_csc = get_vectors(df_train, dictionary)\n\nprint (q1_csc.shape)\nprint (q2_csc.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"31a3e534-9c4b-4e4c-5fb8-4aff62781e55"},"outputs":[],"source":"from sklearn.metrics.pairwise import cosine_similarity as cs\n\ndef get_cosine_similarity(q1_csc, q2_csc):\n    cosine_sim = []\n    for i,j in zip(q1_csc, q2_csc):\n        sim = cs(i,j)\n        cosine_sim.append(sim[0][0])\n    \n    return cosine_sim\n    \ncosine_sim = get_cosine_similarity(q1_csc, q2_csc)\n\nprint (len(cosine_sim))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"28fd7421-a6c4-c864-a364-8961026a736f"},"outputs":[],"source":"from sklearn.ensemble import RandomForestClassifier as RFC\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import GradientBoostingClassifier as GBC\nfrom sklearn.linear_model import LogisticRegression as LR\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.metrics import f1_score, confusion_matrix\nfrom sklearn.pipeline import Pipeline\n\nnp.random.seed(10)\n\ndef train_rfc(X,y):\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=42)\n    \n    svm_models = [('svm', SVC(verbose=1, shrinking=False))]\n    svm_pipeline = Pipeline(svm_models)\n    svm_params = {'svm__kernel' : ['rbf'],\n                  'svm__C' : [0.01,0.1,1],\n                  'svm__gamma' :[0.1,0.2,0.4],\n                  'svm__tol' :[0.001,0.01,0.1],\n                  'svm__class_weight' : [{1:0.8,0:0.2}]}\n\n    rfc_models = [('rfc', RFC())]\n    rfc_pipeline = Pipeline(rfc_models)\n    rfc_params = {'rfc__n_estimators' : [20],\n                  'rfc__max_depth' : [10],\n                  'rfc__min_samples_leaf' : [100]}\n\n    lr_models = [('lr', LR(verbose=1))]\n    lr_pipeline = Pipeline(lr_models)\n    lr_params = {'lr__C': [0.1, 0.01],\n                 'lr__tol': [0.001,0.01],\n                 'lr__max_iter': [200,400],\n                 'lr__class_weight' : [{1:0.8,0:0.2}]}\n\n    gbc_models = [('gbc', GBC(verbose=1))]\n    gbc_pipeline = Pipeline(gbc_models)\n    gbc_params = {'gbc__n_estimators' : [100,200, 400, 800],\n                  'gbc__max_depth' : [40, 80, 160, 320],\n                  'gbc__learning_rate' : [0.01,0.1]}\n\n    grid = zip([svm_pipeline, rfc_pipeline, lr_pipeline, gbc_pipeline],\n               [svm_params, rfc_params, lr_params, gbc_params])\n\n    grid = zip([rfc_pipeline],\n               [rfc_params])\n\n    best_clf = None\n\n    for model_pipeline, param in grid:\n        temp = GridSearchCV(model_pipeline, param_grid=param, cv=4, scoring='f1')\n        temp.fit(X_train, y_train)\n\n        if best_clf is None:\n            best_clf = temp\n        else:\n            if temp.best_score_ > best_clf.best_score_:\n                best_clf = temp\n    \n    model_details = {}\n    model_details[\"CV Accuracy\"] = best_clf.best_score_\n    model_details[\"Model Parameters\"] = best_clf.best_params_\n    model_details[\"Test Data Score\"] = best_clf.score(X_test, y_test)\n    model_details[\"F1 score\"] = f1_score(y_test, best_clf.predict(X_test))\n    model_details[\"Confusion Matrix\"] = str(confusion_matrix(y_test, best_clf.predict(X_test)))\n    \n    return best_clf, model_details\n\nX = np.array(cosine_sim).reshape(-1,1)\ny = df_train.is_duplicate\n\nclf, model_details = train_rfc(X,y)\n\nprint (model_details)"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}