{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train=pd.read_csv(\"/kaggle/input/quora-question-pairs/train.csv.zip\")\nprint(train.shape)\ntest=pd.read_csv(\"/kaggle/input/quora-question-pairs/test.csv\")\nprint(test.shape)\n ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# since there is 1 null value in question 1 and two null values in question 2 we change it to empty string \ntrain['question1']=train['question1'].fillna('')\ntrain['question2']=train['question2'].fillna('')\ny_train=train['is_duplicate']\ntrain=train.drop(['is_duplicate'],axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['question1']=test['question1'].fillna('')\ntest['question2']=test['question2'].fillna('')\n ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# adding related frequencies of the questions\n \ntrain['freq_qid1'] =train.groupby('qid1')['qid1'].transform('count') \ntrain['freq_qid2']=train.groupby('qid2')['qid2'].transform('count')\n\n# adding length of question 1 and question 2 \ntrain['len_q1']=train['question1'].str.len()\ntrain['len_q2']=train['question2'].str.len()\n\n\n\n ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['len_q1']=test['question1'].str.len()\ntest['len_q2']=test['question2'].str.len()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# adding number of words in each questions\ntrain['n_words_q1']=train['question1'].apply(lambda x: len(x.split(\" \")))\ntrain['n_words_q2']=train['question2'].apply(lambda x: len(x.split(\" \")))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['n_words_q1']=test['question1'].apply(lambda x: len(x.split(\" \")))\ntest['n_words_q2']=test['question2'].apply(lambda x: len(x.split(\" \")))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# length of common words in two questions\ndef len_common_words(data):\n    w1=set(map(lambda x: x.lower().strip(),data['question1'].split(\" \")))\n    w2=set(map(lambda x: x.lower().strip(),data['question2'].split(\" \")))\n    return 1.0* len(w1 & w2) \ntrain['len_c-woords']=train.apply(len_common_words,axis=1) \n\n# length of unique common words only\ndef combined_words(data):\n    w1=set(map(lambda x: x.lower().strip(),data['question1'].split(\" \")))\n    w2=set(map(lambda x: x.lower().strip(),data['question2'].split(\" \")))\n    return 1.0* (len(w1)+ len(w2) )\ntrain['combined_words']=train.apply(combined_words,axis=1) \n\n# words share is length of uniquewords divided by total combined words \ndef words_share(data):\n    w1=set(map(lambda x: x.lower().strip(),data['question1'].split(\" \")))\n    w2=set(map(lambda x: x.lower().strip(),data['question2'].split(\" \")))\n    return 1.0* (len(w1 & w2)/(len(w1)+ len(w2) ))\ntrain['words_share']=train.apply(words_share,axis=1) \n\n# frequency of question1 + frequency of question\ntrain['freq_q1q2']=train['freq_qid1'] + train['freq_qid2'] \n\n# absolute value of (question 1' frequency - question 2's frequency)\n\ntrain['abs_diff']=abs(train['freq_qid1'] -train['freq_qid2'] )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['len_c-woords']=test.apply(len_common_words,axis=1) \ntest['combined_words']=test.apply(combined_words,axis=1) \n ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nfrom nltk.stem.porter import *\nfrom bs4 import BeautifulSoup\nimport nltk\n\nfrom nltk.corpus import stopwords\nstopwords=stopwords.words(\"english\")\n\n# def text_preprocess(df):\n#     cleaned=df['question1'].apply(lambda x :[ i for i in x if i not in stopwords])\n#     return cleaned\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\nfrom bs4 import BeautifulSoup\ntrain['question1'] = train['question1'].str.replace(r'[^\\w\\s]+', '')\n \ntrain['question2']=train['question2'].str.replace(r'[^\\w\\s]+', '')\n\ntrain['question1'] = [BeautifulSoup(text).get_text() for text in train['question1'] ]\ntrain['question2'] = [BeautifulSoup(text).get_text() for text in train['question2'] ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['question1'] = test['question1'].str.replace(r'[^\\w\\s]+', '')\n \ntest['question2']=test['question2'].str.replace(r'[^\\w\\s]+', '')\n\ntest['question1'] = [BeautifulSoup(text).get_text() for text in test['question1'] ]\ntest['question2'] = [BeautifulSoup(text).get_text() for text in test['question2'] ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import PorterStemmer\ndef stem_sentences(sentence):\n    porter_stemmer = PorterStemmer()\n    tokens = sentence.split()\n    stemmed_tokens = [porter_stemmer.stem(token) for token in tokens]\n    return ' '.join(stemmed_tokens)\n\ntrain['question1'] = train['question1'].apply(stem_sentences)\ntrain['question2'] = train['question2'].apply(stem_sentences)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['question1'] = test['question1'].apply(stem_sentences)\ntest['question2'] = test['question2'].apply(stem_sentences)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now performing advanced featurizations \ndef featurization(q1,q2):\n    features=[0,0,0,0,0,0,0,0]\n    q1=q1.split()\n    q2=q2.split()\n    \n    if len(q1) == 0 or len(q2) == 0:\n        return features\n    \n    #without stop words\n    q1_nostop=set([each for each in q1 if each not in stopwords])\n    q2_nostop=set([each for each in q2 if each not in stopwords])\n    \n    #with stop words\n    q1_stop=set([each for each in q1 if each in stopwords])\n    q2_stop=set([each for each in q2 if each in stopwords])\n    #common non-stop words\n    common_non_stop_c=len(q1_nostop.intersection(q2_nostop))\n    \n    #length of common stop words\n    common_stop_c=len(q1_stop.intersection(q2_stop))\n    \n    # len of common unique words count from all together\n    common_all_c=len(set(q1).intersection(set(q2)))\n    \n    # cmin,cmax for without stop words\n    cmin_nostop=common_non_stop_c/(min(len(q1_nostop),len(q2_nostop))+0.0000001)\n    cmax_nostop=common_non_stop_c/(max(len(q1_nostop),len(q2_nostop))+0.0000001)\n    \n    #cmin,cmax with stop words\n    cmin_stop=common_stop_c/(min(len(q1_stop),len(q2_stop)) +0.0000001)\n    cmax_stop=common_stop_c/(max(len(q1_stop),len(q2_stop))+0.0000001)\n    \n    #cminall,cmaxall\n    cmin_all=common_all_c/(min(len(q1),len(q2))+0.0000001)\n    cmax_all=common_all_c/(max(len(q1),len(q2))+0.0000001)\n    \n    last_same=int(q1[-1]==q2[-1])\n    first_same=int(q1[0]==q2[0])\n    \n    \n    features[0]=cmin_nostop\n    features[1]=cmax_nostop\n    features[2]=cmin_stop\n    features[3]=cmax_stop\n    features[4]=cmin_all\n    features[5]=cmax_all\n    features[6]=first_same\n    features[7]=last_same\n    return features\n    \n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from fuzzywuzzy import fuzz","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef get_features(dataframe):\n    \n    token_features = dataframe.apply(lambda x: featurization(x[\"question1\"], x[\"question2\"]), axis=1)\n    \n    dataframe[\"cmin_nostop\"]    = list(map(lambda x: x[0], token_features))\n    dataframe[\"cmax_nostop\"]   = list(map(lambda x: x[1], token_features))\n    dataframe[\"cmin_stop\"]     = list(map(lambda x: x[2], token_features))\n    dataframe[\"cmax_stop\"]      = list(map(lambda x: x[3], token_features))\n    dataframe[\"cmin_all\"]      = list(map(lambda x: x[4], token_features))\n    dataframe[\"cmax_all\"] = list(map(lambda x: x[5], token_features))\n    dataframe[\"first_same\"]  = list(map(lambda x: x[6], token_features))\n    dataframe[\"last_same\"] = list(map(lambda x: x[7], token_features))\n    \n    dataframe[\"fuzz_ratio\"]            = dataframe.apply(lambda x: fuzz.QRatio(x[\"question1\"], x[\"question2\"]), axis=1)\n    dataframe[\"token_set_ratio\"]       = dataframe.apply(lambda x: fuzz.token_set_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n    dataframe[\"token_sort_ratio\"]      = dataframe.apply(lambda x: fuzz.token_sort_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n    \n    \n    return dataframe\n\n\ntrain=get_features(train)\n\n \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntest=get_features(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\n \n# questions = list(train['question1']) + list(train['question2'])\n \n \n \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# tfidf = TfidfVectorizer(min_df=1)\n# tfidf.fit_transform(questions)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  print(tfidf.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# q1vec=tfidf.transform(train['question1'])\n# q2vec=tfidf.transform(train['question2'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from scipy.sparse import hstack\n# from scipy.sparse import coo_matrix, hstack\n# import scipy.sparse as sp\n# combined=hstack((q1vec,q2vec))\n# print(type(combined))\n# print(type(train))\n# # a=combined.todense()\n# # print(type(a))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if 'question1' or 'question2' in (train.coulmns or test.columns):\n    train=train.drop(['question1','question2'],axis=1)\n    test=test.drop(['question1','question2'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# final_data=hstack((train, combined),format=\"csr\",dtype='float64')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# len(tfidf.get_feature_names())\n# tfidf = TfidfVectorizer()\n# tfidf.fit_transform(list(train['question1'][10])+list(train['question2'][1]))\n# print(tfidf.get_feature_names())\n# total=list(train['question1'][0])+list(train['question2'][0])\n# total\n# train['question2'][0]\n# t=['what is the step by step guid to invest in share market in india','what is the step by step guid to invest in share market']\n# q1=['what is the step by step guid to invest in share market']\n# q2=['what is the step by step guid to invest in share market']\n# tfidf = TfidfVectorizer()\n# tfidf.fit_transform(t)\n# print(tfidf.get_feature_names())\n# result=tfidf.transform(q1)\n# print(result.toarray())\n# print(\"*\"*16)\n# re=tfidf.transform(q2)\n# print(re.toarray())\n# train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\n# model = KNeighborsClassifier(n_neighbors=5)\n# model.fit(final_data,y_train)\n# predicted=model.predict(final_data)\nfrom sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier()\nmodel.fit(train,y_train)\npredicted=model.predict(train)\nscore=accuracy_score(predicted,y_train)\nprint(\"the accuracy socre is : \",score)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score=accuracy_score(predicted,y_train)\nprint(\"the accuracy socre is : \",score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train=train.drop(['qid1','qid2','freq_qid1','freq_qid2','freq_q1q2','words_share'],axis=1)\n ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train=train.drop(['abs_diff'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fpredict=model.predict(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.metrics import log_loss\n# logloss=log_loss(predicted,y_train)\n# print(\"the  log loss is : \", logloss)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result=pd.DataFrame({'test_id':test.test_id,'is_duplicate':fpredict})\nresult.to_csv('mysubmission.csv', index=False)  \nprint(\"success!!!\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}