{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra|\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,roc_curve,auc\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport pywt\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\n\nfrom glob import glob\nimport scipy\nfrom scipy.signal import butter, lfilter, convolve, boxcar\nfrom scipy.signal import freqz\nfrom scipy.fftpack import fft, ifft\nimport os\nfrom sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def wavelet_denoising(x, wavelet='db2', level=3):\n    coeff = pywt.wavedec(x, wavelet, mode=\"per\")\n    sigma = (1/0.6745) * madev(coeff[-level])\n    uthresh = sigma * np.sqrt(2 * np.log(len(x)))\n    coeff[1:] = (pywt.threshold(i, value=uthresh, mode='hard') for i in coeff[1:])\n    return pywt.waverec(coeff, wavelet, mode='per')\ndef madev(d, axis=None):\n    \"\"\" Mean absolute deviation of a signal \"\"\"\n    return np.mean(np.absolute(d - np.mean(d, axis)), axis)\ndef prepare_data_train(fname):\n    \"\"\" read and prepare training data \"\"\"\n    # Read data\n    data = pd.read_csv(fname)\n    # events file\n    events_fname = fname.replace('_data','_events')\n    # read event file\n    labels= pd.read_csv(events_fname)\n    clean=data.drop(['id' ], axis=1)#remove id\n    labels=labels.drop(['id' ], axis=1)#remove id\n    return  clean,labels\n\ndef prepare_data_test(fname):\n    \"\"\" read and prepare test data \"\"\"\n    # Read data\n    data = pd.read_csv(fname)\n    return data\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler\nscaler= StandardScaler()\ndef data_preprocess_train(X):\n    X_prep=scaler.fit_transform(X)\n    #do here your preprocessing\n    return X_prep\ndef data_preprocess_test(X):\n    X_prep=scaler.transform(X)\n    #do here your preprocessing\n    return X_prep","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def wavelet_denoising(x, wavelet='db2', level=3):\n    coeff = pywt.wavedec(x, wavelet, mode=\"per\")\n    sigma = (1/0.6745) * madev(coeff[-level])\n    uthresh = sigma * np.sqrt(2 * np.log(len(x)))\n    coeff[1:] = (pywt.threshold(i, value=uthresh, mode='hard') for i in coeff[1:])\n    return pywt.waverec(coeff, wavelet, mode='per')\ndef madev(d, axis=None):\n    \"\"\" Mean absolute deviation of a signal \"\"\"\n    return np.mean(np.absolute(d - np.mean(d, axis)), axis)\ndef prepare_data_train(fname):\n    \"\"\" read and prepare training data \"\"\"\n    # Read data\n    data = pd.read_csv(fname)\n    # events file\n    events_fname = fname.replace('_data','_events')\n    # read event file\n    labels= pd.read_csv(events_fname)\n    clean=data.drop(['id' ], axis=1)#remove id\n    labels=labels.drop(['id' ], axis=1)#remove id\n    return  clean,labels\n\ndef prepare_data_test(fname):\n    \"\"\" read and prepare test data \"\"\"\n    # Read data\n    data = pd.read_csv(fname)\n    return data\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler\nscaler= StandardScaler()\ndef data_preprocess_train(X):\n    X_prep=scaler.fit_transform(X)\n    #do here your preprocessing\n    return X_prep\ndef data_preprocess_test(X):\n    X_prep=scaler.transform(X)\n    #do here your preprocessing\n    return X_prep","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import zipfile\nwith zipfile.ZipFile(\"../input/grasp-and-lift-eeg-detection/test.zip\",\"r\") as z:\n    z.extractall(\".\")\nwith zipfile.ZipFile(\"../input/grasp-and-lift-eeg-detection/train.zip\",\"r\") as z:\n    z.extractall(\".\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def wavelet_denoising(x, wavelet='db2', level=3):\n    coeff = pywt.wavedec(x, wavelet, mode=\"per\")\n    sigma = (1/0.6745) * madev(coeff[-level])\n    uthresh = sigma * np.sqrt(2 * np.log(len(x)))\n    coeff[1:] = (pywt.threshold(i, value=uthresh, mode='hard') for i in coeff[1:])\n    return pywt.waverec(coeff, wavelet, mode='per')\ndef madev(d, axis=None):\n    \"\"\" Mean absolute deviation of a signal \"\"\"\n    return np.mean(np.absolute(d - np.mean(d, axis)), axis)\ndef prepare_data_train(fname):\n    \"\"\" read and prepare training data \"\"\"\n    # Read data\n    data = pd.read_csv(fname)\n    # events file\n    events_fname = fname.replace('_data','_events')\n    # read event file\n    labels= pd.read_csv(events_fname)\n    clean=data.drop(['id' ], axis=1)#remove id\n    labels=labels.drop(['id' ], axis=1)#remove id\n    return  clean,labels\n\ndef prepare_data_test(fname):\n    \"\"\" read and prepare test data \"\"\"\n    # Read data\n    data = pd.read_csv(fname)\n    return data\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler\nscaler= StandardScaler()\ndef data_preprocess_train(X):\n    X_prep=scaler.fit_transform(X)\n    #do here your preprocessing\n    return X_prep\ndef data_preprocess_test(X):\n    X_prep=scaler.transform(X)\n    #do here your preprocessing\n    return X_prep","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subjects = range(1,6)\nfrom glob import glob\nimport pandas as pd\nids_tot = []\npred_tot = []\nX_train_butter = []\nfrom sklearn.model_selection import train_test_split\nimport numpy as  np\n\n###loop on subjects and 8 series for train data + 2 series for test data\ny_raw= []\nraw = []\ny_rawt= []\nrawt = []\n\n    \n    ################ READ DATA ################################################\n    \n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for subject in subjects:fnames =  sorted(glob('train/subj%d_series*_data.csv' % (subject)))\n\n\n#    fnames =  glob('../content/train/subj1_series1_events.csv')\n#    fnames =  glob('../content/train/subj1_series1_data.csv')\nfor fname in fnames:\n    data,labels=prepare_data_train(fname)\n    raw.append(data)\n    y_raw.append(labels)\n\nfor fname in fnames:\n    with open(fname) as myfile:\n        head = [next(myfile) for x in range(10)]\n        \nX = pd.concat(raw)\ny = pd.concat(y_raw)\n    #transform in numpy array\n    #transform train data in numpy array\nX_train =np.asarray(X.astype(float))\ny_train = np.asarray(y.astype(float))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler,Normalizer,MinMaxScaler\nscaler= StandardScaler()\ndef data_preprocess_train(X):\n    X_prep=scaler.fit_transform(X)\n    #do here your preprocessing\n    return X_prep\nfs = 500.0\nlowcut = 7.0\nhighcut = 30.0\n\n\nx_train_butter=wavelet_denoising(X_train)\nx_train=data_preprocess_train(x_train_butter)\nsplitrate=-x_train.shape[0]//5*2\nxval=x_train[splitrate:splitrate//2]\nyval=y_train[splitrate:splitrate//2]\nxtest=x_train[splitrate//2:]\nytest=y_train[splitrate//2:]\nxtrain=x_train[:splitrate]\nytrain=y_train[:splitrate]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nknn_clf = KNeighborsClassifier(n_neighbors = 50 ,weights='distance')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn_clf.fit(xtrain,ytrain)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\ncross_val_score(knn_clf, xtrain, ytrain, cv=10, scoring=\"accuracy\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_train = knn_clf.predict(xtrain)\nprint(\"Accuracy = \",accuracy_score(ytrain,predictions_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = knn_clf.predict(xtest)\nprint(\"Accuracy = \",accuracy_score(ytest,predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"l=[\"Handstart (HS)\",\"Grasping (GS)\",\"Lift (LT)\",\"Hold (HD)\",\"Replace (RP)\",\"Release (RL)\"]\npredictions_df = pd.DataFrame(predictions, columns = l)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"predictions_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}