{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Please find my answers to the assignment questions in-line.","metadata":{}},{"cell_type":"markdown","source":"### Imports","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt \nimport time\nimport sys\nimport re\nimport gc\nfrom sklearn.model_selection import train_test_split, StratifiedShuffleSplit\nimport nltk\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import confusion_matrix, plot_confusion_matrix\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.stem import WordNetLemmatizer,PorterStemmer\nfrom nltk.corpus import stopwords","metadata":{"execution":{"iopub.status.busy":"2022-02-23T01:47:36.994628Z","iopub.execute_input":"2022-02-23T01:47:36.994969Z","iopub.status.idle":"2022-02-23T01:47:38.428975Z","shell.execute_reply.started":"2022-02-23T01:47:36.994888Z","shell.execute_reply":"2022-02-23T01:47:38.428293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!unzip /kaggle/input/job-salary-prediction/Train_rev1.zip","metadata":{"execution":{"iopub.status.busy":"2022-02-23T01:47:38.430205Z","iopub.execute_input":"2022-02-23T01:47:38.430405Z","iopub.status.idle":"2022-02-23T01:47:43.43704Z","shell.execute_reply.started":"2022-02-23T01:47:38.43038Z","shell.execute_reply":"2022-02-23T01:47:43.436272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv(\"Train_rev1.csv\")\ndata = data.sample(2500, random_state = 42) # Randomly selecting 2500 samples from data\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-23T01:47:52.87813Z","iopub.execute_input":"2022-02-23T01:47:52.878427Z","iopub.status.idle":"2022-02-23T01:47:58.221953Z","shell.execute_reply.started":"2022-02-23T01:47:52.878399Z","shell.execute_reply":"2022-02-23T01:47:58.221057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assigning whether we are above or below 75th Percentile\ndata['Percentile'] = data['SalaryNormalized'].rank(pct = True)\ndata.loc[data['Percentile'] > 0.75, 'Target'] = 'Above'\ndata.loc[data['Percentile'] < 0.75, 'Target'] = 'Below'\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-23T01:47:58.842019Z","iopub.execute_input":"2022-02-23T01:47:58.843018Z","iopub.status.idle":"2022-02-23T01:47:58.876386Z","shell.execute_reply.started":"2022-02-23T01:47:58.842977Z","shell.execute_reply":"2022-02-23T01:47:58.875671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Build a classification model with text (full job description) as the predictor.\nWhat is the accuracy of your model? Show the confusion matrix. Also show the top 10 words (excluding stopwords) that are most indicative of (i) high salary ii) low salary\n\n\n---------\nThe accuracy of the model is ~75% on the test set.\n\n**Low Salary (Below 75th Percentile):**\nMost important words are **shifts, teacher, teachers, weekends, telesales, hospitality, chef, monday, caterer, friday, secondary**. From a high-level overview, job-postings related to telesales, teaching, cooking (chef, catering) might be associated with low salary. Additionally, mention of days like Monday and Friday in the Job Descriptions are a sign of low salary as well.\n\n**High Salary (Above 75th Percentile):**\nMost important words are **alignment, mod, implementations, emea, milestones, worlds, soa, architects, multisite, cmd**. It appears that job postings that mention of EMEA (Europe, the Middle East and Africa), soa (service-oriented architecture), , ","metadata":{}},{"cell_type":"markdown","source":"### Pre-processing text","metadata":{}},{"cell_type":"code","source":"lemmatizer = WordNetLemmatizer()\nstemmer = PorterStemmer() \n\ndef preprocess(sentence):\n    sentence=str(sentence)\n    sentence = sentence.lower()\n    sentence=sentence.replace('{html}',\"\") \n    cleanr = re.compile('<.*?>')\n    cleantext = re.sub(cleanr, '', sentence)\n    rem_url=re.sub(r'http\\S+', '',cleantext)\n    rem_num = re.sub('[0-9]+', '', rem_url)\n    tokenizer = RegexpTokenizer(r'\\w+')\n    tokens = tokenizer.tokenize(rem_num)  \n    filtered_words = [w for w in tokens if len(w) > 2 if not w in stopwords.words('english')]\n    #stem_words=[stemmer.stem(w) for w in filtered_words]\n    #lemma_words=[lemmatizer.lemmatize(w) for w in stem_words]\n    lemma_words=[lemmatizer.lemmatize(w) for w in filtered_words]\n    return \" \".join(filtered_words)\n\ndata['FullDescription_Clean']=data['FullDescription'].map(lambda s:preprocess(s)) ","metadata":{"execution":{"iopub.status.busy":"2022-02-23T01:52:36.939812Z","iopub.execute_input":"2022-02-23T01:52:36.940635Z","iopub.status.idle":"2022-02-23T01:53:41.952112Z","shell.execute_reply.started":"2022-02-23T01:52:36.940597Z","shell.execute_reply":"2022-02-23T01:53:41.951227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Binary Counter\n\nIndicate whether word exists or not in the individual description.","metadata":{}},{"cell_type":"code","source":"# Splitting data\nX, y = data['FullDescription_Clean'], data['Target']\n\n# Count Vectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer(binary = True) \ntdata = cv.fit_transform(X) \nft = cv.get_feature_names() \nfull_set = list(zip(list(map(lambda row:dict(zip(ft,row)),tdata.toarray())), y))\n\n# Train-test split\ntrain_set ,test_set = train_test_split(full_set,test_size=0.2, random_state=42) ","metadata":{"execution":{"iopub.status.busy":"2022-02-23T01:53:41.953937Z","iopub.execute_input":"2022-02-23T01:53:41.954175Z","iopub.status.idle":"2022-02-23T01:53:51.582266Z","shell.execute_reply.started":"2022-02-23T01:53:41.954147Z","shell.execute_reply":"2022-02-23T01:53:51.581313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model \nclassifier = nltk.NaiveBayesClassifier.train(train_set)\nprint(nltk.classify.accuracy(classifier, test_set))","metadata":{"execution":{"iopub.status.busy":"2022-02-23T01:53:51.583846Z","iopub.execute_input":"2022-02-23T01:53:51.58415Z","iopub.status.idle":"2022-02-23T01:55:59.963377Z","shell.execute_reply.started":"2022-02-23T01:53:51.584112Z","shell.execute_reply":"2022-02-23T01:55:59.962545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Custom Function to get Top Labels for each class\ndef imp_features(self, n=30):\n    for unique_label in list(set(self._labels)):\n        # Determine the most relevant features, and display them.\n        cpdist = self._feature_probdist\n        print(f\"Most Informative Features for {unique_label}:\\n\")\n        ctr=1 # Counter\n        for (fname, fval) in self.most_informative_features(100000000):\n            def labelprob(l):\n                return cpdist[l, fname].prob(fval)\n            labels = sorted(\n                (l for l in self._labels if fval in cpdist[l, fname].samples()),\n                key=lambda element: (-labelprob(element), element),\n                reverse=True,\n            )\n            l0 = labels[0]\n            l1 = labels[-1]\n            if len(labels) == 1 or l0 == unique_label:\n                continue\n            if cpdist[l0, fname].prob(fval) == 0:\n                ratio = \"INF\"\n            else:\n                ratio = \"%8.1f\" % (\n                    cpdist[l1, fname].prob(fval) / cpdist[l0, fname].prob(fval)\n                )\n            print(\n                \"%24s = %-14r %6s : %-6s = %s : 1.0\"\n                % (fname, fval, (\"%s\" % l1)[:6], (\"%s\" % l0)[:6], ratio)\n            )\n            ctr+=1\n            print(ctr)\n            if ctr == n:\n                break\n\nimp_features(classifier, 20)","metadata":{"execution":{"iopub.status.busy":"2022-02-23T01:55:59.965678Z","iopub.execute_input":"2022-02-23T01:55:59.967027Z","iopub.status.idle":"2022-02-23T01:56:00.530562Z","shell.execute_reply.started":"2022-02-23T01:55:59.966988Z","shell.execute_reply":"2022-02-23T01:56:00.529806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extracting labels\nlabels = [classifier.classify(i[0]) for i in test_set]\ny_test = [i[1] for i in test_set]\n\n# Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\nprint(\"Confusion Matrix:\\n\")\nmat = confusion_matrix(y_test, labels)\nsns.heatmap(mat, annot=True, fmt='g')\nplt.xlabel('true label')\nplt.ylabel('predicted label');\nplt.show()\n\n# Classification Report\nfrom sklearn import metrics\nprint(\"\\nClassification Report:\\n\", metrics.classification_report(y_test, labels))","metadata":{"execution":{"iopub.status.busy":"2022-02-23T01:56:00.531638Z","iopub.execute_input":"2022-02-23T01:56:00.531837Z","iopub.status.idle":"2022-02-23T01:56:49.941022Z","shell.execute_reply.started":"2022-02-23T01:56:00.531812Z","shell.execute_reply":"2022-02-23T01:56:49.940185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. If you wanted to increase the accuracy of the model above, how can you accomplish this using the dataset you have?\n---------\n\nWe have already incorporated **text pre-processing** techniques such as removing stop words, lemmatization, lower-casing, removing numbers. \n\n**Vectorization:** Count Vectorizer (Frequency Counting), Tfidf Vectorizer (Frequency Counting penalizing common words) can be used as an alternative to our simple example which simply indicates whether or not a word is present in the document or not.\n\n**N-gram:** Bigrams are tokens of words based on two-words per token, trigrams based on three words. These can be used instead of our unigram approach to see if it possibly increases accuracy.\n\n**Log-probabilities:** Naive (independence/naive assumption) involves multiplication of probabilities (leading to smaller numbers). So in our case floating point precision of computers can introduce a lower cap. In order to circumvent this, log probabilities can be used. \n\n**Additional Features:** Features like **Job Title, Location, Contract Type, Category** can be predictors for each job posting in addition to our previous approach. Since, our predictions of salaries are based on text data only, adding information such as Seniority, Department, Location, Full-time/Part-time information can significantly improve our model's performance.\n\n**Sentiment Analysis Results** can also be incorporated as a feature.\n\n**Synonyms:** The performance of Naive Bayes can degrade if the data contains highly correlated features. This is because the highly correlated features are voted for twice in the model, over inflating their importance.","metadata":{}},{"cell_type":"markdown","source":"# End of Assignment","metadata":{}}]}