{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"from nltk.stem.porter import *\r\nimport pandas as pd\r\nimport numpy as np\r\nimport re\r\nfrom bs4 import BeautifulSoup\r\nfrom sklearn.pipeline import Pipeline\r\nfrom sklearn.decomposition import TruncatedSVD\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom sklearn.svm import SVC\r\nfrom sklearn.feature_extraction.text import TfidfVectorizer\r\nfrom sklearn.feature_extraction import text\r\n# array declarations\r\nsw=[]\r\ns_data = []\r\ns_labels = []\r\nt_data = []\r\nt_labels = []\r\n#stopwords tweak - more overhead\r\nstop_words = ['http','www','img','border','0','1','2','3','4','5','6','7','8','9','a','the','be','a','about','above','after','again','against','all']\r\nstop_words = text.ENGLISH_STOP_WORDS.union(stop_words)\r\nfor stw in stop_words:\r\n    sw.append(\"q\"+stw)\r\n    sw.append(\"z\"+stw)\r\nstop_words = text.ENGLISH_STOP_WORDS.union(sw)\r\n\r\n#load data\r\ntrain = pd.read_csv(\"../input/train.csv\").fillna(\"\")\r\ntest  = pd.read_csv(\"../input/test.csv\").fillna(\"\")\r\n\r\n#remove html, remove non text or numeric, make query and title unique features for counts using prefix (accounted for in stopwords tweak)\r\nstemmer = PorterStemmer()\r\n## Stemming functionality\r\nclass stemmerUtility(object):\r\n    \"\"\"Stemming functionality\"\"\"\r\n    @staticmethod\r\n    def stemPorter(review_text):\r\n        porter = PorterStemmer()\r\n        preprocessed_docs = []\r\n        for doc in review_text:\r\n            final_doc = []\r\n            for word in doc:\r\n                final_doc.append(porter.stem(word))\r\n                #final_doc.append(wordnet.lemmatize(word)) #note that lemmatize() can also takes part of speech as an argument!\r\n            preprocessed_docs.append(final_doc)\r\n        return preprocessed_docs\r\n\r\n\r\nfor i in range(len(train.id)):\r\n    s=(\" \").join([\"q\"+ z for z in BeautifulSoup(train[\"query\"][i]).get_text(\" \").split(\" \")]) + \" \" + (\" \").join([\"z\"+ z for z in BeautifulSoup(train.product_title[i]).get_text(\" \").split(\" \")]) + \" \" + BeautifulSoup(train.product_description[i]).get_text(\" \")\r\n    s=re.sub(\"[^a-zA-Z0-9]\",\" \", s)\r\n    s= (\" \").join([stemmer.stem(z) for z in s.split(\" \")])\r\n    s_data.append(s)\r\n    s_labels.append(str(train[\"median_relevance\"][i]))\r\nfor i in range(len(test.id)):\r\n    s=(\" \").join([\"q\"+ z for z in BeautifulSoup(test[\"query\"][i]).get_text().split(\" \")]) + \" \" + (\" \").join([\"z\"+ z for z in BeautifulSoup(test.product_title[i]).get_text().split(\" \")]) + \" \" + BeautifulSoup(test.product_description[i]).get_text()\r\n    s=re.sub(\"[^a-zA-Z0-9]\",\" \", s)\r\n    s= (\" \").join([stemmer.stem(z) for z in s.split(\" \")])\r\n    t_data.append(s)\r\n#create sklearn pipeline, fit all, and predit test data\r\nclf = Pipeline([('v',TfidfVectorizer(min_df=5, max_df=500, max_features=None, strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(1, 2), use_idf=True, smooth_idf=True, sublinear_tf=True, stop_words = 'english')), ('svd', TruncatedSVD(n_components=560, algorithm='randomized', n_iter=6, random_state=None, tol=0.0)), ('scl', StandardScaler(copy=True, with_mean=True, with_std=True)), ('svm', SVC(C=11.0, kernel='rbf', degree=3, gamma=0.0, coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, random_state=None))])\r\nclf.fit(s_data, s_labels)\r\nt_labels = clf.predict(t_data)\r\n\r\n#output results for submission\r\nwith open(\"submission_v3.csv\",\"w\") as f:\r\n    f.write(\"id,prediction\\n\")\r\n    for i in range(len(t_labels)):\r\n        f.write(str(test.id[i])+\",\"+str(t_labels[i])+\"\\n\")\r\nf.close()\r\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":0}