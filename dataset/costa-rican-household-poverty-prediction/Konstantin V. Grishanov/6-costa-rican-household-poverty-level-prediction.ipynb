{"cells":[{"metadata":{"_uuid":"7fc8777a10280df4b68a28129908697a20929986"},"cell_type":"markdown","source":"I publish this kernel only at the request of Luis Teyerin, thanks to whose hosted this interesting contest. \nPlease do not judge strictly for the lack of detailed comments and the abundance of bikes in the code.\n\nFor those who are too lazy to explore the code, I will briefly describe the approach:\n\n1. Feats: nothing remarkable, everything is about the same as many in this competition.\n2. Model. \n    * I built two pretty good base models: LogisticRegression (0.437) and LGBMClassifier (0.434). \n    * Then I strengthened each of them with something like a bagging (FoldsEstemator in code), teaching on different subsamples and averaging the result. \n    *  The final step was to mix the two models obtained through averaging (0.446 on LB). The peculiarity of the last step was the weighting of the result in proportion by probabilities of the classes and the use of additional hyperparameter regulating the weights of the basic classifiers in the final decision.\n\nI borrowed the idea of averaging classes from Ilya V. Shchurov: https://www.kaggle.com/ischurov/more-feature-eng-lgb-5-fold-early-stopping# and developed it a bit by trying different averaging options. In the same place, I got the idea to use some new feats (dispersions/min/max).\n\nI hid the source code of some tools to not complicate the code."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from sklearn.base import clone, BaseEstimator, TransformerMixin\nfrom sklearn.model_selection import StratifiedKFold\nimport numpy as np\nfrom collections import Counter\n\n\nclass FoldsEstimator(BaseEstimator, TransformerMixin):\n\n    def __init__(self, base_clf, cv=None, predict_mode='vote', proba_mode='mean'):\n        self.base_clf = base_clf\n        cv = cv if cv is not None else StratifiedKFold(n_splits=5, shuffle=True, random_state=25)\n        self.cv = cv\n        self.clfs = []\n        self.predict_mode = predict_mode\n        self.proba_mode = proba_mode\n\n    def fit(self, X, y):\n        if self.predict_mode == 'off':\n            self.base_clf.fit(X, y)\n        else:\n            self.clfs = []\n            for train, test in self.cv.split(X, y):\n                clf = clone(self.base_clf)\n                clf.fit(X[list(train), :], y.iloc[list(train)])\n                self.clfs.append(clf)\n        return self\n\n    def predict(self, X, mode=None):  # vote, mean, proba_wmean, prob, off\n        mode = mode if mode is not None else self.predict_mode\n        if mode == 'vote':\n            res = self.predict_vote(X)\n        elif mode == 'mean':\n            res = self.predict_mean(X)\n        elif mode == 'prob':\n            res = self.predict_by_proba(X)\n        elif mode == 'proba_wmean' or mode == 'predict_weighted_mean':\n            res = self.predict_weighted_mean(X)\n        elif mode == 'off':\n            res = self.predict_simple(X)\n        return np.array(res)\n\n    def predict_proba(self, X):\n        probas = self._predict_probas(X)\n#        print(probas.shape)\n        if self.proba_mode == 'mean':\n            return probas.mean(axis=0)\n        elif self.proba_mode == 'max':\n            return probas.max(axis=0)\n        raise Exception(f'Unknown proba mode: {self.predict_mode}')\n\n    def _predicts(self, X):\n        predicts = []\n        for clf in self.clfs:  # BaseEstimator\n            predicts.append(clf.predict(X)[:, np.newaxis])\n        return np.hstack(tuple(predicts))\n\n    def _predict_probas(self, X):\n        probas = []\n        for clf in self.clfs:\n            probas.append(clf.predict_proba(X))\n        return np.array(probas)\n\n    def predict_vote(self, X):\n        predicts = self._predicts(X)\n #       print(predicts.shape)\n        mean_probas = self.predict_proba(X)\n #       print(mean_probas.shape)\n        res = []\n        for i, predict in enumerate(zip(*predicts.T)):\n            counter_ = Counter()\n            counter_.update(predict)\n            counter_ = sorted(list(counter_.items()), key=lambda x: -x[1])\n            max_votes = counter_[0][1]\n            alts = [c[0] for c in counter_ if c[1] == max_votes]\n            if len(alts) == 1:\n                prediction = alts[0]\n            else:\n                probas = mean_probas[i, :][[i_ - 1 for i_ in alts]]\n                prediction = alts[np.argmax(probas)]\n            res.append(prediction)\n        return res\n\n    def predict_mean(self, X):\n        return np.round(self._predicts(X).mean(axis=1)).astype(int)\n\n    def predict_by_proba(self, X):\n        proba = self.predict_proba(X)\n        return np.argmax(proba, axis=1) + 1\n\n    def predict_weighted_mean(self, X):\n        predicts = self._predicts(X)\n        probas = self._predict_probas(X)\n        weights = probas.max(axis=2).T\n        print(X.shape, probas.shape, weights.shape, predicts.shape)\n        # (2377, 45) (5, 2377, 4) (5, 2377) (2377, 5)\n        res = np.round((predicts * weights).sum(axis=1) / weights.sum(axis=1)).astype('int')[:, np.newaxis]\n        # print(X.shape, probas.shape, weights.shape, predicts.shape, probas.shape, res.shape)\n        return res\n\n    def predict_simple(self, X):\n        return self.base_clf.predict(X)\n\n\nclass MetaEstimator(BaseEstimator, TransformerMixin):\n    def __init__(self, estimators, estimators_weight='equal', predicts_weight='equal', mode='mean'):\n        self.estimators = estimators\n        self.estimators_weight = estimators_weight  # equal, array of weights\n        self.predicts_weight = predicts_weight  # equal, proba\n        self.mode = mode\n\n    def fit(self, X, y):\n        for est in self.estimators:\n            est.fit(X, y)\n        return self\n\n    def get_weights(self, X):\n        weights = self.get_predict_weights(X) * self.get_estimator_weights()\n        return weights / weights.sum(axis=1)[:, np.newaxis]\n\n    def get_estimator_weights(self):\n        if self.estimators_weight == 'equal':\n            return np.array([[1 / len(self.estimators)] * len(self.estimators)])\n        return self.estimators_weight\n\n    def get_predict_weights(self, X):\n        if self.predicts_weight == 'equal':\n            return np.array([[1 / len(self.estimators)] * len(self.estimators)] * X.shape[0])\n        if self.predicts_weight == 'proba':\n            return self.get_weights_by_prob(X)\n        raise Exception(f\"Unknown predict_weight method: {self.predicts_weight}\")\n\n    def get_weights_by_prob(self, X):\n        probs = self._predict_probas(X).max(axis=2).T\n        return probs / probs.sum(axis=1)[:, np.newaxis]\n\n    def predict(self, X):\n        res = None\n        for est in self.estimators:\n            predict = est.predict(X)[:, np.newaxis]\n            res = predict if res is None else np.hstack([res, predict])\n        weights = self.get_weights(X)\n        return np.round((res * weights).sum(axis=1)).astype('int')\n\n    def _predicts(self, X):\n        predicts = []\n        for clf in self.estimators:  # BaseEstimator\n            predicts.append(list(clf.predict(X)))\n        return np.array(tuple(predicts))\n\n    def _predict_probas(self, X):\n        probas = []\n        for clf in self.estimators:\n            probas.append(clf.predict_proba(X))\n        return np.array(tuple(probas))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"738ac3acb65736ed9e498e6ca8c11535cbe7c4cd","_kg_hide-input":true},"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Fri Jul  6 08:16:44 2018\n\n@author: Konstantin V. Grishanov\n\"\"\"\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder\nfrom sklearn.base import TransformerMixin, BaseEstimator\nfrom copy import deepcopy\nfrom scipy.stats import norm\nfrom datetime import datetime\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.base import clone\nfrom collections import Counter\n\n\n\nclass XBaseTransformer(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Base class for Transformers\n    \"\"\"\n    def __init__(\n            self,\n            none_value=None,\n            none_value_strategy='const',\n            transformer=None,\n            select_columns=None,\n    ):\n        self.none_value = none_value\n        self.none_value_strategy = none_value_strategy\n        self.none_value_dict = {}\n\n        self.columns = []\n        self.transformers = {}\n        self.transformer = transformer\n        self.select_columns = select_columns\n\n        self.debug = False\n        self.debug_csv = False\n        self.debug_name = 'new'\n\n\n    def dprint(self, *argv):\n        if self.debug:\n            print(*argv)\n\n    def fit(self, X, y=None):\n        \"\"\"\n        Fitting transformer\n        \n        :param X: DataFrame of data to transform\n        :param y: Series of target variable\n        :returns: self\n        \"\"\"\n        #print('Fit:', X.shape)\n        self.dprint(datetime.now(), 'Start fit...' + str(self.__class__), X.shape)\n        self.set_columns(X, y)\n        for v in self.columns:\n            x = self.get_one_var(X, v)\n            self.fit_none_value(x, v)\n            self.before_fit_var(x, v)\n            self.fit_var(x, v)\n            self.after_fit_var(x, v)\n        self.dprint(datetime.now(), 'End fit ' + str(self.__class__))\n        return self\n\n    def transform(self, X, y=None):\n        \"\"\"\n        Transform X by fitted parameters\n        \n        :param X: DataFrame of data to transform\n        :param y: Series of target variable\n        :returns: DataFrame\n        \"\"\"\n        #print('Transform:', X.shape)\n        self.dprint(datetime.now(), 'Start transform...' + str(self.__class__), X.shape)\n        df = X.copy(deep=True)\n        df = self.before_transform(df)\n        for v in self.columns:\n            self.before_transform_var(self.get_one_var(df, v), v)\n            df[v] = self.transform_var(self.get_one_var(df, v), v)\n            self.after_transform_var(self.get_one_var(df, v), v)\n        self.dprint(datetime.now(), 'End transform' + str(self.__class__))\n        return self.prepare_transform_result(df)\n\n    def prepare_var(self, x, varname):\n        \"\"\"\n        Prepare one var of Dataframe to transform or fitting\n        By default called from fit_prepare_var and transform_prepare_var\n        \n        :param x: Series to transform\n        :param varname: colname in DataFrame\n        :returns: Series transformed x\n        \"\"\"\n        none_value = self.get_none_value(varname)\n        if none_value is None:\n            return x\n        return x.fillna(none_value)\n\n    def fit_var(self, x, varname):\n        \"\"\"\n        Fit one var\n        \n        :param x: Series\n        :param varname: string\n        \"\"\"\n        self.transformers[varname] = self.create_transformer(x, varname).fit(\n            self.fit_prepare_var(x, varname)\n        )\n        \n    def before_fit_var(self, x, varname):\n        \"\"\"\n        \"\"\"\n        pass\n    \n    def after_fit_var(self, x, varname):\n        \"\"\"\n        \"\"\"\n        pass\n\n    def fit_prepare_var(self, x, varname=None):\n        \"\"\"\n        Prepare one var for fit\n        \n        :param x: Series\n        :param varname: string varname\n        :returns: Series\n        \"\"\"\n        # print('fit ' + varname)\n        return self.prepare_var(x, varname)\n\n    def transform_var(self, x, varname):\n        # print(self.__class__, varname)\n        return self.transformers[varname].transform(\n            self.transform_prepare_var(x, varname)\n        )\n \n    def before_transform_var(self, x, varname):\n        pass\n    \n    def after_transform_var(self, x, varname):\n        pass\n\n    def transform_prepare_var(self, x, varname=None):\n        # print('transform')\n        return self.prepare_var(x, varname)\n\n    def create_transformer(self, x, varname):\n        return deepcopy(self.transformer)\n\n    def set_columns(self, X, y=None):\n        self.columns = (\n            list(X.columns) if self.select_columns is None\n            else self.select_columns\n        )\n\n    def get_one_var(self, X, v):\n        return X[v]\n\n\n    def prepare_transform_result(self, X):\n        cols = self.columns\n        #cols.sort()\n        if self.debug_csv:\n            X[cols].to_csv(self.debug_filename('vars'))\n        return X[cols]\n\n    def before_transform(self, X):\n        return X\n\n    def calc_default_value(self, x, varname, strategy, default_value):\n        if strategy == 'mode':\n            return x.mode()[0]\n        return default_value\n\n    def fit_none_value(self, x, varname):\n        self.none_value_dict[varname] = self.calc_default_value(\n            x, varname, self.none_value_strategy, self.none_value\n        )\n\n    def get_none_value(self, varname):\n        return self.none_value_dict[varname]\n\n    def debug_filename(self, name):\n        return str(self.__class__)[8:-2] + '-' + name + '-new.txt'\n\n\n\n\n\nclass XLabelEncoder(XBaseTransformer):\n\n    def __init__(\n            self,\n            none_value='__NONE__',\n            none_value_strategy='const',\n            transformer=LabelEncoder(),\n            select_columns=None,\n            quantity_treshold=0,\n            other_label='__OTHER__',\n            other_label_strategy='const'\n    ):\n        super().__init__(\n            none_value=none_value,\n            none_value_strategy=none_value_strategy,\n            transformer=transformer,\n            select_columns=select_columns\n        )\n\n        self.quantity_treshold = quantity_treshold\n        self.other_label = other_label\n        self.other_label_strategy = other_label_strategy\n        self.value_dicts = {}\n        self.other_label_dict = {}\n\n    def prepare_var(self, x, varname):\n        return x.fillna(self.get_none_value(varname)).astype('str').apply(\n            lambda x_:\n            self.value_dicts[varname][x_] if x_ in self.value_dicts[varname]\n            else self.get_other_value(varname)\n        )\n\n    def get_other_value(self, varname):\n        return self.other_label_dict[varname]\n\n    def init_value_dict(self, x, varname):\n        counts = x.fillna(self.get_none_value(varname)).astype('str').value_counts()\n        value_dict = {v:v for v in counts.index if counts[v] > self.quantity_treshold}\n        self.init_other_label_dict(x, varname, value_dict, counts)\n        self.value_dicts[varname] = value_dict\n\n    def init_other_label_dict(self, x, varname, value_dict, counts):\n        self.other_label_dict[varname] = str(\n            self.calc_default_value(\n                x, varname, 'mode', self.other_label\n            ) if len(counts) == len(value_dict)\n            else self.calc_default_value(\n                x, varname, self.other_label_strategy, self.other_label\n            )\n        )\n\n    def before_fit_var(self, x, varname):\n        self.init_value_dict(x, varname)\n\n\nclass XLabelZcoder(XLabelEncoder):\n    def __init__(\n            self,\n            none_value='__NONE__',\n            none_value_strategy='const',\n            select_columns=None,\n            quantity_treshold=0,\n            other_label='__OTHER__',\n            other_label_strategy='const',\n            class_label_varname='Target',\n            alpha=0.05\n    ):\n        super().__init__(\n            none_value=none_value,\n            none_value_strategy=none_value_strategy,\n            transformer=None,\n            select_columns=select_columns,\n            quantity_treshold=quantity_treshold,\n            other_label=other_label,\n            other_label_strategy=other_label_strategy\n        )\n\n        self.alpha=alpha\n        self.class_label_varname=class_label_varname\n        self.y = None\n        self.z = None\n        self.default_pleft = None\n\n    def fit(self, X, y=None):\n        self.z = norm.ppf(1 - self.alpha)\n        self.y = X[self.class_label_varname]\n        y_counts = self.y.value_counts()\n        self.default_pleft = self.uleft(y_counts[-1], y_counts[1])\n        return super().fit(X, y)\n\n    def zleft(self, n0, n1):\n        z = self.z\n        p = n1 / (n0 + n1)\n        return p - z * np.sqrt(p * (1 - p) / (n0 + n1))\n\n    def uleft(self, n0, n1):\n        z = self.z\n        n = n0 + n1\n        p = n1 / n\n        return 1 / (1 + (z ** 2) / n) * (p + (z ** 2) / 2 / n - np.sqrt(p * (1 - p) / n + (z ** 2) / 4 / n ** 2))\n\n    def fit_var(self, x, varname):\n        \"\"\"\n        Fit one var\n\n        :param x: Series\n        :param varname: string\n        \"\"\"\n        cross_tab = pd.crosstab(\n            self.fit_prepare_var(x, varname),\n            self.y\n        )\n\n        dict_p = {\n            ind: self.uleft(cross_tab.loc[ind][-1], cross_tab.loc[ind][1]) for ind in cross_tab.index\n        }\n\n        value_dict = {\n            ind: dict_p[self.value_dicts[varname][ind]] for ind in self.value_dicts[varname]\n        }\n\n        other_label = self.get_other_value(varname)\n        other_value = dict_p[other_label] if other_label in dict_p else self.default_pleft\n        value_dict[other_label] = other_value\n        self.other_label_dict[varname] = other_value\n        self.value_dicts[varname] = value_dict\n\n\n\n    def transform_var(self, x, varname):\n        return self.transform_prepare_var(x, varname)\n\n\n\nclass XScaler(XBaseTransformer):\n    def __init__(\n            self,\n            none_value=0,\n            none_value_strategy='const',\n            transformer=MinMaxScaler(),\n            select_columns=None,\n            is_none_vars_suffix='_none',\n            scale_none_value=False, #TODO: убить, связав с none_value_srategy\n            return_is_none_vars=True #TODO: убить, связав с none_value_srategy\n    ):\n        super().__init__(\n            none_value=none_value,\n            none_value_strategy=none_value_strategy,\n            transformer=transformer,\n            select_columns=select_columns\n        )\n\n        self.return_is_none_vars = return_is_none_vars\n        self.scale_none_value = scale_none_value\n\n        self.is_none_vars_suffix = is_none_vars_suffix\n\n        self.is_none_columns = []\n        self.is_none_df = None\n\n\n    def fit_prepare_var(self, x, varname):\n        if self.scale_none_value:\n            return x.fillna(self.get_none_value(varname))[:, np.newaxis]\n        return x[x.isnull() == False][:, np.newaxis]\n\n    def transform_prepare_var(self, x, varname):\n        return x.fillna(self.get_none_value(varname))[:, np.newaxis]\n\n    def prepare_transform_result(self, X):\n        #self.is_none_columns.sort()\n        #with open(str(self.__class__)[8:-2] + '-none.txt', 'a') as f:\n        #    f.write(','.join(self.is_none_columns) + '\\n')\n        cols = self.columns + (self.is_none_columns if self.return_is_none_vars else [])\n        #cols.sort()\n        if self.debug_csv:\n            X[cols].to_csv(self.debug_filename('vars'))\n        return X[cols]\n\n    def before_transform(self, X):\n        for v_is_none in self.is_none_columns:\n            v = v_is_none[:-len(self.is_none_vars_suffix)]\n            X[v_is_none] = X[v].isnull()*1\n        self.X_tr = X\n        return X\n\n    def set_columns(self, X, y=None):\n        super().set_columns(X, y)\n        self.is_none_columns = []\n\n    def get_is_none_varname(self, varname):\n        return varname + self.is_none_vars_suffix\n\n    def transform_var(self, x, varname):\n\n        x_tr = super().transform_var(x, varname).ravel()\n\n        if self.get_is_none_varname(varname) in self.is_none_columns:\n            x_is_none = x.isnull()*1\n            if self.return_is_none_vars:\n                if self.is_none_df is None:\n                    self.is_none_df = pd.DataFrame()\n                self.is_none_df[self.get_is_none_varname(varname)] = x_is_none\n\n            if self.scale_none_value == False:\n                x_tr = x_tr * (1 - x_is_none) + self.get_none_value(varname) * x_is_none\n\n        return x_tr\n\n    def calc_default_value(self, x, varname, strategy, default_value):\n        if strategy == 'mean':\n            return x.mean()\n        elif strategy == 'median':\n            return x.median()\n        elif strategy == 'flag_var':\n            return 0\n        return super().calc_default_value(x, varname, strategy, default_value)\n\n    #def before_fit_var(self, x, varname):\n        #obs_count = len(x)\n        #if x.count() < obs_count:\n        #    is_nan_name = varname + self.is_none_vars_suffix\n        #    self.is_none_columns.append(is_nan_name)\n\n    def fit_none_value(self, x, varname):\n        #if  self.#self.none_value_strategy == 'flag_var':\n        obs_count = len(x)\n        if x.count() < obs_count:\n            is_nan_name = varname + self.is_none_vars_suffix\n            self.is_none_columns.append(is_nan_name)\n        super().fit_none_value(x, varname)\n\n    def fit(self, X, y=None):\n        if self.none_value_strategy in ['mean', 'median', 'mode']:\n            self.scale_none_value = True\n        elif self.none_value_strategy == 'flag_var':\n            self.scale_none_value = False\n            self.return_is_none_vars = True\n        # print(self.return_is_none_vars)\n        return super().fit(X, y)\n\n\nclass XVarSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, varnames=None, mode='names'):\n        self.varnames = varnames\n        self.mode = mode\n\n    def fit(self, X, y=None):\n        return self\n\n    def _select_by_names(self, X, y=None):\n        return X[self.varnames]\n\n    def _select_by_colnum(self, X, y=None):\n        return np.array(X)[:, self.varnames]\n\n    def transform(self, X, y=None):\n        if self.varnames is None:\n            return X\n        if self.mode == 'names':\n            return  self._select_by_names(X, y)\n        return self._select_by_colnum(X, y)\n\n\nclass XNominalPairs(XLabelEncoder):\n    def __init__(\n            self,\n            select_columns,\n            none_value='__NONE__',\n            none_value_strategy='const',\n            transformer=LabelEncoder(),\n            quantity_treshold=0,\n            other_label='__OTHER__',\n            other_label_strategy='const'\n    ):\n        super().__init__(\n            select_columns=select_columns,\n            none_value=none_value,\n            none_value_strategy=none_value_strategy,\n            transformer=transformer,\n            quantity_treshold=quantity_treshold,\n            other_label=other_label,\n            other_label_strategy=other_label_strategy\n        )\n\n\n    def get_one_var(self, X, varname):\n        # print(varname, varname.split('_'), self.columns)\n        var1, var2 = varname.split('_')\n        none_value = self.none_value\n        x = X[var1].fillna(self.none_value) + '_' + X[var2].fillna(self.none_value)\n        if self.debug_csv:\n            x.to_csv(self.debug_filename(varname))\n        return x\n\n\nclass XEstimatorVar(BaseEstimator, TransformerMixin):\n    def __init__(self, est):\n        self.est = est\n        self.est_model = None\n\n    def fit(self, X, y):\n        self.est_model = self.est.fit(X, y)\n        return self\n\n    def transform(self, X):\n        return self.est_model.predict_proba(X)[:, 1][:, np.newaxis]\n\n\nclass XFilter(BaseEstimator, TransformerMixin):\n    def __init__(self, condition=None):\n        self.condition = condition\n\n    def fit(self, X, y):\n        return self\n\n    def transform(self, X):\n        return X[self.condition(X)]\n\n\nclass XLambda(BaseEstimator, TransformerMixin):\n    def __init__(self, transforms=[], copy=True):\n        self.transforms = transforms\n        self.copy = copy\n\n    def fit(self, X, y):\n        return self\n\n    def transform(self, X):\n        if self.copy:\n            X = X.copy()\n        for lambda_, cols in self.transforms:\n            X[cols] = X[cols].apply(lambda x: x.map(lambda_))\n        return X\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dfb7e190e303d364c6cf97715470199f88fee4d5","_kg_hide-input":true},"cell_type":"code","source":"import re\nimport numpy as np\nimport pandas as pd\n\n\nclass FeatureGenerator:\n    feature_counter = 0\n    feature_prefix = 'new_feat_'\n\n    def __init__(self, name=None, transformer=None, var_type='numeric', fillna=None):\n        self.name = name or self._name()\n        self.var_type = var_type\n        self.fillna = fillna\n        if transformer is not None:\n            self.transformer = transformer\n\n    def fit(self, X, y=None, X_test=None):\n        return self\n\n    def fit_generate(self, X, y=None, X_test=None):\n        self.fit(X, y, X_test).generate(X, y)\n\n    def generate(self, X, y=None):\n        if self.check_column_exists(X):\n            # raise self.FeatureGeneratorException('Column name exists!')\n            print(f'Column {self.name} exists!')\n        self._generate(X, y)\n        self._fillna(X, y)\n\n    def _fillna(self, X, y=None):\n        if self.fillna is not None:\n            X[self.name] = X[self.name].fillna(self.fillna)\n\n    def _generate(self, X, y=None):\n        X[self.name] = self.transformer(X)\n\n    def check_column_exists(self, X):\n        if type(self.name) == str:\n            names = [self.name]\n        else:\n            names = self.name\n        for name in names:\n            if name in X.columns:\n                return True\n        return False\n\n    def _name(self):\n        name = self.feature_prefix + str(self.feature_counter)\n        self.feature_counter += 1\n        return name\n\n    class FeatureGeneratorException(Exception):\n        pass\n\n\nclass FeatureGeneratorList:\n    def __init__(self, feature_generators):\n        self.feature_generators = feature_generators\n\n    def fit(self, X, y=None, X_test=None):\n        for feat_generator in self.feature_generators:\n            feat_generator.fit(X, y, X_test)\n        return self\n\n    def generate(self, X, y=None):\n        for feat_generator in self.feature_generators:\n            feat_generator.generate(X, y)\n\n    def fit_generate(self, X, y=None, X_test=None):\n        for feat_generator in self.feature_generators:  # type: FeatureGenerator\n            feat_generator.fit_generate(X, y, X_test)\n            if X_test is not None:\n                feat_generator.generate(X_test)\n\n    def new_feats(self, var_type=None):\n        new_feats = []\n        for feat_generator in self.feature_generators:\n            if var_type is None or feat_generator.var_type == var_type:\n                if type(feat_generator.name) == str:\n                    new_feats.append(feat_generator.name)\n                else:\n                    new_feats = new_feats + feat_generator.name\n        return new_feats\n\n\nclass NormByGroup(FeatureGenerator):\n    def __init__(self, source_var, group_var, name=None, transformer=None, mode='norm', exclude_mean=True, test_x=None):\n        self.source_var = source_var\n        self.group_var = group_var\n        self.mode = mode\n        self.mean = {}\n        self.std = {}\n        self.exclude_mean = exclude_mean\n        self.test_x = test_x\n        super().__init__(name=name, transformer=transformer)\n\n    def fit(self, X, y=None, test_X=None):\n        X_ = X if test_X is None else X.append(test_X)\n        self.mean = dict(X_.groupby([self.group_var])[self.source_var].mean())\n        self.std = dict(X_.groupby([self.group_var])[self.source_var].std(1))\n        return self\n\n    def _lambda(self, v, group):\n        if self.mode == 'index':\n            return v / self.mean[group]\n        return (v - self.mean[group] * self.exclude_mean) / self.std[group]\n\n    def _generate(self, X, y=None):\n        X[self.name] = [\n            self._lambda(v, group) for v, group in zip(X[self.source_var], X[self.group_var])\n        ]\n\n    def _name(self):\n        return self.source_var + f'_{self.mode}_by_' + self.group_var\n\n\nclass LatentFeats(FeatureGenerator):\n    def __init__(self, n_feats, source_feats, name=None, transformer=None, test_x=None, mode='transform', var_type='numeric'):\n        self.n_feats = n_feats\n        self.mode = mode\n        self.source_feats = source_feats\n        self.index_var = 'idhogar'\n        self.index_indicator = 'parentesco1'\n        self.name_prefix = name\n        self.test_X = test_x\n        super().__init__(transformer=transformer(n_feats), var_type=var_type)\n\n    def fit(self, X, y=None, test_X=None):\n        X_ = X if test_X is None else X.append(test_X)\n        self.transformer.fit(X_[self.source_feats][X_[self.index_indicator] == 1].fillna(0))\n        return self\n\n    def _generate(self, X, y=None):\n        if self.mode == 'transform':\n            values = self.transformer.transform(X[X[self.index_indicator] == 1][self.source_feats].fillna(0))\n            for i, varname in enumerate(self.name):\n                X[varname] = X[self.index_var].map({\n                    index: value for index, value in zip(X[X[self.index_indicator] == 1][self.index_var], values[:, i])\n                })\n        else:\n            values = self.transformer.predict(\n                X[X[self.index_indicator] == 1][self.source_feats].fillna(0)\n            )\n            X[self.name_prefix] = X[self.index_var].map({\n                index: value for index, value in zip(X[X[self.index_indicator] == 1][self.index_var], values)\n            })\n\n    def _name(self):\n        if self.mode == 'transform':\n            return [f'{self.name_prefix}_{i}' for i in range(0, self.n_feats)]\n        return self.name_prefix\n\n\nclass OneHotDecoder(FeatureGenerator):\n    def __init__(self, name=None, transformer=None, columns=None, regexp=None):\n        super().__init__(name=name, transformer=transformer, var_type='nominal')\n        self.columns = columns\n        self.regexp = regexp\n        self.codes = None\n\n    def fit(self, X, y=None, test_X=None):\n        if self.columns is None:\n            self.columns = [c for c in X.columns if re.match(self.regexp, c)]\n        self.codes = list(range(1, len(self.columns) + 1))\n        return self\n\n    def _generate(self, X, y=None):\n        X[self.name] = [int(np.dot(self.codes, v)) for v in X[self.columns].values]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"05ee0c45e6a6b1ddaadd76a91e468788a3fa6b89","_kg_hide-input":true},"cell_type":"code","source":"from sklearn.exceptions import DataConversionWarning, UndefinedMetricWarning\nimport warnings\nwarnings.filterwarnings(action='ignore', category=DataConversionWarning)\nwarnings.filterwarnings(action='ignore', category=FutureWarning)\nwarnings.filterwarnings(action='ignore', category=UndefinedMetricWarning)\nwarnings.filterwarnings(action='ignore', category=DeprecationWarning)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d68f5e0c50b444ad8fadf796869173c610a7a12"},"cell_type":"code","source":"from sklearn.cluster import KMeans\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold, GridSearchCV\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.decomposition import PCA\nfrom statsmodels.stats.weightstats import zconfint\nfrom sklearn.linear_model import LogisticRegression","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"201a14cb7568e3521eb0bd5d9b58cf3f62ea4e80"},"cell_type":"markdown","source":"### Reading data and preparing feat generators"},{"metadata":{"trusted":true,"_uuid":"8f4cdd1abe020e462464d1540517701d796b8b14"},"cell_type":"code","source":"df = pd.read_csv('../input/train.csv')\nhh_head = df.parentesco1 == 1\n\ndf[['meaneduc', 'SQBmeaned', 'v18q1', 'rez_esc']] = df[['meaneduc', 'SQBmeaned', 'v18q1', 'rez_esc']].fillna(0)\nmaterial_feats = [\n    c for c in df.columns \n    if re.match(r'^(?:pared|piso|techo|abasta|sanitario|energcocinar|elimbasu|epared|etecho).*$', c)\n] + ['public', 'planpri', 'noelec', 'coopele']\n\neducation_by_age_mean = df.groupby(df.age.map(lambda x: x if x < 65 else 65))['escolari'].median()\neducation_by_age_mean = {i:education_by_age_mean[i if i < 65 else 65] for i in range(0, df.age.max()+1)}\n\nfeature_generators = FeatureGeneratorList([\n# One-hot decode:\n    OneHotDecoder(name='region', regexp='^lugar\\d+$'),\n    OneHotDecoder(name='outside_wall', regexp='^pared.*$'),\n    OneHotDecoder(name='floor', regexp='^piso.*$'),\n    OneHotDecoder(name='roof', regexp='^techo.*$'),\n    OneHotDecoder(name='water_provision', regexp='^abastagua.*$'),\n    OneHotDecoder(name='electricity', columns=['public', 'planpri', 'noelec', 'coopele']),\n    OneHotDecoder(name='toilet', regexp='^sanitario\\d+$'),\n    OneHotDecoder(name='cook_energy', regexp='^energcocinar\\d+$'),\n    OneHotDecoder(name='rubbish', regexp='^elimbasu\\d+$'),\n    OneHotDecoder(name='wall_state', regexp='^epared\\d+$'),\n    OneHotDecoder(name='roof_state', regexp='^etecho\\d+$'),\n    OneHotDecoder(name='family_state', regexp='^estadocivil\\d+$'),\n    OneHotDecoder(name='family_head', regexp='^parentesco\\d+$'),\n    OneHotDecoder(name='education', regexp='^instlevel\\d+$'),\n    OneHotDecoder(name='property_state', regexp='^tipovivi\\d+$'),\n# Rent feats:   \n    FeatureGenerator(\n        name='rent_by_room', \n        transformer=lambda df: df.v2a1/df.rooms\n    ),\n    FeatureGenerator(\n        name='rent_by_18',\n         transformer=lambda df: df.idhogar.map(\n            dict(df.groupby('idhogar')['v2a1'].mean()/df[df.age>=18].groupby('idhogar')['idhogar'].count())\n        ),\n    ),       \n    FeatureGenerator(\n        name='dis_any',\n        transformer=lambda df: df.idhogar.map(dict(df.groupby('idhogar')['dis'].any()*1)),\n        var_type='binary'\n    ),\n    FeatureGenerator(\n        name='dis_sum',\n        transformer=lambda df: df.idhogar.map(dict(df.groupby('idhogar')['dis'].sum())),\n    ),\n    FeatureGenerator(\n        name='age_mean',\n        transformer=lambda df: df.idhogar.map(dict(df.groupby('idhogar')['age'].mean())),\n    ),\n    FeatureGenerator(\n        name='age_std', \n        transformer=lambda df: df.idhogar.map(\n            dict(df.groupby('idhogar')['age'].std(1))\n        )\n    ),\n    FeatureGenerator(\n        name='age_median',\n        transformer=lambda df: df.idhogar.map(dict(df.groupby('idhogar')['age'].median())),\n    ),\n    FeatureGenerator(\n        name='mobilification',\n        transformer=lambda df: df.idhogar.map(\n            dict(df.groupby('idhogar')['qmobilephone'].mean() / df.groupby('idhogar')['tamhog'].mean())\n        ),\n    ),\n     FeatureGenerator(\n        name='mobilification_10',\n        transformer=lambda df: df.idhogar.map(\n            dict(df.groupby('idhogar')['qmobilephone'].mean() / df[df.age > 10].groupby('idhogar')['idhogar'].count())\n        ),\n    ),\n    FeatureGenerator(\n        name='mobilification_18',\n        transformer=lambda df: df.idhogar.map(\n            dict(df.groupby('idhogar')['qmobilephone'].mean() / df[df.age > 18].groupby('idhogar')['idhogar'].count())\n        ),\n    ),\n    FeatureGenerator(\n        name='mobilification_adult',\n        transformer=lambda df: df.idhogar.map(\n            dict(\n                df.groupby('idhogar')['qmobilephone'].mean() / df[\n                    (df.age >= 14) & (df.age<66)\n                ].groupby('idhogar')['idhogar'].count())\n        ),\n    ),\n    FeatureGenerator(\n        name='bedrooms_share',\n        transformer=lambda df: df.idhogar.map(\n            dict(df.groupby('idhogar')['bedrooms'].mean() / df.groupby('idhogar')['rooms'].mean())\n        ),\n    ),\n    LatentFeats(5, material_feats, 'material_cluster', KMeans, mode='predict'),\n    FeatureGenerator(\n        name='educ_balanced', \n        transformer=lambda df: df.escolari - df.age.map(education_by_age_mean)\n    ),\n    FeatureGenerator(\n        name='educ_balanced_std', \n        transformer=lambda df: df.idhogar.map(\n            dict(df.groupby('idhogar')['educ_balanced'].std(1))\n        )\n    ),\n    FeatureGenerator(\n        name='max_educ_years_18',\n        transformer=lambda df: df.idhogar.map(\n            dict(df[df.age>=18].groupby('idhogar')['escolari'].max())\n        ),\n    ),\n    FeatureGenerator(\n        name='min_educ_years',\n         transformer=lambda df: df.idhogar.map(\n            dict(df.groupby('idhogar')['escolari'].min())\n        ),\n    ),\n    FeatureGenerator(\n        name='educ_balanced_max_18',\n         transformer=lambda df: df.idhogar.map(\n            dict(df[df.age>=18].groupby('idhogar')['educ_balanced'].max())\n        ),\n    ),    \n    FeatureGenerator(\n        name='educ_balanced_mean',\n         transformer=lambda df: df.idhogar.map(\n            dict(df.groupby('idhogar')['educ_balanced'].mean())\n        ),\n    ),\n    FeatureGenerator(\n        name='educ_balanced_25_45',\n         transformer=lambda df: df.idhogar.map(\n            dict(df[(df.age>=25) & (df.age<=45)].groupby('idhogar')['educ_balanced'].mean())\n        ),\n        fillna=0\n    ),\n    FeatureGenerator(\n        name='educ_balanced_46_',\n         transformer=lambda df: df.idhogar.map(\n            dict(df[(df.age>=46)].groupby('idhogar')['educ_balanced'].mean())\n        ),\n        fillna=0\n    ),\n    FeatureGenerator(\n        name='educ_balanced_progress1',\n         transformer=lambda df: df.educ_balanced_25_45 - df.educ_balanced_46_,\n        fillna=0\n    ),\n    FeatureGenerator(\n        name='educ_balanced_10_30',\n         transformer=lambda df: df.idhogar.map(\n            dict(df[(df.age>=10) & (df.age<=30)].groupby('idhogar')['educ_balanced'].mean())\n        ),\n        fillna=0\n    ),\n    FeatureGenerator(\n        name='educ_balanced_31_',\n         transformer=lambda df: df.idhogar.map(\n            dict(df[(df.age>=31)].groupby('idhogar')['educ_balanced'].mean())\n        ),\n        fillna=0\n    ),\n    FeatureGenerator(\n        name='educ_balanced_progress2',\n         transformer=lambda df: df.educ_balanced_10_30 - df.educ_balanced_31_,\n        fillna=0\n    ),\n    LatentFeats(10, material_feats, 'material_pca', PCA),\n\n    # means / indexes by means by region groups:\n    NormByGroup('v2a1', 'region', name='rent_norm_by_region'),\n    NormByGroup('rent_by_room', 'region'),\n    NormByGroup('v2a1', 'region', name='rent_index_by_region', mode='index'),\n    NormByGroup('rent_by_room', 'region', mode='index'),\n    NormByGroup('meaneduc', 'region'),\n    NormByGroup('meaneduc', 'region', mode='index'),\n    NormByGroup('educ_balanced', 'region'),\n    NormByGroup('educ_balanced', 'region', mode='index'),\n])\n\nage_groups = [(0, 10), (11, 18), (19, 65), (66, 200)]\ndef age_int_transformer(int_, sex=None):\n    _age = int_[0]\n    age_ = int_[1]\n    sex_ = sex\n    def func_(df):\n        age = df.age * (1 if sex_ is None else 2*df[sex_] - 1)\n        age.name='age'\n        return df.idhogar.map(\n            dict(df[['idhogar']].join(\n                age.apply(lambda x: 1 if _age <= x <= age_ else 0)\n            ).groupby('idhogar').age.sum()/df.groupby('idhogar').age.count())\n        )\n    return func_\n\nfeature_generators.feature_generators.extend([FeatureGenerator(\n    name=f'age_{gr[0]}_{gr[1]}_share',\n    transformer = age_int_transformer(gr)\n) for gr in age_groups])\n\n\nfeature_generators.feature_generators.extend([FeatureGenerator(\n    name=f'male_{gr[0]}_{gr[1]}_share',\n    transformer = age_int_transformer(gr, 'male')\n) for gr in age_groups])\n\nfeature_generators.feature_generators.extend([FeatureGenerator(\n    name=f'female_{gr[0]}_{gr[1]}_share',\n    transformer = age_int_transformer(gr, 'female')\n) for gr in age_groups])\n\n\n# feature_generators.fit_generate(df, df.Target)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c48422b925fd17c6e07308b671d6be1f01f2d8f9"},"cell_type":"markdown","source":"### Feats, selected for base models"},{"metadata":{"trusted":true,"_uuid":"5078eca776bac1bfc2a5754a80f202fb005d207d"},"cell_type":"code","source":"# LGBMClassifier feats\ngb_num_vars = ['age_mean', 'educ_balanced_mean', 'educ_balanced_index_by_region', 'age_std', 'age', 'educ_balanced_std', 'age_median', 'mobilification', 'meaneduc', 'educ_balanced_progress2', 'educ_balanced_norm_by_region', 'educ_balanced_progress1', 'educ_balanced_max_18', 'overcrowding', 'male_19_65_share', 'edjefe', 'max_educ_years_18', 'age_19_65_share', 'educ_balanced_25_45', 'rooms', 'v2a1', 'educ_balanced_10_30', 'bedrooms_share', 'min_educ_years', 'dependency', 'r4t2', 'rent_by_room', 'edjefa', 'mobilification_10', 'female_19_65_share', 'educ_balanced_31_', 'r4t1', 'educ_balanced_46_', 'r4h2', 'age_11_18_share', 'bedrooms', 'rent_by_room_norm_by_region', 'age_66_200_share', 'educ_balanced', 'female_66_200_share', 'rent_by_18', 'escolari', 'rent_norm_by_region', 'age_0_10_share', 'r4h3', 'tamviv', 'hogar_mayor', 'rent_by_room_index_by_region', 'female_0_10_share', 'male_11_18_share', 'r4m3', 'dis_sum']\ngb_cat_vars = ['region', 'roof_state', 'v18q', 'outside_wall', 'family_state', 'cook_energy', 'dis', 'floor', 'education', 'wall_state', 'property_state', 'male', 'toilet', 'area1', 'rubbish']\n\n# LogisticRegression feats\nlr_num_vars =  ['v18q1', 'r4h2', 'r4t2', 'tamviv', 'hogar_nin', 'hogar_adul', 'hogar_mayor', 'dependency', 'edjefa', 'meaneduc', 'qmobilephone', 'SQBescolari', 'SQBhogar_total', 'SQBhogar_nin', 'SQBdependency', 'SQBmeaned', 'age_mean', 'mobilification_adult', 'educ_balanced_mean', 'rent_by_room_norm_by_region', 'rent_by_room_index_by_region', 'meaneduc_index_by_region', 'male_0_10_share', 'male_19_65_share', 'male_66_200_share', 'female_66_200_share']\nlr_bin_vars =  ['v14a', 'refrig', 'v18q', 'eviv1', 'eviv3', 'dis', 'estadocivil2', 'estadocivil4', 'estadocivil6', 'instlevel3', 'instlevel8', 'tipovivi2', 'tipovivi3', 'computer', 'television', 'lugar1', 'lugar3', 'lugar6', 'area1']\nlr_log_vars =  ['v2a1', 'rent_by_room_index_by_region', 'SQBedjefe', 'rent_by_room', 'SQBhogar_total', 'SQBmeaned', 'rent_index_by_region', 'age']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"41bed92869b6f8e878231ea82d7eb6ca843f45cf"},"cell_type":"markdown","source":"### Base models pipelines"},{"metadata":{"trusted":true,"_uuid":"fc5a710239688c83fc0de5fa66a30d12fae10d76"},"cell_type":"code","source":"lr_pipeline = Pipeline(steps=[\n    ('clean', XLambda(transforms=[\n        (\n            lambda x: 0 if x=='no' else 1 if x=='yes' else float(x),\n            ['edjefe', 'edjefa', 'dependency']\n        ), (\n            lambda x: np.log(1 + x),\n            lr_log_vars\n        )\n    ])),\n    ('feats', FeatureUnion(transformer_list=[\n        ('scaled', XScaler(select_columns=lr_num_vars, transformer=StandardScaler(), return_is_none_vars=False)),\n        ('binary', XVarSelector(lr_bin_vars))\n    ])),\n    # We use some sort of bagging for classifier improvements.\n    ('clf', FoldsEstimator(\n        LogisticRegression(C=0.1, penalty='l2', class_weight='balanced', random_state=25),\n        predict_mode='mean'\n    ))\n])\n\ngb_pipeline = Pipeline(steps=[\n    ('clean', XLambda(transforms=[\n        (\n            lambda x: 0 if x=='no' else 1 if x=='yes' else float(x),\n            ['edjefe', 'edjefa', 'dependency']\n        ), (\n            lambda x: np.log(1 + x),\n            []\n        )\n    ])),\n    ('feats', FeatureUnion(transformer_list=[\n        ('scaled', XScaler(\n            select_columns = gb_num_vars, \n            transformer=MinMaxScaler(), \n            none_value = -100, return_is_none_vars=False,\n        )),\n        ('nominal', XVarSelector(gb_cat_vars))\n    ])),\n\n    # We use some sort of bagging for classifier improvements.\n    ('clf', FoldsEstimator(\n        LGBMClassifier(learning_rate=0.05,                      \n                        n_estimators=200,\n                        num_leaves=20,\n                        max_depth=15,\n                        class_weight='balanced',\n                        objective='multinominal',\n                        boosting='dart'),\n        cv=StratifiedKFold(\n            n_splits=10,\n            shuffle=True,\n            random_state=39\n        ), predict_mode='mean'\n    ))\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dfa81f6e3d5f6cce4bf27668afed03275e05dbce","_kg_hide-input":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin, clone\nclass XEstimatorVar(BaseEstimator, TransformerMixin):\n    def __init__(self, est):\n        self.est = est\n        self.est_model = None\n\n    def fit(self, X, y):\n        self.est_model = self.est.fit(X, y)\n        return self\n\n    def transform(self, X):\n        res = self.est_model.predict(X)\n        return np.array(res)[:, np.newaxis]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cc8740801886dfa9babfd16c973bbb1153a536c4","_kg_hide-input":true},"cell_type":"code","source":"class WeightedMeanEstimator(BaseEstimator, TransformerMixin):\n    def __init__(self, w=[0.5, 0.5]):\n        self.w = w\n    \n    def fit(self, X, y):\n        return self\n    \n    def predict(self, X):\n        return [int(r) for r in np.round((X[:, 0] * self.w[0] + X[:, 1] * self.w[1]), 0)]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1350f94b2b4ff9354b5620018741342a37e05cff"},"cell_type":"markdown","source":"### Final mix of classifiers"},{"metadata":{"trusted":true,"_uuid":"610a956522fdff9b04e48d6ae4e8df87245491ee"},"cell_type":"code","source":"sum_pipeline= MetaEstimator(\n    estimators=[\n        lr_pipeline,\n        gb_pipeline\n    ], \n    estimators_weight=np.array([0.5, 0.5]),\n    predicts_weight='proba'\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2709935f2501b0cc74bb85418dd9f1d7205d698a","_kg_hide-input":true},"cell_type":"code","source":"cv = GridSearchCV(\n    sum_pipeline, \n    param_grid={'estimators_weight': [[w, 1-w] for w in np.linspace(0.2, 0.8, 7)]}, \n    cv=StratifiedKFold(\n        n_splits=5,\n        shuffle=True,\n        random_state=25\n    ),\n    scoring='f1_macro'    \n)\n\nif False:\n    df_test = pd.read_csv('../input/test.csv')\n    feature_generators.fit_generate(df, df.Target, X_test=df_test)\n    cv.fit(df[hh_head], df.Target[hh_head])\n    print(cv.best_score_, cv.best_params_)\n    print()\n    print([cv.cv_results_[f'split{i}_test_score'][cv.best_index_] for i in range(5)])\n    print(zconfint([cv.cv_results_[f'split{i}_test_score'][cv.best_index_] for i in range(5)]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ad4a921b77a744be23acc38a5186a2b21b69a899"},"cell_type":"code","source":"df_test = pd.read_csv('../input/test.csv')\nfeature_generators.fit_generate(df, df.Target, X_test=df_test) # generate new feats on train dataset\nsum_pipeline.fit(df[hh_head], df.Target[hh_head])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1a25ab80bd605d32cecfe8d767a315355a9b414c"},"cell_type":"code","source":"feature_generators.generate(df_test, None) # generate new feats on test dataset\nres = pd.DataFrame(df_test.Id)\nres['Target'] = sum_pipeline.predict(df_test)\nres.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5011b7a080365ccb2ba8f8089ed35112a205ae1b"},"cell_type":"markdown","source":"### Creating submissions\nOne of submissions have #6 score on LB 0.446"},{"metadata":{"trusted":true,"_uuid":"0f249ed852454c94ffe86e8773a1b583483ad2f9"},"cell_type":"code","source":"for w in np.linspace(0.2, 0.8, 7):\n    sum_pipeline.estimators_weight = [1-w, w]\n    res['Target'] = sum_pipeline.predict(df_test)\n    rounded_w = np.round(w, 2)\n    res.to_csv(f'submission-{rounded_w}.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}