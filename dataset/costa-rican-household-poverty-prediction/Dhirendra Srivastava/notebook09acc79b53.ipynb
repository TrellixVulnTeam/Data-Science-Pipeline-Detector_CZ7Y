{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<hr>\n\n<p>\n<font size=10px color=\"navy\"><center><b>Income Qualification Project\n<P>","metadata":{}},{"cell_type":"markdown","source":"\n<hr>   \n    \n<font size = 5px  color ='red'><center><b>Identify the level of income qualification needed for the families in Latin America.</b></font>\n    \n<hr>\n    \n    \n    \n<font size = 5px><b>Problem Statement Scenario:</b> </font>\n    \n    \n    \n    \nMany social programs have a hard time making sure the right people are given enough aid. It’s tricky when a program focuses on the poorest segment of the population. This segment of population can’t provide the necessary income and expense records to prove that they qualify.\n\nIn Latin America, a popular method called Proxy Means Test (PMT) uses an algorithm to verify income qualification. With PMT, agencies use a model that considers a family’s observable household attributes like the material of their walls and ceiling or the assets found in their homes to classify them and predict their level of need. While this is an improvement, accuracy remains a problem as the region’s population grows and poverty declines.\n\nThe Inter-American Development Bank (IDB) believes that new methods beyond traditional econometrics, based on a dataset of Costa Rican household characteristics, might help improve PMT’s performance.\n\n\n   \n<hr>\n","metadata":{}},{"cell_type":"markdown","source":"<font color = \"#191970\" size=6px><center><b>Import Required Libraries</b></font>","metadata":{}},{"cell_type":"code","source":"# basic\nimport pandas as pd, numpy as np\n\n# Viusalization \nimport seaborn as sns\nfrom matplotlib import pyplot as plt, style\n\n# Feature Engineering/ Feature Selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import model_selection\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.preprocessing import StandardScaler\n\n\n\n# Model Bulding and performance metrics\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\n\n# Set warnings off\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:21:03.122718Z","iopub.execute_input":"2021-05-26T05:21:03.123132Z","iopub.status.idle":"2021-05-26T05:21:03.130116Z","shell.execute_reply.started":"2021-05-26T05:21:03.123097Z","shell.execute_reply":"2021-05-26T05:21:03.129124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font color = \"#191970\" size=6px><center><b>Data Collection","metadata":{}},{"cell_type":"code","source":"# Training Data \n\ndf_train = pd.read_csv(\"/kaggle/input/costa-rican-household-poverty-prediction/train.csv\")\n\n# Testing Data\n\ndf_test = pd.read_csv(\"/kaggle/input/costa-rican-household-poverty-prediction/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:21:03.135553Z","iopub.execute_input":"2021-05-26T05:21:03.136207Z","iopub.status.idle":"2021-05-26T05:21:03.617294Z","shell.execute_reply.started":"2021-05-26T05:21:03.136166Z","shell.execute_reply":"2021-05-26T05:21:03.616386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font color = \"#191970\" size=8px><center><b>Data Cleaning And EDA","metadata":{}},{"cell_type":"code","source":"# Viewing The Shape of test amd traim data\n\nprint(\"\\n\")\nprint(\"Shape of Train data : \",df_train.shape)\nprint(\"Shape of Test data : \",df_test.shape)\nprint(\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:21:03.618894Z","iopub.execute_input":"2021-05-26T05:21:03.619329Z","iopub.status.idle":"2021-05-26T05:21:03.62551Z","shell.execute_reply.started":"2021-05-26T05:21:03.619286Z","shell.execute_reply":"2021-05-26T05:21:03.624487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##       1. Identify the output variable.","metadata":{}},{"cell_type":"code","source":"# Test Data Doesnt Have Output column therefore we check the missing column in test data\n\nprint(\"\\n\")\nfor column in df_train.columns:\n    if column not in df_test.columns:\n        print(\"Output Variable : \",column)\nprint(\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:21:03.627041Z","iopub.execute_input":"2021-05-26T05:21:03.627351Z","iopub.status.idle":"2021-05-26T05:21:03.646075Z","shell.execute_reply.started":"2021-05-26T05:21:03.62732Z","shell.execute_reply":"2021-05-26T05:21:03.644887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Understand the type of data.","metadata":{}},{"cell_type":"code","source":"# Info of data\n\ndf_train.info()","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:21:03.647634Z","iopub.execute_input":"2021-05-26T05:21:03.64808Z","iopub.status.idle":"2021-05-26T05:21:03.6873Z","shell.execute_reply.started":"2021-05-26T05:21:03.648045Z","shell.execute_reply":"2021-05-26T05:21:03.685985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Getting List Of Data types For each Columns \n\nprint(\"\\n\")\nprint('Integer Columns :', list(df_train.select_dtypes(np.int64).columns))\nprint(\"\\n\")\nprint('Float Columns :', list(df_train.select_dtypes(np.float64).columns))\nprint(\"\\n\")\nprint('Categorical Columns :', list(df_train.select_dtypes(np.object).columns))\nprint(\"\\n\")\n","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:21:03.691Z","iopub.execute_input":"2021-05-26T05:21:03.691571Z","iopub.status.idle":"2021-05-26T05:21:03.712883Z","shell.execute_reply.started":"2021-05-26T05:21:03.691531Z","shell.execute_reply":"2021-05-26T05:21:03.711645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Count Of each Datatype cols\n\nprint(\"\\n\")\nprint(\" Count for each datatype columns : \\n\\n\",df_train.dtypes.value_counts())\nprint(\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:21:03.717107Z","iopub.execute_input":"2021-05-26T05:21:03.717597Z","iopub.status.idle":"2021-05-26T05:21:03.727804Z","shell.execute_reply.started":"2021-05-26T05:21:03.717548Z","shell.execute_reply":"2021-05-26T05:21:03.726901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Exploring categorical Features\n\nprint('\\n')\nfor column in df_train.columns:\n    type = df_train[column].dtype\n    if type == 'object':\n        print(column)\n\nprint('\\n')","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:21:03.728984Z","iopub.execute_input":"2021-05-26T05:21:03.729302Z","iopub.status.idle":"2021-05-26T05:21:03.743723Z","shell.execute_reply.started":"2021-05-26T05:21:03.729273Z","shell.execute_reply":"2021-05-26T05:21:03.742338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<hr>\n\n<font color = \"red\" size=4px><b> Data Dictionary For Above Object Variables </font>\n\n- Id : Unique ID\n- idhogar :   Household level identifier\n- dependency :   Dependency rate, calculated = (number of members of the household younger than 19 or older than 64)/(number of member of household between 19 and 64)\n- edjefe :   Years of education of Male head of household, based on the interaction of escolari (years of education), head of household and gender, yes=1 and no=0\n- edjefa :   Years of education of Female head of household, based on the interaction of escolari (years of education), head of household and gender, yes=1 and no=0\n    \n<font color = \"blue\" size=4px> <b>Interpretation :</b>*</font><font color = \"red\" size=4px> As Id and idhogar will have no significant variance for prediction,  we will drop them</font>\n<hr>","metadata":{}},{"cell_type":"code","source":"# Print Categorical Features\n\nprint('\\n')\nfor column in df_train.columns:\n    type = df_train[column].dtype\n    if type == 'object':\n        print(column)\n\nprint('\\n')","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:21:03.7452Z","iopub.execute_input":"2021-05-26T05:21:03.745605Z","iopub.status.idle":"2021-05-26T05:21:03.754067Z","shell.execute_reply.started":"2021-05-26T05:21:03.745568Z","shell.execute_reply":"2021-05-26T05:21:03.752498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking For Unique Values of each Categorical Features\n\nprint(\"\\n\")\nprint(\"Dependency : \\n\",df_train.dependency.value_counts())\nprint(\"\\n\")\nprint(\"edjefe : \\n\",df_train.edjefe.value_counts())\nprint(\"\\n\")\nprint(\"edjefe :\\n \",df_train.edjefe.value_counts())\nprint(\"\\n\")\n","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:21:03.756138Z","iopub.execute_input":"2021-05-26T05:21:03.756507Z","iopub.status.idle":"2021-05-26T05:21:03.789834Z","shell.execute_reply.started":"2021-05-26T05:21:03.756446Z","shell.execute_reply":"2021-05-26T05:21:03.788664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function for encoding categorical values in column\n\ndef encode_to_numeric(value):\n    if value == 'yes':\n        return(float(1))\n    elif value == 'no':\n        return(float(0))\n    else:\n        return(float(value))\n    \n# Applying Fucntion to Categorical Features in train data\n\ncols=['dependency','edjefe','edjefa']\n\nfor col in cols:\n    df_train[col] = df_train[col].apply(encode_to_numeric)\n\n# Applying Fucntion to Categorical Features in test data\n\nfor col in cols:\n    df_test[col] = df_test[col].apply(encode_to_numeric)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:21:03.79159Z","iopub.execute_input":"2021-05-26T05:21:03.79188Z","iopub.status.idle":"2021-05-26T05:21:03.868272Z","shell.execute_reply.started":"2021-05-26T05:21:03.791853Z","shell.execute_reply":"2021-05-26T05:21:03.867135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check if any categorical Features Remaining..\n\nprint(\"\\n\")\nprint(\" Count for each datatype columns : \\n\\n\",df_train.dtypes.value_counts())\nprint(\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:21:03.869758Z","iopub.execute_input":"2021-05-26T05:21:03.870063Z","iopub.status.idle":"2021-05-26T05:21:03.879005Z","shell.execute_reply.started":"2021-05-26T05:21:03.870032Z","shell.execute_reply":"2021-05-26T05:21:03.877371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Count how many null values are existing in columns.","metadata":{}},{"cell_type":"code","source":"# Check for missing Values:\n\n# Missing Values for integer features\nprint(\"\\n\")\nnull_counts_int = df_train.select_dtypes(np.int64).isnull().sum()\nprint(\"Integer Variables : \",null_counts_int[null_counts_int > 0])\nprint(\"\\n\")\n\n# Missing Values for Float features\n\nnull_counts_float = df_train.select_dtypes(np.float64).isnull().sum()\nprint(\"Float Variables :\\n\\n \",null_counts_float[null_counts_float > 0])\nprint(\"\\n\")\n\n# Missing Values in Target Variable\n\nprint('Target variable : ', df_train.Target.isna().sum())\nprint(\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:21:03.881223Z","iopub.execute_input":"2021-05-26T05:21:03.881736Z","iopub.status.idle":"2021-05-26T05:21:03.92975Z","shell.execute_reply.started":"2021-05-26T05:21:03.881678Z","shell.execute_reply":"2021-05-26T05:21:03.928607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<hr>\n\n<font color = \"red\" size=4px><b> Data Dictionary For Above Float Variables </font>\n\n- v2a1 : Monthly rent payment \n- v18q1 : number of tablets household owns\n- rez_esc : Years behind in school\n\n- SQBmeaned : square of the mean years of education of adults (>=18) in the household\n- meaneduc : average years of education for adults (18+)\n    \n\n<hr>","metadata":{}},{"cell_type":"markdown","source":"<font color = \"maroon\" size=4px> Lets Check the missing rent payments with respect to house ownership status","metadata":{}},{"cell_type":"code","source":"# Variables Indication House Ownership Status: \n\nownership_status_cols = [column for column in df_train if column.startswith('tipovivi')]\n\n# Plot Missing Rents count for each Ownership status\n\nstyle.use('ggplot')\ndf_train.loc[df_train.v2a1.isnull(), ownership_status_cols].sum().plot.bar(figsize = (10, 8), color = 'royalblue', edgecolor = 'k', linewidth = 3)\nplt.xticks(ticks=[0, 1, 2, 3, 4],labels = ['Owns and Paid Off', 'Owns and Paying', 'Rented', 'Precarious', 'Other'])\nplt.title('Missing Rents count for each Ownership status', size = 15,pad=15);                                                              ","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:21:03.933791Z","iopub.execute_input":"2021-05-26T05:21:03.934153Z","iopub.status.idle":"2021-05-26T05:21:04.187343Z","shell.execute_reply.started":"2021-05-26T05:21:03.934121Z","shell.execute_reply":"2021-05-26T05:21:04.185724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<hr>\n\n<font color = \"blue\" size=4px> <b>Interpretation :</b></font><font color = \"red\" size=4px> From above Intepretation we can infere that house which is owned and fully paid has most missing rent and which makes sense, so we will fill all the missing rents with 0</font>\n   \n<hr>","metadata":{}},{"cell_type":"code","source":"# Filling missing rents with zero for train and test data\n\ndf_train.v2a1.fillna(0,inplace=True)\ndf_test.v2a1.fillna(0,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:21:04.189317Z","iopub.execute_input":"2021-05-26T05:21:04.189777Z","iopub.status.idle":"2021-05-26T05:21:04.197259Z","shell.execute_reply.started":"2021-05-26T05:21:04.189735Z","shell.execute_reply":"2021-05-26T05:21:04.196175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font color = \"maroon\" size=4px>Lets Check the missing number of tablets household owns with respect to (v18q) if head of house (parentesco1) owns a tablet or not","metadata":{}},{"cell_type":"code","source":"# rows for head of house\n\nhead_of_house = df_train.loc[df_train.parentesco1 == 1].copy()\n\n# Group the number of talets a household owns with head of house owing a tablet or not\n\nCount_na_talets = pd.DataFrame(head_of_house.groupby('v18q')['v18q1'].apply(lambda x: x.isnull().sum())).reset_index().rename(columns={'v18q1':'missing Value Count for number of tablest household owns','v18q':'Head of house Owning Tablet'})\n\n# Plotting The count of Missing tablets count of household with respect to Ownership of tablet of Head of House\n\nplt.figure(figsize = (12,6))\nsns.barplot(Count_na_talets['Head of house Owning Tablet'],Count_na_talets['missing Value Count for number of tablest household owns'],color='cyan')\nplt.title('The count of Missing tablets count of household with respect to Ownership of tablet of Head of House')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:21:04.199208Z","iopub.execute_input":"2021-05-26T05:21:04.199875Z","iopub.status.idle":"2021-05-26T05:21:04.381134Z","shell.execute_reply.started":"2021-05-26T05:21:04.199823Z","shell.execute_reply":"2021-05-26T05:21:04.380161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<hr>\n\n<font color = \"blue\" size=4px> <b>Interpretation :<b></font> <font color = \"red\" size=4px> From above Intepretation we can infere that if Head of House doesnt owns a tablet, there will be zero number of tablets a household owns , so we will replace all missing count value with 0 </font>\n<hr>","metadata":{}},{"cell_type":"code","source":"# Filling missing tablets value with zero in train amd test data sets\n\nfor column in [df_train,df_test]:\n    column['v18q1'].fillna(0,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:21:04.382645Z","iopub.execute_input":"2021-05-26T05:21:04.382967Z","iopub.status.idle":"2021-05-26T05:21:04.388977Z","shell.execute_reply.started":"2021-05-26T05:21:04.382933Z","shell.execute_reply":"2021-05-26T05:21:04.38789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font color = \"maroon\" size=4px> Lets Look at rez_esc : Years behind in school with null values(7928),\n    columns related is Age lets compare it \n\n","metadata":{}},{"cell_type":"code","source":"df_train[df_train['rez_esc'].notnull()]['age'].describe()","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:21:04.390846Z","iopub.execute_input":"2021-05-26T05:21:04.391263Z","iopub.status.idle":"2021-05-26T05:21:04.41349Z","shell.execute_reply.started":"2021-05-26T05:21:04.39123Z","shell.execute_reply":"2021-05-26T05:21:04.412326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font color = \"maroon\" size=3px> min age is 7 and max is 17 \n","metadata":{}},{"cell_type":"code","source":"# rez_esc has some value when except the nulls lets Verify it\n\ndf_train.loc[df_train['rez_esc'].isnull()]['age'].describe()","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:21:04.415045Z","iopub.execute_input":"2021-05-26T05:21:04.415374Z","iopub.status.idle":"2021-05-26T05:21:04.431562Z","shell.execute_reply.started":"2021-05-26T05:21:04.415342Z","shell.execute_reply":"2021-05-26T05:21:04.43003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font color = \"maroon\" size=3px> the column has value when not null , thus we will not drop it","metadata":{}},{"cell_type":"code","source":"# lets find the age which is behind school in range of AGE 7-17\n\ndf_train.loc[df_train.rez_esc.isna() & (df_train.age > 7) & (df_train.age < 17)]['age']","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:21:04.433326Z","iopub.execute_input":"2021-05-26T05:21:04.433791Z","iopub.status.idle":"2021-05-26T05:21:04.445589Z","shell.execute_reply.started":"2021-05-26T05:21:04.433745Z","shell.execute_reply":"2021-05-26T05:21:04.444367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<hr>\n\n<font color = \"blue\" size=4px> <b>Interpretation :</b></font><font color = \"red\" size=4px> From above Intepretation we can infere that there's only one member in family who's is behind in school with age of 10, lets fill it with zero</font>\n<hr>","metadata":{}},{"cell_type":"code","source":"# Filling null values with Zero in Train and test data\n\nfor column in [df_train,df_test]:\n    column['rez_esc'].fillna(0,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:21:04.447183Z","iopub.execute_input":"2021-05-26T05:21:04.447523Z","iopub.status.idle":"2021-05-26T05:21:04.463102Z","shell.execute_reply.started":"2021-05-26T05:21:04.447487Z","shell.execute_reply":"2021-05-26T05:21:04.461971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<hr>\n\nLets look at meaneduc   (total nulls: 5) : average years of education for adults (18+)  \n why the null values, Lets look at few rows with nulls in meaneduc\n\n\nColumns related to average years of education for adults (18+)  \n- edjefe\n- edjefa\n- instlevel1, =1 no level of education\n- instlevel2, =1 incomplete primary ","metadata":{}},{"cell_type":"code","source":"# Lets eplore few rows of meaneduc null values\n\nmeaneduc_null_rows = df_train[df_train.meaneduc.isnull()].head()\n\ncolumns = ['edjefe','edjefa','instlevel1','instlevel2']\n\nmeaneduc_null_rows[columns][meaneduc_null_rows[columns]['instlevel1'] > 0].count()","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:21:04.464837Z","iopub.execute_input":"2021-05-26T05:21:04.465294Z","iopub.status.idle":"2021-05-26T05:21:04.482885Z","shell.execute_reply.started":"2021-05-26T05:21:04.465237Z","shell.execute_reply":"2021-05-26T05:21:04.481977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<hr>\n\n<font color = \"blue\" size=4px> <b>Interpretation :</b></font><font color = \"red\" size=4px> From above Intepretation we can infere that meaneduc is null when no level of education is 0 , lets fill the missing values with na for both meaneduc and SQBmeaned</font>\n<hr>","metadata":{}},{"cell_type":"code","source":"# Filling The SQBmeaned amd meaneduc with 0 values in train and test data\n\nfor column in [df_train,df_test]:\n    column['meaneduc'].fillna(0,inplace = True)\n    column['SQBmeaned'].fillna(0,inplace = True)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:21:04.484137Z","iopub.execute_input":"2021-05-26T05:21:04.484698Z","iopub.status.idle":"2021-05-26T05:21:04.496171Z","shell.execute_reply.started":"2021-05-26T05:21:04.48466Z","shell.execute_reply":"2021-05-26T05:21:04.494943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lets check for Missing Values \ndf_train.isnull().sum().value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:21:04.497803Z","iopub.execute_input":"2021-05-26T05:21:04.498176Z","iopub.status.idle":"2021-05-26T05:21:04.51827Z","shell.execute_reply.started":"2021-05-26T05:21:04.498139Z","shell.execute_reply":"2021-05-26T05:21:04.51707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Check if there is a house without a family head.","metadata":{}},{"cell_type":"code","source":"# Grouping Household with head of house counts\n\nhouse_wihtout_heads = pd.DataFrame(df_train.groupby(\"idhogar\")['parentesco1'].sum().value_counts().reset_index()).rename(columns={'index':'Heads(Yes/no)','parentesco1':'Household counts'})\n\nhouse_wihtout_heads","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:21:04.520239Z","iopub.execute_input":"2021-05-26T05:21:04.520649Z","iopub.status.idle":"2021-05-26T05:21:04.549074Z","shell.execute_reply.started":"2021-05-26T05:21:04.520612Z","shell.execute_reply":"2021-05-26T05:21:04.5479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<hr>\n\n- parentescol1 : 1 if household head\n\n\n<font color = \"blue\" size=4px> <b>Interpretation :</b></font><font color = \"red\" size=4px> From above Intepretation we can infere that there are <b>15</b> Households Without Family head and <b>2973</b> Households with Family Head</font>\n<hr>","metadata":{}},{"cell_type":"markdown","source":"## 5. Check whether all members of the house have the same poverty level.","metadata":{}},{"cell_type":"code","source":"# Group the housholds with target values and get unique Counts\n\nall_equal = df_train.groupby(\"idhogar\")['Target'].apply(lambda x : x.nunique()==1)\n\n# Get number of Households Which are not having same target value\n\nnot_equal = all_equal[all_equal != True]\n\nprint(\"\\n\")\nprint(\"Number of households with family members not having similar poverty level : \",len(not_equal))\nprint(\"Number of households with family members having similar poverty level : \",len(all_equal))\nprint(\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:21:04.551126Z","iopub.execute_input":"2021-05-26T05:21:04.551597Z","iopub.status.idle":"2021-05-26T05:21:05.106973Z","shell.execute_reply.started":"2021-05-26T05:21:04.55155Z","shell.execute_reply":"2021-05-26T05:21:05.105601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<hr>\n\n<font color = \"blue\" size=4px> <b>Interpretation :</b></font> <font color = \"red\" size=4px> From above Intepretation we can infere that there are <b>85*</b>Households with family members not having similar poverty level and <b>2988</b> with  similar poverty level</font>\n<hr>","metadata":{}},{"cell_type":"markdown","source":"## 6. Set the poverty level of the members and the head of the house same in a family.","metadata":{}},{"cell_type":"code","source":"# Iterate through each household\n\n\nfor household in not_equal.index:\n    \n    # Find the correct label (for the head of household)\n    \n    true_target = int(df_train[(df_train['idhogar'] == household) & (df_train['parentesco1'] == 1.0)]['Target'])\n    \n    # Set the correct label for all members in the household\n    \n    df_train.loc[df_train['idhogar'] == household, 'Target'] = true_target\n    \n    \n# Groupby the household and figure out the number of unique values\n\nall_equal = df_train.groupby('idhogar')['Target'].apply(lambda x: x.nunique() == 1)\n\n# Households where targets are not all equal\n\nnot_equal = all_equal[all_equal != True]\nprint(\"\\n\")\nprint('There are {} households where the family members do not all have the same target.'.format(len(not_equal)))\nprint(\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:21:05.108805Z","iopub.execute_input":"2021-05-26T05:21:05.109269Z","iopub.status.idle":"2021-05-26T05:21:06.021301Z","shell.execute_reply.started":"2021-05-26T05:21:05.109219Z","shell.execute_reply":"2021-05-26T05:21:06.020282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 7. Check if there are any biases in our dataset.","metadata":{}},{"cell_type":"markdown","source":"<hr>\n\n<font color = \"red\" size=4px> Target(Poverty Level):-</font>\n\n- 1 = extreme poverty \n- 2 = moderate poverty\n- 3 = vulnerable households\n- 4 = non vulnerable households ","metadata":{}},{"cell_type":"code","source":"# Getting value_counts for each target\n\ntarget_counts = df_train.Target.value_counts().sort_index()\n\n# Plot The Count for each Poverty Level\nprint(\"\\n\")\ntarget_counts.plot.bar(figsize = (12,6),linewidth=3,edgecolor='k')\nplt.xlabel(\"Poverty Levels\",size = 16)\nplt.ylabel(\"Count\",size = 16)\nplt.xticks([0,1,2,3],['extreme poverty','moderate poverty','vulnerable households','non vulnerable households'],rotation=20)\nplt.title(\"Poverty Level Counts\",pad=30,size=16)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:21:06.023014Z","iopub.execute_input":"2021-05-26T05:21:06.023336Z","iopub.status.idle":"2021-05-26T05:21:06.222936Z","shell.execute_reply.started":"2021-05-26T05:21:06.023303Z","shell.execute_reply":"2021-05-26T05:21:06.22182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<hr>\n\n<font color = \"blue\" size=4px> <b>Interpretation :</b></font><font color = \"red\" size=4px> From above Intepretation we can infere that  Extreme Poverty has samallest count whereas Non Vulnerable households are having highest count , Therefore the <b>Data is Biased<b></font>\n<hr>","metadata":{}},{"cell_type":"markdown","source":"## 8. Clean the data and remove variables which are not significant ","metadata":{}},{"cell_type":"code","source":"# Remove Unwanted columns from train and test\n\ncolumns = ['SQBescolari', 'SQBage', 'SQBhogar_total', 'SQBedjefe','SQBhogar_nin', 'SQBovercrowding', 'SQBdependency', 'SQBmeaned', 'agesq','Id','idhogar']\n\nfor df in [df_train,df_test]:\n    df.drop(columns=columns,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:21:06.224353Z","iopub.execute_input":"2021-05-26T05:21:06.224676Z","iopub.status.idle":"2021-05-26T05:21:06.25384Z","shell.execute_reply.started":"2021-05-26T05:21:06.224645Z","shell.execute_reply":"2021-05-26T05:21:06.252436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 8.1. Remove Features with Zero Variance","metadata":{}},{"cell_type":"code","source":"# Check for Columns with Zero Variance In data Set\n\nvar_thres = VarianceThreshold(threshold=0.001)\n\nfeatures_no_output = df_train.drop(columns='Target')\nvar_thres.fit(features_no_output)\n\n# getting columns with Zero variance\n\nconstant_columns = [column for column in features_no_output.columns if column not in features_no_output.columns[var_thres.get_support()]]\n\nprint(\"Varible with Zero Variance : \",constant_columns)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:21:06.255317Z","iopub.execute_input":"2021-05-26T05:21:06.255704Z","iopub.status.idle":"2021-05-26T05:21:06.302683Z","shell.execute_reply.started":"2021-05-26T05:21:06.255668Z","shell.execute_reply":"2021-05-26T05:21:06.301743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<hr>\n\n<font color = \"blue\" size=4px> <b>Interpretation :</b></font><font color = \"red\" size=4px> From above Intepretation we can infere that </font>\n\n- elimbasu5 : (1 if rubbish disposal mainly by throwing in river, creek or sea). is having zero varaince \n- pisoother : 1 if predominant material on the floor is other\n- planpri  : 1 electricity from private plant\n    \n <font color = \"red\" size=4px> are having almost zero Variance ,so we will drop it<>/font\n    \n<hr>","metadata":{}},{"cell_type":"code","source":"# Dropping columns with zeri variance in Train and test data\n\nfor column in [df_train,df_test]:\n    column.drop(constant_columns,axis=1,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:21:06.304135Z","iopub.execute_input":"2021-05-26T05:21:06.304494Z","iopub.status.idle":"2021-05-26T05:21:06.328922Z","shell.execute_reply.started":"2021-05-26T05:21:06.304445Z","shell.execute_reply":"2021-05-26T05:21:06.327769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 5.2. Lets Remove Features which are Highly Correlated","metadata":{}},{"cell_type":"code","source":"# Creating a function through which we can select highly correlated features and dropping the first feature that is correlated with any other feature\n\ndef correlation(dataset, threshold):\n    col_corr = set() # Set of all names of correlated columns\n    corr_matrix = dataset.corr() # Correlation Matrix\n    \n    for i in range(len(corr_matrix.columns)):\n        for j in range(i):\n            if abs(corr_matrix.iloc[i, j]) > threshold:    # we are interested in absolute coeff value\n                    \n                colname = corr_matrix.columns[i] # getting the name of columns\n                col_corr.add(colname)\n    return(col_corr)\n","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:21:06.330448Z","iopub.execute_input":"2021-05-26T05:21:06.330859Z","iopub.status.idle":"2021-05-26T05:21:06.347811Z","shell.execute_reply.started":"2021-05-26T05:21:06.330821Z","shell.execute_reply":"2021-05-26T05:21:06.34673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Getting the Features with High Correlation\n\nfeatures_no_output = df_train.drop(columns=['Target'])\n\ncorr_features = correlation(features_no_output,0.90) # Setting Threshold as features having correlation above 85%\nprint(\"\\n\")\nprint(\"Correlated Features :\\n \",corr_features)\nprint(\"\\n\")\nprint(\"No. of Features Correlated: \",len(corr_features))","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:21:06.354942Z","iopub.execute_input":"2021-05-26T05:21:06.35534Z","iopub.status.idle":"2021-05-26T05:21:07.064185Z","shell.execute_reply.started":"2021-05-26T05:21:06.355309Z","shell.execute_reply":"2021-05-26T05:21:07.062804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<hr>\n\n<font color = \"blue\" size=4px> <b>Interpretation :</b></font><font color = \"red\" size=4px> From above Intepretation we can infere that Using Pearson Correlation we were able find out 10 Highly Correlated Features with <b>90% Correlation</b> with other Features In DataSet</font>\n\n<font color = \"red\" size=4px> We can remove those features to avoid dimentionality curse</font>\n\n<hr>","metadata":{}},{"cell_type":"code","source":"# Dropping the Highly Correlated Features From Train and Test Data set\n\nfor df in [df_train,df_test]:\n    df.drop(columns=corr_features,inplace = True)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:21:07.066667Z","iopub.execute_input":"2021-05-26T05:21:07.067146Z","iopub.status.idle":"2021-05-26T05:21:07.087939Z","shell.execute_reply.started":"2021-05-26T05:21:07.067095Z","shell.execute_reply":"2021-05-26T05:21:07.086626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check if any correlated features Remaining\n\ncorr_features = correlation(df_train,0.90) # Setting Threshold as features having correlation above 85%\nprint(\"\\n\")\nprint(\"No. of Features Correlated: \",len(corr_features))\nprint(\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:21:07.090009Z","iopub.execute_input":"2021-05-26T05:21:07.090521Z","iopub.status.idle":"2021-05-26T05:21:07.740234Z","shell.execute_reply.started":"2021-05-26T05:21:07.090323Z","shell.execute_reply":"2021-05-26T05:21:07.738907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font color = \"#191970\" size=8px><center><b> Model Building","metadata":{}},{"cell_type":"markdown","source":"#### 1. Defining Features and Target","metadata":{}},{"cell_type":"code","source":"Features = df_train.drop(columns='Target')\nTarget = df_train.Target","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:21:07.742167Z","iopub.execute_input":"2021-05-26T05:21:07.742656Z","iopub.status.idle":"2021-05-26T05:21:07.754193Z","shell.execute_reply.started":"2021-05-26T05:21:07.742606Z","shell.execute_reply":"2021-05-26T05:21:07.752605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 1.1 define Train Test Split","metadata":{}},{"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(Features,Target,random_state = 1)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:21:07.755803Z","iopub.execute_input":"2021-05-26T05:21:07.756234Z","iopub.status.idle":"2021-05-26T05:21:07.779857Z","shell.execute_reply.started":"2021-05-26T05:21:07.756196Z","shell.execute_reply":"2021-05-26T05:21:07.778771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2. Standardise the data","metadata":{}},{"cell_type":"code","source":"Features_std = StandardScaler().fit_transform(Features)\nx_train_std = StandardScaler().fit_transform(x_train)\nx_test_std = StandardScaler().fit_transform(x_test)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:21:07.781293Z","iopub.execute_input":"2021-05-26T05:21:07.78161Z","iopub.status.idle":"2021-05-26T05:21:07.878659Z","shell.execute_reply.started":"2021-05-26T05:21:07.78158Z","shell.execute_reply":"2021-05-26T05:21:07.877584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Shape Of Split\nprint(\"Shape Of Train : \",x_train_std.shape)\nprint(\"Shape Of Test  : \",x_test_std.shape)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:21:07.880432Z","iopub.execute_input":"2021-05-26T05:21:07.880813Z","iopub.status.idle":"2021-05-26T05:21:07.886897Z","shell.execute_reply.started":"2021-05-26T05:21:07.880779Z","shell.execute_reply":"2021-05-26T05:21:07.885968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3. Model Training","metadata":{}},{"cell_type":"code","source":"# Instantiate estimator Object\n\nrfmodel1 = RandomForestClassifier(criterion='entropy', max_depth=25, min_samples_leaf=1,\n                       min_samples_split=2, n_estimators=5000,n_jobs=-1,random_state=1)\n\n# Fit The Model\n\nrfmodel1.fit(x_train_std,y_train)\n\n# Performance Metrics\nprint(\"\\n\")\nprint(\"Train Score : \",rfmodel1.score(x_train_std,y_train))\nprint(\"Test  Score : \",rfmodel1.score(x_test_std,y_test))","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:21:07.88913Z","iopub.execute_input":"2021-05-26T05:21:07.88952Z","iopub.status.idle":"2021-05-26T05:21:51.493971Z","shell.execute_reply.started":"2021-05-26T05:21:07.889455Z","shell.execute_reply":"2021-05-26T05:21:51.492899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### ","metadata":{}},{"cell_type":"code","source":"# KFold Cross Validation\n\nkf = KFold(n_splits=10,shuffle=True,random_state=1)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:21:51.495601Z","iopub.execute_input":"2021-05-26T05:21:51.496005Z","iopub.status.idle":"2021-05-26T05:21:51.500755Z","shell.execute_reply.started":"2021-05-26T05:21:51.495971Z","shell.execute_reply":"2021-05-26T05:21:51.499564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Instantiate Estimator Object \n\nrfModel2 = RandomForestClassifier()\n\n# Fit the Model\n\nrfModel2.fit(Features_std,Target)\n\n#Predictions\n\nprediction = model_selection.cross_val_predict(rfModel2,Features_std,Target,cv=kf)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:21:51.502328Z","iopub.execute_input":"2021-05-26T05:21:51.502789Z","iopub.status.idle":"2021-05-26T05:22:07.37554Z","shell.execute_reply.started":"2021-05-26T05:21:51.502747Z","shell.execute_reply":"2021-05-26T05:22:07.374473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Classification Report : \\n\\n\",metrics.classification_report(Target,prediction))","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:22:07.377333Z","iopub.execute_input":"2021-05-26T05:22:07.378148Z","iopub.status.idle":"2021-05-26T05:22:07.402649Z","shell.execute_reply.started":"2021-05-26T05:22:07.378098Z","shell.execute_reply":"2021-05-26T05:22:07.401442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.crosstab(Target,prediction)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:22:07.404491Z","iopub.execute_input":"2021-05-26T05:22:07.405267Z","iopub.status.idle":"2021-05-26T05:22:07.44347Z","shell.execute_reply.started":"2021-05-26T05:22:07.405204Z","shell.execute_reply":"2021-05-26T05:22:07.44251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<hr>\n\n<font color = \"blue\" size=4px> <b>Interpretation :</b></font><font color = \"red\" size=4px> Model Achieved <b>95% Accuracy</b> cosidering all Features</font>\n\n<font color = \"red\" size=4px> Lets now take Most Significant Features</font>\n\n<hr>","metadata":{}},{"cell_type":"markdown","source":"### Get Important Features","metadata":{}},{"cell_type":"code","source":"# Get Important Features\n\nlabels = list(x_train)\nfeature_importances = pd.DataFrame({'Importance':rfmodel1.feature_importances_},index=labels).sort_values(by='Importance',ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:22:07.444961Z","iopub.execute_input":"2021-05-26T05:22:07.445288Z","iopub.status.idle":"2021-05-26T05:22:08.830531Z","shell.execute_reply.started":"2021-05-26T05:22:07.445255Z","shell.execute_reply":"2021-05-26T05:22:08.829152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Features Having almost zero variance\n\nfeature_importances[feature_importances.Importance < 0.0001]","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:22:08.832629Z","iopub.execute_input":"2021-05-26T05:22:08.833033Z","iopub.status.idle":"2021-05-26T05:22:08.847146Z","shell.execute_reply.started":"2021-05-26T05:22:08.832995Z","shell.execute_reply":"2021-05-26T05:22:08.845384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lets plot them\n\nfeature_importances.Importance.sort_values(ascending=True).plot(kind='barh', figsize=(20, 20))\nplt.xlabel('Importance')\n\nplt.title(\"Feature Importances\")\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:22:08.849106Z","iopub.execute_input":"2021-05-26T05:22:08.849513Z","iopub.status.idle":"2021-05-26T05:22:10.750692Z","shell.execute_reply.started":"2021-05-26T05:22:08.849436Z","shell.execute_reply":"2021-05-26T05:22:10.749522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Number Of Features : \", feature_importances.value_counts().sum())","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:22:10.752178Z","iopub.execute_input":"2021-05-26T05:22:10.75255Z","iopub.status.idle":"2021-05-26T05:22:10.761753Z","shell.execute_reply.started":"2021-05-26T05:22:10.752512Z","shell.execute_reply":"2021-05-26T05:22:10.760692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<hr>\n\n<font color = \"blue\" size=4px> <b>Interpretation :</b></font><font color = \"red\" size=4px> From above Figure we can infere that Most features are having almost <b>zero significance</b> to the model</font>\n\n<font color = \"red\" size=4px> Lets now take Most Significant Features with <b>Threshold Of 0.01</b></font>\n\n<hr>","metadata":{}},{"cell_type":"code","source":"# Taking most Significant Features\n\nfeature_importances = feature_importances[feature_importances.Importance > 0.01]\n\nfeature_importances.Importance.sort_values(ascending=True).plot(kind='barh', figsize=(15, 11),color='royalblue')\nplt.xlabel('Importance')\nplt.title(\"Feature Importances\")\nplt.show()\n\nprint(\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:22:10.763006Z","iopub.execute_input":"2021-05-26T05:22:10.763528Z","iopub.status.idle":"2021-05-26T05:22:11.180681Z","shell.execute_reply.started":"2021-05-26T05:22:10.76345Z","shell.execute_reply":"2021-05-26T05:22:11.179428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Number Of Features : \", feature_importances.value_counts().sum())","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:22:11.181987Z","iopub.execute_input":"2021-05-26T05:22:11.182309Z","iopub.status.idle":"2021-05-26T05:22:11.192152Z","shell.execute_reply.started":"2021-05-26T05:22:11.182279Z","shell.execute_reply":"2021-05-26T05:22:11.190959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<hr>\n\n<font color = \"blue\" size=4px> <b>Interpretation :</b></font><font color = \"red\" size=4px> From above Figure we can infere that <b>meaneduc and dependency</b> are having highest significance to the model</font>\n\n<font color = \"red\" size=4px> We have reduced Features From <b>118 to 32</b></font>\n\n<hr>","metadata":{}},{"cell_type":"markdown","source":"<font color = \"#191970\" size=8px><center><b> Model Building With Significant Variables","metadata":{}},{"cell_type":"code","source":"# Getting Most Significant Features names\n\nSig_features = feature_importances.index","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:22:11.193843Z","iopub.execute_input":"2021-05-26T05:22:11.194216Z","iopub.status.idle":"2021-05-26T05:22:11.203211Z","shell.execute_reply.started":"2021-05-26T05:22:11.194177Z","shell.execute_reply":"2021-05-26T05:22:11.201933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 1. Defining Features and Target","metadata":{}},{"cell_type":"code","source":"Features = df_train[Sig_features]\nTarget = df_train.Target","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:22:11.204628Z","iopub.execute_input":"2021-05-26T05:22:11.204926Z","iopub.status.idle":"2021-05-26T05:22:11.220963Z","shell.execute_reply.started":"2021-05-26T05:22:11.204896Z","shell.execute_reply":"2021-05-26T05:22:11.219584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 1.1 define Train Test Split","metadata":{}},{"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(Features,Target,random_state = 1)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:22:11.222715Z","iopub.execute_input":"2021-05-26T05:22:11.223279Z","iopub.status.idle":"2021-05-26T05:22:11.238705Z","shell.execute_reply.started":"2021-05-26T05:22:11.223231Z","shell.execute_reply":"2021-05-26T05:22:11.237612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2. Standardise the data","metadata":{}},{"cell_type":"code","source":"Features_std = StandardScaler().fit_transform(Features)\nx_train_std = StandardScaler().fit_transform(x_train)\nx_test_std = StandardScaler().fit_transform(x_test)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:22:11.240753Z","iopub.execute_input":"2021-05-26T05:22:11.241286Z","iopub.status.idle":"2021-05-26T05:22:11.284836Z","shell.execute_reply.started":"2021-05-26T05:22:11.241238Z","shell.execute_reply":"2021-05-26T05:22:11.28371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Shape Of Split\nprint(\"Shape Of Train : \",x_train_std.shape)\nprint(\"Shape Of Test  : \",x_test_std.shape)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:22:11.286731Z","iopub.execute_input":"2021-05-26T05:22:11.287188Z","iopub.status.idle":"2021-05-26T05:22:11.293428Z","shell.execute_reply.started":"2021-05-26T05:22:11.287142Z","shell.execute_reply":"2021-05-26T05:22:11.292414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3. Grid Search","metadata":{}},{"cell_type":"code","source":"# Grid Search \ngrid_rf2= GridSearchCV(RandomForestClassifier(),param_grid={'n_estimators':[100],'criterion':['entropy'],\n                                                             'max_depth':[30,35],'min_samples_split':[1,2,3],\n                                                             'min_samples_leaf':[1,2,3]},n_jobs=-1)\n\ngrid_rf2.fit(x_train_std,y_train)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:22:11.295078Z","iopub.execute_input":"2021-05-26T05:22:11.295393Z","iopub.status.idle":"2021-05-26T05:22:38.247587Z","shell.execute_reply.started":"2021-05-26T05:22:11.295362Z","shell.execute_reply":"2021-05-26T05:22:38.246155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Getting Best Parameters \ngrid_rf2.best_params_","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:22:38.249566Z","iopub.execute_input":"2021-05-26T05:22:38.249943Z","iopub.status.idle":"2021-05-26T05:22:38.257029Z","shell.execute_reply.started":"2021-05-26T05:22:38.249908Z","shell.execute_reply":"2021-05-26T05:22:38.255758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3. Model Training","metadata":{}},{"cell_type":"code","source":"# Instantiate estimator Object\n\nrfmodel2 = RandomForestClassifier(criterion='entropy', max_depth=31, min_samples_leaf=1,\n                       min_samples_split=2, n_estimators=1000,n_jobs=-1,random_state=1)\n\n# Fit The Model\n\nrfmodel2.fit(x_train_std,y_train)\n\n# Performance Metrics\nprint(\"\\n\")\nprint(\"Train Score : \",rfmodel2.score(x_train_std,y_train))\nprint(\"Test  Score : \",rfmodel2.score(x_test_std,y_test))\nprint(\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:22:38.258626Z","iopub.execute_input":"2021-05-26T05:22:38.25897Z","iopub.status.idle":"2021-05-26T05:22:46.183964Z","shell.execute_reply.started":"2021-05-26T05:22:38.258939Z","shell.execute_reply":"2021-05-26T05:22:46.182749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Cross Validation","metadata":{}},{"cell_type":"code","source":"# KFold Cross Validation\n\nkf = KFold(n_splits=10,shuffle=True,random_state=1)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:22:46.185785Z","iopub.execute_input":"2021-05-26T05:22:46.186225Z","iopub.status.idle":"2021-05-26T05:22:46.191148Z","shell.execute_reply.started":"2021-05-26T05:22:46.186178Z","shell.execute_reply":"2021-05-26T05:22:46.190038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Instantiate Estimator Object \n\nrfModel2_sig = RandomForestClassifier(criterion='entropy', max_depth=31, min_samples_leaf=1,\n                       min_samples_split=2, n_estimators=1000,n_jobs=-1,random_state=1)\n\n# Fit the Model\n\nrfModel2_sig.fit(Features_std,Target)\n\n#Predictions\n\nprediction = model_selection.cross_val_predict(rfModel2_sig,Features_std,Target,cv=kf)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:22:46.193037Z","iopub.execute_input":"2021-05-26T05:22:46.193487Z","iopub.status.idle":"2021-05-26T05:23:58.153526Z","shell.execute_reply.started":"2021-05-26T05:22:46.193415Z","shell.execute_reply":"2021-05-26T05:23:58.152351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Classification Report : \\n\\n\",metrics.classification_report(Target,prediction))","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:23:58.155562Z","iopub.execute_input":"2021-05-26T05:23:58.155964Z","iopub.status.idle":"2021-05-26T05:23:58.178072Z","shell.execute_reply.started":"2021-05-26T05:23:58.155926Z","shell.execute_reply":"2021-05-26T05:23:58.177113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.crosstab(Target,prediction)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:23:58.181015Z","iopub.execute_input":"2021-05-26T05:23:58.181342Z","iopub.status.idle":"2021-05-26T05:23:58.211174Z","shell.execute_reply.started":"2021-05-26T05:23:58.18131Z","shell.execute_reply":"2021-05-26T05:23:58.210183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<hr>\n\n<font color = \"blue\" size=4px> <b>Interpretation :</b></font>\n\n<font color = \"red\" size=4px> Model Achieved <b>97% Accuracy</b> cosidering Significant Features</font>\n\n<font color = \"red\" size=4px> We increased Accuracy From <b>95% to 97%</b> below are Classification report:</font>\n    \n<font color = \"blue\" size=4px> <b>CLassification Report Model With All Features :</b></font> \n    \n  \n               precision    recall  f1-score   \n\n           1       0.99      0.88      0.93   \n           2       0.98      0.90      0.94   \n           3       0.99      0.83      0.90   \n           4       0.93      1.00      0.97   \n\n    accuracy                           0.95   \n\n\n    \n    \n<font color = \"blue\" size=4px> <b>CLassification Report Model With Significant :</b></font>  \n    \n    \n               precision    recall  f1-score   \n\n           1       0.98      0.93      0.96   \n           2       0.98      0.94      0.96   \n           3       0.99      0.91      0.95    \n           4       0.96      1.00      0.98      \n\n    accuracy                           0.97   \n \n \n\n    \n    \n    \n<hr>","metadata":{}},{"cell_type":"markdown","source":"## 8. Now lets do prediction on Unseen Test Data","metadata":{}},{"cell_type":"markdown","source":"#### 1. Select Most significant Features For test Data Set","metadata":{}},{"cell_type":"code","source":"df_test_sig = df_test[Sig_features]\n\ndf_test_sig.shape","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:23:58.212329Z","iopub.execute_input":"2021-05-26T05:23:58.212794Z","iopub.status.idle":"2021-05-26T05:23:58.22248Z","shell.execute_reply.started":"2021-05-26T05:23:58.21276Z","shell.execute_reply":"2021-05-26T05:23:58.221408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2. Standardise the data","metadata":{}},{"cell_type":"code","source":"df_test_sig = StandardScaler().fit_transform(df_test_sig)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:23:58.224244Z","iopub.execute_input":"2021-05-26T05:23:58.225021Z","iopub.status.idle":"2021-05-26T05:23:58.269035Z","shell.execute_reply.started":"2021-05-26T05:23:58.224966Z","shell.execute_reply":"2021-05-26T05:23:58.267669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3. Prediction","metadata":{}},{"cell_type":"code","source":"#Predictions\n\nPrediction = pd.Series(rfModel2_sig.predict(df_test_sig))","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:23:58.271044Z","iopub.execute_input":"2021-05-26T05:23:58.271526Z","iopub.status.idle":"2021-05-26T05:23:59.786325Z","shell.execute_reply.started":"2021-05-26T05:23:58.271474Z","shell.execute_reply":"2021-05-26T05:23:59.785131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 4. Plot The Count for each Predicted Poverty Level","metadata":{}},{"cell_type":"code","source":"predicted_counts = Prediction.value_counts().sort_index()\n\n# Plot The Count for each Predicted Poverty Level\nprint(\"\\n\")\npredicted_counts.plot.bar(figsize = (12,6),linewidth=3,edgecolor='k',color='royalblue')\nplt.xlabel(\"Poverty Levels\",size = 16)\nplt.ylabel(\"Count\",size = 16)\nplt.xticks([0,1,2,3],['extreme poverty','moderate poverty','vulnerable households','non vulnerable households'],rotation=20)\nplt.title(\"Poverty Level Counts (Predicted)\",pad=30,size=16)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:23:59.787826Z","iopub.execute_input":"2021-05-26T05:23:59.788114Z","iopub.status.idle":"2021-05-26T05:23:59.979298Z","shell.execute_reply.started":"2021-05-26T05:23:59.788087Z","shell.execute_reply":"2021-05-26T05:23:59.977706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets Read test data to get id coloum\n\ntest_df_submission =  pd.read_csv(\"/kaggle/input/costa-rican-household-poverty-prediction/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:31:02.009627Z","iopub.execute_input":"2021-05-26T05:31:02.010045Z","iopub.status.idle":"2021-05-26T05:31:02.284545Z","shell.execute_reply.started":"2021-05-26T05:31:02.01001Z","shell.execute_reply":"2021-05-26T05:31:02.283591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df_submission.columns","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:31:16.779045Z","iopub.execute_input":"2021-05-26T05:31:16.779491Z","iopub.status.idle":"2021-05-26T05:31:16.786337Z","shell.execute_reply.started":"2021-05-26T05:31:16.779439Z","shell.execute_reply":"2021-05-26T05:31:16.785561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save Prediction in Csv File\nmy_submission = pd.DataFrame({'Id': test_df_submission.Id, 'Target': Prediction})\n# you could use any filename. We choose submission here\nmy_submission.to_csv('submission.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:31:42.438519Z","iopub.execute_input":"2021-05-26T05:31:42.43908Z","iopub.status.idle":"2021-05-26T05:31:42.499659Z","shell.execute_reply.started":"2021-05-26T05:31:42.439032Z","shell.execute_reply":"2021-05-26T05:31:42.498773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}