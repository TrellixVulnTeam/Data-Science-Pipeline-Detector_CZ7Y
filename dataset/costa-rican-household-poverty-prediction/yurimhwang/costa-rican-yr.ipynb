{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-30T13:12:18.017867Z","iopub.execute_input":"2021-07-30T13:12:18.018277Z","iopub.status.idle":"2021-07-30T13:12:18.035017Z","shell.execute_reply.started":"2021-07-30T13:12:18.018245Z","shell.execute_reply":"2021-07-30T13:12:18.033494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Id - 각 행의 고유 식별자입니다.\n# 목표 - 목표는 소득 수준 그룹을 나타내는 서수 변수입니다.\n# 1 = 극심한 빈곤\n# 2 = 중간 빈곤\n# 3 = 취약 가구\n# 4 = 취약하지 않은 가구\n# idhogar - 각 가구의 고유 식별자입니다. 이것은 가구 전체 기능 등을 만드는 데 사용할 수 있습니다. 지정된 가구의 모든 행은 이 식별자에 대해 일치하는 값을 갖습니다.\n# parentesco1 - 이 사람이 가장인지 여부를 나타냅니다.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd \nimport joblib\n\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline \n\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom sklearn.metrics import f1_score\n\nfrom joblib import Parallel, delayed\nfrom sklearn.base import clone\nfrom sklearn.ensemble import VotingClassifier, ExtraTreesClassifier, RandomForestClassifier\nfrom sklearn.utils import class_weight\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2021-07-30T15:47:31.214282Z","iopub.execute_input":"2021-07-30T15:47:31.214783Z","iopub.status.idle":"2021-07-30T15:47:34.018449Z","shell.execute_reply.started":"2021-07-30T15:47:31.214638Z","shell.execute_reply":"2021-07-30T15:47:34.01733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\n# this only transforms the idhogar field, the other things this function used to do are done elsewhere\ndef encode_data(df):\n    df['idhogar'] = LabelEncoder().fit_transform(df['idhogar'])\n\n# plot feature importance for sklearn decision trees    \ndef feature_importance(forest, X_train, display_results=True):\n    ranked_list = []\n    zero_features = []\n    \n    importances = forest.feature_importances_\n\n    indices = np.argsort(importances)[::-1]\n    \n    if display_results:\n        # Print the feature ranking\n        print(\"Feature ranking:\")\n\n    for f in range(X_train.shape[1]):\n        if display_results:\n            print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]) + \" - \" + X_train.columns[indices[f]])\n        \n        ranked_list.append(X_train.columns[indices[f]])\n        \n        if importances[indices[f]] == 0.0:\n            zero_features.append(X_train.columns[indices[f]])\n            \n    return ranked_list, zero_features","metadata":{"execution":{"iopub.status.busy":"2021-07-30T15:53:45.63245Z","iopub.execute_input":"2021-07-30T15:53:45.632828Z","iopub.status.idle":"2021-07-30T15:53:45.640629Z","shell.execute_reply.started":"2021-07-30T15:53:45.632794Z","shell.execute_reply":"2021-07-30T15:53:45.63953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def do_features(df):\n    feats_div = [('children_fraction', 'r4t1', 'r4t3'), \n                 ('working_man_fraction', 'r4h2', 'r4t3'),\n                 ('all_man_fraction', 'r4h3', 'r4t3'),\n                 ('human_density', 'tamviv', 'rooms'),\n                 ('human_bed_density', 'tamviv', 'bedrooms'),\n                 ('rent_per_person', 'v2a1', 'r4t3'),\n                 ('rent_per_room', 'v2a1', 'rooms'),\n                 ('mobile_density', 'qmobilephone', 'r4t3'),\n                 ('tablet_density', 'v18q1', 'r4t3'),\n                 ('mobile_adult_density', 'qmobilephone', 'r4t2'),\n                 ('tablet_adult_density', 'v18q1', 'r4t2'),\n                ]\n    \n    feats_sub = [('people_not_living', 'tamhog', 'tamviv'),\n                 ('people_weird_stat', 'tamhog', 'r4t3')]\n\n    for f_new, f1, f2 in feats_div:\n        df['fe_' + f_new] = (df[f1] / df[f2]).astype(np.float32)       \n    for f_new, f1, f2 in feats_sub:\n        df['fe_' + f_new] = (df[f1] - df[f2]).astype(np.float32)\n    \n    # aggregation rules over household\n    aggs_num = {'age': ['min', 'max', 'mean'],\n                'escolari': ['min', 'max', 'mean']\n               }\n    \n    aggs_cat = {'dis': ['mean']}\n    for s_ in ['estadocivil', 'parentesco', 'instlevel']:\n        for f_ in [f_ for f_ in df.columns if f_.startswith(s_)]:\n            aggs_cat[f_] = ['mean', 'count']\n\n    # aggregation over household\n    for name_, df_ in [('18', df.query('age >= 18'))]:\n        df_agg = df_.groupby('idhogar').agg({**aggs_num, **aggs_cat}).astype(np.float32)\n        df_agg.columns = pd.Index(['agg' + name_ + '_' + e[0] + \"_\" + e[1].upper() for e in df_agg.columns.tolist()])\n        df = df.join(df_agg, how='left', on='idhogar')\n        del df_agg\n\n    # Drop id's\n    df.drop(['Id'], axis=1, inplace=True)\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2021-07-30T15:53:46.092942Z","iopub.execute_input":"2021-07-30T15:53:46.093286Z","iopub.status.idle":"2021-07-30T15:53:46.104566Z","shell.execute_reply.started":"2021-07-30T15:53:46.093255Z","shell.execute_reply":"2021-07-30T15:53:46.103406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert one hot encoded fields to label encoding\ndef convert_OHE2LE(df):\n    tmp_df = df.copy(deep=True)\n    for s_ in ['pared', 'piso', 'techo', 'abastagua', 'sanitario', 'energcocinar', 'elimbasu', \n               'epared', 'etecho', 'eviv', 'estadocivil', 'parentesco', \n               'instlevel', 'lugar', 'tipovivi',\n               'manual_elec']:\n        if 'manual_' not in s_:\n            cols_s_ = [f_ for f_ in df.columns if f_.startswith(s_)]\n        elif 'elec' in s_:\n            cols_s_ = ['public', 'planpri', 'noelec', 'coopele']\n        sum_ohe = tmp_df[cols_s_].sum(axis=1).unique()\n        #deal with those OHE, where there is a sum over columns == 0\n        if 0 in sum_ohe:\n            print('The OHE in {} is incomplete. A new column will be added before label encoding'\n                  .format(s_))\n            # dummy colmn name to be added\n            col_dummy = s_+'_dummy'\n            # add the column to the dataframe\n            tmp_df[col_dummy] = (tmp_df[cols_s_].sum(axis=1) == 0).astype(np.int8)\n            # add the name to the list of columns to be label-encoded\n            cols_s_.append(col_dummy)\n            # proof-check, that now the category is complete\n            sum_ohe = tmp_df[cols_s_].sum(axis=1).unique()\n            if 0 in sum_ohe:\n                 print(\"The category completion did not work\")\n        tmp_cat = tmp_df[cols_s_].idxmax(axis=1)\n        tmp_df[s_ + '_LE'] = LabelEncoder().fit_transform(tmp_cat).astype(np.int16)\n        if 'parentesco1' in cols_s_:\n            cols_s_.remove('parentesco1')\n        tmp_df.drop(cols_s_, axis=1, inplace=True)\n    return tmp_df","metadata":{"execution":{"iopub.status.busy":"2021-07-30T15:53:46.821153Z","iopub.execute_input":"2021-07-30T15:53:46.821493Z","iopub.status.idle":"2021-07-30T15:53:46.831205Z","shell.execute_reply.started":"2021-07-30T15:53:46.821461Z","shell.execute_reply":"2021-07-30T15:53:46.829834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Read in the data and clean it up","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/costa-rican-household-poverty-prediction/train.csv')\ntest = pd.read_csv('../input/costa-rican-household-poverty-prediction/test.csv')\n\ntest_ids = test.Id","metadata":{"execution":{"iopub.status.busy":"2021-07-30T15:53:48.42641Z","iopub.execute_input":"2021-07-30T15:53:48.426786Z","iopub.status.idle":"2021-07-30T15:53:48.930174Z","shell.execute_reply.started":"2021-07-30T15:53:48.426753Z","shell.execute_reply":"2021-07-30T15:53:48.929273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process_df(df_):\n    # encode the idhogar\n    encode_data(df_)\n    \n    # create aggregate features\n    return do_features(df_)\n\ntrain = process_df(train)\ntest = process_df(test)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T13:14:47.733898Z","iopub.execute_input":"2021-07-30T13:14:47.734402Z","iopub.status.idle":"2021-07-30T13:14:48.161669Z","shell.execute_reply.started":"2021-07-30T13:14:47.734372Z","shell.execute_reply":"2021-07-30T13:14:48.160841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# some dependencies are Na, fill those with the square root of the square\ntrain['dependency'] = np.sqrt(train['SQBdependency'])\ntest['dependency'] = np.sqrt(test['SQBdependency'])\n\n# fill \"no\"s for education with 0s\ntrain.loc[train['edjefa'] == \"no\", \"edjefa\"] = 0\ntrain.loc[train['edjefe'] == \"no\", \"edjefe\"] = 0\ntest.loc[test['edjefa'] == \"no\", \"edjefa\"] = 0\ntest.loc[test['edjefe'] == \"no\", \"edjefe\"] = 0\n\n# if education is \"yes\" and person is head of household, fill with escolari\ntrain.loc[(train['edjefa'] == \"yes\") & (train['parentesco1'] == 1), \"edjefa\"] = train.loc[(train['edjefa'] == \"yes\") & (train['parentesco1'] == 1), \"escolari\"]\ntrain.loc[(train['edjefe'] == \"yes\") & (train['parentesco1'] == 1), \"edjefe\"] = train.loc[(train['edjefe'] == \"yes\") & (train['parentesco1'] == 1), \"escolari\"]\n\ntest.loc[(test['edjefa'] == \"yes\") & (test['parentesco1'] == 1), \"edjefa\"] = test.loc[(test['edjefa'] == \"yes\") & (test['parentesco1'] == 1), \"escolari\"]\ntest.loc[(test['edjefe'] == \"yes\") & (test['parentesco1'] == 1), \"edjefe\"] = test.loc[(test['edjefe'] == \"yes\") & (test['parentesco1'] == 1), \"escolari\"]\n\n# this field is supposed to be interaction between gender and escolari, but it isn't clear what \"yes\" means, let's fill it with 4\ntrain.loc[train['edjefa'] == \"yes\", \"edjefa\"] = 4\ntrain.loc[train['edjefe'] == \"yes\", \"edjefe\"] = 4\n\ntest.loc[test['edjefa'] == \"yes\", \"edjefa\"] = 4\ntest.loc[test['edjefe'] == \"yes\", \"edjefe\"] = 4\n\n# convert to int for our models\ntrain['edjefe'] = train['edjefe'].astype(\"int\")\ntrain['edjefa'] = train['edjefa'].astype(\"int\")\ntest['edjefe'] = test['edjefe'].astype(\"int\")\ntest['edjefa'] = test['edjefa'].astype(\"int\")\n\n# create feature with max education of either head of household\ntrain['edjef'] = np.max(train[['edjefa','edjefe']], axis=1)\ntest['edjef'] = np.max(test[['edjefa','edjefe']], axis=1)\n\n# fill some nas\ntrain['v2a1']=train['v2a1'].fillna(0)\ntest['v2a1']=test['v2a1'].fillna(0)\n\ntest['v18q1']=test['v18q1'].fillna(0)\ntrain['v18q1']=train['v18q1'].fillna(0)\n\ntrain['rez_esc']=train['rez_esc'].fillna(0)\ntest['rez_esc']=test['rez_esc'].fillna(0)\n\ntrain.loc[train.meaneduc.isnull(), \"meaneduc\"] = 0\ntrain.loc[train.SQBmeaned.isnull(), \"SQBmeaned\"] = 0\n\ntest.loc[test.meaneduc.isnull(), \"meaneduc\"] = 0\ntest.loc[test.SQBmeaned.isnull(), \"SQBmeaned\"] = 0\n\n# fix some inconsistencies in the data - some rows indicate both that the household does and does not have a toilet, \n# if there is no water we'll assume they do not\ntrain.loc[(train.v14a ==  1) & (train.sanitario1 ==  1) & (train.abastaguano == 0), \"v14a\"] = 0\ntrain.loc[(train.v14a ==  1) & (train.sanitario1 ==  1) & (train.abastaguano == 0), \"sanitario1\"] = 0\n\ntest.loc[(test.v14a ==  1) & (test.sanitario1 ==  1) & (test.abastaguano == 0), \"v14a\"] = 0\ntest.loc[(test.v14a ==  1) & (test.sanitario1 ==  1) & (test.abastaguano == 0), \"sanitario1\"] = 0","metadata":{"execution":{"iopub.status.busy":"2021-07-30T13:14:56.132812Z","iopub.execute_input":"2021-07-30T13:14:56.133429Z","iopub.status.idle":"2021-07-30T13:14:56.258233Z","shell.execute_reply.started":"2021-07-30T13:14:56.133395Z","shell.execute_reply":"2021-07-30T13:14:56.257497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_test_apply_func(train_, test_, func_):\n    test_['Target'] = 0\n    xx = pd.concat([train_, test_])\n\n    xx_func = func_(xx)\n    train_ = xx_func.iloc[:train_.shape[0], :]\n    test_  = xx_func.iloc[train_.shape[0]:, :].drop('Target', axis=1)\n\n    del xx, xx_func\n    return train_, test_","metadata":{"execution":{"iopub.status.busy":"2021-07-30T13:15:03.832343Z","iopub.execute_input":"2021-07-30T13:15:03.833088Z","iopub.status.idle":"2021-07-30T13:15:03.840221Z","shell.execute_reply.started":"2021-07-30T13:15:03.833035Z","shell.execute_reply":"2021-07-30T13:15:03.839359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert the one hot fields into label encoded\ntrain, test = train_test_apply_func(train, test, convert_OHE2LE)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T13:15:09.768982Z","iopub.execute_input":"2021-07-30T13:15:09.769468Z","iopub.status.idle":"2021-07-30T13:15:11.374903Z","shell.execute_reply.started":"2021-07-30T13:15:09.769439Z","shell.execute_reply":"2021-07-30T13:15:11.373764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Geo aggregates\n\n","metadata":{}},{"cell_type":"code","source":"cols_2_ohe = ['eviv_LE', 'etecho_LE', 'epared_LE', 'elimbasu_LE', \n              'energcocinar_LE', 'sanitario_LE', 'manual_elec_LE',\n              'pared_LE']\ncols_nums = ['age', 'meaneduc', 'dependency', \n             'hogar_nin', 'hogar_adul', 'hogar_mayor', 'hogar_total',\n             'bedrooms', 'overcrowding']\n\ndef convert_geo2aggs(df_):\n    tmp_df = pd.concat([df_[(['lugar_LE', 'idhogar']+cols_nums)],\n                        pd.get_dummies(df_[cols_2_ohe], \n                                       columns=cols_2_ohe)],axis=1)\n\n    geo_agg = tmp_df.groupby(['lugar_LE','idhogar']).mean().groupby('lugar_LE').mean().astype(np.float32)\n    geo_agg.columns = pd.Index(['geo_' + e for e in geo_agg.columns.tolist()])\n    \n    del tmp_df\n    return df_.join(geo_agg, how='left', on='lugar_LE')\n\n# add some aggregates by geography\ntrain, test = train_test_apply_func(train, test, convert_geo2aggs)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T13:15:35.445602Z","iopub.execute_input":"2021-07-30T13:15:35.445987Z","iopub.status.idle":"2021-07-30T13:15:35.58929Z","shell.execute_reply.started":"2021-07-30T13:15:35.445957Z","shell.execute_reply":"2021-07-30T13:15:35.588008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# add the number of people over 18 in each household\ntrain['num_over_18'] = 0\ntrain['num_over_18'] = train[train.age >= 18].groupby('idhogar').transform(\"count\")\ntrain['num_over_18'] = train.groupby(\"idhogar\")[\"num_over_18\"].transform(\"max\")\ntrain['num_over_18'] = train['num_over_18'].fillna(0)\n\ntest['num_over_18'] = 0\ntest['num_over_18'] = test[test.age >= 18].groupby('idhogar').transform(\"count\")\ntest['num_over_18'] = test.groupby(\"idhogar\")[\"num_over_18\"].transform(\"max\")\ntest['num_over_18'] = test['num_over_18'].fillna(0)\n\n# add some extra features, these were taken from another kernel\ndef extract_features(df):\n    df['bedrooms_to_rooms'] = df['bedrooms']/df['rooms']\n    df['rent_to_rooms'] = df['v2a1']/df['rooms']\n    df['tamhog_to_rooms'] = df['tamhog']/df['rooms'] # tamhog - size of the household\n    df['r4t3_to_tamhog'] = df['r4t3']/df['tamhog'] # r4t3 - Total persons in the household\n    df['r4t3_to_rooms'] = df['r4t3']/df['rooms'] # r4t3 - Total persons in the household\n    df['v2a1_to_r4t3'] = df['v2a1']/df['r4t3'] # rent to people in household\n    df['v2a1_to_r4t3'] = df['v2a1']/(df['r4t3'] - df['r4t1']) # rent to people under age 12\n    df['hhsize_to_rooms'] = df['hhsize']/df['rooms'] # rooms per person\n    df['rent_to_hhsize'] = df['v2a1']/df['hhsize'] # rent to household size\n    df['rent_to_over_18'] = df['v2a1']/df['num_over_18']\n    # some households have no one over 18, use the total rent for those\n    df.loc[df.num_over_18 == 0, \"rent_to_over_18\"] = df[df.num_over_18 == 0].v2a1\n    \nextract_features(train)    \nextract_features(test) ","metadata":{"execution":{"iopub.status.busy":"2021-07-30T13:15:41.50275Z","iopub.execute_input":"2021-07-30T13:15:41.503155Z","iopub.status.idle":"2021-07-30T13:15:41.730862Z","shell.execute_reply.started":"2021-07-30T13:15:41.503124Z","shell.execute_reply":"2021-07-30T13:15:41.729676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# drop duplicated columns\nneedless_cols = ['r4t3', 'tamhog', 'tamviv', 'hhsize', 'v18q', 'v14a', 'agesq',\n                 'mobilephone', 'female', ]\n\ninstlevel_cols = [s for s in train.columns.tolist() if 'instlevel' in s]\n\nneedless_cols.extend(instlevel_cols)\n\ntrain = train.drop(needless_cols, axis=1)\ntest = test.drop(needless_cols, axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T13:15:46.017732Z","iopub.execute_input":"2021-07-30T13:15:46.018129Z","iopub.status.idle":"2021-07-30T13:15:46.034551Z","shell.execute_reply.started":"2021-07-30T13:15:46.018097Z","shell.execute_reply":"2021-07-30T13:15:46.03342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Split the data\n\ndef split_data(train, y, sample_weight=None, households=None, test_percentage=0.20, seed=None):\n    # uncomment for extra randomness\n#     np.random.seed(seed=seed)\n    \n    train2 = train.copy()\n    \n    # pick some random households to use for the test data\n    cv_hhs = np.random.choice(households, size=int(len(households) * test_percentage), replace=False)\n    \n    # select households which are in the random selection\n    cv_idx = np.isin(households, cv_hhs)\n    X_test = train2[cv_idx]\n    y_test = y[cv_idx]\n\n    X_train = train2[~cv_idx]\n    y_train = y[~cv_idx]\n    \n    if sample_weight is not None:\n        y_train_weights = sample_weight[~cv_idx]\n        return X_train, y_train, X_test, y_test, y_train_weights\n    \n    return X_train, y_train, X_test, y_test","metadata":{"execution":{"iopub.status.busy":"2021-07-30T13:16:09.129319Z","iopub.execute_input":"2021-07-30T13:16:09.129686Z","iopub.status.idle":"2021-07-30T13:16:09.137279Z","shell.execute_reply.started":"2021-07-30T13:16:09.129651Z","shell.execute_reply":"2021-07-30T13:16:09.136385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = train.query('parentesco1==1')\n# X = train.copy()\n\n# pull out and drop the target variable\ny = X['Target'] - 1\nX = X.drop(['Target'], axis=1)\n\nnp.random.seed(seed=None)\n\ntrain2 = X.copy()\n\ntrain_hhs = train2.idhogar\n\nhouseholds = train2.idhogar.unique()\ncv_hhs = np.random.choice(households, size=int(len(households) * 0.15), replace=False)\n\ncv_idx = np.isin(train2.idhogar, cv_hhs)\n\nX_test = train2[cv_idx]\ny_test = y[cv_idx]\n\nX_train = train2[~cv_idx]\ny_train = y[~cv_idx]\n\n# train on entire dataset\nX_train = train2\ny_train = y\n\ntrain_households = X_train.idhogar","metadata":{"execution":{"iopub.status.busy":"2021-07-30T13:16:13.428385Z","iopub.execute_input":"2021-07-30T13:16:13.42876Z","iopub.status.idle":"2021-07-30T13:16:13.464865Z","shell.execute_reply.started":"2021-07-30T13:16:13.428726Z","shell.execute_reply":"2021-07-30T13:16:13.464012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# figure out the class weights for training with unbalanced classes\n\n# 불균형한 데이터 가중치\ny_train_weights = class_weight.compute_sample_weight('balanced', y_train, indices=None)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T13:16:19.155353Z","iopub.execute_input":"2021-07-30T13:16:19.155705Z","iopub.status.idle":"2021-07-30T13:16:19.163366Z","shell.execute_reply.started":"2021-07-30T13:16:19.155667Z","shell.execute_reply":"2021-07-30T13:16:19.162275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# drop some features which aren't used by the LGBM or have very low importance\n# 중요도 낮은 변수 drop\nextra_drop_features = [\n 'agg18_estadocivil1_MEAN',\n 'agg18_estadocivil6_COUNT',\n 'agg18_estadocivil7_COUNT',\n 'agg18_parentesco10_COUNT',\n 'agg18_parentesco11_COUNT',\n 'agg18_parentesco12_COUNT',\n 'agg18_parentesco1_COUNT',\n 'agg18_parentesco2_COUNT',\n 'agg18_parentesco3_COUNT',\n 'agg18_parentesco4_COUNT',\n 'agg18_parentesco5_COUNT',\n 'agg18_parentesco6_COUNT',\n 'agg18_parentesco7_COUNT',\n 'agg18_parentesco8_COUNT',\n 'agg18_parentesco9_COUNT',\n 'geo_elimbasu_LE_4',\n 'geo_energcocinar_LE_1',\n 'geo_energcocinar_LE_2',\n 'geo_epared_LE_0',\n 'geo_hogar_mayor',\n 'geo_manual_elec_LE_2',\n 'geo_pared_LE_3',\n 'geo_pared_LE_4',\n 'geo_pared_LE_5',\n 'geo_pared_LE_6',\n 'num_over_18',\n 'parentesco_LE',\n 'rez_esc']","metadata":{"execution":{"iopub.status.busy":"2021-07-30T13:16:24.875942Z","iopub.execute_input":"2021-07-30T13:16:24.876296Z","iopub.status.idle":"2021-07-30T13:16:24.882357Z","shell.execute_reply.started":"2021-07-30T13:16:24.876266Z","shell.execute_reply":"2021-07-30T13:16:24.881172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_drop_cols = extra_drop_features + [\"idhogar\",  'parentesco1']","metadata":{"execution":{"iopub.status.busy":"2021-07-30T13:16:29.705638Z","iopub.execute_input":"2021-07-30T13:16:29.706115Z","iopub.status.idle":"2021-07-30T13:16:29.7102Z","shell.execute_reply.started":"2021-07-30T13:16:29.70608Z","shell.execute_reply":"2021-07-30T13:16:29.709492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Fit a voting classifier\n# 4\nopt_parameters = {'max_depth':35, 'eta':0.1, 'silent':0, 'objective':'multi:softmax', 'min_child_weight': 1, 'num_class': 4, 'gamma': 2.0, 'colsample_bylevel': 0.9, 'subsample': 0.84, 'colsample_bytree': 0.88, 'reg_lambda': 0.40 }\n# 5\nopt_parameters = {'max_depth':35, 'eta':0.15, 'silent':1, 'objective':'multi:softmax', 'min_child_weight': 2, 'num_class': 4, 'gamma': 2.5, 'colsample_bylevel': 1, 'subsample': 0.95, 'colsample_bytree': 0.85, 'reg_lambda': 0.35 }\n# 6\n# opt_parameters = {'max_depth':35, 'eta':0.15, 'silent':0, 'objective':'multi:softmax', 'min_child_weight': 2, 'num_class': 4, 'gamma': 2.75, 'colsample_bylevel': 0.95, 'subsample': 0.95, 'colsample_bytree': 0.85, 'reg_lambda': 0.35 }\n# # 7\n# opt_parameters = {'max_depth':35, 'eta':0.12, 'silent':0, 'objective':'multi:softmax', 'min_child_weight': 2, 'num_class': 4, 'gamma': 3.25, 'colsample_bylevel': 0.95, 'subsample': 0.88, 'colsample_bytree': 0.88, 'reg_lambda': 0.35 }\n\ndef evaluate_macroF1_lgb(predictions, truth):  \n    # this follows the discussion in https://github.com/Microsoft/LightGBM/issues/1483\n    pred_labels = predictions.argmax(axis=1)\n    truth = truth.get_label()\n    f1 = f1_score(truth, pred_labels, average='macro')\n    return ('macroF1', 1-f1) \n\nfit_params={\"early_stopping_rounds\":500,\n            \"eval_metric\" : evaluate_macroF1_lgb, \n            \"eval_set\" : [(X_train,y_train), (X_test,y_test)],\n            'verbose': False,\n           }\n\ndef learning_rate_power_0997(current_iter):\n    base_learning_rate = 0.1\n    min_learning_rate = 0.02\n    lr = base_learning_rate  * np.power(.995, current_iter)\n    return max(lr, min_learning_rate)\n\nfit_params['verbose'] = 50","metadata":{"execution":{"iopub.status.busy":"2021-07-30T13:16:44.004956Z","iopub.execute_input":"2021-07-30T13:16:44.005307Z","iopub.status.idle":"2021-07-30T13:16:44.016708Z","shell.execute_reply.started":"2021-07-30T13:16:44.005279Z","shell.execute_reply":"2021-07-30T13:16:44.015583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.random.seed(100)\n\ndef _parallel_fit_estimator(estimator1, X, y, sample_weight=None, threshold=True, **fit_params):\n    estimator = clone(estimator1)\n    \n    # randomly split the data so we have a test set for early stopping\n    if sample_weight is not None:\n        X_train, y_train, X_test, y_test, y_train_weight = split_data(X, y, sample_weight, households=train_households)\n    else:\n        X_train, y_train, X_test, y_test = split_data(X, y, None, households=train_households)\n        \n    # update the fit params with our new split\n    fit_params[\"eval_set\"] = [(X_test,y_test)]\n    \n    # fit the estimator\n    if sample_weight is not None:\n        if isinstance(estimator1, ExtraTreesClassifier) or isinstance(estimator1, RandomForestClassifier):\n            estimator.fit(X_train, y_train)\n        else:\n            _ = estimator.fit(X_train, y_train, sample_weight=y_train_weight, **fit_params)\n    else:\n        if isinstance(estimator1, ExtraTreesClassifier) or isinstance(estimator1, RandomForestClassifier):\n            estimator.fit(X_train, y_train)\n        else:\n            _ = estimator.fit(X_train, y_train, **fit_params)\n    \n    if not isinstance(estimator1, ExtraTreesClassifier) and not isinstance(estimator1, RandomForestClassifier) and not isinstance(estimator1, xgb.XGBClassifier):\n        best_cv_round = np.argmax(estimator.evals_result_['validation_0']['mlogloss'])\n        best_cv = np.max(estimator.evals_result_['validation_0']['mlogloss'])\n        best_train = estimator.evals_result_['train']['macroF1'][best_cv_round]\n    else:\n        best_train = f1_score(y_train, estimator.predict(X_train), average=\"macro\")\n        best_cv = f1_score(y_test, estimator.predict(X_test), average=\"macro\")\n        print(\"Train F1:\", best_train)\n        print(\"Test F1:\", best_cv)\n        \n    # reject some estimators based on their performance on train and test sets\n    if threshold:\n        # if the valid score is very high we'll allow a little more leeway with the train scores\n        if ((best_cv > 0.37) and (best_train > 0.75)) or ((best_cv > 0.44) and (best_train > 0.65)):\n            return estimator\n\n        # else recurse until we get a better one\n        else:\n            print(\"Unacceptable!!! Trying again...\")\n            return _parallel_fit_estimator(estimator1, X, y, sample_weight=sample_weight, **fit_params)\n    \n    else:\n        return estimator\n    \nclass VotingClassifierLGBM(VotingClassifier):\n    '''\n    This implements the fit method of the VotingClassifier propagating fit_params\n    '''\n    def fit(self, X, y, sample_weight=None, threshold=True, **fit_params):\n        \n        if isinstance(y, np.ndarray) and len(y.shape) > 1 and y.shape[1] > 1:\n            raise NotImplementedError('Multilabel and multi-output'\n                                      ' classification is not supported.')\n\n        if self.voting not in ('soft', 'hard'):\n            raise ValueError(\"Voting must be 'soft' or 'hard'; got (voting=%r)\"\n                             % self.voting)\n\n        if self.estimators is None or len(self.estimators) == 0:\n            raise AttributeError('Invalid `estimators` attribute, `estimators`'\n                                 ' should be a list of (string, estimator)'\n                                 ' tuples')\n\n        if (self.weights is not None and\n                len(self.weights) != len(self.estimators)):\n            raise ValueError('Number of classifiers and weights must be equal'\n                             '; got %d weights, %d estimators'\n                             % (len(self.weights), len(self.estimators)))\n\n        names, clfs = zip(*self.estimators)\n        self._validate_names(names)\n\n        n_isnone = np.sum([clf is None for _, clf in self.estimators])\n        if n_isnone == len(self.estimators):\n            raise ValueError('All estimators are None. At least one is '\n                             'required to be a classifier!')\n\n        self.le_ = LabelEncoder().fit(y)\n        self.classes_ = self.le_.classes_\n        self.estimators_ = []\n\n        transformed_y = self.le_.transform(y)\n\n        self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n                delayed(_parallel_fit_estimator)(clone(clf), X, transformed_y,\n                                                 sample_weight=sample_weight, threshold=threshold, **fit_params)\n                for clf in clfs if clf is not None)\n\n        return self","metadata":{"execution":{"iopub.status.busy":"2021-07-30T13:17:40.375946Z","iopub.execute_input":"2021-07-30T13:17:40.376423Z","iopub.status.idle":"2021-07-30T13:17:40.398852Z","shell.execute_reply.started":"2021-07-30T13:17:40.376394Z","shell.execute_reply":"2021-07-30T13:17:40.397871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## voting classifiers는 '다수결 분류'\n\n## 1. Hard Voting Classifier : 여러 모델을 생성하고 그 성과(결과)를 비교합니다\n## 이 때 classifier의 결과들을 집계하여 가장 많은 표를 얻는 클래스를 최종 예측값으로 정하는 것을 Hard Voting Classifier\n## 예를 들어, 최종결과를 1로 예측한 모델이 3개, 2로 예측한 모델이 1개이면 Hard Voting Classifier의 최종 결과(예측)은 1이 됨\n\n\n\n## 2. Soft Voting Classifier\n## 앙상블에 사용되는 모든 분류기가 클래스의 확률을 예측할 수 있을 때 사용\n## 각 분류기의 예측을 평균 내어 확률이 가장 높은 클래스로 예측 (가중치 투표)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clfs = []\nfor i in range(15):\n    clf = xgb.XGBClassifier(random_state=217+i, n_estimators=300, learning_rate=0.15, n_jobs=4, **opt_parameters)\n    \n    clfs.append(('xgb{}'.format(i), clf))\n    \nvc = VotingClassifierLGBM(clfs, voting='soft')\ndel(clfs)\n\n#Train the final model with learning rate decay\n_ = vc.fit(X_train.drop(xgb_drop_cols, axis=1), y_train, sample_weight=y_train_weights, threshold=False, **fit_params)\n\nclf_final = vc.estimators_[0]","metadata":{"execution":{"iopub.status.busy":"2021-07-30T13:23:58.799721Z","iopub.execute_input":"2021-07-30T13:23:58.800152Z","iopub.status.idle":"2021-07-30T13:30:44.085297Z","shell.execute_reply.started":"2021-07-30T13:23:58.800117Z","shell.execute_reply":"2021-07-30T13:30:44.08448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# params 4 - 400 early stop - 15 estimators - l1 used features - weighted\nglobal_score = f1_score(y_test, clf_final.predict(X_test.drop(xgb_drop_cols, axis=1)), average='macro')\nvc.voting = 'soft'\nglobal_score_soft = f1_score(y_test, vc.predict(X_test.drop(xgb_drop_cols, axis=1)), average='macro')\nvc.voting = 'hard'\nglobal_score_hard = f1_score(y_test, vc.predict(X_test.drop(xgb_drop_cols, axis=1)), average='macro')\n\nprint('Validation score of a single LGBM Classifier: {:.4f}'.format(global_score))\nprint('Validation score of a VotingClassifier on 3 LGBMs with soft voting strategy: {:.4f}'.format(global_score_soft))\nprint('Validation score of a VotingClassifier on 3 LGBMs with hard voting strategy: {:.4f}'.format(global_score_hard))","metadata":{"execution":{"iopub.status.busy":"2021-07-30T13:30:44.086678Z","iopub.execute_input":"2021-07-30T13:30:44.08712Z","iopub.status.idle":"2021-07-30T13:30:44.245093Z","shell.execute_reply.started":"2021-07-30T13:30:44.087086Z","shell.execute_reply":"2021-07-30T13:30:44.244286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# see which features are not used by ANY models\nuseless_features = []\ndrop_features = set()\ncounter = 0\nfor est in vc.estimators_:\n    ranked_features, unused_features = feature_importance(est, X_train.drop(xgb_drop_cols, axis=1), display_results=False)\n    useless_features.append(unused_features)\n    if counter == 0:\n        drop_features = set(unused_features)\n    else:\n        drop_features = drop_features.intersection(set(unused_features))\n    counter += 1\n    \ndrop_features","metadata":{"execution":{"iopub.status.busy":"2021-07-30T13:51:33.386035Z","iopub.execute_input":"2021-07-30T13:51:33.386405Z","iopub.status.idle":"2021-07-30T13:51:35.433159Z","shell.execute_reply.started":"2021-07-30T13:51:33.386375Z","shell.execute_reply":"2021-07-30T13:51:35.431947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ranked_features = feature_importance(clf_final, X_train.drop(xgb_drop_cols, axis=1))","metadata":{"execution":{"iopub.status.busy":"2021-07-30T13:51:38.314826Z","iopub.execute_input":"2021-07-30T13:51:38.31521Z","iopub.status.idle":"2021-07-30T13:51:38.474649Z","shell.execute_reply.started":"2021-07-30T13:51:38.315178Z","shell.execute_reply":"2021-07-30T13:51:38.473688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Random Forest\net_drop_cols = ['agg18_age_MAX', 'agg18_age_MEAN', 'agg18_age_MIN', 'agg18_dis_MEAN',\n       'agg18_escolari_MAX', 'agg18_escolari_MEAN', 'agg18_escolari_MIN',\n       'agg18_estadocivil1_COUNT', 'agg18_estadocivil1_MEAN',\n       'agg18_estadocivil2_COUNT', 'agg18_estadocivil2_MEAN',\n       'agg18_estadocivil3_COUNT', 'agg18_estadocivil3_MEAN',\n       'agg18_estadocivil4_COUNT', 'agg18_estadocivil4_MEAN',\n       'agg18_estadocivil5_COUNT', 'agg18_estadocivil5_MEAN',\n       'agg18_estadocivil6_COUNT', 'agg18_estadocivil6_MEAN',\n       'agg18_estadocivil7_COUNT', 'agg18_estadocivil7_MEAN',\n       'agg18_parentesco10_COUNT', 'agg18_parentesco10_MEAN',\n       'agg18_parentesco11_COUNT', 'agg18_parentesco11_MEAN',\n       'agg18_parentesco12_COUNT', 'agg18_parentesco12_MEAN',\n       'agg18_parentesco1_COUNT', 'agg18_parentesco1_MEAN',\n       'agg18_parentesco2_COUNT', 'agg18_parentesco2_MEAN',\n       'agg18_parentesco3_COUNT', 'agg18_parentesco3_MEAN',\n       'agg18_parentesco4_COUNT', 'agg18_parentesco4_MEAN',\n       'agg18_parentesco5_COUNT', 'agg18_parentesco5_MEAN',\n       'agg18_parentesco6_COUNT', 'agg18_parentesco6_MEAN',\n       'agg18_parentesco7_COUNT', 'agg18_parentesco7_MEAN',\n       'agg18_parentesco8_COUNT', 'agg18_parentesco8_MEAN',\n       'agg18_parentesco9_COUNT', 'agg18_parentesco9_MEAN'] #+ ['parentesco_LE', 'rez_esc']\n\net_drop_cols.extend([\"idhogar\", \"parentesco1\", 'fe_rent_per_person', 'fe_rent_per_room',\n       'fe_tablet_adult_density', 'fe_tablet_density'])","metadata":{"execution":{"iopub.status.busy":"2021-07-30T13:52:00.840724Z","iopub.execute_input":"2021-07-30T13:52:00.841134Z","iopub.status.idle":"2021-07-30T13:52:00.848582Z","shell.execute_reply.started":"2021-07-30T13:52:00.8411Z","shell.execute_reply":"2021-07-30T13:52:00.847455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# do the same thing for some extra trees classifiers\nets = []    \nfor i in range(10):\n    rf = RandomForestClassifier(max_depth=None, random_state=217+i, n_jobs=4, n_estimators=700, min_impurity_decrease=1e-3, min_samples_leaf=2, verbose=0, class_weight=\"balanced\")\n    ets.append(('rf{}'.format(i), rf))   \n\nvc2 = VotingClassifierLGBM(ets, voting='soft')    \n_ = vc2.fit(X_train.drop(et_drop_cols, axis=1), y_train, threshold=False)  ","metadata":{"execution":{"iopub.status.busy":"2021-07-30T13:52:05.668243Z","iopub.execute_input":"2021-07-30T13:52:05.668935Z","iopub.status.idle":"2021-07-30T13:52:35.482696Z","shell.execute_reply.started":"2021-07-30T13:52:05.668885Z","shell.execute_reply":"2021-07-30T13:52:35.481341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# w/ threshold, extra drop cols\nvc2.voting = 'soft'\nglobal_rf_score_soft = f1_score(y_test, vc2.predict(X_test.drop(et_drop_cols, axis=1)), average='macro')\nvc2.voting = 'hard'\nglobal_rf_score_hard = f1_score(y_test, vc2.predict(X_test.drop(et_drop_cols, axis=1)), average='macro')\n\nprint('Validation score of a VotingClassifier on 3 LGBMs with soft voting strategy: {:.4f}'.format(global_rf_score_soft))\nprint('Validation score of a VotingClassifier on 3 LGBMs with hard voting strategy: {:.4f}'.format(global_rf_score_hard))","metadata":{"execution":{"iopub.status.busy":"2021-07-30T13:52:35.484981Z","iopub.execute_input":"2021-07-30T13:52:35.485479Z","iopub.status.idle":"2021-07-30T13:52:39.625319Z","shell.execute_reply.started":"2021-07-30T13:52:35.485431Z","shell.execute_reply":"2021-07-30T13:52:39.624101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# w/o threshold, extra drop cols\nvc2.voting = 'soft'\nglobal_rf_score_soft = f1_score(y_test, vc2.predict(X_test.drop(et_drop_cols, axis=1)), average='macro')\nvc2.voting = 'hard'\nglobal_rf_score_hard = f1_score(y_test, vc2.predict(X_test.drop(et_drop_cols, axis=1)), average='macro')\n\nprint('Validation score of a VotingClassifier on 3 LGBMs with soft voting strategy: {:.4f}'.format(global_rf_score_soft))\nprint('Validation score of a VotingClassifier on 3 LGBMs with hard voting strategy: {:.4f}'.format(global_rf_score_hard))","metadata":{"execution":{"iopub.status.busy":"2021-07-30T13:55:09.660482Z","iopub.execute_input":"2021-07-30T13:55:09.660893Z","iopub.status.idle":"2021-07-30T13:55:13.902025Z","shell.execute_reply.started":"2021-07-30T13:55:09.660855Z","shell.execute_reply":"2021-07-30T13:55:13.900986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# see which features are not used by ANY models\nuseless_features = []\ndrop_features = set()\ncounter = 0\nfor est in vc2.estimators_:\n    ranked_features, unused_features = feature_importance(est, X_train.drop(et_drop_cols, axis=1), display_results=False)\n    useless_features.append(unused_features)\n    if counter == 0:\n        drop_features = set(unused_features)\n    else:\n        drop_features = drop_features.intersection(set(unused_features))\n    counter += 1\n    \ndrop_features","metadata":{"execution":{"iopub.status.busy":"2021-07-30T13:55:16.673996Z","iopub.execute_input":"2021-07-30T13:55:16.67442Z","iopub.status.idle":"2021-07-30T13:55:18.142893Z","shell.execute_reply.started":"2021-07-30T13:55:16.674386Z","shell.execute_reply":"2021-07-30T13:55:18.141888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def combine_voters(data, weights=[0.5, 0.5]):\n    # do soft voting with both classifiers\n    vc.voting=\"soft\"\n    vc1_probs = vc.predict_proba(data.drop(xgb_drop_cols, axis=1))\n    vc2.voting=\"soft\"\n    vc2_probs = vc2.predict_proba(data.drop(et_drop_cols, axis=1))\n    \n    final_vote = (vc1_probs * weights[0]) + (vc2_probs * weights[1])\n    predictions = np.argmax(final_vote, axis=1)\n    \n    return predictions","metadata":{"execution":{"iopub.status.busy":"2021-07-30T13:55:20.849653Z","iopub.execute_input":"2021-07-30T13:55:20.850059Z","iopub.status.idle":"2021-07-30T13:55:20.857902Z","shell.execute_reply.started":"2021-07-30T13:55:20.850024Z","shell.execute_reply":"2021-07-30T13:55:20.856347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combo_preds = combine_voters(X_test, weights=[0.5, 0.5])\nglobal_combo_score_soft = f1_score(y_test, combo_preds, average='macro')\nglobal_combo_score_soft","metadata":{"execution":{"iopub.status.busy":"2021-07-30T13:55:24.604221Z","iopub.execute_input":"2021-07-30T13:55:24.604601Z","iopub.status.idle":"2021-07-30T13:55:26.74594Z","shell.execute_reply.started":"2021-07-30T13:55:24.604565Z","shell.execute_reply":"2021-07-30T13:55:26.744893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combo_preds = combine_voters(X_test, weights=[0.4, 0.6])\nglobal_combo_score_soft= f1_score(y_test, combo_preds, average='macro')\nglobal_combo_score_soft","metadata":{"execution":{"iopub.status.busy":"2021-07-30T13:55:29.34435Z","iopub.execute_input":"2021-07-30T13:55:29.344877Z","iopub.status.idle":"2021-07-30T13:55:31.57949Z","shell.execute_reply.started":"2021-07-30T13:55:29.344843Z","shell.execute_reply":"2021-07-30T13:55:31.578388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combo_preds = combine_voters(X_test, weights=[0.6, 0.4])\nglobal_combo_score_soft = f1_score(y_test, combo_preds, average='macro')\nglobal_combo_score_soft","metadata":{"execution":{"iopub.status.busy":"2021-07-30T13:55:34.824392Z","iopub.execute_input":"2021-07-30T13:55:34.824821Z","iopub.status.idle":"2021-07-30T13:55:36.963099Z","shell.execute_reply.started":"2021-07-30T13:55:34.824766Z","shell.execute_reply":"2021-07-30T13:55:36.962149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prepare Submission","metadata":{}},{"cell_type":"code","source":"y_subm = pd.DataFrame()\ny_subm['Id'] = test_ids","metadata":{"execution":{"iopub.status.busy":"2021-07-30T13:55:59.974947Z","iopub.execute_input":"2021-07-30T13:55:59.975343Z","iopub.status.idle":"2021-07-30T13:55:59.990847Z","shell.execute_reply.started":"2021-07-30T13:55:59.975306Z","shell.execute_reply":"2021-07-30T13:55:59.989905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vc.voting = 'soft'\ny_subm_lgb = y_subm.copy(deep=True)\ny_subm_lgb['Target'] = vc.predict(test.drop(xgb_drop_cols, axis=1)) + 1\n\nvc2.voting = 'soft'\ny_subm_rf = y_subm.copy(deep=True)\ny_subm_rf['Target'] = vc2.predict(test.drop(et_drop_cols, axis=1)) + 1\n\ny_subm_ens = y_subm.copy(deep=True)\ny_subm_ens['Target'] = combine_voters(test) + 1","metadata":{"execution":{"iopub.status.busy":"2021-07-30T13:56:05.582168Z","iopub.execute_input":"2021-07-30T13:56:05.582762Z","iopub.status.idle":"2021-07-30T13:56:27.42519Z","shell.execute_reply.started":"2021-07-30T13:56:05.582725Z","shell.execute_reply":"2021-07-30T13:56:27.42397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datetime import datetime\nnow = datetime.now()\n\nsub_file_lgb = 'submission_soft_XGB_{:.4f}_{}.csv'.format(global_score_soft, str(now.strftime('%Y-%m-%d-%H-%M')))\nsub_file_rf = 'submission_soft_RF_{:.4f}_{}.csv'.format(global_rf_score_soft, str(now.strftime('%Y-%m-%d-%H-%M')))\nsub_file_ens = 'submission_ens_{:.4f}_{}.csv'.format(global_combo_score_soft, str(now.strftime('%Y-%m-%d-%H-%M')))\n\ny_subm_lgb.to_csv(sub_file_lgb, index=False)\ny_subm_rf.to_csv(sub_file_rf, index=False)\ny_subm_ens.to_csv(sub_file_ens, index=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T13:56:27.426618Z","iopub.execute_input":"2021-07-30T13:56:27.42695Z","iopub.status.idle":"2021-07-30T13:56:27.587721Z","shell.execute_reply.started":"2021-07-30T13:56:27.426919Z","shell.execute_reply":"2021-07-30T13:56:27.586922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}