{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Costa Rican Household Poverty Level Prediction : Data Visualization, Feature Engineering, PCA, Estimators & Model Building\n\n제작자: 이영준\n        \n        RandomForestClassifier\n        GradientBoostingClassifier\n        KNeighborsClassifier\n        XGBoost\n        LightGBM\n\nFirst perform modeling with default parameter values and get accuracy.\n\nf. Then perform tuning using Bayesian Optimization. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Ignore Warnings\nimport warnings\nwarnings.filterwarnings(action=\"ignore\", module=\"scipy\", message=\"^internal gelsd\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Clear memory -  https://ipython.readthedocs.io/en/stable/interactive/magics.html#magic-reset\n# Resets the namespace by removing all names defined by the user\n# -f : force reset without asking for confirmation.\n%reset -f","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Call data manipulation libraries\nimport pandas as pd\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting libraries to plot feature importance\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read the train Data File 5 rows × 142 columns\ntrain = pd.read_csv(\"../input/train.csv\")\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read the test Data File  5 rows × 142 columns\ntest = pd.read_csv(\"../input/test.csv\")\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems that there are 130 integer columns, 8 float columns, and 5 object columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"test.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Integer Columns\n\nLet's look at the distribution of unique values in the integer columns. For each column, we'll count the number of unique values and show the result in a bar plot.\n\nFor near to 100 columns, it seems that 2 unique values ( Boolean 0 or 1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#https://www.kaggle.com/willkoehrsen/a-complete-introduction-and-walkthrough\ntrain.select_dtypes(np.int64).nunique().value_counts().sort_index().plot.bar(color = 'orange', \n                                                                             figsize = (8, 6),\n                                                                            edgecolor = 'k', linewidth = 2);\nplt.xlabel('Number of Unique Values'); plt.ylabel('Count');\nplt.title('Count of Unique Values in Integer Columns');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 8 columns with Float Datatypes. \n\nFollowing graphs shows the distributions of the float columns colored by the value of the Target. With these plots, we can see if there is a significant difference in the variable distribution depending on the household poverty level."},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import OrderedDict\n\nplt.figure(figsize = (20, 16))\nplt.style.use('fivethirtyeight')\n\n# Color mapping\ncolors = OrderedDict({1: 'red', 2: 'orange', 3: 'blue', 4: 'green'})\npoverty_mapping = OrderedDict({1: 'extreme', 2: 'moderate', 3: 'vulnerable', 4: 'non vulnerable'})\n\n# Iterate through the float columns\nfor i, col in enumerate(train.select_dtypes('float')):\n    ax = plt.subplot(4, 2, i + 1)\n    # Iterate through the poverty levels\n    for poverty_level, color in colors.items():\n        # Plot each poverty level as a separate line\n        sns.kdeplot(train.loc[train['Target'] == poverty_level, col].dropna(), \n                    ax = ax, color = color, label = poverty_mapping[poverty_level])\n        \n    plt.title(f'{col.capitalize()} Distribution'); plt.xlabel(f'{col}'); plt.ylabel('Density')\n\nplt.subplots_adjust(top = 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Object Column\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.select_dtypes('object').head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nId - a unique identifier for each row. \n\nidhogar - this is a unique identifier for each household. This can be used to create household-wide features, etc. All rows in a given household will have a matching value for this identifier.\n\ndependency - Dependency rate, calculated = (number of members of the household younger than 19 or older than 64)/(number of member of household between 19 and 64)\n\nedjefe - years of education of male head of household, based on the interaction of escolari (years of education), head of household and gender, yes=1 and no=0\n\nedjefa - years of education of female head of household, based on the interaction of escolari (years of education), head of household and gender, yes=1 and no=0\n\nid & idhogar are seems to be identifier column. For remaining 3 object columns, we can map it to 'no' to 0 & 'yes' to 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"mapping = {\"yes\": 1, \"no\": 0}\n\n# Apply same operation to both train and test\nfor df in [train, test]:\n    # Fill in the values with the correct mapping\n    df['dependency'] = df['dependency'].replace(mapping).astype(np.float64)\n    df['edjefa'] = df['edjefa'].replace(mapping).astype(np.float64)\n    df['edjefe'] = df['edjefe'].replace(mapping).astype(np.float64)\n\ntrain[['dependency', 'edjefa', 'edjefe']].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Target - the target is an ordinal variable indicating groups of income levels.\n1 = extreme poverty\n2 = moderate poverty\n3 = vulnerable households\n4 = non vulnerable households "},{"metadata":{"trusted":true},"cell_type":"code","source":"target_values = train['Target'].value_counts()\ntarget_values = pd.DataFrame(target_values)\ntarget_values['Household_type'] = target_values.index\nmappy = {4: \"NonVulnerable\", 3: \"Moderate Poverty\", 2: \"Vulnerable\", 1: \"Extereme Poverty\"}\ntarget_values['Household_type'] = target_values.Household_type.map(mappy)\ntarget_values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(style = 'whitegrid', font_scale=1.4)\nfig = plt.subplots(figsize=(15, 8))\nax = sns.barplot(x = 'Household_type', y = 'Target', data = target_values, palette='Accent', ci = None).set_title('Distribution of Poverty in Households')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Target'].value_counts().plot(kind='pie',  autopct='%1.1f%%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sns.countplot(x=\"v2a1\",data=train)\nsns.set(style = 'whitegrid', font_scale=1.4)\nfig = plt.subplots(figsize=(15, 8))\n\nsns.countplot(x=\"rooms\", hue= \"Target\", data=train, palette=\"Accent\").set_title('# of Rooms in Households for Diff Proverty Class')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(style = 'whitegrid', font_scale=1.4)\nfig = plt.subplots(figsize=(15, 8))\n\nsns.countplot(x=\"r4h3\", hue= \"Target\", data=train, palette=\"Accent\").set_title('# of Males in Households for Diff Proverty Class')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(style = 'whitegrid', font_scale=1.4)\nfig = plt.subplots(figsize=(15, 8))\n\nsns.countplot(x=\"refrig\", hue= \"Target\", data=train, palette=\"Accent\").set_title('# of Refrigrator in Households for Diff Proverty Class')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fill in missing values (NULL values) either using mean or median (if the attribute is numeric) or most-frequently occurring value if the attribute is 'object' or categorical."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of missing in each column\nmissing = pd.DataFrame(train.isnull().sum()).rename(columns = {0: 'total'})\n\n# Create a percentage missing\nmissing['percent'] = missing['total'] / len(train)\n\nmissing.sort_values('percent', ascending = False).head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of missing in each column - Test Data\nmissing = pd.DataFrame(test.isnull().sum()).rename(columns = {0: 'total'})\n\n# Create a percentage missing\nmissing['percent'] = missing['total'] / len(test)\n\nmissing.sort_values('percent', ascending = False).head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"same columns with missing values are observed in test!"},{"metadata":{"trusted":true},"cell_type":"code","source":"train[['meaneduc', 'SQBmeaned']].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The point is that if we observe outliers in data, we should fill in NAs with median, otherwise it's ok to fill in with mean values. In the table, 50% is the median value, mean is mean :) Here it's fine to use mean values\n\n\nOther 3 columns we fill in with 0's temporarily\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#train\ntrain['meaneduc'].fillna(train['meaneduc'].mean(), inplace = True)\ntrain['SQBmeaned'].fillna(train['SQBmeaned'].mean(), inplace = True)\n#the same for test\ntest['meaneduc'].fillna(test['meaneduc'].mean(), inplace = True)\ntest['SQBmeaned'].fillna(test['SQBmeaned'].mean(), inplace = True)\ntrain['rez_esc'].fillna(0, inplace = True)\ntrain['v18q1'].fillna(0, inplace = True)\ntrain['v2a1'].fillna(0, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Feature Engine"},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_features(df):\n    df['bedrooms_to_rooms'] = df['bedrooms']/df['rooms']\n    df['rent_to_rooms'] = df['v2a1']/df['rooms']\n    df['rent_to_bedrooms'] = df['v2a1']/df['bedrooms']\n    df['tamhog_to_rooms'] = df['tamhog']/df['rooms'] # tamhog - size of the household\n    df['tamhog_to_bedrooms'] = df['tamhog']/df['bedrooms']\n    df['r4t3_to_tamhog'] = df['r4t3']/df['tamhog'] # r4t3 - Total persons in the household\n    df['r4t3_to_rooms'] = df['r4t3']/df['rooms'] # r4t3 - Total persons in the household\n    df['r4t3_to_bedrooms'] = df['r4t3']/df['bedrooms']\n    df['rent_to_r4t3'] = df['v2a1']/df['r4t3']\n    df['v2a1_to_r4t3'] = df['v2a1']/(df['r4t3'] - df['r4t1'])\n    df['hhsize_to_rooms'] = df['hhsize']/df['rooms']\n    df['hhsize_to_bedrooms'] = df['hhsize']/df['bedrooms']\n    df['rent_to_hhsize'] = df['v2a1']/df['hhsize']\n    df['qmobilephone_to_r4t3'] = df['qmobilephone']/df['r4t3']\n    df['qmobilephone_to_v18q1'] = df['qmobilephone']/df['v18q1']\n    \n\nextract_features(train)\nextract_features(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape,test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of missing in each column\nmissing = pd.DataFrame(train.isnull().sum()).rename(columns = {0: 'total'})\n\n# Create a percentage missing\nmissing['percent'] = missing['total'] / len(train)\n\nmissing.sort_values('percent', ascending = False).head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['qmobilephone_to_v18q1'].fillna(0, inplace = True)\n\n\ntest['qmobilephone_to_v18q1'].fillna(0, inplace = True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting data into dependent and independent variable\n# X is the independent variables matrix\nX = train.drop('Target', axis = 1)\n\n# y is the dependent variable vector\ny = train.Target","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.drop(['Id','idhogar'], inplace = True, axis=1)\nX.drop(['qmobilephone_to_v18q1'], inplace = True, axis=1)\nX.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scaling Features\nfrom sklearn.preprocessing import StandardScaler\nss = StandardScaler()\n\nX_ss = ss.fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"PCA"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\npca = PCA(n_components=0.95)\nX_PCA = pca.fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Split in Train and Test"},{"metadata":{"trusted":true},"cell_type":"code","source":"# split into train/test and resample the data\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nX_train, X_test, y_train, y_test = train_test_split(X_PCA, y, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Random Forest Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)\nrf = rf.fit(X_train, y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = rf.predict(X_test)\ny_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('    Accuracy Report: Random Forest Model\\n', classification_report(y_test, y_pred))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Decision Tree Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier(max_depth=3, random_state=42)\ndt = dt.fit(X_train, y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred1 = dt.predict(X_test)\ny_pred1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('    Accuracy Report: Decision Tree Model\\n', classification_report(y_test, y_pred1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Gradient Boost Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier as gbm\n\ngbc = gbm()\ngbc = gbc.fit(X_train, y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred2 = gbc.predict(X_test)\ny_pred2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('    Accuracy Report: Gradient Boost Model\\n', classification_report(y_test, y_pred2))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"KNeighborsClassifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nkn = KNeighborsClassifier(n_neighbors=4)\nkn = kn.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred3 = kn.predict(X_test)\ny_pred3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(' Accuracy Report: K Neighbors Model\\n', classification_report(y_test, y_pred3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"  XGBoost\n "},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost.sklearn import XGBClassifier as XGB\nxgb = XGB()\nxgb = xgb.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred4 = xgb.predict(X_test)\ny_pred4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Accuracy Report: XGB Model\\n', classification_report(y_test, y_pred4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" LightGBM"},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\nlightgbm = lgb.LGBMClassifier()\nlightgbm = lightgbm.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred5 = lightgbm.predict(X_test)\ny_pred5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Accuracy Report: Light GBM Model\\n', classification_report(y_test, y_pred5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"}},"nbformat":4,"nbformat_minor":1}