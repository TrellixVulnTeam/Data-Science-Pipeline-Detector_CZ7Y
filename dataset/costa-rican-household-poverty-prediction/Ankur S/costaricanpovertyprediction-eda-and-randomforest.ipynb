{"cells":[{"metadata":{},"cell_type":"markdown","source":"### <font color = blue>Brief Description : \n**We need to classify the persons as 1,2,3 or 4 class based on the Costa Rican household characteristics in this dataset.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Basic essential libraries >>>\nimport numpy as np\nimport pandas as pd\nimport os \nimport warnings\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nwarnings.filterwarnings('ignore')\nimport time\nimport sys","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Accessing the data >>> \ntrain = train = pd.read_csv(\"../input/train-costa-rica/train.csv\")\nprint('Shape : ',train.shape)\npd.options.display.max_columns = None\ntrain.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <font color = darkorange> Part - 1 : Data Analysis and Preprocessing "},{"metadata":{},"cell_type":"markdown","source":"<font color = dodgerblue>**Handing Missing Values**"},{"metadata":{"trusted":true},"cell_type":"code","source":"Null_Df = pd.DataFrame({'Null_Count':train.isnull().sum(),\n                        'Null_Perc %':round(train.isnull().sum()*100/train.shape[0],2)})\nNull_Df[Null_Df.Null_Count!=0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**<font color=red>Handling missing values :**`v2a1_num --> Monthly rent payment`"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Mean -->',train.v2a1_num.mean())\nprint('Median -->',train.v2a1_num.median())\nprint('Std -->',train.v2a1_num.std())\nsns.distplot(train[train.v2a1_num.notnull()].v2a1_num)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# As we can see that all the data is centred at on point only hence we can replace the missing values with average values.\nprint('Mean BEFORE Change : ',train.v2a1_num.mean())\ntrain.v2a1_num.fillna(train.v2a1_num.mean(),inplace=True)\nprint('Mean AFTER Change : ',train.v2a1_num.mean())\nprint('\\nMissing Values :')\ntrain.v2a1_num.isnull().value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**<font color=red>Handling missing values :**`v18q1_num  --> number of tablets household owns`"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Mean -->',train.v18q1_num.mean())\nprint('Median -->',train.v18q1_num.median())\nprint('Std -->',train.v18q1_num.std())\n\nsns.distplot(train[train.v18q1_num.notnull()].v18q1_num)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**`Important :`** As we can see that all the data is centred at one point however we can not take average in this case because this column represents \"Number of tablets per household\" and if we replace by average it will be in decimals but a physical object can not be in decimals. \n\nHence we will replace the missing values by **`median`**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.v18q1_num.fillna(train.v18q1_num.median(),inplace=True)\nprint('\\nMissing Values :')\ntrain.v18q1_num.isnull().value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**<font color=red>Handling missing values :** `rez_esc_num  --> Years behind in school`\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Mean -->',train.rez_esc_num.mean())\nprint('Median -->',train.rez_esc_num.median())\nprint('Std -->',train.rez_esc_num.std())\n\nsns.distplot(train[train.rez_esc_num.notnull()].rez_esc_num)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# \"Unlike\" v18q1_num, this number can be in decimals hence we will replace the missing values with mean value >>>  \nprint('Mean BEFORE Change : ',train.rez_esc_num.mean())\ntrain.rez_esc_num.fillna(train.rez_esc_num.mean(),inplace=True)\nprint('Mean AFTER Change : ',train.rez_esc_num.mean())\nprint('\\nMissing Values :')\ntrain.rez_esc_num.isnull().value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**<font color=red>Handling Missing values :**  `\"meaneduc_num\" and \"SQBmeaned_num\"`\n\nAs the number of values missing are very less hence we will use method of 'ffill' which means the data from just previous cell will be copied to the next missing cells."},{"metadata":{"trusted":true},"cell_type":"code","source":"train.meaneduc_num.fillna(method='ffill',inplace=True)\ntrain.SQBmeaned_num.fillna(method='ffill',inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Null_Df = pd.DataFrame({'Null_Count':train.isnull().sum(),\n                        'Null_Perc %':round(train.isnull().sum()*100/train.shape[0],2)})\nNull_Df[Null_Df.Null_Count!=0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color=red>***So No Null values***"},{"metadata":{},"cell_type":"markdown","source":"# <font color=royalblue> Plan Of Attack:\n    \n1. Divide the train data into Numerical, Categorical and Dependent var\n2. Categorical : We don't need to encode categorical columns as the columns are already encoded as 0 or 1\n3. Numerical : We have to scale them. But we will scale them using both methods \"StandardScaler\" and \"MinMaxScaler\" so that we can use any of the scaled data as per requirement to increase accuracy. \n4. After scaling, we will combine both Categorical and Numerical\n5. Next we will split this combined data and dependent data into train and test datasets as we need to calculate the accuracy of our prediction model. \n6. Then we will try to apply differnt methodology like KNN, RandomForest and lastly ANN (if required)\n7. We will also try to use the Validation method K-Fold and dimension reduction method PCA (if required)\n8. Finally after the model is prepared, we will apply on test data provided with this project and will generate the submission file. "},{"metadata":{},"cell_type":"markdown","source":"### <font color =darkmagenta>Split the data - Numerical, Categorical and Dependent"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Below code is to generate the separate lists of numerical and categorical columns.  \n# This is required as in this dataset, categorical and numerical columns have same type of values i.e. int or float instead of str value for...\n# ... categorical values which is usual for other datasets. Hence in those datasets, it is easy to filter the numerical and categorical columns...\n# ... using dtype. Hence for this dataset, I renamed the column names manually in excel by checking the column description given on the project's website. \n\nnumerical = []\ncategorical = []\nj = 0\nfor i in train.columns:\n    if train.columns.str.contains('_num',regex=True)[j]:\n        numerical.append(i)\n    else:\n        categorical.append(i)\n    j+=1\nprint('Numerical Columns : %s\\n'%len(numerical))\nprint('Categorical Columns : %s\\n'%len(categorical))\nprint('Total Columns :%s'%train.shape[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <font color=green> Cleaning the data\nThe data contains mixed values means along with the numerical values, the data contains string values within the same columns. Hence we have to clean this data"},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_numerical = train.loc[:,numerical]\ntemp_categorical = train.loc[:,categorical]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"obj_num = []\nfor i in temp_numerical.columns:\n    if str(temp_numerical[i].dtype) =='object':\n        obj_num.append(i)\nprint('Numerical columns with str values : ',obj_num)        \nobj_cat = []\nfor i in temp_categorical.columns:\n    if str(temp_categorical[i].dtype) =='object':\n        obj_cat.append(i)\nprint('Categorical columns with str values : ',obj_cat) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**`Cleaning Categorical`** : Well we really would not include the two columns found with str in categorical category as those columns can not contribute in the training of model because they are just IDs/Unique Values hence can not predict any pattern. "},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_categorical.drop(['Id', 'idhogar'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**`Cleaning Numerical`** : For this, lets plot the histogram and see how the numerical values are spread so that we can decide how to replace the str values. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Run below to know what are the str values >>> \n#test_temp_numerical.dependency_num.value_counts().index\n#temp_numerical.edjefe_num.value_counts().index\n#temp_numerical.edjefa_num.value_counts().index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,5))\nplt.subplot(1,3,1)\nsns.distplot(temp_numerical[(temp_numerical.edjefe_num != 'yes')& (temp_numerical.edjefe_num != 'no')].edjefe_num.astype('int'))\n\nplt.subplot(1,3,2)\nsns.distplot(temp_numerical[(temp_numerical.edjefa_num != 'yes')& (temp_numerical.edjefa_num != 'no')].edjefa_num.astype('int'))\n\nplt.subplot(1,3,3)\nsns.distplot(temp_numerical[(temp_numerical.dependency_num != 'yes')& (temp_numerical.dependency_num != 'no')].dependency_num.astype('float'))\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**<font color = steelblue>Data above is spread almost in a similar way. So we will replace the values with method : Mean of values :-)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"t1 = time.time()\nmean_edjefe = temp_numerical[(temp_numerical.edjefe_num != 'yes')& (temp_numerical.edjefe_num != 'no')].edjefe_num.astype('int').mean()\nmean_edjefa = temp_numerical[(temp_numerical.edjefa_num != 'yes')& (temp_numerical.edjefa_num != 'no')].edjefa_num.astype('int').mean()\nmean_dependency = temp_numerical[(temp_numerical.dependency_num != 'yes')& (temp_numerical.dependency_num != 'no')].dependency_num.astype('float').mean()\nedjefe = []\nedjefa = []\ndependency = []\nfor i in range(temp_numerical.shape[0]):\n    if temp_numerical.edjefe_num[i] in ['yes','no']:\n        edjefe.append(mean_edjefe)\n    else:\n        edjefe.append(temp_numerical.edjefe_num[i])\n    \n    if temp_numerical.edjefa_num[i] in ['yes','no']:\n        edjefa.append(mean_edjefa)\n    else:\n        edjefa.append(temp_numerical.edjefa_num[i])\n        \n    if temp_numerical.dependency_num[i] in ['yes','no']:\n        dependency.append(mean_dependency)\n    else:\n        dependency.append(temp_numerical.dependency_num[i])\n    \n    if i%100==0:        # Small patch just to keep track of progress of loop. \n        print('Rows Processed --> %s'%i,end='\\r')\n\ntemp_numerical.edjefe_num = edjefe\ntemp_numerical.edjefa_num = edjefa\ntemp_numerical.dependency_num = dependency\n\n# Changing the type of the columns >>> \n\ntemp_numerical.edjefe_num = temp_numerical.edjefe_num.astype('int64')\ntemp_numerical.edjefa_num = temp_numerical.edjefa_num.astype('int64')\ntemp_numerical.dependency_num = temp_numerical.dependency_num.astype('float64')\n\nt2 = time.time()\nprint('Time Elapsed : %s'%(t2-t1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,5))\nplt.subplot(1,3,1)\nsns.distplot(temp_numerical.edjefe_num)\nplt.subplot(1,3,2)\nsns.distplot(temp_numerical.edjefa_num)\nplt.subplot(1,3,3)\nsns.distplot(temp_numerical.dependency_num)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After replacing the str values with numerical, the data is quite skewed to one point for all three columns however there was no other option but to replace the values or otherwise just remove the whole columns themselves. \n\nWe will try to remove the columns if we do not get good results. (good accuracy in the model). Lets move further for the scaling exercise >>> "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making the copies of the above datasets >>> \n# WHY making copies ? Because we have processed the two types of data separately. Hence now if we make any changes to ...\n# ...the copies and want fresh data again then we can just run this cell >>> \n\nx_train_numerical = temp_numerical.copy()\nx_train_categorical = temp_categorical.copy()\n\n# This data would have the dependent variable as well hence we need to drop that >>> \nx_train_categorical.drop('Target',axis=1,inplace=True)\n\ny_train_full = train.loc[:,'Target']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <font color =darkblue> Scaling the Numerical data\n    \nWe will scale using both methods in separate cells hence whichever type is required, just run that cell !!!"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Indicator var >>> Run This Cell >>>\nstd_scal = 0\nMinMax =0\n\n# Hence after running any of the scaling method's cell, run \"# Scaling method Used >>> \" cell as well to mention which...\n# ... method being used for scaling. ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# StandardScaler >>> \n\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\nscaled = sc.fit_transform(x_train_numerical)\n\n# Converting array type back to dataframe >>> \nx_train_scaled = pd.DataFrame(scaled)\nx_train_scaled.columns = x_train_numerical.columns\nstd_scal+=1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# MinMaxScaler >>> \n\nfrom sklearn.preprocessing import MinMaxScaler\n\nsc = MinMaxScaler()\nscaled = sc.fit_transform(x_train_numerical)\n\n# Converting array type back to dataframe >>> \nx_train_scaled = pd.DataFrame(scaled)\nx_train_scaled.columns = x_train_numerical.columns\nMinMax+=1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scaling method Used >>> \nif MinMax >= std_scal:\n    print('Scaling Method Utilized : \"MinMaxScaler\"')\nelse:\n    print('Scaling Method Utilized : \"StandardScaler\"')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <font color = tomato>Combining Data : Numerical - Scaled + Categorical"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Shape of Numerical : %s'%x_train_numerical.shape[1])\nprint('Shape of Categorical : %s'%x_train_categorical.shape[1])\nx_train_full = x_train_scaled.merge(x_train_categorical,left_index=True,right_index=True)\nprint('Shape of Combined : %s'%x_train_full.shape[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <font color = darkblue> Part - 2 : Creating Prediction Model "},{"metadata":{},"cell_type":"markdown","source":"## <font color = limegreen> Algorithm : KNN"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"t1 = time.time()\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n# Creating the copies of data to be inserted >>> \nx_train = x_train_full.copy()\ny_train = y_train_full.copy()\n\nx_train,x_test,y_train,y_test = train_test_split(x_train,y_train,test_size=0.2,random_state=123)\n\n# Find optimal neigbors >>> \nA = []\nB = []\nfor i in range(11,500,40):\n    classifier = KNeighborsClassifier(n_neighbors=i,metric='euclidean').fit(x_train,y_train)\n    y_pred = classifier.predict(x_test)\n    A.append(i)\n    B.append(round(accuracy_score(y_test,y_pred)*100,3))\n    \ntable = pd.DataFrame({'Neighbors':A,'Accuracy':B})\nplt.figure(figsize=(15,5))\nplt.plot(table.Neighbors,table.Accuracy,marker='^',mec='black',mfc='r',ms=10,color='b')\nplt.xticks(np.arange(11,500,40))\nplt.grid()\nplt.show()\n\nt2 = time.time()\nprint('Time Elapsed : %s'%(t2-t1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**<font color=royalblue>With above run, we can conclude that the highest accuracy we can achieve with neighbors = 11**"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n# Creating the copies of data to be inserted >>> \nx_train = x_train_full.copy()\ny_train = y_train_full.copy()\n\nx_train,x_test,y_train,y_test = train_test_split(x_train,y_train,test_size=0.2,random_state=123)\n\n# Run KNN >>> \nclassifier = KNeighborsClassifier(n_neighbors=10,metric='euclidean').fit(x_train,y_train)\ny_pred = classifier.predict(x_test)\n\naccuracy_knn = round(accuracy_score(y_test,y_pred)*100,3)\nprint('Accuracy via KNN : ',accuracy_knn)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <font color=fuchsia>Algorithm : RandomForest\nLets run RandomForest Algorithm and see how it performs"},{"metadata":{"trusted":true},"cell_type":"code","source":"t1 = time.time()\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n# Creating the copies of data to be inserted >>> \nx_train = x_train_full.copy()\ny_train = y_train_full.copy()\n\nx_train,x_test,y_train,y_test = train_test_split(x_train,y_train,test_size=0.2,random_state=123)\n\n# Find optimal number of trees  >>> \nA = []\nB = []\nfor i in range(11,500,40):\n    classifier = RandomForestClassifier(n_estimators=i,criterion='entropy',random_state=123).fit(x_train,y_train)\n    y_pred = classifier.predict(x_test)\n    A.append(i)\n    B.append(round(accuracy_score(y_test,y_pred)*100,3))\n    \n    print('Processing Estimators : %s'%i,end='\\r')\n    \ntable = pd.DataFrame({'Neighbors':A,'Accuracy':B})\nplt.figure(figsize=(15,5))\nplt.plot(table.Neighbors,table.Accuracy,marker='^',mec='black',mfc='b',ms=10,color='r',linestyle='--')\nplt.xticks(np.arange(11,500,40))\nplt.grid()\nplt.show()\n\nt2 = time.time()\nprint('Time Elapsed : %s'%(t2-t1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**<font color=royalblue>Lets create the model using the estimators = 211**"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n# Creating the copies of data to be inserted >>> \nx_train = x_train_full.copy()\ny_train = y_train_full.copy()\n\nx_train,x_test,y_train,y_test = train_test_split(x_train,y_train,test_size=0.2,random_state=123)\n\n# Run RandomForest >>> \nclassifier = classifier = RandomForestClassifier(n_estimators=211,criterion='entropy',random_state=123).fit(x_train,y_train)\ny_pred = classifier.predict(x_test)\n\naccuracy = round(accuracy_score(y_test,y_pred)*100,3)\nprint('Accuracy via RandomForest : ',accuracy)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <font color=blueviolet>Model Validation : K-Fold"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing essential libraries >>> \n\nfrom sklearn.model_selection import cross_val_score\n\n# run K-Fold >>>\naccuracies = cross_val_score(estimator = classifier,X = x_train,y=y_train,cv=10)\n\n# Results >>>\n# -------------\nprint('Actual Accuracy : %s'%round((accuracies.mean()*100),2)+' %')\nprint('Standard Deviation of Accuracy : %s'%round((accuracies.std()*100),2)+' %')\nprint('RandomFt Accuracy :%s'%accuracy+' %')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <font color = royalblue> Remarks for K-Fold :\n<font color = red>**From above, it is clear that the model accuracy is stable as the mean accuracy doesn't have high difference from the accuracy calculated by RandomForest model directly.**\n<font color = red\\>  \n    \n<font color =black>Now we will apply this model on the test data\n---\n    "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Accessing the test data >>> \ntest = pd.read_csv(\"../input/costa-rican-household-poverty-prediction/test.csv\")\nprint('Shape : ',test.shape)\ntest.head(2)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"test.drop(['Id','idhogar'],axis=1,inplace=True)\nprint('Shape : ',test.shape)\ntest.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Tagging of \"Numerical\" columns**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Instead of manually renaming the columns, for test data I will try to add suffix \"_num\" to the numerical columns based on their values >>> \nnumerical = []\ncategorical = []\ncolumns = test.columns\nfor i in columns:\n    if len(test[i].unique()) >2:\n        test.rename(columns={i:i+'_num'},inplace=True)\n        numerical.append(i+'_num')\n    else:\n        categorical.append(i)\n        \nprint('Numerical Columns : %s\\n'%len(numerical))\nprint('Categorical Columns : %s\\n'%len(categorical))\nprint('Total Columns :%s'%test.shape[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Finding Missing Data (if any)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"Null_Df = pd.DataFrame({'Null_Count':test.isnull().sum(),\n                        'Null_Perc %':round(test.isnull().sum()*100/test.shape[0],2)})\nNull_Df[Null_Df.Null_Count!=0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Hanlding the missing values same as train data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"test.v2a1_num.fillna(test.v2a1_num.mean(),inplace=True)\ntest.v18q1_num.fillna(test.v18q1_num.median(),inplace=True)\ntest.rez_esc_num.fillna(test.rez_esc_num.mean(),inplace=True)\ntest.meaneduc_num.fillna(method='ffill',inplace=True)\ntest.SQBmeaned_num.fillna(method='ffill',inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Null_Df = pd.DataFrame({'Null_Count':test.isnull().sum(),\n                        'Null_Perc %':round(test.isnull().sum()*100/test.shape[0],2)})\nNull_Df[Null_Df.Null_Count!=0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color=red>***So NO Null Values !!!***"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_temp_numerical = test.loc[:,numerical]\ntest_temp_categorical = test.loc[:,categorical]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Validating if all the train data columns match test data columns**"},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = temp_numerical.columns\nnum = []\nfor i in test_temp_numerical.columns:\n    if i not in columns:\n        num.append(i)\nprint('Column not matching : ',num)\n        \ncat=[]\ncolumns = temp_categorical.columns\nfor i in test_temp_categorical.columns:\n    if i not in columns:\n        cat.append(i)\nprint('Column not matching : ',cat)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color=red>***So No mismatched columns!!!***"},{"metadata":{},"cell_type":"markdown","source":"**Clearing Data : Handling Mixed Values**"},{"metadata":{"trusted":true},"cell_type":"code","source":"obj_num = []\nfor i in test_temp_numerical.columns:\n    if str(test_temp_numerical[i].dtype) =='object':\n        obj_num.append(i)\nprint('Numerical columns with str values : ',obj_num)        \nobj_cat = []\nfor i in test_temp_categorical.columns:\n    if str(test_temp_categorical[i].dtype) =='object':\n        obj_cat.append(i)\nprint('Categorical columns with str values : ',obj_cat) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Run below to know what are the str values >>> \n#test_temp_numerical.dependency_num.value_counts().index\n#test_temp_numerical.edjefe_num.value_counts().index\n#test_temp_numerical.edjefa_num.value_counts().index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,5))\nplt.subplot(1,3,1)\nsns.distplot(test_temp_numerical[(test_temp_numerical.edjefe_num != 'yes')& (test_temp_numerical.edjefe_num != 'no')].edjefe_num.astype('int'))\n\nplt.subplot(1,3,2)\nsns.distplot(test_temp_numerical[(test_temp_numerical.edjefa_num != 'yes')& (test_temp_numerical.edjefa_num != 'no')].edjefa_num.astype('int'))\n\nplt.subplot(1,3,3)\nsns.distplot(test_temp_numerical[(test_temp_numerical.dependency_num != 'yes')& (test_temp_numerical.dependency_num != 'no')].dependency_num.astype('float'))\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**<font color = steelblue>Just like the train data, we will handle str values for test data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"t1 = time.time()\nmean_edjefe = test_temp_numerical[(test_temp_numerical.edjefe_num != 'yes')& (test_temp_numerical.edjefe_num != 'no')].edjefe_num.astype('int').mean()\nmean_edjefa = test_temp_numerical[(test_temp_numerical.edjefa_num != 'yes')& (test_temp_numerical.edjefa_num != 'no')].edjefa_num.astype('int').mean()\nmean_dependency = test_temp_numerical[(test_temp_numerical.dependency_num != 'yes')& (test_temp_numerical.dependency_num != 'no')].dependency_num.astype('float').mean()\nedjefe = []\nedjefa = []\ndependency = []\nfor i in range(test_temp_numerical.shape[0]):\n    if test_temp_numerical.edjefe_num[i] in ['yes','no']:\n        edjefe.append(mean_edjefe)\n    else:\n        edjefe.append(test_temp_numerical.edjefe_num[i])\n    \n    if test_temp_numerical.edjefa_num[i] in ['yes','no']:\n        edjefa.append(mean_edjefa)\n    else:\n        edjefa.append(test_temp_numerical.edjefa_num[i])\n        \n    if test_temp_numerical.dependency_num[i] in ['yes','no']:\n        dependency.append(mean_dependency)\n    else:\n        dependency.append(test_temp_numerical.dependency_num[i])\n    \n    if i%100==0:        # Small patch just to keep track of progress of loop. \n        print('Rows Processed --> %s'%i,end='\\r')\n\ntest_temp_numerical.edjefe_num = edjefe\ntest_temp_numerical.edjefa_num = edjefa\ntest_temp_numerical.dependency_num = dependency\n\n# Changing the type of the columns >>> \n\ntest_temp_numerical.edjefe_num = test_temp_numerical.edjefe_num.astype('int64')\ntest_temp_numerical.edjefa_num = test_temp_numerical.edjefa_num.astype('int64')\ntest_temp_numerical.dependency_num = test_temp_numerical.dependency_num.astype('float64')\n\nt2 = time.time()\nprint('Time Elapsed : %s'%(t2-t1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Making Copies of the datasets**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making the copies of the above datasets >>> \n# WHY making copies ? Because we have processed the two types of data separately. Hence now if we make any changes to ...\n# ...the copies and want fresh data again then we can just run this cell >>> \n\nx_test_numerical = test_temp_numerical.copy()\nx_test_categorical = test_temp_categorical.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Scaling the Numerical Data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# MinMaxScaler >>> \n\nfrom sklearn.preprocessing import MinMaxScaler\n\nsc = MinMaxScaler()\nscaled = sc.fit_transform(x_test_numerical)\n\n# Converting array type back to dataframe >>> \nx_test_scaled = pd.DataFrame(scaled)\nx_test_scaled.columns = x_test_numerical.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Combining Numerical + Categorical**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Shape of Numerical : %s'%x_test_numerical.shape[1])\nprint('Shape of Categorical : %s'%x_test_categorical.shape[1])\nx_test_full = x_test_scaled.merge(x_test_categorical,left_index=True,right_index=True)\nprint('Shape of Combined : %s'%x_test_full.shape[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fitting Model & Making Prediction "},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_pred = classifier.predict(x_test_full)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_test = pd.read_csv(\"../input/costa-rican-household-poverty-prediction/test.csv\") \n# Accessing raw test again to get the Id column which is dropped in the original data. ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_file = pd.DataFrame({'Id':data_test.Id,'Target':y_test_pred})\nsubmission_file.Target.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_file.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_file.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}