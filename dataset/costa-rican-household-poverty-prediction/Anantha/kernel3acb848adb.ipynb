{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Costa-Rican Poverty Line Prediction\n\nThe Inter-American Development Bank is asking the Kaggle community for help with income qualification for some of the world's poorest families. Are you up for the challenge?\n\nHere's the backstory: Many social programs have a hard time making sure the right people are given enough aid. It’s especially tricky when a program focuses on the poorest segment of the population. The world’s poorest typically can’t provide the necessary income and expense records to prove that they qualify.\n\nIn Latin America, one popular method uses an algorithm to verify income qualification. It’s called the Proxy Means Test (or PMT). With PMT, agencies use a model that considers a family’s observable household attributes like the material of their walls and ceiling, or the assets found in the home to classify them and predict their level of need.\n\nWhile this is an improvement, accuracy remains a problem as the region’s population grows and poverty declines.\n\nTo improve on PMT, the IDB (the largest source of development financing for Latin America and the Caribbean) has turned to the Kaggle community. They believe that new methods beyond traditional econometrics, based on a dataset of Costa Rican household characteristics, might help improve PMT’s performance.\n\nBeyond Costa Rica, many countries face this same problem of inaccurately assessing social need. If Kagglers can generate an improvement, the new algorithm could be implemented in other countries around the world."},{"metadata":{},"cell_type":"markdown","source":"## Calling required libraries for the work"},{"metadata":{"trusted":false},"cell_type":"code","source":"# essential libraries\nimport numpy as np \nimport pandas as pd\n# for data visulization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n#for data processing\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import  OneHotEncoder as ohe\nfrom sklearn.preprocessing import StandardScaler as ss\nfrom sklearn.compose import ColumnTransformer as ct\nfrom sklearn.impute import SimpleImputer\nfrom imblearn.over_sampling import SMOTE, ADASYN\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import train_test_split\n\n# for modeling estimators\nfrom sklearn.ensemble import RandomForestClassifier as rf\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier as gbm\nfrom xgboost.sklearn import XGBClassifier\nimport lightgbm as lgb\n\n# for measuring performance\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import auc, roc_curve\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import average_precision_score\nimport sklearn.metrics as metrics\nfrom xgboost import plot_importance\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import confusion_matrix\n\n#for tuning parameters\nfrom bayes_opt import BayesianOptimization\nfrom skopt import BayesSearchCV\nfrom eli5.sklearn import PermutationImportance\n\n# Misc.\nimport os\nimport time\nimport gc\nimport random\nfrom scipy.stats import uniform\nimport warnings","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Reading the data"},{"metadata":{"trusted":false},"cell_type":"code","source":"pd.options.display.max_columns = 150\n\n# Read in data\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Explore data and perform data visualization"},{"metadata":{"trusted":false},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train.info()   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.countplot(\"Target\", data=train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":" sns.countplot(x=\"r4t3\",hue=\"Target\",data=train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.countplot(x=\"v18q\",hue=\"Target\",data=train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.countplot(x=\"v18q1\",hue=\"Target\",data=train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.countplot(x=\"tamhog\",hue=\"Target\",data=train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.countplot(x=\"hhsize\",hue=\"Target\",data=train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.countplot(x=\"abastaguano\",hue=\"Target\",data=train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.countplot(x=\"noelec\",hue=\"Target\",data=train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train.select_dtypes('object').head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"\n\nyes_no_map = {'no':0,'yes':1}\ntrain['dependency'] = train['dependency'].replace(yes_no_map).astype(np.float32)\ntrain['edjefe'] = train['edjefe'].replace(yes_no_map).astype(np.float32)\ntrain['edjefa'] = train['edjefa'].replace(yes_no_map).astype(np.float32)\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Converting categorical objects into numericals "},{"metadata":{"trusted":false},"cell_type":"code","source":"train[[\"dependency\",\"edjefe\",\"edjefa\"]].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fill in missing values (NULL values)  using 1 for yes and 0 for no"},{"metadata":{"trusted":false},"cell_type":"code","source":" # Number of missing in each column\nmissing = pd.DataFrame(train.isnull().sum()).rename(columns = {0: 'total'})\n\n# Create a percentage missing\nmissing['percent'] = missing['total'] / len(train)\n\nmissing.sort_values('percent', ascending = False).head(10)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train['v18q1'] = train['v18q1'].fillna(0)\ntest['v18q1'] = test['v18q1'].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train['v2a1'] = train['v2a1'].fillna(0)\ntest['v2a1'] = test['v2a1'].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train['rez_esc'] = train['rez_esc'].fillna(0)\ntest['rez_esc'] = test['rez_esc'].fillna(0)\ntrain['SQBmeaned'] = train['SQBmeaned'].fillna(0)\ntest['SQBmeaned'] = test['SQBmeaned'].fillna(0)\ntrain['meaneduc'] = train['meaneduc'].fillna(0)\ntest['meaneduc'] = test['meaneduc'].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Checking for missing values again to confirm that no missing values present\n# Number of missing in each column\nmissing = pd.DataFrame(train.isnull().sum()).rename(columns = {0: 'total'})\n\n# Create a percentage missing\nmissing['percent'] = missing['total'] / len(train)\n\nmissing.sort_values('percent', ascending = False).head(10)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dropping unnecesary columns"},{"metadata":{"trusted":false},"cell_type":"code","source":"train.drop(['Id','idhogar',\"dependency\",\"edjefe\",\"edjefa\"], inplace = True, axis =1)\n\ntest.drop(['Id','idhogar',\"dependency\",\"edjefe\",\"edjefa\"], inplace = True, axis =1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dividing the data into predictors & target"},{"metadata":{"trusted":false},"cell_type":"code","source":"y = train.iloc[:,137]\ny.unique()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X = train.iloc[:,1:138]\nX.shape\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Scaling  numeric features & applying PCA to reduce features"},{"metadata":{"trusted":false},"cell_type":"code","source":"\nmy_imputer = SimpleImputer()\nX = my_imputer.fit_transform(X)\nscale = ss()\nX = scale.fit_transform(X)\npca = PCA(0.95)\nX = pca.fit_transform(X)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Final features selected for modeling"},{"metadata":{"trusted":false},"cell_type":"code","source":"X.shape, y.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Splitting the data into train & test "},{"metadata":{"trusted":false},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(\n                                                    X,\n                                                    y,\n                                                    test_size = 0.2)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modelling"},{"metadata":{},"cell_type":"markdown","source":"## Modelling with Random Forest"},{"metadata":{"trusted":false},"cell_type":"code","source":" \nmodelrf = rf()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"start = time.time()\nmodelrf = modelrf.fit(X_train, y_train)\nend = time.time()\n(end-start)/60","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"classes = modelrf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"(classes == y_test).sum()/y_test.size ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Performing tuning using Bayesian Optimization."},{"metadata":{"trusted":false},"cell_type":"code","source":"bayes_cv_tuner = BayesSearchCV(\n    #  Place your estimator here with those parameter values\n    #      that you DO NOT WANT TO TUNE\n    rf(\n       n_jobs = 2         # No need to tune this parameter value\n      ),\n\n    # 2.12 Specify estimator parameters that you would like to change/tune\n    {\n        'n_estimators': (100, 500),           # Specify integer-values parameters like this\n        'criterion': ['gini', 'entropy'],     # Specify categorical parameters as here\n        'max_depth': (4, 100),                # integer valued parameter\n        'max_features' : (10,64),             # integer-valued parameter\n        'min_weight_fraction_leaf' : (0,0.5, 'uniform')   # Float-valued parameter\n    },\n\n    # 2.13\n    n_iter=32,            # How many points to sample\n    cv = 3                # Number of cross-validation folds\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Start optimization\nbayes_cv_tuner.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#  Get list of best-parameters\nbayes_cv_tuner.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#  Get what average accuracy was acheived during cross-validation\nbayes_cv_tuner.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#  What accuracy is available on test-data\nbayes_cv_tuner.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#  And what all sets of parameters were tried?\nbayes_cv_tuner.cv_results_['params']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Accuracy improved from 71.91% to 76.20%"},{"metadata":{},"cell_type":"markdown","source":"## Modelling with ExtraTreeClassifier"},{"metadata":{"trusted":false},"cell_type":"code","source":"modeletf = ExtraTreesClassifier()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"start = time.time()\nmodeletf = modeletf.fit(X_train, y_train)\nend = time.time()\n(end-start)/60","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"classes = modeletf.predict(X_test)\n\nclasses","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"(classes == y_test).sum()/y_test.size","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Performing tuning using Bayesian Optimization."},{"metadata":{"trusted":false},"cell_type":"code","source":"bayes_cv_tuner = BayesSearchCV(\n    #  Place your estimator here with those parameter values\n    #      that you DO NOT WANT TO TUNE\n    ExtraTreesClassifier( ),\n\n    # 2.12 Specify estimator parameters that you would like to change/tune\n    {   'n_estimators': (100, 500),           # Specify integer-values parameters like this\n        'criterion': ['gini', 'entropy'],     # Specify categorical parameters as here\n        'max_depth': (4, 100),                # integer valued parameter\n        'max_features' : (10,64),             # integer-valued parameter\n        'min_weight_fraction_leaf' : (0,0.5, 'uniform')   # Float-valued parameter\n    },\n\n    n_iter=32,            # How many points to sample\n    cv = 2            # Number of cross-validation folds\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Start optimization\nbayes_cv_tuner.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#  Get list of best-parameters\nbayes_cv_tuner.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#  Get what average accuracy was acheived during cross-validation\nbayes_cv_tuner.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#  What accuracy is available on test-data\nbayes_cv_tuner.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#  And what all sets of parameters were tried?\nbayes_cv_tuner.cv_results_['params']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modelling with KNeighborsClassifier"},{"metadata":{"trusted":false},"cell_type":"code","source":"modelneigh = KNeighborsClassifier(n_neighbors=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"start = time.time()\nmodelneigh = modelneigh.fit(X_train, y_train)\nend = time.time()\n(end-start)/60\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"classes = modelneigh.predict(X_test)\n\nclasses","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"(classes == y_test).sum()/y_test.size ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Performing tuning using Bayesian Optimization."},{"metadata":{"trusted":false},"cell_type":"code","source":"bayes_cv_tuner = BayesSearchCV(\n    #  Place your estimator here with those parameter values\n    #      that you DO NOT WANT TO TUNE\n    KNeighborsClassifier(\n       n_neighbors=4         # No need to tune this parameter value\n      ),\n    {\"metric\": [\"euclidean\", \"cityblock\"]},\n    n_iter=32,            # How many points to sample\n    cv = 2            # Number of cross-validation folds\n   )","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Start optimization\nbayes_cv_tuner.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#  Get list of best-parameters\nbayes_cv_tuner.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#  Get what average accuracy was acheived during cross-validation\nbayes_cv_tuner.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#  What accuracy is available on test-data\nbayes_cv_tuner.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#  And what all sets of parameters were tried?\nbayes_cv_tuner.cv_results_['params']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modelling with GradientBoostingClassifier"},{"metadata":{"trusted":false},"cell_type":"code","source":"modelgbm=gbm()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"start = time.time()\nmodelgbm = modelgbm.fit(X_train, y_train)\nend = time.time()\n(end-start)/60\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"classes = modelgbm.predict(X_test)\n\nclasses","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"(classes == y_test).sum()/y_test.size ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Performing tuning using Bayesian Optimization."},{"metadata":{"trusted":false},"cell_type":"code","source":"bayes_cv_tuner = BayesSearchCV(\n    #  Place your estimator here with those parameter values\n    #      that you DO NOT WANT TO TUNE\n    gbm(\n               # No need to tune this parameter value\n      ),\n\n    # 2.12 Specify estimator parameters that you would like to change/tune\n    {\n        'n_estimators': (100, 500),           # Specify integer-values parameters like this\n        \n        'max_depth': (4, 100),                # integer valued parameter\n        'max_features' : (10,64),             # integer-valued parameter\n        'min_weight_fraction_leaf' : (0,0.5, 'uniform')   # Float-valued parameter\n    },\n\n    # 2.13\n    n_iter=32,            # How many points to sample\n    cv = 2                # Number of cross-validation folds\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Start optimization\nbayes_cv_tuner.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#  Get list of best-parameters\nbayes_cv_tuner.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#  Get what average accuracy was acheived during cross-validation\nbayes_cv_tuner.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#  What accuracy is available on test-data\nbayes_cv_tuner.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#  And what all sets of parameters were tried?\nbayes_cv_tuner.cv_results_['params']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modelling with XGBClassifier"},{"metadata":{"trusted":false},"cell_type":"code","source":"modelxgb=XGBClassifier()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"start = time.time()\nmodelxgb = modelxgb.fit(X_train, y_train)\nend = time.time()\n(end-start)/60","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"classes = modelxgb.predict(X_test)\n\nclasses","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"(classes == y_test).sum()/y_test.size ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Performing tuning using Bayesian Optimization."},{"metadata":{"trusted":false},"cell_type":"code","source":"bayes_cv_tuner = BayesSearchCV(\n    #  Place your estimator here with those parameter values\n    #      that you DO NOT WANT TO TUNE\n    XGBClassifier(\n       n_jobs = 2         # No need to tune this parameter value\n      ),\n\n    # 2.12 Specify estimator parameters that you would like to change/tune\n    {\n        'n_estimators': (100, 500),           # Specify integer-values parameters like this\n        'criterion': ['gini', 'entropy'],     # Specify categorical parameters as here\n        'max_depth': (4, 100),                # integer valued parameter\n        'max_features' : (10,64),             # integer-valued parameter\n        'min_weight_fraction_leaf' : (0,0.5, 'uniform')   # Float-valued parameter\n    },\n\n    # 2.13\n    n_iter=32,            # How many points to sample\n    cv = 3                # Number of cross-validation folds\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Start optimization\nbayes_cv_tuner.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#  Get list of best-parameters\nbayes_cv_tuner.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#  Get what average accuracy was acheived during cross-validation\nbayes_cv_tuner.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#  What accuracy is available on test-data\nbayes_cv_tuner.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#  And what all sets of parameters were tried?\nbayes_cv_tuner.cv_results_['params']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modelling with Light Gradient Booster"},{"metadata":{"trusted":false},"cell_type":"code","source":"modellgb = lgb.LGBMClassifier(max_depth=-1, learning_rate=0.1, objective='multiclass',\n                             random_state=None, silent=True, metric='None', \n                             n_jobs=4, n_estimators=5000, class_weight='balanced',\n                             colsample_bytree =  0.93, min_child_samples = 95, num_leaves = 14, subsample = 0.96)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"start = time.time()\nmodellgb = modellgb.fit(X_train, y_train)\nend = time.time()\n(end-start)/60","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"classes = modellgb.predict(X_test)\n\nclasses","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"(classes == y_test).sum()/y_test.size ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Performing tuning using Bayesian Optimization."},{"metadata":{"trusted":false},"cell_type":"code","source":"bayes_cv_tuner = BayesSearchCV(\n    #  Place your estimator here with those parameter values\n    #      that you DO NOT WANT TO TUNE\n    lgb.LGBMClassifier(\n       n_jobs = 2         # No need to tune this parameter value\n      ),\n\n    # 2.12 Specify estimator parameters that you would like to change/tune\n    {\n        'n_estimators': (100, 500),           # Specify integer-values parameters like this\n        'criterion': ['gini', 'entropy'],     # Specify categorical parameters as here\n        'max_depth': (4, 100),                # integer valued parameter\n        'max_features' : (10,64),             # integer-valued parameter\n        'min_weight_fraction_leaf' : (0,0.5, 'uniform')   # Float-valued parameter\n    },\n\n    # 2.13\n    n_iter=32,            # How many points to sample\n    cv = 3                # Number of cross-validation folds\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"\n# Start optimization\nbayes_cv_tuner.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#  Get list of best-parameters\nbayes_cv_tuner.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#  Get what average accuracy was acheived during cross-validation\nbayes_cv_tuner.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#  What accuracy is available on test-data\nbayes_cv_tuner.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#  And what all sets of parameters were tried?\nbayes_cv_tuner.cv_results_['params']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"                                            ACCURACY                         ACCURACY\n                                with default parameters             with  parameters tuned with Bayesian                                                                                  Optimization \n        RandomForestClassifier         77.87                               85.61\n        KNeighborsClassifier           80.70                               81.85 \n        ExtraTreesClassifier           77.98                               86.97\n        GradientBoostingClassifier     80.75                               91.42 \n        XGBoost                        78.03                               91.57\n        LightGBM                       93.41                               92.05 "}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"}},"nbformat":4,"nbformat_minor":1}