{"cells":[{"metadata":{"_uuid":"b1ba8edf674a2154be9ab2c56a7399de300defb7"},"cell_type":"markdown","source":"This is a comprehensive kernel including **data cleansing** , **traditional machine learning** algorithms, **xgboost**  combined with lasso and **lightgbm**.\n\nBut there won't be too much feature engineering and any other tricks. Here I just skip them and the farthest I  go is just to make the raw data work for algorithm. Still feature engineering is absolutely when you need to get a higher score    : )\n"},{"metadata":{"_uuid":"5dc467302bc611424c5e63914dc34fa12edbe42c"},"cell_type":"markdown","source":"> **import lib**\n\nWe import the libraries we need here all at once."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom pandas import Series,DataFrame\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport seaborn as sns\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.model_selection import StratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e337f1039fe279f7314f7647a508fee5ab0a8927"},"cell_type":"markdown","source":"> **Set**\n\nJust to show the data more completely in interactive environment."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":" pd.set_option('display.width', 1000) \n\npd.set_option('display.max_rows', 200) \n\npd.set_option('display.max_columns', 200) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"49e90db6b3f5ceb5d68280d3baa129b4ad01d533"},"cell_type":"markdown","source":"> **Read data file**"},{"metadata":{"trusted":true,"_uuid":"94d332958286394fd6c6a169a4d751d42f211ed2"},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\n\ntest = pd.read_csv('../input/test.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ec91018dd607d48949d2478a41b722348b729806"},"cell_type":"markdown","source":">** Data Cleansing**\n\nThis data cleaning part is kind of raw, it only include filling NaNs and type tranforming.\n\nAnd I found a curious question that deleting redundant variables like 'SQB's will decrease the below algorithms' score. So I am gonna let it be : )."},{"metadata":{"trusted":true,"_uuid":"fa0ebdbd85d34940412c2a863c146172fb6dfc23"},"cell_type":"code","source":"\nsum_id = test['Id']\ndel test['Id']\n\nY = train.Target.values.astype(int)\n\ndel train['Target']\n\nall_data = pd.concat((train.loc[:,'v2a1':'agesq'],\n                      test.loc[:,'v2a1':'agesq']))\ndel all_data['idhogar']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5905474c323132f908fead2692bd610f67f9ca1a"},"cell_type":"markdown","source":"> fill NaNs with median and 0"},{"metadata":{"trusted":true,"_uuid":"edbf94963fb8db6d6aac825c8d90aee96c743118"},"cell_type":"code","source":"#------------ fill NaNs --------------\n\nall_data.isnull().any()\n\nall_data[\"v2a1\"].fillna(all_data[\"v2a1\"].median(), inplace=True)\nall_data[\"v18q1\"].fillna(0, inplace=True)\nall_data[\"rez_esc\"].fillna(0, inplace=True)\nall_data[\"meaneduc\"].fillna(all_data[\"meaneduc\"].median(), inplace=True)\nall_data[\"SQBmeaned\"].fillna(all_data[\"SQBmeaned\"].median(), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a3744a51b9863ff9ae43e3bb073c84c37c7e6538"},"cell_type":"markdown","source":"> There are some variables are 'object' type,we need to transform it into int or float."},{"metadata":{"trusted":true,"_uuid":"20013b836deee41a62c4faa0bcbecd43c4a446e2"},"cell_type":"code","source":"#------------- digitalizing -----------\n\nall_data.loc[all_data[\"dependency\"]==\"yes\",\"dependency\"]=0.25      \nall_data.loc[all_data[\"dependency\"]==\"no\",\"dependency\"]=8\nall_data.loc[all_data[\"edjefe\"]==\"yes\",\"edjefe\"]=1\nall_data.loc[all_data[\"edjefe\"]==\"no\",\"edjefe\"]=0      \nall_data.loc[all_data[\"edjefa\"]==\"yes\",\"edjefa\"]=1\nall_data.loc[all_data[\"edjefa\"]==\"no\",\"edjefa\"]=0  \n\nall_data['dependency'] = all_data['dependency'].astype('float')\nall_data['edjefe'] = all_data['edjefe'].astype('float')\nall_data['edjefa'] = all_data['edjefa'].astype('float')\n\n\ntrain = all_data[:train.shape[0]]\ntest = all_data[train.shape[0]:]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"08cd040aac70b5c043b3f13de92a27658820c6e1"},"cell_type":"markdown","source":"> **Predicting**\n\nHere are some traditional mechine learning algorithm applying on this problem."},{"metadata":{"_uuid":"0d83efb0db256982c3e1fe1f1770d270371b2701"},"cell_type":"markdown","source":"**1.RandomForest**"},{"metadata":{"trusted":true,"_uuid":"97ef0ad39e4b23127671d89d0020f8db81e72f6e"},"cell_type":"code","source":"#------------------ Predicting ------------------------------------------------\n\n\n#---------- 1 RandomForest ----------------- Score: 0.366\n\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n\nrandom_forest = RandomForestClassifier(n_estimators=100)\n\nrandom_forest.fit(train, Y)\n\n\nX_train, X_val, y_train,y_val = train_test_split(train,Y,test_size=0.3, random_state=42) \nprint('trainning accuracy：\\n',random_forest.score(X_train, y_train))\nprint('validation accuracy：\\n',random_forest.score(X_val, y_val))\n\nprint('RandomForest Accuracy：\\n',random_forest.score(train, Y))\n\npred_RF = random_forest.predict(test)\n\nsol_RF = pd.DataFrame({'Id':sum_id.values, 'Target':pred_RF}) \n\nsol_RF.to_csv('pred_RF.csv',index=None) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2bd23ce5819e1f05f39093efefceda4738ae9a85"},"cell_type":"markdown","source":" **2. Decision Tree**"},{"metadata":{"trusted":true,"_uuid":"5dbb36ed2b0678683ea5eedafebe798ea2211895"},"cell_type":"code","source":"#---------- 2 DecisionTree ----------------- Score: 0.352\n\nfrom sklearn.tree import DecisionTreeClassifier\n\nDT=DecisionTreeClassifier()\n\nDT.fit(train,Y)\nX_train, X_val, y_train,y_val = train_test_split(train,Y,test_size=0.3, random_state=42)\n\nprint('Accuracy on training：\\n',DT.score(X_train, y_train))\nprint('Accuracy on validation：\\n',DT.score(X_val, y_val))\nprint('DecisionTree Accuracy：\\n',DT.score(train, Y))\n\npred_DT = (DT.predict(test))\n\nsol_DT = pd.DataFrame({'Id':sum_id.values, 'Target':pred_DT}) \n\nsol_DT.to_csv('pred_DT.csv',index=None) \n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d56bede583291dfd6498b700c58f003237047a1e"},"cell_type":"markdown","source":"**3. LogisticRegression**"},{"metadata":{"trusted":true,"_uuid":"9f53778557c42f05f5f8ccee33aa391cfd7506a9"},"cell_type":"code","source":"#---------- 3 LogisticRegression ----------------- Score: 0.253\n\nfrom sklearn.linear_model import LogisticRegression\n\nLR = LogisticRegression()\n\nLR.fit(train, Y)\nX_train, X_val, y_train,y_val = train_test_split(train,Y,test_size=0.3, random_state=42) \nprint('Accuracy on training：\\n',LR.score(X_train, y_train)) \nprint('Accuracy on validation：\\n',LR.score(X_val, y_val))\nprint('LogisticRegression Accuracy：\\n',LR.score(train, Y))\n\npred = LR.predict(test)\n\npred = pd.DataFrame({'Id':sum_id.values, 'Target':pred}) \n\npred.to_csv('pred_LR.csv',index=None) \n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0b3b1588fd0e420fe062d860096da23ac00b942e"},"cell_type":"markdown","source":"**4. kNN** "},{"metadata":{"trusted":true,"_uuid":"68ab5f484875e25a74e7732e55f546726cc72087"},"cell_type":"code","source":"#---------- 4 kNN ----------------- Score: 0.308\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors = 3)\n\nknn.fit(train, Y)\nX_train, X_val, y_train,y_val = train_test_split(train,Y,test_size=0.3, random_state=42) \n\nprint('Accuracy on training：\\n',knn.score(X_train, y_train)) \nprint('Accuracy on validation：\\n',knn.score(X_val, y_val))\nprint('kNN Accuracy：\\n',knn.score(train, Y))\n\npred = knn.predict(test)\n\npred = pd.DataFrame({'Id':sum_id.values, 'Target':pred}) \n\npred.to_csv('pred_kNN.csv',index=None)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5a84c21b9517874667d1ac3659bc0f2e30a73a69"},"cell_type":"markdown","source":"**5. NaiveBayes**"},{"metadata":{"trusted":true,"_uuid":"edf3c3d588956c59217f40222066ea06062cf96f"},"cell_type":"code","source":"#---------- 5 NaiveBayes Gaussian ----------------- Score: 0.373\n \nfrom sklearn.naive_bayes import GaussianNB\n\ngaussian = GaussianNB()\n\ngaussian.fit(train, Y)\nX_train, X_val, y_train,y_val = train_test_split(train,Y,test_size=0.3, random_state=42) \n\nprint('Accuracy on training：\\n',gaussian.score(X_train, y_train)) \nprint('Accuracy on validation：\\n',gaussian.score(X_val, y_val))\nprint('gaussian Accuracy：\\n',gaussian.score(train, Y))\n\npred_NB = gaussian.predict(test)\n\nsol_NB = pd.DataFrame({'Id':sum_id.values, 'Target':pred_NB})\n\nsol_NB.to_csv('pred_NaiveBayes.csv',index=None) \n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dfa09eadd55a2e5ff7d06d0adc985090f52e3469"},"cell_type":"markdown","source":"**6. LinearRegression**\n\nApplying LinearRegression on this issue this way won't work as a classifier and it generate floats.\n\nSo I manually transform the result into int type.\n\nIt is a kind of indirect way."},{"metadata":{"trusted":true,"_uuid":"a3aa46bd70d17325257b92854dcd2bd7e2a325df"},"cell_type":"code","source":"#---------- 6 LinearRegression ----------------- Grade: 0.346\n# doesn't output int\nfrom sklearn.linear_model import LinearRegression\n\nLR = LinearRegression()\n\nLR.fit(train, Y)\nX_train, X_val, y_train,y_val = train_test_split(train,Y,test_size=0.3, random_state=42) \n\nprint('Accuracy on training：\\n',LR.score(X_train, y_train)) \nprint('Accuracy on validation：\\n',LR.score(X_val, y_val))\nprint('LinearRegression Accuracy：\\n',LR.score(train, Y))\n\npred = LR.predict(test)\n  \npred = pd.DataFrame({'Id':sum_id.values, 'Target':pred}) \n\npred.loc[pred[\"Target\"] < 1.5,\"Target\"] = 1\npred.loc[(1.5 <= pred[\"Target\"]) & (pred[\"Target\"] < 2.5),\"Target\"] = 2\npred.loc[(2.5 <= pred[\"Target\"]) & ( pred[\"Target\"] < 3.5),\"Target\"] = 3\npred.loc[3.5 <= pred[\"Target\"],\"Target\"] = 4\npred['Target'] = pred['Target'].astype('int')\npred.to_csv('pred_Linear.csv',index=None) \n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e5023f5943e3432f246aa8ffa57d099c92159629"},"cell_type":"markdown","source":"> **XGBoost combined with lasso**"},{"metadata":{"trusted":true,"_uuid":"beaa28a6c11cd79bdb8685d9baa160ccc1a6079b"},"cell_type":"code","source":"#========================== lasso ridge xgb ============================== # Score: 0.363\n\nfrom sklearn.linear_model import Ridge, RidgeCV, ElasticNet, LassoCV, LassoLarsCV\nfrom sklearn.model_selection import cross_val_score\n\ndef rmse_cv(model):\n    rmse= np.sqrt(-cross_val_score(model, train, Y, scoring=\"neg_mean_squared_error\", cv = 5))\n    return(rmse)\n\nmodel_ridge = Ridge()\n\nalphas = [0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 50, 75]\ncv_ridge = [rmse_cv(Ridge(alpha = alpha)).mean() \n            for alpha in alphas]\n\ncv_ridge = pd.Series(cv_ridge, index = alphas)\ncv_ridge.plot(title = \"Validation - Just Do It\")\nplt.xlabel(\"alpha\")\nplt.ylabel(\"rmse\")\n\nprint(cv_ridge.min())\n\nmodel_lasso = LassoCV(alphas = [1, 0.1, 0.001, 0.0005]).fit(train, Y)\n\nprint(rmse_cv(model_lasso).mean())\n\ncoef = pd.Series(model_lasso.coef_, index = train.columns)\n\nprint(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")\n\nimp_coef = pd.concat([coef.sort_values().head(10),\n                     coef.sort_values().tail(10)])\nmatplotlib.rcParams['figure.figsize'] = (8.0, 10.0)\nimp_coef.plot(kind = \"barh\")\nplt.title(\"Coefficients in the Lasso Model\")\n\n#let's look at the residuals as well:\nmatplotlib.rcParams['figure.figsize'] = (6.0, 6.0)\n\npreds = pd.DataFrame({\"preds\":model_lasso.predict(train), \"true\":Y})\npreds[\"residuals\"] = preds[\"true\"] - preds[\"preds\"]\npreds.plot(x = \"preds\", y = \"residuals\",kind = \"scatter\")\n\n#-------------- xgboosting -------------\n\nimport xgboost as xgb\n\ndtrain = xgb.DMatrix(train, label = Y)\ndtest = xgb.DMatrix(test)\n\nparams = {\"max_depth\":2, \"eta\":0.1}\nmodel = xgb.cv(params, dtrain,  num_boost_round=500, early_stopping_rounds=100)\n\nmodel.loc[30:,[\"test-rmse-mean\", \"train-rmse-mean\"]].plot()\n\nmodel_xgb = xgb.XGBRegressor(n_estimators=360, max_depth=2, learning_rate=0.1) #the params were tuned using xgb.cv\nmodel_xgb.fit(train, Y)\n\nxgb_preds = model_xgb.predict(test)\nlasso_preds = model_lasso.predict(test)\n\npredictions = pd.DataFrame({\"xgb\":xgb_preds, \"lasso\":lasso_preds})\npredictions.plot(x = \"xgb\", y = \"lasso\", kind = \"scatter\")\n\npreds = 0.3*lasso_preds + 0.7*xgb_preds\n\nsolution = pd.DataFrame({\"Id\":sum_id.values, \"Target\":preds})\nsolution.loc[solution[\"Target\"] < 1.5,\"Target\"] = 1\nsolution.loc[(1.5 <= solution[\"Target\"]) & (solution[\"Target\"] < 2.5),\"Target\"] = 2\nsolution.loc[(2.5 <= solution[\"Target\"]) & ( solution[\"Target\"] < 3.5),\"Target\"] = 3\nsolution.loc[3.5 <= solution[\"Target\"],\"Target\"] = 4\nsolution['Target'] = solution['Target'].astype('int')\nsolution.to_csv(\"ridge_sol.csv\", index = False) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cfe717d4610d8d58df545dd41e08022f0e8c10f8"},"cell_type":"markdown","source":"> **Lightgbm**\n\nThis LGB with early-stopping is from[https://www.kaggle.com/ischurov/more-feature-eng-lgb-5-fold-early-stopping](http://)\n\nThanks to his work."},{"metadata":{"trusted":true,"_uuid":"e838465bea4c0465804fe263cfb34133aac5db7f"},"cell_type":"code","source":"#===================== Lightgbm ========================================= Score: 0.424\n\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\n\nclf = lgb.LGBMClassifier(class_weight='balanced', boosting_type='dart',\n                         drop_rate=0.9, min_data_in_leaf=100, \n                         max_bin=255,\n                         n_estimators=500,\n                         bagging_fraction=0.01,\n                         min_sum_hessian_in_leaf=1,\n                         importance_type='gain',\n                         learning_rate=0.1, \n                         max_depth=-1, \n                         num_leaves=31)\nkf = StratifiedKFold(n_splits=5, shuffle=True)\n# partially based on https://www.kaggle.com/c0conuts/xgb-k-folds-fastai-pca\nY = pd.Series(Y)\npredicts = []\n\nfor train_index, test_index in kf.split(train, Y):\n    print(\"###\")\n    X_train, X_val = train.iloc[train_index], train.iloc[test_index]\n    y_train, y_val = Y.iloc[train_index], Y.iloc[test_index]\n    clf.fit(X_train, y_train, eval_set=[(X_val, y_val)], \n            early_stopping_rounds=20)\n    predicts.append(clf.predict(test))\n    \npredict = pd.DataFrame(np.array(sum_id),\n                             columns=['Id'],\n                             index=test.index)\npredict['Target'] = np.array(predicts).mean(axis=0).round().astype(int)\npredict.to_csv('predict.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"50a2c67da506a191f21cf577651e26ef2dc680ec"},"cell_type":"markdown","source":"**Thanks if you read it through,  please leave a comment if you have any question, any feedback will be appreciated!**"},{"metadata":{"_uuid":"742d85d640a3f371c6df34a84e6da644f0f94617"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"396aa9c978c09923cf98042352ad1ef35fa1ccdb"},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}