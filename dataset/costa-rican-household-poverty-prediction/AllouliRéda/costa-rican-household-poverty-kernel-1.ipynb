{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Compréhension du problème"},{"metadata":{},"cell_type":"markdown","source":"L'objectif, sur la plateforme kaggle, du concours \"Costa Rican Household Poverty Level Prediction\" est de développer un modèle de prédiction du niveau de pauvereté d'un foyer en s'appuyant d'une part sur les caractéristiques individuelles de ces membres et d'autre part des caractéristiques du foyer.\n\nLes données de ce concours consistent en deux fichiers : train.csv et test.csv.\n\nCes fichiers contiennent respectivement 9557 et 23856 observations.\n\nLes deux fichiers contiennent 142 variables commununes. Ces variables peuvent être soit de niveau individu soit du niveau foyer.\n\nLe fichier d'apprentissage \"train.csv\" dispose d'une variable en plus : \"Target\". Cette dernière correspond à la variable à prédire.\n\nLa définition des 143 variables est disponible sur le site kaggle : Costa Rican Household Poverty Level Prediction.\n\n\nLa variable \"Target\" peut prendre une des 4 modalités : 1 à 4. La valeur 1 correspond au niveau extrême de pauvereté et le niveau 4 au niveau \"non vulnérable\".\n\n1 = Extrême\n2 = Modérée \n3 = Vulnérable\n4 = Non vulnérable\n\nOn est donc face à un problème :\n    - d'apprentissage supervisé\n    - Et de classification multi-classe\n"},{"metadata":{},"cell_type":"markdown","source":"## Objectifs"},{"metadata":{},"cell_type":"markdown","source":"L'objectif est de prédire le niveau de pauvereté au niveau de chaque foyer. \n\nLes observations sont de niveau individu. Les variables peuvent être soit de niveau individu soit de niveau foyer.\n\nChaque individu dispose d'un identifiant unique (id).\n\nChaque foyer est caractérisé par un identifant unique (idhogar) et un \"Représentant du foyer\" (parentesco1 = 1).\n\nPour la prédiction :\n- nous serons amené à agréger les variables de niveau individu pour créer des variables de niveau foyer.\n- A faire de prédiction au niveau de chaque individu. Cependant pour le scoring, la seule prédiction prise en compte pour un foyer est celle de son représentant.\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"## Métrique"},{"metadata":{},"cell_type":"markdown","source":"La métrique proposée dans le cadre de ce concours est le Macro F1-Score.\n\nIl s'agit donc d'une moyenne du F1-score de chaque classe.\n\nà compléter"},{"metadata":{},"cell_type":"markdown","source":"## Méthodologie"},{"metadata":{},"cell_type":"markdown","source":"Le but utlime de ce projet est de prédire le niveau de pauvereté de chaque foyer.\nCependant avant de tester différents modèles, il est important de comprendre la problématique et les données.\n\nLa méthodologie suivie pour modèliser ce niveau de pauvereté est la suivante :\n    \n    - Compréhension du problème\n    - Analyse exploratoire des données\n    - Préparation des données / variables pour le modèle (Feature engineering)\n    - Modèlisation\n    - Choix du modèle\n    - Conclusion"},{"metadata":{},"cell_type":"markdown","source":"# Chargement des modules / fonctions et Import des données"},{"metadata":{},"cell_type":"markdown","source":"## Chargement des modules"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data manipulation\nimport pandas as pd\nimport numpy as np\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Affichage Image\nfrom IPython.display import Image\nfrom IPython.display import display\n\n# MàJ de certains paramétres (Affichage et manipulation des données)\n%matplotlib inline\nplt.style.use('fivethirtyeight')\nplt.rcParams['font.size'] = 18\nplt.rcParams['patch.edgecolor'] = 'k'\n\n\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\n\n\n#plt.style.use('seaborn-whitegrid')\nsns.set(style = 'darkgrid')\n\n# Outils de modélisation\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score, make_scorer\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.pipeline import Pipeline\n\n\nfrom sklearn.svm import LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.linear_model import LogisticRegressionCV, RidgeClassifierCV\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.model_selection import StratifiedKFold\nimport lightgbm as lgb\n\n\n# Macro F1 score\nscorer = make_scorer(f1_score, greater_is_better=True, average = 'macro')\n\n\n# Fonctions de réequilibrage des données\nfrom imblearn.over_sampling import SMOTE","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Import des données"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Le path vers les fichiers de données en local\n#filepath ='/Users/moi/Documents/CoursDauphine/Module3/supervised/costa-rican-household-poverty-prediction'\n#filepath_train = '/Users/moi/Documents/CoursDauphine/Module3/supervised/costa-rican-household-poverty-prediction/train.csv'\n#filepath_test = '/Users/moi/Documents/CoursDauphine/Module3/supervised/costa-rican-household-poverty-prediction/test.csv'\n\n#filepath_submission = '/Users/moi/Documents/CoursDauphine/Module3/supervised/costa-rican-household-poverty-prediction/sample_submission.csv' ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#df_train = pd.read_csv(filepath_train, sep=',', decimal='.')\n#df_test = pd.read_csv(filepath_test, sep=',', decimal='.')\n#df_submit = pd.read_csv(filepath_submission, sep=',', decimal='.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('../input/train.csv', sep=',', decimal='.')\ndf_test = pd.read_csv('../input/test.csv', sep=',', decimal='.')\ndf_submit = pd.read_csv('../input/sample_submission.csv', sep=',', decimal='.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_submit.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Analyse exploratoire des données"},{"metadata":{},"cell_type":"markdown","source":"## Analyse des variables explicatives"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" df_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Les deux fichiers de données contiennent plusieurs types de variables :\n    - Booléen (catégorie)\n    - Chaîne de caractères (identifiants)\n    - décimal (v2a1 : Montant du loyer )\n\nLa commande df_train.info() permet d'afficher une vue globale des types de données du dataframe : "},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Le jeu de données d'apprentissage contient :\n- 130 variables de types integer. Ces variables représentent des variables de type \"Category\" ou \"Ordinal\"\n- 8 variables de types float. Ces variables correspondent à des montants et/ou à des taux.\n- 5 variables de type object. Une partie de ces variables représentent des identifiants qui ne seraient pas utiles pour la partie modèlisation. Cependant, l'autre partie doit être analysée et étudiées. \n\nEtudions la distribution de chaque type de variable.\n\nLe jeux de données de test contient une variable en moins de type integer. Cette dernière correspond à la variable à prédire (Target)."},{"metadata":{},"cell_type":"markdown","source":"*Etude des différents types de variables du jeux de données d'apprentissage*"},{"metadata":{},"cell_type":"markdown","source":"**Variables de type \"Integer\"**"},{"metadata":{},"cell_type":"markdown","source":"Etudions la distribution des variables de type integer :"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.select_dtypes(include=['int64']).nunique().value_counts().sort_index().plot.bar(color = 'blue', figsize = (8,6), edgecolor ='k', linewidth = 2)\n\nplt.xlabel('Nombre de valeurs uniques prise par la variables'); plt.ylabel('Nombre de colonnes');\nplt.title('Nombre de colonnes de type Integer avec ces valeurs uniques');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Les variables avec deux valeurs uniques représentent des booléens (0 ou 1). La plupart des variables sont de type booléen."},{"metadata":{},"cell_type":"markdown","source":"**Variables de type \"Float\"**"},{"metadata":{},"cell_type":"markdown","source":"Etudions la distribution des variables de type \"float\" en fonction du niveau de pauvereté."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"from collections import OrderedDict\n\nplt.figure(figsize = (20, 16))\nplt.style.use('fivethirtyeight')\n\n# Color mapping\ncolors = OrderedDict({1: 'red', 2: 'orange', 3: 'blue', 4: 'green'})\npoverty_mapping = OrderedDict({1: 'extreme', 2: 'moderate', 3: 'vulnerable', 4: 'non vulnerable'})\n\nfor i,col in enumerate(df_train.select_dtypes(include=['float64'])):\n\n    ax = plt.subplot(6, 2, i + 1)\n    # Iterate through the poverty levels\n    for poverty_level, color in colors.items():\n        # Plot each poverty level as a separate line\n        sns.kdeplot(df_train.loc[df_train['Target'] == poverty_level, col].dropna(), \n                    ax = ax, color = color, label = poverty_mapping[poverty_level])\n        \n    plt.title(f'{col.capitalize()} Distribution'); plt.xlabel(f'{col}'); plt.ylabel('Density')\n\n\nplt.subplots_adjust(top = 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A priori les variables Niveau moyen d'éducation(\"meaneduc\") et niveau de pauvereté de l'individu(\"Target\") sont liées.\nEn effet, plus le niveau moyen d'éducation des adultes dans le foyer est elevé plus le niveau de pauvereté est sévère (donc le degré de pauvereté est moins sévère).\n\nLe théme de l'éducation est potentiellement un des axes influent dans la calcul du niveau de pauvereté d'un foyer. "},{"metadata":{},"cell_type":"markdown","source":"**Variables de type \"object\"**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.select_dtypes(include = ['object']).head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Les deux attributs Id et idhogar permettent d'identifier respectivement un individu et un foyer.\nCes deux attributs ne seront pas utilisés dans les modèles. Ils resteront de type \"object\".\n\nLes 3 attributs dependency, edjefe et edjefa contiennent à la fois des valeurs numériques et de deux autres valeurs de type caractères \"yes\" et \"no\".\n\nD'après la définition des variables, le \"yes\" et le \"no\" correspondent respectivement à 1 et 0.\n\nNous allons donc remplacer ces deux valeurs des les deux jeux de données df_train et df_test.\n\nNous allons donc commencer par concaténer les deux jeux de données"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merge des deux dataframe train et test et ajout de la colonne Target aux données de test\ndf_test['Target'] = np.nan\ndata = df_train.append(df_test, ignore_index = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Target'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"map_dict = {'yes':1,'no':0}\n\ndata[\"dependency\"] = data[\"dependency\"].replace(map_dict).astype('float64')\ndata[\"edjefe\"] = data[\"edjefe\"].replace(map_dict).astype('float64')\ndata[\"edjefa\"] = data[\"edjefa\"].replace(map_dict).astype('float64')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"dependency\"] = data[\"dependency\"].astype('float64')\ndata[\"edjefe\"] = data[\"edjefe\"].astype('float64')\ndata[\"edjefa\"] = data[\"edjefa\"].astype('float64')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Etude des incohérences dans le jeu de données"},{"metadata":{},"cell_type":"markdown","source":"Certains individus du même foyer ont des labels (Target) différents. Il s'agit d'une anomalie dans le jeu de données.\nLe label à considérer est celui du l'individu tête de foyer."},{"metadata":{"trusted":true},"cell_type":"code","source":"data[data[\"Target\"].notnull()].groupby(\"idhogar\")[\"Target\"].nunique().value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Il y a 85 individus avec un label différent de celui de l'individu tête de foyer."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Liste des individus concernés\ndata_multi_target = data[data[\"Target\"].notnull()].groupby(\"idhogar\")[\"Target\"].apply(lambda x : x.nunique()==1)\n\ndata_multi_target = data_multi_target[data_multi_target != True]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_multi_target.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_multi_target.index[1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ci-dessous un exemple de foyer multi label :"},{"metadata":{"trusted":true},"cell_type":"code","source":"data[data[\"idhogar\"] == data_multi_target.index[0]][['idhogar', 'parentesco1', 'Target']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for id in data_multi_target.index:\n    True_label = data[(data['idhogar'] == id) & ( data['parentesco1'] == 1)][\"Target\"] \n    data.loc[data['idhogar'] == id, \"Target\"]  = np.where(True_label.notnull(), True_label, data.loc[data['idhogar'] == id, \"Target\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Target'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Etude des données manquantes"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of missing in each column\nmissing = pd.DataFrame(data.isnull().sum()).rename(columns = {0: 'total'})\n\n# Create a percentage missing\nmissing['percent'] = missing['total'] / len(data)\n\nmissing.sort_values('percent', ascending = False).head(10).drop('Target')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Les colonnes avec des données manquantes :\n- v2a1    : loyer mensuel\n- v18q1   : Nbre de tablette ds la maison\n- rez_esc :  Years behind in school\n- meaneduc :  Average years of education for adults\n- SQBmeaned : square of the mean years of education of adults (>=18) in the household"},{"metadata":{},"cell_type":"markdown","source":"*Variable v18q1*"},{"metadata":{},"cell_type":"markdown","source":"Cette variable est liée à la variable de type individu v18q. Cette dernière indique si l'individu dispose d'une tablette ou pas.\nv18q ne contient de valeurs manquantes. Nous allons donc déduire la variable au niveau foyer v18q1 à partir de la variable de type individu."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution des nombres de tablettes dans le foyer\ndata[data['parentesco1'] == 1]['v18q1'].value_counts().sort_index().plot.bar( figsize = (8,6),edgecolor ='k', linewidth = 2)\n# Formatting","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[data['parentesco1'] == 1]['v18q1'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Répartition de la variable de type individu par rapport à la variable de type foyer\ndata[data['parentesco1'] == 1].groupby('v18q1')['v18q'].sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On en déduit des deux répartitions ci-dessus que les valeurs manquantes correspondent à 0"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['v18q1'] = data['v18q1'].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Variable v2a1*"},{"metadata":{},"cell_type":"markdown","source":"Cette variable correspond au montant du loyer.\nElle est non renseignée pour 24263 observations.\n\nLes variables ci-dessous permettent de distinguer les locataires, des propriétaires.\nIl serait donc intéresser d'étudier la distribution de ces variables.\n\nSi la variable \"tipovivi1\" est égale à 1, la famille est propriétaire de l'appartement et n'a plus de crédit dessus. Le loyer est égale donc à 0\n\n- tipovivi1 =1 own and fully paid house\n- tipovivi2 =1 own,  paying in installments\"\n- tipovivi3 =1 rented\n- tipovivi4 =1 precarious\n- tipovivi5 =1 other(assigned,  borrowed)\""},{"metadata":{"trusted":true},"cell_type":"code","source":"tipovivi = data.columns.str.startswith('tipovivi')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col = [\"v2a1\",\"tipovivi1\",\"tipovivi2\",\"tipovivi3\",\"tipovivi4\",\"tipovivi5\"]\ndata.loc[(data[\"v2a1\"].isnull() ) & (data[\"parentesco1\"] ==1),col].sum().plot.bar()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"La majorité des reponsable de foyer n'ayant de loyer renseigné sont propriétaire de leurs maisons.\n\nPour les autres situations, nous ne disposons pas d'assez de données pour en déduire la cause.\n\nPour les maisons dont le responsable du foyer est propriétaire, le montant du loyer sera alimenté avec 0 pour les autres nous allons les topées \"Données manquantes\"."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.loc[(data['tipovivi1'] == 1 & data['v2a1'].isnull() ), 'v2a1'] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pour les autres valeurs manquantes et vu le manque d'informations, on propose d'imputer par la médiane (Voir partie modèle)\n\ndata.loc[data['v2a1'].isnull(), 'v2a1'] = 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Variable rez_esc*\n\nCette variable correspond au nombre d'années en dehors de l'école.\nCette varaible n'a de sens que pour les individus âgés de 7 à 19 ans.\nSi l'âge de l'individu est en dehors de cet intervalle, cette variable vaut 0 (Voir descritpion de la variable + compléments d'informations dans le forum)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# On vérifie s'il n y a pas des incohérences entre l'âge et cette variable.\ndata[data['rez_esc'].notnull()]['age'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"L'intervalle d'âge pour les données sourcées est cohérent (entre 7 et 19 ans)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Imputation des observation ayant un âge en dehors de l'intervalle 7 à 19 ans\ndata.loc[((data['age'] > 19) | (data['age'] < 7)) & (data['rez_esc'].isnull()) ,'rez_esc'] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# La valeur ma maximale prise par cette variable est 5. Or certaines obsevations ont des valeurs supérieurs.\n\ndata['rez_esc'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# On va donc remplacer toute valeur supérieur à 5 par le max\n\ndata.loc[data['rez_esc'] > 5, 'rez_esc']  = 5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pour les autres valeurs observations et vu le manque d'informations, on propose d'imputer par la médiane (Voir partie modèle)\ndata.loc[data['rez_esc'].isnull(),'rez_esc' ] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Variable meaneduc*"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Au vu du nombre d'observations concernées, on impute avec 0\ndata.loc[data['meaneduc'].isnull(),'meaneduc' ] = 0\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Variable SQBmeaned*"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pour être cohérent avec l'imputation ci-dessus, on impute avec 0\ndata.loc[data['SQBmeaned'].isnull(), 'SQBmeaned'] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Etude des interractions entre les variables"},{"metadata":{},"cell_type":"markdown","source":"Le dataframe head ci-dessous contient uniquement les individus responsable de chaque foyer."},{"metadata":{"trusted":true},"cell_type":"code","source":"head = df_train.loc[(data[\"parentesco1\"] ==1) & (data[\"Target\"].notnull())].copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Distribution de la variable Target*"},{"metadata":{"trusted":true},"cell_type":"code","source":"distrib_target = head['Target'].value_counts().reset_index().rename(columns = {'index' : 'level' })","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"distrib_target['Household_type'] = distrib_target['level'].map(poverty_mapping)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"distrib_target.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(style = 'whitegrid', font_scale=1.4)\nfig = plt.subplots(figsize=(15, 8))\nax = sns.barplot(x = 'Household_type', y = 'Target', data = distrib_target, palette='Accent', ci = None).set_title('Distribution de la pauvereté')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Le jeux de données n'est pas équilibré. Les modèles auront du mal à prédire la classe minoritaire(\"Extreme\").\nNous allons par la suite étudier des méthodes pour ré-équilibrer ce jeu de données."},{"metadata":{},"cell_type":"markdown","source":"## Future engineering"},{"metadata":{},"cell_type":"markdown","source":"L'objectif de ce projet est de prévoir le niveau de pauvereté au niveau de chaque foyer.\nIl convient, dans le cadre de cette section, d'agréger les données au niveau individu et de créer de nouvelles variables par foyer.\n\nNous allons commencer par distinguer deux types de variables :\n - Variables de niveau individu\n - Varibales de niveau foyer\n\nLes variables de type individu seront agrégées et retraitées pour en extraire de nouvelles variables de type foyer."},{"metadata":{},"cell_type":"markdown","source":"*Identifiants*"},{"metadata":{"trusted":true},"cell_type":"code","source":"ids = ['Id', 'idhogar']\ntarget = ['Target']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Variables de niveau individu*"},{"metadata":{"trusted":true},"cell_type":"code","source":"ind_bool = ['v18q', 'dis', 'male', 'female', 'estadocivil1', 'estadocivil2', 'estadocivil3', \n            'estadocivil4', 'estadocivil5', 'estadocivil6', 'estadocivil7', \n            'parentesco1', 'parentesco2',  'parentesco3', 'parentesco4', 'parentesco5', \n            'parentesco6', 'parentesco7', 'parentesco8',  'parentesco9', 'parentesco10', \n            'parentesco11', 'parentesco12', 'instlevel1', 'instlevel2', 'instlevel3', \n            'instlevel4', 'instlevel5', 'instlevel6', 'instlevel7', 'instlevel8', \n            'instlevel9', 'mobilephone']\n\nind_ordered = ['rez_esc', 'escolari', 'age']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Variables de niveau foyer*"},{"metadata":{"trusted":true},"cell_type":"code","source":"hh_bool = ['hacdor', 'hacapo', 'v14a', 'refrig', 'paredblolad', 'paredzocalo', \n           'paredpreb','pisocemento', 'pareddes', 'paredmad',\n           'paredzinc', 'paredfibras', 'paredother', 'pisomoscer', 'pisoother', \n           'pisonatur', 'pisonotiene', 'pisomadera',\n           'techozinc', 'techoentrepiso', 'techocane', 'techootro', 'cielorazo', \n           'abastaguadentro', 'abastaguafuera', 'abastaguano',\n            'public', 'planpri', 'noelec', 'coopele', 'sanitario1', \n           'sanitario2', 'sanitario3', 'sanitario5',   'sanitario6',\n           'energcocinar1', 'energcocinar2', 'energcocinar3', 'energcocinar4', \n           'elimbasu1', 'elimbasu2', 'elimbasu3', 'elimbasu4', \n           'elimbasu5', 'elimbasu6', 'epared1', 'epared2', 'epared3',\n           'etecho1', 'etecho2', 'etecho3', 'eviv1', 'eviv2', 'eviv3', \n           'tipovivi1', 'tipovivi2', 'tipovivi3', 'tipovivi4', 'tipovivi5', \n           'computer', 'television', 'lugar1', 'lugar2', 'lugar3',\n           'lugar4', 'lugar5', 'lugar6', 'area1', 'area2']\n\nhh_ordered = [ 'rooms', 'r4h1', 'r4h2', 'r4h3', 'r4m1','r4m2','r4m3', 'r4t1',  'r4t2', \n              'r4t3', 'v18q1', 'tamhog','tamviv','hhsize','hogar_nin',\n              'hogar_adul','hogar_mayor','hogar_total',  'bedrooms', 'qmobilephone']\n\nhh_cont = ['v2a1', 'dependency', 'edjefe', 'edjefa', 'meaneduc', 'overcrowding']\n           \n           ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Variables d'ordre 2 (squared)*"},{"metadata":{"trusted":true},"cell_type":"code","source":"sqr_ = ['SQBescolari', 'SQBage', 'SQBhogar_total', 'SQBedjefe', \n        'SQBhogar_nin', 'SQBovercrowding', 'SQBdependency', 'SQBmeaned', 'agesq']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Analyse de niveau foyer"},{"metadata":{"trusted":true},"cell_type":"code","source":"head = data.loc[(data[\"parentesco1\"] ==1), ids + hh_bool + hh_ordered + hh_cont + target].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"head.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"head.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Corrélation entre les variables*"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Matrice de corrélation\ncorr_matrix = head.corr()\n\n# Select upper triangle of correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n\n# Find index of feature columns with correlation greater than 0.95\nto_drop = [column for column in upper.columns if any(abs(upper[column]) > 0.95)]\n\nto_drop","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plusieurs variables liées à la taille du foyer et à l'âge sont très largement corrélées avec d'autres variables du jeu de données."},{"metadata":{},"cell_type":"markdown","source":"Variable tamhog"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(corr_matrix.loc[corr_matrix['tamhog'].abs() > 0.95, corr_matrix['tamhog'].abs() > 0.95],\n            annot=True, cmap = plt.cm.autumn_r, fmt='.3f');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"La taille du foyer est représentée dans le jeu de données par plusieurs variables : tamhog, hhsize, hogar_total.\nNous n'allons garder qu'une seule variables parmis les variables ci-dessous."},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nr4t3, tamhog, hhsize et hogar_total sont des variables représentant le nombre de personnes dans le foyer.\nNous allons en garder seulement une variable : hhsize.\n\"\"\"\nhead.drop(labels = ['tamhog', 'hogar_total', 'r4t3'], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Variable coopele"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(corr_matrix.loc[corr_matrix['coopele'].abs() > 0.95, corr_matrix['coopele'].abs() > 0.95],\n            annot=True, cmap = plt.cm.autumn_r, fmt='.3f');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cette variable est très corrélée avec la variable \"public\" représentant un approvisionnement public en électricité. Nous n'allons pas supprimer dernière. Il servira par la suite à créer une nouvelle variable."},{"metadata":{},"cell_type":"markdown","source":"Variable area2"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(corr_matrix.loc[corr_matrix['area2'].abs() > 0.95, corr_matrix['area2'].abs() > 0.95],\n            annot=True, cmap = plt.cm.autumn_r, fmt='.3f');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# La variable area1 est très corrélées avec la variable area2\nhead.drop(labels = 'area2', axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nous allons créer des variables ordinales (avec ordre) de niveau foyer à partir des variables de :\n- Etat de l'état des murs, du toit et du sol\n- Disponibilité des toilettes dans le foyer, à l'extérieur et/ou sans toilette\n- Type d'approvisionnement en électricité\n- Statut du foyer : Propriétaire ...etc"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Etat des murs\nhead['epared_argmax'] = np.argmax(np.array(head[['epared1', 'epared2', 'epared3']]),\n                           axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Etat du toit\nhead['etecho_argmax'] = np.argmax(np.array(head[['etecho1', 'etecho2', 'etecho3']]),\n                           axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Etat du sol\nhead['eviv_argmax'] = np.argmax(np.array(head[['eviv1', 'eviv2', 'eviv3']]),\n                           axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Disponibilité de l'eau inside/outside/no water\nhead['abastagua_argmax'] = np.argmax(np.array(head[[ 'abastaguano',  'abastaguafuera','abastaguadentro']]),\n                           axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Connexion à un réseau élctriqueno elec/coopérative/public/privé\nhead['elec_argmax'] = np.argmax(np.array(head[['noelec','coopele', 'public', 'planpri']]),\n                           axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Propriétaire maison\nhead['tipovivi_argmax'] = np.argmax(np.array(head[['tipovivi5','tipovivi4', 'tipovivi3', 'tipovivi2','tipovivi1']]),\n                           axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"La variable suivante permet de classer les foyers par ordre croissant selon s'ils disposent ou pas d'un réfrigirateur, un ordinateur et une télévision."},{"metadata":{"trusted":true},"cell_type":"code","source":"head['electro_house'] = head['refrig'] + head['computer'] + head['television'] ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"La variable suivante représente la différence entre le nombre de personnes du foyer et le nombre de personnes vivantes dans ce foyer"},{"metadata":{"trusted":true},"cell_type":"code","source":"head['diff_tamviv_hhsize'] = head['tamviv'] - head['hhsize']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Suppression des variables utilisées ci-dessous pour la création de nouvelles variables*"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Variables utilisées dans la création de nouvelles variables\nhead.drop(labels = [ 'epared1', 'epared2', 'epared3','etecho1', 'etecho2', 'etecho3',\n                          'eviv1', 'eviv2', 'eviv3',\n                          'abastaguano',  'abastaguafuera','abastaguadentro',\n                          'noelec','coopele', 'public', 'planpri',\n                          'refrig', 'computer','television',\n                          'tipovivi5','tipovivi4', 'tipovivi3', 'tipovivi2','tipovivi1',\n                          'tamviv']\n                 , axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"head.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Analyse de niveau indivdidu"},{"metadata":{"trusted":true},"cell_type":"code","source":"# On ajoute les deux variables tamviv et hhsize pour calculer des moyennes par foyer\nindiv = data[ids + ind_bool+ ind_ordered + target +['tamviv','hhsize']].copy()\nindiv.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"L'objectif de cette analyse est de créer, à partir des données individuelles, de nouvelles variables de niveau foyer (Agrégation par idhogar)."},{"metadata":{},"cell_type":"markdown","source":"*Corrélation entre les variables*"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Matrice de corrélation\ncorr_matrix = indiv.corr()\n\n# Select upper triangle of correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n\n# Find index of feature columns with correlation greater than 0.95\nto_drop = [column for column in upper.columns if any(abs(upper[column]) > 0.95)]\n\nto_drop","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Variable female"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(corr_matrix.loc[corr_matrix['female'].abs() > 0.95, corr_matrix['female'].abs() > 0.95],\n            annot=True, cmap = plt.cm.autumn_r, fmt='.3f');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sans surprise la variable female est entièrement corrélée à la variable male."},{"metadata":{"trusted":true},"cell_type":"code","source":"# La variable female est très corrélée avec male\nindiv.drop(labels = 'male', axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"la variable suivante \"instlevel_argmax\" permet d'avoir le niveau max d'éducation de chaque individu."},{"metadata":{"trusted":true},"cell_type":"code","source":"indiv['instlevel_argmax'] = np.argmax(np.array(indiv[['instlevel1', 'instlevel2','instlevel3','instlevel4','instlevel5','instlevel6','instlevel7','instlevel8','instlevel9']]),\n                           axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"indiv['indiv_digital'] = indiv['mobilephone'] + indiv['v18q']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"indiv.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Variables utilisées dans la création de nouvelles variables\nindiv.drop(labels = ['instlevel1', 'instlevel2','instlevel3','instlevel4','instlevel5',\n                           'instlevel6','instlevel7','instlevel8','instlevel9',\n                    'mobilephone', 'v18q'\n                    ]\n                 , axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nous allons créer des variables ordinales (avec ordre) de niveau individu à partir des variables de :\n- Niveau d'éducation\n- .."},{"metadata":{"trusted":true},"cell_type":"code","source":"indiv_pivot = pd.pivot_table(indiv,index=[\"idhogar\"],\n                            values=[\n                                'age',\n                                'rez_esc', \n                                'escolari',\n                                'dis',\n                                'female',\n                                'instlevel_argmax',\n                                'indiv_digital',\n                                'tamviv',\n                                'hhsize' \n                                   ],\n                            aggfunc={\n                             'age':np.sum,\n                             'rez_esc':np.sum,\n                             'escolari':np.sum, \n                             'dis' : np.sum,\n                             'female' : np.sum,\n                             'instlevel_argmax':[np.max,np.min,np.mean] , \n                             'indiv_digital':[np.max,np.min,np.mean] , \n                             'tamviv':[np.max,np.min] , \n                             'hhsize':[np.max,np.min]\n                             \n                },\n                fill_value=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"indiv_toframe = pd.DataFrame(indiv_pivot.to_records())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"indiv_toframe.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Renommage des colonnes\nindiv_toframe.columns = [hdr.replace(\"('\", \"\").replace(\"', '\", \"_\").replace(\"')\", \"\") \\\n                     for hdr in indiv_toframe.columns]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Création de variables de niveau foyer*"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Age moyen par foyer\nindiv_toframe['mean_age_hhsize'] = indiv_toframe['age_sum'] / indiv_toframe['hhsize_amax']\n# Age moyen des personnes vivant ds la foyer\nindiv_toframe['mean_age_tamviv'] = indiv_toframe['age_sum'] / indiv_toframe['tamviv_amax']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Nbre d'années d'éducation moyen par foyer\nindiv_toframe['mean_escolari_hhsize'] = indiv_toframe['escolari_sum'] / indiv_toframe['hhsize_amax']\n# Nbre d'années d'éducation moyen des personnes vivant ds la foyer\nindiv_toframe['mean_escolari_tamviv'] = indiv_toframe['escolari_sum'] / indiv_toframe['tamviv_amax']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Nbre d'années d'éducation moyen par foyer\nindiv_toframe['mean_rez_esc_hhsize'] = indiv_toframe['rez_esc_sum'] / indiv_toframe['hhsize_amax']\n# Nbre d'années d'éducation moyen des personnes vivant ds la foyer\nindiv_toframe['mean_rez_esc_tamviv'] = indiv_toframe['rez_esc_sum'] / indiv_toframe['tamviv_amax']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Nbre moyen de femmes par foyer\nindiv_toframe['mean_female_hhsize'] = indiv_toframe['female_sum'] / indiv_toframe['hhsize_amax']\n# NNbre moyen de femmes vivant ds la foyer\nindiv_toframe['mean_female_tamviv'] = indiv_toframe['female_sum'] / indiv_toframe['tamviv_amax']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Nbre moyen de personnes handicapée par foyer\nindiv_toframe['mean_dis_hhsize'] = indiv_toframe['dis_sum'] / indiv_toframe['hhsize_amax']\n# Nbre moyen de personnes handicapéevivant ds la foyer\nindiv_toframe['mean_dis_tamviv'] = indiv_toframe['dis_sum'] / indiv_toframe['tamviv_amax']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"indiv_toframe.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Merge des deux dataframe head et indiv*"},{"metadata":{"trusted":true},"cell_type":"code","source":"head_agg = head.merge(indiv_toframe, on = 'idhogar',\n                             how = 'left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"head_agg.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"head_agg[head_agg['Target'].notnull()].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"head_agg[head_agg['Target'].isnull()].shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modèles de prédiction"},{"metadata":{},"cell_type":"markdown","source":"# Préparation des données d'apprentissage et de test"},{"metadata":{},"cell_type":"markdown","source":"Le dataframe \"head\" est désormais prêt à être utilisé dans les modèles de prédiction.\n\nLe jeu de données est déséquilibré, la classe 4 contient beaucoup plus d'observations que les autres classes.\n\nNous allons utiliser la méthode SMOTE pour rééquilibrer ce jeu de données.\n\nNous allons comparer les performances des deux modèles Random Forest et le gradient boosting sur les données brutes et les données après réequilibrage."},{"metadata":{},"cell_type":"markdown","source":"*Split des données brutes*"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Labels for training\ntrain_labels = np.array(list(head_agg[head_agg['Target'].notnull()]['Target'].astype(np.uint8)))\n\n# Extract the training data\ntrain_set = head_agg[head_agg['Target'].notnull()].drop(labels = ['Id', 'idhogar', 'Target'], axis = 1)\ntest_set = head_agg[head_agg['Target'].isnull()].drop(labels = ['Id', 'idhogar', 'Target'], axis = 1)\n\n# Submission base which is used for making submissions to the competition\nsubmission_base = df_test[['Id', 'idhogar']].copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Réequilibrage des données*"},{"metadata":{"trusted":true},"cell_type":"code","source":"sm = SMOTE(random_state = 33)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set_balanced, train_labels_balanced = sm.fit_sample(train_set, train_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set.shape\ntrain_set_balanced.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels_balanced.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_set.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_base.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"L'objet de l'étape suivante est d'imputer avec la mediane des valeurs manquantes puis de normaliser les variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"features = list(train_set.columns)\n# Imputation avec la médiane\n\npipeline = Pipeline([('imputer', Imputer(strategy = 'median')), \n                      ('scaler', MinMaxScaler())])\n\n# Fit and transform training data\ntrain_set = pipeline.fit_transform(train_set)\ntest_set = pipeline.transform(test_set)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit and transform training balanced data\ntrain_set_balanced = pipeline.fit_transform(train_set_balanced)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modèle de base : Random Forest"},{"metadata":{},"cell_type":"markdown","source":"Modèle de Random forest avec 100 arbres et 10 k-fold en validation croisée"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_no_smote = RandomForestClassifier(n_estimators=100, random_state=10, \n                               n_jobs = -1)\n# 10 fold cross validation\ncv_score_no_smote = cross_val_score(model_no_smote, train_set, train_labels, cv = 10, scoring = scorer)\n\nprint(f'10 Fold Cross Validation F1 Score = {round(cv_score_no_smote.mean(), 4)} with std = {round(cv_score_no_smote.std(), 4)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_smote = RandomForestClassifier(n_estimators=100, random_state=10, \n                               n_jobs = -1)\n# 10 fold cross validation\ncv_score_smote = cross_val_score(model_smote, train_set_balanced, train_labels_balanced, cv = 10, scoring = scorer)\n\nprint(f'10 Fold Cross Validation F1 Score = {round(cv_score_smote.mean(), 4)} with std = {round(cv_score_smote.std(), 4)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Après rééquilibrage des données, on passe d'un score F1 Macro de 0.3368 à 0.8824.\nL'écart type à doublé mais il reste très acceptable."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Variables les plus influentes (feature importance)*"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_no_smote.fit(train_set, train_labels)\n\n# Varibales importantes sur le jeu de données non rééquilibré\nfeature_importances_no_smote = pd.DataFrame({'feature': features, 'importance': model_no_smote.feature_importances_})\nfeature_importances_no_smote.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_smote.fit(train_set_balanced, train_labels_balanced)\n\n# Varibales importantes sur le jeu de données rééquilibré\nfeature_importances_smote = pd.DataFrame({'feature': features, 'importance': model_smote.feature_importances_})\nfeature_importances_smote.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_feature_importances(df, n = 10, threshold = None):\n    \"\"\"Plots n most important features. Also plots the cumulative importance if\n    threshold is specified and prints the number of features needed to reach threshold cumulative importance.\n    Intended for use with any tree-based feature importances. \n    \n    Args:\n        df (dataframe): Dataframe of feature importances. Columns must be \"feature\" and \"importance\".\n    \n        n (int): Number of most important features to plot. Default is 15.\n    \n        threshold (float): Threshold for cumulative importance plot. If not provided, no plot is made. Default is None.\n        \n    Returns:\n        df (dataframe): Dataframe ordered by feature importances with a normalized column (sums to 1) \n                        and a cumulative importance column\n    \n    Note:\n    \n        * Normalization in this case means sums to 1. \n        * Cumulative importance is calculated by summing features from most to least important\n        * A threshold of 0.9 will show the most important features needed to reach 90% of cumulative importance\n    \n    \"\"\"\n    plt.style.use('fivethirtyeight')\n    \n    # Sort features with most important at the head\n    df = df.sort_values('importance', ascending = False).reset_index(drop = True)\n    \n    # Normalize the feature importances to add up to one and calculate cumulative importance\n    df['importance_normalized'] = df['importance'] / df['importance'].sum()\n    df['cumulative_importance'] = np.cumsum(df['importance_normalized'])\n    \n    plt.rcParams['font.size'] = 12\n    \n    # Bar plot of n most important features\n    df.loc[:n, :].plot.barh(y = 'importance_normalized', \n                            x = 'feature', color = 'darkgreen', \n                            edgecolor = 'k', figsize = (12, 8),\n                            legend = False, linewidth = 2)\n\n    plt.xlabel('Normalized Importance', size = 18); plt.ylabel(''); \n    plt.title(f'{n} Most Important Features', size = 18)\n    plt.gca().invert_yaxis()\n    \n    \n    if threshold:\n        # Cumulative importance plot\n        plt.figure(figsize = (8, 6))\n        plt.plot(list(range(len(df))), df['cumulative_importance'], 'b-')\n        plt.xlabel('Number of Features', size = 16); plt.ylabel('Cumulative Importance', size = 16); \n        plt.title('Cumulative Feature Importance', size = 18);\n        \n        # Number of features needed for threshold cumulative importance\n        # This is the index (will need to add 1 for the actual number)\n        importance_index = np.min(np.where(df['cumulative_importance'] > threshold))\n        \n        # Add vertical line to plot\n        plt.vlines(importance_index + 1, ymin = 0, ymax = 1.05, linestyles = '--', colors = 'red')\n        plt.show();\n        \n        print('{} features required for {:.0f}% of cumulative importance.'.format(importance_index + 1, \n                                                                                  100 * threshold))\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"norm_fi_no_smote  = plot_feature_importances(feature_importances_no_smote, threshold=0.95)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"norm_fi_smote = plot_feature_importances(feature_importances_smote, threshold=0.95)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Les variables les plus importantes et comme on peut s'y attendre sont liées à l'éducation.\n"},{"metadata":{},"cell_type":"markdown","source":"## Modèle : Light Gradient Boosting"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_ids = list(head.loc[head['Target'].isnull(), 'idhogar'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def macro_f1_score(labels, predictions):\n    # Reshape the predictions as needed\n    predictions = predictions.reshape(len(np.unique(labels)), -1 ).argmax(axis = 0)\n    \n    metric_value = f1_score(labels, predictions, average = 'macro')\n    \n    # Return is name, value, is_higher_better\n    return 'macro_f1', metric_value, True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Paramétres pour un test\nparams = {'boosting_type': 'dart', \n                  'colsample_bytree': 0.88, \n                  'learning_rate': 0.028, \n                   'min_child_samples': 10, \n                   'num_leaves': 36, 'reg_alpha': 0.76, \n                   'reg_lambda': 0.43, \n                   'subsample_for_bin': 40000, \n                   'subsample': 0.54, \n                   'class_weight': 'balanced'}\n    \n# Modèles\nmodel = lgb.LGBMClassifier(**params, objective = 'multiclass', \n                               n_jobs = -1, n_estimators = 10000,\n                               random_state = 10)\n    \n# Using stratified kfold cross validation\nstrkfold = StratifiedKFold(n_splits = 5, shuffle = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"strkfold.get_n_splits(train_set_balanced, train_labels_balanced)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for train_index, test_index in strkfold.split(train_set, train_labels):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    X_train, X_test = train_set[train_index], train_set[test_index]\n    y_train, y_test = train_labels[train_index], train_labels[test_index]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for train_index, test_index in strkfold.split(train_set_balanced, train_labels_balanced):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    X_train_balanced, X_test_balanced = train_set_balanced[train_index], train_set_balanced[test_index]\n    y_train_balanced, y_test_balanced = train_labels_balanced[train_index], train_labels_balanced[test_index]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_lgb = model.fit(X_train, y_train, early_stopping_rounds =20, \n                  eval_metric = macro_f1_score,\n                  eval_set = [(X_train, y_train), (X_test, y_test)],\n                  eval_names = ['train', 'valid'],\n                  verbose = 200)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = model_lgb.predict(test_set)\npredictions = pd.DataFrame({'idhogar': test_ids,\n                            'Target': predictions})\n\n     # Make a submission dataframe\nsubmission = submission_base.merge(predictions, \n                                       on = 'idhogar',\n                                       how = 'left').drop(labels = ['idhogar'], axis = 1)\n    \n    # Fill in households missing a head\nlgb_submission = submission['Target'] = submission['Target'].fillna(4).astype(np.int8)\n\n\nlgb_submission.to_csv('lgb_submission.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_lgb_balanced = model.fit(X_train_balanced, y_train_balanced, early_stopping_rounds = 20, \n                  eval_metric = macro_f1_score,\n                  eval_set = [(X_train_balanced, y_train_balanced), (X_test_balanced, y_test_balanced)],\n                  eval_names = ['train_balanced', 'valid_balanced'],\n                  verbose = 200)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = model_lgb_balanced.predict(test_set)\npredictions = pd.DataFrame({'idhogar': test_ids,\n                            'Target': predictions})\n\n     # Make a submission dataframe\nsubmission = submission_base.merge(predictions, \n                                       on = 'idhogar',\n                                       how = 'left').drop(labels = ['idhogar'], axis = 1)\n    \n    # Fill in households missing a head\nlgb_balanced_submission = submission['Target'] = submission['Target'].fillna(4).astype(np.int8)\n\n\nlgb_balanced_submission.to_csv('lgb_submission.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_balanced_submission.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##  Prédiction"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_ids = list(head.loc[head['Target'].isnull(), 'idhogar'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(test_ids)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def submit(model, train, train_labels, test, test_ids):\n    \"\"\"Train and test a model on the dataset\"\"\"\n    \n    # Train on the data\n    model.fit(train, train_labels)\n    predictions = model.predict(test)\n    predictions = pd.DataFrame({'idhogar': test_ids,\n                               'Target': predictions})\n\n     # Make a submission dataframe\n    submission = submission_base.merge(predictions, \n                                       on = 'idhogar',\n                                       how = 'left').drop(labels = ['idhogar'], axis = 1)\n    \n    # Fill in households missing a head\n    submission['Target'] = submission['Target'].fillna(4).astype(np.int8)\n\n    return submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_submission = submit(RandomForestClassifier(n_estimators = 100, \n                                              random_state=10, n_jobs = -1), \n                         train_set, train_labels, test_set, test_ids)\n\nrf_submission.to_csv('rf_submission.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_submission.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_balanced_submission = submit(RandomForestClassifier(n_estimators = 100, \n                                              random_state=10, n_jobs = -1), \n                         train_set_balanced, train_labels_balanced, test_set, test_ids)\n\nrf_balanced_submission.to_csv('rf_balanced_submission.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_balanced_submission.shape","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{"height":"calc(100% - 180px)","left":"10px","top":"150px","width":"211px"},"toc_section_display":true,"toc_window_display":true}},"nbformat":4,"nbformat_minor":1}