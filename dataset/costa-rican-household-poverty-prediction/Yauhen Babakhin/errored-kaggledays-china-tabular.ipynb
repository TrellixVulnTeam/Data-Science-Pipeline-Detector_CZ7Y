{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import warnings\nwarnings.simplefilter(action=\"ignore\")\n\nimport os\nimport random\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn import preprocessing\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import StratifiedKFold\n\n# Seed Everything\nseed = 13\nrandom.seed(seed)\nos.environ[\"PYTHONHASHSEED\"] = str(seed)\nnp.random.seed(seed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Target encoder definition"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\n\n\nclass TargetEncoder:\n    def __init__(self, target, alpha=5):\n        self.target = target\n        self.alpha = alpha\n\n    def fit_transform(self, train, categorical):\n        self.train = train\n        self.categorical = categorical\n\n        # Create 5-fold cross-validation\n        kf = KFold(n_splits=5, random_state=1, shuffle=True)\n        train_feature = np.zeros(len(train))\n\n        # For each folds split\n        for train_index, test_index in kf.split(train):\n            cv_train, cv_test = train.iloc[train_index], train.iloc[test_index]\n\n            # Calculate out-of-fold statistics and apply to cv_test\n            cv_test_feature = self._test_mean_target_encoding(cv_train, cv_test)\n\n            # Save new feature for this particular fold\n            train_feature[test_index] = cv_test_feature\n        return train_feature\n\n    def transform(self, test):\n        \n        # Get test target-encoded feature\n        test_feature = self._test_mean_target_encoding(self.train, test)\n\n        return test_feature\n\n    def _test_mean_target_encoding(self, train, test):\n        # Calculate global mean on the train data\n        global_mean = train[self.target].mean()\n\n        # Group by the categorical feature and calculate its properties\n        train_groups = train.groupby(self.categorical)\n        category_sum = train_groups[self.target].sum()\n        category_size = train_groups.size()\n\n        # Calculate smoothed mean target statistics\n        train_statistics = (category_sum + global_mean * self.alpha) / (category_size + self.alpha)\n\n        # Apply statistics to the test data and fill new categories\n        test_feature = test[self.categorical].map(train_statistics).fillna(global_mean)\n        return test_feature.values\n\n\nfrom sklearn.metrics import f1_score\n\ndef evaluate_macroF1_lgb(truth, predictions):  \n    # Follows the discussion in https://github.com/Microsoft/LightGBM/issues/1483\n    pred_labels = predictions.reshape(len(np.unique(truth)),-1).argmax(axis=0)\n    f1 = f1_score(truth, pred_labels, average=\"macro\")\n    \n    return (\"macroF1\", f1, True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Read data"},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_data(train_path, test_path):\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n    \n    return train, test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Preprocess data"},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_data(train, test):\n    \n    # Concatenate train and test data together\n    data = pd.concat([train, test], sort=False)\n\n    # Drop duplicate and useless features\n    features_to_drop = [\"area2\", \"tamhog\", \"hhsize\", \"agesq\"]\n    features_to_drop += [x for x in data.columns if \"SQB\" in x]\n    data.drop(features_to_drop, axis=1, inplace=True)\n\n    # Transform original One-Hot-encoded features to a single column\n    for ohe in [\n        \"pared\",\n        \"piso\",\n        \"techo\",\n        \"abastagua\",\n        \"sanitario\",\n        \"energcocinar\",\n        \"elimbasu\",\n        \"epared\",\n        \"etecho\",\n        \"eviv\",\n        \"lugar\",\n        \"tipovivi\",\n        \"electricity\",\n    ]:\n        if ohe != \"electricity\":\n            ohe_cols = [x for x in train.columns if x.startswith(ohe)]\n        else:\n            ohe_cols = [\"public\", \"planpri\", \"noelec\", \"coopele\"]\n\n        data[ohe] = np.where(\n            data[ohe_cols].sum(axis=1) == 0, \"NEW_CAT\", data[ohe_cols].idxmax(axis=1)\n        )\n        data.drop(ohe_cols, axis=1, inplace=True)\n\n    # Fill in the missing data\n    data.fillna(-999, inplace=True)\n\n    train = data[: len(train)]\n    test = data[-len(test) :]\n\n    return train, test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Generate some features"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"def generate_features(train, test):\n    data = pd.concat([train, test], sort=False)\n\n    # Some feature engineering from: https://www.kaggle.com/gaxxxx/exploratory-data-analysis-lightgbm\n    data[\"adult\"] = data[\"hogar_adul\"] - data[\"hogar_mayor\"]\n    data[\"dependency_count\"] = data[\"hogar_nin\"] + data[\"hogar_mayor\"]\n    data[\"dependency\"] = np.where(data[\"adult\"] == 0, 1, data[\"dependency_count\"] / data[\"adult\"])\n    data[\"child_percent\"] = data[\"hogar_nin\"] / data[\"hogar_total\"]\n    data[\"elder_percent\"] = data[\"hogar_mayor\"] / data[\"hogar_total\"]\n    data[\"adult_percent\"] = data[\"hogar_adul\"] / data[\"hogar_total\"]\n\n    data[\"rent_per_bedroom\"] = data[\"v2a1\"] / data[\"bedrooms\"]\n    data[\"male_per_bedroom\"] = data[\"r4h3\"] / data[\"bedrooms\"]\n    data[\"female_per_bedroom\"] = data[\"r4m3\"] / data[\"bedrooms\"]\n    data[\"bedrooms_per_person_household\"] = data[\"hogar_total\"] / data[\"bedrooms\"]\n\n    data[\"escolari_age\"] = data[\"escolari\"] / data[\"age\"]\n\n    # Groupping features by a household (ID is idhogar)\n    aggr_mean_list = [\"rez_esc\", \"dis\", \"male\", \"female\"]\n    aggr_mean_list += [f\"estadocivil{x}\" for x in range(1, 8)]\n    aggr_mean_list += [f\"parentesco{x}\" for x in range(2, 13)]\n    aggr_mean_list += [f\"instlevel{x}\" for x in range(1, 10)]\n\n    other_list = [\"escolari\", \"age\", \"escolari_age\"]\n\n    for item in aggr_mean_list:\n        data[item + \"_mean\"] = data.groupby(\"idhogar\")[item].transform(\"mean\")\n\n    for item in other_list:\n        for function in [\"mean\", \"std\", \"min\", \"max\", \"sum\"]:\n            data[item + \"_\" + function] = (\n                data.groupby(\"idhogar\")[item].transform(function).fillna(0)\n            )\n\n    train = data[: len(train)]\n    test = data[-len(test) :]\n\n    return train, test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Machine Learning pipeline"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read the data\ntrain, test = read_data(\n    train_path=\"../input/costa-rican-household-poverty-prediction/train.csv\",\n    test_path=\"../input/costa-rican-household-poverty-prediction/test.csv\",\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preprocess the data\ntrain, test = preprocess_data(train, test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generate some features\ntrain, test = generate_features(train, test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Keep only the heads of the households\ntrain = train[train[\"parentesco1\"] == 1]\n\n# Transform target variables to the labels\ntarget_encoder = preprocessing.LabelEncoder()\ny = target_encoder.fit_transform(train[\"Target\"])\n\n# Drop all the ID variables\nX = train.drop([\"Id\", \"idhogar\", \"parentesco1\"], axis=1)\nX_test = test.drop([\"Id\", \"idhogar\", \"parentesco1\", \"Target\"], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n    \"learning_rate\": 0.1,\n    \"objective\": \"multiclass\",\n    \"metric\": \"multi_logloss\",\n    \"n_estimators\": 1000,\n    \"class_weight\": \"balanced\",\n    \"colsample_bytree\": 0.9,\n    \"subsample\": 0.8,\n    \"subsample_freq\": 1,\n    \"num_class\": 4,\n    \"lambda_l2\": 1,\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train a model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Stratified K-Fold\nnum_folds = 5\nskf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=1)\n\n# Initialize variables\ny_preds = np.zeros((len(X_test), 4))\nval_scores = []\n\nfor fold_n, (train_index, valid_index) in enumerate(skf.split(X, y)):\n    \n    # Get cross-validation split\n    X_train = X.iloc[train_index]\n    X_valid = X.iloc[valid_index]\n    \n    y_train = y[train_index]\n    y_valid = y[valid_index]\n    \n    # Transform all the categorical features into Target-Encoded\n    for f in X_train.columns:\n        if X_train[f].dtype == \"object\" and f not in [\"Id\", \"idhogar\", \"Target\"]:\n            te = TargetEncoder(target=\"Target\", alpha=5)\n\n            X_train[f] = te.fit_transform(X_train, f)\n            X_valid[f] = te.transform(X_valid)\n            X_test[f] = te.transform(X_test)\n            \n    X_train = X_train.drop([\"Target\"], axis=1)\n    X_valid = X_valid.drop([\"Target\"], axis=1)\n\n    # Train LightGBM model\n    clf = lgb.LGBMClassifier(**params)\n    clf = clf.fit(\n        X_train,\n        y_train,\n        eval_set=[(X_valid, y_valid)],\n        verbose=100,\n        early_stopping_rounds=200,\n    )\n\n    # Make validation predictions\n    y_pred_valid = clf.predict(X_valid)\n    \n    importances = pd.DataFrame({\"feature\": X_train.columns, \"importance\": clf.feature_importances_})\n\n    # Evaluate the validation score\n    score = f1_score(y_valid, y_pred_valid, average=\"macro\")\n    val_scores.append(score)\n    print(f\"Fold {fold_n}. F1 Score: {score:.5f}\\n\")\n    \n    # Make predictions on the test set (summing up the folds)\n    y_preds += clf.predict_proba(X_test) / num_folds\n\nprint(\"Overall F1 Score: {:.3f}\".format(np.mean(val_scores) + np.std(val_scores)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"importances","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsns.barplot(data=importances.sort_values(\"importance\", ascending=False).head(10), x=\"importance\", y=\"feature\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Make test predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_preds = np.argmax(y_preds, axis=1)\n\nsubmission = pd.DataFrame(\n    {\n        \"Id\": test[\"Id\"],\n        \"Target\": target_encoder.inverse_transform(y_preds).astype(int),\n    }\n)\nsubmission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.Target.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.Target.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}