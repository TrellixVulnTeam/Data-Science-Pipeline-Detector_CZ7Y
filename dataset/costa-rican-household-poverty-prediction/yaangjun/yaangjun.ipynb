{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\npd.set_option('display.Max_column', None)\npd.set_option('display.Max_row', None)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nplt.style.use('fivethirtyeight')\nplt.rcParams['font.size'] = 18\nplt.rcParams['patch.edgecolor'] = 'k'\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")\nprint(train.shape)\nprint(test.shape)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.select_dtypes(np.int64).nunique().value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Check"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.select_dtypes(np.int64).nunique().value_counts().sort_index().plot.bar(color='blue', figsize = (8,6), edgecolor='k', linewidth=2)\n# data type이 int 인거만 뽑아서 unique value의 갯수를 세서, unique값별로 column의 갯수를 세서, index로 정리해서 plot\nplt.xlabel('Number of Unique Values')\nplt.ylabel('Count')\nplt.title('Count of Unique Values in Integer Columns')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import OrderedDict\n\nplt.figure(figsize = (20,12))\nplt.style.use('fivethirtyeight')\n\n#Color mapping\n# OrderedDict : value뿐만아니라 순서까지 관리하는 Dict.\ncolors = OrderedDict({ 1: 'red', 2 : 'orange', 3 : 'blue', 4 : 'green'})\npoverty_mapping = OrderedDict({ 1: 'extreme', 2: 'moderate', 3:'vulnerable', 4: 'non vulnerable'})\n\n# Iterate through the float columns \n# enumerate : 반복문의 index와 value를 튜플로 반환함\nfor i, col in enumerate(train.select_dtypes('float')):\n    ax = plt.subplot(4 , 2, i+1)\n    # Iterate through the poverty levels\n    for poverty_level, color in colors.items():\n         # Plot each poverty level as a separate line\n            sns.kdeplot(train.loc[train['Target'] == poverty_level, col ]. dropna(),\n                       ax = ax, color = color, label = poverty_mapping[poverty_level])\n            #kdeplot : 커널 밀도(kernel density)는 커널이라는 함수를 겹치는 방법으로 히스토그램보다 부드러운 형태의 분포 곡선을 보여주는 방법\n            \n    plt.title(f'{col.capitalize()} Distribution')\n    # f''안에 {}가 들어가면 변수를 사용할 수 있음. \n    # .capitalize() -> 첫글자만 대문자\n    plt.xlabel(f'{col}')\n    plt.ylabel('Density')\n\nplt.subplots_adjust(top=2) # 서브플롯간 간격 조절 가능함","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.select_dtypes('object').head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mapping = {'yes' : 1, 'no' : 0}\n\n#Apply same operation to both train and test\nfor df in [train, test]:\n    df['dependency'] = df['dependency'].replace(mapping).astype(np.float64)\n    df['edjefe'] = df['edjefe'].replace(mapping).astype(np.float64)\n    df['edjefa'] = df['edjefa'].replace(mapping).astype(np.float64)\n    \ntrain[['dependency', 'edjefe', 'edjefa']].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[['dependency', 'edjefe', 'edjefa']].describe().transpose()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,10))\n\nfor i , col in enumerate(['dependency', 'edjefe', 'edjefa']):\n    ax = plt.subplot(3, 1, i+1)\n    for poverty_level, color in colors.items():\n        sns.kdeplot(train.loc[train['Target'] == poverty_level, col].dropna(),\n                   ax = ax, color = color, label = poverty_mapping[poverty_level])\n        \n    plt.title(f'{col.capitalize()} Distribution')\n    plt.xlabel(f'{col}')\n    plt.ylabel('Density')\n    \nplt.subplots_adjust(top = 2)\n               \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":" Exploring Label Distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"test['Target'] = np.nan\ndata = train.append(test, ignore_index = True)\n# append를 사용하면 concat처럼 아래 붙음\nprint(data.shape)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Heads of household\nheads = data.loc[data['parentesco1'] == 1].copy()\n\n#Labels for training\ntrain_labels = data.loc[(data['Target'].notnull()) & (data['parentesco1'] == 1), ['Target', 'idhogar']]\n\n#value counts of target\nlabel_counts = train_labels['Target'].value_counts().sort_index()\n\n# Bar plot of occurrences of each label\nlabel_counts.plot.bar(figsize=(8,6), color=colors.values(), edgecolor = 'k', linewidth=2)\n\n#Formatting\nplt.xlabel('Poverty Level')\nplt.ylabel('Count')\nplt.xticks([x - 1 for x in poverty_mapping.keys()],\n          list(poverty_mapping.values()), rotation = 60)\nplt.title('Poverty Level Breakdown')\n\nlabel_counts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Identify Errors"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Groupby the household and figure out the number of unique values\nall_equal = train.groupby('idhogar')['Target'].apply(lambda x : x.nunique() == 1)\n\n# households where targets are not all equal\nnot_equal = all_equal[all_equal != True]\nprint('There are {} households where the family members do not all have the same target.'.format(len(not_equal)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train['idhogar'] == not_equal.index[0]][['idhogar', 'parentesco1', 'Target']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Families without Heads of Household"},{"metadata":{"trusted":true},"cell_type":"code","source":"households_leader = train.groupby('idhogar')['parentesco1'].sum()\n\n# Find households without a head\nhouseholds_no_head = train.loc[train['idhogar'].isin(households_leader[households_leader == 0].index), :]\n\nprint('There are {} households without a head.'.format(households_no_head['idhogar'].nunique()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find households without a head and where labels are different\nhouseholds_no_head_equal = households_no_head.groupby('idhogar')['Target'].apply(lambda x : x.nunique() == 1)\nprint('{} Households with no head have different labels'.format(sum(households_no_head_equal == False)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Correct Errors"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Iterate through each household\nfor household in not_equal.index:\n    #Find the correct label (for the head of household)\n    true_target = int(train[ (train['idhogar'] == household) & (train['parentesco1'] == 1.0)]['Target'])\n    \n    #SEt the correct label for all members in the household\n    train.loc[train['idhogar'] == household , 'Target'] = true_target\n    \n#Groupby the household and figure out the number of unique values\nall_equal = train.groupby('idhogar')['Target'].apply(lambda x: x.nunique() == 1)\n\n# Households where targets are not all equal\nnot_equal = all_equal[all_equal != True]\nprint('There are {} households where the family members do not all have the same Target.'.format(len(not_equal)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Missing variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of missing in each column\nmissing = pd.DataFrame(data.isnull().sum()).rename(columns = {0 : 'total'})\n#Create a percentage missing\nmissing['percent'] = missing['total'] / len(data)\n\nmissing.sort_values('percent', ascending = False).drop('Target').head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Function to Plot Value Counts"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_value_counts(df, col, heads_only = False):\n    if heads_only:\n        df = df.loc[df['parentesco1'] == 1].copy()\n    \n    plt.figure(figsize=( 8 ,6 ))\n    df[col].value_counts().sort_index().plot.bar(color ='blue', edgecolor='k', linewidth = 2)\n    plt.xlabel(f'{col}')\n    plt.title(f'{col} Value Counts')\n    plt.ylabel('Count')\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_value_counts(heads, 'v18q1')\n# heads : 위에서 가장의 데이터만 뽑아서 복사해놓음","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"heads.groupby('v18q')['v18q1'].apply(lambda x: x.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['v18q1'] = data['v18q1'].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Variables indicating home ownership\nown_variables = [x for x in data if x.startswith('tipo')]\n\n# Plot of the home ownership variables for home missing rent payments\ndata.loc[data['v2a1'].isnull(), own_variables].sum().plot.bar(figsize=(10,8), color = 'green', edgecolor = 'k', linewidth=2)\nplt.xticks([0,1,2,3,4], ['Own and Paid off', 'Owns and Paying', 'Rented', 'Precarious', 'Other'], rotation=60)\nplt.title('Home Ownership Status  for Households Missing Rent Payments', size=18)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fill in house holds that own the house with 0 rent payment\ndata.loc[(data['tipovivi1'] == 1), 'v2a1'] = 0\n\n#Create missing rent payment column\ndata['v2a1-missing'] = data['v2a1'].isnull()\ndata['v2a1-missing'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.loc[data['rez_esc'].notnull()]['age'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.loc[data['rez_esc'].isnull()]['age'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# If individual is over 19 or younger than 7 ans missing years behind, set it to 0\ndata.loc[ (((data['age']>19) | (data['age']<7)) & (data['rez_esc'].isnull())), 'rez_esc'] = 0\n\n# Add a flag for those between 7 and 19 with a missing value\ndata['rez_esc-missing'] = data['rez_esc'].isnull()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.loc[data['rez_esc'] > 5 , 'rez_esc'] = 5","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plot Two Categorical Variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_categoricals(x, y, data, annotate = True):\n    \"\"\"Plot counts of two categoricals.\n    Size is raw count for each grouping.\n    Percentages are for a given value of y.\"\"\"\n    \n    # Raw counts \n    raw_counts = pd.DataFrame(data.groupby(y)[x].value_counts(normalize = False))\n    raw_counts = raw_counts.rename(columns = {x: 'raw_count'})\n    \n    # Calculate counts for each group of x and y\n    counts = pd.DataFrame(data.groupby(y)[x].value_counts(normalize = True))\n    \n    # Rename the column and reset the index\n    counts = counts.rename(columns = {x: 'normalized_count'}).reset_index()\n    counts['percent'] = 100 * counts['normalized_count']\n    \n    # Add the raw count\n    counts['raw_count'] = list(raw_counts['raw_count'])\n    \n    plt.figure(figsize = (14, 10))\n    # Scatter plot sized by percent\n    plt.scatter(counts[x], counts[y], edgecolor = 'k', color = 'lightgreen',s = 100 * np.sqrt(counts['raw_count']), marker = 'o', alpha = 0.6, linewidth = 1.5)\n    \n    if annotate:\n        # Annotate the plot with text\n        for i, row in counts.iterrows():\n            # Put text with appropriate offsets\n            plt.annotate(xy = (row[x] - (1 / counts[x].nunique()),  row[y] - (0.15 / counts[y].nunique())), color = 'navy',text = f\"{round(row['percent'], 1)}%\")\n        \n    # Set tick marks\n    plt.yticks(counts[y].unique())\n    plt.xticks(counts[x].unique())\n    \n    # Transform min and max to evenly space in square root domain\n    sqr_min = int(np.sqrt(raw_counts['raw_count'].min()))\n    sqr_max = int(np.sqrt(raw_counts['raw_count'].max()))\n        \n    # 5 sizes for legend\n    msizes = list( range(sqr_min, sqr_max, int((sqr_max - sqr_min) / 5)))\n    markers = []\n    print(msizes)\n    \n    # Markers for legend\n    for size in msizes:\n        markers.append(plt.scatter([], [], s = 100 * size, label = f'{int(round(np.square(size) / 100) * 100)}',  color = 'lightgreen', alpha = 0.6, edgecolor = 'k', linewidth = 1.5))\n        \n    # Legend and formatting\n    plt.legend(handles = markers, title = 'Counts',labelspacing = 3, handletextpad = 2, fontsize = 16, loc = (1.10, 0.19))\n    \n    plt.annotate(f'* Size represents raw count while % is for a given y value.', xy = (0, 1), xycoords = 'figure points', size = 10)\n    \n    # Adjust axes limits\n    plt.xlim((counts[x].min() - (6 / counts[x].nunique()), counts[x].max() + (6 / counts[x].nunique())))\n    plt.ylim((counts[y].min() - (4 / counts[y].nunique()), counts[y].max() + (4 / counts[y].nunique())))\n    plt.grid(None)\n    plt.xlabel(f\"{x}\"); plt.ylabel(f\"{y}\"); plt.title(f\"{y} vs {x}\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_categoricals('rez_esc', 'Target', data);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_categoricals('escolari', 'Target', data, annotate =False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_value_counts(data[(data['rez_esc-missing'] == 1)], 'Target')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_value_counts(data[(data['v2a1-missing'] == 1)], 'Target')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"id_ = ['Id', 'idhogar', 'Target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ind_bool = ['v18q', 'dis', 'male', 'female', 'estadocivil1', 'estadocivil2', 'estadocivil3', \n            'estadocivil4', 'estadocivil5', 'estadocivil6', 'estadocivil7', \n            'parentesco1', 'parentesco2',  'parentesco3', 'parentesco4', 'parentesco5', \n            'parentesco6', 'parentesco7', 'parentesco8',  'parentesco9', 'parentesco10', \n            'parentesco11', 'parentesco12', 'instlevel1', 'instlevel2', 'instlevel3', \n            'instlevel4', 'instlevel5', 'instlevel6', 'instlevel7', 'instlevel8', \n            'instlevel9', 'mobilephone', 'rez_esc-missing']\n\nind_ordered = ['rez_esc', 'escolari', 'age']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hh_bool = ['hacdor', 'hacapo', 'v14a', 'refrig', 'paredblolad', 'paredzocalo', \n           'paredpreb','pisocemento', 'pareddes', 'paredmad',\n           'paredzinc', 'paredfibras', 'paredother', 'pisomoscer', 'pisoother', \n           'pisonatur', 'pisonotiene', 'pisomadera',\n           'techozinc', 'techoentrepiso', 'techocane', 'techootro', 'cielorazo', \n           'abastaguadentro', 'abastaguafuera', 'abastaguano',\n            'public', 'planpri', 'noelec', 'coopele', 'sanitario1', \n           'sanitario2', 'sanitario3', 'sanitario5',   'sanitario6',\n           'energcocinar1', 'energcocinar2', 'energcocinar3', 'energcocinar4', \n           'elimbasu1', 'elimbasu2', 'elimbasu3', 'elimbasu4', \n           'elimbasu5', 'elimbasu6', 'epared1', 'epared2', 'epared3',\n           'etecho1', 'etecho2', 'etecho3', 'eviv1', 'eviv2', 'eviv3', \n           'tipovivi1', 'tipovivi2', 'tipovivi3', 'tipovivi4', 'tipovivi5', \n           'computer', 'television', 'lugar1', 'lugar2', 'lugar3',\n           'lugar4', 'lugar5', 'lugar6', 'area1', 'area2', 'v2a1-missing']\n\nhh_ordered = [ 'rooms', 'r4h1', 'r4h2', 'r4h3', 'r4m1','r4m2','r4m3', 'r4t1',  'r4t2', \n              'r4t3', 'v18q1', 'tamhog','tamviv','hhsize','hogar_nin',\n              'hogar_adul','hogar_mayor','hogar_total',  'bedrooms', 'qmobilephone']\n\nhh_cont = ['v2a1', 'dependency', 'edjefe', 'edjefa', 'meaneduc', 'overcrowding']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sqr_ = ['SQBescolari', 'SQBage', 'SQBhogar_total', 'SQBedjefe', \n        'SQBhogar_nin', 'SQBovercrowding', 'SQBdependency', 'SQBmeaned', 'agesq']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x= ind_bool + ind_ordered + id_ + hh_bool + hh_ordered + hh_cont + sqr_\n\nfrom collections import Counter\n\nprint('There are no repeats : ', np.all(np.array(list(Counter(x).values())) == 1 ))\n# np.all : 모든 원소가 참인지 평가하는 함수\nprint('We covered every variable : ', len(x) == data.shape[1])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lmplot('age', 'SQBage', data = data, fit_reg=False)\nplt.title('Squared Age vs Age');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lmplot('edjefe', 'SQBedjefe', data=data, fit_reg=False)\nplt.title('SQBedjefe vs edjefe');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.drop(columns = sqr_)\ndata.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"heads = data.loc[data['parentesco1'] == 1, :]\nheads = heads[id_ + hh_bool+ hh_cont + hh_ordered]\nheads.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix = heads.corr()\n#Select upper triangle of correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n# Find index of feature columns with correlation greater than 0.95\nto_drop = [column for column in upper.columns if any(abs(upper[column]) > 0.95)]\n\nto_drop\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tamhog_corr = corr_matrix.loc[corr_matrix['tamhog'].abs() > 0.9, corr_matrix['tamhog'].abs() > 0.9]\ntamhog_corr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(tamhog_corr, annot=True, cmap=plt.cm.autumn_r, fmt='.3f')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"heads = heads.drop(columns = ['tamhog', 'hogar_total', 'r4t3'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lmplot('tamviv', 'hhsize', data, fit_reg=False, size = 8);\nplt.title('Household size vs number of persons living in the household');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"heads['hhsize-diff'] = heads['tamviv'] - heads['hhsize']\nplot_categoricals('hhsize-diff', 'Target', heads)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix = heads.corr()\n#Select upper triangle of correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n# Find index of feature columns with correlation greater than 0.95\nto_drop = [column for column in upper.columns if any(abs(upper[column]) > 0.95)]\n\nto_drop","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix.loc[corr_matrix['coopele'].abs() > 0.9, corr_matrix['coopele'].abs() > 0.9]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"elec = []\n\nfor i , row in heads.iterrows():\n    if row['noelec'] == 1:\n        elec.append(0)\n    elif row['coopele'] == 1:\n        elec.append(1)\n    elif row['public'] == 1:\n        elec.append(2)\n    elif row['planpri'] == 1:\n        elec.append(3)\n    else:\n        elec.append(np.nan)\n    \nheads['elec'] = elec\nheads['elec-missing'] = heads['elec'].isnull()\n\n# heads = heads.drop(columns =['noelec', 'coopele', 'public', 'planpri'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_categoricals('elec', 'Target', heads)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"heads = heads.drop(columns = 'area2')\nheads.groupby('area1')['Target'].value_counts(normalize = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"heads.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"heads['walls'] = np.argmax(np.array(heads[['epared1', 'epared2', 'epared3']]), axis = 1)\n# heads = heads.drop(columns = ['epared1', 'epared2', 'epared3'])\nplot_categoricals('walls', 'Target', heads)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"heads['roof'] = np.argmax(np.array(heads[['etecho1', 'etecho2', 'etecho3']]), axis = 1)\nheads = heads.drop(columns = ['etecho1', 'etecho2', 'etecho3'])\nheads['floor'] = np.argmax(np.array(heads[['eviv1', 'eviv2', 'eviv3']]), axis = 1)\n# heads = heads.drop(columns = ['eviv1', 'eviv2', 'eviv3'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"heads['walls+roof+floor'] = heads['walls'] + heads['roof'] + heads['floor']\n\nplot_categoricals('walls+roof+floor', 'Target', heads, annotate = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"counts = pd.DataFrame(heads.groupby(['walls+roof+floor'])['Target'].value_counts(normalize=True)).rename(columns={'Target' : 'Normalized Count'}).reset_index()\ncounts.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"heads['warning'] = 1 * (heads['sanitario1'] + \n                         (heads['elec'] == 0) + \n                         heads['pisonotiene'] + \n                         heads['abastaguano'] + \n                         (heads['cielorazo'] == 0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10,6))\nsns.violinplot(x='warning', y ='Target', data= heads)\nplt.title('Target vs Warning Variable');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_categoricals('warning', 'Target', data = heads)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Owns a refrigerator, computer, tablet, and television\nheads['bonus'] = 1 * (heads['refrig'] + \n                      heads['computer'] + \n                      (heads['v18q1'] > 0) + \n                      heads['television'])\n\nsns.violinplot('bonus', 'Target', data = heads,\n                figsize = (10, 6));\nplt.title('Target vs Bonus Variable');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"heads['phones-per-capita'] = heads['qmobilephone'] / heads['tamviv']\nheads['tablets-per-capita'] = heads['v18q1'] / heads['tamviv']\nheads['rooms-per-capita'] = heads['rooms'] / heads['tamviv']\nheads['rent-per-capita'] = heads['v2a1'] / heads['tamviv']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import spearmanr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_corrs(x,y):\n    spr = spearmanr(x,y).correlation\n    pcr = np.corrcoef(x,y)[0,1]\n    \n    data = pd.DataFrame({'x': x , 'y' : y})\n    plt.figure(figsize = (6,4))\n    sns.regplot('x', 'y', data= data, fit_reg = False)\n    plt.title(f'Spearman : {round(spr,2)}; Pearson : {round(pcr,2)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = np.array(range(100))\ny = x **2\n\nplot_corrs(x,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_heads = heads.loc[heads['Target'].notnull(), :].copy()\npcorrs = pd.DataFrame(train_heads.corr()['Target'].sort_values()).rename(columns = {'Target':'pcorr'}).reset_index()\npcorrs = pcorrs.rename(columns = {'index' : 'feature'})\nprint('Most negatively correlated variables')\nprint(pcorrs.head())\nprint('\\nMost positively correlated variables')\nprint(pcorrs.dropna().tail())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore', category = RuntimeWarning)\n\nfeats = []\nscorr = []\npvalues = []\n\nfor c in heads:\n    if heads[c].dtype != 'object':\n        feats.append(c)\n        \n        scorr.append(spearmanr(train_heads[c], train_heads['Target']).correlation)\n        pvalues.append(spearmanr(train_heads[c], train_heads['Target']).pvalue)\n\nscorrs = pd.DataFrame({'feature': feats , 'scorr' : scorr, 'pvalue' : pvalues}).sort_values('scorr')\n\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Most negative Spearman correlations:')\nprint(scorrs.head())\nprint('\\nMost positive Spearman correlations:')\nprint(scorrs.dropna().tail())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corrs = pcorrs.merge(scorrs, on='feature')\ncorrs['diff'] = corrs['pcorr'] - corrs['scorr']\ncorrs.sort_values('diff').head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corrs.sort_values('diff').dropna().tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lmplot('dependency', 'Target', fit_reg = True, data=train_heads, x_jitter = 0.05, y_jitter=0.05)\nplt.title('Target vs Dependency');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lmplot('rooms-per-capita', 'Target', fit_reg = True, data=train_heads, x_jitter = 0.05, y_jitter=0.05)\nplt.title('Target vs rooms per captiva');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"variables = ['Target', 'dependency', 'warning', 'walls+roof+floor', 'meaneduc',\n             'floor', 'r4m1', 'overcrowding']\n\ncorr_mat = train_heads[variables].corr().round(2)\n\nplt.rcParams['font.size'] = 18\nplt.figure(figsize = (12,12))\nsns.heatmap(corr_mat, vmin = -0.5, vmax = 0.8, center = 0, cmap = plt.cm.RdYlGn_r, annot= True);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot_data.isnull().sum() -> meaneduc에 null 값이 있어서 임의로 내가 넣음\n\ntrain_heads['meaneduc'].fillna(train_heads['meaneduc'].mean(), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nplot_data = train_heads[['Target', 'dependency', 'walls+roof+floor', 'meaneduc', 'overcrowding']]\n\ngrid = sns.PairGrid(data = plot_data, size = 4, diag_sharey=False, hue='Target', hue_order = [4,3,2,1],\n                   vars = [x for x in list(plot_data.columns) if x != 'Target'])\n\ngrid.map_upper(plt.scatter, alpha=0.8, s=20)\n\ngrid.map_diag(sns.kdeplot)\n\ngrid.map_lower(sns.kdeplot, cmap=plt.cm.OrRd_r)\ngrid = grid.add_legend()\nplt.suptitle('Feature Plots Colored By Target', size = 32, y = 1.05);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"household_feats = list(heads.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ind = data[id_ + ind_bool + ind_ordered]\nind.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix = ind.corr()\n\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n\nto_drop = [column for column in upper.columns if any(abs(upper[column]) > 0.95)]\n\nto_drop","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ind = ind.drop(columns = 'male')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ind[[c for c in ind if c.startswith('instl')]].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ind['inst'] = np.argmax(np.array(ind[[c for c in ind if c.startswith('instl')]]), axis = 1)\nplot_categoricals('inst', 'Target', ind, annotate= False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10,8))\nsns.violinplot(x= 'Target', y='inst', data = ind)\nplt.title('Education Distribution by Target');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ind.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ind['escolari/age'] = ind['escolari'] / ind['age']\n\nplt.figure(figsize = (10,8))\nsns.violinplot('Target', 'escolari/age', data= ind);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ind['inst/age'] = ind['inst'] / ind['age']\nind['tech'] = ind['v18q'] + ind['mobilephone']\nind['tech'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"range_ = lambda x : x.max() - x.min()\nrange_.__name__ = 'range_'\n\nind_agg = ind.drop(columns = 'Target').groupby('idhogar').agg(['min', 'max', 'sum', 'count', 'std', range_])\nind_agg.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_col = []\nfor c in ind_agg.columns.levels[0]:\n    for stat in ind_agg.columns.levels[1]:\n        new_col.append(f'{c}-{stat}')\n        \nind_agg.columns = new_col\nind_agg.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ind_agg.iloc[:,[0,1,2,3,6,7,8,9]].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix = ind_agg.corr()\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k = 1).astype(np.bool))\n\nto_drop = [column for column in upper.columns if any(abs(upper[column])> 0.95)]\n\nprint(f'There ar {len(to_drop)} correlated columns to remove.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ind_agg = ind_agg.drop(columns = to_drop)\nind_feats = list(ind_agg.columns)\n\nfinal = heads.merge(ind_agg, on ='idhogar', how = 'left')\n\nprint('Rinal features shape : ', final.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Final Data Exploration"},{"metadata":{"trusted":true},"cell_type":"code","source":"corrs = final.corr()['Target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corrs.sort_values().head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corrs.sort_values().dropna().tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_categoricals('escolari-max', 'Target', final, annotate=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10,6))\nsns.violinplot(x='Target', y ='escolari-max', data=final)\nplt.title('Max Schooling by Target');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.boxplot(x='Target', y = 'escolari-max', data=final)\nplt.title('Max Schooling by Target');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10, 6))\nsns.boxplot(x = 'Target', y = 'meaneduc', data = final);\nplt.xticks([0, 1, 2, 3], poverty_mapping.values())\nplt.title('Average Schooling by Target');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10, 6))\nsns.boxplot(x = 'Target', y = 'overcrowding', data = final);\nplt.xticks([0, 1, 2, 3], poverty_mapping.values())\nplt.title('Overcrowding by Target');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"head_gender = ind.loc[ind['parentesco1'] == 1 , ['idhogar', 'female']]\nfinal = final.merge(head_gender, on = 'idhogar', how='left').rename(columns={'female' : 'female-head'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final.groupby('female-head')['Target'].value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.violinplot(x='female-head', y='Target', data=final)\nplt.title('Target by Female head of Household');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize= (8,8))\nsns.boxplot(x='Target', y='meaneduc', hue='female-head', data=final)\nplt.title('Average Education by Target and Female Head of Household', size=16);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final.groupby('female-head')['meaneduc'].agg(['mean', 'count'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Machine Learning Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score, make_scorer\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.pipeline import Pipeline\n\nscorer = make_scorer(f1_score, greater_is_better=True, average='macro')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels = np.array(list(final[final['Target'].notnull()]['Target'].astype(np.uint8)))\n\ntrain_set = final[final['Target'].notnull()].drop(columns = ['Id', 'idhogar', 'Target'])\ntest_set = final[final['Target'].isnull()].drop(columns = ['Id', 'idhogar', 'Target'])\n\nsubmission_base = test[['Id', 'idhogar']].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = list(train_set.columns)\n\n#imputer는 각 속성의 중앙값을 계산하고, 그 결과를 statistics_ 인스턴스 변수에 저장합니다. \n\npipeline = Pipeline([('imputer', Imputer(strategy = 'median')),\n                     ('scaler', MinMaxScaler())])\n\ntrain_set = pipeline.fit_transform(train_set)\ntest_set = pipeline.transform(test_set)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = RandomForestClassifier(n_estimators = 100, random_state=10, n_jobs= -1)\n\ncv_score = cross_val_score(model, train_set, train_labels, cv=10 , scoring = scorer)\n\nprint(f'10 Fold Cross Validation F1 Score= {round(cv_score.mean(), 4)} with std = {round(cv_score.std(), 4)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Feature Importances"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(train_set, train_labels)\n\nfeature_importances = pd.DataFrame({'feature' : features, 'importance':model.feature_importances_})\nfeature_importances.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_feature_importances(df, n=10, threshold =None):\n    plt.style.use('fivethirtyeight')\n    \n    df = df.sort_values('importance', ascending=False).reset_index(drop=True)\n    # index를 reset하며 원래 index가 새로운 column에 저장되는데 그걸 drop시킴\n    \n    df['importance_normalized'] = df['importance'] / df['importance'].sum()\n    df['cumulative_importance'] = np.cumsum(df['importance_normalized'])\n    # 누적합\n    \n    plt.rcParams['font.size'] = 12\n    df.loc[:n, :].plot.barh(y = 'importance_normalized',\n                            x = 'feature', color='darkgreen',\n                           edgecolor = 'k', figsize = (12,8),\n                           legend = False, linewidth = 2)\n    \n    plt.xlabel('Normalized Importance', size = 18)\n    plt.ylabel('')\n    plt.title(f'{n} Most Important Features', size=18)\n    plt.gca().invert_yaxis()\n    \n    if threshold:\n        plt.figure(figsize = (8, 6))\n        plt.plot(list(range(len(df))), df['cumulative_importance'], 'b-')\n        plt.xlabel('Number of Features', size = 16); \n        plt.ylabel('Cumulative Importance', size = 16); \n        plt.title('Cumulative Feature Importance', size = 18);\n        \n        # Number of features needed for threshold cumulative importance\n        # This is the index (will need to add 1 for the actual number)\n        importance_index = np.min(np.where(df['cumulative_importance'] > threshold))\n        \n        # Add vertical line to plot\n        plt.vlines(importance_index + 1, ymin = 0, ymax = 1.05, linestyles = '--', colors = 'red')\n        plt.show();\n        \n        print('{} features required for {:.0f}% of cumulative importance.'.format(importance_index + 1, \n                                                                                  100 * threshold))\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"norm_f1 = plot_feature_importances(feature_importances, threshold=0.95)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def kde_target(df, variable):\n    colors = {1:'red', 2:'orange', 3:'blue', 4:'green'}\n    \n    plt.figure(figsize=(12,8))\n    \n    df = df[df['Target'].notnull()]\n    \n    for level in df['Target'].unique():\n        subset = df[df['Target'] == level].copy()\n        sns.kdeplot(subset[variable].dropna(),\n                    label = f'Poverty Level : {level}',\n                    color = colors[int(subset['Target'].unique())])\n    plt.xlabel(variable)\n    plt.ylabel('Density')\n    plt.title('{} Distribution'.format(variable.capitalize()));","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kde_target(final, 'meaneduc')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kde_target(final, 'escolari/age-range_')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import LinearSVC\nfrom sklearn.naive_bayes import GaussianNB #\nfrom sklearn.neural_network import MLPClassifier #\nfrom sklearn.linear_model import LogisticRegressionCV, RidgeClassifierCV \nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis #\nfrom sklearn.neighbors import KNeighborsClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nfrom sklearn.exceptions import ConvergenceWarning\n\nwarnings.filterwarnings('ignore', category = ConvergenceWarning)\nwarnings.filterwarnings('ignore', category = DeprecationWarning)\nwarnings.filterwarnings('ignore', category =UserWarning)\n\nmodel_results = pd.DataFrame(columns = ['model', 'cv_mean', 'cv_std'])\n\ndef cv_model(train, train_labels, model, name, model_results=None):\n    cv_scores = cross_val_score(model, train, train_labels, cv = 10, scoring=scorer, n_jobs = -1)\n    print(f'10 Fold CV Score : {round(cv_scores.mean() , 5)} with std : {round(cv_scores.std(), 5)}')\n    \n    if model_results is not None:\n        model_results = model_results.append(pd.DataFrame({'model' : name,\n                                                           'cv_mean' : cv_scores.mean(),\n                                                           'cv_std' : cv_scores.std()},\n                                                           index = [0]),\n                                            ignore_index = True)\n        \n        return model_results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_results = cv_model(train_set, train_labels, LinearSVC(), 'LSVC', model_results)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_results = cv_model(train_set, train_labels, GaussianNB(), 'GNB', model_results)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_results = cv_model(train_set, train_labels, MLPClassifier(hidden_layer_sizes = (32,64,128,64,32)), 'MLP', model_results)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_results = cv_model(train_set, train_labels, LinearDiscriminantAnalysis(), 'LDA', model_results)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_results = cv_model(train_set, train_labels, RidgeClassifierCV(), 'RIDGE', model_results)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for n in [5,10,20]:\n    print(f'\\nKNN with {n} neighbors\\n')\n    model_results = cv_model(train_set, train_labels,\n                             KNeighborsClassifier(n_neighbors = n),\n                             f'knn-{n}', model_results)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesClassifier\n\nmodel_results = cv_model(train_set, train_labels ,\n                         ExtraTreesClassifier(n_estimators = 100 , random_state = 10),\n                        'EXT', model_results)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Comparing Model Performance","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_results = cv_model(train_set, train_labels,\n                         RandomForestClassifier(100, random_state = 10),\n                        'RF', model_results)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_results.set_index('model', inplace = True)\nmodel_results['cv_mean'].plot.bar(color = 'orange', figsize = (8,6),\n                                 yerr = list(model_results['cv_std']),\n                                 edgecolor = 'k', linewidth = 2)\nplt.title('Model F1 Score Results')\nplt.ylabel('Mean F1 Score (with error bar)');\nmodel_results.reset_index(inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Making A Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_ids = list(final.loc[final['Target'].isnull(), 'idhogar'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def submit(model, train, train_labels, test, test_ids):\n    model.fit(train, train_labels)\n    predictions = model.predict(test)\n    predictions = pd.DataFrame({'idhogar' : test_ids,\n                                'Target' : predictions})\n    \n    submission = submission_base.merge(predictions,\n                                      on = 'idhogar',\n                                      how='left').drop(columns = ['idhogar'])\n    \n    submission['Target'] = submission['Target'].fillna(4).astype(np.int8)\n    \n    return submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_submission = submit(RandomForestClassifier(n_estimators = 100,\n                                             random_state=10, n_jobs = -1),\n                      train_set, train_labels, test_set, test_ids)\n\nrf_submission.to_csv('rf_submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set = pd.DataFrame(train_set, columns = features)\n\ncorr_matrix = train_set.corr()\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\nto_drop = [column for column in upper.columns if any(abs(upper[column]) > 0.95)]\n\nto_drop","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set = train_set.drop(columns = to_drop)\ntrain_set.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_set = pd.DataFrame(test_set, columns=features)\ntrain_set, test_set = train_set.align(test_set, axis = 1, join ='inner')\nfeatures = list(train_set.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import RFECV\nestimator = RandomForestClassifier(random_state = 10 , n_estimators = 100, n_jobs= -1)\nselector = RFECV(estimator, step = 1, cv =3, scoring=scorer, n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"selector.fit(train_set, train_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(selector.grid_scores_)\nplt.xlabel('Number of Features')\nplt.ylabel('Macro F1 Score')\nplt.title('Feature Selection Scores')\nselector.n_features_\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rankings = pd.DataFrame({'feature' : list(train_set.columns), 'rank': list(selector.ranking_)}).sort_values('rank')\nrankings.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_selected = selector.transform(train_set)\ntest_selected = selector.transform(test_set)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"selected_features = train_set.columns[np.where(selector.ranking_==1)]\ntrain_selected = pd.DataFrame(train_selected, columns = selected_features)\ntest_selected = pd.DataFrame(test_selected, columns = selected_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_results = cv_model(train_selected, train_labels, model, 'RF-SEL', model_results)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_results.set_index('model', inplace = True)\nmodel_results['cv_mean'].plot.bar(color = 'orange', figsize = (8,6),\n                                  yerr = list(model_results['cv_std']),\n                                  edgecolor='k', linewidth=2)\nplt.title('Model F1 Score Results')\nplt.ylabel('Mean F1  Score(with error bar)')\nmodel_results.reset_index(inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Upgrading Model : Gradient Boosting Maching"},{"metadata":{"trusted":true},"cell_type":"code","source":"def macro_f1_score(labels, predictions):\n    predictions = predictions.reshape(len(np.unique(labels)), -1).argmax(axis = 0)\n    metric_value = f1_score(labels, predictions, average ='macro')\n    return 'macro_f1', metric_value, True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nimport lightgbm as lgb\nfrom IPython.display import display\n\ndef model_gbm(features, labels, test_features, test_ids, nfolds = 5, return_preds = False, hyp = None):\n    \"\"\"Model using the GBM and cross validation.\n       Trains with early stopping on each fold.\n       Hyperparameters probably need to be tuned.\"\"\"\n    \n    feature_names = list(features.columns)\n\n    # Option for user specified hyperparameters\n    if hyp is not None:\n        # Using early stopping so do not need number of esimators\n        if 'n_estimators' in hyp:\n            del hyp['n_estimators']\n        params = hyp\n    \n    else:\n        # Model hyperparameters\n        params = {'boosting_type': 'dart', \n                  'colsample_bytree': 0.88, \n                  'learning_rate': 0.028, \n                   'min_child_samples': 10, \n                   'num_leaves': 36, 'reg_alpha': 0.76, \n                   'reg_lambda': 0.43, \n                   'subsample_for_bin': 40000, \n                   'subsample': 0.54, \n                   'class_weight': 'balanced'}\n    \n    # Build the model\n    model = lgb.LGBMClassifier(**params, objective = 'multiclass', \n                               n_jobs = -1, n_estimators = 10000,\n                               random_state = 10)\n    \n    # Using stratified kfold cross validation\n    strkfold = StratifiedKFold(n_splits = nfolds, shuffle = True)\n    \n    # Hold all the predictions from each fold\n    predictions = pd.DataFrame()\n    importances = np.zeros(len(feature_names))\n    \n    # Convert to arrays for indexing\n    features = np.array(features)\n    test_features = np.array(test_features)\n    labels = np.array(labels).reshape((-1 ))\n    \n    valid_scores = []\n    \n    # Iterate through the folds\n    for i, (train_indices, valid_indices) in enumerate(strkfold.split(features, labels)):\n        \n        # Dataframe for fold predictions\n        fold_predictions = pd.DataFrame()\n        \n        # Training and validation data\n        X_train = features[train_indices]\n        X_valid = features[valid_indices]\n        y_train = labels[train_indices]\n        y_valid = labels[valid_indices]\n        \n        # Train with early stopping\n        model.fit(X_train, y_train, early_stopping_rounds = 100, eval_metric = macro_f1_score,\n                  eval_set = [(X_train, y_train), (X_valid, y_valid)] , eval_names = ['train', 'valid'], verbose = 200)\n        \n        # Record the validation fold score\n        valid_scores.append(model.best_score_['valid']['macro_f1'])\n        \n        # Make predictions from the fold as probabilities\n        fold_probabilitites = model.predict_proba(test_features)\n        \n        # Record each prediction for each class as a separate column\n        for j in range(4):\n            fold_predictions[(j + 1)] = fold_probabilitites[:, j]\n            \n        # Add needed information for predictions \n        fold_predictions['idhogar'] = test_ids\n        fold_predictions['fold'] = (i+1)\n        \n        # Add the predictions as new rows to the existing predictions\n        predictions = predictions.append(fold_predictions)\n        \n        # Feature importances\n        importances += model.feature_importances_ / nfolds   \n        \n        # Display fold information\n        display(f'Fold {i + 1}, Validation Score: {round(valid_scores[i], 5)}, Estimators Trained: {model.best_iteration_}')\n\n    # Feature importances dataframe\n    feature_importances = pd.DataFrame({'feature': feature_names,\n                                        'importance': importances})\n    \n    valid_scores = np.array(valid_scores)\n    display(f'{nfolds} cross validation score: {round(valid_scores.mean(), 5)} with std: {round(valid_scores.std(), 5)}.')\n    \n    # If we want to examine predictions don't average over folds\n    if return_preds:\n        predictions['Target'] = predictions[[1, 2, 3, 4]].idxmax(axis = 1)\n        predictions['confidence'] = predictions[[1, 2, 3, 4]].max(axis = 1)\n        return predictions, feature_importances\n    \n    # Average the predictions over folds\n    predictions = predictions.groupby('idhogar', as_index = False).mean()\n    \n    # Find the class and associated probability\n    predictions['Target'] = predictions[[1, 2, 3, 4]].idxmax(axis = 1)\n    predictions['confidence'] = predictions[[1, 2, 3, 4]].max(axis = 1)\n    predictions = predictions.drop(columns = ['fold'])\n    \n    # Merge with the base to have one prediction for each individual\n    submission = submission_base.merge(predictions[['idhogar', 'Target']], on = 'idhogar', how = 'left').drop(columns = ['idhogar'])\n        \n    # Fill in the individuals that do not have a head of household with 4 since these will not be scored\n    submission['Target'] = submission['Target'].fillna(4).astype(np.int8)\n    \n    # return the submission and feature importances along with validation scores\n    return submission, feature_importances, valid_scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%capture --no-display\npredictions, gbm_fi = model_gbm(train_set , train_labels, test_set, test_ids, return_preds=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%capture\nsubmission, gbm_fi, valid_scores = model_gbm(train_set, train_labels, test_set, test_ids, return_preds = False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = submission[['id', 'Target']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('gbm_baseline.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\n%%capture\nsubmission, gbm_fi, valid_scores = model_gbm(train_set, train_labels, test_set, test_ids, nfolds=10, return_preds=False)\n'''","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}