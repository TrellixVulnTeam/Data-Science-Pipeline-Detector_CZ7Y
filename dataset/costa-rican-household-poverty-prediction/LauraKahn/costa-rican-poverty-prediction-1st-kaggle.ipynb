{"cells":[{"metadata":{"collapsed":true,"_uuid":"0dabedd4fa4a52c84403d441bda9e148c99a9822"},"cell_type":"markdown","source":"# Costa Rican Household Poverty Level Prediction \n\nDevelop a machine learning model that can predict the poverty level of households using both individual and household characteristics. "},{"metadata":{"_uuid":"e57b61661f2f9b2037d643cc12a12e0e16b0af55"},"cell_type":"markdown","source":"## Problem and Data Explanation\n\nThe data for this competition is provided in two files: train.csv and test.csv. The training set has 9557 rows and 143 columns while the testing set has 23856 rows and 142 columns. Each row represents one individual and each column is a feature, either unique to the individual, or for the household of the individual. The training set has one additional column, Target, which represents the poverty level on a 1-4 scale and is the label for the competition. A value of 1 is the most extreme poverty.\n\nThis is a supervised multi-class classification machine learning problem:\n\nSupervised: provided with the labels for the training data\nMulti-class classification: Labels are discrete values with 4 classes"},{"metadata":{"_uuid":"374e02e88da73bb06b162dd5e9502c5529990d94"},"cell_type":"markdown","source":"## Objective\nThe objective is to predict poverty on a household level. We are given data on the individual level with each individual having unique features but also information about their household. In order to create a dataset for the task, we'll have to perform some aggregations of the individual data for each household. Moreover, we have to make a prediction for every individual in the test set, but \"ONLY the heads of household are used in scoring\" which means we want to predict poverty on a household basis.\n\nImportant note: while all members of a household should have the same label in the training data, there are errors where individuals in the same household have different labels. In these cases, we are told to use the label for the head of each household, which can be identified by the rows where parentesco1 == 1.0. We will cover how to correct this in the notebook (for more info take a look at the competition main discussion).\n\nThe Target values represent poverty levels as follows:\n\n__1 = extreme poverty__<br> \n__2 = moderate poverty__ <br> \n__3 = vulnerable households__<br> \n__4 = non vulnerable households__<br>"},{"metadata":{"_uuid":"e1fe2f26cd949e82e918a42642561398af03f721"},"cell_type":"markdown","source":"## Data Descriptions\nThe explanations for all 143 columns can be found in the competition documentation, but a few to note are below:\n\nId: a unique identifier for each individual, this should not be a feature that we use.\n\nidhogar: a unique identifier for each household. This variable is not a feature, but will be used to group individuals by household as all individuals in a household will have the same identifier.\n\nparentesco1: indicates if this person is the head of the household.\n\nTarget: the label, which should be equal for all members in a household\n\nWhen we make a model, we'll train on a household basis with the label for each household the poverty level of the head of household. The raw data contains a mix of both household and individual characteristics and for the individual data, we will have to find a way to aggregate this for each household. Some of the individuals belong to a household with no head of household which means that unfortunately we can't use this data for training. These issues with the data are completely typical of real-world data and hence this problem is great preparation for the datasets you'll encounter in a data science job."},{"metadata":{"_uuid":"1ad7f8341f35ed06c1e126a0ef23879520b208d3"},"cell_type":"markdown","source":"## Metric\nUltimately we want to build a machine learning model that can predict the integer poverty level of a household. Our predictions will be assessed by the __Macro F1 Score__. You may be familiar with the standard F1 score for binary classification problems which is the harmonic mean of precision and recall:\n\nF1=21recall+1precision=2⋅precision⋅recallprecision+recall\n \nFor mutli-class problems, we have to average the F1 scores for each class. The macro F1 score averages the F1 score for each class without taking into account label imbalances.\n\nMacro F1=F1 Class 1+F1 Class 2+F1 Class 3+F1 Class 44\n \nIn other words, the number of occurrences of each label does not figure into the calculation when using macro (while it does when using the \"weighted\" score). (For more information on the differences, look at the Scikit-Learn Documention for F1 Score or this Stack Exchange question and answers. If we want to assess our performance, we can use the code:\n\nfrom sklearn.metrics import f1_score\nf1_score(y_true, y_predicted, average = 'macro`)\nFor this problem, the labels are imbalanced, which makes it a little strange to use macro averaging for the evaluation metric, but that's a decision made by the organizers and not something we can change! In your own work, you want to be aware of label imbalances and choose a metric accordingly."},{"metadata":{"_uuid":"0322b88bc8ca8a823a19991dcbc0f9c2bc93dc30"},"cell_type":"markdown","source":"## Roadmap\nThe end objective is a machine learning model that can predict the poverty level of a household. However, before we get carried away with modeling, it's important to understand the problem and data. Also, we want to evaluate numerous models before choosing one as the \"best\" and after building a model, we want to investigate the predictions. Our roadmap is therefore as follows:\n\n- Understand the problem (we're almost there already)\n- Exploratory Data Analysis\n- Feature engineering to create a dataset for machine learning\n- Compare several baseline machine learning models\n- Try more complex machine learning models\n- Optimize the selected model\n- Investigate model predictions in context of problem\n- Draw conclusions and lay out next steps\n\nThe steps laid out above are iterative meaning that while we will go through them one at a time, we might go back to an earlier step and revisit some of our decisions. In general, data science is a non-linear pracice where we are constantly evaluating our past decisions and making improvements. In particular, feature engineering, modeling, and optimization are steps that we often repeat because we never know if we got them right the first time!\n\n### Getting Started\nFor the EDA we'll examine any interesting anomalies, trends, correlations, or patterns that can be used for feature engineering and for modeling. We'll make sure to investigate our data both quantitatively (with statistics) and visually (with figures).\n\nOnce we have a good grasp of the data and any potentially useful relationships, we can do some feature engineering (the most important part of the machine learning pipeline) and establish a baseline model. This won't get us to the top of the leaderboard, but it will provide a strong foundation to build on!\n\nWith all that info in mind (don't worry if you haven't got all the details), let's get started!"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"192c01a796a3e1cbfc4cfb6fd5d0eab960cc89e1"},"cell_type":"code","source":"#Import necessary libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set a few plotting defaults\n%matplotlib inline\nplt.style.use('fivethirtyeight')\nplt.rcParams['font.size'] = 18\nplt.rcParams['patch.edgecolor'] = 'k'","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"9f93c41f9320ecd865a93a045bc5e7bf4726571d","collapsed":true},"cell_type":"code","source":"#Read in Data and look at Summary Information\npd.options.display.max_columns = 150\n\n# Read in data\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"49206000ab2b5e48c1d58341cfb5557cbd466c33","collapsed":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"144905e581e5b9bb1ebd950522e5e471af488310","collapsed":true},"cell_type":"code","source":"test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"9f474cc06a989c4c9ae1687ef271fe8768670793","collapsed":true},"cell_type":"code","source":"test","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"41dfea663fd6d9516eac125d384014b372f87cba","collapsed":true},"cell_type":"code","source":"Ids = test['Id'].unique()\nIds","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ab8d94c67d871311e4adf1fb6f66505ef32004a8","collapsed":true},"cell_type":"code","source":"test.groupby('SQBescolari').describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"3d4fec0b31ae72e5014778587dc0d49745579fe9","collapsed":true},"cell_type":"code","source":"train.groupby('SQBescolari').describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"3b1b7b73ebb6b30cb52991fdc95497f460fa2773","collapsed":true},"cell_type":"code","source":"#Let's look at the distribution of the 'Target' variable from the training data\nfig=plt.figure(figsize=(10,10))\ntrain.hist(column='Target')\nplt.xlabel('Poverty Level')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"33a8881c13265973b9ba5e04737c58798901b115","collapsed":true},"cell_type":"code","source":"#Now let's compare a few variables in the training and test datasets.\n\nx = test['SQBescolari']\ny = train['SQBescolari']\n\nfrom matplotlib import pyplot\npyplot.hist(x, label='Test')\npyplot.hist(y, label='Train',color='purple')\npyplot.legend(loc='upper right')\npyplot.title('SQBescolari')\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"f872c6890aff9e807d4cd14c7ae31f298942cd0f","collapsed":true},"cell_type":"code","source":"#To explore SQBovercrowding variable, a histogram won't show enough info so let's do a line plot\nx = test['SQBovercrowding']\ny = train['SQBovercrowding']\n\nfrom matplotlib import pyplot as plt\n\nplt.plot(x, label='Test', marker='o')\nplt.plot(y, label='Train',color='purple')\nplt.legend(loc='upper left')\nplt.title('SQBovercrowding')\nplt.show()\n\n#Well that isn't extremely helpful since there don't appear to be any patterns. \n\n#Let's look at the outlier data points a bit more in each of the Test and Training data sets.","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ae2abb42b7ca32935b7d2a2094d73566f8088a8a","collapsed":true},"cell_type":"code","source":"#Let's zoom in on the test outliers by changing the x and y axis limits\nplt.plot(x, label='Test', marker='o')\nplt.xlim(16000,20100)\nplt.ylim(80,180)\nplt.title('SQBovercrowding Test')\nplt.show()\n\n#The outlier values in the test dataset seem to be 100 and 170.\n#Most values in the training dataset are between 0-40; and 0-50 in the test dataset.","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"64d75386f3cb7b16831fa054137aa3e983ba6e27","collapsed":true},"cell_type":"code","source":"#Let's look more closely at the variable 'SQBescolari' (square years of education) that has values 0, 1, 4 and 9\n#And where the Target is 1 or 2 (extreme or moderate poverty)\n\nSQBescolari_train = train.query('SQBescolari <=9' and 'Target <=2')\nSQBescolari_train\n\n#There are 755 with Target of 1, 1597 with Target of 2, and 2352 with either 1 or 2 \n#out of the total train sample size of 9553.\n\n#We still don't know if the 'SQBage' or 'SQBhogar_total' are significant variables so let's keep for now.","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"8dbfadd2af1362f62f32f9f6e063debed1cc93ab","collapsed":true},"cell_type":"code","source":"#Explore relationship between 'SQBescolari', 'SQBage', 'SQBhogar_total', 'SQBovercrowding' and 'Target'\n#Output feature in training dataset\n\nimport seaborn as sns\n\n#Calculate the correlation matrix\ncorr = SQBescolari_train.corr()\n\n#Plot the heatmap\nsns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns)\n\n#From the heatmap, we see 'SQBage' and 'SQBhogar_total' have 0 correlation so these variables can be deleted.\n\n#SQBescolari is positively correlated (0.4) and 'SQBovercrowding' is highly negatively correlated (-0.25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"0b7ff8f0ef50bdc3ec1a93df0fc76ed3e525e7a4","collapsed":true},"cell_type":"code","source":"trainclean = SQBescolari_train[['Id','SQBescolari', 'SQBovercrowding']]\ntrainclean","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bbd633d61c48f5949ff16d4c42011d012d713752"},"cell_type":"markdown","source":"## Continue Exploratory Data Analysis with Test Data"},{"metadata":{"trusted":false,"_uuid":"4c956fadd7ed13aa6c130af2d9947f6e31a3d122","collapsed":true},"cell_type":"code","source":"test.groupby('SQBovercrowding').describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"57416881073f5b7535dc2466847e4c3010a2e0fc","collapsed":true},"cell_type":"code","source":"#From looking at the training dataset above, we see Target of 1 or 2 when 'SQBovercrowding' has values between \n#0.111 and 12.25 so let's query with this in mind.\n\nSQBescolari_test= test.query('SQBescolari <=9')\nSQBescolari_test\n\n#3162 have 0 years SQBescolari, 3879 1 years, 769 have 4 years, 1051 have 9 years out of 23856 test sample size.\n##(about 37% of total sample size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"2667ddb782a0488d120f5a31059083d30d2f58c4","collapsed":true},"cell_type":"code","source":"#Now let's find unique values for SQBovercrowding\nSQBescolari_test.SQBovercrowding.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"b43f28c8cfa67b40e2b3bd31c95cbf81bf0338b5","collapsed":true},"cell_type":"code","source":"#Clean up SQBescolari_test dataframe to get rid of unnecessary columsn\ntestclean = SQBescolari_test[['Id','SQBescolari', 'SQBovercrowding']]\ntestclean","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"88de100126b8a2238e806ccf4bf1233c9ddc8bee","collapsed":true},"cell_type":"code","source":"#Let's see if we can visualize the relationship (if any) between SQBescolari & SQBovercrowding\n\nimport seaborn as sns\n\n#Calculate the correlation matrix\ncorr = testclean.corr()\n\n#Plot the heatmap\nsns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns)\n\n#It appears that the two variables are highly linearly correlated.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"183b5aa9dac9675d8f9709904c8eaac86626df3c"},"cell_type":"markdown","source":"## Duplicate Features\n\n- 'SQBage' and 'agesq' are duplicates of 'age'\n- 'SQBedjefe', 'edjefe'[101] and 'SQBmeaned' are duplicate metrics of 'SQBescolari'\n- 'SQBhogar_nin' is duplicate metric of 'SQBhogar_total'"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"040b4aa0b4e4de1671da42a06cb8eeeb86760f6f"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"99394c9ef181acc61115a3a53bbb933e77b4315f","collapsed":true},"cell_type":"code","source":"#Build a quick baseline Random Forest Classifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n#Define input and output features\nytrain = train.iloc[:,-1] #Define target variable as last column of data frame (see https://www.shanelynn.ie/select-pandas-dataframe-rows-and-columns-using-iloc-loc-and-ix/)\nXtrain = train.drop('Target', axis=1)\n\n#Fill NAs\nXtrain = Xtrain.fillna(-999)\n\n#label encoder\nfor c in train.columns[train.dtypes == 'object']:\n    Xtrain[c] = Xtrain[c].factorize()[0]\n\nrf = RandomForestClassifier()\nrf.fit(Xtrain,ytrain)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"0938a1bd1fce17a1d2e2028f6290af32a880497d","collapsed":true},"cell_type":"code","source":"#Test the model\n\n#Create a copy to work with\nXtest = test.copy()\n\n#Save and drop labels\nytest = Xtrain\nXtest = Xtrain.iloc[0:141]\n\n#Fill NAs\nXtest = Xtest.fillna(-999)\n\n#Make the prediction\nypredictions = rf.predict(ytest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"3f8ce4e98b8a29790d58ae7194902737bacef636","collapsed":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nprint(\"=== Confusion Matrix ===\")\nprint(confusion_matrix(ytest, ypredictions))\nprint('\\n')\nprint(\"=== Classification Report ===\")\nprint(classification_report(ytest, ypredictions))\nprint('\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"de7e56040c06c98000ca7847be8663ea7ca38ca1","collapsed":true},"cell_type":"code","source":"#Random Forest Classifier Model Metrics\n\n# Use the forest's predict method on the test data\npredictions = rf.predict(Xtest)\n# Calculate the absolute errors\nerrors = abs(predictions - ytest)\n\n#Macro F1 Score is Model Evaluation Metric\nfrom sklearn.metrics import f1_score\n\nprint(\"=== Macro F1 Score ===\")\nf1_score (ytrain, ypredictions, average='macro')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"59541de3a644ba01b145c85a77741dd08bed63df","collapsed":true},"cell_type":"code","source":"#Now add these tuned parameters to the model to see if we can improve results\nrfc = RandomForestClassifier(n_estimators=1000, max_depth=300, max_features='auto')\nrfc.fit(Xfeatures_train,yfeatures_train)\nrfc_predict = rfc.predict(Xfeatures_test)\n\nprint(\"=== Macro F1 Score ===\")\nf1_score (ytrain, ypredictions, average='macro')\n\n#Notice there is no change in the macro F1 score with hypertuned parameters.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ce81ec7f32bc0504e727fa4c5de78d45b73e989e"},"cell_type":"markdown","source":"More Exploratory Data Analysis - Feature Selection"},{"metadata":{"trusted":false,"_uuid":"59db1cb4e99d155de9c129418ec72db4a726bff4","collapsed":true},"cell_type":"code","source":"#Plot Feature Importance\nplt.figure(figsize=(10,10)) #Increased figure size to see which features are most interesting\nplt.plot(rf.feature_importances_, 'bo') #change to points to see individual feature points.\nplt.xticks(np.arange(Xtrain.shape[1], Xtrain.columns.tolist, rotation=vertical))\nplt.xlabel('Features')\nplt.xlim(90,140)\nplt.show()\n#TLet's take a closer look at the outliers to see which features might affect the model the most.","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"bca56a236e845a3b21c40aaaf1c394658dcff732","collapsed":true},"cell_type":"code","source":"import numpy as np\nnp.set_printoptions(threshold=np.inf)  #https://stackoverflow.com/questions/1987694/how-to-print-the-full-numpy-array\n\nprint(\"-Here are the predicted Poverty Level Targets-\")\nypredictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"4e307623cfd88847ea98dc99c2233b32f8afbbb0","collapsed":true},"cell_type":"code","source":"#Read in Results Data\nsubmission = pd.read_csv('CR_Kaggle_LKahn.csv')\nsubmission.head(5)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"d22aa01db6fe58b5587483dbd031b5cce4922a80"},"cell_type":"code","source":"submission.to_csv('./Submission_log_RF.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"0da79f7409c99163b9da80c0af47b2caab00093e","collapsed":true},"cell_type":"code","source":"#Next, let's try a NN to see if we can improve F1 macro score\nfrom sklearn.neural_network import MLPClassifier\n\n#Create a copy to work with\nXtrain = train.copy()\n\n#Save and drop labels\nytrain = Xtrain.iloc[:,-1] #Define target variable as last column of data frame (see https://www.shanelynn.ie/select-pandas-dataframe-rows-and-columns-using-iloc-loc-and-ix/)\nXtrain = Xtrain.drop('Target', axis=1)\n\n#Fill NAs\nXtrain = Xtrain.fillna(-999)\n\n#label encoder\nfor c in train.columns[train.dtypes == 'object']:\n    Xtrain[c] = Xtrain[c].factorize()[0]\n\nMLP= MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5,2),random_state=1)\nMLP.fit(Xtrain,ytrain)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"4e1706fb443e534839afc3f8cad7c6a1b3f1da5b"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.5"}},"nbformat":4,"nbformat_minor":1}