{"cells":[{"metadata":{"_uuid":"d8c4c2ac0812146353bc464987e0887fda5a9d63"},"cell_type":"markdown","source":"** Costa Rica Household Poverty Level Forecast **\n\nThe Inter-American Development Bank asked the Kaggle community to help some of the most impoverished families in the world.\n\nThe detailed problem description is available at Official Website: \n https://www.kaggle.com/c/costa-rican-household-poverty-prediction\n\n\nDirectly speaking, we need to develop a model according to people's family conditions, ceilings in the home, the living environment, and other available information to predict the level of poverty.\n\n\nThis task is a Kaggle kernel competition, so the code must be submitted through the kernel instead of the CSV prediction results. \n\nThis article provides less explanation on outlier processing, but contains the complete submission process:\n\n1 Data exploration and data preprocessing\n1.1 Review of the questions\n1.2 Exploratory data analysis and outlier processing\n\n2 Feature engineering\n2.1 New Feature Construction\n2.2 Synthesis of individual-level features and those of family-level\n\n3 Model construction and tuning\n3.1 Use and integration of LightGBM\n\n\nThere are in total 142 features, and please check their meanings when necessary:\nhttps://www.kaggle.com/c/costa-rican-household-poverty-prediction/data\n\nAnd here is a helpful basic walkthrough before you get your hands dirty:\nhttps://www.kaggle.com/willkoehrsen/a-complete-introduction-and-walkthrough\n\nIn this demo, a simple LightGBM can achieve accuracy that ranks around No.10. I did not primarily deal with outliers or tune parameters. **Applied machine learning is basically feature engineering! Spend more time understanding the features and their connections.**\n\nRegarding the following questions, I have referred to some related papers to understand them better.\n1. How to deal with the imbalanced data.\n2. How learning rate decay affects LightGBM training."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Data manipulation\nimport pandas as pd\nimport numpy as np\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set a few plotting defaults\n%matplotlib inline\nplt.style.use('fivethirtyeight')\nplt.rcParams['font.size'] = 18\nplt.rcParams['patch.edgecolor'] = 'k'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d96032f126b2a2eb7a5c600b54085e08e4400243"},"cell_type":"markdown","source":"** Read in data **\nTraining data has 9557 rows and 143 columns: 142 features, plus a target column\n\nTest data has 23856 rows.\n\nTarget column: classification of personal poverty, with 4 levels\n1 = extreme\n2 = moderate\n3 = vulnerable\n4 = not vulnerable\n\nThe accuracy evaluation index is Macro F1 : (F1 Class 1 + F1 Class 2 + F1 Class 3 + F1 Class 4) / 4"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"pd.options.display.max_columns = 150\n\n# Read in data\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4f315c4d1943facdd8997f286e49f23502f7fd7c"},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0aa8f269a05b27b888fcd96e88908c29e80a4bbc"},"cell_type":"code","source":"from collections import OrderedDict","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d6c3bb2a052485a9157aa036d3b19771c9872628"},"cell_type":"markdown","source":"** Exploratory data analysis **\nThis data set is relatively complete. I show some graphs representing the relationship between some features and targets.  \nIdeally, a informatic feature should be able to seprate four leverls of the target. \n\nFeature examples\nV2a1: Monthly rent\nV18q1: Number of tables owned by the family"},{"metadata":{"trusted":true,"_uuid":"b77416e6186ff18a6d759dc0fc2d4feb24426490"},"cell_type":"code","source":"plt.figure(figsize = (20, 12))\nplt.style.use('fivethirtyeight')\n\n# Color mapping\ncolors = OrderedDict({1: 'red', 2: 'orange', 3: 'blue', 4: 'green'})\npoverty_mapping = OrderedDict({1: 'extreme', 2: 'moderate', 3: 'vulnerable', 4: 'non vulnerable'})\n\nfor i,col in enumerate(train.select_dtypes('float')):\n    ax=plt.subplot(4,2,i+1)\n    for poverty_level,color in colors.items():\n        sns.kdeplot(train.loc[train['Target'] == poverty_level, col].dropna(), \n                    ax = ax, color = color, label = poverty_mapping[poverty_level])\n        plt.title(f'{col.capitalize()} Distribution'); plt.xlabel(f'{col}'); plt.ylabel('Density')\n        \nplt.subplots_adjust(top = 2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"731f9dad6356d8637a15754cc7aae92e34115a7f"},"cell_type":"markdown","source":"Checking the data type in the data set, I found that the continues and categorical variables are mixed in some columns and thus those variables need further processing. The explanation of several characteristics is as follows:\n\ndependency: Calculate dependency ratio = (Number of family members under 19 or over 64) / (Number of family members between 19 and 64)\n\nedjefe: years of education of male heads of household, based on escolari (years of education), head of household and gender, yes = 1, no = 0\n\nedjefa: years of education of female heads of household, based on escolari (years of education), head of household and gender, yes = 1, no = 0"},{"metadata":{"trusted":true,"_uuid":"25b737c0f20c8b68f55467d3fb501f4c400c64f2"},"cell_type":"code","source":"train.select_dtypes('object').head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"efc6bf0a266371c4174af87213aa0090a438565f"},"cell_type":"markdown","source":"Replace yes no with 1 and 0"},{"metadata":{"trusted":true,"_uuid":"f33b086abf05b5dfdf6b67ef9bf3fa2849ae5f92"},"cell_type":"code","source":"map={'yes':1,\"no\":0}\nfor df in [train,test]:\n    df['dependency']=df['dependency'].replace(map).astype(np.float64)\n    df['edjefe']=df['edjefe'].replace(map).astype(np.float64)\n    df['edjefa']=df['edjefa'].replace(map).astype(np.float64)\ntrain[['dependency','edjefe','edjefa']].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"44352b3b343099d3dba1d41b4cb5637eaf717998"},"cell_type":"markdown","source":"Distributions"},{"metadata":{"trusted":true,"_uuid":"dd96985d49ffee734dbbb3ebbb01440f65b4458d"},"cell_type":"code","source":"plt.figure(figsize = (20, 16))\nplt.style.use('ggplot')\n\n# Color mapping\ncolors = OrderedDict({1: 'red', 2: 'orange', 3: 'blue', 4: 'green'})\npoverty_mapping = OrderedDict({1: 'extreme', 2: 'moderate', 3: 'vulnerable', 4: 'non vulnerable'})\n\nfor i,col in enumerate(['dependency','edjefe','edjefa']):\n    ax=plt.subplot(3,1,i+1)\n    for poverty_level,color in colors.items():\n        sns.kdeplot(train.loc[train['Target'] == poverty_level, col].dropna(), \n                    ax = ax, color = color, label = poverty_mapping[poverty_level])\n        plt.title(f'{col.capitalize()} Distribution'); plt.xlabel(f'{col}'); plt.ylabel('Density')\n        \n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ae3d082e4de4403c4d121e96d834dc99eb2b8570"},"cell_type":"markdown","source":"Combine test and train data"},{"metadata":{"trusted":true,"_uuid":"e7ab1a6f2050de1aad13d34451929c599888fc6d"},"cell_type":"code","source":"test['Target']=np.nan\ndata=train.append(test,ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b41ebc657e8e5c1b1f938ae92f6775d8a04af9fe"},"cell_type":"markdown","source":"Among the four levels of poverty, there are very few \"extremely poor\" people, and the number of samples in the four categories is severely uneven. In this way, machine learning models rarely have the chance to learn from samples belonging to \"extremely poor\" level, resulting in inaccurate predictions.\nTwo common ways to deal with imbalanced datasets are:\n1. Bootstrapping: Generate new samples to balance the dataset.\n2. Weight the samples, interfering optimization process.\nIn this problem, I utilize the latter strategy.\n\nThe original idea comes from  \"Logistic Regression in Rare Events Data, King, Zen, 2001.\", a work proposed logistic regression correction under imbalanced samples. The author re-designs the process of the maximum likelihood estimation so that it works for imbalanced data using two tricks. \n\n1. Prior Correction. Prior correction is to modify the value of the regression coefficient directly. The magnitude of the correction is directly related to the proportion class.\n\n2. Weighting. In the estimation, the category with fewer samples has more significant weight, contributing more to the total error. \n\n\nFor LigthGBM, this is how weighting works:\nhttps://stackoverflow.com/questions/34389624/what-does-sample-weight-do-to-the-way-a-decisiontreeclassifier-works-in-skle"},{"metadata":{"trusted":true,"_uuid":"13b8b4ca27e2f021eab2f2bb144685f374247d26"},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier as DTC\n\nX = [[0],[1],[2]] # 3 simple training examples\nY = [ 1,  2,  1 ] # class labels\n\ndtc = DTC(max_depth=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d4b1f11302cb712e90ef4af358c671c643e10968"},"cell_type":"code","source":"#no weighting\ndtc.fit(X,Y)\n\nprint (dtc.tree_.threshold)\n# [0.5, -2, -2]\n#gini-index\nprint ( dtc.tree_.impurity)\n# [0.44444444, 0, 0.5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"012d1369936036e7fe0a94550aaea21bf68057df"},"cell_type":"code","source":"#weigthing the samples\ndtc.fit(X,Y,sample_weight=[1,2,3])\n\nprint (dtc.tree_.threshold)\n# [1.5, -2, -2]\nprint (dtc.tree_.impurity)\n# [0.44444444, 0.44444444, 0.]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"357ed14273d50a7e55536220eae355a5c9e688a1"},"cell_type":"markdown","source":"It can be found that after the sample weight is added, the tree splitting standard and the leaf node purity will change accordingly. In this way, the estimation process is changed."},{"metadata":{"trusted":true,"_uuid":"4cacc12fd116fbd36220a660a85b323dca278a0e"},"cell_type":"code","source":"data['Target'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"45625e94162fac5078fd85de625185eb13bd8755"},"cell_type":"markdown","source":"Since the test data calculates accuracy only based on head of household, in the trainning we only look at the poverty level of the head of household (['parentesco1'] == 1).\nAll heads of households are directly extracted as the training set, and there are 2973 heads of households."},{"metadata":{"trusted":true,"_uuid":"ce0b08d3c828914703b180c46dd93ff2fe9efb10"},"cell_type":"code","source":"heads=data.loc[data['parentesco1'] == 1].copy()\ntrain_labels=data.loc[(data['Target'].notnull())&(data['parentesco1']==1),['Target','idhogar']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c2c5746c59576b6c67e198a7dd5e4b423c826023"},"cell_type":"code","source":"len(train_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cf95478bc40923bf3d203e2c02bd136152d8a1df"},"cell_type":"code","source":"type_counts=train_labels['Target'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7d539c71fed304567afdc73bef31d5eac5ce1606"},"cell_type":"code","source":"type_counts=pd.DataFrame(type_counts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"47a7b63f11eb2f4bc0528a79db5e6585b20070a4"},"cell_type":"code","source":"type_counts['level']=type_counts.index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0849312073f8266dfc4a660f442db83fca212c39"},"cell_type":"code","source":"type_counts['level']=type_counts['level'].replace(poverty_mapping)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"203372d150fad92d52dd5044471fae38dd8e7607"},"cell_type":"code","source":"type_counts['Count']=type_counts['Target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"18504a5f9b829fafa02657a613e6a97651c8dcc0"},"cell_type":"code","source":"ax = sns.barplot(x=\"level\", y=\"Count\", data=type_counts)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b5c0a5b77429f676c033eacca32b15cc44a36a86"},"cell_type":"markdown","source":"The description tells that each family can only have one poverty level, so we need to know whether some families were labeled more than one level."},{"metadata":{"trusted":true,"_uuid":"e73c37dac1db8b8a3ab1af6e3b94f795c2f19b19"},"cell_type":"code","source":"all_equal=train.groupby('idhogar')['Target'].apply(lambda x: x.nunique()==1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"75259fc470484a468ebe73fd1388adcb2cac7791"},"cell_type":"code","source":"print(len(all_equal))\nall_equal.head()\ntype(all_equal)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bbeeb382e1253ca89125c01d071dbc57f276446e"},"cell_type":"code","source":"not_equal= all_equal[all_equal!=True]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c2618b9e53f12b90495292578b4d8ee9192825b0"},"cell_type":"code","source":"not_equal.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2ea98faadce2cf7b04a26942c3618454e8784383"},"cell_type":"code","source":"example=train.loc[train['idhogar']==not_equal.index[0],['idhogar', 'parentesco1', 'Target']]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e78bd5f56f5ef81a3ecc05e54d566a4c345309ce"},"cell_type":"markdown","source":"And here is an example of data that we do not expect"},{"metadata":{"trusted":true,"_uuid":"0a90b9ce0447b5e49c1c900603ccbb80f7356c31"},"cell_type":"code","source":"example","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"88230e8f6c275d67bf60cc62a8099d485907c62d"},"cell_type":"code","source":"households_leader=train.groupby('idhogar')['parentesco1'].sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0784cb5b4646d318cadda8bbb5e0f98e8c843e0e"},"cell_type":"code","source":"households_leader.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"451d781fc41b871f3a84bd1f4c895471f4b09916"},"cell_type":"markdown","source":"Some are some families that have more than 1 leader ;D"},{"metadata":{"trusted":true,"_uuid":"5f744ed9bf0d5bc647418e965a6068ecb5914a33"},"cell_type":"code","source":"households_leader_flase=train.loc[train['idhogar'].isin(households_leader\n                                                       [households_leader !=1].index),:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"074f09d1aea292647f87ca05c31fe67dbb467eda"},"cell_type":"code","source":"households_leader_flase[['idhogar', 'parentesco1', 'Target']]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dda544fcea8e072832b5ea5800a7061307730a4c"},"cell_type":"markdown","source":"If a household has more than one level of poverty, the poverty level of the head of household is considered to be accurate, and poverty levels of others are modified so that they are consistent with that of the head"},{"metadata":{"trusted":true,"_uuid":"52ae2e8d4e3f6f984a36c10b8445021a311d7920"},"cell_type":"code","source":"for household in not_equal.index:\n    true_target= int(train[(train['idhogar']==household) & \n                          (train['parentesco1']==1.0)]['Target'])\n    train.loc[train['idhogar']==household, 'Target']=true_target","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1df9d2853031ff4596e3d4daba2369e28323a603"},"cell_type":"markdown","source":"Next, handle missing values and outliers"},{"metadata":{"trusted":true,"_uuid":"560190aff714862c66d4f04fcc00751d8d23d4b0"},"cell_type":"code","source":"missing = pd.DataFrame(data.isnull().sum()).rename(columns={0:'total'})\nmissing['Percent']=missing['total']/len(data)\nmissing.sort_values('Percent',ascending=False).head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c8f57e3b32a8929d6e06af5765ee1be7fb7fcc3e"},"cell_type":"markdown","source":""},{"metadata":{"trusted":true,"_uuid":"833302e68395b69fbc0913a2631cba7c1c3343ae"},"cell_type":"code","source":"heads=data.loc[data['parentesco1']==1].copy()\nplt.figure(figsize=(8,6))\nheads['v18q1'].value_counts().sort_index().plot.bar()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f03d391eda60f2ea62d879c9f750b6f901d03164"},"cell_type":"code","source":"heads.groupby('v18q')['v18q1'].apply(lambda x:x.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fdf0ee89b419746e340120182871ee3f0e17c527"},"cell_type":"code","source":"data['v18q1']=data['v18q1'].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2c4817ca23e4613d1ee7ba8b160bab6b1274480c"},"cell_type":"markdown","source":"Handle v2a1 based on rent / buying status (tipoxxxx)"},{"metadata":{"trusted":true,"_uuid":"19c853c286688be92c0103868885e9bd17a0dbc9"},"cell_type":"code","source":"own_variables=[x for x in data if x.startswith('tipo')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"73f8065bed5b10bb7a2fe443f033612d31e0e30d"},"cell_type":"code","source":"own_variables","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f7e89785ec16f67b697a909dac0752d21129f5e3"},"cell_type":"markdown","source":"Meanings of those features:\ntipovivi1 = 1, finish paying your own house\ntipovivi2 = 1, during payment\ntipovivi3 = 1, rented\ntipovivi4 = 1, unstable\ntipovivi5 = 1, other allocations, borrowed"},{"metadata":{"trusted":true,"_uuid":"f18dd9ffc320ffe3201b6852de64269128aa9033"},"cell_type":"code","source":"data.loc[data['v2a1'].isnull(),own_variables].sum().plot.bar()\nplt.xticks([0, 1, 2, 3, 4],\n           ['Owns and Paid Off', 'Owns and Paying', 'Rented', 'Precarious', 'Other'],\n          rotation = 60)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e5fce4aa9640773c491cd4235f886e9f32ad6ae3"},"cell_type":"code","source":"owns=pd.DataFrame(data.loc[data['tipovivi3']==1,'Target'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"efd941608ee936efbea326a62936932977fd6fe8"},"cell_type":"code","source":"owns['Target'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3a4ed210efdb37762b05804d77fad6aa808bf53a"},"cell_type":"markdown","source":"So an nature idea is that families what have already bought a house will not need to pay rent. Following this idea, modify some of missing values in v2a1"},{"metadata":{"trusted":true,"_uuid":"a56e47bc76394a3a623b466d8cfa6ef7ad7c3669"},"cell_type":"code","source":"# Fill in households that own the house with 0 rent payment\ndata.loc[(data['tipovivi1'] == 1), 'v2a1'] = 0\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d30c6824483b5c08b770cbcd37e196bfa573f58e"},"cell_type":"markdown","source":"For the case where v2a1 is still missing, I add a column called ‘v2a1-missing’ to the dataset to indicate whether v2a1 is vacant for each row."},{"metadata":{"trusted":true,"_uuid":"d0b76d361c4f1e4cfa8bafb4b903fab9625c058f"},"cell_type":"code","source":"\n# Create missing rent payment column\ndata['v2a1-missing'] = data['v2a1'].isnull()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f27757351157fd99c28f6b74651bdf9ffb0c99d7"},"cell_type":"code","source":"data['v2a1-missing'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"47a6b66a8b0646dae4351a8ad80dba3af3c6823a"},"cell_type":"markdown","source":"According to discussions of others, tipovivi3 is used here to fill the remaining missing values"},{"metadata":{"trusted":true,"_uuid":"eaafb1a9f8126cd8e9b2c92d15d6689df02cc71c"},"cell_type":"code","source":"data['v2a1'] = data['v2a1'].fillna(value=data['tipovivi3'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"97c93c91d4a6d52bf69699a07afd1ac36b9a5a2a"},"cell_type":"code","source":"data['v2a1'].head(20)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"38092c158ab3fefb8cc1a647ee186e7976f1cb8b"},"cell_type":"markdown","source":"\nThen handle the rez_esc vacancy. This feature represents the number of years behind school. So it is only meaningful for people aged 7-19. \nThen, for people who exceed this range, I add 0 and also a column called rez_esc-missing to indicate whether it is still missing."},{"metadata":{"trusted":true,"_uuid":"bffeae1091d0f9937da6c8b2ca6f17593404b7dd"},"cell_type":"code","source":"data.loc[((data['age']>19) | (data['age']<7)) & (data['rez_esc'].isnull()), 'rez_esc']=0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c099efbb5d9e2d0e407c741ada070fe00b2dbecb"},"cell_type":"code","source":"data['rez_esc-missing'] = data['rez_esc'].isnull()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"69fce08fda493a1d58e12012c2d8b5349ee02433"},"cell_type":"code","source":"data['rez_esc']=data['rez_esc'].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6ec8d0a233d6f7b019ae9435dd4dbcaf532a090e"},"cell_type":"markdown","source":"There is another guy who is 97 years behind school. This is funny. "},{"metadata":{"trusted":true,"_uuid":"3530e72db18ba1cc422eecffb802ab72a03052cd"},"cell_type":"code","source":"data['rez_esc'].plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a663e6d8747ab2786b5baf4c3e036c8396236480"},"cell_type":"code","source":"data.loc[data['rez_esc'] > 5, 'rez_esc'] = 5","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8239c3d41c5622b9b2e96703a22301f29f9188d6"},"cell_type":"markdown","source":"After a preliminary understanding of the data and the processing of missing values and outliers, I can now do feature engineering.\n First, clarify the type and category of the variable:\n\nTypes of:\nbool (true / false), ordered (hierarchical), cout (continuous)\n\ncategory:\nind (for individuals), hh (for families)\n\nFor individual-level variables, family-level synthesis can be done, such as calculating the average income, highest education, age range, etc. for all people in a family.\n\nBesides, id and sqr are the ordinal class and the square class of the known variable, which does not require much processing.\n\nI do feature engineering to\n1. Create new variables, such as the size of the house in the household / total number of people, reflecting the degree of congestion\n2. Individual and family-level variables synthesis\n\nThe specific meaning of the feature can be found on the official website. Many new features here are shared by Kaggle's discussion and posts from others.\n\n**"},{"metadata":{"trusted":true,"_uuid":"0d4cbbebd49cc897bbcec9660871b3a133890218"},"cell_type":"code","source":"id_ = ['Id', 'idhogar', 'Target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cb772a47a981999e2a5bac8ac4fdb3afdcba5a61"},"cell_type":"code","source":"ind_bool = ['v18q', 'dis', 'male', 'female', 'estadocivil1', 'estadocivil2', 'estadocivil3', \n            'estadocivil4', 'estadocivil5', 'estadocivil6', 'estadocivil7', \n            'parentesco1', 'parentesco2',  'parentesco3', 'parentesco4', 'parentesco5', \n            'parentesco6', 'parentesco7', 'parentesco8',  'parentesco9', 'parentesco10', \n            'parentesco11', 'parentesco12', 'instlevel1', 'instlevel2', 'instlevel3', \n            'instlevel4', 'instlevel5', 'instlevel6', 'instlevel7', 'instlevel8', \n            'instlevel9', 'mobilephone', 'rez_esc-missing']\n\nind_ordered = ['rez_esc', 'escolari', 'age']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a10e7967bfa8ef96418c96ea106afd45fd793499"},"cell_type":"code","source":"hh_bool = ['hacdor', 'hacapo', 'v14a', 'refrig', 'paredblolad', 'paredzocalo', \n           'paredpreb','pisocemento', 'pareddes', 'paredmad',\n           'paredzinc', 'paredfibras', 'paredother', 'pisomoscer', 'pisoother', \n           'pisonatur', 'pisonotiene', 'pisomadera',\n           'techozinc', 'techoentrepiso', 'techocane', 'techootro', 'cielorazo', \n           'abastaguadentro', 'abastaguafuera', 'abastaguano',\n            'public', 'planpri', 'noelec', 'coopele', 'sanitario1', \n           'sanitario2', 'sanitario3', 'sanitario5',   'sanitario6',\n           'energcocinar1', 'energcocinar2', 'energcocinar3', 'energcocinar4', \n           'elimbasu1', 'elimbasu2', 'elimbasu3', 'elimbasu4', \n           'elimbasu5', 'elimbasu6', 'epared1', 'epared2', 'epared3',\n           'etecho1', 'etecho2', 'etecho3', 'eviv1', 'eviv2', 'eviv3', \n           'tipovivi1', 'tipovivi2', 'tipovivi3', 'tipovivi4', 'tipovivi5', \n           'computer', 'television', 'lugar1', 'lugar2', 'lugar3',\n           'lugar4', 'lugar5', 'lugar6', 'area1', 'area2', 'v2a1-missing']\n\nhh_ordered = [ 'rooms', 'r4h1', 'r4h2', 'r4h3', 'r4m1','r4m2','r4m3', 'r4t1',  'r4t2', \n              'r4t3', 'v18q1', 'tamhog','tamviv','hhsize','hogar_nin',\n              'hogar_adul','hogar_mayor','hogar_total',  'bedrooms', 'qmobilephone']\n\nhh_cont = ['v2a1', 'dependency', 'edjefe', 'edjefa', 'meaneduc', 'overcrowding']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"747203347daccb3ef5abd5404c3d466993fec2d2"},"cell_type":"code","source":"sqr_ = ['SQBescolari', 'SQBage', 'SQBhogar_total', 'SQBedjefe', \n        'SQBhogar_nin', 'SQBovercrowding', 'SQBdependency', 'SQBmeaned', 'agesq']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ee4c927a1a26bf7d5eb276c4fc9f165a3384eff9"},"cell_type":"code","source":"heads = data.loc[data['parentesco1'] == 1, :]\nheads = heads[id_ + hh_bool + hh_cont + hh_ordered]\nheads.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"938b6a0ec00e30d6b03a8eb0aa2d1d3d174dd243"},"cell_type":"markdown","source":"For example, counting the number of electronic products in households."},{"metadata":{"trusted":true,"_uuid":"c2b39d56fd5bf35d3f14f8ae4e693a847866342a"},"cell_type":"code","source":"elec = []\n\n# Assign values\nfor i, row in heads.iterrows():\n    if row['noelec'] == 1:\n        elec.append(0)\n    elif row['coopele'] == 1:\n        elec.append(1)\n    elif row['public'] == 1:\n        elec.append(2)\n    elif row['planpri'] == 1:\n        elec.append(3)\n    else:\n        elec.append(np.nan)\n        \n# Record the new variable and missing flag\nheads['elec'] = elec\nheads['elec-missing'] = heads['elec'].isnull()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a251e65d9c94334a1f8e5d0ee1d71e4a56d8ab2c"},"cell_type":"code","source":"heads = heads.drop(columns = 'area2')\n\nheads.groupby('area1')['Target'].value_counts(normalize = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8f58abb84fe2d59d9d431a1c712ac0adf1b1f83b"},"cell_type":"code","source":"# Wall ordinal variable\nheads['walls'] = np.argmax(np.array(heads[['epared1', 'epared2', 'epared3']]),\n                           axis = 1)\n\n# heads = heads.drop(columns = ['epared1', 'epared2', 'epared3'])\n#plot_categoricals('walls', 'Target', heads)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d6bcc550fc11a33f0f4cca8f0666ddc9f71f36dd"},"cell_type":"code","source":"heads['epared2'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2ad2ebaeb3c8b9c78e41b53ebd0322423d87edce"},"cell_type":"code","source":"# Roof ordinal variable\nheads['roof'] = np.argmax(np.array(heads[['etecho1', 'etecho2', 'etecho3']]),\n                           axis = 1)\n#heads = heads.drop(columns = ['etecho1', 'etecho2', 'etecho3'])\n\n# Floor ordinal variable\nheads['floor'] = np.argmax(np.array(heads[['eviv1', 'eviv2', 'eviv3']]),\n                           axis = 1)\n# heads = heads.drop(columns = ['eviv1', 'eviv2', 'eviv3'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"87b04260a00a67c5de847b6ba9d10eeba3189b01"},"cell_type":"code","source":"# Create new feature\nheads['walls+roof+floor'] = heads['walls'] + heads['roof'] + heads['floor']\n\n#plot_categoricals('walls+roof+floor', 'Target', heads, annotate=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8b0477e25f437a1723f4e45c09d563ca20b743bb"},"cell_type":"code","source":"# No toilet, no electricity, no floor, no water service, no ceiling\nheads['warning'] = 1 * (heads['sanitario1'] + \n                         (heads['elec'] == 0) + \n                         heads['pisonotiene'] + \n                         heads['abastaguano'] + \n                         (heads['cielorazo'] == 0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4c37aa96a22aa7b37380a6e3047f7504f6e4ffaf"},"cell_type":"code","source":"# Owns a refrigerator, computer, tablet, and television\nheads['bonus'] = 1 * (heads['refrig'] + \n                      heads['computer'] + \n                      (heads['v18q1'] > 0) + \n                      heads['television'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0b2407728720c28e10af136bbfd5f6ab8eee0fc1"},"cell_type":"code","source":"heads['phones-per-capita'] = heads['qmobilephone'] / heads['tamviv']\nheads['tablets-per-capita'] = heads['v18q1'] / heads['tamviv']\nheads['rooms-per-capita'] = heads['rooms'] / heads['tamviv']\nheads['rent-per-capita'] = heads['v2a1'] / heads['tamviv']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b616cc924c69286a2374e7bbc4a3c7943a900cb2"},"cell_type":"code","source":"#feature from other notebook\nheads.loc[(heads.v14a ==  1) & (heads.sanitario1 ==  1) & (heads.abastaguano == 0), \"v14a\"] = 0\nheads.loc[(heads.v14a ==  1) & (heads.sanitario1 ==  1) & (heads.abastaguano == 0), \"sanitario1\"] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"61c5f31fbc6e2dd4af3d5e161a0c3ac813ded9b8"},"cell_type":"code","source":"ind = data[id_ + ind_bool + ind_ordered]\nind.shape\nind['escolari/age'] = ind['escolari'] / ind['age']\n\nplt.figure(figsize = (10, 8))\nsns.violinplot('Target', 'escolari/age', data = ind);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"610fe0db0507173e69fc34d25d9da0f8dfa079f4"},"cell_type":"markdown","source":""},{"metadata":{"trusted":true,"_uuid":"b01b644a66377fed8159e7b81891b694dc91debc"},"cell_type":"code","source":"# Define custom function\nrange_ = lambda x: x.max() - x.min()\nrange_.__name__ = 'range_'\n\n# Group and aggregate\nind_agg = ind.drop(columns = 'Target').groupby('idhogar').agg(['min', 'max', 'sum', 'count', 'std', range_])\nind_agg.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a5eea42bb998d878bdb1a03478fcb17a6a298ac4"},"cell_type":"code","source":"# Rename the columns\nnew_col = []\nfor c in ind_agg.columns.levels[0]:\n    for stat in ind_agg.columns.levels[1]:\n        new_col.append(f'{c}-{stat}')\n        \nind_agg.columns = new_col\nind_agg.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9d717c0c6106102117610ca0b2e3b3bc89e3c290"},"cell_type":"code","source":"ind_feats = list(ind_agg.columns)\n\n# Merge on the household id\nfinal = heads.merge(ind_agg, on = 'idhogar', how = 'left')\n\nprint('Final features shape: ', final.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b33ec4e4963030165beeed3da523deb9ee32e5db"},"cell_type":"markdown","source":""},{"metadata":{"trusted":true,"_uuid":"01a1a26b48b628fc28689dc6942ca154fd825da7"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score, make_scorer\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.pipeline import Pipeline\n\n# Custom scorer for cross validation\nscorer = make_scorer(f1_score, greater_is_better=True, average = 'macro')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5bf78576fe1b6335d0757eeb8e057fc0b16ac251"},"cell_type":"code","source":"# Labels for training\ntrain_labels = np.array(list(final[final['Target'].notnull()]['Target'].astype(np.uint8)))\n\n# Extract the training data\ntrain_set = final[final['Target'].notnull()].drop(columns = ['Id', 'Target'])\ntest_set = final[final['Target'].isnull()].drop(columns = ['Id',  'Target'])\n\n# Submission base which is used for making submissions to the competition\nsubmission_base = test[['Id', 'idhogar']].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d800d80bf4eff44c7b989ad9d3d2911922e77ced"},"cell_type":"code","source":"train_set['adult'] = train_set['hogar_adul'] - train_set['hogar_mayor']\ntrain_set['dependency_count'] = train_set['hogar_nin'] + train_set['hogar_mayor']\ntrain_set['dependency'] = train_set['dependency_count'] / train_set['adult']\ntrain_set['child_percent'] = train_set['hogar_nin']/train_set['hogar_total']\ntrain_set['elder_percent'] = train_set['hogar_mayor']/train_set['hogar_total']\ntrain_set['adult_percent'] = train_set['hogar_adul']/train_set['hogar_total']\ntest_set['adult'] = test_set['hogar_adul'] - test_set['hogar_mayor']\ntest_set['dependency_count'] = test_set['hogar_nin'] + test_set['hogar_mayor']\ntest_set['dependency'] = test_set['dependency_count'] / test_set['adult']\ntest_set['child_percent'] = test_set['hogar_nin']/test_set['hogar_total']\ntest_set['elder_percent'] = test_set['hogar_mayor']/test_set['hogar_total']\ntest_set['adult_percent'] = test_set['hogar_adul']/test_set['hogar_total']\n\ntrain_set['rent_per_adult'] = train_set['v2a1']/train_set['hogar_adul']\ntrain_set['rent_per_person'] = train_set['v2a1']/train_set['hhsize']\ntest_set['rent_per_adult'] = test_set['v2a1']/test_set['hogar_adul']\ntest_set['rent_per_person'] = test_set['v2a1']/test_set['hhsize']\n\ntrain_set['overcrowding_room_and_bedroom'] = (train_set['hacdor'] + train_set['hacapo'])/2\ntest_set['overcrowding_room_and_bedroom'] = (test_set['hacdor'] + test_set['hacapo'])/2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"14972540f68ce0ca0792af2fbc5f6af79e33c1bd"},"cell_type":"code","source":"train_set['r4h1_percent_in_male'] = train_set['r4h1'] / train_set['r4h3']\ntrain_set['r4m1_percent_in_female'] = train_set['r4m1'] / train_set['r4m3']\ntrain_set['r4h1_percent_in_total'] = train_set['r4h1'] / train_set['hhsize']\ntrain_set['r4m1_percent_in_total'] = train_set['r4m1'] / train_set['hhsize']\ntrain_set['r4t1_percent_in_total'] = train_set['r4t1'] / train_set['hhsize']\ntest_set['r4h1_percent_in_male'] = test_set['r4h1'] / test_set['r4h3']\ntest_set['r4m1_percent_in_female'] = test_set['r4m1'] / test_set['r4m3']\ntest_set['r4h1_percent_in_total'] = test_set['r4h1'] / test_set['hhsize']\ntest_set['r4m1_percent_in_total'] = test_set['r4m1'] / test_set['hhsize']\ntest_set['r4t1_percent_in_total'] = test_set['r4t1'] / test_set['hhsize']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"07a2a61dbd58495b8c006b51f76bf3730c75cd1b"},"cell_type":"code","source":"\ntrain_set['rent_per_room'] = train_set['v2a1']/train_set['rooms']\ntrain_set['bedroom_per_room'] = train_set['bedrooms']/train_set['rooms']\ntrain_set['elder_per_room'] = train_set['hogar_mayor']/train_set['rooms']\ntrain_set['adults_per_room'] = train_set['adult']/train_set['rooms']\ntrain_set['child_per_room'] = train_set['hogar_nin']/train_set['rooms']\ntrain_set['male_per_room'] = train_set['r4h3']/train_set['rooms']\ntrain_set['female_per_room'] = train_set['r4m3']/train_set['rooms']\ntrain_set['room_per_person_household'] = train_set['hhsize']/train_set['rooms']\n\ntest_set['rent_per_room'] = test_set['v2a1']/test_set['rooms']\ntest_set['bedroom_per_room'] = test_set['bedrooms']/test_set['rooms']\ntest_set['elder_per_room'] = test_set['hogar_mayor']/test_set['rooms']\ntest_set['adults_per_room'] = test_set['adult']/test_set['rooms']\ntest_set['child_per_room'] = test_set['hogar_nin']/test_set['rooms']\ntest_set['male_per_room'] = test_set['r4h3']/test_set['rooms']\ntest_set['female_per_room'] = test_set['r4m3']/test_set['rooms']\ntest_set['room_per_person_household'] = test_set['hhsize']/test_set['rooms']\n\ntrain_set['rent_per_bedroom'] = train_set['v2a1']/train_set['bedrooms']\ntrain_set['edler_per_bedroom'] = train_set['hogar_mayor']/train_set['bedrooms']\ntrain_set['adults_per_bedroom'] = train_set['adult']/train_set['bedrooms']\ntrain_set['child_per_bedroom'] = train_set['hogar_nin']/train_set['bedrooms']\ntrain_set['male_per_bedroom'] = train_set['r4h3']/train_set['bedrooms']\ntrain_set['female_per_bedroom'] = train_set['r4m3']/train_set['bedrooms']\ntrain_set['bedrooms_per_person_household'] = train_set['hhsize']/train_set['bedrooms']\n\ntest_set['rent_per_bedroom'] = test_set['v2a1']/test_set['bedrooms']\ntest_set['edler_per_bedroom'] = test_set['hogar_mayor']/test_set['bedrooms']\ntest_set['adults_per_bedroom'] = test_set['adult']/test_set['bedrooms']\ntest_set['child_per_bedroom'] = test_set['hogar_nin']/test_set['bedrooms']\ntest_set['male_per_bedroom'] = test_set['r4h3']/test_set['bedrooms']\ntest_set['female_per_bedroom'] = test_set['r4m3']/test_set['bedrooms']\ntest_set['bedrooms_per_person_household'] = test_set['hhsize']/test_set['bedrooms']\n\ntrain_set['tablet_per_person_household'] = train_set['v18q1']/train_set['hhsize']\ntrain_set['phone_per_person_household'] = train_set['qmobilephone']/train_set['hhsize']\ntest_set['tablet_per_person_household'] = test_set['v18q1']/test_set['hhsize']\ntest_set['phone_per_person_household'] = test_set['qmobilephone']/test_set['hhsize']\n\ntrain_set['age_12_19'] = train_set['hogar_nin'] - train_set['r4t1']\ntest_set['age_12_19'] = test_set['hogar_nin'] - test_set['r4t1']    \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"44f20f3c7783b86ac89aabff5f7aa4a38df5ca62"},"cell_type":"code","source":"train_set['num_over_18'] = 0\ntrain_set['num_over_18'] = train_set[train.age >= 18].groupby('idhogar').transform(\"count\")\ntrain_set['num_over_18'] = train_set.groupby(\"idhogar\")[\"num_over_18\"].transform(\"max\")\ntrain_set['num_over_18'] = train_set['num_over_18'].fillna(0)\n\ntest_set['num_over_18'] = 0\ntest_set['num_over_18'] = test_set[test.age >= 18].groupby('idhogar').transform(\"count\")\ntest_set['num_over_18'] = test_set.groupby(\"idhogar\")[\"num_over_18\"].transform(\"max\")\ntest_set['num_over_18'] = test_set['num_over_18'].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3cd5dfb1b287efd818218a53d373a702e62e626a"},"cell_type":"code","source":"train_set=train_set.drop(columns='idhogar')\ntest_set=test_set.drop(columns='idhogar')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f9b4bad8704621a4de8fc7119f5e5f50061abf2b"},"cell_type":"markdown","source":"After feature engineering, division brought me some inf. Turning those inf into null values."},{"metadata":{"trusted":true,"_uuid":"f42a60309fb6a08d65aa66a9f316982278e86558"},"cell_type":"code","source":"#deal with nan and inf\ntrain_set=train_set.replace([np.inf, -np.inf], np.nan)\ntest_set=test_set.replace([np.inf, -np.inf], np.nan)\ntrain_set.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6ace1644f8f64eb41d67a19404b54adf394ea101"},"cell_type":"code","source":"train_set.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"48dade35899d3d3a7566063afb2a648fc09d6d75"},"cell_type":"code","source":"# drop duplicated columns\nneedless_cols = ['r4t3', 'tamhog', 'tamviv', 'hhsize', 'v14a']\ntrain_set = train_set.drop(needless_cols, axis=1)\ntest_set = test_set.drop(needless_cols, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ebd5a89753b69cdbaf6013575eca0886d370cbb2"},"cell_type":"code","source":"features = list(train_set.columns)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"67b0eaeb725f301f78ac59dfd31d1023e052adb8"},"cell_type":"markdown","source":"I do not have any idea how to deal with other missing values, so, I just set them to 0"},{"metadata":{"trusted":true,"_uuid":"150b4055b8e63f933a39eb58eb02e6d068c8ccf5"},"cell_type":"code","source":"train_set= train_set.fillna(0)\ntest_set= test_set.fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d9b3c35a7ef067251e0465343e35575c55c26cbb"},"cell_type":"code","source":"test_ids = list(final.loc[final['Target'].isnull(), 'idhogar'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8bba985714dffd508a404e6c3469f94a1fc27d1f"},"cell_type":"markdown","source":""},{"metadata":{"trusted":true,"_uuid":"253e341870369167eedc071f66099e138ebde929"},"cell_type":"code","source":"train_set = pd.DataFrame(train_set, columns = features)\n\n# Create correlation matrix\ncorr_matrix = train_set.corr()\n\n# Select upper triangle of correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n\n# Find index of feature columns with correlation greater than 0.95\nto_drop = [column for column in upper.columns if any(abs(upper[column]) > 1)]\n\nto_drop","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"90329060edb96544090be71435f85e9f8d2c3cbc"},"cell_type":"code","source":"train_set = train_set.drop(columns = to_drop)\ntrain_set.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"32c0932ac345de5b8c59a74e391230180bb0414f"},"cell_type":"code","source":"test_set = pd.DataFrame(test_set, columns = features)\ntrain_set, test_set = train_set.align(test_set, axis = 1, join = 'inner')\nfeatures = list(train_set.columns)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"207f07642c7f714ffdd29f4d3bec3d91a129e657"},"cell_type":"markdown","source":"Recursive Feature Elimination with Random Forest is used for feature selections. I do not know whether it will work or not. Only test data will tell."},{"metadata":{"trusted":true,"_uuid":"ba2c8a17844592317cefec52b52c6ca20d5d137c"},"cell_type":"code","source":"from sklearn.feature_selection import RFECV\n\n# Create a model for feature selection\nestimator = RandomForestClassifier(random_state = 10, n_estimators = 100,  n_jobs = -1)\n\n# Create the object\nselector = RFECV(estimator, step = 1, cv = 3, scoring= scorer, n_jobs = -1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e5a60f0952b04d5fbb6056a2a8f4c1712541c2cf"},"cell_type":"code","source":"selector.fit(train_set, train_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3ddade2aa4c3f2ef843ba90183180eba27992166"},"cell_type":"code","source":"plt.plot(selector.grid_scores_);\n\nplt.xlabel('Number of Features'); plt.ylabel('Macro F1 Score'); plt.title('Feature Selection Scores');\nselector.n_features_","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"09a04a0aeb3e2eb0a535c272b07eea8c8117d9eb"},"cell_type":"markdown","source":"We find 105 features, so I will build a model based on this number."},{"metadata":{"trusted":true,"_uuid":"0e3460b7cadfa31f3e6f8b6d44c30edb70f88983"},"cell_type":"code","source":"train_selected = selector.transform(train_set)\ntest_selected = selector.transform(test_set)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7a1fe058b39c0b05c96f90f485674c8d49f876b0"},"cell_type":"code","source":"# Convert back to dataframe\nselected_features = train_set.columns[np.where(selector.ranking_==1)]\ntrain_selected = pd.DataFrame(train_selected, columns = selected_features)\ntest_selected = pd.DataFrame(test_selected, columns = selected_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2fbe1ff01c0e267b399c8e4688059c1300ba12a9"},"cell_type":"code","source":"def macro_f1_score(labels, predictions):\n    # Reshape the predictions as needed\n    predictions = predictions.reshape(len(np.unique(labels)), -1 ).argmax(axis = 0)\n    \n    metric_value = f1_score(labels, predictions, average = 'macro')\n    \n    # Return is name, value, is_higher_better\n    return 'macro_f1', metric_value, True","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8bd31560bf30b5ebbda9b857db65eeba9634f617"},"cell_type":"markdown","source":"\nAdd learning rate decay to GBM algorithm[Stochastic Gradient Boosting Jerome H. Friedman* March 26, 1999 ...]\nTo my knowledge, learning rate decay is popular for deel learning algorithms, but here we do see improvments if it is implemented. "},{"metadata":{"trusted":true,"_uuid":"6cf389d8ce802cc72b23ef0d37edc707d32cc62d"},"cell_type":"code","source":"\n\ndef learning_rate_power_0997(current_iter):\n    base_learning_rate = 0.1\n    min_learning_rate = 0.02\n    lr = base_learning_rate  * np.power(.995, current_iter)\n    return max(lr, min_learning_rate)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e048691af85b1f82e7006b18750eaead0e0c8693"},"cell_type":"markdown","source":"The training set is much smaller, so we need to pay attention to overfitting carefully. \n"},{"metadata":{"trusted":true,"_uuid":"ffc453d75b634c1947fa03bdee70f1724eaf0b75"},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nimport lightgbm as lgb\nfrom IPython.display import display\n\ndef model_gbm(features, labels, test_features, test_ids, \n              nfolds = 5, return_preds = False, hyp = None,random_state_int=101,early_stopping=300):\n    \"\"\"Model using the GBM and cross validation.\n       Trains with early stopping on each fold.\n       Hyperparameters probably need to be tuned.\"\"\"\n    \n    feature_names = list(features.columns)\n\n    # Option for user specified hyperparameters\n    if hyp is not None:\n        # Using early stopping so do not need number of esimators\n        if 'n_estimators' in hyp:\n            del hyp['n_estimators']\n        params = hyp\n    \n    else:\n        # Model hyperparameters\n        params = {\n                  'colsample_bytree': 0.88, \n                  'learning_rate': 0.028, \n                   'min_child_samples': 10, \n                   'num_leaves': 36, \n                  \n                   'subsample': 0.54, \n                   'class_weight': 'balanced'}\n    \n    # Build the model\n    model = lgb.LGBMClassifier(**params, objective = 'multiclass', \n                               n_jobs = 4, n_estimators = 10000,\n                               random_state = 10)\n    \n    # Using stratified kfold cross validation\n    strkfold = StratifiedKFold(n_splits = nfolds, shuffle = True, random_state= random_state_int)\n    \n    # Hold all the predictions from each fold\n    predictions = pd.DataFrame()\n    importances = np.zeros(len(feature_names))\n    \n    # Convert to arrays for indexing\n    features = np.array(features)\n    test_features = np.array(test_features)\n    labels = np.array(labels).reshape((-1 ))\n    \n    valid_scores = []\n    \n    # Iterate through the folds\n    for i, (train_indices, valid_indices) in enumerate(strkfold.split(features, labels)):\n        \n        # Dataframe for fold predictions\n        fold_predictions = pd.DataFrame()\n        \n        # Training and validation data\n        X_train = features[train_indices]\n        X_valid = features[valid_indices]\n        y_train = labels[train_indices]\n        y_valid = labels[valid_indices]\n        \n        # Train with early stopping\n        model.fit(X_train, y_train, early_stopping_rounds =early_stopping, \n                  eval_metric = macro_f1_score,\n                  eval_set = [(X_train, y_train), (X_valid, y_valid)],\n                  eval_names = ['train', 'valid'],\n                  callbacks=[lgb.reset_parameter(learning_rate=learning_rate_power_0997)])\n        \n        # Record the validation fold score\n        valid_scores.append(model.best_score_['valid']['macro_f1'])\n        \n        # Make predictions from the fold as probabilities\n        fold_probabilitites = model.predict_proba(test_features)\n        \n        # Record each prediction for each class as a separate column\n        for j in range(4):\n            fold_predictions[(j + 1)] = fold_probabilitites[:, j]\n            \n        # Add needed information for predictions \n        fold_predictions['idhogar'] = test_ids\n        fold_predictions['fold'] = (i+1)\n        \n        # Add the predictions as new rows to the existing predictions\n        predictions = predictions.append(fold_predictions)\n        \n        # Feature importances\n        importances += model.feature_importances_ / nfolds   \n        \n        # Display fold information\n        display(f'Fold {i + 1}, Validation Score: {round(valid_scores[i], 5)}, Estimators Trained: {model.best_iteration_}')\n\n    # Feature importances dataframe\n    feature_importances = pd.DataFrame({'feature': feature_names,\n                                        'importance': importances})\n    \n    valid_scores = np.array(valid_scores)\n    display(f'{nfolds} cross validation score: {round(valid_scores.mean(), 5)} with std: {round(valid_scores.std(), 5)}.')\n    \n    # If we want to examine predictions don't average over folds\n    if return_preds:\n        predictions['Target'] = predictions[[1, 2, 3, 4]].idxmax(axis = 1)\n        predictions['confidence'] = predictions[[1, 2, 3, 4]].max(axis = 1)\n        return predictions, feature_importances\n    \n    # Average the predictions over folds\n    predictions = predictions.groupby('idhogar', as_index = False).mean()\n    \n    # Find the class and associated probability\n    predictions['Target'] = predictions[[1, 2, 3, 4]].idxmax(axis = 1)\n    predictions['confidence'] = predictions[[1, 2, 3, 4]].max(axis = 1)\n    predictions = predictions.drop(columns = ['fold'])\n    \n    # Merge with the base to have one prediction for each individual\n    submission = submission_base.merge(predictions[['idhogar', 'Target']], on = 'idhogar', how = 'left').drop(columns = ['idhogar'])\n        \n    # Fill in the individuals that do not have a head of household with 4 since these will not be scored\n    submission['Target'] = submission['Target'].fillna(4).astype(np.int8)\n    \n    # return the submission and feature importances along with validation scores\n    return submission, feature_importances, valid_scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e309ebfe43034963fadf1a15c2564647d70e958f"},"cell_type":"code","source":"hyp1 = {\n                  'colsample_bytree': 0.88, \n                  'learning_rate': 0.08, \n                   'min_child_samples': 90, 'num_leaves': 34, 'subsample': 0.94, 'reg_lambda': 0.5, \n                   'class_weight': 'balanced'}\n\nhyp2 = {\n                  'colsample_bytree': 0.88, \n                  'learning_rate': 0.08, \n                   'min_child_samples': 90, 'num_leaves': 14, 'subsample': 0.94, 'reg_lambda': 0.5, \n                   'class_weight': 'balanced'}\n\nhyp3 = {\n                  'colsample_bytree': 0.78, \n                  'learning_rate': 0.08, \n                   'min_child_samples': 45, 'num_leaves': 14, 'subsample': 0.64, 'reg_lambda': 0.1, \n                   'class_weight': 'balanced'}\n\nhyp4 = {\n                  'colsample_bytree': 0.72, \n                  'learning_rate': 0.08, \n                   'min_child_samples': 30, 'num_leaves': 18, 'subsample': 0.64, 'reg_lambda': 0.1, \n                   'class_weight': 'balanced'}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3e3aa42708be44d216fe2097d7024fcc221579ff"},"cell_type":"markdown","source":"Train the models"},{"metadata":{"trusted":true,"_uuid":"61aa4e7a7261dd502b7b6f6c4625e5fe0407960c"},"cell_type":"code","source":"%%capture --no-display\n\nsubmission1, gbm_fi_selected, valid_scores_selected = model_gbm(train_set, train_labels, test_set, test_ids,hyp = hyp1,random_state_int=103,early_stopping=300)\nsubmission2, gbm_fi_selected, valid_scores_selected = model_gbm(train_set, train_labels, test_set, test_ids,hyp = hyp1,random_state_int=103,early_stopping=300)\nsubmission3, gbm_fi_selected, valid_scores_selected = model_gbm(train_selected, train_labels, test_selected, test_ids)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"50f48f2d24851ef899ea7baee568b99c1ec0ff7b"},"cell_type":"markdown","source":"No learning rate decay, train them again."},{"metadata":{"trusted":true,"_uuid":"861435a98e77259b0bbea3627d2b3e075f405614"},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nimport lightgbm as lgb\nfrom IPython.display import display\n\ndef model_gbm_2(features, labels, test_features, test_ids, \n              nfolds = 5, return_preds = False, hyp = None,random_state_int=101,early_stopping=300):\n    \"\"\"Model using the GBM and cross validation.\n       Trains with early stopping on each fold.\n       Hyperparameters probably need to be tuned.\"\"\"\n    \n    feature_names = list(features.columns)\n\n    # Option for user specified hyperparameters\n    if hyp is not None:\n        # Using early stopping so do not need number of esimators\n        if 'n_estimators' in hyp:\n            del hyp['n_estimators']\n        params = hyp\n    \n    else:\n        # Model hyperparameters\n        params = {\n                  'colsample_bytree': 0.88, \n                  'learning_rate': 0.028, \n                   'min_child_samples': 10, \n                   'num_leaves': 36, \n                  \n                   'subsample': 0.54, \n                   'class_weight': 'balanced'}\n    \n    # Build the model\n    model = lgb.LGBMClassifier(**params, objective = 'multiclass', \n                               n_jobs = 4, n_estimators = 10000,\n                               random_state = 10)\n    \n    # Using stratified kfold cross validation\n    strkfold = StratifiedKFold(n_splits = nfolds, shuffle = True, random_state= random_state_int)\n    \n    # Hold all the predictions from each fold\n    predictions = pd.DataFrame()\n    importances = np.zeros(len(feature_names))\n    \n    # Convert to arrays for indexing\n    features = np.array(features)\n    test_features = np.array(test_features)\n    labels = np.array(labels).reshape((-1 ))\n    \n    valid_scores = []\n    \n    # Iterate through the folds\n    for i, (train_indices, valid_indices) in enumerate(strkfold.split(features, labels)):\n        \n        # Dataframe for fold predictions\n        fold_predictions = pd.DataFrame()\n        \n        # Training and validation data\n        X_train = features[train_indices]\n        X_valid = features[valid_indices]\n        y_train = labels[train_indices]\n        y_valid = labels[valid_indices]\n        \n        # Train with early stopping\n        model.fit(X_train, y_train, early_stopping_rounds =early_stopping, \n                  eval_metric = macro_f1_score,\n                  eval_set = [(X_train, y_train), (X_valid, y_valid)],\n                  eval_names = ['train', 'valid'],\n                  verbose = 200)\n        \n        # Record the validation fold score\n        valid_scores.append(model.best_score_['valid']['macro_f1'])\n        \n        # Make predictions from the fold as probabilities\n        fold_probabilitites = model.predict_proba(test_features)\n        \n        # Record each prediction for each class as a separate column\n        for j in range(4):\n            fold_predictions[(j + 1)] = fold_probabilitites[:, j]\n            \n        # Add needed information for predictions \n        fold_predictions['idhogar'] = test_ids\n        fold_predictions['fold'] = (i+1)\n        \n        # Add the predictions as new rows to the existing predictions\n        predictions = predictions.append(fold_predictions)\n        \n        # Feature importances\n        importances += model.feature_importances_ / nfolds   \n        \n        # Display fold information\n        display(f'Fold {i + 1}, Validation Score: {round(valid_scores[i], 5)}, Estimators Trained: {model.best_iteration_}')\n\n    # Feature importances dataframe\n    feature_importances = pd.DataFrame({'feature': feature_names,\n                                        'importance': importances})\n    \n    valid_scores = np.array(valid_scores)\n    display(f'{nfolds} cross validation score: {round(valid_scores.mean(), 5)} with std: {round(valid_scores.std(), 5)}.')\n    \n    # If we want to examine predictions don't average over folds\n    if return_preds:\n        predictions['Target'] = predictions[[1, 2, 3, 4]].idxmax(axis = 1)\n        predictions['confidence'] = predictions[[1, 2, 3, 4]].max(axis = 1)\n        return predictions, feature_importances\n    \n    # Average the predictions over folds\n    predictions = predictions.groupby('idhogar', as_index = False).mean()\n    \n    # Find the class and associated probability\n    predictions['Target'] = predictions[[1, 2, 3, 4]].idxmax(axis = 1)\n    predictions['confidence'] = predictions[[1, 2, 3, 4]].max(axis = 1)\n    predictions = predictions.drop(columns = ['fold'])\n    \n    # Merge with the base to have one prediction for each individual\n    submission = submission_base.merge(predictions[['idhogar', 'Target']], on = 'idhogar', how = 'left').drop(columns = ['idhogar'])\n        \n    # Fill in the individuals that do not have a head of household with 4 since these will not be scored\n    submission['Target'] = submission['Target'].fillna(4).astype(np.int8)\n    \n    # return the submission and feature importances along with validation scores\n    return submission, feature_importances, valid_scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d47bda397dc13856b0b01807400abc59517fdc3a"},"cell_type":"code","source":"%%capture --no-display\nsubmission4, gbm_fi_selected, valid_scores_selected = model_gbm_2(train_set, train_labels, test_set, test_ids,hyp = hyp3,random_state_int=103,early_stopping=300)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"356233d00535fab2734912d60c15cda539d2403a"},"cell_type":"code","source":"suball=submission1.merge(submission2, on='Id')\nsuball=suball.merge(submission4, on='Id')\nsuball=suball.merge(submission3, on='Id')\nsuball_2=suball.merge(submission4, on='Id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ed23dd4c3ba260e346b5312fb6f9d220289a65b6"},"cell_type":"markdown","source":"Vote for the final result, take the majority as the result for submission"},{"metadata":{"trusted":true,"_uuid":"eec773bfda9c98e4ee23ff5f3490f912d146b677"},"cell_type":"code","source":"#\nsuball_2['index']=suball_2.index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5d287f140d3a13f9200a236ae5a5aa342b59e364"},"cell_type":"code","source":"modle_13= pd.DataFrame(suball_2.mode(axis=1)[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bad901fb65666fbbfb25a2d01ebe63e8ca6e0f54"},"cell_type":"code","source":"modle_13['index']=modle_13.index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d0a1d6fede9e9357cd17b553a2b9947eaf9eeb6d"},"cell_type":"code","source":"modle_13.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1c9d35ddd5ea139f33d00f219dbdfc1f37cab11d"},"cell_type":"code","source":"final = suball_2.merge(modle_13, on ='index')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"029b1f1aa5cdfdb381a8b8f3bea1ffb85c6ea83a"},"cell_type":"code","source":"final_res=final[['Id',0]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"947fa2865ca81268d3e23d9ff3afa5bfcf11a6d3"},"cell_type":"code","source":"final_res.columns=['Id','Target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c107ba77a8355a6c5673ef932f4d0334884fe38d"},"cell_type":"code","source":"final_res.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}