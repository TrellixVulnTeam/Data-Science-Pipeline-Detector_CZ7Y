{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Costa Rican Household Poverty Level Prediction**\n\nProblem and Data Explanation The data for this competition is provided in two files: train.csv and test.csv. The training set has 9557 rows and 143 columns while the testing set has 23856 rows and 142 columns. Each row represents one individual and each column is a feature, either unique to the individual, or for the household of the individual. The training set has one additional column, Target, which represents the poverty level on a 1-4 scale and is the label for the competition. A value of 1 is the most extreme poverty.\n\nThe Target values represent poverty levels as follows:\n\n1 = extreme poverty \n2 = moderate poverty \n3 = vulnerable households \n4 = non vulnerable households\n\nThe explanations for all 143 columns can be found in the [competition documentation](https://www.kaggle.com/c/costa-rican-household-poverty-prediction/data), but a few to note are below:\n\n**Id:** a unique identifier for each individual, this should not be a feature that we use!\n**idhogar:** a unique identifier for each household. This variable is not a feature, but will be used to group individuals by household as all individuals in a household will have the same identifier. \n**parentesco1:** indicates if this person is the head of the household. \n**Target:** the label, which should be equal for all members in a household\n\nThis is a supervised multi-class classification machine learning problem:\n\n**Supervised:** provided with the labels for the training data\n**Multi-class classification: ** Labels are discrete values with 4 classes\n\n![![Screen%20Shot%202019-05-03%20at%201.07.30%20PM.png](attachment:Screen%20Shot%202019-05-03%20at%201.07.30%20PM.png)](https://www.habitatforhumanity.org.uk/wp-content/uploads/2017/10/Housing-poverty-Costa-Rica--1200x600-c-default.jpg)\n\n**Objectives:**\n\nObjective of this kernel is to perform modeling with the following estimators with default parameters & get accuracy\n\n**Modeling Estimaters**\n    1. GradientBoostingClassifier\n    2. RandomForestClassifier\n    3. KNeighborsClassifier\n    4. ExtraTreesClassifier\n    5. XGBoost\n    6. LightGBM\n\n**Tuning:**\nPerform tuning using Bayesian Optimization & compare the accuracy of the estimators."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# for data visulization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# for modeling estimators\nfrom sklearn.ensemble import RandomForestClassifier as rf\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier as gbm\nfrom xgboost.sklearn import XGBClassifier\nimport lightgbm as lgb\n\n#for data processing\nfrom sklearn.model_selection import train_test_split\n\n#for tuning parameters\nfrom bayes_opt import BayesianOptimization\nfrom skopt import BayesSearchCV\nfrom eli5.sklearn import PermutationImportance\n\n\n# Misc.\nimport os\nimport time\nimport gc\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Summarize Data**\nWe will start out by understanding the data that we have by looking at itâ€™s structure.\n\n**Load Data**\nStart by loading the CSV data from file into memory as a data frame. We know the names of the data provided, so we will set those names when loading the data from the file."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Read in data\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n\nids=test['Id']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Explore data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Perform data visualization**\n\nA graph is a lot more telling about the distribution and relationships of attributes.\n\nNevertheless, it is important to take your time and review the statistics first. Each time you review the data a different way, you open yourself up to noticing different aspects and potentially achieving different insights into the problem.\n\nWe can refer about [data visualization in Pandas](http://pandas.pydata.org/pandas-docs/stable/user_guide/visualization.html)."},{"metadata":{},"cell_type":"markdown","source":"**Feature Distributions**\nThe first and easy property to review is the distribution of each attribute.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.plot(figsize = (12,10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Feature-Target Relationships**\n\nThe next important relationship to explore is that of each attribute to the \"Target\" attribute."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(\"Target\", data=train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" sns.countplot(x=\"r4t3\",hue=\"Target\",data=train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x=\"hhsize\",hue=\"Target\",data=train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Feature-Feature Relationships**\n\nThe final important relationship to explore is that of the relationships between the attributes.\n\nWe can review the relationships between attributes by looking at the distribution of the interactions of each pair of attributes.\n\nThis uses a built function to create a matrix of scatter plots of all attributes versus all attributes. The diagonal where each attribute would be plotted against itself shows the Kernel Density Estimation of the attribute instead."},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas.plotting import scatter_matrix\nscatter_matrix(train.select_dtypes('float'), alpha=0.2, figsize=(26, 20), diagonal='kde')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The below are Distribution plots using seaborn"},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import OrderedDict\n\nplt.figure(figsize = (20, 16))\nplt.style.use('fivethirtyeight')\n\n# Color mapping\ncolors = OrderedDict({1: 'red', 2: 'orange', 3: 'blue', 4: 'green'})\npoverty_mapping = OrderedDict({1: 'extreme', 2: 'moderate', 3: 'vulnerable', 4: 'non vulnerable'})\n\n# Iterate through the float columns\nfor i, col in enumerate(train.select_dtypes('float')):\n    ax = plt.subplot(4, 2, i + 1)\n    # Iterate through the poverty levels\n    for poverty_level, color in colors.items():\n        # Plot each poverty level as a separate line\n        sns.kdeplot(train.loc[train['Target'] == poverty_level, col].dropna(), \n                    ax = ax, color = color, label = poverty_mapping[poverty_level])\n        \n    plt.title(f'{col.capitalize()} Distribution'); plt.xlabel(f'{col}'); plt.ylabel('Density')\n\nplt.subplots_adjust(top = 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Object Columns**\nThe last column type is object which we can view as follows."},{"metadata":{"trusted":true},"cell_type":"code","source":"train.select_dtypes('object').head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yes_no_map = {'no':0,'yes':1}\ntrain['dependency'] = train['dependency'].replace(yes_no_map).astype(np.float32)\ntrain['edjefe'] = train['edjefe'].replace(yes_no_map).astype(np.float32)\ntrain['edjefa'] = train['edjefa'].replace(yes_no_map).astype(np.float32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yes_no_map = {'no':0,'yes':1}\ntest['dependency'] = test['dependency'].replace(yes_no_map).astype(np.float32)\ntest['edjefe'] = test['edjefe'].replace(yes_no_map).astype(np.float32)\ntest['edjefa'] = test['edjefa'].replace(yes_no_map).astype(np.float32)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Converting categorical objects into numericals**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train[[\"dependency\",\"edjefe\",\"edjefa\"]].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[[\"dependency\",\"edjefe\",\"edjefa\"]].hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (16, 12))\n\n# Iterate through the float columns\nfor i, col in enumerate(['dependency', 'edjefa', 'edjefe']):\n    ax = plt.subplot(3, 1, i + 1)\n    # Iterate through the poverty levels\n    for poverty_level, color in colors.items():\n        # Plot each poverty level as a separate line\n        sns.kdeplot(train.loc[train['Target'] == poverty_level, col].dropna(), \n                    ax = ax, color = color, label = poverty_mapping[poverty_level])\n        \n    plt.title(f'{col.capitalize()} Distribution'); plt.xlabel(f'{col}'); plt.ylabel('Density')\n\nplt.subplots_adjust(top = 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Fill in missing values (NULL values) using 1 for yes and 0 for no**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of missing in each column\nmissing = pd.DataFrame(train.isnull().sum()).rename(columns = {0: 'total'})\n\n# Create a percentage missing\nmissing['percent'] = missing['total'] / len(train)\n\nmissing.sort_values('percent', ascending = False).head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['v18q1'] = train['v18q1'].fillna(0)\ntest['v18q1'] = test['v18q1'].fillna(0)\ntrain['v2a1'] = train['v2a1'].fillna(0)\ntest['v2a1'] = test['v2a1'].fillna(0)\n\ntrain['rez_esc'] = train['rez_esc'].fillna(0)\ntest['rez_esc'] = test['rez_esc'].fillna(0)\ntrain['SQBmeaned'] = train['SQBmeaned'].fillna(0)\ntest['SQBmeaned'] = test['SQBmeaned'].fillna(0)\ntrain['meaneduc'] = train['meaneduc'].fillna(0)\ntest['meaneduc'] = test['meaneduc'].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking for missing values again to confirm that no missing values present\n# Number of missing in each column\nmissing = pd.DataFrame(train.isnull().sum()).rename(columns = {0: 'total'})\n\n# Create a percentage missing\nmissing['percent'] = missing['total'] / len(train)\n\nmissing.sort_values('percent', ascending = False).head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking for missing values again to confirm that no missing values present\n# Number of missing in each column\nmissing = pd.DataFrame(test.isnull().sum()).rename(columns = {0: 'total'})\n\n# Create a percentage missing\nmissing['percent'] = missing['total'] / len(train)\n\nmissing.sort_values('percent', ascending = False).head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Dropping unnecesary columns**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop(['Id','idhogar'], inplace = True, axis =1)\n\ntest.drop(['Id','idhogar'], inplace = True, axis =1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape, test.shape\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Dividing the data into predictors & target**"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train.iloc[:,140]\ny.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train.iloc[:,1:141]\nX.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Splitting the data into train & test**"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(\n                                                    X,\n                                                    y,\n                                                    test_size = 0.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Modeling**"},{"metadata":{},"cell_type":"markdown","source":"**Modelling with GradientBoostingClassifier**"},{"metadata":{"trusted":true},"cell_type":"code","source":"modelgbm=gbm()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.time()\nmodelgbm = modelgbm.fit(X_train, y_train)\nend = time.time()\n(end-start)/60","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classes = modelgbm.predict(X_test)\n\nclasses","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(classes == y_test).sum()/y_test.size ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Performing tuning using Bayesian Optimization.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"bayes_cv_tuner = BayesSearchCV(\n    #  Place your estimator here with those parameter values\n    #      that you DO NOT WANT TO TUNE\n    gbm(\n               # No need to tune this parameter value\n      ),\n\n    # 2.12 Specify estimator parameters that you would like to change/tune\n    {\n        'n_estimators': (100, 500),           # Specify integer-values parameters like this\n        \n        'max_depth': (4, 100),                # integer valued parameter\n        'max_features' : (10,64),             # integer-valued parameter\n        'min_weight_fraction_leaf' : (0,0.5, 'uniform')   # Float-valued parameter\n    },\n\n    # 2.13\n    n_iter=32,            # How many points to sample\n    cv = 2                # Number of cross-validation folds\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Start optimization\nbayes_cv_tuner.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Get list of best-parameters\nbayes_cv_tuner.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"modelgbmTuned=gbm(\n               max_depth=31,\n               max_features=29,\n               min_weight_fraction_leaf=0.02067,\n               n_estimators=489)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.time()\nmodelgbmTuned = modelgbmTuned.fit(X_train, y_train)\nend = time.time()\n(end-start)/60","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ygbm=modelgbmTuned.predict(X_test)\nygbmtest=modelgbmTuned.predict(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Get what average accuracy was acheived during cross-validation\nbayes_cv_tuner.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  What accuracy is available on test-data\nbayes_cv_tuner.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  And what all sets of parameters were tried?\nbayes_cv_tuner.cv_results_['params']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\n**Modeling with Random Forest**"},{"metadata":{"trusted":true},"cell_type":"code","source":"modelrf = rf()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.time()\nmodelrf = modelrf.fit(X_train, y_train)\nend = time.time()\n(end-start)/60","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classes = modelrf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(classes == y_test).sum()/y_test.size ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Performing tuning using Bayesian Optimization.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"bayes_cv_tuner = BayesSearchCV(\n    #  Place your estimator here with those parameter values\n    #      that you DO NOT WANT TO TUNE\n    rf(\n       n_jobs = 2         # No need to tune this parameter value\n      ),\n\n    # 2.12 Specify estimator parameters that you would like to change/tune\n    {\n        'n_estimators': (100, 500),           # Specify integer-values parameters like this\n        'criterion': ['gini', 'entropy'],     # Specify categorical parameters as here\n        'max_depth': (4, 100),                # integer valued parameter\n        'max_features' : (10,64),             # integer-valued parameter\n        'min_weight_fraction_leaf' : (0,0.5, 'uniform')   # Float-valued parameter\n    },\n\n    # 2.13\n    n_iter=32,            # How many points to sample\n    cv = 3                # Number of cross-validation folds\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Start optimization\nbayes_cv_tuner.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Get list of best-parameters\nbayes_cv_tuner.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"modelrfTuned=rf(criterion=\"gini\",\n               max_depth=88,\n               max_features=41,\n               min_weight_fraction_leaf=0.1,\n               n_estimators=285)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.time()\nmodelrfTuned = modelrfTuned.fit(X_train, y_train)\nend = time.time()\n(end-start)/60","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yrf=modelrfTuned.predict(X_test)\nyrf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Get what average accuracy was acheived during cross-validation\nbayes_cv_tuner.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  What accuracy is available on test-data\nbayes_cv_tuner.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  And what all sets of parameters were tried?\nbayes_cv_tuner.cv_results_['params']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Modelling with KNeighborsClassifier**"},{"metadata":{"trusted":true},"cell_type":"code","source":"modelneigh = KNeighborsClassifier(n_neighbors=7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.time()\nmodelneigh = modelneigh.fit(X_train, y_train)\nend = time.time()\n(end-start)/60","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classes = modelneigh.predict(X_test)\n\nclasses","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(classes == y_test).sum()/y_test.size ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Performing tuning using Bayesian Optimization.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"bayes_cv_tuner = BayesSearchCV(\n    #  Place your estimator here with those parameter values\n    #      that you DO NOT WANT TO TUNE\n    KNeighborsClassifier(\n       n_neighbors=7         # No need to tune this parameter value\n      ),\n    {\"metric\": [\"euclidean\", \"cityblock\"]},\n    n_iter=32,            # How many points to sample\n    cv = 2            # Number of cross-validation folds\n   )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Start optimization\nbayes_cv_tuner.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Get list of best-parameters\nbayes_cv_tuner.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"modelneighTuned = KNeighborsClassifier(n_neighbors=7,\n               metric=\"cityblock\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.time()\nmodelneighTuned = modelneighTuned.fit(X_train, y_train)\nend = time.time()\n(end-start)/60","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yneigh=modelneighTuned.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yneightest=modelneighTuned.predict(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Get what average accuracy was acheived during cross-validation\nbayes_cv_tuner.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  What accuracy is available on test-data\nbayes_cv_tuner.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  And what all sets of parameters were tried?\nbayes_cv_tuner.cv_results_['params']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Modelling with ExtraTreeClassifier**"},{"metadata":{"trusted":true},"cell_type":"code","source":"modeletf = ExtraTreesClassifier()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.time()\nmodeletf = modeletf.fit(X_train, y_train)\nend = time.time()\n(end-start)/60","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classes = modeletf.predict(X_test)\n\nclasses","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(classes == y_test).sum()/y_test.size","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Performing tuning using Bayesian Optimization.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"bayes_cv_tuner = BayesSearchCV(\n    #  Place your estimator here with those parameter values\n    #      that you DO NOT WANT TO TUNE\n    ExtraTreesClassifier( ),\n\n    # 2.12 Specify estimator parameters that you would like to change/tune\n    {   'n_estimators': (100, 500),           # Specify integer-values parameters like this\n        'criterion': ['gini', 'entropy'],     # Specify categorical parameters as here\n        'max_depth': (4, 100),                # integer valued parameter\n        'max_features' : (10,64),             # integer-valued parameter\n        'min_weight_fraction_leaf' : (0,0.5, 'uniform')   # Float-valued parameter\n    },\n\n    n_iter=32,            # How many points to sample\n    cv = 2            # Number of cross-validation folds\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Start optimization\nbayes_cv_tuner.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Get list of best-parameters\nbayes_cv_tuner.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"modeletfTuned=ExtraTreesClassifier(criterion=\"entropy\",\n               max_depth=100,\n               max_features=64,\n               min_weight_fraction_leaf=0.0,\n               n_estimators=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.time()\nmodeletfTuned = modeletfTuned.fit(X_train, y_train)\nend = time.time()\n(end-start)/60","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yetf=modeletfTuned.predict(X_test)\nyetftest=modeletfTuned.predict(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Get what average accuracy was acheived during cross-validation\nbayes_cv_tuner.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  What accuracy is available on test-data\nbayes_cv_tuner.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  And what all sets of parameters were tried?\nbayes_cv_tuner.cv_results_['params']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Modelling with XGBoosterClassifier**"},{"metadata":{"trusted":true},"cell_type":"code","source":"modelxgb=XGBClassifier()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.time()\nmodelxgb = modelxgb.fit(X_train, y_train)\nend = time.time()\n(end-start)/60","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classes = modelxgb.predict(X_test)\n\nclasses","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(classes == y_test).sum()/y_test.size ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Performing tuning using Bayesian Optimization.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"bayes_cv_tuner = BayesSearchCV(\n    #  Place your estimator here with those parameter values\n    #      that you DO NOT WANT TO TUNE\n    XGBClassifier(\n       n_jobs = 2         # No need to tune this parameter value\n      ),\n\n    # 2.12 Specify estimator parameters that you would like to change/tune\n    {\n        'n_estimators': (100, 300),           # Specify integer-values parameters like this\n        'criterion': ['gini', 'entropy'],     # Specify categorical parameters as here\n        'max_depth': (4, 100),                # integer valued parameter\n        'max_features' : (10,64),             # integer-valued parameter\n        'min_weight_fraction_leaf' : (0,0.5, 'uniform')   # Float-valued parameter\n    },\n\n    # 2.13\n    n_iter=32,            # How many points to sample\n    cv = 3                # Number of cross-validation folds\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Start optimization\nbayes_cv_tuner.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Get list of best-parameters\nbayes_cv_tuner.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"modelxgbTuned=XGBClassifier(criterion=\"gini\",\n               max_depth=85,\n               max_features=47,\n               min_weight_fraction_leaf=0.035997,\n               n_estimators=178)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.time()\nmodelxgbTuned = modelxgbTuned.fit(X_train, y_train)\nend = time.time()\n(end-start)/60","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#yxgb=modelxgbTuned.predict(X_test)\n#yxgbtest=modelxgbTuned.predict(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Get what average accuracy was acheived during cross-validation\nbayes_cv_tuner.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  What accuracy is available on test-data\nbayes_cv_tuner.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  And what all sets of parameters were tried?\nbayes_cv_tuner.cv_results_['params']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Modelling with Light Gradient Booster**"},{"metadata":{"trusted":true},"cell_type":"code","source":"modellgb = lgb.LGBMClassifier(max_depth=-1, learning_rate=0.1, objective='multiclass',\n                             random_state=None, silent=True, metric='None', \n                             n_jobs=4, n_estimators=5000, class_weight='balanced',\n                             colsample_bytree =  0.93, min_child_samples = 95, num_leaves = 14, subsample = 0.96)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.time()\nmodellgb = modellgb.fit(X_train, y_train)\nend = time.time()\n(end-start)/60","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classes = modellgb.predict(X_test)\n\nclasses","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(classes == y_test).sum()/y_test.size ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Performing tuning using Bayesian Optimization.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"bayes_cv_tuner = BayesSearchCV(\n    #  Place your estimator here with those parameter values\n    #      that you DO NOT WANT TO TUNE\n    lgb.LGBMClassifier(\n       n_jobs = 2         # No need to tune this parameter value\n      ),\n\n    # 2.12 Specify estimator parameters that you would like to change/tune\n    {\n        'n_estimators': (100, 500),           # Specify integer-values parameters like this\n        'criterion': ['gini', 'entropy'],     # Specify categorical parameters as here\n        'max_depth': (4, 100),                # integer valued parameter\n        'max_features' : (10,64),             # integer-valued parameter\n        'min_weight_fraction_leaf' : (0,0.5, 'uniform')   # Float-valued parameter\n    },\n\n    # 2.13\n    n_iter=32,            # How many points to sample\n    cv = 3                # Number of cross-validation folds\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Start optimization\nbayes_cv_tuner.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Get list of best-parameters\nbayes_cv_tuner.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"modellgbTuned = lgb.LGBMClassifier(criterion=\"entropy\",\n               max_depth=35,\n               max_features=14,\n               min_weight_fraction_leaf=0.18611,\n               n_estimators=148)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.time()\nmodellgbTuned = modellgbTuned.fit(X_train, y_train)\nend = time.time()\n(end-start)/60","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ylgb=modellgbTuned.predict(X_test)\nylgbtest=modellgbTuned.predict(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Get what average accuracy was acheived during cross-validation\nbayes_cv_tuner.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  What accuracy is available on test-data\nbayes_cv_tuner.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  And what all sets of parameters were tried?\nbayes_cv_tuner.cv_results_['params']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**BUILDING a new dataset with predicted results with all these models**"},{"metadata":{"trusted":true},"cell_type":"code","source":"NewTrain = pd.DataFrame()\n#NewTrain['yrf'] = yrf.tolist()\nNewTrain['yetf'] = yetf.tolist()\nNewTrain['yneigh'] = yneigh.tolist()\nNewTrain['ygbm'] = ygbm.tolist()\n#NewTrain['yxgb'] = yxgb.tolist()\nNewTrain['ylgb'] = ylgb.tolist()\n\nNewTrain.head(5), NewTrain.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NewTest = pd.DataFrame()\n#NewTest['yrf'] = yrftest.tolist()\nNewTest['yetf'] = yetftest.tolist()\nNewTest['yneigh'] = yneightest.tolist()\nNewTest['ygbm'] = ygbmtest.tolist()\n#NewTest['yxgb'] = yxgbtest.tolist()\nNewTest['ylgb'] = ylgbtest.tolist()\nNewTest.head(5), NewTest.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NewModel=rf(criterion=\"entropy\",\n               max_depth=87,\n               max_features=4,\n               min_weight_fraction_leaf=0.0,\n               n_estimators=600)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.time()\nNewModel = NewModel.fit(NewTrain, y_test)\nend = time.time()\n(end-start)/60","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ypredict=NewModel.predict(NewTest)\nypredict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#submit=pd.DataFrame({'Id': ids, 'Target': ylgbtest})\nsubmit=pd.DataFrame({'Id': ids, 'Target': ypredict})\nsubmit.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit.to_csv('submit.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv('../input/sample_submission.csv')\nsub['target'] = ypredict\nsub.drop(sub.columns[[1]], axis=1, inplace=True)\nsub.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":".... Contine Tuning, Analysing ... "}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}