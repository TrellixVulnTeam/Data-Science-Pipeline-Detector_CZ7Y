{"cells":[{"metadata":{"_uuid":"292d47c2e7bef27beaa4e6907731f713105a03c5"},"cell_type":"markdown","source":"Before going into the Kernel,\n\nI REFERED ON THIS KERNEL https://www.kaggle.com/willkoehrsen/a-complete-introduction-and-walkthrough\nI translated into Korean for my studying\n\nI'D LIKE TO GIVE HUGE THANKS TO HIM"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/train.csv\") \ntest = pd.read_csv(\"../input/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d87d34a026c5edab688ed5464d2c0587f50169af"},"cell_type":"markdown","source":"# Costa Rican Household Poverty Level Prediction<br />\nWelcome to another Kaggle challenge! The objective of the Costa Rican Household Poverty Level Prediction contest is to develop a machine learning model that can predict the poverty level of households using both individual and household characteristics. This \"data science for good\" project offers the opportunity to put our skills towards a task more beneficial to society than getting people to click on ads!<br />\n=> 가구 개개인 과 가구의 특징을 이용하여 가구의 빈층단계를 예측하는 모델을 만드는 것\n\n* In this notebook, we will walk through a complete machine learning solution: <br />\n* first, get introduced to the problem, [문제를 도출]<br />\n* then perform a thorough Exploratory Data Analysis of the dataset, [EDA를 통한 데이터 탐색] <br />\n* work on feature engineering, <br />\n* try out multiple machine learning models, [여러개의 머신러닝 모델에 대입]<br />\n* select a model, [모델을 선택] <br />\n* work to optimize the model, [모델을 최적화] <br /> \n* and finally, inspect the outputs of the model and draw conclusions. [결론도출] <br />\n\n__While this notebook may not get us to the top of the leaderboard, it is meant to be used as a teaching tool to give you a solid foundation to build on for future machine learning projects. Kaggle projects can teach us a lot about machine learning, but several of the strategies used to get to the very top of the leaderboard are not best practices, so here we'll stick to building a very good - although not quite first place - machine learning solution. While Kaggle projects are competitions, I think they are best described as \"a machine learning education\" disguised as a contest!\"\n\nIf you are looking to follow-up on this work, I have additional work including a kernel on using Automated Feature Engineering with Featuretools for this problem (with slightly higher leaderboard score). (If you enjoy my writing style and explanations, I write for Towards Data Science)\n\nProblem and Data Explanation <br />\nThe data for this competition is provided in two files: train.csv and test.csv. <br />\nThe training set has 9557 rows and 143 columns [훈련세트의 정보] <br />\nwhile the testing set has 23856 rows and 142 columns. [테스트셋의 정보] <br />\nEach row represents one individual and each column is a feature, either unique to the individual[개인의 고유값], or for the household of the individual.[각 가구별 고윳값] <br />\nThe training set has one additional column,  Target, which represents the poverty level on a 1-4 scale and is the label for the competition. <br />\n[우리가 예측하고자 하는 결과 값이 들어있는 컬럼] <br />\nA value of 1 is the most extreme poverty. <br />\n\nThis is a supervised multi-class classification machine learning problem: <br />\n=> 지도학습 && 다중분류 <br />\n\nSupervised: provided with the labels for the training data <br />\nMulti-class classification: Labels are discrete values with 4 classes <br />\n\nObjective <br />\nThe objective is to predict poverty on a household level. <br /> \n=> 목표는 가구단위로 빈곤단계를 구분하는 것 <br />\nWe are given data on the individual level with each individual having unique features but also information about their household. <br />\n=> 정보는 가구원 개개인에 대한 유니크한 정보를 포함하며 가구 전체의 유니크한 정보도 포함한다 <br />\nIn order to create a dataset for the task, we'll have to perform some aggregations of the individual data for each household. <br /> \n=> 그래서 위를 위한 데이터셋을 만들기 위해서는 각각의 가정을 개개인의 데이터로 합쳐둘 필요가 있다 <br />\nMoreover, we have to make a prediction for every individual in the test set,  <br />\n=> 테스트셋에 있는 모든 개별 인원들에게 예측을 실행해야 한다 <br />\nbut \"ONLY the heads of household are used in scoring\" which means we want to predict poverty on a household basis <br />.\n=> 하지만 무조건 가장에 대해서만 점수를 반영한다 <br />\n\nImportant note: <br />\nwhile all members of a household should have the same label in the training data, <br /> \n=> 가구의 모든 구성원들이 무조건 같은 단계를 받아야 한다 <br />\nthere are errors where individuals in the same household have different labels. <br /> \n=> 하지만 일부 가구의 일부 구성원들은 다른 구성원과는 다른 단계를 가지고 있는 경우도 있다. <br />\nIn these cases, we are told to use the label for the head of each household, <br />\n=> 이경우에는 우리는 가구의 가장의 label을 이용할 것이다 <br />\nwhich can be identified by the rows where parentesco1 == 1.0. <br />\n\nWe will cover how to correct this in the notebook (for more info take a look at the competition main discussion). <br />\n=> 왜 이 접근법이 맞는지는 Discussion에서 참고하길 바란다 <br />\n\nThe Target values represent poverty levels as follows: <br />\n=> 단계는 아래와 같다 <br />\n\n1 = extreme poverty [극빈층] <br />\n2 = moderate poverty [적정 수준의 빈곤]<br />\n3 = vulnerable households [취약계층] <br />\n4 = non vulnerable households [취약계층이 아님]<br />\n\nThe explanations for all 143 columns can be found in the competition documentation, but a few to note are below: <br />\n=> 143개의 columns를 가지고 있다 그 중 일부 컬럼에 대하 설명 <br />\n\nId: a unique identifier for each individual, this should not be a feature that we use! <br />\n=> 개인에게 주어지는 식별자 <br />\nidhogar: a unique identifier for each household. This variable is not a feature, but will be used to group individuals by household as all individuals in a household will have the same identifier. <br />\n=> 각각의 가구를 구분하는 식별자 <br />\nparentesco1: indicates if this person is the head of the household. <br />\n=> 가장을 나타내는 컬럼<br />\nTarget: the label, which should be equal for all members in a household<br />\n=> 모든 가구들은 라벨이 모두 동일해야 한다.\n\nWhen we make a model, we'll train on a household basis with the label for each household the poverty level of the head of household. <br />\n=> 예측을 하는데 있어서 가장의 정보만 사용할 것임<br />\nThe raw data contains a mix of both household and individual characteristics and for the individual data, <br />\n=> 기존의 데이터는 개개인에 대한 정보와 가구에 대한 정보를 함께가지고 있음<br />\nwe will have to find a way to aggregate this for each household. <br />\n=> 각각의 가구별로 확인 가능하도록 데이터를 합치는 방법을 모색해야 한다<br />\nSome of the individuals belong to a household with no head of household which means that unfortunately we can't use this data for training. <br />\n=> 몇몇의 가구는 가장이 없는 경우가 있는데, 이 경우는 사용하지 않도록 한다<br />\nThese issues with the data are completely typical of real-world data and hence this problem is great preparation for the datasets you'll encounter in a data science job!<br />\n=> 위의 문제들은 실제에서 가져오는 데이터에서 많이 일어날 수 있는 문제임.\n\nMetric [평가지표]<br />\nUltimately we want to build a machine learning model that can predict the integer poverty level of a household. <br />\n궁극적으로 우리가 원하는 것은 각각의 가구의 빈곤레벨을 예측하는 것<br />\nOur predictions will be assessed by the Macro F1 Score. You may be familiar with the standard F1 score for binary classification problems which is the harmonic mean of precision and recall:<br />\n우리는 Macro F1 Score를 사용할 것이다. 기존의 F1 score는 정밀도와 재현율의 조화평균한 값이다\n\n** 조화평균 [역수들을 산술평균(Average) 한 것들의 역수]<br />\nhttps://blog.naver.com/hyuiin/221367087739 \n\nF1=21recall+1precision=2⋅precision⋅recallprecision+recall\n \nFor mutli-class problems, we have to average the F1 scores for each class. The macro F1 score averages the F1 score for each class without taking into account label imbalances.\n\nMacro F1=F1 Class 1+F1 Class 2+F1 Class 3+F1 Class 44\n \nIn other words, the number of occurrences of each label does not figure into the calculation when using macro (while it does when using the \"weighted\" score). <br />\n=> 각 라벨이 등장한 횟수의 경우에는 계산에 적용되지 않을 것이다 macro를 사용하게 되면<br />\n(For more information on the differences, look at the Scikit-Learn Documention for F1 Score or this Stack Exchange question and answers. If we want to assess our performance, we can use the code:\n\nfrom sklearn.metrics import f1_score<br />\nf1_score(y_true, y_predicted, average = 'macro`)"},{"metadata":{"_uuid":"e65ca716b85b7687c7120e9cea8e74f5b42b55d8"},"cell_type":"markdown","source":"For this problem, the labels are imbalanced, which makes it a little strange to use macro averaging for the evaluation metric, <br />\n=> 우리가 예측하려는 라벨이 불균형이 있음 (평가 지표로 macro averaging을 사용하는 것이 조금 이상하게 느껴질 수 있음)<br />\nbut that's a decision made by the organizers and not something we can change! In your own work, you want to be aware of label imbalances and choose a metric accordingly.<br />\n=> 그러나 그것은 이미 지정된 것이기 때문에 상관할 필요 없음 단지 라벨의 분균형과 그것에 해당하는 평가지표 방법을 고른 것이라고 생각하면 된다.\n\nRoadmap<br />\nThe end objective is a machine learning model that can predict the poverty level of a household. <br />\n=> 우리의 최종목표는 빈곤게층을 예측하는 것이다<br />\nHowever, before we get carried away with modeling, it's important to understand the problem and data. <br />\n=> 알아보기전에 우리가 직면한 문제와 데이터에 대해서 알아보도록 하자<br />\nAlso, we want to evaluate numerous models before choosing one as the \"best\" and after building a model, we want to investigate the predictions. Our roadmap is therefore as follows:<br />\n=> 우리의 로드맵은 아래의 순서와 같다\n\nUnderstand the problem (we're almost there already)<br />\nExploratory Data Analysis<br />\nFeature engineering to create a dataset for machine learning<br />\nCompare several baseline machine learning models<br />\nTry more complex machine learning models<br />\nOptimize the selected model<br />\nInvestigate model predictions in context of problem<br />\nDraw conclusions and lay out next steps\n\nThe steps laid out above are iterative meaning that while we will go through them one at a time, we might go back to an earlier step and revisit some of our decisions. <br />\nIn general, data science is a non-linear pracice where we are constantly evaluating our past decisions and making improvements. <br />\nIn particular, feature engineering, modeling, and optimization are steps that we often repeat because we never know if we got them right the first time!\n\nGetting Started<br />\nWe have a pretty good grasp of the problem, so we'll move into the Exploratory Data Analysis (EDA) and feature engineering. <br />\nFor the EDA we'll examine any interesting anomalies, trends, correlations, or patterns that can be used for feature engineering and for modeling. <br />\n=> 우리는 특성공학과 모델릴에 쓰이는 흥미로운 잘못된 점, 추세, 상관관계 도는 패턴을 탐구할 것입니다<br />\nWe'll make sure to investigate our data both quantitatively (with statistics) and visually (with figures).<br />\n=> 우리의 데이터를 양적으로 그리고 시각적으로 탐사해봅시다.\n\nOnce we have a good grasp of the data and any potentially useful relationships, <br />\n=> 위의 탐사를 성공적으로 끝낸 후에야<br />\nwe can do some feature engineering (the most important part of the machine learning pipeline) and establish a baseline model. <br />\n=> 특성공학이나 기반모델을 설게할 수 있을 것입니다<br />\nThis won't get us to the top of the leaderboard, but it will provide a strong foundation to build on!<br />\n\nWith all that info in mind (don't worry if you haven't got all the details), let's get started!\n\nImports\nWe'll use a familiar stack of data science libraries: Pandas, numpy, matplotlib, seaborn, and eventually sklearn for modeling."},{"metadata":{"trusted":true,"_uuid":"eb03c92bac591af97551958d1018cb7b9dd62a24"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f7f4cfeaa248ee00ed0bc1e6ab3256295784fe12"},"cell_type":"markdown","source":"That gives us a look at all of the columns which don't appear to be in any order. To get a quick overview of the data we use df.info()."},{"metadata":{"trusted":true,"_uuid":"4b74908cc5ca4ab72120f6cbf369e109cea0a774"},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"278828b31ed026cd92846bd934f55fb01ee9d003"},"cell_type":"markdown","source":"This tells us there are 130 integer columns, 8 float (numeric) columns, and 5 object columns. <br />\n=> 130개의 정수, 8개의 실수 그리고 5개의 객체 컬럼이 있습니다.<br />\nThe integer columns probably represent Boolean variables (that take on either 0 or 1) or ordinal variables with discrete ordered values. <br />\n=> 정수인 컬럼은 불리언 값일 수도 있지만 순서를 가진 ordinal한 값일 가능성이 있습니다<br />\nThe object columns might pose an issue because they cannot be fed directly into a machine learning model.<br />\n=> 객체컬럼은 머신러닝 모델에서 학습될 수 없기 때문에 어떠한 조치를 취해주어야 합니다\n\nLet's glance at the test data which has many more rows (individuals) than the train. It does have one fewer column because there's no Target!<br />\n=> 테스트 데이터셋을 보게 된다면 테스트 데이터셋에는 타겟 값이 없기 대문에 정수컬럼이 하나 없음을 알 수 있습니다."},{"metadata":{"trusted":true,"_uuid":"8276c9593e68d98ebb153ca536ec32252d9fe106"},"cell_type":"code","source":"test.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cb925feded68a67edb8c0fe742685a5f490e87e3"},"cell_type":"markdown","source":"Integer Columns<br />\nLet's look at the distribution of unique values in the integer columns. For each column, we'll count the number of unique values and show the result in a bar plot.<br />\n=> 위에서 말했듯이 정수컬럼의 유형을 구분하기 위해서 각각의 컬럼이 어떤 유니크한 개별값을 갖는지를 나타내보겠습니다."},{"metadata":{"trusted":true,"_uuid":"13a32a919e1b15f9fafe9d900e2a792105c32a00"},"cell_type":"code","source":"df_int_cols = train.select_dtypes('int').apply(lambda x:x.nunique()).reset_index()\ndf_int_cols.columns = ['var_name','count']\ndf_int_cols.groupby('count')['var_name'].size().plot(kind='bar')\n\nplt.xlabel('Number of Unique Values'); plt.ylabel('Count');\nplt.title('Count of Unique Values in Integer Columns');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d123b4191044c047e5fc733cb4832b5100ecc552"},"cell_type":"markdown","source":"The columns with only 2 unique values represent Booleans (0 or 1). <br />\n=> 값이 두 개 밖에 없다는 것은 논리형을 뜻합니다.<br />\nIn a lot of cases, this boolean information is already on a household level.<br />\n=> 이러한 논리형 정보는 가구 전체의 단계를 위해 이미 준비되었습니다<br />\nFor example, the refrig column says whether or not the household has a refrigerator. <br />\n=> 그 중 하나로 refig는 가구가 냉장고를 가지는지 아닌지를 가르는 정보입니다.<br />\nWhen it comes time to make features from the Boolean columns that are on the household level, we will not need to aggregate these. <br />\n=> 이러한 논리형 정보는 가구당 정보이기 때문에 따로 취합할 필요성은 없는 듯 합니다<br />\nHowever, the Boolean columns that are on the individual level will need to be aggregated.<br />\n=> 그러나 일부 개개인을 위한 논리형 정보는 취합의 여지가 있습니다.\n\nFloat Columns [실수형 컬럼]<br />\nAnother column type is floats which represent continuous variables.<br />\n=> 연속형 변수를 대표하는 실수형 컬럼입니다<br />\nWe can make a quick distribution plot to show the distribution of all float columns. <br />\n=> 모든 정수형 컬럼의 분포를 간단하게 시각화해보려고 합니다<br />\nWe'll use an OrderedDict to map the poverty levels to colors because this keeps the keys and values in the same order as we specify (unlike a regular Python dictionary).<br />\n=> 우리는 일반 딕셔너리 셩태와는 달리 정렬된 딕셔너리를 사용하려고 합니다. 왜냐하면 키와 값이 같은 같은 순서로 유지할 필요가 있기 때문입니다.\n\nThe following graphs shows the distributions of the float columns colored by the value of the Target. <br />\n=> 다음의 그래프들은 타켓의 값 별로 칠해진 실수형 컬럼의 분포를 보여줄 것입니다.<br />\nWith these plots, we can see if there is a significant difference in the variable distribution depending on the household poverty level.<br />\n=> 이 그래프들로 각각의 빈곤 레벨에 따른 개별 컬럼의 분포를 볼 수 있습니다. <br />"},{"metadata":{"trusted":true,"_uuid":"f6590a0580fc6e9004e0ba3106893b244fed240f"},"cell_type":"code","source":"from collections import OrderedDict\n\ncolors = OrderedDict({1: 'red', 2: 'orange', 3: 'blue', 4: 'green'})\npoverty_mapping = OrderedDict({1: 'extreme', 2: 'moderate', 3: 'vulnerable', 4: 'non vulnerable'})\n\nfig,ax = plt.subplots(4,2,figsize=[20,16])\n\nfor i,col in enumerate(train.select_dtypes('float').columns):\n    ax = plt.subplot(4,2,i+1)\n    for proverty_level,color in colors.items():\n        sns.kdeplot(train.loc[train['Target']==proverty_level,col].dropna(),ax=ax,color=color,label=poverty_mapping[proverty_level])\n    \n    ax.set(title=f'{col.capitalize()} Distribution',xlabel=f'{col}',ylabel='Density')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e679afcce95db0a502c765c60d0f7ccfe41da59b"},"cell_type":"markdown","source":"Later on we'll calculate correlations between the variables and the Target to gauge the relationships between the features, <br />\n=> 나중에 피쳐들 간에 관게를 보기 위해 각각의 변수들과 타켓의 상관관계를 계산할 것이다 <br />\nbut these plots can already give us a sense of which variables may be most \"relevant\" to a model. <br />\n=> 이러한 그림은 이미 연관성에 대한 관점을 조금은 보여주는 듯하다<br />\nFor example, the meaneduc, representing the average education of the adults in the household appears to be related to the poverty level: a higher average adult education leads to higher values of the target which are less severe levels of poverty. <br />\n=> 간단히 예를들면 meaneduc의 경우에는 빈곤계층 단계와 관련이 있어보이는 것 같다: 교육 수준 정도가 높을 수록 빈곤계층 단계가 더 4로 높아지는 것을 보여준다. 이는 교육수준이 높을 수록 빈곤의 수준이 낮게 됨을 볼 수 있다.<br />\nThe theme of the importance of education is one we will come back to again and again in this notebook!<br />\n=> 교육의 중요성을 실감할 수 있다."},{"metadata":{"_uuid":"ea5ae6aa82c714402dba4a785ba9d276b53794e4"},"cell_type":"markdown","source":"Object Columns [객체형 컬럼]<br />\nThe last column type is object which we can view as follows."},{"metadata":{"trusted":true,"_uuid":"a93745b69f539082769e2961e4b9ccccc4391609"},"cell_type":"code","source":"train.select_dtypes('object').head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"afa7d37513c6ba32011f6da1d85ef9c6c35afd88"},"cell_type":"markdown","source":"The Id and idhogar object types make sense because these are identifying variables. <br />\nId와 idhogar가 객체형 자료인 것은 맞는 말인 것 같다<br />\nHowever, the other columns seem to be a mix of strings and numbers which we'll need to address before doing any machine learning. According to the documentation for these columns:<br />\n그러나 다른 컬럼들은 머신러닝 모델에 넣기전에 처리를 거쳐야 할 문자형과 숫자형 자료의 혼합형인 듯하다. 다음은 해당 컬럼들에 대한 정보이다<br />\ndependency: Dependency rate, calculated = (number of members of the household younger than 19 or older than 64)/(number of member of household between 19 and 64)\n\nedjefe: years of education of male head of household, based on the interaction of escolari (years of education), head of household and gender, yes=1 and no=0\n\nedjefa: years of education of female head of household, based on the interaction of escolari (years of education), head of household and gender, yes=1 and no=0\n\nThese explanations clear up the issue. For these three variables, \"yes\" = 1 and \"no\" = 0. We can correct the variables using a mapping and convert to floats.<br />\n위들 컬럼을 보면 yes는 1, no는 0 으로 mapping할 필요가 있어 보인다."},{"metadata":{"trusted":true,"_uuid":"7e1c3aee027fe5d42081c12d238ddb2fa84a854a"},"cell_type":"code","source":"mapping = {'yes':1,'no':0}\n\nneedToChange = train.select_dtypes('object').columns[-1:1:-1]\n\nfor df in [train,test]:\n    for col in needToChange:\n        df[col] = df[col].replace(mapping).astype(np.float64)\n        \ntrain.loc[:,needToChange].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"424c83a84c57f650e294d384e7d585fc9a6a0817"},"cell_type":"code","source":"fig,ax = plt.subplots(3,1,figsize=[16,18])\n\nfor i,col in enumerate(needToChange):\n    ax = plt.subplot(3,1,i+1)\n    for proverty_level,color in colors.items():\n        sns.kdeplot(train.loc[train['Target']==proverty_level,col].dropna(),ax=ax,color=color,label=poverty_mapping[proverty_level])\n    \n    ax.set(title=f'{col.capitalize()} Distribution',xlabel=f'{col}',ylabel='Density')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d0cd7c98ae6472931147c9f8651929ef582ac0d7"},"cell_type":"markdown","source":"These variables are now correctly represented as numbers and can be fed into a machine learning model.<br />\n=> 이제 이 변수들은 완전히 숫저화 되엇으며 머신러닝 모델에 넣을 준비가 되었다\n\nTo make operations like that above a little easier, we'll join together the training and testing dataframes. <br />\n=> 위의 작업을 통해 우리는 훈련데이터와 테스트셋 데이터에 모두 적용할 수 있었다<br />\nThis is important once we start feature engineering because we want to apply the same operations to both dataframes so we end up with the same features. <br />\n=> 이는 feature engineering하는데 있어서 핵심적이다<br />\nLater we can separate out the sets based on the Target.<br />\n=> 이후에 타겟값을 기초로하여 훈련셋과 테스트셋을 다시 나눌 수 있을 것이다."},{"metadata":{"trusted":true,"_uuid":"b069e825951686eb604ced4c6f83b7019ab6b73f"},"cell_type":"code","source":"# Add null Target column to test\ntest['Target'] = np.nan\ndata = train.append(test, ignore_index = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"947a83e2fb9981ec847f2c8224a13549a30c3d0d"},"cell_type":"markdown","source":"## Exploring Label Distribution<br />\nNext, we can get an idea of how imbalanced the problem is by looking at the distribution of labels.<br />\n=> 다음으로 어떻게 라벨 불균형 문제를 해결해야 하는지에 대한 아이디어를 얻을 수 있을 것이다.<br />\nThere are four possible integer levels, indicating four different levels of poverty. <br />\n=> 알다시피 우리는 네개의 빈부수준을 나태내는 단계를 가지고 있다 <br />\nTo look at the correct labels, we'll subset only to the columns where parentesco1 == 1 because this is the head of household, the correct label for each household.<br />\n=> 정확한 라벨들을 보기 위해서 우리는 가장의 서브셋만을 이용할 것이다\n\nThe bar plot below shows the distribution of training labels (since there are no testing labels).<br />\n아래의 그림은 훈련 데이터셋의 타겟 라벨의 값의 분포를 보여준다"},{"metadata":{"trusted":true,"_uuid":"3edc333d9963558b2dbdd22ac491a5b70a1ddd1e"},"cell_type":"code","source":"heads = data.loc[data['parentesco1'] == 1].copy()\n\ntrain_labels = data.loc[(data['Target'].notnull()) & (data['parentesco1'] == 1),['Target','idhogar']]\n\nlabel_counts = train_labels['Target'].value_counts().sort_index()\n\nlabel_counts.plot(kind='bar',figsize=[8,6],color=colors.values(),edgecolor='k',linewidth = 2)\n\nplt.xlabel('Poverty Level'); plt.ylabel('Count'); \nplt.xticks([x - 1 for x in poverty_mapping.keys()], \n           list(poverty_mapping.values()), rotation = 60)\nplt.title('Poverty Level Breakdown');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8b3a2cd083adaed184824a803222818a001d7186"},"cell_type":"markdown","source":"We are dealing with an imbalanced class problem (which makes it intriguing why the contest organizers choose the macro F1 score as the metric instead of weighted F1!). <br />\n=> 위의 그림을 통해 우리는 클래스 불균형 문제를 다루고 있음을 알 수 있습니다 (이는 왜 컨테스트의 주최자가 가중 F1 점수가 아닌 Macro F1점수를 택했는지 의문점을 들게 합니다)<br />\nThere are many more households that classify as non vulnerable than in any other category. The extreme poverty class is the smallest (I guess this should make us optimistic!).<br />\n=> 다른 카테고리보다 취약계층이 아닌 가구들이 더 많음을 알 수 있습니다. 그리고 극도의 빈곤게층은 가장 작음을 또한 할 수 있습니다.\n\nOne problem with imbalanced classification problems is that the machine learning model can have a difficult time predicting the minority classes because it sees far less examples. <br />\n=> 클래스 불균형문제는 머신러닝 모델이 소수의 클래스들을 예측하는데 문제점이 발생하는 원인일 수 있습니다. 왜냐하면 그것에 대한 예제가 적기 때문입니다.\n\nThink about this in human terms: if we are classifiying poverty and we see far more cases of no poverty than extreme poverty, it will make it more difficult for us to identify the high poverty households because of less exposure. [인간의 관점에서의 이해를 돕는 예시]\n\nOne potential method to address class imbalanceds is through oversampling (which is covered in more advanced notebooks).<br />\n=> 따라서 이러한 클래스 불균형 문제를 해결하는 방법은 오버샘플링 이라는 방법이 존재합니다."},{"metadata":{"_uuid":"83f2f60f7bc15cb4e8685cd222ab8d9f3aa30bb5"},"cell_type":"markdown","source":"## Addressing Wrong Labels [잘못된 라벨 살펴보기]<br />\nAs with any realistic dataset, the Costa Rican Poverty data has some issues. <br />\n=> 우리가 살펴볼 데이터셋에도 실제 데이터셋이 가지고 있는 문제와 같은 것이 존재합니다.<br />\nTypically, 80% of a data science project will be spent cleaning data and fixing anomalies/errors. <br />\n80%의 데이터 사이언스 프로젝트는 데이터를 정제하고 오류를 수정하는데 시간을 사용합니다.<br />\nThese can be either human entry errors, measurement errors, or sometimes just extreme values that are correct but stand out. <br />\n=> 이것은 아마도 사람이 데이터를 넣는데 있어서 에러이거나, 측정오류이거나 또는 어떤 극한의 이상치인 경우가 있습니다.<br />\nFor this problem, some of the labels are not correct because individuals in the same household have a different poverty level. <br />\n=> 이문제로 인해서, 몇개의 라벨은 같은 가구이지만 타겟 라벨이 다른 문제가 발생합니다.<br />\nWe're not told why this may be the case, but we are told to use the head of household as the true label.<br />\n=> 이 컴피티션에서는 왜인지는 모르겠지만, 우리는 가구의 가장의 라벨이 맞다는 것을 계속 언급해 왔습니다.\n\nThat information makes our job much easier, but in a real-world problem, we would have to figure out the reason Why the labels are wrong and how to address the issue on our own. <br />\n=> 이러한 정보는 우리가 하는 일을 더욱더 쉽게 하지만, 실제 영역에서는 우리는 왜 라벨들이 다른지에 대해서 찾아내야만 합니다.<br />\nThis section fixes the issue with the labels although it is not strictly necessary: I kept it in the notebook just to show how we may deal with this issue.<br />\n=> 이렇게 라벨을 고치는 것이 필수는 아니지만 우리는 고쳐보려고합니다.\n\nIdentify Errors<br />\nFirst we need to find the errors before we can correct them. <br />\n일단 오류를 정정하기 전에 오류를 수집해봅시다.<br />\nTo find the households with different labels for family members, we can group the data by the household and then check if there is only one unique value of the Target.<br />\n다른 라벨을 가진 가족을 찾아내기 위해서 우리는 데이터들을 가구별로 그룹을 짓고 타겟의 값이 고유한지 살펴보려고합니다."},{"metadata":{"trusted":true,"_uuid":"305318f9ad99cc3ce5734886969d754a15734d91"},"cell_type":"code","source":"all_equal = train.groupby('idhogar')['Target'].apply(lambda x:x.nunique() == 1)\n\nnot_equal = all_equal[all_equal != True]\nprint('There are {} households where the family members do not all have the same target.'.format(len(not_equal)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c771d34cd6769ddcc79053af692362545f7669ab"},"cell_type":"markdown","source":"Let's look at one example.\n=> 그렇다면 어떤 한 가족의 예시를 봅시다"},{"metadata":{"trusted":true,"_uuid":"09a14f45ec213716676cd95c48d71b4fbc050af4"},"cell_type":"code","source":"train[train['idhogar'] == not_equal.index[0]][['idhogar', 'parentesco1', 'Target']]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9ad6ae9cefbc16f2f125fa934ef34c2980e691ed"},"cell_type":"markdown","source":"The organizers tell us that the correct label is that for the head of household, where parentesco1 == 1. <br />\n=> 대회의 주최자는 가장의 라벨은 정확하다고 언급했습니다..<br />\nFor this household, the correct label is 3 for all members. <br />\n=> 그래서 위의 경우에는 모든 가족들에게 3이라는 값을 할당하는 것이 옳습니다.<br />\nWe can correct this (as shown later) by reassigning all the individuals in this household the correct poverty level.<br />\n=> 우리는 가장의 라벨 값을 잘못 개재되어 있는 라벨에 넣어 줄겁니다.<br />\nIn the real-world, you might have to make the tough decision of how to address the problem by yourself (or with the help of your team).<br />\n=> 실제로는, 어떻게 이러한 문제를 해결할지 고민을 해봐야 할 것입니다.\n\n## Families without Heads of Household [가장이 없는 가구]<br />\nWe can correct all the label discrepancies by assigning the individuals in the same household the label of the head of household. <br />\n=> 우리는 이러한 타겟 라벨 불일치를 가장의 타겟 라벨을 할당해 줌으로써 옳게 만들어 줄 수 있습니다<br />\nBut wait, you may ask: \"What if there are households without a head of household? And what if the members of those households have differing values of the label?\"<br />\n=> 그러나 간단한 질문을 해봅시다 \"만약 가장이 없는 가구가 있다면? 그리고 그 구성원들이 다른 타겟 라벨을 가지고 있다면?\"<br />\nWell, since you asked, let's investigate exactly that question!<br />\n=> 음... 그렇다면 자세히 이 질문에 대해서 알아보도록 합시다"},{"metadata":{"trusted":true,"_uuid":"58b4a783ab32306a02b16e7308861fac968369c6"},"cell_type":"code","source":"household_leader = train.groupby('idhogar')['parentesco1'].sum()\n\nhousehold_nohead = train.loc[train['idhogar'].isin(household_leader[household_leader==0].index),:]\nprint('There are {} households without a head.'.format(household_nohead['idhogar'].nunique()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5a95c0c55193a84f6178cf562fe18dc7ed0f8ca8"},"cell_type":"code","source":"household_nohead_equal = household_nohead.groupby('idhogar')['Target'].apply(lambda x:x.nunique() == 1)\nprint('{} Households with no head have different labels.'.format(sum(household_nohead_equal == False)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"df0a15a6897ee3671a95092b98fb173a927e6cda"},"cell_type":"markdown","source":"Well that's a relief! <br />\n=> 다행입니다<br />\nThis means that we don't have to worry about a household both where there is no head AND the members have different values of the label! <br />\n=> 위를 통해 우리가 우려할 뻔 했던 두 가지 상황이 동시에 일어나는 경우는 없다는 것을 파악했습니다.<br />\nFor this problem, according to the organizers, if a household does not have a head, then there is no true label. <br />\n=> 이 문제 때문에, 컴피티션의 계획자는 가장이 없는 가구는 제대로된 라벨을 가지고 있지 않다고 언급했습니다.<br />\nTherefore, we actually won't use any of the households without a head for training Nonetheless, it's still a good exercise to go through this process of investigating the data!<br />\n=> 그러므로 우리는 가장없는 가구의 구성원들을 사용하지 않을 것입니다. \n\n## Correct Errors [오류수정]<br />\nNow we can correct labels for the households that do have a head AND the members have different poverty levels.<br />\n=> 이제 우리는 각각의 가구의 타겟 라벨을 가장의 타겟라벨로 바꿔주도록 해봅시다."},{"metadata":{"trusted":true,"_uuid":"575eddb84551e9b9ec3024aa0aacebdfd03194c7"},"cell_type":"code","source":"for household in not_equal.index:\n    true_label = int(train.loc[(train['idhogar'] == household) & (train['parentesco1'] == 1)]['Target'])\n    train.loc[train['idhogar'] == household,'Target'] = true_label\n    \nall_equal = train.groupby('idhogar')['Target'].apply(lambda x:x.nunique() == 1)\nnot_equal = all_equal[all_equal != True]\n\nprint('There are {} households where the family members do not all have the same target.'.format(len(not_equal)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ce14d1ee97463fa25ccb29c2d9a6adc5b471b244"},"cell_type":"markdown","source":"Since we are only going to use the heads of household for the labels, this step is not completely necessary but it shows a workflow for correcting data errors like you may encounter in real life. Don't consider it extra work, just practice for your career!<br />\n=> 우리가 가장의 라벨만 사용하겠다고 했기 때문에, 이 과정은 별로 불필요해보이지만 기 과정은 실제 데이터셋에서 만날수 있는 에러를 고치는 방안중 하나입니다. 그렇기 때문에 귀찮은 일을 했다고 생각하지 말아줬으면 합니다.\n\n## Missing Variables [손실 값]<br />\nOne of the most important steps of exploratory data analysis is finding missing values in the data and determining how to handle them. <br />\n=> EDA에서 가장 중요한 과정은 손실 값들을 찾아내고 이를 어떻게 다룰지 결정하는 것입니다.\n\nMissing values have to be filled in before we use a machine learning model and we need to think of the best strategy for filling them in based on the feature: this is where we'll have to start digging into the data definitions.<br />\n=> 손실 값은 머신러닝 모델을 이용하기 전에 채워져야 합니다. 그리고 이를 채우는데 적절한 전략을 생각해봐야합니다. 이는 우리가 데이터의 정의를 살펴봐야하는 곳입니다.<br />\n\nFirst we can look at the percentage of missing values in each column.<br />\n=> 일단은 각각의 컬럼의 손실 값들의 비율을 계산해 봅시다."},{"metadata":{"trusted":true,"_uuid":"4cf1f082e5befb20ad7c417374cb59b7070d6ac2"},"cell_type":"code","source":"missing = pd.DataFrame(data.isnull().sum()).rename(columns={0:'total'})\nmissing['percent'] = missing['total'] / len(data)\nmissing.sort_values(by='percent',ascending=False).head(10).drop('Target')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2180cfb951c540a872f7be59533cb172d53c3c1e"},"cell_type":"markdown","source":"We don't have to worry about the Target becuase we made that NaN for the test data. However, we do need to address the other 3 columns with a high percentage of missing values.<br />\n=> 타겟값에 있어서 손실 값은 고려하지 않아도 됩니다. 왜냐하면 우리는 테스트셋에 대해서는 임의로 손실 값을 만드어 주었기 때문이죠. 그러나 3개의 높은 비율의 손실 값을 가진 컬럼을 찾아냈습니다.\n\nv18q1: Number of tablets\n\nLet's start with v18q1 which indicates the number of tablets owned by a family. <br />\n=> 가족이 소유한 tablet의 수를 나타내는 컬럼에 대해서 알아 봅시다<br />\nWe can look at the value counts of this variable. \n\nSince this is a household variable, it only makes sense to look at it on a household level, so we'll only select the rows for the head of household.<br />\n=> 이것은 가구별 변수이기 대문에, 가구당 레벨에서 이해를 하는 것이 옳아보입니다. 그래서 우리는 가장에 대해서만 알아보도록 합시다\n\n## Function to Plot Value Counts [해당 컬럼의 값의 수를 시각화]<br />\nSince we might want to plot value counts for different columns, we can write a simple function that will do it for us!"},{"metadata":{"trusted":true,"_uuid":"a9734f8f3e750a978d86bdf316dd2e7d205f04e1"},"cell_type":"code","source":"def plot_value_counts(df,col,head_only=False):\n    if head_only:\n        df = df.loc[df['parentesco1']==1,col]\n    plt.figure(figsize=[8,6])\n    df[col].value_counts().sort_index().plot(kind='bar',color='blue',edgecolor='k',linewidth=2)\n    plt.xlabel(f'{col}')\n    plt.title(f'{col} Value Counts')\n    plt.ylabel('Count')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e4e079c32df5b943197d93e1791b4fbe62d91f96"},"cell_type":"code","source":"plot_value_counts(heads,'v18q1')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b244625563d72e71b6cbe15d7015b41105bc024d"},"cell_type":"markdown","source":"It looks like the most common number of tablets to own is 1 if we go only by the data that is present. <br />\n=> 존재하는 데이터만 고려했을 때 1대의 테블릿을 보유한 숫자가 가장 많아 보입니다.<br />\nHowever, we also need to think about the data that is missing. <br />\n=> 그러나 데이터가 손실 되었음을 염두할 필요가 있습니다.<br />\nIn this case, it could be that families with a nan in this category just do not own a tablet! <br />\n=> 그래서, 해당 수치가 손실된 가족의 경우에는 테블릿을 가지고 있지 않을 것이라고 추측됩니다.<br />\nIf we look at the data definitions, we see that v18q indicates whether or not a family owns a tablet. <br />\n=>v18q컬럼을 보게되면 해당 컬럼은 해당가족이 테블릿을 가지고 있는지 없는지 여부를 나타냅니다<br />\nWe should investigate this column combined with the number of tablets to see if our hypothesis holds.<br />\n=> 그래서 우리의 가정이 맞는지 확인하기 위해서 해당 컬럼을 같이 알아볼 필요가 있을 듯합니다.\n\nWe can groupby the value of v18q (which is 1 for owns a tablet and 0 for does not) and then calculate the number of null values for v18q1. <br />\n=> 우리는 그 컬럼을 그룹할 수 있습니다.<br />\nThis will tell us if the null values represent that the family does not own a tablet.<br />\n=> 이 값이 null인 경우는 가족이 테블릿을 가지고 있지 않음을 알 수 있습니다."},{"metadata":{"trusted":true,"_uuid":"5dc6c198acd592202025eab1fc5db8624c3bff87"},"cell_type":"code","source":"heads.groupby('v18q')['v18q1'].apply(lambda x:x.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"04b3f4659d6a856b6f570c053228d481bdf01b5f"},"cell_type":"markdown","source":"Well, that solves the issue! Every family that has nan for v18q1 does not own a tablet. Therefore, we can fill in this missing value with zero.<br />\n=> v18q1에서 테블릿을 가지지 못한 모든 가족들이 nan값을 가짐을 찾아냈습니다. 그래서 우리는 빈 값을 0으로 채워줄 것입니다."},{"metadata":{"trusted":true,"_uuid":"6bec003ff1aee3c860ac0659aac2f8f3749908ac"},"cell_type":"code","source":"data['v18q1'] = data['v18q1'].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c7396b6044bfe694db3a13c780c36ea0620e32b4"},"cell_type":"markdown","source":"## v2a1: Monthly rent payment<br />\n=> 매달 지불하는 월세\n\nThe next missing column is v2a1 which represents the montly rent payment.<br />\n=> 해당 컬럼은 매달 지불하는 월세의 정도를 나타냅니다\n\nIn addition to looking at the missing values of the monthly rent payment, it will be interesting to also look at the distribution of tipovivi_, the columns showing the ownership/renting status of the home. <br />\n=> 이 컬럼의 손실 값을 보는데 있어서, tipoviv_(집의 소유/대여 구분을 타나내는 컬럼)을 참고해봐야 할 것 같습니다<br />\nFor this plot, we show the ownership status of those homes with a nan for the monthyl rent payment.<br />\n=> 이 그림에서는 우리는 매달 내는 월세가 nan인 집의 소유상태를 보려고 합니다"},{"metadata":{"trusted":true,"_uuid":"1d01bdccdb6509031ce0cb5a4fa6401e53ab2906"},"cell_type":"code","source":"own_variables = [x for x in data if x.startswith('tipo')]\n\ndata.loc[data['v2a1'].isnull(),own_variables].sum().plot(kind='bar',figsize=[10,8],color='green',edgecolor='k',linewidth=2)\n\nplt.xticks([0,1,2,3,4], ['Owns and Paid Off', 'Owns and Paying', 'Rented', 'Precarious', 'Other'], rotation = 60)\nplt.title('Home Ownership Status for Households Missing Rent Payments', size = 18);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"be71ac33c56b98af7e027bb43406cc8764d59ab5"},"cell_type":"markdown","source":"The meaning of the home ownership variables is below:<br />\n=> 해당 갑의 정보는 아래와 같습니다\n\ntipovivi1, =1 own and fully paid house<br />\ntipovivi2, \"=1 own,  paying in installments\"<br />\ntipovivi3, =1 rented<br />\ntipovivi4, =1 precarious<br />\ntipovivi5, \"=1 other(assigned,  borrowed)\"\n\nWe've solved the issue! Well, mostly: the households that do not have a monthly rent payment generally own their own home. <br />\n=> 우리는 또다른 문제를 해결했습니다. 매달 내는 월세의 값이 없는 이유는 대부분 이미 그들 소유의 집에 살고 있기 때문입니다<br />\nIn a few other situations, we are not sure of the reason for the missing information.<br />\n=> 몇몇 다른 상황들에 대해서는 왜 손실 값을 가지는지는 확신할 수 없을 것 같습니다\n\nFor the houses that are owned and have a missing monthly rent payment, we can set the value of the rent payment to zero. <br />\n=>그래서 해당하는 경우의 월세값은 0으로 설정 할 수 있을 것 같습니다<br />\nFor the other homes, we can leave the missing values to be imputed but we'll add a flag (Boolean) column indicating that these households had missing values.<br />\n=> 다른 집들의 경우에는 손실 값으로 남겨두지만, 하나의 논리형 컬럼을 이용하여 손실 값을 가지고 있음을 표시해둘 것입니다."},{"metadata":{"trusted":true,"_uuid":"687d55d7278148238e67e058d92a2f84f46d9c98"},"cell_type":"code","source":"data.loc[data['tipovivi1'] == 1,'v2a1'] = 0\n\ndata['v2a1-missing'] = data['v2a1'].isnull()\ndata['v2a1-missing'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2f422b2f2c35ce65adcacbffcff7d8c830c8d0c2"},"cell_type":"markdown","source":"## rez_esc: years behind in school<br />\n=> 학교를 떠난 후의 년수\n\nThe last column with a high percentage of missing values is rez_esc indicating years behind in school.<br />\nFor the families with a null value, is possible that they have no children currently in school.<br />\n=> 손실 값을 가지고 있는 가족들에 대하여, 최근에 학교에 진학한 아이들이 없을 수 있다<br />\nLet's test this out by finding the ages of those who have a missing value in this column and the ages of those who do not have a missing value.<br />\n=> 그래서 해당 컬럼의 손실값을 가지고있는 사람들의 연령과 손실값을 가지고 있지 않는 사람들의 연령을 비교해보자"},{"metadata":{"trusted":true,"_uuid":"d7d8886d80429e3b2084dafb99c0c884dc5af8b5"},"cell_type":"code","source":"data.loc[data['rez_esc'].notnull(),'age'].describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c90711eea761110ff6f1557b58103e49c8fad33f"},"cell_type":"markdown","source":"What this tells us is that the oldest age with a missing value is 17.\n\nFor anyone older than this, maybe we can assume that they are simply not in school. Let's look at the ages of those who have a missing value.<br />\n=> 17살보다 나이가 많은 사람들에 대해, 우리는 학교에 다니고 있지 않다고 가정할 수 있을 것 같습니다. 그렇다면 손실값을 가지고 있는 사람들에 대해서 알아볼까요?"},{"metadata":{"trusted":true,"_uuid":"3502f68801d2a46117ea103b9e5d2868bcf042d8"},"cell_type":"code","source":"data.loc[data['rez_esc'].isnull(),'age'].describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cc9360782aa1b5bafbda8bfa13b58a3f7262a5a3"},"cell_type":"markdown","source":"If we read through some of the discussions for this competition, we learn that this variable is only defined for individuals between 7 and 19. <br />\n=> 만약 이 컴피티션에 대한 디스커션을 읽어봤다면, 우리는 이 값이 7살과 19살 사이에 사람들에게만 있음을 알 수 있습니다.<br />\nAnyone younger or older than this range presumably has no years behind and therefore the value should be set to 0. <br />\n=> 그래서 이 연령에 해당하지 못하는 사람들에게는 0의 값을 부여해야 할 것 같습니다.<br />\nFor this variable, if the individual is over 19 and they have a missing value, or if they are younger than 7 and have a missing value we can set it to zero. <br />\nFor anyone else, we'll leave the value to be imputed and add a boolean flag.<br />\n=> 그래서 그 연령에 해당하지 않는 사람들의 경우에는 논리형 컬럼으로 남겨둘 것입니다."},{"metadata":{"trusted":true,"_uuid":"debc20a334fc6ca9ca64f4077384f7226b39cf1d"},"cell_type":"code","source":"# If individual is over 19 or younger than 7 and missing years behind, set it to 0\ndata.loc[((data['age'] > 19) | (data['age'] < 7)) & (data['rez_esc'].isnull()), 'rez_esc'] = 0\n\n# Add a flag for those between 7 and 19 with a missing value\ndata['rez_esc-missing'] = data['rez_esc'].isnull()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8cb7b90e332182882affe203acf4bf24cf29da1c"},"cell_type":"markdown","source":"There is also one outlier in the rez_esc column. <br />\n=> rez_esc 컬럼에 하나의 이상치를 찾아 볼 수 있습니다.<br />\nAgain, if we read through the competition discussions, we learn that the maximum value for this variable is 5. Therefore, any values above 5 should be set to 5.<br />\n=> 디스커션을 읽었다면 해당 값의 최대치는 5입니다. 따라서 5의 값을 넘는 것들에게 모두 5를 할당해 줍시다"},{"metadata":{"trusted":true,"_uuid":"cc0d2e4e4c6255d700c24d7da25b5c061d112017"},"cell_type":"code","source":"data.loc[data['rez_esc']>5,'rez_esc'] = 5","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"da4a482f36ed9b6d4068e5f8aca1855d89cda3dc"},"cell_type":"markdown","source":"## Plot Two Categorical Variables [두개의 카테고리컬한 변수에 대한 시각화]<br />\nTo show how two categorical variables interact with one another, there are a number of plotting options: scatterplots, faceted bar plots, boxplots, etc. <br />\n=> 두 개의 카테고리컬한 변수들에 대해서 시각화하는 방법에는: 산포도, 박스그림 등이 있습니다<br />\nI wasn't satisfied with any of these choices so I wrote the function below, which essentially is a scatterplot of two categoricals where the size of the points represent the percentage of a given y-value represented by each x-value.\n=>"},{"metadata":{"trusted":true,"_uuid":"8d76729ef123410848dc4eac34954e81e9b017d3"},"cell_type":"code","source":"def plot_categoricals(x, y, data, annotate = True):\n    \"\"\"Plot counts of two categoricals.\n    Size is raw count for each grouping.\n    Percentages are for a given value of y.\"\"\"\n    \n    # Raw counts \n    raw_counts = pd.DataFrame(data.groupby(y)[x].value_counts(normalize = False))\n    raw_counts = raw_counts.rename(columns = {x: 'raw_count'})\n    \n    # Calculate counts for each group of x and y\n    counts = pd.DataFrame(data.groupby(y)[x].value_counts(normalize = True))\n    \n    # Rename the column and reset the index\n    counts = counts.rename(columns = {x: 'normalized_count'}).reset_index()\n    counts['percent'] = 100 * counts['normalized_count']\n    \n    # Add the raw count\n    counts['raw_count'] = list(raw_counts['raw_count'])\n    \n    plt.figure(figsize = (14, 10))\n    # Scatter plot sized by percent\n    plt.scatter(counts[x], counts[y], edgecolor = 'k', color = 'lightgreen',\n                s = 100 * np.sqrt(counts['raw_count']), marker = 'o',\n                alpha = 0.6, linewidth = 1.5)\n    \n    if annotate:\n        # Annotate the plot with text\n        for i, row in counts.iterrows():\n            # Put text with appropriate offsets\n            plt.annotate(xy = (row[x] - (1 / counts[x].nunique()), \n                               row[y] - (0.15 / counts[y].nunique())),\n                         color = 'navy',\n                         s = f\"{round(row['percent'], 1)}%\")\n        \n    # Set tick marks\n    plt.yticks(counts[y].unique())\n    plt.xticks(counts[x].unique())\n    \n    # Transform min and max to evenly space in square root domain\n    sqr_min = int(np.sqrt(raw_counts['raw_count'].min()))\n    sqr_max = int(np.sqrt(raw_counts['raw_count'].max()))\n    \n    # 5 sizes for legend\n    msizes = list(range(sqr_min, sqr_max,\n                        int(( sqr_max - sqr_min) / 5)))\n    markers = []\n    \n    # Markers for legend\n    for size in msizes:\n        markers.append(plt.scatter([], [], s = 100 * size, \n                                   label = f'{int(round(np.square(size) / 100) * 100)}', \n                                   color = 'lightgreen',\n                                   alpha = 0.6, edgecolor = 'k', linewidth = 1.5))\n        \n    # Legend and formatting\n    plt.legend(handles = markers, title = 'Counts',\n               labelspacing = 3, handletextpad = 2,\n               fontsize = 16,\n               loc = (1.10, 0.19))\n    \n    plt.annotate(f'* Size represents raw count while % is for a given y value.',\n                 xy = (0, 1), xycoords = 'figure points', size = 10)\n    \n    # Adjust axes limits\n    plt.xlim((counts[x].min() - (6 / counts[x].nunique()), \n              counts[x].max() + (6 / counts[x].nunique())))\n    plt.ylim((counts[y].min() - (4 / counts[y].nunique()), \n              counts[y].max() + (4 / counts[y].nunique())))\n    plt.grid(None)\n    plt.xlabel(f\"{x}\"); plt.ylabel(f\"{y}\"); plt.title(f\"{y} vs {x}\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b7d1c270da3db6d66c219509150b4108d6b7275a"},"cell_type":"code","source":"plot_categoricals('rez_esc', 'Target', data);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a751bf4f8fd21f0713cf13fb95a8e8a75819f921"},"cell_type":"markdown","source":"The size of the markers represents the raw count. <br />\n=> 하나의 점의 크기는 개수를 뜻합니다<br />\nTo read the plot, choose a given y-value and then read across the row. <br />\n=> 표를 읽으려면, 임의의 세로축 값을 고르고 가로로 하나씩 읽으면 됩니다.<br />\nFor example, with a poverty level of 1, 93% of individuals have no years behind with a total count of around 800 individuals and about 0.4% of individuals are 5 years behind with about 50 total individuals in this category.<br />\n=> 예를들면 빈곤단계가 1인경우에는, 93퍼센트의 개개인이 years behind가 0이며 이 크기는 대략 800명 정도 된다는 것을 의미합니다.<br />\nThis plot attempts to show both the overall counts and the within category proportion; it's not perfect , but I gave it a shot!"},{"metadata":{"trusted":true,"_uuid":"c352261d680f37a1a222696f0e2b3e68466a666e"},"cell_type":"code","source":"plot_categoricals('escolari', 'Target', data, annotate = False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5657974fc0b4883a6e1cbfa7527eb6b4cb7016f0"},"cell_type":"markdown","source":"The remaining missing values in each column will be filled in, a process known as Imputation. <br />\n=> 남은 각 컬럼의 손실 값들은 Imputation이라고 거치는 손실 값을 채우는 과정을 거치게 될 것입니다.<br />\nThere are several types of imputation commonly used, and one of the simplest and most effective methods is to fill in the missing values with the median of the column.<br />\n=> 몇몇의 imputation 타입은 주로 사용되며, 그중 가장 간단하고 효과적인 방법은 중앙값을 넣는 것입니다."},{"metadata":{"_uuid":"55362739f412b4de837e2b312f42f3d57b6e5513"},"cell_type":"markdown","source":"As a final step with the missing values, we can plot the distribution of target for the case where either of these values are missing.<br />\n=> 손실값의 마지막 단게로서, 우리는 손실값들의 타겟 라벨의 분포를 그려볼 것입니다."},{"metadata":{"trusted":true,"_uuid":"e83e2daac3e7721ce3ab8e602b77ad5b88a90138"},"cell_type":"code","source":"plot_value_counts(data[(data['rez_esc-missing'] == 1)], 'Target')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"803b6801bf46e358cb96202455dbfa363b067b43"},"cell_type":"code","source":"plot_value_counts(data[(data['v2a1-missing'] == 1)], 'Target')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aecffc2765d7d8563975c6f3b23972fd5aac6f30"},"cell_type":"markdown","source":"This looks like it could be an indicator of more poverty given the higher prevalence of 2: moderate poverty.\n\nThis represents an important point: sometimes the missing information is just as important as the information you are given.<br />\n=> 가끔은 손실 값들이 중요한 정보를 가지고 있다는 것을 보여주기도 합니다."},{"metadata":{"_uuid":"fdf637b3683a67b1957f9553f3382bf00ea57120"},"cell_type":"markdown","source":"## Feature Engineering\n\nThere is plenty more exploratory data analysis we can do, but first we should work on consolidating our data at a household level. <br />\n=> 우리가 할 수 있는 EDA는 굉장히 많지만, 우리의 데이터들을 가구당 단위로 합병하는 작업을 진행해야합니다.<br />\nWe already have some of the information for each household, but for training, we will need all of the information summarized for each household. <br />\n=> 우리는 각각의 가구별 정보를 조금 가지고 있지만, 훈련을 위해서는 각각의 가구를 위해 정보를 간략화할 필요가 있습니다.<br />\nThis means grouping the individuals in a house (groupby) and performing an aggregation (agg) of the individual variables.<br />\n=> 이는 개개인을 가구별로 그룹을 짓고 개별 값을 병합해야합니다.\n\nIn another notebook, I show how we can use automated feature engineering to do this, and automated feature engineering should be a standard part of the machine learning workflow. <br />\n=> 필자는 feature engineering에 있어서 자동화된 방법을하려고 하며, 이러한 방법은 머신러닝 작업 흐름에서 굉장히 기본적인 부분입니다<br />\nRight now, we'll stick to doing this by hand, but definitely take a look at automated feature engineering in Featuretools.<br />\n=> 우리는 수작업으로 할 것이지만, 자동적으로 하는 방법을 꼭 참고해보시길 바랍니다\n\n## Column Definitions [컬럼의 의미]<br />\nSometimes in data science we have to get our hands dirty digging through the data or do tedious tasks that take a lot of time. <br />\n=> 데이터 사이언스에서 우리는 직접 데이터에서 무언가를 찾아내야하고 시간이 많이걸리는 지루한 작업들을 해야할 수 있습니다.<br />\nThis is that part of the analysis: we have to define the columns that are at an individual level and at a household level using the data decsriptions. <br />\n=> 이것은 분석의 단계입니다: 각각의 컬럼을 정의하고 어느 것이 개개인의 수준인가 또는 가구별 수준인지 설명을 통해 알아내야합니다<br />\nThere is simply no other way to identify which variables at are the household level than to go through the variables themselves in the data description. <br />\n=> 데이터 설명을 보지 않고서는 이것을 밝혀낼만한 다른 방법은 없을 것입니다.<br />\nExcept, I've already done this for you, so all you have to do is copy and paste!<br />\n=> 필자는 이미 작업을 해두었으니, 해야할 일은 가져다 쓰는 것입니다.\n\nWe'll define different variables because we need to treat some of them in a different manner. <br />\n=> 다른 방법으로 몇몇이 사용되기 때문에 이들을 위해 몇몇의 다른 변수를 선언할 것입니다<br />\nOnce we have the variables defined on each level, we can work to start aggregating them as needed.<br />\n=> 각각의 레벨에서 쓰여질 변수들이 정의되고 난 뒤에, 이를 사용하기 위해서 병합을 시작할 것입니다.\n\nThe process is as follows<br />\n=> 해당 과정은 아래의 과정과 같습니다.\n\n1. Break variables into household level and invididual level<br />\n=> 변수를 가구단위 와 개별단위로 구분<br />\n2. Find suitable aggregations for the individual level data<br />\n=> 개개인 수준의 데이터를 위해서 적절한 병합방법을 찾습니다<br />\n    Ordinal variables can use statistical aggregations<br />\n    => Ordinal 값들은 통계적 병합에 사용될 것이며<br />\n    Boolean variables can also be aggregated but with fewer stats<br />\n    => 논리형 자료들은 적은 통계치들에 병합될 것이며<br />\n3. Join the individual aggregations to the household level data<br />\n=> 개인 수준의 데이터를 가구별 수준의 데이터에 합치고\n\n\n## Define Variable Categories[변수의 카테고리들을 정의]\n\nThere are several different categories of variables:\n\n1.Individual Variables: these are characteristics of each individual rather than the household<br />\n    Boolean: Yes or No (0 or 1)<br />\n    Ordered Discrete: Integers with an ordering<br />\n2. Household variables<br />\n    Boolean: Yes or No<br />\n    Ordered Discrete: Integers with an ordering<br />\n    Continuous numeric<br />\n3. Squared Variables: derived from squaring variables in the data<br />\n4. Id variables: identifies the data and should not be used as features\n\nBelow we manually define the variables in each category. This is a little tedious, but also necessary.<br />\n=> 아래에 우리는 각각의 카테고리의 변수들을 정의해두었습니다."},{"metadata":{"trusted":true,"_uuid":"55dbc4a1bdaa6492198809b6e21a10b9536bd66b"},"cell_type":"code","source":"id_ = ['Id', 'idhogar', 'Target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0d47e93c79280ac8732b5fd424f0329cc21d772f"},"cell_type":"code","source":"ind_bool = ['v18q', 'dis', 'male', 'female', 'estadocivil1', 'estadocivil2', 'estadocivil3', \n            'estadocivil4', 'estadocivil5', 'estadocivil6', 'estadocivil7', \n            'parentesco1', 'parentesco2',  'parentesco3', 'parentesco4', 'parentesco5', \n            'parentesco6', 'parentesco7', 'parentesco8',  'parentesco9', 'parentesco10', \n            'parentesco11', 'parentesco12', 'instlevel1', 'instlevel2', 'instlevel3', \n            'instlevel4', 'instlevel5', 'instlevel6', 'instlevel7', 'instlevel8', \n            'instlevel9', 'mobilephone', 'rez_esc-missing']\n\nind_ordered = ['rez_esc', 'escolari', 'age']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f44f5b99d4ee4034fec132adfb0a7539ad963e0d"},"cell_type":"code","source":"hh_bool = ['hacdor', 'hacapo', 'v14a', 'refrig', 'paredblolad', 'paredzocalo', \n           'paredpreb','pisocemento', 'pareddes', 'paredmad',\n           'paredzinc', 'paredfibras', 'paredother', 'pisomoscer', 'pisoother', \n           'pisonatur', 'pisonotiene', 'pisomadera',\n           'techozinc', 'techoentrepiso', 'techocane', 'techootro', 'cielorazo', \n           'abastaguadentro', 'abastaguafuera', 'abastaguano',\n            'public', 'planpri', 'noelec', 'coopele', 'sanitario1', \n           'sanitario2', 'sanitario3', 'sanitario5',   'sanitario6',\n           'energcocinar1', 'energcocinar2', 'energcocinar3', 'energcocinar4', \n           'elimbasu1', 'elimbasu2', 'elimbasu3', 'elimbasu4', \n           'elimbasu5', 'elimbasu6', 'epared1', 'epared2', 'epared3',\n           'etecho1', 'etecho2', 'etecho3', 'eviv1', 'eviv2', 'eviv3', \n           'tipovivi1', 'tipovivi2', 'tipovivi3', 'tipovivi4', 'tipovivi5', \n           'computer', 'television', 'lugar1', 'lugar2', 'lugar3',\n           'lugar4', 'lugar5', 'lugar6', 'area1', 'area2', 'v2a1-missing']\n\nhh_ordered = [ 'rooms', 'r4h1', 'r4h2', 'r4h3', 'r4m1','r4m2','r4m3', 'r4t1',  'r4t2', \n              'r4t3', 'v18q1', 'tamhog','tamviv','hhsize','hogar_nin',\n              'hogar_adul','hogar_mayor','hogar_total',  'bedrooms', 'qmobilephone']\n\nhh_cont = ['v2a1', 'dependency', 'edjefe', 'edjefa', 'meaneduc', 'overcrowding']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4db83fa6743f6b1886bbf711ccd2d2b8514690fa"},"cell_type":"code","source":"sqr_ = ['SQBescolari', 'SQBage', 'SQBhogar_total', 'SQBedjefe', \n        'SQBhogar_nin', 'SQBovercrowding', 'SQBdependency', 'SQBmeaned', 'agesq']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"de45d6e4b44b1e457793b1fe55e05c2da5eb63b3"},"cell_type":"markdown","source":"Let's make sure we covered all of the variables and didn't repeat any.<br />\n=> 혹시 반복되는 것은 없는지 확인해봅시다."},{"metadata":{"trusted":true,"_uuid":"3dcbbd1dc50e266d0894f9b7f5bc4f3cf6f02a7d"},"cell_type":"code","source":"x = ind_bool + ind_ordered + id_ + hh_bool + hh_ordered + hh_cont + sqr_\n\nfrom collections import Counter\n\nprint('There are no repeats: ', np.all(np.array(list(Counter(x).values())) == 1))\nprint('We covered every variable: ', len(x) == data.shape[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6a544466c5feddd3b1b3933aa02819c211915e27"},"cell_type":"code","source":"np.all(np.array(list(Counter(x).values())) == 1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5570c0f9f3d55051e86adf3bd3f93580d6f02ff4"},"cell_type":"markdown","source":"## Squared Variables [제곱된 값들] <br />\nFirst, the easiest step: we'll remove all of the squared variables.<br />\n=> 가장 쉬운 방법으로, 우리는 모든 이와 같은 값들을 지울 수 있습니다.<br />\nSometimes variables are squared or transformed as part of feature engineering because it can help linear models learn relationships that are non-linear. <br />\n=> 그러나 가끔씩 우리는 이러한 성격을 지닌 값들이 비선형관계를 선형모델이 학습을 하도록 도와주기 때문에 사용합니다.<br />\nHowever, since we will be using more complex models, these squared features are redundant. <br />\n=> 그러나 더욱 더 복잡한 모델을 사용할 것이기 때문에, 위와 같은 특징들은 중복이 됩니다.<br />\nThey are highly correlated with the non-squared version, and hence can actually hurt our model by adding irrelevant information and also slowing down training.<br />\n=> 이 값들은 제곱이 취해지기 전의 컬럼과 굉장히 높은 상관관계를 보이며, 이는 더 나아가 우리의 모델에 이롭지 않을 수 있습니다.<br />\n\nFor an example, let's take a look at SQBage vs age<br />\n=> 예시로 알아봅시다"},{"metadata":{"trusted":true,"_uuid":"e51acebb0b3f604da7fe0035a347390c558114b3"},"cell_type":"code","source":"sns.lmplot('age','SQBage',data=data,fit_reg=False)\nplt.title('Squared Age versus Age')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fd056e14c75013d42103e08f081d4ea2d8972874"},"cell_type":"markdown","source":"These variables are highly correlated, and we don't need to keep both in our data.<br />\n=> 위와 같이 이러한 성격의 값은 괴앚ㅇ히 상관관계가 커서 둘 다 모두를 데이터셋에 남겨둘 필요가 없습니다."},{"metadata":{"trusted":true,"_uuid":"5067f380c70c0446da4454d143f7f9a6becbae3c"},"cell_type":"code","source":"data = data.drop(columns = sqr_)\ndata.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"48bd39e58730a452e126ceab1ade405891118e2a"},"cell_type":"markdown","source":"## Id Variables<br />\nThese are pretty simple: they will be kept as is in the data since we need them for identification.<br />\n=> Id 값은 각각을 식별하기 위해 반드시 필요하므로 삭제등을 하지 않습니다."},{"metadata":{"_uuid":"797ee469f49fa5a19b94505b27a9006370a84907"},"cell_type":"markdown","source":"## Household Level Variables<br />\nFirst let's subset to the heads of household and then to the household level variables.<br />\n=> 가장들의 서브셋을 만들고 가구별 컬럼들을 추출합니다."},{"metadata":{"trusted":true,"_uuid":"ead4ff5f715fc0597982be6ac5c3ae0eb5a42765"},"cell_type":"code","source":"heads = data.loc[data['parentesco1']== 1,:]\nheads = heads[id_ + hh_bool + hh_cont + hh_ordered]\nheads.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"121108259b6e270d6d743644d06ba03cf6a11fcb"},"cell_type":"markdown","source":"For most of the household level variables, we can simply keep them as is: since we want to make predictions for each household, we use these variables as features. <br />\n=> 대부분의 가구별 수준의 변수들에 대해, 우리가 가구별로 예측을 하려고 할 때에는 위의 변수들을 사용합니다<br />\nHowever, we can also remove some redundant variables and also add in some more features derived from existing data.<br />\n=> 그러나 이 데이터들에서도 중복되는 값들을 제거해야 하며 며쳧의 특징들은 존재하는 데이터로부터 파생된 경우가 있습니다.<br />\n\nRedundant Household Variables [중복된 가구단위의 변수]\n\nLet's take a look at the correlations between all of the household variables. <br />\n=> 가구단위의 변수들의 상관관게를 알아보도록 합시다<br />\nIf there are any that are too highly correlated, then we might want to remove one of the pair of highly correlated variables.<br />\n=>만약 너무 많이 높은 상관관계를 보이는 것이 있다면, 우리는 그 쌍중 하나를 삭제할 필요가 있습니다.<br />\n\nThe following code identifies any variables with a greater than 0.95 absolute magnitude correlation.<br />\n=> 다음의 코드는 절대값 0.95이상의 상관관게를 가지는 변수들을 알아내는 코드입니다."},{"metadata":{"trusted":true,"_uuid":"c3a227cb024737e084452e82e2d4e0f7e10f46aa"},"cell_type":"code","source":"corr_matrix = heads.corr()\n\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape),k=1).astype(np.bool))\n\nto_drop = [col for col in upper.columns if any(abs(upper[col]) > 0.95)]\n\nto_drop","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5d022ad11ba588660f11a245d5a7bac8dfdb6eb0"},"cell_type":"markdown","source":"These show one out of each pair of correlated variables. To find the other pair, we can subset the corr_matrix.<br />\n=> 관련된 변수의 각각의 쌍중 하나를 보여줌. 다른 쌍을 찾기위해서는 corr_matrix를 만든다"},{"metadata":{"trusted":true,"_uuid":"588b13bb3f4bfbd135f9016ce447ade76be6aa8b"},"cell_type":"code","source":"corr_matrix.loc[corr_matrix['tamhog'].abs()>0.9,corr_matrix['tamhog'].abs()>0.9]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"afb284a1fc3e995ef6bf8697db36e6346e847e44"},"cell_type":"code","source":"sns.heatmap(corr_matrix.loc[corr_matrix['tamhog'].abs() > 0.9, corr_matrix['tamhog'].abs() > 0.9],annot=True, cmap = plt.cm.autumn_r, fmt='.3f');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"443bbc48b1f71d99b62f4e9d82756abe6d702b52"},"cell_type":"markdown","source":"\"tamhog는 가구의 사이즈를 뜻함\"<br />\n\nThere are several variables here having to do with the size of the house:<br />\n=> 집의 사이즈와 관련 있는 변수들은 아래와 같습니다.\n\nr4t3, Total persons in the household [집에 살고 있는 총인원 수]<br />\ntamhog, size of the household [가구의 크기]<br />\ntamviv, number of persons living in the household [가구에 살고 있는 사람의 수]<br />\nhhsize, household size [가구의 크기]<br />\nhogar_total, # of total individuals in the household [가구에 있는 개별 인원의 총합]<br />\n\nThese variables are all highly correlated with one another. <br />\n=> 이러한 값들은 서로 큰 연관성을 가지고 있습니다.<br />\nIn fact, hhsize has a perfect correlation with tamhog and hogar_total. <br />\n=> 실제로 hhsize는 tamhog나 hogar_total과 완벽한 선형성을 보여줍니다<br />\nWe will remove these two variables because the information is redundant. We can also remove r4t3 because it has a near perfect correlation with hhsize.<br />\n=> 이 정보는 중복이기 때문에 두 변수를 제거해 줄 것입니다. r4t3또한 거의 완벽한 중복에 가깝기 때문에 지워줄 것입니다.\n\ntamviv is not necessarily the same as hhsize because there might be family members that are not living in the household. Let's visualize this difference in a scatterplot.<br />\n=>tamviv는 hhsize와 필수적으로 같을 필요는 없습니다. 왜냐하면 가구에 살고 있지 않는 가족들도 있기 때문이죠"},{"metadata":{"trusted":true,"_uuid":"fc74ec6fae97be81c3935f4e2bf3cdfc4843a2de"},"cell_type":"code","source":"heads = heads.drop(columns = ['tamhog','hogar_total','r4t3'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1f08fe3be8fac98277b062994190699ab6109d57"},"cell_type":"code","source":"sns.lmplot('tamviv','hhsize',data,fit_reg=False,size=8)\nplt.title('Household size vs number of persons living in the household');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"10aa2d182e4d91fc684fb481dd218fcc5d551638"},"cell_type":"markdown","source":"We see for a number of cases, there are more people living in the household than there are in the family. <br />\n=> 우리는 다양한 케이스들을 봤고, 가족의 수보다 가구에 살고 있는 사람들이 많습니다<br />\nThis gives us a good idea for a new feature: the difference between these two measurements!<br />\n=> 이는 우리가 두 인원의 차이를 새로운 feature로 쓰는 인사이트를 제공합니다.\n\nLet's make this new feature.<br />\n=> 새로운 feature를 만들어 봅시다."},{"metadata":{"trusted":true,"_uuid":"cf07d2375d6a5331b0b5460153e65dd43f090a5b"},"cell_type":"code","source":"heads['hhsize-diff'] = heads['tamviv'] - heads['hhsize']\nplot_categoricals('hhsize-diff','Target',heads)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f01a1ba53b1a7a3a712626ba53e0cd55b506074a"},"cell_type":"markdown","source":"Even though most households do not have a difference, there are a few that have more people living in the household than are members of the household.<br />\n=> 비록 많은 가구들이 차이를 가지고 있지 않더라도, 몇몇의 차이가 나는 가구들이 존재합니다.\n\nLet's move on to the other redundant variables. First we can look at coopele<br />\n=> 이제는 다른 변수로 넘어가 봅시다"},{"metadata":{"trusted":true,"_uuid":"3cf20602ff39e1f3ba9f5639498b23a8b19eb553"},"cell_type":"code","source":"corr_matrix.loc[corr_matrix['coopele'].abs() > 0.9,corr_matrix['coopele'].abs() > 0.9]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4c16020ea0e337180ded1877dc2a5827058b5304"},"cell_type":"markdown","source":"These variables indicate where the electricity in the home is coming from. <br />\n=> 이 변수들은 집의 전기가 어디서 오는지를 가르킵니다.<br />\nThere are four options, and the families that don't have one of these two options either have no electricity (noelec) or get it from a private plant (planpri).<br />\n=> 여기에는 네가지 옵션이 있습니다. 그리고 전기가 없거나 개인 발전을 하는 선택지를 동시에 선택할 수 없습니다."},{"metadata":{"_uuid":"72eea4ad0af9482fd0c37835f3554ff9529dc781"},"cell_type":"markdown","source":"## Creating Ordinal Variable<br />\nI'm going to compress these four variables into one by creating an ordinal variable. I'm going to choose the mapping myself, based on the data decriptions:<br />\n=> 네가지 컬럼을 하나로 합칠 것입니다. 합치는 데 있어서 매핑은 아래와 같습니다.\n\n0: No electricity<br />\n1: Electricity from cooperative<br />\n2: Electricity from CNFL, ICA, ESPH/JASEC<br />\n3: Electricity from private plant<br />\n\nAn ordered variable has an inherent ordering, and for this we choose our own based on the domain knowledge. <br />\n=> <br />\nAfter we create this new ordered variable, we can drop the four others. <br />\n=> 이를 통해 하나의 컬럼을 만들면 다른 컬럼들을 없애줍니다<br />\nThere are several households that do not have a variable here, so we will use a nan (which will be filled in during imputation) and add a Boolean column indicating there was no measure for this variable.<br />\n=> 만약 값이 없다면 나중에 채워줄 것이므로 해당하는 값을 구분하기 위해서 논리형 컬럼을 추가하여줍니다"},{"metadata":{"trusted":true,"_uuid":"2b85a9ac8436de333d955db269337ab978257b81"},"cell_type":"code","source":"elec = []\n\nfor i,row in heads.iterrows():\n    if row['noelec'] == 1:\n        elec.append(0)\n    elif row['coopele'] == 1:\n        elec.append(1)\n    elif row['public'] == 1:\n        elec.append(2)\n    elif row['planpri'] == 1:\n        elec.append(3)\n    else:\n        elec.append(np.nan)\n        \nheads['elec'] = elec\nheads['elec-missing'] = heads['elec'].isnull()\n\nheads = heads.drop(columns = ['noelec','coopele','public','planpri'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"806282ad4f5b63edec4490978178d2ab66a87972"},"cell_type":"code","source":"plot_categoricals('elec','Target',heads)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4423c2e0569c76d2aaa51ac5364a885dccbf994e"},"cell_type":"markdown","source":"We can see that for every value of the Target, the most common source of electricity is from one of the listed providers.<br />\n=> 그림에서도 볼 수 있듯이 가장 평범한 방법은 공식적인 공급자에게 받는 것입니다.\n\nThe final redundant column is area2. <br />\n=> 마지막 컬럼은 area2입니다.<br />\nThis means the house is in a rural zone, but it's redundant because we have a column indicating if the house is in a urban zone. <br />\n=> 이 컬럼은 집이 교외에 있는지를 물어보는 것입니다. 그러나 이미 집이 도심에 있는지에 대한 컬럼이 있기 때문에 해당 컬럼은 중복되었다고 할 수 있습니다.<br />\nTherefore, we can drop this column.<br />\n=> 그러므로 해당 컬럼을 제거합니다."},{"metadata":{"trusted":true,"_uuid":"c00fb56309e90c8d79124b0bf17cda8206c91ceb"},"cell_type":"code","source":"corr_matrix.loc[corr_matrix['area2'].abs() >0.9, corr_matrix['area2'].abs()>0.9]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d741df6b9b4ce45166621291881f1174bacf6479"},"cell_type":"code","source":"heads = heads.drop(columns='area2')\n\nheads.groupby('area1')['Target'].value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d7468f4d44cf827bf1fe0ad99fd0e3002269ac9f"},"cell_type":"markdown","source":"It seems like households in an urban area (value of 1) are more likely to have lower poverty levels than households in a rural area (value of 0).<br />\n=> 위에 의하면 도심에 사는 가구가 취약계층들을 별로 가지고 있는 것을 알 수 있습니다."},{"metadata":{"_uuid":"2748746e0504d7a3e1600a049748827a07fa896b"},"cell_type":"markdown","source":"## Creating Ordinal Variables <br />\nFor the walls, roof, and floor of the house, there are three columns each: the first indicating 'bad', the second 'regular', and the third 'good'. <br />\n=> 집의 벽, 지붕, 바닥에 대해 세개의 컬럼이 있습니다. <br />\nWe could leave the variables as booleans, but to me it makes more sense to turn them into ordinal variables because there is an inherent order: bad < regular < good. <br />\n=> 우리는 이들을 논리형으로 남길 수 있습니다. 하지만 필자는 이를 순서형으로 바꾸었으면 합니다 나쁨 보통 좋음 순으로<br />\nTo do this, we can simply find whichever column is non-zero for each household using np.argmax.\n\nOnce we have created the ordinal variable, we are able to drop the original variables."},{"metadata":{"trusted":true,"_uuid":"c8612083f8db09018405204ac4b1cf1bed776886"},"cell_type":"code","source":"heads['walls'] = np.argmax(np.array(heads[['epared1','epared2','epared3']]),axis=1)\nheads.drop(columns=['epared1','epared2','epared3'])\n\nplot_categoricals('walls','Target',heads)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"43f301d9b101d1aa643c88c7c655cf1c25330664"},"cell_type":"code","source":"# Roof ordinal variable\nheads['roof'] = np.argmax(np.array(heads[['etecho1', 'etecho2', 'etecho3']]),\n                           axis = 1)\nheads = heads.drop(columns = ['etecho1', 'etecho2', 'etecho3'])\n\n# Floor ordinal variable\nheads['floor'] = np.argmax(np.array(heads[['eviv1', 'eviv2', 'eviv3']]),\n                           axis = 1)\nheads = heads.drop(columns = ['eviv1', 'eviv2', 'eviv3'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8578b9ff323257e30b89b6712e41a888a1af051b"},"cell_type":"markdown","source":"## Feature Construction<br />\nIn addition to mapping variables to ordinal features, we can also create entirely new features from the existing data, known as feature construction. <br />\n=> 기존에 존재하는 데이터를 가지고 새로운 컬럼을 만들어 낼 수도 있습니다.<br />\nFor example, we can add up the previous three features we just created to get an overall measure of the quality of the house's structure.<br />\n=> 예를들면, 우리가 만들었던 세가지의 컬럼을 결합한 컬럼을 만들 수 있습니다"},{"metadata":{"trusted":true,"_uuid":"f5a6109d1074044a4085721a3b63ba3411a8c618"},"cell_type":"code","source":"heads['wall+roof+floor'] = heads['walls']+heads['roof']+heads['floor']\n\nplot_categoricals('wall+roof+floor','Target',heads,annotate=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ef72a84c44b27eddf084e0b0433fd1b495454690"},"cell_type":"markdown","source":"This new feature may be useful because it seems like a Target of 4 (the lowest poverty level) tends to have higher values of the 'house quality' variable. <br />\n=> 이는 생각보다 유용합니다. 왜냐하면 4 타겟 라벨을 가진 값들이 대체로 집의 품질이 좋다고 판단할 수 있기 때문입니다.<br />\nWe can also look at this in a table to get the fine-grained details."},{"metadata":{"trusted":true,"_uuid":"2fceb85b265190c5d27626a772e375236fdcc08b"},"cell_type":"code","source":"count = pd.DataFrame(heads.groupby('wall+roof+floor')['Target'].value_counts(normalize=True)).rename(columns={'Target':'Normalized Count'}).reset_index()\ncount.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2b076821aa292a0fca288835d044e90a9c14eb2d"},"cell_type":"markdown","source":"## The next variable will be a warning about the quality of the house. <br />\n=> 다음으로는 집의 위험도 정도를 나타내보자<br />\nIt will be a negative value, with -1 point each for no toilet, electricity, floor, water service, and ceiling."},{"metadata":{"trusted":true,"_uuid":"537e877a67a83f1ebbfeac927d305a6de80af7e1"},"cell_type":"code","source":"heads['warning'] = 1 * (heads['sanitario1'] + (heads['elec'] == 0) + heads['pisonotiene'] + heads['abastaguano'] + (heads['cielorazo'] == 0))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8ade2bdf360e25c58828fd2afd36c765319de8ec"},"cell_type":"markdown","source":"We can keep using our plot_categoricals function to visualize these relationships, but seaborn also has a number of plotting options that can work with categoricals. <br />\nOne is the violinplot which shows the distribution of a variable on the y axis with the width of each plot showing the number of observations in that category."},{"metadata":{"trusted":true,"_uuid":"4e0f90de273c501b3116edac022b392a7e287795"},"cell_type":"code","source":"plt.figure(figsize = (10, 6))\nsns.violinplot(x = 'warning', y = 'Target', data = heads);\nplt.title('Target vs Warning Variable');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d63e00c0a01d6fabea64beabdaece1b569360b4c"},"cell_type":"code","source":"plot_categoricals('warning', 'Target', data = heads)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5794df39ddb295ec5e5cc41e2216c6139bcfb3aa"},"cell_type":"markdown","source":"The violinplot is not great here because it smooths out the categorical variable with the effect that it looks as if the Target can take on lesser and greater values than in reality. <br />\n=> 바이올린 플롯은 여기서는 그리 좋아보이지 않습니다. 왜냐하면 만약 타겟이 실제보다 작거나 크면 보이는데 카테고리 변수가 영향을 받기 때문입니다.<br />\nNonetheless, we can see a high concentration of households that have no warning signs and have the lowest level of poverty. <br />\n=> 그럼에도 불구하고 우리는 많은 가구들이 위험요소가 적으며 빈곤여부가 적다는 것을 알 수 있습니다.<br />\nIt looks as if this may be a useful feature, but we can't know for sure until we get to modeling!<br />\n=> 유용한 것 처럼 보이지만, 모델링때 쓰여질 지는 모르겠습니다.\n\nThe final household feature we can make for now is a bonus where a family gets a point for having a refrigerator, computer, tablet, or television."},{"metadata":{"trusted":true,"_uuid":"1471f8e57c7cb18bd2104fe07677ae372fc022a4"},"cell_type":"code","source":"# Owns a refrigerator, computer, tablet, and television\nheads['bonus'] = 1 * (heads['refrig'] + \n                      heads['computer'] + \n                      (heads['v18q1'] > 0) + \n                      heads['television'])\n\nsns.violinplot('bonus', 'Target', data = heads,\n                figsize = (10, 6));\nplt.title('Target vs Bonus Variable');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ee9bca14733d3e684ca85f2eeaa6daa0a98d8db1"},"cell_type":"markdown","source":"Per Capita Features<br />\nAdditional features we can make calculate the number of certain measurements for each person in the household."},{"metadata":{"trusted":true,"_uuid":"c62621036350c79ccb357caca0472c4b896f5f34"},"cell_type":"code","source":"heads['phones-per-capita'] = heads['qmobilephone'] / heads['tamviv']\nheads['tablets-per-capita'] = heads['v18q1'] / heads['tamviv']\nheads['rooms-per-capita'] = heads['rooms'] / heads['tamviv']\nheads['rent-per-capita'] = heads['v2a1'] / heads['tamviv']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ec487d7c69d2dcb1db27911eb0425e13c0df9b32"},"cell_type":"markdown","source":"## Exploring Household Variables [가구별 변수]<br />\nAfter going to all the trouble of getting our features in order, now we can take a look at them in relation to the Target. <br />\n=> 순서를 가진 컬럼들의 문제들을 모두 해결 했으니 이제 타겟 라벨과의 관계를 살펴봅시다<br />\nWe've already done a little of this, but now we can try to quantify relationships.<br />\n=> 이미 조금했지만, 양적인 관계를 ㅏㄹ펴봅시다\n\n## Measuring Relationships<br />\nThere are many ways for measuring relationships between two variables. Here we will examine two of these:<br />\n=> 관계를 측정하는 방법에는 여러방법이 있지만 우리는 그중 두가지만 채택합시다\n\n   **The Pearson Correlation: from -1 to 1 measuring the linear relationship between two variables [일반적으로 사용되는 상관계수]<br />\n   The Spearman Correlation: from -1 to 1 measuring the monotonic relationship between two variables [해당 지표가 점차적으로 증가 또는 감소하는 관게를 보여주는 지표]<br />**\n\nThe Spearman correlation is 1 if as one variable increases, the other does as well, even if the relationship is not linear. \n\nOn the other hand, the Pearson correlation can only be one if the increase is exactly linear. These are best illustrated by example."},{"metadata":{"trusted":true,"_uuid":"ffe70b22f305762e124c1a4f3d216da711ddc5bb"},"cell_type":"code","source":"from scipy.stats import spearmanr\n\ndef plot_corrs(x,y):\n    spr = spearmanr(x,y).correlation\n    pcr = np.corrcoef(x,y)[0,1]\n    \n    data = pd.DataFrame({'x':x,'y':y})\n    plt.figure(figsize=[6,4])\n    sns.regplot('x','y',data=data,fit_reg=False)\n    plt.title(f'Spearman: {round(spr, 2)}; Pearson: {round(pcr, 2)}');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"07eb952d2f4ea538d7fde24ab6138f96e752e178"},"cell_type":"code","source":"x = np.array(range(100))\ny = x ** 2\n\nplot_corrs(x, y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"23e71c3b04131d035a7c0a5c4b475554cf75e452"},"cell_type":"markdown","source":"The Spearman correlation is often considered to be better for ordinal variables such as the Target or the years of education. <br />\n=> Spearman 상관계수는 타겟이나 교육년수와 같은 순서적인 변수들에게 좋을 것이며<br />\nMost relationshisp in the real world aren't linear, and although the Pearson correlation can be an approximation of how related two variables are, it's inexact and not the best method of comparison.<br />\n실제로는 모든 관계가 선형일리는 없으며, 비록 Pearson 상관계수가 두 변수가 어떻게 관계있는지에 대한 대략적인 것을 보여 줄지라도 비교에 있어서 항상 최고의 방법은 아니다"},{"metadata":{"trusted":true,"_uuid":"6404b1be19b3f721fb8d6e6a5fe4d771b72586be"},"cell_type":"code","source":"x = np.array([1, 1, 1, 2, 3, 3, 4, 4, 4, 5, 5, 6, 7, 8, 8, 9, 9, 9])\ny = np.array([1, 2, 1, 1, 1, 1, 2, 2, 2, 2, 1, 3, 3, 2, 4, 2, 2, 4])\n\nplot_corrs(x, y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e1a4337ae70868aaffae356fc7f24fedc5c62e8c"},"cell_type":"markdown","source":"In most cases, the values are very similar."},{"metadata":{"trusted":true,"_uuid":"57fe98ea173b0defea61b2ed8eec323ccd63ab46"},"cell_type":"code","source":"x = np.array(range(-19, 20))\ny = 2 * np.sin(x)\n\nplot_corrs(x, y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"590d5438dfffbee2ade9a25dc2fb5bb4ff66a636"},"cell_type":"markdown","source":"First, we'll calculate the Pearson correlation of every variable with the Target."},{"metadata":{"trusted":true,"_uuid":"1cd89386c5fcd7f850d0f390db2363eaaade918a"},"cell_type":"code","source":"train_heads = heads.loc[heads['Target'].notnull(),:].copy()\n\npcorrs = pd.DataFrame(train_heads.corr()['Target'].sort_values()).rename(columns = {'Target':'pcorr'}).reset_index()\npcorrs = pcorrs.rename(columns= {'index':'feature'})\n\nprint('Most negatively correlated variables:')\nprint(pcorrs.head())\n\nprint('\\nMost positively correlated variables:')\nprint(pcorrs.dropna().tail())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a90687624db612dc05912da0f97d65022e9162d9"},"cell_type":"markdown","source":"For the negative correlations, as we increase the variable, the Target decreases indicating the poverty severity increases. <br />\n=> 부의 상관관계는 임의의 해당 상관관게의 값이 증가하면 타겟라벨은 감소할 것입니다(이는 더 빈곤하다는 것을 의미함)<br />\nTherefore, as the warning increases, the poverty level also increases which makes sense because this was meant to show potential bad signs about a house. <br />\n=> 그러므로 warning이 증가하면 부의 타겟 라벨은 더 빈곤하다는 것을 의미할 것입니다..<br />\nThe hogar_nin is the number of children 0 - 19 in the family which also makes sense: younger children can be financial source of stress on a family leading to higher levels of poverty. <br />\n=> hogar_nin은 0-19세의 아이들의 수인데 이 또한 말이 맞는듯합니다: 어린 아이들은 경제적인 지원이 많이 필요하기 때문에 가족의 부의 수준을 떨어뜨릴 것입니다.<br />\nOr, families with lower socioeconomic status have more children in the hopes that one of them will be able to succeed.<br />\n=> 또는 <br />\nWhatever the explanation, there is a real link between family size and poverty<br />\n=> \n\nOn the other hand, for the positive correlations, a higher value means a higher value of Target indicating the poverty severity decreases. <br />\n=> 반면 양의 관계에서는, 위의 부의 관계와 반대로 생각하시면 됩니다.<br />\nThe most highly correlated household level variable is meaneduc, the average education level of the adults in the household. <br />\n=> 가장 높은 수치는 meaneduc이며, 이는 가정의 어른들의 평균 교육 단게를 의미합니다.<br />\nThis relationship between education and poverty intuitively makes sense: greater levels of education generally correlate with lower levels of poverty.<br />\n=> 교육 수준과 부의 단계는 또한 말이 맞는 것 같습니다.<br />\nWe don't necessarily know which causes which, but we do know these tend to move in the same direction.<br />\n=> 어느것이 타겟라벨을 움직이는지 면밀히 파악할 필요는 없지만, 양의 방향으로 같이 관계를 가진다는 정도로 알아둘 필요가 있습니다.\n\nThe general guidelines for correlation values are below, but these will change depending on who you ask (source for these):<br />\n=> 아래는 상관관계를 해석하는 수치별 정도입니다.\n\n.00-.19 “very weak”<br />\n.20-.39 “weak”<br />\n.40-.59 “moderate”<br />\n.60-.79 “strong”<br />\n.80-1.0 “very strong”<br />\nWhat these correlations show is that there are some weak relationships that hopefully our model will be able to use to learn a mapping from the features to the Target.\n\nNow we can move on to the Spearman correlation."},{"metadata":{"trusted":true,"_uuid":"51a05bb7e1b264e591d285f12b05cc55cc33522c"},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore', category = RuntimeWarning)\n\nfeats = []\nscorr = []\npvalues = []\n\n# Iterate through each column\nfor c in heads:\n    # Only valid for numbers\n    if heads[c].dtype != 'object':\n        feats.append(c)\n        \n        # Calculate spearman correlation\n        scorr.append(spearmanr(train_heads[c], train_heads['Target']).correlation)\n        pvalues.append(spearmanr(train_heads[c], train_heads['Target']).pvalue)\n\nscorrs = pd.DataFrame({'feature': feats, 'scorr': scorr, 'pvalue': pvalues}).sort_values('scorr')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"102f90fc5a7023a0cb4afbdf7d08bcb46e410e70"},"cell_type":"markdown","source":"The Spearman correlation coefficient calculation also comes with a pvalue indicating the significance level of the relationship. <br />\n=> Spearman 상관계수는 관계의 유의수준을 결정하는 pvalue와 함께 계산합니다.<br />\nAny pvalue less than 0.05 is genearally regarded as significant, although since we are doing multiple comparisons, we want to divide the p-value by the number of comparisons, a process known as the Bonferroni correction.<br />\n=> 다중 비교를 하더라도 임의의 pvalue가 0.05보다 작은 경우 우리는 유의하다고 판단합니다. 그래서 비교 횟수별로 p-value를 나눕니다. 이 과정을 베루누이 상관관계라고 합니다."},{"metadata":{"trusted":true,"_uuid":"2b5eea9719c4eda0e72a117254957501b8b64f10"},"cell_type":"code","source":"print('Most negative Spearman correlations:')\nprint(scorrs.head())\nprint('\\nMost positive Spearman correlations:')\nprint(scorrs.dropna().tail())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a40078d3816e306eed0910a698afe808a0acd050"},"cell_type":"markdown","source":"For the most part, the two methods of calculating correlations are in agreement. Just out of curiousity, we can look for the values that are furthest apart."},{"metadata":{"trusted":true,"_uuid":"3617a995efc9b44262540b311634fbaf8b7ee440"},"cell_type":"code","source":"corrs = pcorrs.merge(scorrs, on = 'feature')\ncorrs['diff'] = corrs['pcorr'] - corrs['scorr']\n\ncorrs.sort_values('diff').head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5fcbc1d6bf052df4cc91b6b6fe8a72ad048520f5"},"cell_type":"code","source":"corrs.sort_values('diff').dropna().tail()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"48ff4ddd92bd2ee559b78a35a2d9a500835d6743"},"cell_type":"markdown","source":"The largest discrepancy in the correlations is dependency. We can make a scatterplot of the Target versus the dependency to visualize the relationship. We'll add a little jitter to the plot because these are both discrete variables."},{"metadata":{"trusted":true,"_uuid":"e5156657d0d2fe158562fcf6f7146ca23ddb84a3"},"cell_type":"code","source":"sns.lmplot('dependency', 'Target', fit_reg = True, data = train_heads, x_jitter=0.05, y_jitter=0.05);\nplt.title('Target vs Dependency');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5e1d0a082e70f7a0ab9b2215c018e211362f6eb5"},"cell_type":"markdown","source":"It's hard to see the relationship, but it's slightly negative: as the dependency increases, the value of the Target decreases. <br />\n=> 상관관계를 보긴 어렵지만 약간의 부의 상관관게를 가짐을 의미합니다.<br />\nThis makes sense: the dependency is the number of dependent individuals divided by the number of non-dependents. <br />\n=> 이는 dependency가 부양해야할 가족 / 부양하는 가족 을 의미하기 때문에 이를 유추할수 있습니다.<br />\nAs we increase this value, the poverty severty tends to increase: having more dependent family members (who usually are non-working) leads to higher levels of poverty because they must be supported by the non-dependent family members."},{"metadata":{"trusted":true,"_uuid":"d8de99da2bcb695de52e76cadea7ce24ab91fa1f"},"cell_type":"code","source":"sns.lmplot('rooms-per-capita', 'Target', fit_reg = True, data = train_heads, x_jitter=0.05, y_jitter=0.05);\nplt.title('Target vs Rooms Per Capita');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"595f14e59beb3c80a1453fe7e56e7a1abf4c7d2f"},"cell_type":"markdown","source":"Correlation Heatmap<br />\nOne of my favorite plots is the correlation heatmap because it shows a ton of info in one image. For the heatmap, we'll pick 7 variables and show the correlations between themselves and with the target."},{"metadata":{"trusted":true,"_uuid":"d1695cf8311e4daa893ba4d49ae9831457446ab5"},"cell_type":"code","source":"variables = ['Target', 'dependency', 'warning', 'wall+roof+floor', 'meaneduc',\n             'floor', 'r4m1', 'overcrowding']\n\n# Calculate the correlations\ncorr_mat = train_heads[variables].corr().round(2)\n\n# Draw a correlation heatmap\nplt.rcParams['font.size'] = 18\nplt.figure(figsize = (12, 12))\nsns.heatmap(corr_mat, vmin = -0.5, vmax = 0.8, center = 0, \n            cmap = plt.cm.RdYlGn_r, annot = True);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6b96ef425bd674dec26ce3d077fa6706a1ee7227"},"cell_type":"markdown","source":"This plot shows us that there are a number of variables that have a weak correlation with the Target. There are also high correlations between some variables (such as floor and walls+roof+floor) which could pose an issue because of collinearity."},{"metadata":{"_uuid":"0c91e487d60d5e4584e12997e7fd47e5c2e8cc0d"},"cell_type":"markdown","source":"Features Plot\nFor the final exploration of the household level data, we can make a plot of some of the most correlated variables with the Target. This shows scatterplots on the upper triangle, kernel density estimate (kde) plots on the diagonal, and 2D KDE plots on the lower triangle."},{"metadata":{"trusted":true,"_uuid":"0b0f74bd48431496f493d818e8452fb48809155e"},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nplot_data = train_heads[['Target', 'dependency', 'wall+roof+floor','meaneduc', 'overcrowding']]\n\ngrid = sns.PairGrid(data=plot_data,size=4,diag_sharey=False,hue='Target',hue_order=[4,3,2,1],vars = [col for col in list(plot_data.columns) if col != 'Target'])\n\ngrid.map_upper(plt.scatter,alpha=0.8,s=20)\n\ngrid.map_diag(sns.kdeplot)\n\ngrid.map_lower(sns.kdeplot,cmap=plt.cm.OrRd_r)\ngrid = grid.add_legend()\nplt.suptitle('Feature Plots Colored By Target', size = 32, y = 1.05);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"18de03d887b8003fbca85ba69ea032b7ca68aeac"},"cell_type":"markdown","source":"We'll leave the feature engineering of the household variables for now. Later, we can come back to this step if we are not pleased with the model performance."},{"metadata":{"trusted":true,"_uuid":"f1c7b3251420b2fb527c8a827987901bb63cd1ee"},"cell_type":"code","source":"household_feats = list(heads.columns)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"259c92af0f053143a74e3690850445fd0a2121fe"},"cell_type":"markdown","source":"Individual Level Variables [개별 단위 변수]<br />\nThere are two types of individual level variables: Boolean (1 or 0 for True or False) and ordinal (discrete values with a meaningful ordering).<br />\n=> 개별 단위 변수에는 논리형과 순서형이 있습니다."},{"metadata":{"trusted":true,"_uuid":"c65b54124a285615e6663e6bfdd2ef8b173b136d"},"cell_type":"code","source":"ind = data[id_ + ind_bool + ind_ordered]\nind.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fbcee694709d08828ada17349518da2db14efa19"},"cell_type":"markdown","source":"Redundant Individual Variables [개별 단위 변수에 대한 중복처리]<br />\nWe can do the same process we did with the household level variables to identify any redundant individual variables. <br />\n=> 우리는 개별 단위 변수에도 유사한 방법을 적용하려고 합니다<br />\nWe'll focus on any variables that have an absolute magnitude of the correlation coefficient greater than 0.95.<br />\n=> 상관계수가 0.95이상인 값들에 대해서만 확인해 봅시다"},{"metadata":{"trusted":true,"_uuid":"4ca1f63bd887a2acb4512a0c1292c05d2243ffd2"},"cell_type":"code","source":"corr_matrix = ind.corr()\n\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape),k=1).astype(np.bool))\n\nto_drop = [col for col in upper.columns if any(abs(upper[col]) > 0.95)]\n\nto_drop","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"16f2a39756fb31c3308a76b6b51b376f591ff2de"},"cell_type":"markdown","source":"This is simply the opposite of male! We can remove the male flag."},{"metadata":{"trusted":true,"_uuid":"fe743a60c01e4597877bd78cb35b7753b86ec1ae"},"cell_type":"code","source":"ind = ind.drop(columns = 'male')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8fbec3660ecb23a6136f8b0d75b43bd6fd7cf654"},"cell_type":"markdown","source":"Creating Ordinal Variables [순서척도 만들기]<br />\nMuch as we did with the household level data, we can map existing columns to an ordinal variable.<br />\n=> 가구단위에서 했듯이 논리형 컬럼들을 결합하여 하나의 순서척도 컬럼을 만들 수 있습니다.<br />\nHere we will focus on the instlevel_ variables which indicate the amount of education an individual has from instlevel1: no level of education to instlevel9: postgraduate education.<br />\n=> instlevel_ 변수들은 개개인의 교육의 정도를 나타내는데 1부터 9까지 존재합니다.\n\nTo create the ordinal variable, for each individual, we will simply find which column is non-zero. <br />\n=> 순서척도화 하기위해 각 개별 변수들에 0이 아닌 컬럼들을 찾기만 하면 됩니다.<br />\nThe education has an inherent ordering (higher is better) so this conversion to an ordinal variable makes sense in the problem context.<br />\n=> 교육수준은 높은 순서가 더 좋은 교육수준을 나타내도록 합시다"},{"metadata":{"trusted":true,"_uuid":"c3d1ef7ab5902962cb3b4fd40d6926ea5abb0b81"},"cell_type":"code","source":"ind[[col for col in ind.columns if col.startswith('instlevel')]].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d2eba85a3b6a139a15841261e7c8e349cb2bcf5c"},"cell_type":"code","source":"ind['inst'] = np.argmax(np.array(ind[[col for col in ind.columns if col.startswith('instlevel')]]),axis=1)\n\nplot_categoricals('inst','Target',ind,annotate=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2635a25efab93bbb131fc1f8cf398c6399a0dbea"},"cell_type":"markdown","source":"Higher levels of education seem to correspond to less extreme levels of poverty. <br />\n=> 높은 교육수준은 타겟라벨이 높은 수준으로 상관관계를 가지며<br />\nWe do need to keep in mind this is on an individual level though and we eventually will have to aggregate this data at the household level.<br />\n=> 우리는 개별적인 컬럼을 전체 가구수준의 컬럼으로 합치려는 작업을 해야 한다는 것을 명심해야한다."},{"metadata":{"trusted":true,"_uuid":"75f571bfecb8a65ec928cf13388c1d6bc90ec180"},"cell_type":"code","source":"plt.figure(figsize = (10, 8))\nsns.violinplot(x = 'Target', y = 'inst', data = ind);\nplt.title('Education Distribution by Target');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5d9d1e9978f7a9cb67da812b1c38064cfd0ec570"},"cell_type":"code","source":"# Drop the education columns\nind = ind.drop(columns = [c for c in ind if c.startswith('instlevel')])\nind.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5831070ed39aad7b5a5459ac795be54d3051e861"},"cell_type":"markdown","source":"Feature Construction<br />\nWe can make a few features using the existing data. <br />\n=> 존재하는 데이터를 이용해서 몇개의 컬럼들을 만들어 봅시다.<br />\nFor example, we can divide the years of schooling by the age."},{"metadata":{"trusted":true,"_uuid":"e76871ab0daa231c9792876d95142b6962f201eb"},"cell_type":"code","source":"ind['escolari/age'] = ind['escolari'] / ind['age']\n\nplt.figure(figsize = (10, 8))\nsns.violinplot('Target', 'escolari/age', data = ind);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"67eac55ce27d583bf51b2207f5d16fac28a62439"},"cell_type":"markdown","source":"We can also take our new variable, inst, and divide this by the age. The final variable we'll name tech: this represents the combination of tablet and mobile phones."},{"metadata":{"trusted":true,"_uuid":"44c98a13b09e903d24a636d3e22605f31f9f45c2"},"cell_type":"code","source":"ind['inst/age'] = ind['inst'] / ind['age']\nind['tech'] = ind['v18q'] + ind['mobilephone']\nind['tech'].describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1ff4cac12e048dea78f66240972b1cdaac1589d2"},"cell_type":"markdown","source":"## Feature Engineering through Aggregations<br />\nIn order to incorporate the individual data into the household data, we need to aggregate it for each household. <br />\n=> 개별 데이터를 가구별 데이터로 변환하기 위해서 각각의 가구별로 병합을 할 필요가 있습니다.<br />\nThe simplest way to do this is to groupby the family id idhogar and then agg the data. <br />\n=> 가장 간단한 방법으로는 가구별 식별자로 그룹후에 그룹함수를 사용하는 것입니다<br />\nFor the aggregations for ordered or continuous variables, we can use six, five of which are built in to pandas, and one of which we define ourselves range_. <br />\n=> 순서 및 연속적인 변수에 대해서 그룹함수를 사용하는데 있어서 여섯가지가 있으며, 한가지는 직접 정의해서 사용할 것입니다.<br />\nThe boolean aggregations can be the same, but this will create many redundant columns which we will then need to drop. <br />\n=> 논리형 변수도 결합하는 것이 같지만, 이는 우리가 나중에 버릴 수도 있는 중복된 컬럼들이 생기는 경우가 될 수 있습니다.<br />\nFor this case, we'll use the same aggregations and then go back and drop the redundant columns.<br />\n=> 우리는 같은 결합정책을 사용하고 나중에 돌아가서 중복되는 컬럼들을 제거 할 것입니다."},{"metadata":{"trusted":true,"_uuid":"ae50fc530a66f8e87e6e3f1d8937a7e9f0dc3fb5"},"cell_type":"code","source":"range_ = lambda x:x.max() - x.min()\nrange_.__name__ = 'range_'\n\nind_agg = ind.drop(columns='Target').groupby('idhogar').agg(['min','max','sum','count','std',range_])\nind_agg.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9a2faeb5848badc014527955882580d91d7d7888"},"cell_type":"code","source":"new_cols = []\n\nfor col in ind_agg.columns.levels[0]:\n    for stat in ind_agg.columns.levels[1]:\n        new_cols.append(f'{col}-{stat}')\n        \nind_agg.columns = new_cols\nind_agg.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"09ca0cae70b4102160c62e30e3ec83d0e2e47a89"},"cell_type":"code","source":"ind_agg.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"62dc8a0d787d89c3e8a288654efb968f9f9b7106"},"cell_type":"markdown","source":"Feature Selection\nAs a first round of feature selection, we can remove one out of every pair of variables with a correlation greater than 0.95."},{"metadata":{"trusted":true,"_uuid":"a51d0e5b334edda921461dc7e8ef4a5494c86130"},"cell_type":"code","source":"corr_matrix = ind_agg.corr()\n\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape),k=1).astype(np.bool))\n\nto_drop = [col for col in upper.columns if any(abs(upper[col]) > 0.95)]\n\nprint(f'There are {len(to_drop)} correlated columns to remove.')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ac268064e88429de97783fdb8835235a3c289892"},"cell_type":"markdown","source":"We'll drop the columns and then merge with the heads data to create a final dataframe."},{"metadata":{"trusted":true,"_uuid":"e3a87f048eca382e0f9e830f3fd4af328bf42b99"},"cell_type":"code","source":"ind_agg = ind_agg.drop(columns=to_drop)\nind_feats = list(ind_agg.columns)\n\nfinal = heads.merge(ind_agg,on='idhogar',how='left')\n\nprint('Final features shape: ', final.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7323865746c7ab637593aeaa4cf5199c284e5f00"},"cell_type":"code","source":"final.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bc99d15d4b0faa67aa60a43213bb3198cafa3a12"},"cell_type":"markdown","source":"Final Data Exploration\nWe'll do a little bit of exploration."},{"metadata":{"trusted":true,"_uuid":"a5e1ac6c16ac352b1ba77bad03340c94985c9f89"},"cell_type":"code","source":"corrs = final.corr()['Target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e84cd0582b1ff2604355bb036f9c647eff19aa0f"},"cell_type":"code","source":"corrs.sort_values().head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4450f8524823188a3796d19fc8eb3334fafc2921"},"cell_type":"code","source":"corrs.sort_values().dropna().tail()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c988c661cbb13fe277a4abf78779bed0764e227b"},"cell_type":"markdown","source":"We can see some of the variables that we made are highly correlated with the Target. <br />\n=> 타겟 라벨과 높은 관련성을 보이는 변수들을 찾아낼 수 있습니다.<br />\nWhether these variables are actually useful will be determined in the modeling stage.<br />\n=> 이 정보는 모델링 단계에서 유용할지 아닐지가 결정될 것입니다.<br />"},{"metadata":{"trusted":true,"_uuid":"eb47d5120a33f6314b55d3fd8e389f73bb7d4a92"},"cell_type":"code","source":"plot_categoricals('escolari-max', 'Target', final, annotate=False);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"87f5e91c9075b50685fc6b82fa8d3092a228d331"},"cell_type":"code","source":"plt.figure(figsize = (10, 6))\nsns.violinplot(x = 'Target', y = 'escolari-max', data = final);\nplt.title('Max Schooling by Target');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8b48decdf12e05ba4b634762769631d60f40a523"},"cell_type":"code","source":"plt.figure(figsize = (10, 6))\nsns.boxplot(x = 'Target', y = 'escolari-max', data = final);\nplt.title('Max Schooling by Target');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"33d5c9151b71552023b9cd5c30debc1aa9481ca6"},"cell_type":"code","source":"plt.figure(figsize = (10, 6))\nsns.boxplot(x = 'Target', y = 'meaneduc', data = final);\nplt.xticks([0, 1, 2, 3], poverty_mapping.values())\nplt.title('Average Schooling by Target');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"86398f05fb29122e805a4d14b4408bcc7059a73b"},"cell_type":"code","source":"plt.figure(figsize = (10, 6))\nsns.boxplot(x = 'Target', y = 'overcrowding', data = final);\nplt.xticks([0, 1, 2, 3], poverty_mapping.values())\nplt.title('Overcrowding by Target');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d4be55849c12a836dd69154a0e033293f76b2457"},"cell_type":"markdown","source":"One other feature that might be useful is the gender of the head of household. <br />\n=> 다른 중요할 것으로 예상되는 특징은 가장의 성별입니다.<br />\nSince we aggregated the data, we'll have to go back to the individual level data and find the gender for the head of household.<br />\n=> 우리가 데이터를 합친 이후로, 이제 개별 데이터에 대해서 돌아보고 가장의 성별에 대해서 알아봅시다."},{"metadata":{"trusted":true,"_uuid":"a0f86a6448f034b7a45f12e60dd4f516416f557b"},"cell_type":"code","source":"head_gender = ind.loc[ind['parentesco1'] == 1, ['idhogar', 'female']]\nfinal = final.merge(head_gender, on = 'idhogar', how = 'left').rename(columns = {'female': 'female-head'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"274cf9ffbc744e8161553ae3bd935e471b623e0f"},"cell_type":"code","source":"final.groupby('female-head')['Target'].value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c493c1def8e06a55713e2d318cc3a9b5aa6acc13"},"cell_type":"markdown","source":"It looks like households where the head is female are slightly more likely to have a severe level of poverty."},{"metadata":{"trusted":true,"_uuid":"1be35e27bdd391d1f3f6dd539fce5d0c8e7d3962"},"cell_type":"code","source":"sns.violinplot(x = 'female-head', y = 'Target', data = final);\nplt.title('Target by Female Head of Household');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"df7bd1a0cb91a375861db11efc5f90f8558baef0"},"cell_type":"markdown","source":"We can also look at the difference in average education by whether or not the family has a female head of household."},{"metadata":{"trusted":true,"_uuid":"8ea5b6ad373fc66f014dbd074c60678801ee2750"},"cell_type":"code","source":"plt.figure(figsize = (8, 8))\nsns.boxplot(x = 'Target', y = 'meaneduc', hue = 'female-head', data = final);\nplt.title('Average Education by Target and Female Head of Household', size = 16);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d5902a0b1f7e36e46d580920a556cc105d44a1c6"},"cell_type":"markdown","source":"It looks like at every value of the Target, households with female heads have higher levels of education. Yet, we saw that overall, households with female heads are more likely to have severe poverty."},{"metadata":{"trusted":true,"_uuid":"2110a842f1736bebf12388af83822f70936498e5"},"cell_type":"code","source":"final.groupby('female-head')['meaneduc'].agg(['mean', 'count'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7a2c4bbab1411e88b8841e8dc87258074c7552c2"},"cell_type":"markdown","source":"Overall, the average education of households with female heads is slightly higher than those with male heads. <br />\n=> 전반적으로 가장이 남자인 경우보다 여자인 경우가 더 교육기간이 긴 것을 확인할 수 있다.<br />\nI'm not too sure what to make of this, but it seems right to me.<br />\n=> 왜 이렇게 되었는지는 모르겠지만 이러한 통찰을 얻을 수 있었습니다."},{"metadata":{"_uuid":"33e44af4f486e1aca00a651438d213cb0f9442f8"},"cell_type":"markdown","source":"## Machine Learning Modeling<br />\nOnce feature engineering/construction is done, we can get started with the machine learning! <br />\n=> feature engineering/construction이 끝나고 난후에 우리는 머신러닝 모델을 만들 시간입니다.<br />\nAll of our data (both training and testing) is aggregated for each household and so can be directly used in a model. <br />\n=> 우리의 모든 데이터는 각각의 가구별로 이미 나누어졌습니다. 이제 이를 바로 모델에 사용하면 될 것 같습니다.<br />\nTo first show the process of modeling, we'll use the capable Random Forest Classifier in Scikit-Learn.<br />\n=> 모델링의 첫번째 단계로 RandonForestClassifier를 사용할 것입니다.<br />\nThis probably won't get us to the top of the leaderboard, but it will allow us to establish a baseline.<br />\n=> 이는 점수판에 최상위에 오르게는 하지 않지만 적절한 초석을 다지기엔 좋습니다.<br />\nLater we'll try several other models including the powerful Gradient Boosting Machine.<br />\n=> 추후에 Gradient Boosting을 활용하여 더 강력한 모델을 만들겠습니다\n\nTo assess our model, we'll use 10-fold cross validation on the training data.<br />\n=> 우리의 모델을 평가하기 위해서 10겹 교차검증을 사용하겠습니다<br />\nThis will essentially train and test the model 10 times using different splits of the training data. <br />\n=> 이는 10번에 거쳐서 10겹에 나눠진 데이터 별로 훈련과 테스트를 진행합니다.<br />\n10-fold cross validation is an effective method for estimating the performance of a model on the test set. <br />\n=> 10겹 교차검증은 모델의 성능을 측정하는데 효과적인 방법입니다.<br />\nWe want to look at the average performance in cross validation as well as the standard deviation to see how much scores change between the folds. <br />\n=> 우리는 일반적인 방법을 거친것과 10겹 교차검증을 거친 것의 점수 또한 비교해 볼 것입니다.<br />\nWe use the F1 Macro measure to evaluate performance."},{"metadata":{"trusted":true,"_uuid":"c70095f9ce0d2867ca7a897d9b7d9e76f4a27c94"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score, make_scorer\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.pipeline import Pipeline\n\n# Custom scorer for cross validation\nscorer = make_scorer(f1_score, greater_is_better=True, average = 'macro')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"82e49b84bd5df42cdc25d3ddb54c14a29048b1aa"},"cell_type":"code","source":"train_labels = np.array(list(final.loc[final['Target'].notnull(),'Target'].astype(np.uint8)))\n\ntrain_set = final[final['Target'].notnull()].drop(columns=['Id','idhogar','Target'])\n\ntest_set = final[final['Target'].isnull()].drop(columns=['Id','idhogar','Target'])\n\nsubmission_base = test[['Id', 'idhogar']].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f4dfecc46b8085c44cac9d66d9465990901ce624"},"cell_type":"code","source":"train_set.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0ff5c79cd03365d5c0eaccd820fc9086b787e8f1"},"cell_type":"markdown","source":"Because we are going to be comparing different models, we want to scale the features (limit the range of each column to between 0 and 1).<br />\n=> 우리는 다른 모델을 비교할 것이기 때문에 우리의 특징들을 scaling할 필요가 있어보입니다.<br />\nFor many ensemble models this is not necessary, but when we use models that depend on a distance metric, such as KNearest Neighbors or the Support Vector Machine, feature scaling is an absolute necessity. <br />\n=> 대부분의 앙상블 모델에는 불필요하지만, 거리와 관련된 척도인 K 최근접 이웃이나 서포트 벡터머신 과 같은 머신러닝 모델을 이용한다면 이런 스케일링은 필수적입니다.<br />\nWhen comparing different models, it's always safest to scale the features. <br />\n=> 다른 모델을 비교할 때 스케일링하는 것은 가장 안전한 방법중에 하나입니다.<br />\nWe also impute the missing values with the median of the feature.<br />\n=> 그리고 우리는 각 특징의 중앙값을 손실치들에 넣어줄 것입니다.\n\n**For imputing missing values and scaling the features in one step, we can make a pipeline.** <br />\n**=> 파이프라인을 이용하면 손실치를 넣거나 스케일링을 하는데 간단하게 처리할 수있습니다.**<br />\nThis will be fit on the training data and used to transform the training and testing data."},{"metadata":{"trusted":true,"_uuid":"e2711974660f0618d896dce7892e7ae00e85ffb1"},"cell_type":"code","source":"features = list(train_set.columns)\n\npipeline = Pipeline([('imputer',Imputer(strategy='median')),('scaler',MinMaxScaler())])\n\ntrain_set = pipeline.fit_transform(train_set)\ntest_set = pipeline.transform(test_set)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cf0aa9da53e4c8becc933e41908c89f814221fc1"},"cell_type":"markdown","source":"The data has no missing values and is scaled between zero and one. This means it can be directly used in any Scikit-Learn model."},{"metadata":{"trusted":true,"_uuid":"bcfb7147512ebaebe32a6ba553b2f7098de9a7f9"},"cell_type":"code","source":"model = RandomForestClassifier(n_estimators=100, random_state=10, n_jobs = -1)\n# 10 fold cross validation\ncv_score = cross_val_score(model, train_set, train_labels, cv = 10, scoring = scorer)\n\nprint(f'10 Fold Cross Validation F1 Score = {round(cv_score.mean(), 4)} with std = {round(cv_score.std(), 4)}')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"74ade0f2e481520b9556a8599c4836c6cb70da06"},"cell_type":"markdown","source":"That score is not great, but it will serve as a baseline and leaves us plenty of room to improve!"},{"metadata":{"_uuid":"826247c223ef603e71e30742593a30ddc70d1f94"},"cell_type":"markdown","source":"## Feature Importances<br />\nWith a tree-based model, we can look at the feature importances which show a relative ranking of the usefulness of features in the model. <br />\n=> 트리 모델로 우리는 모델에서 특징의 필요없는 정도를 나타내는 상대적 랭킹을 볼 수 있습니다.<br />\nThese represent the sum of the reduction in impurity at nodes that used the variable for splitting, but we don't have to pay much attention to the absolute value. Instead we'll focus on relative scores.<br />\n=> 이는 변수를 분기하는데 사용되는 각각의 노드에 불 순수성에서 reduction의 합을 나타내며, 이들의 실제값에는 많은 관심을 두지 않습니다. 대신에 우리는 상대적인 점수에 초점을 맞출 것입니다.\n\nIf we want to view the feature importances, we'll have to train a model on the whole training set. Cross validation does not return the feature importances.<br />\n만약 feature_importances 의 수치를 보고 싶다면, 모델을 전체 트레이닝 셋에 훈련시키고 해라."},{"metadata":{"trusted":true,"_uuid":"61a7134899fddfc10bd61062a4c09747a21b60d7"},"cell_type":"code","source":"model.fit(train_set,train_labels)\n\nindices = np.argsort(model.feature_importances_)[::-1]\nimportances = model.feature_importances_\n\nfor i in range(len(features)):\n    print(\"%d) feature importances of %s %f\"%(i,features[indices[i]],importances[indices[i]]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4897ac778d029e192ed9a713853505491b3e1d70"},"cell_type":"code","source":"# def plot_feature_importances(df,feature_label,importance_label,n=10,ascending=False):\n    \n#     plt.style.use('fivethirtyeight')\n    \n#     df = df.sort_values(by=importance_label,ascending=ascending).reset_index()\n#     df.loc[list(range(n)),][importance_label].plot(kind='bar')\n#     plt.xticks(list(range(n)),df.loc[list(range(n)),][feature_label])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9720a6b5b0d4fcff8b019d56e06f61b5444de888"},"cell_type":"code","source":"def plot_feature_importances(df, n = 10, threshold = None):\n    \"\"\"Plots n most important features. Also plots the cumulative importance if\n    threshold is specified and prints the number of features needed to reach threshold cumulative importance.\n    Intended for use with any tree-based feature importances. \n    \n    Args:\n        df (dataframe): Dataframe of feature importances. Columns must be \"feature\" and \"importance\".\n    \n        n (int): Number of most important features to plot. Default is 15.\n    \n        threshold (float): Threshold for cumulative importance plot. If not provided, no plot is made. Default is None.\n        \n    Returns:\n        df (dataframe): Dataframe ordered by feature importances with a normalized column (sums to 1) \n                        and a cumulative importance column\n    \n    Note:\n    \n        * Normalization in this case means sums to 1. \n        * Cumulative importance is calculated by summing features from most to least important\n        * A threshold of 0.9 will show the most important features needed to reach 90% of cumulative importance\n    \n    \"\"\"\n    plt.style.use('fivethirtyeight')\n    \n    # Sort features with most important at the head\n    df = df.sort_values('importance', ascending = False).reset_index(drop = True)\n    \n    # Normalize the feature importances to add up to one and calculate cumulative importance\n    df['importance_normalized'] = df['importance'] / df['importance'].sum()\n    df['cumulative_importance'] = np.cumsum(df['importance_normalized'])\n    \n    plt.rcParams['font.size'] = 12\n    \n    # Bar plot of n most important features\n    df.loc[:n, :].plot.barh(y = 'importance_normalized', \n                            x = 'feature', color = 'darkgreen', \n                            edgecolor = 'k', figsize = (12, 8),\n                            legend = False, linewidth = 2)\n\n    plt.xlabel('Normalized Importance', size = 18); plt.ylabel(''); \n    plt.title(f'{n} Most Important Features', size = 18)\n    plt.gca().invert_yaxis()\n    \n    \n    if threshold:\n        # Cumulative importance plot\n        plt.figure(figsize = (8, 6))\n        plt.plot(list(range(len(df))), df['cumulative_importance'], 'b-')\n        plt.xlabel('Number of Features', size = 16); plt.ylabel('Cumulative Importance', size = 16); \n        plt.title('Cumulative Feature Importance', size = 18);\n        \n        # Number of features needed for threshold cumulative importance\n        # This is the index (will need to add 1 for the actual number)\n        importance_index = np.min(np.where(df['cumulative_importance'] > threshold))\n        \n        # Add vertical line to plot\n        plt.vlines(importance_index + 1, ymin = 0, ymax = 1.05, linestyles = '--', colors = 'red')\n        plt.show();\n        \n        print('{} features required for {:.0f}% of cumulative importance.'.format(importance_index + 1, \n                                                                                  100 * threshold))\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6682ed9a5f5ee7d4fc2b29df9efafa85df357ec5"},"cell_type":"code","source":"temp_df = pd.DataFrame({'feature':features,'importance':importances})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"30964b14194e102433be97b118048ea195a219cb"},"cell_type":"code","source":"norm_fi = plot_feature_importances(temp_df, threshold=0.95)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6c119f0ec0913f9d49686de25d0dc91140e6f62b"},"cell_type":"markdown","source":"Education reigns supreme! The most important variable is the average amount of education in the household, followed by the maximum education of anyone in the household. I have a suspicion these variables are highly correlated (collinear) which means we may want to remove one of them from the data. The other most important features are a combination of variables we created and variables that were already present in the data. <br />\n=>  교육은 정말 중요해보여요! 가장 중요한 변수는 각 가정에서 교육의 평균양이었스비다. 그리고 나는 이러한 변수들이 상당히 연관관계가 클 것 같습니다. 그래서 우리는 그들을 지워주려고 합니다. 다른 중요한 특징흔 우리가 만들었넌 변수들의 조합이고 그것들은 이미 데이터 안에 존재합니다 \n\nIt's interesting that we only need 106 of the ~180 features to account for 90% of the importance. This tells us that we may be able to remove some of the features. However, feature importances don't tell us which direction of the feature is important (for example, we can't use these to tell whether more or less education leads to more severe poverty) they only tell us which features the model considered relevant.<br />\n=>106에서 180개 정도의 피쳐만 있어도 90%의 importance를 달성한 것을 알 수 있습니다. 이는 우리가 몇몇 feature들을 지워줘야 한다는 이야기 입니다. 그러나 feature_importances는 특징의 어느 방향이 중요한지 말해주지 않습니다. 그들은 단지 어느 특징이 모델과 관련이 있었는지만 알려줍니다. (어느 컬럼이 부의 수준과 관련이 있다는 것은 이야기 해주지 않습니다.)"},{"metadata":{"trusted":true,"_uuid":"12e6479c5057b2627f475487dcccb2840650c513"},"cell_type":"code","source":"def kde_target(df, variable):\n    \"\"\"Plots the distribution of `variable` in `df` colored by the `Target` column\"\"\"\n    \n    colors = {1: 'red', 2: 'orange', 3: 'blue', 4: 'green'}\n\n    plt.figure(figsize = (12, 8))\n    \n    df = df[df['Target'].notnull()]\n    \n    for level in df['Target'].unique():\n        subset = df[df['Target'] == level].copy()\n        sns.kdeplot(subset[variable].dropna(), \n                    label = f'Poverty Level: {level}', \n                    color = colors[int(subset['Target'].unique())])\n\n    plt.xlabel(variable); plt.ylabel('Density');\n    plt.title('{} Distribution'.format(variable.capitalize()));","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d9e2da7195c4219eda8b185a5072df3e46cafe86"},"cell_type":"code","source":"kde_target(final, 'meaneduc')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"31010a242db34100c221974dd96e5ad08151dce3"},"cell_type":"code","source":"kde_target(final, 'escolari/age-range_')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a727060042fb5422e33c45a2b38685654863954d"},"cell_type":"markdown","source":"**Model Selection** <br />\nNow that we have a good set of features, it's time to get into the modeling. <br />\n=> 이제 우리는 좋은 변수들의 집합을 가지고 있습니다. 이제 모델링을 해볼 차례입니다.\nWe already tried one basic model, the Random Forest Classifier which delivered a best macro F1 of 0.35. <br />\n=> 우리는 이미 RandomForestClassifier로 0.35점의 성과를 냈습니다. <br />\nHowever, in machine learning, there is no way to know ahead of time which model will work best for a given dataset. <br />\n=> 그러나 머신러닝에서 어느 모델이 주어진 데이터셋에서 최고로 작동을 자랄지 아는 방법이 없습니다. <br />\nThe following plot shows that there are some problems where even Gaussian Naive Bayes will outperform a gradient boosting machine. <br />\n=> 다음의 그림은 가우시안 나이브 베이즈가 그라디언트 부스팅보다 좋은 성적을 낸적이 있는 문제도 있다는 것입니다.<br />\nThis is from an excellent paper by Randal Olson that discusses many points of machine learning"},{"metadata":{"_uuid":"5b0b937e4d59c83acaa2f95b2f190231d269465f"},"cell_type":"markdown","source":"What this plot tells us is that we have to try out a number of different models to see which is optimal. <br />\n=> 이 플롯이 말하고자 하는 바는 어느 것이 최적인지를 찾기위해서는 여러가지 다른 모델들을 테스트 해봐야 한다는 것입니다. <br />\nMost people eventually settle on the gradient boosting machine and we will try that out, but for now we'll take a look at some of the other options. There are literally dozens (maybe hundreds) of multi-class machine <br />\n=>많은 사람들이 끝내 그라디언트 부스팅에 머물지만, 우리는 다른 선택지들을 좀 살펴볼 필요가 있다. 여기에 굉장히 많은 선택지들이 있다.<br />\nlearning models if we look at the Scikit-Learn documentation. We don't have to try them all, but we should sample from the options. <br />\n=> 다는 해보지 못하겠지만 일부라도 실험해보도록 하자 <br /> \n\nWhat we want to do is write a function that can evaluate a model <br />. \n=> 우리는 각 모델을 을 평가할 수 있는 함수를 짜야한다. <br />\nThis will be pretty simple since we already wrote most of the code. \n=> 이는 간단할 것이다.\nIn addition to the Random Forest Classifier, we'll try eight other Scikit-Learn models. <br />\nLuckily, this dataset is relatively small and we can rapidly iterate through the models. <br />\n=> 운좋게도 우리의 모델은 작기 때문에 상대적으로 빠르게 될 것이다 <br />\nWe will make a dataframe to hold the results and the function will add a row to the dataframe for each model. <br />\n=> 결과를 담을 데이터프레임을 만들고 각모델에 대해서 결과를 데이터프레임에 행에 추가해 줄 것이다.<br />"},{"metadata":{"trusted":true,"_uuid":"507bd6d657f427b13ca419de968fffb51e6231c0"},"cell_type":"code","source":"# Model imports\nfrom sklearn.svm import LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.linear_model import LogisticRegressionCV, RidgeClassifierCV\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.neighbors import KNeighborsClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e2b164a359789c73191dc0301e928a0cad776763"},"cell_type":"code","source":"import warnings \nfrom sklearn.exceptions import ConvergenceWarning\n\n# Filter out warnings from models\nwarnings.filterwarnings('ignore', category = ConvergenceWarning)\nwarnings.filterwarnings('ignore', category = DeprecationWarning)\nwarnings.filterwarnings('ignore', category = UserWarning)\n\n# Dataframe to hold results\nmodel_results = pd.DataFrame(columns = ['model', 'cv_mean', 'cv_std'])\n\ndef cv_model(train, train_labels, model, name, model_results=None):\n    \"\"\"Perform 10 fold cross validation of a model\"\"\"\n    \n    cv_scores = cross_val_score(model, train, train_labels, cv = 10, scoring=scorer, n_jobs = -1)\n    print(f'10 Fold CV Score: {round(cv_scores.mean(), 5)} with std: {round(cv_scores.std(), 5)}')\n    \n    if model_results is not None:\n        model_results = model_results.append(pd.DataFrame({'model': name, \n                                                           'cv_mean': cv_scores.mean(), \n                                                            'cv_std': cv_scores.std()},\n                                                           index = [0]),\n                                             ignore_index = True)\n\n        return model_results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"62ebb6b2e750a5f615ffceffabfe3c768b4ebf90"},"cell_type":"code","source":"model_results = cv_model(train_set,train_labels,LinearSVC(),'LSVC',model_results)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"475a2fa6af450490585c742fb06bf88bc785cb1e"},"cell_type":"markdown","source":"That's one model to cross off the list (although we didn't perform hyperparameter tuning so the actual performance could possibly be improved). <br />"},{"metadata":{"trusted":true,"_uuid":"39eacbc80f25fbabd6196e9521808e4e1ae242d3"},"cell_type":"code","source":"model_results = cv_model(train_set,train_labels,GaussianNB(),'GNB',model_results)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7b0880e2bc5dfffdf1e261c4b91a6e71d90f16b2"},"cell_type":"markdown","source":"That performance is very poor. I don't think we need to revisit the Gaussian Naive Bayes method (although there are problems on which it can outperform the Gradient Boosting Machine). <br />\n=> 가우시안 나이브 베이즈 모델의 성능은 굉장히 형편없었다."},{"metadata":{"trusted":true,"_uuid":"dc9d326394f8ede9a13d9d3dd69add239c422163"},"cell_type":"code","source":"model_results = cv_model(train_set,train_labels,MLPClassifier(hidden_layer_sizes=(32,64,128,64,32)),'MLP',model_results)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"74fb72ff954cb9b65a2d41f5adc42a96477d1739"},"cell_type":"markdown","source":"The multi-layer perceptron (a deep neural network) has decent performance. This might be an option if we are able to hyperparameter tune the network. However, the limited amount of data could be an issue with a neural network as these generally require hundreds of thousands of examples to learn effectively. <br />\n=> 다층 퍼세브론은 굉장한 성능을 보여주었습니다. 우리가 튜닝을 좀한다면 더 좋아질 것 같습니다. 그러나 신경망에서 작은 양의 데이터는 문제가 될 수 있습니다."},{"metadata":{"trusted":true,"_uuid":"2a9aadc28bddb4e6019de209ceddedda9e508748"},"cell_type":"code","source":"model_results = cv_model(train_set, train_labels, \n                          LinearDiscriminantAnalysis(), \n                          'LDA', model_results)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"34b2c9e66efa027566272a89937c72412117ed43"},"cell_type":"markdown","source":"If you run LinearDiscriminantAnalysis without filtering out the UserWarnings, you get many messages saying \"Variables are collinear.\" This might give us a hint that we want to remove some collinear features! We might want to try this model again after removing the collinear variables because the score is comparable to the random forest. <br />\n=> 해당 모델을 경고창을 없애지 않고 한다면 위와 같은 경고메시지를 만날 수 있는데 이는, 유사한 성격을 가진 컬럼이 많아서 다중 공선성의 문제가 생길 수 있기 때문입니다. 하지만 우리가 제일 처음에 가이드 라인으로 사용했던 랜덤포레스트와 비슷한 점수를 내기 때문에 공선성 문제를 해결하고 이 모델을 다시 시험해 볼 것입니다."},{"metadata":{"trusted":true,"_uuid":"3e0d06f02644181c5446fb3238f9c3f3a1892d0a"},"cell_type":"code","source":"model_results = cv_model(train_set, train_labels, RidgeClassifierCV(), 'RIDGE', model_results)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9b1a4560fb506b0925b490c335ccba2acaf148f6"},"cell_type":"markdown","source":"The linear model (with ridge regularization) does surprisingly well. This might indicate that a simple model can go a long way in this problem (although we'll probably end up using a more powerful method). <br />\n=> 선형모델은 생각보다 잘 되었습니다. \n"},{"metadata":{"trusted":true,"_uuid":"6b2f407b824137d1823848d84de751fbba2cf38d"},"cell_type":"code","source":"for n in [5, 10, 20]:\n    print(f'\\nKNN with {n} neighbors\\n')\n    model_results = cv_model(train_set, train_labels, \n                             KNeighborsClassifier(n_neighbors = n),\n                             f'knn-{n}', model_results)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d3e0d98258014b672db2855b9fdde9167897fbea"},"cell_type":"markdown","source":"As one more attempt, we'll consider the ExtraTreesClassifier, a variant on the random forest using ensembles of decision trees as well. <br />\n=> 마지막으로는 결정트리에 앙상블을 이용한 랜덤포레스트의 변형인 ExtraTreeClassifier를 이용해보겠습니다.\n"},{"metadata":{"trusted":true,"_uuid":"2e730cf898da32092abe1f2a4db80aebc6b3786c"},"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesClassifier\n\nmodel_results = cv_model(train_set, train_labels, \n                         ExtraTreesClassifier(n_estimators = 100, random_state = 10),\n                         'EXT', model_results)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ebd14376cf8aa27116ca67901cb4db3e5e51b921"},"cell_type":"markdown","source":"## Comparing Model Performance\nWith the modeling results in a dataframe, we can plot them to see which model does the best."},{"metadata":{"trusted":true,"_uuid":"cd9a46ad5e5d605676fa10a1b3505737f5005ee0"},"cell_type":"code","source":"model_results = cv_model(train_set, train_labels, \n                         RandomForestClassifier(n_estimators = 100, random_state = 10),\n                         'RF', model_results)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bce7bc8a1800cf941c4685482a8c0532e8a7fda6"},"cell_type":"code","source":"models = model_results['model']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bb17e90bb8536223f11dc4864a915224860e9269"},"cell_type":"code","source":"model_results_ = model_results.set_index('model')\nmodel_results_['cv_mean'].plot(kind='bar',color='orange',figsize=(8,6),edgecolor='k',linewidth=2, yerr=list(model_results['cv_std']))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5214390c6757a1e8d037393b36c9a2c1a8dc8993"},"cell_type":"markdown","source":"The most likely candidate seems to be the Random Forest because it does best right out of the box. <br />\n=> 많은 사람들은 랜덤포레스트가 최고의 방법이라고 볼 수 있다. <br />\nWhile we didn't tune any of the hyperparameters so the comparison between models is not perfect, these results reflect those of many other Kaggle competitiors finding that tree-based ensemble methods (including the Gradient Boosting Machine) perform very well on structured datasets. <br />\n=> 그러나 우리는 아무런 파라미터 튜닝을 하지 않았기 때문에 이 모델들은 완벽하지 않다, 이 결과는 왜 많은 캐글의 이용자들이 트리기반의 아앙블 메소드를 쓰는지 보여준다 <br />\nHyperparameter performance does improve the performance of machine learning models, but we don't have time to try all possible combinations of settings for all models.  <br />\n=> 하이퍼 파라미터 튜닝은 머신런이 모델의 성능을 향상시켜주지만, 모든 모델의 파라미터 튜닝을 하고 있기엔 시간이 없다 <br />\nThe graph below (from the paper by Randal Olson) shows the effect of hyperparameter tuning versus the default values in Scikit-Learn. "},{"metadata":{"_uuid":"ad884f3553323f363a54a0dafcc0bebbf8351e91"},"cell_type":"markdown","source":"In most cases the accuracy gain is less than 10% so the worst model is probably not suddenly going to become the best model through tuning. <br />\n=> 대부분의 경우에 정확도를 많아도 10퍼센트까지만 올릴 수 있다. \n\nFor now we'll say the random forest does the best. Later we'll look at using the Gradient Boosting Machine, although not implemented in Scikit-Learn. <br />\n=> 지금은 랜덤포레스트가 최고라고 말할 수 있지만, 나중에 우리가 사용하게될 그라디언트 부스팅 모델에 대해서 알아볼 것이다. <br />\nInstead we'll be using the more powerful LightGBM version. Now, let's turn to making a submission using the random forest. <br />\n=> 우리는 더 강력한 LightGBM모델을 사용할 것이다."},{"metadata":{"_uuid":"c782d11af9f0bb69d1d04e7e279143e7eaa9da93"},"cell_type":"markdown","source":"## Making a Submission\nIn order to make a submission, we need the test data. Fortunately, we have the test data formatted in exactly the same manner as the train data. <br />\n=> 제출을 위해, 우리는 테스트 데이터가 필요하다. 운좋게도 우리는 테스트 데이터를 훈련데이터를 전처리한 것과 동일하게 처리해 두었다.\n\nThe format of a testing submission is shown below. Although we are making predictions for each household, we actually need one row per individual (identified by the Id) but only the prediction for the head of household is scored. <br />\n=>  테스트 셋의 결과 제출은 아래와 같다. 비록 우리가 모든 가구들에 대해 결과를 예측했더라도 우리는 개인 row당 하나의 타겟 값을 원한다. 그러나 가장의 것만 점수에 합산 될 것이다.\n\nId,Target <br />\nID_2f6873615,1 <br />\nID_1c78846d2,2 <br />\nID_e5442cf6a,3 <br />\nID_a8db26a79,4 <br />\nID_a62966799,4 <br />\n\nThe submission_base will have all the individuals in the test set since we have to have a \"prediction\" for each individual while the test_ids will only contain the idhogar from the heads of households. <br />\n=> submissin_base는 모든 개개인에 대한 정보를 다 가지고 있다. 그런데 우리가 기잔 test_id는 가구별로 예측이 되어있다 <br />\nWhen predicting, we only predict for each household and then we merge the predictions dataframe with all of the individuals on the household id (idhogar). <br />\n=> 예측시에, 우리는 가구 단위로 예측을 했고, 우리는 두 개의 데이터프레임을 left outer join을 통해 모든 가구원들에게 가구별 타겟 값을 할당할 것이다. <br />\nThis will set the Target to the same value for everyone in a household. For the test households without a head of household, we can just set these predictions to 4 since they will not be scored. <br />\n=> 이것은 모든 구성원들에게 값이 돌아가지만, 가장이 없는 가구의 경우에는 예측이 되지 않기 때문에 4로 배정한다."},{"metadata":{"trusted":true,"_uuid":"2300b1b08844eb8e2f05f27c8a8a634583ca42f5"},"cell_type":"code","source":"test_ids = list(final.loc[final['Target'].isnull(),'idhogar'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9cbb6de4b9fdf7130114bdd3e4f00e8863f987b4"},"cell_type":"markdown","source":"The function below takes in a model, a training set, the training labels, and a testing set and performs the following operations: <br />\n\nTrains the model on the training data using fit<br />\nMakes predictions on the test data using predict<br />\nCreates a submission dataframe that can be saved and uploaded to the competition<br />"},{"metadata":{"trusted":true,"_uuid":"e7acb3c9bed61fe263f9ac815ea2547d94fc7633"},"cell_type":"code","source":"def submit(model,train,train_labels,test,test_ids):\n    \n    model.fit(train,train_labels)\n    predictions = model.predict(test)\n    predictions = pd.DataFrame({'idhogar':test_ids,'Target':predictions})\n    \n    submission = submission_base.merge(predictions,on='idhogar',how='left').drop(columns=['idhogar'])\n    \n    submission['Target'] = submission['Target'].fillna(4).astype(np.int8)\n    \n    return submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6fe5534b2225b8648225f5ebbb76994d16eac02c"},"cell_type":"code","source":"rf_submission = submit(RandomForestClassifier(n_estimators = 100, \n                                              random_state=10, n_jobs = -1), \n                         train_set, train_labels, test_set, test_ids)\n\nrf_submission.to_csv('rf_submission.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6d03b888f5380072df30bf17d855d2a2cd0075a1"},"cell_type":"markdown","source":"## Feature Selection \nOne potential method for improving model performance is feature selection. This is the process where we try to keep only the most useful features for our model. \"Most useful\" can mean many different things, and there are numerous heuristics for selecting the most important features. For feature selection in this notebook, we'll first remove any columns with greater than 0.95 correlation (we already did some of this during feature engineering) and then we'll apply recursive feature elimination with the Scikit-Learn library. <br />\n모델의 성능을 평가하기 위한 하나의 방법은 feature_selection이다. 이는 우리의 모델에서 유의미한 컬럼들만 남기기 위해서 진행하는 것이다. 가장 유용하다는 것은 많은 다른 것들을 의미하고, 이런 가장 중요한 컬럼들 찾는데 많은 경험적 지식들이 있다. 이 커널에서는 우리는 0.95이상의 상관관께 이상을 가지는 임의의 컬럼들을 지우고, **Recursive feature elimination** 기법을 이용할 것이다\n\n\nFirst up are the correlations. 0.95 is an arbitrary threshold - feel free to change the values and see how the performance changes! <br />\n일단은 0.95를 임계점으로 하여 컬럼들을 찾아내자\n"},{"metadata":{"trusted":true,"_uuid":"2a2cb47d134fe6337087c13b8bab2fdf4faf99db"},"cell_type":"code","source":"train_set = pd.DataFrame(train_set,columns=features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0575147349f33e3560149bf3c23b5d972fd72db3"},"cell_type":"code","source":"corr_matrix = train_set.corr()\n\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape),k=1).astype(np.bool))\n\nto_drop = [column for column in upper.columns if any(abs(upper[column]) >0.95)]\n\nto_drop","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1aa95d551d6d514779093f444308ee69000ada14"},"cell_type":"code","source":"print(train_set.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ab93290820fe382eb6664c1e645b384b6fbfa30e"},"cell_type":"code","source":"train_set = train_set.drop(to_drop,axis=1)\nprint(train_set.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"68b1698668ee7ae588e415c5fe5d283320945a08"},"cell_type":"code","source":"test_set = pd.DataFrame(test_set,columns=features)\nprint(test_set.shape)\ntrain_set, test_set = train_set.align(test_set,axis=1,join='inner')\nfeatures = list(train_set.columns)\nprint(test_set.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2fad1e6f2ba60b987a64db68d8967e1c3d43e049"},"cell_type":"markdown","source":"## Recursive Feature Elimination with Random Forest\nThe RFECV in Sklearn stands for Recursive Feature Elimination with Cross Validation. The selector operates using a model with feature importances in an iterative manner. At each iteration, it removes either a fraction of features or a set number of features. The iterations continue until the cross validation score no longer improves. <br />\n=> RFECV는 교차검증을 이용한 반복적 특징 제거법입니다. 선택자는 반복적인 방법을 통해서 모델의 feature_importance를 진행합니다. 각각의 반복에서, RFECV는 특징은 부분이나 특징의 개수를 점점 줄여갑니다. 이 반복은 더이상 교차검증 점수가 향상되지 않을 때 까지 진행합니다.\n\nTo create the selector object, we pass in the the model, the number of features to remove at each iteration, the cross validation folds, our custom scorer, and any other parameters to guide the selection. <br />\n선택자 객체를 생성하기 위해, 우리는 모델을 넘기고, 각각의 반복시행마다 없앨 컬럼들의 개수, 교차검증의 겹수, 우리의 custom scorer 그리고 선택자가 참고할 수 있는 다른 파라미터들을 넘겨줍니다."},{"metadata":{"trusted":true,"_uuid":"4020d74376d8c9b3f39369f360a6fb8400229190"},"cell_type":"code","source":"from sklearn.feature_selection import RFECV\n\nestimator = RandomForestClassifier(random_state=10,n_estimators=100,n_jobs=-1)\n\nselector = RFECV(estimator,step=1,cv=5,scoring=scorer,n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"264dbfa67ab1f89b2ce62775aca5f5fdad49ab07"},"cell_type":"markdown","source":"Then we fit the selector on the training data as with any other sklearn model. This will continue the feature selection until the cross validation scores no longer improve. <br />\n=> 다음으로는 선택자를 훈련세트를 이용하여 학습시킵니다. 이는 교차검증점수가 더 이상 낮아질 때가 없을 때 까지 계속해서 진행 할 것입니다."},{"metadata":{"trusted":true,"_uuid":"f3be2412c0e37e65b076148795efaea39f7cbf30"},"cell_type":"code","source":"selector.fit(train_set,train_labels)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"11ca3c4695c17c3d1d9f87f29f2664924dee7080"},"cell_type":"markdown","source":"We can investigate the object to see the training scores for each iteration. The following code will plot the validation scores versus the number of features for the training. <br />\n=> 우리는 각각의 반복마다 훈련 점수를 볼 수 있습니다. 다음의 코드는 교차검증 점수와 컬럼의 개수를 그림화 한 것입니다."},{"metadata":{"trusted":true,"_uuid":"79a85b83a03ee78c89a9d18d8933ac4ac056e3c9"},"cell_type":"code","source":"plt.plot(selector.grid_scores_)\n\nplt.xlabel('Number of Features')\nplt.ylabel('Macro F1 Score')\nplt.title('Feature Selection Scores')\nselector.n_features_","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c10405b3ad69ab998657f16726567fe18c0ed815"},"cell_type":"markdown","source":"We can see that the score improves as we add features up until 96 features. According to the selector, this is the optimal number of features. <br />\n우리는 96개의 feature를 넣을 때가지만 해도 점수의 향상을 알 수 있었습니다. selector에 따르면 최적의 피쳐의 수는 정해졌습니다.\n\nThe rankings of each feature can be found by inspecting the trained object. These represent essentially the importance of features averaged over the iterations. Features can share the same ranking, and only features with a rank of 1 are retained. <br />\n각각의 컬럼들의 랭킹은 훈련된 선택자 객체를 통해 알 수 있습니다. 이들은 반복을 거듭함에 따라 평균된 중요도를 나타냅니다. 특징들은 같은 랭킹을 공유할수 있습니다. rank가 1인 컬럼들만 사용될 것입니다."},{"metadata":{"trusted":true,"_uuid":"03a500e9c551656b7f021fa49cca138b3a3bb1f2"},"cell_type":"code","source":"rankings = pd.DataFrame({'feature':list(train_set.columns),'rank':list(selector.ranking_)}).sort_values(by='rank')\nrankings.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"72748e5e9fabcc8da4c8f440e193c1de1823672f"},"cell_type":"markdown","source":"Finally, we select the features and then evaluate in cross validation. <br />\n=> 마지막으로 우리들의 교차검증 결과를 확인해 봅시다."},{"metadata":{"trusted":true,"_uuid":"d09bcbc79caba0f3f98b15c54d9f5e3ed0a1a3cf"},"cell_type":"code","source":"train_selected = selector.transform(train_set)\ntest_selected = selector.transform(test_set)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"66f301f9f8bc18ac9e9854e7513e975e4f04694d"},"cell_type":"code","source":"train_selected","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b37a795c7834beff226ab568706d173ac1e60e9d"},"cell_type":"code","source":"# np.where(selector.ranking_==1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"465d5cfdac502ca7278ae172b52c5656995e20eb"},"cell_type":"code","source":"selected_features = train_set.columns[np.where(selector.ranking_==1)]\ntrain_selected = pd.DataFrame(train_selected,columns=selected_features)\ntest_selected = pd.DataFrame(test_selected,columns=selected_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"313c432729da43350f0199a6f0fe445b749790e6"},"cell_type":"code","source":"train_selected.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3510bbb267c46f588be00f1823ff297f68f51170"},"cell_type":"code","source":"model_results = cv_model(train_selected,train_labels,model,'RF-SEL',model_results)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"095dc948f5f6ead77e5e1891786936229dd629e0"},"cell_type":"code","source":"model_results.set_index('model', inplace = True)\nmodel_results_ = model_results\n\nmodel_results_['cv_mean'].plot.bar(color = 'orange', figsize = (8, 6),\n                                  yerr = list(model_results['cv_std']),\n                                 edgecolor = 'k', linewidth = 2)\nplt.title('Model F1 Score Results');\nplt.ylabel('Mean F1 Score (with error bar)');\nmodel_results.reset_index(inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7207d9660033150f7f1f41a2ba58c694b30397c2"},"cell_type":"markdown","source":"# Upgrading Our Model: Gradient Boosting Machine\nAfter using the Random Forest and getting decent scores, it's time to step up and use the gradient boosting machine. If you spend any time on Kaggle, you'll notice that the Gradient Boosting Machine (GBM) wins a high percentage of competitions where the data is structured (in tables) and the datasets are not that large (less than a million observations). <br />\n=> 우리가 랜덤포레스트를 통해 좋은 점수를 받은 이후에, 그라디언트 부스팅모델을 이용할 차례입니다. 만약 독자가 캐글에 익숙한 사용자라면 GBM모델이 많은 데이터가 정형화되어있고 데이터셋이 크지 않은 컴피티션에서 승리한 것을 보았을 것입니다.<br />\n\nI won't go too much into the details here, but instead will focus on the implementation. We'll use the GBM in LightGBM, although there are also options in Scikit-Learn, XGBOOST, and CatBoost. The first set of hyperparameters we'll use were based on those I've found have worked well for other problems. <br />\n=> 많은 자세한 것들을 언급하진 않겠지만 실행에 초점을 맞춰봅시다. 우리는 LightGBM을 사용할 것이지만 다른 여러 선택지가 사이킷런,XGBOOST,CatBoost가 있습니다. 첫번 째 하이퍼 파라미터의 세트는 우리가 위에서 찾아왔던 것들을 기반으로 세팅하겠습니다.<br />\n\n## Choosing Number of Estimators with Early Stopping\nTo choose the number of estimators (the number of decision trees in the ensemble, called n_estimators or num_boost_rounds), we'll use early stopping with 5-fold cross validation. This will keep adding estimators until the performance as measured by the Macro F1 Score has not increased for 100 training rounds. To use this metric, we'll have to define a custom metric. <br />\n=>반복시행의 수를 선택하기 위해 우리는 5겹 교차검증을 이용할 것입니다. 이것은 우리가 이용하는 척도 Macro F1 수치가 100번의 훈련 반복시행에 있어서 증가하지 않을 때 까지 반복시행을 추가합니다. 이 수치를 이용하기 위해 custom metric을 이용합시다.<br />"},{"metadata":{"trusted":true,"_uuid":"a1cd104cf93b6e92f2a78a7d29fdd973b2d7a719"},"cell_type":"code","source":"def macro_f1_score(labels, predictions):\n    # Reshape the predictions as needed\n    # 행으로만 이루어진 결과치를 4열씩 차례대로 배치하고 그 4열에서 max를 각각 추출\n    predictions = predictions.reshape(len(np.unique(labels)), -1 ).argmax(axis = 0)\n    \n    metric_value = f1_score(labels, predictions, average = 'macro')\n    \n    # Return is name, value, is_higher_better\n    return 'macro_f1', metric_value, True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e0bf2ef14d7020b8f6cfaa96b802dcc25c594000"},"cell_type":"markdown","source":"## Light Gradient Boosting Machine Implementation\nThe function below implements training the gradient boosting machine with Stratified Kfold cross validation and early stopping to prevent overfitting to the training data (although this can still occur). The function performs training with cross validation and records the predictions in probability for each fold. To see how this works, we can return the predictions from each fold and then we'll return a submission to upload to the competition. <br />\n=> 아래의 함수는 K겹 교차검증을 이용한 그라디언트 부스팅 모델을 학습시키는 함수이며, early stopping은 훈련데이터가 과적합되는 것을 막는 파라미터입니다. 이 함수는 교차검증을 통해 훈련시키고 각 계층마다의 예측에 대한 확률을 기록합니다. 이것이 어떻게 작동하는지를 알기 위해서 각 폴드별 예측치를 반환하는 prediction을 반환하고 컴피티션의 업로드를 위해서 submission 또한 업로드 할 것 입니다.<br />\n\nChoosing hyperparameters for the Gradient Boosting Machine can be tough and generally is done through model optimization. In this notebook, we'll use a set of hyperparameters that I've found work well on previous problems (although they will not necessarily translate to this competition). <br />\n=> 그라디언트 부스팅에서 하이퍼파라미터를 고르는 것은 어려운일입니다. 그리고 이는 모델 최적화를 통해 이루어질 것입니다. 여기서는 일련의 하이퍼 파라미터들을 세팅할 것입니다.<br />\n\nWe set the n_estimators to 10000 but we won't actually reach this number because we are using early stopping which will quit training estimators when the cross validation metric does not improve for early_stopping_rounds. There's a lot going on in this function, and read through it carefully to make sure you have it all! I've tried to make the comments and code straightforward. (The display is used to show custom information during training in combination with %%capture so we don't have to see all the LightGBM information during training). <br />\n=> 우리는 n_estimators를 10000으로 세팅하였지만, 우리는 교차검증 점수가 더 이상 오르지 않는 순간을 캐치하고 이를 중단 시키는 early_stopping을 설정하기 때문에 만 번의 반복 시행까지는 가지 않을 것입니다. 많은 것들이 이 함수에서 이루어집니다. 이를 모두 읽고 여러분의 것으로 만드세요. 코드에 주석들을 달아 놨습니다.<br />"},{"metadata":{"trusted":true,"_uuid":"92bcf25298e646266f2b7920a738531e7e5a6b0e"},"cell_type":"code","source":"# from sklearn.model_selection import StratifiedKFold\n# import lightgbm as lgb\n\n# feature_names = features\n\n# params = {\n#     'boosting_type':'dart',\n#     'colsample_bytree':0.88,\n#     'learning_rate':0.028,\n#     'min_child_samples':10,\n#     'num_leaves':36, 'reg_alpha':0.76,\n#     'reg_lambda':0.43,\n#     'subsample_for_bin':40000,\n#     'subsample':0.54,\n#     'class_weight':'balanced'\n# }\n\n# model = lgb.LGBMClassifier(**params,objective='multiclass',n_jobs=-1,n_estimators=1000,random_state=10)\n\n# SKFOLD = StratifiedKFold(n_splits=5,shuffle=True)\n\n# predictions = pd.DataFrame()\n# importances = np.zeros(len(feature_names))\n\n# features = np.array(train_set)\n# test_features = np.array(test_set)\n# labels = np.array(train_labels).reshape((-1))\n\n# valid_scores = []\n\n# for i,(train_indices,valid_indices) in enumerate(SKFOLD.split(features,labels)):\n    \n#     fold_predictions = pd.DataFrame()\n    \n#     X_train,X_valid = features[train_indices],features[valid_indices]\n#     y_train,y_valid = labels[train_indices],labels[valid_indices]\n    \n#     model.fit(X_train,y_train,early_stopping_rounds=100,eval_metric=macro_f1_score,eval_set=[(X_train,y_train),(X_valid,y_valid)],eval_names=['train','valid'],verbose=200)\n    \n#     valid_scores.append(model.best_score_['valid']['macro_f1'])\n    \n#     fold_probabilities = model.predict_proba(test_features)\n    \n#     for j in range(4):\n#         fold_predictions[(j+1)] = fold_probabilities[:,j]\n        \n#     fold_predictions['idhogar'] = test_ids\n#     fold_predictions['fold'] = (i+1)\n    \n#     predictions = predictions.append(fold_predictions)\n    \n#     importances += model.feature_importances_ / 5\n    \n#     display(f'Fold {i + 1}, Validation Score: {round(valid_scores[i], 5)}, Estimators Trained: {model.best_iteration_}')\n    \n#     predictions['Target'] = predictions[[1, 2, 3, 4]].idxmax(axis = 1)\n#     predictions['confidence'] = predictions[[1, 2, 3, 4]].max(axis = 1)\n    \n#     submission = submission_base.merge(predictions[['idhogar','Target']],on='idhogar',how='left').drop(columns=['idhogar'])\n#     submission['Target'] = submission['Target'].fillna(4).astype(np.int8)\n#     break\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"19fed6edf3958be476bce1acea5b8fbcf53128a0"},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nimport lightgbm as lgb\nfrom IPython.display import display\n\ndef model_gbm(features, labels, test_features, test_ids, \n              nfolds = 5, return_preds = False, hyp = None):\n    \"\"\"Model using the GBM and cross validation.\n       Trains with early stopping on each fold.\n       Hyperparameters probably need to be tuned.\"\"\"\n    \n    feature_names = list(features.columns)\n\n    # Option for user specified hyperparameters\n    if hyp is not None:\n        # Using early stopping so do not need number of esimators\n        if 'n_estimators' in hyp:\n            del hyp['n_estimators']\n        params = hyp\n    \n    else:\n        # Model hyperparameters\n        params = {'boosting_type': 'dart', \n                  'colsample_bytree': 0.88, \n                  'learning_rate': 0.028, \n                   'min_child_samples': 10, \n                   'num_leaves': 36, 'reg_alpha': 0.76, \n                   'reg_lambda': 0.43, \n                   'subsample_for_bin': 40000, \n                   'subsample': 0.54, \n                   'class_weight': 'balanced'}\n    \n    # Build the model\n    model = lgb.LGBMClassifier(**params, objective = 'multiclass', \n                               n_jobs = -1, n_estimators = 10000,\n                               random_state = 10)\n    \n    # Using stratified kfold cross validation\n    strkfold = StratifiedKFold(n_splits = nfolds, shuffle = True)\n    \n    # Hold all the predictions from each fold\n    predictions = pd.DataFrame()\n    importances = np.zeros(len(feature_names))\n    \n    # Convert to arrays for indexing\n    features = np.array(features)\n    test_features = np.array(test_features)\n    labels = np.array(labels).reshape((-1 ))\n    \n    valid_scores = []\n    \n    # Iterate through the folds\n    for i, (train_indices, valid_indices) in enumerate(strkfold.split(features, labels)):\n        \n        # Dataframe for fold predictions\n        fold_predictions = pd.DataFrame()\n        \n        # Training and validation data\n        X_train = features[train_indices]\n        X_valid = features[valid_indices]\n        y_train = labels[train_indices]\n        y_valid = labels[valid_indices]\n        \n        # Train with early stopping\n        model.fit(X_train, y_train, early_stopping_rounds = 100, \n                  eval_metric = macro_f1_score,\n                  eval_set = [(X_train, y_train), (X_valid, y_valid)],\n                  eval_names = ['train', 'valid'],\n                  verbose = 200)\n        \n        # Record the validation fold score\n        valid_scores.append(model.best_score_['valid']['macro_f1'])\n        \n        # Make predictions from the fold as probabilities\n        fold_probabilitites = model.predict_proba(test_features)\n        \n        # Record each prediction for each class as a separate column\n        for j in range(4):\n            fold_predictions[(j + 1)] = fold_probabilitites[:, j]\n            \n        # Add needed information for predictions \n        fold_predictions['idhogar'] = test_ids\n        fold_predictions['fold'] = (i+1)\n        \n        # Add the predictions as new rows to the existing predictions\n        predictions = predictions.append(fold_predictions)\n        \n        # Feature importances\n        importances += model.feature_importances_ / nfolds   \n        \n        # Display fold information\n        display(f'Fold {i + 1}, Validation Score: {round(valid_scores[i], 5)}, Estimators Trained: {model.best_iteration_}')\n\n    # Feature importances dataframe\n    feature_importances = pd.DataFrame({'feature': feature_names,\n                                        'importance': importances})\n    \n    valid_scores = np.array(valid_scores)\n    display(f'{nfolds} cross validation score: {round(valid_scores.mean(), 5)} with std: {round(valid_scores.std(), 5)}.')\n    \n    # If we want to examine predictions don't average over folds\n    if return_preds:\n        predictions['Target'] = predictions[[1, 2, 3, 4]].idxmax(axis = 1)\n        predictions['confidence'] = predictions[[1, 2, 3, 4]].max(axis = 1)\n        return predictions, feature_importances\n    \n    # Average the predictions over folds\n    predictions = predictions.groupby('idhogar', as_index = False).mean()\n    \n    # Find the class and associated probability\n    predictions['Target'] = predictions[[1, 2, 3, 4]].idxmax(axis = 1)\n    predictions['confidence'] = predictions[[1, 2, 3, 4]].max(axis = 1)\n    predictions = predictions.drop(columns = ['fold'])\n    \n    # Merge with the base to have one prediction for each individual\n    submission = submission_base.merge(predictions[['idhogar', 'Target']], on = 'idhogar', how = 'left').drop(columns = ['idhogar'])\n        \n    # Fill in the individuals that do not have a head of household with 4 since these will not be scored\n    submission['Target'] = submission['Target'].fillna(4).astype(np.int8)\n    \n    # return the submission and feature importances along with validation scores\n    return submission, feature_importances, valid_scores","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e834efff341a9810bff3abf5dcb1f506ce1e0aa9"},"cell_type":"markdown","source":"## Cross Validation with Early Stopping Notes\n\n> 참고. Variance와 Bias https://www.slideshare.net/freepsw/boosting-bagging-vs-boosting [boosting 기법의 이해]\n\nCross validation with early stopping is one of the most effective methods for preventing overfitting on the training set because it prevents us from continuing to add model complexity once it is clear that validation scores are not improving. Repeating this process across multiple folds helps to reduce the bias that comes from using a single fold. Early stopping also lets us train the model much quicker. Overall, early stopping with cross validation is the best method to select the number of estimators in the Gradient Boosting Machine and should be our default technique when we desig an implementation. <br />\n=> early stopping을 Cross validation 하는 것은 훈련세트에서 과적합을 막는 가장 효과적인 방법입니다. 왜냐하면 이는 우리가 모델 복잡도를 지속적으로 추가하는 것을 막기 때문이며, validation 점수가 향상되지 않을때 이는 명확해집니다. 여러겹을 거쳐서 이 과정을 반복하는 것은 하나의 단일 계층으로 했을 때 발생할 수 있는 bias를 줄여준다(bias가 낮으면 올바른 예측치를 찾아간다는 뜻 상충되는 개념으로 variance가 있음 위의 참고 url에 두 용어에 대한 개념이 있음). early stopping은 또한 우리가 훈련 모델 학습을 더 빠르게 해준다. 전반적으로 교차검증을 이용한 early stopping은 그라디언트 부스팅 모델에서 반복시행의 수를 선택하는 최고의 방법이며 실행을 하는데 있어서 가장 기초적인 지식이며 기술이다.  <br />"},{"metadata":{"trusted":true,"_uuid":"35c273bc1ac564a6466c62bd0a1972b651c7c512"},"cell_type":"code","source":"predictions, gbm_fi = model_gbm(train_set,train_labels,test_set,test_ids,return_preds=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a9bfb2c370160f45c676fbd7b592288f1f19bd86"},"cell_type":"markdown","source":"The power of the Gradient Boosting Machine can be seen here! The cross validation score blows away anything we've done previously. <br />\n=> 그라디언트 부스팅 모델의 힘은 여기서 발휘된다. <br />\n\nLet's take a look at the predictions to understand what is going on with the predictions in each fold. <br />\n=> 각각의 계층별 진행된 예측이 어떻게 되었는지 이해를 위해 prediction 변수를 살펴보자<br />"},{"metadata":{"trusted":true,"_uuid":"bf0c1b9d19cfc5803db4f35ab4cf19e485a9c828"},"cell_type":"code","source":"predictions.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cfca41f17396f293bfa8b15742ec24243ec86996"},"cell_type":"markdown","source":""},{"metadata":{"trusted":true,"_uuid":"ee12fb1793713b347630f6adc58066d570ac7af8"},"cell_type":"markdown","source":"For each fold, the 1, 2, 3, 4 columns represent the probability for each Target. The Target is the maximum of these with the confidence the probability. We have the predictions for all 5 folds, so we can plot the confidence in each Target for the different folds. <br />\n각각의 계층에 대해 1,2,3,4는 각 타겟의 확률을 나타낸다. 이들의 최대 값은 확률의 confidence값으로 되어있다. 우리는 5개의 계층에 대해서 진행했고 각 계층별로 자세히 확인 가능하도록 그림을 그려줄 수 있다.<br />"},{"metadata":{"trusted":true,"_uuid":"3b6176936ea00213a15ad66e8d7e3f22477790be"},"cell_type":"code","source":"plt.rcParams['font.size'] = 18\n\n# Kdeplot\ng = sns.FacetGrid(predictions, row = 'fold', hue = 'Target', size = 3, aspect = 4)\ng.map(sns.kdeplot, 'confidence');\ng.add_legend();\n\nplt.suptitle('Distribution of Confidence by Fold and Target', y = 1.05);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"13d6d983b6575f1c2eab60738a795dbfe9c828cf"},"cell_type":"markdown","source":"What we see here is that the confidence for each class if relatively low. It does appear that the model has greater confidence in Target=4 predictions which makes sense because of the class imbalance and the high prevalence of this label. <br />\n=> 우리가 알수 있는 것은 각 클래스의 confidence가 상대적으로 낮다는 것이다. 우리가 이전에도 알아차렸던 것 처럼 이 문제에는 class imbalance문제가 있기 때문에 Target=4 에 대한 confidence가 굉장히 높다는 것을 알 수 있다.\n\nAnother way to look at the information is as a violinplot. This shows the same information, with the number of observations related to the width of the plot. <br />\n=> 이를 보는 다른 방법은 바이올린 플롯을 이용하는 것이다."},{"metadata":{"trusted":true,"_uuid":"b29b5b008696c3ce6257d9f482fdd388d6315929"},"cell_type":"code","source":"plt.figure(figsize = (24, 12))\nsns.violinplot(x = 'Target', y = 'confidence', hue = 'fold', data = predictions);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f59f14add50e5621b0f23eb7bad990f3d38fc2a6"},"cell_type":"markdown","source":"Overall, these results show the issue with imbalanced class problems: our model cannot distinguish very well between the classes that are underrepresented. Later we'll look at predictions themselves and see where our model is \"confused\". For now, we can generate a submission file and submit it to the competition. <br />\n=> 이 또한 우리가 직면한 클래스 불일치 문제가 있다. 우리의 모델은 빈도수가 적게 등장하는 클래스들을 잘 구분하지 못한다. 추후에 prediction을 보고 우리의 모델이 문제가 있다는것을 살펴보자. 일단은 제출을 해보도록하자.\n\nWhen we actually make predictions for each household, we average the predictions from each of the folds. Therefore, we are essentially using multiple models since each one is trained on a slightly different fold of the data. The gradient boosting machine is already an ensemble machine learning model, and now we are using it almost as a meta-ensemble by averaging predictions from several gbms. <br />\n=> 우리가 각 가구별로 예측치를 만들때, 우리는 각 폴드로부터 나오는 예측치들을 평균했다. 그러므로 각각의 모델이 데이터의 다른 계층으로 훈련되었기 때문에 여러개의 모델을 사용하는 것은 필수적이다. 그라디언트 부스팅 모델은 이미 앙상블 모델이고, 우리는 가리디언트 부스팅 모델을 다른 그라디언트 부스팅 모델들로부터 예측치들을 평균한 meta-ensemble로써 활용한다.\n\nThis process is shown in the code below."},{"metadata":{"trusted":true,"_uuid":"b1703b2b971f2be1fa515783f5da2571a822e3da"},"cell_type":"code","source":"predictions.loc[predictions['idhogar'] == '000a08204',:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8d2c815c1cf82a687a7776e203e4d5cf2e6f9926"},"cell_type":"code","source":"predictions.groupby('idhogar', as_index = False).mean().head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d9c2ce138b1617b15dda5cc5ceb39e99b90496e"},"cell_type":"code","source":"# Average the predictions over folds\npredictions = predictions.groupby('idhogar', as_index = False).mean()\n\n# Find the class and associated probability\npredictions['Target'] = predictions[[1, 2, 3, 4]].idxmax(axis = 1)\npredictions['confidence'] = predictions[[1, 2, 3, 4]].max(axis = 1)\npredictions = predictions.drop(columns = ['fold'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"108114a8649ced5dd72a7add8c310ddcb30ad148"},"cell_type":"code","source":"predictions.loc[predictions['idhogar'] == '000a08204',:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a785b0b54c22b732878055b112132d29995ed59e"},"cell_type":"code","source":"# Plot the confidence by each target\nplt.figure(figsize = (10, 6))\nsns.boxplot(x = 'Target', y = 'confidence', data = predictions);\nplt.title('Confidence by Target');\n\nplt.figure(figsize = (10, 6))\nsns.violinplot(x = 'Target', y = 'confidence', data = predictions);\nplt.title('Confidence by Target');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9a40e95d73416374068fcab7af6f1048f957aae6"},"cell_type":"markdown","source":"We can have the function instead return the actual submission file. This takes the average predictions across the five folds, in effectm combining 5 different models, each one trained on a slghtly different subset of the data. <br />\n=> 우리는 실제 제출파일을 반환하는 함수를 가지고 있다. 이는 5개의 계층을 평균내었으며, 각각의 모델은 데이터의 다른 겹들로 훈련되어 약간씩 차이가 있다."},{"metadata":{"trusted":true,"_uuid":"c4ef9f74cb5d8a05a4b426a14eb44eb4bcc258f0"},"cell_type":"code","source":"%%capture\nsubmission, gbm_fi, valid_scores = model_gbm(train_set, train_labels, \n                                             test_set, test_ids, return_preds=False)\n\nsubmission.to_csv('gbm_baseline.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"368ff42946c50b12a21d20b8092b3e42a63f1b89"},"cell_type":"code","source":"_ = plot_feature_importances(gbm_fi, threshold=0.95)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"97267bb7774e0326fc6d61b657084842dcf39538"},"cell_type":"markdown","source":"The gbm seems to think the most important features are those derived from ages. The education variables also show up in the most important features. <br />\n=> 그라디언트 부스팅 모델은 나이를 가장 중요한 특징ㅇ라고 생각한 것같다. 교육 변수 또한 중요한 변수로 생각하고 있다.\n\n## Try Selected Features\nThe next step with the LightGBM is to try the features that were selected through recursive feature elimination.<br />\n다음으로는 우리가 Recursive Feature Elimination을 통해 얻은 피쳐들로 모델을 만들어 보자"},{"metadata":{"trusted":true,"_uuid":"3d77fa8971a6aeb54884faef979a4e0388b347fb"},"cell_type":"code","source":"submission, gbm_fi_selected, valid_scores_selected = model_gbm(train_selected, train_labels, \n                                                               test_selected, test_ids)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d64afc89ad2857d5bc0d459954aa91222f0f8684"},"cell_type":"code","source":"model_results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0f2e9fe57ce90e3668d9f7ed3ae68319c75acfc5"},"cell_type":"code","source":"model_results = model_results.append(pd.DataFrame({'model': [\"GBM\", \"GBM_SEL\"], \n                                                   'cv_mean': [valid_scores.mean(), valid_scores_selected.mean()],\n                                                   'cv_std':  [valid_scores.std(), valid_scores_selected.std()]}),\n                                                sort = True)\nmodel_results.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1be0a74e4b2f4135d69ab27cf0276b8aafd3886e"},"cell_type":"code","source":"model_results_ = model_results.set_index('model')\nmodel_results_['cv_mean'].plot.bar(color = 'orange', figsize = (8, 6),\n                                  yerr = list(model_results['cv_std']),\n                                 edgecolor = 'k', linewidth = 2)\nplt.title('Model F1 Score Results');\nplt.ylabel('Mean F1 Score (with error bar)');\n# model_results.reset_index(inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6b32c823713beda18c939bb8b63e0a3c0a190d5b"},"cell_type":"markdown","source":"The massive advantage of the gradient boosting machine is on display here. For a final step, let's try using 10-folds with both sets and add them to the plot. <br />"},{"metadata":{"trusted":true,"_uuid":"49a8cf4dee31ced12688b781e020f7f50fa42761"},"cell_type":"code","source":"submission, gbm_fi, valid_scores = model_gbm(train_set, train_labels, test_set, test_ids, \n                                             nfolds=10, return_preds=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b5594f4fd98697258f657e4d9663f0d1b8ff0608"},"cell_type":"code","source":"submission.to_csv('gbm_10fold.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2efe90351a06f5a335d7f3b26d6f15655ab7d18b"},"cell_type":"markdown","source":""},{"metadata":{"trusted":true,"_uuid":"f6638f15ef79235bf036f626dbad2b80a4b951f7"},"cell_type":"code","source":"submission, gbm_fi_selected, valid_scores_selected = model_gbm(train_selected, train_labels, test_selected, test_ids,\n                                                               nfolds=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f7b6f5a47b04e8bc68c003b7808e52c5410c715a"},"cell_type":"code","source":"submission.to_csv('gmb_10fold_selected.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"22702fde76e798b630039bbd8d0246e36be60d95"},"cell_type":"code","source":"model_results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5b51f8adcc7906f45e3fb4ba10ff850a79122bd2"},"cell_type":"code","source":"model_results = model_results.append(pd.DataFrame({'model': [\"GBM_10Fold\", \"GBM_10Fold_SEL\"], \n                                                   'cv_mean': [valid_scores.mean(), valid_scores_selected.mean()],\n                                                   'cv_std':  [valid_scores.std(), valid_scores_selected.std()]}),\n                                    sort = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6de81697bb7e3483f89744a535bade59cb9c49a4"},"cell_type":"code","source":"model_results_ = model_results.set_index('model')\nmodel_results_['cv_mean'].plot.bar(color = 'orange', figsize = (8, 6),\n                                  yerr = list(model_results['cv_std']),\n                                 edgecolor = 'k', linewidth = 2)\nplt.title('Model F1 Score Results');\nplt.ylabel('Mean F1 Score (with error bar)');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"74df11764b036a9033d35d9c4f0e0db350a9a357"},"cell_type":"code","source":"model_results","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2cddaeee51d8bb4210009e612974827fad515b26"},"cell_type":"markdown","source":"The best model seems to be the Gradient Boosting Machine trained with 10 folds on the selected features. This model has not yet been optimized, but we might be able to get a little more performance through optimization. <br />\n가장 최고의 모델은 선택된 특징들로 이용한 10겹 교차검증으로 훈련된 그라디언트 부스팅 모델인것 같습니다. [번역자는 그림에서 보다시피 다르지만 해석은 같게 하겠습니다.] 이 모델은 아직 최적화가 되어있지 않습니다. 그러나 최적화를 통해서 약간의 성능향상을 할 수 있을 것입니다."},{"metadata":{"trusted":true,"_uuid":"573fb6523926ee67303738456a62494ab3e4857b"},"cell_type":"code","source":"print(f\"There are {gbm_fi_selected[gbm_fi_selected['importance'] == 0].shape[0]} features with no importance.\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e61c1e67dd830e51a4f20c674554862bddfa2591"},"cell_type":"markdown","source":"Well that should make us feel better! All of the features we are using have some importance to the Gradient Boosting Machine. It might be a good idea to go back and retry feature selection but with the GBM since that is the model we are using. <br />\n=> 그라디언트 부스팅 머신에 모든 피쳐들이 사용되고 있다는 것은 좋은 징조인 것 같습니다! 다시 돌아가서 feature selection을 재시도 하는 것은 좋은 생각인 것 같지만 우리가 사용하고 있는 GBM모델을 이용해야 할 것입니다.\n\nThe next step to take is Model Optimization, the process of getting the most from a machine learning model. <br />\n다음 단계는 머신러닝모델에서 가장 중요한 모델 최적화 단계입니다"},{"metadata":{"_uuid":"614598a57a2df0fd59b14e527d1417ff371987f0"},"cell_type":"markdown","source":"# Model Optimization\nModel optimization is the process of extracting the best performance from a machine learning model by tuning the hyperparameters through cross-validation. This is necessary because the best model hyperparameters are different for every dataset. <br />\n=> 모델 최적화는 머신러닝모델에서 하이퍼 파라미터를 교차검증을 통해 튜닝하여 최고의 성능을 뽑아내기 위한 과정입니다. 이는 최고의 모델 하이퍼파라미터가 데이터 세트들마다 다르기 때문에 필수적입니다.\n\nIn this notebook, we won't get too far into model tuning, but there are multiple options:\n여기서는 모델 튜닝에 대해서 그렇게 깊게 다루지는 않겠지만 여러가지 옵션들이 있습니다.\n\n1. Manual\n2. Grid Search\n3. Random Search\n4. Automated Optimization\n\nWe will stick to 4. because it is generally the most efficient method and can easily be implemented in a number of libraries, including Hyperopt, which uses a modified version of Bayesian Optimization with the Tree Parzen Estimator.<br />\n=> 우리는 네번째에 대해서 알아볼 것이며, 이는 가장 효율적인 방법이면서 많은 라이브러리들로 쉽게 실행가능하기 때문입니다. 우리는 Hyperopt를 사용할 것인데 이는 Tree Parzen Estimator를 활용한 Bayesian Optimization의 수정된 버전을 사용합니다.\n\n## Model Tuning with Hyperopt\nBayesian optimization requires 4 parts: <br />\n=> Bayesian optimization은 네가지 파트로 구성됩니다.\n\n* Objective function: what we want to maximize (or minimize) <br />\n최대화 최소화 시키려는 목적 함수 <br />\n* Domain space: region over which to search <br />\n찾으려는 region<br />\n* Algorithm for choosing next hyperparameters: uses past results to suggest next values <br />\n다음 파라미터를 고르는 알고리즘: 두번 째의 결과값을 값으로 사용함<br />\n* Results history: saves the past results <br />\n결과 history: 과거 결과들일 기록함 <br />\n\nI've written previously about using Hyperopt, so here we'll stick to the implementation.<br />\n우리는 이를 이용해서 실행해 봅시다 <br />\n"},{"metadata":{"trusted":true,"_uuid":"6315e6b9fe295afdb3ee96ce7d204e7106b87b61"},"cell_type":"code","source":"from hyperopt import hp, tpe, Trials, fmin, STATUS_OK\nfrom hyperopt.pyll.stochastic import sample","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c5bb76b49240adaf1b6968eb8c372451a923d9f5"},"cell_type":"code","source":"import csv\nimport ast\nfrom timeit import default_timer as timer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f5c2d3bfe1014269a9d91f6b95fe07a78c96dd8f"},"cell_type":"code","source":"def objective(hyperparameters,nfolds=5):\n    \n    global ITERATION\n    ITERATION += 1\n    \n    subsample = hyperparameters['boosting_type'].get('subsample',1.0)\n    subsample_freq = hyperparameters['boosting_type'].get('subsample_freq',0)\n    \n    boosting_type = hyperparameters['boosting_type']['boosting_type']\n    \n    if boosting_type == 'dart':\n        hyperparameters['drop_rate'] = hyperparameters['boosting_type']['drop_rate']\n        \n    hyperparameters['subsample'] = subsample\n    hyperparameters['subsample_freq'] = subsample_freq\n    hyperparameters['boosting_type'] = boosting_type\n    \n    if not hyperparameters['limit_max_depth']:\n        hyperparameters['max_depth'] = -1\n        \n    for parameter_name in ['max_depth','num_leaves','subsample_for_bin','min_child_samples','subsample_freq']:\n        hyperparameters[parameter_name] = int(hyperparameters[parameter_name])\n        \n    if 'n_estimators' in hyperparameters:\n        del hyperparameters['n_estimators']\n    \n    strkfold = StratifiedKFold(n_splits=nfolds, shuffle=True)\n    \n    features = np.array(train_selected)\n    # ????\n    labels = np.array(train_labels).reshape((-1))\n    \n    valid_scores = []\n    best_estimators = []\n    run_times =[]\n    \n    model = lgb.LGBMClassifier(**hyperparameters,class_weight='balanced',n_jobs=-1,metric='None',n_estimators=10000)\n    \n    for i,(train_indices,valid_indices) in enumerate(strkfold.split(features,labels)):\n        \n        X_train,X_valid = features[train_indices],features[valid_indices]\n        y_train,y_valid = labels[train_indices],labels[valid_indices]\n        \n        start = timer()\n        \n        model.fit(X_train,y_train,early_stopping_rounds=100,eval_metric=macro_f1_score,eval_set=[(X_train,y_train),(X_valid,y_valid)],eval_names=['train','valid'],verbose=400)\n        \n        end = timer()\n        \n        valid_scores.append(model.best_score_['valid']['macro_f1'])\n        best_estimators.append(model.best_iteration_)\n        \n        run_times.append(end-start)\n        \n    score = np.mean(valid_scores)\n    score_std = np.std(valid_scores)\n    loss = 1 - score\n    \n    run_time = np.mean(run_times)\n    run_time_std = np.std(run_times)\n    \n    estimators = int(np.mean(best_estimators))\n    hyperparameters['n_estimators'] = estimators\n    \n    of_connection = open(OUT_FILE,'a')\n    writer = csv.writer(of_connection)\n    writer.writerow([loss,hyperparameters,ITERATION,run_time,score,score_std])\n    of_connection.close()\n    \n    if ITERATION % PROGRESS == 0:\n        display(f'Iteration: {ITERATION}, Current Score: {round(score, 4)}.')\n    \n    return {'loss': loss, 'hyperparameters': hyperparameters, 'iteration': ITERATION,\n            'time': run_time, 'time_std': run_time_std, 'status': STATUS_OK, \n            'score': score, 'score_std': score_std}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"52203576bb36af43d8143d6afceabb6ac38047a6"},"cell_type":"markdown","source":"### 2. Search Space\nThe domain is the entire range of values over which we want to search. The only difficult part is the subsample ratio which must be set to 1.0 if the boosting_type=\"goss\". Feel free to play around with the values here. <br />\n=> 도메인은 우리가 찾고자하는 값들의 전체 범위입니다. 서브샘플 비율은 어려운 부분이며, 만약 boosting_type이 'goss'이면 무조건 1.0으로 세팅해야합니다. 자유롭게 도메인들을 둘러보도록 합시다.\n\n> https://github.com/hyperopt/hyperopt/wiki/FMin [hyperopt에 대하여]"},{"metadata":{"trusted":true,"_uuid":"3e1558fc2879d93b41a0976e83b15381d8626fba"},"cell_type":"code","source":"# Define the search space\nspace = {\n    'boosting_type': hp.choice('boosting_type', \n                              [{'boosting_type': 'gbdt', \n                                'subsample': hp.uniform('gdbt_subsample', 0.5, 1),\n                                'subsample_freq': hp.quniform('gbdt_subsample_freq', 1, 10, 1)}, \n                               {'boosting_type': 'dart', \n                                 'subsample': hp.uniform('dart_subsample', 0.5, 1),\n                                 'subsample_freq': hp.quniform('dart_subsample_freq', 1, 10, 1),\n                                 'drop_rate': hp.uniform('dart_drop_rate', 0.1, 0.5)},\n                                {'boosting_type': 'goss',\n                                 'subsample': 1.0,\n                                 'subsample_freq': 0}]),\n    'limit_max_depth': hp.choice('limit_max_depth', [True, False]),\n    'max_depth': hp.quniform('max_depth', 1, 40, 1),\n    'num_leaves': hp.quniform('num_leaves', 3, 50, 1),\n    'learning_rate': hp.loguniform('learning_rate', \n                                   np.log(0.025), \n                                   np.log(0.25)),\n    'subsample_for_bin': hp.quniform('subsample_for_bin', 2000, 100000, 2000),\n    'min_child_samples': hp.quniform('min_child_samples', 5, 80, 5),\n    'reg_alpha': hp.uniform('reg_alpha', 0.0, 1.0),\n    'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.0),\n    'colsample_bytree': hp.uniform('colsample_by_tree', 0.5, 1.0)\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cfe2857785361dee5a90431e7aa735fe4d408b80"},"cell_type":"code","source":"sample(space)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a2c623b5f149e241353126453ebcba7209e83761"},"cell_type":"markdown","source":"### 3. Algorithm\nThe algorithm for choosing the next values is the Tree Parzen Estimator which uses Bayes rule for constructing a surrogate model of the objective function. Instead of maximizing the objective function, the algorithm maximizes the Expected Improvement (EI) of the surrogate model. <br />\n=> 다음 값을 고르는 알고리즘은 목적함수의 임시 모델을 만들기위한 베이즈이론을 따르는 Tree Parzen Estimator이다. 목적함수를 최대화 시키는 것 대신에 알고리즘은 임시 모델의 Expected Improvement를 최대화시킨다. \n"},{"metadata":{"trusted":true,"_uuid":"a3b22e57bde913a9f17c229925b316d0d4f9b5e2"},"cell_type":"code","source":"algo = tpe.suggest","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f47cebead962dbf3cc90bb20148bb8f0737fb897"},"cell_type":"markdown","source":"### 4. Results History\nWe'll use two different methods for recording results: <br />\n결과를 기록하는데 있어서 두가지 방법을 사용할 것이다.\n\n1. Trials object: stores everything returned from the objective function <br />\n목적함수로부터 반환되는 모든 것을 저장 <br />\n2. Write to a csv file on every iteration <br />\n모든 반복들을 csv파일로 쓰기 <br />\n\nI like using multiple methods for tracking progress because it means redundancy. One way may fail, but hopefully both will not! The csv file can be used to monitor the method while it is running and the Trials object can be saved and then reloaded to resume optimization. <br />\n=> 나는 결과를 추적하는데 여러방법을 사용하는 것을 좋아하는데 왜냐하면 결과 추적은 중복의 수단이 되기 때문이다. 하나의 방법이 실패하면 다른 것은 실패하지 않을 것이다. csv파일은 실행되는 것을 관찰하는 방법이며, 이렇게 수행들의 객체는 저장되고 최적화를 재개하기 위해 다시 로드된다."},{"metadata":{"trusted":true,"_uuid":"bd6d6b825a5c72275f4b9f03dcedb814bbe5035b"},"cell_type":"code","source":"trials = Trials()\n\nOUT_FILE = 'optimization.csv'\nof_connection = open(OUT_FILE,'w')\nwriter = csv.writer(of_connection)\n\nMAX_EVALS = 100\nPROGRESS = 10\nN_FOLDS = 5\nITERATION = 0\n\n# Write column names\nheaders = ['loss', 'hyperparameters', 'iteration', 'runtime', 'score', 'std']\nwriter.writerow(headers)\nof_connection.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f2c6a018b3a98ef4bbf6425107eebd554c87ebb7"},"cell_type":"code","source":"display(\"Running Optimization for {} Trials.\".format(MAX_EVALS))\n\n# Run optimization\nbest = fmin(fn = objective, space = space, algo = tpe.suggest, trials = trials,\n            max_evals = MAX_EVALS)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"94b4c9082bbf4879733fcbaf58ee6112c792bc18"},"cell_type":"markdown","source":"To resume training, we can pass in the same trials object and increase the max number of iterations. For later use, the trials can be saved as json."},{"metadata":{"trusted":true,"_uuid":"ad16e42068a604fe39bc4d5e7f776d643ca6bca9"},"cell_type":"code","source":"import json\n\n# Save the trial results\nwith open('trials.json', 'w') as f:\n    f.write(json.dumps(str(trials)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4f0106294b6aa3db05f995fba473f41f031b4a3f"},"cell_type":"markdown","source":"Using Optimized Model\nOnce the optimization procedure has finished, we can use the best results for modeling."},{"metadata":{"trusted":true,"_uuid":"a5b38a8c796cfd0275a6160871a97f677e3fc682"},"cell_type":"code","source":"results = pd.read_csv(OUT_FILE).sort_values('loss', ascending = True).reset_index()\nresults.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b05fd3bbc0895858172f6d0210ba5d543eef4517"},"cell_type":"code","source":"plt.figure(figsize = (8, 6))\nsns.regplot('iteration', 'score', data = results);\nplt.title(\"Optimization Scores\");\nplt.xticks(list(range(1, results['iteration'].max() + 1, 3)));","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f1cfad7d814b2c3801825be7e2eb28ac2f325d60"},"cell_type":"code","source":"best_hyp = ast.literal_eval(results.loc[0, 'hyperparameters'])\nbest_hyp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d330a38838ac3754cddece27d3931efbf256b661"},"cell_type":"code","source":"submission, gbm_fi, valid_scores = model_gbm(train_selected, train_labels, \n                                             test_selected, test_ids, \n                                             nfolds = 10, return_preds=False)\n\nmodel_results = model_results.append(pd.DataFrame({'model': [\"GBM_OPT_10Fold_SEL\"], \n                                                   'cv_mean': [valid_scores.mean()],\n                                                   'cv_std':  [valid_scores.std()]}),\n                                    sort = True).sort_values('cv_mean', ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9a277eae3a454cadb26292b3731ceed720b95817"},"cell_type":"code","source":"submission, gbm_fi, valid_scores = model_gbm(train_set, train_labels, \n                                             test_set, test_ids, \n                                             nfolds = 10, return_preds=False)\n\nmodel_results = model_results.append(pd.DataFrame({'model': [\"GBM_OPT_10Fold\"], \n                                                   'cv_mean': [valid_scores.mean()],\n                                                   'cv_std':  [valid_scores.std()]}),\n                                    sort = True).sort_values('cv_mean', ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4eab93cb1308729ab63430979b7e6a768331a26f"},"cell_type":"code","source":"model_results.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bf082eb231c96b212e2de02c0932e3226c40acbc"},"cell_type":"code","source":"submission, gbm_fi, valid_scores = model_gbm(train_selected, train_labels, \n                                             test_selected, test_ids, \n                                             nfolds = 10, return_preds=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"68a147f11546d4ad8cfa9bae31b85c1729b187ca"},"cell_type":"code","source":"submission.to_csv('gbm_opt_10fold_selected.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7c27b4fbe31c3121d8862697736bfb9eaca88300"},"cell_type":"markdown","source":"At this point, to improve our performance, we can continue with the optimization, try more feature engineering, stack or ensemble additional models, or look at more experimental methods such as dimension reduction or oversampling. However, the scores we are getting are fairly good and I'm going to move on to a crucial part of machine learning: investigating the predictions to see where our model is wrong. <br />\n우리의 성능을 높이기 위해 최적화를 계속할 수 있고, 다른 feature engineering을 할수도 있고 추가적인 스태킹 또는 앙상블 모델이나 또는 차원축소 또는 오버샘플링과 같은 실험적인 방법을 할수 있습니다. 그러나 점수는 생각보다도 좋으며, 그래서 머신러닝에 있어서 좀더 중요한 부분인, 예측치에서 우리의 모델이 잘못된 부분을 조사해보려고 한다.<br />\n\nWhile Kaggle is great for presenting realistic datasets, the methods needed to get to the very top of the leaderboard are not generally used in the real-world. Past some level of performance, accuracy takes a back seat to interpretability. People want to know why a model makes the predictions it does, much as they would want a human to be able to explain her decisions. We could work on squeezing some more performance from our model, but right now, our energy is better spent investigating our model. <br />\n캐글은 실제 데이터셋을 제공하는 좋은 곳이지만, 실제로 높은 점수를 차지하는 사람들이 하는 방법들은 실제로 사용되지 않는다. 일정 수준의 성능에서, 정확도는 의미를해석하는 것보다 뒷전에 있다. 사람들은 왜 모델이 예측을 그렇게 했는지 알고 싶어한다. 우리는 모델의 더좋은 성능을 위해 짜낼 수 있지만, 그것 보다는 우리의 에너지를 우리의 모델을 조사하는데 초점을 맞춰보도록 하자."},{"metadata":{"trusted":true,"_uuid":"c5199b11dcac299cbac42c081f435a0a62ac913f"},"cell_type":"code","source":"_ = plot_feature_importances(gbm_fi)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4585311eb36c431334709d6e85d90389bc04ad19"},"cell_type":"markdown","source":"## Investigate Predictions\nAs a first attempt at looking into our model, we can visualize the distribution of predicted labels on the test data. We would expect these to show the same distribution as on the training data. Since we are concerned with household predictions, we'll look at only the predictions for each house and compare with that in the training data. <br />\n=>우리의 모델을 보는 첫번째 시도로, 우리는 테스트데이터에 대한 예측된 라벨값들의 분포를 시각화할 수 있습니다. 우리는 이 분포가 훈련세트의 라벨분포와 동일하기를 바라고 있습니다. 우리가 가구예측을 해왔던것처럼, 우리는 각각의 가구의 데이터를 훈련세트의 라벨과 비교할 것입니다.\n\nThe following histrograms are normalize meaning that they show the relative frequency instead of the absolute counts. This is necessary because the raw counts differ in the training and testing data. <br />\n=> 다음의 히스토그램은 절대적인 카운트가 아닌 상대적인 빈도를 나타내고 있습니다. 왜냐하면 실제적인 카운트는 테스트셋과 훈련셋이 다르기 때문입니다."},{"metadata":{"trusted":true,"_uuid":"f5678944e9ba39098492962ef0b7642beb7aed88"},"cell_type":"code","source":"preds = submission_base.merge(submission,on='Id',how='left')\npreds = pd.DataFrame(preds.groupby('idhogar')['Target'].mean())\n\nfig,axes = plt.subplots(1,2,sharey=True,figsize=[12,6])\nheads['Target'].sort_index().plot.hist(normed=True,edgecolor=r'k',linewidth=2,ax=axes[0])\naxes[0].set_xticks([1,2,3,4])\naxes[0].set_xticklabels(poverty_mapping.values(),rotation=60)\naxes[0].set_title('Train Label Distribution')\n\n\npreds['Target'].sort_index().plot.hist(normed = True, \n                                       edgecolor = 'k',\n                                       linewidth = 2,\n                                       ax = axes[1])\naxes[1].set_xticks([1, 2, 3, 4]);\naxes[1].set_xticklabels(poverty_mapping.values(), rotation = 60)\nplt.subplots_adjust()\nplt.title('Predicted Label Distribution');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9d4786cb1538eb0d07c8a19fc23216abf71ca866"},"cell_type":"code","source":"heads['Target'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"89e1133b8608d45a3e3a545410f20f1e4aff57ff"},"cell_type":"code","source":"preds['Target'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9d835c237953a48f5c02efdbf935da7456d1f414"},"cell_type":"markdown","source":"The predicted distribution looks close to the training distribution although there are some differences. Depending on the run of the notebook, the results you see may change, but for this edition, the 4s are underrepresented in the predictions and the 3s are overrepresented.\n\nOne potentially method for dealing with imbalanced classification problems is oversampling the minority class, which is easy to do in Python using the imbalanced learn library. We won't explore that option here."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}