{"cells":[{"metadata":{"trusted":true,"_uuid":"7a6531cb444f216a7ced01c1b7904f65b2ef4a62"},"cell_type":"code","source":"%matplotlib inline  \nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as py\n\n\n\npy.rcParams['figure.figsize'] = [10, 8]\npy.style.use('ggplot')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3f5a44c38529787000f9aab363a56690946435b0"},"cell_type":"markdown","source":"opis zmiennych: https://www.kaggle.com/c/costa-rican-household-poverty-prediction\n świetny notebook: https://www.kaggle.com/willkoehrsen/a-complete-introduction-and-walkthrough/notebook - wiele pomysłów jest żywo z niego przekopiowanych!"},{"metadata":{"trusted":true,"_uuid":"62d6543f5778d4e8963ad44498d7cc59edf9a849"},"cell_type":"code","source":"#początki\nimport os\nprint(os.listdir(\"../input\"))\ndata_train = pd.read_csv(\"../input/costa-rican-household-poverty-prediction/train.csv\")\ndata_test = pd.read_csv(\"../input/costa-rican-household-poverty-prediction/test.csv\")\n#data_train = pd.read_csv(\"../input/train.csv\")\n#data_test = pd.read_csv(\"../input/test.csv\")\n#funkcja ex_desc wyświetla dla danej kolumny jej opis wyciągnięty ze strony kaggla, przydaje się tych zmiennych jest od cholery\ndesc = pd.read_csv(\"../input/descriptions/descriptions.csv\", sep = \",\", header = None)\ndesc.columns = ['variable', 'description']\n\ndef ex_desc(var_name):\n    output = str(desc.loc[desc['variable'] == var_name]['description']).split('\\n')[0][3:]\n    return output\n\nlabels = {1 : \"extreme poverty\", 2 : \"moderate poverty\", 3 : \"vulnerable households\", 4 : \"non vulnerable households\"}\nid_cols = ['Id', 'idhogar', 'parentesco1', 'Target']\nid_colst = ['Id', 'idhogar', 'parentesco1']\n\nprint(\"rozmiar tabeli train {} wierszy {} i kolumny\".format(data_train.shape[0], data_train.shape[1]))\nprint(\"rozmiar tabeli test {} wierszy {} i kolumny\".format(data_test.shape[0], data_test.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5be6820b876faa8e7e4281518ff63b6dcade1b3f"},"cell_type":"markdown","source":"# naprawa danych - w obrębie jednego gospodarstwa występują różne wartości Target"},{"metadata":{"trusted":true,"_uuid":"009364ed97ceaf5e5642c4610a5a003abc8641ac"},"cell_type":"code","source":"to_correct = data_train.groupby('idhogar')['Target'].apply(lambda x: x.nunique() == 1)\nto_correct = to_correct[to_correct == False]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3421709bde52022dcefe43d16d80d17b43653161"},"cell_type":"code","source":"houses_no_head = 0\nhouses_wrong_labels = 0\nfor idx in to_correct.index:\n    all_household_members = data_train[data_train.idhogar == idx]\n    head_label = all_household_members[all_household_members['parentesco1'] == 1][['Target']]\n    if head_label is not None:\n        data_train[data_train.idhogar == idx]['Target'] == head_label\n        houses_wrong_labels += 1\n    else:\n        houses_no_head += 1\n        \nprint(\"Domostwa ze złą etykietą {} domostwa z brakiem głowy {} - to nie jest błąd \".format(houses_wrong_labels, houses_no_head))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"634d073ef761e2fb66297a327a90f06cfbcb373d"},"cell_type":"code","source":"data_train.groupby('parentesco1').size()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3b6fea4b8077377a70b6b2e96277cf0187939b19"},"cell_type":"markdown","source":"czyli ok ok 3 osoby przypadają na gospodarstwo jak to wygląda dokładniej na histogramie"},{"metadata":{"trusted":true,"_uuid":"16d92195c18f98a6a89e43258dae0d4a8a4950fa"},"cell_type":"code","source":"data_train['tamviv'].plot.hist(normed = 1, bins = list(range(0, 20)))","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"69cff5dae7e388adc56ffa2814b72d48ae729365"},"cell_type":"markdown","source":"# inżynieria cech"},{"metadata":{"trusted":true,"_uuid":"8ab62e42d5bdcd2e01d15a82f9c9c62914095788"},"cell_type":"code","source":"data_object = data_train.select_dtypes(include = 'object')\nfor col in data_object.columns:\n    print(col)\n    print(np.unique(data_object[col]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5645cdf9e04949175128f96a058c3420e695f50d"},"cell_type":"markdown","source":"pierwsze dwie cechy są identyfikatorami więc nic nie znaczą, pozostałe dotyczą edukacji zmienie ich tych aby można było na nich modelować. \nPrzy okazji trzeba zamienić wartoścci yes i no na 1 0."},{"metadata":{"trusted":true,"_uuid":"27542d3ff6b4bb81fd58be64e436b21ed44695c3"},"cell_type":"code","source":"mapping={\"yes\" : 1, \"no\": 0}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"14793c0e0695c10f9237a541fa3aab3c6167117a"},"cell_type":"code","source":"data_train['dependency'] = data_train['dependency'].replace(mapping).astype(np.float64)\ndata_train['edjefa'] = data_train['edjefa'].replace(mapping).astype(np.float64)\ndata_train['edjefe'] = data_train['edjefe'].replace(mapping).astype(np.float64)\ndata_test['dependency'] = data_test['dependency'].replace(mapping).astype(np.float64)\ndata_test['edjefa'] = data_test['edjefa'].replace(mapping).astype(np.float64)\ndata_test['edjefe'] = data_test['edjefe'].replace(mapping).astype(np.float64)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d361a8d21d01c62ef3724999480550e8718d6931"},"cell_type":"markdown","source":"## obsługa missingów"},{"metadata":{"trusted":true,"_uuid":"780d90bdfaadbe5e032486808708e634a2bd997e"},"cell_type":"code","source":"missings = data_train.isnull().sum()\nmissings = missings.loc[missings != 0]\nfor i in range(0, len(missings.index)):\n    print(missings.index[i], missings[i], ex_desc(missings.index[i]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7128bef48c8800fcbb93a8a365ddb2ccb5aa28b1"},"cell_type":"markdown","source":"1. v2a1, Monthly rent payment\n2. v18q1, number of tablets household owns\n3. rez_esc, Years behind in school\n4. meaneduc,average years of education for adults (18+)"},{"metadata":{"trusted":true,"_uuid":"e76b15e49068c4ce4e3ee74171b2706670326c2a"},"cell_type":"code","source":"# If individual is over 19 or younger than 7 and missing years behind, set it to 0\ndata_train.loc[((data_train['age'] > 19) | (data_train['age'] < 7)) & (data_train['rez_esc'].isnull()), 'rez_esc'] = 0\ndata_test.loc[((data_test['age'] > 19) | (data_test['age'] < 7)) & (data_test['rez_esc'].isnull()), 'rez_esc'] = 0\n\n# Add a flag for those between 7 and 19 with a missing value\ndata_train['rez_esc-missing'] = data_train['rez_esc'].isnull()\ndata_test['rez_esc-missing'] = data_test['rez_esc'].isnull()\n\ndata_train.loc[data_train['rez_esc'] > 5, 'rez_esc'] = 5\ndata_test.loc[data_test['rez_esc'] > 5, 'rez_esc'] = 5\n\ndata_train['rez_esc'] = data_train['rez_esc'].fillna(0)\ndata_test['rez_esc'] = data_test['rez_esc'].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5c25c2d7bd1980cccf479b791aee747183a08399"},"cell_type":"code","source":"# Fill in households that own the house with 0 rent payment\ndata_train.loc[(data_train['tipovivi1'] == 1), 'v2a1'] = 0\n\n# Create missing rent payment column\ndata_train['v2a1-missing'] = data_train['v2a1'].isnull()\ndata_train['v2a1-missing'].value_counts()\n\ndata_test['v2a1-missing'] = data_test['v2a1'].isnull()\ndata_test['v2a1-missing'].value_counts()\n\n# Fill in households that own the house with 0 rent payment\ndata_test.loc[(data_test['tipovivi1'] == 1), 'v2a1'] = 0\n\n\n# Create missing rent payment column\ndata_test['v2a1-missing'] = data_test['v2a1'].isnull()\ndata_test['v2a1-missing'].value_counts()\ndata_train['v2a1-missing'] = data_train['v2a1'].isnull()\ndata_train['v2a1-missing'].value_counts()\n# Fill rest of them with zeros\ndata_train['v2a1'] = data_train['v2a1'].fillna(0)\ndata_test['v2a1'] = data_test['v2a1'].fillna(0)\n\n\n\ndata_train['v18q1'] = data_train['v18q1'].fillna(0)\ndata_test['v18q1'] = data_test['v18q1'].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"29ca878aa7a4e16046015d4470d0c6432e351b14"},"cell_type":"code","source":"# blind shot\ndata_train['meaneduc'] = data_train['meaneduc'].fillna(0)\ndata_test['meaneduc'] = data_test['meaneduc'].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2c5c0fab6d75b0d24e645dc9b5be0ea50afa508d"},"cell_type":"markdown","source":"\n\n## usunięcie cech ^2"},{"metadata":{"_uuid":"37c82fbebe0ccc0402aab8a4dfba0a015307a71d"},"cell_type":"markdown","source":"przy regresji - która jest metodą liniową takie cechy mogłby pomóc wyłapać zależności nieliniowe ale ja nie zamierzam regresji wykorzystywać ;)"},{"metadata":{"trusted":true,"_uuid":"b00cad86c8793858b61ef7ac842a1a2afe2328c5"},"cell_type":"code","source":"sqr_ = ['SQBescolari', 'SQBage', 'SQBhogar_total', 'SQBedjefe', 'SQBhogar_nin', 'SQBovercrowding', 'SQBdependency', 'SQBmeaned', 'agesq']\nprint(sqr_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8dba88ed776c1ed8bc948e980a71bcba18465dea"},"cell_type":"code","source":"data_train = data_train.drop(columns = sqr_)\ndata_test = data_test.drop(columns = sqr_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4a8aaa2f2144ee30021232997eb1cc2fa58625f7"},"cell_type":"code","source":"data_train['sex'] = data_train['female']\ndata_train = data_train.drop(columns = ['female', 'male'])\ndata_test['sex'] = data_test['female']\ndata_test = data_test.drop(columns = ['female', 'male'])\nnp.unique(data_test['sex'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6bc9166a64acace2942a3831022a8fd2a81716f8"},"cell_type":"markdown","source":"## stan budynku  - redukcja cech + nowa"},{"metadata":{"_uuid":"1f10255a6b8bc47a9e905706d9d7b5aa1f90304c"},"cell_type":"markdown","source":"zmienne epared* etacho* eviv* opisują stan scian/sufitu/podłogi można je połączyć w jedną cechę także stan jest opisywany przez jedną z trzech wartosci bad - regular - good"},{"metadata":{"trusted":true,"_uuid":"3f5e6e71f7314f1d90d5f88adb904c9a12f58c6f"},"cell_type":"code","source":"data_train['walls'] = np.argmax(np.array(data_train[['epared1', 'epared2', 'epared3']]),axis = 1)\ndata_train['roof'] = np.argmax(np.array(data_train[['etecho1', 'etecho2', 'etecho3']]),axis = 1)\ndata_train['floor'] = np.argmax(np.array(data_train[['eviv1', 'eviv2', 'eviv3']]),axis = 1)\n\ndata_test['walls'] = np.argmax(np.array(data_test[['epared1', 'epared2', 'epared3']]),axis = 1)\ndata_test['roof'] = np.argmax(np.array(data_test[['etecho1', 'etecho2', 'etecho3']]),axis = 1)\ndata_test['floor'] = np.argmax(np.array(data_test[['eviv1', 'eviv2', 'eviv3']]),axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"86f52a89a9fb050c2833a0834882530b399c9cff"},"cell_type":"code","source":"to_drop = ['epared1', 'epared2', 'epared3', 'etecho1','etecho2', 'etecho3',  'eviv1', 'eviv2', 'eviv3']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"492634b61d4eba5f80d33f88f96450cc45f3cbb6"},"cell_type":"code","source":"data_train['walls+roof+floor'] = data_train['walls'] + data_train['roof'] + data_train['floor']\ndata_test['walls+roof+floor'] = data_test['walls'] + data_test['roof'] + data_test['floor']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"30a66d022e16c288cd4d9bf292c1223fcb014a5b"},"cell_type":"code","source":"data_train = data_train.drop(columns=to_drop)\ndata_test = data_test.drop(columns=to_drop)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9c81582465832e557ffffd9b3a05766091bb5512"},"cell_type":"markdown","source":"## jaki jest % udział rodziny w całkowitej liczbie osób mieszkających w domu"},{"metadata":{"trusted":true,"_uuid":"377c174eb4b472297c5be2d0bc3a6dafdfd7dc29"},"cell_type":"code","source":"data_train['hhsize_diff'] = (data_train['tamviv'] - data_train['hhsize'])/data_train['tamviv']\ndata_test['hhsize_diff'] = (data_test['tamviv'] - data_test['hhsize'])/data_test['tamviv']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0fc74d885819fd3135a3c0bc663cf00ab7539863"},"cell_type":"code","source":"data_train['has_dis'] = 0\nl = 0\nfor ll in np.unique(data_train[['idhogar']]):\n    vals = data_train.loc[data_train.idhogar == ll,'dis'].values\n    if [1] in vals:\n        l += 1\n        data_train.loc[data_train['idhogar'] == ll,'has_dis'] = np.sum(vals)/len(vals)\n        #dump_data['idhogar'] = np.where(dump_data.idhogar.values == ll, 1, dump_data.has_dis.values)\n        \nprint(\"%d hh has dis\" % l)\n\ndata_test['has_dis'] = 0\nl = 0\nfor ll in np.unique(data_test[['idhogar']]):\n    vals = data_test.loc[data_test.idhogar == ll,'dis'].values\n    if [1] in vals:\n        l += 1\n        data_test.loc[data_test['idhogar'] == ll,'has_dis'] = np.sum(vals)/len(vals)\n        #dump_data['idhogar'] = np.where(dump_data.idhogar.values == ll, 1, dump_data.has_dis.values)\n        \nprint(\"%d hh has dis\" % l)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e0d0e0316694d7225f6b3d0c4c49b3ed0fea55a5"},"cell_type":"code","source":"#columns_to_drop = ['pca_%d'%d for d in range(1, 6)] + ['ica_%d'%d for d in range(1,6)]\n#data_train = data_train.drop(columns = columns_to_drop)\n#data_test = data_test.drop(columns = columns_to_drop)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ab43dd517101ed2a565bb8fa023f4dd09d287ee9"},"cell_type":"code","source":"'''\nfrom sklearn.decomposition import FastICA, PCA\n\nnb_pca = 6\nica = FastICA(nb_pca)\npca = PCA()\npca.fit(data_train.drop(columns=id_cols))\nica_train = ica.fit_transform(data_train.drop(columns=id_cols))\npca_train = pca.fit_transform(data_train.drop(columns=id_cols))\n\nfor i in range(0, nb_pca):\n    ica_lab = 'ica_%d' % int(i+1)\n    pca_lab = 'pca_%d' % int(i+1)\n    data_train[ica_lab] = ica_train[:,i]\n    data_train[pca_lab] = pca_train[:,i]\n \npca = PCA()\npca.fit(data_test.drop(columns=id_colst))\npca_test = pca.fit_transform(data_test.drop(columns=id_colst))\nica_test = ica.fit_transform(data_test.drop(columns=id_colst))\n\nfor i in range(0, nb_pca):\n    ica_lab = 'ica_%d' % int(i+1)\n    pca_lab = 'pca_%d' % int(i+1)\n    data_test[ica_lab] = ica_test[:,i]\n    data_test[pca_lab] = pca_test[:,i]\n    '''","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b7350b2c652722f419d4f1027e2a418604e0ab87"},"cell_type":"markdown","source":"pomysły na cechy które przyszły mi do głowy"},{"metadata":{"trusted":true,"_uuid":"41baad0057583105df48f71550bfb67e4e4d0ad2"},"cell_type":"code","source":"data_train['sex_diff'] = 0\ndata_train['adults_mean_age'] = 0\ndata_train['mean_kid_age'] = 0\ndata_train['lone_old_man'] = 0\ndata_train['non_family_members'] = 0\n\nfor ll in np.unique(data_train[['idhogar']]):\n    vals = data_train.loc[data_train.idhogar == ll,'age'].values\n    vals_sex = data_train.loc[data_train.idhogar == ll,'sex'].values\n    vals_nf = data_train.loc[data_train.idhogar == ll,'parentesco12'].values\n    nf_sum = np.sum(vals_nf)\n    nf_len = len(vals_nf)\n    if len(vals[vals>18]) !=0:\n        ad_age_adult = vals[vals>18].mean(dtype='int')\n    if len(vals[vals<18]) !=0:\n        ad_age_kids = vals[vals<18].mean(dtype='int')\n    \n    if len(vals[vals >= 60]) == 1:\n        data_train.loc[data_train['idhogar'] == ll,'lone_old_man'] = 1\n        \n    if nf_sum != 0 :\n        data_train.loc[data_train['idhogar'] == ll,'non_family_members'] = nf_sum / nf_len\n        \n    data_train.loc[data_train['idhogar'] == ll,'adults_mean_age'] = ad_age_adult\n    data_train.loc[data_train['idhogar'] == ll,'mean_kids_age'] = ad_age_kids\n    \n    \n    data_train.loc[data_train['idhogar'] == ll,'sex_diff'] = np.sum(vals_sex)/len(vals_sex) \n        \ndata_train['electrinics'] = (data_train['v18q1'] + 2*data_train['computer'] + 2*data_train['television'] + data_train['qmobilephone'])/data_train['tamviv']\ndata_train['elec_cap'] = data_train['v18q'] + data_train['computer'] + data_train['television'] + data_train['mobilephone']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"179ef0620d9f0e0a6adb7aaa42952bd2b7a194f1"},"cell_type":"code","source":"data_test['sex_diff'] = 0\ndata_test['adults_mean_age'] = 0\ndata_test['mean_kid_age'] = 0\ndata_test['lone_old_man'] = 0\ndata_test['non_family_members'] = 0\n\nfor ll in np.unique(data_test[['idhogar']]):\n    vals = data_test.loc[data_test.idhogar == ll,'age'].values\n    vals_sex = data_test.loc[data_test.idhogar == ll,'sex'].values\n    vals_nf = data_test.loc[data_test.idhogar == ll,'parentesco12'].values\n    nf_sum = np.sum(vals_nf)\n    nf_len = len(vals_nf)\n    if len(vals[vals>18]) !=0:\n        ad_age_adult = vals[vals>18].mean(dtype='int')\n    if len(vals[vals<18]) !=0:\n        ad_age_kids = vals[vals<18].mean(dtype='int')\n    \n    if len(vals[vals >= 60]) == 1:\n        data_test.loc[data_test['idhogar'] == ll,'lone_old_man'] = 1\n        \n    if nf_sum != 0 :\n        data_test.loc[data_test['idhogar'] == ll,'non_family_members'] = nf_sum / nf_len\n        \n    data_test.loc[data_test['idhogar'] == ll,'adults_mean_age'] = ad_age_adult\n    data_test.loc[data_test['idhogar'] == ll,'mean_kids_age'] = ad_age_kids\n    \n    \n    data_test.loc[data_test['idhogar'] == ll,'sex_diff'] = np.sum(vals_sex)/len(vals_sex) \n        \ndata_test['electrinics'] = (data_test['v18q1'] + 2*data_test['computer'] + 2*data_test['television'] + data_test['qmobilephone'])/data_test['tamviv']\ndata_test['elec_cap'] = data_test['v18q'] + data_test['computer'] + data_test['television'] + data_test['mobilephone']\n\n\n# male female diff\n#oldest persons\n#mean adult age\n#mean kid age\n\n#max education of head\n# electrinic per capita v18q1 computer television qmobilephone\n#","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0dbcb77e5a09e9a3e04d73cd6d14b87edffc4ae9"},"cell_type":"code","source":"def create_agg_features(list_of_features, DF):\n    list_of_agg = ['min', 'max', 'mean', 'std']\n    \n    for feature in list_of_features:\n        for li in list_of_agg:\n            \n            DF['%s_%s'%(feature, li)] = 0\n            \n    for feature in list_of_features:\n    \n        \n        for  ll in np.unique(DF[['idhogar']]):\n            vals = DF.loc[DF.idhogar == ll,feature].values\n            min_1, max_1, mean_1, std_1 = np.min(vals), np.max(vals), np.mean(vals), np.std(vals)\n            \n            DF.loc[DF['idhogar'] == ll ,'%s_min'%feature ] = min_1\n            DF.loc[DF['idhogar'] == ll ,'%s_max'%feature ] = max_1\n            DF.loc[DF['idhogar'] == ll ,'%s_mean'%feature ] = mean_1\n            DF.loc[DF['idhogar'] == ll ,'%s_std'%feature ] = std_1\n            \n    return DF\n\ndef create_agg_features_sum(list_of_features, DF):\n    for feature in list_of_features:\n            DF['%s_%s'%(feature, 'sum')] = 0\n            \n    for feature in list_of_features:\n    \n        for  ll in np.unique(DF[['idhogar']]):\n            vals = DF.loc[DF.idhogar == ll,feature].values\n            \n            DF.loc[DF['idhogar'] == ll ,'%s_sum'%feature ] = np.sum(vals, dtype = int)            \n    return DF","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"909957ee60e9046c0b600ec612d7ab1a1b6a6a35"},"cell_type":"code","source":"def flg_to_categorical(feature, data_frame):\n    \n    list_of_features = [c  for c in data_frame.columns if c.startswith(feature)==True ]\n    new_feature = feature + '_cat'\n    print(list_of_features, \"-->\", new_feature)\n    \n    data_frame[new_feature] = np.argmax(data_frame[list_of_features].values, axis = 1)\n    data_frame = data_frame.drop(columns = list_of_features)\n    return data_frame\n\ndef scaled_features(list_of_features, DF):\n    \n    for feature in list_of_features:\n        DF['%s_%s' % (feature, 'scaled')] = DF[feature] / DF['hogar_total']\n    \n    return DF\n    \ndef flg_to_cat_v2(list_of_features, new_feature, data_frame):\n\n    #list_of_features = ['public', 'planpri', 'noelec', 'coopele']\n    #new_feature = 'elect_source' + '_cat'\n    print(list_of_features, \"-->\", new_feature)\n    \n    data_frame[new_feature] = np.argmax(data_frame[list_of_features].values, axis = 1)\n    data_frame = data_frame.drop(columns = list_of_features)\n    return data_frame","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"57c394f387b81794a6128506f013a9ed78066b38"},"cell_type":"code","source":"parentesco_like = ['parentesco' +str(i) for i in range(1, 13)]\n\ndata_train = create_agg_features_sum(parentesco_like, data_train)\ndata_test = create_agg_features_sum(parentesco_like, data_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"59e4e2869be6e7d7be7a78e869227bd47f158e0b"},"cell_type":"code","source":"estadocivil = ['estadocivil' + str(i) for i in range(1, 7)]\n\ndata_train = create_agg_features_sum(estadocivil, data_train)\ndata_test = create_agg_features_sum(estadocivil, data_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f8b53d197cc383891475aae7e02d4df242092b1c"},"cell_type":"code","source":"data_train = create_agg_features(['age'], data_train)\ndata_test = create_agg_features(['age'], data_test)\n\ndata_train = create_agg_features(['escolari'], data_train)\ndata_test = create_agg_features(['escolari'], data_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0634e2663d64f5bc670f02fbe1c8ed62a9a4fd24"},"cell_type":"code","source":"data_train = scaled_features(['hogar_nin', 'hogar_adul', 'hogar_mayor','v2a1','v18q1','qmobilephone'], data_train)\ndata_test = scaled_features(['hogar_nin', 'hogar_adul', 'hogar_mayor','v2a1','v18q1','qmobilephone'], data_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6fccc972b216412fd1dcbf2f15767705420f941c"},"cell_type":"code","source":"data_train = flg_to_cat_v2(['public', 'planpri', 'noelec', 'coopele'], 'elect_source',data_train)\ndata_test = flg_to_cat_v2(['public', 'planpri', 'noelec', 'coopele'], 'elect_source',data_test)\n\ndata_train = flg_to_cat_v2([ 'abastaguano',  'abastaguafuera','abastaguadentro'], 'water_source', data_train)\ndata_test = flg_to_cat_v2([ 'abastaguano',  'abastaguafuera','abastaguadentro'], 'water_source', data_test)\n\n\ndata_train = flg_to_categorical('tipovivi', data_train)\ndata_test = flg_to_categorical('tipovivi', data_test)\ndata_train = flg_to_categorical('sanitario', data_train)\ndata_test = flg_to_categorical('sanitario', data_test)\ndata_train = flg_to_categorical('energcocinar', data_train)\ndata_test = flg_to_categorical('energcocinar', data_test)\ndata_train = flg_to_categorical('instlevel', data_train)\ndata_train = flg_to_categorical('elimbasu', data_train)\ndata_test = flg_to_categorical('instlevel', data_test)\ndata_test = flg_to_categorical('elimbasu', data_test)\ndata_train = create_agg_features(['instlevel_cat'], data_train)\ndata_test = create_agg_features(['instlevel_cat'], data_test)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cfda293422f89f21904585fa6e48e1c0aec3135b"},"cell_type":"markdown","source":" ** Korelacje"},{"metadata":{"trusted":true,"_uuid":"93756d1e4774fb801d8ed21654a11813568d7f3d"},"cell_type":"code","source":"data_train = flg_to_categorical('estadocivil', data_train)\ndata_test = flg_to_categorical('estadocivil', data_test)\n\nparentes_co_like = ['parentesco%d'%i for i in range(2, 13)] + ['parentesco1_sum', 'parentesco2_sum']\n\ndata_train = data_train.drop(columns = parentes_co_like)\ndata_test = data_test.drop(columns = parentes_co_like)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a75d8dd883ace0e3998d1271ad58430f1681fb29"},"cell_type":"code","source":"for i in  data_train.columns:\n    print(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3aa9c9368d61f2636e13dba753c9d04a89147997"},"cell_type":"code","source":"corr_all  = data_train.drop(columns=['Target', 'idhogar', 'parentesco1']).select_dtypes(exclude='object')\n\ncorr_all = corr_all.corr()\n\n# trzeba usunąć diagonalę macierzy korelacji\nnp.fill_diagonal(corr_all.values, 0)\n\ncorr_all.head()\n\nto_drop = [col for col in corr_all.columns if any(abs(corr_all[col]) > 0.95) ]\n\nto_drop = np.unique(to_drop)\nfor td in to_drop:\n    print(\"%s %s\" % (td, ex_desc(td)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3a73c415cd4aa6f1a2ebf9084b274baef97959a5"},"cell_type":"code","source":"hh_like = ['r4t3', 'hhsize', 'tamviv', 'tamhog', 'hogar_total' ]\ndata_train[hh_like].corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7ef2aa71a1fafeb49fdf3e6d1f980fc65f86c526"},"cell_type":"code","source":"to_drop_22 = ['hhsize', 'tamhog', 'hogar_total', 'rooms']\ndata_train = data_train.drop(columns = to_drop_22)\ndata_test = data_test.drop(columns = to_drop_22)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"709bf198edf2c75e9d04e8b6e94b24cf241f92d5"},"cell_type":"code","source":"to_drop = list(set(to_drop) - set(to_drop_22))\nsize_like_var = data_train[to_drop]\nprint(size_like_var.corr().iloc[:5,:5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c4441c95535a3b434b549ab1e046290e868e77aa"},"cell_type":"code","source":"to_drop_v1 = [ 'area1','age', 'pca_1', 'pca_2','pca_5','ica_1','ica_4','ica_5', 'ica_6', 'hhsize_diff', 'tamhog']\nto_drop = list(set(to_drop) - set(to_drop_v1))\nsize_like_var = data_train[to_drop]\nprint(size_like_var.corr())\n#print(to_drop)\ndata_train = data_train.drop(columns = to_drop)\ndata_test = data_test.drop(columns = to_drop)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"28e08d32b65a874536f18c9352915c61c40c84ae"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\n\nn_estimators = 150\nmax_depth = 6\nRFC =RandomForestClassifier(n_estimators = n_estimators, max_depth=max_depth)\n\nX = data_train.drop(columns=id_cols)\nY = np.ravel(data_train[['Target']])\nprint(X.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"66369a25093f4024006048737e66d55635e45134"},"cell_type":"code","source":"RFC.fit(X,Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b21e5e0676737eede16b9101db4d3ff95dd096b9","_kg_hide-output":false},"cell_type":"code","source":"importances = RFC.feature_importances_\nindices = np.argsort(importances)[::-1]\nfor i in range(0, 100):\n    idx = indices[i]\n    print(\"{0:2} {1:20} {2:5} {3:10}\" \\\n          .format(i+1, X.columns[idx], np.round(importances[idx], 3), ex_desc(X.columns[idx])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d6541e2697c22622948cc18ccaeedd90eefb8cbb"},"cell_type":"markdown","source":"to_drop = X.columns[indices[85:]]\ndata_train = data_train.drop(columns = to_drop)\ndata_test = data_test.drop(columns = to_drop)\nprint(\"dropped %d features \" % len(to_drop))"},{"metadata":{"trusted":true,"_uuid":"7e61940261ade20d2c54228d3043a930acb57aec"},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import f1_score","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"70adc8ea3723975f0aadcc010e380f21e9314fc5"},"cell_type":"markdown","source":"Used dat set in modeling"},{"metadata":{"trusted":true,"_uuid":"c7c38df34d707a41cde2d54df195cc7d6a56d89b"},"cell_type":"code","source":"train_data = data_train.loc[data_train.parentesco1 == 1,]\ntest_data = data_test.loc[data_test.parentesco1 == 1,]\nprint(data_train.shape, data_test.shape)\nprint(train_data.shape, test_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"566b8e32e656a221d4159a1d252aa25a2e2cf63d"},"cell_type":"markdown","source":"**Modelowanie**\nhttps://lightgbm.readthedocs.io/en/latest/Python-Intro.html#setting-parameters"},{"metadata":{"_uuid":"8c5a627272fb9effd146c7d2c5a924a8e7d412d1"},"cell_type":"markdown","source":"hyperopt + xgboost -> https://www.kaggle.com/felipeleiteantunes/xgboost-hyperopt-cv-via-python-api"},{"metadata":{"trusted":true,"_uuid":"8174c5107894359f1f134d6439a0b3b7de3c5337"},"cell_type":"code","source":"import xgboost as xgb\n#import lightgbm as xgb\nfrom hyperopt import hp, tpe\nfrom sklearn.metrics import f1_score, classification_report\nfrom sklearn.model_selection import train_test_split\n\n\nimport random\nimport itertools\nfrom hyperopt.pyll.base import scope\nfrom hyperopt.pyll.stochastic import sample\nfrom hyperopt import STATUS_OK, Trials, fmin, hp, tpe\nN_HYPEROPT_PROBES = 5\nEARLY_STOPPING = 70 #change to 80\nHOLDOUT_SEED = 123456\nHOLDOUT_SIZE = 0.10\nHYPEROPT_ALGO =  tpe.rand.suggest\nSEED0 = random.randint(1,1000000000)\nNB_CV_FOLDS = 4 #chagne to 5\nNB_ROUNDS = 400\n\ndef f1_macro(preds, dtrain):\n    labels = dtrain.get_label()\n    return 'f1-macro', f1_score(labels, preds, average = 'macro')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"da7d08712a861bd07c1072804d9e6c3d8eb69230"},"cell_type":"code","source":"space ={\n    'booster '    : 'dart',\n    'silent'      : 1,\n    'objective'   : 'multi:softmax',\n    'num_class'   : 4,\n    'class_weights': 'balanced',\n    \n    'drop_rate'     : hp.uniform('drop_rate', 0.05, 0.5),\n    'subsample': hp.uniform('dart_subsample', 0.5, 1),\n    'subsample_freq': hp.quniform('dart_subsample_freq', 1, 10, 1),\n    'num_boost_rounds' : NB_ROUNDS,\n    #'num_boost_rounds' :hp.choice('num_boost_rounds', np.arange(100, 500, 10, dtype=int)),\n     \n    'max_depth'   : hp.choice(\"max_depth\", np.arange(2, 30, 1, dtype='int')),\n    'num_leaves': hp.quniform('num_leaves', 3, 50, 1),\n   \n    'lambda_l1'       : hp.uniform('lambda_l1', 1e-4, 1e-6 ),\n    'lambda_l2'      : hp.uniform('lambda_l2', 1e-4, 1e-6 ),\n    \n    'max_delta_step'   : hp.choice('max_delta_step', np.arange(0,5,   dtype=int)),\n    'min_child_weight ': hp.loguniform('min_sum_hessian_in_leaf', 0, 2.3),\n    'learning_rate'    : hp.loguniform('learning_rate', -6.9, -2.3),\n    \n    'seed'             : hp.randint('seed',2000000)\n   }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"372c80f2d606f3a73e0d219f5cf8d5e0f6cdd349"},"cell_type":"code","source":"data_to_fit = train_data.drop(columns = ['Id', 'idhogar', 'parentesco1'])\nX, y =  data_to_fit.drop(columns = ['Target']).values, data_to_fit[['Target']].values\ny = np.ravel(y - 1).astype(int)\ny","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"be9a76640a1e6088898e5922e2e3ef4d67488d35"},"cell_type":"code","source":"print(\"{0: .3} {1: .4}\".format(1.11225352355345, .235346345345345346456456))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c144ffc5e3cf2b9d144cdb1f335e48fa3c62b747"},"cell_type":"code","source":"def objective(space):\n    \n    global X, y, curr_best_score, best_params\n\n    \n    params = sample(space)\n    #model = xgb.XGBClassifier(**params, n_jobs = -1, n_estimators = NB_ROUNDS)\n    \n    cv_scores = []\n    nb_rounds = []\n       \n    for cv_ in range(0, NB_CV_FOLDS):\n        Xtrain,  Xtest, ytrain, ytest = train_test_split(X, y, random_state=np.random.randint(0, 1e+8), test_size=0.3, stratify = y )\n        \n        Xtrain = xgb.DMatrix(Xtrain, ytrain)\n        Xtest = xgb.DMatrix(Xtest, ytest)\n        \n        model = xgb.train(params, Xtrain, \n                          #early_stopping_rounds = EARLY_STOPPING,\n                          evals = [(Xtest, 'test')],\n                          feval = f1_macro,\n                          verbose_eval = False)\n        \n        preds = model.predict(Xtest, ntree_limit=params['num_boost_rounds'])\n        cv_scores.append(f1_score(ytest, preds, average = 'macro'))\n        #nb_rounds.append(model.best_iteration)\n        \n\n    score, score_std = np.mean(cv_scores), np.std(cv_scores)\n\n    print( 'cv_score={0: .3f} +- {1: .3f} BEST_SCORE: {2: .3f}'.format( score, score_std, curr_best_score ) )\n    \n    if score > curr_best_score:\n        best_params = params\n        curr_best_score  = score\n        \n        \n#         do_submit = True\n\n#     if do_submit:\n#         submit_guid = uuid4()\n\n#         print('Compute submissions guid={}'.format(submit_guid))\n\n#         y_submission = gbm_model.predict(xgb_test, ntree_limit = n_rounds)\n#         submission_filename = 'xgboost_score={:13.11f}_submission_guid={}.csv'.format(score,submit_guid)\n#         pd.DataFrame(\n#         {'id':test_id, 'target':y_submission}\n#         ).to_csv(submission_filename, index=False)\n       \n    loss = 1 - score\n    return {'loss': loss, 'status': STATUS_OK}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"27ff2b7ca10960f86b15bf380fe4b4215c69b767","_kg_hide-output":true},"cell_type":"code","source":"\ncurr_best_score = 0\ntrials = Trials()\nbest_params = None\nfit_report = None\nbest = fmin(fn=objective,\n                     space=space,\n                     algo=HYPEROPT_ALGO,\n                     max_evals=N_HYPEROPT_PROBES,\n                     trials=trials,\n                     verbose=0)\n\nprint('-'*50)\nprint('The best params:')\nprint( best )\nprint('\\n\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8dcf94ab74bd69a914b5b57c35a8d4d161e632de"},"cell_type":"code","source":"print(curr_best_score)\nprint('params:\\n',best_params)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9d9e9c40a27db9e4749399f6fcc792d6ce825ad4","_kg_hide-output":false},"cell_type":"code","source":"X, y =  data_train.drop(columns = id_cols), np.ravel(data_train[['Target']]-1)\nXtest = test_data.drop(columns = id_colst)\nxgb_fulldata = xgb.DMatrix(X, y)\nxgb_test = xgb.DMatrix(Xtest)\nbest_model = xgb.train(best_params, xgb_fulldata, verbose_eval=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c1bc1603de1e5c404098c67a4371aae0e08ba63a"},"cell_type":"code","source":"xgb.plot_importance(best_model, height = 0.4, max_num_features=40)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ba56e726f273ec9cbfa04e941cfc0dcc9664e6fa"},"cell_type":"code","source":"#make submission\nytestpreds = best_model.predict(xgb_test).astype('int') + 1\nrest_of_ids = data_test.Id\n\nmy_submission = pd.DataFrame({'Id': data_test.Id, 'Target': 4})\n\nfor idd,val in zip(test_data.Id, ytestpreds):\n        my_submission.loc[my_submission.Id == idd,'Target'] = val\n\n\nmy_submission.to_csv('submission.csv', index=False)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dd8f86a6f863a555a75f12c673f569d53ecd82f7"},"cell_type":"code","source":"for x,y in zip(data_train.drop(columns = ['Target']).columns, data_test.columns):\n    if x != y:\n        print(x,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a757bbf5bd4485cb38dd6dd6124cdc752217d903"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}