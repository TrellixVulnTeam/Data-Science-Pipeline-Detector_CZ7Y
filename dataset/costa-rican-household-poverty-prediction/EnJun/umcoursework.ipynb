{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Title: \n#### Predict Costa Rican Household Poverty Level\n\n## Group Members:\n- Cheah Jun Yitt (WQD180107)\n- Choong En Jun (WQD180113)\n- Choo Jian Wei (WQD180124)\n- Tan Yin Yen (WQD180108)\n\n## Overview and Motivation:\nMany social programs have a hard time making sure the right people are given enough aid. It’s especially tricky when a program focuses on the poorest segment of the population. The world’s poorest typically can’t provide the necessary income and expense records to prove that they qualify.   \n\nThe Inter-American Development Bank (IDB) works to improve lives in Latin America and the Caribbean by providing financial and technical support for countries working to reduce poverty and inequality. Household poverty is an important key to IDB’s strategies. \n\nIn Latin America, one popular method uses an algorithm to verify income qualification. It’s called the Proxy Means Test (or PMT). With PMT, agencies use a model that considers a family’s observable household attributes like the material of their walls and ceiling, or the assets found in the home to classify them and predict their level of need.\n\n<https://www.kaggle.com/c/costa-rican-household-poverty-prediction>\n"},{"metadata":{},"cell_type":"markdown","source":"# Asking a Question\n### How can we help with predicting the poverty level of some of the world's poorest household, so that they will have enough and proper financial aid?\n"},{"metadata":{},"cell_type":"markdown","source":"# Objective\n1. Study the relationship between certain household factors and poverty levels.  \n2. Identify household factors that are important and meaningful on predicting the poverty levels. \n3. Predict the poverty levels using the important household factors.  \n4. Compare the performance of various models used on predicting the poverty levels.  "},{"metadata":{},"cell_type":"markdown","source":"## Data:\n\n<div style=\"float:left;\">\n    <table>\n    <tr><td>Title of Dataset</td><td>Costa Rican Household characteristics Dataset</td></tr>\n    <tr><td>Year</td><td>2018</td></tr>\n    <tr><td>Number of Observations</td><td>9557 observations on Train Dataset<br>23856 observations on Test Dataset</td></tr>\n    <tr><td>Number of Variables</td><td>142 columns</td></tr>\n    <tr><td>Source</td><td>Uploaded for Kaggle Community Competition by Inter-American Development Bank (IDB)<br>\n         <a href=\"https://www.kaggle.com/c/costa-rican-household-poverty-prediction/data\">https://www.kaggle.com/c/costa-rican-household-poverty-prediction/data</a></td></tr>\n    <tr><td>Purpose of Dataset</td><td>To challenge existing Proxy Means Test (PMT) algorithm on household poverty prediction</td></tr>\n    </table>\n</div>\n\n\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"## Table of Content\n- Package Installation \n- Reading the Data\n- Data Cleaning and Data Preprocessing\n    - Dealing with Outliers\n    - Dealing with Missing Data\n- Visualization\n- Modelling\n- Model Evaluation\n- Conclusion"},{"metadata":{},"cell_type":"markdown","source":"# Package Installation\n### Installation of lightgbm package"},{"metadata":{"trusted":true},"cell_type":"code","source":"# !pip install lightgbm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Load python packages"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Data manipulation\nimport pandas as pd\nimport numpy as np\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\npd.options.display.max_columns = 150\n\n#for machine learning\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import f1_score\nimport lightgbm as lgb\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reading the Data"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntrain.info()\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"test = pd.read_csv('../input/test.csv')\ntest.info()\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Cleaning and Data Pre-processing\nThere are multiple discrepancies in the dataset. Also, we refer to the Kaggle discussion for additional insights regarding the data. \n<br>Following can be said about the raw dataset:\n<ol>\n  <li>Each line represents to an individual in the household</li>\n    <li>The target (poverty level) is segregated to 4 levels (1: extreme poverty, 2: moderate poverty, 3: vulnerable households, 4: non-vulnerable households)</li>\n    <li>There are individuals without household</li>\n    <li>There are individuals within the same household that does not share the same target poverty level</li>\n    <li>Prediction is scored based on head of household only</li>\n</ol>\n\n<br>\nWithout futher due, we perform our own cleaning and preparation of the dataset\n\n"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"train.describe()\ntest.describe()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dealing with Outliers\n**If we look carefully, There is an outlier for attribute rez_esc in test data.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(test.loc[:,\"rez_esc\"].describe())\ntest.loc[test.loc[:,\"rez_esc\"]==99,\"rez_esc\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that there is only one outlier =99, the rest of the test data is okay.  \nAccording to answer from kaggle competition host, the value can be safely changed to 5.  \n<https://www.kaggle.com/c/costa-rican-household-poverty-prediction/discussion/61403>"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"test.loc[test.loc[:,\"rez_esc\"]==99,\"rez_esc\"]=5\ntest.loc[:,\"rez_esc\"].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dealing with Missing Data\n**Now we will deal with missing values in both test and train dataset.**  \n- We will first check for missing values in the columns"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"train_na= pd.DataFrame((train.isnull().sum().values),index=train.columns, columns=['isNA']).sort_values(by=['isNA'],ascending=False)\nif train_na.loc[train_na.loc[:,'isNA']>0,:].shape[0]>1 :\n    print(\"Train Dataset\")\n    print(train_na.loc[train_na.loc[:,'isNA']> 0,])\nelse:\n    print('no NA in train set')\n\ntest_na= pd.DataFrame((test.isnull().sum().values),index=test.columns, columns=['isNA']).sort_values(by=['isNA'],ascending=False)\nif train_na.loc[train_na.loc[:,'isNA']>0,:].shape[0]>1 :\n    print(\"\")\n    print(\"Test Dataset\")\n    print(test_na.loc[test_na.loc[:,'isNA']> 0,])\nelse:\n    print('no NA in test set')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We can see the missing values are largely from:**\n\n- ***rez_esc: years behind in school:***\n\n- ***v18q1: number of tablets household owns***\n\n- ***v2a1: monthly rent payment***\n\n- ***meaneduc: average years of education for adults***\n\n- ***SQBmeaned is the square of meaneduc***\n"},{"metadata":{},"cell_type":"markdown","source":"For the variable ***rez_esc (years behind in school)***,  \ndata is only available if the age of individual is between 7 to 17 years old.  \nWe will set 0 to all other null values."},{"metadata":{"trusted":true},"cell_type":"code","source":"rez_esc_age=train.loc[train['rez_esc'].isnull()==False, 'age']\n\nplt.hist(x=rez_esc_age,)\nplt.xticks(np.arange(min(rez_esc_age), max(rez_esc_age)+1, 1.0),rotation = 60),\nplt.ylabel('frequence of rez_esc')\nplt.xlabel('Age')\nplt.title('Non-null rez_esc Frequency according to age')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***v2a1: monthly rent payment***\n    \n-    this depends on tipovivi2 and tipovivi3, v2a1 is NA if tipovivi2 or tipovivi3 is 0\n    tipovivi2 (a true false statement if an individual owns the house and is paying installment). \n    tipovivi3 (a true false statement if an individual is renting the house). \n    We will assume 0 for NA in v2a1"},{"metadata":{"trusted":true},"cell_type":"code","source":"tipos=[x for x in train if x.startswith('tipo')]\nrentNA_status=train.loc[train['v2a1'].isnull(), tipos].sum()\nplt.bar(tipos,rentNA_status,align='center')\nplt.xticks([0,1,2,3,4],['Owns and Paid off','Owns and Paying', 'Renting','Precarious','Other'],rotation = 60),\nplt.ylabel('Frequency')\nplt.title(\"Missing Rental 'v2a1' according to Home Ownership Status\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***v18q1: number of tablets household owns***\n\n  -  This depends on v18q (a true false statement if an individual own a tablet). v18q1 is NA if v18q is 0\n    We will assume 0 for NA in v18q1"},{"metadata":{"trusted":true},"cell_type":"code","source":"Tablet_status=train.loc[train['v18q1'].isnull(), 'v18q']\nplt.hist(x=Tablet_status)\nplt.xticks([0,1,2],['Do not Own a Table','Owns a Tablet'],rotation=60),\nplt.ylabel('Frequency missing value on v18q1')\nplt.xlabel('Individual Tablet Ownership (v18q)')\nplt.title('Missing value on household tablet ownership vs individual tablet ownership')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***meaneduc: average years of education for adults***\n  -  We will replace this with mode\n    \n***SQBmeaned is the square of meaneduc***\n  - replace with square of replaced meaneduc"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nplt.hist(x=train['meaneduc'],bins=int(train['meaneduc'].max())+1)\nplt.xticks(np.arange(min(train['meaneduc']), max(train['meaneduc'])),rotation=60),\nplt.ylabel('Frequency')\nplt.xlabel('average years of education for adults (18+)')\nplt.title('Histogram for meaneduc')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Impute Missing Values\n1. Impute ***meaneduc*** with mode (mode = 6)\n2. Impute ***SQBmeaned*** with squared of mode of ***meaneduc*** (mode = 36)\n3. Impute ***rez_esc***, ***v18q1*** and ***v2a1*** with 0 "},{"metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"train.loc[:,\"meaneduc\"].mode()\n#train: mode for meaneduc is 6 replace NA with 6, replace SQBmeaned NA to 36\ntrain.loc[train.loc[:,\"meaneduc\"].isnull()==True,\"meaneduc\"] = 6\ntrain.loc[train.loc[:,\"SQBmeaned\"].isnull()==True,\"SQBmeaned\"] = 36\n\ntest.loc[:,\"meaneduc\"].mode()\n#test: mode for meaneduc is 6 replace NA with 6, replace SQBmeaned NA to 36\ntest.loc[test.loc[:,\"meaneduc\"].isnull()==True,\"meaneduc\"] = 6\ntest.loc[test.loc[:,\"SQBmeaned\"].isnull()==True,\"SQBmeaned\"] = 36\n\n\n#Replace all NA values for remaining 3 attributes with 0no\ntrain.loc[train.loc[:,\"rez_esc\"].isnull()==True,\"rez_esc\"] = 0\ntrain.loc[train.loc[:,\"v18q1\"].isnull()==True,\"v18q1\"] = 0\ntrain.loc[train.loc[:,\"v2a1\"].isnull()==True,\"v2a1\"] = 0\n\ntest.loc[test.loc[:,\"rez_esc\"].isnull()==True,\"rez_esc\"] = 0\ntest.loc[test.loc[:,\"v18q1\"].isnull()==True,\"v18q1\"] = 0\ntest.loc[test.loc[:,\"v2a1\"].isnull()==True,\"v2a1\"] = 0\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Check for Missing Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check for missing values again:\ntrain_na= pd.DataFrame((train.isnull().sum().values),index=train.columns, columns=['isNA']).sort_values(by=['isNA'],ascending=False)\nif train_na.loc[train_na.loc[:,'isNA']>0,:].shape[0]>1 :\n    train_na.loc[train_na.loc[:,'isNA']> 0,]\n\nelse:\n    print('No NA in train set')\n\ntest_na= pd.DataFrame((test.isnull().sum().values),index=test.columns, columns=['isNA']).sort_values(by=['isNA'],ascending=False)\nif train_na.loc[train_na.loc[:,'isNA']>0,:].shape[0]>1 :\n    test_na.loc[test_na.loc[:,'isNA']> 0,]\nelse:\n    print('No NA in test set')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Check for Data Inconsistency","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Check for Data Inconsistency"},{"metadata":{},"cell_type":"markdown","source":"### Investigate if all individuals in the household have the same poverty target"},{"metadata":{"trusted":true},"cell_type":"code","source":"target_Discrepancy=(train.groupby('idhogar')['Target'].nunique()>1)\nnum_unique_households = train[\"idhogar\"].unique().shape[0]\nprint('There are',target_Discrepancy.sum(),'households with contradicting targets, out of', num_unique_households, 'households in the train dataset.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let's see the data for 85 households that have discrepancy in target poverty level**"},{"metadata":{"trusted":true},"cell_type":"code","source":"Discrepancy_Index=(train.groupby('idhogar')['Target'].transform('nunique')>1)\nHHID_Discrepancy=train.loc[Discrepancy_Index,'idhogar'].unique()\n#household with contradicting target\ntrain.loc[train['idhogar'].isin(HHID_Discrepancy),['idhogar','parentesco1','Target']].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Judging from the data, the household head target might not be necessary true. Although prediction scoring is based on household head target, we should be able to safely replace the household target using the mode target of the household.**\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfor HH in HHID_Discrepancy:\n    Targets= (train.loc[train['idhogar']==HH,'Target'])\n\n    if Targets.mode().shape[0] >1:\n        for i in Targets.index:\n            if train.loc[i,'parentesco1']==1:\n                HeadTarget= train.loc[i,\"Target\"]    \n        for i in Targets.index:\n            train.loc[i,'Target']=HeadTarget\n    elif Targets.mode().shape[0]==1:\n        for i in Targets.index:\n            TrueTarget=int(Targets.mode())\n            train.loc[i,'Target']=TrueTarget\n        \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Check for household targets discrepancy again for confirmation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"target_Discrepancy=(train.groupby('idhogar')['Target'].nunique()>1)\n\nprint('There are ',target_Discrepancy.sum(),'households with contradicting targets, out of 2988 households in the train dataset')\n\ntrain.head()\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Remove (SQBescolari, SQBage, SQBHogar_ttal,SQBedjefe, SQBhogar_nin,SQBovercrowding, SQBdependency, SQBMeaned, agesq)**  \nThere are 9 columns where the attributes are the squared of other attributes. We do not need those in our model as the model are smart enough to detect non-linear relationship.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train=train.drop(columns=train.columns[133:142],axis=1)\ntest=test.drop(columns=test.columns[133:142],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.shape)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualization"},{"metadata":{},"cell_type":"markdown","source":"## Household Level Features Visualization"},{"metadata":{},"cell_type":"markdown","source":"**We Can Segregate the attributes into different categories**<br>\n1st is by Household level or Individual level<br>\n2nd is by Numerical or Categorical True/False<br>\n\nSo we will have 4 types of data<br>\n-Household Numerical<br>\n-Individual Numerical<br>\n-Household Categorical<br>\n-Individual Categorical<br>\n\nFor simple EDA, we will only look at training dataset, and plot a box plot on the numerical household variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"#These are Household Numerical columns. Attribute \"Dependency\" is not included since it has some yes and no elements\nnumColumnsHH= [\"v2a1\",\"rooms\",\"v18q1\",\"qmobilephone\",\"r4h3\",\"r4m3\",\"r4t3\",\"tamhog\",\"hhsize\",\"hogar_nin\",\"hogar_adul\",\n               \"hogar_mayor\",\"hogar_total\",\"meaneduc\",\"bedrooms\",\"overcrowding\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax =plt.subplots(6,3, figsize=(18,15))\nplt.subplots_adjust(top=2.0)\nx=0\ny=0\nfor i in range(0,len(numColumnsHH)):\n    if (i>0) & (i %3==0):\n        x=x+1\n        y=0\n    sns.boxplot(x=\"Target\",y=numColumnsHH[i], data=train, ax=ax[x,y])\n    ax[x,y].title.set_text(numColumnsHH[i])\n    y=y+1\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**At one glance the box plot is useful compare the 4 targets' summary statistic of each variables plotted.**  \nSo far, the box plot make sense as we look at the slight difference in the variable mean between targets.  \n1) number of mobile phone per househould - mean is lower in poverty level 1 and 2 indicating less affordability  \n2) number of children \"hogar_nin\" - mean is lower in poverty level 3 & 4 indicating less commitment  \n3) number of adults \"hogar_adul\" - mean is lower in poverty level 1 meaning less earning power  \n4) mean education \"mean educ: - mean has a upwarding bias from poverty level 1 to 4  \n5) overcrowding - mean has a decreasing bias from poverty level 1 to 4  \n"},{"metadata":{},"cell_type":"markdown","source":"## Visualize Living Conditions by Poverty Level"},{"metadata":{},"cell_type":"markdown","source":"**For this section, we use data points for head of household only. We want to avoid having frequencies that were inflated by the number of individuals in a house.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"cond_data = train.query(\"parentesco1==1\").copy()\ncond_data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n### Visualize Various Household Conditions related Attributes by Poverty Level"},{"metadata":{"trusted":true},"cell_type":"code","source":"wall = [\"paredblolad\",\"paredzocalo\",\"paredpreb\",\"pareddes\",\"paredmad\",\"paredzinc\",\"paredfibras\",\"paredother\"]\nfloor = [\"pisomoscer\",\"pisocemento\",\"pisoother\",\"pisonatur\",\"pisonotiene\",\"pisomadera\"]\nroof = ['techozinc','techoentrepiso','techocane','techootro', 'cielorazo']\nwater = [\"abastaguadentro\",\"abastaguafuera\",\"abastaguano\"]\nelectric = [\"public\",\"planpri\",\"noelec\",\"coopele\"]\ntoilet = ['sanitario1','sanitario2','sanitario3','sanitario5','sanitario6']\nenergy = ['energcocinar1','energcocinar2','energcocinar3','energcocinar4']\nrubbish = ['elimbasu1','elimbasu2','elimbasu3','elimbasu4','elimbasu5','elimbasu6']\nwall_cond = ['epared1','epared2','epared3']\nroof_cond = ['etecho1','etecho2','etecho3']\nfloor_cond = ['eviv1','eviv2','eviv3']\n\ncol_types = [wall, floor, roof, water, electric, toilet, energy, rubbish, wall_cond, roof_cond, floor_cond]\ntitles = ['wall', 'floor', 'roof', 'water', 'electric', 'toilet', 'energy', 'rubbish', 'wall condition', 'roof condition', 'floor condition']\n\nfig, ax =plt.subplots(len(col_types)//3 + 1, 3, figsize=(30,10))\nplt.subplots_adjust(top=2.0)\nx=0\ny=0\nfor i, cols in enumerate(col_types):\n    if (i>0) & (i % 3==0):\n        x=x+1\n        y=0\n    title = titles[i]\n    plot_df = pd.melt(cond_data, id_vars=['Target'], value_vars=cols).groupby([\"Target\",\"variable\"]).apply(lambda x: np.mean(x))\n    plot_df[\"variable\"] = plot_df.index.get_level_values(1)\n    plot_df = plot_df.rename(columns = dict(value=\"Percentage Frequency\", variable = \"Attribute\"))\n    sns.barplot(x=\"Attribute\", y = \"Percentage Frequency\", hue=\"Target\", data=plot_df, ax=ax[x,y])\n    ax[x,y].title.set_text(title)\n    y=y+1\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on the plot above, Household with Target=4 tend to have a wall made of block or brick.<br>\nAlso, Households with Target=4 tend to have floor made of mosaic, ceramic, terrazo. (pisomoscer = 1)<bR>\n    \nIf we look at the living condition scoring variables for wall (epared1), roof (etecho1), and floor (eviv1), we can see that the households inclined towards extreme poverty level are highly concentrated towards bad living conditions for the three variables mentioned.\n\n"},{"metadata":{},"cell_type":"markdown","source":"## Visualize Features by Geographical Locations"},{"metadata":{},"cell_type":"markdown","source":"**As we have look at the features based on targets, we shall also view at it based on geographical locations.**"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"region = [\"lugar1\",\"lugar2\",\"lugar3\",\"lugar4\",\"lugar5\",\"lugar6\"]\narea= [\"area1\",\"area2\"]\n\ncol_types = [region, area]\ntitles = ['region', 'area']\n\nfig, ax =plt.subplots(1,2, figsize=(15,5))\nplt.subplots_adjust(top=1.0)\nx=0\ny=0\nfor i, cols in enumerate(col_types):\n    if (i>0) & (i % 3==0):\n        x=x+1\n        y=0\n    title = titles[i]\n    plot_df = pd.melt(cond_data, id_vars=['Target'], value_vars=cols).groupby([\"Target\",\"variable\"]).apply(lambda x: np.mean(x))\n    plot_df[\"variable\"] = plot_df.index.get_level_values(1)\n    plot_df = plot_df.rename(columns = dict(value=\"Percentage Frequency\", variable = \"Attribute\"))\n    sns.barplot(x=\"Attribute\", y = \"Percentage Frequency\", hue=\"Target\", data=plot_df, ax=ax[y])\n    ax[y].title.set_text(title)\n    y=y+1\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that that area 1, urban area has a higher convenctration of non-vulnerable poverty level group.\nAs for the regions, lugar 1 which is the central and has a healthy increasing population trend towards poverty non-vulnerabe group. \nWhereas in other region, lugar 2 to 6 population trend have concentration gradually decreasing from poverty extreme toward poverty non-vulnerable."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"geoTrain= train.loc[train['parentesco1'] == 1, :].copy()\ngeoTrain['lugar'] = np.argmax(np.array(cond_data[[\"lugar1\",\"lugar2\",\"lugar3\",\"lugar4\",\"lugar5\",\"lugar6\"]]),\n                           axis = 1)\ngeoTrain['lugar']=geoTrain['lugar'].replace({5:6,4:5,3:4,2:3,1:2,0:1})\n","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"wall = [\"paredblolad\",\"paredzocalo\",\"paredpreb\",\"pareddes\",\"paredmad\",\"paredzinc\",\"paredfibras\",\"paredother\"]\nfloor = [\"pisomoscer\",\"pisocemento\",\"pisoother\",\"pisonatur\",\"pisonotiene\",\"pisomadera\"]\nroof = ['techozinc','techoentrepiso','techocane','techootro', 'cielorazo']\nwater = [\"abastaguadentro\",\"abastaguafuera\",\"abastaguano\"]\nelectric = [\"public\",\"planpri\",\"noelec\",\"coopele\"]\ntoilet = ['sanitario1','sanitario2','sanitario3','sanitario5','sanitario6']\nenergy = ['energcocinar1','energcocinar2','energcocinar3','energcocinar4']\nrubbish = ['elimbasu1','elimbasu2','elimbasu3','elimbasu4','elimbasu5','elimbasu6']\nwall_cond = ['epared1','epared2','epared3']\nroof_cond = ['etecho1','etecho2','etecho3']\nfloor_cond = ['eviv1','eviv2','eviv3']\n\ncol_types = [wall, floor, roof, water, electric, toilet, energy, rubbish, wall_cond, roof_cond, floor_cond]\ntitles = ['wall', 'floor', 'roof', 'water', 'electric', 'toilet', 'energy', 'rubbish', 'wall condition', 'roof condition', 'floor condition']\n\nfig, ax =plt.subplots(len(col_types)//3 + 1, 3, figsize=(30,10))\nplt.subplots_adjust(top=2.0)\nx=0\ny=0\nfor i, cols in enumerate(col_types):\n    if (i>0) & (i % 3==0):\n        x=x+1\n        y=0\n    title = titles[i]\n    plot_df = pd.melt(geoTrain, id_vars=['lugar'], value_vars=cols).groupby([\"lugar\",\"variable\"]).apply(lambda x: np.mean(x))\n    plot_df[\"variable\"] = plot_df.index.get_level_values(1)\n    plot_df = plot_df.rename(columns = dict(value=\"Percentage Frequency\", variable = \"Attribute\"))\n    sns.barplot(x=\"Attribute\", y = \"Percentage Frequency\", hue=\"lugar\", data=plot_df, ax=ax[x,y])\n    ax[x,y].title.set_text(title)\n    y=y+1\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above plots, we can see that lugar 2 to 6 consistently tops the chart for bad condition of the wall (epared1), roof (etecho1), and floor (eviv1). Perhaps visually, we can deduce that Pacífico Central (lugar 3) and Brunca (lugar 4) have it worst among all other regions.   "},{"metadata":{},"cell_type":"markdown","source":"**Now let's look at numerical features based on geographical location to see if we can find anything interesting.**"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"fig, ax =plt.subplots(6,3, figsize=(18,15))\nplt.subplots_adjust(top=2.0)\nx=0\ny=0\nfor i in range(0,len(numColumnsHH)):\n    if (i>0) & (i %3==0):\n        x=x+1\n        y=0\n    sns.boxplot(x=\"lugar\",y=numColumnsHH[i], data=geoTrain, ax=ax[x,y])\n    ax[x,y].title.set_text(numColumnsHH[i])\n    y=y+1\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In general, the boxplot tells us what we already suspected. lugar 3 shows the highest mean for overcrowding indicating worst living conditions in this geographical region. "},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering"},{"metadata":{},"cell_type":"markdown","source":"**Since prediction is scored only for the household head. We need to make new features that is household level and not individual.**  \n**Besides, we have created a new scoring system for education level (*instlevel*), by giving higher score to individual that has completed tertiary education.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Setting new features for household specific in train data\n\n#Number of Adults not including seniors >65\ntrain['Adults']=train['hogar_adul']-train['hogar_mayor']\n#Number of children < 19yo and seniors>65\ntrain['Dependents']=train['hogar_nin']+train['hogar_mayor']\n#Number of teenager from 12 to 19\ntrain['Teenagers']=train['hogar_nin']-train['r4t1']\n#Dependency is number of dependents per adults. This replaces the original dependency data from dataset.\ntrain['dependency']=train['Dependents']/train['Adults']\n#Percentage of Adults in household\ntrain['P_Adults']=train['Adults']/train['hogar_total']\n#Percentage of Male Adults in household\ntrain['P_Adults_Male']=train['r4h3']/train['hogar_total']\n#Percentage Female Adults in household\ntrain['P_Adults_Female']=train['r4m3']/train['hogar_total']\n#Percentage Children <19yo in household\ntrain['P_Children']=train['hogar_nin']/train['hogar_total']\n#Percentage of Seniors in household\ntrain['P_Seniors']=train['hogar_mayor']/train['hogar_total']\n#Percentage of Teenagers in household\ntrain['P_Teenagers']=train['Teenagers']/train['hogar_total']\n#Rent per person in household\ntrain['RentHH']=train['v2a1']/train['hogar_total']\n#Rent per Adult in household\ntrain['RentAdults'] = train['v2a1']/train['Adults']\ntrain['RentAdults'] = train['RentAdults'].fillna(train['v2a1']) # Replace NA value with the Rent value itself (Assume Adults = 0 as Adults = 1, there has to be 1 adult to pay the Rent amount.)\n\n#Tablet per person in household\ntrain['Tablet_PP']=train['v18q1']/train['hogar_total']\n#Mobile Phone per person in household\ntrain['Phone_PP']=train['qmobilephone']/train['hogar_total']\n#Bedroom per person in household\ntrain['Bedroom_PP']=train['bedrooms']/train['hogar_total']\n#Appliance scoring. Higher the better\ntrain['Appliances']=train['refrig']+train['computer']+train['television']\n#Number of Appliances per person\ntrain['Appliances_PP']=train['Appliances']/test['hogar_total']\n\n#Household size Difference\ntrain['HHS_Diff']=train['tamviv']-train['hhsize']\n\n#Number of Adults per room\ntrain[\"AdultsPerRoom\"] = train['Adults']/train['rooms']\n#Number of Dependents per room\ntrain[\"DependentsPerRoom\"] = train['Dependents']/train['rooms']\n#Number of Teenagers per room\ntrain[\"TeenagersPerRoom\"] = train['Teenagers']/train['rooms']\n\n#Number of Males per room\ntrain[\"MalesPerRoom\"] = train['r4h3']/train['rooms']\n#Number of Females per room\ntrain[\"FemalesPerRoom\"] = train['r4m3']/train['rooms']\n\n#Percentage of rooms that are bedrooms\ntrain[\"BedroomPerRoom\"] =  train['bedrooms']/train['rooms']\ntrain[\"RentPerRoom\"] =  train['v2a1']/train['rooms']\n\n#Years of Schooling over Age\ntrain[\"Schooling_Age\"]  = train[\"escolari\"]/train[\"age\"]\n\n#Years behind schooling vs Years of schooling proportion\ntrain['SchoolingProp'] = train['rez_esc']/train['escolari']\n\n\n#New Scoring For Education Level\ntrain[\"EduLevel\"] = 0 \ntrain.loc[train[\"instlevel9\"] == 1,\"EduLevel\"] = 6\ntrain.loc[train[\"instlevel8\"] == 1,\"EduLevel\"] = 5 #higher scoring for completing tertiary education\ntrain.loc[train[\"instlevel7\"] == 1,\"EduLevel\"] = 3\ntrain.loc[train[\"instlevel5\"] == 1,\"EduLevel\"] = 2\ntrain.loc[(train[[\"instlevel4\",\"instlevel3\",\"instlevel6\"]].sum(axis = 1) > 0),\"EduLevel\"] = 1\n\ntrain.head()\n\n#We replicate the same for test data since we need the same features for prediction\n\ntest['Adults']=test['hogar_adul']-test['hogar_mayor']\ntest['Dependents']=test['hogar_nin']+test['hogar_mayor']\ntest['Teenagers']=test['hogar_nin']-test['r4t1']\ntest['dependency']=test['Dependents']/test['Adults']\ntest['P_Adults']=test['Adults']/test['hogar_total']\ntest['P_Adults_Male']=test['r4h3']/test['hogar_total']\ntest['P_Adults_Female']=test['r4m3']/test['hogar_total']\ntest['P_Children']=test['hogar_nin']/test['hogar_total']\ntest['P_Seniors']=test['hogar_mayor']/test['hogar_total']\ntest['P_Teenagers']=test['Teenagers']/test['hogar_total']\ntest['RentHH']=test['v2a1']/test['hogar_total']\n\ntest['RentAdults']=test['v2a1']/test['Adults']\ntest['RentAdults'] = test['RentAdults'].fillna(test['v2a1']) # Replace NA value with the Rent value itself (Assume Adults = 0 as Adults = 1, there has to be 1 adult to pay the Rent amount.)\n\ntest['Tablet_PP']=test['v18q1']/test['hogar_total']\ntest['Phone_PP']=test['qmobilephone']/test['hogar_total']\ntest['Bedroom_PP']=test['bedrooms']/test['hogar_total']\ntest['Appliances']=test['refrig']+test['computer']+test['television']\ntest['Appliances_PP']=test['Appliances']/test['hogar_total']\ntest['HHS_Diff']=test['tamviv']-test['hhsize']\n\ntest[\"AdultsPerRoom\"] = test['Adults']/test['rooms']\ntest[\"DependentsPerRoom\"] = test['Dependents']/test['rooms']\ntest[\"TeenagersPerRoom\"] = test['Teenagers']/test['rooms']\n\ntest[\"MalesPerRoom\"] = test['r4h3']/test['rooms']\ntest[\"FemalesPerRoom\"] = test['r4m3']/test['rooms']\n\ntest[\"BedroomPerRoom\"] =  test['bedrooms']/test['rooms']\ntest[\"RentPerRoom\"] =  test['v2a1']/test['rooms']\n\ntest[\"Schooling_Age\"]  = test[\"escolari\"]/test[\"age\"]\ntest['SchoolingProp'] = test['rez_esc']/test['escolari']\n\n#New Scoring For Education Level\ntest[\"EduLevel\"] = 0 \ntest.loc[test[\"instlevel9\"] == 1,\"EduLevel\"] = 6\ntest.loc[test[\"instlevel8\"] == 1,\"EduLevel\"] = 5 #higher scoring for completing tertiary education\ntest.loc[test[\"instlevel7\"] == 1,\"EduLevel\"] = 3\ntest.loc[test[\"instlevel5\"] == 1,\"EduLevel\"] = 2\ntest.loc[(test[[\"instlevel4\",\"instlevel3\",\"instlevel6\"]].sum(axis = 1) > 0),\"EduLevel\"] = 1\n\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Aggregate Features at the Household Level\n**Now we want to aggregate existing features to be representable at the household level.**   \n1. We aggregate variables in 'List_Mean' by taking the mean across all individuals in their respective household.\n2. We aggregate variables in 'List_Summary' by taking the mean, standard deviation, min, max, and sum across all individuals in their respective household.\n3. We added 'age_range' and 'escolari_range', which calculates the range of age and years of schooling across all individuals in their respective household.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"List_Mean = ['rez_esc', 'male', 'female', 'estadocivil1', 'estadocivil2', 'estadocivil3', 'estadocivil4', 'estadocivil5',\n             'estadocivil6', 'estadocivil7', 'parentesco2','parentesco3', 'parentesco4', 'parentesco5', 'parentesco6', 'parentesco7',\n             'parentesco8', 'parentesco9', 'parentesco10', 'parentesco11', 'parentesco12','instlevel1', 'instlevel2', 'instlevel3',\n             'instlevel4', 'instlevel5', 'instlevel6', 'instlevel7', 'instlevel8', 'instlevel9','overcrowding']\n\nList_Summary = ['age', 'escolari','dis','EduLevel', 'Schooling_Age']\n\ntrainGP = pd.DataFrame()\ntestGP = pd.DataFrame()\n\nfor item in List_Mean:\n    group_train_mean = train[item].groupby(train['idhogar']).mean()\n    group_test_mean = test[item].groupby(test['idhogar']).mean()\n    new_col = item + '_mean'\n    trainGP[new_col] = group_train_mean\n    testGP[new_col] = group_test_mean\n\nfor item in List_Summary:\n    for function in ['mean','std','min','max','sum']:\n        group_train = train[item].groupby(train['idhogar']).agg(function)\n        group_test = test[item].groupby(test['idhogar']).agg(function)\n        new_col = item + '_' + function\n        trainGP[new_col] = group_train\n        testGP[new_col] = group_test\n        \n#adding one final feature\ntrainGP['age_range']=trainGP['age_max']-trainGP['age_min']\ntestGP['age_range']=testGP['age_max']-testGP['age_min']\ntrainGP['escolari_range']=trainGP['escolari_max']-trainGP['escolari_min']\ntestGP['escolari_range']=testGP['escolari_max']-testGP['escolari_min']\n\n# Impute 0 to std columns (taking standard deviation on 1 value will yield NA)\ntrainGP[\"age_std\"] = trainGP[\"age_std\"].fillna(0)\ntrainGP[\"escolari_std\"] = trainGP[\"escolari_std\"].fillna(0)\ntrainGP[\"dis_std\"] = trainGP[\"dis_std\"].fillna(0)\ntrainGP[\"EduLevel_std\"] = trainGP[\"EduLevel_std\"].fillna(0)\ntrainGP[\"Schooling_Age_std\"] = trainGP[\"Schooling_Age_std\"].fillna(0)\n\ntestGP[\"age_std\"] = testGP[\"age_std\"].fillna(0)\ntestGP[\"escolari_std\"] = testGP[\"escolari_std\"].fillna(0)\ntestGP[\"dis_std\"] = testGP[\"dis_std\"].fillna(0)\ntestGP[\"EduLevel_std\"] = testGP[\"EduLevel_std\"].fillna(0)\ntestGP[\"Schooling_Age_std\"] = testGP[\"Schooling_Age_std\"].fillna(0)\n    \ntrainGP.head()\ntestGP.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Features about Living Conditions"},{"metadata":{},"cell_type":"markdown","source":"Here, we defined 3 Living Conditions: GoodCondition, AverageCondition and Bad Condition. These features were inspired based on the plots above."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Sufficiency Features (self-defined conditions for a sufficient living conditions)\n\n# =1 if predominant material on the outside wall is block or brick\n# =1 if predominant material on the floor is mosaic, ceramic, terrazo\n# =1 if the house has ceiling\n# =1 if water provision inside the dwelling\n# =1 electricity from CNFL,ICE, ESPH/JASEC\n# =1 toilet connected to sewer or cesspool\n# =1 main source of energy used for cooking electricity\n# =1 if rubbish disposal mainly by tanker truck\n# =1 if walls are good\n# =1 if roof are good\n# =1 if floor are good\n\ntrain[\"GoodCondition\"] = train[\"paredblolad\"] + \\\n                                train[\"pisomoscer\"] + \\\n                                train[\"cielorazo\"] + \\\n                                train[\"abastaguadentro\"] + \\\n                                train[\"public\"] + \\\n                                train[\"sanitario2\"] + \\\n                                train[\"energcocinar2\"] + \\\n                                train[\"elimbasu1\"] + \\\n                                train[\"epared3\"] + \\\n                                train[\"etecho3\"] + \\\n                                train[\"eviv3\"] \ntrain[\"GoodCondition\"] = train[\"GoodCondition\"]/11   # Take the mean to get a GoodCondition score between 0 and 1     \n\n# =1 if predominant material on the outside wall is socket (wood, zinc or abesto) OR prefabricated or cement\n# =1 if predominant material on the roof is metal foil or zink\n# =1 if water provision outside the dwelling\n# =1 electricity from cooperative\n# =1 toilet connected to  septic tank\n# =1 main source of energy used for cooking gas\n#  =1 if rubbish disposal mainly by botan hollow or buried\n# =1 if walls are regular\n#  =1 if roof are regular\n#  =1 if floor are regular\ntrain[\"AverageCondition\"] = (train[\"paredzocalo\"] + train[\"paredpreb\"])/2 + \\\n                                train[\"techozinc\"] + \\\n                                train[\"abastaguafuera\"] + \\\n                                train[\"coopele\"] + \\\n                                train[\"sanitario3\"] + \\\n                                train[\"energcocinar3\"] + \\\n                                train[\"elimbasu2\"] + \\\n                                train[\"epared2\"] + \\\n                                train[\"etecho2\"] + \\\n                                train[\"eviv2\"] \n\ntrain[\"AverageCondition\"] = train[\"AverageCondition\"]/10   # Take the mean to get a GoodCondition score between 0 and 1     \n\n\n# =1 if predominant material on the outside wall is waste material OR wood OR zinc\n# =1 if predominant material on the floor is cement OR wood OR no floor\n# =1 if no water provision\n# =1 no electricity in the dwelling\n# =1 no toilet in the dwelling OR toilet connected to black hole or letrine\n# =1 no main source of energy used for cooking (no kitchen) OR  =1 main source of energy used for cooking wood charcoal\n# =1 if rubbish disposal mainly by burning\n# =1 if walls are bad\n# =1 if roof are bad\n# =1 if floor are bad\ntrain[\"BadCondition\"] = (train[\"pareddes\"] + train[\"paredmad\"] + train[\"paredzinc\"])/3 + \\\n                        (train[\"pisocemento\"] + train[\"pisonotiene\"] + train[\"pisomadera\"])/3 + \\\n                        train[\"abastaguano\"] + \\\n                        train[\"noelec\"] + \\\n                        (train[\"sanitario1\"] + train[\"sanitario5\"])/2 + \\\n                        (train[\"energcocinar1\"] + train[\"energcocinar4\"])/2 + \\\n                        train[\"elimbasu3\"] + \\\n                        train[\"epared1\"] + \\\n                        train[\"etecho1\"] + \\\n                        train[\"eviv1\"]\n\ntrain[\"BadCondition\"] = train[\"BadCondition\"]/10   # Take the mean to get a GoodCondition score between 0 and 1     \n\n# add the 3 features to the test set\ntest[\"GoodCondition\"] = test[\"paredblolad\"] + \\\n                                test[\"pisomoscer\"] + \\\n                                test[\"cielorazo\"] + \\\n                                test[\"abastaguadentro\"] + \\\n                                test[\"public\"] + \\\n                                test[\"sanitario2\"] + \\\n                                test[\"energcocinar2\"] + \\\n                                test[\"elimbasu1\"] + \\\n                                test[\"epared3\"] + \\\n                                test[\"etecho3\"] + \\\n                                test[\"eviv3\"] \ntest[\"GoodCondition\"] = test[\"GoodCondition\"]/11   # Take the mean to get a GoodCondition score between 0 and 1     \n\ntest[\"AverageCondition\"] = (test[\"paredzocalo\"] + test[\"paredpreb\"])/2 + \\\n                                test[\"techozinc\"] + \\\n                                test[\"abastaguafuera\"] + \\\n                                test[\"coopele\"] + \\\n                                test[\"sanitario3\"] + \\\n                                test[\"energcocinar3\"] + \\\n                                test[\"elimbasu2\"] + \\\n                                test[\"epared2\"] + \\\n                                test[\"etecho2\"] + \\\n                                test[\"eviv2\"] \n\ntest[\"AverageCondition\"] = test[\"AverageCondition\"]/10   # Take the mean to get a GoodCondition score between 0 and 1     \n\ntest[\"BadCondition\"] = (test[\"pareddes\"] + test[\"paredmad\"] + test[\"paredzinc\"])/3 + \\\n                        (test[\"pisocemento\"] + test[\"pisonotiene\"] + test[\"pisomadera\"])/3 + \\\n                        test[\"abastaguano\"] + \\\n                        test[\"noelec\"] + \\\n                        (test[\"sanitario1\"] +test[\"sanitario5\"])/2 + \\\n                        (test[\"energcocinar1\"] + test[\"energcocinar4\"])/2 + \\\n                        test[\"elimbasu3\"] + \\\n                        test[\"epared1\"] + \\\n                        test[\"etecho1\"] + \\\n                        test[\"eviv1\"]\n\ntest[\"BadCondition\"] = test[\"BadCondition\"]/10   # Take the mean to get a GoodCondition score between 0 and 1     ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now we merge the aggregated features, all existing features and added features for the head of household.**\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"trainGP = trainGP.reset_index()\ntestGP = testGP.reset_index()\n\ntrainML = pd.merge(train, trainGP, on='idhogar')\ntestML = pd.merge(test, testGP, on='idhogar')\n\ntrainML = trainML.query(\"parentesco1==1\") #use only head of household for prediction\n\ntrainML.fillna(value=0, inplace=True)\ntestML.fillna(value=0, inplace=True)\ntrainML = trainML.replace([np.inf, -np.inf], 0)\ntestML = testML.replace([np.inf, -np.inf], 0)\n          \n\ntrain.shape, test.shape, trainGP.shape, testGP.shape,  trainML.shape, testML.shape ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Check for Missing Values before Modelling\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"trainML.isna().any().any(),testML.isna().any().any()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Record the ID for submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = testML.copy()[['Id']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Reduction\n**Remove features that are irrelevant such as 'idhogar','Id','tamhog','r4t3','hhsize','hogar_adul','edjefe', and 'edjefa'.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"trainML.drop(columns=['idhogar','Id','tamhog','r4t3','hhsize','hogar_adul','edjefe','edjefa'],inplace=True)\ntestML.drop(columns=['idhogar','Id','tamhog','r4t3','hhsize','hogar_adul','edjefe','edjefa'],inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Correlation of features variables with the Target"},{"metadata":{"trusted":true},"cell_type":"code","source":"correlation=trainML.corr()\ncorrelation = correlation['Target'].sort_values(ascending=False)\nprint(f'The most 20 positively correlated feature: \\n{correlation.head(20)}')\nprint('*'*50)\nprint(f'The most 20 negatively correlated feature: \\n{correlation.tail(20)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Notice that there is a few attributes with NaN. We will remove them.**\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"columns_remove = ['elimbasu5','estadocivil1','parentesco1','parentesco2','parentesco3','parentesco4','parentesco5','parentesco6','parentesco7','parentesco8','parentesco9','parentesco10','parentesco11','parentesco12']\ntrainML.drop(columns=columns_remove, inplace=True)\ntestML.drop(columns=columns_remove, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modelling\n**With the train and test data finally preprocessed.** \n**We will start by fitting a Support Vector Machine (SVM) model on the training data.**\n\n\n**Note that we will not split the training data into train and test set, because the test performance can be assessed by submitting the submission file to Kaggle.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = trainML.drop(columns=['Target'])\ny = trainML['Target']\n\nXtest = testML.copy()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nsvm_model = SVC(gamma='auto')\nsvm_model.fit(X, y) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_prediction_svm = svm_model.predict(Xtest)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We will also train a Random Forest Classifier on the training data.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\n#Create a Gaussian Classifier\nrf_model = RandomForestClassifier(n_estimators=100)\nrf_model.fit(X,y)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_prediction_rf = rf_model.predict(Xtest)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Next, We will train a Light GBM model with a few parameters adjusted from Misha Lisovyi.**    \n*source: <https://www.kaggle.com/mlisovyi/lighgbm-hyperoptimisation-with-f1-macro>*  "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import f1_score\n\ndef evaluate_macroF1_lgb(truth, predictions): \n    predictions = np.resize(predictions, new_shape=(4,len(truth)))\n    pred_labels = predictions.argmax(axis=0)\n    f1 = f1_score(truth, pred_labels, average='macro')\n    eval_name, val, is_higher_better = \"macroF1\", f1, True\n    return (eval_name, val, is_higher_better) \n\n#parameter value is copied\n# clf = lgb.LGBMClassifier(max_depth=-1, learning_rate=0.01, objective='multiclass',\n#                              random_state=123, silent=True, metric='None', \n#                              n_jobs=4, n_estimators=5000, class_weight='balanced',\n#                              colsample_bytree =  0.93, min_child_samples = 95, num_leaves = 15, subsample = 0.96)\nclf = lgb.LGBMClassifier(max_depth=12, learning_rate=0.005, objective='multiclass',\n                             random_state=69, silent=True, metric='None', \n                             n_jobs=4, n_estimators=2500, class_weight='balanced',\n                             colsample_bytree =  0.89, min_child_samples = 80, num_leaves = 14, subsample = 0.96)\n\nclf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Stratified K-Fold Cross-Validation\n**We will perform a Stratified K-Fold Cross Validation to train K = 5 Light GBM models and aggregate the predictions by means of 'hard voting' (majority votes).**  \n**We use Stratified K-Fold Cross-Validation here because the Target is unbalanced. Stratification ensure that each fold is representative of all strata of the data. So, this ensures that each class is (approximately) equally represented across each test fold.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"kfold = 5\nkf = StratifiedKFold(n_splits=kfold, shuffle=True, random_state = 123)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Quick check to make sure the train and test dataset have the same columns size**"},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape, Xtest.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train the Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"val_predictions_logloss = []\ntest_predictions_logloss = []\ntest_predictions_logloss_proba = []\nfor train_index, val_index in kf.split(X, y):\n    print(\"=======\")\n    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n    clf.fit(X_train, y_train, eval_set=[(X_val, y_val)], eval_metric=\"logloss\",\n            early_stopping_rounds=400, verbose=200)\n    val_predictions_logloss.append(clf.predict(X_val)) \n    test_predictions_logloss.append(clf.predict(Xtest)) # store predictions on test set for Kaggle Submission\n    test_predictions_logloss_proba.append(clf.predict_proba(Xtest)) # store predictions probability on test set for Kaggle Submission\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Features Importance\n**We visualize the features importance generated from the trained Light GBM model.**\n**The top 100 features were plotted.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"indices = np.argsort(clf.feature_importances_)[::-1]\ntop_100_indices = indices[:100]\n\n# Visualise these with a barplot\nplt.subplots(figsize=(20, 15))\ng = sns.barplot(y=X.columns[top_100_indices], x = clf.feature_importances_[top_100_indices], orient='h')\ng.set_xlabel(\"Relative importance\",fontsize=12)\ng.set_ylabel(\"Features\",fontsize=12)\ng.tick_params(labelsize=9)\ng.set_title(\"LightGBM feature importance\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**From the plot above, we understand that the following variables (and their derived forms) are among the most important variables on predicting the poverty level of a household.** \n1. Years of Schooling\n2. Age\n3. Education Level\n4. Ownership of appliances\n5. Ownership of phones/tablets\n6. Overcrowding\n7. Number of Dependencies\n8. House conditions \n9. Rent\n"},{"metadata":{},"cell_type":"markdown","source":"# Feature Selection\n**We will attempt to re-train the model by selecting the top 100 features based on the previously trained Light GBM model features importance output. The intuition is to remove variables that have very little influence in predicting the Target (poverty level) and give the model more focus on the important variables.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"X2 = X.iloc[:,top_100_indices]\nXtest2 = testML.copy().iloc[:,top_100_indices]\n\n# Re-training on top 100 features\ntest_predictions2 = []\ntest_predictions_proba2 = []\nfor train_index, val_index in kf.split(X2, y):\n    print(\"=======\")\n    X_train, X_val = X2.iloc[train_index], X2.iloc[val_index]\n    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n    clf.fit(X_train, y_train, eval_set=[(X_val, y_val)], eval_metric=\"logloss\",\n            early_stopping_rounds=400, verbose=200)\n    test_predictions2.append(clf.predict(Xtest2)) # store predictions on test set for Kaggle Submission\n    test_predictions_proba2.append(clf.predict_proba(Xtest2)) \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Evaluation"},{"metadata":{},"cell_type":"markdown","source":"## Test Predictions"},{"metadata":{},"cell_type":"markdown","source":"**Finally, we will write the test predictions predicted from the SVM model, Random Forest Classifier, Light GBM using all the features, and Light GBM using the top-100 features to 4 csv files for submission on Kaggle**\n<https://www.kaggle.com/c/costa-rican-household-poverty-prediction/submissions>\n\n\n**For Light GBM predictions, we took the mode of the predictions made by the 5 Light GBM models that we trained earlier using 5-fold Stratified Cross Validation. Taking the mode is also known as \"hard\" voting.**  \n**We also examine the proportion of the predicted Target.**\n"},{"metadata":{},"cell_type":"markdown","source":"**The performance is evaluated based on macro F1 score.**  \nF-measure source: <https://www.toyota-ti.ac.jp/Lab/Denshi/COIN/people/yutaka.sasaki/F-measure-YS-26Oct07.pdf>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# SVM Model Predictions\nsubmission_svm = submission.copy()\nsubmission_svm['Target'] = np.array(test_prediction_svm).astype(int)\nsubmission_svm.to_csv(\"submission_01.csv\", index=False)\nprint(submission_svm['Target'].value_counts()/submission_svm['Target'].value_counts().sum())\nprint(\"The Macro F1 Score (Test) is: 0.28218\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://i.imgur.com/EN4j8aR.jpg\" alt=\"Site Logo\" width=\"750\" height=\"500\">"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random Forest Model Predictions\nsubmission_rf = submission.copy()\nsubmission_rf['Target'] = np.array(test_prediction_rf).astype(int)\nsubmission_rf.to_csv(\"submission_02.csv\", index=False)\nprint(submission_rf['Target'].value_counts()/submission_rf['Target'].value_counts().sum())\nprint(\"The Macro F1 Score (Test) is: 0.37434\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://imgur.com/WNddSnG.jpg\" alt=\"Site Logo\" width=\"750\" height=\"500\">"},{"metadata":{"trusted":true},"cell_type":"code","source":"# All Features Light GBM Model Predictions\nsubmission_full_logloss = submission.copy()\nvoted_test_predictions = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=test_predictions_logloss) # Take the mode\nsubmission_full_logloss['Target'] = np.array(voted_test_predictions).astype(int)\nsubmission_full_logloss.to_csv(\"submission_03.csv\", index=False)\nprint(submission_full_logloss['Target'].value_counts()/submission_full_logloss['Target'].value_counts().sum())\nprint(\"The Macro F1 Score (Test) is: 0.43970\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://imgur.com/0HQxoaY.jpg\" alt=\"Site Logo\" width=\"750\" height=\"500\">"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Top-100 Features Light GBM Model Predictions\nsubmission_top100_logloss = submission.copy()\nvoted_test_predictions = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=test_predictions2) # Take the mode\nsubmission_top100_logloss['Target'] = np.array(voted_test_predictions).astype(int)\nsubmission_top100_logloss.to_csv(\"submission_04.csv\", index=False)\nprint(submission_top100_logloss['Target'].value_counts()/submission_top100_logloss['Target'].value_counts().sum())\nprint(\"The Macro F1 Score (Test) is: 0.44019\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://i.imgur.com/ZccyTHj.jpg\" alt=\"Site Logo\" width=\"750\" height=\"500\">"},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\n\nWe find that our Light GBM model trained on all features performed the best, with a macro F1 score of 0.44019. This placed us at 45th place on the public leaderboard.\n<img src=\"https://i.imgur.com/CKTP0Tf.jpg\" alt=\"Site Logo\" width=\"750\" height=\"500\">\n\nOverall, the highest score in this Kaggle competition is slightly below 0.5, that is hardly good. \n\nOne of the particpants, <a href=\"https://www.kaggle.com/willkoehrsen/a-complete-introduction-and-walkthrough\">Will Koehrsen</a> has plotted a PCA plot and mentioned:\n   <body>\n      <img src=\"https://imgur.com/KhtcZPy.jpg\" alt=\"Site Logo\" width=\"300\" height=\"300\">\n   </body>\n   <blockquote>It's difficult to see much clustering in these plots, which indicates that separating the poverty levels is hard given the available data. Machine learning models are only as effective as the data we give to them, and sometimes the data does not have enough information.</blockquote>\n\nIt is true that based on the exploration on the data that we have performed, most of the attributes (toal 142 columns) are not significant in helping with predicting poverty class. On the original data given, there are many discrepancy in the data and raise some doubts on the quality of the data collected. \n\nOn the bright side, all good things have to start from step one. This competition had combined the bright minds on kaggle and brought about great insights and perspective on what features might be important and what might be not. The highest scoring kernels are consistenly product of a few machine learning methods such as Light GBM and XGboost. While the list of features importance varies accross machine learning models and on how participants preprocess their data, there are a few that frequent the top feature importance list such as:<br>\n1) mean education<br>\n2) dependency<br>\n\nRoads are not built in a day. With the knowledge sharing available on kaggle kernels, The Inter-American Development Bank (IDB)  organizer can make data driven decision on what data to collect and what not to collect. That will help them to place more emphasis on the more important task and hopefully develop a much better set of data through selective collection. "}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"}},"nbformat":4,"nbformat_minor":1}