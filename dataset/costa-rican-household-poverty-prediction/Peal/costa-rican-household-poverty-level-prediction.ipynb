{"cells":[{"metadata":{},"cell_type":"markdown","source":"[](http://)Costa Rican Household Poverty Level Prediction : This is a multi-class classification problem.\nThe target variable has 4 different classfication of poverty levels - Extreme, Moderate, Vulnerable and non-vulnerable.\nThe dataset has different rows for every individual member of the household and the head  of the household is identified with one of the predictors : parentesco1 = 1.\nObjective is to predict the target class (poverty level) using feature engineering and a combination of various models while tuning the  hyper parameters using different techniques like Grid Search, Random Search, Bayesian. "},{"metadata":{},"cell_type":"markdown","source":"[Invoking Libraries/classes/functions](http://)"},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime\nimport math\n\nimport time\nimport os\nimport gc\nimport random\nfrom scipy.stats import uniform\n#  Processing data\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import  OneHotEncoder as ohe\nfrom sklearn.preprocessing import StandardScaler as ss\nfrom sklearn.compose import ColumnTransformer as ct\n\n# Data imputation\nfrom sklearn.impute import SimpleImputer\n\nfrom sklearn.decomposition import PCA\nfrom sklearn import metrics\n\nimport scikitplot as skplt\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import average_precision_score\nimport sklearn.metrics as metrics\nfrom sklearn.metrics import classification_report\n\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.metrics import f1_score, make_scorer\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\n\n\nfrom xgboost.sklearn import XGBClassifier\nfrom bayes_opt import BayesianOptimization\n\nfrom skopt import BayesSearchCV\n#  Model pipelining\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import make_pipeline\n\n\n# 1.7 Model evaluation metrics\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import auc, roc_curve\n\n# 1.8\nimport matplotlib.pyplot as plt\nfrom xgboost import plot_importance\n\n# 1.9 Needed for Bayes optimization\nfrom sklearn.model_selection import cross_val_score\n\nfrom bayes_opt import BayesianOptimization\n\n\nfrom sklearn.model_selection import StratifiedKFold\nimport lightgbm as lgb\n\n# 3.6 Change ipython options to display all data columns\npd.options.display.max_columns = 400","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"[Loading Data](http://)\n1) Loading Train and test dataset.\n2) Updating target column in Test dataset with null value"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 2.1 Read cdata/test files\ntrain = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")\n\nprint(train.shape)    #9557 X 143\nprint(test.shape)     # 23856 X 142\n\ntrain.dtypes\ntest.dtypes\n#train.columns\ntrain_col_names = list(train.columns)\ntest_col_names = list(test.columns)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The test data does not have a target column. Since we are planning to do  feature engineering on this dataset, the objective is to combine train and test into a single data frame so that all the actions are propoagated in both the test and train data. Subsquently we will be splitting the data set into train and test\nAlso verifying the datatypes of the dataset reveals that 130 columns are int64, 8 of them are float and 5 columns are object"},{"metadata":{"trusted":true},"cell_type":"code","source":"### adding Target column  to test with null value\ntest['Target'] = np.nan\ntest.dtypes\ncdata = pd.concat([train,test],\n                axis = 0,            # Stack one upon another (rbind)\n                ignore_index = True\n                )\nprint(cdata.shape)     #33413 X 143\n\nprint(train.dtypes.value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"[Feature Engineering](http://)"},{"metadata":{},"cell_type":"markdown","source":"Feature Engineering\nStep 1\nThe dataset has mutiple columns with binary values of '0' and '1' for the corresponding attributes. For e,g the area attribute for each resident in the household is classified as 'Urban' & \"Rural' with 2 binary columns area1(Urban)  = 0 / 1   and area2 (Rural) = 0/1 \nAs a first step , we can combine these 2 binary columns into a single column for area_definition with a nominal value of '1' for Urban and '2' for Rural. Likewise for all the binary features, the multiple columns can be combined into a single column with mapping definition for the actual values.\nObjective\n1) Combining multiple columns with binary values to a single column with nominal values - reducing features . While doing so , it is important to check if the values are undefined for all the binary variables for any specific row and in which case a strategy needs to be adopted to manually insert a value for the rows with missing values\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"##############################################################################\n## combine 8 binary features for level of education ( 9 levels)  into a single column instlevel\n###############################################################################\ndel_columns =[]  #initializing list of columns to be combined\ncat_columns = [] # initializing a list of Categorical columns that will be needed for PCA\n#Checking if the columns are undefined for any row.\n((cdata['instlevel1'] + cdata['instlevel2'] + cdata['instlevel3'] + cdata['instlevel4'] +\n  cdata['instlevel5'] + cdata['instlevel6'] + cdata['instlevel7'] + cdata['instlevel8'] + \n  cdata['instlevel9']) == 0).sum() \n## 3 rows does not have instlevel defined for any rows\ncdata.loc[((cdata['instlevel1'] + cdata['instlevel2'] + cdata['instlevel3'] + cdata['instlevel4'] +\n            cdata['instlevel5'] + cdata['instlevel6'] + cdata['instlevel7'] + cdata['instlevel8'] + \n            cdata['instlevel9']) == 0)].rez_esc\n##escloari - years of schooling is 0, rez_esc is nan, age is 44,45 & 97 years for all the 3 rows, Target is 3 & 4\n### meaneduc is 4, 4 & 3 years for these 3 records,  Safe to classify these records as 2 - Incomplete Primary   \ncdata.loc[((cdata['instlevel1'] + cdata['instlevel2'] + cdata['instlevel3'] + cdata['instlevel4'] +\n            cdata['instlevel5'] + cdata['instlevel6'] + cdata['instlevel7'] + cdata['instlevel8'] + \n            cdata['instlevel9']) == 0),['age','escolari','rez_esc','meaneduc','Target']]\ncdata.loc[((cdata['instlevel1'] + cdata['instlevel2'] + cdata['instlevel3'] + cdata['instlevel4'] +\n            cdata['instlevel5'] + cdata['instlevel6'] + cdata['instlevel7'] + cdata['instlevel8'] + \n            cdata['instlevel9']) == 0),'instlevel2'] = 1\neducation_mapping = ({1:'No education', 2: 'Incomplete primary', 3 :'Complete Primary', 4 :'Incomplete secondary', 5: 'Complete secondary', 6: 'Incomplete technical Secondary' , 7: 'Complete technical Secondary',8 : 'Undergraduate and higher', 9: 'Postgraduate and higher'})\ncdata.loc[(cdata['instlevel1'] == 1), 'instlevel'] = 1\ncdata.loc[(cdata['instlevel2'] == 1), 'instlevel'] = 2\ncdata.loc[(cdata['instlevel3'] == 1), 'instlevel'] = 3\ncdata.loc[(cdata['instlevel4'] == 1), 'instlevel'] = 4\ncdata.loc[(cdata['instlevel5'] == 1), 'instlevel'] = 5\ncdata.loc[(cdata['instlevel6'] == 1), 'instlevel'] = 6\ncdata.loc[(cdata['instlevel7'] == 1), 'instlevel'] = 7\ncdata.loc[(cdata['instlevel8'] == 1), 'instlevel'] = 8\ncdata.loc[(cdata['instlevel9'] == 1), 'instlevel'] = 9\n\ndel_columns = del_columns + ['instlevel1', 'instlevel2','instlevel3', 'instlevel4','instlevel5']\ndel_columns = del_columns + ['instlevel6', 'instlevel7','instlevel8', 'instlevel9']\ncat_columns = ['instlevel']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###############################################################################\n## combine 12 binary features for level of education ( 9 levels)  into a single column instlevel\n###############################################################################\n(cdata['parentesco1'] ==1).sum() - 2973\n## Checking if values are defined for more than 1 of these columns\n((cdata['parentesco1'] + cdata['parentesco2'] + cdata['parentesco3'] + cdata['parentesco4'] +\n  cdata['parentesco5'] + cdata['parentesco6'] + cdata['parentesco7'] + cdata['parentesco8'] + \n  cdata['parentesco9'] + cdata['parentesco10'] + cdata['parentesco11'] + cdata['parentesco12'] ) > 1).sum() \n\n##define the mapping values \nfamily_mapping = ({1: 'Household head', 2 : 'Spouse/Partner', 3: 'Son/daughter', 4 : 'Stepson/daughter', 5: 'Son/Daughter in law', 6: 'Grandson/Daughter', 7:'Mother/Father', 8:'Father/Mother in law', 9: 'Brother/Sister', 10: 'Brother/Sister in law', 11: 'Other familymember', 12 :'Non family member'})\ncdata.loc[(cdata['parentesco1'] == 1), 'parent_level'] = 1\ncdata.loc[(cdata['parentesco2'] == 1), 'parent_level'] = 2\ncdata.loc[(cdata['parentesco3'] == 1), 'parent_level'] = 3\ncdata.loc[(cdata['parentesco4'] == 1), 'parent_level'] = 4\ncdata.loc[(cdata['parentesco5'] == 1), 'parent_level'] = 5\ncdata.loc[(cdata['parentesco6'] == 1), 'parent_level'] = 6\ncdata.loc[(cdata['parentesco7'] == 1), 'parent_level'] = 7\ncdata.loc[(cdata['parentesco8'] == 1), 'parent_level'] = 8\ncdata.loc[(cdata['parentesco9'] == 1), 'parent_level'] = 9\ncdata.loc[(cdata['parentesco10'] == 1), 'parent_level'] = 10\ncdata.loc[(cdata['parentesco11'] == 1), 'parent_level'] = 11\ncdata.loc[(cdata['parentesco12'] == 1), 'parent_level'] = 12\n\ndel_columns = del_columns + ['parentesco1', 'parentesco2','parentesco3', 'parentesco4','parentesco5']\ndel_columns = del_columns + ['parentesco6', 'parentesco7','parentesco8', 'parentesco9','parentesco10']\ndel_columns = del_columns + ['parentesco11', 'parentesco12']\ncat_columns = cat_columns + ['parent_level']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"######################################################################################\n###########combine 7 binary features for Marital Status ( 7 levels)  into a single column mar_sts\n#####################################################################################\n## Checking if values are defined for more than 1 of these columns\n((cdata['estadocivil1'] + cdata['estadocivil2'] + cdata['estadocivil3'] + cdata['estadocivil4'] +\n  cdata['estadocivil5'] + cdata['estadocivil6'] + cdata['estadocivil7']  ) > 1).sum() \n\nMar_sts_mapping = ({1: 'Less than 10', 2 : 'Free or Coupled', 3: 'Married', 4 : 'Divorced', 5: 'Separated', 6: 'Widow-er', 7:'Single'})\ncdata.loc[(cdata['estadocivil1'] == 1), 'mar_sts'] = 1\ncdata.loc[(cdata['estadocivil2'] == 1), 'mar_sts'] = 2\ncdata.loc[(cdata['estadocivil3'] == 1), 'mar_sts'] = 3\ncdata.loc[(cdata['estadocivil4'] == 1), 'mar_sts'] = 4\ncdata.loc[(cdata['estadocivil5'] == 1), 'mar_sts'] = 5\ncdata.loc[(cdata['estadocivil6'] == 1), 'mar_sts'] = 6\ncdata.loc[(cdata['estadocivil7'] == 1), 'mar_sts'] = 7\ndel_columns = del_columns + ['estadocivil1', 'estadocivil2', 'estadocivil3', 'estadocivil4','estadocivil5','estadocivil6', 'estadocivil7']\ncat_columns = cat_columns + ['mar_sts']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##############################################################################\n## combine 6 binary features for Region ( lugar1 to lugar6)  into a single column lugar\n##################################################################################\nregion_mapping = ({1: 'Central', 2: 'Chorotega', 3: 'PacÃ­fico central', 4: 'Brunca' , 5 : 'Huetar AtlÃ¡ntica', 6:'Huetar Norte' })\nRegion  = ['Central', 'Chorotega', 'PacÃ­fico central','Brunca', 'Huetar AtlÃ¡ntica', 'Huetar Norte']\n\ncdata.loc[(cdata['lugar1'] == 1), 'lugar'] = 1\ncdata.loc[(cdata['lugar2'] == 1), 'lugar'] = 2\ncdata.loc[(cdata['lugar3'] == 1), 'lugar'] = 3\ncdata.loc[(cdata['lugar4'] == 1), 'lugar'] = 4\ncdata.loc[(cdata['lugar5'] == 1), 'lugar'] = 5\ncdata.loc[(cdata['lugar6'] == 1), 'lugar'] = 6\ndel_columns = del_columns + ['lugar1', 'lugar2', 'lugar3', 'lugar4','lugar5','lugar6']\ncat_columns = cat_columns + ['lugar']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##############################################################################\n## combine 2 binary features for Area ( area1 & area2)  into a single column area\n## Mapping for Urban and Rural Zones\n###############################################################################\narea_mapping = ({1: 'Urban', 2: 'Rural'})\nArea  = ['Urban', 'Rural']\ncdata.loc[(cdata['area1'] == 1), 'area'] = 1\ncdata.loc[(cdata['area2'] == 1), 'area'] = 2\nsns.countplot(cdata.area.map(area_mapping), data = cdata) ## distribution by Zone - urban / rural\ndel_columns = del_columns + ['area1', 'area2']\ncat_columns = cat_columns + ['area']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##############################################################################\n## combine 5 binary features for house ownership status  ( tipoviv11 to tipovivi5)  into a single column lugar\n##################################################################################\n## Checking if values are defined for more than 1 of these columns\n((cdata['tipovivi1'] + cdata['tipovivi2'] + cdata['tipovivi3'] + cdata['tipovivi4'] +\n  cdata['tipovivi5']  ) > 1).sum() \nhouse_mapping = ({1: 'Owned and paid', 2: 'Owned and paying install', 3: 'Rented', 4: 'Precarious' , 5 : 'Other' })\ncdata.loc[(cdata['tipovivi1'] == 1), 'tipovivi'] = 1\ncdata.loc[(cdata['tipovivi2'] == 1), 'tipovivi'] = 2\ncdata.loc[(cdata['tipovivi3'] == 1), 'tipovivi'] = 3\ncdata.loc[(cdata['tipovivi4'] == 1), 'tipovivi'] = 4\ncdata.loc[(cdata['tipovivi5'] == 1), 'tipovivi'] = 5\n\ndel_columns = del_columns + ['tipovivi1', 'tipovivi2', 'tipovivi3','tipovivi4', 'tipovivi5']\ncat_columns = cat_columns + ['tipovivi']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##############################################################################\n## combine 5 binary features for sanitation status  ( sanitario1 to sanitario6)  into a single column lugar\n##################################################################################\n## Checking if values are defined for more than 1 of these columns\n((cdata['sanitario1'] + cdata['sanitario2'] + cdata['sanitario3'] + cdata['sanitario5'] +\n  cdata['sanitario6']  ) > 1).sum()\nsanit_mapping = ({1: 'No Toilet', 2: 'Connected to Sewer', 3: 'Connected to Septic T', 4: 'Connected to BH/L' , 5 : 'Connected to Others' })\ncdata.loc[(cdata['sanitario1'] == 1), 'sanitario'] = 1\ncdata.loc[(cdata['sanitario2'] == 1), 'sanitario'] = 2\ncdata.loc[(cdata['sanitario3'] == 1), 'sanitario'] = 3\ncdata.loc[(cdata['sanitario5'] == 1), 'sanitario'] = 4\ncdata.loc[(cdata['sanitario6'] == 1), 'sanitario'] = 5\n\ndel_columns = del_columns + ['sanitario1', 'sanitario2', 'sanitario3','sanitario5', 'sanitario6']\ncat_columns = cat_columns + ['sanitario']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##############################################################################\n## combine 5 binary features for cooking energy source  ( energcocinar1 to energcocinar4)  into a single column energy\n##################################################################################\n## Checking if values are defined for more than 1 of these columns\n((cdata['energcocinar1'] + cdata['energcocinar2'] + cdata['energcocinar3'] + cdata['energcocinar4']) > 1).sum()\ncook_energy_mapping = ({1: 'No Kitchen', 2: 'Electricity', 3: 'Cooking gas', 4: 'Charcoal'  })\ncdata.loc[(cdata['energcocinar1'] == 1), 'energco'] = 1\ncdata.loc[(cdata['energcocinar2'] == 1), 'energco'] = 2\ncdata.loc[(cdata['energcocinar3'] == 1), 'energco'] = 3\ncdata.loc[(cdata['energcocinar4'] == 1), 'energco'] = 4\n\n\ndel_columns = del_columns + ['energcocinar1', 'energcocinar2', 'energcocinar3','energcocinar4']\ncat_columns = cat_columns + ['energco']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##############################################################################\n## combine 6 binary features for rubbish disposal  ( elimbasu1 to elimbasu1)  into a single column elimbasu\n##################################################################################\n## Checking if values are defined for more than 1 of these columns\n((cdata['elimbasu1'] + cdata['elimbasu2'] + cdata['elimbasu3'] + cdata['elimbasu4'] +\n  cdata['elimbasu5'] + cdata['elimbasu6'] ) > 1).sum()\nrubbish_mapping = ({1: 'tanker truck', 2: 'hollow or buried', 3: 'burning', 4: 'throwing in UOC space' , 5 : 'throwing in river' , 6 :'Other'})\ncdata.loc[(cdata['elimbasu1'] == 1), 'elimbasu'] = 1\ncdata.loc[(cdata['elimbasu2'] == 1), 'elimbasu'] = 2\ncdata.loc[(cdata['elimbasu3'] == 1), 'elimbasu'] = 3\ncdata.loc[(cdata['elimbasu4'] == 1), 'elimbasu'] = 4\ncdata.loc[(cdata['elimbasu5'] == 1), 'elimbasu'] = 5\ncdata.loc[(cdata['elimbasu6'] == 1), 'elimbasu'] = 6\ndel_columns = del_columns + ['elimbasu1', 'elimbasu2', 'elimbasu3','elimbasu4', 'elimbasu5', 'elimbasu6']\ncat_columns = cat_columns + ['elimbasu']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##############################################################################\n## combine 2 binary features for male/ female  ( into a single column sex\n##################################################################################\nsex_mapping = ({1: 'Male', 2: 'Female'})\ncdata.loc[(cdata['male'] == 1), 'sex'] = 1\ncdata.loc[(cdata['female'] == 1), 'sex'] = 2\n\ndel_columns = del_columns + ['male', 'female']\ncat_columns = cat_columns + ['sex']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"######################################################################################\n###########combine 8 binary features for Predominant material on outside wall ( 8 levels)  into a single column out_wall\n#####################################################################################\n## Checking if values are defined for more than 1 of these columns\n((cdata['paredblolad'] + cdata['paredzocalo'] + cdata['paredpreb'] + cdata['pareddes'] +\n  cdata['paredmad'] + cdata['paredzinc'] + cdata['paredfibras'] + cdata['paredother'] ) > 1).sum() \n\nwall_mat_mapping = ({1: 'block or brick', 2 : 'Socket', 3: 'pre-fab or cement', 4 : 'waste material', 5: 'Wood', 6: 'Zinc', 7:'natural fiber',8:'Other'})\ncdata.loc[(cdata['paredblolad'] == 1), 'wall_mat'] = 1\ncdata.loc[(cdata['paredzocalo'] == 1), 'wall_mat'] = 2\ncdata.loc[(cdata['paredpreb'] == 1), 'wall_mat'] = 3\ncdata.loc[(cdata['pareddes'] == 1), 'wall_mat'] = 4\ncdata.loc[(cdata['paredmad'] == 1), 'wall_mat'] = 5\ncdata.loc[(cdata['paredzinc'] == 1), 'wall_mat'] = 6\ncdata.loc[(cdata['paredfibras'] == 1), 'wall_mat'] = 7\ncdata.loc[(cdata['paredother'] == 1), 'wall_mat'] = 8\n\ndel_columns = del_columns + ['paredblolad', 'paredzocalo', 'paredpreb', 'pareddes','paredmad','paredzinc', 'paredfibras','paredother']\ncat_columns = cat_columns + ['wall_mat']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"######################################################################################\n###########combine 6 binary features for Floor material ( 6 levels)  into a single column out_wall\n#####################################################################################\n## Checking if values are defined for more than 1 of these columns\n((cdata['pisomoscer'] + cdata['pisocemento'] + cdata['pisoother'] + cdata['pisonatur'] +\n  cdata['pisonotiene'] + cdata['pisomadera']  ) > 1).sum() \n\nfloor_mat_mapping = ({1: 'Mosaic/Ceramic', 2 : 'Cement', 3: 'Other', 4 : 'Natural material', 5: 'No Floor', 6: 'Wood'})\ncdata.loc[(cdata['pisomoscer'] == 1), 'floor_mat'] = 1\ncdata.loc[(cdata['pisocemento'] == 1), 'floor_mat'] = 2\ncdata.loc[(cdata['pisoother'] == 1), 'floor_mat'] = 3\ncdata.loc[(cdata['pisonatur'] == 1), 'floor_mat'] = 4\ncdata.loc[(cdata['pisonotiene'] == 1), 'floor_mat'] = 5\ncdata.loc[(cdata['pisomadera'] == 1), 'floor_mat'] = 6\n\ndel_columns = del_columns + ['pisomoscer', 'pisocemento', 'pisoother', 'pisonatur','pisonotiene','pisomadera']\ncat_columns = cat_columns + ['floor_mat']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"######################################################################################\n###########combine 4 binary features for Roof material ( 6 levels)  into a single column out_wall\n#####################################################################################\n## Checking if values are defined for more than 1 of these columns\n((cdata['techozinc'] + cdata['techoentrepiso'] + cdata['techocane'] + cdata['techootro']  ) > 1).sum() \n\n((cdata['techozinc'] + cdata['techoentrepiso'] + cdata['techocane'] + cdata['techootro']  ) == 0).sum() ## 66 rows with undefined\n\ncdata.loc[(cdata['techozinc'] + cdata['techoentrepiso'] + cdata['techocane'] + cdata['techootro'] ==0  ),['tipovivi4','Target']]\n\ncdata.loc[(cdata['techozinc'] + cdata['techoentrepiso'] + cdata['techocane'] + cdata['techootro'] ==0)].tipovivi.value_counts()  # 40 in precarious\ncdata.loc[(cdata['techozinc'] + cdata['techoentrepiso'] + cdata['techocane'] + cdata['techootro'] ==0)].Target.value_counts()  # 50 in moderate/extreme poverty\n\n\ncdata[((cdata.Target == 1) | (cdata.Target == 2))].techootro.value_counts().nunique()  ## Roof material value = 1 \n## can update techootro to 1 for these 66 rows\ncdata.loc[((cdata['techozinc'] + cdata['techoentrepiso'] + cdata['techocane'] + cdata['techootro']  ) == 0),'techootro'] = 1\n\nroof_mat_mapping = ({1: 'Metal foil/Zinc', 2 : 'Fiber Cement', 3: 'Natural Fiber', 4 : 'Other'})\ncdata.loc[(cdata['techozinc'] == 1), 'roof_mat'] = 1\ncdata.loc[(cdata['techoentrepiso'] == 1), 'roof_mat'] = 2\ncdata.loc[(cdata['techocane'] == 1), 'roof_mat'] = 3\ncdata.loc[(cdata['techootro'] == 1), 'roof_mat'] = 4\n\n\ndel_columns = del_columns + ['techozinc', 'techoentrepiso', 'techocane', 'techootro']\ncat_columns = cat_columns + ['roof_mat']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"######################################################################################\n###########combine 3 binary features for Water Provision ( 3 levels)  into a single column \n#####################################################################################\n## Checking if values are defined for more than 1 of these columns\n((cdata['abastaguadentro'] + cdata['abastaguafuera'] + cdata['abastaguano']  ) > 1).sum() \n\nwater_sts_mapping = ({1: 'Inside', 2 : 'Outside', 3: 'no water provision'})\ncdata.loc[(cdata['abastaguadentro'] == 1), 'water_sts'] = 1\ncdata.loc[(cdata['abastaguafuera'] == 1), 'water_sts'] = 2\ncdata.loc[(cdata['abastaguano'] == 1), 'water_sts'] = 3\n\ndel_columns = del_columns + ['abastaguadentro', 'abastaguafuera', 'abastaguano']\ncat_columns = cat_columns + ['water_sts']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"######################################################################################\n###########combine 3 binary features for Electricity( 4 levels)  into a single column \n#####################################################################################\n## Checking if values are defined for more than 1 of these columns\n((cdata['public'] + cdata['planpri'] + cdata['noelec']  + cdata['coopele']) > 1).sum() \n##  Checking if these columns are undefined for any rows\n((cdata['public'] + cdata['planpri'] + cdata['noelec'] + cdata['coopele']) == 0).sum()   #15 rows with undefined  \ncdata.loc[((cdata['public'] + cdata['planpri'] + cdata['noelec'] + cdata['coopele']) == 0),['tipovivi1','Target']]\ncdata.loc[((cdata['public'] + cdata['planpri'] + cdata['noelec'] + cdata['coopele']) == 0)].tipovivi1.value_counts()  ##10 rows Own house\ncdata.loc[((cdata['public'] + cdata['planpri'] + cdata['noelec'] + cdata['coopele']) == 0)].Target.value_counts() ## 9 + 4 in 1 & 2\ncdata.public.value_counts()  ## 8459 rows 88%  with public\ncdata.planpri.value_counts()  ##  3 rows\ncdata.noelec.value_counts()   ## 21 rows\ncdata.coopele.value_counts()  ## 1059 rows\ncdata[((cdata.Target == 1) | (cdata.Target == 2))].public.value_counts()\n##updating with majority -public\ncdata.loc[((cdata['public'] + cdata['planpri'] + cdata['noelec'] + cdata['coopele']) == 0),['public']] = 1\nelectricity_sts_mapping = ({1: 'CNFL,ICE,ESPH/JASEC', 2 : 'Private', 3: 'Cooperative' , 4: 'None'})\ncdata.loc[(cdata['public'] == 1), 'elect_sts'] = 1\ncdata.loc[(cdata['planpri'] == 1), 'elect_sts'] = 2\ncdata.loc[(cdata['coopele'] == 1), 'elect_sts'] = 3\ncdata.loc[(cdata['noelec'] == 1), 'elect_sts'] = 4\n\n\ndel_columns = del_columns + ['public', 'planpri', 'coopele', 'noelec']\ncat_columns = cat_columns + ['elect_sts']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"######################################################################################\n###########combine 3 binary features for wall condition( 3 levels)  into a single column \n#####################################################################################\n## Checking if values are defined for more than 1 of these columns\n((cdata['epared1'] + cdata['epared2'] + cdata['epared3']  ) > 1).sum() \n\nwall_sts_mapping = ({1: 'Bad', 2 : 'Regular', 3: 'Good'})\ncdata.loc[(cdata['epared1'] == 1), 'wall_sts'] = 1\ncdata.loc[(cdata['epared2'] == 1), 'wall_sts'] = 2\ncdata.loc[(cdata['epared3'] == 1), 'wall_sts'] = 3\n\ndel_columns = del_columns + ['epared1', 'epared2', 'epared3']\ncat_columns = cat_columns + ['wall_sts']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"######################################################################################\n###########combine 3 binary features for roof  condition( 3 levels)  into a single column \n#####################################################################################\n## Checking if values are defined for more than 1 of these columns\n((cdata['etecho1'] + cdata['etecho2'] + cdata['etecho3']  ) > 1).sum() \n##  Checking if these columns are undefined for any rows\n((cdata['etecho1'] + cdata['etecho2'] + cdata['etecho3']  ) == 0).sum() \nroof_sts_mapping = ({1: 'Bad', 2 : 'Regular', 3: 'Good'})\ncdata.loc[(cdata['etecho1'] == 1), 'roof_sts'] = 1\ncdata.loc[(cdata['etecho2'] == 1), 'roof_sts'] = 2\ncdata.loc[(cdata['etecho3'] == 1), 'roof_sts'] = 3\n\ndel_columns = del_columns + ['etecho1', 'etecho2', 'etecho3']\ncat_columns = cat_columns + ['roof_sts']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"####################################################################################\n######################################################################################\n###########combine 3 binary features for Floor  condition( 3 levels)  into a single column \n#####################################################################################\n## Checking if values are defined for more than 1 of these columns\n((cdata['eviv1'] + cdata['eviv2'] + cdata['eviv3']  ) > 1).sum() \n\nfloor_sts_mapping = ({1: 'Bad', 2 : 'Regular', 3: 'Good'})\ncdata.loc[(cdata['eviv1'] == 1), 'floor_sts'] = 1\ncdata.loc[(cdata['eviv2'] == 1), 'floor_sts'] = 2\ncdata.loc[(cdata['eviv3'] == 1), 'floor_sts'] = 3\n\ndel_columns = del_columns + ['eviv1', 'eviv2', 'eviv3']\ncat_columns = cat_columns + ['floor_sts']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(del_columns)\nprint(len(del_columns))\n\n#cdata.drop(del_columns, inplace = True, axis =1)\n\nprint(cdata.shape)   #9557 X 70\nprint(cdata.columns)\nprint(cat_columns)\nlen(cat_columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"######## Feature Engineering\n######## Step 2 \n####   there are 5 different columns with identical information for household size\n### i) r4t3, Total persons in the household, ii) tamhog, size of the household, iii) tamviv, number of ###persons living in the household  iv) hhsize, household size  v) hogar_total, # of total individuals in the household\n#########  Check if these 5 columns are identical and if so , we can proceed to delete 4 of these duplicate columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"cdata[ ['r4t3','tamhog','tamviv','hhsize','hogar_total' ]]\n((cdata['r4t3'] + cdata['tamhog'] + cdata['tamviv'] + cdata['hhsize'] + cdata['hogar_total' ] % 5) != 0).sum()  #9557\n## these columns are identical as all these columns have the same values\n## Retaining column 'hogar_total' and deleting the rest of the columns as we dont need them\n\ndel_columns = del_columns + ['r4t3', 'tamhog', 'tamviv', 'hhsize']\ncdata.drop(del_columns, inplace = True, axis =1)\n\ncdata.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"####################################################\n#####Feature Engineering\n#### Step 3\n####  Checking null values in data and adopting a strategy for imputation"},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"###############################################################################################################\n#Checking null values in data #################################################################################\n###############################################################################################################\ncdata.isnull().any() \ncdata.columns[cdata.isnull().any()].tolist()\n# 5 columns has null values - v2a1, v18q1, rez_esc, meaneduc, SQBmeaned\n###############################################################################","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# v2a1 - Monthly Rent payment is null..check how does this relate to tipovivi3, =1 rented and \n#tipovivi : 1 : Owned and paid', 2: 'Owned and paying install', 3: 'Rented', 4: 'Precarious' , 5 : 'Other' \ncdata.v2a1.isnull().sum() # 6860 rows has null value\ncdata.v2a1.isnull().value_counts()   #6860 row is True - Null\ncdata[cdata.v2a1.isnull()].tipovivi.value_counts()   #rented is 0 where monthly rent payment is null ;1, 4 & 5\ncdata['v2a1'] = cdata['v2a1'].fillna(0)  ##  update as 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###############################################################################\n# v18q1 - number of tablets household owns  has null values;check how does this relate to v18q  =1 owns tablet\ncdata.v18q1.isnull().sum()  #7342 rows has null values\ncdata.v18q1.isnull().value_counts()  #7342 rows with nulls\ncdata[cdata['v18q'] ==0].v18q.value_counts()  ## all of them does not own a mobile, v18q = 0\ncdata[cdata.v18q1.isnull()].v18q.value_counts()   # owns tablet is 0 where # of tablets  is null -7342 does not own tablet\ncdata['v18q1'] = cdata['v18q1'].fillna(0)   # fill NaN with zeroes for # of mobiles","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##########################################################################################\n# meaneduc & SQBmeaned - Mean education has null values and check how does this feature relate to instlevel1 - no level of education\n###########################################################################################\nnp.sum(cdata.meaneduc.isnull())  # 5 rows has null values for meaneduc\nnp.sum(cdata.SQBmeaned.isnull()) # 5 rows has null values\ncdata.loc[(cdata.meaneduc.isnull()),['SQBmeaned','rez_esc', 'meaneduc']]  ## all of these columns have NAN values SQBmeaned, rez_esc, meaneduc\n###check further for level of education --intlevel\ncdata[cdata.meaneduc.isnull()].instlevel.value_counts()  #  4 rows 'No education, 1 row  'incomplete primary\n## Can safely initialize meaneduc & SQBmeaned  to 0 with the above data \ncdata['meaneduc'] = cdata['meaneduc'].fillna(0) \ncdata['SQBmeaned'] = cdata['SQBmeaned'].fillna(0) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###############################################################################################################\n# rez_esc - years behind in school\n################################################################################################################\nnp.sum(cdata.rez_esc.isnull())    #7928 rows has null values - 83%\nnp.sum((cdata.rez_esc.isnull()) & (cdata.age < 7) | (cdata.age > 19))   ## 7578 rows can be set to 0 \n\ncdata.loc[(cdata.rez_esc.isnull()) & (cdata.age < 7) | (cdata.age > 19), 'rez_esc'] = 0\nnp.sum(cdata.rez_esc.isnull())    ## 350 rows are still null..","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"####################################################\n#####Feature Engineering\n#### Step 4\n####  Checking Object variables in data and adopting a strategy for converting to numeric"},{"metadata":{"trusted":true},"cell_type":"code","source":"##################################################################################\n######dealing with Object variables in the data\n###############################################################################\ncdata.select_dtypes(object).nunique()\n#Dropping column Id as it does not have any significance for the modeling\n#cdata.drop(['Id'], inplace = True, axis =1)\ncdata.dependency.value_counts() ### 2192 - yes  1747 no ; should be numerical\ncdata.edjefe.value_counts()   ### 3762 no  123 yes ;; should be numerical\ncdata.edjefa.value_counts()   ### 6230 no   69 yes\nX= cdata.loc[(cdata['dependency'] == 'yes')]\nX1= cdata.loc[(cdata['dependency'] == 'no')]\n##  Replacing yes / no  with sqrt of SQBdependency as per Kaggle Data description\ncdata.loc[(cdata['dependency'] == 'yes'), 'dependency'] = cdata.SQBdependency.map(lambda SQBdependency: math.sqrt(SQBdependency))\ncdata.loc[(cdata['dependency'] == 'no'), 'dependency'] = cdata.SQBdependency.map(lambda SQBdependency: math.sqrt(SQBdependency))\nX2= cdata.loc[(cdata['edjefe'] == 'no')]\n##  Replacing yes with 1 and no with 0 as per Kaggle Data description\ncdata.loc[(cdata['edjefe'] == 'no'), 'edjefe'] = 0\ncdata.loc[(cdata['edjefe'] == 'yes'), 'edjefe'] = 1\n\ncdata.loc[(cdata['edjefa'] == 'no'), 'edjefa'] = 0\ncdata.loc[(cdata['edjefa'] == 'yes'), 'edjefa'] = 1\n##  converting to Float              \ncdata['dependency'] = cdata['dependency'].astype(np.float64)\ncdata['edjefe'] = cdata['edjefe'].astype(np.float64)\ncdata['edjefa'] = cdata['edjefa'].astype(np.float64)\ncdata.dtypes\ncdata.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"####################################################\n#####Feature Engineering\n#### Step 5\n####  Adding a new column for total devices owned as this may help the modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"################################################################################\n## refrig, v18q1, computer, television are household level variables\ncdata['devices_owned'] = cdata['refrig'] + cdata['computer'] + cdata['television'] + cdata['v18q1']\n## total ownership of devices will have a better correlation with the target \ncdata.loc[(cdata['v18q1'] > 1), ['v18q1', 'Target','parent_level']]\ncdata.groupby('area')['Target'].apply(lambda x: x.isnull().sum())   #Total count of Target with null value is 23,856","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"####################################################\n#####Feature Engineering\n#### Step 6\n####  All the square values are duplicate of the corresponding variables in the data set and have duplicate information\n#### these variables can be skipped"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Consolidating\n## Skipping the Squared columns as they are all square of the existing features and \n## are getting picked up in the feature importance of the baseline and training models\ncat_columns\ndata_num_columns = ['v2a1','rooms','v18q1','r4h1','r4h2','r4h3','r4m1','r4m2','r4m3','r4t1','r4t2','escolari','rez_esc',\n                    'hogar_nin','hogar_adul','hogar_mayor','hogar_total','dependency','edjefe','edjefa',\n                    'meaneduc','bedrooms','overcrowding','qmobilephone','age','devices_owned']\n      \nobject_columns = ['Id','idhogar','Target']                  \ndata_cat_columns = ['hacdor','hacapo','v14a','refrig','v18q','cielorazo','dis','computer','television','mobilephone']\ntotal_cat_columns = data_cat_columns + cat_columns\nTotal_columns = data_num_columns + total_cat_columns + object_columns\nprint('Total :',len(Total_columns), 'Numerical :', len(data_num_columns), 'Categorical :', len(total_cat_columns))\n\nFloat64_columns = cat_columns + ['devices_owned']\n## Changing the categorical variables from  float64 columns to int64\ncdata[Float64_columns] = cdata[Float64_columns].astype(np.int64)\nlen(total_cat_columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As this point we have only 1 column with a null value for 250 rows where we need to have an imputation strategy\nImputing columns with mean thru the simple imputer"},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"###############################################################################\n# 8.2  Imputing data. 350 rows have null value for rez_esc    \n# Updating with mean  \nimp = SimpleImputer(strategy=\"mean\")     \ncdata['rez_esc'] = imp.fit_transform(cdata[['rez_esc']])\ncdata.columns[cdata.isnull().any()].tolist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"[](http:/Interpreting and analysing dataset/)\n1) It is important to understand the dataset and the correlation, spread of the features before we get into the modelling exercise\n2) We will try and make use of seaborn Visualization techniques to see if we can make some sense of the Dataset "},{"metadata":{"trusted":true},"cell_type":"code","source":"################################################################################\n#### Visualizations\n################################################################################\npoverty_mapping = ({1: 'Extreme-1', 2: 'Moderate-2', 3: 'Vulnerable-3', 4: 'Non vulnerable-4'})\nax1 = sns.countplot(cdata.Target.map(poverty_mapping), data = cdata)\nax1.set_ylabel(\"Count\", fontname=\"Arial\", fontsize=18)\n# Set the title to Comic Sans\nax1.set_title(\"Distribution of Poverty - Combined\", fontname='Arial', fontsize=18)\n# Set the font name for axis tick labels to be Comic Sans\nfor tick in ax1.get_xticklabels():\n    tick.set_fontname(\"Arial\")\n    tick.set_fontsize(18)\n\nax1.set_xticklabels(ax1.get_xticklabels(), rotation = 45)\nplt.show()\n## Extreme Poverty is about 10% of the training data set. Target values are null for Test and \n## hence does not reflect in the data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize = (5,5))\nax = fig.add_subplot(111)\nOwns  = ['Tablet', 'Refrigerator', 'Computer','Television', 'Mobile']\nnames = ['v18q', 'refrig', 'computer', 'television','mobilephone']\nfor i,j in enumerate(names):\n     ax = plt.subplot(2,3,i+1) \n     sns.countplot(cdata.Target.map(poverty_mapping),        \n              hue= cdata[j],    # Target Distribution by ownership \n              data = cdata,\n              ax = ax)\n     ax.set_ylabel(\"Count\", fontname=\"Arial\", fontsize=18)\n     ax.set_title(\"Owns \" + Owns[i], fontname='Arial', fontsize=18)\n# Set the title to Comic Sans\nplt.show()\n## Ownership of Tablet, Computer and Television is very minimal for the Extreme  poverty class\n## Suprisingly ownership of Refreigerator and mobile is relatively higher for the extreme poverty class\n## Almost everyone owns mobile regardless of target class mapping","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize = (5,5))\nax = fig.add_subplot(111)\nexplode = [0.2,0,0,0,0,0]\nsns.countplot(cdata.lugar.map(region_mapping), data = cdata) # distribution by Region\nfig = plt.figure(figsize = (5,5))\nax = fig.add_subplot(111)\nplt.pie(cdata.lugar.value_counts(), labels = Region, data = cdata, explode = explode, autopct='%1.1f%%',shadow=True, startangle=90)  # distribution by Region\nfig = plt.figure(figsize = (5,5))\nax = fig.add_subplot(111)\nsns.countplot(cdata.Target.map(poverty_mapping),        \n              hue= cdata.lugar.map(region_mapping),    # Target Distribution by Region\n              data = cdata)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize = (5,5))\nax = fig.add_subplot(111)\nsns.countplot(cdata.area.map(area_mapping), data = cdata) #","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, axes = plt.subplots(2, 2, figsize=(7, 7))\nsns.boxplot(x=\"Target\", y=\"age\", data=cdata, ax=axes[0, 0])\nsns.boxplot(x=\"Target\", y=\"meaneduc\", data=cdata, ax=axes[0, 1])\nsns.boxplot(x=\"Target\", y=\"devices_owned\", data=cdata, ax=axes[1, 0])\nsns.boxplot(x=\"Target\", y=\"hogar_total\", data=cdata, ax=axes[1, 1])\nplt.show()\n## age is skewed to the right for all target classes\n## meaneduc is slightly skewed to the right for all target classes\n## Total Devices/ appliances owned only by the vulnerable and non-vulnerable households. Also 50% or \n## more of the data equal the maximum , hence there is no median line.Almost all of the households \n## in the target classes owns 2 or more ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"AS per the Kaggle data set the classification needs to be done at the household level. though there are rows in the dataset where the head of the household definition where parentesco1  definition is missing , it is not a significant volume and hence we can ignore theses rows.\nExtracting the data where parentesco1 = 1 (new variable parent_level = 1)  for our modelling purposes"},{"metadata":{"trusted":true},"cell_type":"code","source":"### Plotting a matrix scatterplot of the numeric variables \n\npd.plotting.scatter_matrix(cdata.loc[:, [\"rez_esc\", \"meaneduc\", \"age\",'devices_owned','hogar_total']], diagonal=\"kde\")\n\nplt.show()\n### As expected, these features doesnt appear to have any correlation between them","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize = (5,5))\nax = fig.add_subplot(111)\nsns.countplot(cdata.instlevel.map(education_mapping),        # Variable whose distribution is of interest\n              hue= \"Target\",    # Distribution will be gender-wise\n              data = cdata)\n## Target class of Extreme and moderate poverty is directly linked to the level of education and concentrated \n## at levels of 'No education', ' Incomplete primary', 'Incomplete secondary, 'Completed primary' ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split age into age categories\nprint(cdata.age.min())\nprint(cdata.age.max())\ncdata['age_cat'] = pd.cut( cdata.age,\n                          [0,7,19,30,50,80,100], \n                          include_lowest=True,\n                          labels= [\"Toddler\",\"Student\",\"Adult\",\"Mature Adult\",\"Senior\",\"VSenior\"]\n                          )\n\ncdata['age_cat'].value_counts()\nfig = plt.figure(figsize = (5,5))\nax = fig.add_subplot(111)\ncdata['age_cat'].value_counts().plot.bar()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize = (5,5))\nax = fig.add_subplot(111)\nsns.countplot(\"age_cat\",        # Variable whose distribution is of interest\n              hue= \"Target\",    # Distribution will be gender-wise\n              data = cdata)\n## uniform distribution of age across all Target classes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_values = [1,2,3,4]\nf, axes = plt.subplots(4, 2, figsize=(7, 7))\nfor j,Target in enumerate(target_values,0):\n    subset = cdata[cdata['Target'] == Target]\n   \n    # Draw the density plot\n    sns.distplot(subset['meaneduc'], hist = False, kde = True,\n                  label = Target, ax = axes[j,0])\n    sns.distplot(subset['rez_esc'], hist = False, kde = True,\n                  label = Target, ax = axes[j,1])\nplt.show()\n## Mean education peaks around 6-7 years for extreme & moderate poverty levels\n## Mean education peaks around 10 years for non-vulnerable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(20, 10))\nsns.boxplot(x='Target', y = 'r4h3',ax = ax, data = cdata, hue = 'Target' )\nax.set_title('Number of men in households', size = 25)\nplt.show()\n## Median is around 2 men per household  for the 3 target classes - Extreme, moderate & Vulnerable\n## Max & Median converges at 2 men per household for the non-vulnerable class","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(20, 10))\nsns.boxplot(x='Target', y = 'hogar_total',ax = ax, data = cdata, hue = 'Target' )\nax.set_title('Total Heads in households', size = 25)\n\n## ## Median is around 4 men per household  for all the  target classes \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"[Splitting Dataset  ](http://)\n1) Extracting data for the household as predictions will be done at the household - parent_level ==  1\n2) Dropping the object columns from the dataset  as they will not be needed for the modelling\n3) Splitting the combined data back to TRain and Test -- Using the Target variable -  Null for the Test dataset and notnull for the Train\n4) Standard Scaling the numerical data and one hot encoding the categorical variables\n4) Splitting into Train & Test for modelling\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Extracting data for the household\n## Extracting columns as per Total columns\ncdata.loc[(cdata['parent_level'] == 1)].Target.value_counts()\nheads = cdata.loc[cdata['parent_level'] == 1, Total_columns]\nprint(heads.shape)  # 10307 X 57\nheads.columns\nheads.groupby('area')['Target'].apply(lambda x: x.isnull().sum())  ## total count of Target with null value is 7334\nheads.groupby('area')['Target'].apply(lambda x: x.notnull().sum())  ## total count of Target without  null value is 2973","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### separating Target for Train \n################################################################################\ny_train = np.array(list(heads[heads['Target'].notnull()]['Target'].astype(np.uint8)))\nprint(y_train.shape )  # (2973,)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"################################################################################\n### separating Train & Test\n################################################################################\nX_train = heads.loc[heads['Target'].notnull(), :]\n\nX_test =  heads.loc[heads['Target'].isnull(), :]\nX_train.drop(object_columns, inplace = True, axis =1)\nX_test.drop(object_columns, inplace = True, axis =1)\n\n\nprint(type(X_train))\nprint(type(X_test))\nprint(X_train.shape)  ## (2973 X 54)\nprint(X_test.shape)   ## (7334 X 54)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1)Scaling the numerical columns in the dataset with Standard scaler \n2) using one hot encoder for the categorical columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"#################################################################################\n##Standard Scaler for numeric and one hot encoding for categorical\n##################################################################################\nnum = (\"numtrans\", ss() , data_num_columns)\ncolTrans = ct([num])\nX_trans_num = colTrans.fit_transform(X_train)\nX_trans_num.shape              # 2973 X 26\ntype(X_trans_num)    #numpy array\nX_num = pd.DataFrame(X_trans_num, columns = data_num_columns)\nX_num.shape                    #2973 X 26\n#################################################################################\n## One Hot encoding for the categorical columns\n#############################################################################\ncat = (\"cattrans\", ohe(), total_cat_columns)\n#  Instantiate column transformer object\ncolTrans = ct([cat])\n\nX_trans_cat = colTrans.fit_transform(X_train)\nprint(X_trans_cat.shape )             # 2973 X 99\nprint(type(X_trans_cat))\nX_cat = pd.DataFrame(X_trans_cat.todense()).add_prefix('cat_')\n## storing random projection column names\nrp_columns = list(X_cat.columns)\nprint(X_cat.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#################################################################################\n#Combine the numerical & categorical arrays after transformation\n#################################\n\nX_trans = np.array(np.hstack([X_trans_num,X_trans_cat.todense()]))\nprint(X_trans.shape )                      #2973 X 125\nprint(type(X_trans))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Splitting Train and Test"},{"metadata":{"trusted":true},"cell_type":"code","source":"########################################################################################\nX_mtrain, X_mtest, y_mtrain, y_mtest = train_test_split(X_trans,\n                                                    y_train,\n                                                    test_size=0.25,\n                                                    stratify = y_train,\n                                                    shuffle = True\n                                                    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#######################################################################################\nprint(X_mtrain.shape)      # 2229 X 125\nprint(y_mtrain.shape)      # 2229\nprint(X_mtest.shape)       # 744 X 125\nprint(y_mtest.shape)       # 744\ntype(X_mtrain)\ntype(X_mtest)\n######################################################################################","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"[Modelling](http://)### Modelling \n1) This is a multi-class classification problem and hence we will be evaluating the models on the accuracy and F1 Macro score.\n\nReference : \nhttps://datascience.stackexchange.com/questions/15989/micro-average-vs-macro-average-performance-in-a-multiclass-classification-settin\nMicro- and macro-averages (for whatever metric) will compute slightly different things, and thus their interpretation differs. A macro-average will compute the metric independently for each class and then take the average (hence treating all classes equally), whereas a micro-average will aggregate the contributions of all classes to compute the average metric. In a multi-class classification setup, micro-average is preferable if you suspect there might be class imbalance (i.e you may have many more examples of one class than of other classes).\n\nTo illustrate why, take for example precision Pr=TP(TP+FP). Let's imagine you have a One-vs-All (there is only one correct class output per example) multi-class classification system with four classes and the following numbers when tested:\n\nClass A: 1 TP and 1 FP\nClass B: 10 TP and 90 FP\nClass C: 1 TP and 1 FP\nClass D: 1 TP and 1 FP\nYou can see easily that PrA=PrC=PrD=0.5, whereas PrB=0.1.\n\nA macro-average will then compute: Pr=0.5+0.1+0.5+0.54=0.4\nA micro-average will compute: Pr=1+10+1+12+100+2+2=0.123"},{"metadata":{},"cell_type":"markdown","source":"[1) Random Forest Classifier](http://)\n1) Predict outcome using random forest calssifier. \n2) This will be ba baseline version and we will mostly be using default parameters which are not tuned\n3) the Accuracy and F1 macro score will serve as the indicator and used to compare the performance of the other model that we will build "},{"metadata":{"trusted":true},"cell_type":"code","source":"######################################################################################\n### Modelling\n#### Using Random Forest to baseline our 1st model\n#####################################################################################\nscorer = make_scorer(f1_score, greater_is_better=True, average = 'macro')\nrf_model = RandomForestClassifier(n_estimators=500, random_state=10, \n                               n_jobs = -1)\n# 10 fold cross validation\ncv_score = cross_val_score(rf_model, X_mtrain, y_mtrain, cv = 10, scoring = scorer)\n\nprint(f'10 Fold Cross Validation F1 Score = {round(cv_score.mean(), 4)} with std = {round(cv_score.std(), 4)}')\nrf_model.fit(X_mtrain, y_mtrain)\n#feature_importances = pd.DataFrame({'feature': features, 'importance': model.feature_importances_})\nfeatures = data_num_columns + rp_columns\nfeature_imp = pd.DataFrame({'feature':features, 'importance':rf_model.feature_importances_}).sort_values(by = \"importance\", ascending=False)\nfeature_imp\nfeature_imp.head()\ng = sns.barplot(x = feature_imp.iloc[  :5,  1] , y = feature_imp.iloc[ :5, 0])\ng.set_xticklabels(g.get_xticklabels(),rotation=90)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"[2) xgboost and tuning using Grid search](http://)"},{"metadata":{},"cell_type":"markdown","source":"############################################################################################\n[](http://)###### Using xgboost and Grid Search \n#######################################"},{"metadata":{"trusted":true},"cell_type":"code","source":"steps_xg = [('pca', PCA()),\n            ('xg',  XGBClassifier(silent = False,\n                                  n_jobs=2)        \n            )\n            ]\n\n# Instantiate Pipeline object\npipe_xg = Pipeline(steps_xg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nparameters = {'xg__learning_rate':  [0.03, 0.05],\n              'xg__n_estimators':   [100,  400],\n              'xg__max_depth':      [4,6],\n              'pca__n_components' : [0.8,0.95]\n              }                             \n\n# 7  Grid Search (16 * 2) iterations\n#    Create Grid Search object \n\nclf = GridSearchCV(pipe_xg,            # pipeline object\n                   parameters,         # possible parameters\n                   n_jobs = 2,         # USe parallel cpu threads\n                   cv =5 ,             # No of folds\n                   verbose =2,         \n                   scoring = ['accuracy'],  # Metrics for performance \n                   refit = 'accuracy'  # Refitting final model which maximise auc\n                   )\n                              \n# 7.2.  fitting data to pipeline\nstart = time.time()\nclf.fit(X_mtrain, y_mtrain)\nend = time.time()\n(end - start)/60               # 25 minutes\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Best score: {clf.best_score_} \")\nprint(f\"Best parameter set {clf.best_params_}\")\n\n\n# 7.4. Make predictions\ny_m1pred = clf.predict(X_mtest)\n\n\n# 7.5 Accuracy\naccuracy = accuracy_score(y_mtest, y_m1pred)\nprint(f\"Accuracy: {accuracy * 100.0}\")\n\n######### Accuracy with Grid Search is 68.01%   which is pretty low ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"[3) xgboost and tuning using Random Search](http://)"},{"metadata":{},"cell_type":"markdown","source":"######################################################################\n[](http://)##  xgboost with Random Search\n#####################################################################"},{"metadata":{"trusted":true},"cell_type":"code","source":"parameters = {'xg__learning_rate':  uniform(0, 1),\n              'xg__n_estimators':   range(500,1000),\n              'xg__max_depth':      range(3,25),\n              'pca__n_components' : range(20, 30)}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rs = RandomizedSearchCV(pipe_xg,\n                        param_distributions=parameters,\n                        scoring= [ 'accuracy'],\n                        n_iter=10,          # Max combination of\n                                            # parameter to try. Default = 10\n                        verbose = 3,\n                        refit = 'accuracy',\n                        n_jobs = 2,          # Use parallel cpu threads\n                        cv = 5               # No of folds.\n                                             # So n_iter * cv combinations\n                        )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# random search - Using 10 iterations\nstart = time.time()\nrs.fit(X_mtrain, y_mtrain)\nend = time.time()\n(end - start)/60\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluate\nprint(f\"Best score: {rs.best_score_} \")\nprint(f\"Best parameter set: {rs.best_params_} \")\n\n\n# Make predictions\ny_m2pred = rs.predict(X_mtest)\n\n\n# Accuracy\naccuracy = accuracy_score(y_mtest, y_m2pred)\nprint(f\"Accuracy: {accuracy * 100.0}\")\n## Accuracy is 66.39% ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"############### FF. Fitting parameters in our model ##############\n###############    Model Importance   #################\n\n#  Model with parameters of grid search\nmodel_gs = XGBClassifier(\n                    learning_rate = clf.best_params_['xg__learning_rate'],\n                    max_depth = clf.best_params_['xg__max_depth'],\n                    n_estimators=clf.best_params_['xg__max_depth']\n                    )\n\n#  Model with parameters of random search\nmodel_rs = XGBClassifier(\n                    learning_rate = rs.best_params_['xg__learning_rate'],\n                    max_depth = rs.best_params_['xg__max_depth'],\n                    n_estimators=rs.best_params_['xg__max_depth']\n                    )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Modeling with both parameters\nstart = time.time()\nmodel_gs.fit(X_mtrain, y_mtrain)\nmodel_rs.fit(X_mtrain, y_mtrain)\nend = time.time()\n(end - start)/60","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Predictions with both models\ny_pred_gs = model_gs.predict(X_mtest)\ny_pred_rs = model_rs.predict(X_mtest)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Accuracy from both models\naccuracy_gs = accuracy_score(y_mtest, y_pred_gs)\naccuracy_rs = accuracy_score(y_mtest, y_pred_rs)\nprint(\"Accuracy using Grid search optimization is \" ,accuracy_gs)\nprint(\"Accuracy using Random search optimization is \" , accuracy_rs)\n\nprint(classification_report(y_mtest, y_pred_gs, target_names=['1','2','3','4']))\nprint(classification_report(y_mtest, y_pred_rs, target_names=['1','2','3','4']))\n#  Get feature importances from both models\n\nmodel_gs.feature_importances_\nmodel_rs.feature_importances_\nplot_importance(model_gs)\nplot_importance(model_rs)\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Precision, Recall & F1 Macros metrics across the different classes in the above falls far below expectations."},{"metadata":{},"cell_type":"markdown","source":"[4) Bayesian and Extra tree classifier](http://)\nTuning hyperparameters using Bayesian and modelling with Extra Trees Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"para_set_rf = {\n           'max_features' :   (10, 20),\n           'n_estimators':   (500,1000),               # any number between 500 to 1000\n           'max_depth':      (5,30),                 # any depth between 5 to 30\n           'n_components' :  (50,100)                 # any number between 50 to 100\n            }\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef xg_eval_et(max_features,n_estimators, max_depth,n_components):\n    # 12.1 Make pipeline. Pass parameters directly here\n    pipe_xg1 = make_pipeline (ss(),                        \n                              PCA(n_components=int(round(n_components))),\n                              ExtraTreesClassifier (\n                                           criterion='gini',\n                                           n_jobs=2,\n                                           max_features=int(round(max_features)),\n                                           max_depth=int(round(max_depth)),\n                                           n_estimators=int(round(n_estimators))\n                                           )\n                             )\n\n    # 12.2 Now fit the pipeline and evaluate\n    cv_result = cross_val_score(estimator = pipe_xg1,\n                                X= X_mtrain,\n                                y = y_mtrain,\n                                cv = 5,\n                                n_jobs = 2,\n                                scoring = 'f1_macro'\n                                ).mean()             # take the average of all results\n\n\n    # 12.3 Finally return maximum/average value of result\n    return cv_result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgBO = BayesianOptimization(\n                             xg_eval_et,     # Function to evaluate performance.\n                             para_set_rf     # Parameter set from where parameters will be selected\n                             )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gp_params = {\"alpha\": 1e-5}      # Initialization parameter for gaussian","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.time()\nxgBO.maximize(init_points=10,    # Number of randomly chosen points to\n                                 # sample the target function before\n                                 #  fitting the gaussian Process (gp)\n                                 #  or gaussian graph\n               n_iter=10,        # Total number of times the\n               #acq=\"ucb\",       # ucb: upper confidence bound\n                                 #   process is to be repeated\n                                 # ei: Expected improvement\n               # kappa = 1.0     # kappa=1 : prefer exploitation; kappa=10, prefer exploration\n              **gp_params\n               )\nend = time.time()\n(end-start)/60","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(xgBO.max)\n### Model with the best parameters\nmodel_bet = ExtraTreesClassifier (\n                    criterion='gini',\n                    max_depth = 30,\n                    max_features = 20,\n                    n_estimators=852\n                     )\n\nstart = time.time()\nmodel_bet.fit(X_mtrain, y_mtrain)\nend = time.time()\n(end - start)/60\n\ny_pred_mbet = model_bet.predict(X_mtest)\naccuracy_xgB_et = accuracy_score(y_mtest, y_pred_mbet)\nprint(\"Accuracy using Bayesian Optimization is \" , accuracy_xgB_et)\nprint(\"F1 macro score is: {}\".format(f1_score(y_mtest, y_pred_mbet,average='macro')))\nprint(classification_report(y_mtest, y_pred_mbet, target_names=['1','2','3','4']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_imp = pd.DataFrame({'feature':features, 'importance':model_bet.feature_importances_}).sort_values(by = \"importance\", ascending=False)\nfeature_imp\nfeature_imp.head()\ng = sns.barplot(x = feature_imp.iloc[  :5,  1] , y = feature_imp.iloc[ :5, 0])\ng.set_xticklabels(g.get_xticklabels(),rotation=90)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"[5): Bayesian & Random Forest](http://)\nTuning Hyperparameters using Bayesian Optimization and modelling with Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"para_set_rf = {\n           'max_features' :   (10, 20),\n           'n_estimators':   (500,1000),               # any number between 500 to 1000\n           'max_depth':      (5,30),                 # any depth between 3 to 10\n           'n_components' :  (50,100)                 # any number between 50 to 100\n            }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Create a function that when passed some parameters evaluates results using cross-validation\n#  This function is used by BayesianOptimization() object\n\ndef xg_eval_rf(max_features,n_estimators, max_depth,n_components):\n    # 12.1 Make pipeline. Pass parameters directly here\n    pipe_xg1 = make_pipeline (ss(),                        \n                              PCA(n_components=int(round(n_components))),\n                              RandomForestClassifier (\n                                           criterion='gini',\n                                           n_jobs=2,\n                                           max_features=int(round(max_features)),\n                                           max_depth=int(round(max_depth)),\n                                           n_estimators=int(round(n_estimators))\n                                           )\n                             )\n\n    # fit the pipeline and evaluate\n    cv_result = cross_val_score(estimator = pipe_xg1,\n                                X= X_mtrain,\n                                y = y_mtrain,\n                                cv = 2,\n                                n_jobs = 2,\n                                scoring = 'f1_macro'\n                                ).mean()             # take the average of all results\n\n\n    #  return maximum/average value of result\n    return cv_result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgBO = BayesianOptimization(\n                             xg_eval_rf,     # Function to evaluate performance.\n                             para_set_rf     # Parameter set from where parameters will be selected\n                             )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 13. Gaussian process parameters\n#     Modulate intelligence of Bayesian Optimization process\ngp_params = {\"alpha\": 1e-5}      # Initialization parameter for gaussian\n                                 # process.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 14. Fit/train (so-to-say) the BayesianOptimization() object\n#     Start optimization. 25minutes\n#     Our objective is to maximize performance (results)\nstart = time.time()\nxgBO.maximize(init_points=5,    # Number of randomly chosen points to\n                                 # sample the target function before\n                                 #  fitting the gaussian Process (gp)\n                                 #  or gaussian graph\n               n_iter=10,        # Total number of times the\n               #acq=\"ucb\",       # ucb: upper confidence bound\n                                 #   process is to be repeated\n                                 # ei: Expected improvement\n               # kappa = 1.0     # kappa=1 : prefer exploitation; kappa=10, prefer exploration\n              **gp_params\n               )\nend = time.time()\n(end-start)/60","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgBO.res\nprint(xgBO.max)\n### Model with the best parameters\nmodel_brf = RandomForestClassifier (\n                    criterion='gini',\n                    max_depth = 30,\n                    max_features = 19,\n                    n_estimators=500\n                     )\n\nstart = time.time()\nmodel_brf.fit(X_mtrain, y_mtrain)\nend = time.time()\n(end - start)/60","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_mbrf = model_brf.predict(X_mtest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_xgB_rf = accuracy_score(y_mtest, y_pred_mbrf)\nprint(\"Accuracy using Bayesian Optimization is \" , accuracy_xgB_rf)\nprint(\"F1 macro score is: {}\".format(f1_score(y_mtest, y_pred_mbrf,average='macro')))\nprint(classification_report(y_mtest, y_pred_mbrf, target_names=['1','2','3','4']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_imp = pd.DataFrame({'feature':features, 'importance':model_brf.feature_importances_}).sort_values(by = \"importance\", ascending=False)\nfeature_imp\nfeature_imp.head()\ng = sns.barplot(x = feature_imp.iloc[  :5,  1] , y = feature_imp.iloc[ :5, 0])\ng.set_xticklabels(g.get_xticklabels(),rotation=90)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"[6)Bayesian Optimization and gradient boosting technique](http://)"},{"metadata":{"trusted":true},"cell_type":"code","source":"para_set = {\n           'learning_rate':  (0, 1),                 # any value between 0 and 1\n           'n_estimators':   (500,1000),               # any number between 500 to 1000\n           'max_depth':      (5,30),                 # any depth between 3 to 10\n           'n_components' :  (50,100)                 # any number between 20 to 30\n            }\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 12 Create a function that when passed some parameters\n#    evaluates results using cross-validation\n#    This function is used by BayesianOptimization() object\n\ndef xg_eval(learning_rate,n_estimators, max_depth,n_components):\n    # 12.1 Make pipeline. Pass parameters directly here\n    pipe_xg1 = make_pipeline (ss(),                        # Why repeat this here for each evaluation?\n                              PCA(n_components=int(round(n_components))),\n                              XGBClassifier(\n                                           silent = True,\n                                           objective='multi:softmax',\n                                           booster='gbtree',\n                                           n_jobs=2,\n                                           learning_rate=learning_rate,\n                                           max_depth=int(round(max_depth)),\n                                           n_estimators=int(round(n_estimators))\n                                           )\n                             )\n\n    # 12.2 Now fit the pipeline and evaluate\n    cv_result = cross_val_score(estimator = pipe_xg1,\n                                X= X_mtrain,\n                                y = y_mtrain,\n                                cv = 2,\n                                n_jobs = 2,\n                                scoring = 'f1_macro'\n                                ).mean()             # take the average of all results\n\n\n    # 12.3 Finally return maximum/average value of result\n    return cv_result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#      Instantiate BayesianOptimization() object\n#      This object  can be considered as performing an internal-loop\n#      i)  Given parameters, xg_eval() evaluates performance\n#      ii) Based on the performance, set of parameters are selected\n#          from para_set and fed back to xg_eval()\n#      (i) and (ii) are repeated for given number of iterations\n#\nxgBO = BayesianOptimization(\n                             xg_eval,     # Function to evaluate performance.\n                             para_set     # Parameter set from where parameters will be selected\n                             )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gp_params = {\"alpha\": 1e-5}      # Initialization parameter for gaussian\n                                 # process.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 14. Fit/train  the BayesianOptimization() object\n#     Start optimization. 25minutes\n#     Our objective is to maximize performance (results)\nstart = time.time()\nxgBO.maximize(init_points=5,    # Number of randomly chosen points to\n                                 # sample the target function before\n                                 #  fitting the gaussian Process (gp)\n                                 #  or gaussian graph\n               n_iter=20,        # Total number of times the\n               #acq=\"ucb\",       # ucb: upper confidence bound\n                                 #   process is to be repeated\n                                 # ei: Expected improvement\n               # kappa = 1.0     # kappa=1 : prefer exploitation; kappa=10, prefer exploration\n              **gp_params\n               )\nend = time.time()\n(end-start)/60","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgBO.res\nprint(xgBO.max)\n### Model with the best parameters\nmodel_xgB =XGBClassifier(\n                    learning_rate = 0.7518,\n                    max_depth = 30,\n                    n_estimators=503,\n                    n_components = 99\n                    )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.time()\nmodel_xgB.fit(X_mtrain, y_mtrain)\nend = time.time()\n(end - start)/60\n\ny_pred_mxgB = model_xgB.predict(X_mtest)\naccuracy_xgB = accuracy_score(y_mtest, y_pred_mxgB)\nprint(\"Accuracy using Bayesian Optimization is \" , accuracy_xgB)\nprint(\"F1 macro score is: {}\".format(f1_score(y_mtest, y_pred_mxgB,average='macro')))\nprint(classification_report(y_mtest, y_pred_mxgB, target_names=['1','2','3','4']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_imp = pd.DataFrame({'feature':features, 'importance':model_xgB.feature_importances_}).sort_values(by = \"importance\", ascending=False)\nfeature_imp\nfeature_imp.head()\ng = sns.barplot(x = feature_imp.iloc[  :5,  1] , y = feature_imp.iloc[ :5, 0])\ng.set_xticklabels(g.get_xticklabels(),rotation=90)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"[7) Using Light gbm and Bayesian Optimization](http://)"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nmodel = lgb.LGBMClassifier(                # Using Classifier\n                          objective='multiclass',\n                          class_weight= 'balanced',\n                          metric='None',   # This output must match with what\n                                          #  we specify as input to Bayesian model\n                          n_jobs=2,\n                          verbose=0\n                          )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 22.1 Parameter search space for selected modeler\nparams = {\n        'num_leaves': (5,45),              # Maximum tree leaves for base learners.\n        'feature_fraction': (0.1, 0.9),\t   # Randomly select part of features on each iteration\n        'bagging_fraction': (0.8, 1),\t\t  # Randomly select part of data without resampling\n        'max_depth': (1, 50),              # Maximum tree depth for base learners, -1 means no limit.\n        'learning_rate': (0.01, 1.0, 'log-uniform'), # Prob of interval 1 to 10 is same as 10 to 100\n                                                     # Equal prob of selection from 0.01 to 0.1, 0.1\n                                                     # to 1\n                                                     #  Boosting learning rate.\n        'min_child_samples': (1, 50),         # Minimum number of data needed in a child (leaf)\n        'max_bin': (100, 1000),               # max number of bins that feature\n                                              #  values will be bucketed in\n                                              # small number of bins may reduce\n                                              # training accuracy but may increase\n                                              # general power (deal with over-fitting)\n        'subsample': (0.01, 1.0, 'uniform'),  # Subsample ratio of the training instance (default: 1)\n        'subsample_freq': (0, 10),            #   Frequence of subsample, <=0 means no enable (default = 0).\n        'colsample_bytree': (0.01, 1.0, 'uniform'), #  Subsample ratio of columns when constructing each tree (default:1).\n        'min_child_weight': (0, 10),         # Minimum sum of instance weight (hessian) needed in a child (leaf).\n        'subsample_for_bin': (100000, 500000), #  Number of samples for constructing bins(default: 200000)\n        'reg_lambda': (1e-9, 1000, 'log-uniform'),  # L2 regularization term on weights.\n        'reg_alpha': (1e-9, 1.0, 'log-uniform'),\n        'scale_pos_weight': (1e-6, 500, 'log-uniform'), #used only in binary application\n                                                        # weight of labels with positive class\n        'n_estimators': (500, 2000)  # Number of boosted trees to fit (default: 100).\n        }\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cvStrategy = StratifiedKFold(\n                             n_splits=3,\n                             shuffle=True,\n                             random_state=42\n                            )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bayes_cv_tuner = BayesSearchCV(\n                              estimator = model,    # rf, lgb, xgb, nn etc--Black box\n                              search_spaces = params,  # Specify params as required by the estimator\n                              scoring = 'accuracy',  # Input to Bayes function\n                                                    # modeler should return this\n                                                    # peformence metric\n                              cv = cvStrategy,      # Optional. Determines the cross-validation splitting strategy.\n                                                    #           Can be cross-validation generator or an iterable,\n                                                    #           Possible inputs for cv are: - None, to use the default 3-fold cv,\n                                                    #           - integer, to specify the number of folds in a (Stratified)KFold,\n                                                    #           - An object to be used as a cross-validation generator.\n                              n_jobs = 2,           # Start two parallel threads for processing\n                              n_iter = 10,        # Reduce to save time\n                              verbose = 0,\n                              refit = True,       #  Refit the best estimator with the entire dataset\n                              random_state = 42\n                               )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.time()\nresult = bayes_cv_tuner.fit(\n                           X_mtrain,       # Note that we use normal train data\n                           y_mtrain       #  rather than lgb train-data matrix\n                           )\n\nend = time.time()\n(end - start)/60","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bayes_cv_tuner.best_estimator_\n\nbst_bayes = bayes_cv_tuner.best_estimator_\nbst_bayes\n# 23.1 Train the best estimator\nbst_bayes.fit(X_mtrain, y_mtrain)\n#  Make predictions\ny_mpred_lgbm = bst_bayes.predict(X_mtest)\naccuracy_gen = accuracy_score(y_mtest, y_mpred_lgbm)\nprint(accuracy_gen)   # 68.68% %\nprint(\"F1 macro score is: {}\".format(f1_score(y_mtest, y_mpred_lgbm,average='macro')))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_mtest, y_mpred_lgbm, target_names=['1','2','3','4']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Precision, Recall & F1 Macros metrics across the different classes in the above falls far below expectations.interestingly, the scores for the non-vulnerable class is the best."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Plot feature importances...')\nax = lgb.plot_importance(bst_bayes, max_num_features=10)\nax.tick_params(labelsize=20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"[Conclusion & Next Steps]() and Next Steps\n\n1) The performance of the models did not meet desired expectations due to low accuracy ( 67-69%) with an F1-Macro score of 0.37-0.41. One of the reasons could be that the volume of the data set was reduced after extracting the data at the household level. Likely that total train records of 2229 was not a sufficient volume to train the models in a multi-classification model.\n\nNext steps\n1) Try and build the model with the entire dataset and apply the prediction to the household level. This may be a better approach instead of extracting the data at the household level for the modeling exercise. If we use the entire dataset for the modeling we will have a much larger volume of data that we can train the models on which can subsequently be used for prediction.\n\n2) Another approach can be try and tune the xgbm & lgbm model with different parameters -for lgbm it appeared that we were overfitting the data with large # of estimators - range (500-2000).\n\n3) We can also try using the KNN algorithm and check the outcome."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}