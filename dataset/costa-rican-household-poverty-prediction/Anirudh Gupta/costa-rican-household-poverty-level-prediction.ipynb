{"cells":[{"metadata":{"_uuid":"cbb767a6ce7a389511a40311cd4f01f1b09ad270"},"cell_type":"markdown","source":"# Costa Rican Household Poverty Level Prediction\n\n\nThe objective of the Costa Rican Household Poverty Level Prediction contest is to develop a machine learning model that can predict the poverty level of households using both individual and household characteristics. This \"data science for good\" project offers the opportunity to put our skills towards a task more beneficial to society than getting people to click on ads!\n\n## Problem and Data Explanation\n\nThe data for this competition is provided in two files: `train.csv` and `test.csv`. The training set has 9557 rows and 143 columns while the testing set has 23856 rows and 142 columns. Each row represents __one individual__ and each column is a __feature, either unique to the individual, or for the household of the individual__. The training set has one additional column, `Target`, which represents the poverty level on a 1-4 scale and is the label for the competition. A value of 1 is the most extreme poverty. \n\nThis is a __supervised multi-class classification machine learning problem__:\n\n* __Supervised__: provided with the labels for the training data\n* __Multi-class classification__: Labels are discrete values with 4 classes\n\n### Objective\n\nThe objective is to predict poverty on a __household level__. We are given data on the individual level with each individual having unique features but also information about their household. In the dataset for the task, we'll have to perform some _aggregations of the individual data_ for each household. Moreover, we have to make a prediction for every individual in the test set, but _\"ONLY the heads of household are used in scoring\"_ which means we want to predict poverty on a household basis. \n\n__Important note: while all members of a household should have the same label in the training data, there are errors where individuals in the same household have different labels. In these cases, we are told to use the label for the head of each household, which can be identified by the rows where `parentesco1 == 1.0`.__ [competition main discussion](https://www.kaggle.com/c/costa-rican-household-poverty-prediction/discussion/61403)\n\nThe `Target` values represent poverty levels as follows:\n\n    1 = extreme poverty \n    2 = moderate poverty \n    3 = vulnerable households \n    4 = non vulnerable households\n\nThe explanations for all 143 columns can be found in the [competition documentation](https://www.kaggle.com/c/costa-rican-household-poverty-prediction/data), but a few to note are below:\n\n* __Id__: a unique identifier for each individual, this should not be a feature that we use! \n* __idhogar__: a unique identifier for each household. This variable is not a feature, but will be used to group individuals by household as all individuals in a household will have the same identifier.\n* __parentesco1__: indicates if this person is the head of the household.\n* __Target__: the label, which should be equal for all members in a household\n\n### Metric\n\nPredictions will be assessed by the __Macro F1 Score.__ \n\n```\nfrom sklearn.metrics import f1_score\nf1_score(y_true, y_predicted, average = 'macro`)\n```\n\n## Roadmap\n\na. Explore data and perform data visualization\nb. Fill in missing values (NULL values) either using mean or median (if the attribute is numeric) or most-frequently occurring value if the attribute is 'object' or categorical.\nb. Perform feature engineering, may be using some selected features and only from numeric features.\nc. Scale numeric features, AND IF REQUIRED, perform One HOT Encoding of categorical features\nd. IF number of features is very large, please do not forget to do PCA.\ne. Select some estimators for your work. May be select some (or all) of these:\n\n        GradientBoostingClassifier\n        RandomForestClassifier\n        KNeighborsClassifier\n        ExtraTreesClassifier\n        XGBoost\n        LightGBM\n   \n   First perform modeling with default parameter values and get accuracy.\n\nf. Then perform tuning using Bayesian Optimization. \n"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# 1.0 Call libraries\n\n# Data manipulation\n%reset -f\nimport pandas as pd\nimport numpy as np\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\n\n# Set a few plotting defaults\n%matplotlib inline\n\nplt.style.use('fivethirtyeight')\nplt.rcParams['font.size'] = 18\nplt.rcParams['patch.edgecolor'] = 'k'\n\n# 1.0.1 For measuring time elapsed\nimport time\n\nfrom collections import OrderedDict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1.1 Working with imbalanced data\n# http://contrib.scikit-learn.org/imbalanced-learn/stable/generated/imblearn.over_sampling.SMOTE.html\n# Check imblearn version number as:\n#   import imblearn;  imblearn.__version__\nfrom imblearn.over_sampling import SMOTE, ADASYN\n\n# 1.2 Processing data\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import  OneHotEncoder as ohe\nfrom sklearn.preprocessing import StandardScaler as ss\nfrom sklearn.compose import ColumnTransformer as ct\n\n# 1.3 Data imputation\nfrom sklearn.impute import SimpleImputer\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1.4 Model building\nfrom sklearn.linear_model import LogisticRegression\n\n# 1.5 for ROC graphs & metrics\nimport scikitplot as skplt\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import average_precision_score\nimport sklearn.metrics as metrics\n\n# to make this notebook's output stable across runs\n#Somehow this is not happening as o/p of models is not consistent\nnp.random.seed(42)\n\n# Ignore useless warnings (see SciPy issue #5998)\nimport warnings\nfrom sklearn.exceptions import ConvergenceWarning\n\n# Filter out warnings from models\n\nwarnings.filterwarnings(action=\"ignore\", module=\"scipy\", message=\"^internal gelsd\")\nwarnings.filterwarnings('ignore', category = ConvergenceWarning)\nwarnings.filterwarnings('ignore', category = DeprecationWarning)\nwarnings.filterwarnings('ignore', category = UserWarning)\nwarnings.filterwarnings('ignore', category = FutureWarning)\n\n# 1.9 Misc\nimport gc","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a6f26316452144bfeff36474c36c32d1334668ba","trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score, make_scorer\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import make_pipeline\n\n\n# Custom scorer for cross validation\nscorer = make_scorer(f1_score, greater_is_better=True, average = 'macro')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1.3 Dimensionality reduction\nfrom sklearn.decomposition import PCA\n\n# 1.4 Data splitting and model parameter search\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom bayes_opt import BayesianOptimization\n\n# 1.5 Modeling modules\n# conda install -c anaconda py-xgboost\nfrom xgboost.sklearn import XGBClassifier","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"78a7710f39bab271c4320f42b9bbc7e3f9a4bd2f","trusted":true},"cell_type":"code","source":"# Model imports\nfrom sklearn.svm import LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.linear_model import LogisticRegressionCV, RidgeClassifierCV\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9c541a166077ce34eadcde7a702b4e1f0d3bbba6"},"cell_type":"markdown","source":"### Read in Data and Look at Summary Information"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"pd.options.display.max_columns = 150\n\n# Read in data\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n\n#train = pd.read_csv('train.csv')\n#test = pd.read_csv('test.csv')\ntrain.head()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8ae7f4f9dc01ad847283246327c7db467ba76498"},"cell_type":"markdown","source":"That gives us a look at all of the columns which don't appear to be in any order. To get a quick overview of the data we define `ExamineData`."},{"metadata":{"trusted":true},"cell_type":"code","source":"# 3.0 Let us understand train data\n# 3.1 Begin by defining some functions\ndef ExamineData(x):\n    \"\"\"Prints various data charteristics, given x\n    \"\"\"\n    print(\"Data shape:\", x.shape)\n    print(\"\\nColumns:\", x.columns)\n    print(\"\\nData types\\n\", x.dtypes)\n    print(\"\\nDescribe data\\n\", x.describe())\n    print(\"\\nData\\n\", x.head(2))\n    print (\"\\nSize of data:\", np.sum(x.memory_usage()))    # Get size of dataframes\n    print(\"\\nAre there any NULLS\\n\", np.sum(x.isnull()))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b180722a8e9a1688463f0340e4830e37a5586c4a","trusted":true},"cell_type":"code","source":"# 3.2 start examining data - commented after analysis due to large data dump on screen.\n#ExamineData(train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"35a4c874b82d853208f32d0d1dbaed422045f914"},"cell_type":"markdown","source":"This tells us there are 130 integer columns, 8 float (numeric) columns, and 5 object columns. The integer columns probably represent Boolean variables (that take on either 0 or 1) or [ordinal variables](https://www.ma.utexas.edu/users/mks/statmistakes/ordinal.html) with discrete ordered values. The object columns might pose an issue because they cannot be fed directly into a machine learning model.\n\nLet's glance at the test data which has many more rows (individuals) than the train. It does have one fewer column because there's no Target!"},{"metadata":{"_uuid":"61687f25325218b77d4d75804044c7083da85eb7","trusted":true},"cell_type":"code","source":"# commented after analysis due to large data dump on screen.\n#ExamineData(test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3c0d0640f3867d2c290ac58a55639fefb08875c0"},"cell_type":"markdown","source":"#### Integer Columns\n\nLet's look at the distribution of unique values in the integer columns. For each column, we'll count the number of unique values and show the result in a bar plot."},{"metadata":{},"cell_type":"markdown","source":"#### Define plotting function `PlotKDE`\n\n KDE plots of column values provided as 'x'\n    The following graphs shows the distributions of the float columns \n    colored by the value of the Target. \n    With these plots, we can see if there is a significant difference in the \n    variable distribution depending on the household poverty level.\n"},{"metadata":{"_uuid":"c22a1876cf1d8a717b0baab8564acb4a7f531b51","scrolled":false,"trusted":true},"cell_type":"code","source":"def PlotKDE(x):\n    \n\n    plt.figure(figsize = (20, 15))\n#    plt.style.use('fivethirtyeight')\n#    plt.style.available\n    plt.style.use('seaborn-pastel')\n\n    # Color mapping\n    colors = OrderedDict({1: 'red', 2: 'orange', 3: 'blue', 4: 'green'})\n    poverty_mapping = OrderedDict({1: 'extreme', 2: 'moderate', 3: 'vulnerable', 4: 'non vulnerable'})\n\n    # Iterate through the columns\n    for i, col in enumerate(x):\n        ax = plt.subplot(8, 5, i + 1)\n        # Iterate through the poverty levels\n        for poverty_level, color in colors.items():\n            # Plot each poverty level as a separate line\n            sns.kdeplot(train.loc[train['Target'] == poverty_level, col].dropna(), \n                        ax = ax, color = color, label = poverty_mapping[poverty_level])\n        \n        plt.title(f'{col.capitalize()} Distribution'); plt.xlabel(f'{col}'); plt.ylabel('Density')\n\n    plt.subplots_adjust(top = 2)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7db394756abedd215885bc7d6b9dc2c767964b35","trusted":true},"cell_type":"code","source":"#PlotKDE(train.select_dtypes('int64'))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1a9db9a6caa3b77b6a6f0184644c1e11b2ed53d5"},"cell_type":"markdown","source":"The columns with only 2 unique values represent Booleans (0 or 1). In a lot of cases, this boolean information is already on a household level. For example, the `refrig` column says whether or not the household has a refrigerator. When it comes time to make features from the Boolean columns that are on the household level, we will _not need to aggregate_ these. However, the Boolean columns that are on the individual level will need to be aggregated. \n\n#### Float Columns\n\nAnother column type is floats which represent continuous variables. We can make a quick distribution plot to show the distribution of all float columns. We'll use an [`OrderedDict`](https://pymotw.com/2/collections/ordereddict.html) to map the poverty levels to colors because this keeps the keys and values in the same order as we specify (unlike a regular Python dictionary).\n\nThe following graphs shows the distributions of the `float` columns colored by the value of the `Target`. With these plots, we can see if there is a significant difference in the variable distribution depending on the household poverty level."},{"metadata":{"trusted":true},"cell_type":"code","source":"# 3.4 Visual examination of float columns\nPlotKDE(train.select_dtypes('float'))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"79ec245f541336c564fdce3d013b0b988627b6e5"},"cell_type":"markdown","source":"Later on we'll calculate correlations between the variables and the `Target` to gauge the relationships between the features, but these plots can already give us a sense of which variables may be most \"relevant\" to a model. For example, the `meaneduc`, representing the average education of the adults in the household appears to be related to the poverty level: __a higher average adult education leads to higher values of the target which are less severe levels of poverty__. The theme of the importance of education is one we will come back to again and again in this notebook! "},{"metadata":{"_uuid":"355fd9746ec30af6661cf97a1243790730a5df99"},"cell_type":"markdown","source":"#### Object Columns\n"},{"metadata":{"_uuid":"d1e660b7e06b81b61bec81a82be4a5854115d09a"},"cell_type":"markdown","source":"The `Id` and `idhogar` object types make sense because these are identifying variables. However, the other columns seem to be a mix of strings and numbers which we'll need to address before doing any machine learning. According to the documentation for these columns:\n\n* `dependency`: Dependency rate, calculated = (number of members of the household younger than 19 or older than 64)/(number of member of household between 19 and 64)\n* `edjefe`: years of education of male head of household, based on the interaction of escolari (years of education), head of household and gender, yes=1 and no=0\n* `edjefa`: years of education of female head of household, based on the interaction of escolari (years of education), head of household and gender, yes=1 and no=0\n\ndependency, edjefe, edjefa:\n        For these three variables, \"yes\" = 1 and \"no\" = 0 \n        We can correct the variables using a mapping and convert to floats."},{"metadata":{"_uuid":"9b110b76084ba14045c4c69ce909148d91ce0ecc","trusted":true},"cell_type":"code","source":"# 3.5 Object data types\n\ntrain.select_dtypes('object').head()\n\nmapObj = {\"yes\": 1, \"no\": 0}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"acf4a663dd5d6713dad025ef6674b547cc02690f","trusted":true},"cell_type":"code","source":"# Apply same operation to both train and test\nfor df in [train, test]:\n    # Fill in the values with the correct mapping\n    df['dependency'] = df['dependency'].replace(mapObj).astype(np.float64)\n    df['edjefa'] = df['edjefa'].replace(mapObj).astype(np.float64)\n    df['edjefe'] = df['edjefe'].replace(mapObj).astype(np.float64)\n\ntrain[['dependency', 'edjefa', 'edjefe']].describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"55af5988853fd538f674b07f93d5655c133ed744","trusted":true},"cell_type":"code","source":"PlotKDE(train.select_dtypes('float')) # the parameters are now classified as float","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e2fb233b0d5876e71ba33e6b5a8d29e2429fab94"},"cell_type":"markdown","source":"These variables are now correctly represented as numbers and can be fed into a machine learning model. \n\nJoining `test` and `train` dataframes before starting with Feature Engineering.\n    In feature engineering the same operations should be applied to both dataframes so we end up with same set of features.\n    Later we can separate out the sets based on the `Target` value.\n    Test data will have 'null' values in `Target` Column.\n#### Mental Note Anirudh"},{"metadata":{"_uuid":"59a54206932e6414745226a101097b69b7845107","trusted":true},"cell_type":"code","source":"# 4.1 filling up column Target in test with nan\ntest['Target'] = np.nan\n\n#4.2 appending test to train\nX = train.append(test, ignore_index = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#4.3.1 Shape\ntrain.shape #(9557, 143)\ntest.shape #(23856, 143)\nX.shape #Sum of test and train: (33413, 143)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#4.3.2 info\ntrain.info()\ntest.info()\nX.info() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"for X: mem usage and RangeIndex = train + test"},{"metadata":{"_uuid":"423ee29cdbb0975eb9d8f6f729e1595f401acbc3"},"cell_type":"markdown","source":"## Exploring Label Distribution\n"},{"metadata":{"_uuid":"6ed7b957df6b758a5bf586c682289edcb1a78f66","trusted":true},"cell_type":"code","source":"#4.4 Exploring Data distribution across classes\n#4.4.1 Extract the records for heads of household where 'parentesco1==1'\nX_heads = X.loc[X['parentesco1']==1].copy() #Make a copy to preserve X\nX_heads.info() #10307 entries, 0 to 33409\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#4.4.2 look at label distribution where 'Target is notnull'\nX_heads_labels = X_heads.loc[(X_heads['Target'].notnull()), ['Target']]\nX_heads_labels_counts = X_heads_labels['Target'].value_counts().sort_index()\nX_heads_labels_counts ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2876c882d7be2bc7ab58bb0dd0dc09c09bcc65a5"},"cell_type":"markdown","source":"imbalanced class with many more households classified as 4.0 i.e. non vulnerable"},{"metadata":{"_uuid":"4b4e7cebd091b34469e78c123eac55410dfb8e77"},"cell_type":"markdown","source":"## Addressing Wrong Labels\n\n####4.5 Exploring classification of members in a household and \n####    correcting errors as per directions in the challenge i.e. head-of-household as correct label\n"},{"metadata":{"_uuid":"81b7d45bfe8fd0feb09c307c5f2c9f00a6f8bc73","trusted":true},"cell_type":"code","source":"#4.5.1 Grouping by headh of household 'idhogar' and adding \n#train.groupby('idhogar').size() #Length: 2988\ntrain_ok = train.groupby('idhogar')['Target'].apply(lambda x: x.nunique() == 1)                    \ntrain_ok.size #2988 households","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#4.5.2 Identify the labels with errors\ntrain_notok = train_ok[train_ok != True]\ntrain_notok.size #85 labels have errors","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c3c17e4f9a62990e08d9704d51917b0ace16d54f","trusted":true},"cell_type":"code","source":"#4.5.3 View one example of incorrect labels\ntrain[train['idhogar'] == train_notok.index[2]][['Id', 'idhogar', 'parentesco1', 'Target']]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#4.5.4 Fix the labels correctly\n\nfor not_ok_id in train_notok.index:\n    # Find correct Target value for head of household\n    # not_ok_id\n    ok_target = int(train[(train['idhogar'] == not_ok_id) & (train['parentesco1'] == 1.0)]['Target'])\n    \n    # Set the correct label for all members in the household\n    train.loc[train['idhogar'] == not_ok_id, 'Target'] = ok_target","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking - Trying query function of dataframe\ntrain_check = pd.DataFrame(train.groupby('idhogar')['Target'].apply(lambda x: x.nunique() == 1))\ntrain_check.query(\"Target != True\") #Empty DataFrame here is a Success!","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.6 How many families are there without head of household\nX_heads.info() has 10307 entries i.e. ['parentesco1']==1\n    train_ok.size returns 2988 households i.e. groupby('idhogar')\n    If a household does not have a head, then there is no golden value of a label.\n    We can't use any training data wherein household is without a head\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#4.6.1 how many huseholds have parentesco1\ntrain_heads = pd.DataFrame(train.groupby('idhogar')['parentesco1'].sum())\ntrain_heads.size #2988 records\ntrain_heads.query(\"parentesco1 > 1\").count() #just checking -- 0 households have more than one head of household\ntrain_heads.query(\"parentesco1 == 1\").count() #just checking -- 2973 households OK\n\ntrain_heads.query(\"parentesco1 < 1\").count() #15 households do not have head of household\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#4.6.2 How many of the households have sum(parentesco1) computed in 4.6.1 as zero\n\"\"\" Cannot use these households data \"\"\"\ntrain_heads_no = train_heads.query(\"parentesco1 == 0\") #15 unique 'idhogar's\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5c309e0206872b450409ec6bc78e74f49bcaf206"},"cell_type":"markdown","source":"## Missing Values\n\nfind out missing values by column"},{"metadata":{"_uuid":"47dd23a8222bd1d89512662dcb299b2ee6b6b04e","trusted":true},"cell_type":"code","source":"# Number of missing in each column\nmissing = pd.DataFrame(X.isnull().sum()).rename(columns = {0: 'total'})\n\n# Create a percentage missing\nmissing['percent'] = missing['total'] / len(X)\n\nmissing.sort_values('percent', ascending = False).head(7).drop('Target')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7134602d0d3cd3045fb97eaaa724592b2d23553a"},"cell_type":"markdown","source":" `Target` was dropped because we made that `NaN` for test data.\n\n__v18q1__: Number of tablets owned by a family\n\nThis is a household variable so only select rows for head of household.\n\n"},{"metadata":{"_uuid":"7d5df5fccf64cd3ccecbb1c1bf2c2133a77c31e5","trusted":true},"cell_type":"code","source":"#X_heads['v18q1'].value_counts().sort_index()\nX_heads['v18q1'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_heads['v18q1'].value_counts().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_heads['v18q1'].isnull().describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cd53530fcdb192d6f7b6512c33ca134cf7984bed"},"cell_type":"markdown","source":"while '1' is most common value, 8044 `null` exist in this category. \n `v18q` indicates whether a family owns a tablet.\n\n`groupby`  `v18q` and check `v18q1` for null values.\nif all 8044 Null are on '0' we know these families do not own a tablet"},{"metadata":{"_uuid":"19c44193187c0db95649a60447b6e0fe43703795","trusted":true},"cell_type":"code","source":"X_heads.groupby('v18q')['v18q1'].apply(lambda x: x.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"adff3153e886d7adc1ced34c157a747fd0c4e9a1"},"cell_type":"markdown","source":"we can fill in missing value for `v18q1` with zero."},{"metadata":{"_uuid":"20291bd2766b006bca3255034c4d1432e8024b0d","trusted":true},"cell_type":"code","source":"X['v18q1'] = X['v18q1'].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"42662b00e496efdd18cf3fc9cf497604cf873896"},"cell_type":"markdown","source":"__v2a1__: Monthly rent payment\n\nThe next missing column is `v2a1` which represents the montly rent payment. "},{"metadata":{"_uuid":"d072b3df8b6486cb90c1e9832094c7e13680a856"},"cell_type":"markdown","source":"we use the home ownership variable below:\n\n    tipovivi1 =1 own and fully paid house\n    tipovivi2 =1 own,  paying installment\n    tipovivi3 =1 rented\n    tipovivi4 =1 precarious\n    tipovivi5 =1 other\n    "},{"metadata":{"_uuid":"6ecfd2a5e15efa4ffe5436f43e84dbb5ea929314","trusted":true},"cell_type":"code","source":"# Fill in households that own the house with 0 rent payment\nX.loc[(X['tipovivi1'] == 1), 'v2a1'] = 0\n\n# Create missing rent payment column\nX['v2a1-missing'] = X['v2a1'].isnull()\n\nX['v2a1-missing'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2e147ff8825964c53c634b05ad6653ebf8390539"},"cell_type":"markdown","source":"__rez_esc__: years behind in school\n\nFinding the ages of those who have a missing value in this column and the ages of those who do not have a missing value."},{"metadata":{"_uuid":"d9576b2de07fbab360435f26d3d869f4f3b8a269","trusted":true},"cell_type":"code","source":"X.loc[X['rez_esc'].notnull()]['age'].describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b7f67c2d888db26770ff3c0e108d003b34137d4a"},"cell_type":"markdown","source":"oldest age with missing value is 17"},{"metadata":{"_uuid":"b8b91108b4f7029b0642dd68052cc4b6d5906316","trusted":true},"cell_type":"code","source":"X.loc[X['rez_esc'].isnull()]['age'].describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fdc641191009f43a7d3407f46c41dc89bb0fc551"},"cell_type":"markdown","source":"For this variable, if age > 19 or age < 7 and missing value we set it to zero (outside of school)\nothers let the imputation take care of it"},{"metadata":{"_uuid":"791b94a06ade9896fd5a7d6dc38152f831642a6a","trusted":true},"cell_type":"code","source":"# If individual is over 19 or younger than 7 and missing years behind, set it to 0\nX.loc[((X['age'] > 19) | (X['age'] < 7)) & (X['rez_esc'].isnull()), 'rez_esc'] = 0\n\n# Add a flag for those between 7 and 19 with a missing value\nX['rez_esc-missing'] = X['rez_esc'].isnull()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bce88b0ebef5a8d3eeb3ca4f978a0d4a69b1e185"},"cell_type":"markdown","source":"There is also one outlier in the `rez_esc` column. the maximum value for this variable is 5."},{"metadata":{"_uuid":"18a972cc48d9e4818777afb7b06eab7e1f97855a","trusted":true},"cell_type":"code","source":"X.loc[X['rez_esc'] > 5, 'rez_esc'] = 5","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"989a4051c0166d6d910ff526fc30f50a3b58f969"},"cell_type":"markdown","source":"# Feature Engineering\n\n"},{"metadata":{"_uuid":"81fa4da4a8733fdfdd756e0a946ffb8a799fc51c"},"cell_type":"markdown","source":"### Define Variable Categories\n\n1. Individual Variables\n    * Boolean\n    * Integers with an ordering\n2. Household variables\n    * Boolean\n    * Integers with an ordering\n    * Continuous\n3. Squared Variables: derived variables\n4. Id variables: not used\n"},{"metadata":{"_uuid":"9e5a3b03ca064e4dc055e785730d1a43af037277","trusted":true},"cell_type":"code","source":"id_ = ['Id', 'idhogar', 'Target']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dc6f4e6c5f343e917d65a1d8e6c1043236bcdce1","trusted":true},"cell_type":"code","source":"ind_bool = ['v18q', 'dis', 'male', 'female', 'estadocivil1', 'estadocivil2', 'estadocivil3', \n            'estadocivil4', 'estadocivil5', 'estadocivil6', 'estadocivil7', \n            'parentesco1', 'parentesco2',  'parentesco3', 'parentesco4', 'parentesco5', \n            'parentesco6', 'parentesco7', 'parentesco8',  'parentesco9', 'parentesco10', \n            'parentesco11', 'parentesco12', 'instlevel1', 'instlevel2', 'instlevel3', \n            'instlevel4', 'instlevel5', 'instlevel6', 'instlevel7', 'instlevel8', \n            'instlevel9', 'mobilephone', 'rez_esc-missing']\n\nind_ordered = ['rez_esc', 'escolari', 'age']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b382ddbea1cd6ff25b71f13b9263a6565ebadd57","trusted":true},"cell_type":"code","source":"hh_bool = ['hacdor', 'hacapo', 'v14a', 'refrig', 'paredblolad', 'paredzocalo', \n           'paredpreb','pisocemento', 'pareddes', 'paredmad',\n           'paredzinc', 'paredfibras', 'paredother', 'pisomoscer', 'pisoother', \n           'pisonatur', 'pisonotiene', 'pisomadera',\n           'techozinc', 'techoentrepiso', 'techocane', 'techootro', 'cielorazo', \n           'abastaguadentro', 'abastaguafuera', 'abastaguano',\n            'public', 'planpri', 'noelec', 'coopele', 'sanitario1', \n           'sanitario2', 'sanitario3', 'sanitario5',   'sanitario6',\n           'energcocinar1', 'energcocinar2', 'energcocinar3', 'energcocinar4', \n           'elimbasu1', 'elimbasu2', 'elimbasu3', 'elimbasu4', \n           'elimbasu5', 'elimbasu6', 'epared1', 'epared2', 'epared3',\n           'etecho1', 'etecho2', 'etecho3', 'eviv1', 'eviv2', 'eviv3', \n           'tipovivi1', 'tipovivi2', 'tipovivi3', 'tipovivi4', 'tipovivi5', \n           'computer', 'television', 'lugar1', 'lugar2', 'lugar3',\n           'lugar4', 'lugar5', 'lugar6', 'area1', 'area2', 'v2a1-missing']\n\nhh_ordered = [ 'rooms', 'r4h1', 'r4h2', 'r4h3', 'r4m1','r4m2','r4m3', 'r4t1',  'r4t2', \n              'r4t3', 'v18q1', 'tamhog','tamviv','hhsize','hogar_nin',\n              'hogar_adul','hogar_mayor','hogar_total',  'bedrooms', 'qmobilephone']\n\nhh_cont = ['v2a1', 'dependency', 'edjefe', 'edjefa', 'meaneduc', 'overcrowding']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2771c641c24bb3ed21eb655320a4ad85066a2cd6","trusted":true},"cell_type":"code","source":"sqr_ = ['SQBescolari', 'SQBage', 'SQBhogar_total', 'SQBedjefe', \n        'SQBhogar_nin', 'SQBovercrowding', 'SQBdependency', 'SQBmeaned', 'agesq']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8aa08338c60ae323e4e4d6654f1c4380befd5c76","trusted":true},"cell_type":"code","source":"x = ind_bool + ind_ordered + id_ + hh_bool + hh_ordered + hh_cont + sqr_\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"39d839a711f0783285ff0ca570366f045eb995f1"},"cell_type":"markdown","source":"#### Squared Variables\n\nRemoving all of the squared variables these features are redundant and highly correlated"},{"metadata":{"_uuid":"b258b0875757bc29016c020164b21021b63a7bcc","trusted":true},"cell_type":"code","source":"# Remove squared variables\nX = X.drop(columns = sqr_)\nX.shape #(33413, 136)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"68b5104b73966a357e6c278fccb562f52ba29863"},"cell_type":"markdown","source":"## Id Variables\n\nkept as is for identification\n\n## Household Variables\n\nheads of household"},{"metadata":{"_uuid":"f3c177da83980642540c763666f6859d59d506aa","trusted":true},"cell_type":"code","source":"heads = X.loc[X['parentesco1'] == 1, :]\nheads = heads[id_ + hh_bool + hh_cont + hh_ordered]\nheads.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"747de840f4fe3374f8ef95637b9948a1be5a737b"},"cell_type":"markdown","source":"## Feature Construction\n"},{"metadata":{"_uuid":"9884e666ffb55adacc01931675170f39ab0886cf"},"cell_type":"markdown","source":"Household feature we create is a `bonus` where a family gets a point for having a refrigerator, computer, tablet, or television"},{"metadata":{"_uuid":"edf1cf9043391a9af16c45e19e84bbc91dce052a","trusted":true},"cell_type":"code","source":"# Owns a refrigerator, computer, tablet, and television\nheads['bonus'] = 1 * (heads['refrig'] + \n                      heads['computer'] + \n                      (heads['v18q1'] > 0) + \n                      heads['television'])\n\nsns.violinplot('bonus', 'Target', data = heads,\n                figsize = (10, 6));\nplt.title('Target vs Bonus Variable');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"97bfbf33dddaed18073ba92bda62305cc38a770f"},"cell_type":"markdown","source":"## Per Capita Features\n\nper-capita features in the household"},{"metadata":{"_uuid":"cc7c411527e8fd9955e8ab83bcec4c78322c6355","trusted":true},"cell_type":"code","source":"heads['phones-per-capita'] = heads['qmobilephone'] / heads['tamviv']\nheads['tablets-per-capita'] = heads['v18q1'] / heads['tamviv']\nheads['rooms-per-capita'] = heads['rooms'] / heads['tamviv']\nheads['rent-per-capita'] = heads['v2a1'] / heads['tamviv']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cb4e91d2ce09ff60e61ec1f3be16932ed7d0acd5","trusted":true},"cell_type":"code","source":"household_feats = list(heads.columns)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"de174018faf038f680d405baa68a2aba93ab1b77"},"cell_type":"markdown","source":"# Individual Level Variables\n\nThere are two types of individual level variables: Boolean (1 or 0 for True or False) and ordinal (discrete values with a meaningful ordering). "},{"metadata":{"_uuid":"88e7cc6cd8acab811702215a2dc2b08b8e203dd2","trusted":true},"cell_type":"code","source":"ind = X[id_ + ind_bool + ind_ordered]\nind.shape #(33413, 40)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b5feba5e8b3843a9e255497497184b5e8ab4754f"},"cell_type":"markdown","source":"we have both male and female, remove the male column."},{"metadata":{"_uuid":"e34832557ff343d282c11259eb1e1e4c477b0158","trusted":true},"cell_type":"code","source":"ind = ind.drop(columns = 'male')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"132986030dc283109318780a2f6330a75b72e3aa"},"cell_type":"markdown","source":"### Feature Construction\n\nWe can make a few features using the existing data. For example, we can divide the years of schooling by the age."},{"metadata":{"_uuid":"f673326c23b7d9b0806924f05af22c0601a07798","trusted":true},"cell_type":"code","source":"ind['escolari/age'] = ind['escolari'] / ind['age']\n\nplt.figure(figsize = (10, 8))\nsns.violinplot('Target', 'escolari/age', data = ind);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fbaef320687392da25e028e2021e5d5be574bb7e"},"cell_type":"markdown","source":"## Feature Engineering through Aggregations\n\naggregate individual data for each household.\n `groupby`  `idhogar` and `agg`"},{"metadata":{"_uuid":"f59c9612532cfc67c2551268370b662d03fe84ed","trusted":true},"cell_type":"code","source":"# Group and aggregate\nind_agg = ind.drop(columns = 'Target').groupby('idhogar').agg(['min', 'max', 'sum', 'count', 'std'])\nind_agg.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1eb6430fcba5d962ade9aa7e866bb6f16892057e"},"cell_type":"markdown","source":"185 features. rename the columns to keep track"},{"metadata":{"_uuid":"b444e21422941ca1c26b0b3404c3e2a16d5af991","trusted":true},"cell_type":"code","source":"# Rename the columns\nnew_col = []\nfor c in ind_agg.columns.levels[0]:\n    for stat in ind_agg.columns.levels[1]:\n        new_col.append(f'{c}-{stat}')\n        \nind_agg.columns = new_col\nind_agg.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f196a265f84ec22177da1822d0d732ff6f886d13","trusted":true},"cell_type":"code","source":"ind_agg.iloc[:, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]].head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d1f7bc5fc7635bfbee47507666fa844fbeef8c50"},"cell_type":"markdown","source":"merge `ind` with `heads` for final dataset"},{"metadata":{"_uuid":"bccbcdbad813f7c1596fd8209de3df46e18db199","trusted":true},"cell_type":"code","source":"ind_feats = list(ind_agg.columns)\n\n# Merge on the household id\nfinal = heads.merge(ind_agg, on = 'idhogar', how = 'left')\n\nprint('Final features shape: ', final.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3e0b6e903c0780560d5e3b4283dcca8be3a79cf3","trusted":true},"cell_type":"code","source":"final.head() #289 columns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"18187d9d6e401a27362372a3ec361a40bc157d25"},"cell_type":"markdown","source":"Gender for head of household"},{"metadata":{"_uuid":"bc12c2723bb2867b7890bba79fe0be1887ba3a4f","trusted":true},"cell_type":"code","source":"head_gender = ind.loc[ind['parentesco1'] == 1, ['idhogar', 'female']]\nfinal = final.merge(head_gender, on = 'idhogar', how = 'left').rename(columns = {'female': 'female-head'})","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1ba7d757545ead1b25ab27bd2bf149ac9819e598","trusted":true},"cell_type":"code","source":"final.groupby('female-head')['Target'].value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b251e03cfb73b70ecb1563150ed6ce5a5318cdb3"},"cell_type":"markdown","source":"households with head as female are more likely to be poorer"},{"metadata":{"_uuid":"03b2c39589c3488a9f1241aed8760718fb9f3176","trusted":true},"cell_type":"code","source":"sns.violinplot(x = 'female-head', y = 'Target', data = final);\nplt.title('Target by Female Head of Household');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7d16a374bc37c67c427a2dac99d5be12fed49ee3"},"cell_type":"markdown","source":"# Machine Learning Modeling\n\ngetting started! \nRandom Forest Classifier to establish a baseline. \nLater Gradient Boosting Machine.\n\nTo assess our model, we'll use 10-fold cross validation on the training data.\n`F1 Macro` measure to evaluate performance."},{"metadata":{"_uuid":"8037345f25c39fdaa811adfd38f65714678359a2","trusted":true},"cell_type":"code","source":"# Labels for training\ntrain_labels = np.array(list(final[final['Target'].notnull()]['Target'].astype(np.uint8)))\n\n# Extract the training data\ntrain_set = final[final['Target'].notnull()].drop(columns = ['Id', 'idhogar', 'Target'])\ntest_set = final[final['Target'].isnull()].drop(columns = ['Id', 'idhogar', 'Target'])\n\n# Submission base which is used for making submissions to the competition\nsubmission_base = test[['Id', 'idhogar']].copy()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e4459fe1d50077ba09995fb01664ade706905d10","trusted":true},"cell_type":"code","source":"features = list(train_set.columns)\n\npipeline = Pipeline([('imputer', Imputer(strategy = 'median')), \n                      ('scaler', MinMaxScaler())])\n\n# Fit and transform training data\ntrain_set = pipeline.fit_transform(train_set)\ntest_set = pipeline.transform(test_set)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model-1: RandomForestClassifier\n"},{"metadata":{"_uuid":"b1b0fde920b9cb7caa2653e417b9cfb7cb3db075","trusted":true},"cell_type":"code","source":"model_RF = RandomForestClassifier(n_estimators=100, random_state=10, \n                               n_jobs = -1)\n# 10 fold cross validation\ncv_score_RF = cross_val_score(model_RF, train_set, train_labels, cv = 10, scoring = scorer)\n\nprint(f'10 Fold Cross Validation F1 Score = {round(cv_score_RF.mean(), 4)} with std = {round(cv_score_RF.std(), 4)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model-2: LogisticRegression With L2 Penalty"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_LRL2 = LogisticRegression(C=0.1, penalty='l2', random_state=10, n_jobs = -1)\n# 10 fold cross validation\ncv_score_LRL2 = cross_val_score(model_LRL2, train_set, train_labels, cv = 10, scoring = scorer)\n\nprint(f'10 Fold Cross Validation F1 Score = {round(cv_score_LRL2.mean(), 4)} with std = {round(cv_score_LRL2.std(), 4)}')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b68367d25404464a1e62f49eab871f4fd97d75e8"},"cell_type":"markdown","source":"F1 Score = 0.2933 with std = 0.0455; lower than baseline 0.3368 - along expected lines, but better to check! "},{"metadata":{"_uuid":"d93a45c46832d0b7804a112925d19bb5739e586b"},"cell_type":"markdown","source":"## Feature Importances\n\nWith a tree-based model, we can look at the feature importances which show a relative ranking of the usefulness of features in the model. These represent the sum of the reduction in impurity at nodes that used the variable for splitting, but we don't have to pay much attention to the absolute value. Instead we'll focus on relative scores.\n\nIf we want to view the feature importances, we'll have to train a model on the whole training set. Cross validation does not return the feature importances. "},{"metadata":{"_uuid":"76e8463384bd4b81f6d6ba8288b0684571bac380","trusted":true},"cell_type":"code","source":"model_RF.fit(train_set, train_labels)\n\n# Feature importances into a dataframe\nfeature_importances = pd.DataFrame({'feature': features, 'importance': model_RF.feature_importances_})\nfeature_importances.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a474c17b1394ef6c008c6b9133fd40b4d8f04e98"},"cell_type":"markdown","source":"plot the feature importances for visual analysis. The number of relevant features will give us some data points for PCA. "},{"metadata":{"_uuid":"d9912b9bdd0204aea423ec6e610f7788726ed038","trusted":true},"cell_type":"code","source":"def plot_feature_importances(df, n = 15, threshold = 0.95):\n    \"\"\"Plots n most important features. Also plots the cumulative importance \n    \n    Args:\n        df (dataframe): Dataframe of feature importances. Columns must be \"feature\" and \"importance\".\n    \n        n (int): Number of most important features to plot. Default is 15.\n    \n        threshold (float): Threshold for cumulative importance plot. Default is 95%.\n        \n    Returns:\n        df (dataframe): Dataframe ordered by feature importances with a normalized column (sums to 1) \n                        and a cumulative importance column\n    \n    Note:\n    \n        * Normalization in this case means sums to 1. \n        * Cumulative importance is calculated by summing features from most to least important\n        * A threshold of 0.95 will show the most important features needed to reach 95% of cumulative importance\n    \n    \"\"\"\n    plt.style.use('fivethirtyeight')\n    \n    # Sort features with most important at the head\n    df = df.sort_values('importance', ascending = False).reset_index(drop = True)\n    \n    # Normalize the feature importances to add up to one and calculate cumulative importance\n    df['importance_normalized'] = df['importance'] / df['importance'].sum()\n    df['cumulative_importance'] = np.cumsum(df['importance_normalized'])\n    \n    plt.rcParams['font.size'] = 12\n    \n    # Bar plot of n most important features\n    df.loc[:n, :].plot.barh(y = 'importance_normalized', \n                            x = 'feature', color = 'darkgreen', \n                            edgecolor = 'k', figsize = (12, 8),\n                            legend = False, linewidth = 2)\n\n    plt.xlabel('Normalized Importance', size = 18); plt.ylabel(''); \n    plt.title(f'{n} Most Important Features', size = 18)\n    plt.gca().invert_yaxis()\n    \n    \n    if threshold:\n        # Cumulative importance plot\n        plt.figure(figsize = (8, 6))\n        plt.plot(list(range(len(df))), df['cumulative_importance'], 'b-')\n        plt.xlabel('Number of Features', size = 16); plt.ylabel('Cumulative Importance', size = 16); \n        plt.title('Cumulative Feature Importance', size = 18);\n        \n        # Number of features needed for threshold cumulative importance\n        # This is the index (will need to add 1 for the actual number)\n        importance_index = np.min(np.where(df['cumulative_importance'] > threshold))\n        \n        # Add vertical line to plot\n        plt.vlines(importance_index + 1, ymin = 0, ymax = 1.05, linestyles = '--', colors = 'red')\n        plt.show();\n        \n        print('{} features required for {:.0f}% of cumulative importance.'.format(importance_index + 1, \n                                                                                  100 * threshold))\n    \n    return df\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c0d04a9495e7be357599a1c91ea196908efd0624","trusted":true},"cell_type":"code","source":"norm_fi = plot_feature_importances(feature_importances, threshold=0.95)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5403f4fdc5a5bd8192a2ec3a09f062d8573f3c1b"},"cell_type":"markdown","source":"__Education & Age__ education and Age related parameters are most relevant. \nInteresting to see that two of the top three parameters escolari-max and escolari/age-sum are in top three.\nActually analysis of top parameters reveal that they are education and age related.\n\nWe need 185 of the 287 features to account for 95% of the importance. \nThis will be useful to tune PCA."},{"metadata":{"_uuid":"fb3574b0f4d56a904729cc7c800e5619e3e89e8f"},"cell_type":"markdown","source":"# Model Selection\n\nThe baseline model: Random Forest Classifier with F1 Score = 0.3368. \n\n1. write a function that can evaluate a model. \n2. try other models from SciKit-learn or h2o.\n3. make a dataframe to hold the results for each model."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dataframe to hold results\nmodel_results = pd.DataFrame(columns = ['model', 'cv_mean', 'cv_std'])\n\nmodel_results = model_results.append(pd.DataFrame({\n    'model': 'RandomForestClassifier', \n    'cv_mean': cv_score_RF.mean(), \n    'cv_std': cv_score_RF.std()}, \n    index = [0]),\n                                     ignore_index = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_results = model_results.append(pd.DataFrame({\n    'model': 'LogisticRegression', \n    'cv_mean': cv_score_LRL2.mean(), \n    'cv_std': cv_score_LRL2.std()}, \n    index = [0]),\n                                     ignore_index = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"665cd410e664a6092b756a888727f166fbb5a5ed","trusted":true},"cell_type":"code","source":"\ndef cv_model(train, train_labels, model, name, model_results=None):\n    \"\"\"Perform 10 fold cross validation of a model\"\"\"\n    \n    cv_scores = cross_val_score(model, train, train_labels, cv = 10, scoring=scorer, n_jobs = -1)\n    print(f'10 Fold CV Score: {round(cv_scores.mean(), 5)} with std: {round(cv_scores.std(), 5)}')\n    \n    if model_results is not None:\n        model_results = model_results.append(pd.DataFrame({'model': name, \n                                                           'cv_mean': cv_scores.mean(), \n                                                            'cv_std': cv_scores.std()},\n                                                           index = [0]),\n                                             ignore_index = True)\n\n        return model_results","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model-3: LinearSVC"},{"metadata":{"_uuid":"ab07945c331294a68d600de58e4ed9c2739473bf","trusted":true},"cell_type":"code","source":"model_results = cv_model(train_set, train_labels, LinearSVC(), \n                         'LSVC', model_results)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2ea1ec18bda63d33c6e6482de5361d1728f41bbd"},"cell_type":"markdown","source":"LSVC : 10 Fold CV Score: 0.28658 with std: 0.04568 -- lower than baseline 0.3368"},{"metadata":{},"cell_type":"markdown","source":"# Model-4: GaussianNB"},{"metadata":{"_uuid":"e5c13ea9fb13134e3b25d59bedf5b9f974e1517c","trusted":true},"cell_type":"code","source":"model_results = cv_model(train_set, train_labels, \n                         GaussianNB(), 'GNB', model_results)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2ea1ec18bda63d33c6e6482de5361d1728f41bbd"},"cell_type":"markdown","source":"GaussianNB : 10 Fold CV Score: 0.18349 with std: 0.04377 -- lower than baseline 0.3368"},{"metadata":{},"cell_type":"markdown","source":"# Model-5: MLPClassifier"},{"metadata":{"_uuid":"8b1dca57fa07e406f1fe81fa320b3b97ab078d8e","trusted":true},"cell_type":"code","source":"model_results = cv_model(train_set, train_labels, \n                         MLPClassifier(hidden_layer_sizes=(32, 64, 128, 64, 32)),\n                         'MLP', model_results)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2ea1ec18bda63d33c6e6482de5361d1728f41bbd"},"cell_type":"markdown","source":"MLPClassifier : 10 Fold CV Score: 0.30368 with std: 0.0615 -- lower than baseline 0.3368\n\nHowever, with tuning can get better."},{"metadata":{},"cell_type":"markdown","source":"# Model-6: LinearDiscriminantAnalysis"},{"metadata":{"_uuid":"2310f6d79426e16c9e3af6f8d4bfe03aa847d1b7","trusted":true},"cell_type":"code","source":"model_results = cv_model(train_set, train_labels, \n                          LinearDiscriminantAnalysis(), \n                          'LDA', model_results)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2ea1ec18bda63d33c6e6482de5361d1728f41bbd"},"cell_type":"markdown","source":"LinearDiscriminantAnalysis : 10 Fold CV Score: 0.32088 with std: 0.0568 -- slightly lower than baseline 0.3368\n\nHowever, with tuning can get better."},{"metadata":{},"cell_type":"markdown","source":"# Model-7: RidgeClassifierCV"},{"metadata":{"_uuid":"5c5c3d651e96cfe01af34f93e6faa501fa35a94f","trusted":true},"cell_type":"code","source":"model_results = cv_model(train_set, train_labels, \n                         RidgeClassifierCV(), 'RIDGE', model_results)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2ea1ec18bda63d33c6e6482de5361d1728f41bbd"},"cell_type":"markdown","source":"RidgeClassifierCV : 10 Fold CV Score: 0.28046 with std: 0.03179 -- much lower than baseline 0.3368\n"},{"metadata":{},"cell_type":"markdown","source":"# Model-8: KNeighborsClassifier"},{"metadata":{"_uuid":"b10b93b0caed61b6b54df8acfdda3e144c6499cf","trusted":true},"cell_type":"code","source":"for n in [5, 10, 20]:\n    print(f'\\nKNN with {n} neighbors\\n')\n    model_results = cv_model(train_set, train_labels, \n                             KNeighborsClassifier(n_neighbors = n),\n                             f'knn-{n}', model_results)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2ea1ec18bda63d33c6e6482de5361d1728f41bbd"},"cell_type":"markdown","source":"KNeighborsClassifier:\n\nKNN with 5 neighbors - 10 Fold CV Score: 0.34332 with std: 0.0298 -- better than baseline 0.3368 (first model to do so)\n\nKNN with 10 neighbors - 10 Fold CV Score: 0.33688 with std: 0.03935 -- equal to baseline 0.3368\n\nKNN with 20 neighbors - 10 Fold CV Score: 0.30568 with std: 0.03353 -- lower than baseline 0.3368\n\nConclusions from KNN:\n1. KNN with 5 neighbors perfomrs best\n2. KNN performance goes down with increase in number of neighbors\n"},{"metadata":{},"cell_type":"markdown","source":"# Model-9: ExtraTreesClassifier"},{"metadata":{"_uuid":"eeab91bfb4b953e0c7aa5112625b65268537c662","trusted":true},"cell_type":"code","source":"model_results = cv_model(train_set, train_labels, \n                         ExtraTreesClassifier(n_estimators = 100, random_state = 10),\n                         'EXT', model_results)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2ea1ec18bda63d33c6e6482de5361d1728f41bbd"},"cell_type":"markdown","source":"ExtraTreesClassifier : 10 Fold CV Score: 0.3276 with std: 0.03164 -- slightly lower than baseline 0.3368\n"},{"metadata":{"_uuid":"4f4a3dc32b62b3fcc625e54f8d92afd9f1de2e7d"},"cell_type":"markdown","source":"## Comparing Model Performance\n\nWith the modeling results in a dataframe, we can plot them to see which model does the best."},{"metadata":{"_uuid":"8cbd9d6729288337c5c3cace44af254ebd480fce","trusted":true},"cell_type":"code","source":"model_results.set_index('model', inplace = True)\nmodel_results['cv_mean'].plot.bar(color = 'aqua', figsize = (8, 6),\n                                  yerr = list(model_results['cv_std']),\n                                  edgecolor = 'k', linewidth = 2)\nplt.title('Model F1 Score Results');\nplt.ylabel('Mean F1 Score (with error bar)');\nmodel_results.reset_index(inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3e9b271483c048ab27e584577a94f0dc4a2e47e6"},"cell_type":"markdown","source":"KNN-5 performs the best with RandomFOrest coming in close. \nAlso the Std_dev on error of KNN5 is much tighter than the random forest"},{"metadata":{"_uuid":"22bee5cc58b40acd2badce0579283671ef488ed6"},"cell_type":"markdown","source":"#  Model using xgBoost\n\nAfter Random Forest, use the gradient boosting machine xgBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set = pd.DataFrame(train_set, columns = features)\ntrain_set.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_set = pd.DataFrame(test_set, columns = features)\ntest_set.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = list(train_set.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Bayesian Optimization"},{"metadata":{"trusted":true},"cell_type":"code","source":"############### GG. Tuning using Bayes Optimization ############\n\"\"\"\n11. Step 1: Define BayesianOptimization function.\n\"\"\"\n# 11.1 Which parameters to consider and what is each one's range\npara_set = {\n           'learning_rate':  (0, 1),                 # any value between 0 and 1\n           'n_estimators':   (10,100),               # any number between 50 to 300\n           'max_depth':      (6,20),                 # any depth between 3 to 10\n           'n_components' :  (150,200)               # any number between 150 to 190\n            }\n\n# 11.2 Create a function that when passed some parameters\n#    evaluates results using cross-validation\n#    This function is used by BayesianOptimization() object\n\ndef xg_eval(learning_rate,n_estimators, max_depth,n_components):\n    # 12.1 Make pipeline. Pass parameters directly here\n    pipe_xg1 = make_pipeline (ss(),                        # Why repeat this here for each evaluation?\n                              PCA(n_components=int(round(n_components))),\n                              XGBClassifier(\n                                           silent = False,\n                                           n_jobs=2,\n                                           learning_rate=learning_rate,\n                                           max_depth=int(round(max_depth)),\n                                           n_estimators=int(round(n_estimators))\n                                           )\n                             )\n\n    # 12.2 Now fit the pipeline and evaluate\n    \"\"\"Perform 10 fold cross validation of a model\"\"\"\n    cv_result = cross_val_score(estimator = pipe_xg1,\n                                X = train_set,\n                                y = train_labels,\n                                cv = 10,\n                                n_jobs = -1,\n                                scoring = scorer\n                                ).mean()             # take the mean/max of all results\n\n\n    # 12.3 Finally return maximum/average value of result\n    return cv_result\n\n#    return cv_result, pipe_xg1\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 12 This is the main workhorse\n#      Instantiate BayesianOptimization() object\n#\nxgBO = BayesianOptimization(\n                             xg_eval,     # Function to evaluate performance.\n                             para_set     # Parameter set from where parameters will be selected\n                             )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 13. Gaussian process parameters\n#     Modulate intelligence of Bayesian Optimization process\ngp_params = {\"alpha\": 1e-5}      # Initialization parameter for gaussian\n                                 # process.\n\n# 14. Fit/train (so-to-say) the BayesianOptimization() object\n#     Start optimization. 25minutes\n#     Our objective is to maximize performance (results)\nstart = time.time()\nxgBO.maximize(init_points=10,    # Number of randomly chosen points to\n                                 # sample the target function before\n                                 #  fitting the gaussian Process (gp)\n                                 #  or gaussian graph\n               n_iter=15,        # Total number of times the\n               #acq=\"ucb\",       # ucb: upper confidence bound\n                                 #   process is to be repeated\n                                 # ei: Expected improvement\n               # kappa = 1.0     # kappa=1 : prefer exploitation; kappa=10, prefer exploration\n              **gp_params\n               )\nend = time.time()\n(end-start)/60\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 15. Get values of parameters that maximise the objective\n#xgBO.res\ntype(xgBO.res) #If type is list then call max directly","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#xgBO.res['max']\nxgBO.max","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgBO.max['params']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgBO.max['params']['learning_rate']\nxgBO.max['params']['n_estimators']\nxgBO.max['params']['max_depth']\nxgBO.max['params']['n_components']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Best set of parameters recommended by Bayesian Optimization:\n\n{'learning_rate': 0.9167937871301227,\n 'max_depth': 19.94179375518867,\n 'n_components': 178.42829982693257,\n 'n_estimators': 10.011309079810477}\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_score_xgBO = xg_eval(\n    xgBO.max['params']['learning_rate'],\n    xgBO.max['params']['n_estimators'],\n    xgBO.max['params']['max_depth'],\n    xgBO.max['params']['n_components']\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_results = model_results.append(pd.DataFrame({\n    'model': 'XGBClassifier', \n    'cv_mean': cv_score_xgBO.mean(), \n    'cv_std': cv_score_xgBO.std()}, \n    index = [0]),\n                                     ignore_index = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_score_xgBO.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_score_xgBO.std()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"SO the much vaunted Bayesian Optimization gives only a marginal improvement in accuracy \n\nfrom a baseline score of 0.3368 we are now at 0.3087795925140086\n\nthis is disappointing. \n"},{"metadata":{},"cell_type":"markdown","source":"### NEXT Steps - time permitting\n1. Try SVD instead of PCA\n2. Implement EvolutionaryAlgorithmSearchCV instead of Bayesian Optimization\n3. Implement lightGBM instead of xgBoost\n\n\n4. Predict using xgBoost / lightGBM\n5. Do some more data manipulation to extract predictions for head of household (optional)\n6. export results to csv with id and idhogar"},{"metadata":{},"cell_type":"markdown","source":"## Running xgBoost - fit"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe_xg1 = make_pipeline (ss(),\n                          PCA(n_components=int(round(xgBO.max['params']['n_components']))),\n                          XGBClassifier(\n                              silent = False,\n                              n_jobs=-1,\n                              learning_rate=xgBO.max['params']['learning_rate'],\n                              max_depth=int(round(xgBO.max['params']['max_depth'])),\n                              n_estimators=int(round(xgBO.max['params']['n_estimators']))\n                          )\n                         )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe_xg1.fit(train_set, train_labels)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"de34b3435f0ff50a2c437799867de0689fc91c7c"},"cell_type":"markdown","source":"# Make predictions on test data - xgBoost\n"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"test_set.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = pipe_xg1.predict(test_set)\n#predictions = [round(value) for value in test_labels]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions.size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"de34b3435f0ff50a2c437799867de0689fc91c7c"},"cell_type":"markdown","source":"# joining predictions with ID and idhogar\n"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"submission_base.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_ids = list(final.loc[final['Target'].isnull(), 'idhogar'])\npredictions = pd.DataFrame({'idhogar': test_ids,\n                               'Target': predictions})\n\n# Make a submission dataframe\nsubmission = submission_base.merge(predictions, \n                                   on = 'idhogar',\n                                   how = 'left').drop(columns = ['idhogar'])\n    \n# Fill in households missing a head\nsubmission['Target'] = submission['Target'].fillna(4).astype(np.int8)\nsubmission.to_csv('Anirudh_submission.csv', index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"}},"nbformat":4,"nbformat_minor":1}