{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n#import numpy as np # linear algebra\n#import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Import Libraries and Data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Import Libraries\nimport os,sys\nimport numpy as np\nimport pandas as pd\nimport seaborn as sb\nimport matplotlib.pyplot as plt # we only need pyplot\nimport matplotlib.cm as cm\n#sb.set() # set the default Seaborn style for graphics\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\nfrom sklearn.utils.class_weight import compute_sample_weight\nfrom sklearn import metrics\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\n\nimport xgboost as xgb\nfrom xgboost.sklearn import XGBClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = pd.read_csv('/kaggle/input/costa-rican-household-poverty-prediction/train.csv')\n# Drop target data\ntrain_target = train_data['Target']\ntrain_data.drop(['Target'],axis=1,inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Feature Engineering based on Data Exploration"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check columns with nan values and their nan values counts - missing data\ndef findNanCol(orig_df,print_flag=False):\n    df_missing = orig_df.isnull()\n    col_list_missing_data = []\n    for col in df_missing.columns.values.tolist():\n        try:\n            x=df_missing[col].value_counts()[1]\n            if print_flag:\n                print(\"Column {}: {} missing value counts\".format(col,x))\n            col_list_missing_data.append(col)\n        except:\n            continue\n    return col_list_missing_data\n\n\n# Get all col with type Objects to check if there more than one type and for labelencoder\ndef findObjectTypeCol(orig_df,print_flag=False):\n    obj_col_list = []\n    for col in orig_df.columns.values.tolist():\n        if orig_df[col].dtypes == \"object\":\n            if print_flag:\n                print(col)\n            obj_col_list.append(col)\n    return obj_col_list\n\n# Fill NA values in dataset appropriately\ndef fillMissingValues(train_data):\n    \n    # Fill the nans in v2a1 with 0s as most have paid for their own house and thus no rent payment.\n    train_data['v2a1'] = train_data['v2a1'].fillna(0).astype('float64')\n    \n    # Fill nan with 0s as only v18q with 0s have nan values for v18q1\n    # Furthermore, we will get the avg num of laptops per household member which is a better feature.\n    # The new feature will be created in 3rd stage of feature engineering.\n    train_data['v18q1'] = train_data['v18q1'].fillna(0).astype('float64')\n    \n    # Fill nans in rez_esc with zeros as this feature is meant for age between 7 and 19 as per definition.\n    train_data['rez_esc'] = train_data['rez_esc'].fillna(0).astype('float64')\n    \n    # It turns out that the number of household members 18+ is zero which have givesn the nan value.\n    # Therefore, we will convert \"meanedu\" values to zero.\n    train_data['meaneduc'] = train_data['meaneduc'].fillna(0).astype('float64')\n    train_data['SQBmeaned'] = train_data['SQBmeaned'].fillna(0).astype('float64')\n    \n    return train_data\n\n# Calculate depenedncy rate for those missing values\ndef getDependencyRate(train_data):\n    for i,val in enumerate(train_data['dependency']):\n        if val == 'no':\n            train_data.loc[i,'dependency'] = 0\n        elif val == 'yes':\n            total_num = float(train_data.loc[i,'hogar_total'])\n            num_abv_65_and_below_19 = float(train_data.loc[i,'hogar_nin']) + float(train_data.loc[i,'hogar_mayor'])\n            train_data.loc[i,'dependency'] = num_abv_65_and_below_19/(total_num - num_abv_65_and_below_19)\n\n    train_data['dependency'] = train_data['dependency'].astype('float64')\n    return train_data\n\n# Encodes features with more than one data type\ndef encoder(train_data):\n    dic = {'yes' : '1', 'no' : '0'}\n    \n    train_data.drop(['Id'],axis=1,inplace=True)\n    train_data['idhogar'] = LabelEncoder().fit_transform(train_data['idhogar'])\n    train_data = getDependencyRate(train_data)\n    train_data['edjefe']= train_data['edjefe'].replace(dic).astype('float64')\n    train_data['edjefa'] = train_data['edjefa'].replace(dic).astype('float64')\n    \n    return train_data\n\n# Converts str type columns to float\ndef convertToFloat(train_data,obj_col_list,col_list_missing_data):\n    obj_col_list.extend(col_list_missing_data)\n    for col in train_data.columns.values.tolist():\n        if col not in obj_col_list:\n            train_data[col] = train_data[col].astype('float64')\n    return train_data\n\n# Adds and modifies new features\ndef addAndModifyFeatures(train_data):\n    ### Definition:\n    # r4t3: Total number of individuals in houshold including domestic employees/friends/tenants.\n    # hogar_total = tamgog = hhsize: Total number of houshold members excluding domestic employees/friends/tenants.\n    # tamviv: Unclear definition so will be dropped.\n    \n    ### Add new features\n    \n    train_data['v2a1_per_room'] = train_data['v2a1']/train_data['rooms']\n    # It is logical for a household to account only for tables among household members.\n    train_data['v18q1_per_household_member'] = train_data['v18q1']/train_data['hogar_total']\n    # It is logical for a household to account only for mobile phones among household members.\n    train_data['qmobileph_per_household_member'] = train_data['qmobilephone']/train_data['hogar_total']\n    # It is logical for a household to account only for rooms among household members. Having non\n    # household members to stay is subjective and not considered as essentials.\n    train_data['rooms_per_household_member'] = train_data['rooms']/train_data['hogar_total']\n     # It is logical for a household to account only for bedrooms among household members. Having non\n    # household members to stay is subjective and not considered as essentials.\n    train_data['bedroom_per_household_member'] = train_data['bedrooms']/train_data['hogar_total']\n    # Number of non household members in the house(domestic employees/friends/tenants)\n    train_data['Num_of_non_household_members'] = train_data['r4t3']-train_data['hogar_total']\n    # Proportion of household adults aged btw 19 and 65 ---> (adults-old_aged)/total\n    train_data['hogar_adul_btw_19_and_65'] = (train_data['hogar_adul']-train_data['hogar_mayor'])/train_data['hogar_total']\n    \n    \n    ### Modify to obtain proportion features\n    \n    # The feature values below includes non houshold mmbers as mentioned in data exploration notebook.\n    # Therefore, the values are divided by total number of people in house.\n    train_data['r4h1'] = train_data['r4h1']/train_data['r4t3']\n    train_data['r4h2'] = train_data['r4h2']/train_data['r4t3']\n    train_data['r4h3'] = train_data['r4h3']/train_data['r4t3']\n    train_data['r4m1'] = train_data['r4m1']/train_data['r4t3']\n    train_data['r4m2'] = train_data['r4m2']/train_data['r4t3']\n    train_data['r4m3'] = train_data['r4m3']/train_data['r4t3']\n    train_data['r4t1'] = train_data['r4t1']/train_data['r4t3']\n    train_data['r4t2'] = train_data['r4t2']/train_data['r4t3']\n\n    # The feature values below excludes non houshold mmbers as verified in data exploration notebook.\n    # Therefore, the values are divided by number of houehold members.\n    train_data['hogar_nin'] = train_data['hogar_nin']/train_data['hogar_total']\n    train_data['hogar_adul'] = train_data['hogar_adul']/train_data['hogar_total']\n    train_data['hogar_mayor'] = train_data['hogar_mayor']/train_data['hogar_total'] \n    \n    ### Remove feature col as some are duplicates and \n    # most of these col are total number which is included in proportion calculation\n    train_data.drop(['idhogar','v18q1','rooms','bedrooms','r4t3','hogar_total','agesq','hhsize','tamviv','tamhog'],axis=1,inplace=True)\n    \n    return train_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applies feature engineering in one function\ndef applyFE(train_data):\n    # List of columns that have missing values and their corresponding number of missing value counts\n    col_list_missing_data = findNanCol(train_data)\n    \n    # List of columns that needs to be verified as they in object type\n    obj_col_list = findObjectTypeCol(train_data)\n    \n    # Convert columns not having missing data or not having many object types to float\n    train_data = convertToFloat(train_data,obj_col_list,col_list_missing_data)\n    \n    ### STAGE 1\n    ### Dealing with features with missing values.\n    # Fills missing values\n    train_data = fillMissingValues(train_data)\n    \n    ### STAGE 2\n    ### Dealing with features with object types.\n    # Encodes features with more than one data type\n    train_data = encoder(train_data)\n    \n    ### STAGE 3\n    ### Dealing with new and modified features.\n    # Encodes features with more than one data type\n    train_data = addAndModifyFeatures(train_data)\n    \n    return train_data\n\n# Get components of predicted labels\ndef getConstituentsOfPredicted(cf,pred_label,angle=45):\n\n    # cf[actual-1][predicted-1]\n    p13 = cf[0][pred_label-1]\n    p23 = cf[1][pred_label-1]\n    p33 = cf[2][pred_label-1]\n    p43 = cf[3][pred_label-1]\n\n    # Data to plot\n    pred_labels = ['extreme poverty', 'moderate poverty', 'vulnerable households','non vulnerable households']\n    sizes = [p13, p23, p33, p43]\n    colors = ['gold', 'yellowgreen', 'lightcoral', 'lightskyblue']\n\n    # Plot\n    plt.pie(sizes, labels=pred_labels, colors=colors, autopct='%1.0f%%', shadow=False, startangle=angle, textprops={'fontsize': 14})\n    plt.title(\"Constituents of Costa Rican Household Poverty Levels predicted as \"+pred_labels[pred_label-1],fontsize=20)\n\n    plt.axis('equal')\n    plt.show()\n    \n    return p13,p23,p33,p43\n\n# Get components of actual labels\ndef getPredictionOfActualLabels(p,cf,actual_label,angle=45):\n    \n    # Data to plot\n    labels = ['extreme poverty', 'moderate poverty', 'vulnerable households','non vulnerable households']\n    sizes = [p[0], p[1], p[2], p[3]]\n    colors = ['gold', 'yellowgreen', 'lightcoral', 'lightskyblue']\n\n    # Plot\n    plt.pie(sizes, labels=labels, colors=colors, autopct='%1.0f%%', shadow=False, startangle=angle, textprops={'fontsize': 14})\n    plt.title(\"Predictions of Costa Rican Household Poverty Levels with actual labels as \"+labels[actual_label-1],fontsize=20)\n\n    plt.axis('equal')\n    plt.show()\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature Engineering for train data\ntrain_data = applyFE(train_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = train_data.copy()\ny_train = train_target.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Model: RandomForest"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hyperparameters that are tuned for gridsearch\nparam_grid = { \n    'n_estimators': [270],\n    'max_depth' : [14]\n}\n\n# Model initializer\nrfc=RandomForestClassifier(class_weight = 'balanced_subsample', n_estimators = 200,max_depth = 16)\n##rfc=RandomForestClassifier(class_weight = 'balanced', random_state=0)\n\n# Specific scoring method\nscorer = metrics.make_scorer(metrics.f1_score, average = 'weighted')\n# GridSearch\nrf_cv = GridSearchCV(estimator=rfc, param_grid=param_grid, scoring=scorer, cv= 3)\n# Fit train data\nrf_cv.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## View the accuracy score\nprint('Best score for training data:', rf_cv.best_score_,\"\\n\") \n\n## View the best parameters for the model found using grid search\nprint('Best n_estimators:',rf_cv.best_estimator_.n_estimators,\"\\n\") \nprint('Best max_depth:',rf_cv.best_estimator_.max_depth,\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict validate data and evaluate for validate data\ny_pred_validate = rf_cv.predict(x_train)\nprint(\"Classification report for rf model %s:\\n%s\\n\"\n      % (rf_cv, metrics.classification_report(y_train, y_pred_validate)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submit for RF"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test\n# Read the test CSV Data\npath = '/kaggle/input/costa-rican-household-poverty-prediction/test.csv'\ntest_data = pd.read_csv(path)\ntest_data = applyFE(test_data)\ny_pred = rf_cv.predict(test_data)\n# Read submission file\npath = '/kaggle/input/costa-rican-household-poverty-prediction/sample_submission.csv'\ntest = pd.read_csv(path)\ntest['Target'] = y_pred\ntest.to_csv(\"submission.csv\", index= False)\n#gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}