{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Data file\nimport numpy as np\nimport pandas as pd\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Import\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom xgboost.sklearn import XGBClassifier \nfrom lightgbm import LGBMClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.svm import SVC","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/costa-rican-household-poverty-prediction/train.csv', header=0)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/costa-rican-household-poverty-prediction/test.csv', header=0)\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('/kaggle/input/costa-rican-household-poverty-prediction/sample_submission.csv', header=0)\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_mid = train.copy()\ntrain_mid['train_or_test'] = 'train'\n\ntest_mid = test.copy()\ntest_mid['train_or_test'] = 'test'\n\ntest_mid['Target'] = 9\n\nalldata = pd.concat([train_mid, test_mid], sort=False, axis=0).reset_index(drop=True)\n\nprint('The size of the train data:' + str(train.shape))\nprint('The size of the test data:' + str(test.shape))\nprint('The size of the submission data:' + str(submission.shape))\nprint('The size of the alldata data:' + str(alldata.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Deleting unnecessary column\ndel alldata['idhogar']\ndel alldata['dependency']\ndel alldata['edjefa']\ndel alldata['edjefe']\n\n# Median number insert to NaN in alldata\nalldata.fillna(alldata.median(), inplace=True)\n\n# Split alldata into train and test\ntrain = alldata.query('train_or_test == \"train\"')\ntest = alldata.query('train_or_test == \"test\"')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"alldata","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check how many column show and change number of it show \npd.get_option(\"display.max_columns\")\npd.set_option('display.max_columns', 150)\n\n# Check how many rows show and change number of it show \npd.get_option(\"display.max_rows\")\npd.set_option('display.max_rows', 150)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"alldata.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Definition Target feature and drop some unnecessary column\ntarget_col = 'Target'\ndrop_col = ['Id', 'Target', 'train_or_test']\n\ntrain_feature = train.drop(columns=drop_col)\ntrain_target = train[target_col]\ntest_feature = test.drop(columns=drop_col)\nsubmission_id = test['Id'].values\n\nX_train, X_test, y_train, y_test = train_test_split(train_feature, train_target, test_size=0.2, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# RandomForest==============\n\nrf = RandomForestClassifier()\nrf.fit(X_train, y_train)\nprint('='*20)\nprint('RandomForestClassifier')\nprint(f'accuracy of train set: {rf.score(X_train, y_train)}')\nprint(f'accuracy of test set: {rf.score(X_test, y_test)}')\n\nrf_prediction = rf.predict(test_feature)\nrf_prediction\n\n# SVC==============\n\nsvc = SVC(verbose=True, random_state=0)\nsvc.fit(X_train, y_train)\nprint('='*20)\nprint('SVC')\nprint(f'accuracy of train set: {svc.score(X_train, y_train)}')\nprint(f'accuracy of test set: {svc.score(X_test, y_test)}')\n\nsvc_prediction = svc.predict(test_feature)\nsvc_prediction\n\n# LinearSVC==============\n\nlsvc = LinearSVC(verbose=True)\nlsvc.fit(X_train, y_train)\nprint('='*20)\nprint('LinearSVC')\nprint(f'accuracy of train set: {lsvc.score(X_train, y_train)}')\nprint(f'accuracy of test set: {lsvc.score(X_test, y_test)}')\n\nlsvc_prediction = lsvc.predict(test_feature)\nlsvc_prediction\n\n# k-近傍法（k-NN）==============\n\nknn = KNeighborsClassifier(n_neighbors=3) #引数は分類数\nknn.fit(X_train, y_train)\nprint('='*20)\nprint('KNeighborsClassifier')\nprint(f'accuracy of train set: {knn.score(X_train, y_train)}')\nprint(f'accuracy of test set: {knn.score(X_test, y_test)}')\n\nknn_prediction = knn.predict(test_feature)\nknn_prediction\n\n# 決定木==============\n\ndecisiontree = DecisionTreeClassifier(max_depth=3, random_state=0)\ndecisiontree.fit(X_train, y_train)\nprint('='*20)\nprint('DecisionTreeClassifier')\nprint(f'accuracy of train set: {decisiontree.score(X_train, y_train)}')\nprint(f'accuracy of test set: {decisiontree.score(X_test, y_test)}')\n\ndecisiontree_prediction = decisiontree.predict(test_feature)\ndecisiontree_prediction\n\n# SGD Classifier==============\n\nsgd = SGDClassifier()\nsgd.fit(X_train, y_train)\nprint('='*20)\nprint('SGD Classifier')\nprint(f'accuracy of train set: {sgd.score(X_train, y_train)}')\nprint(f'accuracy of test set: {sgd.score(X_test, y_test)}')\n\nsgd_prediction = sgd.predict(test_feature)\nsgd_prediction\n\n# Gradient Boosting Classifier==============\n\ngradientboost = GradientBoostingClassifier(random_state=0)\ngradientboost.fit(X_train, y_train)\nprint('='*20)\nprint('GradientBoostingClassifier')\nprint(f'accuracy of train set: {gradientboost.score(X_train, y_train)}')\nprint(f'accuracy of test set: {gradientboost.score(X_test, y_test)}')\n\ngradientboost_prediction = gradientboost.predict(test_feature)\ngradientboost_prediction\n\n# VotingClassifier==============\n\nfrom sklearn.ensemble import VotingClassifier\n\nestimators = [\n  (\"rf\", rf),\n  (\"svc\", svc),\n  (\"lsvc\", lsvc),\n  (\"knn\", knn),\n  (\"decisiontree\", decisiontree),\n  (\"sgd\", sgd),\n  (\"gradientboost\", gradientboost),\n]\n\nvote = VotingClassifier(estimators=estimators)\nvote.fit(X_train, y_train)\nprint('='*20)\nprint('VotingClassifier')\nprint(f'accuracy of train set: {vote.score(X_train, y_train)}')\nprint(f'accuracy of test set: {vote.score(X_test, y_test)}')\n\nvote_prediction = vote.predict(test_feature)\nvote_prediction\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}