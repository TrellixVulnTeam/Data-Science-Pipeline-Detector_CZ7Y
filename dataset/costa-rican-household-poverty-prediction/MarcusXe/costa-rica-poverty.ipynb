{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sys, os, subprocess\nimport pandas as pd\nimport numpy as np\nfrom sklearn.utils import shuffle\nimport seaborn as sns \nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:00:22.291467Z","iopub.execute_input":"2021-11-27T15:00:22.292114Z","iopub.status.idle":"2021-11-27T15:00:22.837009Z","shell.execute_reply.started":"2021-11-27T15:00:22.291899Z","shell.execute_reply":"2021-11-27T15:00:22.835991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rawtrainingdata = pd.read_csv(\"../input/costa-rican-household-poverty-prediction/train.csv\")\nttest = pd.read_csv(\"../input/costa-rican-household-poverty-prediction/test.csv\")\nrawtrainingdata.head(20)","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:00:22.838825Z","iopub.execute_input":"2021-11-27T15:00:22.839092Z","iopub.status.idle":"2021-11-27T15:00:23.495109Z","shell.execute_reply.started":"2021-11-27T15:00:22.83906Z","shell.execute_reply":"2021-11-27T15:00:23.494505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ttest.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:00:23.496458Z","iopub.execute_input":"2021-11-27T15:00:23.496849Z","iopub.status.idle":"2021-11-27T15:00:23.523614Z","shell.execute_reply.started":"2021-11-27T15:00:23.496818Z","shell.execute_reply":"2021-11-27T15:00:23.522986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rawtrainingdata['Target'].value_counts()\nprint(rawtrainingdata.describe())\nprint(\"Distribution of the targets\")\nsns.countplot('Target',data=rawtrainingdata)","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:00:23.525644Z","iopub.execute_input":"2021-11-27T15:00:23.526041Z","iopub.status.idle":"2021-11-27T15:00:24.143162Z","shell.execute_reply.started":"2021-11-27T15:00:23.526011Z","shell.execute_reply":"2021-11-27T15:00:24.141913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ttest.info()","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:00:24.144918Z","iopub.execute_input":"2021-11-27T15:00:24.145804Z","iopub.status.idle":"2021-11-27T15:00:24.167818Z","shell.execute_reply.started":"2021-11-27T15:00:24.145746Z","shell.execute_reply":"2021-11-27T15:00:24.167178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainingds = rawtrainingdata.drop('Target',axis=1)\nalldatads = trainingds.append(ttest,sort=True)\nalldatads['dependency'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:00:24.169072Z","iopub.execute_input":"2021-11-27T15:00:24.169933Z","iopub.status.idle":"2021-11-27T15:00:24.227813Z","shell.execute_reply.started":"2021-11-27T15:00:24.169895Z","shell.execute_reply":"2021-11-27T15:00:24.226715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mapping = {\"yes\": 1, \"no\": 0}\nalldatads['dependency'] = alldatads['dependency'].replace(mapping).astype(np.float64)\nalldatads['edjefa'] = alldatads['edjefa'].replace(mapping).astype(np.float64)\nalldatads['edjefe'] = alldatads['edjefe'].replace(mapping).astype(np.float64)\n\nalldatads[['dependency', 'edjefa', 'edjefe']].describe()","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:00:24.229169Z","iopub.execute_input":"2021-11-27T15:00:24.229385Z","iopub.status.idle":"2021-11-27T15:00:24.346334Z","shell.execute_reply.started":"2021-11-27T15:00:24.229358Z","shell.execute_reply":"2021-11-27T15:00:24.345405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"alldatads.loc[alldatads['rez_esc'] == 99.0 , 'rez_esc'] = 5","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:00:24.347562Z","iopub.execute_input":"2021-11-27T15:00:24.347789Z","iopub.status.idle":"2021-11-27T15:00:24.354281Z","shell.execute_reply.started":"2021-11-27T15:00:24.347757Z","shell.execute_reply":"2021-11-27T15:00:24.353409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Number of missing in each column\nmissing = pd.DataFrame(alldatads.isnull().sum()).rename(columns = {0: 'total'})\n\n# Create a percentage missing\nmissing['percent'] = missing['total'] / len(alldatads)\nmissing.sort_values('percent', ascending = False).head(10)","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:00:24.357174Z","iopub.execute_input":"2021-11-27T15:00:24.357866Z","iopub.status.idle":"2021-11-27T15:00:24.390811Z","shell.execute_reply.started":"2021-11-27T15:00:24.357824Z","shell.execute_reply":"2021-11-27T15:00:24.390018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For tablets per household, pad NaN with 0\nalldatads['v18q1'] = alldatads['v18q1'].fillna(0)\n\n# For Monthly rental payment, look at column if household fully own the house, if yes than no rent\nalldatads.loc[(alldatads['tipovivi1'] == 1), 'v2a1'] = 0\nalldatads.loc[(alldatads['tipovivi2'] == 1), 'v2a1'] = 0\nalldatads['v2a1'] = alldatads['v2a1'].fillna(alldatads['v2a1'].median())\n\n# For years behind studies, if it is underage or overage, change NAN to 0\nalldatads.loc[((alldatads['age'] > 19) | (alldatads['age'] < 7)) & (alldatads['rez_esc'].isnull()), 'rez_esc'] = 0\nalldatads['rez_esc-missing'] = alldatads['rez_esc'].isnull() #kiv\nalldatads['rez_esc'] = alldatads['rez_esc'].fillna(alldatads['rez_esc'].median())\n\n# For electricity, create a new column for electricity for categorical\nelec = []\nfor i, row in alldatads.iterrows():\n    if row['noelec'] == 1:\n        elec.append(0)\n    elif row['coopele'] == 1:\n        elec.append(1)\n    elif row['public'] == 1:\n        elec.append(2)\n    elif row['planpri'] == 1:\n        elec.append(3)\n    else:\n        elec.append(np.nan)\nalldatads['elec'] = elec\nalldatads['elec-missing'] = alldatads['elec'].isnull() #kiv\nalldatads['elec'] = alldatads['elec'].fillna(alldatads['elec'].mode().tolist()[0])\n\n# drop all useless electricity columns\nalldatads = alldatads.drop(columns = ['noelec', 'coopele', 'public', 'planpri'])\n\n# Make a new variable called walls that encapsulates all wall types\nalldatads['walls'] = np.argmax(np.array(alldatads[['epared1', 'epared2', 'epared3']]), axis = 1)\nalldatads = alldatads.drop(columns = ['epared1', 'epared2', 'epared3'])\n\n# Make a new variable called roof that encapsulates all roof types\nalldatads['roof'] = np.argmax(np.array(alldatads[['etecho1', 'etecho2', 'etecho3']]), axis = 1)\nalldatads = alldatads.drop(columns = ['etecho1', 'etecho2', 'etecho3'])\n\n# floor that encapsulates all types\nalldatads['floor'] = np.argmax(np.array(alldatads[['eviv1', 'eviv2', 'eviv3']]),\n                           axis = 1)\nalldatads = alldatads.drop(columns = ['eviv1', 'eviv2', 'eviv3'])\n\n# flush that encapsulates all types\nalldatads['flush'] = np.argmax(np.array(alldatads[[\"sanitario1\",'sanitario5', 'sanitario2', 'sanitario3',\"sanitario6\"]]),\n                           axis = 1)\nalldatads = alldatads.drop(columns = [\"sanitario1\",'sanitario5', 'sanitario2', 'sanitario3',\"sanitario6\"])\n\n# drop squared columns\nalldatads = alldatads[[x for x in alldatads if not x.startswith('SQB')]]\nalldatads = alldatads.drop(columns = ['agesq'])\n\n# water provision that encapsulates all types\nalldatads['waterprovision'] = np.argmax(np.array(alldatads[['abastaguano', 'abastaguafuera', 'abastaguadentro']]),\n                           axis = 1)\nalldatads = alldatads.drop(columns = ['abastaguano', 'abastaguafuera', 'abastaguadentro'])\n\n# education level encapsulating all types\nalldatads['inst'] = np.argmax(np.array(alldatads[[c for c in alldatads if c.startswith('instl')]]), axis = 1)\nalldatads = alldatads.drop(columns = [c for c in alldatads if c.startswith('instlevel')])\n\n# cooking encapsulating all types\nalldatads['waterprovision'] = np.argmax(np.array(alldatads[['energcocinar1','energcocinar4', 'energcocinar3', 'energcocinar2']]),\n                           axis = 1)\nalldatads = alldatads.drop(columns = ['energcocinar1','energcocinar4', 'energcocinar3', 'energcocinar2'])\n\n# if meaneduc is null take value of years of schooling\nalldatads.loc[pd.isnull(alldatads['meaneduc']), 'meaneduc'] = alldatads.loc[pd.isnull(alldatads['meaneduc']), 'escolari']","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:00:24.393955Z","iopub.execute_input":"2021-11-27T15:00:24.394207Z","iopub.status.idle":"2021-11-27T15:00:26.943403Z","shell.execute_reply.started":"2021-11-27T15:00:24.394176Z","shell.execute_reply":"2021-11-27T15:00:26.942415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Number of missing in each column\nmissing = pd.DataFrame(alldatads.isnull().sum()).rename(columns = {0: 'total'})\n\n# Create a percentage missing\nmissing['percent'] = missing['total'] / len(alldatads)\nmissing.sort_values('percent', ascending = False).head(10)","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:00:26.94475Z","iopub.execute_input":"2021-11-27T15:00:26.945008Z","iopub.status.idle":"2021-11-27T15:00:26.973831Z","shell.execute_reply.started":"2021-11-27T15:00:26.944976Z","shell.execute_reply":"2021-11-27T15:00:26.973019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(alldatads)","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:00:26.974907Z","iopub.execute_input":"2021-11-27T15:00:26.975155Z","iopub.status.idle":"2021-11-27T15:00:26.996167Z","shell.execute_reply.started":"2021-11-27T15:00:26.975108Z","shell.execute_reply":"2021-11-27T15:00:26.995225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split our combine sets into training and test\ntrainingset = alldatads.iloc[0:9557,:]\ntestset = alldatads.iloc[9557:33413,:]\ntrainingset = trainingset.drop(['Id','idhogar'],axis=1)\ntrainingtargets = rawtrainingdata['Target']\n\n# Obtain copy of full training set\nfulltrainingset = trainingset.copy()\nfulltrainingsettargets = trainingtargets.copy()\n\n# Split our training set into training and validation\ntrainingset, trainingtargets = shuffle(trainingset, trainingtargets, random_state = 8)\nvalidationdata, validationtargets, trainingdata, trainingdatatargets = trainingset[:500], trainingtargets[:500], trainingset[500:], trainingtargets[500:]\n\n# Drop all unnecessary columns, but hang on to the test ids\nnewtestset = testset.copy()\nnewtestset.drop(['idhogar'],axis=1,inplace=True)\ntestset.drop(['Id', 'idhogar'],axis=1,inplace=True)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:00:26.997658Z","iopub.execute_input":"2021-11-27T15:00:26.998039Z","iopub.status.idle":"2021-11-27T15:00:27.039513Z","shell.execute_reply.started":"2021-11-27T15:00:26.998008Z","shell.execute_reply":"2021-11-27T15:00:27.038227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(trainingdata)","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:00:27.041177Z","iopub.execute_input":"2021-11-27T15:00:27.04172Z","iopub.status.idle":"2021-11-27T15:00:27.063598Z","shell.execute_reply.started":"2021-11-27T15:00:27.041673Z","shell.execute_reply":"2021-11-27T15:00:27.062938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import lightgbm as lgb\nimport xgboost as xgb\nimport sklearn.model_selection as model_selection\nfrom sklearn.metrics import f1_score, make_scorer, classification_report\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import svm\nfrom time import time\nfrom numpy import mean\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.utils import class_weight\nfrom sklearn.model_selection import learning_curve\n","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:00:27.065104Z","iopub.execute_input":"2021-11-27T15:00:27.065584Z","iopub.status.idle":"2021-11-27T15:00:27.794192Z","shell.execute_reply.started":"2021-11-27T15:00:27.065548Z","shell.execute_reply":"2021-11-27T15:00:27.7933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# initialise cross-validation parameters\ncv = KFold(n_splits=5, random_state=1, shuffle=True)\nmodels_best_f1 = {}","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:00:27.795478Z","iopub.execute_input":"2021-11-27T15:00:27.795731Z","iopub.status.idle":"2021-11-27T15:00:27.801251Z","shell.execute_reply.started":"2021-11-27T15:00:27.795697Z","shell.execute_reply":"2021-11-27T15:00:27.799842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_learning_curve(\n    estimator,\n    title,\n    X,\n    y,\n    axes=None,\n    ylim=None,\n    cv=None,\n    n_jobs=None,\n    train_sizes=np.linspace(0.1, 1.0, 5),\n):\n    \"\"\"\n    Generate 3 plots: the test and training learning curve, the training\n    samples vs fit times curve, the fit times vs score curve.\n\n    Parameters\n    ----------\n    estimator : estimator instance\n        An estimator instance implementing `fit` and `predict` methods which\n        will be cloned for each validation.\n\n    title : str\n        Title for the chart.\n\n    X : array-like of shape (n_samples, n_features)\n        Training vector, where ``n_samples`` is the number of samples and\n        ``n_features`` is the number of features.\n\n    y : array-like of shape (n_samples) or (n_samples, n_features)\n        Target relative to ``X`` for classification or regression;\n        None for unsupervised learning.\n\n    axes : array-like of shape (3,), default=None\n        Axes to use for plotting the curves.\n\n    ylim : tuple of shape (2,), default=None\n        Defines minimum and maximum y-values plotted, e.g. (ymin, ymax).\n\n    cv : int, cross-validation generator or an iterable, default=None\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n          - None, to use the default 5-fold cross-validation,\n          - integer, to specify the number of folds.\n          - :term:`CV splitter`,\n          - An iterable yielding (train, test) splits as arrays of indices.\n\n        For integer/None inputs, if ``y`` is binary or multiclass,\n        :class:`StratifiedKFold` used. If the estimator is not a classifier\n        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validators that can be used here.\n\n    n_jobs : int or None, default=None\n        Number of jobs to run in parallel.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    train_sizes : array-like of shape (n_ticks,)\n        Relative or absolute numbers of training examples that will be used to\n        generate the learning curve. If the ``dtype`` is float, it is regarded\n        as a fraction of the maximum size of the training set (that is\n        determined by the selected validation method), i.e. it has to be within\n        (0, 1]. Otherwise it is interpreted as absolute sizes of the training\n        sets. Note that for classification the number of samples usually have\n        to be big enough to contain at least one sample from each class.\n        (default: np.linspace(0.1, 1.0, 5))\n    \"\"\"\n    if axes is None:\n        _, axes = plt.subplots(1, 3, figsize=(20, 5))\n\n    axes[0].set_title(title)\n    if ylim is not None:\n        axes[0].set_ylim(*ylim)\n    axes[0].set_xlabel(\"Training examples\")\n    axes[0].set_ylabel(\"Score\")\n\n    train_sizes, train_scores, test_scores, fit_times, _ = learning_curve(\n        estimator,\n        X,\n        y,\n        cv=cv,\n        n_jobs=n_jobs,\n        train_sizes=train_sizes,\n        return_times=True,\n    )\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    fit_times_mean = np.mean(fit_times, axis=1)\n    fit_times_std = np.std(fit_times, axis=1)\n\n    # Plot learning curve\n    axes[0].grid()\n    axes[0].fill_between(\n        train_sizes,\n        train_scores_mean - train_scores_std,\n        train_scores_mean + train_scores_std,\n        alpha=0.1,\n        color=\"r\",\n    )\n    axes[0].fill_between(\n        train_sizes,\n        test_scores_mean - test_scores_std,\n        test_scores_mean + test_scores_std,\n        alpha=0.1,\n        color=\"g\",\n    )\n    axes[0].plot(\n        train_sizes, train_scores_mean, \"o-\", color=\"r\", label=\"Training score\"\n    )\n    axes[0].plot(\n        train_sizes, test_scores_mean, \"o-\", color=\"g\", label=\"Cross-validation score\"\n    )\n    axes[0].legend(loc=\"best\")\n\n    # Plot n_samples vs fit_times\n    axes[1].grid()\n    axes[1].plot(train_sizes, fit_times_mean, \"o-\")\n    axes[1].fill_between(\n        train_sizes,\n        fit_times_mean - fit_times_std,\n        fit_times_mean + fit_times_std,\n        alpha=0.1,\n    )\n    axes[1].set_xlabel(\"Training examples\")\n    axes[1].set_ylabel(\"fit_times\")\n    axes[1].set_title(\"Scalability of the model\")\n\n    # Plot fit_time vs score\n    axes[2].grid()\n    axes[2].plot(fit_times_mean, test_scores_mean, \"o-\")\n    axes[2].fill_between(\n        fit_times_mean,\n        test_scores_mean - test_scores_std,\n        test_scores_mean + test_scores_std,\n        alpha=0.1,\n    )\n    axes[2].set_xlabel(\"fit_times\")\n    axes[2].set_ylabel(\"Score\")\n    axes[2].set_title(\"Performance of the model\")\n\n    return plt","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:00:27.803059Z","iopub.execute_input":"2021-11-27T15:00:27.803421Z","iopub.status.idle":"2021-11-27T15:00:27.826953Z","shell.execute_reply.started":"2021-11-27T15:00:27.80338Z","shell.execute_reply":"2021-11-27T15:00:27.825903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LGBM","metadata":{}},{"cell_type":"markdown","source":"Hyperparameter tuning for LGBM","metadata":{}},{"cell_type":"code","source":"# Do gridsearch to find the best model f1_score, Breadth first search down the parameters, gbdt 0.5 120, 49\nparameter_grid = {\n    'boosttype': ['dart'],            # dart wins hands down here singularly\n    'learningrate': [0.1, 0.5, 1.1],\n    'num_of_estimators': [100, 130, 150, 170, 200],\n    'num_of_leaves': [30, 49, 70],\n    'L1 reg': [0.01],              \n    'L2 reg': [0.01]              \n}\n","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:00:27.829114Z","iopub.execute_input":"2021-11-27T15:00:27.829536Z","iopub.status.idle":"2021-11-27T15:00:27.844479Z","shell.execute_reply.started":"2021-11-27T15:00:27.829495Z","shell.execute_reply":"2021-11-27T15:00:27.843743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note: the CV process is commented out as it takes long to tune. Not required for submission.","metadata":{}},{"cell_type":"code","source":"# best_f1_score = 0\n# best_parameters = dict()\n# best_lgbm_model = None\n# start = time()\n# for atype in parameter_grid['boosttype']:\n#   for lr in parameter_grid['learningrate']:\n#     for est in parameter_grid['num_of_estimators']:\n#       for leaves in parameter_grid['num_of_leaves']:\n#         for L1 in parameter_grid['L1 reg']:\n#           for L2 in parameter_grid['L2 reg']:\n#             print(\"CV with boosttype={}, lr={}, est={}, leaves={}, l1={}, l2={}\".format(atype,\n#                                                                                        lr,\n#                                                                                        est,\n#                                                                                        leaves,\n#                                                                                        L1,\n#                                                                                        L2))\n#             lgmodel = lgb.LGBMClassifier(metric = \"\", num_class = 4)\n#             hyperparameters = {'boosting_type': atype,\n#                   'colsample_bytree': 0.9843467236959204,\n#                   'learning_rate': lr,\n#                   'min_child_samples': 44,\n#                   'num_leaves': leaves,\n#                   'reg_alpha': L1,      \n#                   'reg_lambda': L2,     \n#                   'subsample': 0.6299872254632797,\n#                   'subsample_for_bin': 40611\n#                   }\n            \n#             lgbm_model = lgb.LGBMClassifier(**hyperparameters, class_weight = 'balanced', max_depth=-1, objective = 'multiclass', n_jobs = -1, n_estimators = est)\n#             f1_scores = cross_val_score(lgbm_model, trainingset, trainingtargets, scoring='f1_macro', cv=cv, n_jobs=-1)\n#             mean_f1_score = round(mean(f1_scores), 3)\n#             print(\"\\tf1 score: {}\".format(mean_f1_score))\n#             if mean_f1_score > best_f1_score:\n#                 best_f1_score = mean_f1_score\n#                 best_parameters = lgbm_model.get_params()\n#                 best_lgbm_model = lgbm_model\n# print(\"Best f1 score: \", best_f1_score)\n# print(\"Best model: \")\n# print(best_parameters)\n# end = time()\n# print(\"Time elapsed: \", round(end - start, 3))\n# models_best_f1['lgbm'] = best_f1_score\n# print(\"Training accuracy: \", accuracy_score(trainingtargets, training_predictions))\n# print(\"F1 Score: \", f1_score(trainingtargets, training_predictions, average='micro'))\n# print(\"Classification Report: \")\n# print(classification_report(trainingtargets, training_predictions))","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:00:27.845936Z","iopub.execute_input":"2021-11-27T15:00:27.846179Z","iopub.status.idle":"2021-11-27T15:00:27.858729Z","shell.execute_reply.started":"2021-11-27T15:00:27.84613Z","shell.execute_reply":"2021-11-27T15:00:27.858045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Creation of the tuned LGBM model","metadata":{}},{"cell_type":"code","source":"# Now train on the full training data using found hyper parameters\nlgmodel = lgb.LGBMClassifier(metric = \"\", num_class = 4)\nlgbm_hyperparameters = {'boosting_type': 'dart',\n                        'n_estimators': 200,\n                        'colsample_bytree': 0.9,\n                        'learning_rate': 0.5,\n                        'min_child_samples': 44,\n                        'num_leaves': 70,\n                        'reg_alpha': 0.01,      \n                        'reg_lambda': 0.01,     \n                        'subsample': 0.6299872254632797,\n                        'subsample_for_bin': 40611\n                          }\n\nlgbm_model = lgb.LGBMClassifier(**lgbm_hyperparameters, class_weight = 'balanced', max_depth=-1, objective = 'multiclass', n_jobs = -1, random_state=42)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:00:27.860164Z","iopub.execute_input":"2021-11-27T15:00:27.860926Z","iopub.status.idle":"2021-11-27T15:00:27.877569Z","shell.execute_reply.started":"2021-11-27T15:00:27.860842Z","shell.execute_reply":"2021-11-27T15:00:27.876259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To plot the learning curve of LGBM","metadata":{}},{"cell_type":"code","source":"# plot_learning_curve(lgbm_model, 'LGBM', trainingset, trainingtargets, cv=cv, n_jobs=-1)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:00:27.878991Z","iopub.execute_input":"2021-11-27T15:00:27.879293Z","iopub.status.idle":"2021-11-27T15:00:27.893656Z","shell.execute_reply.started":"2021-11-27T15:00:27.87926Z","shell.execute_reply":"2021-11-27T15:00:27.892893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# MLP","metadata":{}},{"cell_type":"markdown","source":"Hyperparameter tuning of MLP","metadata":{}},{"cell_type":"code","source":"hidden_sizes = [128, 256, 512]\nhidden_layers = [2, 3, 5, 8]\nlrs = [0.002, 0.005, 0.007]\na = [0.03,0.04]","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:00:27.894735Z","iopub.execute_input":"2021-11-27T15:00:27.894984Z","iopub.status.idle":"2021-11-27T15:00:27.907455Z","shell.execute_reply.started":"2021-11-27T15:00:27.894953Z","shell.execute_reply":"2021-11-27T15:00:27.906457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# best_f1_score = 0\n# best_parameters = dict()\n# best_mlp_model = None\n# start = time()\n# for hs in hidden_sizes:\n#     for num_layers in hidden_layers:\n#         for lr in lrs:\n#             for alpha in a:\n#                 print(\"CV with hs={}, layers={}, lr={}, alpha={}\".format(hs,\n#                                                                          num_layers,\n#                                                                          lr,\n#                                                                          alpha))\n#                 hidden_layer_sizes = tuple(hs for i in range(num_layers))\n#                 mlp_model = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes, max_iter=3000,activation = 'relu',solver='adam',random_state=1,learning_rate_init = lr, alpha = alpha)\n#                 f1_scores = cross_val_score(mlp_model, trainingset, trainingtargets, scoring='f1_macro', cv=cv, n_jobs=-1)\n#                 mean_f1_score = round(mean(f1_scores), 3)\n#                 print(\"\\tf1 score: {}\".format(mean_f1_score))\n#                 if mean_f1_score > best_f1_score:\n#                     best_f1_score = mean_f1_score\n#                     best_parameters = mlp_model.get_params()\n#                     best_mlp_model = mlp_model\n# print(\"Best f1 score: \", best_f1_score)\n# print(\"Best model: \")\n# print(best_parameters)\n# end = time()\n# print(\"Time elapsed: \", round(end - start, 3))\n# models_best_f1['mlp'] = best_f1_score","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:00:27.908946Z","iopub.execute_input":"2021-11-27T15:00:27.909886Z","iopub.status.idle":"2021-11-27T15:00:27.921367Z","shell.execute_reply.started":"2021-11-27T15:00:27.90984Z","shell.execute_reply":"2021-11-27T15:00:27.920182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Creation of the tuned MLP model\n","metadata":{}},{"cell_type":"code","source":"hidden_layer_sizes = (128, 128, 128, 128, 128)\nlr = 0.005\nalpha = 0.03\nmlp_model = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes, max_iter=3000, activation = 'relu',solver='adam',random_state=1,learning_rate_init = lr, alpha = alpha)","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:00:27.922716Z","iopub.execute_input":"2021-11-27T15:00:27.923011Z","iopub.status.idle":"2021-11-27T15:00:27.938442Z","shell.execute_reply.started":"2021-11-27T15:00:27.922922Z","shell.execute_reply":"2021-11-27T15:00:27.937514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To plot the learning curve of MLP","metadata":{}},{"cell_type":"code","source":"# plot_learning_curve(mlp_model, 'Multilayer Perceptron', trainingset, trainingtargets, cv=cv, n_jobs=-1)","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:00:27.939737Z","iopub.execute_input":"2021-11-27T15:00:27.939968Z","iopub.status.idle":"2021-11-27T15:00:27.950059Z","shell.execute_reply.started":"2021-11-27T15:00:27.93994Z","shell.execute_reply":"2021-11-27T15:00:27.949202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# XGBOOST","metadata":{}},{"cell_type":"markdown","source":"Hyperparameter tuning of xgboost","metadata":{}},{"cell_type":"code","source":"# max_depth is the maximum number of nodes allowed from the root to the farthest leaf of a tree. \n# higher max_depth -> more complex trees but more prone to overfitting\n# min_child_weight is the minimum weight required in order to create a new node in the tree.\n# lower min_child_weight -> more complex trees but also more prone to overfitting\n# subsample is the ratio of training instances.\n# eta is the learning rate of the model.\ngridsearch_params = [\n    (max_depth, min_child_weight, subsample)\n    for max_depth in range(15, 24, 2)\n    for min_child_weight in range(1, 6, 2)\n    for subsample in [i/10. for i in range(8,11)]\n]\nn_estimators = 200\ndata_dmatrix = xgb.DMatrix(data=trainingset,label=trainingtargets)\nparams = {\"objective\": \"multi:softprob\", \"num_class\": 4, 'alpha': 10, 'colsample_bytree': 0.4}","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:00:27.951734Z","iopub.execute_input":"2021-11-27T15:00:27.951965Z","iopub.status.idle":"2021-11-27T15:00:28.098348Z","shell.execute_reply.started":"2021-11-27T15:00:27.951936Z","shell.execute_reply":"2021-11-27T15:00:28.097616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define initial best params and f1_score\n# best_f1_score = 0\n# best_xgb_model = None\n# best_params = None\n# for max_depth, min_child_weight, subsample in gridsearch_params:\n#     print(\"CV with max_depth={}, min_child_weight={}, subsample={}\".format(max_depth, \n#                                                                            min_child_weight,\n#                                                                            subsample))\n\n#     # Run CV\n#     xgb_model = xgb.XGBClassifier(objective='multi:softprob',\n#                                   num_class=4,\n#                                   alpha=10,\n#                                   colsample_bytree=0.4,\n#                                   max_depth=max_depth,\n#                                   min_child_weight=min_child_weight,\n#                                   subsample=subsample,\n#                                   n_estimators=n_estimators,\n#                                   early_stopping_rounds=10)\n#     f1_scores = cross_val_score(xgb_model, trainingset, trainingtargets, scoring='f1_macro', cv=cv, n_jobs=-1)\n#     mean_f1_score = round(mean(f1_scores), 3)\n#     print(\"\\tf1 score: {}\".format(mean_f1_score))\n#     if mean_f1_score > best_f1_score:\n#         best_f1_score = mean_f1_score\n#         best_parameters = (max_depth, min_child_weight, subsample)\n#         best_xgb_model = xgb_model\n# print(\"Best params: \\nmax_depth: {}, min_child_weight: {}, subsample: {}. f1 score: {}\".format(best_parameters[0], best_parameters[1], best_parameters[2], best_f1_score))\n# end = time()\n# print(\"Time elapsed: \", round(end - start, 3))\n# models_best_f1['xgb'] = best_f1_score","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:00:28.099904Z","iopub.execute_input":"2021-11-27T15:00:28.100637Z","iopub.status.idle":"2021-11-27T15:00:28.105338Z","shell.execute_reply.started":"2021-11-27T15:00:28.100598Z","shell.execute_reply":"2021-11-27T15:00:28.104722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Creation of the tuned xgboost model","metadata":{}},{"cell_type":"code","source":"params = {\"objective\": \"multi:softprob\", \"num_class\": 4, 'colsample_bytree': 1, 'alpha': 10, 'max_depth': 13, 'alpha': 10, 'min_child_weight': 1, 'subsample': 0.8, 'eta': 0.1}\nxgb_model = xgb.XGBClassifier(objective='multi:softprob',\n                              num_class=4,\n                              colsample_bytree=0.4,\n                              max_depth=21,\n                              alpha=10,\n                              min_child_weight=1,\n                              subsample=1.0,\n                              n_estimators=999,\n                              early_stopping_rounds=10)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:00:28.10936Z","iopub.execute_input":"2021-11-27T15:00:28.110001Z","iopub.status.idle":"2021-11-27T15:00:28.121365Z","shell.execute_reply.started":"2021-11-27T15:00:28.109963Z","shell.execute_reply":"2021-11-27T15:00:28.120405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To plot the learning curve of xgboost","metadata":{}},{"cell_type":"code","source":"# plot_learning_curve(xgb_model, 'XGBoost', trainingset, trainingtargets, cv=cv, n_jobs=-1)","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:00:28.122747Z","iopub.execute_input":"2021-11-27T15:00:28.123235Z","iopub.status.idle":"2021-11-27T15:00:28.137562Z","shell.execute_reply.started":"2021-11-27T15:00:28.123199Z","shell.execute_reply":"2021-11-27T15:00:28.136507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# KNN","metadata":{}},{"cell_type":"markdown","source":"Hyperparameter tuning for KNN","metadata":{}},{"cell_type":"code","source":"k_nearest_neighbours = [2, 5, 10, 20, 50, 100, 200]\n","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:00:28.139287Z","iopub.execute_input":"2021-11-27T15:00:28.139845Z","iopub.status.idle":"2021-11-27T15:00:28.149679Z","shell.execute_reply.started":"2021-11-27T15:00:28.1398Z","shell.execute_reply":"2021-11-27T15:00:28.148643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# best_f1_score = 0\n# best_parameters = dict()\n# best_knn_model = None\n# start = time()\n# for num_neighbours in k_nearest_neighbours:\n#     print(\"CV with num_neighbours={}\".format(num_neighbours))\n#     knn_model = KNeighborsClassifier(n_neighbors=num_neighbours)\n#     f1_scores = cross_val_score(knn_model, trainingset, trainingtargets, scoring='f1_macro', cv=cv, n_jobs=-1)\n#     mean_f1_score = round(mean(f1_scores), 3)\n#     print(\"\\tf1 score: {}\".format(mean_f1_score))\n#     if mean_f1_score > best_f1_score:\n#         best_f1_score = mean_f1_score\n#         best_parameters = knn_model.get_params()\n#         best_knn_model = knn_model\n# print(\"Best f1 score: \", best_f1_score)\n# print(\"Best model: \")\n# print(best_parameters)\n# end = time()\n# print(\"Time elapsed: \", round(end - start, 3))\n# models_best_f1['knn'] = best_f1_score","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:00:28.151323Z","iopub.execute_input":"2021-11-27T15:00:28.151787Z","iopub.status.idle":"2021-11-27T15:00:28.161724Z","shell.execute_reply.started":"2021-11-27T15:00:28.151751Z","shell.execute_reply":"2021-11-27T15:00:28.160768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Creation of the tuned KNN model","metadata":{}},{"cell_type":"code","source":"knn_model = KNeighborsClassifier(n_neighbors=2)","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:00:28.163263Z","iopub.execute_input":"2021-11-27T15:00:28.163519Z","iopub.status.idle":"2021-11-27T15:00:28.173442Z","shell.execute_reply.started":"2021-11-27T15:00:28.163486Z","shell.execute_reply":"2021-11-27T15:00:28.172311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To plot the learning curve of KNN","metadata":{}},{"cell_type":"code","source":"# plot_learning_curve(knn_model, 'K-Nearest Neighbours', trainingset, trainingtargets, cv=cv, n_jobs=-1)","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:00:28.174781Z","iopub.execute_input":"2021-11-27T15:00:28.175039Z","iopub.status.idle":"2021-11-27T15:00:28.185871Z","shell.execute_reply.started":"2021-11-27T15:00:28.175005Z","shell.execute_reply":"2021-11-27T15:00:28.184882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Random Forest","metadata":{}},{"cell_type":"markdown","source":"Hyperparameter tuning of random forest","metadata":{}},{"cell_type":"code","source":"n_classifiers = [100, 200, 300, 500, 700, 1000]","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:00:28.187519Z","iopub.execute_input":"2021-11-27T15:00:28.187997Z","iopub.status.idle":"2021-11-27T15:00:28.197915Z","shell.execute_reply.started":"2021-11-27T15:00:28.187962Z","shell.execute_reply":"2021-11-27T15:00:28.196767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# best_f1_score = 0\n# best_parameters = dict()\n# best_rf_model = None\n# start = time()\n# for num_classifiers in n_classifiers:\n#     print(\"CV with num_classifiers={}\".format(num_classifiers))\n#     rf_model = RandomForestClassifier(n_jobs=-1, n_estimators=num_classifiers, class_weight=\"balanced\")\n#     f1_scores = cross_val_score(rf_model, trainingset, trainingtargets, scoring='f1_macro', cv=cv, n_jobs=-1)\n#     mean_f1_score = round(mean(f1_scores), 3)\n#     print(\"\\tf1 score: {}\".format(mean_f1_score))\n#     if mean_f1_score > best_f1_score:\n#         best_f1_score = mean_f1_score\n#         best_parameters = rf_model.get_params()\n#         best_rf_model = rf_model\n# print(\"Best f1 score: \", best_f1_score)\n# print(\"Best model: \")\n# print(best_parameters)\n# end = time()\n# print(\"Time elapsed: \", round(end - start, 3))\n# models_best_f1['rf'] = best_f1_score","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:00:28.199589Z","iopub.execute_input":"2021-11-27T15:00:28.200578Z","iopub.status.idle":"2021-11-27T15:00:28.21028Z","shell.execute_reply.started":"2021-11-27T15:00:28.200531Z","shell.execute_reply":"2021-11-27T15:00:28.209297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Creation of the tuned random forest model","metadata":{}},{"cell_type":"code","source":"rf_model = RandomForestClassifier(n_jobs=-1, n_estimators=700, class_weight=\"balanced\")","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:00:28.211875Z","iopub.execute_input":"2021-11-27T15:00:28.212348Z","iopub.status.idle":"2021-11-27T15:00:28.227371Z","shell.execute_reply.started":"2021-11-27T15:00:28.212308Z","shell.execute_reply":"2021-11-27T15:00:28.226554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To plot the learning curve of random forest","metadata":{}},{"cell_type":"code","source":"# plot_learning_curve(rf_model, 'Random Forest', trainingset, trainingtargets, cv=cv, n_jobs=-1)","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:00:28.228969Z","iopub.execute_input":"2021-11-27T15:00:28.22971Z","iopub.status.idle":"2021-11-27T15:00:28.239872Z","shell.execute_reply.started":"2021-11-27T15:00:28.229674Z","shell.execute_reply":"2021-11-27T15:00:28.238955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Multi-class SVM","metadata":{}},{"cell_type":"markdown","source":"Hyperparameter tuning of SVM","metadata":{}},{"cell_type":"code","source":"cs = [0.01,0.02,0.03,0.1,0.2,0.3,1,2,3,4,5,6]","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:00:28.241313Z","iopub.execute_input":"2021-11-27T15:00:28.241911Z","iopub.status.idle":"2021-11-27T15:00:28.252758Z","shell.execute_reply.started":"2021-11-27T15:00:28.241873Z","shell.execute_reply":"2021-11-27T15:00:28.251561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# best_f1_score = 0\n# best_parameters = dict()\n# best_svm_model = None\n# start = time()\n# # rbf kernel so not tuning degree\n# for c in cs:\n#     print(\"CV with c={}\".format(c))\n#     svm_model = svm.SVC(kernel='rbf', C=c)\n#     f1_scores = cross_val_score(svm_model, trainingset, trainingtargets, scoring='f1_macro', cv=cv, n_jobs=-1)\n#     mean_f1_score = round(mean(f1_scores), 3)\n#     print(\"\\tf1 score: {}\".format(mean_f1_score))\n#     if mean_f1_score > best_f1_score:\n#         best_f1_score = mean_f1_score\n#         best_parameters = svm_model.get_params()\n#         best_svm_model = svm_model\n# print(\"Best f1 score: \", best_f1_score)\n# print(\"Best model: \")\n# print(best_parameters)\n# end = time()\n# print(\"Time elapsed: \", round(end - start, 3))\n# models_best_f1['svm'] = best_f1_score","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:00:28.254433Z","iopub.execute_input":"2021-11-27T15:00:28.254922Z","iopub.status.idle":"2021-11-27T15:00:28.26539Z","shell.execute_reply.started":"2021-11-27T15:00:28.254811Z","shell.execute_reply":"2021-11-27T15:00:28.264465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Creation of the tuned SVM model","metadata":{}},{"cell_type":"code","source":"svm_model = svm.SVC(kernel='rbf', C=3)","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:00:28.267526Z","iopub.execute_input":"2021-11-27T15:00:28.268464Z","iopub.status.idle":"2021-11-27T15:00:28.276837Z","shell.execute_reply.started":"2021-11-27T15:00:28.268411Z","shell.execute_reply":"2021-11-27T15:00:28.276156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To plot the learning curve of SVM","metadata":{}},{"cell_type":"code","source":"# plot_learning_curve(svm_model, 'Support Vector Machine', trainingset, trainingtargets, cv=cv, n_jobs=-1)","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:00:28.278227Z","iopub.execute_input":"2021-11-27T15:00:28.279151Z","iopub.status.idle":"2021-11-27T15:00:28.287102Z","shell.execute_reply.started":"2021-11-27T15:00:28.279085Z","shell.execute_reply":"2021-11-27T15:00:28.286223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating bagging classifiers from models","metadata":{}},{"cell_type":"code","source":"lgbm_bagging_clf = BaggingClassifier(base_estimator=lgbm_model, n_estimators=1)\nmlp_bagging_clf = BaggingClassifier(base_estimator=mlp_model, n_estimators=1)\nxgb_bagging_clf = BaggingClassifier(base_estimator=xgb_model, n_estimators=1)\nrf_bagging_clf = BaggingClassifier(base_estimator=rf_model, n_estimators=1)\nknn_bagging_clf = BaggingClassifier(base_estimator=knn_model, n_estimators=1)\nsvm_bagging_clf = BaggingClassifier(base_estimator=svm_model, n_estimators=1)","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:00:28.289327Z","iopub.execute_input":"2021-11-27T15:00:28.290082Z","iopub.status.idle":"2021-11-27T15:00:28.303031Z","shell.execute_reply.started":"2021-11-27T15:00:28.290022Z","shell.execute_reply":"2021-11-27T15:00:28.302372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Combining classifiers into a voting classifier","metadata":{}},{"cell_type":"code","source":"lgbm_f1 = 0.916\nmlp_f1 = 0.772\nxgb_f1 = 0.919\nrf_f1 = 0.896\nknn_f1 = 0.501\n\nweights = [lgbm_f1, mlp_f1, xgb_f1, rf_f1, knn_f1]\nmodels = [('lgbm', lgbm_bagging_clf), ('mlp', mlp_bagging_clf), ('xgb', xgb_bagging_clf), ('rf', rf_bagging_clf), ('knn', knn_bagging_clf)]","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:00:28.304671Z","iopub.execute_input":"2021-11-27T15:00:28.305186Z","iopub.status.idle":"2021-11-27T15:00:28.315066Z","shell.execute_reply.started":"2021-11-27T15:00:28.305093Z","shell.execute_reply":"2021-11-27T15:00:28.314271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating the LGBM bagging ensemble","metadata":{}},{"cell_type":"code","source":"clfs = []\nlgbm_model = lgb.LGBMClassifier(**lgbm_hyperparameters, class_weight = 'balanced', max_depth=-1, objective = 'multiclass', n_jobs = -1, random_seed=39)\nlgbm_bagging_clf = BaggingClassifier(base_estimator=lgbm_model, n_estimators=15)\nclfs.append(('lgbm{}'.format(i), lgbm_bagging_clf))\ncombined_voting_clf = VotingClassifier(estimators=clfs, weights=weights, voting='hard')","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:00:28.316577Z","iopub.execute_input":"2021-11-27T15:00:28.317223Z","iopub.status.idle":"2021-11-27T15:00:28.326518Z","shell.execute_reply.started":"2021-11-27T15:00:28.317189Z","shell.execute_reply":"2021-11-27T15:00:28.325814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fitting train data and submitting predictions","metadata":{}},{"cell_type":"code","source":"target_weights = class_weight.compute_sample_weight('balanced', trainingtargets, indices=None)\nlgbm_bagging_clf.fit(fulltrainingset, fulltrainingsettargets, sample_weight=target_weights)\ntest_predictions = lgbm_bagging_clf.predict(testset)\nsubmission = pd.DataFrame({'id': newtestset.Id, 'Target': test_predictions})\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:00:28.327714Z","iopub.execute_input":"2021-11-27T15:00:28.329024Z","iopub.status.idle":"2021-11-27T15:02:44.843015Z","shell.execute_reply.started":"2021-11-27T15:00:28.32898Z","shell.execute_reply":"2021-11-27T15:02:44.8423Z"},"trusted":true},"execution_count":null,"outputs":[]}]}