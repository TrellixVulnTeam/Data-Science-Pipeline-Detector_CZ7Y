{"cells":[{"metadata":{"_uuid":"c6e1ccce811e7a1d76282fcb8a13edf92672f834"},"cell_type":"markdown","source":"## Split the data\n\nWe split the data by household to avoid leakage, since rows belonging to the same household usually have the same target. Since we filter the data to only include heads of household this isn't technically necessary, but it provides an easy way to use the entire training data set if we want to do that.\n\nNote that after splitting the data we overwrite the train data with the entire data set so we can train on all of the data. The split_data function does the same thing without overwriting the data, and is used within the training loop to (hopefully) approximate a K-Fold split. "},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd \n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline \n\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom sklearn.metrics import f1_score\nfrom joblib import Parallel, delayed\nfrom sklearn.base import clone\nfrom sklearn.ensemble import VotingClassifier, ExtraTreesClassifier, RandomForestClassifier\nfrom sklearn.utils import class_weight\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.preprocessing import LabelEncoder","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/costa-rican-household-poverty-prediction/train.csv\")\ntest = pd.read_csv(\"../input/costa-rican-household-poverty-prediction/test.csv\")\ntest_ids = test.Id\n\ntrain.drop(['Id'], axis=1, inplace=True)\ntest.drop(['Id'], axis=1, inplace=True)\n\ntrain_length = train.shape[0]\ntest_length = test.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Problem:\n# The whole thing is like hanging on a threshold, literally any structural / logical changes will cause the results to change (drop).\n\n# Current situation: encode train and test's idhogar separely, then process data\n    \n# If we don't encode / encode together, then the train_test split will be different from original, the result will drop.\n\n# If we encode separately but process data together, then the train and test will share some idhogar, causing things like num_over_18 to mess up.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\n# this only transforms the idhogar field, the other things this function used to do are done elsewhere\ndef encode_data(df):\n    df['idhogar'] = LabelEncoder().fit_transform(df['idhogar'])\n    \ndef do_features(df):\n    feats_div = [('children_fraction', 'r4t1', 'r4t3'), \n                 ('working_man_fraction', 'r4h2', 'r4t3'),\n                 ('all_man_fraction', 'r4h3', 'r4t3'),\n                 ('human_density', 'tamviv', 'rooms'),\n                 ('human_bed_density', 'tamviv', 'bedrooms'),\n                 ('rent_per_person', 'v2a1', 'r4t3'),\n                 ('rent_per_room', 'v2a1', 'rooms'),\n                 ('mobile_density', 'qmobilephone', 'r4t3'),\n                 ('tablet_density', 'v18q1', 'r4t3'),\n                 ('mobile_adult_density', 'qmobilephone', 'r4t2'),\n                 ('tablet_adult_density', 'v18q1', 'r4t2'),\n                ]\n    \n    feats_sub = [('people_not_living', 'tamhog', 'tamviv'),\n                 ('people_weird_stat', 'tamhog', 'r4t3')]\n\n    for f_new, f1, f2 in feats_div:\n        df['fe_' + f_new] = (df[f1] / df[f2]).astype(np.float32)       \n    for f_new, f1, f2 in feats_sub:\n        df['fe_' + f_new] = (df[f1] - df[f2]).astype(np.float32)\n    \n    # aggregation rules over household\n    aggs_num = {'age': ['min', 'max', 'mean'],\n                'escolari': ['min', 'max', 'mean']\n               }\n    \n    aggs_cat = {'dis': ['mean']}\n    for s_ in ['estadocivil', 'parentesco', 'instlevel']:\n        for f_ in [f_ for f_ in df.columns if f_.startswith(s_)]:\n            aggs_cat[f_] = ['mean', 'count']\n\n    # aggregation over household\n    for name_, df_ in [('18', df.query('age >= 18'))]:\n        df_agg = df_.groupby('idhogar').agg({**aggs_num, **aggs_cat}).astype(np.float32)\n        df_agg.columns = pd.Index(['agg' + name_ + '_' + e[0] + \"_\" + e[1].upper() for e in df_agg.columns.tolist()])\n        df = df.join(df_agg, how='left', on='idhogar')\n        del df_agg\n\n    # Drop id's\n#     df.drop(['Id'], axis=1, inplace=True)\n    \n    return df\n\n# convert one hot encoded fields to label encoding\ndef convert_OHE2LE(df):\n    tmp_df = df.copy(deep=True)\n    for s_ in ['pared', 'piso', 'techo', 'abastagua', 'sanitario', 'energcocinar', 'elimbasu', \n               'epared', 'etecho', 'eviv', 'estadocivil', 'parentesco', \n               'instlevel', 'lugar', 'tipovivi',\n               'manual_elec']:\n        if 'manual_' not in s_:\n            cols_s_ = [f_ for f_ in df.columns if f_.startswith(s_)]\n        elif 'elec' in s_:\n            cols_s_ = ['public', 'planpri', 'noelec', 'coopele']\n        sum_ohe = tmp_df[cols_s_].sum(axis=1).unique()\n        #deal with those OHE, where there is a sum over columns == 0\n        if 0 in sum_ohe:\n            print('The OHE in {} is incomplete. A new column will be added before label encoding'\n                  .format(s_))\n            # dummy colmn name to be added\n            col_dummy = s_+'_dummy'\n            # add the column to the dataframe\n            tmp_df[col_dummy] = (tmp_df[cols_s_].sum(axis=1) == 0).astype(np.int8)\n            # add the name to the list of columns to be label-encoded\n            cols_s_.append(col_dummy)\n            # proof-check, that now the category is complete\n            sum_ohe = tmp_df[cols_s_].sum(axis=1).unique()\n            if 0 in sum_ohe:\n                 print(\"The category completion did not work\")\n        tmp_cat = tmp_df[cols_s_].idxmax(axis=1)\n        tmp_df[s_ + '_LE'] = LabelEncoder().fit_transform(tmp_cat).astype(np.int16)\n        if 'parentesco1' in cols_s_:\n            cols_s_.remove('parentesco1')\n        tmp_df.drop(cols_s_, axis=1, inplace=True)\n    return tmp_df\n\ndef process_df(df_):\n    # encode the idhogar\n    encode_data(df_)\n    \n    # create aggregate features\n    return do_features(df_)\n\ntrain = process_df(train)\ntest = process_df(test)\n\n# some dependencies are Na, fill those with the square root of the square\ntrain['dependency'] = np.sqrt(train['SQBdependency'])\ntest['dependency'] = np.sqrt(test['SQBdependency'])\n\n# fill \"no\"s for education with 0s\ntrain.loc[train['edjefa'] == \"no\", \"edjefa\"] = 0\ntrain.loc[train['edjefe'] == \"no\", \"edjefe\"] = 0\ntest.loc[test['edjefa'] == \"no\", \"edjefa\"] = 0\ntest.loc[test['edjefe'] == \"no\", \"edjefe\"] = 0\n\n# if education is \"yes\" and person is head of household, fill with escolari\ntrain.loc[(train['edjefa'] == \"yes\") & (train['parentesco1'] == 1), \"edjefa\"] = train.loc[(train['edjefa'] == \"yes\") & (train['parentesco1'] == 1), \"escolari\"]\ntrain.loc[(train['edjefe'] == \"yes\") & (train['parentesco1'] == 1), \"edjefe\"] = train.loc[(train['edjefe'] == \"yes\") & (train['parentesco1'] == 1), \"escolari\"]\n\ntest.loc[(test['edjefa'] == \"yes\") & (test['parentesco1'] == 1), \"edjefa\"] = test.loc[(test['edjefa'] == \"yes\") & (test['parentesco1'] == 1), \"escolari\"]\ntest.loc[(test['edjefe'] == \"yes\") & (test['parentesco1'] == 1), \"edjefe\"] = test.loc[(test['edjefe'] == \"yes\") & (test['parentesco1'] == 1), \"escolari\"]\n\n# this field is supposed to be interaction between gender and escolari, but it isn't clear what \"yes\" means, let's fill it with 4\ntrain.loc[train['edjefa'] == \"yes\", \"edjefa\"] = 4\ntrain.loc[train['edjefe'] == \"yes\", \"edjefe\"] = 4\n\ntest.loc[test['edjefa'] == \"yes\", \"edjefa\"] = 4\ntest.loc[test['edjefe'] == \"yes\", \"edjefe\"] = 4\n\n# convert to int for our models\ntrain['edjefe'] = train['edjefe'].astype(\"int\")\ntrain['edjefa'] = train['edjefa'].astype(\"int\")\ntest['edjefe'] = test['edjefe'].astype(\"int\")\ntest['edjefa'] = test['edjefa'].astype(\"int\")\n\n# create feature with max education of either head of household\ntrain['edjef'] = np.max(train[['edjefa','edjefe']], axis=1)\ntest['edjef'] = np.max(test[['edjefa','edjefe']], axis=1)\n\n# fill some nas\ntrain['v2a1']=train['v2a1'].fillna(0)\ntest['v2a1']=test['v2a1'].fillna(0)\n\ntest['v18q1']=test['v18q1'].fillna(0)\ntrain['v18q1']=train['v18q1'].fillna(0)\n\ntrain['rez_esc']=train['rez_esc'].fillna(0)\ntest['rez_esc']=test['rez_esc'].fillna(0)\n\ntrain.loc[train.meaneduc.isnull(), \"meaneduc\"] = 0\ntrain.loc[train.SQBmeaned.isnull(), \"SQBmeaned\"] = 0\n\ntest.loc[test.meaneduc.isnull(), \"meaneduc\"] = 0\ntest.loc[test.SQBmeaned.isnull(), \"SQBmeaned\"] = 0\n\n# fix some inconsistencies in the data - some rows indicate both that the household does and does not have a toilet, \n# if there is no water we'll assume they do not\ntrain.loc[(train.v14a ==  1) & (train.sanitario1 ==  1) & (train.abastaguano == 0), \"v14a\"] = 0\ntrain.loc[(train.v14a ==  1) & (train.sanitario1 ==  1) & (train.abastaguano == 0), \"sanitario1\"] = 0\n\ntest.loc[(test.v14a ==  1) & (test.sanitario1 ==  1) & (test.abastaguano == 0), \"v14a\"] = 0\ntest.loc[(test.v14a ==  1) & (test.sanitario1 ==  1) & (test.abastaguano == 0), \"sanitario1\"] = 0\n\ndef train_test_apply_func(train_, test_, func_):\n    test_['Target'] = 0\n    xx = pd.concat([train_, test_])\n\n    xx_func = func_(xx)\n    train_ = xx_func.iloc[:train_.shape[0], :]\n    test_  = xx_func.iloc[train_.shape[0]:, :].drop('Target', axis=1)\n\n    del xx, xx_func\n    return train_, test_\n\n# convert the one hot fields into label encoded\ntrain, test = train_test_apply_func(train, test, convert_OHE2LE)\n\ncols_2_ohe = ['eviv_LE', 'etecho_LE', 'epared_LE', 'elimbasu_LE', \n              'energcocinar_LE', 'sanitario_LE', 'manual_elec_LE',\n              'pared_LE']\ncols_nums = ['age', 'meaneduc', 'dependency', \n             'hogar_nin', 'hogar_adul', 'hogar_mayor', 'hogar_total',\n             'bedrooms', 'overcrowding']\n\ndef convert_geo2aggs(df_):\n    tmp_df = pd.concat([df_[(['lugar_LE', 'idhogar']+cols_nums)],\n                        pd.get_dummies(df_[cols_2_ohe], \n                                       columns=cols_2_ohe)],axis=1)\n\n    geo_agg = tmp_df.groupby(['lugar_LE','idhogar']).mean().groupby('lugar_LE').mean().astype(np.float32)\n    geo_agg.columns = pd.Index(['geo_' + e for e in geo_agg.columns.tolist()])\n    \n    del tmp_df\n    return df_.join(geo_agg, how='left', on='lugar_LE')\n\n# add some aggregates by geography\ntrain, test = train_test_apply_func(train, test, convert_geo2aggs)\n\n# add some extra features, these were taken from another kernel\ndef extract_features(df):\n#     tmp_df = pd.concat([df[(['lugar_LE', 'idhogar']+cols_nums)],\n#                         pd.get_dummies(df[cols_2_ohe], \n#                                        columns=cols_2_ohe)],axis=1)\n\n#     geo_agg = tmp_df.groupby(['lugar_LE','idhogar']).mean().groupby('lugar_LE').mean().astype(np.float32)\n#     geo_agg.columns = pd.Index(['geo_' + e for e in geo_agg.columns.tolist()])\n#     df = df.join(geo_agg, how = 'left', on = 'lugar_LE')\n    \n    df['num_over_18'] = 0\n    df['num_over_18'] = df[df.age >= 18].groupby('idhogar').transform(\"count\")\n    df['num_over_18'] = df.groupby(\"idhogar\")[\"num_over_18\"].transform(\"max\")\n    df['num_over_18'] = df['num_over_18'].fillna(0)\n\n    df['bedrooms_to_rooms'] = df['bedrooms']/df['rooms']\n    df['rent_to_rooms'] = df['v2a1']/df['rooms']\n    df['tamhog_to_rooms'] = df['tamhog']/df['rooms'] # tamhog - size of the household\n    df['r4t3_to_tamhog'] = df['r4t3']/df['tamhog'] # r4t3 - Total persons in the household\n    df['r4t3_to_rooms'] = df['r4t3']/df['rooms'] # r4t3 - Total persons in the household\n    df['v2a1_to_r4t3'] = df['v2a1']/df['r4t3'] # rent to people in household\n    df['v2a1_to_r4t3'] = df['v2a1']/(df['r4t3'] - df['r4t1']) # rent to people under age 12\n    df['hhsize_to_rooms'] = df['hhsize']/df['rooms'] # rooms per person\n    df['rent_to_hhsize'] = df['v2a1']/df['hhsize'] # rent to household size\n    df['rent_to_over_18'] = df['v2a1']/df['num_over_18']\n    df.loc[df.num_over_18 == 0, \"rent_to_over_18\"] = df[df.num_over_18 == 0].v2a1\n    \nextract_features(train)    \nextract_features(test)   \n\n# test[\"Target\"] = 0\n# combined = pd.concat([train, test])\n# combined.loc[combined.num_over_18 == 0, \"rent_to_over_18\"] = combined[combined.num_over_18 == 0].v2a1\n# train = combined.iloc[:train_length, :]\n# test = combined.iloc[train_length:, :]\n# test.drop(columns = [\"Target\"], inplace = True)\n\n# drop duplicated columns\nneedless_cols = ['r4t3', 'tamhog', 'tamviv', 'hhsize', 'v18q', 'v14a', 'agesq',\n                 'mobilephone', 'female', ]\n\ninstlevel_cols = [s for s in train.columns.tolist() if 'instlevel' in s]\n\nneedless_cols.extend(instlevel_cols)\n\nprint(needless_cols)\n\ntrain = train.drop(needless_cols, axis=1)\ntest = test.drop(needless_cols, axis=1)\n\ntrain.to_csv(\"train_processed.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e14a9619ca6516b225b55bf65d6a9e423d6b5fc7","trusted":true},"cell_type":"code","source":"np.random.seed(seed=None)\n\n# We are only interested in heads of household\n# There are 2973 heads of household in the train set\nX = train[train.parentesco1 == 1]\n\n# Extract their poverty level and set it aside as y\ny = X['Target'] - 1\nX.drop(['Target'], axis=1, inplace = True)\n\nX_ids = X.idhogar\n\n# Since our training data is so imbalanced, we shall not allocate the same weight to all target classes\n# Instead, target classes that are more \"rare\" shall get higher weight\n# y_train_weights = class_weight.compute_sample_weight('balanced', y_train, indices=None)\ny_weights = class_weight.compute_sample_weight('balanced', y, indices=None)\n\n# Let's take a look\n# Indeed, target class 1, which is the rarest, gets the highest weight\n# Then weight decreases as the Target class becomes more common\n# print(pd.DataFrame(dict(Target = y_train+1, Weight = y_train_weights)).drop_duplicates().sort_values(by = [\"Target\"]).reset_index(drop = True))\nprint(pd.DataFrame(dict(Target = y, Weight = y_weights)).drop_duplicates().sort_values(by = [\"Target\"]).reset_index(drop = True))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def split_data(train, y, sample_weight=None, test_percentage=0.20):   \n    # pick some random households to use for the test data\n    test_idx = np.random.choice(train.index, size = int(train.shape[0] * test_percentage), replace = False)\n    \n    X_test = train.loc[test_idx]\n    y_test = y.loc[test_idx]\n\n    X_train = train.loc[~train.index.isin(test_idx)]\n    y_train = y.loc[~train.index.isin(test_idx)]\n    \n    if sample_weight is not None:\n        y_train_weights = sample_weight[~train.index.isin(test_idx)]\n        return X_train, y_train, X_test, y_test, y_train_weights\n    \n    return X_train, y_train, X_test, y_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X, train_y, val_X, val_y, train_y_weights = split_data(X, y, y_weights, test_percentage = 0.30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate_macroF1_lgb(predictions, truth):  \n    # this follows the discussion in https://github.com/Microsoft/LightGBM/issues/1483\n    pred_labels = predictions.argmax(axis=1)\n    truth = truth.get_label()\n    f1 = f1_score(truth, pred_labels, average='macro')\n    return ('macroF1', 1-f1) \n\nfit_params={\"early_stopping_rounds\":500,\n            \"eval_metric\" : evaluate_macroF1_lgb, \n            'verbose': False,\n           }\n\nxgb_ = xgb.XGBClassifier(random_state=217, n_jobs=-1)\nfit_params[\"eval_set\"] = [(val_X.drop(columns = [\"idhogar\", \"parentesco1\"]),val_y)]\nmodel = xgb_.fit(train_X.drop(columns = [\"idhogar\", \"parentesco1\"]), train_y, sample_weight = train_y_weights, **fit_params)\n\nuseless_xgb_cols = train_X.drop(columns = [\"idhogar\", \"parentesco1\"]).columns[model.feature_importances_ == 0]\n\n# Useless rf columns\nrf_drop_columns = ['agg18_age_MAX', 'agg18_age_MEAN', 'agg18_age_MIN', 'agg18_dis_MEAN',\n                   'agg18_escolari_MAX', 'agg18_escolari_MEAN', 'agg18_escolari_MIN',\n                   'agg18_estadocivil1_COUNT', 'agg18_estadocivil1_MEAN',\n                   'agg18_estadocivil2_COUNT', 'agg18_estadocivil2_MEAN',\n                   'agg18_estadocivil3_COUNT', 'agg18_estadocivil3_MEAN',\n                   'agg18_estadocivil4_COUNT', 'agg18_estadocivil4_MEAN',\n                   'agg18_estadocivil5_COUNT', 'agg18_estadocivil5_MEAN',\n                   'agg18_estadocivil6_COUNT', 'agg18_estadocivil6_MEAN',\n                   'agg18_estadocivil7_COUNT', 'agg18_estadocivil7_MEAN',\n                   'agg18_parentesco10_COUNT', 'agg18_parentesco10_MEAN',\n                   'agg18_parentesco11_COUNT', 'agg18_parentesco11_MEAN',\n                   'agg18_parentesco12_COUNT', 'agg18_parentesco12_MEAN',\n                   'agg18_parentesco1_COUNT', 'agg18_parentesco1_MEAN',\n                   'agg18_parentesco2_COUNT', 'agg18_parentesco2_MEAN',\n                   'agg18_parentesco3_COUNT', 'agg18_parentesco3_MEAN',\n                   'agg18_parentesco4_COUNT', 'agg18_parentesco4_MEAN',\n                   'agg18_parentesco5_COUNT', 'agg18_parentesco5_MEAN',\n                   'agg18_parentesco6_COUNT', 'agg18_parentesco6_MEAN',\n                   'agg18_parentesco7_COUNT', 'agg18_parentesco7_MEAN',\n                   'agg18_parentesco8_COUNT', 'agg18_parentesco8_MEAN',\n                   'agg18_parentesco9_COUNT', 'agg18_parentesco9_MEAN',\n                   'parentesco_LE', 'rez_esc', \"idhogar\", \"parentesco1\",\n                   'fe_rent_per_person', 'fe_rent_per_room','fe_tablet_adult_density', \n                   'fe_tablet_density'\n                  ]\n\nrf = RandomForestClassifier(random_state=217, n_jobs=-1)\nrf.fit(train_X.drop(columns = rf_drop_columns), train_y)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"useless_rf_cols = train_X.drop(columns = rf_drop_columns).columns[rf.feature_importances_ == 0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d9bcfbfcfc0b5b7e3aaeb2b0c8495bd92fdf51a3","trusted":true},"cell_type":"code","source":"xgb_drop_columns = useless_xgb_cols.tolist() + [\"idhogar\", \"parentesco1\"]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2dc384aeb44db2454978df78fdbb84b2b1ff3ced"},"cell_type":"markdown","source":"# XGBoost + Random Forests\n\nThe parameters are optimised with a random search in this kernel: https://www.kaggle.com/mlisovyi/lighgbm-hyperoptimisation-with-f1-macro"},{"metadata":{"_uuid":"629cf88dd0596a98d58487495e5b096636e2586a","trusted":true},"cell_type":"code","source":"def evaluate_macroF1_lgb(predictions, truth):  \n    # this follows the discussion in https://github.com/Microsoft/LightGBM/issues/1483\n    pred_labels = predictions.argmax(axis=1)\n    truth = truth.get_label()\n    f1 = f1_score(truth, pred_labels, average='macro')\n    return ('macroF1', 1-f1) \n\nfit_params={\"early_stopping_rounds\":500,\n            \"eval_metric\" : evaluate_macroF1_lgb, \n            'verbose': False,\n           }","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"101a2fd45bbbe6c7fb351513803550d4edeef2b3","trusted":true},"cell_type":"code","source":"np.random.seed(100)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"37b909c2aa273651b7bb57c69b939760f14f38f7","scrolled":true,"trusted":true},"cell_type":"code","source":"xgbs = []\n\nxgb_opt_parameters = {'n_estimators':300, 'learning_rate':0.15, 'max_depth':35, 'eta':0.15, \n                      'objective':'multi:softmax', 'min_child_weight': 2, 'num_class': 4, 'gamma': 2.5, \n                      'colsample_bylevel': 1, 'subsample': 0.95, 'colsample_bytree': 0.85, 'reg_lambda': 0.35 }\n\nxgb_opt_parameters = {'colsample_bytree': 0.9396309324969985, 'gamma': 0.0019562316326860586, 'learning_rate': 0.013457753609263417, 'max_depth': 26, 'max_leaves': 191, 'min_child_weight': 7, 'subsample': 0.8053153785018299}\n\n\nfor i in range(15):\n    xgb_ = xgb.XGBClassifier(random_state=217+i, n_jobs=-1, **xgb_opt_parameters)\n    train_X_train, train_y_train, train_X_val, train_y_val, train_y_train_weights = split_data(train_X.drop(columns = xgb_drop_columns), train_y, sample_weight=train_y_weights)\n    fit_params[\"eval_set\"] = [(train_X_val,train_y_val)]\n    xgb_.fit(train_X_train, train_y_train, sample_weight = train_y_train_weights, **fit_params)\n    xgbs.append(xgb_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aa255a424ccbd3839015c82777963dc6b79d8cde"},"cell_type":"code","source":"rfs = []\n\nrf_opt_parameters = {\"max_depth\": None, \"n_estimators\": 500, \"min_impurity_decrease\": 1e-3, \n                    \"min_samples_leaf\": 2, \"class_weight\": \"balanced\"}\n\nfor i in range(10):\n    rf = RandomForestClassifier(random_state=217+i, n_jobs=-1, **rf_opt_parameters)\n    rf.fit(train_X.drop(columns = rf_drop_columns), train_y)\n    rfs.append(rf) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def combine_voters(data, weights=[0.5, 0.5]):\n    vc1_probs = predict_proba(xgbs, data.drop(xgb_drop_columns, axis=1))\n    vc2_probs = predict_proba(rfs, data.drop(rf_drop_columns, axis=1))\n\n    final_vote = (vc1_probs * weights[0]) + (vc2_probs * weights[1])\n    predictions = np.argmax(final_vote, axis=1)\n    return predictions\n\ndef predict_proba(estimators, data):\n    pred = np.zeros((len(data),4))\n    for est in estimators:\n        pred += est.predict_proba(data)\n    norm_pred = pred/len(estimators)\n    \n    return norm_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# vc1_probs = predict_proba(xgbs, train.drop(xgb_drop_columns + ['Target'], axis=1))\n# vc2_probs = predict_proba(rfs, train.drop(rf_drop_columns + ['Target'], axis=1))\n    \n# combined_probs = pd.concat([pd.DataFrame(vc1_probs), pd.DataFrame(vc2_probs)], axis = 1)\n\n# combined_probs = combined_probs[train.parentesco1 == 1]\n# probs_sum = vc1_probs + vc2_probs\n# prediction = np.argmax(probs_sum, axis = 1)\n# prediction = prediction[train.parentesco1 == 1]\n\n# combined_votes = combine_voters(train.drop(columns = [\"Target\"]), weights = [0.48, 0.52])[train.parentesco1 == 1]\n# combined_probs[\"Prediction\"] = combined_votes + 1\n# combined_probs[\"Actual\"] = y + 1\n# # We will create another column called Manual Prediction to test our thresholding\n# combined_probs[\"Manual_Prediction\"] = combined_votes + 1\n\n# # Before manual tuning: f1 score = 0.8345588156251473\n# print(f1_score(combined_probs[\"Actual\"], combined_probs[\"Prediction\"], average = \"macro\"))\n\n# combined_probs[\"prob_2\"] = 0.48* combined_probs.iloc[:, 1] + 0.52* combined_probs.iloc[:, 5]\n# combined_probs[\"prob_3\"] = combined_probs.iloc[:, 2] + combined_probs.iloc[:, 6]\n\n# # Manual tuning:\n# # Rule 1: If predict 2 but combined confidence < 0.8, kick to 4\n# combined_probs.loc[(combined_probs.Prediction == 2) & (combined_probs.prob_2 < 0.55), \"Manual_Prediction\"] = 4\n# # Rule 2: If predict 3 but combined confidence < 0.8, kick to 4\n# # combined_probs.loc[(combined_probs.Prediction == 3) & (combined_probs.prob_3 < 0.575), \"Manual_Prediction\"] = 4\n\n# # After manual tuningL f1 score\n# print(f1_score(combined_probs[\"Actual\"], combined_probs[\"Manual_Prediction\"], average = \"macro\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vc1_probs = predict_proba(xgbs, test.drop(xgb_drop_columns, axis=1))\nvc2_probs = predict_proba(rfs, test.drop(rf_drop_columns, axis=1))\n\ncombined_probs = pd.concat([pd.DataFrame(vc1_probs), pd.DataFrame(vc2_probs)], axis = 1)\n\nprobs_sum = vc1_probs + vc2_probs\nprediction = np.argmax(probs_sum, axis = 1)\n\ncombined_votes = combine_voters(test, weights = [0.48, 0.52]) + 1\n\ncombined_probs[\"Prediction\"] = combined_votes\n\n# Test\n# We will create another column called Manual Prediction to test our thresholding\ncombined_probs[\"Manual_Prediction\"] = combined_votes\n\ncombined_probs[\"prob_1\"] = combined_probs.iloc[:, 0] + combined_probs.iloc[:, 4]\ncombined_probs[\"prob_2\"] = combined_probs.iloc[:, 1] + combined_probs.iloc[:, 5]\ncombined_probs[\"prob_3\"] = combined_probs.iloc[:, 2] + combined_probs.iloc[:, 6]\ncombined_probs[\"prob_4\"] = combined_probs.iloc[:, 3] + combined_probs.iloc[:, 7]\n\n\ncombined_probs.loc[(combined_probs.Prediction == 1) & (combined_probs.prob_1 < 0.54), \"Manual_Prediction\"] = 2\n# The next line is useless, since for those rows with prediction == 2, none of the rows is smaller than 0.52\n# combined_probs.loc[(combined_probs.Prediction == 2) & (combined_probs.prob_2 < 0.52), \"Manual_Prediction\"] = 3\ncombined_probs.loc[(combined_probs.Prediction == 3) & (combined_probs.prob_3 < 0.54), \"Manual_Prediction\"] = 4\ncombined_probs.loc[(combined_probs.Prediction == 4) & (combined_probs.prob_4 < 0.52), \"Manual_Prediction\"] = 3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# How many heads of households have been reclassified?\ncombined_probs[(test.parentesco1 == 1) & (combined_probs.Prediction != combined_probs.Manual_Prediction)].shape[0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"78749eec7f69bcc8c587278a2c1a43ac8b5832e3"},"cell_type":"markdown","source":"# Prepare submission"},{"metadata":{"_uuid":"32bfd69fe130005cb88865399c460ac00c7b1574","trusted":true},"cell_type":"code","source":"submission = pd.DataFrame()\nsubmission['Id'] = test_ids\n# submission['Target'] = combine_voters(test, weights = [0.48, 0.52]) + 1\nsubmission[\"Target\"] = combined_probs[\"Manual_Prediction\"]\nsubmission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}