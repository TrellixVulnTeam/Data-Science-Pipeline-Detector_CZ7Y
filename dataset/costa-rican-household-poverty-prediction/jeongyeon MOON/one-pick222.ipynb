{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport tensorflow as tf\nimport numpy as np\ntrain =pd.read_csv('../input/train.csv')\ntest =pd.read_csv('../input/test.csv')\ntrain_h = train.loc[train[\"parentesco1\"]==1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"97605a6e6bba67ab737ede2960319e2080d65195"},"cell_type":"code","source":"def drop_feature(df_x1):\n    #결과값 y에 해당하는 값 삭제 \n    # df_x1 = df_x1.drop([\"Target\"],1)\n    df_x1 = df_x1.fillna(0)\n    #세대주성별 교육년수 \n    df_x1['dependency'] = np.sqrt(df_x1['SQBdependency'])\n    df_x1.loc[df_x1['edjefa'] == \"no\", \"edjefa\"] = 0\n    df_x1.loc[df_x1['edjefe'] == \"no\", \"edjefe\"] = 0\n    df_x1.loc[(df_x1['edjefa'] == \"yes\") & (df_x1['parentesco1'] == 1), \"edjefa\"] = df_x1.loc[(df_x1['edjefa'] == \"yes\") & (df_x1['parentesco1'] == 1), \"escolari\"]\n    df_x1.loc[(df_x1['edjefe'] == \"yes\") & (df_x1['parentesco1'] == 1), \"edjefe\"] = df_x1.loc[(df_x1['edjefe'] == \"yes\") & (df_x1['parentesco1'] == 1), \"escolari\"]\n    df_x1.loc[df_x1['edjefa'] == \"yes\", \"edjefa\"] = 4\n    df_x1.loc[df_x1['edjefe'] == \"yes\", \"edjefe\"] = 4\n    df_x1['edjefe'] = df_x1['edjefe'].astype(\"int\")\n    df_x1['edjefa'] = df_x1['edjefa'].astype(\"int\")\n    df_x1['dependency'] = df_x1['dependency'].astype(\"int\")\n    df_x1['edjef'] = np.max(df_x1[['edjefa','edjefe']], axis=1)\n    df_x1['v2a1']=df_x1['v2a1'].fillna(0)\n    df_x1['v18q1']=df_x1['v18q1'].fillna(0)\n    df_x1['rez_esc']=df_x1['rez_esc'].fillna(0)\n    df_x1.loc[df_x1.meaneduc.isnull(), \"meaneduc\"] = 0\n    df_x1.loc[df_x1.SQBmeaned.isnull(), \"SQBmeaned\"] = 0\n    #중복정보 제거\n    df_x1 = df_x1.drop([\"dependency\",\"female\",\"area2\",\"hacdor\",\"hacapo\",\"bedrooms\",\"r4h3\",\"r4m3\"],1) \n    #수학적으로 의미가 없는 값 제거\n    df_x1 = df_x1.drop([\"Id\",\"SQBescolari\", \"SQBage\", \"SQBhogar_total\", \"SQBedjefe\", \"SQBhogar_nin\", \"SQBovercrowding\",\"idhogar\"],1) \n    #세대주와의 관계열 제거\n    df_x1 = df_x1.drop([\"parentesco1\",\"parentesco2\",\"parentesco3\",\"parentesco4\",\"parentesco5\",\"parentesco6\",\"parentesco7\",\"parentesco8\",\n                       \"parentesco9\",\"parentesco10\",\"parentesco11\",\"parentesco12\"],1)\n    #집을 소유하고 있는 사람들에게 1값부여\n    #df_x1[\"house\"] = df_x1.loc[df_x1[\"v2a1\"] == 0, \"v2a1\"]\n    #df_x1[\"house\"] = df_x1[\"house\"].fillna(1)\n    # 집세 임시 제거\n    #df_x1['lent'] = (df_x1[\"v2a1\"])\n    df_x1 = df_x1.drop([\"v2a1\"],1)\n    df_x1['lent'] = df_x1['tamviv']-df_x1['tamhog']\n    df_x1 = df_x1.drop([\"r4t1\",\"r4t2\",\"r4t3\",\"tamhog\",\"tamviv\"],1) \n    # 중복 데이터 : 태블릿 수 삭제\n    df_x1 = df_x1.drop([\"v18q1\"],1) \n    #벽,지붕, 바닥 좋고 나쁨 정도 중복 \n    df_x1 = df_x1.drop([\"epared1\",\"epared2\",\"epared3\",\"etecho1\",\"etecho2\",\"etecho3\"],1) \n    #모든 열이 0인 경우 삭제 \n    df_x1 = df_x1.drop([\"elimbasu5\", \"estadocivil1\"],1) \n    #상관관계가 높은 사항들 제거\n    df_x1 = df_x1.drop([\"pisocemento\",\"overcrowding\",\"hhsize\"],1)\n    # 개인별 삭제 (실험 해 보아라~~~~): 정배픽\n    #df_x1 = df_x1.drop([\"age\",\"lugar1\",\"lugar6\",\"sanitario2\",\"coopele\"],1)\n    #정연픽\n    # df_x1 = df_x1.drop([\"hogar_nin\",\"hogar_adul\",\"hogar_mayor\",\"hogar_total\",\"paredblolad\",\"meaneduc\",\"qmobilephone\"],1)\n    df_x1 = df_x1.drop([\"SQBdependency\",\"SQBmeaned\",\"agesq\"],1)\n    #진수픽\n    \n    return df_x1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8487bad5c447cc15981dc3ad443acd29f26e840b"},"cell_type":"code","source":"train_h_drop = drop_feature(train_h)\n#rain_h_drop.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9ee09afe5f5aeef17d60a1b152b1def044ffe361"},"cell_type":"code","source":"def xavier_init(n_inputs, n_outputs, uniform=True):\n    if uniform:\n        # 6 was used in the paper.\n        init_range = tf.sqrt(6.0 / (n_inputs + n_outputs))\n        return tf.random_uniform_initializer(-init_range, init_range)\n    else:\n        # 3 gives us approximately the same limints as above since this repicks\n        # values greater than 2 standard deviations from the mean.\n        stddev = tf.sqrt(3.0 / (n_inputs + n_outputs))\n        return tf.truncated_normal_initializer(stddev=stddev)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4f1150642c757b97cdc02fa9717bd7aaf00ac759"},"cell_type":"code","source":"df_x1 = train_h_drop.drop([\"Target\"],1)\ndf_y = train_h['Target']-1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5dc3bf91435db13cabbf289e38d061b8005155d7"},"cell_type":"code","source":"tf.reset_default_graph()\n# Review : Learning rate and Evaluation\nimport tensorflow as tf\nimport random\nimport matplotlib.pyplot as plt\n\nplaceholder_num = len(df_x1.columns)\n\n\nx_data = df_x1\nsess = tf.Session()\ny_data = tf.one_hot(df_y, depth = 4).eval(session=sess)\ny_data = tf.reshape(y_data, shape=[-1,4]).eval(session=sess)\nprint(y_data)\ntf.set_random_seed(999)  # reproducibility\n\n\n\n# parameters\nlearning_rate = 0.001\ni =64\n\n\nX = tf.placeholder(tf.float32, [None, placeholder_num])\nY = tf.placeholder(tf.float32, [None, 4])\nkeep_prob = tf.placeholder(tf.float32)\n\n\nW1 = tf.get_variable(\"W1\", shape=[placeholder_num, i], initializer=tf.contrib.layers.xavier_initializer())\nb1 = tf.Variable(tf.random_normal([i]))\nL1 = tf.nn.relu(tf.matmul(X, W1) + b1)\nL1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n\n\nW2 = tf.get_variable(\"W2\", shape=[i, i], initializer=tf.contrib.layers.xavier_initializer())\nb2 = tf.Variable(tf.random_normal([i]))\nL2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\nL2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n\nW3 = tf.get_variable(\"W3\", shape=[i, i], initializer=tf.contrib.layers.xavier_initializer())\nb3 = tf.Variable(tf.random_normal([i]))\nL3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\nL3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n\n\nW4 = tf.get_variable(\"W4\", shape=[i, i], initializer=tf.contrib.layers.xavier_initializer())\nb4 = tf.Variable(tf.random_normal([i]))\nL4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\nL4 = tf.nn.dropout(L4, keep_prob=keep_prob)\n\nW5 = tf.get_variable(\"W5\", shape=[i, i], initializer=tf.contrib.layers.xavier_initializer())\nb5 = tf.Variable(tf.random_normal([i]))\nL5 = tf.nn.relu(tf.matmul(L4, W5) + b5)\nL5 = tf.nn.dropout(L5, keep_prob=keep_prob)\n\nW6 = tf.get_variable(\"W6\", shape=[i, i], initializer=tf.contrib.layers.xavier_initializer())\nb6 = tf.Variable(tf.random_normal([i]))\nL6 = tf.nn.relu(tf.matmul(L5, W6) + b6)\nL6 = tf.nn.dropout(L6, keep_prob=keep_prob)\n\nW7 = tf.get_variable(\"W7\", shape=[i, i], initializer=tf.contrib.layers.xavier_initializer())\nb7 = tf.Variable(tf.random_normal([i]))\nL7 = tf.nn.relu(tf.matmul(L6, W7) + b7)\nL7 = tf.nn.dropout(L7, keep_prob=keep_prob)\n\nW8 = tf.get_variable(\"W8\", shape=[i, i], initializer=tf.contrib.layers.xavier_initializer())\nb8 = tf.Variable(tf.random_normal([i]))\nL8 = tf.nn.relu(tf.matmul(L7, W8) + b8)\nL8 = tf.nn.dropout(L8, keep_prob=keep_prob)\n\nW9 = tf.get_variable(\"W9\", shape=[i, 4], initializer=tf.contrib.layers.xavier_initializer())\nb9 = tf.Variable(tf.random_normal([4]))\nL9 = tf.nn.relu(tf.matmul(L8, W9) + b9)\n\n# W10 = tf.get_variable(\"W10\", shape=[i, i], initializer=tf.contrib.layers.xavier_initializer())\n# b10 = tf.Variable(tf.random_normal([i]))\n# L10 = tf.nn.relu(tf.matmul(L9, W10) + b10)\n# L10 = tf.nn.dropout(L10, keep_prob=keep_prob)\n\n\n# W11 = tf.get_variable(\"W11\", shape=[i, i], initializer=tf.contrib.layers.xavier_initializer())\n# b11 = tf.Variable(tf.random_normal([i]))\n# L11 = tf.nn.relu(tf.matmul(L10, W11) + b11)\n# L11 = tf.nn.dropout(L11, keep_prob=keep_prob)\n\n# W12 = tf.get_variable(\"W12\", shape=[i, i], initializer=tf.contrib.layers.xavier_initializer())\n# b12 = tf.Variable(tf.random_normal([i]))\n# L12 = tf.nn.relu(tf.matmul(L11, W12) + b12)\n# L12 = tf.nn.dropout(L12, keep_prob=keep_prob)\n\n# W13 = tf.get_variable(\"W13\", shape=[i, 4], initializer=tf.contrib.layers.xavier_initializer())\n# b13 = tf.Variable(tf.random_normal([4]))\n# L13 = tf.nn.relu(tf.matmul(L12, W13) + b13)\n\n# W11 = tf.get_variable(\"W11\", shape=[i, 4], initializer=tf.contrib.layers.xavier_initializer())\n# b11 = tf.Variable(tf.random_normal([4]))\n# L11 = tf.nn.relu(tf.matmul(L10, W11) + b11)\n\nhypothesis = tf.matmul(L8, W9) + b9\n\n\n# define cost/loss & optimizer\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\ncorrect_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true,"_uuid":"11aba1cfe0eb4311cf1c4877271e9ff1667fe2ec"},"cell_type":"code","source":"# Launch the graph in a session.\nsess = tf.Session()\n\n# Initializes global variables in the graph.\nsess.run(tf.global_variables_initializer())\n\nfor step in range(5001):\n    sess.run(optimizer, feed_dict={X: x_data, Y: y_data, keep_prob: 0.7})\n    if step % 1000 == 0 or step < 100:\n        loss, acc = sess.run([cost, accuracy], feed_dict={\n                             X: x_data, Y: y_data, keep_prob: 0.7})\n        print(\"Step: {:5}, \\t Loss: {:.3f}, \\t Acc: {:.2%}\".format(\n            step, loss, acc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b4a8f1fc7ed00eac68b579fa38f6f36f435bfb6e"},"cell_type":"code","source":"df2=drop_feature(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"28777d59852d544df00a9b93130badd8f1f66838"},"cell_type":"code","source":"df2 =  df2.values.tolist()\ntest_data = df2\n\n\npred_val = sess.run(hypothesis, feed_dict={X: test_data, keep_prob: 1.0})\npred_idx = sess.run(tf.argmax(pred_val, 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d9dfe1693d8d30954585b23d1bb1dd454cbc5bf5"},"cell_type":"code","source":"pred_idx = pred_idx +1\nsubmission = pd.DataFrame({'Id' : test.Id, 'Target' : pred_idx})\nsubmission.head()\nsubmission.to_csv(\"submissions.csv\", index =False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2c368e7c2a72f7e6bf31feda03e519147b196fb0"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}