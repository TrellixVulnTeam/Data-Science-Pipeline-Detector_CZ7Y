{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt #for plotting\nimport seaborn as sns #for visualization\nfrom plotly.offline import init_notebook_mode, iplot\n\nimport plotly.graph_objs as go\nfrom plotly import tools\nimport plotly.figure_factory as ff\nfrom collections import OrderedDict #to make sorted Dictionary\n\n# Set a few plotting defaults\n%matplotlib inline\nplt.style.use('dark_background')\nplt.rcParams['font.size'] = 15\nplt.rcParams['patch.edgecolor'] = 'k'\n\n\n\n\n# Suppress warnings from pandas\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/costa-rican-household-poverty-prediction/train.csv')\ntest = pd.read_csv('../input/costa-rican-household-poverty-prediction/test.csv')\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['Target']=np.nan","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA and Data Cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.select_dtypes(['object']).head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will use ID idhogar later. since idhogar is unique identification of each household. It is not need to be considered as problem feature"},{"metadata":{},"cell_type":"markdown","source":"Dependency: Number of Household younger than 19 or older than 64.,head of household, yes=1 and no=0\nedjefe: years of education of male head of household,based on escolari, yes=1,no=0\nedjefa: years of education of female head of household,based on interaction of escolari, yes=1, no=0\nHence , we can replace yes and no with 1 and 0 respectively"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Converting Object DataType to Integer\ntrain['dependency']=train['dependency'].replace(('yes','no'),(1,0)).astype(np.float64)\ntrain['edjefe']=train['edjefe'].replace(('yes','no'),(1,0)).astype(np.float64)\ntrain['edjefa']=train['edjefa'].replace(('yes','no'),(1,0)).astype(np.float64)\ntest['dependency']=test['dependency'].replace(('yes','no'),(1,0)).astype(np.float64)\ntest['edjefe']=test['edjefe'].replace(('yes','no'),(1,0)).astype(np.float64)\ntest['edjefa']=test['edjefa'].replace(('yes','no'),(1,0)).astype(np.float64)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[['dependency','edjefe','edjefa']].describe()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's look at integer data types.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.select_dtypes(np.int64).nunique().value_counts().sort_index().plot.bar(color='red',edgecolor='k',linewidth=2)\nplt.xlabel('Number of Unique Values')\nplt.ylabel('Count')\nplt.title('Number of Unique Values in Integer Columns')\nprint(train.select_dtypes(np.int64).nunique().value_counts())\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Maximum number of interger columns has 2 unique values which is boolean values . This boolean expression is on household level. Since, we need final prediction on household level, we will not need these boolean columns to aggregate. However, boolean expressions on individual level is needed to be aggregated."},{"metadata":{},"cell_type":"markdown","source":"# Exploring Label Distribution"},{"metadata":{},"cell_type":"markdown","source":"There are four possible integer levels describing about Poverty levels. 1: extreme poverty,2:moderate poverty,3:vulnerable poverty,4: non-vulnerable poverty. We need to optimally assign poverty level to household than individuals.i.e. ['parentsco1']==1 has to be considered as household head and indicate poverty data of ['idhogar'] rather that different poverty data for each individual in same houshold."},{"metadata":{},"cell_type":"markdown","source":"Let's see distribution of training labels"},{"metadata":{},"cell_type":"markdown","source":"To make optimal analysis of this dataset, let's first combine train and test dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"data=pd.concat([train,test],axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"head=data.loc[data['parentesco1']==1].copy()\n\n#Labels\nlabels=data.loc[(data['Target'].notnull()) & (data['parentesco1']==1),['Target','idhogar']]\n\n#Value counts of Target\nlabel_counts=labels['Target'].value_counts().sort_index()\n\n# color mapping\ncolors=OrderedDict({1:'red',2:'orange',3:'blue',4:'green'})\npoverty_mapping=OrderedDict({1:'extreme',2:'moderate',3:'vulnerable',4:'non-vulnerable'})\n\n#visualization of labels distribution\nlabel_counts.plot.bar(figsize=(10,8),color=colors.values(),edgecolor='k',linewidth=3)\n\n#Formatting x-axis and y-axis\nplt.xlabel('Poverty Level')\nplt.ylabel('Count of housholds')\nplt.xticks([x-1 for x in poverty_mapping.keys()],list(poverty_mapping.values()),rotation=60)\nplt.title('Poverty Level Breakdown')\nprint(label_counts)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The problem with this label distribution is imbalance class. The number of households considered non-vulnerable is more than number of household in other poverty indicators. \nThis imbalanced distribution may later led to classification problem while designing machine learning model. This may lead to inaccurate predictions. We have to address this issue."},{"metadata":{},"cell_type":"markdown","source":"# Wrong Labels"},{"metadata":{},"cell_type":"markdown","source":"The other major issue which is needed to be addressed is to see that poverty targets are assigned on basis of households and not basis of individuals. If individuals within same [idhogar] is assigned different [Target] labels, we have to correct it, as according to requirement of this question, poverty of household is defined according to poverty label of household head i.e. [parentesco1].\n\nLet's work on this and find equal and non-equal individual poverty labels of individuals in same households"},{"metadata":{"trusted":true},"cell_type":"code","source":"#group by household id and see target label of each individual in household\n#categorise into equal labels and non-equal labels\nequal_labels=train.groupby('idhogar')['Target'].apply(lambda x:x.nunique()==1)\n\nnon_equal_labels=equal_labels[equal_labels ==False]\nprint('There are {} number of houselholds who have different target labels'.format(len(non_equal_labels)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Example\ntrain[train['idhogar'] == non_equal_labels.index[0]][['idhogar', 'parentesco1', 'Target']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The summary of this problem demanded that we should define household income target according to head of household target variable i.e. [parentesco1]. In above example head of household has poverty target as 3 while 2 members of same household has different target. We have to correct it by replacing target variables of individual with target variable of head.\n"},{"metadata":{},"cell_type":"markdown","source":"# Head of Household"},{"metadata":{},"cell_type":"markdown","source":"Another major discrepancy which is in this dataset is that many families does not have assigned head of households. Let's find out how many such households exists."},{"metadata":{"trusted":true},"cell_type":"code","source":"head_household=train.groupby('idhogar')['parentesco1'].sum()\nno_head=train.loc[train['idhogar'].isin(head_household[head_household==0].index),:]\nprint('No. of households with no head are {}'.format(no_head['idhogar'].nunique()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let's analyze labels of households with no heads"},{"metadata":{"trusted":true},"cell_type":"code","source":"no_head_label=no_head.groupby('idhogar')['Target'].apply(lambda x:x.nunique()==1)\nprint('{} Households with no head have different labels.'.format(sum(no_head_label == False)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thus there households with no heads assigned, have same Target values"},{"metadata":{},"cell_type":"markdown","source":"Now, equalizing Target variables for all members of household with different Target variables as that of Head of household."},{"metadata":{"trusted":true},"cell_type":"code","source":"for household in non_equal_labels.index:\n    #find correct label\n    true_target=int(train[(train['idhogar']==household) &(train['parentesco1']==1)]['Target'])\n    \n    #set correct label for all member of household\n    train.loc[train['idhogar']==household,'Target']=true_target\n    \n#Check if changes has been implemented\nall_equal=train.groupby('idhogar')['Target'].apply(lambda x: x.nunique()==1)\n\nnot_equal=all_equal[all_equal==False]\n\nprint('No of household with different Target variables are {}'.format(len(not_equal)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, all households has similar Target poverty levels."},{"metadata":{},"cell_type":"markdown","source":"# Working on Missing Values"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"train.isnull().sum().sort_values(0,ascending=False).to_frame().head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 5 major Features which has missing values. Let's explore them"},{"metadata":{},"cell_type":"markdown","source":"rez_esc: Years behind in school"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data.loc[data['rez_esc'].notnull()]['age'].describe())\nprint(train.loc[train['rez_esc'].notnull()]['age'].describe())\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Description of non null data shows that school going children are between 7 and 19 years of age.\nLet's explore null data"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data.loc[data['rez_esc'].isnull()]['age'].describe())\nprint(train.loc[train['rez_esc'].isnull()]['age'].describe())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Null data shows that minimum age of null data is 0 and maximum age is 97. According to description of problem,anyone below age of 0 and above age of 17 is non school going person. Let's replace this data with 0."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.loc[((data['age']<7 )| (data['age']>17))&(data['rez_esc'].isnull()),'rez_esc']=0\ntrain.loc[((train['age']<7 )| (train['age']>17))&(train['rez_esc'].isnull()),'rez_esc']=0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['missing_value']=data['rez_esc'].isnull()\ntrain['missing_value']=train['rez_esc'].isnull()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#explore any data anamoly\ndata['rez_esc'].describe()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now there is an issue here. Maximum value of rez_esc according to problem description has to be 5. However, this dataset has maximum value of 99. Let's replace all values above 5 to be 5"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.loc[data['rez_esc']>5,'rez_esc']=5\ntrain.loc[train['rez_esc']>5,'rez_esc']=5\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## v18q1\n\nv18q1 descibes number of tablets in houehold"},{"metadata":{},"cell_type":"markdown","source":"nan value in tablet column can be assumed as that househlod does not have a tablet an thus replace it by 0.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['v18q1']=data['v18q1'].fillna(0)\ntrain['v18q1']=train['v18q1'].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## v2a1\n\nv2a1: Monthly rent payment"},{"metadata":{},"cell_type":"markdown","source":"In addition to looking at the missing values of the monthly rent payment, it will be interesting to also look at the distribution of tipovivi_, the columns showing the ownership/renting status of the home. For this plot, we show the ownership status of those homes with a nan for the monthyl rent payment."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Variables indicating home ownership\nown_variables = [x for x in data if x.startswith('tipo')]\n\n\n# Plot of the home ownership variables for home missing rent payments\ndata.loc[data['v2a1'].isnull(), own_variables].sum().plot.bar(figsize = (10, 8),\n                                                                        color = 'green',\n                                                              edgecolor = 'k', linewidth = 2);\nplt.xticks([0, 1, 2, 3, 4],\n           ['Owns and Paid Off', 'Owns and Paying', 'Rented', 'Precarious', 'Other'],\n          rotation = 60)\nplt.title('Home Ownership Status for Households Missing Rent Payments', size = 18);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fill in households that own the house with 0 rent payment\ndata.loc[(data['tipovivi1'] == 1), 'v2a1'] = 0\n\n# Create missing rent payment column\ndata['v2a1-missing'] = data['v2a1'].isnull()\n\ndata['v2a1-missing'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The meaning of the home ownership variables is below:\n\ntipovivi1, =1 own and fully paid house\ntipovivi2, \"=1 own,  paying in installments\"\ntipovivi3, =1 rented\ntipovivi4, =1 precarious\ntipovivi5, \"=1 other(assigned,  borrowed)\"\n"},{"metadata":{},"cell_type":"markdown","source":"We've solved the issue! Well, mostly: the households that do not have a monthly rent payment generally own their own home. In a few other situations, we are not sure of the reason for the missing information.\n\nFor the houses that are owned and have a missing monthly rent payment, we can set the value of the rent payment to zero. For the other homes, we can leave the missing values to be imputed but we'll add a flag (Boolean) column indicating that these households had missing values."},{"metadata":{},"cell_type":"markdown","source":"Number of households in train dataset with null values is equal to owning a house"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['v2a1'].isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sum_tipo=train['tipovivi1'].sum()+train['tipovivi2'].sum()+train['tipovivi4'].sum()+train['tipovivi5'].sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rent_miss=train.loc[(train['tipovivi2']==1)&(train['v2a1'].notna()),:].shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sum_tipo-rent_miss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rent_miss","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the data it can be derived that missing value in 'v2a1' is due to people owning house,or for other precarious reason not known . Out of households who own a household on installments, 961 households also hold a house on rent"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.loc[(train['tipovivi1']==1)&(train['v2a1'].isna()),'v2a1']=0\ndata.loc[(data['tipovivi1'] == 1) & (data['v2a1'].isna()), 'v2a1'] = 0\ntrain.loc[(train['tipovivi2']==1)&(train['v2a1'].isna()),'v2a1']=0\ndata.loc[(data['tipovivi2']==1)&(data['v2a1'].isna()),'v2a1']=0\ntrain.loc[(train['tipovivi4']==1)&(train['v2a1'].isna()),'v2a1']=0\ndata.loc[(data['tipovivi4']==1)&(data['v2a1'].isna()),'v2a1']=0\ntrain.loc[(train['tipovivi5']==1)&(train['v2a1'].isna()),'v2a1']=0\ndata.loc[(data['tipovivi5']==1)&(data['v2a1'].isna()),'v2a1']=0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['v2a1'].isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Finding Outliers"},{"metadata":{},"cell_type":"markdown","source":"Inter-quartile range(IQR) is used to identify outliers in the dataset. IQR is the difference between the 75th and 25th percentile of the data. It is measure of dispersion along the lines of standard deviation. During this analysis, features were found which had incosistent values like integer and boolean string (yes/no) mixed together. These needs to be removed and is the main focus of the next step."},{"metadata":{"trusted":true},"cell_type":"code","source":"for cols in data.columns[1:]:\n    if cols in ['idhogar', 'dependency', 'edjefe', 'edjefa']:\n        continue\n    percentile75 = np.percentile(data[cols].fillna(0), 75)\n    percentile25 = np.percentile(data[cols].fillna(0), 25)\n    threshold = (percentile75 - percentile25) * 1.5\n    lower, upper = (percentile25 - threshold), (percentile75 + threshold)\n    # identify outliers\n    outliers = data.loc[(data[cols] < lower) & (data[cols] > upper)]\n    if len(outliers) > 0:\n        print('Feature: {}. Identified outliers: {}'.format(cols, len(outliers)))\nfor cols in train.columns[1:]:\n    if cols in ['idhogar', 'dependency', 'edjefe', 'edjefa']:\n        continue\n    percentile75 = np.percentile(train[cols].fillna(0), 75)\n    percentile25 = np.percentile(train[cols].fillna(0), 25)\n    threshold = (percentile75 - percentile25) * 1.5\n    lower, upper = (percentile25 - threshold), (percentile75 + threshold)\n    # identify outliers\n    outliers = train.loc[(train[cols] < lower) & (train[cols] > upper)]\n    if len(outliers) > 0:\n        print('Feature: {}. Identified outliers: {}'.format(cols, len(outliers)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering"},{"metadata":{},"cell_type":"markdown","source":"Creating Ordinal Variable\nI'm going to compress these four variables into one by creating an ordinal variable. I'm going to choose the mapping myself, based on the data decriptions:\n\n0: No electricity\n1: Electricity from cooperative\n2: Electricity from CNFL, ICA, ESPH/JASEC\n3: Electricity from private plant\n\nAn ordered variable has an inherent ordering, and for this we choose our own based on the domain knowledge. After we create this new ordered variable, we can drop the four others. There are several households that do not have a variable here, so we will use a nan (which will be filled in during imputation) and add a Boolean column indicating there was no measure for this variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"elec = []\n\n# Assign values\nfor i, row in head.iterrows():\n    if row['noelec'] == 1:\n        elec.append(0)\n    elif row['coopele'] == 1:\n        elec.append(1)\n    elif row['public'] == 1:\n        elec.append(2)\n    elif row['planpri'] == 1:\n        elec.append(3)\n    else:\n        elec.append(np.nan)\n        \n# Record the new variable and missing flag\nhead['elec'] = elec\nhead['elec-missing'] = head['elec'].isnull()\n\n# Remove the electricity columns\nhead = head.drop(columns = ['noelec', 'coopele', 'public', 'planpri'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The final redundant column is area2. This means the house is in a rural zone, but it's redundant because we have a column indicating if the house is in a urban zone. Therefore, we can drop this column.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"heads = head.drop(columns = 'area2')\n\nheads.groupby('area1')['Target'].value_counts(normalize = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creating Ordinal Variables\nFor the walls, roof, and floor of the house, there are three columns each: the first indicating 'bad', the second 'regular', and the third 'good'. We could leave the variables as booleans, but to me it makes more sense to turn them into ordinal variables because there is an inherent order: bad < regular < good. To do this, we can simply find whichever column is non-zero for each household using np.argmax.\n\nOnce we have created the ordinal variable, we are able to drop the original variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Wall ordinal variable\nheads['walls'] = np.argmax(np.array(heads[['epared1', 'epared2', 'epared3']]),\n                           axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Construction"},{"metadata":{},"cell_type":"markdown","source":"In addition to mapping variables to ordinal features, we can also create entirely new features from the existing data, known as feature construction. For example, we can add up the previous three features we just created to get an overall measure of the quality of the house's structure."},{"metadata":{},"cell_type":"markdown","source":"The next variable will be a warning about the quality of the house. It will be a negative value, with -1 point each for no toilet, electricity, floor, water service, and ceiling."},{"metadata":{"trusted":true},"cell_type":"code","source":"# No toilet, no electricity, no floor, no water service, no ceiling\nheads['warning'] = 1 * (heads['sanitario1'] + \n                         (heads['elec'] == 0) + \n                         heads['pisonotiene'] + \n                         heads['abastaguano'] + \n                         (heads['cielorazo'] == 0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10, 6))\nsns.violinplot(x = 'warning', y = 'Target', data = heads);\nplt.title('Target vs Warning Variable');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The violinplot is not great here because it smooths out the categorical variable with the effect that it looks as if the Target can take on lesser and greater values than in reality. Nonetheless, we can see a high concentration of households that have no warning signs and have the lowest level of poverty. It looks as if this may be a useful feature, but we can't know for sure until we get to modeling!"},{"metadata":{},"cell_type":"markdown","source":"The final household feature we can make for now is a bonus where a family gets a point for having a refrigerator, computer, tablet, or television."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Owns a refrigerator, computer, tablet, and television\nheads['bonus'] = 1 * (heads['refrig'] + \n                      heads['computer'] + \n                      (heads['v18q1'] > 0) + \n                      heads['television'])\n\nsns.violinplot('bonus', 'Target', data = heads,\n                figsize = (10, 6));\nplt.title('Target vs Bonus Variable');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Correlation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use only training data\ntrain_heads = heads.loc[heads['Target'].notnull(), :].copy()\n\npcorrs = pd.DataFrame(train_heads.corr()['Target'].sort_values()).rename(columns = {'Target': 'pcorr'}).reset_index()\npcorrs = pcorrs.rename(columns = {'index': 'feature'})\n\nprint('Most negatively correlated variables:')\nprint(pcorrs.head())\n\nprint('\\nMost positively correlated variables:')\nprint(pcorrs.dropna().tail())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For the negative correlations, as we increase the variable, the Target decreases indicating the poverty severity increases. Therefore, as the warning increases, the poverty level also increases which makes sense because this was meant to show potential bad signs about a house. The hogar_nin is the number of children 0 - 19 in the family which also makes sense: younger children can be financial source of stress on a family leading to higher levels of poverty. Or, families with lower socioeconomic status have more children in the hopes that one of them will be able to succeed. Whatever the explanation, there is a real link between family size and poverty\n\nOn the other hand, for the positive correlations, a higher value means a higher value of Target indicating the poverty severity decreases. The most highly correlated household level variable is meaneduc, the average education level of the adults in the household. This relationship between education and poverty intuitively makes sense: greater levels of education generally correlate with lower levels of poverty. We don't necessarily know which causes which, but we do know these tend to move in the same direction.\n\nThe general guidelines for correlation values are below, but these will change depending on who you ask (source for these):\n\n.00-.19 “very weak”\n.20-.39 “weak”\n.40-.59 “moderate”\n.60-.79 “strong”\n.80-1.0 “very strong”\nWhat these correlations show is that there are some weak relationships that hopefully our model will be able to use to learn a mapping from the features to the Target."},{"metadata":{"trusted":true},"cell_type":"code","source":"variables = ['Target', 'dependency', 'warning', 'meaneduc'\n             , 'r4m1', 'overcrowding']\n\n# Calculate the correlations\ncorr_mat = train_heads[variables].corr().round(2)\n\n# Draw a correlation heatmap\nplt.rcParams['font.size'] = 18\nplt.figure(figsize = (12, 12))\nsns.heatmap(corr_mat, vmin = -0.5, vmax = 0.8, center = 0, \n            cmap = plt.cm.RdYlGn_r, annot = True);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploring Individual Variables"},{"metadata":{},"cell_type":"markdown","source":"There are two types of individual level variables: Boolean (1 or 0 for True or False) and ordinal (discrete values with a meaningful ordering)."},{"metadata":{"trusted":true},"cell_type":"code","source":"id_ = ['Id', 'idhogar', 'Target']\nind_bool = ['v18q', 'dis', 'male', 'estadocivil1', 'estadocivil2', 'estadocivil3', \n            'estadocivil4', 'estadocivil5', 'estadocivil6', 'estadocivil7', \n            'parentesco1', 'parentesco2',  'parentesco3', 'parentesco4', 'parentesco5', \n            'parentesco6', 'parentesco7', 'parentesco8',  'parentesco9', 'parentesco10', \n            'parentesco11', 'parentesco12', 'instlevel1', 'instlevel2', 'instlevel3', \n            'instlevel4', 'instlevel5', 'instlevel6', 'instlevel7', 'instlevel8', \n            'instlevel9', 'mobilephone']\n\nind_ordered = ['rez_esc', 'escolari', 'age']\n\n\nind = data[id_ + ind_bool + ind_ordered]\nind.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create correlation matrix\ncorr_matrix = ind.corr()\n\n# Select upper triangle of correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n\n# Find index of feature columns with correlation greater than 0.95\nto_drop = [column for column in upper.columns if any(abs(upper[column]) > 0.95)]\n\nto_drop","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ind = ind.drop(columns = 'male')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating Ordinal Variables"},{"metadata":{},"cell_type":"markdown","source":"Much as we did with the household level data, we can map existing columns to an ordinal variable. Here we will focus on the instlevel_ variables which indicate the amount of education an individual has from instlevel1: no level of education to instlevel9: postgraduate education.\n\nTo create the ordinal variable, for each individual, we will simply find which column is non-zero. The education has an inherent ordering (higher is better) so this conversion to an ordinal variable makes sense in the problem context."},{"metadata":{"trusted":true},"cell_type":"code","source":"ind[[c for c in ind if c.startswith('instl')]].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ind['inst'] = np.argmax(np.array(ind[[c for c in ind if c.startswith('instl')]]), axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10, 8))\nsns.violinplot(x = 'Target', y = 'inst', data = ind);\nplt.title('Education Distribution by Target');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Higher levels of education seem to correspond to less extreme levels of poverty. We do need to keep in mind this is on an individual level though and we eventually will have to aggregate this data at the household level."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop the education columns\nind = ind.drop(columns = [c for c in ind if c.startswith('instlevel')])\nind.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ind['escolari/age'] = ind['escolari'] / ind['age']\n\nplt.figure(figsize = (10, 8))\nsns.violinplot('Target', 'escolari/age', data = ind);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can also take our new variable, inst, and divide this by the age. The final variable we'll name tech: this represents the combination of tablet and mobile phones."},{"metadata":{"trusted":true},"cell_type":"code","source":"ind['inst/age'] = ind['inst'] / ind['age']\nind['tech'] = ind['v18q'] + ind['mobilephone']\nind['tech'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Engineering through Aggregations"},{"metadata":{},"cell_type":"markdown","source":"In order to incorporate the individual data into the household data, we need to aggregate it for each household. The simplest way to do this is to groupby the family id idhogar and then agg the data. For the aggregations for ordered or continuous variables, we can use six, five of which are built in to pandas, and one of which we define ourselves range_. The boolean aggregations can be the same, but this will create many redundant columns which we will then need to drop. For this case, we'll use the same aggregations and then go back and drop the redundant columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define custom function\nrange_ = lambda x: x.max() - x.min()\nrange_.__name__ = 'range_'\n\n# Group and aggregate\nind_agg = ind.drop(columns = 'Target').groupby('idhogar').agg(['min', 'max', 'sum', 'count', 'std', range_])\nind_agg.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rename the columns\nnew_col = []\nfor c in ind_agg.columns.levels[0]:\n    for stat in ind_agg.columns.levels[1]:\n        new_col.append(f'{c}-{stat}')\n        \nind_agg.columns = new_col\nind_agg.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ind_agg = ind_agg.drop(columns = to_drop)\nind_feats = list(ind_agg.columns)\n\n# Merge on the household id\nfinal = heads.merge(ind_agg, on = 'idhogar', how = 'left')\n\nprint('Final features shape: ', final.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corrs = final.corr()['Target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corrs.sort_values().head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corrs.sort_values().dropna().tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10, 6))\nsns.violinplot(x = 'Target', y = 'escolari-max', data = final);\nplt.title('Max Schooling by Target');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10, 6))\nsns.boxplot(x = 'Target', y = 'escolari-max', data = final);\nplt.title('Max Schooling by Target');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10, 6))\nsns.boxplot(x = 'Target', y = 'meaneduc', data = final);\nplt.xticks([0, 1, 2, 3], poverty_mapping.values())\nplt.title('Average Schooling by Target');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10, 6))\nsns.boxplot(x = 'Target', y = 'overcrowding', data = final);\nplt.xticks([0, 1, 2, 3], poverty_mapping.values())\nplt.title('Overcrowding by Target');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Machine Learning Model"},{"metadata":{},"cell_type":"markdown","source":"Once feature engineering/construction is done, we can get started with the machine learning! All of our data (both training and testing) is aggregated for each household and so can be directly used in a model. To first show the process of modeling, we'll use the capable Random Forest Classifier in Scikit-Learn. This probably won't get us to the top of the leaderboard, but it will allow us to establish a baseline. Later we'll try several other models including the powerful Gradient Boosting Machine.\n\nTo assess our model, we'll use 10-fold cross validation on the training data. This will essentially train and test the model 10 times using different splits of the training data. 10-fold cross validation is an effective method for estimating the performance of a model on the test set. We want to look at the average performance in cross validation as well as the standard deviation to see how much scores change between the folds. We use the F1 Macro measure to evaluate performance."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score, make_scorer\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.pipeline import Pipeline\n\n# Custom scorer for cross validation\nscorer = make_scorer(f1_score, greater_is_better=True, average = 'macro')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Labels for training\ntrain_labels = np.array(list(final[final['Target'].notnull()]['Target'].astype(np.uint8)))\n\n# Extract the training data\ntrain_set = final[final['Target'].notnull()].drop(columns = ['Id', 'idhogar', 'Target'])\ntest_set = final[final['Target'].isnull()].drop(columns = ['Id', 'idhogar', 'Target'])\n\n# Submission base which is used for making submissions to the competition\nsubmission_base = test[['Id', 'idhogar']].copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Because we are going to be comparing different models, we want to scale the features (limit the range of each column to between 0 and 1). For many ensemble models this is not necessary, but when we use models that depend on a distance metric, such as KNearest Neighbors or the Support Vector Machine, feature scaling is an absolute necessity. When comparing different models, it's always safest to scale the features. We also impute the missing values with the median of the feature.\n\nFor imputing missing values and scaling the features in one step, we can make a pipeline. This will be fit on the training data and used to transform the training and testing data."},{"metadata":{"trusted":true},"cell_type":"code","source":"features = list(train_set.columns)\n\npipeline = Pipeline([('imputer', Imputer(strategy = 'median')), \n                      ('scaler', MinMaxScaler())])\n\n# Fit and transform training data\ntrain_set = pipeline.fit_transform(train_set)\ntest_set = pipeline.transform(test_set)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = RandomForestClassifier(n_estimators=100, random_state=10, \n                               n_jobs = -1)\n# 10 fold cross validation\ncv_score = cross_val_score(model, train_set, train_labels, cv = 10, scoring = scorer)\n\nprint(f'10 Fold Cross Validation F1 Score = {round(cv_score.mean(), 4)} with std = {round(cv_score.std(), 4)}')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}