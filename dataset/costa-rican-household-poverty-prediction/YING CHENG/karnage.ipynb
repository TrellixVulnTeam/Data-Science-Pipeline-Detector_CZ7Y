{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split as tts, cross_val_score as cv, RepeatedStratifiedKFold as rsk\nfrom sklearn.ensemble import RandomForestClassifier as rf, ExtraTreesClassifier as et, BaggingClassifier as bc\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score\nfrom sklearn.utils import class_weight\nimport lightgbm as lgb","metadata":{"execution":{"iopub.status.busy":"2021-11-18T06:55:34.892014Z","iopub.execute_input":"2021-11-18T06:55:34.892338Z","iopub.status.idle":"2021-11-18T06:55:37.20527Z","shell.execute_reply.started":"2021-11-18T06:55:34.892304Z","shell.execute_reply":"2021-11-18T06:55:37.20422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd ","metadata":{"execution":{"iopub.status.busy":"2021-11-18T06:55:37.206971Z","iopub.execute_input":"2021-11-18T06:55:37.207194Z","iopub.status.idle":"2021-11-18T06:55:37.212849Z","shell.execute_reply.started":"2021-11-18T06:55:37.207167Z","shell.execute_reply":"2021-11-18T06:55:37.211975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%ls","metadata":{"execution":{"iopub.status.busy":"2021-11-18T06:55:37.214298Z","iopub.execute_input":"2021-11-18T06:55:37.214626Z","iopub.status.idle":"2021-11-18T06:55:37.997419Z","shell.execute_reply.started":"2021-11-18T06:55:37.214574Z","shell.execute_reply":"2021-11-18T06:55:37.996128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv(\"/kaggle/input/halooo/train_df_renamed_1.csv\")\ndf_test = pd.read_csv(\"/kaggle/input/halooo/test_df_renamed_1.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-11-18T06:55:37.999105Z","iopub.execute_input":"2021-11-18T06:55:37.999908Z","iopub.status.idle":"2021-11-18T06:55:38.635014Z","shell.execute_reply.started":"2021-11-18T06:55:37.999863Z","shell.execute_reply":"2021-11-18T06:55:38.63412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_equal = df_train.groupby('Household level identifier')['Target'].apply(lambda x: x.nunique() == 1)\nnot_equal = all_equal[all_equal != True]\nprint('No of households where target values are not all the same: %s'%(len(not_equal)))","metadata":{"execution":{"iopub.status.busy":"2021-11-18T06:55:38.639322Z","iopub.execute_input":"2021-11-18T06:55:38.639552Z","iopub.status.idle":"2021-11-18T06:55:38.846493Z","shell.execute_reply.started":"2021-11-18T06:55:38.639525Z","shell.execute_reply":"2021-11-18T06:55:38.845319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# e.g. of households with different target values\n\n# COMMENT OUT\ndf_train[df_train['Household level identifier'] == not_equal.index[0]][['Household level identifier', '=1 if household head', 'Target']]","metadata":{"execution":{"iopub.status.busy":"2021-11-18T06:55:38.848128Z","iopub.execute_input":"2021-11-18T06:55:38.848425Z","iopub.status.idle":"2021-11-18T06:55:38.87002Z","shell.execute_reply.started":"2021-11-18T06:55:38.848393Z","shell.execute_reply":"2021-11-18T06:55:38.869184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"households_with_head = df_train.groupby('Household level identifier')['=1 if household head'].sum()\n\n\nhouseholds_no_heads = df_train.loc[df_train['Household level identifier'].isin(households_with_head[households_with_head == 0].index), :]\nprint('No. of households with no heads: %s' %households_no_heads['Household level identifier'].nunique())","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-11-18T06:55:38.872442Z","iopub.execute_input":"2021-11-18T06:55:38.873095Z","iopub.status.idle":"2021-11-18T06:55:38.891077Z","shell.execute_reply.started":"2021-11-18T06:55:38.873057Z","shell.execute_reply":"2021-11-18T06:55:38.890212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"households_nh_equal = households_no_heads.groupby('Household level identifier')['Target'].apply(lambda x:x.nunique()==1)","metadata":{"execution":{"iopub.status.busy":"2021-11-18T06:55:38.892578Z","iopub.execute_input":"2021-11-18T06:55:38.893261Z","iopub.status.idle":"2021-11-18T06:55:38.900542Z","shell.execute_reply.started":"2021-11-18T06:55:38.893222Z","shell.execute_reply":"2021-11-18T06:55:38.899776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('No. of households with no heads & have different labels: %s' %sum(households_nh_equal==False))","metadata":{"execution":{"iopub.status.busy":"2021-11-18T06:55:38.901901Z","iopub.execute_input":"2021-11-18T06:55:38.902293Z","iopub.status.idle":"2021-11-18T06:55:38.912956Z","shell.execute_reply.started":"2021-11-18T06:55:38.902246Z","shell.execute_reply":"2021-11-18T06:55:38.912206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-18T06:55:38.914408Z","iopub.execute_input":"2021-11-18T06:55:38.915226Z","iopub.status.idle":"2021-11-18T06:55:38.925251Z","shell.execute_reply.started":"2021-11-18T06:55:38.915186Z","shell.execute_reply":"2021-11-18T06:55:38.924685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# change households' targets with different target labels to be same as that household head\nfor h in not_equal.index:\n    label = int(df_train[(df_train['Household level identifier'] == h) & (df_train['=1 if household head'] == 1)]['Target'])\n    df_train.loc[df_train['Household level identifier'] == h, 'Target'] = label\n    \nall_equal_1 = df_train.groupby('Household level identifier')['Target'].apply(lambda x:x.nunique()==1)","metadata":{"execution":{"iopub.status.busy":"2021-11-18T06:55:38.926587Z","iopub.execute_input":"2021-11-18T06:55:38.926831Z","iopub.status.idle":"2021-11-18T06:55:39.497627Z","shell.execute_reply.started":"2021-11-18T06:55:38.926802Z","shell.execute_reply":"2021-11-18T06:55:39.496779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a = df_train.groupby('Household level identifier')['Target'].apply(lambda x: x.nunique() == 1)\nnot_equal = a[a != True]\nprint('No of households where target values are not all the same: %s'%(len(not_equal)))","metadata":{"execution":{"iopub.status.busy":"2021-11-18T06:55:39.498799Z","iopub.execute_input":"2021-11-18T06:55:39.499025Z","iopub.status.idle":"2021-11-18T06:55:39.803403Z","shell.execute_reply.started":"2021-11-18T06:55:39.498995Z","shell.execute_reply":"2021-11-18T06:55:39.802527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#let's look at the distribution of the classes\ntarget = df_train.values[:, -1]\ncounter = Counter(target)\nfor i, j in counter.items():\n    percent = j/len(target)*100\n    print('Class = %s, Count = %d, Percentage = %.3f%%' %(i, j, percent))","metadata":{"execution":{"iopub.status.busy":"2021-11-18T06:55:39.804547Z","iopub.execute_input":"2021-11-18T06:55:39.804805Z","iopub.status.idle":"2021-11-18T06:55:39.851372Z","shell.execute_reply.started":"2021-11-18T06:55:39.804768Z","shell.execute_reply":"2021-11-18T06:55:39.850519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So we see that not all the classes are equally distributed. This indicates that we may have to use means such as adding weights for multiclass classification for imbalanced dataset.","metadata":{}},{"cell_type":"code","source":"df_train.drop(df_train.loc[:, 'escolari squared':'Age squared'].columns,\n                            axis = 1, inplace = True)\ndf_train.head()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-11-18T06:55:39.854811Z","iopub.execute_input":"2021-11-18T06:55:39.855047Z","iopub.status.idle":"2021-11-18T06:55:39.888479Z","shell.execute_reply.started":"2021-11-18T06:55:39.855017Z","shell.execute_reply":"2021-11-18T06:55:39.887458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We don't actually need to include household level identifier and Id during training as they are actually irrelevant data. Therefore, we should drop them too.","metadata":{}},{"cell_type":"code","source":"df_train.drop(['Household level identifier', 'Id'], axis = 1,\n             inplace = True)\ndf_train.head()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-11-18T06:55:39.89076Z","iopub.execute_input":"2021-11-18T06:55:39.891807Z","iopub.status.idle":"2021-11-18T06:55:39.921316Z","shell.execute_reply.started":"2021-11-18T06:55:39.891671Z","shell.execute_reply":"2021-11-18T06:55:39.92049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Functions for preprocessing data","metadata":{}},{"cell_type":"code","source":"#need to normalise some of the columns\ndef prepData(df):\n    unnormal_cols = selectUnnormalised(df)\n    x = df.iloc[:, :-1]\n    y = df.iloc[:, -1:]\n    xtrain, xtest, ytrain, ytest = tts(x,y,test_size = 0.3, random_state = 42)\n    xTrain, xTest = normalise(unnormal_cols, xtrain, xtest)\n    yTrain, yTest = ytrain.values.ravel(), ytest.values.ravel()\n    return xTrain.values, xTest.values, yTrain, yTest\n\n#getting the columns with non-normalised values\ndef selectUnnormalised(df):\n    normCol = df.columns[df.isin([0,1]).all()] #getting the binary columns\n    dfCols_list = list(df.columns)\n    unnormCols = list(set(dfCols_list)-set(normCol))\n    \n    #remove target because we shouldn't normalise that, but rather encode\n    unnormCols.remove('Target')\n    return unnormCols\n\n#normalising data in training set\ndef normalise(unnormCols, xTrain, xTest):\n    #normalise training data\n    toBeNorm_train = xTrain[[i for i in unnormCols]]\n    ss = StandardScaler()\n    std_scale = ss.fit(toBeNorm_train)\n    xTrain_norm = std_scale.transform(toBeNorm_train)\n    \n    #covert numpy array to df\n    xTrain_normCol = pd.DataFrame(xTrain_norm, index = toBeNorm_train.index,\n                                 columns = toBeNorm_train.columns)\n    xTrain.update(xTrain_normCol)\n    \n    #normalise test data using mean and SD of training set\n    toBeNorm_test = xTest[[i for i in unnormCols]]\n    xTest_norm = std_scale.transform(toBeNorm_test)\n    xTest_normCol = pd.DataFrame(xTest_norm, index = toBeNorm_test.index,\n                                columns = toBeNorm_test.columns)\n    xTest.update(xTest_normCol)\n    \n    return xTrain, xTest\n\n","metadata":{"execution":{"iopub.status.busy":"2021-11-18T06:55:39.922806Z","iopub.execute_input":"2021-11-18T06:55:39.923291Z","iopub.status.idle":"2021-11-18T06:55:39.93713Z","shell.execute_reply.started":"2021-11-18T06:55:39.923257Z","shell.execute_reply":"2021-11-18T06:55:39.936225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fuctions for training models","metadata":{}},{"cell_type":"code","source":"#training models\n# we start with bagging classifier\ndef trainBG(xtrain, xtest, ytrain,  ytest):\n    #get list of accuracies\n    accEst_train = []\n    accEst_test = []\n    accFeat_train = []\n    accFeat_test = []\n    accSam_train = []\n    accSam_test = []\n    \n    #to vary the parameters\n    num_estimators = [500,800,1000,1250,1650]\n    max_feature = [20,50,75,100,129]\n    max_sample = [100,200,225,275,300]\n    \n    \n    \n    #train with varying num_estimators\n    print(\"Training with varying num_estimators...\")\n    print(\"Completed run: \\t\")\n    for i in range(0,len(num_estimators)):\n       \n        bc_clf = bc(n_estimators = num_estimators[i],\n                   max_features = max_feature[0], max_samples = max_sample[0], n_jobs = 5)\n        bc_clf.fit(xtrain, ytrain)\n        accEst_train.append(bc_clf.score(xtrain, ytrain))\n        accEst_test.append(bc_clf.score(xtest,ytest))\n        print(str(i+1), end = \"\\t\")\n        \n    best_est = num_estimators[np.argmax(accEst_test)]\n    \n    #train with varying max_features\n    print(\"\\nTraining with varying max_features...\")\n    print(\"Completed run: \\t\")\n    for i in range(0,len(max_feature)):\n        \n        bc_clf = bc(n_estimators = best_est,\n                   max_features = max_feature[i], max_samples = max_sample[0], n_jobs = 5)\n        bc_clf.fit(xtrain, ytrain)\n        accFeat_train.append(bc_clf.score(xtrain, ytrain))\n        accFeat_test.append(bc_clf.score(xtest,ytest))\n        print(str(i+1), end = \"\\t\")\n        \n    best_numFeat = max_feature[np.argmax(accFeat_test)]\n    \n    #train with varying max_sample\n    print(\"\\nTraining with varying max_sample...\")\n    print(\"Completed run: \\t\")\n    for i in range(0,len(max_sample)):\n        \n        bc_clf = bc(n_estimators = best_est,\n                   max_features = best_numFeat, max_samples = max_sample[i], n_jobs = 5)\n        bc_clf.fit(xtrain, ytrain)\n        accSam_train.append(bc_clf.score(xtrain, ytrain))\n        accSam_test.append(bc_clf.score(xtest,ytest))\n        print(str(i+1), end = \"\\t\")\n        \n    best_numSam = max_sample[np.argmax(accSam_test)]\n    \n    print(\"\\n\\nBest parameters for...\\nnum_estimators: %d\\tmax_features: %d\\tmax_samples: %d\" %(best_est, best_numFeat, best_numSam))\n    \n    return accEst_train, accEst_test, accFeat_train, accFeat_test, accSam_train, accSam_test, best_est, best_numFeat, best_numSam\n\ndef trainRF_or_ET(modelName, xtrain, xtest, ytrain,  ytest):\n    accEst_train = []\n    accEst_test = []\n    accDepth_train = []\n    accDepth_test = []\n    \n    #to vary the parameters\n    num_estimators = [500,800,1000,1250,1650]\n    max_deep = [35,50,75,100,125]\n    \n    #train for varying num_estimators\n    print(\"Training with varying num_estimators...\")\n    print(\"Completed run: \\t\")\n    for i in range(0,len(num_estimators)):\n        if(modelName == \"random forest\"):\n            model = rf(n_estimators = num_estimators[i],\n                       max_depth = max_deep[0], class_weight ='balanced')\n            \n        elif(modelName == \"extra tree\"):\n            model = et(n_estimators = num_estimators[i],\n                      max_depth = max_deep[0], class_weight = 'balanced')\n        \n        model.fit(xtrain,ytrain)\n        accEst_train.append(model.score(xtrain, ytrain))\n        accEst_test.append(model.score(xtest, ytest))\n        print(str(i+1), end = \"\\t\")\n    \n    best_est = num_estimators[np.argmax(accEst_test)]\n    \n    #train for varying max_depth\n    print(\"\\nTraining with varying max_depth...\")\n    print(\"Completed run: \\t\")\n    for i in range(0,len(max_deep)):\n        if(modelName == \"random forest\"):\n            model = rf(n_estimators = best_est,\n                       max_depth = max_deep[i], class_weight ='balanced')\n            \n        elif(modelName == \"extra tree\"):\n            model = et(n_estimators = best_est,\n                      max_depth = max_deep[i], class_weight = 'balanced')\n        \n        model.fit(xtrain,ytrain)\n        accDepth_train.append(model.score(xtrain, ytrain))\n        accDepth_test.append(model.score(xtest, ytest))\n        print(str(i+1), end = \"\\t\")\n    \n    best_depth = max_deep[np.argmax(accDepth_test)]\n    \n    print(\"\\n\\nBest parameters for...\\nnum_estimators: %d\\tmax_depth: %d\" %(best_est, best_depth))\n\n    return accEst_train, accEst_test, accDepth_train, accDepth_test, best_est, best_depth","metadata":{"execution":{"iopub.status.busy":"2021-11-18T06:55:39.938499Z","iopub.execute_input":"2021-11-18T06:55:39.939005Z","iopub.status.idle":"2021-11-18T06:55:39.96502Z","shell.execute_reply.started":"2021-11-18T06:55:39.938959Z","shell.execute_reply":"2021-11-18T06:55:39.964219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xTrain, xTest, yTrain, yTest = prepData(df_train)","metadata":{"execution":{"iopub.status.busy":"2021-11-18T06:55:39.966392Z","iopub.execute_input":"2021-11-18T06:55:39.967127Z","iopub.status.idle":"2021-11-18T06:55:40.116594Z","shell.execute_reply.started":"2021-11-18T06:55:39.967022Z","shell.execute_reply":"2021-11-18T06:55:40.115629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"estTrain_bg, estTest_bg, featureTrain_bg, featureTest_bg, sampleTrain_bg, sampleTest_bg, num_est_bg, max_feat_bg, max_samp_bg= trainBG(xTrain, xTest, yTrain, yTest)","metadata":{"execution":{"iopub.status.busy":"2021-11-18T06:55:40.117884Z","iopub.execute_input":"2021-11-18T06:55:40.118096Z","iopub.status.idle":"2021-11-18T06:56:32.256167Z","shell.execute_reply.started":"2021-11-18T06:55:40.118069Z","shell.execute_reply":"2021-11-18T06:56:32.255521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"estTrain_rf, estTest_rf, depthTrain_rf, depthTest_rf, num_est_rf, max_deep_rf = trainRF_or_ET(\"random forest\", xTrain, xTest, yTrain, yTest)","metadata":{"execution":{"iopub.status.busy":"2021-11-18T06:56:32.257484Z","iopub.execute_input":"2021-11-18T06:56:32.258313Z","iopub.status.idle":"2021-11-18T06:59:21.113202Z","shell.execute_reply.started":"2021-11-18T06:56:32.25827Z","shell.execute_reply":"2021-11-18T06:59:21.112284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"estTrain_et, estTest_et, depthTrain_et, depthTest_et, num_est_et, max_deep_et = trainRF_or_ET(\"extra tree\", xTrain, xTest, yTrain, yTest)","metadata":{"execution":{"iopub.status.busy":"2021-11-18T06:59:21.114468Z","iopub.execute_input":"2021-11-18T06:59:21.114688Z","iopub.status.idle":"2021-11-18T07:01:53.897797Z","shell.execute_reply.started":"2021-11-18T06:59:21.114661Z","shell.execute_reply":"2021-11-18T07:01:53.896973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plot graphs","metadata":{}},{"cell_type":"code","source":"#plot graph for bagging classifier\ntitle = [\"Varying n_estimators\", \"Varying max_features with best n_estimators\", \"Varying max_samples with best n_estimators and max_features\"]\nfig, ax = plt.subplots(3,1, figsize = (10,15))\n\nnum_estimators = [500,800,1000,1250,1650]\nmax_feature = [20,50,75,100,129]\nmax_sample = [100,200,225,275,300]\n\nax[0].plot(num_estimators, estTrain_bg, \"-b\", label = \"Train\")\nax[0].plot(num_estimators, estTest_bg, \"-r\", label = \"Test\")\nax[0].legend(loc = \"upper right\")\nax[0].set_title(title[0])\nax[0].set_xlabel(\"Numbers of num_estimators\")\nax[0].set_ylabel(\"Accuracy\")\n\nax[1].plot(max_feature, featureTrain_bg, \"-b\", label = \"Train\")\nax[1].plot(max_feature, featureTest_bg, \"-r\", label = \"Test\")\nax[1].legend(loc = \"upper right\")\nax[1].set_title(title[1])\nax[1].set_xlabel(\"Numbers of max_features\")\nax[1].set_ylabel(\"Accuracy\")\n\nax[2].plot(max_sample, sampleTrain_bg, \"-b\", label = \"Train\")\nax[2].plot(max_sample, sampleTest_bg, \"-r\", label = \"Test\")\nax[2].legend(loc = \"upper right\")\nax[2].set_title(title[2])\nax[2].set_xlabel(\"Numbers of max_samples\")\nax[2].set_ylabel(\"Accuracy\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-18T07:01:53.899744Z","iopub.execute_input":"2021-11-18T07:01:53.900061Z","iopub.status.idle":"2021-11-18T07:01:54.500388Z","shell.execute_reply.started":"2021-11-18T07:01:53.900019Z","shell.execute_reply":"2021-11-18T07:01:54.499452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the graphs above, as we continue using the best parameters that give us the highest accuracy for test set, it is clear that the accuracy of the model rises. \n\nThe optimal paramaters will be set as such:\n- n_estimators = 500\n- max_features = 129\n- max_samples = 300\n\nWe shall take a look at the classification accuracy of the bagging classifier with the optimal parameters","metadata":{}},{"cell_type":"code","source":"bag = bc(n_estimators = num_est_bg, max_features = max_feat_bg, max_samples = max_samp_bg)\nbag.fit(xTrain, yTrain)\ny_pred = bag.predict(xTest)\nprint(\"Classification accuracy: {:.2f}\".format(bag.score(xTest, yTest)))\nprint(\"F1 score: {:.2f}\".format(f1_score(yTest, y_pred, average = 'macro')))","metadata":{"execution":{"iopub.status.busy":"2021-11-18T07:01:54.501899Z","iopub.execute_input":"2021-11-18T07:01:54.50223Z","iopub.status.idle":"2021-11-18T07:01:59.747013Z","shell.execute_reply.started":"2021-11-18T07:01:54.502185Z","shell.execute_reply":"2021-11-18T07:01:59.746148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plot graph for random forest\nfig, ax = plt.subplots(2,1, figsize = (10,10))\n\ntitle = [\"Varying n_estimators\", \"Varying max_depth with best n_estimators\"]\n\nnum_estimators = [500,800,1000,1250,1650]\nmax_deep = [35,50,75,100,125]\n\nax[0].plot(num_estimators, estTrain_rf, \"-b\", label = \"Train\")\nax[0].plot(num_estimators, estTest_rf, \"-r\", label = \"Test\")\nax[0].legend(loc = \"upper right\")\nax[0].set_title(title[0])\nax[0].set_xlabel(\"Number of n_estimators\")\nax[0].set_ylabel(\"Accuracy\")\n\nax[1].plot(max_deep, depthTrain_rf, \"-b\", label = \"Train\")\nax[1].plot(max_deep, depthTest_rf, \"-r\", label = \"Test\")\nax[1].legend(loc = \"upper right\")\nax[1].set_title(title[1])\nax[1].set_xlabel(\"Number of max_depth\")\nax[1].set_ylabel(\"Accuracy\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-18T07:01:59.748208Z","iopub.execute_input":"2021-11-18T07:01:59.748461Z","iopub.status.idle":"2021-11-18T07:02:00.133162Z","shell.execute_reply.started":"2021-11-18T07:01:59.748424Z","shell.execute_reply":"2021-11-18T07:02:00.132297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Random forest classifier seems to perform much more accurately and the test accuracy is relatively consistent. Therefore, we shall follow the indicated optimal parameters and create a model using them. \n\nOptimal parameters:\n- n_estimators = 500\n- max_depth = 100\n- class_weight = 'balanced'\n\nNext, we shall assess the classification accuracy of the random forest classifier.","metadata":{}},{"cell_type":"code","source":"rff = rf(n_estimators = num_est_rf, max_depth = max_deep_rf, class_weight = 'balanced')\nrff.fit(xTrain, yTrain)\ny_pred = rff.predict(xTest)\nprint(\"Classification accuracy: {:.2f}\".format(rff.score(xTest, yTest)))\nprint(\"F1 score: {:.2f}\".format(f1_score(yTest, y_pred, average = 'macro')))","metadata":{"execution":{"iopub.status.busy":"2021-11-18T07:02:00.134964Z","iopub.execute_input":"2021-11-18T07:02:00.135563Z","iopub.status.idle":"2021-11-18T07:02:17.761897Z","shell.execute_reply.started":"2021-11-18T07:02:00.135518Z","shell.execute_reply":"2021-11-18T07:02:17.760943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plotting graph for extra trees\nfig, ax = plt.subplots(2,1, figsize = (10,10))\n\ntitle = [\"Varying n_estimators\", \"Varying max_depth with best n_estimators\"]\n\nnum_estimators = [500,800,1000,1250,1650]\nmax_deep = [35,50,75,100,125]\n\nax[0].plot(num_estimators, estTrain_et, \"-b\", label = \"Train\")\nax[0].plot(num_estimators, estTest_et, \"-r\", label = \"Test\")\nax[0].legend(loc = \"upper right\")\nax[0].set_title(title[0])\nax[0].set_xlabel(\"Number of n_estimators\")\nax[0].set_ylabel(\"Accuracy\")\n\nax[1].plot(max_deep, depthTrain_et, \"-b\", label = \"Train\")\nax[1].plot(max_deep, depthTest_et, \"-r\", label = \"Test\")\nax[1].legend(loc = \"upper right\")\nax[1].set_title(title[1])\nax[1].set_xlabel(\"Number of max_depth\")\nax[1].set_ylabel(\"Accuracy\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-18T07:02:17.763195Z","iopub.execute_input":"2021-11-18T07:02:17.763434Z","iopub.status.idle":"2021-11-18T07:02:18.13206Z","shell.execute_reply.started":"2021-11-18T07:02:17.763405Z","shell.execute_reply":"2021-11-18T07:02:18.131214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems like extra trees is performing slightly better than random forest. As there are no indications of extreme overfitting and that the test accuracy is pretty consistent, we shall use the recommended optimal parameters and assess the accuracy score of the extra trees classifier model. \n\nOptimal parameters:\n- n_estimators = 500\n- max_depth = 35\n- class_weight = 'balanced'","metadata":{}},{"cell_type":"code","source":"ett = et(n_estimators =num_est_et, max_depth = max_deep_et, class_weight = 'balanced')\nett.fit(xTrain, yTrain)\ny_pred = ett.predict(xTest)\nprint(\"Classification accuracy: {:.2f}\".format(ett.score(xTest, yTest)))\nprint(\"F1 score: {:.2f}\".format(f1_score(yTest, y_pred, average = 'macro')))","metadata":{"execution":{"iopub.status.busy":"2021-11-18T07:02:18.133296Z","iopub.execute_input":"2021-11-18T07:02:18.133526Z","iopub.status.idle":"2021-11-18T07:02:32.613548Z","shell.execute_reply.started":"2021-11-18T07:02:18.133497Z","shell.execute_reply":"2021-11-18T07:02:32.612677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Using LightGBM classifier","metadata":{}},{"cell_type":"code","source":"# build the model\nlgb_clf = lgb.LGBMClassifier(max_depth=-1, learning_rate=0.03, objective='multiclass',\n                             random_state=None, silent=True, metric='multi_logloss', \n                             n_jobs=4, n_estimators=5000, class_weight='balanced',\n                             colsample_bytree =  0.89, min_child_samples = 90, num_leaves = 56, subsample = 0.96)\n\n# fit data into the model and predict the test set\nlgb_clf.fit(xTrain, yTrain, eval_set=[(xTest, yTest)], \n            early_stopping_rounds=400, verbose=100)\ny_pred = lgb_clf.predict(xTest)","metadata":{"execution":{"iopub.status.busy":"2021-11-18T07:02:32.615112Z","iopub.execute_input":"2021-11-18T07:02:32.615547Z","iopub.status.idle":"2021-11-18T07:02:45.930326Z","shell.execute_reply.started":"2021-11-18T07:02:32.6155Z","shell.execute_reply":"2021-11-18T07:02:45.929616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Classification accuracy: {:.2f}\".format(lgb_clf.score(xTest, yTest)))\nprint(\"F1 score : {:.2f}\".format(f1_score(yTest, y_pred, average = 'macro')))","metadata":{"execution":{"iopub.status.busy":"2021-11-18T07:02:45.931378Z","iopub.execute_input":"2021-11-18T07:02:45.932394Z","iopub.status.idle":"2021-11-18T07:02:46.3522Z","shell.execute_reply.started":"2021-11-18T07:02:45.93235Z","shell.execute_reply":"2021-11-18T07:02:46.351554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We conclude that **LightGBM** classifier model is the most suitable for classification of the poverty classes.","metadata":{}},{"cell_type":"markdown","source":"## Using only household heads","metadata":{}},{"cell_type":"code","source":"df_headsOnly = df_train[df_train['=1 if household head'] == 1]\ndf_headsOnly.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-18T07:02:46.353432Z","iopub.execute_input":"2021-11-18T07:02:46.356636Z","iopub.status.idle":"2021-11-18T07:02:46.384742Z","shell.execute_reply.started":"2021-11-18T07:02:46.356591Z","shell.execute_reply":"2021-11-18T07:02:46.3838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xTrain, xTest, yTrain, yTest = prepData(df_headsOnly)","metadata":{"execution":{"iopub.status.busy":"2021-11-18T07:02:46.385946Z","iopub.execute_input":"2021-11-18T07:02:46.386271Z","iopub.status.idle":"2021-11-18T07:02:46.46742Z","shell.execute_reply.started":"2021-11-18T07:02:46.386237Z","shell.execute_reply":"2021-11-18T07:02:46.466779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Bagging classifier\nestTrain_bg, estTest_bg, featureTrain_bg, featureTest_bg, sampleTrain_bg, sampleTest_bg, num_est_bg, max_feat_bg, max_samp_bg= trainBG(xTrain, xTest, yTrain, yTest)","metadata":{"execution":{"iopub.status.busy":"2021-11-18T07:02:46.468607Z","iopub.execute_input":"2021-11-18T07:02:46.46933Z","iopub.status.idle":"2021-11-18T07:03:43.047484Z","shell.execute_reply.started":"2021-11-18T07:02:46.469292Z","shell.execute_reply":"2021-11-18T07:03:43.046789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Random forest classifier\nestTrain_rf, estTest_rf, depthTrain_rf, depthTest_rf, num_est_rf, max_deep_rf = trainRF_or_ET(\"random forest\", xTrain, xTest, yTrain, yTest)","metadata":{"execution":{"iopub.status.busy":"2021-11-18T07:03:43.04916Z","iopub.execute_input":"2021-11-18T07:03:43.049853Z","iopub.status.idle":"2021-11-18T07:04:48.491482Z","shell.execute_reply.started":"2021-11-18T07:03:43.049802Z","shell.execute_reply":"2021-11-18T07:04:48.490491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Extra trees classifier\nestTrain_et, estTest_et, depthTrain_et, depthTest_et, num_est_et, max_deep_et = trainRF_or_ET(\"extra tree\", xTrain, xTest, yTrain, yTest)","metadata":{"execution":{"iopub.status.busy":"2021-11-18T07:04:48.494867Z","iopub.execute_input":"2021-11-18T07:04:48.495108Z","iopub.status.idle":"2021-11-18T07:05:32.534033Z","shell.execute_reply.started":"2021-11-18T07:04:48.495079Z","shell.execute_reply":"2021-11-18T07:05:32.53314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we assess the models' performance...","metadata":{}},{"cell_type":"code","source":"bag_hh = bc(n_estimators = num_est_bg, max_features = max_feat_bg, max_samples = max_samp_bg)\nbag_hh.fit(xTrain, yTrain)\ny_pred = bag_hh.predict(xTest)\nprint(\"Bagging\")\nprint(\"Classification accuracy: {:.2f}\".format(bag_hh.score(xTest, yTest)))\nprint(\"F1 score: {:.2f}\".format(f1_score(yTest, y_pred, average = 'macro')))","metadata":{"execution":{"iopub.status.busy":"2021-11-18T07:05:32.5358Z","iopub.execute_input":"2021-11-18T07:05:32.53602Z","iopub.status.idle":"2021-11-18T07:05:40.486079Z","shell.execute_reply.started":"2021-11-18T07:05:32.535992Z","shell.execute_reply":"2021-11-18T07:05:40.485241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rff_hh = rf(n_estimators = num_est_rf, max_depth = max_deep_rf, class_weight = 'balanced')\nrff_hh.fit(xTrain, yTrain)\ny_pred = rff_hh.predict(xTest)\nprint(\"Random Forest:\")\nprint(\"Classification accuracy: {:.2f}\".format(rff_hh.score(xTest, yTest)))\nprint(\"F1 score: {:.2f}\".format(f1_score(yTest, y_pred, average = 'macro')))","metadata":{"execution":{"iopub.status.busy":"2021-11-18T07:05:40.487726Z","iopub.execute_input":"2021-11-18T07:05:40.488016Z","iopub.status.idle":"2021-11-18T07:05:47.238236Z","shell.execute_reply.started":"2021-11-18T07:05:40.487974Z","shell.execute_reply":"2021-11-18T07:05:47.237259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ett_hh = et(n_estimators =num_est_et, max_depth = max_deep_et, class_weight = 'balanced')\nett_hh.fit(xTrain, yTrain)\ny_pred = ett_hh.predict(xTest)\nprint(\"Extra Tree:\")\nprint(\"Classification accuracy: {:.2f}\".format(ett_hh.score(xTest, yTest)))\nprint(\"F1 score: {:.2f}\".format(f1_score(yTest, y_pred, average = 'macro')))","metadata":{"execution":{"iopub.status.busy":"2021-11-18T07:05:47.239938Z","iopub.execute_input":"2021-11-18T07:05:47.240282Z","iopub.status.idle":"2021-11-18T07:05:50.025716Z","shell.execute_reply.started":"2021-11-18T07:05:47.240242Z","shell.execute_reply":"2021-11-18T07:05:50.02485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# build the model\nlgb_hh = lgb.LGBMClassifier(max_depth=-1, learning_rate=0.03, objective='multiclass',\n                             random_state=None, silent=True, metric='multi_logloss', \n                             n_jobs=4, n_estimators=5000, class_weight='balanced',\n                             colsample_bytree =  0.89, min_child_samples = 90, num_leaves = 56, subsample = 0.96)\n\n# fit data into the model and predict the test set\nlgb_hh.fit(xTrain, yTrain, eval_set=[(xTest, yTest)], \n            early_stopping_rounds=400, verbose=100)\ny_pred = lgb_hh.predict(xTest)","metadata":{"execution":{"iopub.status.busy":"2021-11-18T07:05:50.027111Z","iopub.execute_input":"2021-11-18T07:05:50.027497Z","iopub.status.idle":"2021-11-18T07:05:52.79754Z","shell.execute_reply.started":"2021-11-18T07:05:50.027449Z","shell.execute_reply":"2021-11-18T07:05:52.791824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Classification accuracy: {:.2f}\".format(lgb_clf.score(xTest, yTest)))\nprint(\"F1 score : {:.2f}\".format(f1_score(yTest, y_pred, average = 'macro')))","metadata":{"execution":{"iopub.status.busy":"2021-11-18T07:05:52.799207Z","iopub.execute_input":"2021-11-18T07:05:52.799465Z","iopub.status.idle":"2021-11-18T07:05:52.987652Z","shell.execute_reply.started":"2021-11-18T07:05:52.799434Z","shell.execute_reply":"2021-11-18T07:05:52.986812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Restricting the dataset to only using the household heads seems to result in a much lower accuracy score for the first 3 models. For LGBM, the accuracy was still high at 93%. However, the f1 score across all models were very poor. Hence, we shall train and test on the entire dataset instead.","metadata":{}},{"cell_type":"markdown","source":"## Predicting the test set\n\nWe will pass the test dataset into our best model and write a new csv which will consists of the following columns: <br>\n1. Household id\n2. Individual's ID\n3. Predicted target/class","metadata":{}},{"cell_type":"code","source":"toTest_data = df_test.iloc[:, 2:131]\nidentity = df_test.iloc[:, 1:2]","metadata":{"execution":{"iopub.status.busy":"2021-11-18T07:05:52.990298Z","iopub.execute_input":"2021-11-18T07:05:52.990643Z","iopub.status.idle":"2021-11-18T07:05:53.007401Z","shell.execute_reply.started":"2021-11-18T07:05:52.990594Z","shell.execute_reply":"2021-11-18T07:05:53.006046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#need to prepare special function to get normalised test set\n#need to normalise some of the columns\ndef prepData2(df):\n    unnormal_cols = selectUnnormalised2(df)\n    x_df = normalise2(unnormal_cols, df)\n    return x_df\n\n#getting the columns with non-normalised values\ndef selectUnnormalised2(df):\n    normCol = df.columns[df.isin([0,1]).all()] #getting the binary columns\n    dfCols_list = list(df.columns)\n    unnormCols = list(set(dfCols_list)-set(normCol))\n    \n    return unnormCols\n\n#normalising data in training set\ndef normalise2(unnormCols, df):\n    #normalise training data\n    toBeNorm = df[[i for i in unnormCols]]\n    ss = StandardScaler()\n    std_scale = ss.fit(toBeNorm)\n    x_norm = std_scale.transform(toBeNorm)\n    \n    #covert numpy array to df\n    x_normCols = pd.DataFrame(x_norm, index = toBeNorm.index,\n                                 columns = toBeNorm.columns)\n    df.update(x_normCols)\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2021-11-18T07:05:53.013521Z","iopub.execute_input":"2021-11-18T07:05:53.014288Z","iopub.status.idle":"2021-11-18T07:05:53.023866Z","shell.execute_reply.started":"2021-11-18T07:05:53.014202Z","shell.execute_reply":"2021-11-18T07:05:53.022856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xTest_true = prepData2(toTest_data) #generate normalised actual test set\nxTest_true.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-18T07:05:53.025129Z","iopub.execute_input":"2021-11-18T07:05:53.025513Z","iopub.status.idle":"2021-11-18T07:05:53.254806Z","shell.execute_reply.started":"2021-11-18T07:05:53.025482Z","shell.execute_reply":"2021-11-18T07:05:53.25422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ett_hh = et(n_estimators =num_est_et, max_depth = max_deep_et, class_weight = 'balanced')\n# ett_hh.fit(xTrain, yTrain)\nxTest_truePred = ett_hh.predict(xTest_true)\n\n\n# xTest_truePred = lgb_clf.predict(xTest_true)","metadata":{"execution":{"iopub.status.busy":"2021-11-18T07:11:10.062493Z","iopub.execute_input":"2021-11-18T07:11:10.06285Z","iopub.status.idle":"2021-11-18T07:11:12.430416Z","shell.execute_reply.started":"2021-11-18T07:11:10.062786Z","shell.execute_reply":"2021-11-18T07:11:12.429599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"identity['Target'] = xTest_truePred\nidentity.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-18T07:05:55.682057Z","iopub.execute_input":"2021-11-18T07:05:55.682314Z","iopub.status.idle":"2021-11-18T07:05:55.69359Z","shell.execute_reply.started":"2021-11-18T07:05:55.682284Z","shell.execute_reply":"2021-11-18T07:05:55.692774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#for submission\nidentity.to_csv('/kaggle/working/submission.csv',index = False)","metadata":{"execution":{"iopub.status.busy":"2021-11-18T07:05:55.694943Z","iopub.execute_input":"2021-11-18T07:05:55.695836Z","iopub.status.idle":"2021-11-18T07:05:55.753698Z","shell.execute_reply.started":"2021-11-18T07:05:55.695774Z","shell.execute_reply":"2021-11-18T07:05:55.752926Z"},"trusted":true},"execution_count":null,"outputs":[]}]}