{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Income Qualification\n### DESCRIPTION\nIdentify the level of income qualification needed for the families in Latin America.\n\nProblem Statement Scenario:\nMany social programs have a hard time ensuring that the right people are given enough aid. It’s tricky when a program focuses on the poorest segment of the population. This segment of the population can’t provide the necessary income and expense records to prove that they qualify.\n\nIn Latin America, a popular method called Proxy Means Test (PMT) uses an algorithm to verify income qualification. With PMT, agencies use a model that considers a family’s observable household attributes like the material of their walls and ceiling or the assets found in their homes to\nclassify them and predict their level of need.\n\nWhile this is an improvement, accuracy remains a problem as the region’s population grows and poverty declines.\n\nThe Inter-American Development Bank (IDB)believes that new methods beyond traditional econometrics, based on a dataset of Costa Rican household characteristics, might help improve PMT’s performance.<br>\n\n### <b>Following actions should be performed:<b>\n<ol>\n<li>Identify the output variable.</li>\n<li>Understand the type of data.</li>\n<li>Check if there are any biases in your dataset.</li>\n<li>Check whether all members of the house have the same poverty level.</li>\n<li>Check if there is a house without a family head.</li>\n<li>Set poverty level of the members and the head of the house within a family.</li>\n<li>Count how many null values are existing in columns.</li>\n<li>Remove null value rows of the target variable.</li>\n<li>Predict the accuracy using random forest classifier.</li>\n<li>Check the accuracy using random forest with cross validation.</li>\n<ol>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Core Data fields\nId - a unique identifier for each row.<br>\nTarget - the target is an ordinal variable indicating groups of income levels.<br>\n<ul>\n    <li>1 = extreme poverty </li>\n    <li>2 = moderate poverty </li>\n    <li>3 = vulnerable households </li>\n    <li>4 = non vulnerable households</li>\n</ul><br>\nidhogar - this is a unique identifier for each household. This can be used to create household-wide features, etc. All rows in a given household will have a matching value for this identifier.<br>\nparentesco1 - indicates if this person is the head of the household.<br>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set()\n\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <span style='background:blue;color:white'> Understand the Data </span>","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df_income_train = pd.read_csv(\"../input/costa-rican-household-poverty-prediction/train.csv\")\ndf_income_test =  pd.read_csv(\"../input/costa-rican-household-poverty-prediction/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_income_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_income_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_income_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### NOTE\nThe important piece of information here is that we don’t have ‘Target’ feature in Test Dataset. There are 3 Types of the features:\n    <ul>\n    <li>5 object type</li>\n    <li>130(Train set)/ 129 (test set) integer type</li>\n    <li>8 float type </li>\n    </ul>\n\n\nLets analyze features:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"### List the columns for different datatypes:\nprint('Integer Type: ')\nprint(df_income_train.select_dtypes(np.int64).columns)\nprint('\\n')\nprint('Float Type: ')\nprint(df_income_train.select_dtypes(np.float64).columns)\nprint('\\n')\nprint('Object Type: ')\nprint(df_income_train.select_dtypes(np.object).columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_income_train.select_dtypes('int64').head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Find columns with null values\nnull_counts=df_income_train.select_dtypes('int64').isnull().sum()\nnull_counts[null_counts > 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_income_train.select_dtypes('float64').head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Find columns with null values\nnull_counts=df_income_train.select_dtypes('float64').isnull().sum()\nnull_counts[null_counts > 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_income_train.select_dtypes('object').head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Find columns with null values\nnull_counts=df_income_train.select_dtypes('object').isnull().sum()\nnull_counts[null_counts > 0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### NOTE\nLooking at the different types of data and null values for each feature. We found the following: \n1. No null values for Integer type features. \n2. No null values for object type features. \n3. For float64 types below featufres has null value\n   1. v2a1 6860 \n   2. v18q1 7342 \n   3. rez_esc 7928 \n   4. meaneduc 5 \n   5. SQBmeaned 5","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We also noticed that object type features dependency, edjefe, edjefa have mixed values.<br>\nLets fix the data for features with null values and features with mixed values","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# <span style='background:blue;color:white'> Data Cleaning </span>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's fix first the column with mixed value:\n\nddependency, Dependency rate, calculated = \n(number of members of the household younger than 19 or older than 64)/(number of member of household between 19 and 64)\n\nedjefe=\nyears of education of male head of household, based on the interaction of\nescolari (years of education), head of household and gender, yes=1 and no=0\n\nedjefa: years of education of female head of household, based on the interaction of escolari (years of education), head of household and gender, yes=1 and no=0\n\n<font color='red'>For these three variables, it seems “yes” = 1 and “no” = 0. We can correct the variables using a mapping and convert to floats.</font>\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"mapping={'yes':1,'no':0}\n\nfor df in [df_income_train, df_income_test]:\n    df['dependency'] =df['dependency'].replace(mapping).astype(np.float64)\n    df['edjefe'] =df['edjefe'].replace(mapping).astype(np.float64)\n    df['edjefa'] =df['edjefa'].replace(mapping).astype(np.float64)\n    \ndf_income_train[['dependency','edjefe','edjefa']].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### NOTE\n\nLets fix the column with null values<br>\nAccording to the documentation for these columns:<br>\n\nv2a1 (total nulls: 6860) : Monthly rent payment<br>\nv18q1 (total nulls: 7342) : number of tablets household owns<br>\nrez_esc (total nulls: 7928) : Years behind in school<br>\nmeaneduc (total nulls: 5) : average years of education for adults (18+)<br>\nSQBmeaned (total nulls: 5) : square of the mean years of education of adults (>=18) in the household 142<br>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Lets look at v2a1 (total nulls: 6860) : Monthly rent payment \n\nwhy the null values, Lets look at few rows with nulls in v2a1:\n\n1. Columns related to  Monthly rent payment\n2. tipovivi1, =1 own and fully paid house\n3. tipovivi2, \"=1 own,  paying in installments\"\n4. tipovivi3, =1 rented\n5. tipovivi4, =1 precarious \n6. tipovivi5, \"=1 other(assigned,  borrowed)\"","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = df_income_train[df_income_train['v2a1'].isnull()].head()\n\ncolumns=['tipovivi1','tipovivi2','tipovivi3','tipovivi4','tipovivi5']\ndata[columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Variables indicating home ownership\nown_variables = [x for x in df_income_train if x.startswith('tipo')]\n\n\n# Plot of the home ownership variables for home missing rent payments\ndf_income_train.loc[df_income_train['v2a1'].isnull(), own_variables].sum().plot.bar(figsize = (10, 8),\n                                                                        color = 'green',\n                                                              edgecolor = 'k', linewidth = 2);\nplt.xticks([0, 1, 2, 3, 4],\n           ['Owns and Paid Off', 'Owns and Paying', 'Rented', 'Precarious', 'Other'],\n          rotation = 20)\nplt.title('Home Ownership Status for Households Missing Rent Payments', size = 18);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Looking at the above data it makes sense that when the house is fully paid, there will be no monthly rent payment.\n#Lets add 0 for all the null values.\nfor df in [df_income_train, df_income_test]:\n    df['v2a1'].fillna(value=0, inplace=True)\n\ndf_income_train[['v2a1']].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### NOTE\nLets look at v18q1 (total nulls: 7342) : number of tablets household owns<br>\nwhy the null values, Lets look at few rows with nulls in v18q1<br>\nColumns related to  number of tablets household owns <br>\nv18q, owns a tablet<br>\n\nSince this is a household variable, it only makes sense to look at it on a household level, so we'll only select the rows for the head of household.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Heads of household### NOTE\nheads = df_income_train.loc[df_income_train['parentesco1'] == 1].copy()\nheads.groupby('v18q')['v18q1'].apply(lambda x: x.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (8, 6))\ncol='v18q1'\ndf_income_train[col].value_counts().sort_index().plot.bar(color = 'blue',\n                                             edgecolor = 'k',\n                                             linewidth = 2)\nplt.xlabel(f'{col}'); plt.title(f'{col} Value Counts'); plt.ylabel('Count')\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### NOTE\nLooking at the above data it makes sense that when owns a tablet column is 0, there will be no number of tablets household owns. Lets add 0 for all the null values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for df in [df_income_train, df_income_test]:\n    df['v18q1'].fillna(value=0, inplace=True)\n\ndf_income_train[['v18q1']].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### NOTE\nLets look at rez_esc    (total nulls: 7928) : Years behind in school <br> \n why the null values, Lets look at few rows with nulls in rez_esc <br> \n Columns related to Years behind in school  <br> \n Age in years","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets look at the data with not null values first.\ndf_income_train[df_income_train['rez_esc'].notnull()]['age'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### NOTE\nFrom the above , we see that when min age is 7 and max age is 17 for Years, then the 'behind in school' column has a value.<br>\nLets confirm","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_income_train.loc[df_income_train['rez_esc'].isnull()]['age'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_income_train.loc[(df_income_train['rez_esc'].isnull() & \n                     ((df_income_train['age'] > 7) & (df_income_train['age'] < 17)))]['age'].describe()\n#There is one value that has Null for the 'behind in school' column with age between 7 and 17 ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_income_train[(df_income_train['age'] ==10) & df_income_train['rez_esc'].isnull()].head()\ndf_income_train[(df_income_train['Id'] =='ID_f012e4242')].head()\n#there is only one member in household for the member with age 10 and who is 'behind in school'. This explains why the member is \n#behind in school.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from above we see that  the 'behind in school' column has null values \n# Lets use the above to fix the data\nfor df in [df_income_train, df_income_test]:\n    df['rez_esc'].fillna(value=0, inplace=True)\ndf_income_train[['rez_esc']].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### NOTE\nLets look at meaneduc   (total nulls: 5) : average years of education for adults (18+)  <br>\nwhy the null values, Lets look at few rows with nulls in meaneduc  <br>\nColumns related to average years of education for adults (18+)    <br>\nedjefe, years of education of male head of household, based on the interaction of escolari (years of education),  <br>\nhead of household and gender, yes=1 and no=0  <br>\nedjefa, years of education of female head of household, based on the interaction of escolari (years of education),   <br>\nhead of household and gender, yes=1 and no=0   <br>\ninstlevel1, =1 no level of education  <br>\ninstlevel2, =1 incomplete primary   <br>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = df_income_train[df_income_train['meaneduc'].isnull()].head()\n\ncolumns=['edjefe','edjefa','instlevel1','instlevel2']\ndata[columns][data[columns]['instlevel1']>0].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from the above, we find that meaneduc is null when no level of education is 0\n#Lets fix the data\nfor df in [df_income_train, df_income_test]:\n    df['meaneduc'].fillna(value=0, inplace=True)\ndf_income_train[['meaneduc']].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### NOTE\nLets look at SQBmeaned  (total nulls: 5) : square of the mean years of education of adults (>=18) in the household 142  <br>\nwhy the null values, Lets look at few rows with nulls in SQBmeaned<br>\nColumns related to average years of education for adults (18+)  <br>\nedjefe, years of education of male head of household, based on the interaction of escolari (years of education),<br>\nhead of household and gender, yes=1 and no=0<br>\nedjefa, years of education of female head of household, based on the interaction of escolari (years of education), <br>\nhead of household and gender, yes=1 and no=0 <br>\ninstlevel1, =1 no level of education<br>\ninstlevel2, =1 incomplete primary <br>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = df_income_train[df_income_train['SQBmeaned'].isnull()].head()\n\ncolumns=['edjefe','edjefa','instlevel1','instlevel2']\ndata[columns][data[columns]['instlevel1']>0].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from the above, we find that SQBmeaned is null when no level of education is 0\n#Lets fix the data\nfor df in [df_income_train, df_income_test]:\n    df['SQBmeaned'].fillna(value=0, inplace=True)\ndf_income_train[['SQBmeaned']].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets look at the overall data\nnull_counts = df_income_train.isnull().sum()\nnull_counts[null_counts > 0].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Groupby the household and figure out the number of unique values\nall_equal = df_income_train.groupby('idhogar')['Target'].apply(lambda x: x.nunique() == 1)\n\n# Households where targets are not all equal\nnot_equal = all_equal[all_equal != True]\nprint('There are {} households where the family members do not all have the same target.'.format(len(not_equal)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets check one household\ndf_income_train[df_income_train['idhogar'] == not_equal.index[0]][['idhogar', 'parentesco1', 'Target']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets use Target value of the parent record (head of the household) and update rest. But before that lets check\n# if all families has a head. \n\nhouseholds_head = df_income_train.groupby('idhogar')['parentesco1'].sum()\n\n# Find households without a head\nhouseholds_no_head = df_income_train.loc[df_income_train['idhogar'].isin(households_head[households_head == 0].index), :]\n\nprint('There are {} households without a head.'.format(households_no_head['idhogar'].nunique()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find households without a head and where Target value are different\nhouseholds_no_head_equal = households_no_head.groupby('idhogar')['Target'].apply(lambda x: x.nunique() == 1)\nprint('{} Households with no head have different Target value.'.format(sum(households_no_head_equal == False)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets fix the data\n#Set poverty level of the members and the head of the house within a family.\n# Iterate through each household\nfor household in not_equal.index:\n    # Find the correct label (for the head of household)\n    true_target = int(df_income_train[(df_income_train['idhogar'] == household) & (df_income_train['parentesco1'] == 1.0)]['Target'])\n    \n    # Set the correct label for all members in the household\n    df_income_train.loc[df_income_train['idhogar'] == household, 'Target'] = true_target\n    \n    \n# Groupby the household and figure out the number of unique values\nall_equal = df_income_train.groupby('idhogar')['Target'].apply(lambda x: x.nunique() == 1)\n\n# Households where targets are not all equal\nnot_equal = all_equal[all_equal != True]\nprint('There are {} households where the family members do not all have the same target.'.format(len(not_equal)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### NOTE\nLets look at the dataset and plot head of household and Target","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1 = extreme poverty 2 = moderate poverty 3 = vulnerable households 4 = non vulnerable households \ntarget_counts = heads['Target'].value_counts().sort_index()\ntarget_counts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_counts.plot.bar(figsize = (8, 6),linewidth = 2,edgecolor = 'k',title=\"Target vs Total_Count\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Note\nextreme poverty is the smallest count in the train dataset. The dataset is biased.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Lets look at the Squared Variables<br>\n‘SQBescolari’<br>\n‘SQBage’<br>\n‘SQBhogar_total’<br>\n‘SQBedjefe’<br>\n‘SQBhogar_nin’<br>\n‘SQBovercrowding’<br>\n‘SQBdependency’<br>\n‘SQBmeaned’<br>\n‘agesq’<br>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets remove them\nprint(df_income_train.shape)\ncols=['SQBescolari', 'SQBage', 'SQBhogar_total', 'SQBedjefe', \n        'SQBhogar_nin', 'SQBovercrowding', 'SQBdependency', 'SQBmeaned', 'agesq']\n\n\nfor df in [df_income_train, df_income_test]:\n    df.drop(columns = cols,inplace=True)\n\nprint(df_income_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"id_ = ['Id', 'idhogar', 'Target']\n\nind_bool = ['v18q', 'dis', 'male', 'female', 'estadocivil1', 'estadocivil2', 'estadocivil3', \n            'estadocivil4', 'estadocivil5', 'estadocivil6', 'estadocivil7', \n            'parentesco1', 'parentesco2',  'parentesco3', 'parentesco4', 'parentesco5', \n            'parentesco6', 'parentesco7', 'parentesco8',  'parentesco9', 'parentesco10', \n            'parentesco11', 'parentesco12', 'instlevel1', 'instlevel2', 'instlevel3', \n            'instlevel4', 'instlevel5', 'instlevel6', 'instlevel7', 'instlevel8', \n            'instlevel9', 'mobilephone']\n\nind_ordered = ['rez_esc', 'escolari', 'age']\n\nhh_bool = ['hacdor', 'hacapo', 'v14a', 'refrig', 'paredblolad', 'paredzocalo', \n           'paredpreb','pisocemento', 'pareddes', 'paredmad',\n           'paredzinc', 'paredfibras', 'paredother', 'pisomoscer', 'pisoother', \n           'pisonatur', 'pisonotiene', 'pisomadera',\n           'techozinc', 'techoentrepiso', 'techocane', 'techootro', 'cielorazo', \n           'abastaguadentro', 'abastaguafuera', 'abastaguano',\n            'public', 'planpri', 'noelec', 'coopele', 'sanitario1', \n           'sanitario2', 'sanitario3', 'sanitario5',   'sanitario6',\n           'energcocinar1', 'energcocinar2', 'energcocinar3', 'energcocinar4', \n           'elimbasu1', 'elimbasu2', 'elimbasu3', 'elimbasu4', \n           'elimbasu5', 'elimbasu6', 'epared1', 'epared2', 'epared3',\n           'etecho1', 'etecho2', 'etecho3', 'eviv1', 'eviv2', 'eviv3', \n           'tipovivi1', 'tipovivi2', 'tipovivi3', 'tipovivi4', 'tipovivi5', \n           'computer', 'television', 'lugar1', 'lugar2', 'lugar3',\n           'lugar4', 'lugar5', 'lugar6', 'area1', 'area2']\n\nhh_ordered = [ 'rooms', 'r4h1', 'r4h2', 'r4h3', 'r4m1','r4m2','r4m3', 'r4t1',  'r4t2', \n              'r4t3', 'v18q1', 'tamhog','tamviv','hhsize','hogar_nin',\n              'hogar_adul','hogar_mayor','hogar_total',  'bedrooms', 'qmobilephone']\n\nhh_cont = ['v2a1', 'dependency', 'edjefe', 'edjefa', 'meaneduc', 'overcrowding']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check for redundant household variables\nheads = df_income_train.loc[df_income_train['parentesco1'] == 1, :]\nheads = heads[id_ + hh_bool + hh_cont + hh_ordered]\nheads.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create correlation matrix\ncorr_matrix = heads.corr()\n\n# Select upper triangle of correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n\n# Find index of feature columns with correlation greater than 0.95\nto_drop = [column for column in upper.columns if any(abs(upper[column]) > 0.95)]\n\nto_drop","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"['coopele', 'area2', 'tamhog', 'hhsize', 'hogar_total']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix.loc[corr_matrix['tamhog'].abs() > 0.9, corr_matrix['tamhog'].abs() > 0.9]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(corr_matrix.loc[corr_matrix['tamhog'].abs() > 0.9, corr_matrix['tamhog'].abs() > 0.9],\n            annot=True, cmap = plt.cm.Accent_r, fmt='.3f');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Note\nThere are several variables here having to do with the size of the house:<br>\n r4t3, Total persons in the household<br>\n tamhog, size of the household<br>\n tamviv, number of persons living in the household<br>\n hhsize, household size<br>\n hogar_total, # of total individuals in the household<br>\n These variables are all highly correlated with one another.<br>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cols=['tamhog', 'hogar_total', 'r4t3']\nfor df in [df_income_train, df_income_test]:\n    df.drop(columns = cols,inplace=True)\n\ndf_income_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check for redundant Individual variables\nind = df_income_train[id_ + ind_bool + ind_ordered]\nind.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create correlation matrix\ncorr_matrix = ind.corr()\n\n# Select upper triangle of correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n\n# Find index of feature columns with correlation greater than 0.95\nto_drop = [column for column in upper.columns if any(abs(upper[column]) > 0.95)]\n\nto_drop","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This is simply the opposite of male! We can remove the male flag.\nfor df in [df_income_train, df_income_test]:\n    df.drop(columns = 'male',inplace=True)\n\ndf_income_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets check area1 and area2 also\n# area1, =1 zona urbana \n# area2, =2 zona rural \n#area2 redundant because we have a column indicating if the house is in a urban zone\n\nfor df in [df_income_train, df_income_test]:\n    df.drop(columns = 'area2',inplace=True)\n\ndf_income_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Finally lets delete 'Id', 'idhogar'\ncols=['Id','idhogar']\nfor df in [df_income_train, df_income_test]:\n    df.drop(columns = cols,inplace=True)\n\ndf_income_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <span style='background:blue;color:white'> Predict the accuracy using random forest classifier. </span>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_income_train.iloc[:,0:-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_income_train.iloc[:,-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_features=df_income_train.iloc[:,0:-1] # feature without target\ny_features=df_income_train.iloc[:,-1] # only target\nprint(x_features.shape)\nprint(y_features.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score,confusion_matrix,f1_score,classification_report\n\nx_train,x_test,y_train,y_test=train_test_split(x_features,y_features,test_size=0.2,random_state=1)\nrmclassifier = RandomForestClassifier()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>x_features, y_features:</b> The first parameter is the dataset you're selecting to use.<br>\n<b>train_size</b>: This parameter sets the size of the training dataset. There are three options: None, which is the default, Int, which requires the exact number of samples, and float, which ranges from 0.1 to 1.0.<br>\n<b>test_size</b>: This parameter specifies the size of the testing dataset. The default state suits the training size. It will be set to 0.25 if the training size is set to default.<br>\n<b>random_state</b>: The default mode performs a random split using np.random. Alternatively, you can add an integer using an exact number.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rmclassifier.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_predict = rmclassifier.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(accuracy_score(y_test,y_predict))\nprint(confusion_matrix(y_test,y_predict))\nprint(classification_report(y_test,y_predict))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_predict_testdata = rmclassifier.predict(df_income_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_predict_testdata","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <span style='background:blue;color:white'> Check the accuracy using random forest with cross validation. </span>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold,cross_val_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seed=7\nkfold=KFold(n_splits=5,random_state=seed,shuffle=True)\n\nrmclassifier=RandomForestClassifier(random_state=10,n_jobs = -1)\nprint(cross_val_score(rmclassifier,x_features,y_features,cv=kfold,scoring='accuracy'))\nresults=cross_val_score(rmclassifier,x_features,y_features,cv=kfold,scoring='accuracy')\nprint(results.mean()*100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_trees= 100\n\nrmclassifier=RandomForestClassifier(n_estimators=100, random_state=10,n_jobs = -1)\nprint(cross_val_score(rmclassifier,x_features,y_features,cv=kfold,scoring='accuracy'))\nresults=cross_val_score(rmclassifier,x_features,y_features,cv=kfold,scoring='accuracy')\nprint(results.mean()*100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rmclassifier.fit(x_features,y_features)\nlabels = list(x_features)\nfeature_importances = pd.DataFrame({'feature': labels, 'importance': rmclassifier.feature_importances_})\nfeature_importances=feature_importances[feature_importances.importance>0.015]\nfeature_importances.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_predict_testdata = rmclassifier.predict(df_income_test)\ny_predict_testdata","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importances.sort_values(by=['importance'], ascending=True, inplace=True)\nfeature_importances['positive'] = feature_importances['importance'] > 0\nfeature_importances.set_index('feature',inplace=True)\nfeature_importances.head()\n\nfeature_importances.importance.plot(kind='barh', figsize=(11, 6),color = feature_importances.positive.map({True: 'blue', False: 'red'}))\nplt.xlabel('Importance')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above figure, meaneduc,dependency,overcrowding has significant influence on the model.\n<br>----THE END ---","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}