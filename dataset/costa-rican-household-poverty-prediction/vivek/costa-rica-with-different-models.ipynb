{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n#Calling Required Libraries\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport numpy as np\nimport pandas as pd\n\n#data visualization\nimport seaborn as sns\n\n#Data imputation\nfrom sklearn.impute import SimpleImputer\n\n#Label encoder to convert to dummy variables\nfrom sklearn.preprocessing import LabelEncoder\n\n# 1.2 Processing data\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import  OneHotEncoder as ohe\nfrom sklearn.preprocessing import StandardScaler as ss\nfrom sklearn.compose import ColumnTransformer as ct\nfrom sklearn.decomposition import PCA\n\n#To Balance the classes we use SMOTE Library\nfrom imblearn.over_sampling import SMOTE, ADASYN\n\n#Calling Libraries To Build various Models\nfrom sklearn.ensemble import RandomForestClassifier as rf\nfrom xgboost.sklearn import XGBClassifier\nimport lightgbm as lgb\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\n\n#For ROC graphs & metrics\nfrom sklearn.metrics import confusion_matrix\nimport sklearn.metrics as metrics\nfrom sklearn.metrics import accuracy_score\n\n#Bayesoptimization\nfrom skopt import BayesSearchCV\nimport xgboost as xgb\nfrom sklearn.model_selection import StratifiedKFold\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#Reading the dataset for training the model and validating it.\ntrain=pd.read_csv(\"../input/train.csv\")\n\n#Reading the final Test dataset on which the predictions are done\ntest=pd.read_csv(\"../input/test.csv\")\ntrain.shape\ntest.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.dtypes #to check the data types\ntrain.columns.values #to check the column names\n\n#To check the categorical variables\n[col for col, dt in train.dtypes.items() if dt=='object']#or below\nlist(set(train.columns) - set(train._get_numeric_data().columns))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Males 12 years of age and older\nsns.countplot(\"r4h2\",        # Variable whose distribution is of interest\n              hue= \"Target\",    # Distribution will be gender-wise\n              data = train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#number of persons living in the household\nsns.countplot(\"tamviv\",        # Variable whose distribution is of interest\n              hue= \"Target\",    # Distribution will be gender-wise\n              data = train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#if walls are good\nsns.countplot(\"epared3\",        # Variable whose distribution is of interest\n              hue= \"Target\",    # Distribution will be gender-wise\n              data = train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#toilet connected to  septic tank\nsns.countplot(\"sanitario3\",        # Variable whose distribution is of interest\n              hue= \"Target\",    # Distribution will be gender-wise\n              data = train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure()\nax = fig.add_subplot(111)\n\n#age of the household heads\nfamilyHead=train[train.parentesco1==1]\nax.bar(familyHead[\"Target\"],\n       familyHead[\"age\"],             # x-values or bar-locations\n                     # height of bars\n       color = \"lightblue\",       # inner-bar color (optional)\n       edgecolor=\"darkred\"        # optional\n       )  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#To find out which column have the null values in it\ntrain.isnull().sum()#to find the number of records with null values in each column\ntrain.isna().any()[lambda x: x]#to find the columns which have atleast one null value.\ntrain.columns[train.isnull().any()].tolist()#list of columns with atleast one null value.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"meanimp=SimpleImputer(strategy=\"mean\")\nmodeimp=SimpleImputer(strategy=\"most_frequent\")\n\n#DATA IMPUTATION FOR TRAIN DATASET\ntrain['v2a1'] = meanimp.fit_transform(train[['v2a1']])\ntrain['v18q1'] = modeimp.fit_transform(train[['v18q1']])\ntrain['rez_esc'] = modeimp.fit_transform(train[['rez_esc']])\ntrain['meaneduc'] = meanimp.fit_transform(train[['meaneduc']])\ntrain['SQBmeaned'] = meanimp.fit_transform(train[['SQBmeaned']])\n\n#DATA IMPUTATION FOR TEST DATASET\ntest['v2a1'] = meanimp.fit_transform(test[['v2a1']])\ntest['v18q1'] = modeimp.fit_transform(test[['v18q1']])\ntest['rez_esc'] = modeimp.fit_transform(test[['rez_esc']])\ntest['meaneduc'] = meanimp.fit_transform(test[['meaneduc']])\ntest['SQBmeaned'] = meanimp.fit_transform(test[['SQBmeaned']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#COVERTING CATEGORICAL TO NUMERICAL IN TRAIN DATASET\ntrain['dependency']=train['dependency'].replace({'yes': 1, 'no': 0}).astype(np.float32)\ntrain['edjefe']=train['edjefe'].replace({'yes': 1, 'no': 0}).astype(np.float32)\ntrain['edjefa']=train['edjefa'].replace({'yes': 1, 'no': 0}).astype(np.float32)\n\n#COVERTING CATEGORICAL TO NUMERICAL IN TEST DATASET\ntest['dependency']=test['dependency'].replace({'yes': 1, 'no': 0}).astype(np.float32)\ntest['edjefe']=test['edjefe'].replace({'yes': 1, 'no': 0}).astype(np.float32)\ntest['edjefa']=test['edjefa'].replace({'yes': 1, 'no': 0}).astype(np.float32)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.shape#(23856, 142)\ntrain.shape#(9557, 143)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#To drop the unnecessary columns(unique columns) in Train and Test datasets\ntrain.drop(columns=['Id','idhogar'],inplace=True)#(23856, 140)\ntest.drop(columns=['Id','idhogar'],inplace=True)#(9557, 141)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.shape#(23856, 140)\ntrain.shape#(9557, 141)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Seperating the target variable from predicting variables. \nX = train.iloc[:,0:140]\ny = train.iloc[:,140]\nX.shape\ny.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"PCA PROCESSING\"\"\"\n#using standard scaler standardize the modified train data\nscale = ss()\nX = scale.fit_transform(X)\n#Applying the above logic for test data set also\nfinalTest=scale.fit_transform(test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check the shape of the data after applying PCA.\nX.shape\nfinalTest.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Splitting the trained dataset to build the models\n\nX_train, X_test, y_train, y_test =   train_test_split(X,y,test_size = 0.3, stratify = y)\nX_train.shape\nX_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Building Random forest model\n#  RandomForest modeling\nfrom sklearn.ensemble import RandomForestClassifier as rf\n#Instantiate RandomForest classifier\nrandForModel = rf(n_estimators=50)\nrandForModel = randForModel.fit(X_train, y_train)\nrfmodel = randForModel.predict(X_test)\n# ChecK accuracy\n(rfmodel == y_test).sum()/y_test.size   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Applying Bayesian search approach to tune the model further\n\nbayes_cv_tuner = BayesSearchCV(\n    #  Place your estimator here with those parameter values\n    #      that you DO NOT WANT TO TUNE\n    rf(\n       n_jobs = 2         # No need to tune this parameter value\n      ),\n\n    # 2.12 Specify estimator parameters that you would like to change/tune\n    {\n        'n_estimators': (50, 200),           # Specify integer-values parameters like this\n        'criterion': ['gini', 'entropy'],     # Specify categorical parameters as here\n        'max_depth': (4, 50),                # integer valued parameter\n        'max_features' : (10,64),             # integer-valued parameter\n        'min_weight_fraction_leaf' : (0,0.5, 'uniform')   # Float-valued parameter\n    },\n\n    # 2.13\n    n_iter=20,            # How many points to sample\n    cv = 3                # Number of cross-validation folds\n)\n  \nbayes_cv_tuner.fit(X_train, y_train)\n#To get the best parameter set after bayesian optimization\nbayes_cv_tuner.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Building the random forest model using the best params.\nmodelrfTuned=rf(criterion=\"entropy\",\n               max_depth=21,\n               max_features=64,\n               min_weight_fraction_leaf=0.0,\n               n_estimators=205)\n    \nmodelrfTuned = modelrfTuned.fit(X_train, y_train)\nrfTunedModel=modelrfTuned.predict(X_test)\n(rfTunedModel == y_test).sum()/y_test.size ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Modeling with KNN \nmodelKNN = KNeighborsClassifier(n_neighbors=1)\nmodelKNN1 = modelKNN.fit(X_train, y_train)\nknnModel = modelKNN1.predict(X_test)\n(knnModel == y_test).sum()/y_test.size ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Predicting Test dataset with the tuned rf model\nrfTunedTestdata=modelrfTuned.predict(finalTest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Applying Bayesian search approach to tune the model further\nbayes_cv_tuner = BayesSearchCV(\n    #  Place your estimator here with those parameter values\n    #      that you DO NOT WANT TO TUNE\n    KNeighborsClassifier(\n       n_neighbors=4         # No need to tune this parameter value\n      ),\n    {\"metric\": [\"euclidean\", \"cityblock\"]},\n    n_iter=32,            # How many points to sample\n    cv = 2            # Number of cross-validation folds\n   )\nbayes_cv_tuner.fit(X_train, y_train)\nbayes_cv_tuner.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" #fitting the data for the tuned model\nmodelKNN = KNeighborsClassifier(n_neighbors=1,metric= 'cityblock')\nmodelKNN1 = modelKNN.fit(X_train, y_train)\n#predicting the X_test with tuned model of KNN\nknnTunedMdl = modelKNN1.predict(X_test)\n(knnTunedMdl == y_test).sum()/y_test.size ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Predicting Test dataset with the tuned rf model\nknnTunedTestdata=modelKNN1.predict(finalTest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#3 Modeling using XGB Classifier\nxgbModel=XGBClassifier()\nmodelXGB=xgbModel.fit(X_train, y_train)\nxgbModel = modelXGB.predict(X_test)\n(xgbModel == y_test).sum()/y_test.size ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Applying Bayesian search approach to tune the model further\nbayes_cv_tuner = BayesSearchCV(\n    estimator = XGBClassifier(\n        n_jobs = 1,\n        objective = 'binary:logistic',\n        eval_metric = 'auc',\n        silent=1,\n        tree_method='approx'\n    ),\n    search_spaces = {\n        'learning_rate': (0.01, 1.0, 'log-uniform'),\n        'min_child_weight': (0, 10),\n        'max_depth': (0, 50),\n        'max_delta_step': (0, 20),\n        'subsample': (0.01, 1.0, 'uniform'),\n        'colsample_bytree': (0.01, 1.0, 'uniform'),\n        'colsample_bylevel': (0.01, 1.0, 'uniform'),\n        'reg_lambda': (1e-9, 1000, 'log-uniform'),\n        'reg_alpha': (1e-9, 1.0, 'log-uniform'),\n        'gamma': (1e-9, 0.5, 'log-uniform'),\n        'min_child_weight': (0, 5),\n        'n_estimators': (50, 100),\n        'scale_pos_weight': (1e-6, 500, 'log-uniform')\n    },    \n\n    cv = 2,\n    n_jobs = 3,\n    n_iter = 10,   \n    verbose = 0,\n    refit = True,\n    random_state = 42\n)\nbayes_cv_tuner.fit(X_train, y_train)\nbayes_cv_tuner.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#predicting the X_test with tuned model of XGB.\nxgbTunedModel = bayes_cv_tuner.predict(X_test)\n(xgbTunedModel == y_test).sum()/y_test.size ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Predicting Test dataset with the tuned xgb model\nxgbTunedTestdata=bayes_cv_tuner.predict(finalTest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#LIGHT GBM CLASSIFIER\nimport lightgbm as lgb\nmodelLGBM= lgb.LGBMClassifier()\nLGBMModel=modelLGBM.fit(X_train, y_train)\nlbgModel=LGBMModel.predict(X_test)\n(lbgModel == y_test).sum()/y_test.size ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Applying Bayesian search approach to tune the model further\nbayes_cv_tuner = BayesSearchCV(\n    #  Place your estimator here with those parameter values\n    #      that you DO NOT WANT TO TUNE\n    lgb.LGBMClassifier(\n       n_jobs = 2         # No need to tune this parameter value\n      ),\n\n    # 2.12 Specify estimator parameters that you would like to change/tune\n    {\n        'n_estimators': (50, 100),           # Specify integer-values parameters like this\n        'criterion': ['gini', 'entropy'],     # Specify categorical parameters as here\n        'max_depth': (4, 50),                # integer valued parameter\n        'max_features' : (10,64),             # integer-valued parameter\n        'min_weight_fraction_leaf' : (0,0.5, 'uniform')   # Float-valued parameter\n    },\n\n    # 2.13\n    n_iter=32,            # How many points to sample\n    cv = 3                # Number of cross-validation folds\n)\n    \nbayes_cv_tuner.fit(X_train, y_train) \n#To get the best parameter set after bayesian optimization\nbayes_cv_tuner.best_params_\n   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#predicting the X_test with tuned model of LGB.\nlgbTunedModel=bayes_cv_tuner.predict(X_test)\n(lgbTunedModel == y_test).sum()/y_test.size ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Predicting Test dataset with the tuned LBGM model\nlgbmTunedTestdata=bayes_cv_tuner.predict(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#FINAL MODEL FROM ALL THE DERIVED TUNED MODELS\n#Concatenating all the predicted results and form as a dataframe\ndictOfMdls = {'rfTunedModel': rfTunedModel,'xgbTunedModel':xgbTunedModel,'knnTunedMdl':knnTunedMdl, 'lgbTunedModel': lgbTunedModel}\ntundedDF=pd.DataFrame(data=dictOfMdls)\ntundedDF.shape\nrfmdl=rf(n_estimators=50)\nrfFinalMdl=rfmdl.fit(tundedDF, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Concatenating all the predicted results of final Test data and form as a dataframe\ndictOfTestMdls = {'rfTunedModel': rfTunedTestdata,'xgbTunedModel':xgbTunedTestdata,'knnTunedMdl':knnTunedTestdata, 'lgbTunedModel': lgbmTunedTestdata}\ntundedTestDF=pd.DataFrame(data=dictOfTestMdls)\n\n#Predicting the resultant column for the above using the model created from tuned models of train data.\nfinalTestDataPrdict=rfFinalMdl.predict(tundedTestDF)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testDataset=pd.read_csv(\"../input/test.csv\")\n#Saving the csv file in local with id and target column for final test dataset.\nResultDF=pd.DataFrame(data={'Id':testDataset['Id'],'Target':finalTestDataPrdict})\nResultDF.to_csv('result.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}