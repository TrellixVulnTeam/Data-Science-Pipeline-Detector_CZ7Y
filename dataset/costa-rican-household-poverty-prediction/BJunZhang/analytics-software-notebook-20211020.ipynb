{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-19T19:01:16.201002Z","iopub.execute_input":"2021-10-19T19:01:16.201943Z","iopub.status.idle":"2021-10-19T19:01:16.232924Z","shell.execute_reply.started":"2021-10-19T19:01:16.201822Z","shell.execute_reply":"2021-10-19T19:01:16.232077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport plotly.offline as py\nimport plotly.figure_factory as ff\nimport plotly.graph_objs as go\nimport plotly.io as pio\nimport plotly.express as px\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier,VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import auc,roc_curve,accuracy_score, f1_score, roc_auc_score, log_loss, classification_report, plot_confusion_matrix, recall_score, precision_score, matthews_corrcoef,make_scorer\nfrom sklearn.utils.class_weight import compute_class_weight\n\nfrom imblearn.ensemble import EasyEnsembleClassifier\nfrom imblearn.over_sampling import RandomOverSampler,SMOTE, ADASYN, BorderlineSMOTE, KMeansSMOTE\nfrom imblearn.under_sampling import ClusterCentroids,RandomUnderSampler\n\nimport xgboost as xgb\nfrom xgboost.sklearn import XGBClassifier\nfrom xgboost import plot_importance\nimport lightgbm as lgb\nfrom catboost import CatBoostClassifier\nfrom sklearn.neural_network import MLPClassifier\n\nfrom statistics import mean\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline\nplt.style.use('seaborn-whitegrid')","metadata":{"execution":{"iopub.status.busy":"2021-10-19T19:08:23.058322Z","iopub.execute_input":"2021-10-19T19:08:23.058664Z","iopub.status.idle":"2021-10-19T19:08:27.470566Z","shell.execute_reply.started":"2021-10-19T19:08:23.05863Z","shell.execute_reply":"2021-10-19T19:08:27.469674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_rows', 150)\npd.options.display.max_columns = 150\ntrain = pd.read_csv('/kaggle/input/costa-rican-household-poverty-prediction/train.csv')\ntest = pd.read_csv('/kaggle/input/costa-rican-household-poverty-prediction/test.csv')\ntrain.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-10-19T19:10:55.156247Z","iopub.execute_input":"2021-10-19T19:10:55.156581Z","iopub.status.idle":"2021-10-19T19:10:55.758926Z","shell.execute_reply.started":"2021-10-19T19:10:55.156545Z","shell.execute_reply":"2021-10-19T19:10:55.757863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Train set info:\")\ntrain.info()\nprint('\\n')\nprint(\"Test set info:\")\ntest.info()","metadata":{"execution":{"iopub.status.busy":"2021-10-19T19:11:21.454162Z","iopub.execute_input":"2021-10-19T19:11:21.454843Z","iopub.status.idle":"2021-10-19T19:11:21.495189Z","shell.execute_reply.started":"2021-10-19T19:11:21.454786Z","shell.execute_reply":"2021-10-19T19:11:21.494134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unique_count = []\ncol = []\nfor i in list(train.columns):\n    unique_count.append(train[i].nunique())\n    col.append(i)\n\nrow = ['Unique Values']\npd.DataFrame([unique_count],row,col)","metadata":{"execution":{"iopub.status.busy":"2021-10-19T19:11:40.514502Z","iopub.execute_input":"2021-10-19T19:11:40.514769Z","iopub.status.idle":"2021-10-19T19:11:40.583607Z","shell.execute_reply.started":"2021-10-19T19:11:40.514741Z","shell.execute_reply":"2021-10-19T19:11:40.582519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Distribution of the target, imbalanced multiclass classification\ntrain['Target'].value_counts().plot(kind = \"bar\",color = '#4399c6')\nc1 = train['Target'].value_counts()[1]\nc2 = train['Target'].value_counts()[2]\nc3 = train['Target'].value_counts()[3]\nc4 = train['Target'].value_counts()[4]\n\nprint('Extreme poverty(1):  %d %.2f%%' % (c1,c1/(c1+c2+c3+c4)*100))\nprint('Moderate poverty(2):  %d %.2f%%' % (c2,c2/(c1+c2+c3+c4)*100))\nprint('Vulnerable households(3):  %d %.2f%%' % (c3,c3/(c1+c2+c3+c4)*100))\nprint('Non vulnerable households(4):  %d %.2f%%' % (c4,c4/(c1+c2+c3+c4)*100))","metadata":{"execution":{"iopub.status.busy":"2021-10-19T19:12:16.620206Z","iopub.execute_input":"2021-10-19T19:12:16.6211Z","iopub.status.idle":"2021-10-19T19:12:16.870989Z","shell.execute_reply.started":"2021-10-19T19:12:16.621063Z","shell.execute_reply":"2021-10-19T19:12:16.870293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from the observation, dependency, edjefe, edjefa can be converted into numerical features\ntrain.select_dtypes('object').head()","metadata":{"execution":{"iopub.status.busy":"2021-10-19T19:12:26.417381Z","iopub.execute_input":"2021-10-19T19:12:26.41794Z","iopub.status.idle":"2021-10-19T19:12:26.430724Z","shell.execute_reply.started":"2021-10-19T19:12:26.417903Z","shell.execute_reply":"2021-10-19T19:12:26.429807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert dependency, edjefe, edjefa to numerical features in both train and test set\n# yes -> 1, no -> 0\n\nmapping = {\"yes\": 1, \"no\": 0}\n\nfor i in [train,test]:\n    i['dependency'] = i['dependency'].replace(mapping).astype(np.float64)\n    i['edjefe'] = i['edjefe'].replace(mapping).astype(np.float64)\n    i['edjefa'] = i['edjefa'].replace(mapping).astype(np.float64)\n    \n#train[['dependency', 'edjefa', 'edjefe']].describe()","metadata":{"execution":{"iopub.status.busy":"2021-10-19T19:12:35.34622Z","iopub.execute_input":"2021-10-19T19:12:35.346532Z","iopub.status.idle":"2021-10-19T19:12:35.384073Z","shell.execute_reply.started":"2021-10-19T19:12:35.346499Z","shell.execute_reply":"2021-10-19T19:12:35.383329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_float = list(train.select_dtypes(include = ['float64']).columns)\ntrain_int = list(train.select_dtypes(include = ['int64']).columns)\ntrain_str = list(train.select_dtypes(include = ['object']).columns)\ntrain_int.remove(\"Target\")","metadata":{"execution":{"iopub.status.busy":"2021-10-19T19:12:57.75789Z","iopub.execute_input":"2021-10-19T19:12:57.758755Z","iopub.status.idle":"2021-10-19T19:12:57.770548Z","shell.execute_reply.started":"2021-10-19T19:12:57.758717Z","shell.execute_reply":"2021-10-19T19:12:57.769367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Unify the target under same household with different labels, take head of household as the correct label\n\n# Retrieve the list of the households with different target labels\ntarget_bool = train.groupby('idhogar')['Target'].apply(lambda x: x.nunique() == 1)\nhousehold_false = target_bool[target_bool==False].index.values\n#train[train['idhogar'] == household_false[0]][['idhogar', 'parentesco1', 'Target']]\n\n# Unify the target label under same household \nfor i in household_false:\n    correct_target = train[(train['idhogar'] == i) & (train['parentesco1'] == 1.0)]['Target'].values[0]\n    train.loc[train['idhogar'] == i, 'Target'] = correct_target","metadata":{"execution":{"iopub.status.busy":"2021-10-19T19:13:06.616051Z","iopub.execute_input":"2021-10-19T19:13:06.616948Z","iopub.status.idle":"2021-10-19T19:13:07.007733Z","shell.execute_reply.started":"2021-10-19T19:13:06.616898Z","shell.execute_reply":"2021-10-19T19:13:07.006733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check, all household unified\ntrain.groupby('idhogar')['Target'].apply(lambda x: x.nunique() == 1).value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-10-19T19:13:14.024279Z","iopub.execute_input":"2021-10-19T19:13:14.024586Z","iopub.status.idle":"2021-10-19T19:13:14.214874Z","shell.execute_reply.started":"2021-10-19T19:13:14.02455Z","shell.execute_reply":"2021-10-19T19:13:14.213963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check missing values\nrow = []\nfor i in list(train.columns):\n    row.append(i)\n\nmissing_value = train.isnull().sum().values\ncol = ['Train_total']\n\ntest_dup = test.copy(deep=True)\ntest_dup['Target'] = 0\n\nmissing = pd.DataFrame(missing_value,row,col)\nmissing['Train_percent'] = round(missing['Train_total'] / len(train),4)\nmissing['Test_total'] = test_dup.isnull().sum().values\nmissing['Test_percent'] = round(missing['Test_total'] / len(test_dup),4)\nmissing['Total'] = missing['Train_total'] + missing['Test_total']\nmissing['Percent'] = round(missing['Total'] / (len(train) + len(test_dup)),4)\n\nmissing.sort_values('Percent', ascending = False).head(10)","metadata":{"execution":{"iopub.status.busy":"2021-10-19T19:13:25.808325Z","iopub.execute_input":"2021-10-19T19:13:25.80868Z","iopub.status.idle":"2021-10-19T19:13:25.855404Z","shell.execute_reply.started":"2021-10-19T19:13:25.808644Z","shell.execute_reply":"2021-10-19T19:13:25.854523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Handle part of missing values, imputation will be carried out in the CV phase to choose the best imputation method\n\n# The missing values in v18q1 are those families without owning the tablet verified by v18q = 0\n# So, simply replace na in v18q1 with 0 in both train and test set\ntrain['v18q1'] = train['v18q1'].fillna(0)\ntest['v18q1'] = test['v18q1'].fillna(0)\n\n# Some missing values in v2a1 are those families owning their own houses verified by tipovivi1 = 1\n# So, simply replace those na in v2a1 with 0 in both train and test set\n# The rest of na will be imputated\ntrain.loc[(train['tipovivi1'] == 1), 'v2a1'] = 0\ntest.loc[(test['tipovivi1'] == 1), 'v2a1'] = 0\n\n# For missing values in rez_esc, as defined by the competition, it is only available for those age in between 7 and 19\n# So, replace those na which are not in this age range\n# The rest of na will be imputated\ntrain.loc[((train['age'] > 19) | (train['age'] < 7)) & (train['rez_esc'].isnull()), 'rez_esc'] = 0\ntest.loc[((train['age'] > 19) | (test['age'] < 7)) & (test['rez_esc'].isnull()), 'rez_esc'] = 0","metadata":{"execution":{"iopub.status.busy":"2021-10-19T19:13:38.9444Z","iopub.execute_input":"2021-10-19T19:13:38.945229Z","iopub.status.idle":"2021-10-19T19:13:38.975836Z","shell.execute_reply.started":"2021-10-19T19:13:38.945174Z","shell.execute_reply":"2021-10-19T19:13:38.974943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Recheck missing values\nrow = []\nfor i in list(train.columns):\n    row.append(i)\n\nmissing_value = train.isnull().sum().values\ncol = ['Train_total']\n\ntest_dup = test.copy(deep=True)\ntest_dup['Target'] = 0\n\nmissing = pd.DataFrame(missing_value,row,col)\nmissing['Train_percent'] = round(missing['Train_total'] / len(train),4)\nmissing['Test_total'] = test_dup.isnull().sum().values\nmissing['Test_percent'] = round(missing['Test_total'] / len(test_dup),4)\nmissing['Total'] = missing['Train_total'] + missing['Test_total']\nmissing['Percent'] = round(missing['Total'] / (len(train) + len(test_dup)),4)\n\nmissing.sort_values('Percent', ascending = False).head(5)","metadata":{"execution":{"iopub.status.busy":"2021-10-19T19:13:47.023132Z","iopub.execute_input":"2021-10-19T19:13:47.023771Z","iopub.status.idle":"2021-10-19T19:13:47.069495Z","shell.execute_reply.started":"2021-10-19T19:13:47.023721Z","shell.execute_reply":"2021-10-19T19:13:47.068827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Heat map\ntrain_heat = train.drop(columns = list(train.select_dtypes(include = ['object']).columns))\nprint(\"Correlation Matrix\",\"\\n\")    \ncorrelation=train_heat.corr(method=\"pearson\")\nplt.figure(figsize=(25,10))\nsns.heatmap(correlation,vmax=1,square=True,annot=False)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-19T19:13:59.422428Z","iopub.execute_input":"2021-10-19T19:13:59.422998Z","iopub.status.idle":"2021-10-19T19:14:01.731689Z","shell.execute_reply.started":"2021-10-19T19:13:59.422965Z","shell.execute_reply":"2021-10-19T19:14:01.730524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def correl(df,thresh):\n    cor = df.corr()\n    corrm = np.corrcoef(df.transpose())\n    corr = corrm - np.diagflat(corrm.diagonal())\n    print(\"max corr:\",corr.max(), \", min corr: \", corr.min())\n    c1 = cor.abs().unstack().sort_values(ascending=False)\n    high_cor = c1[c1.values!=1]        \n    print(high_cor[high_cor>thresh].head(100))\ncorrel(train_heat,0.8)","metadata":{"execution":{"iopub.status.busy":"2021-10-19T19:14:10.694517Z","iopub.execute_input":"2021-10-19T19:14:10.694882Z","iopub.status.idle":"2021-10-19T19:14:11.249648Z","shell.execute_reply.started":"2021-10-19T19:14:10.694848Z","shell.execute_reply":"2021-10-19T19:14:11.246703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Without any leakage from validation set in the CV and test set ! ! !\n# It is always true that the squared features which are highly correlated with its corresponding base features\n# Also, the male completely coorelates with female, area1 completely coorelates with area2, they are either-or relationships\nredundant_feature = ['SQBescolari', 'SQBage', 'SQBhogar_total', 'SQBedjefe', \n        'SQBhogar_nin', 'SQBovercrowding', 'SQBdependency', 'SQBmeaned', 'agesq','female','area2']\n\ntrain_drop = train.drop(columns = redundant_feature)\ntest_drop = test.drop(columns = redundant_feature)\n\n# Rename male to sex, 1 is male and 0 is female\ntrain_drop.rename({\"male\": \"sex\"}, axis=1, inplace=True)\ntest_drop.rename({\"male\": \"sex\"}, axis=1, inplace=True)\n\n# Rename area1 to area, 1 is live in urban and 0 is live in rural\ntrain_drop.rename({\"area1\": \"area\"}, axis=1, inplace=True)\ntest_drop.rename({\"area1\": \"area\"}, axis=1, inplace=True)\n\nprint(train_drop.shape,test_drop.shape)","metadata":{"execution":{"iopub.status.busy":"2021-10-19T19:14:25.777537Z","iopub.execute_input":"2021-10-19T19:14:25.77785Z","iopub.status.idle":"2021-10-19T19:14:25.800162Z","shell.execute_reply.started":"2021-10-19T19:14:25.77781Z","shell.execute_reply":"2021-10-19T19:14:25.799258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert idhogar to new features (feature engineering) which are aggregated statistics of the individual under same family\n# Which can then uniquely identify each household\n\n# Features for individual except rez_esc with too many missing values\nindividual_feature = ['v18q', 'dis', 'sex', 'estadocivil1', 'estadocivil2', 'estadocivil3', \n            'estadocivil4', 'estadocivil5', 'estadocivil6', 'estadocivil7', \n            'parentesco1', 'parentesco2',  'parentesco3', 'parentesco4', 'parentesco5', \n            'parentesco6', 'parentesco7', 'parentesco8',  'parentesco9', 'parentesco10', \n            'parentesco11', 'parentesco12', 'instlevel1', 'instlevel2', 'instlevel3', \n            'instlevel4', 'instlevel5', 'instlevel6', 'instlevel7', 'instlevel8', \n            'instlevel9', 'mobilephone', 'escolari', 'age','idhogar']\n\ntrain_ind = train_drop[individual_feature]\ntest_ind = test_drop[individual_feature]\n\ntrain_agg = train_ind.groupby('idhogar').agg(['min', 'max', 'sum', 'mean', 'count'])\ntest_agg = test_ind.groupby('idhogar').agg(['min', 'max', 'sum', 'mean', 'count'])\n\n# Rename the columns\ncol_rename = []\nfor i in train_agg.columns.levels[0]:\n    for j in train_agg.columns.levels[1]:\n        col_rename.append(f'{i}-{j}')\n        \ntrain_agg.columns = col_rename\ntest_agg.columns = col_rename\n\n# Merge the dataframes, drop id and idhogar\n\ny = train_drop['Target']\ntrain_merge = train_drop.merge(train_agg, on = 'idhogar', how = 'left').drop(columns = ['Id','idhogar','Target'])\ntest_merge = test_drop.merge(test_agg, on = 'idhogar', how = 'left').drop(columns = ['Id','idhogar'])","metadata":{"execution":{"iopub.status.busy":"2021-10-19T19:14:47.86539Z","iopub.execute_input":"2021-10-19T19:14:47.865804Z","iopub.status.idle":"2021-10-19T19:14:48.700537Z","shell.execute_reply.started":"2021-10-19T19:14:47.865774Z","shell.execute_reply":"2021-10-19T19:14:48.699385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Xgboost + LightGBM\nseed = np.random.seed(2021)\n\n\ntrain_x = train_merge.copy(deep=True)\ntrain_y = y.copy(deep=True)\ntest_x = test_merge.copy(deep=True)\n\n\n# Imputation, choose different strategies and see which one is better\nstrategy = ['mean','median','most_frequent']\nimp = SimpleImputer(strategy = strategy[1])\nna_feature = ['rez_esc','v2a1','meaneduc']\nfor i in na_feature:\n    train_x[i] = imp.fit_transform(train_x[i].values.reshape(-1,1))[:,0]\n    test_x[i] = imp.transform(test_x[i].values.reshape(-1,1))[:,0]\n    \n# Feature selection: remove features with low variance\nsel = VarianceThreshold(threshold=0.01)\nsel_var=sel.fit_transform(train_x)\ntrain_x_lv = train_x[train_x.columns[sel.get_support(indices=True)]]\ntest_x_lv = test_x[train_x_lv.columns]\n\n# Feature selection: remove one of the highly correlated features\ncorrelation_train = train_x_lv.corr()\nupper = correlation_train.where(np.triu(np.ones(correlation_train.shape), k=1).astype(np.bool))\nto_drop = [column for column in upper.columns if any(abs(upper[column]) > 0.8)]\ntrain_x_cf = train_x_lv.drop(columns = to_drop)\ntest_x_cf = test_x_lv.drop(columns = to_drop)\n\nxgb_params = {'learning_rate': 0.2, 'n_estimators': 400, 'min_child_weight': 3, \n               'subsample':1, 'colsample_bytree': 1, 'gamma': 0.1, 'reg_alpha': 0, 'reg_lambda': 1,\n               'objective':'multi:softprob','num_class':4,'verbosity':0}\nxgb = XGBClassifier(**xgb_params)\n\nlgbm_params = {\n    'objective': 'multiclass',  \n    'nthread': -1, #use all\n    'verbose': -1,\n    'num_iterations' : 200, \n    'learning_rate':0.1,\n    'class_weight':'balanced',\n    'random_state': 2021,\n    'boosting_type':'gbdt', # Tuned Parameter\n    'max_depth':8,\n    'num_leaves':100,\n    'min_child_samples':10,\n    'min_child_weight':0.001,\n    'feature_fraction':0.9,\n    'bagging_fraction':0.9,\n    'bagging_freq':2\n}\nlgbm = lgb.LGBMClassifier(**lgbm_params)\n\n\nvoting = VotingClassifier(estimators=[(\"XGBoost\", xgb),\n                                     (\"LightGBM\",lgbm)], voting='soft',n_jobs=-1)\n\nvoting.fit(train_x_cf,train_y)\nsubmission = pd.DataFrame()\nsubmission['Id'] = test['Id']\nsubmission['Target'] = voting.predict(test_x_cf)\nprint(submission)","metadata":{"execution":{"iopub.status.busy":"2021-10-19T19:26:48.056481Z","iopub.execute_input":"2021-10-19T19:26:48.05709Z","iopub.status.idle":"2021-10-19T19:27:36.084956Z","shell.execute_reply.started":"2021-10-19T19:26:48.05704Z","shell.execute_reply":"2021-10-19T19:27:36.084009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2021-10-19T19:27:53.43891Z","iopub.execute_input":"2021-10-19T19:27:53.439362Z","iopub.status.idle":"2021-10-19T19:27:53.473703Z","shell.execute_reply.started":"2021-10-19T19:27:53.439301Z","shell.execute_reply":"2021-10-19T19:27:53.473065Z"},"trusted":true},"execution_count":null,"outputs":[]}]}