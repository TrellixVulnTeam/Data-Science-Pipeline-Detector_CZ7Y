{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Costa Rican Household Poverty Level Prediction "},{"metadata":{},"cell_type":"markdown","source":"지도학습이자 4중분류의 분류문제이며 예측하고자 하는 대상은 빈곤도를 나타내는 4개의 등급 (1단계가 최빈곤)"},{"metadata":{},"cell_type":"markdown","source":"주어진 몇가지 column을 살펴보면  \n  \nidhogar : 각각의 가구를 구별해주는 unique한 요소이며 같은 가구내에 있는 구성원들은 같은 idhogar를 갖는다  \nparentesco1 : 가장인지의 여부를 알려준다  \n  \nMetric : 점수 평가방식은 Macro F1 score (Class가 여러개인 경우 각 Class별 Macro F1 score의 평균)  \n  \nfrom sklearn.metrics import f1_score  \nf1_score(y_true, y_predicted, average = 'macro`)  \n  \n  \nlabel(빈곤도 등급)이 균일하지않기 때문에 이를 감안하여 분석을 진행해야한다.\n\n"},{"metadata":{},"cell_type":"markdown","source":"# 분석 Roadmap\n  \n1. 문제에 대한 이해 \n2. 데이터 탐색\n3. 머신러닝을 활용하기 위한 Feature engineering\n4. 여러 기본적인 머신러닝 모델간의 비교\n5. 보다 복잡한 머신러닝 모델의 비교 \n6. 모델 최적화\n7. 모델을 통한 예측과 분석\n8. 결론 도출\n  "},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt \nimport seaborn as sns ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.options.display.max_columns  = None # 모든 column 확인","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train.info()\ntrain.info(verbose = 1, null_counts = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.info(verbose = 1, null_counts = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.select_dtypes(np.int64).nunique().value_counts().sort_index().plot.bar(color = 'blue', \n                                                                             figsize = (8, 6),\n                                                                            edgecolor = 'k', linewidth = 2);\nplt.xlabel('Number of Unique Values'); plt.ylabel('Count');\nplt.title('Count of Unique Values in Integer Columns');","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"from collections import OrderedDict\n\nplt.figure(figsize = (20, 16))\nplt.style.use('fivethirtyeight')\n\n# Color mapping\ncolors = OrderedDict({1: 'red', 2: 'orange', 3: 'blue', 4: 'green'})\npoverty_mapping = OrderedDict({1: 'extreme', 2: 'moderate', 3: 'vulnerable', 4: 'non vulnerable'})\n\n# Iterate through the float columns\nfor i, col in enumerate(train.select_dtypes('float')):\n    ax = plt.subplot(4, 2, i + 1)\n    # Iterate through the poverty levels\n    for poverty_level, color in colors.items():\n        # Plot each poverty level as a separate line\n        sns.kdeplot(train.loc[train['Target'] == poverty_level, col].dropna(), \n                    ax = ax, color = color, label = poverty_mapping[poverty_level])\n        \n    plt.title(f'{col.capitalize()} Distribution'); plt.xlabel(f'{col}'); plt.ylabel('Density')\n\nplt.subplots_adjust(top = 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# v2a1, Monthly rent payment\n# v18q1, number of tablets household owns\n# rez_esc, Years behind in school\n# meaneduc,average years of education for adults (18+)\n# overcrowding, # persons per room\n# SQBovercrowding, overcrowding squared\n# SQBdependency, dependency squared\n# SQBmeaned, square of the mean years of education of adults (>=18) in the household","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.select_dtypes('object').head() #dtype이 섞여 있음 ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mapping = {\"yes\": 1, \"no\": 0}\n\n# Apply same operation to both train and test\nfor df in [train, test]:\n    # Fill in the values with the correct mapping\n    df['dependency'] = df['dependency'].replace(mapping).astype(np.float64)\n    df['edjefa'] = df['edjefa'].replace(mapping).astype(np.float64)\n    df['edjefe'] = df['edjefe'].replace(mapping).astype(np.float64)\n\ntrain[['dependency', 'edjefa', 'edjefe']].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (16, 12))\n\n# Iterate through the float columns\nfor i, col in enumerate(['dependency', 'edjefa', 'edjefe']):\n    ax = plt.subplot(3, 1, i + 1)\n    # Iterate through the poverty levels\n    for poverty_level, color in colors.items():\n        # Plot each poverty level as a separate line\n        sns.kdeplot(train.loc[train['Target'] == poverty_level, col].dropna(), \n                    ax = ax, color = color, label = poverty_mapping[poverty_level])\n        \n    plt.title(f'{col.capitalize()} Distribution'); plt.xlabel(f'{col}'); plt.ylabel('Density')\n\nplt.subplots_adjust(top = 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add null Target column to test\ntest['Target'] = np.nan\ndata = train.append(test, ignore_index = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Heads of household\nheads = data.loc[data['parentesco1'] == 1].copy()\n\n# Labels for training\ntrain_labels = data.loc[(data['Target'].notnull()) & (data['parentesco1'] == 1), ['Target', 'idhogar']]\n\n# Value counts of target\nlabel_counts = train_labels['Target'].value_counts().sort_index()\n\n# Bar plot of occurrences of each label\nlabel_counts.plot.bar(figsize = (8, 6), \n                      color = colors.values(),\n                      edgecolor = 'k', linewidth = 2)\n# Formatting\nplt.xlabel('Poverty Level'); plt.ylabel('Count'); \nplt.xticks([x - 1 for x in poverty_mapping.keys()], \n           list(poverty_mapping.values()), rotation = 60)\nplt.title('Poverty Level Breakdown');\n\nlabel_counts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Groupby the household and figure out the number of unique values\nall_equal = train.groupby('idhogar')['Target'].apply(lambda x: x.nunique() == 1)\n\n# Households where targets are not all equal\nnot_equal = all_equal[all_equal != True]\nprint('There are {} households where the family members do not all have the same target.'.format(len(not_equal)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train['idhogar'] == not_equal.index[0]][['idhogar', 'parentesco1', 'Target']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"households_leader = train.groupby('idhogar')['parentesco1'].sum()\n\n# Find households without a head\nhouseholds_no_head = train.loc[train['idhogar'].isin(households_leader[households_leader == 0].index), :]\n\nprint('There are {} households without a head.'.format(households_no_head['idhogar'].nunique()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find households without a head and where labels are different\nhouseholds_no_head_equal = households_no_head.groupby('idhogar')['Target'].apply(lambda x: x.nunique() == 1)\nprint('{} Households with no head have different labels.'.format(sum(households_no_head_equal == False)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Iterate through each household\nfor household in not_equal.index:\n    # Find the correct label (for the head of household)\n    true_target = int(train[(train['idhogar'] == household) & (train['parentesco1'] == 1.0)]['Target'])\n    \n    # Set the correct label for all members in the household\n    train.loc[train['idhogar'] == household, 'Target'] = true_target\n    \n    \n# Groupby the household and figure out the number of unique values\nall_equal = train.groupby('idhogar')['Target'].apply(lambda x: x.nunique() == 1)\n\n# Households where targets are not all equal\nnot_equal = all_equal[all_equal != True]\nprint('There are {} households where the family members do not all have the same target.'.format(len(not_equal)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of missing in each column\nmissing = pd.DataFrame(data.isnull().sum()).rename(columns = {0: 'total'})\n\n# Create a percentage missing\nmissing['percent'] = missing['total'] / len(data)\n\nmissing.sort_values('percent', ascending = False).head(10).drop('Target')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"v18q1: Number of tablet"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_value_counts(df, col, heads_only = False):\n    \"\"\"Plot value counts of a column, optionally with only the heads of a household\"\"\"\n    # Select heads of household\n    if heads_only:\n        df = df.loc[df['parentesco1'] == 1].copy()\n        \n    plt.figure(figsize = (8, 6))\n    df[col].value_counts().sort_index().plot.bar(color = 'blue',\n                                                 edgecolor = 'k',\n                                                 linewidth = 2)\n    plt.xlabel(f'{col}') \n    plt.title(f'{col} Value Counts')\n    plt.ylabel('Count')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_value_counts(heads, 'v18q1')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"heads.groupby('v18q')['v18q1'].apply(lambda x: x.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['v18q1'] = data['v18q1'].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"v2a1 : 월세"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Variables indicating home ownership\nown_variables = [x for x in data if x.startswith('tipo')]\n\n\n# Plot of the home ownership variables for home missing rent payments\ndata.loc[data['v2a1'].isnull(), own_variables].sum().plot.bar(figsize = (10, 8),\n                                                                        color = 'green',\n                                                              edgecolor = 'k', linewidth = 2);\nplt.xticks([0, 1, 2, 3, 4],\n           ['Owns and Paid Off', 'Owns and Paying', 'Rented', 'Precarious', 'Other'],\n          rotation = 60)\nplt.title('Home Ownership Status for Households Missing Rent Payments', size = 18)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"tipovivi1, =1 own and fully paid house  \ntipovivi2, =1 own, paying in installments  \ntipovivi3, =1 rented  \ntipovivi4, =1 precarious  \ntipovivi5, =1 other(assigned,  borrowed)  "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fill in households that own the house with 0 rent payment\ndata.loc[(data['tipovivi1'] == 1), 'v2a1'] = 0\n\n# Create missing rent payment column\ndata['v2a1-missing'] = data['v2a1'].isnull()\n\ndata['v2a1-missing'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"rez_esc: years behind in school\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.loc[data['rez_esc'].notnull()]['age'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.loc[data['rez_esc'].isnull()]['age'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# If individual is over 19 or younger than 7 and missing years behind, set it to 0\ndata.loc[((data['age'] > 19) | (data['age'] < 7)) & (data['rez_esc'].isnull()), 'rez_esc'] = 0\n\n# Add a flag for those between 7 and 19 with a missing value\ndata['rez_esc-missing'] = data['rez_esc'].isnull()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.loc[data['rez_esc'] > 5, 'rez_esc'] = 5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_categoricals(x, y, data, annotate = True):\n    \"\"\"Plot counts of two categoricals.\n    Size is raw count for each grouping.\n    Percentages are for a given value of y.\"\"\"\n    \n    # Raw counts \n    raw_counts = pd.DataFrame(data.groupby(y)[x].value_counts(normalize = False))\n    raw_counts = raw_counts.rename(columns = {x: 'raw_count'})\n    \n    # Calculate counts for each group of x and y\n    counts = pd.DataFrame(data.groupby(y)[x].value_counts(normalize = True))\n    \n    # Rename the column and reset the index\n    counts = counts.rename(columns = {x: 'normalized_count'}).reset_index()\n    counts['percent'] = 100 * counts['normalized_count']\n    \n    # Add the raw count\n    counts['raw_count'] = list(raw_counts['raw_count'])\n    \n    plt.figure(figsize = (14, 10))\n    # Scatter plot sized by percent\n    plt.scatter(counts[x], counts[y], edgecolor = 'k', color = 'lightgreen',\n                s = 100 * np.sqrt(counts['raw_count']), marker = 'o',\n                alpha = 0.6, linewidth = 1.5)\n    \n    if annotate:\n        # Annotate the plot with text\n        for i, row in counts.iterrows():\n            # Put text with appropriate offsets\n            plt.annotate(xy = (row[x] - (1 / counts[x].nunique()), \n                               row[y] - (0.15 / counts[y].nunique())),\n                         color = 'navy',\n                         s = f\"{round(row['percent'], 1)}%\")\n        \n    # Set tick marks\n    plt.yticks(counts[y].unique())\n    plt.xticks(counts[x].unique())\n    \n    # Transform min and max to evenly space in square root domain\n    sqr_min = int(np.sqrt(raw_counts['raw_count'].min()))\n    sqr_max = int(np.sqrt(raw_counts['raw_count'].max()))\n    \n    # 5 sizes for legend\n    msizes = list(range(sqr_min, sqr_max,\n                        int(( sqr_max - sqr_min) / 5)))\n    markers = []\n    \n    # Markers for legend\n    for size in msizes:\n        markers.append(plt.scatter([], [], s = 100 * size, \n                                   label = f'{int(round(np.square(size) / 100) * 100)}', \n                                   color = 'lightgreen',\n                                   alpha = 0.6, edgecolor = 'k', linewidth = 1.5))\n        \n    # Legend and formatting\n    plt.legend(handles = markers, title = 'Counts',\n               labelspacing = 3, handletextpad = 2,\n               fontsize = 16,\n               loc = (1.10, 0.19))\n    \n    plt.annotate(f'* Size represents raw count while % is for a given y value.',\n                 xy = (0, 1), xycoords = 'figure points', size = 10)\n    \n    # Adjust axes limits\n    plt.xlim((counts[x].min() - (6 / counts[x].nunique()), \n              counts[x].max() + (6 / counts[x].nunique())))\n    plt.ylim((counts[y].min() - (4 / counts[y].nunique()), \n              counts[y].max() + (4 / counts[y].nunique())))\n                         \n    plt.grid(None)\n    plt.xlabel(f\"{x}\") \n    plt.ylabel(f\"{y}\")\n    plt.title(f\"{y} vs {x}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_categoricals('escolari', 'Target', data, annotate = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_value_counts(data[(data['rez_esc-missing'] == 1)], \n                  'Target')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_value_counts(data[(data['v2a1-missing'] == 1)], \n                  'Target')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"# training 시키지 않을 변수들입니다. idhogar를 기준으로 데이터들을 묶어서 볼 것입니다. \n# idhogar = 가구당 id\n\nid_ = ['Id', 'idhogar', 'Target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ind_bool = ['v18q', 'dis', 'male', 'female', 'estadocivil1', 'estadocivil2', 'estadocivil3', \n            'estadocivil4', 'estadocivil5', 'estadocivil6', 'estadocivil7', \n            'parentesco1', 'parentesco2',  'parentesco3', 'parentesco4', 'parentesco5', \n            'parentesco6', 'parentesco7', 'parentesco8',  'parentesco9', 'parentesco10', \n            'parentesco11', 'parentesco12', 'instlevel1', 'instlevel2', 'instlevel3', \n            'instlevel4', 'instlevel5', 'instlevel6', 'instlevel7', 'instlevel8', \n            'instlevel9', 'mobilephone', 'rez_esc-missing']\n\nind_ordered = ['rez_esc', 'escolari', 'age']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hh_bool = ['hacdor', 'hacapo', 'v14a', 'refrig', 'paredblolad', 'paredzocalo', \n           'paredpreb','pisocemento', 'pareddes', 'paredmad',\n           'paredzinc', 'paredfibras', 'paredother', 'pisomoscer', 'pisoother', \n           'pisonatur', 'pisonotiene', 'pisomadera',\n           'techozinc', 'techoentrepiso', 'techocane', 'techootro', 'cielorazo', \n           'abastaguadentro', 'abastaguafuera', 'abastaguano',\n            'public', 'planpri', 'noelec', 'coopele', 'sanitario1', \n           'sanitario2', 'sanitario3', 'sanitario5',   'sanitario6',\n           'energcocinar1', 'energcocinar2', 'energcocinar3', 'energcocinar4', \n           'elimbasu1', 'elimbasu2', 'elimbasu3', 'elimbasu4', \n           'elimbasu5', 'elimbasu6', 'epared1', 'epared2', 'epared3',\n           'etecho1', 'etecho2', 'etecho3', 'eviv1', 'eviv2', 'eviv3', \n           'tipovivi1', 'tipovivi2', 'tipovivi3', 'tipovivi4', 'tipovivi5', \n           'computer', 'television', 'lugar1', 'lugar2', 'lugar3',\n           'lugar4', 'lugar5', 'lugar6', 'area1', 'area2', 'v2a1-missing']\n\nhh_ordered = [ 'rooms', 'r4h1', 'r4h2', 'r4h3', 'r4m1','r4m2','r4m3', 'r4t1',  'r4t2', \n              'r4t3', 'v18q1', 'tamhog','tamviv','hhsize','hogar_nin',\n              'hogar_adul','hogar_mayor','hogar_total',  'bedrooms', 'qmobilephone']\n\nhh_cont = ['v2a1', 'dependency', 'edjefe', 'edjefa', 'meaneduc', 'overcrowding']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sqr_ = ['SQBescolari', 'SQBage', 'SQBhogar_total', 'SQBedjefe', \n        'SQBhogar_nin', 'SQBovercrowding', 'SQBdependency', 'SQBmeaned', 'agesq']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = ind_bool + ind_ordered + id_ + hh_bool + hh_ordered + hh_cont + sqr_\n\nfrom collections import Counter\n\nprint('There are no repeats: ', np.all(np.array(list(Counter(x).values())) == 1))\nprint('We covered every variable: ', len(x) == data.shape[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lmplot('age', 'SQBage', data = data, fit_reg=False); \nplt.title('Squared Age versus Age'); # ; 를 붙여주면 깔끔하게 그래프만 보여주네요","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove squared variables\ndata = data.drop(columns = sqr_)\ndata.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"heads = data.loc[data['parentesco1'] == 1, :]\nheads = heads[id_ + hh_bool + hh_cont + hh_ordered]\nprint(heads.shape)\nheads.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create correlation matrix\ncorr_matrix = heads.corr()\n\n# Select upper triangle of correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n\n# Find index of feature columns with correlation greater than 0.95\nto_drop = [column for column in upper.columns if any(abs(upper[column]) > 0.95)]\n\nto_drop","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix.loc[corr_matrix['tamhog'].abs() > 0.9, corr_matrix['tamhog'].abs() > 0.9]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(corr_matrix.loc[corr_matrix['tamhog'].abs() > 0.9, corr_matrix['tamhog'].abs() > 0.9],\n            annot=True, cmap = plt.cm.autumn_r, fmt='.3f');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"r4t3, Total persons in the household  \ntamhog, size of the household  \ntamviv, number of persons living in the household  \nhhsize, household size  \nhogar_total, # of total individuals in the household  "},{"metadata":{"trusted":true},"cell_type":"code","source":"heads = heads.drop(columns = ['tamhog', 'hogar_total', 'r4t3'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lmplot('tamviv', 'hhsize', data, fit_reg=False, size = 8);\nplt.title('Household size vs number of persons living in the household');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"elec = []\n\n# Assign values\nfor i, row in heads.iterrows():\n    if row['noelec'] == 1:\n        elec.append(0)\n    elif row['coopele'] == 1:\n        elec.append(1)\n    elif row['public'] == 1:\n        elec.append(2)\n    elif row['planpri'] == 1:\n        elec.append(3)\n    else:\n        elec.append(np.nan)\n        \n# Record the new variable and missing flag\nheads['elec'] = elec\nheads['elec-missing'] = heads['elec'].isnull()\n\n# Remove the electricity columns\n# heads = heads.drop(columns = ['noelec', 'coopele', 'public', 'planpri'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_categoricals('elec', 'Target', heads)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"heads = heads.drop(columns = 'area2')\nheads.groupby('area1')['Target'].value_counts(normalize = True)\n\n# area1과 Target의 비율입니다. ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"새로운 변수 생성"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Roof ordinal variable\nheads['roof'] = np.argmax(np.array(heads[['etecho1', 'etecho2', 'etecho3']]),\n                           axis = 1)\n# bad soso good 중에 제일 큰 값\n\nheads = heads.drop(columns = ['etecho1', 'etecho2', 'etecho3'])\n\n# Floor ordinal variable\nheads['floor'] = np.argmax(np.array(heads[['eviv1', 'eviv2', 'eviv3']]),\n                           axis = 1)\nheads = heads.drop(columns = ['eviv1', 'eviv2', 'eviv3'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create new feature\nheads['walls+roof+floor'] = heads['walls'] + heads['roof'] + heads['floor']\n\nplot_categoricals('walls+roof+floor', 'Target', heads, annotate=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Owns a refrigerator, computer, tablet, and television\nheads['bonus'] = 1 * (heads['refrig'] + \n                      heads['computer'] + \n                      (heads['v18q1'] > 0) + \n                      heads['television'])\n\nsns.violinplot('bonus', 'Target', data = heads,\n                figsize = (10, 6));\nplt.title('Target vs Bonus Variable');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1 명당의 자료를 만들어 비율을 알 수 있습니다. "},{"metadata":{"trusted":true},"cell_type":"code","source":"heads['phones-per-capita'] = heads['qmobilephone'] / heads['tamviv']\nheads['tablets-per-capita'] = heads['v18q1'] / heads['tamviv']\nheads['rooms-per-capita'] = heads['rooms'] / heads['tamviv']\nheads['rent-per-capita'] = heads['v2a1'] / heads['tamviv']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use only training data\ntrain_heads = heads.loc[heads['Target'].notnull(), :].copy()\n\npcorrs = pd.DataFrame(train_heads.corr()['Target'].sort_values()).rename(columns = {'Target': 'pcorr'}).reset_index()\npcorrs = pcorrs.rename(columns = {'index': 'feature'})\n\nprint('Most negatively correlated variables:')\nprint(pcorrs.head())\n\nprint('\\nMost positively correlated variables:')\nprint(pcorrs.dropna().tail())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"variables = ['Target', 'dependency',  'meaneduc', 'r4m1', 'overcrowding']\n\n# Calculate the correlations\ncorr_mat = train_heads[variables].corr().round(2)\n\n# Draw a correlation heatmap\nplt.rcParams['font.size'] = 18\nplt.figure(figsize = (12, 12))\nsns.heatmap(corr_mat, vmin = -0.5, vmax = 0.8, center = 0, \n            cmap = plt.cm.RdYlGn_r, annot = True);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Individual Level Variables "},{"metadata":{"trusted":true},"cell_type":"code","source":"ind = data[id_ + ind_bool + ind_ordered]\nind.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Redundant Individual Variables "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create correlation matrix\ncorr_matrix = ind.corr()\n\n# Select upper triangle of correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n\n# Find index of feature columns with correlation greater than 0.95\nto_drop = [column for column in upper.columns if any(abs(upper[column]) > 0.95)]\n\nto_drop","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ind = ind.drop(columns = 'male')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ind[[c for c in ind if c.startswith('instl')]].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ind['inst'] = np.argmax(np.array(ind[[c for c in ind if c.startswith('instl')]]), axis = 1)\n\nplot_categoricals('inst', 'Target', ind, annotate = False);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10, 8))\nsns.violinplot(x = 'Target', y = 'inst', data = ind);\nplt.title('Education Distribution by Target');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop the education columns\n# ind = ind.drop(columns = [c for c in ind if c.startswith('instlevel')])\nind.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define custom function\nrange_ = lambda x: x.max() - x.min()\nrange_.__name__ = 'range_'\n\n# Group and aggregate\nind_agg = ind.drop(columns = 'Target').groupby('idhogar').agg(['min', 'max', 'sum', 'count', 'std', range_])\nind_agg.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rename the columns\nnew_col = []\nfor c in ind_agg.columns.levels[0]:\n    for stat in ind_agg.columns.levels[1]:\n        new_col.append(f'{c}-{stat}')\n        \nind_agg.columns = new_col\nind_agg.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create correlation matrix\ncorr_matrix = ind_agg.corr()\n\n# Select upper triangle of correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n\n# Find index of feature columns with correlation greater than 0.95\nto_drop = [column for column in upper.columns if any(abs(upper[column]) > 0.95)]\n\nprint(f'There are {len(to_drop)} correlated columns to remove.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ind_agg = ind_agg.drop(columns = to_drop)\nind_feats = list(ind_agg.columns)\n\n# Merge on the household id\nfinal = heads.merge(ind_agg, on = 'idhogar', how = 'left')\n\nprint('Final features shape: ', final.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score, make_scorer\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.pipeline import Pipeline\n\n# Custom scorer for cross validation\nscorer = make_scorer(f1_score, greater_is_better=True, average = 'macro')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Labels for training\ntrain_labels = np.array(list(final[final['Target'].notnull()]['Target'].astype(np.uint8)))\n\n# Extract the training data\ntrain_set = final[final['Target'].notnull()].drop(columns = ['Id', 'idhogar', 'Target'])\ntest_set = final[final['Target'].isnull()].drop(columns = ['Id', 'idhogar', 'Target'])\n\n# Submission base which is used for making submissions to the competition\nsubmission_base = test[['Id', 'idhogar']].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = list(train_set.columns)\n\npipeline = Pipeline([('imputer', Imputer(strategy = 'median')), \n                      ('scaler', MinMaxScaler())])\n\n# Fit and transform training data\ntrain_set = pipeline.fit_transform(train_set)\ntest_set = pipeline.transform(test_set)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = RandomForestClassifier(n_estimators=100, random_state=10, \n                               n_jobs = -1)\n# 10 fold cross validation\ncv_score = cross_val_score(model, train_set, train_labels, cv = 10, scoring = scorer)\n\nprint(f'10 Fold Cross Validation F1 Score = {round(cv_score.mean(), 4)} with std = {round(cv_score.std(), 4)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(train_set, train_labels)\n\n# Feature importances into a dataframe\nfeature_importances = pd.DataFrame({'feature': features, 'importance': model.feature_importances_})\nfeature_importances.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_feature_importances(df, n = 10, threshold = None):\n    \"\"\"Plots n most important features. Also plots the cumulative importance if\n    threshold is specified and prints the number of features needed to reach threshold cumulative importance.\n    Intended for use with any tree-based feature importances. \n    \n    Args:\n        df (dataframe): Dataframe of feature importances. Columns must be \"feature\" and \"importance\".\n    \n        n (int): Number of most important features to plot. Default is 15.\n    \n        threshold (float): Threshold for cumulative importance plot. If not provided, no plot is made. Default is None.\n        \n    Returns:\n        df (dataframe): Dataframe ordered by feature importances with a normalized column (sums to 1) \n                        and a cumulative importance column\n    \n    Note:\n    \n        * Normalization in this case means sums to 1. \n        * Cumulative importance is calculated by summing features from most to least important\n        * A threshold of 0.9 will show the most important features needed to reach 90% of cumulative importance\n    \n    \"\"\"\n    plt.style.use('fivethirtyeight')\n    \n    # Sort features with most important at the head\n    df = df.sort_values('importance', ascending = False).reset_index(drop = True)\n    \n    # Normalize the feature importances to add up to one and calculate cumulative importance\n    df['importance_normalized'] = df['importance'] / df['importance'].sum()\n    df['cumulative_importance'] = np.cumsum(df['importance_normalized'])\n    \n    plt.rcParams['font.size'] = 12\n    \n    # Bar plot of n most important features\n    df.loc[:n, :].plot.barh(y = 'importance_normalized', \n                            x = 'feature', color = 'darkgreen', \n                            edgecolor = 'k', figsize = (12, 8),\n                            legend = False, linewidth = 2)\n\n    plt.xlabel('Normalized Importance', size = 18); plt.ylabel(''); \n    plt.title(f'{n} Most Important Features', size = 18)\n    plt.gca().invert_yaxis()\n    \n    \n    if threshold:\n        # Cumulative importance plot\n        plt.figure(figsize = (8, 6))\n        plt.plot(list(range(len(df))), df['cumulative_importance'], 'b-')\n        plt.xlabel('Number of Features', size = 16); plt.ylabel('Cumulative Importance', size = 16); \n        plt.title('Cumulative Feature Importance', size = 18);\n        \n        # Number of features needed for threshold cumulative importance\n        # This is the index (will need to add 1 for the actual number)\n        importance_index = np.min(np.where(df['cumulative_importance'] > threshold))\n        \n        # Add vertical line to plot\n        plt.vlines(importance_index + 1, ymin = 0, ymax = 1.05, linestyles = '--', colors = 'red')\n        plt.show();\n        \n        print('{} features required for {:.0f}% of cumulative importance.'.format(importance_index + 1, \n                                                                                  100 * threshold))\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"norm_fi = plot_feature_importances(feature_importances, threshold=0.95)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model imports\nfrom sklearn.svm import LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.linear_model import LogisticRegressionCV, RidgeClassifierCV\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.neighbors import KNeighborsClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings \nfrom sklearn.exceptions import ConvergenceWarning\n\n# Filter out warnings from models\nwarnings.filterwarnings('ignore', category = ConvergenceWarning)\nwarnings.filterwarnings('ignore', category = DeprecationWarning)\nwarnings.filterwarnings('ignore', category = UserWarning)\n\n# Dataframe to hold results\nmodel_results = pd.DataFrame(columns = ['model', 'cv_mean', 'cv_std'])\n\ndef cv_model(train, train_labels, model, name, model_results=None):\n    \"\"\"Perform 10 fold cross validation of a model\"\"\"\n    \n    cv_scores = cross_val_score(model, train, train_labels, cv = 10, scoring=scorer, n_jobs = -1)\n    print(f'10 Fold CV Score: {round(cv_scores.mean(), 5)} with std: {round(cv_scores.std(), 5)}')\n    \n    if model_results is not None:\n        model_results = model_results.append(pd.DataFrame({'model': name, \n                                                           'cv_mean': cv_scores.mean(), \n                                                            'cv_std': cv_scores.std()},\n                                                           index = [0]),\n                                             ignore_index = True)\n\n        return model_results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_results = cv_model(train_set, train_labels, LinearSVC(), \n                         'LSVC', model_results)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_results = cv_model(train_set, train_labels, \n                         GaussianNB(), 'GNB', model_results)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_results = cv_model(train_set, train_labels, \n                         MLPClassifier(hidden_layer_sizes=(32, 64, 128, 64, 32)),\n                         'MLP', model_results)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_results = cv_model(train_set, train_labels, \n                          LinearDiscriminantAnalysis(), \n                          'LDA', model_results)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_results = cv_model(train_set, train_labels, \n                         RidgeClassifierCV(), 'RIDGE', model_results)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for n in [5, 10, 20]:\n    print(f'\\nKNN with {n} neighbors\\n')\n    model_results = cv_model(train_set, train_labels, \n                             KNeighborsClassifier(n_neighbors = n),\n                             f'knn-{n}', model_results)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesClassifier\n\nmodel_results = cv_model(train_set, train_labels, \n                         ExtraTreesClassifier(n_estimators = 100, random_state = 10),\n                         'EXT', model_results)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Comparing Model Performance"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_results = cv_model(train_set, train_labels,\n                          RandomForestClassifier(100, random_state=10),\n                              'RF', model_results)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_results.set_index('model', inplace = True)\nmodel_results['cv_mean'].plot.bar(color = 'orange', figsize = (8, 6),\n                                  yerr = list(model_results['cv_std']),\n                                  edgecolor = 'k', linewidth = 2)\nplt.title('Model F1 Score Results');\nplt.ylabel('Mean F1 Score (with error bar)');\nmodel_results.reset_index(inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def submit(model):\n    model.fit(train_set, train_labels)\n    predictions = model.predict(test_set)\n    predictions = pd.DataFrame({'idhogar': test_ids,\n                               'Target': predictions})\n\n     # Make a submission dataframe\n    submission = submission_base.merge(predictions, \n                                       on = 'idhogar',\n                                       how = 'left').drop(columns = ['idhogar'])\n\n    # Fill in households missing a head\n    submission['Target'] = submission['Target'].fillna(4).astype(np.int8)\n    \n    return submission ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# XGBOOST "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(train_set, train_labels, test_size = 0.2, \n                                                    random_state =42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('train shape', X_train.shape, y_train.shape)\nprint('test shape', X_test.shape, y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost \nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import f1_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_model  = XGBClassifier(objective = 'multi:softmax', random_state = 42)\nxgb_model.fit(X_train, y_train)\ny_pred = xgb_model.predict(X_test)\nf1_score(y_test, y_pred, average = 'macro')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_ids = list(final.loc[final['Target'].isnull(), 'idhogar'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_submission = submit(xgb_model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LightGBM"},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm\nfrom lightgbm import LGBMClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_model  = LGBMClassifier(objective = 'multiclass', random_state = 42)\nlgb_model.fit(X_train, y_train)\ny_pred = lgb_model.predict(X_test)\nprint(y_pred)\nf1_score(y_test, y_pred, average = 'macro')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_submission = submit(lgb_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_submission.to_csv('xgb.csv', index = False)\nlgb_submission.to_csv('lgb.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Advanced LightGBM"},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb \nprint('Train start')\n\nparams = {\"objective\" : \"multiclass\",\n          \"num_class\" : 4,\n          \"num_leaves\" : 60,\n          \"max_depth\": -1,\n          \"learning_rate\" : 0.01,\n          \"bagging_fraction\" : 0.9,  # subsample\n          \"feature_fraction\" : 0.9,  # colsample_bytree\n          \"bagging_freq\" : 5,        # subsample_freq\n          \"bagging_seed\" : 42,\n          \"verbosity\" : -1 }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lgbDataset에서는 label은 0부터 시작해야함 0, 1, 2, 3 \n\ny_tra = y_train - 1\ny_tes = y_test -1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgtrain, lgval = lgb.Dataset(data = X_train, label = y_tra), lgb.Dataset(data = X_test, label = y_tes)\nprint(lgtrain)\nlgb_ad_model = lgb.train(params, lgtrain, num_boost_round = 2000, valid_sets=[lgtrain, lgval], \n                  early_stopping_rounds=100, verbose_eval=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = lgb_ad_model.predict(test_set)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_df_1 = []\npredict_df_2 = []\npredict_df_3 = []\npredict_df_4 = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(predictions)):\n    predict_df_1.append(predictions[i][0])\n    predict_df_2.append(predictions[i][1])    \n    predict_df_3.append(predictions[i][2])    \n    predict_df_4.append(predictions[i][3])    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" df_pred = pd.DataFrame(predict_df_1, columns = ['1'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_pred['2'] = predict_df_2\ndf_pred['3'] = predict_df_3\ndf_pred['4'] = predict_df_4\ndf_pred.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_pred['Target'] = df_pred[['1', '2', '3', '4']].idxmax(axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_pred.to_csv('submission.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"}},"nbformat":4,"nbformat_minor":1}