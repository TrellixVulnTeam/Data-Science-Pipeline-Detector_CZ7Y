{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yTrain = train.Target\nxTrain = train.drop(['Target','Id'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yTrain.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_null = xTrain.isnull().sum()\ntrain_null[train_null != 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xTrain.rez_esc.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xTrain.loc[((xTrain['age'] > 19) | (xTrain['age'] < 7)) & (xTrain['rez_esc'].isnull()), 'rez_esc'] = 0\nxTrain.rez_esc.fillna(xTrain.rez_esc.mean(), inplace=True)\n\nxTrain.rez_esc.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xTrain[\"v18q1\"] = xTrain[\"v18q1\"].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fill in households that own the house with 0 rent payment\nxTrain.loc[(xTrain['tipovivi1'] == 1), 'v2a1'] = 0\nxTrain[\"v2a1\"].fillna(xTrain[\"v2a1\"].mean(), inplace=True)\nxTrain[\"meaneduc\"].fillna(xTrain[\"meaneduc\"].mean(), inplace=True)\nxTrain[\"SQBmeaned\"].fillna(xTrain[\"SQBmeaned\"].mean(), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Difference between people living in house and household size\nxTrain['hhsize-diff'] = xTrain['tamviv'] - xTrain['hhsize']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xTrain['walls'] = np.argmax(np.array(xTrain[['epared1', 'epared2', 'epared3']]), axis = 1)\n\n# Roof ordinal variable\nxTrain['roof'] = np.argmax(np.array(xTrain[['etecho1', 'etecho2', 'etecho3']]), axis = 1)\n\n# Floor ordinal variable\nxTrain['floor'] = np.argmax(np.array(xTrain[['eviv1', 'eviv2', 'eviv3']]), axis = 1)\n\n# Create new feature\nxTrain['walls+roof+floor'] = xTrain['walls'] + xTrain['roof'] + xTrain['floor']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# No toilet, no electricity, no floor, no water service, no ceiling\nxTrain['warning'] = 1 * (xTrain['sanitario1'] + \n                         xTrain['noelec'] + \n                         xTrain['pisonotiene'] + \n                         xTrain['abastaguano'] + \n                         (xTrain['cielorazo'] == 0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Owns a refrigerator, computer, tablet, and television\nxTrain['bonus'] = 1 * (xTrain['refrig'] + \n                      xTrain['computer'] + \n                      (xTrain['v18q1'] > 0) + \n                      xTrain['television'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Per capita features\nxTrain['phones-per-capita'] = xTrain['qmobilephone'] / xTrain['tamviv']\nxTrain['tablets-per-capita'] = xTrain['v18q1'] / xTrain['tamviv']\nxTrain['rooms-per-capita'] = xTrain['rooms'] / xTrain['tamviv']\nxTrain['rent-per-capita'] = xTrain['v2a1'] / xTrain['tamviv']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xTrain['tech'] = xTrain['v18q'] + xTrain['mobilephone']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"string_column = [f for f in xTrain.columns if xTrain.dtypes[f] == 'object']\nstring_column","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xTrain.drop(['idhogar'], axis = 1, inplace = True)\n\nxTrain['dependency'].replace('no', 0, inplace = True)\nxTrain['edjefe'].replace('no', 0, inplace = True)\nxTrain['edjefa'].replace('no', 0, inplace = True)\n\nxTrain['dependency'].replace('yes', 1, inplace = True)\nxTrain['edjefe'].replace('yes', 1, inplace = True)\nxTrain['edjefa'].replace('yes', 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xTrain[['dependency','edjefe','edjefa']].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xTrain[['dependency','edjefe','edjefa']] = xTrain[['dependency','edjefe','edjefa']].apply(pd.to_numeric)\n\n[f for f in xTrain.columns if xTrain.dtypes[f] == 'object']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.options.mode.use_inf_as_na = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# labor force\nxTrain['adult'] = xTrain['hogar_adul'] - xTrain['hogar_mayor']\nxTrain['dependency_count'] = xTrain['hogar_nin'] + xTrain['hogar_mayor']\nxTrain['dependency2'] = xTrain['dependency_count'] / xTrain['adult']\nxTrain['child_percent'] = xTrain['hogar_nin']/xTrain['hogar_total']\nxTrain['elder_percent'] = xTrain['hogar_mayor']/xTrain['hogar_total']\nxTrain['adult_percent'] = xTrain['hogar_adul']/xTrain['hogar_total']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xTrain['rent_per_adult'] = xTrain['v2a1']/xTrain['hogar_adul']\nxTrain['rent_per_person'] = xTrain['v2a1']/xTrain['hhsize']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# male-female ratio\nxTrain['r4h1_percent_in_male'] = xTrain['r4h1'] / xTrain['r4h3']\nxTrain['r4m1_percent_in_female'] = xTrain['r4m1'] / xTrain['r4m3']\nxTrain['r4h1_percent_in_total'] = xTrain['r4h1'] / xTrain['hhsize']\nxTrain['r4m1_percent_in_total'] = xTrain['r4m1'] / xTrain['hhsize']\nxTrain['r4t1_percent_in_total'] = xTrain['r4t1'] / xTrain['hhsize']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# per capito\nxTrain['rent_per_bedroom'] = xTrain['v2a1']/xTrain['bedrooms']\nxTrain['edler_per_bedroom'] = xTrain['hogar_mayor']/xTrain['bedrooms']\nxTrain['adults_per_bedroom'] = xTrain['adult']/xTrain['bedrooms']\nxTrain['child_per_bedroom'] = xTrain['hogar_nin']/xTrain['bedrooms']\nxTrain['male_per_bedroom'] = xTrain['r4h3']/xTrain['bedrooms']\nxTrain['female_per_bedroom'] = xTrain['r4m3']/xTrain['bedrooms']\nxTrain['bedrooms_per_person_household'] = xTrain['hhsize']/xTrain['bedrooms']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_null = xTrain.isnull().sum()\ntrain_null[train_null != 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# when there is no adult in the house, the result will be Nan\nxTrain[\"rent_per_adult\"] = xTrain[\"rent_per_adult\"].fillna(0)\n\n# similar to male/female ratio\nxTrain[\"r4h1_percent_in_male\"] = xTrain[\"r4h1_percent_in_male\"].fillna(0)\nxTrain[\"r4m1_percent_in_female\"] = xTrain[\"r4m1_percent_in_female\"].fillna(0)\n\n# when there is no adult in the household, the value will be inf. So, we should fill in some very large number \nxTrain.dependency2 = xTrain.dependency2.fillna(1e5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_null = xTrain.isnull().sum()\ntrain_null[train_null != 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_val, y_train, y_val = train_test_split(xTrain, yTrain,stratify=yTrain, test_size = 0.15, random_state = 2, shuffle=True)\nx_train.shape, y_train.shape, x_val.shape, y_val.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# credit to my homework from Coursera:\n# Hyperparameter tuning, Regularization and Optimization\n\nimport math\nimport numpy as np\nimport pandas as pd\nimport h5py\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\n\n\ndef softmax(x):\n    if x.ndim == 2:\n        x = x.T\n        x = x - np.max(x, axis=0)\n        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n        return y.T\n\n    x = x - np.max(x)  # address the overflow problem\n    return np.exp(x) / np.sum(np.exp(x))\n\n\ndef initialize_parameters(dims):\n    \"\"\"\n    Initializes parameters to build a neural network with tensorflow. The shapes are:\n\n    Returns:\n    parameters -- a dictionary of tensors containing weights and biases\n    \"\"\"\n    tf.set_random_seed(1)\n    parameters = {}\n    L = len(dims) - 1  # input layer should not be counted\n    for l in range(L - 1):\n        parameters[\"W\" + str(l + 1)] = tf.get_variable(\"W\" + str(l + 1), [dims[l + 1], dims[l]],\n                                                       initializer=tf.contrib.layers.xavier_initializer(seed=1))\n        parameters[\"b\" + str(l + 1)] = tf.get_variable(\"b\" + str(l + 1), [dims[l + 1], 1],\n                                                       initializer=tf.zeros_initializer())\n    parameters[\"W\" + str(L)] = tf.get_variable(\"W\" + str(L), [dims[L], dims[L - 1]],\n                                               initializer=tf.contrib.layers.xavier_initializer(seed=1))\n    parameters[\"b\" + str(L)] = tf.get_variable(\"b\" + str(L), [dims[L], 1],\n                                               initializer=tf.zeros_initializer())\n\n    return parameters\n\n\ndef create_placeholders(n_x, n_y):\n    \"\"\"\n    Creates the placeholders for the tensorflow session.\n\n    \"\"\"\n\n    X = tf.placeholder(tf.float32, [n_x, None], name=\"X\")\n    Y = tf.placeholder(tf.float32, [n_y, None], name=\"Y\")\n\n    return X, Y\n\n\ndef forward_propagation(X, parameters):\n    \"\"\"\n    Implements the forward propagation for the model: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX\n\n    Arguments:\n    X -- input dataset placeholder, of shape (input size, number of examples)\n    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"\n                  the shapes are given in initialize_parameters\n\n    Returns:\n    Z3 -- the output of the last LINEAR unit\n    \"\"\"\n    L = len(parameters) // 2\n    Z = {}\n    A = {}\n    A[\"0\"] = X\n    for l in range(L):\n        Z[str(l + 1)] = tf.add(tf.matmul(parameters[\"W\" + str(l + 1)], A[str(l)]), parameters[\"b\" + str(l + 1)])\n        A[str(l + 1)] = tf.nn.relu(Z[str(l + 1)])\n    return Z[str(L)]\n\n\ndef compute_cost(ZL, Y, reg_constant=0.01):\n    # to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)\n    logits = tf.transpose(ZL)\n    labels = tf.transpose(Y)\n    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels)\n\n    l2_loss = reg_constant * tf.add_n([tf.nn.l2_loss(tf.cast(v, tf.float32)) for v in tf.trainable_variables()])\n    c = loss + l2_loss\n    cost = tf.reduce_mean(c)\n\n    return cost\n\n\ndef random_mini_batches(X, Y, mini_batch_size=64, seed=0):\n    \"\"\"\n    Creates a list of random minibatches from (X, Y)\n\n    Arguments:\n    X -- input data, of shape (input size, number of examples)\n    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n    mini_batch_size -- size of the mini-batches, integer\n\n    Returns:\n    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n    \"\"\"\n\n    np.random.seed(seed)  # To make your \"random\" minibatches the same as ours\n    m = X.shape[1]  # number of training examples\n    C = Y.shape[0]  # number of class\n    mini_batches = []\n\n    # Step 1: Shuffle (X, Y)\n    permutation = list(np.random.permutation(m))\n    shuffled_X = X.values[:, permutation]\n    shuffled_Y = Y[:, permutation].reshape((C, m))\n\n    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n    num_complete_minibatches = math.floor(\n        m / mini_batch_size)  # number of mini batches of size mini_batch_size in your partitionning\n    for k in range(0, num_complete_minibatches):\n        mini_batch_X = shuffled_X[:, k * mini_batch_size: (k + 1) * mini_batch_size]\n        mini_batch_Y = shuffled_Y[:, k * mini_batch_size: (k + 1) * mini_batch_size]\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n\n    # Handling the end case (last mini-batch < mini_batch_size)\n    if m % mini_batch_size != 0:\n        mini_batch_X = shuffled_X[:, (mini_batch_size * num_complete_minibatches):]\n        mini_batch_Y = shuffled_Y[:, (mini_batch_size * num_complete_minibatches):]\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n\n    return mini_batches\n\n\ndef convert_to_one_hot(Y, C):\n    Y = np.eye(C)[Y.reshape(-1)].T\n    return Y\n\n\ndef compute_cost_with_regularization(AL, Y, parameters, lambd):\n    \"\"\"\n    Implement the cost function with L2 regularization. See formula (2) above.\n\n    Arguments:\n    AL -- post-activation, output of forward propagation, of shape (output size, number of examples)\n    Y -- \"true\" labels vector, of shape (output size, number of examples)\n    parameters -- python dictionary containing parameters of the model\n\n    Returns:\n    cost - value of the regularized loss function (formula (2))\n    \"\"\"\n    m = Y.shape[1]\n    L = len(parameters) // 2\n\n    cross_entropy_cost = compute_cost(AL, Y)  # This gives you the cross-entropy part of the cost\n\n    sum = 0\n    for l in range(L):\n        sum += np.sum(np.square(parameters[\"W\" + str(l + 1)]))\n\n    L2_regularization_cost = lambd * sum / (2 * m)\n\n    cost = cross_entropy_cost + L2_regularization_cost\n\n    return cost\n\n\ndef backward_propagation_with_regularization(X, Y, parameters, Z, A, lambd):\n    \"\"\"\n    Implements the backward propagation of our baseline model to which we added an L2 regularization.\n\n    Arguments:\n    X -- input dataset, of shape (input size, number of examples)\n    Y -- \"true\" labels vector, of shape (output size, number of examples)\n    cache -- cache output from forward_propagation()\n    lambd -- regularization hyperparameter, scalar\n\n    Returns:\n    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables\n    \"\"\"\n\n    m = X.shape[1]\n    L = len(parameters) // 2\n    dZ = {}\n    dW = {}\n    db = {}\n    dA = {}\n\n    dZ[str(L)] = A[str(L)] - Y\n\n    dW[str(L)] = 1. / m * np.dot(dZ[str(L)], A[str(L - 1)].T) + (lambd * parameters[\"W\" + str(L)]) / m\n    db[str(L)] = 1. / m * np.sum(dZ[str(L)], axis=1, keepdims=True)\n\n    for l in reversed(range(1, L)):\n        dA[str(l)] = np.dot(parameters[\"W\" + str(l + 1)].T, dZ[str(l + 1)])\n        dZ[str(l)] = np.multiply(dA[str(l)], np.int64(A[str(l)] > 0))\n        dW[str(l)] = 1. / m * np.dot(dZ[str(l)], A[str(l - 1)].T) + (lambd * parameters[\"W\" + str(l)]) / m\n        db[str(l)] = 1. / m * np.sum(dZ[str(l)], axis=1, keepdims=True)\n\n    return dZ, dW, db, dA\n\n\ndef update_parameters_with_adam(parameters, dZ, dW, db, dA, v, s,\n                                t, learning_rate, beta1, beta2, epsilon):\n    \"\"\"\n    Update parameters using Adam\n\n    Arguments:\n    v -- Adam variable, moving average of the first gradient, python dictionary\n    s -- Adam variable, moving average of the squared gradient, python dictionary\n    learning_rate -- the learning rate, scalar.\n    beta1 -- Exponential decay hyperparameter for the first moment estimates\n    beta2 -- Exponential decay hyperparameter for the second moment estimates\n    epsilon -- hyperparameter preventing division by zero in Adam updates\n\n    Returns:\n    parameters -- python dictionary containing your updated parameters\n    v -- Adam variable, moving average of the first gradient, python dictionary\n    s -- Adam variable, moving average of the squared gradient, python dictionary\n    \"\"\"\n\n    L = len(parameters) // 2  # number of layers in the neural networks\n    v_corrected = {}  # Initializing first moment estimate, python dictionary\n    s_corrected = {}  # Initializing second moment estimate, python dictionary\n\n    # Perform Adam update on all parameters\n    for l in range(L):\n        # Moving average of the gradients. Inputs: \"v, grads, beta1\". Output: \"v\".\n        v[\"dW\" + str(l + 1)] = beta1 * v[\"dW\" + str(l + 1)] + (1 - beta1) * dW[str(l + 1)]\n        v[\"db\" + str(l + 1)] = beta1 * v[\"db\" + str(l + 1)] + (1 - beta1) * db[str(l + 1)]\n\n        # Compute bias-corrected first moment estimate. Inputs: \"v, beta1, t\". Output: \"v_corrected\".\n        v_corrected[\"dW\" + str(l + 1)] = v[\"dW\" + str(l + 1)] / (1 - np.power(beta1, t))\n        v_corrected[\"db\" + str(l + 1)] = v[\"db\" + str(l + 1)] / (1 - np.power(beta1, t))\n\n        # Moving average of the squared gradients. Inputs: \"s, grads, beta2\". Output: \"s\".\n        s[\"dW\" + str(l + 1)] = beta2 * s[\"dW\" + str(l + 1)] + (1 - beta2) * np.power(dW[str(l + 1)], 2)\n        s[\"db\" + str(l + 1)] = beta2 * s[\"db\" + str(l + 1)] + (1 - beta2) * np.power(db[str(l + 1)], 2)\n\n        # Compute bias-corrected second raw moment estimate. Inputs: \"s, beta2, t\". Output: \"s_corrected\".\n        s_corrected[\"dW\" + str(l + 1)] = s[\"dW\" + str(l + 1)] / (1 - np.power(beta2, t))\n        s_corrected[\"db\" + str(l + 1)] = s[\"db\" + str(l + 1)] / (1 - np.power(beta2, t))\n\n        # Update parameters.\n        # Inputs: \"parameters, learning_rate, v_corrected, s_corrected, epsilon\". Output: \"parameters\".\n        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * v_corrected[\n            \"dW\" + str(l + 1)] / np.sqrt(s_corrected[\"dW\" + str(l + 1)] + epsilon)\n        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * v_corrected[\n            \"db\" + str(l + 1)] / np.sqrt(s_corrected[\"db\" + str(l + 1)] + epsilon)\n\n    return parameters, v, s\n\n\ndef model(dims, X_train, Y_train, X_test=None, Y_test=None, learning_rate=0.0001,\n          reg=0.01, num_epochs=1500, minibatch_size=32, print_cost=True):\n    \"\"\"\n    Implements a L-layer tensorflow neural network: LINEAR->RELU->LINEAR->RELU->...->LINEAR->SOFTMAX.\n\n    Returns:\n    parameters -- parameters learnt by the model. They can then be used to predict.\n    \"\"\"\n\n    ops.reset_default_graph()  # to be able to rerun the model without overwriting tf variables\n    tf.set_random_seed(1)  # to keep consistent results\n    seed = 3  # to keep consistent results\n    (n_x, m) = X_train.shape  # (n_x: input size, m : number of examples in the train set)\n    n_y = Y_train.shape[0]  # n_y : output size, this is actually the number of class (C)\n    costs = []  # To keep track of the cost\n\n    # Create Placeholders of shape (n_x, n_y)\n    X, Y = create_placeholders(n_x, n_y)\n    # Initialize parameters\n    parameters = initialize_parameters(dims)\n    # Forward propagation: Build the forward propagation in the tensorflow graph\n    ZL = forward_propagation(X, parameters)\n    # Cost function: Add cost function to tensorflow graph\n    cost = compute_cost(ZL, Y, reg)\n    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer.\n    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n\n    # Initialize all the variables\n    init = tf.global_variables_initializer()\n\n    # Start the session to compute the tensorflow graph\n    with tf.Session() as sess:\n\n        # Run the initialization\n        sess.run(init)\n\n        # Do the training loop\n        for epoch in range(num_epochs):\n\n            epoch_cost = 0.  # Defines a cost related to an epoch\n            num_minibatches = int(m / minibatch_size)  # number of minibatches of size minibatch_size in the train set\n            seed = seed + 1\n            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n\n            for minibatch in minibatches:\n                # Select a minibatch\n                (minibatch_X, minibatch_Y) = minibatch\n\n                # IMPORTANT: The line that runs the graph on a minibatch.\n                # Run the session to execute the \"optimizer\" and the \"cost\",\n                # the feedict should contain a minibatch for (X,Y).\n                _, minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})\n\n                epoch_cost += minibatch_cost / num_minibatches\n\n            # Print the cost every epoch\n            if print_cost == True and epoch % 100 == 0:\n                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n            if print_cost == True and epoch % 5 == 0:\n                costs.append(epoch_cost)\n\n        # plot the cost\n        plt.plot(np.squeeze(costs))\n        plt.ylabel('cost')\n        plt.xlabel('iterations (per tens)')\n        plt.title(\"Learning rate =\" + str(learning_rate))\n        plt.show()\n\n        # lets save the parameters in a variable\n        parameters = sess.run(parameters)\n        print(\"Parameters have been trained!\")\n\n        # Calculate the correct predictions\n        correct_prediction = tf.equal(tf.argmax(ZL), tf.argmax(Y))\n\n        # Calculate accuracy on the test set\n        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n\n        print(\"Train Accuracy:\", accuracy.eval({X: X_train, Y: Y_train}))\n        if Y_test is not None:\n            print(\"Test Accuracy:\", accuracy.eval({X: X_test, Y: Y_test}))\n\n        return parameters\n\n\ndef predict(X, parameters):\n    x = tf.placeholder(\"float\", [X.shape[0], X.shape[1]])\n\n    ZL = forward_propagation(x, parameters)\n    p = tf.argmax(ZL)\n\n    sess = tf.Session()\n    prediction = sess.run(p, feed_dict={x: X})\n\n    return prediction\n\n\ndef one_hot_matrix(labels, C):\n    \"\"\"\n    Creates a matrix where the i-th row corresponds to the ith class number and the jth column\n                     corresponds to the jth training example. So if example j had a label i. Then entry (i,j)\n                     will be 1.\n\n    Arguments:\n    labels -- vector containing the labels\n    C -- number of classes, the depth of the one hot dimension\n\n    Returns:\n    one_hot -- one hot matrix\n    \"\"\"\n\n    # Create a tf.constant equal to C (depth), name it 'C'. (approx. 1 line)\n    C = tf.constant(C, name='C')\n\n    # Use tf.one_hot, be careful with the axis (approx. 1 line)\n    one_hot_matrix = tf.one_hot(indices=labels, depth=C, axis=0)\n\n    # Create the session (approx. 1 line)\n    sess = tf.Session()\n\n    # Run the session (approx. 1 line)\n    one_hot = sess.run(one_hot_matrix)\n\n    # Close the session (approx. 1 line). See method 1 above.\n    sess.close()\n\n    return one_hot\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_train = convert_to_one_hot(np.array(y_train), 5)\nY_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = x_train.T\nX_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_val = convert_to_one_hot(np.array(y_val), 5)\nX_val = x_val.T\nY_val.shape, X_val.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### CONSTANTS DEFINING THE MODEL ####\nn_x = X_train.shape[0]\nn_h1 = 200  # hyper parameter\nn_h2 = 200  # hyper parameter\nn_h3 = 100  # hyper parameter\nn_y = Y_train.shape[0]\nlayers_dims = [n_x, n_h1, n_h2, n_h3, n_y]\nlayers_dims","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"parameters = model(layers_dims, X_train, Y_train, X_val, Y_val, learning_rate = 0.001, reg = 0, num_epochs = 1500)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xTest = test.drop(['Id'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_null = xTest.isnull().sum()\ntest_null[test_null != 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xTest[\"v18q1\"] = xTest[\"v18q1\"].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xTest.loc[((xTest['age'] > 19) | (xTest['age'] < 7)) & (xTest['rez_esc'].isnull()), 'rez_esc'] = 0\nxTest.rez_esc.fillna(xTest.rez_esc.mean(), inplace=True)\n\nxTest.rez_esc.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fill in households that own the house with 0 rent payment\nxTest.loc[(xTest['tipovivi1'] == 1), 'v2a1'] = 0\nxTest[\"v2a1\"].fillna(xTest[\"v2a1\"].mean(), inplace=True)\nxTest[\"meaneduc\"].fillna(xTest[\"meaneduc\"].mean(), inplace=True)\nxTest[\"SQBmeaned\"].fillna(xTest[\"SQBmeaned\"].mean(), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Difference between people living in house and household size\nxTest['hhsize-diff'] = xTest['tamviv'] - xTest['hhsize']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xTest['walls'] = np.argmax(np.array(xTest[['epared1', 'epared2', 'epared3']]), axis = 1)\n\n# Roof ordinal variable\nxTest['roof'] = np.argmax(np.array(xTest[['etecho1', 'etecho2', 'etecho3']]), axis = 1)\n\n# Floor ordinal variable\nxTest['floor'] = np.argmax(np.array(xTest[['eviv1', 'eviv2', 'eviv3']]), axis = 1)\n\n# Create new feature\nxTest['walls+roof+floor'] = xTest['walls'] + xTest['roof'] + xTest['floor']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# No toilet, no electricity, no floor, no water service, no ceiling\nxTest['warning'] = 1 * (xTest['sanitario1'] + \n                         xTest['noelec'] + \n                         xTest['pisonotiene'] + \n                         xTest['abastaguano'] + \n                         (xTest['cielorazo'] == 0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Owns a refrigerator, computer, tablet, and television\nxTest['bonus'] = 1 * (xTest['refrig'] + \n                      xTest['computer'] + \n                      (xTest['v18q1'] > 0) + \n                      xTest['television'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Per capita features\nxTest['phones-per-capita'] = xTest['qmobilephone'] / xTest['tamviv']\nxTest['tablets-per-capita'] = xTest['v18q1'] / xTest['tamviv']\nxTest['rooms-per-capita'] = xTest['rooms'] / xTest['tamviv']\nxTest['rent-per-capita'] = xTest['v2a1'] / xTest['tamviv']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xTest['tech'] = xTest['v18q'] + xTest['mobilephone']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_null = xTest.isnull().sum()\ntest_null[test_null != 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xTest = xTest.drop(['idhogar'], axis = 1)\n\nxTest['dependency'].replace('no', 0, inplace = True)\nxTest['edjefe'].replace('no', 0, inplace = True)\nxTest['edjefa'].replace('no', 0, inplace = True)\n\nxTest['dependency'].replace('yes', 1, inplace = True)\nxTest['edjefe'].replace('yes', 1, inplace = True)\nxTest['edjefa'].replace('yes', 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# labor force\nxTest['adult'] = xTest['hogar_adul'] - xTest['hogar_mayor']\nxTest['dependency_count'] = xTest['hogar_nin'] + xTest['hogar_mayor']\nxTest['dependency2'] = xTest['dependency_count'] / xTest['adult']\nxTest['child_percent'] = xTest['hogar_nin']/xTest['hogar_total']\nxTest['elder_percent'] = xTest['hogar_mayor']/xTest['hogar_total']\nxTest['adult_percent'] = xTest['hogar_adul']/xTest['hogar_total']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xTest['rent_per_adult'] = xTest['v2a1']/xTest['hogar_adul']\nxTest['rent_per_person'] = xTest['v2a1']/xTest['hhsize']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# male-female ratio\nxTest['r4h1_percent_in_male'] = xTest['r4h1'] / xTest['r4h3']\nxTest['r4m1_percent_in_female'] = xTest['r4m1'] / xTest['r4m3']\nxTest['r4h1_percent_in_total'] = xTest['r4h1'] / xTest['hhsize']\nxTest['r4m1_percent_in_total'] = xTest['r4m1'] / xTest['hhsize']\nxTest['r4t1_percent_in_total'] = xTest['r4t1'] /xTest['hhsize']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# per capito\nxTest['rent_per_bedroom'] = xTest['v2a1']/xTest['bedrooms']\nxTest['edler_per_bedroom'] = xTest['hogar_mayor']/xTest['bedrooms']\nxTest['adults_per_bedroom'] = xTest['adult']/xTest['bedrooms']\nxTest['child_per_bedroom'] = xTest['hogar_nin']/xTest['bedrooms']\nxTest['male_per_bedroom'] = xTest['r4h3']/xTest['bedrooms']\nxTest['female_per_bedroom'] = xTest['r4m3']/xTest['bedrooms']\nxTest['bedrooms_per_person_household'] = xTest['hhsize']/xTest['bedrooms']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# when there is no adult in the house, the result will be Nan\nxTest[\"rent_per_adult\"] = xTest[\"rent_per_adult\"].fillna(0)\n\n# similar to male/female ratio\nxTest[\"r4h1_percent_in_male\"] = xTest[\"r4h1_percent_in_male\"].fillna(0)\nxTest[\"r4m1_percent_in_female\"] = xTest[\"r4m1_percent_in_female\"].fillna(0)\n\n# when there is no adult in the household, the value will be inf. So, we should fill in some very large number \nxTest.dependency2 = xTest.dependency2.fillna(1e5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_null = xTest.isnull().sum()\ntest_null[test_null != 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = xTest.T\nX_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction = predict(X_test, parameters)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.Series(prediction).value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction[prediction==0] = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['Target'] = prediction\ndf2 = pd.DataFrame({'Id':test['Id'],'Target':test.Target})\ndf2.to_csv(\"submit.csv\",index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}