{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom tqdm import tqdm\nimport os\nprint(os.listdir(\"../input\"))\nimport lightgbm as lgb\nimport gc\nimport featuretools as ft\nes = ft.EntitySet()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7a30c69feef4f83dae947c06609c4a21bc5e9ac5"},"cell_type":"code","source":"path = \"../input/elo-merchant-category-recommendation/\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"66950927223ebd64bd623c7deecc9768c2f706ed"},"cell_type":"code","source":"train = pd.read_csv(path+'train.csv')\ntest = pd.read_csv(path+'test.csv')\nID=test['card_id']\nytrain  = train['target']\nxtrain = train.drop('target',axis=1)\nalldata = pd.concat([test,xtrain])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#merchants = pd.read_csv(path+'merchants.csv')\n#historical_transactions = pd.read_csv(path+'historical_transactions.csv')\n#new_merchant_transactions = pd.read_csv(path+'new_merchant_transactions.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c7c4424f2ffb11fc76e5ad192f68717e87bc8fbb"},"cell_type":"code","source":"#减少数据集占用的内存\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in tqdm(df.columns):\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eaf8c083425cc86a61d4cbb6ea4878fdeb09493f"},"cell_type":"code","source":" #number_feature\nqnum = ['card_id','installments','purchase_amount','numerical_1', 'numerical_2','avg_sales_lag3', 'avg_purchases_lag3',\n        'avg_sales_lag6', 'avg_purchases_lag6','avg_sales_lag12','avg_purchases_lag12']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c9194749c6a2aff4a33c293e3c05c3278768f92e"},"cell_type":"code","source":" def fm_num(qnum)  :\n    merchants = pd.read_csv(path+'merchants.csv')\n    historical_transactions = pd.read_csv(path+'new_merchant_transactions.csv')\n#    historical_transactions = pd.concat([historical_transactions,pd.read_csv(path+'new_merchant_transactions.csv')],axis=1)\n    historical_transactions.drop(['category_1','category_2','merchant_category_id'],axis=1,inplace=True)\n    merchants.drop(['city_id','state_id','subsector_id'],axis=1,inplace=True)\n    historical_transactions = reduce_mem_usage(historical_transactions)\n    merchants = reduce_mem_usage(merchants)\n    historical_transactions = pd.merge(historical_transactions,merchants,on='merchant_id',how='left')\n    historical_transactions = reduce_mem_usage(historical_transactions)\n\n    x1=historical_transactions[qnum]\n    \n    del historical_transactions\n    del merchants\n    print(gc.collect())\n    \n    #开始使用featuretools\n    import featuretools as ft\n    es = ft.EntitySet()\n    es = es.entity_from_dataframe(entity_id=\"traindf\",#设定聚合id\n                                dataframe=alldata,#加载主数据集\n                                index=\"card_id\",\n                       #         time_index=\"first_active_month\",\n                                  #Set Categorica Featrue# 设定类别特征，不设定默认为数值特征\n                                variable_types={\"feature_1\": ft.variable_types.Categorical,\n                                               \"feature_2\": ft.variable_types.Categorical,\n                                               \"feature_3\": ft.variable_types.Categorical}\n                                 )\n\n    x1.reset_index(inplace=True)\n    es = es.entity_from_dataframe(entity_id=\"1df\",\n                                dataframe=x1,#加载需合并的数据集，如有\n                       #         time_index=\"first_active_month\",\n                                 )\n    new_relationship = ft.Relationship(es[\"traindf\"][\"card_id\"],#设定数据集之间关联字段，相当于merge\n                                       es[\"1df\"][\"card_id\"])\n    es = es.add_relationship(new_relationship)\n\n    del x1\n    print(gc.collect())\n\n    feature_matrix,feature_defs = ft.dfs(entityset=es,\n                                          target_entity=\"traindf\",\n                                          verbose=True,\n                                    #      n_jobs=2\n                                         )\n    feature_matrix = reduce_mem_usage(feature_matrix)\n    es = ft.EntitySet()\n    print(gc.collect())\n    return feature_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c723bf5ea4a034b48f62e7c155750d5eddcf1a5d"},"cell_type":"code","source":"feature_matrix1 = fm_num(qnum)\nfeature_matrix1=feature_matrix1.reset_index()\nfeature_matrix1.to_csv('new_num.csv',index=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4e98c372d86e5e14137a7b9ba1fea8e644200d11"},"cell_type":"markdown","source":"qnum = ['installments','purchase_amount','numerical_1', 'numerical_2','avg_sales_lag3', 'avg_purchases_lag3',\n        'avg_sales_lag6', 'avg_purchases_lag6','avg_sales_lag12','avg_purchases_lag12']"},{"metadata":{"trusted":true,"_uuid":"9b1c2c3b43331078d46e8a324cc3073a107bcc77"},"cell_type":"markdown","source":"def fm_cat(qnum)  :\n    merchants = pd.read_csv(path+'merchants.csv')\n    historical_transactions = pd.read_csv(path+'new_merchant_transactions.csv')\n   # historical_transactions = pd.concat([historical_transactions,pd.read_csv(path+'new_merchant_transactions.csv')],axis=1)\n    historical_transactions.drop(['category_1','category_2','merchant_category_id'],axis=1,inplace=True)\n    merchants.drop(['city_id','state_id','subsector_id'],axis=1,inplace=True)\n    historical_transactions = reduce_mem_usage(historical_transactions)\n    merchants = reduce_mem_usage(merchants)\n    historical_transactions = pd.merge(historical_transactions,merchants,on='merchant_id',how='left')\n    historical_transactions = reduce_mem_usage(historical_transactions)\n    \n    x1=historical_transactions.drop(qnum,axis=1)\n    \n    del historical_transactions\n    del merchants\n    print(gc.collect())\n    import featuretools as ft\n    es = ft.EntitySet()    \n    \n    #Set Categorica Featrue\n    variable_types={\"merchant_id\": ft.variable_types.Categorical,\n        \"merchant_group_id\": ft.variable_types.Categorical,\n       \"merchant_category_id\": ft.variable_types.Categorical,\n       \"subsector_id\": ft.variable_types.Categorical,\n       \"category_1\": ft.variable_types.Categorical,\n       \"most_recent_purchases_range\": ft.variable_types.Categorical,\n       \"most_recent_sales_range\": ft.variable_types.Categorical,\n       \"active_months_lag3\": ft.variable_types.Categorical,\n       \"active_months_lag6\": ft.variable_types.Categorical,\n       \"active_months_lag12\": ft.variable_types.Categorical,\n       \"category_4\": ft.variable_types.Categorical,\n       \"city_id\": ft.variable_types.Categorical,\n       \"state_id\": ft.variable_types.Categorical,\n       \"category_2\": ft.variable_types.Categorical,\n        'category_3': ft.variable_types.Categorical,\n         \"month_lag\": ft.variable_types.Categorical,}\n    \n\n    es = es.entity_from_dataframe(entity_id=\"traindf\",\n                                dataframe=alldata,\n                                index=\"card_id\",\n                       #         time_index=\"first_active_month\",\n                                variable_types={\"feature_1\": ft.variable_types.Categorical,\n                                               \"feature_2\": ft.variable_types.Categorical,\n                                               \"feature_3\": ft.variable_types.Categorical}\n                                 )\n\n    x1.reset_index(inplace=True)\n    es = es.entity_from_dataframe(entity_id=\"1df\",\n                                dataframe=x1,\n                                time_index='purchase_date',\n                                variable_types=variable_types\n                                 )\n    new_relationship = ft.Relationship(es[\"traindf\"][\"card_id\"],\n                                       es[\"1df\"][\"card_id\"])\n    es = es.add_relationship(new_relationship)\n\n    del x1\n    print(gc.collect())\n\n    feature_matrix,feature_defs = ft.dfs(entityset=es,\n                                          target_entity=\"traindf\",\n                                          verbose=True,\n                                   #       n_jobs=2\n                                         )\n    feature_matrix = reduce_mem_usage(feature_matrix)\n    es = ft.EntitySet()\n    print(gc.collect())\n    return feature_matrix"},{"metadata":{"trusted":true,"_uuid":"cc68544f1676ae14adfb28be85aec21935840d39"},"cell_type":"markdown","source":"feature_matrix2 = fm_cat(qnum)\nfeature_matrix2=feature_matrix2.reset_index()\nfeature_matrix2.to_csv('new_cat.csv',index=None)"},{"metadata":{"trusted":true,"_uuid":"1ade14cf28c9e7f68086269a06701a8dd58ad6b5"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}