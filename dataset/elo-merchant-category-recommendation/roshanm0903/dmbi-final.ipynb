{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Sampling\nCode to generate samples from training data. (td_<ratio>.csv)\nAll transactions related to the cards_id in the samples would be extracted from historical_transactions.csv and new_merchant_period.csv (hmd_<ratio>.csv)\n    \nNote: This sampling is only for the purposes of finalizing the workflow and for testing purposes as the time to run these for the entire dataset is high.","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom pandas import DataFrame as df\n\n# train_data = pd.read_csv('../input/elo-merchant-category-recommendation/train.csv')\n# td_card = np.unique(train_data['card_id']) # 201917\n\n# ratio = 0.01\n# card_num = int(np.size(td_card, 0) * ratio)\n# sampled_card = np.random.choice(td_card, card_num)\n# del td_card\n\n# samp_c = df(sampled_card,columns=['card_id'])\n# del sampled_card\n\n# filter1 = train_data[\"card_id\"].isin(samp_c.card_id.tolist())\n# td = train_data[filter1]\n# del train_data, filter1\n\n# print(\"Reading historical_transactions...............\")\n# historical_data = pd.read_csv('../input/elo-merchant-category-recommendation/historical_transactions.csv')\n# filter2 = historical_data[\"card_id\"].isin(samp_c.card_id.tolist())\n# hd = historical_data[filter2]\n# del historical_data, filter2\n\n# print(\"Reading new_merchant_transactions...............\")\n# merchant_data = pd.read_csv('../input/elo-merchant-category-recommendation/new_merchant_transactions.csv')\n# filter3 = merchant_data[\"card_id\"].isin(samp_c.card_id.tolist())\n# md = merchant_data[filter3]\n# del merchant_data, filter3\n\n# frames = [hd, md]\n# hmd = pd.concat(frames)\n# print(hmd)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preparation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom pandas import DataFrame as df\nimport datetime\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nimport gc\n\ntransaction_data = pd.read_csv('../input/elodmbiproject/hmd01_1.csv',parse_dates=['purchase_date'])\n\n\n# uncomment the following lines to run the model on the entire dataset\n# hist = pd.read_csv('../input/elo-merchant-category-recommendation/historical_transactions.csv', parse_dates=['purchase_date'])\n# print(hist.shape)\n\n# new_merch = pd.read_csv('../input/elo-merchant-category-recommendation/new_merchant_transactions.csv', parse_dates=['purchase_date'])\n# print(new_merch.shape)\n\n\n# frames = [hist, new_merch]\n# transaction_data = pd.concat(frames)\n# print(transaction_data.shape)\n# del hist, new_merch\n# gc.collect()\n\n\n\n#delete inconsistent data\nindexNames = transaction_data[ transaction_data['city_id'] == -1].index\ntransaction_data.drop(indexNames , inplace=True)\n\nindexNames = transaction_data[ transaction_data['category_2'].isna() ].index\ntransaction_data.drop(indexNames , inplace=True)\n\nindexNames = transaction_data[ transaction_data['installments'] == -1].index\ntransaction_data.drop(indexNames , inplace=True)\n\nindexNames = transaction_data[ transaction_data['installments'] == 999].index\ntransaction_data.drop(indexNames , inplace=True)\n\nindexNames = transaction_data[ transaction_data['purchase_amount'] > 1].index\ntransaction_data.drop(indexNames , inplace=True)\n\n\n# dropping ALL duplicate values \ntransaction_data.drop_duplicates(subset =['card_id','merchant_id','purchase_date' ], keep = 'first', inplace = True) \n\ntransaction_data['subsector_id_copy'] = transaction_data['subsector_id']\n\n#replace other missing values with 0\ntransaction_data  = transaction_data.fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#encoding Y=1, N=0\ntransactions = transaction_data\n\ntransactions['authorized_flag'] = transactions['authorized_flag'].map({'Y':1, 'N':0})\ntransactions[\"category_1\"] = transactions[\"category_1\"].map({'Y':1, 'N':0})\n\n#one hot encoding\ntransactions = pd.get_dummies(transactions, columns=['category_2', 'category_3', 'subsector_id'])\n\n\ntransactions['month_diff'] = ((datetime.datetime.today() - transactions['purchase_date']).dt.days)//30\ntransactions['month_diff'] += transactions['month_lag']\n\ndel transaction_data\ntransactions[:5]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#joining merchant data\n\nmerchants = pd.read_csv('../input/elo-merchant-category-recommendation/merchants.csv')\nmerchants[\"category_1\"] = merchants[\"category_1\"].map({'Y':1, 'N':0})\nmerchants[\"category_4\"] = merchants[\"category_4\"].map({'Y':1, 'N':0})\nmerchants = merchants.rename(columns={\"category_1\": \"mer_category_1\", \"category_2\": \"mer_category_2\",\"category_4\": \"mer_category_4\", \"city_id\":\"mer_city_id\"})\n\nprint(\"merchants file read \")\n\n#columns selected \nmerchant_columns = [\"merchant_id\", \"mer_category_4\", \"mer_city_id\", \"merchant_group_id\" ]\n\nmerchants =merchants[merchant_columns]\n\ntransactions = pd.merge(transactions, merchants, on='merchant_id', how='left')\n\ntransactions[\"same_city\"]=np.where(transactions['city_id'] == transactions['mer_city_id'] , 1, 0)\ndel merchants","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Aggregation of data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def aggregate_transactions(data):\n    \n    data.loc[:, 'purchase_date'] = pd.DatetimeIndex(data['purchase_date']).\\\n                astype(np.int64) * 1e-9\n     \n    agg_func = {\n    'authorized_flag': ['mean'],\n    'category_1': ['mean'],\n    'category_2_1.0': ['mean'],\n    'category_2_2.0': ['mean'],\n    'category_2_3.0': ['mean'],\n    'category_2_4.0': ['mean'],\n    'category_2_5.0': ['mean'],\n    'category_3_A': ['mean'],\n    'category_3_B': ['mean'],\n    'category_3_C': ['mean'],\n    'merchant_id': ['nunique'],\n    'merchant_category_id': ['nunique'],\n    'merchant_group_id': ['nunique'],\n\n    'mer_category_4': ['mean'],\n    \n    'state_id': ['nunique'],\n    'city_id': ['nunique'],\n    'same_city': ['mean'],\n        \n#       'subsector_id': ['nunique'],  \n    'subsector_id_copy': ['nunique'],\n    'subsector_id_1':['mean'],\n    'subsector_id_2':['mean'],\n    'subsector_id_3':['mean'],\n    'subsector_id_4':['mean'],\n    'subsector_id_5':['mean'],\n    'subsector_id_7':['mean'],\n    'subsector_id_8':['mean'],\n    'subsector_id_9':['mean'],\n    'subsector_id_10':['mean'],\n    'subsector_id_11':['mean'],\n    'subsector_id_12':['mean'],\n    'subsector_id_13':['mean'],\n    'subsector_id_14':['mean'],\n    'subsector_id_15':['mean'],\n    'subsector_id_16':['mean'],\n    'subsector_id_17':['mean'],\n    'subsector_id_18':['mean'],\n    'subsector_id_19':['mean'],\n    'subsector_id_20':['mean'],\n    'subsector_id_21':['mean'],\n    'subsector_id_22':['mean'],\n    'subsector_id_23':['mean'],\n    'subsector_id_24':['mean'],\n    'subsector_id_25':['mean'],\n    'subsector_id_26':['mean'],\n    'subsector_id_27':['mean'],\n    'subsector_id_29':['mean'],\n    'subsector_id_30':['mean'],\n    'subsector_id_31':['mean'],\n    'subsector_id_32':['mean'],\n    'subsector_id_33':['mean'],\n    'subsector_id_34':['mean'],\n    'subsector_id_35':['mean'],\n    'subsector_id_36':['mean'],\n    'subsector_id_37':['mean'],\n    'subsector_id_38':['mean'],\n    'subsector_id_39':['mean'],\n    'subsector_id_40':['mean'],\n    'subsector_id_41':['mean'],\n    \n    'purchase_amount': ['sum', 'mean', 'max', 'min', 'std'],\n    'installments': ['sum', 'mean', 'max', 'min', 'std'],\n    'purchase_date': [np.ptp, 'min', 'max'],\n    'month_diff': ['mean', 'max', 'min', 'std'],\n        \n    }\n    \n    agg_transactions = data.groupby(['card_id']).agg(agg_func)\n    \n    agg_transactions.columns = ['_'.join(col).strip() for col in agg_transactions.columns.values]\n    agg_transactions.reset_index(inplace=True)\n    \n    temp_df = (data.groupby('card_id')\n          .size()\n          .reset_index(name='transactions_count'))\n    \n    agg_transactions = pd.merge(temp_df, agg_transactions, on='card_id', how='left')\n    \n    return agg_transactions\n\n\nagg_transactions = aggregate_transactions(transactions)\n\n# uncomment this line to run the models on the entire dataset \n# agg_transactions = pd.read_csv('../input/elodmbiproject/agg_transactions.csv')\n\nagg_transactions[:5]\n\ndel transactions\ngc.collect()\n\n\n#scaling\n\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\n\n\ncolumns_to_scale = [c for c in agg_transactions.columns if c not in ['card_id', 'Unnamed: 0']]\n\nagg_transactions[columns_to_scale ] = scaler.fit_transform(agg_transactions[columns_to_scale])\n\nagg_transactions[:5]\n\n# print(\"writing to csv.........\")\n# agg_transactions.to_csv(r'/kaggle/working/agg_transactions.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Join aggregated data with the training data\n\ndef read_data(input_file):\n    df = pd.read_csv(input_file)\n    df['first_active_month'] = pd.to_datetime(df['first_active_month'])\n    df['elapsed_time'] = (datetime.date(2018, 2, 1) - df['first_active_month'].dt.date).dt.days\n    return df\n\ntrain = read_data('../input/elodmbiproject/td01_1.csv')\n\n#uncomment the following line to run on the entire dataset\n\n# train = read_data('../input/elo-merchant-category-recommendation/train.csv')\nprint(\"training file read \")\n\ntrain = pd.merge(train, agg_transactions, on='card_id', how='left')\n# train.drop(columns=['Unnamed: 0'])\ntrain[['elapsed_time']] = scaler.fit_transform(train[['elapsed_time']])\n\ntrain = pd.get_dummies(train, columns=['feature_1', 'feature_2'])\n\ntrain  = train.fillna(0)\n\ntrain_split = np.split(train, [int(0.9*train.shape[0])] , axis=0)\ntrain_data = train_split[0]\ntest_data = train_split[1]\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#correlations\n\nfeatures = [c for c in train.columns if c not in ['card_id', 'first_active_month', 'Unnamed: 0','target']]\n\nfor colm in features:\n    print(colm ,\" :  \", train[\"target\"].corr(train[colm]) )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# OLS Linear Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport statsmodels.api as sm\n\nnew_train_data = train_data\n\n#Fit linear model\nmodel = sm.OLS(new_train_data[\"target\"],new_train_data[features])\nresults = model.fit()\n\n#create instance of influence\ninfluence = results.get_influence()\n\n#leverage (hat values)\nleverage = influence.hat_matrix_diag\n\n#Cook's D values (and p-values) as tuple of arrays\ncooks_d = influence.cooks_distance\n\n#standardized residuals\nstandardized_residuals = influence.resid_studentized_internal\n\nplt.plot(standardized_residuals ,'.')\nplt.plot(cooks_d[0] ,'.')\n\n# qq plot\nres = results.resid # residuals\nfig = sm.qqplot(res,line='45')\n\nprint(\"Rsq\", results.rsquared)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# OLS after removing someoutliers\nimport matplotlib.pyplot as plt\n\nindexNames = train[ abs(train['target']) > 10].index\ntrain.drop(indexNames , inplace=True)\ntrain_split = np.split(train, [int(0.9*train.shape[0])] , axis=0)\ntrain_data = train_split[0]\ntest_data = train_split[1]\n\n\nnew_train_data = train_data\n\n#Fit linear model\nmodel = sm.OLS(new_train_data[\"target\"],new_train_data[features])\nresults = model.fit()\n\n#create instance of influence\ninfluence = results.get_influence()\n\n#leverage (hat values)\nleverage = influence.hat_matrix_diag\n\n#Cook's D values (and p-values) as tuple of arrays\ncooks_d = influence.cooks_distance\n\n#standardized residuals\nstandardized_residuals = influence.resid_studentized_internal\n\nplt.plot(standardized_residuals ,'.')\nplt.plot(cooks_d[0] ,'.')\n\n# qq plot\nres = results.resid # residuals\nfig = sm.qqplot(res,line='45')\n\nprint(\"Rsq\", results.rsquared)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ensemble Learning\n\n\nReference: https://www.kaggle.com/fabiendaniel/elo-world#3.-Training-the-model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"param = {'num_leaves': 105,\n         'min_data_in_leaf': 66, \n         'objective':'regression',\n         'max_depth': 8,\n         'learning_rate': 0.005,\n         \"boosting\": \"gbdt\",    #change it to rf for random forrests\n         \"feature_fraction\": 0.733,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.9597 ,\n         \"bagging_seed\": 11,\n         \"metric\": 'rmse',\n         \"lambda_l1\": 2.251,\n         \"random_state\": 133,\n         \"verbosity\": -1}\n\n\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\n\nfolds = KFold(n_splits=5, shuffle=True, random_state=9)\n\noof = np.zeros(len(train_data))\npredictions = np.zeros(len(test_data))\n\nfeature_importance_df = pd.DataFrame()\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train_data.values, train_data[\"target\"].values)):\n    print(\"fold n°{}\".format(fold_))\n    trn_data = lgb.Dataset(train_data.iloc[trn_idx][features],\n                           label=train_data[\"target\"].iloc[trn_idx],\n                             )\n    val_data = lgb.Dataset(train_data.iloc[val_idx][features],\n                           label=train_data[\"target\"].iloc[val_idx],\n                            )\n\n    num_round = 10000\n    clf = lgb.train(param,\n                    trn_data,\n                    num_round,\n                    valid_sets = [trn_data, val_data],\n                    verbose_eval=100,\n                    early_stopping_rounds = 200)\n    \n    oof[val_idx] = clf.predict(train_data.iloc[val_idx][features], num_iteration=clf.best_iteration)\n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"feature\"] = features\n    fold_importance_df[\"importance\"] = clf.feature_importance()\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    predictions += clf.predict(test_data[features], num_iteration=clf.best_iteration) / folds.n_splits\n\nprint(\"CV score: {:<8.5f}\".format(mean_squared_error(oof, train_data[\"target\"])**0.5))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Feature importance visualization\nReference: https://www.kaggle.com/fabiendaniel/elo-world#4.-Feature-importance","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = (feature_importance_df[[\"feature\", \"importance\"]]\n        .groupby(\"feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:1000].index)\n\nbest_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]\n\nplt.figure(figsize=(14,25))\nsns.barplot(x=\"importance\",\n            y=\"feature\",\n            data=best_features.sort_values(by=\"importance\",\n                                           ascending=False))\nplt.title('LightGBM Features (avg over folds)')\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Bayseian Optimization\nReference : https://www.kaggle.com/fabiendaniel/hyperparameter-tuning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport pandas as pd\nfrom datetime import datetime\nimport gc\nimport warnings\nfrom bayes_opt import BayesianOptimization\n\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold\nimport warnings\nimport time\nimport sys\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import mean_squared_error\nfrom datetime import datetime\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\ndef timer(start_time=None):\n    if not start_time:\n        start_time = datetime.now()\n        return start_time\n    elif start_time:\n        thour, temp_sec = divmod(\n            (datetime.now() - start_time).total_seconds(), 3600)\n        tmin, tsec = divmod(temp_sec, 60)\n        print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))\n        \n\ndef reduce_mem_usage(df):\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def LGB_CV(\n          max_depth,\n          num_leaves,\n          min_data_in_leaf,\n          feature_fraction,\n          bagging_fraction,\n          lambda_l1\n         ):\n    \n    folds = KFold(n_splits=5, shuffle=True, random_state=15)\n    oof = np.zeros(train.shape[0])\n\n    for fold_, (trn_idx, val_idx) in enumerate(folds.split(train.values, target.values)):\n        print(\"fold n°{}\".format(fold_))\n        trn_data = lgb.Dataset(train.iloc[trn_idx][features],\n                               label=target.iloc[trn_idx],\n                               categorical_feature=categorical_feats)\n        val_data = lgb.Dataset(train.iloc[val_idx][features],\n                               label=target.iloc[val_idx],\n                               categorical_feature=categorical_feats)\n    \n        param = {\n            'num_leaves': int(num_leaves),\n            'min_data_in_leaf': int(min_data_in_leaf), \n            'objective':'regression',\n            'max_depth': int(max_depth),\n            'learning_rate': 0.01,\n            \"boosting\": \"gbdt\",\n            \"feature_fraction\": feature_fraction,\n            \"bagging_freq\": 1,\n            \"bagging_fraction\": bagging_fraction ,\n            \"bagging_seed\": 11,\n            \"metric\": 'rmse',\n            \"lambda_l1\": lambda_l1,\n            \"verbosity\": -1\n        }\n    \n        clf = lgb.train(param,\n                        trn_data,\n                        10000,\n                        valid_sets = [trn_data, val_data],\n                        verbose_eval=500,\n                        early_stopping_rounds = 200)\n        \n        oof[val_idx] = clf.predict(train.iloc[val_idx][features],\n                                   num_iteration=clf.best_iteration)\n        \n        del clf, trn_idx, val_idx\n        gc.collect()\n        \n    return -mean_squared_error(oof, target)**0.5\n\ntrain = pd.read_csv(\"../input/elodmbiproject/td01_1.csv\", index_col=0)\ntrain = reduce_mem_usage(train)\n\ntarget = train['target']\ndel train['target']\n\nfeatures = [c for c in train.columns if c not in ['card_id', 'first_active_month']]\ncategorical_feats = [c for c in features if 'feature_' in c]\n\nLGB_BO = BayesianOptimization(LGB_CV, {\n    'max_depth': (4, 10),\n    'num_leaves': (5, 130),\n    'min_data_in_leaf': (10, 150),\n    'feature_fraction': (0.7, 1.0),\n    'bagging_fraction': (0.7, 1.0),\n    'lambda_l1': (0, 6)\n    })\n\nprint('-'*126)\n\nstart_time = timer(None)\nwith warnings.catch_warnings():\n    warnings.filterwarnings('ignore')\n    LGB_BO.maximize(init_points=2, n_iter=20, acq='ei', xi=0.0)\ntimer(start_time)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}