{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport datetime\nimport time\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom sklearn.linear_model import Ridge, BayesianRidge\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import log_loss\nfrom xgboost import plot_importance","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train=pd.read_csv('../input/train.csv')\ntest=pd.read_csv('../input/test.csv')\nhist=pd.read_csv('../input/historical_transactions.csv')\nnew=pd.read_csv('../input/new_merchant_transactions.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"909684add983374a02698586368ea8fe56bdd245"},"cell_type":"code","source":"#缺失值填充\nfor df in [hist,new]:\n    df['category_2'].fillna(1.0,inplace=True)\n    df['category_3'].fillna('A', inplace=True)\n    df['merchant_id'].fillna('M_ID_00a6ca8a8a', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"880dc4a5e2c804320e5040aba92ef624123e9ad6"},"cell_type":"code","source":"#日期处理及分类特征探索\nhist = pd.get_dummies(hist, columns=['category_2', 'category_3'])\nnew=pd.get_dummies(new,columns=['category_2', 'category_3'])\n\nfor df in [hist,new]:\n    df['purchase_date']=pd.to_datetime(df['purchase_date'])## 将交易日期由字符串改为时间变量\n    df['year']=df['purchase_date'].dt.year\n    df['month']=df['purchase_date'].dt.month\n    df['weekofyear']=df['purchase_date'].dt.weekofyear#一年中的第几周\n    df['dayofweek']=df['purchase_date'].dt.dayofweek#一周中的第几天\n    df['weekend'] =(df['purchase_date'].dt.weekday>=5).astype(int)#星期一到星期日是0-6表示\n    df['hour'] = df['purchase_date'].dt.hour\n    df['category_1']=df['category_1'].map({'Y':1,'N':0})\n    df['authorized_flag']=df['authorized_flag'].map({'Y':1, 'N':0})\n    df['month_diff'] = ((datetime.datetime.today() - df['purchase_date']).dt.days) //30#取整\n    \nhist.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8e530c50bd37df297ce7cf3fc01f0a26f8a16f90"},"cell_type":"code","source":"#特征工程\ndef aggregate_data(df,prefix):\n    df['purchase_date']=pd.DatetimeIndex(df['purchase_date']).astype(np.int64) * 1e-9\n    agg_fun={'authorized_flag':['count','sum','mean'],\n             'category_1':['sum','mean'],\n             'category_2_1.0':['sum','mean'],\n             'category_2_2.0': ['sum', 'mean'],\n             'category_2_3.0': ['sum', 'mean'],\n             'category_2_4.0': ['sum', 'mean'],\n             'category_2_5.0': ['sum', 'mean'],\n             'category_3_A': ['sum', 'mean'],\n             'category_3_B': ['sum', 'mean'],\n             'category_3_C': ['sum', 'mean'],\n             'merchant_id': ['nunique'],\n             'merchant_category_id':['nunique'],\n             'state_id': ['nunique'],\n             'city_id': ['nunique'],\n             'purchase_date': ['min','max',np.ptp],\n             'purchase_amount': ['sum', 'mean', 'max', 'min', 'std'],\n             'installments': ['sum', 'mean', 'max', 'min', 'std'],\n             'month_lag': ['mean','min','max'],\n             'subsector_id':['nunique'],\n             'year':['nunique'],\n             'month':['nunique'],\n             'weekofyear':['nunique'],\n             'weekend': ['sum','mean'],\n             'hour': ['nunique'],\n             'dayofweek': ['nunique'],\n             'month_diff':['mean']\n             }\n    agg_trans=df.groupby('card_id').agg(agg_fun)\n    agg_trans.columns=[prefix+'_'.join(col).strip() for col in agg_trans.columns.values]\n    agg_trans.reset_index(inplace=True)\n    return agg_trans\nhist1=aggregate_data(hist,'hist_')\nnew1=aggregate_data(new,'new_')\nhist1.info()\nnew1.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e1113d5379126a225ee838428ce1a56c43f5b72b"},"cell_type":"code","source":"#生成训练集和测试集\ntrain['card_id']=train['card_id'].astype(str)\nhist1['card_id']=hist1['card_id'].astype(str)\ntraindata=pd.merge(train,hist1,on='card_id',how='left')\ntraindata=pd.merge(traindata,new1,on='card_id',how='left')\ntestdata=pd.merge(test,hist1,on='card_id',how='left')\ntestdata=pd.merge(testdata,new1,on='card_id',how='left')\ntraindata.head()\n#最近一次交易和平均购买间隔\nfor df in [traindata,testdata]:\n    df['hist_new_average']=(df['new_purchase_date_ptp']+df['hist_purchase_date_ptp'])/(df['new_authorized_flag_count']+df['hist_authorized_flag_count'])\n    df['hist_average']=df['hist_purchase_date_ptp']/df['hist_authorized_flag_count']\n    df['new_average']=df['new_purchase_date_ptp']/df['new_authorized_flag_count']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ef8ccb4bc4b5bc102f1ee7519dff070a3a219184"},"cell_type":"code","source":"train=traindata\ntest=testdata\ntarget=train['target']\ndel train['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ce59a11fe5583f47bdd336a3dd265122237f1e1c"},"cell_type":"code","source":"features = [c for c in train.columns if c not in ['card_id', 'first_active_month']]\ncategorical_feats = ['feature_2', 'feature_3']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"866f3de8b7936168f2c981c16099358e03b36c20"},"cell_type":"code","source":"#lgb\nparam = {'objective':'regression',\n         'num_leaves': 80,\n         'min_data_in_leaf': 25,\n         'max_depth': 7,\n         'learning_rate': 0.01,\n         'lambda_l1':0.13,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\":0.85,\n         'bagging_freq':8,\n         \"bagging_fraction\": 0.9 ,\n         \"metric\": 'rmse',\n         \"verbosity\": -1,\n         \"random_state\": 2333}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e72be44f89c59a7f87a8fe608ee1d0595f6bc08f"},"cell_type":"code","source":"folds = KFold(n_splits=5, shuffle=True, random_state=15)\noof = np.zeros(len(train))\npredictions = np.zeros(len(test))\nstart = time.time()\nfeature_importance_df = pd.DataFrame()\ntrain.head(5)\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train.values, target.values)):\n    print(\"fold n°{}\".format(fold_))\n    trn_data = lgb.Dataset(train.iloc[trn_idx][features],\n                           label=target.iloc[trn_idx],\n                           categorical_feature=categorical_feats)\n    val_data = lgb.Dataset(train.iloc[val_idx][features],\n                           label=target.iloc[val_idx],\n                           categorical_feature=categorical_feats )\n    num_round = 10000\n    clf = lgb.train(param,\n                    trn_data,\n                    num_round,\n                    valid_sets = [trn_data, val_data],\n                    verbose_eval=100,\n                    early_stopping_rounds = 200)\n    \n    oof[val_idx] = clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"feature\"] = features\n    fold_importance_df[\"importance\"] = clf.feature_importance()\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    predictions += clf.predict(test[features], num_iteration=clf.best_iteration) / folds.n_splits\n\nprint(\"CV score: {:<8.5f}\".format(mean_squared_error(oof, target)**0.5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f1e702064d16da34da4e64c515bbc905688bef29"},"cell_type":"code","source":"cols = (feature_importance_df[[\"feature\", \"importance\"]]\n        .groupby(\"feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:1000].index)\n\nbest_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]\n\nplt.figure(figsize=(14,25))\nsns.barplot(x=\"importance\",\n            y=\"feature\",\n            data=best_features.sort_values(by=\"importance\",\n                                           ascending=False))\nplt.title('LightGBM Features (avg over folds)')\nplt.tight_layout()\nplt.savefig('lgbm_importances.png')\n# sample_submission = pd.read_csv('../input/sample_submission.csv')\n# sample_submission['target'] = predictions\n# sample_submission.to_csv('submission_lgb.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"191a5d47d5ebff006625309e1e645a9f94cf0808"},"cell_type":"code","source":"##xgb model\nxgb_params = {\n    'booster': 'gbtree',\n    'objective': 'reg:linear',\n    'gamma': 0.1,\n    'max_depth': 6,\n    'eval_metric':'rmse',\n    'lambda': 2,\n    'subsample': 0.7,\n    'colsample_bytree': 0.7,\n    'min_child_weight': 3,\n    'silent': 1,\n    'eta': 0.1,\n    'seed': 1000,\n    'nthread': 4,\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0ea4ccf686871e7eedc6d379f5de00b45d8b55af"},"cell_type":"code","source":"folds= KFold(n_splits=5, shuffle=True, random_state=15)\noof_xgb = np.zeros(len(train))\npredictions_xgb = np.zeros(len(test))\nfor fold_,(trn_idx,val_idx) in enumerate(folds.split(train.values, target.values)):\n    print(\"fold n°{}\".format(fold_))\n    trn_data = xgb.DMatrix(train.iloc[trn_idx][features],\n                           label=target.iloc[trn_idx],\n                           #categorical_feature=categorical_feats\n                          )\n    val_data = xgb.DMatrix(train.iloc[val_idx][features],\n                           label=target.iloc[val_idx],\n                           #categorical_feature=categorical_feats \n                          )\n    watchlist = [(trn_data, 'train'), (val_data, 'valid')]\n    print(\"xgb \" + str(fold_) + \"-\" * 50)\n    num_rounds = 2000\n    xgb_model = xgb.train(xgb_params, trn_data, num_rounds, watchlist, early_stopping_rounds=50, verbose_eval=1000)\n    oof_xgb[val_idx] = xgb_model.predict(xgb.DMatrix(train.iloc[val_idx][features]), ntree_limit=xgb_model.best_ntree_limit+50)\n    predictions_xgb += xgb_model.predict(xgb.DMatrix(test[features]), ntree_limit=xgb_model.best_ntree_limit+50) / folds.n_splits","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"658abd2726f517e6b95f8ad1a05949c0d870f391"},"cell_type":"code","source":"print(\"CV score: {:<8.5f}\".format(mean_squared_error(target.values,oof_xgb)**0.5))\nplot_importance(xgb_model)#\nplt.show()\n#融合\ntotal_sum=0.5*predictions+0.5*predictions_xgb\nsub_df = pd.read_csv('../input/sample_submission.csv')\nsub_df['target'] = total_sum\nsub_df.to_csv('submission_lgbxgb.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}