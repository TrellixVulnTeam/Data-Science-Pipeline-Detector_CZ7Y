{"cells":[{"metadata":{"_uuid":"a9ef20fd4e6471fd2c9e3022edfdddc07a970aca"},"cell_type":"markdown","source":"## P.S. このノートブックの背景にある主なアイデアは、FabienDaniel Kernel Elo_worldからヒントを得たものです。\nhttps://www.kaggle.com/fabiendaniel/elo-world\n\nまずはデータ説明から\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \n#可視化系\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n#機械学習\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold, StratifiedKFold\n#よく見るけど未だわからん\nimport warnings\nimport time\n#osと同じ　メモリ使用量とかも見れる\nimport sys\nimport datetime\n#可視化　ダブってね？\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n#RMSE出すために使う\nfrom sklearn.metrics import mean_squared_error\n#サイキットラーンで使う（回帰？ナイーフベイズ？）\nfrom sklearn.linear_model import BayesianRidge\n#warningとセットで見る\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n#ガベージコレクション\nimport gc","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"84bca783292b677dfe6a267eb2febfef04e86ed1"},"cell_type":"markdown","source":"## 本カーネルには記述してないがデータの説明<br>(trainとtestは少し後で解説してます)\n<br>コンペデータ一覧\n<br>\n<br>Data_Dictionary.xlsx\n<br>　各々のデータの説明を記述してるエクセルファイル\n<br>merchants.csv(本カーネルではなぜか不使用)(データセット内のすべての販売側IDに関する追加情報。)\n<br>　クレカ登録店舗の一覧及びデータ\n<br>historical_transactions.csv\n<br>　クレカ取引履歴　(各card_idについて最大3か月分の過去の取引)\n<br>new_merchant_transactions.csv　\n<br>　(上記履歴データでは訪問されなかったmerchant_idで行われたすべての購入を含む、各card_idの2か月分のデータ。)\n<br>　販売側の新規取引履歴（クレカ側のデータではなく販売店側のデータなのでは？）\n<br>train.csv\n<br>　トレーニングデータ　顧客のIDとカテゴリ分けがされてる\n<br>test.csv\n<br>　テストデータ　同上\n<br>sample_submission.csv\n<br>　予測結果を書き込む所\n<br><br><br>\nElo_Blending=予測精度を上げる為に用意したデータ\n<br><br>\n過去に予測したデータが色々入ってる"},{"metadata":{"trusted":true,"_uuid":"61d2a8eee2e044547a3311c6415c34493be6f2b0"},"cell_type":"code","source":"#このカーネルでは使ってないけれど一応読み込み　解説\nmerchants = pd.read_csv('../input/elo-merchant-category-recommendation/merchants.csv')\nmerchants.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"afce217457292a2b1c44b74d13885d4dc1cedebe"},"cell_type":"markdown","source":"## ↑merchants.csv クレカ登録店舗の一覧及びデータ\n<br>merchant_id＝販売者ID　M_ID_から始まる英数字\n<br>merchant_group_id=販売者グループ　1~113kまである\n<br>merchant_category_id＝販売者カテゴリ　-1~891まで（-1は多分不明）\n<br>subsector_id=販売者カテゴリグループ　-1~41まで(merchant_category_idより狭い)\n<br>\n<br>city_id\t＝市ID　-1~347 -1は不明？\n<br>state_id＝国ID　-1~24 -1は不明？\n<br>\n<br>numerical_1=指標　-0.06前後が多いが184（null?）もある\n<br>numerical_2=指標　同上だが184でなく182(null？)\n<br>\n<br>category_1=カテゴリ　N98%Y2%\n<br>category_2　1~5\n<br>category_4=カテゴリ N71%Y29%\n<br>\n<br>most_recent_sales_range=先月の収益の範囲（通貨単位） - > A> B> C> D 35%> E 53%の５段階評価(基準分からず)\n<br>most_recent_purchases_range=前月の取引量の範囲 - > A> B> C> D 36%> E 52%の５段階評価(基準分からず)\n<br>avg_sales_lag3=3ヶ月移動平均収益　-82.13~9k（直近3ヶ月間の収益を直近のアクティブな月の売上で割った月平均）\n<br>avg_purchases_lag3=3ヶ月移動平均取引　0.3~99(過去3か月の取引の月平均を前のアクティブ月のトランザクションで割ったもの)\n<br>active_months_lag3=過去3ヶ月で取引があった月の数 1~3\n<br>\n<br>avg_sales_lag6=6ヶ月移動平均収益　（直近の6ヶ月間の収益を直近のアクティブな月の売上で割った月平均）\n<br>avg_purchases_lag6=6ヶ月移動平均取引　（過去6か月のトランザクションの月平均を前のアクティブ月のトランザクションで割ったもの）\n<br>active_months_lag6=過去6ヶ月で取引があった月の数\n<br>\n<br>avg_sales_lag12=12ヶ月移動平均収益　（直近の12ヶ月の売上を直近の有効月の売上で割った月平均）\n<br>avg_purchases_lag12=12ヶ月移動平均取引（過去12か月間のトランザクションの月平均を、直前のアクティブ月のトランザクションで割ったもの）\n<br>active_months_lag12=過去12か月で取引があった月の数"},{"metadata":{"trusted":true,"_uuid":"79e3284df5ca059e4cd83a50991ca4619b84708d"},"cell_type":"code","source":"#データ読み込み parse_dateは指定したカラムをタイムスタンプ型として読み込む　今回はperchase_date=購入日を使用している\nnew_transactions = pd.read_csv('../input/elo-merchant-category-recommendation/new_merchant_transactions.csv', parse_dates=['purchase_date'])\nhistorical_transactions = pd.read_csv('../input/elo-merchant-category-recommendation/historical_transactions.csv', parse_dates=['purchase_date'])\nnew_transactions.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"346318653046a6909cf9ffc9b046dddbf1985e8f"},"cell_type":"markdown","source":"## ↑new_merchant_transactions.csv 販売側の新規取引履歴\n<br>card_id＝カードID\n<br>month_lag＝基準日(2018年2月？）までの月差\n<br>purchase_date＝購入日\n<br>\n<br>authorized_flag＝取引完了フラグ　（承認された場合はY、拒否された場合はN）\n<br>\n<br>category_1＝カテゴリ　N97%　Y3%\n<br>category_2＝カテゴリ　1～5\n<br>category_3＝カテゴリ　A47％　B43%\n<br>\n<br>installments＝購入の分割数　－１～９９９（９９９回分割は不正取引ぽい）\n<br>\n<br>merchant_category_id＝販売者カテゴリID\n<br>subsector_id＝販売者カテゴリグループID\n<br>merchant_id＝販売者ID\n<br>\n<br>purchase_amount＝購入量\n<br>\n<br>city_id＝市ID\n<br>state_id＝州ID"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"historical_transactions.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e02a384bb9e6a7432a09c05606bb7f9cb7e4248e"},"cell_type":"markdown","source":"## ↑historical_transactions.csv\tクレカ取引履歴\n###new_merchant_transactions.csv 販売側の新規取引履歴とカラム内容は同じ\n<br>card_id=カード識別子\n<br>month_lag=基準日(2018年2月？）までの月差\n<br>purchase_date=購入日\n<br>authorized_flag=カード決済完了率　完了したらY、失敗したらN\n<br>category_3=カテゴリ　A~C\n<br>installments=購入の分割数\n<br>category_1=カテゴリ　YかN\n<br>merchant_category_id=販売者カテゴリID\n<br>subsector_id=販売者カテゴリグループID\n<br>merchant_id=販売者ID\n<br>purchase_amount=購入量\n<br>city_id=市ID\n<br>state_id=州ID\n<br>category_2=カテゴリ　１～４"},{"metadata":{"trusted":true,"_uuid":"864b2e4e318537693c7a89e329085ed0bba4bb1c"},"cell_type":"code","source":"#メモリ使用量を減らす関数を定義\ndef reduce_mem_usage(df, verbose=True):\n    #型宣言\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    #メモリの消費割合を格納\n    start_mem = df.memory_usage().sum() / 1024**2\n    #カラム名を一つずつ引っ張ってくる\n    for col in df.columns:\n        #カラムの型をcol_typeに代入\n        col_type = df[col].dtypes\n        #型が数字だったら\n        if col_type in numerics:\n            #c_minの最低値をc_maxに最大値を入れる\n            c_min = df[col].min()\n            c_max = df[col].max()\n            #型名の最初3文字がint~型だったら\n            if str(col_type)[:3] == 'int':\n                #c_minがint8の数値範囲にあればint8型で格納する\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                #同様にint16で    \n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                #同様にint32    \n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                #同様にint64    \n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                #同様にfloat16\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                #同様にfloat32    \n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    #同様にfloat64\n                    df[col] = df[col].astype(np.float64)    \n    #メモリの消費割合の算出\n    end_mem = df.memory_usage().sum() / 1024**2\n    #算出？？？？わからん\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"42b3094580d454155aa0311d3655e11d1b16b833"},"cell_type":"code","source":"#authorized_flag 決済完遂フラグとカテゴリ１　カラムをダミー変数化\n#こっちはメソッド作り　Yを１　Nを０に置換\ndef binarize(df):\n    for col in ['authorized_flag', 'category_1']:\n        df[col] = df[col].map({'Y':1, 'N':0})\n    return df\n#new_merchant_transactions.csv(販売側の新規取引履歴)\n#とhistorical_transactions.csv(カード会社側取引履歴)を上のメソッドでダミー変数化\nhistorical_transactions = binarize(historical_transactions)\nnew_transactions = binarize(new_transactions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4e2955f831ec14aa278a8dbb945f7e7d2f36c1c0"},"cell_type":"code","source":"%%time\n#↑はメモリ使用量計測\n#trainデータの整形メソッド\ndef read_data(input_file):\n    #csvをdfに格納\n    df = pd.read_csv(input_file)\n    #trainデータの初購入月(first_active_month) をタイムスタンプ型に変換\n    df['first_active_month'] = pd.to_datetime(df['first_active_month'])\n    # 2018年2月1日(基準日っぽい)ー初購入月(first_active_month)の日付　で日付だけをelapsed_timeに格納\n    df['elapsed_time'] = (datetime.date(2018, 2, 1) - df['first_active_month'].dt.date).dt.days\n    return df\n#trainデータとtestデータの読み込み\ntrain = read_data('../input/elo-merchant-category-recommendation/train.csv')\ntest = read_data('../input/elo-merchant-category-recommendation/test.csv')\n#trainデータのtarget　　　　　　　　　履歴および評価期間の2ヶ月後に算出された\n#　　　　　　　　　　　　　　　　　　　ロイヤリティ数値スコア？？をtarget変数に格納して\n\ntarget = train['target']\n#trainデータのtargetカラムは削除している\ndel train['target']\n#ガベージコレクション\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"697685a942de8e6013c59d4ea98d36e73ba3f2a8"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ffdcbbcbc3ebc87865b7adf556eda31eafce343b"},"cell_type":"markdown","source":"## train.csv トレーニングデータ =　test.csv テストデータ\n<br>\n<br>#card_id　　　　　　　　カードID　　　　\n<br>#first_active_month　　初購入月　　　　\n<br>#feature_1            　　匿名カードの分類機能(ゴールド会員？)\n<br>#feature_2　　　　　　３段階に分かれている\n<br>#feature_3\n<br>#elapsed_time　　　　2018年2月1日から初購入月が何日前か"},{"metadata":{"_uuid":"d6dc102dfc4aa39c2462b1962b12b82f91681de4"},"cell_type":"markdown","source":"## フィーチャーエンジニアリング（特徴量創出）"},{"metadata":{"trusted":true,"_uuid":"9ff66c3f26f8ba5f7005bd633be4d52ff8a4e35a"},"cell_type":"code","source":"%%time\n#historical_transactions.csv\tクレカ取引履歴のカテゴリ２と３をダミー変数化する\nhistorical_transactions = pd.get_dummies(historical_transactions, columns=['category_2', 'category_3'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ab36926431445f54b344eec638c74a2252c1345a"},"cell_type":"code","source":"#category_2_1.0やcategory_3_Aができて０か１が入っている\nhistorical_transactions.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5e83148a801fc4aeb14976228a27fa9bc4a53b29"},"cell_type":"code","source":"#上記と同様にnew_merchant_transactions.csv 販売側の新規取引履歴のカテゴリ２と３\nnew_transactions = pd.get_dummies(new_transactions, columns=['category_2', 'category_3'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"da60ac7ac854b27dc7e143e7db03e5b94c58b752"},"cell_type":"code","source":"new_transactions.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2674244ab202ad59a2a2e4618785a358fa7818f2"},"cell_type":"code","source":"#historical_transactionsとnew_transactionsをメモリ使用量を減らすメソッドにかけてる\nhistorical_transactions = reduce_mem_usage(historical_transactions)\nnew_transactions = reduce_mem_usage(new_transactions)\n#メモリー使用量が1304Mb減少(54.8%)　newは84.24Mb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"520f4b5f3469e68935bc50ab13f3e3b459df25d1"},"cell_type":"code","source":"#クレカ取引履歴(historical_transactions.csv)の\n#authorized_flag=カード決済完了フラグの合計と平均をagg_fun変数に格納\nagg_fun = {'authorized_flag': ['sum', 'mean']}\n#そのagg_funを使って　クレカ取引履歴(historical_transactions.csv)のカードID(card_id)を主キーとして\n#グループバイ\nauth_mean = historical_transactions.groupby(['card_id']).agg(agg_fun)\n\n#ガベージコレクション\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5c00f5be3f82e7b396e4fbd7c873cb8f6d8c88c3"},"cell_type":"code","source":"auth_mean.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a254713c459bb33260e928c576f39f1f6e6e327a"},"cell_type":"code","source":"#上記のカラム名を引っ張ってきて　stripで空白削除して_でタイトルと結合\nauth_mean.columns = ['_'.join(col).strip() for col in auth_mean.columns.values]\nauth_mean.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f3bff80b9c589b1daf5edefce54dc6e6be239455"},"cell_type":"code","source":"#リセットインデックス（番号を↑から降りなおし）\nauth_mean.reset_index(inplace=True)\nauth_mean.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cf003a95ffbe259e1c600642f1226093280e1158"},"cell_type":"code","source":"#authorized_flag=カード決済完了フラグが\n#完了の１をauthorized_transaxtionsへ\n#失敗の０をhistorical_transaxtionsに分けてデータ格納\nauthorized_transactions = historical_transactions[historical_transactions['authorized_flag'] == 1]\nhistorical_transactions = historical_transactions[historical_transactions['authorized_flag'] == 0]\n#ガベージコレクション\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"faf6ab02f683d9affdafebbf25990c08cd7c89e8"},"cell_type":"code","source":"authorized_transactions.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"75baadcdbea5df3b8864672b7723d0f74ceb2ec6"},"cell_type":"code","source":"historical_transactions.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"982e0cfee0b57adb5ab16bb5f8fb583d91e8df99"},"cell_type":"code","source":"%%time\n#上記の２つのデータの購入日(purchase_date)の月だけを抜き出して購入月(purchase_month)作成\nhistorical_transactions['purchase_month'] = historical_transactions['purchase_date'].dt.month\nauthorized_transactions['purchase_month'] = authorized_transactions['purchase_date'].dt.month\n#new_merchant_transactions.csv(販売側の新規取引履歴)も購入日から購入月作成\nnew_transactions['purchase_month'] = new_transactions['purchase_date'].dt.month\n#ガベージコレクション\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1a98566efad4451a3fc0624eabfc960e88a51a71"},"cell_type":"code","source":"%%time\ndef aggregate_transactions(history):\n    #purchase_dateを時系列のインデックスとしてint64に型変換してるだろうというのはわかるが\n    #1e-9=0.000000001がわからない（9桁で表示とか？）\n    history.loc[:, 'purchase_date'] = pd.DatetimeIndex(history['purchase_date']).\\\n                                      astype(np.int64) * 1e-9\n    #agg_func変数に作成したい項目を追加(np.ptpは値の範囲？)\n    agg_func = {\n        'category_1': ['sum', 'mean'],\n        'category_2_1.0': ['mean'],\n        'category_2_2.0': ['mean'],\n        'category_2_3.0': ['mean'],\n        'category_2_4.0': ['mean'],\n        'category_2_5.0': ['mean'],\n        'category_3_A': ['mean'],\n        'category_3_B': ['mean'],\n        'category_3_C': ['mean'],\n        'merchant_id': ['nunique'],\n        'merchant_category_id': ['nunique'],\n        'state_id': ['nunique'],\n        'city_id': ['nunique'],\n        'subsector_id': ['nunique'],\n        'purchase_amount': ['sum', 'mean', 'max', 'min', 'std'],\n        'installments': ['sum', 'mean', 'max', 'min', 'std'],\n        'purchase_month': ['mean', 'max', 'min', 'std'],\n        'purchase_date': [np.ptp, 'min', 'max'],\n        'month_lag': ['min', 'max']\n        }\n    #引数の渡されたデータをagg_funncにカードID(card_id)を主キーにしてグループバイ\n    agg_history = history.groupby(['card_id']).agg(agg_func)\n    #カラム名をタイトルと結合\n    agg_history.columns = ['_'.join(col).strip() for col in agg_history.columns.values]\n    #リセットインデックス\n    agg_history.reset_index(inplace=True)\n    \n    #カードID(card_id)グループバイしたのをsizeで要素数？？？を取得して\n    #リセットインデックス(transactions_count)という項目名で\n    df = (history.groupby('card_id')\n          .size()\n          .reset_index(name='transactions_count'))\n    #agg_historyに上記のdfとその上のagg_historyをカードIDで外部結合する\n    agg_history = pd.merge(df, agg_history, on='card_id', how='left')\n    #agg_historyを返す\n    return agg_history\n#ガベージコレクション\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2526461f802a582cda77ab12c3257474d89245b3"},"cell_type":"code","source":"%%time\n#上記のメソッドをクレカ取引履歴(historical_transactions.csv)にかけてる\nhistory = aggregate_transactions(historical_transactions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"adb7d7cfb0f7e4ad4e46491d48c99dc74ad10078"},"cell_type":"code","source":"history.head()\n#正直transactions_countとpurchase_dateがなんでこうなってるのか\n#未だ分かりません・・・","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aeb76aea0f297ecb5160802237b381eb04d26c95"},"cell_type":"code","source":"#分かり辛いが　カードID(card_id）以外のカラム名の先頭にhist_をつける\nhistory.columns = ['hist_' + c if c != 'card_id' else c for c in history.columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4d4d2a0d6cf287b399ab8f2e2fabccaa2b7160e5"},"cell_type":"code","source":"history[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"19bfcad03556056426f375e59cab3e4acb5ec9a8"},"cell_type":"code","source":"#ガベージコレクション\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"96904742cf44918cb4ee4d616118eb1c7fecae64"},"cell_type":"code","source":"%%time\n#上記と同じ事をカード決済完了フラグが完了になっていたデータで行う\nauthorized = aggregate_transactions(authorized_transactions)\nauthorized.columns = ['auth_' + c if c != 'card_id' else c for c in authorized.columns]\nauthorized[:5]\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1dad4221cab54b379d76343864b9ee036b4a1242"},"cell_type":"code","source":"%%time\n#上のaggregate_transactionsメソッドを#new_merchant_transactions.csv(販売側の新規取引履歴)にかけてる\nnew = aggregate_transactions(new_transactions)\n#上記のauth_をnew_にしてるだけ\nnew.columns = ['new_' + c if c != 'card_id' else c for c in new.columns]\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e72b5498f4e11c48d031d3b9b957dd8c213593b8"},"cell_type":"code","source":"new[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"68823b9423e4e8528c2d4c13ac234d2ba86f147e"},"cell_type":"code","source":"%%time\n\ndef aggregate_per_month(history):\n    #渡されたデータのカードID(card_id)とmonth_lag=基準日(2018年2月？）までの月差でグループバイ\n    grouped = history.groupby(['card_id', 'month_lag'])\n\n    #agg_funcに欲しい式情報を入れてる\n    agg_func = {\n            'purchase_amount': ['count', 'sum', 'mean', 'min', 'max', 'std'],\n            'installments': ['count', 'sum', 'mean', 'min', 'max', 'std'],\n            }\n    #上記のagg_funkをかけてる\n    intermediate_group = grouped.agg(agg_func)\n    #カラム名を空白削除と先頭にタイトル名_の形で結合\n    intermediate_group.columns = ['_'.join(col).strip() for col in intermediate_group.columns.values]\n    #リセットインデックス\n    intermediate_group.reset_index(inplace=True)\n    #上記のデータをカードID(card_id)の平均と標準偏差でグループバイ\n    final_group = intermediate_group.groupby('card_id').agg(['mean', 'std'])\n    #カラム名を同様に変更\n    final_group.columns = ['_'.join(col).strip() for col in final_group.columns.values]\n    final_group.reset_index(inplace=True)\n    #この値を返す\n    return final_group\n#___________________________________________________________\n#historical_transactions.csv\tクレカ取引履歴を上記のメソッドにかける\nfinal_group =  aggregate_per_month(historical_transactions) \n#１０行表示\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"435fb5bdca6746e501d6571dea12ac37d8432e1e"},"cell_type":"code","source":"final_group[:10]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"27de2f9fa94c828c4ef52d522479354661259914"},"cell_type":"markdown","source":"# 今まで作成したデータをtrain(及びtest)データに結合"},{"metadata":{"trusted":true,"_uuid":"01d91735e133bc1f28ef3ed69d4db22d1fe7f9ec"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"02b89ec80279ba0c914ae6069a28527ae5c7d69e"},"cell_type":"markdown","source":"クレカ取引履歴(historical_transactions.csv)のカード決済完了フラグが失敗だったデータの\n<br>取引回数、カテゴリ１の合計平均、カテゴリ２の平均、カテゴリ３の平均、購入量の合計平均最大最小標準偏差\n<br>分割回数の合計平均最大最小標準偏差、購入月の平均最小標準偏差、購入日の範囲最小最大、何日前購入の最小最大\n<br>を追加"},{"metadata":{"trusted":true,"_uuid":"70a197153c9e84afc1d0fb85d2daeb7ce7030bde"},"cell_type":"code","source":"#trainデータとhistorical_transactions.csv\tクレカ取引履歴整形後の\n#決済完了フラグが失敗だったデータをカードID(card_id)で左外部結合\n#testデータも\ntrain = pd.merge(train, history, on='card_id', how='left')\ntest = pd.merge(test, history, on='card_id', how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f3bb1e241e414fb8a6cf0fe48d9e32c875166dbf"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1519a16e0ffac641cf9a7d6ece6439c069e1cd12"},"cell_type":"markdown","source":"クレカ取引履歴(historical_transactions.csv)のカード決済完了フラグが成功だったデータの\n<br>取引回数、カテゴリ１の合計平均、カテゴリ２の平均、カテゴリ３の平均、購入量の合計平均最大最小標準偏差\n<br>分割回数の合計平均最大最小標準偏差、購入月の平均最小標準偏差、購入日の範囲最小最大、何日前購入の最小最大\n<br>を追加"},{"metadata":{"trusted":true,"_uuid":"d0551e90318ff4d9ba8707f92cd15341d1458d21"},"cell_type":"code","source":"#trainデータとhistorical_transactions.csv\tクレカ取引履歴整形後の\n#決済完了フラグが完了だったデータをカードID(card_id)で左外部結合\n#testデータも\ntrain = pd.merge(train, authorized, on='card_id', how='left')\ntest = pd.merge(test, authorized, on='card_id', how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"126c34ccacdc1b0b66ca27f3a94077b4fce8c3e3"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dc996b2fa394a3c366a53c1989ba17cf648054bf"},"cell_type":"markdown","source":"new_merchant_transactions.csv(販売側の新規取引履歴)\n<br>取引回数、カテゴリ１の合計平均、カテゴリ２の平均、カテゴリ３の平均、購入量の合計平均最大最小標準偏差\n<br>分割回数の合計平均最大最小標準偏差、購入月の平均最小標準偏差、購入日の範囲最小最大、何日前購入の最小最大\n<br>を追加"},{"metadata":{"trusted":true,"_uuid":"38f248fa4f1053c66c017e307e7a2e88867edbf7"},"cell_type":"code","source":"#trainデータとnew_merchant_transactions.csv(販売側の新規取引履歴)を左外部結合\n#testデータも\ntrain = pd.merge(train, new, on='card_id', how='left')\ntest = pd.merge(test, new, on='card_id', how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"697931555ac284c7d9daee45deace70830a7a0f2"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7fb4a11237570717bcc185e7bed14c07acaab17d"},"cell_type":"markdown","source":"### historical_transactions.csv\tクレカ取引履歴の購入量(purchase_amount) と分割回数の\n### レコード数、合計、平均、最小値、最大、標準偏差のカラム毎のカードID毎の平均と標準偏差を追加"},{"metadata":{"trusted":true,"_uuid":"0e1b425015da67ea7d1b38a1bb75969f72f5c14d"},"cell_type":"code","source":"#trainデータに上記で算出したfinal_groupを左外部結合\ntrain = pd.merge(train, final_group, on='card_id', how='left')\ntest = pd.merge(test, final_group, on='card_id', how='left')\n\n#ガベージコレクション\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1acf9e28b26416760c26fa8db4f3c42ab9e39988"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c84017393e0095e962a45e27a65d2bf9b2a087cd"},"cell_type":"markdown","source":"## カードID毎の決済完了回数と決済平均値を追加"},{"metadata":{"trusted":true,"_uuid":"e68fd4241eec5668dadf947859e4288788f34494"},"cell_type":"code","source":"#trainデータに\n#クレカ取引履歴(historical_transactions.csv)の\n#authorized_flag=カード決済完了フラグの合計と平均をagg_fun変数に格納\n#上記と左外部結合\ntrain = pd.merge(train, auth_mean, on='card_id', how='left')\ntest = pd.merge(test, auth_mean, on='card_id', how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9d2025bb1abdcfb085d1dbf0e7ebaac928055b11"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"610005c9b6b4b216912271d75fefed1c57827f5b"},"cell_type":"code","source":"#trainデータとtestデータの件数を出してる　201917行の139列　123623行の139列\nprint(\"Train Shape:\", train.shape)\nprint(\"Test Shape:\", test.shape)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c594e3dbb1b78a62aaa67df3d54121cc778d6c5d"},"cell_type":"code","source":"#カードID(card_id)又は初購入月(first_active_month)以外のカラムを引っ張り出してる　\nfeatures = [c for c in train.columns if c not in ['card_id', 'first_active_month']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ec66a51935ed06186b6963ba0d3ca5056878852f"},"cell_type":"code","source":"features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"95d8530892e8dbad3d5648291bd366c38ef5a08a"},"cell_type":"code","source":"#featureで始まる奴だけ抜き出し\ncategorical_feats = [c for c in features if 'feature_' in c]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"af1b649e9d41173dd1579d5bbf5009be41ddb93c"},"cell_type":"code","source":"categorical_feats","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fb2639ed1e06c39284838e0caab25b967f58c62d"},"cell_type":"markdown","source":"## LightGBM"},{"metadata":{"trusted":true,"_uuid":"31f678cbb341661e1e56fea10b5127ceb5376f95"},"cell_type":"code","source":"#LGBMのパラメータ設定\n#葉っぱの数(値が大きいほうが良いが、大きすぎると詰まる事がある)\nparam = {'num_leaves': 31,\n         #葉っぱの最低数\n         'min_data_in_leaf': 25,\n         #regression(回帰手法)\n         'objective':'regression',\n         #max_depth(決定木の深さ)num_leaves(葉っぱの数)　以下は対応表\n         #1は２\n         #2は4\n         #3は8\n         #7は128\n         #10は1024\n#このルールを覚えてしまえば，特に問題なさそうである．例えば\"XGBoost\"の max_depth=6 の設定と同等にするには，num_leaves=64 と設定すればよい．\n         'max_depth': 7,\n         #学習度合い(値が小さいほうが良い)\n         #勾配降下のような学習重み。num_roundは、実行する学習ステップの数、つまり構築するツリーの数。\n         #高いと学習率が上がるが過学習しやすくなる。ラウンド数を２倍し、etaを２で割る。学習に２倍時間がかかるが、モデルは良くなる。\n         'learning_rate': 0.01,\n         #何かで使う？\n         'lambda_l1':0.13,\n         #LightGBM = GBDT(Gradient boosting decision tree) + GOSS(Gradient-based One-Side Sampling) + EFB(Exclusive Feature Bundling)\n         #三種ある内の一つ。　詳細はhttps://qiita.com/Sa_qiita/items/7aa98c5df4019a7197ffで\n         \"boosting\": \"gbdt\",\n         #各木を作成するときの列におけるサブサンプルの割合　デフォ１\n         #過学習している場合はこの値を下げる。\n         \"feature_fraction\":0.85,\n         #サブサンプルを生成する際のトレーニングデータの抽出割合。たとえば、0.5に設定すると、\n         #XGBoost はデータの半分をランダムに選んで木を成長させることで、オーバーフィッティングを防ぎます。\n         #上記の説明からすると８割トレーニングデータにするのかな？\n         'bagging_freq':8,\n         #使用するオブジェクトの割合を制御するパラメータ。0と1の間の値。\n         \"bagging_fraction\": 0.9 ,\n         #スコアの算出方法？ RMSE（Root Mean Square Error）平均平方二乗誤差\n         \"metric\": 'rmse',\n         #警告レベルの表示（計算には関係ない）\n         \"verbosity\": -1,\n         #ランダムシードに相当する。ここを大きくしてもスコアが安定した方が良い（？）\n         \"random_state\": 2333}\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"086dbd944d919fd9e63e8d4e36f56fe6696441de"},"cell_type":"code","source":"#%%time\n#StratifiedKFold (y, k):分割後のデータセット内のラベルの比率を保ったまま、データをk個に分割。と説明があったので\n#５分割してデータの各階層をシャッフルしてランダムシードを設定している？（random_stateが来るのは変な気もするが）\nfolds = KFold(n_splits=5, shuffle=True, random_state=15)\n#trainデータの長さの分だけ0のデータを格納する\noof = np.zeros(len(train))\n#同じ処理をtestデータにも行う\npredictions = np.zeros(len(test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7cbc532515ec4a74c6e93ece3b7ba8d9bfaa13b3"},"cell_type":"code","source":"#この時点の時間を記録しておいてもう１回time.time()で処理時間を図る\nstart = time.time()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"596ccf17fe145448d9c055a51661f1206682f027","scrolled":false},"cell_type":"code","source":"#データフレームの作成\nfeature_importance_df = pd.DataFrame()\n#enumerateはインデックス番号の取得\n#split（X、y =なし、groups =なし）\n#X ： 配列のような形（n_samples、n_features）\n#学習データ。ここで、n_samplesはサンプル数、n_featuresは特徴数です。\n#y ： 配列のような形（n_samples、）\n#教師あり学習問題のための目標変数\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train.values, target.values)):\n    print(\"fold n°{}\".format(fold_))\n    #ilocは列行を番号指定  \n    trn_data = lgb.Dataset(train.iloc[trn_idx][features], label=target.iloc[trn_idx], categorical_feature=categorical_feats)\n    val_data = lgb.Dataset(train.iloc[val_idx][features], label=target.iloc[val_idx], categorical_feature=categorical_feats)\n    #演算の周回数\n    num_round = 10000\n    #lgbmの設定でcallback関数？を設定している　下記に設定項目の説明を記述しておきますが分からないのもある。。ぐぬぬ\n    \n    #early_stopping_rounds（int またはNone 、オプション（デフォルト= None ）） - 早期停止を有効にします。\n    #検証スコアが向上しなくなるまで、モデルは学習します。\n    #検証スコアは、early_stopping_roundsトレーニングを継続するために少なくともラウンドごとに改善する必要があります。\n    #少なくとも1つの検証データと1つのメトリックが必要です。複数ある場合は、それらすべてをチェックします。\n    #しかし、トレーニングデータは無視されます。best_iteration早期停止ロジックが設定によって有効にされている場合、\n    #最高のパフォーマンスを持つ反復のインデックスがフィールドに保存されます\n    \n    #verbose_eval（ブール値または整数値、オプション（デフォルト= True ）） -\n    #少なくとも1つの検証データが必要です。Trueの場合、有効セットの評価メトリックは各ブースティングステージで出力されます。\n    #intの場合、すべてのverbose_evalブースティングステージで有効なセットの評価メトリックが出力されます。\n    #最後のブースティングステージまたはを使って見つけたブースティングステージearly_stopping_roundsも印刷されます。\n    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=100, early_stopping_rounds = 200)\n    print(clf)\n    #サイキットラーンの予測　１番目の引数は使うデータ　２番目の引数は目的変数　予測するデータ\n    oof[val_idx] = clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n    #データフレーム作成\n    fold_importance_df = pd.DataFrame()\n    #trainのカラム名を格納したのを追加\n    fold_importance_df[\"feature\"] = features\n    #予測データを格納？\n    fold_importance_df[\"importance\"] = clf.feature_importance()\n    print(fold_importance_df)\n    #foldを一つついか？\n    fold_importance_df[\"fold\"] = fold_ + 1\n    #最初のデータフレームに追加\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    #分からない\n    predictions += clf.predict(test[features], num_iteration=clf.best_iteration) / folds.n_splits\n\nprint(\"CV score: {:<8.5f}\".format(mean_squared_error(oof, target)**0.5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c8d51de724cacaecd8bcea801a300cd7bd264c24"},"cell_type":"code","source":"#結局importanceがなんなのかわかんねぇ。。。。LGBMの演算で出た結果だと思うけど。。。\nfeature_importance_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d4e6238d116da873d987fcd4a1a185c4fa019c0f"},"cell_type":"code","source":"#上記をfeatureで集計して平均出して昇順にインデックスを並べてる\ncols = (feature_importance_df[[\"feature\", \"importance\"]]\n        .groupby(\"feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:1000].index)\ncols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d3d40f4504abbf78dd5424f88f818be6e8d3e991"},"cell_type":"code","source":"#???feature_importance_dfを複製しただけ？？？？\nbest_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]\nbest_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0567a0de44267518be6bfffb7da335ef32ef02c1"},"cell_type":"code","source":"#importanceの高い順から可視化しただけ\nplt.figure(figsize=(14,25))\nsns.barplot(x=\"importance\",\n            y=\"feature\",\n            data=best_features.sort_values(by=\"importance\",\n                                           ascending=False))\nplt.title('LightGBM Features (avg over folds)')\nplt.tight_layout()\nplt.savefig('lgbm_importances.png')\nprint(gc.collect())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ee1a4d6d99f76cd21d07b9d23adab0f7e86073e1"},"cell_type":"markdown","source":"## 繰り返しLGBMのkfoldアプローチ\n\nRepeatedKFoldは、K-Foldをn回繰り返します。 KFoldをn回実行して、繰り返しごとに異なる分割を生成する必要がある場合に使用できます。"},{"metadata":{"trusted":true,"_uuid":"cb689e64d3a3804ba00ceec187167554ba7b74c8"},"cell_type":"code","source":"lgbparam = {'num_leaves': 31,\n            'boosting_type': 'rf',\n             'min_data_in_leaf': 25, \n             'objective':'regression',\n             'max_depth': -1,\n             'learning_rate': 0.005,\n             \"min_child_samples\": 20,\n             \"feature_fraction\": 0.9,\n             \"bagging_freq\": 1,\n             \"bagging_fraction\": 0.9 ,\n             \"bagging_seed\": 11,\n             \"metric\": 'rmse',\n             \"lambda_l1\": 0.2,\n             \"verbosity\": -1,\n            #並列処理を行うスレッド数の指定\n             \"nthread\": 4,\n             \"random_state\": 4590}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1f91ae1c9f72c89774db61a1215b72767adf7061"},"cell_type":"code","source":"#RepeatedKFoldを使う\nfrom sklearn.model_selection import RepeatedKFold\n#データを５分割して２回繰り返す？\nfolds = RepeatedKFold(n_splits=5, n_repeats=2, random_state=4520)\n#さっきもやってたなこれ\noof_lgb = np.zeros(len(train))\npredictions_lgb = np.zeros(len(test))\n#処理時間計測用\nstart = time.time()\n#データフレーム作成\nfeature_importance_df = pd.DataFrame()\n#以下も同じなので割愛\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train.values, target.values)):\n    print(\"fold n°{}\".format(fold_))\n    trn_data = lgb.Dataset(train.iloc[trn_idx][features], label=target.iloc[trn_idx], categorical_feature=categorical_feats)\n    val_data = lgb.Dataset(train.iloc[val_idx][features], label=target.iloc[val_idx], categorical_feature=categorical_feats)\n\n    num_round = 11000\n    clf = lgb.train(lgbparam, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=100, early_stopping_rounds = 100)\n    oof_lgb[val_idx] = clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"feature\"] = features\n    fold_importance_df[\"importance\"] = clf.feature_importance()\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    predictions_lgb += clf.predict(test[features], num_iteration=clf.best_iteration) / (5 * 2)\n\nprint(\"CV score: {:<8.5f}\".format(mean_squared_error(oof_lgb, target)**0.5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ca42f5e049363147bcf1092324a485f30c1c788b"},"cell_type":"code","source":"#ここも同じ\ncols = (feature_importance_df[[\"feature\", \"importance\"]]\n        .groupby(\"feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:1000].index)\n\nbest_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]\n\nplt.figure(figsize=(14,25))\nsns.barplot(x=\"importance\",\n            y=\"feature\",\n            data=best_features.sort_values(by=\"importance\",\n                                           ascending=False))\nplt.title('LightGBM Features (avg over folds)')\nplt.tight_layout()\nplt.savefig('lgbm_importances.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8dd4f00f55a9bbf8006efa86ecbc82defe0c3ea6"},"cell_type":"code","source":"#カードIDが入ったデータフレーム作成\nsub_df = pd.DataFrame({\"card_id\":test[\"card_id\"].values})\nsub_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"41f02bcb6900eaebf5a80d2c1f429c2810568c2f"},"cell_type":"code","source":"#ここに予測結果を放り込む\nsub_df[\"target\"] = predictions\nsub_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7ba42f7a60e4c3ed0bc63b663a71b4b88bc5fc74"},"cell_type":"code","source":"#csv出力\nsub_df.to_csv(\"submit_lgb.csv\", index=False)\n#さっきと同じ事を後者の演算で出来た予測で格納\nsub_df1 = pd.DataFrame({\"card_id\":test[\"card_id\"].values})\nsub_df1[\"target\"] = predictions_lgb\nsub_df1.to_csv(\"submit_lgb1.csv\", index=False)\nsub_df1.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c114ab1abc534514ba3c6f366b5926fc1dba04ba"},"cell_type":"markdown","source":"## スタッキング"},{"metadata":{"trusted":true,"_uuid":"72f3ffef8b6007e5238c4cd5cc2401235ecbd095"},"cell_type":"code","source":"#予測が入ってたデータ\noof","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"816f02a174932ebbfbe501862416fe00f322dbc4"},"cell_type":"code","source":"oof_lgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"503b195803982a78f7b774681b427f79bc85de82"},"cell_type":"code","source":"#２つを結合して昇順に並べてる\ntrain_stack = np.vstack([oof,oof_lgb]).transpose()\ntest_stack = np.vstack([predictions,predictions_lgb]).transpose()\n#データを５分割して繰り返しは１回？\nfolds = RepeatedKFold(n_splits=5,n_repeats=1,random_state=4520)\n#０の配列つくるやつ\noof_stack = np.zeros(train_stack.shape[0])\npredictions_stack = np.zeros(test_stack.shape[0])\n#同じ事やってる？\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train_stack, target)):\n    print(\"fold n°{}\".format(fold_))\n    trn_data, trn_y = train_stack[trn_idx], target.iloc[trn_idx].values\n    val_data, val_y = train_stack[val_idx], target.iloc[val_idx].values\n\n    print(\"-\" * 10 + \"Stacking \" + str(fold_) + \"-\" * 10)\n#     cb_model = CatBoostRegressor(iterations=3000, learning_rate=0.1, depth=8, l2_leaf_reg=20, bootstrap_type='Bernoulli',  eval_metric='RMSE', metric_period=50, od_type='Iter', od_wait=45, random_seed=17, allow_writing_files=False)\n#     cb_model.fit(trn_data, trn_y, eval_set=(val_data, val_y), cat_features=[], use_best_model=True, verbose=True)\n    #ここで線形回帰モジュールを引っ張ってきてる？\n    clf = BayesianRidge()\n    clf.fit(trn_data, trn_y)\n    \n    oof_stack[val_idx] = clf.predict(val_data)\n    predictions_stack += clf.predict(test_stack) / 5\n\n\nnp.sqrt(mean_squared_error(target.values, oof_stack))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a363731164f95df970c8e2c088493bbe577452f1"},"cell_type":"code","source":"#提出ファイル書き換え\nsample_submission = pd.read_csv('../input/elo-merchant-category-recommendation/sample_submission.csv')\nsample_submission['target'] = predictions_stack\nsample_submission.to_csv('Bayesian_Ridge_Stacking.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3fa4751157c219f73ffdbc78444b06391aebfbfe"},"cell_type":"code","source":"#Blend1は今回の結果をBlend2は今回の結果を２割　3.695.csvが２割　combining_submission(1)が６割\nsample_submission = pd.read_csv('../input/elo-merchant-category-recommendation/sample_submission.csv')\nsample1 = pd.read_csv(\"../input/elo-blending/3.695.csv\")\nsample2 = pd.read_csv(\"../input/elo-blending/combining_submission (1).csv\")\nsample_submission['target'] = predictions * 0.5 + predictions_lgb * 0.5\nsample_submission.to_csv(\"Blend1.csv\", index = False)\nsample_submission['target'] = sample_submission['target'] * 0.2 + sample1['target'] * 0.2 + sample2['target'] * 0.6\nsample_submission.to_csv('Blend2.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6fa57916fb593f1cef89055aadc479f8baecc82c"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}