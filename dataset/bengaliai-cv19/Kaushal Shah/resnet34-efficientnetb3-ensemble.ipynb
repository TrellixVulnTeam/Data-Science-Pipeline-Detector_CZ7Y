{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install /kaggle/input/efficientnet-pytorch/efficientnet_pytorch-0.6.1-py3-none-any.whl\n!pip install /kaggle/input/ptmodels/pretrainedmodels-0.7.4-py3-none-any.whl","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import gc\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom matplotlib import pyplot as plt\n\nimport torch\nimport torch.utils.data as data_utils\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom torch import nn\nimport cv2\n\n# from torchsummary import summary\nfrom torchvision import transforms,models\nimport torch.nn.functional as F\nfrom tqdm import tqdm\nfrom torch import Tensor\n\n# from efficientnet_pytorch import EfficientNet\nfrom collections import OrderedDict\n\nimport albumentations\nfrom PIL import Image\nfrom albumentations.core.transforms_interface import ImageOnlyTransform\nfrom albumentations.augmentations import functional as Fun\nfrom PIL import Image, ImageOps, ImageEnhance\nimport pretrainedmodels\n\nfrom albumentations.core.transforms_interface import DualTransform\n\nfrom sklearn.model_selection import train_test_split\nfrom efficientnet_pytorch import EfficientNet\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ResNet34(nn.Module):\n    def __init__(self, pretrained):\n        super(ResNet34, self).__init__()\n        if pretrained is True:\n            self.model = pretrainedmodels.__dict__[\"resnet34\"](pretrained=\"imagenet\")\n        else:\n            self.model = pretrainedmodels.__dict__[\"resnet34\"](pretrained=None)\n        \n        self.l0 = nn.Linear(512, 168)\n        self.l1 = nn.Linear(512, 11)\n        self.l2 = nn.Linear(512, 7)\n\n    def forward(self, x):\n        bs, _, _, _ = x.shape\n        x = self.model.features(x)\n        x = F.adaptive_avg_pool2d(x, 1).reshape(bs, -1)\n        l0 = self.l0(x)\n        l1 = self.l1(x)\n        l2 = self.l2(x)\n        return l0, l1, l2","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"class EfficientNetWrapper(nn.Module):\n    def __init__(self, pretrained):\n        super(EfficientNetWrapper, self).__init__()\n        \n        # Load imagenet pre-trained model \n        self.effNet = EfficientNet.from_name('efficientnet-b3')\n        \n        # Appdend output layers based on our date\n        self.fc_root = nn.Linear(in_features=1000, out_features=168)\n        self.fc_vowel = nn.Linear(in_features=1000, out_features=11)\n        self.fc_consonant = nn.Linear(in_features=1000, out_features=7)\n        \n    def forward(self, X):\n        output = self.effNet(X)\n        output_root = self.fc_root(output)\n        output_vowel = self.fc_vowel(output)\n        output_consonant = self.fc_consonant(output)\n        \n        return output_root, output_vowel, output_consonant","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MODEL_MEAN = (0.485, 0.456, 0.406)\nMODEL_STD = (0.229, 0.224, 0.225)\nIMG_HEIGHT = 137\nIMG_WIDTH = 236","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class BengaliDatasetTest:\n    def __init__(self, df, img_height, img_width, mean, std):\n        \n        self.image_ids = df.image_id.values\n        self.img_arr = df.iloc[:, 1:].values\n\n        self.aug = albumentations.Compose([\n            albumentations.Resize(img_height, img_width, always_apply=True),\n            albumentations.Normalize(mean, std, always_apply=True)\n        ])\n\n\n    def __len__(self):\n        return len(self.image_ids)\n    \n    def __getitem__(self, item):\n        image = self.img_arr[item, :]\n        img_id = self.image_ids[item]\n        \n        image = image.reshape(137, 236).astype(float)\n        image = Image.fromarray(image).convert(\"RGB\")\n        image = self.aug(image=np.array(image))[\"image\"]\n        image = np.transpose(image, (2, 0, 1)).astype(np.float32)\n        \n\n        return {\n            \"image\": torch.tensor(image, dtype=torch.float),\n            \"image_id\": img_id\n        }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_predict():\n    img_ids_list = []\n    FOLDS = 5\n    TEST_BATCH_SIZE = 12\n\n    final_img_ids = []\n    \n    ffinal_g_pred = []\n    ffinal_v_pred = []\n    ffinal_c_pred = []\n\n    for file_idx in range(4):\n        final_g_pred = []\n        final_v_pred = []\n        final_c_pred = []\n        df = pd.read_parquet(f\"/kaggle/input/bengaliai-cv19/test_image_data_{file_idx}.parquet\")\n\n        dataset = BengaliDatasetTest(df=df,\n                                    img_height=IMG_HEIGHT,\n                                    img_width=IMG_WIDTH,\n                                    mean=MODEL_MEAN,\n                                    std=MODEL_STD)\n        \n        del df\n        gc.collect()\n        torch.cuda.empty_cache()\n\n        data_loader = torch.utils.data.DataLoader(\n            dataset=dataset,\n            batch_size= TEST_BATCH_SIZE,\n            shuffle=False\n        )\n        \n        for fold in range(FOLDS):\n            \n            model1 = EfficientNetWrapper(False)\n            model2 = ResNet34(False)\n            model3 = EfficientNetWrapper(False)\n            \n            g_pred1, v_pred1, c_pred1 = [], [], []\n            g_pred2, v_pred2, c_pred2 = [], [], []\n            g_pred3, v_pred3, c_pred3 = [], [], []\n            \n            model1.load_state_dict(torch.load(f\"/kaggle/input/bengaliaieffnetb3/efficientNet_fold{fold}.pth\", map_location=DEVICE))\n            model1.to(DEVICE)\n            model1.eval()\n            \n            model2.load_state_dict(torch.load(f\"/kaggle/input/resnet34originalsize/resnet34_fold{fold}.pth\", map_location=DEVICE))\n            model2.to(DEVICE)\n            model2.eval()\n            \n            model3.load_state_dict(torch.load(f\"/kaggle/input/effnetb3-cutmix-mixup/efficientNet_fold{fold}.pth\", map_location=DEVICE))\n            model3.to(DEVICE)\n            model3.eval()\n\n            for bi, d in tqdm(enumerate(data_loader)):\n                image = d[\"image\"]\n                img_id = d[\"image_id\"]\n                image = image.to(DEVICE, dtype=torch.float32)\n                \n                g1, v1, c1 = model1(image)\n                g2, v2, c2 = model2(image)\n                g3, v3, c3 = model3(image)\n\n                for ii, imid in enumerate(img_id):\n                    g_pred1.append(g1[ii].cpu().detach().numpy())\n                    v_pred1.append(v1[ii].cpu().detach().numpy())\n                    c_pred1.append(c1[ii].cpu().detach().numpy())\n                    \n                    g_pred2.append(g2[ii].cpu().detach().numpy())\n                    v_pred2.append(v2[ii].cpu().detach().numpy())\n                    c_pred2.append(c2[ii].cpu().detach().numpy())\n                    \n                    g_pred3.append(g3[ii].cpu().detach().numpy())\n                    v_pred3.append(v3[ii].cpu().detach().numpy())\n                    c_pred3.append(c3[ii].cpu().detach().numpy())\n                    if fold == 0:\n                        final_img_ids.append(imid)\n\n            final_g_pred.append(g_pred1)\n            final_v_pred.append(v_pred1)\n            final_c_pred.append(c_pred1)\n            \n            final_g_pred.append(g_pred2)\n            final_v_pred.append(v_pred2)\n            final_c_pred.append(c_pred2)\n            \n            final_g_pred.append(g_pred3)\n            final_v_pred.append(v_pred3)\n            final_c_pred.append(c_pred3)\n            \n            del g_pred1\n            del g_pred2\n            del g_pred3\n            del c_pred1\n            del c_pred2\n            del c_pred3\n            del v_pred1\n            del v_pred2\n            del v_pred3\n            gc.collect()\n            torch.cuda.empty_cache()\n             \n        ffinal_g_pred.append(final_g_pred)\n        ffinal_v_pred.append(final_v_pred)\n        ffinal_c_pred.append(final_c_pred)\n        \n        del final_g_pred\n        del final_v_pred\n        del final_c_pred\n        del data_loader\n        del dataset\n        gc.collect()\n        torch.cuda.empty_cache()\n      \n    del model1\n    del model2\n    del model3\n    gc.collect()\n    torch.cuda.empty_cache()\n    return ffinal_g_pred, ffinal_v_pred, ffinal_c_pred, final_img_ids","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_g_pred, final_v_pred, final_c_pred, final_img_ids = model_predict()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_g = np.argmax(np.mean(np.array(final_g_pred), axis=1).reshape(-1, 168), axis=1)\nfinal_v = np.argmax(np.mean(np.array(final_v_pred), axis=1).reshape(-1, 11), axis=1)\nfinal_c = np.argmax(np.mean(np.array(final_c_pred), axis=1).reshape(-1, 7), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del final_g_pred\ndel final_v_pred\ndel final_c_pred\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = []\nfor ii, imid in enumerate(final_img_ids):\n    predictions.append((f\"{imid}_grapheme_root\", final_g[ii]))\n    predictions.append((f\"{imid}_vowel_diacritic\", final_v[ii]))\n    predictions.append((f\"{imid}_consonant_diacritic\", final_c[ii]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del final_g\ndel final_v\ndel final_c\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.DataFrame(predictions, columns=[\"row_id\", \"target\"])\nsub.to_csv(\"submission.csv\", index=False)\nsub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":4}