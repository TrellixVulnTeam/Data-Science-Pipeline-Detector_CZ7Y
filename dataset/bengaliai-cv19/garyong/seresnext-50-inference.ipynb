{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inputs = [\"test_image_data_0.parquet\",\"test_image_data_1.parquet\",\"test_image_data_2.parquet\",\"test_image_data_3.parquet\"]\n#inputs = [\"train_image_data_0.parquet\",\"train_image_data_1.parquet\",\"train_image_data_2.parquet\",\"train_image_data_3.parquet\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DIR = \"../input/bengaliai-cv19/\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nimport pandas as pd\nimport torch\nimport numpy as np\nimport cv2\nfrom PIL import Image\nclass EvalDataset(Dataset):\n    def __init__(self,df,transform=None,aug=None,norm=None):\n        self.norm = norm\n        self.df = df\n        self.data = 255 - self.df.iloc[:, 1:].values.reshape(-1, 137, 236).astype(np.uint8)\n        self.transform = transform\n        self.aug = aug\n        \n    def __getitem__(self, idx):\n        name = self.df.iloc[idx,0]\n        image_arr = cv2.resize(self.data[idx],(128,128))\n        image_arr = image_arr/255.0\n        if self.norm != None:\n            mean = self.norm['mean']\n            std = self.norm['std']\n            image_arr = (image_arr -  mean)/std\n        image_arr = torch.from_numpy(image_arr.astype(np.float32))\n        return image_arr.unsqueeze(0).repeat(3,1,1),name\n\n\n    def __len__(self):\n        return len(self.data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\nsys.path.insert(0,\"../input/pretrainedmodels/pretrainedmodels-0.7.4\")\nimport pretrainedmodels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torchvision import models\nimport torch.nn as nn\n# Easier to split stuff up and backpropagate\nclass SEModule(nn.Module):\n    def __init__(self, channels=2048, reduction=16):\n        super(SEModule, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc1 = nn.Conv2d(channels, channels // reduction, kernel_size=1,\n                             padding=0)\n        self.relu = nn.ReLU(inplace=True)\n        self.fc2 = nn.Conv2d(channels // reduction, channels, kernel_size=1,\n                             padding=0)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        module_input = x\n        x = self.avg_pool(x)\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.sigmoid(x)\n        return module_input * x\n    \nclass MyModel(nn.Module):\n  def __init__(self,pretrained=True):\n    super().__init__()\n    if pretrained:\n        self.model = pretrainedmodels.__dict__[\"se_resnext50_32x4d\"](pretrained=\"imagenet\")\n    else:\n        self.model = pretrainedmodels.__dict__[\"se_resnext50_32x4d\"](pretrained=None)\n    self.model = nn.Sequential(*list(self.model.children())[:-2])\n    \n    self.se_g = SEModule()\n    self.se_v = SEModule()\n    self.se_c = SEModule()\n    \n    self.avg_pool = nn.AdaptiveAvgPool2d(1)\n    \n    self.fc_g = nn.Sequential(nn.Linear(2048,512), nn.ReLU(), nn.Dropout(p=0.2), nn.Linear(512,168))\n    self.fc_v = nn.Sequential(nn.Linear(2048,512), nn.ReLU(), nn.Dropout(p=0.2), nn.Linear(512,11))\n    self.fc_c = nn.Sequential(nn.Linear(2048,512), nn.ReLU(), nn.Dropout(p=0.2), nn.Linear(512,7))\n    \n  def forward(self,x):\n    x = self.model(x)\n    \n    g = self.se_g(x)\n    v = self.se_v(x)\n    c = self.se_c(x)\n    \n    g = torch.flatten(self.avg_pool(g),1)\n    v = torch.flatten(self.avg_pool(v),1)\n    c = torch.flatten(self.avg_pool(c),1)\n    \n    g = self.fc_g(g)\n    v = self.fc_v(v)\n    c = self.fc_c(c)\n    return g,v,c","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def kaggle_out(model,dataloader,device):\n    model.eval()\n    grapheme_output = []\n    vowel_output = []\n    consonant_output = []\n    img_id_ls = []\n    for data in dataloader:\n        inputs = data[0]\n        image_id = data[1]\n        inputs = inputs.to(device)\n        with torch.no_grad():\n            g,v,c = model(inputs)\n            grapheme_preds = g.argmax(dim=1)\n            vowel_preds = v.argmax(dim=1)\n            consonant_preds = c.argmax(dim=1)\n        for i in range(len(image_id)):\n            img_id_ls.append(image_id[i])\n            grapheme_output.append(grapheme_preds[i].cpu().detach().item())\n            vowel_output.append(vowel_preds[i].cpu().detach().item())\n            consonant_output.append(consonant_preds[i].cpu().detach().item())\n    return img_id_ls,grapheme_output,vowel_output,consonant_output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = []\nmean = 13.4/255\nstd = 40.8/255\nmodel = MyModel(pretrained=False)\nmodel.load_state_dict(torch.load('../input/seresnext-50-train/seresnext_50.pth'))\nmodel.to(device)\nfor inp in inputs:\n    df = pd.read_parquet(DIR+inp)\n    kaggle_dataset = EvalDataset(df,norm={'mean':mean,'std':std})\n    dl = DataLoader(kaggle_dataset,batch_size=128,num_workers=4)\n    img,gra,vow,con = kaggle_out(model,dl,device)\n    for i in range(len(img)):\n        results.append((f\"{img[i]}_grapheme_root\",gra[i]))\n        results.append((f\"{img[i]}_vowel_diacritic\",vow[i]))\n        results.append((f\"{img[i]}_consonant_diacritic\",con[i]))\ndf2 = pd.DataFrame(results, columns=[\"row_id\", \"target\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = df2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}