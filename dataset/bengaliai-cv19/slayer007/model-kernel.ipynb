{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport time\nfrom pathlib import Path# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train=pd.read_csv('/kaggle/input/bengaliai-cv19/train.csv')\ntest=pd.read_csv('/kaggle/input/bengaliai-cv19/test.csv')\nsample_sub=pd.read_csv('/kaggle/input/bengaliai-cv19/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# start=time.time()\n# train0=pd.read_parquet('/kaggle/input/bengaliai-cv19/train_image_data_0.parquet')\n# print(f\"TRAIN0 SHAPE: {train0.shape}  and it took {time.time()-start} seconds\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# current=time.time()\n# train1=pd.read_parquet('/kaggle/input/bengaliai-cv19/train_image_data_1.parquet')\n# print(f\"TRAIN1 SHAPE: {train1.shape}  and it took {time.time()-current} seconds\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# current=time.time()\n# train2=pd.read_parquet('/kaggle/input/bengaliai-cv19/train_image_data_2.parquet')\n# print(f\"TRAIN2 SHAPE: {train2.shape}  and it took {time.time()-current} seconds\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# current=time.time()\n# train3=pd.read_parquet('/kaggle/input/bengaliai-cv19/train_image_data_3.parquet')\n# print(f\"TRAIN3 SHAPE: {train3.shape}  and it took {time.time()-current} seconds\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from pathlib import Path\n# featherdir = Path('/kaggle/input/bengaliaicv19feather')\n# start=time.time()\n# train0 = pd.read_feather(featherdir/'train_image_data_0.feather')\n# train1 = pd.read_feather(featherdir/'train_image_data_1.feather')\n# train2 = pd.read_feather(featherdir/'train_image_data_2.feather')\n# train3 = pd.read_feather(featherdir/'train_image_data_3.feather')\n# print(f\"Time to load {time.time()-start} seconds\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# del train0\n# del train1\n# del train2\n# del train3\n# import gc\n# gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"PARQUET READ\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train0.to_feather('train0.feather')\n# del train0\n# train1.to_feather('train1.feather')\n# del train1\n# train2.to_feather('train2.feather')\n# del train2\n# train3.to_feather('train3.feather')\n# del train3\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# train0=train0.iloc[:,1:]\n# train1=train1.iloc[:,1:]\n# train2=train2.iloc[:,1:]\n# train3=train3.iloc[:,1:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm\nimport cv2\ndef resize(df, size=64, need_progress_bar=True):\n    resized = {}\n    if need_progress_bar:\n        for i in tqdm(range(df.shape[0])):\n            image = cv2.resize(df.loc[df.index[i]].values.reshape(137,236),(size,size))\n            resized[df.index[i]] = image.reshape(-1)\n            \n    else:\n        for i in range(df.shape[0]):\n            image = cv2.resize(df.loc[df.index[i]].values.reshape(137,236),(size,size))\n            resized[df.index[i]] = image.reshape(-1)\n            \n    resized = pd.DataFrame(resized).T\n    resized.columns=resized.columns.astype(str)\n    return resized","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def reduce_mem_usage(df):\n#     \"\"\" \n#     iterate through all the columns of a dataframe and \n#     modify the data type to reduce memory usage.        \n#     \"\"\"\n#     start_mem = df.memory_usage().sum() / 1024**2\n#     print(('Memory usage of dataframe is {:.2f}' \n#                      'MB').format(start_mem))\n    \n#     for col in df.columns:\n#         col_type = df[col].dtype\n        \n#         if col_type != object:\n#             c_min = df[col].min()\n#             c_max = df[col].max()\n#             if str(col_type)[:3] == 'int':\n#                 if c_min > np.iinfo(np.int8).min and c_max <\\\n#                   np.iinfo(np.int8).max:\n#                     df[col] = df[col].astype(np.int8)\n#                 elif c_min > np.iinfo(np.int16).min and c_max <\\\n#                    np.iinfo(np.int16).max:\n#                     df[col] = df[col].astype(np.int16)\n#                 elif c_min > np.iinfo(np.int32).min and c_max <\\\n#                    np.iinfo(np.int32).max:\n#                     df[col] = df[col].astype(np.int32)\n#                 elif c_min > np.iinfo(np.int64).min and c_max <\\\n#                    np.iinfo(np.int64).max:\n#                     df[col] = df[col].astype(np.int64)  \n#             else:\n#                 if c_min > np.finfo(np.float16).min and c_max <\\\n#                    np.finfo(np.float16).max:\n#                     df[col] = df[col].astype(np.float16)\n#                 elif c_min > np.finfo(np.float32).min and c_max <\\\n#                    np.finfo(np.float32).max:\n#                     df[col] = df[col].astype(np.float32)\n#                 else:\n#                     df[col] = df[col].astype(np.float64)\n#         else:\n#             df[col] = df[col].astype('category')\n#     end_mem = df.memory_usage().sum() / 1024**2\n#     print(('Memory usage after optimization is: {:.2f}' \n#                               'MB').format(end_mem))\n#     print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \n#                                              / start_mem))\n    \n#     return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### train0\nfeatherdir = Path('/kaggle/input/bengaliaicv19feather')\nstart=time.time()\ntrain0=pd.read_parquet('/kaggle/input/bengaliai-cv19/train_image_data_0.parquet')\n# train0=pd.read_feather(featherdir/'train_image_data_0.feather')\nprint(f\"TRAIN0 SHAPE: {train0.shape}  and it took {time.time()-start} seconds\")\ntrain0=train0.iloc[:,1:]\ntrain0_r=resize(train0)\ntrain0_r=(train0_r-np.mean(train0_r))/(train0_r.max()-train0_r.min())\nimport gc\ndel train0\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train0_r.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train0_r.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import gc\n# del train0\n# gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###### train1\ncurrent=time.time()\ntrain1=pd.read_parquet('/kaggle/input/bengaliai-cv19/train_image_data_1.parquet')\n# train1=pd.read_feather(featherdir/'train_image_data_1.feather')\nprint(f\"TRAIN1 SHAPE: {train1.shape}  and it took {time.time()-current} seconds\")\ntrain1=train1.iloc[:,1:]\n# train1_r=resize(train1)/255\ntrain1_r=resize(train1)\ntrain1_r=(train1_r-np.mean(train1_r))/(train1_r.max()-train1_r.min())\ndel train1\ngc.collect()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train1_r.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"################################ CODE TO CHECK MEMORY USAGE################&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n# gl.info(memory_usage='deep')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# del train1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"######## train2\ncurrent=time.time()\ntrain2=pd.read_parquet('/kaggle/input/bengaliai-cv19/train_image_data_2.parquet')\n# train2=pd.read_feather(featherdir/'train_image_data_2.feather')\nprint(f\"TRAIN2 SHAPE: {train2.shape}  and it took {time.time()-current} seconds\")\ntrain2=train2.iloc[:,1:]\n# train2_r=resize(train2)/255\ntrain2_r=resize(train2)\ntrain2_r=(train2_r-np.mean(train2_r))/(train2_r.max()-train2_r.min())\ndel train2\ngc.collect()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# del train2\n# gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########### train3\ncurrent=time.time()\ntrain3=pd.read_parquet('/kaggle/input/bengaliai-cv19/train_image_data_3.parquet')\n# train3=pd.read_feather(featherdir/'train_image_data_3.feather')\nprint(f\"TRAIN3 SHAPE: {train3.shape}  and it took {time.time()-current} seconds\")\ntrain3=train3.iloc[:,1:]\n# train3_r=resize(train3)/255\ntrain3_r=resize(train3)\ntrain3_r=(train3_r-np.mean(train3_r))/(train3_r.max()-train3_r.min())\ndel train3\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# del train3\n# gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"resized train\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xtrain0=train0_r.to_numpy()\ndel train0_r\ngc.collect()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xtrain1=train1_r.to_numpy()\ndel train1_r\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xtrain2=train2_r.to_numpy()\ndel train2_r\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xtrain3=train3_r.to_numpy()\ndel train3_r\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xtrain=np.concatenate((xtrain0,xtrain1,xtrain2,xtrain3),axis=0)\ndel xtrain1\ndel xtrain2\ndel xtrain3\ndel xtrain0\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xtrain.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_root=train['grapheme_root'].values\ny_vowel=train['vowel_diacritic'].values\ny_consonant=train['consonant_diacritic'].values\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xtrain=xtrain.reshape(-1,64,64,1).astype('float32')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_vowel.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"############################# IMPORTING LIBRARIES CNN ##########################\nimport tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Dropout,Activation,BatchNormalization,Flatten,Conv2D,MaxPool2D\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom keras import layers,regularizers\nfrom keras.optimizers import Adam,RMSprop\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(layers.Conv2D(64, (3,3), padding='same', input_shape=(64, 64, 1)))\nmodel.add(layers.BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=\"uniform\"))\nmodel.add(layers.LeakyReLU(alpha=0.1))\nmodel.add(layers.Conv2D(64,  (3,3), padding='same'))\nmodel.add(layers.BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=\"uniform\"))\nmodel.add(layers.LeakyReLU(alpha=0.1))\nmodel.add(layers.Conv2D(64,  (3,3), padding='same'))\nmodel.add(layers.BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=\"uniform\"))\nmodel.add(layers.LeakyReLU(alpha=0.1))\n\nmodel.add(layers.MaxPooling2D(2, 2))\nmodel.add(layers.Dropout(0.2))\n\nmodel.add(layers.Conv2D(128, (3,3), padding='same'))\nmodel.add(layers.BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=\"uniform\"))\nmodel.add(layers.LeakyReLU(alpha=0.1))\nmodel.add(layers.Conv2D(128, (3,3), padding='same'))\nmodel.add(layers.BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=\"uniform\"))\nmodel.add(layers.LeakyReLU(alpha=0.1))\nmodel.add(layers.Conv2D(128, (3,3), padding='same'))\nmodel.add(layers.BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=\"uniform\"))\nmodel.add(layers.LeakyReLU(alpha=0.1))\n\nmodel.add(layers.MaxPooling2D(2,2))\nmodel.add(layers.Dropout(0.2)) \n\nmodel.add(layers.Conv2D(256, (3,3), padding='same'))\nmodel.add(layers.BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=\"uniform\"))\nmodel.add(layers.LeakyReLU(alpha=0.1))\nmodel.add(layers.Conv2D(256, (3,3), padding='same'))\nmodel.add(layers.BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=\"uniform\"))\nmodel.add(layers.LeakyReLU(alpha=0.1))\n\nmodel.add(layers.MaxPooling2D(2,2))\nmodel.add(layers.Dropout(0.2))\n\n\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(1000))\nmodel.add(layers.LeakyReLU(alpha=0.1))\n\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.Dense(512))\nmodel.add(layers.LeakyReLU(alpha=0.1))\n\nmodel.add(layers.BatchNormalization())\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # MODEL\n# model = Sequential()\n# model.add(Conv2D(filters=32, kernel_size=(3, 3), padding='SAME', activation='relu', input_shape=(64, 64, 1)))\n# model.add(Conv2D(filters=32, kernel_size=(3, 3), padding='SAME', activation='relu'))\n# model.add(BatchNormalization(momentum=0.15))\n# model.add(Conv2D(filters=32, kernel_size=(3, 3), padding='SAME', activation='relu'))\n# model.add(BatchNormalization(momentum=0.15))\n# model.add(MaxPool2D(pool_size=(2, 2)))\n# model.add(Conv2D(filters=32, kernel_size=(5, 5), padding='SAME', activation='relu'))\n# model.add(Dropout(rate=0.3))\n\n# model.add(Conv2D(filters=64, kernel_size=(3, 3), padding='SAME', activation='relu'))\n# model.add(Conv2D(filters=64, kernel_size=(3, 3), padding='SAME', activation='relu'))\n# model.add(BatchNormalization(momentum=0.15))\n# model.add(Conv2D(filters=64, kernel_size=(3, 3), padding='SAME', activation='relu'))\n# model.add(BatchNormalization(momentum=0.15))\n# model.add(MaxPool2D(pool_size=(2, 2)))\n# model.add(Conv2D(filters=64, kernel_size=(5, 5), padding='SAME', activation='relu'))\n# model.add(Dropout(rate=0.3))\n\n# model.add(Flatten())\n# model.add(Dense(1000, activation = \"relu\"))\n# model.add(Dropout(0.20))\n# model.add(Dense(512, activation = \"relu\"))\n# model.add(Dropout(0.20))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import clone_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_root=clone_model(model)\nmodel_consonant=clone_model(model)\nmodel_vowel=clone_model(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_root.add(Dense(168, activation = 'softmax'))\nmodel_vowel.add(Dense(11, activation = 'softmax'))\nmodel_consonant.add(Dense(7, activation = 'softmax'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_root.compile(optimizer=\"adam\", loss=['categorical_crossentropy'], metrics=['accuracy'])\nmodel_vowel.compile(optimizer=\"adam\", loss=['categorical_crossentropy'], metrics=['accuracy'])\nmodel_consonant.compile(optimizer=\"adam\", loss=['categorical_crossentropy'], metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_root.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# learning_rate_reduction = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', \n#                                             patience=3, \n#                                             verbose=1,\n#                                             factor=0.5, \n#                                             min_lr=0.00001)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# datagen = ImageDataGenerator(\n#             featurewise_center=False,  # set input mean to 0 over the dataset\n#             samplewise_center=False,  # set each sample mean to 0\n#             featurewise_std_normalization=False,  # divide inputs by std of the dataset\n#             samplewise_std_normalization=False,  # divide each input by its std\n#             zca_whitening=False,  # apply ZCA whitening\n#             rotation_range=8,  # randomly rotate images in the range (degrees, 0 to 180)\n#             zoom_range = 0.15, # Randomly zoom image \n#             width_shift_range=0.15,  # randomly shift images horizontally (fraction of total width)\n#             height_shift_range=0.15,  # randomly shift images vertically (fraction of total height)\n#             horizontal_flip=False,  # randomly flip images\n#             vertical_flip=False)  # randomly flip images\n\n\n#         # This will just calculate parameters required to augment the given data. This won't perform any augmentations\n# datagen.fit(xtrain)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"model created\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"######################### ROOT #########################","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.utils import to_categorical","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_root.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_root = pd.get_dummies(y_root)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# xx = to_categorical(y_root, 168)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_root=y_root.to_numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(y_root)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"200840/3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def shuffle(matrix, target, test_proportion):\n    ratio = int(matrix.shape[0]/test_proportion) #should be int\n    X_train = matrix[ratio:,:]\n    X_test =  matrix[:ratio,:]\n    Y_train = target[ratio:,:]\n    Y_test =  target[:ratio,:]\n    return X_train, X_test, Y_train, Y_test\n\nX_train, X_val, Y_train, Y_val = shuffle(xtrain, y_root, 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('reached')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# from sklearn.model_selection import train_test_split\n# X_train, X_val, Y_train, Y_val = train_test_split(xtrain,y_root, test_size=0.08, random_state=42)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\ndatagen = ImageDataGenerator(\n    rotation_range=12, \n    zoom_range=0.2, \n    width_shift_range=0.2, \n    height_shift_range=0.2,\n#     rescale=1./255\n)\ndatagen.fit(X_train)\nlearning_rate_reduction = tf.keras.callbacks.ReduceLROnPlateau( \n    monitor='loss',    # Quantity to be monitored.\n    factor=0.5,       # Factor by which the learning rate will be reduced. new_lr = lr * factor\n    patience=3,        # The number of epochs with no improvement after which learning rate will be reduced.\n    verbose=1,         # 0: quiet - 1: update messages.\n    mode=\"auto\",       # {auto, min, max}. In min mode, lr will be reduced when the quantity monitored has stopped decreasing; \n                       # in the max mode it will be reduced when the quantity monitored has stopped increasing; \n                       # in auto mode, the direction is automatically inferred from the name of the monitored quantity.\n    min_delta=0.0001,  # threshold for measuring the new optimum, to only focus on significant changes.\n    cooldown=0,        # number of epochs to wait before resuming normal operation after learning rate (lr) has been reduced.\n    min_lr=0     # lower bound on the learning rate.\n    )\n\nes = EarlyStopping(monitor='loss', mode='auto', verbose=1, patience=300, restore_best_weights=True)\nmodel_root.fit_generator(datagen.flow(X_train, Y_train, batch_size=128),\n                              steps_per_epoch=len(X_train)//128,\n                              epochs=40,\n                              validation_data=(np.array(X_val),np.array(Y_val)),\n                              validation_steps=50,\n                              callbacks=[learning_rate_reduction, es])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model_root.fit(X_train, Y_train, batch_size=1024, epochs=20, validation_data=(X_val, Y_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del X_train\ndel X_val\ndel Y_train\ndel Y_val\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del y_root\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"root done !\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##################### CONSONANT ##################################","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# y_consonant = to_categorical(y_consonant, 7)\ny_consonant.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_consonant = pd.get_dummies(y_consonant)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_consonant=y_consonant.to_numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_val, Y_train, Y_val = shuffle(xtrain, y_consonant, 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_train, X_val, Y_train, Y_val = train_test_split(xtrain,y_consonant, test_size=0.08, random_state=42)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from tensorflow.keras.preprocessing.image import ImageDataGenerator\ndatagen = ImageDataGenerator(\n    rotation_range=12, \n    zoom_range=0.2, \n    width_shift_range=0.2, \n    height_shift_range=0.2,\n#     rescale=1./255\n)\ndatagen.fit(X_train)\nlearning_rate_reduction = tf.keras.callbacks.ReduceLROnPlateau( \n    monitor='loss',    # Quantity to be monitored.\n    factor=0.5,       # Factor by which the learning rate will be reduced. new_lr = lr * factor\n    patience=3,        # The number of epochs with no improvement after which learning rate will be reduced.\n    verbose=1,         # 0: quiet - 1: update messages.\n    mode=\"auto\",       # {auto, min, max}. In min mode, lr will be reduced when the quantity monitored has stopped decreasing; \n                       # in the max mode it will be reduced when the quantity monitored has stopped increasing; \n                       # in auto mode, the direction is automatically inferred from the name of the monitored quantity.\n    min_delta=0.0001,  # threshold for measuring the new optimum, to only focus on significant changes.\n    cooldown=0,        # number of epochs to wait before resuming normal operation after learning rate (lr) has been reduced.\n    min_lr=0     # lower bound on the learning rate.\n    )\n\nes = EarlyStopping(monitor='loss', mode='auto', verbose=1, patience=300, restore_best_weights=True)\nmodel_consonant.fit_generator(datagen.flow(X_train, Y_train, batch_size=128),\n                              steps_per_epoch=len(X_train)//128,\n                              epochs=40,\n                              validation_data=(np.array(X_val),np.array(Y_val)),\n                              validation_steps=50,\n                              callbacks=[learning_rate_reduction, es])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model_consonant.fit(X_train, Y_train, batch_size=1024, epochs=20, validation_data=(X_val, Y_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del X_train\ngc.collect()\ndel X_val\ngc.collect()\ndel Y_train\ngc.collect()\ndel Y_val\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del y_consonant\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"consonant done !\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################## VOWEL ######################","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# y_vowel = to_categorical(y_vowel, 11)\ny_vowel.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_vowel = pd.get_dummies(y_vowel)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_vowel=y_vowel.to_numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_val, Y_train, Y_val = shuffle(xtrain, y_vowel, 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_train, X_val, Y_train, Y_val = train_test_split(xtrain,y_vowel, test_size=0.08, random_state=42)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from tensorflow.keras.preprocessing.image import ImageDataGenerator\ndatagen = ImageDataGenerator(\n    rotation_range=12, \n    zoom_range=0.2, \n    width_shift_range=0.2, \n    height_shift_range=0.2,\n#     rescale=1./255\n)\ndatagen.fit(X_train)\nlearning_rate_reduction = tf.keras.callbacks.ReduceLROnPlateau( \n    monitor='loss',    # Quantity to be monitored.\n    factor=0.5,       # Factor by which the learning rate will be reduced. new_lr = lr * factor\n    patience=3,        # The number of epochs with no improvement after which learning rate will be reduced.\n    verbose=1,         # 0: quiet - 1: update messages.\n    mode=\"auto\",       # {auto, min, max}. In min mode, lr will be reduced when the quantity monitored has stopped decreasing; \n                       # in the max mode it will be reduced when the quantity monitored has stopped increasing; \n                       # in auto mode, the direction is automatically inferred from the name of the monitored quantity.\n    min_delta=0.0001,  # threshold for measuring the new optimum, to only focus on significant changes.\n    cooldown=0,        # number of epochs to wait before resuming normal operation after learning rate (lr) has been reduced.\n    min_lr=0     # lower bound on the learning rate.\n    )\n\nes = EarlyStopping(monitor='loss', mode='auto', verbose=1, patience=300, restore_best_weights=True)\nmodel_vowel.fit_generator(datagen.flow(X_train, Y_train, batch_size=128),\n                              steps_per_epoch=len(X_train)//128,\n                              epochs=40,\n                              validation_data=(np.array(X_val),np.array(Y_val)),\n                              validation_steps=50,\n                              callbacks=[learning_rate_reduction, es])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model_vowel.fit(X_train, Y_train, batch_size=1024, epochs=20, validation_data=(X_val, Y_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del X_train\ndel X_val\ndel Y_train\ndel Y_val\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del y_vowel\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"vowel done !\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model_dict = {\n#     'grapheme_root': model_root,\n#     'vowel_diacritic': model_vowel,\n#     'consonant_diacritic': model_consonant\n# }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del xtrain\ndel train\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Pkl_root = \"model_root.pkl\" \nPkl_consonant = \"model_consonant.pkl\"  \nPkl_vowel = \"model_vowel.pkl\"  \n\n\nwith open(Pkl_root, 'wb') as file:  \n    pickle.dump(model_root, file)\nwith open(Pkl_consonant, 'wb') as file:  \n    pickle.dump(model_consonant, file)\nwith open(Pkl_vowel, 'wb') as file:  \n    pickle.dump(model_vowel, file)    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# preds_dict = {\n#     'grapheme_root': [],\n#     'vowel_diacritic': [],\n#     'consonant_diacritic': []\n# }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# components = ['consonant_diacritic', 'grapheme_root', 'vowel_diacritic']\n# target=[] # model predictions placeholder\n# row_id=[] # row_id place holder\n# for i in tqdm(range(4)):\n#     df_test_img = pd.read_parquet('/kaggle/input/bengaliai-cv19/test_image_data_{}.parquet'.format(i)) \n#     df_test_img.set_index('image_id', inplace=True)\n\n#     X_test = resize(df_test_img, need_progress_bar=True)/255\n#     X_test = X_test.values.reshape(-1, 64, 64, 1)\n\n#     for pred in preds_dict:\n#         preds_dict[pred]=np.argmax(model_dict[pred].predict(X_test), axis=1)\n\n#     for k,id in enumerate(df_test_img.index.values):  \n#         for i,comp in enumerate(components):\n#             id_sample=id+'_'+comp\n#             row_id.append(id_sample)\n#             target.append(preds_dict[comp][k])\n#     del df_test_img\n#     del X_test\n#     gc.collect()\n\n# df_sample = pd.DataFrame(\n#     {\n#         'row_id': row_id,\n#         'target':target\n#     },\n#     columns = ['row_id','target'] \n# )\n# df_sample.to_csv('submission.csv',index=False)\n# # del target\n# # del row_id\n# # gc.collect()\n# df_sample.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Model DONE !\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}