{"cells":[{"metadata":{},"cell_type":"markdown","source":"This kernel is a copy of a [kernel](https://www.kaggle.com/kaushal2896/bengali-graphemes-starter-eda-multi-output-cnn). But with a lot of comments. It is used only for educational purposes."},{"metadata":{},"cell_type":"markdown","source":"**Загрузка библиотек и начальный анализ входных данных**"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"## Блок частоупотребимых библиотек по всем темам анализа данных\n# линейная алгебра https://pythonworld.ru/numpy/1.html  https://habr.com/ru/post/352678/  https://python-scripts.com/numpy\nimport numpy as np\n# чтение файлов и обработка данных https://proglib.io/p/pandas-tricks   https://khashtamov.com/ru/pandas-introduction/ https://habr.com/ru/company/ods/blog/322626/\nimport pandas as pd \n# sklearn - препроцессинг, метрики моделей, валидация моделей, выбор признако, выбор моделей,\n# модели машинного обучения: классификация, регрессия, кластеризция, понижение размерности...\n# Это основная библиотека для классического машинного обучения https://scikit-learn.org/stable/     https://habr.com/ru/company/mlclass/blog/247751/\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\n# Визуализация https://python-scripts.com/matplotlib  https://pythonworld.ru/novosti-mira-python/scientific-graphics-in-python.html\nimport matplotlib.image as mpimg\nfrom matplotlib import pyplot as plt\n# https://habr.com/ru/company/ods/blog/323210/  https://nagornyy.me/courses/data-science/intro-to-seaborn/\nimport seaborn as sns # более высокоуровневая и легкая в использовании\n## конец блока частоупотребимых библиотек\n\n## Блок библиотек связанных с задачами компьютерного зрения\n# Алгоритмы обработки изображений https://tproger.ru/translations/opencv-python-guide/  https://arboook.com/kompyuternoe-zrenie/osnovnye-operatsii-s-izobrazheniyami-v-opencv-3-python/\nimport cv2\n# https://habr.com/ru/post/451074/   https://pythonru.com/biblioteki/osnovnye-vozmozhnosti-biblioteki-python-imaging-library-pillow-pil\nimport PIL.Image as Image, PIL.ImageDraw as ImageDraw, PIL.ImageFont as ImageFont\n## конец блока библиотек для работы с изображениями\n\n## Блок работы с нейронными сетями\n# keras - высокоуровневая библиотека для работы с нейронными сетями. Работает как надстройка над tensorflow или theano (на выбор)\n# видео лекции от Созыкина А.В. https://www.youtube.com/watch?v=GX7qxV5nh5o&list=PLtPJ9lKvJ4oiz9aaL_xcZd-x0qd8G0VN_\n# https://www.youtube.com/watch?v=52U4BG0ENiM&list=PLtPJ9lKvJ4oi5ATzKmmp6FznCHmnhVoey\n# туториал https://riptutorial.com/ru/keras/topic/8695/%D0%BD%D0%B0%D1%87%D0%B0%D0%BB%D0%BE-%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D1%8B-%D1%81-%D0%BA%D0%B5%D1%80%D0%B0%D1%81%D0%BE%D0%BC\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n# крутой и краткий туториал на habr https://habr.com/ru/company/ods/blog/325432/\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.models import clone_model\nfrom tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPool2D, Dropout, BatchNormalization, Input, GlobalAveragePooling2D\n# Оптимизаторы https://habr.com/ru/post/318970/\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\n# предварительно обученные нейронные сети\n# Архитектуры в Керас https://www.pyimagesearch.com/2017/03/20/imagenet-vggnet-resnet-inception-xception-keras/\n# Архитектуры своими руками https://towardsdatascience.com/cnn-architectures-a-deep-dive-a99441d18049\n# tensorflow docs https://www.tensorflow.org/api_docs/python/tf/keras/applications\nfrom tensorflow.keras.applications.vgg16 import VGG16\nfrom tensorflow.keras.applications.vgg19 import VGG19\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\nfrom tensorflow.keras.applications.densenet import DenseNet121, DenseNet169, DenseNet201 \nfrom tensorflow.keras.applications.xception import Xception\nfrom tensorflow.keras.applications.resnet50 import ResNet50\nfrom tensorflow.keras.applications.resnet_v2 import ResNet50V2, ResNet101V2, ResNet152V2\n# пример нейронки с несколькими выходами https://www.pyimagesearch.com/2018/06/04/keras-multiple-outputs-and-multiple-losses/\n## конец блока нейронок\n\n## Блок вспомогательных библиотек\n# визуализация прогресса https://stackoverflow.com/questions/42212810/tqdm-in-jupyter-notebook-prints-new-progress-bars-repeatedly\nfrom tqdm.auto import tqdm \n# позволяет выбирать список файлов по шаблону пути https://pythonworld.ru/moduli/modul-glob.html\nfrom glob import glob \n# время и сборщик мусора https://all-python.ru/osnovy/modul-time.html  https://asvetlov.blogspot.com/2013/05/gc.html http://www.ilnurgi1.ru/docs/python/modules/gc.html\nimport time, gc\n## конец блока вспомогательных библиотек","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Просмотр входных данных"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"from pathlib import Path\nfeatherdir = Path('/kaggle/input/bengaliaicv19feather') # папка с изображениями в формате feather\nimport os\n#выведем все файлы в директории \"input/\"\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Любые результаты, которые вы записываете в текущий каталог, сохраняются как выходные данные.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Сравним скорость работы с файлами \"feather\" и \"parquet\""},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%time\n# train_image_df0 = pd.read_feather(featherdir/'train_image_data_0.feather')\n# train_image_df1 = pd.read_feather(featherdir/'train_image_data_1.feather')\n# train_image_df2 = pd.read_feather(featherdir/'train_image_data_2.feather')\n# train_image_df3 = pd.read_feather(featherdir/'train_image_data_3.feather')\n# train_image_df3.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%time\n# d = pd.read_parquet(f'/kaggle/input/bengaliai-cv19/train_image_data_1.parquet')\n# d.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# del train_image_df0\n# del train_image_df1\n# del train_image_df2\n# del train_image_df3\n# del d\n# gc.collect()\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# данные для тренировки\ntrain_df_ = pd.read_csv('/kaggle/input/bengaliai-cv19/train.csv')\n# данные для тестирования алгоритма\ntest_df_ = pd.read_csv('/kaggle/input/bengaliai-cv19/test.csv')\n# данные для нахождения составных частей графемы (корня графемы, гласного и согласного диакретических знаков)\nclass_map_df = pd.read_csv('/kaggle/input/bengaliai-cv19/class_map.csv')\n# пример оформления файла ответов\nsample_sub_df = pd.read_csv('/kaggle/input/bengaliai-cv19/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df_.head() # смотрим какие данные в тренировочной таблице","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  смотрим количество уникальных значений в каждом столбце\ntrain_df_.grapheme_root.nunique(), train_df_.vowel_diacritic.nunique(), train_df_.consonant_diacritic.nunique(), train_df_.grapheme.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df_.head()  # смотрим какие данные в тестовой таблице","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_sub_df.head()  # смотрим какие данные в примере файла ответов","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# таблица, которая ставит в соответствие некой полной графемы G ее состовляющие:\n# корень графемы, гласные диакритические знаки и согласные диакритические знаки\nG = 5\nclass_map_df[class_map_df.label==G]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_map_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_map_df.component_type.nunique(), class_map_df.label.nunique(), class_map_df.component.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" # смотрим размеры наборов данных\nprint(f'Size of training data: {train_df_.shape}')\nprint(f'Size of test data: {test_df_.shape}')\nprint(f'Size of class map: {class_map_df.shape}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis\nИсследовательский анализ данных (EDA) - это подход к анализу наборов данных для обобщения их основных характеристик, часто с помощью визуальных методов."},{"metadata":{"trusted":true},"cell_type":"code","source":"HEIGHT = 236 # высота изображения\nWIDTH = 236 # ширина изображения\n# если хотите глубже понять, что происходит - раскоментируйте принты\ndef get_n(df, field, n, top=True):\n    '''функция для выбора топ N графем желаемого типа (field)\n        df - data frame с графемами (в нашем случае test_df_)\n        field - часть графемы, которую хотим проанализировать. Может принимать 3 значения (grapheme_root, vowel_diacritic, consonant_diacritic)\n        n - число знаков, которые мы хотим увидеть\n        top - (True/False) признак сортировки (по убывание/ по возрастанию)\n    '''\n    # группируем датасет по выбранной части графемы и считаем количество появлений каждого типа графемы. Сортируем, отсекам топ\n    top_graphemes = df.groupby([field]).size().reset_index(name='counts')['counts'].sort_values(ascending=not top)[:n]\n#     print(top_graphemes)\n    top_grapheme_roots = top_graphemes.index # отдельно выбираем индексы (номера частей графем)\n#     print(top_grapheme_roots)\n    top_grapheme_counts = top_graphemes.values # отдельно выбираем значение (количества) вхождения этой части графемы в набор данных\n#     print(top_grapheme_counts)\n    # находим выбранные нами части графем в наборе с рисунками этиъ частей графем\n    top_graphemes = class_map_df[class_map_df['component_type'] == field].reset_index().iloc[top_grapheme_roots]\n#     print(top_graphemes)\n    top_graphemes.drop(['component_type', 'label'], axis=1, inplace=True) # удаляем ненужные нам столбцы\n#     print(top_graphemes)\n    top_graphemes.loc[:, 'count'] = top_grapheme_counts # добавляем к номерам и рисункам частей графем количества их вхождения\n#     print(top_graphemes)\n    return top_graphemes\n\ndef image_from_char(char):\n    '''функция отображения символов из таблицы с данными в рисунки желаемого размера\n        char - часть графемы, которую хотим отрисовать в увеличенном масштабе\n    '''\n    image = Image.new('RGB', (WIDTH, HEIGHT)) # задаем трехканальный тип изображения и его размерность\n    draw = ImageDraw.Draw(image) # создаем объект для отрисовки\n    myfont = ImageFont.truetype('/kaggle/input/kalpurush-fonts/kalpurush-2.ttf', 120) # выбираем тип шрифта и его размер\n    w, h = draw.textsize(char, font=myfont) # преобразовуем табличную графему к выбранному шрифту и размерам шрифта\n    draw.text(((WIDTH - w) / 2,(HEIGHT - h) / 3), char, font=myfont) # рисуем символ, отцентровав его на полотне изображения\n\n    return image","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ### Число уникальных значений по типам графем, которые требуется предсказать"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Number of unique grapheme roots: {train_df_[\"grapheme_root\"].nunique()}')\nprint(f'Number of unique vowel diacritic: {train_df_[\"vowel_diacritic\"].nunique()}')\nprint(f'Number of unique consonant diacritic: {train_df_[\"consonant_diacritic\"].nunique()}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Топ 10 наиболее используемых корней графем"},{"metadata":{"trusted":true},"cell_type":"code","source":"top_10_roots = get_n(train_df_, 'grapheme_root', 10)\ntop_10_roots","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# создаем сетку 2 на 5, для более компактного отображения символов и задаем размер их отображения\nf, ax = plt.subplots(2, 5, figsize=(16, 8))\nax = ax.flatten()\n# отрисовываем в цикле найденные топ N изображений частей графем\nfor i in range(10):\n    ax[i].imshow(image_from_char(top_10_roots['component'].iloc[i]), cmap='Greys')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ### Топ 10 наименее используемых корней графем в наборе"},{"metadata":{"trusted":true},"cell_type":"code","source":"bottom_10_roots = get_n(train_df_, 'grapheme_root', 10, False)\nbottom_10_roots","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(2, 5, figsize=(16, 8))\nax = ax.flatten()\n\nfor i in range(10):\n    ax[i].imshow(image_from_char(bottom_10_roots['component'].iloc[i]), cmap='Greys')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ### Топ 5 гласных диакритических знаков в тренировочном наборе данных"},{"metadata":{"trusted":true},"cell_type":"code","source":"top_5_vowels = get_n(train_df_, 'vowel_diacritic', 5)\ntop_5_vowels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(1, 5, figsize=(16, 8))\nax = ax.flatten()\n\nfor i in range(5):\n    ax[i].imshow(image_from_char(top_5_vowels['component'].iloc[i]), cmap='Greys')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ### Топ 5 согласных диакритических знаков в тренировочном наборе данных"},{"metadata":{"trusted":true},"cell_type":"code","source":"top_5_consonants = get_n(train_df_, 'consonant_diacritic', 5)\ntop_5_consonants","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(1, 5, figsize=(16, 8))\nax = ax.flatten()\n\nfor i in range(5):\n    ax[i].imshow(image_from_char(top_5_consonants['component'].iloc[i]), cmap='Greys')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df_ = train_df_.drop(['grapheme'], axis=1, inplace=False) # убираем из тренировочного набора данных рисунки графем","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# преобразовуем тип данных к типу, который занимает меньше места в памяти \ntrain_df_[['grapheme_root', 'vowel_diacritic', 'consonant_diacritic']] = train_df_[['grapheme_root', 'vowel_diacritic', 'consonant_diacritic']].astype('uint8')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"IMG_SIZE=64 # определяем входной размер изображений для нейронной сети\nN_CHANNELS=1 # определяем число каналов цвета для нейронной сети","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Применим некоторые преобразования над изображениями (credits: [this kernel](https://www.kaggle.com/shawon10/bangla-graphemes-image-processing-deep-cnn)). Изменим их размер и отцентруем графемы."},{"metadata":{"trusted":true},"cell_type":"code","source":"def resize(df, size=64, need_progress_bar=True):\n    '''функция преобразовывает изображения к квадратной форме необходимого размера\n        df - набор данных с изображениями исходного размера\n        size - ширина и высота изображения после преобразования\n        need_progress_bar - вывод строки состояния\n        return - дата фрейм содержащий изображения нового размера\n    '''\n    resized = {}\n    resize_size=size\n    # если нужно выводить строку состояния, то оборачиваем цикл в tqdm, иначе нет\n    if need_progress_bar:\n        for i in tqdm(range(df.shape[0])):\n            # извлекаем строку с одним изображением и преобразуем вектор в матрицу изображения исходного размера\n            image=df.loc[df.index[i]].values.reshape(137,236)\n            # приводим изображение к бинарному\n            _, thresh = cv2.threshold(image, 30, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n            # применяем алгоритм поиска всех вероятных контуров изображения\n            contours, _ = cv2.findContours(thresh,cv2.RETR_LIST,cv2.CHAIN_APPROX_SIMPLE)[-2:]\n\n            ls_xmin = []\n            ls_ymin = []\n            ls_xmax = []\n            ls_ymax = []\n            # для всех найденных контуров\n            for cnt in contours:\n                # находим координаты прямоугольника описанного вокруг найденного контура изображения\n                x,y,w,h = cv2.boundingRect(cnt)\n                # помещаем координаты углов прямоугольника, каждый в свой массив\n                ls_xmin.append(x)\n                ls_ymin.append(y)\n                ls_xmax.append(x + w)\n                ls_ymax.append(y + h)\n            #  после перебора всех возможных контуров, находим крайние относительно центра координаты прямоугольника, чтобы наверняка захватить все изображение\n            xmin = min(ls_xmin)\n            ymin = min(ls_ymin)\n            xmax = max(ls_xmax)\n            ymax = max(ls_ymax)\n            # обрезаем изображение по полученным координатам (теперь у нас есть изображение графемы, занимающее максимум объема изображения)\n            roi = image[ymin:ymax,xmin:xmax]\n            # меняем размер изображения на необходимый нам\n            resized_roi = cv2.resize(roi, (resize_size, resize_size),interpolation=cv2.INTER_AREA)\n            # добавляем новое изображение в словарь\n            resized[df.index[i]] = resized_roi.reshape(-1)\n    else:\n        for i in range(df.shape[0]):\n            image=df.loc[df.index[i]].values.reshape(137,236)\n            _, thresh = cv2.threshold(image, 30, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n            contours, _ = cv2.findContours(thresh,cv2.RETR_LIST,cv2.CHAIN_APPROX_SIMPLE)[-2:]\n\n            ls_xmin = []\n            ls_ymin = []\n            ls_xmax = []\n            ls_ymax = []\n            for cnt in contours:\n                x,y,w,h = cv2.boundingRect(cnt)\n                ls_xmin.append(x)\n                ls_ymin.append(y)\n                ls_xmax.append(x + w)\n                ls_ymax.append(y + h)\n            xmin = min(ls_xmin)\n            ymin = min(ls_ymin)\n            xmax = max(ls_xmax)\n            ymax = max(ls_ymax)\n\n            roi = image[ymin:ymax,xmin:xmax]\n            resized_roi = cv2.resize(roi, (resize_size, resize_size),interpolation=cv2.INTER_AREA)\n            resized[df.index[i]] = resized_roi.reshape(-1)\n    # создаем из полученного словаря с изображениями нового размера дата фрейм и возвращаем его\n    resized = pd.DataFrame(resized).T\n    return resized","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_dummies(df):\n    cols = []\n    for col in df:\n        cols.append(pd.get_dummies(df[col].astype(str)))\n    return pd.concat(cols, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d = pd.DataFrame({1:[8,1,3,4,5,6,],2:[0,15,54,0,4,8,],3:[10,11,45,0,7,9,],4:[0,1,3,4,5,6,]}).T\nprint(d)\ncols = []\nfor col in d:\n#     print(col)\n#     print(pd.get_dummies(d[col].astype(str)))\n    cols.append(pd.get_dummies(d[col].astype(str)))\npd.concat(cols, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Базовая модель"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Создаем базовую модель блока свертки с использованием Keras\ndef make_conv_block(input_tensor, num_filters):\n    '''функция создает блок из сверточных слоев, слоя пакетной нормализации (BatchNormalization), слоя MaxPool2D и Dropout\n            df - набор данных с изображениями исходного размера\n            size - ширина и высота изображения после преобразования\n            need_progress_bar - вывод строки состояния\n    return - дата фрейм содержащий изображения нового размера'''\n    # серия сверточных слоев с одинаковым размером kernel_size\n    model = Conv2D(filters=num_filters, kernel_size=(3, 3), padding='SAME', activation='relu')(input_tensor)\n    model = Conv2D(filters=num_filters, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n    model = Conv2D(filters=num_filters, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n    model = Conv2D(filters=num_filters, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n    model = BatchNormalization(momentum=0.15)(model) # слой мини-пакетной нормализации\n    model = MaxPool2D(pool_size=(2, 2))(model) # слой уменьшения размерности (в 2 раза)\n    model = Conv2D(filters=num_filters, kernel_size=(5, 5), padding='SAME', activation='relu')(model) # еще одна свертка с ядром большего размера (5,5)\n    model = Dropout(rate=0.3)(model) # слой регуляризации (случайно \"замораживает\" часть нейронов в слое, чтобы избежать переобучения)\n    return model\n\n# создаем серию однотипных блоков с разным количеством фильтров\ninputs = Input(shape = (IMG_SIZE, IMG_SIZE, N_CHANNELS)) # входной слой \nmodel = inputs # небольшой трюк, связанный с тем, что inputs нам понадобиться и дальше, поэтому его нельзя переопределять и преобразовывать\n# последовательно применяем созданный блок сверток со все нарастающей глубиной фильтров\nfor num_filters in [32, 64, 128, 256]:\n    conv_block = make_conv_block(model, num_filters)\n    model = conv_block\n# преобразовываем выход последнего сверточного блока в плоский вектор\nmodel = Flatten()(model)\nmodel = Dense(1024, activation = \"relu\")(model) # добавляем полносвязный слой нейронной сети\nmodel = Dropout(rate=0.3)(model) #  слой регуляризации\ndense = Dense(512, activation = \"relu\")(model) # еще один полносвязный слой\nhead_root = Dense(168, activation = 'softmax')(dense) # выход нейронной сети отвечающий за классификацию графем\nhead_vowel = Dense(11, activation = 'softmax')(dense) # выход нейронной сети отвечающий за классификацию гласных диакректических знаков\nhead_consonant = Dense(7, activation = 'softmax')(dense) # выход нейронной сети отвечающий за классификацию согласных диакректических знаков\n# создаем модель\nmodel = Model(inputs=inputs, outputs=[head_root, head_vowel, head_consonant])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## если хотите использовать предварительно обученную нейронную сеть\n## на данный момент необходимо еще добавить преобразование изображений к трехканальным, чтобы данная модель работала\n# vgg19_net = VGG19(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))\n# vgg19_net.trainable = False\n# model = vgg19_net.output\n# model = Flatten()(model)\n# model = Dense(1024, activation = \"relu\")(model) # добавляем полносвязный слой нейронной сети\n# model = Dropout(rate=0.3)(model) #  слой регуляризации\n# dense = Dense(512, activation = \"relu\")(model) # еще один полносвязный слой\n# head_root = Dense(168, activation = 'softmax')(dense) # выход нейронной сети отвечающий за классификацию графем\n# head_vowel = Dense(11, activation = 'softmax')(dense) # выход нейронной сети отвечающий за классификацию гласных диакректических знаков\n# head_consonant = Dense(7, activation = 'softmax')(dense) # выход нейронной сети отвечающий за классификацию согласных диакректических знаков\n# создаем модель\n# model = Model(inputs=vgg19_net.input, outputs=[head_root, head_vowel, head_consonant])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary() # выводим описание модели","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Визуализируем CNN с 3 выходами"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.utils import plot_model\nplot_model(model, to_file='model.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) # компилируем модель","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Установим скорость обучения для каждого выхода нейронной сети.\n# Так же зададим правило, что, если 3 (patience=3) эпохи подряд точность не возрастает, то делим скорость обучения пополам (factor=0.5)\nlearning_rate_reduction_root = ReduceLROnPlateau(monitor='dense_2_accuracy', \n                                            patience=3, \n                                            verbose=1,\n                                            factor=0.5, \n                                            min_lr=0.00001)\nlearning_rate_reduction_vowel = ReduceLROnPlateau(monitor='dense_3_accuracy', \n                                            patience=3, \n                                            verbose=1,\n                                            factor=0.5, \n                                            min_lr=0.00001)\nlearning_rate_reduction_consonant = ReduceLROnPlateau(monitor='dense_4_accuracy', \n                                            patience=3, \n                                            verbose=1,\n                                            factor=0.5, \n                                            min_lr=0.00001)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 256\nepochs = 30","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MultiOutputDataGenerator(keras.preprocessing.image.ImageDataGenerator):\n    '''класс наследующий класс ImageDataGenerator Keras\n    '''\n    def flow(self,\n             x,\n             y=None,\n             batch_size=32,\n             shuffle=True,\n             sample_weight=None,\n             seed=None,\n             save_to_dir=None,\n             save_prefix='',\n             save_format='png',\n             subset=None):\n        '''переопределяет функцию flow родительского класса'''\n        targets = None\n        target_lengths = {}\n        ordered_outputs = []\n        for output, target in y.items():\n            if targets is None:\n                targets = target\n            else:\n                targets = np.concatenate((targets, target), axis=1)\n            target_lengths[output] = target.shape[1]\n            ordered_outputs.append(output)\n\n\n        for flowx, flowy in super().flow(x, targets, batch_size=batch_size,\n                                         shuffle=shuffle):\n            target_dict = {}\n            i = 0\n            for output in ordered_outputs:\n                target_length = target_lengths[output]\n                target_dict[output] = flowy[:, i: i + target_length]\n                i += target_length\n\n            yield flowx, target_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"HEIGHT = 137\nWIDTH = 236","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training loop"},{"metadata":{"trusted":true},"cell_type":"code","source":"histories = []\n# считываем по очереди учебные наборы изображений и делаем джоин train_df_ с этими изображениями, чтобы определить их метки\nfor i in range(4):\n    train_df = pd.merge(pd.read_feather(featherdir/f'train_image_data_{i}.feather'), train_df_, on='image_id').drop(['image_id'], axis=1)\n    \n    # Визуализируйте несколько образцов текущего учебного набора данных\n    fig, ax = plt.subplots(nrows=3, ncols=4, figsize=(16, 8))\n    count=0\n    for row in ax:\n        for col in row:\n            col.imshow(resize(train_df.drop(['grapheme_root', 'vowel_diacritic', 'consonant_diacritic'], axis=1).iloc[[count]], size=IMG_SIZE,need_progress_bar=False).values.reshape(-1).reshape(IMG_SIZE, IMG_SIZE).astype(np.float64))\n            count += 1\n    plt.show()\n    \n    # удаляем из тренировочных данных метки классов\n    X_train = train_df.drop(['grapheme_root', 'vowel_diacritic', 'consonant_diacritic'], axis=1)\n    X_train = resize(X_train, size=IMG_SIZE)/255 # преобразовываем изображение к размеру, который ожидает на вход нейронная сеть и нормализуем размеры пикселей\n    \n    # CNN принимает изображения в форме `(batch_size, h, w, channel)`, поэтому изменяйте форму изображений\n    X_train = X_train.values.reshape(-1, IMG_SIZE, IMG_SIZE, N_CHANNELS)\n    # преобразовываем метки классов из целочисленных значений к векторам. \n    # К примеру согласные занки: вектор-столбец Y_train_consonant содержит значения от 0 до 6, а теперь он станет матрицей из 7 столбцов такой же длины\n    # И если i-тое значение было 5, то теперь это будет вектор строка [0, 0, 0, 0, 0, 1, 0]\n    Y_train_root = pd.get_dummies(train_df['grapheme_root']).values\n    Y_train_vowel = pd.get_dummies(train_df['vowel_diacritic']).values\n    Y_train_consonant = pd.get_dummies(train_df['consonant_diacritic']).values\n\n    print(f'Training images: {X_train.shape}')\n    print(f'Training labels root: {Y_train_root.shape}')\n    print(f'Training labels vowel: {Y_train_vowel.shape}')\n    print(f'Training labels consonants: {Y_train_consonant.shape}')\n\n    # Разделите данные на набор для обучения и проверки\n    x_train, x_test, y_train_root, y_test_root, y_train_vowel, y_test_vowel, y_train_consonant, y_test_consonant = train_test_split(X_train, Y_train_root, Y_train_vowel, Y_train_consonant, test_size=0.08, random_state=666)\n    del train_df\n    del X_train\n    del Y_train_root, Y_train_vowel, Y_train_consonant\n\n    # Увеличение данных для создания большего количества обучающих данных\n    datagen = MultiOutputDataGenerator(\n        featurewise_center=False,  # Смещение центра - установим значение 0, для всего набора данных\n        samplewise_center=False,  # Смещение центра -  установим значение 0, для каждой выборки\n        featurewise_std_normalization=False,  # уберем нормализацию для всего набора данных\n        samplewise_std_normalization=False,  # уберем нормализацию для каждой выборки\n        zca_whitening=False,  \n        rotation_range=12,  # случайным образом поворачивать изображения в диапазоне (градусы, 0 to 180)  (8)\n        zoom_range = 0.20, # Случайное увеличение изображения (0.15)\n        width_shift_range=0.15,  # случайное смещение изображений по горизонтали (доля от общей ширины)\n        height_shift_range=0.15,  # произвольно сдвигать изображения по вертикали (доля от общей высоты)\n        horizontal_flip=False,  # случайно перевернуть изображения относительно горизонтали\n        vertical_flip=False)  # случайно перевернуть изображения относительно вертикали\n\n\n    # Вычислим параметры, необходимые для дополнения данных. Но пока не будем выполнять какие-либо дополнения\n    datagen.fit(x_train)\n\n    # Обучим модель\n    history = model.fit_generator(datagen.flow(x_train, {'dense_2': y_train_root, 'dense_3': y_train_vowel, 'dense_4': y_train_consonant}, batch_size=batch_size),\n                              epochs = epochs, validation_data = (x_test, [y_test_root, y_test_vowel, y_test_consonant]), \n                              steps_per_epoch=x_train.shape[0] // batch_size, \n                              callbacks=[learning_rate_reduction_root, learning_rate_reduction_vowel, learning_rate_reduction_consonant])\n\n    histories.append(history)\n    \n    # Delete to reduce memory usage\n    del x_train\n    del x_test\n    del y_train_root\n    del y_test_root\n    del y_train_vowel\n    del y_test_vowel\n    del y_train_consonant\n    del y_test_consonant\n    gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# если необходимо, сохраните модель для дальнейшего использования\nname_model = 'own_model_1.h5'\nmodel.save(name_model)\n# model = load_model(name_model) # загрузить готовую модель для дальнейшего использования","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Строим графики потерь и точности**"},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\ndef plot_loss(his, epoch, title):\n    plt.style.use('ggplot')\n    plt.figure()\n    plt.plot(np.arange(0, epoch), his.history['loss'], label='train_loss')\n    plt.plot(np.arange(0, epoch), his.history['dense_3_loss'], label='train_root_loss')\n    plt.plot(np.arange(0, epoch), his.history['dense_4_loss'], label='train_vowel_loss')\n    plt.plot(np.arange(0, epoch), his.history['dense_5_loss'], label='train_consonant_loss')\n    \n    plt.plot(np.arange(0, epoch), his.history['val_dense_3_loss'], label='val_train_root_loss')\n    plt.plot(np.arange(0, epoch), his.history['val_dense_4_loss'], label='val_train_vowel_loss')\n    plt.plot(np.arange(0, epoch), his.history['val_dense_5_loss'], label='val_train_consonant_loss')\n    \n    plt.title(title)\n    plt.xlabel('Epoch #')\n    plt.ylabel('Loss')\n    plt.legend(loc='upper right')\n    plt.show()\n\ndef plot_acc(his, epoch, title):\n    plt.style.use('ggplot')\n    plt.figure()\n    plt.plot(np.arange(0, epoch), his.history['dense_3_accuracy'], label='train_root_acc')\n    plt.plot(np.arange(0, epoch), his.history['dense_4_accuracy'], label='train_vowel_accuracy')\n    plt.plot(np.arange(0, epoch), his.history['dense_5_accuracy'], label='train_consonant_accuracy')\n    \n    plt.plot(np.arange(0, epoch), his.history['val_dense_3_accuracy'], label='val_root_acc')\n    plt.plot(np.arange(0, epoch), his.history['val_dense_4_accuracy'], label='val_vowel_accuracy')\n    plt.plot(np.arange(0, epoch), his.history['val_dense_5_accuracy'], label='val_consonant_accuracy')\n    plt.title(title)\n    plt.xlabel('Epoch #')\n    plt.ylabel('Accuracy')\n    plt.legend(loc='upper right')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for dataset in range(4):\n    plot_loss(histories[dataset], epochs, f'Training Dataset: {dataset}')\n    plot_acc(histories[dataset], epochs, f'Training Dataset: {dataset}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del histories\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_dict = {\n    'grapheme_root': [],\n    'vowel_diacritic': [],\n    'consonant_diacritic': []\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"components = ['consonant_diacritic', 'grapheme_root', 'vowel_diacritic']\ntarget=[] # список предсказаний модели\nrow_id=[] # список id меток предсказаний\nfor i in range(4):\n#     df_test_img = pd.read_parquet(f'/kaggle/input/bengaliai-cv19/test_image_data_{i}.parquet') # читаем тестовый набор\n    df_test_img = pd.read_feather(featherdir/f'test_image_data_{i}.feather')  # читаем тестовый набор\n    df_test_img.set_index('image_id', inplace=True) # устанавливаем столбец с номером изображения, как индекс\n\n    X_test = resize(df_test_img, size=IMG_SIZE,need_progress_bar=False)/255 # меняем размер изображения и нормализуем его\n    X_test = X_test.values.reshape(-1, IMG_SIZE, IMG_SIZE, N_CHANNELS) # меняем размерность маьрицы изображения с учетом измерения для мини-пакетов\n    \n    preds = model.predict(X_test) # выполняем предсказание\n    \n    # перебираем три выхода нейронной сети\n    for i, p in enumerate(preds_dict):\n        preds_dict[p] = np.argmax(preds[i], axis=1) # находим вектор наиболее вероятных предсказаний для текущего типа классификатора (например для корней графем)\n    # записываем предсказания в формате, который требуется для сабмита\n    for k,id in enumerate(df_test_img.index.values):  \n        for i,comp in enumerate(components):\n            id_sample=id+'_'+comp\n            row_id.append(id_sample)\n            target.append(preds_dict[comp][k])\n    del df_test_img\n    del X_test\n    gc.collect()\n# преобразовываем полученные метки и предсказания в dataframe\ndf_sample = pd.DataFrame(\n    {\n        'row_id': row_id,\n        'target':target\n    },\n    columns = ['row_id','target'] \n)\n# записываем файл сабмита\ndf_sample.to_csv('submission.csv',index=False)\ndf_sample.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"108585220d384f1d87f3c8444587ae5b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"IntProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"IntProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"100%","description_tooltip":null,"layout":"IPY_MODEL_e4855ad98fad40deb899d57d401ae8c3","max":50210,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f0b119ea39cc420ea11296b456d078d8","value":50210}},"12ab2aed987f4a2e9bed87f8df6fbace":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b2e73348f1084afe92c3f03035d8606b","IPY_MODEL_b57249febd6c457baa5d4ba55f379ec1"],"layout":"IPY_MODEL_c9970f6bf5c54ff8808f97fdeb286846"}},"3540aa1072634a95b56adbc63b871c44":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"570a0b20e27c418687a1a1b252721597":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"62710fc6ed3346d5894184aa95d3a0dd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"75b62e2e75604016b1b25b7dfafd055e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"7dc300f0a85241fe8385df414e8a5c97":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"819ad327d8de4df6ab1c59de568544d6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b1f711e1aaf24dc197f468a9a1d2dac2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b2e45ab503b34cd68d5ffd0f81cb33f0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b2e73348f1084afe92c3f03035d8606b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"IntProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"IntProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"100%","description_tooltip":null,"layout":"IPY_MODEL_3540aa1072634a95b56adbc63b871c44","max":50210,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cfd07d8d18ed471fb9ea717da22823f0","value":50210}},"b57249febd6c457baa5d4ba55f379ec1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dc99860fe34c4945b98db54863554527","placeholder":"​","style":"IPY_MODEL_570a0b20e27c418687a1a1b252721597","value":" 50210/50210 [00:34&lt;00:00, 1472.92it/s]"}},"c40188b4079d4a32a3c40d43714ffdeb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"IntProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"IntProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"100%","description_tooltip":null,"layout":"IPY_MODEL_62710fc6ed3346d5894184aa95d3a0dd","max":50210,"min":0,"orientation":"horizontal","style":"IPY_MODEL_75b62e2e75604016b1b25b7dfafd055e","value":50210}},"c9970f6bf5c54ff8808f97fdeb286846":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cd962aa4c7514f50a817344fcc4281c0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7dc300f0a85241fe8385df414e8a5c97","placeholder":"​","style":"IPY_MODEL_b2e45ab503b34cd68d5ffd0f81cb33f0","value":" 50210/50210 [00:34&lt;00:00, 1453.38it/s]"}},"cfd07d8d18ed471fb9ea717da22823f0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"dc99860fe34c4945b98db54863554527":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ddd90cb7822f4ceca0f5bbdb623d9153":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_108585220d384f1d87f3c8444587ae5b","IPY_MODEL_cd962aa4c7514f50a817344fcc4281c0"],"layout":"IPY_MODEL_e175ccd71bbc41ff8aa02b43f00de935"}},"e175ccd71bbc41ff8aa02b43f00de935":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e4855ad98fad40deb899d57d401ae8c3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f0b119ea39cc420ea11296b456d078d8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"f8a313a8731e4aee9ba940aa7bc795b0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c40188b4079d4a32a3c40d43714ffdeb","IPY_MODEL_ffa184123d1f45c5a7296f99465f2a08"],"layout":"IPY_MODEL_fcd628bac3174930bdb3727134928c69"}},"fcd628bac3174930bdb3727134928c69":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ffa184123d1f45c5a7296f99465f2a08":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_819ad327d8de4df6ab1c59de568544d6","placeholder":"​","style":"IPY_MODEL_b1f711e1aaf24dc197f468a9a1d2dac2","value":" 50210/50210 [00:34&lt;00:00, 1436.32it/s]"}}},"version_major":2,"version_minor":0}}},"nbformat":4,"nbformat_minor":1}