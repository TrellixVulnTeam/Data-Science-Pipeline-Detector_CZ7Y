{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n%config Completer.use_jedi = False #auto complete\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport dask.dataframe as dd\nimport gc\nimport albumentations\nfrom PIL import Image\n\nimport torch\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n    \n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n!pip install iterative-stratification\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**create folder**","metadata":{}},{"cell_type":"code","source":"if __name__ == \"__main__\":\n  df = pd.read_csv(\"../input/bengaliai-cv19/train.csv\")\n  print(df.head())\n  df.loc[:, 'kfold'] = -1\n\n  df = df.sample(frac = 1).reset_index(drop=True)\n\n  X = df.image_id.values\n  y = df[[\"grapheme_root\", \"vowel_diacritic\", \"consonant_diacritic\"]].values\n\n  mskf = MultilabelStratifiedKFold(n_splits= 5)\n\n  for folder, (train_, validation_) in enumerate(mskf.split(X, y)):\n    print(\"Train: \", train_, \"Validation: \", validation_)\n    df.loc[validation_, 'kfold'] = folder\n  \n  print(df.kfold.value_counts())\n  df.to_csv(\"./train_folder.csv\", index = False)\n\ndel df \ndel mskf\ndel X\ndel y\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**check dataframes**","metadata":{}},{"cell_type":"code","source":"# df1 = pd.read_parquet(\"../input/bengaliai-cv19/train_image_data_0.parquet\")\n# df1.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**create image pickels** \n(to read images)","metadata":{}},{"cell_type":"code","source":"import joblib\nimport glob #list files\nfrom tqdm import tqdm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir image_pickels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from dask.distributed import Client\n# client = Client(n_workers=1, threads_per_worker=4, processes=False, memory_limit='16GB')\n# client","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df=0\n# image_ids = 0\n# image_array = 0\n# if __name__ == \"__main__\":\n#     files = glob.glob(\"../input/bengaliai-cv19/train_*.parquet\")\n#     for f in files:\n#         del df \n#         del image_ids\n#         del image_array\n#         gc.collect()\n#         df = pd.read_parquet(f)\n#         image_ids = df.image_id.values\n#         df = df.drop(\"image_id\", axis = 1)\n#         image_array = df.values\n#         for j, img_id in tqdm(enumerate(image_ids), total = len(image_ids)):\n#             joblib.dump(image_array[j, :], f\"./image_pickels/{img_id}.pkl\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=0\nimage_ids = 0\nimage_array = 0\nif __name__ == \"__main__\":\n    df = pd.read_parquet(\"../input/bengaliai-cv19/train_image_data_0.parquet\")\n    image_ids = df.image_id.values\n    df = df.drop(\"image_id\", axis = 1)\n    image_array = df.values\n    for j, img_id in tqdm(enumerate(image_ids), total = len(image_ids)):\n        joblib.dump(image_array[j, :], f\"./image_pickels/{img_id}.pkl\")\ndel df \ndel image_ids\ndel image_array\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=0\nimage_ids = 0\nimage_array = 0\nif __name__ == \"__main__\":\n    df = pd.read_parquet(\"../input/bengaliai-cv19/train_image_data_1.parquet\")\n    image_ids = df.image_id.values\n    df = df.drop(\"image_id\", axis = 1)\n    image_array = df.values\n    for j, img_id in tqdm(enumerate(image_ids), total = len(image_ids)):\n        joblib.dump(image_array[j, :], f\"./image_pickels/{img_id}.pkl\")\ndel df \ndel image_ids\ndel image_array\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=0\nimage_ids = 0\nimage_array = 0\nif __name__ == \"__main__\":\n    df = pd.read_parquet(\"../input/bengaliai-cv19/train_image_data_2.parquet\")\n    image_ids = df.image_id.values\n    df = df.drop(\"image_id\", axis = 1)\n    image_array = df.values\n    for j, img_id in tqdm(enumerate(image_ids), total = len(image_ids)):\n        joblib.dump(image_array[j, :], f\"./image_pickels/{img_id}.pkl\")\ndel df \ndel image_ids\ndel image_array\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=0\nimage_ids = 0\nimage_array = 0\nif __name__ == \"__main__\":\n    df = pd.read_parquet(\"../input/bengaliai-cv19/train_image_data_3.parquet\")\n    image_ids = df.image_id.values\n    df = df.drop(\"image_id\", axis = 1)\n    image_array = df.values\n    for j, img_id in tqdm(enumerate(image_ids), total = len(image_ids)):\n        joblib.dump(image_array[j, :], f\"./image_pickels/{img_id}.pkl\")\ndel df \ndel image_ids\ndel image_array\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del df \ndel image_ids\ndel image_array\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**dataset check/view**","metadata":{}},{"cell_type":"code","source":"class TrainD:\n    def __init__(self, folder, img_height, img_width, mean, std ):\n        df = pd.read_csv(\"./train_folder.csv\")\n        y = df[[\"image_id\", \"grapheme_root\", \"vowel_diacritic\", \"consonant_diacritic\", \"kfold\"]]\n        df = df[df.kfold.isin(folder)].reset_index(drop = True)\n        \n        self.image_ids = df.image_id.values\n        self.grapheme_root = df.grapheme_root.values        \n        self.vowel_diacritic = df.vowel_diacritic.values        \n        self.consonant_diacritic = df.consonant_diacritic.values   \n        \n        if len(folder) == 1:\n            self.aug = albumentations.Compose([\n                albumentations.Resize(img_height, img_width, always_apply = True),\n                albumentations.Normalize(mean, std, always_apply = True)\n            ])\n        else:         \n            self.aug = albumentations.Compose([\n                albumentations.Resize(img_height, img_width, always_apply = True),\n                albumentations.ShiftScaleRotate(shift_limit=0.0625, scale_limit=.01, rotate_limit=5, p=0.9),\n                albumentations.Normalize(mean, std, always_apply = True)\n            ])\n        \n    def __len__(self):\n        return len(self.image_ids)\n    def __getitem__(self, item):\n        image = joblib.load(f\"./image_pickels/{self.image_ids[item]}.pkl\")\n        image = image.reshape(137,236).astype(float)\n        image = Image.fromarray(image).convert(\"RGB\")\n        image = self.aug(image= np.array(image))[\"image\"]\n        image = np.transpose(image, (2,0,1)).astype(np.float32)\n        return {\n            'image': torch.tensor(image, dtype = torch.float),\n            'grapheme_root': torch.tensor(self.grapheme_root[item], dtype = torch.long),\n            'vowel_diacritic': torch.tensor(self.vowel_diacritic[item], dtype = torch.long),\n            'consonant_diacritic': torch.tensor(self.consonant_diacritic[item], dtype = torch.long),\n\n        }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = TrainD(folder=[0,1], img_height = 137, img_width=236, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"idx = 69\nimg = dataset[idx][\"image\"]\nprint(dataset[idx][\"grapheme_root\"])\nprint(dataset[idx][\"vowel_diacritic\"])\nprint(dataset[idx][\"consonant_diacritic\"])\nnpimg = img.numpy()\nplt.imshow(np.transpose(npimg, (1,2,0)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**model**","metadata":{}},{"cell_type":"code","source":"import sys\npt= \"../input/pretrained-models/pretrained-models.pytorch-master\"\nsys.path.insert(0, pt)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pretrainedmodels\nimport torch.nn as nn\nfrom torch.nn import functional as fnc\n\nclass ResNet34(nn.Module):\n    def __init__(self, pretrained):\n        super(ResNet34, self).__init__()\n        if pretrained is True:\n            self.model = pretrainedmodels.__dict__[\"resnet34\"](pretrained=\"imagenet\")\n        else:\n            self.model = pretrainedmodels.__dict__[\"resnet34\"](pretrained=None)\n\n        self.l0 = nn.Linear(512,168)\n        self.l1 = nn.Linear(512,11)\n        self.l2 = nn.Linear(512,7)\n\n    def forward(self, x):\n        bs, _,_, _ = x.shape\n        x = self.model.features(x)\n        x = fnc.adaptive_avg_pool2d(x, 1).reshape(bs, -1)\n        l0 = self.l0(x)\n        l1 = self.l1(x)\n        l2 = self.l2(x)\n        return l0, l1, l2\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**model dispatcher**","metadata":{}},{"cell_type":"code","source":"Model_dispatcher = {\n    'resnet34': ResNet34\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**training**","metadata":{}},{"cell_type":"code","source":"#variables\nimport ast\n\ndevice = \"cuda\"\ntrainFoldCSV = \"./train_folder.csv\"\nimgHeight = 137\nimgWidth = 236\nepochs = 50\nbaseModel = 'resnet34'\n\ntrainSize = 64 #128/256 \ntestSize = 8\n\nmodelMean = ast.literal_eval(\"(0.485, 0.456, 0.406)\")\nmodelStd = ast.literal_eval(\"(0.229, 0.224, 0.225)\")\n\ntrainFolder = ast.literal_eval(\"(0, 1, 2, 3)\")\nvalidFolder = ast.literal_eval(\"(4,)\")\n#testFolder\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(dataset, data_loader, model, optimizer):\n    model.train()\n\n    for bi, d in tqdm(enumerate(data_loader), total=int(len(dataset)/data_loader.batch_size)):\n        image = d[\"image\"]\n        grapheme_root = d[\"grapheme_root\"]\n        vowel_diacritic = d[\"vowel_diacritic\"]\n        consonant_diacritic = d[\"consonant_diacritic\"]\n\n        image = image.to(device, dtype = torch.float)\n        grapheme_root = grapheme_root.to(device, dtype = torch.long)\n        vowel_diacritic = vowel_diacritic.to(device, dtype = torch.long)\n        consonant_diacritic = consonant_diacritic.to(device, dtype = torch.long)\n\n        optimizer.zero_grad()\n        outputs = model(image)\n        targets = (grapheme_root, vowel_diacritic, consonant_diacritic)\n        loss = loss_fn(outputs, targets)\n\n        loss.backward()\n        optimizer.step()\n\n\n\ndef evaluate(dataset, data_loader, model):\n    model.eval()\n    final_loss = 0\n    counter = 0\n    \n    for bi, d in tqdm(enumerate(data_loader), total=int(len(dataset)/data_loader.batch_size)):\n        counter = counter + 1\n        image = d[\"image\"]\n        grapheme_root = d[\"grapheme_root\"]\n        vowel_diacritic = d[\"vowel_diacritic\"]\n        consonant_diacritic = d[\"consonant_diacritic\"]\n\n        image = image.to(device, dtype = torch.float)\n        grapheme_root = grapheme_root.to(device, dtype = torch.long)\n        vowel_diacritic = vowel_diacritic.to(device, dtype = torch.long)\n        consonant_diacritic = consonant_diacritic.to(device, dtype = torch.long)\n\n        outputs = model(image)\n        targets = (grapheme_root, vowel_diacritic, consonant_diacritic)\n        loss = loss_fn(outputs, targets)\n        final_loss = final_loss + loss\n    return final_loss / counter\n\ndef loss_fn(outputs, targets):\n    o1, o2, o3 = outputs\n    t1, t2, t3 = targets\n    l1 = nn.CrossEntropyLoss()(o1, t1)\n    l2 = nn.CrossEntropyLoss()(o2, t2)\n    l3 = nn.CrossEntropyLoss()(o3, t3)\n    return (l1+ l2+ l3) / 3\n\n\ndef main():\n    model = ResNet34(pretrained = True)\n    model.to(device)\n    \n    \n\n    train_dataset = TrainD(\n        folder =trainFolder,\n        img_height = imgHeight,\n        img_width = imgWidth,\n        mean= modelMean,\n        std = modelStd\n    )\n\n    train_loader = torch.utils.data.DataLoader(\n        dataset = train_dataset,\n        batch_size = trainSize,\n        shuffle = True,\n        num_workers = 4\n    )\n\n    valid_dataset = TrainD(\n        folder =validFolder,\n        img_height = imgHeight,\n        img_width = imgWidth,\n        mean= modelMean,\n        std = modelStd\n    )\n\n    valid_loader = torch.utils.data.DataLoader(\n        dataset = valid_dataset,\n        batch_size = testSize,\n        shuffle = False,\n        num_workers = 4\n    )\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode = \"min\", patience = 5, factor = 0.3, verbose = True)\n\n#     if torch.cuda.device_count() >1:\n#         model = nn.DataParallel(model)\n    #earlystopping\n    for epoch in range(epochs):\n        train(train_dataset, train_loader, model, optimizer)\n        val_score = evaluate(valid_dataset, valid_loader, model)\n        scheduler.step(val_score)\n        torch.save(model.state_dict(), f\"{baseModel}_fold{validation_folder[0]}.bin\")\n\n\nmain() ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainFolder = ast.literal_eval(\"(0, 1, 2, 4)\")\nvalidFolder = ast.literal_eval(\"(3,)\")\n#testFolder\nmain()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainFolder = ast.literal_eval(\"(0, 1, 3, 4)\")\nvalidFolder = ast.literal_eval(\"(2)\")\n#testFolder\nmain()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainFolder = ast.literal_eval(\"(0, 2, 3, 4)\")\nvalidFolder = ast.literal_eval(\"(1,)\")\n#testFolder\nmain()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainFolder = ast.literal_eval(\"(1, 2, 3, 4)\")\nvalidFolder = ast.literal_eval(\"(0,)\")\n#testFolder\nmain()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**test set**","metadata":{}},{"cell_type":"code","source":"testSize = 32\nclass TestD:\n    def __init__(self, df, img_height, img_width, mean, std ):\n#         df = pd.read_csv(\"./train_folder.csv\")\n#         y = df[[\"image_id\", \"grapheme_root\", \"vowel_diacritic\", \"consonant_diacritic\", \"kfold\"]]\n#         df = df[df.kfold.isin(folder)].reset_index(drop = True)\n        \n        self.image_ids = df.image_id.values\n        self.img_arr = df.iloc[:, 1:].values\n        \n#         self.grapheme_root = df.grapheme_root.values        \n#         self.vowel_diacritic = df.vowel_diacritic.values        \n#         self.consonant_diacritic = df.consonant_diacritic.values   \n        \n        self.aug = albumentations.Compose([\n            albumentations.Resize(img_height, img_width, always_apply = True),\n            albumentations.Normalize(mean, std, always_apply = True)\n        ])\n\n        \n    def __len__(self):\n        return len(self.image_ids)\n    def __getitem__(self, item):\n        image = self.img_arr[item, :]\n        img_id = self.image_ids[item]\n        image = image.reshape(137,236).astype(float)\n        image = Image.fromarray(image).convert(\"RGB\")\n        image = self.aug(image= np.array(image))[\"image\"]\n        image = np.transpose(image, (2,0,1)).astype(np.float32)\n        return {\n            'image': torch.tensor(image, dtype = torch.float),\n            'image_id': img_id\n\n        }","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}