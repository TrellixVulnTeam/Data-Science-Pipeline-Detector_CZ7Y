{"cells":[{"metadata":{},"cell_type":"markdown","source":"This is a script based on this [Discussion](https://www.kaggle.com/c/bengaliai-cv19/discussion/126054) and his original [notebook](https://www.kaggle.com/pestipeti/fast-ensemble-5-folds-20-minutes). \n\nI made a single model version of his script. The credit is all his, and it's my fault if there is any implementation error in this script."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import gc\nimport numpy as np\nimport pandas as pd\nimport time\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport pyarrow.parquet as pq\nimport cv2\n\nimport sys\nsys.path.append('/kaggle/input/utilities/')\nimport SeResNeXt\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def crop_image_only_outside(img, tol=0):\n    mask = img > tol\n    m, n = img.shape\n    mask0, mask1 = mask.any(0), mask.any(1)\n    col_start, col_end = mask0.argmax(), n - mask0[::-1].argmax()\n    row_start, row_end = mask1.argmax(), m - mask1[::-1].argmax()\n    return img[row_start:row_end, col_start:col_end]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_PATH = '/kaggle/input/bengaliai-cv19/'\nsample_submission = pd.read_csv(\"../input/bengaliai-cv19/sample_submission.csv\")\nnum_samples = sample_submission.shape[0] // 3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TARGET_SIZE = 64\nBATCH_SIZE = 96\nN_WORKERS = 4\nHEIGHT = 137\nWIDTH = 236\n\nclass GraphemeValidationDataset(Dataset):\n    def __init__(self, num_samples):\n        self.num_samples = num_samples\n        self.images = torch.zeros(num_samples, TARGET_SIZE, TARGET_SIZE, dtype=torch.uint8)\n        img_id = 0\n        \n        print('start reading in datas.')\n        for i in range(4):\n\n            datafile = DATA_PATH + '/test_image_data_{}.parquet'.format(i)\n            parq = pq.read_pandas(datafile, columns=[str(x) for x in range(32332)]).to_pandas()\n            parq = 255 - parq.iloc[:, :].values.reshape(-1, HEIGHT, WIDTH).astype(np.uint8)\n\n            for idx, image in enumerate(parq):\n                image = (image * (255.0 / image.max())).astype(np.uint8)\n                image = crop_image_only_outside(image,80)\n                image = cv2.resize(image, (TARGET_SIZE, TARGET_SIZE))\n                self.images[img_id, :, :] = torch.from_numpy(image.astype(np.uint8))\n                img_id = img_id + 1\n                \n        del parq\n        gc.collect()\n        print('finish reading in datas.')\n\n    def __len__(self):\n        return self.num_samples\n\n    def __getitem__(self, idx):\n        return self.images[idx].unsqueeze(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Identity(torch.nn.Module):\n    def __init__(self):\n        super(Identity, self).__init__()\n\n    def forward(self, x):\n        return x\n    \n    \nclass MyNet(nn.Module):\n    def __init__(self):\n        super(MyNet, self).__init__()\n        self.backbone = SeResNeXt.se_resnext101()\n        self.backbone.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.backbone.fc = Identity()\n        self.fc1 = nn.Linear(2048, 11)  # vowel_diacritic\n        self.fc2 = nn.Linear(2048, 168)  # grapheme_root\n        self.fc3 = nn.Linear(2048, 7)  # consonant_diacritic\n\n    def forward(self, x):\n        x = x / 255.\n        x = self.backbone(x)\n        x = x.view(x.size(0), -1)\n        vowel_diacritic = self.fc1(x)\n        grapheme_root = self.fc2(x)\n        consonant_diacritic = self.fc3(x)\n        return vowel_diacritic, grapheme_root, consonant_diacritic\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bengali_dataset = GraphemeValidationDataset(num_samples=num_samples)\ndata_loader_test = DataLoader(bengali_dataset, batch_size=BATCH_SIZE, num_workers=N_WORKERS, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = MyNet()\n\nMODEL_PATH = '/kaggle/input/submission10/0597.pth'\ndevice = torch.device('cuda:0')\ncheckpoint = torch.load(MODEL_PATH, map_location=device)\nif isinstance(checkpoint, dict) and 'state_dict' in checkpoint:\n    model.load_state_dict(checkpoint['state_dict'])\nelse:\n    model.load_state_dict(checkpoint)\n\nmodel.eval()\nmodel.to(device)\n\ndel checkpoint","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('start inference')\n\nresults = np.zeros((3, num_samples), dtype=np.int)\n\ntic = time.perf_counter()\nfor batch_idx, images in enumerate(data_loader_test):\n\n    images = images.float().to(device)\n\n    with torch.no_grad():\n        vowel_diacritic, grapheme_root, consonant_diacritic = model(images)\n\n        start = batch_idx * BATCH_SIZE\n        end = min((batch_idx + 1) * BATCH_SIZE, num_samples)\n\n        results[0, start:end] = consonant_diacritic.argmax(1).cpu().detach().numpy()\n        results[1, start:end] = grapheme_root.argmax(1).cpu().detach().numpy()\n        results[2, start:end] = vowel_diacritic.argmax(1).cpu().detach().numpy()\n        \n    del images\n    del vowel_diacritic, grapheme_root, consonant_diacritic\n\ndel model\ngc.collect()\n\nprint('finish inference in {:.2f} sec.'.format(time.perf_counter()-tic))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result_reshape = results.reshape(3*num_samples, order='F')\nsample_submission = pd.read_csv(\"../input/bengaliai-cv19/sample_submission.csv\")\nsample_submission.target = np.hstack(result_reshape)\n\nsample_submission.to_csv('submission.csv', index=False)\nprint('finish writing submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}