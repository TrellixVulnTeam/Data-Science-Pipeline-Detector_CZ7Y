{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport os\nimport matplotlib.pyplot as plt\nimport pylab\nimport librosa\nimport librosa.display\nimport numpy as np\nimport pandas as pd\nimport io\nimport tarfile\nimport PIL\nimport boto3\nfrom fastai import *\nfrom fastai.vision import *\nimport cv2\nfrom tqdm import tqdm_notebook as tqdm\nimport zipfile\nimport io\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n    #for filename in filenames:\n        #print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"sz = 128\nbs = 128\nnfolds = 4 #keep the same split as the initial dataset\nfold = 0\nSEED = 2019\nTRAIN = '../input/grapheme-imgs-128x128/'\nLABELS = '../input/bengaliai-cv19/train.csv'\narch = models.resnet34\n\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(LABELS)\nnunique = list(df.nunique())[1:-1]\nprint(nunique)\ndf.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"HEIGHT = 137\nWIDTH = 236\nSIZE = 128\n\nTEST = ['/kaggle/input/bengaliai-cv19/test_image_data_0.parquet',\n         '/kaggle/input/bengaliai-cv19/test_image_data_1.parquet',\n         '/kaggle/input/bengaliai-cv19/test_image_data_2.parquet',\n         '/kaggle/input/bengaliai-cv19/test_image_data_3.parquet']\n\nOUT_TEST = 'test.zip'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def bbox(img):\n    rows = np.any(img, axis=1)\n    cols = np.any(img, axis=0)\n    rmin, rmax = np.where(rows)[0][[0, -1]]\n    cmin, cmax = np.where(cols)[0][[0, -1]]\n    return rmin, rmax, cmin, cmax\n\ndef crop_resize(img0, size=SIZE, pad=16):\n    #crop a box around pixels large than the threshold \n    #some images contain line at the sides\n    ymin,ymax,xmin,xmax = bbox(img0[5:-5,5:-5] > 80) #try 60\n    #cropping may cut too much, so we need to add it back\n    xmin = xmin - 13 if (xmin > 13) else 0\n    ymin = ymin - 10 if (ymin > 10) else 0\n    xmax = xmax + 13 if (xmax < WIDTH - 13) else WIDTH\n    ymax = ymax + 10 if (ymax < HEIGHT - 10) else HEIGHT\n    img = img0[ymin:ymax,xmin:xmax]\n    #remove lo intensity pixels as noise\n    img[img < 28] = 0\n    lx, ly = xmax-xmin,ymax-ymin\n    l = max(lx,ly) + pad\n    #make sure that the aspect ratio is kept in rescaling\n    img = np.pad(img, [((l-ly)//2,), ((l-lx)//2,)], mode='constant')\n    return cv2.resize(img,(size,size))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_parquet(TEST[0])\nn_imgs = 3\nfig, axs = plt.subplots(n_imgs, 2, figsize=(10, 5*n_imgs))\n\nfor idx in range(n_imgs):\n    #somehow the original input is inverted\n    img0 = 255 - df.iloc[idx, 1:].values.reshape(HEIGHT, WIDTH).astype(np.uint8)\n    #normalize each image by its max val\n    img = (img0*(255.0/img0.max())).astype(np.uint8)\n    img = crop_resize(img)\n\n    axs[idx,0].imshow(img0)\n    axs[idx,0].set_title('Original image')\n    axs[idx,0].axis('off')\n    axs[idx,1].imshow(img)\n    axs[idx,1].set_title('Crop & resize')\n    axs[idx,1].axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(LABELS)\nnunique = list(df.nunique())[1:-1]\nprint(nunique)\ndf.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pathimg = datapath4file('../input/test-bengali')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stats = ([0.0692], [0.2051])\ndata1 = (ImageList.from_df(df, path='.', folder=TRAIN, suffix='.png', \n        cols='image_id')\n        .split_by_idx(range(fold*len(df)//nfolds,(fold+1)*len(df)//nfolds))\n        .label_from_df(cols=['grapheme_root'])\n        .transform(get_transforms(do_flip=False,max_warp=0.1), size=sz, padding_mode='zeros')\n        .databunch(bs=bs)).normalize(stats)\n\n#data1.show_batch()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stats = ([0.0692], [0.2051])\ndata2 = (ImageList.from_df(df, path='.', folder=TRAIN, suffix='.png', \n        cols='image_id')\n        .split_by_idx(range(fold*len(df)//nfolds,(fold+1)*len(df)//nfolds))\n        .label_from_df(cols=['vowel_diacritic'])\n        .transform(get_transforms(do_flip=False,max_warp=0.1), size=sz, padding_mode='zeros')\n        .databunch(bs=bs)).normalize(stats)\n\n#data2.show_batch()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stats = ([0.0692], [0.2051])\ndata3 = (ImageList.from_df(df, path='.', folder=TRAIN, suffix='.png', \n        cols='image_id')\n        .split_by_idx(range(fold*len(df)//nfolds,(fold+1)*len(df)//nfolds))\n        .label_from_df(cols=['consonant_diacritic'])\n        .transform(get_transforms(do_flip=False,max_warp=0.1), size=sz, padding_mode='zeros')\n        .databunch(bs=bs)).normalize(stats)\n\n#data3.show_batch()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torchvision.models import resnet34","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def my_resnet(pretrained=False, progress=True, **kwargs):\n    m = resnet34(pretrained=False, progress=True, **kwargs)\n    m.load_state_dict(torch.load(\"/kaggle/input/resnet34/resnet34.pth\"))\n    return m","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn1 = cnn_learner(data1, my_resnet)\nlearn2 = cnn_learner(data2, my_resnet)\nlearn3 = cnn_learner(data3, my_resnet)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn1.load('/kaggle/input/saved-models/stage-1')\nlearn2.load('/kaggle/input/saved-models/stage-2')\nlearn3.load('/kaggle/input/saved-models/stage-3')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"file_name = []\npred = []\nfor fname in TEST:\n    df1 = pd.read_parquet(fname)\n    #the input is inverted\n    data = 255 - df1.iloc[:, 1:].values.reshape(-1, HEIGHT, WIDTH).astype(np.uint8)\n    for idx in tqdm(range(len(df1))):\n        name = df1.iloc[idx,0]\n        #normalize each image by its max val\n        img = (data[idx]*(255.0/data[idx].max())).astype(np.uint8)\n        img = crop_resize(img)\n    # to make prediction with {model}.predict converting to tensor is important\n        img = np.stack((img,)*3, axis=-1)\n        img = pil2tensor(img,np.float32).div_(255)\n        c=int(learn3.predict(Image(img))[0])\n        pred.append(c)\n        file_name.append(str(name)+'_'+'consonant_diacritic')\n        g=int(learn1.predict(Image(img))[0])\n        pred.append(g)\n        file_name.append(str(name)+'_'+'grapheme_root')\n        v=int(learn2.predict(Image(img))[0])\n        pred.append(v)\n        file_name.append(str(name)+'_'+'vowel_diacritic')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"x_tot,x2_tot = [],[]\nrow_id1, catag1 = [], []\n\nfor i in range(1):\n    df = pd.read_parquet(TEST[i])\n    n_imgs=np.size(df,0)\n    for idx in range(n_imgs):\n        #somehow the original input is inverted\n        img0 = 255 - df.iloc[idx, 1:].values.reshape(HEIGHT, WIDTH).astype(np.uint8)\n        #normalize each image by its max val\n        img = (img0*(255.0/img0.max())).astype(np.uint8)\n        img = crop_resize(img)\n        img = np.stack((img,)*3, axis=-1)\n        #img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        img_fastai = Image(pil2tensor(img, dtype=np.uint8))\n        row_id1.append(df.image_id[idx]+\"_consonant_diacritic\")\n        row_id1.append(df.image_id[idx]+\"_grapheme_root\")\n        row_id1.append(df.image_id[idx]+\"_vowel_diacritic\")\n        catag1.append(int(learn3.predict(img_fastai)[0]))\n        catag1.append(int(learn1.predict(img_fastai)[0]))\n        catag1.append(int(learn2.predict(img_fastai)[0]))\"\"\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"[](https://www.kaggle.com/gaur128/bengali-graphemems-model-inference?scriptVersionId=29481219)\nhttps://www.kaggle.com/venky2506/fastai-inference-128x128-v2"},{"metadata":{"trusted":true},"cell_type":"code","source":"'''df = pd.read_parquet(TEST[3])\nn_imgs=np.size(df,0)\nfor idx in range(n_imgs):\n    #somehow the original input is inverted\n    img0 = 255 - df.iloc[idx, 1:].values.reshape(HEIGHT, WIDTH).astype(np.uint8)\n    #normalize each image by its max val\n    img = (img0*(255.0/img0.max())).astype(np.uint8)\n    img = crop_resize(img)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img_fastai = Image(pil2tensor(img, dtype=np.uint8))'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"''''class GraphemeDataset(Dataset):\n    def __init__(self, fname):\n        self.df = pd.read_parquet(fname)\n        self.data = 255 - self.df.iloc[:, 1:].values.reshape(-1, HEIGHT, WIDTH).astype(np.uint8)\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        name = self.df.iloc[idx,0]\n        #normalize each image by its max val\n        img = (self.data[idx]*(255.0/self.data[idx].max())).astype(np.uint8)\n        img = crop_resize(img)\n        img = (img.astype(np.float32)/255.0 - stats[0])/stats[1]\n        return img, name'''\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#nworkers = 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''p1,p2,p3 = int(learn1.predict(Image(x.float()))[0]), int(learn2.predict(Image(x.float()))[0]), int(learn3.predict(Image(x.float()))[0])\n            for idx,name in enumerate(y):\n                row_id += [f'{name}_grapheme_root',f'{name}_vowel_diacritic',\n                           f'{name}_consonant_diacritic']\n                target += [p1,p2,p3]'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''p1,p2,p3 = int(learn1.predict(Image(x.float()))[0]), int(learn2.predict(Image(x.float()))[0]), int(learn3.predict(Image(x.float()))[0])\n            for idx,name in enumerate(y):\n                row_id += [f'{name}_grapheme_root',f'{name}_vowel_diacritic',\n                           f'{name}_consonant_diacritic']\n                target += [p1,p2,p3]'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"''''row_id,target = [],[]\nfor fname in TEST:\n    ds = GraphemeDataset(fname)\n    dl = DataLoader(ds, batch_size=bs, num_workers=nworkers, shuffle=False)\n    with torch.no_grad():\n        for x,y in tqdm(dl):\n            print(x)'''\n ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''sub_df = pd.DataFrame({'row_id': row_id, 'target': target})\nsub_df.to_csv('submission.csv', index=False)\nsub_df.head(100)'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''#real\nx_tot,x2_tot = [],[]\nrow_id, catag = [], []\n\nfor i in range(4):\n    df = pd.read_parquet(TEST[i])\n    n_imgs=np.size(df,0)\n    for idx in range(n_imgs):\n        #somehow the original input is inverted\n        img0 = 255 - df.iloc[idx, 1:].values.reshape(HEIGHT, WIDTH).astype(np.uint8)\n        #normalize each image by its max val\n        img = (img0*(255.0/img0.max())).astype(np.uint8)\n        img = crop_resize(img)\n        cv2.imwrite('output.png',img)\n        A=open_image('output.png') \n        row_id.append(df.image_id[idx]+\"_consonant_diacritic\")\n        row_id.append(df.image_id[idx]+\"_grapheme_root\")\n        row_id.append(df.image_id[idx]+\"_vowel_diacritic\")\n        catag.append(int(learn3.predict(A)[0]))\n        catag.append(int(learn1.predict(A)[0]))\n        catag.append(int(learn2.predict(A)[0]))\n    del df\n    #print(int(learn1.predict(A)[0]))\n    #img2 = cv2.cvtColor(np.float32(img), cv2.COLOR_BGR2RGB)\n    #img_fastai = Image(pil2tensor(img2, dtype=np.float32))\n    #print(int(learn1.predict(img_fastai)[0]))'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"row_id = pd.Series(file_name)\ncatag = pd.Series(pred)\nframe = { 'row_id': row_id , 'target': catag} \nresult = pd.DataFrame(frame)\nresult.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del file_name\ndel pred\ndel row_id\ndel catag\ndel df\ndel frame","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#!rm -rf  /kaggle/working/output.png/*","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}