{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom tqdm.auto import tqdm\nfrom glob import glob\nimport time, gc\nimport cv2\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def resize(df, size=64, need_progress_bar=True):\n    resized = {}\n    resize_size=64\n    if need_progress_bar:\n        for i in tqdm(range(df.shape[0])):\n            image=df.loc[df.index[i]].values.reshape(137,236)\n\n            _, thresh = cv2.threshold(image, 30, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n            contours, _ = cv2.findContours(thresh,cv2.RETR_LIST,cv2.CHAIN_APPROX_SIMPLE)[-2:]\n\n            idx = 0 \n            ls_xmin = []\n            ls_ymin = []\n            ls_xmax = []\n            ls_ymax = []\n            for cnt in contours:\n                idx += 1\n                x,y,w,h = cv2.boundingRect(cnt)\n                ls_xmin.append(x)\n                ls_ymin.append(y)\n                ls_xmax.append(x + w)\n                ls_ymax.append(y + h)\n            xmin = min(ls_xmin)\n            ymin = min(ls_ymin)\n            xmax = max(ls_xmax)\n            ymax = max(ls_ymax)\n\n            roi = image[ymin:ymax,xmin:xmax]\n            resized_roi = cv2.resize(roi, (resize_size, resize_size),interpolation=cv2.INTER_AREA)\n            resized[df.index[i]] = resized_roi.reshape(-1)\n    else:\n        for i in range(df.shape[0]):\n            #image = cv2.resize(df.loc[df.index[i]].values.reshape(137,236),(size,size),None,fx=0.5,fy=0.5,interpolation=cv2.INTER_AREA)\n            image=df.loc[df.index[i]].values.reshape(137,236)\n\n\n            _, thresh = cv2.threshold(image, 30, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n            contours, _ = cv2.findContours(thresh,cv2.RETR_LIST,cv2.CHAIN_APPROX_SIMPLE)[-2:]\n\n            idx = 0 \n            ls_xmin = []\n            ls_ymin = []\n            ls_xmax = []\n            ls_ymax = []\n            for cnt in contours:\n                idx += 1\n                x,y,w,h = cv2.boundingRect(cnt)\n                ls_xmin.append(x)\n                ls_ymin.append(y)\n                ls_xmax.append(x + w)\n                ls_ymax.append(y + h)\n            xmin = min(ls_xmin)\n            ymin = min(ls_ymin)\n            xmax = max(ls_xmax)\n            ymax = max(ls_ymax)\n\n            roi = image[ymin:ymax,xmin:xmax]\n            resized_roi = cv2.resize(roi, (resize_size, resize_size),interpolation=cv2.INTER_AREA)\n            resized[df.index[i]] = resized_roi.reshape(-1)\n    resized = pd.DataFrame(resized).T\n    return resized","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"IMG_SIZE=64\nN_CHANNELS=1\n\nbatch_size = 256\nepochs = 20\n\nHEIGHT = 137\nWIDTH = 236","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Model, clone_model, Sequential\nfrom keras.layers import Dense, Conv2D, Flatten, MaxPool2D, Dropout, BatchNormalization, Input\nfrom keras.layers import AveragePooling2D, GlobalAveragePooling2D\nfrom keras.applications.resnet50 import ResNet50\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.optimizers import RMSprop\nfrom keras.regularizers import l2\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport keras.backend as K\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### CNN Branching Model(Multi-output)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def cnn_branching(i):\n    inputs = Input(shape = (IMG_SIZE, IMG_SIZE, 1))\n    model = Conv2D(filters=32, kernel_size=(3, 3), padding='SAME', activation='relu',\n                   kernel_regularizer = l2(0.001))(inputs)\n    model = Conv2D(filters=32, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n    model = Conv2D(filters=32, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n    model = Conv2D(filters=32, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n    model = BatchNormalization(momentum=0.15)(model)\n    model = MaxPool2D(pool_size=(2, 2))(model)\n    model = Dropout(rate=0.3)(model)\n    \n    model = Conv2D(filters=64, kernel_size=(3, 3), padding='SAME', activation='relu',\n                   kernel_regularizer = l2(0.001))(model)\n    model = Conv2D(filters=64, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n    model = Conv2D(filters=64, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n    model = Conv2D(filters=64, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n    model = BatchNormalization(momentum=0.15)(model)\n    model = MaxPool2D(pool_size=(2, 2))(model)\n    model = Dropout(rate=0.3)(model)\n    \n    model = Conv2D(filters=128, kernel_size=(3, 3), padding='SAME', activation='relu',\n                   kernel_regularizer = l2(0.001))(model)\n    model = Conv2D(filters=128, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n    model = Conv2D(filters=128, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n    model = Conv2D(filters=128, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n    model = BatchNormalization(momentum=0.15)(model)\n    model = MaxPool2D(pool_size=(2, 2))(model)\n    model = Dropout(rate=0.3)(model)\n    \n    _x = Conv2D(filters=256, kernel_size=(3, 3), padding='SAME', activation='relu',\n                kernel_regularizer = l2(0.001))(model)\n    _x = Conv2D(filters=256, kernel_size=(3, 3), padding='SAME', activation='relu')(_x)\n    _x = Conv2D(filters=256, kernel_size=(3, 3), padding='SAME', activation='relu')(_x)\n    _x = Conv2D(filters=256, kernel_size=(3, 3), padding='SAME', activation='relu')(_x)\n    _x = BatchNormalization(momentum=0.15)(_x)\n    _x = MaxPool2D(pool_size=(2, 2))(_x)\n    _x = Dropout(rate=0.3)(_x)\n\n    _x = Flatten()(_x)\n    # _x = Dense(1024, activation = \"relu\")(_x)\n    _x = Dropout(rate=0.3)(_x)\n    _x = Dense(512, activation = \"relu\")(_x)\n    root = Dense(168, activation = 'softmax', name = 'root_'+str(i))(_x)\n    \n    #for vowel and consonant\n    _x1 = Flatten()(model)\n    _x1 = Dense(256, activation = 'relu')(_x1)\n    _x1 = Dropout(rate=0.3)(_x1)\n    dense_1 = Dense(256, activation = 'relu')(_x1)\n    vowel = Dense(11, activation='softmax', name = 'vowel_'+str(i))(dense_1)\n    consonant = Dense(7, activation='softmax', name = 'consonant_'+str(i))(dense_1)\n\n    model = Model(inputs=inputs, outputs=[root, vowel, consonant])\n    model.compile(optimizer = 'adam',\n              loss='categorical_crossentropy',\n              loss_weights = {'root_'+str(i): 2, 'vowel_'+str(i):1, 'consonant_'+str(i):1},\n              metrics=['accuracy'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = cnn_branching(0)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow import keras\nclass MultiOutputDataGenerator(keras.preprocessing.image.ImageDataGenerator):\n\n    def flow(self,\n             x,\n             y=None,\n             batch_size=32,\n             shuffle=True,\n             sample_weight=None,\n             seed=None,\n             save_to_dir=None,\n             save_prefix='',\n             save_format='png',\n             subset=None):\n\n        targets = None\n        target_lengths = {}\n        ordered_outputs = []\n        for output, target in y.items():\n            if targets is None:\n                targets = target\n            else:\n                targets = np.concatenate((targets, target), axis=1)\n            target_lengths[output] = target.shape[1]\n            ordered_outputs.append(output)\n\n\n        for flowx, flowy in super().flow(x, targets, batch_size=batch_size,\n                                         shuffle=shuffle):\n            target_dict = {}\n            i = 0\n            for output in ordered_outputs:\n                target_length = target_lengths[output]\n                target_dict[output] = flowy[:, i: i + target_length]\n                i += target_length\n\n            yield flowx, target_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df_ = pd.read_csv('/kaggle/input/bengaliai-cv19/train.csv')\ntest_df_ = pd.read_csv('/kaggle/input/bengaliai-cv19/test.csv')\ntrain_df_ = train_df_.drop(['grapheme'], axis=1, inplace=False)\ntrain_df_[['grapheme_root', 'vowel_diacritic', 'consonant_diacritic']] = train_df_[['grapheme_root', 'vowel_diacritic', 'consonant_diacritic']].astype('uint8')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_dict = {\n    'grapheme_root': [],\n    'vowel_diacritic': [],\n    'consonant_diacritic': []\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set a learning rate annealer. Learning rate will be half after 3 epochs if accuracy is not increased\ndef lr_reduction(i):\n    learning_rate_reduction_root = ReduceLROnPlateau(monitor='root_'+str(i)+'_accuracy', \n                                                patience=3, \n                                                verbose=1,\n                                                factor=0.5, \n                                                min_lr=0.000001)\n    learning_rate_reduction_vowel = ReduceLROnPlateau(monitor='vowel_'+str(i)+'_accuracy', \n                                                patience=3, \n                                                verbose=1,\n                                                factor=0.5, \n                                                min_lr=0.000001)\n    learning_rate_reduction_consonant = ReduceLROnPlateau(monitor='vowel_'+str(i)+'_accuracy', \n                                                patience=3, \n                                                verbose=1,\n                                                factor=0.5, \n                                                min_lr=0.000001)\n    return learning_rate_reduction_root, learning_rate_reduction_vowel, learning_rate_reduction_consonant","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()\nnets = 2\nhistories = []\nmodels = []\nmodel_1 = cnn_branching(1)\nmodel_2 = cnn_branching(2)\n# model_3 = cnn_branching(3)\nroot_1, vowel_1, cons_1 = lr_reduction(1)\nroot_2, vowel_2, cons_2 = lr_reduction(2)\n# root_3, vowel_3, cons_3 = lr_reduction(3)\n\n\nfor i in range(4):\n    hist = []\n    print(\"######################training_loop_{}################################\".format(i))\n    train_df = pd.merge(pd.read_parquet(f'/kaggle/input/bengaliai-cv19/train_image_data_{i}.parquet'), train_df_, on='image_id').drop(['image_id'], axis=1)\n    \n    \n    X_train = train_df.drop(['grapheme_root', 'vowel_diacritic', 'consonant_diacritic'], axis=1)\n    X_train = resize(X_train)/255\n    \n    # CNN takes images in shape `(batch_size, h, w, channels)`, so reshape the images\n    X_train = X_train.values.reshape(-1, IMG_SIZE, IMG_SIZE, N_CHANNELS)\n    \n    Y_train_root = pd.get_dummies(train_df['grapheme_root']).values\n    Y_train_vowel = pd.get_dummies(train_df['vowel_diacritic']).values\n    Y_train_consonant = pd.get_dummies(train_df['consonant_diacritic']).values\n\n    print(f'Training images: {X_train.shape}')\n    print(f'Training labels root: {Y_train_root.shape}')\n    print(f'Training labels vowel: {Y_train_vowel.shape}')\n    print(f'Training labels consonants: {Y_train_consonant.shape}')\n\n    # Divide the data into training and validation set\n    x_train, x_test, y_train_root, y_test_root, y_train_vowel, y_test_vowel, y_train_consonant, y_test_consonant = train_test_split(X_train, Y_train_root, Y_train_vowel, Y_train_consonant, test_size=0.08, random_state=666)\n    del train_df\n    del X_train\n    del Y_train_root, Y_train_vowel, Y_train_consonant\n\n    # Data augmentation for creating more training data\n    datagen = MultiOutputDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.15, # Randomly zoom image \n        width_shift_range=0.15,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.15,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\n\n    # This will just calculate parameters required to augment the given data. This won't perform any augmentations\n    datagen.fit(x_train)\n\n    # Fit the model\n    print(\"CNN----------------------------1\")\n    hist_1 = model_1.fit_generator(datagen.flow(x_train, {'root_1': y_train_root, 'vowel_1': y_train_vowel, 'consonant_1': y_train_consonant}, batch_size=batch_size),\n                            epochs = epochs, validation_data = (x_test, [y_test_root, y_test_vowel, y_test_consonant]), \n                            steps_per_epoch=x_train.shape[0] // batch_size, \n                            #   class_weight = class_weights,\n                            callbacks=[root_1, vowel_1, cons_1]\n                            )\n    print(\"CNN----------------------------2\")\n    hist_2 = model_2.fit_generator(datagen.flow(x_train, {'root_2': y_train_root, 'vowel_2': y_train_vowel, 'consonant_2': y_train_consonant}, batch_size=batch_size),\n                        epochs = epochs, validation_data = (x_test, [y_test_root, y_test_vowel, y_test_consonant]), \n                        steps_per_epoch=x_train.shape[0] // batch_size, \n                        #   class_weight = class_weights,\n                        callbacks=[root_2, vowel_2, cons_2]\n                        )\n#     print(\"CNN----------------------------3\")\n#     hist_3 = model_3.fit_generator(datagen.flow(x_train, {'root_3': y_train_root, 'vowel_3': y_train_vowel, 'consonant_3': y_train_consonant}, batch_size=batch_size),\n#                         epochs = epochs, validation_data = (x_test, [y_test_root, y_test_vowel, y_test_consonant]), \n#                         steps_per_epoch=x_train.shape[0] // batch_size, \n#                         #   class_weight = class_weights,\n#                         callbacks=[root_3, vowel_3, cons_3]\n#                         )\n\n    hist.append(hist_1)\n    hist.append(hist_2)\n#     hist.append(hist_3)\n    histories.append(hist)\n    del hist_1\n    del hist_2\n#     del hist_3\n    # Delete to reduce memory usage\n    del x_train\n    del x_test\n    del y_train_root\n    del y_test_root\n    del y_train_vowel\n    del y_test_vowel\n    del y_train_consonant\n    del y_test_consonant\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_loss(his, epoch, title):\n    plt.style.use('ggplot')\n    plt.figure()\n    plt.plot(np.arange(0, epoch), his.history['loss'], label='train_loss')\n    plt.plot(np.arange(0, epoch), his.history['dense_3_loss'], label='train_root_loss')\n    plt.plot(np.arange(0, epoch), his.history['dense_6_loss'], label='train_vowel_loss')\n    plt.plot(np.arange(0, epoch), his.history['dense_7_loss'], label='train_consonant_loss')\n    \n    plt.plot(np.arange(0, epoch), his.history['val_dense_3_loss'], label='val_train_root_loss')\n    plt.plot(np.arange(0, epoch), his.history['val_dense_6_loss'], label='val_train_vowel_loss')\n    plt.plot(np.arange(0, epoch), his.history['val_dense_7_loss'], label='val_train_consonant_loss')\n    \n    plt.title(title)\n    plt.xlabel('Epoch #')\n    plt.ylabel('Loss')\n    plt.legend(loc='upper right')\n    plt.show()\n\ndef plot_acc(his, epoch, title):\n    plt.style.use('ggplot')\n    plt.figure()\n    plt.plot(np.arange(0, epoch), his.history['dense_3_accuracy'], label='train_root_accuracy')\n    plt.plot(np.arange(0, epoch), his.history['dense_6_accuracy'], label='train_vowel_accuracy')\n    plt.plot(np.arange(0, epoch), his.history['dense_7_accuracy'], label='train_consonant_accuracy')\n    \n    plt.plot(np.arange(0, epoch), his.history['val_dense_3_accuracy'], label='val_root_accuracy')\n    plt.plot(np.arange(0, epoch), his.history['val_dense_6_accuracy'], label='val_vowel_accuracy')\n    plt.plot(np.arange(0, epoch), his.history['val_dense_7_accuracy'], label='val_consonant_accuracy')\n    plt.title(title)\n    plt.xlabel('Epoch #')\n    plt.ylabel('Accuracy')\n    plt.legend(loc='upper right')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for dataset in range(4):\n#     for i in range(nets):\n#         plot_loss(histories[dataset][i], epochs, f'Training Dataset: {dataset},'+f\"model_{i}\")\n#         plot_acc(histories[dataset][i], epochs, f'Training Dataset: {dataset},'+f\"model_{i}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del histories\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# preds_dict = {\n#     'grapheme_root': [],\n#     'vowel_diacritic': [],\n#     'consonant_diacritic': []\n# }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_dict = {\n    'grapheme_root': [],\n    'vowel_diacritic': [],\n    'consonant_diacritic': []\n}\ncomponents = ['consonant_diacritic', 'grapheme_root', 'vowel_diacritic']\ntarget=[] # model predictions placeholder\nrow_id=[] # row_id place holder\n\n#nets: 모델개수 models list에 각 모델 넣어주기\n# nets = 2\nmodels = [0] * nets\nmodels[0] = model_1\nmodels[1] = model_2\n# models[2] = model_3\n\n\nfor i in range(4):\n    df_test_img = pd.read_parquet('/kaggle/input/bengaliai-cv19/test_image_data_{}.parquet'.format(i)) \n    df_test_img.set_index('image_id', inplace=True)\n\n    X_test = resize(df_test_img, need_progress_bar=False)/255\n    X_test = X_test.values.reshape(-1, IMG_SIZE, IMG_SIZE, N_CHANNELS)\n\n    results_roots = np.zeros((X_test.shape[0], 168))\n    results_vowels = np.zeros((X_test.shape[0], 11))\n    results_consonants = np.zeros((X_test.shape[0], 7))\n    results_roots = results_roots.astype('float')\n    results_vowels = results_vowels.astype('float')\n    results_consonants = results_consonants.astype('float')\n\n    for i in range(nets):\n        results_roots += models[i].predict(X_test)[0]\n        results_vowels += models[i].predict(X_test)[1]\n        results_consonants += models[i].predict(X_test)[2]\n      #preds[0] = results_roots\n      #preds[1] = results_vowels\n      #preds[2] = results_consonants\n\n    results_roots = np.argmax(results_roots, axis = 1)\n    results_vowels = np.argmax(results_vowels, axis = 1)\n    results_consonants = np.argmax(results_consonants, axis = 1)\n\n    preds_dict['grapheme_root'] = results_roots\n    preds_dict['vowel_diacritic'] = results_vowels\n    preds_dict['consonant_diacritic'] = results_consonants\n\n    for k,id in enumerate(df_test_img.index.values):\n        for i,comp in enumerate(components):\n            id_sample=id+'_'+comp\n            row_id.append(id_sample)\n            target.append(preds_dict[comp][k])\n    \n    \n    del df_test_img\n    del X_test\n\n    gc.collect()\n\ndf_sample = pd.DataFrame(\n    {\n        'row_id': row_id,\n        'target':target\n    },\n    columns = ['row_id','target'] \n)\ndf_sample.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}