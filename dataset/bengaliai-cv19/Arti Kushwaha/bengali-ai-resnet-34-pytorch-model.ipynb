{"cells":[{"metadata":{},"cell_type":"markdown","source":"Please upvote kernel if you like it..\n## Data Exploration and Visualization- \nPlease click this [kernel](https://www.kaggle.com/artikwh/exploring-data-and-getting-images) for data exploration and getting images from data.\n"},{"metadata":{},"cell_type":"markdown","source":"## Load Packages-"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport torch, torchvision\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm.notebook import tqdm\nfrom torchvision import transforms,models\nimport gc\n\n# Ignore warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Adding all four parquet files and get full data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 = pd.read_parquet(\"/kaggle/input/bengaliai-cv19/train_image_data_0.parquet\")\ndf2 = pd.read_parquet(\"/kaggle/input/bengaliai-cv19/train_image_data_1.parquet\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df3 = pd.read_parquet(\"/kaggle/input/bengaliai-cv19/train_image_data_2.parquet\")\ndf4 = pd.read_parquet(\"/kaggle/input/bengaliai-cv19/train_image_data_3.parquet\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_data = pd.concat([df1, df2, df3, df4], ignore_index = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_csv = pd.read_csv(\"/kaggle/input/bengaliai-cv19/train.csv\")\ndel df1, df2, df3, df4\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from PIL import Image\nimport matplotlib.pyplot as plt\nimg_0 = full_data.iloc[0, 1:].values.reshape(137,236)\nimg = Image.fromarray(img_0.astype('uint8'), 'L')\nplt.figure(figsize=(4,4))\nplt.axis('off')\nplt.imshow(img)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dataset Class-"},{"metadata":{"trusted":true},"cell_type":"code","source":"class GraphemeDataset(Dataset):\n    def __init__(self, csv_file, df, transform = None, train = True):\n        self.label = csv_file\n        self.df = df\n        self.transform = transform\n        self.train = train\n        \n    def __len__(self):\n        return len(self.label)\n    \n    def __getitem__(self, indx):\n        img = self.df.iloc[indx,1:].values.reshape(137,236)\n        img = Image.fromarray(img.astype('uint8'), 'L')\n        \n        if self.transform:\n            img = self.transform(img)\n            \n        if self.train == True:\n            label1 = self.label.iloc[indx, 1]\n            label2 = self.label.iloc[indx, 2]\n            label3 = self.label.iloc[indx, 3]\n            return img, label1, label2, label3\n        else:\n            img_id = self.df.iloc[indx, 0]\n            return img, img_id \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transform = transforms.Compose([transforms.Resize((224, 224)),\n                                transforms.Grayscale(3),\n                                transforms.ToTensor(), \n                                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = GraphemeDataset(train_csv, full_data, transform = transform, train = True)\ntrain_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for data in train_loader: \n    img, label1, label2, label3 = data\n    print(img[0].shape)\n    print(label1[0].shape)\n    img = np. transpose(img[0], (1,2,0))\n    img= np.squeeze(img)\n    plt.imshow(img)\n    break","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ResNet34 Model-"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_gr = torchvision.models.resnet34()\nmodel_vd = torchvision.models.resnet34()\nmodel_cd = torchvision.models.resnet34()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_gr.fc = nn.Linear(512, 168)\nmodel_vd.fc = nn.Linear(512, 11)\nmodel_cd.fc = nn.Linear(512, 7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device= torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_gr.to(device)\nmodel_vd.to(device)\nmodel_cd.to(device)\noptimizer_gr = torch.optim.Adam (model_gr.parameters(), lr = 0.01)\noptimizer_vd = torch.optim.Adam (model_vd.parameters(), lr = 0.01)\noptimizer_cd = torch.optim.Adam (model_cd.parameters(), lr = 0.01)\nCriterion_gr = nn.CrossEntropyLoss()\nCriterion_vd = nn.CrossEntropyLoss()\nCriterion_cd = nn.CrossEntropyLoss()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 20\nmodel_gr.train()\nmodel_vd.train()\nmodel_cd.train()\nlosses = []\naccuracy = []\nfor epoch in tqdm(range(epochs)):\n    print(\"epochs {}/{}\".format(epoch+1, epochs))\n    acc = 0.0\n    for idx , data in tqdm(enumerate(train_loader)):\n        img, label1, label2, label3 = data\n        img, label1, label2, label3 = img.to(device), label1.to(device), label2.to(device), label3.to(device)\n        optimizer_gr.zero_grad()\n        optimizer_vd.zero_grad()\n        optimizer_cd.zero_grad()\n        output1 = model_gr(img)\n        output2 = model_vd(img)\n        output3 = model_cd(img)\n        loss1= Criterion_gr(output1, label1) \n        loss2= Criterion_vd(output2, label2)\n        loss3= Criterion_cd(output3, label3)\n        total_loss = loss1 + loss2 + loss3\n        acc += (output1.argmax(1) == label1).float().mean()\n        acc += (output2.argmax(1) == label2).float().mean()\n        acc += (output3.argmax(1) == label3).float().mean()\n        loss1.backward()\n        loss2.backward()\n        loss3.backward()\n        optimizer_gr.step()\n        optimizer_vd.step()\n        optimizer_cd.step()\n        del img, label1, label2, label3\n        torch.cuda.empty_cache()\n        \n    losses.append(total_loss)\n    accuracy.append(acc/len(trainloader)*3)\n    print('acc: {:.3f}%'.format(acc/len(train_loader)*3))\n    print('loss:{:.3f}%'.format(total_loss))\ntorch.save(model_gr.state_dict, 'gr_resnet34_20epochs_saved_weights.pth')\ntorch.save(model_vd.state_dict, 'vd_resnet34_20epochs_saved_weights.pth')\ntorch.save(model_cd.state_dict, 'cd_resnet34_20epochs_saved_weights.pth')\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1, 2, figsize = (15,5))\nax[0].plot(losses)\nax[0].set_title(\"Loss\")\nax[1].plot(accuracy)\nax[1].set_title(\"Accuracy\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 = pd.read_parquet(\"/kaggle/input/bengaliai-cv19/test_image_data_0.parquet\")\ndf2 = pd.read_parquet(\"/kaggle/input/bengaliai-cv19/test_image_data_1.parquet\")\ndf3 = pd.read_parquet(\"/kaggle/input/bengaliai-cv19/test_image_data_2.parquet\")\ndf4 = pd.read_parquet(\"/kaggle/input/bengaliai-cv19/test_image_data_3.parquet\")\nfull_test_data = pd.concat([df1, df2, df3, df4], ignore_index = True)\ntest_csv= pd.read_csv(\"/kaggle/input/bengaliai-cv19/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = GraphemeDataset(test_csv, full_test_data, transform = transform, train = False)\ntest_loader = torch.utils.data.DataLoader(test_data, batch_size=4, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_gr.load_state_dict(torch.load(\"gr_resnet34_20epochs_saved_weights.pth\"))\nmodel_vd.load_state_dict(torch.load(\"vd_resnet34_20epochs_saved_weights.pth\"))\nmodel_cd.load_state_dict(torch.load(\"cd_resnet34_20epochs_saved_weights.pth\"))\nmodel_gr.to(device)\nmodel_vd.to(device)\nmodel_cd.to(device)\nmodel_gr.eval()\nmodel_vd.eval()\nmodel_cd.eval()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = []\nfor img, img_id in tqdm(test_loader):\n    img = img.to(device)\n    pred1 = model_gr(img)\n    pred2 = model_vd(img)\n    pred3 = model_cd(img)\n    _, ind1 = torch.max(pred1, 1)\n    _, ind2 = torch.max(pred2, 1)\n    _, ind3 = torch.max(pred3, 1)\n    output1 = ind1.squeeze().cpu().numpy()\n    output2 = ind2.squeeze().cpu().numpy()\n    output3 = ind3.squeeze().cpu().numpy()\n    predictions.append(output1)\n    predictions.append(output2)\n    predictions.append(output3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('/kaggle/input/bengaliai-cv19/sample_submission.csv')\nsubmission['target'] = predictions\nsubmission.to_csv('submission.csv', index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}