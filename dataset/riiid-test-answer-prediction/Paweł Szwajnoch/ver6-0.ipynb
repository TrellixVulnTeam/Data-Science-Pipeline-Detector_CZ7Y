{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport tensorflow as tf\nimport math\n\ntypes = {\n    'row_id': 'int64',\n    'timestamp': 'int64',\n    'user_id': 'int32',\n    'content_id': 'int16',\n    'content_type_id': 'int8',\n    'task_container_id': 'int16',\n    'user_answer': 'int8',\n    'answered_correctly': 'int8',\n    'prior_question_elapsed_time': 'float32', \n    'prior_question_had_explanation': 'boolean'\n}        \n\ndata = pd.read_csv('/kaggle/input/riiid-test-answer-prediction/train.csv', dtype = types, nrows = 110000)\ndata = data.sort_values(by=['user_id'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = pd.read_csv('/kaggle/input/riiid-test-answer-prediction/train.csv', dtype = types)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.DataFrame()\ntest = pd.DataFrame()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"user_ids = mylist = list(set(data['user_id'].values))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for user_id in user_ids:\n    tmp = data[data['user_id'] == user_id]\n    length = tmp.shape[0]\n    train_len = math.floor(0.9 * length)\n    test_len = length - train_len\n    train = pd.concat([train, tmp.head(train_len)])\n    test = pd.concat([test, tmp.tail(test_len)])\ntrain = train.sort_values(by=['row_id'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def minmax(df):\n    return (df-df.min())/(df.max()-df.min())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def normalize_struct(struct):\n    struct['all_users_task_container_id_count'] = minmax(struct['all_users_task_container_id_count'])\n    struct['this_user_task_container_id_count'] = minmax(struct['this_user_task_container_id_count'])\n    struct['all_users_content_id_count'] = minmax(struct['all_users_content_id_count'])\n    struct['this_user_question_count'] = minmax(struct['this_user_question_count'])\n    struct['this_user_lectures_count'] = minmax(struct['this_user_lectures_count'])\n    struct['this_user_lectures_count_task_container_id'] = minmax(struct['this_user_lectures_count_task_container_id'])\n    struct['part'] = minmax(struct['part'])\n    struct['most_difficult_tag_count'] = minmax(struct['most_difficult_tag_count'])\n    struct['prior_question_elapsed_time'] = minmax(struct['prior_question_elapsed_time'])\n    return struct.fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_data(data, history=None, get_ans=True):\n    \n    if history is None:\n        history = data\n    \n    # wykluczenie wykładów\n    history_no_lectures = history[history.content_type_id == 0]\n    \n    # wykluczenie pytań\n    history_only_lectures = history[history.content_type_id == 1]\n    \n    # wykluczenie wykładów\n    no_lectures = data[data.content_type_id == 0] \n\n    # wykluczenie pytań\n    only_lectures = data[data.content_type_id == 1] \n    \n    # rozszerzenie tabeli pytań o skuteczność, ilość pytań, ilość poprawnych\n    questions_types = {\n    'question_id': 'int16',\n    'bundle_id': 'int16',\n    'correct_answer': 'int8',\n    'part': 'int8',\n    'tags': 'string'\n    }\n    questions = pd.read_csv('/kaggle/input/riiid-test-answer-prediction/questions.csv', dtype = questions_types)\n    questions.tags = [tags.split() if type(tags) is str else tags for tags in questions.tags.values]\n    df = history_no_lectures.groupby(['content_id']).agg({'answered_correctly': ['mean', 'count', 'sum']})\n    df = df['answered_correctly']\n    questions = questions.merge(df, left_on = 'question_id', right_on = 'content_id', how = \"left\")\n    questions = questions.fillna({'mean': 0.0, 'count': 0, 'sum': 0})\n    questions = questions.rename(columns={'mean': 'accuracy', 'count': 'count_all', 'sum': 'count_correct'})\n    \n    \n    # manipulacja tagami\n    questions_with_tags = questions[questions.tags.isna() == False]\n    tags = questions_with_tags.tags.values\n\n    tags_set = set([item for elem in tags for item in elem])\n    tags_list = list(tags_set)\n    tags_df = pd.DataFrame()\n\n    for tag in tags_list:\n        df = questions_with_tags[questions_with_tags.tags.apply(lambda l: tag in l)]\n        tmp_df = df.agg({'count_all': ['sum'], 'count_correct': ['sum']})\n        tmp_df['tag'] = tag\n        tmp_df['amount_questions_with_tag'] = len(df)\n        tmp_df = tmp_df.set_index('tag')\n        tags_df = tags_df.append(tmp_df)\n\n    tags_df['accuracy'] = tags_df['count_correct'] / tags_df['count_all']\n    tags_df = tags_df.sort_values(by='accuracy')\n    \n    \n    struct = [] # dane statystyczne o danych wejściowych (pytaniach)\n    struct_correct = [] # informacja czy na pytanie użytkownik odpowiedział poprawnie\n    # ^ długości tablic te same, odpowiadające sobie indeksy\n\n    for index, row in no_lectures.iterrows():\n        # --- prior_question_had_explanation\n        prior_question_had_explanation = int(row['prior_question_had_explanation']) if type(row['prior_question_had_explanation']) is bool else 0\n\n        # --- all_users_task_container_id_accuracy \n        df = history_no_lectures[history_no_lectures['task_container_id'] == row['task_container_id']]\n        df = df.agg({'answered_correctly': ['mean', 'count']})\n        all_users_task_container_id_accuracy = df.values[0][0]\n\n        # --- all_users_task_container_id_count\n        all_users_task_container_id_count = df.values[1][0]\n\n        # --- this_user_task_container_id_accuracy\n        df = history_no_lectures[history_no_lectures['task_container_id'] == row['task_container_id']]\n        df = df[df['user_id'] == row['user_id']]\n        df = df.agg({'answered_correctly': ['mean', 'count']})\n        this_user_task_container_id_accuracy = df.values[0][0]\n\n        # --- this_user_task_container_id_count\n        this_user_task_container_id_count = df.values[1][0]\n\n        # --- all_users_content_id_accuracy\n        df = history_no_lectures[history_no_lectures['content_id'] == row['content_id']]\n        df = df.agg({'answered_correctly': ['mean', 'count']})\n        all_users_content_id_accuracy = df.values[0][0]\n\n        # --- all_users_content_id_count\n        all_users_content_id_count = df.values[1][0]\n\n        # --- this_user_accuracy\n        df = history_no_lectures[history_no_lectures['user_id'] == row['user_id']]\n        df = df.agg({'answered_correctly': ['mean', 'count']})\n        this_user_accuracy = df.values[0][0]\n\n        # --- this_user_question_count\n        this_user_question_count = df.values[1][0]\n\n        # --- this_user_lectures_count\n        df = history_no_lectures[history_no_lectures['user_id'] == row['user_id']]\n        this_user_lectures_count = df.shape[0]\n\n        # --- this_user_lectures_count_task_container_id\n        df = df[df['task_container_id'] == row['task_container_id']]\n        this_user_lectures_count_task_container_id = df.shape[0]\n\n        # --- part\n        this_question = questions[questions['question_id'] == row['content_id']]\n        part = this_question['part'].values[0]\n\n        # --- all_users_part_accuracy\n        parts = questions[questions['part'] == part].agg({'count_all': ['sum'], 'count_correct': ['sum']})\n        parts['accuracy'] = parts['count_correct'] / parts['count_all']\n        all_users_part_accuracy = parts['accuracy'].values[0]\n\n        # --- most_difficult_tag_accuracy\n        this_question_tags = this_question.tags.values[0]\n        this_question_tags = tags_df[tags_df.index.isin(this_question_tags)]\n        most_difficult_tag_accuracy = this_question_tags['accuracy'].values[0]\n\n        # --- most_difficult_tag_count\n        most_difficult_tag_count = this_question_tags['count_all'].values[0]\n\n        # --- prior_question_elapsed_time\n        prior_question_elapsed_time = row['prior_question_elapsed_time']\n\n        struct.append({\n            'prior_question_had_explanation': prior_question_had_explanation,\n            'all_users_task_container_id_accuracy': all_users_task_container_id_accuracy,\n            'all_users_task_container_id_count': all_users_task_container_id_count,\n            'this_user_task_container_id_accuracy': this_user_task_container_id_accuracy,\n            'this_user_task_container_id_count': this_user_task_container_id_count,\n            'all_users_content_id_accuracy': all_users_content_id_accuracy,\n            'all_users_content_id_count': all_users_content_id_count,\n            'this_user_accuracy': this_user_accuracy,\n            'this_user_question_count': this_user_question_count,\n            'this_user_lectures_count': this_user_lectures_count,\n            'this_user_lectures_count_task_container_id': this_user_lectures_count_task_container_id,\n            'part': part,\n            'all_users_part_accuracy': all_users_part_accuracy,\n            'most_difficult_tag_accuracy': most_difficult_tag_accuracy,\n            'most_difficult_tag_count': most_difficult_tag_count,\n            'prior_question_elapsed_time': prior_question_elapsed_time \n        })\n        print(index)\n        if get_ans == True:\n            struct_correct.append(row['answered_correctly'])\n    return struct, struct_correct","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_struct_raw, train_struct_correct_raw = get_data(train)\ntrain_struct = pd.DataFrame(train_struct_raw)\ntrain_struct = train_struct.fillna(0)\ntrain_struct = normalize_struct(train_struct)\n\ntrain_struct_correct = np.array(train_struct_correct_raw)\ntrain_struct['answered_correctly'] = train_struct_correct\ntrain_struct.to_csv('train_struct_final')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_struct_raw, test_struct_correct_raw = get_data(test, train)\ntest_struct = pd.DataFrame(test_struct_raw)\ntest_struct = test_struct.fillna(0)\ntest_struct = normalize_struct(test_struct)\n\ntest_struct_correct = np.array(test_struct_correct_raw)\ntest_struct['answered_correctly'] = test_struct_correct\ntest_struct.to_csv('test_struct')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = tf.keras.Sequential([tf.keras.layers.Dense(16, activation='sigmoid'), tf.keras.layers.Dense(10, activation='sigmoid'), tf.keras.layers.Dense(10, activation='sigmoid'), tf.keras.layers.Dense(1)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='mean_squared_error', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x = pd.DataFrame()\nfor i in range(0, 9):\n    train_x = train_struct.sample(10000)\n    train_model_values_correct = train_x['answered_correctly'].values\n    train_model_values = train_x.drop(columns=['answered_correctly']).values\n    model.fit(train_model_values, train_model_values_correct, epochs=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model.fit(train_struct.values, train_struct_correct, epochs=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_struct","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_model_values_correct = test_struct['answered_correctly'].values\ntest_model_values = test_struct.drop(columns=['answered_correctly']).values\npredictions = model.predict(test_model_values)\npredictions = pd.DataFrame(predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_struct['answered_correctly'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"binary_pred = [1 if i > 0 else 0 for i in predictions[0].values]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pred_norm = minmax(predictions[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# binary_pred_norm = [1 if i > 0.15 else 0 for i in pred_norm.values]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"am_correct = 0\ntp = 0\ntn = 0\nfp = 0\nfn = 0\nfor i in range(0, len(binary_pred)):\n    if binary_pred[i] == test_model_values_correct[i]:\n        if binary_pred[i] == 0:\n            tn = tn + 1\n        else:\n            tp = tp + 1\n    else:\n        if binary_pred[i] == 0:\n            fn = fn + 1\n        else:\n            fp = fp + 1\n            \n               \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spec = tp / (tp + fn)\nsens = tn / (tn + fp)\nprec = tp / (tp + fp)\nnprec = fp / (tp + fp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Spec')\nprint(spec)\nprint('Sens')\nprint(sens)\nprint('Prec')\nprint(prec)\nprint('NPrec')\nprint(nprec)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save('my_model.h5')\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}