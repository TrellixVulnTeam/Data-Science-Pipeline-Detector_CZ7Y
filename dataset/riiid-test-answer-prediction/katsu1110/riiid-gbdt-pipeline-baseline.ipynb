{"cells":[{"metadata":{},"cell_type":"markdown","source":"GBDT seems to be working well in this competition. Why not ensembling them? For ensembling the rank ensemble is used, which typically boosts the AUC score.\n\nThis notebook is heavily based on \n\n- https://www.kaggle.com/lgreig/simple-lgbm-baseline\n- https://www.kaggle.com/jsylas/riiid-lgbm-starter\n\nPlease upvote these notebooks too."},{"metadata":{},"cell_type":"markdown","source":"# Config"},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED = 42\nEARLY_STOP = 40\nVERBOSE = 1000\nSTART_IDX = 80000000\nTEST_SIZE = 0.2\nWEIGHTS = [0.8, 0.1, 0.1] # LGB, XGB, CatB","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocess"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Used most of coding from this kernel \nimport optuna\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostClassifier\nfrom  sklearn.tree import DecisionTreeClassifier\nfrom  sklearn.model_selection import train_test_split\nimport operator\n\n# visualize\nimport matplotlib.pyplot as plt\nimport matplotlib.style as style\nimport seaborn as sns\nfrom matplotlib import pyplot\nfrom matplotlib.ticker import ScalarFormatter\nsns.set_context(\"talk\")\nstyle.use('fivethirtyeight')\n\nimport riiideducation\nimport dask.dataframe as dd\nimport  pandas as pd\nimport numpy as np\nfrom sklearn.metrics import roc_auc_score\n\nenv = riiideducation.make_env()\ntrain= pd.read_csv('/kaggle/input/riiid-test-answer-prediction/train.csv',\n                usecols=[1, 2, 3,4,7,8,9], dtype={'timestamp': 'int64', 'user_id': 'int32' ,'content_id': 'int16','content_type_id': 'int8','answered_correctly':'int8','prior_question_elapsed_time': 'float32','prior_question_had_explanation': 'boolean'}\n              )\ntrain = train[train.content_type_id == False]\n#arrange by timestamp\ntrain = train.sort_values(['timestamp'], ascending=True)\n\ntrain.drop(['timestamp','content_type_id'], axis=1,   inplace=True)\n\nresults_c = train[['content_id','answered_correctly']].groupby(['content_id']).agg(['mean'])\nresults_c.columns = [\"answered_correctly_content\"]\n\nresults_u = train[['user_id','answered_correctly']].groupby(['user_id']).agg(['mean', 'sum'])\nresults_u.columns = [\"answered_correctly_user\", 'sum']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#reading in question df\nquestions_df = pd.read_csv('/kaggle/input/riiid-test-answer-prediction/questions.csv',\n                            usecols=[0,1, 3,4],\n                            dtype={'question_id': 'int16',\n                              'part': 'int8','bundle_id': 'int8','tags': 'str'}\n                          )\ntag = questions_df[\"tags\"].str.split(\" \", n = 10, expand = True) \ntag.columns = ['tags1','tags2','tags3','tags4','tags5','tags6']\n\nquestions_df =  pd.concat([questions_df,tag],axis=1)\nquestions_df['tags1'] = pd.to_numeric(questions_df['tags1'], errors='coerce')\nquestions_df['tags2'] = pd.to_numeric(questions_df['tags2'], errors='coerce')\nquestions_df['tags3'] = pd.to_numeric(questions_df['tags3'], errors='coerce')\nquestions_df['tags4'] = pd.to_numeric(questions_df['tags4'], errors='coerce')\nquestions_df['tags5'] = pd.to_numeric(questions_df['tags5'], errors='coerce')\nquestions_df['tags6'] = pd.to_numeric(questions_df['tags6'], errors='coerce')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"X = train.iloc[START_IDX:,:]\nX['prior_question_had_explanation'].fillna(False, inplace=True)\nX = pd.merge(X, results_u, on=['user_id'], how=\"left\")\nX = pd.merge(X, results_c, on=['content_id'], how=\"left\")\nX = pd.merge(X, questions_df, left_on = 'content_id', right_on = 'question_id', how = 'left')\n\nX=X[X.answered_correctly!= -1 ]\nX=X.sort_values(['user_id'])\nY = X[[\"answered_correctly\"]]\nX = X.drop([\"answered_correctly\"], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Training data shrinked from {:,} records ({:,} users) to {:,} records ({:,} users).'.format(train.shape[0], train['user_id'].nunique(), X.shape[0], X['user_id'].nunique()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train test split"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nlb_make = LabelEncoder()\nX[\"prior_question_had_explanation_enc\"] = lb_make.fit_transform(X[\"prior_question_had_explanation\"])\nX.head()\n\nX = X[['answered_correctly_user', 'answered_correctly_content', 'sum',\n       'bundle_id','part','prior_question_elapsed_time','prior_question_had_explanation_enc',\n       'tags1','tags2','tags3']]\nX.fillna(0.5,  inplace=True)\n\nXt, Xv, Yt, Yv = train_test_split(X, Y, test_size = TEST_SIZE, shuffle=False, random_state=SEED)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Models"},{"metadata":{"trusted":true},"cell_type":"code","source":"# =============================\n# LGB \n# =============================\nlgb_params = {\n    'n_estimators': 24000,\n    'objective': 'binary',\n    'boosting_type': 'gbdt',\n    'metric': 'auc', \n    'max_depth': 7,\n    'learning_rate': 0.08,\n    'subsample': 0.72,\n    'subsample_freq': 4,\n    'feature_fraction': 0.7,\n    'lambda_l1': 1,\n    'lambda_l2': 1,\n    'seed': SEED,\n    'early_stopping_rounds': EARLY_STOP\n}\n\ndef fit_lgb(params, Xt, Yt, Xv, Yv):\n    # prepare datasets\n    lgb_train = lgb.Dataset(Xt, Yt)\n    lgb_eval = lgb.Dataset(Xv, Yv)\n    \n    # fit\n    model = lgb.train(params, lgb_train, valid_sets=[lgb_train, lgb_eval], verbose_eval=VERBOSE)\n    \n    # predict\n    val_pred = model.predict(Xv)\n    \n    # CV score\n    score = roc_auc_score(Yv, val_pred)\n    print(f\"AUC = {score}\")\n    \n    # feature importance\n    fi = pd.DataFrame()\n    fi['features'] = Xt.columns.values.tolist()\n    fi['importance'] = model.feature_importance(importance_type=\"gain\")\n    \n    return model, fi","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# =============================\n# XGB \n# =============================\nxgb_params = {\n    'colsample_bytree': 0.7,\n    'objective': 'binary:logistic',\n    'eval_metric': 'auc',\n    'learning_rate': 0.08,\n    'max_depth': 7,\n    'subsample': 1,\n    'min_child_weight': 4,\n    'gamma': 0.24,\n    'alpha': 0,\n    'lambda': 1,\n    'seed': SEED,\n    'n_estimators': 24000\n}\n            \ndef fit_xgb(params, Xt, Yt, Xv, Yv):\n    # model\n    model = xgb.XGBClassifier(**params)\n    model.fit(Xt, Yt, eval_set=[(Xv, Yv)], early_stopping_rounds=EARLY_STOP, verbose=VERBOSE)\n    \n    # predict\n    val_pred = model.predict(Xv)\n    \n    # CV score\n    score = roc_auc_score(Yv, val_pred)\n    print(f\"AUC = {score}\")\n    \n    # feature importance\n    fi = pd.DataFrame()\n    fi['features'] = Xt.columns.values.tolist()\n    fi['importance'] = 0\n    importance = model.get_booster().get_score(importance_type='gain')\n    importance = sorted(importance.items(), key=operator.itemgetter(1))\n    df = pd.DataFrame(importance, columns=['feature', 'fscore'])\n    df['fscore'] = df['fscore'] / df['fscore'].sum()\n    for i, f in enumerate(Xt.columns.values.tolist()):\n        try:\n            fi.loc[fi['features'] == f, 'importance'] = df.loc[df['feature'] == f, \"fscore\"].values[0]\n        except: # ignored by XGB\n            continue\n    \n    return model, fi","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# =============================\n# catb \n# =============================\ncatb_params = { \n    'task_type': \"CPU\",\n    'learning_rate': 0.08, \n    'iterations': 24000,\n    'colsample_bylevel': 0.7,\n    'random_seed': SEED,\n    'loss_function': 'Logloss',\n    'eval_metric': 'AUC',\n    'use_best_model': True,\n    'early_stopping_rounds': EARLY_STOP\n}\n            \ndef fit_catb(params, Xt, Yt, Xv, Yv):\n    # model\n    model = CatBoostClassifier(**params)\n    model.fit(Xt, Yt, eval_set=(Xv, Yv), verbose=VERBOSE)\n    \n    # feature importance\n    fi = pd.DataFrame()\n    fi['features'] = Xt.columns.values.tolist()\n    fi['importance'] = model.get_feature_importance()\n    \n    # predict\n    val_pred = model.predict(Xv)\n    \n    # CV score\n    score = roc_auc_score(Yv, val_pred)\n    print(f\"AUC = {score}\")\n    \n    return model, fi","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model fitting"},{"metadata":{},"cell_type":"markdown","source":"## LGB"},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_model, fi = fit_lgb(lgb_params, Xt, Yt, Xv, Yv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x='importance', y='features', data=fi.sort_values(by='importance', ascending=False))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## XGB"},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_model, fi = fit_xgb(xgb_params, Xt, Yt, Xv, Yv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x='importance', y='features', data=fi.sort_values(by='importance', ascending=False))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CatBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"catb_model, fi = fit_catb(catb_params, Xt, Yt, Xv, Yv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x='importance', y='features', data=fi.sort_values(by='importance', ascending=False))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"OK, how come does LGB outperform???"},{"metadata":{},"cell_type":"markdown","source":"# Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"# rank ensemble for better AUC score\npreds_df = pd.DataFrame()\npreds_df['xgb'] = xgb_model.predict(Xv)\npreds_df['lgb'] = lgb_model.predict(Xv)\npreds_df['catb'] = catb_model.predict(Xv)\n\ny_true = np.array(Yv)\nval_pred = np.zeros(len(y_true))\nfor i, f in enumerate(preds_df.columns.values.tolist()):\n    preds_df[f] = preds_df[f].rank(pct=True)\n    val_pred += WEIGHTS[i] * preds_df[f].values\n\nprint('Validation score (AUC) = {}'.format(roc_auc_score(y_true, val_pred)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test =  pd.read_csv('/kaggle/input/riiid-test-answer-prediction/example_test.csv')\ntest[\"prior_question_had_explanation_enc\"] = lb_make.fit_transform(test[\"prior_question_had_explanation\"])\ntest = pd.merge(test, results_u, on=['user_id'],  how=\"left\")\ntest = pd.merge(test, results_c, on=['content_id'],  how=\"left\")\ntest = pd.merge(test, questions_df, left_on = 'content_id', right_on = 'question_id', how = 'left')\ntest.fillna(0.5, inplace=True)\n\n# rank ensemble for better AUC score\ntest_ = test[['answered_correctly_user', 'answered_correctly_content', 'sum',\n       'bundle_id','part','prior_question_elapsed_time','prior_question_had_explanation_enc',\n       'tags1','tags2','tags3']]\npreds_df = pd.DataFrame()\npreds_df['xgb'] = xgb_model.predict(test_)\npreds_df['lgb'] = lgb_model.predict(test_)\npreds_df['catb'] = catb_model.predict(test_)\n\ny_pred = np.zeros(test_.shape[0])\nfor i, f in enumerate(preds_df.columns.values.tolist()):\n    preds_df[f] = preds_df[f].rank(pct=True)\n    y_pred += WEIGHTS[i] * preds_df[f].values\n\ntest['answered_correctly'] = pd.Series(y_pred).rank(pct=True)\n\nresults_c = train[['content_id','answered_correctly']].groupby(['content_id']).agg(['mean'])\nresults_c.columns = [\"answered_correctly_content\"]\n\nresults_u = train[['user_id','answered_correctly']].groupby(['user_id']).agg(['mean', 'sum'])\nresults_u.columns = [\"answered_correctly_user\", 'sum']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"iter_test = env.iter_test()\nfor (test_df, sample_prediction_df) in iter_test:\n    test_df = pd.merge(test_df, results_u, on=['user_id'],  how=\"left\")\n    test_df = pd.merge(test_df, results_c, on=['content_id'],  how=\"left\")\n    test_df = pd.merge(test_df, questions_df, left_on = 'content_id', right_on = 'question_id', how = 'left')\n    test_df['answered_correctly_user'].fillna(0.5, inplace=True)\n    test_df['answered_correctly_content'].fillna(0.5, inplace=True)\n    test_df['sum'].fillna(0, inplace=True)\n    test_df['prior_question_had_explanation'].fillna(False, inplace=True)\n    test_df[\"prior_question_had_explanation_enc\"] = lb_make.fit_transform(test_df[\"prior_question_had_explanation\"])\n    \n    test_ = test_df[['answered_correctly_user', 'answered_correctly_content', 'sum',\n       'bundle_id','part','prior_question_elapsed_time','prior_question_had_explanation_enc',\n       'tags1','tags2','tags3']]\n    preds_df = pd.DataFrame()\n    preds_df['xgb'] = xgb_model.predict(test_)\n    preds_df['lgb'] = lgb_model.predict(test_)\n    preds_df['catb'] = catb_model.predict(test_)\n    \n    y_pred = np.zeros(test_.shape[0])\n    for i, f in enumerate(preds_df.columns.values.tolist()):\n        preds_df[f] = preds_df[f].rank(pct=True)\n        y_pred += WEIGHTS[i] * preds_df[f].values\n\n    test_df['answered_correctly'] = pd.Series(y_pred).rank(pct=True)\n    env.predict(test_df.loc[test_df['content_type_id'] == 0, ['row_id', 'answered_correctly']])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}