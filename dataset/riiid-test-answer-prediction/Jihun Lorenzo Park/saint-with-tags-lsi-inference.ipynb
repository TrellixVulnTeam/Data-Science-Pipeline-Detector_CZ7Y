{"cells":[{"metadata":{},"cell_type":"markdown","source":"Inference notebook of [this kernel](https://www.kaggle.com/jihunlorenzopark/saint-with-tags-lsi)."},{"metadata":{"execution":{"iopub.execute_input":"2021-01-05T08:34:24.121663Z","iopub.status.busy":"2021-01-05T08:34:24.120913Z","iopub.status.idle":"2021-01-05T08:34:24.128452Z","shell.execute_reply":"2021-01-05T08:34:24.127852Z"},"papermill":{"duration":0.026642,"end_time":"2021-01-05T08:34:24.128571","exception":false,"start_time":"2021-01-05T08:34:24.101929","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Flags for debug\nTEST_MODE = False\nMOCK_MODE = False\nSKIP_ADD_FEATURE = False\n\nLOCAL = False\n\nSAINT_PICKLE_PATH = \"../input/saint-final\"\nSAINT_MODEL_PATH = \"../input/saint-final/saintv113.pth\"","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-01-05T08:34:24.166758Z","iopub.status.busy":"2021-01-05T08:34:24.166073Z","iopub.status.idle":"2021-01-05T08:34:24.167981Z","shell.execute_reply":"2021-01-05T08:34:24.168616Z"},"papermill":{"duration":0.024047,"end_time":"2021-01-05T08:34:24.168756","exception":false,"start_time":"2021-01-05T08:34:24.144709","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"if MOCK_MODE:\n    import pandas as pd\n    from pathlib import Path\n    import sqlite3\n    import riiideducation\n    from sklearn.metrics import roc_auc_score\n    from tqdm.notebook import tqdm","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2021-01-05T08:34:24.203941Z","iopub.status.busy":"2021-01-05T08:34:24.203264Z","iopub.status.idle":"2021-01-05T08:34:26.63188Z","shell.execute_reply":"2021-01-05T08:34:26.632407Z"},"papermill":{"duration":2.447741,"end_time":"2021-01-05T08:34:26.632567","exception":false,"start_time":"2021-01-05T08:34:24.184826","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"import psutil\nimport joblib\nimport pandas as pd\nimport numpy as np\nimport gc\nfrom sklearn.metrics import roc_auc_score\nfrom collections import defaultdict\nfrom tqdm import tqdm\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport pickle\nimport random\nimport os\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\nif LOCAL is False:\n    import riiideducation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_group():\n    group = None\n    for i in range(10):\n        with open(f\"{SAINT_PICKLE_PATH}/{i}groupv1.pickle\", \"rb\") as f:\n            if group is None:\n                group = pickle.load(f)\n            else:\n                group = pd.concat([group, pickle.load(f)])\n        gc.collect()\n    gc.collect()\n    return group","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"group = load_group()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-01-05T08:34:26.670063Z","iopub.status.busy":"2021-01-05T08:34:26.669404Z","iopub.status.idle":"2021-01-05T08:34:26.674809Z","shell.execute_reply":"2021-01-05T08:34:26.675319Z"},"papermill":{"duration":0.025519,"end_time":"2021-01-05T08:34:26.675481","exception":false,"start_time":"2021-01-05T08:34:26.649962","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Random seed\nSEED = 123\n\n# Function to seed everything\ndef seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \nseed_everything(SEED)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.027869,"end_time":"2021-01-05T08:36:07.103978","exception":false,"start_time":"2021-01-05T08:36:07.076109","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# SAINT Model"},{"metadata":{"execution":{"iopub.execute_input":"2021-01-05T08:36:07.184659Z","iopub.status.busy":"2021-01-05T08:36:07.179746Z","iopub.status.idle":"2021-01-05T08:36:07.199617Z","shell.execute_reply":"2021-01-05T08:36:07.198941Z"},"papermill":{"duration":0.0675,"end_time":"2021-01-05T08:36:07.19974","exception":false,"start_time":"2021-01-05T08:36:07.13224","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"MAX_SEQ = 100\nn_skill = 13523\nn_part = 7\nn_et = 300\nn_lt = 1441\nn_lsi = 128\nDROPOUT = 0.1\nEMBED_SIZE = 256\nBATCH_SIZE = 256\n\ndef future_mask(seq_length):\n    future_mask = np.triu(np.ones((seq_length, seq_length)), k=1).astype('bool')\n    return torch.from_numpy(future_mask)\n\nclass FFN(nn.Module):\n    def __init__(self, state_size = 200, forward_expansion = 1, bn_size=MAX_SEQ - 1):\n        super(FFN, self).__init__()\n        self.state_size = state_size\n        \n        self.lr1 = nn.Linear(state_size, forward_expansion * state_size)\n        self.relu = nn.ReLU()\n        self.lr2 = nn.Linear(forward_expansion * state_size, state_size)\n        self.dropout = nn.Dropout(0.2)\n        \n    def forward(self, x):\n        x = self.relu(self.lr1(x))\n        x = self.lr2(x)\n        return self.dropout(x)\n\nclass Encoder(nn.Module):\n    def __init__(self, n_skill, n_pt=7, n_lsi=n_lsi, max_seq=100, embed_dim=128, dropout = DROPOUT, forward_expansion = 1, num_layers=1, heads = 8):\n        super(Encoder, self).__init__()\n        self.n_skill, self.embed_dim = n_skill, embed_dim\n        self.embedding = nn.Embedding(n_skill + 1, embed_dim)\n        self.pos_embedding = nn.Embedding(max_seq, embed_dim)\n        self.dropout = nn.Dropout(dropout)\n        \n        self.n_pt = n_pt\n        self.pt_embedding = nn.Embedding(n_pt + 1, embed_dim)\n        self.n_lsi = n_lsi\n        self.lsi_embedding = nn.Embedding(n_lsi + 1, embed_dim)\n        self.layer_normal = nn.LayerNorm(embed_dim) \n        \n    def forward(self, x, question_ids, pt_x, lsi_x):\n        device = x.device\n        x = self.embedding(x)\n        pt_x = self.pt_embedding(pt_x)\n        lsi_x = self.lsi_embedding(lsi_x)\n        pos_id = torch.arange(x.size(1)).unsqueeze(0).to(device)\n        pos_x = self.pos_embedding(pos_id)\n        x = self.dropout(self.layer_normal(x + pos_x + pt_x + lsi_x))\n        return x\n\n    \nclass Decoder(nn.Module):\n    def __init__(self, n_et=n_et, n_lt=n_lt, max_seq=100, embed_dim=128, dropout = DROPOUT, forward_expansion = 1, num_layers=1, heads = 8):\n        super(Decoder, self).__init__()\n        self.embed_dim = embed_dim\n        self.pos_embedding = nn.Embedding(max_seq, embed_dim)\n        self.dropout = nn.Dropout(dropout)\n        \n        self.n_response = 2\n        self.n_et = n_et\n        self.n_lt = n_lt\n        self.response_embedding = nn.Embedding(self.n_response + 2, embed_dim)\n        self.et_embedding = nn.Embedding(self.n_et + 2, embed_dim)\n        self.lt_embedding = nn.Embedding(self.n_lt + 2, embed_dim)\n        self.layer_normal = nn.LayerNorm(embed_dim) \n        \n    def forward(self, c, et, lt):\n        device = c.device\n        c = self.response_embedding(c)\n        pos_id = torch.arange(c.size(1)).unsqueeze(0).to(device)\n        pos_x = self.pos_embedding(pos_id)\n        et = self.et_embedding(et)\n        lt = self.lt_embedding(lt)\n        x = self.dropout(self.layer_normal(c + pos_x + et + lt))\n        return x\n    \nclass SAINTModel(nn.Module):\n    def __init__(self, n_skill, n_pt=7, n_lsi=n_lsi, n_et=n_et, n_lt=n_lt, max_seq=100, embed_dim=128, dropout = DROPOUT, forward_expansion = 1, enc_layers=3, dec_layers=3, heads = 8):\n        super(SAINTModel, self).__init__()\n        self.encoder = Encoder(n_skill, n_pt, n_lsi, max_seq, embed_dim, dropout, forward_expansion, num_layers=enc_layers)\n        self.decoder = Decoder(n_et, n_lt, max_seq, embed_dim, dropout, forward_expansion, num_layers=dec_layers)\n        self.transformer = torch.nn.Transformer(embed_dim, heads, enc_layers, dec_layers, embed_dim*forward_expansion, dropout)\n        \n        self.ffn = FFN(embed_dim, forward_expansion = forward_expansion)\n        self.pred = nn.Linear(embed_dim, 1)\n        self.layer_normal = nn.LayerNorm(embed_dim) \n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x, question_ids, pt_x, lsi_x, c, et, lt):\n        ex = self.encoder(x, question_ids, pt_x, lsi_x)\n        dx = self.decoder(c, et, lt)\n        \n        ex = ex.permute(1, 0, 2)\n        dx = dx.permute(1, 0, 2)\n        \n        device = ex.device\n        mask = future_mask(ex.size(0)).to(device)\n        att_output = self.transformer(ex, dx, src_mask=mask, tgt_mask=mask)\n        att_output = self.layer_normal(att_output)\n        att_output = att_output.permute(1, 0, 2) # att_output: [s_len, bs, embed] => [bs, s_len, embed]\n\n        x = self.ffn(att_output)\n        x = self.dropout(self.layer_normal(x + att_output))\n        x = self.pred(x)\n        \n        return x.squeeze(-1)\n    \nclass TestDataset(Dataset):\n    def __init__(self, samples, test_df, n_skill=n_skill, n_et=n_et, n_lt=n_lt, n_pt=7,  max_seq=MAX_SEQ): \n        super(TestDataset, self).__init__()\n        self.samples = samples\n        self.user_ids = [x for x in test_df[\"user_id\"].unique()]\n        self.test_df = test_df\n        self.n_skill = n_skill\n        self.n_et = n_et\n        self.n_lt = n_lt\n        self.n_pt = n_pt\n        self.max_seq = max_seq\n\n    def __len__(self):\n        return self.test_df.shape[0]\n\n    def __getitem__(self, index):\n        test_info = self.test_df.iloc[index]\n\n        user_id = test_info[\"user_id\"]\n        target_id_new = test_info[\"content_id\"]\n        part_new = test_info[\"part\"]\n        lsi_topic_new = test_info[\"lsi_topic\"]\n\n        content_id_seq = np.zeros(self.max_seq, dtype=int)\n        answered_correctly_seq = np.zeros(self.max_seq, dtype=int)\n        c_seq = np.zeros(self.max_seq, dtype=int)\n        lag_seq = np.zeros(self.max_seq, dtype=int)\n        elapsed_time_seq = np.zeros(self.max_seq, dtype=int)\n        part_seq = np.zeros(self.max_seq, dtype=int)\n        lsi_topic_seq = np.zeros(self.max_seq, dtype=int)\n\n        if user_id in self.samples.index:\n            content_id, answered_correctly, lag, elapsed_time, part, lsi_topic = self.samples[user_id]\n#             pd.DataFrame({\n#                 \"content_id\": content_id,\n#                 \"answered_correctly\": answered_correctly,\n#                 \"lag\": lag,\n#                 \"elapsed_time\": elapsed_time,\n#                 \"part\": part,\n#                 \"lsi_topic\": lsi_topic,\n#             }).to_pickle(\"a.pkl\")\n            seq_len = len(content_id)\n\n            if seq_len >= self.max_seq:\n                content_id_seq[:] = content_id[-self.max_seq:]\n                answered_correctly_seq[:] = answered_correctly[-self.max_seq:]\n                c_seq[:] = answered_correctly[-self.max_seq:] + 1\n                lag_seq[:] = lag[-self.max_seq:] + 1\n                elapsed_time_seq[:] =  elapsed_time[-self.max_seq:] + 1\n                part_seq[:] = part[-self.max_seq:]\n                lsi_topic_seq[:] = lsi_topic[-self.max_seq:]\n            else:\n                content_id_seq[-seq_len:] = content_id\n                c_seq[-seq_len:] = answered_correctly[:] + 1\n                lag_seq[-seq_len:] = lag[:] + 1\n                elapsed_time_seq[-seq_len:] = elapsed_time[:] + 1\n                part_seq[-seq_len:] = part\n                lsi_topic_seq[-seq_len:] = lsi_topic    \n        \n        target_id = content_id_seq\n        label = answered_correctly_seq\n        \n        x = np.append(content_id_seq.copy()[1:], target_id_new)\n        pt_x = np.append(part_seq.copy()[1:], part_new)\n        lsi_x = np.append(lsi_topic_seq.copy()[1:], lsi_topic_new)\n        c = c_seq.copy()\n        et = elapsed_time_seq.copy()\n        lt = lag_seq.copy()\n        \n#         pd.DataFrame({\n#                 \"x\": x,\n#                 \"pt_x\": pt_x,\n#                 \"lsi_x\": lsi_x,\n#                 \"c\": c,\n#                 \"et\": et,\n#                 \"lt\": lt,\n#             }).to_pickle(\"b.pkl\")\n        \n        return x, target_id, pt_x, lsi_x, c, et, lt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Main changes are possibility of forward expansion and stacking of encoding layers\ndef create_model():\n    return SAINTModel(n_skill, n_pt=7, n_lsi=n_lsi, n_et=n_et, n_lt=n_lt, max_seq=MAX_SEQ, embed_dim=EMBED_SIZE, forward_expansion=1, enc_layers=2, dec_layers=2, heads=8, dropout=0.1)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.028374,"end_time":"2021-01-05T08:36:07.25619","exception":false,"start_time":"2021-01-05T08:36:07.227816","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# Load SAINT Model"},{"metadata":{"execution":{"iopub.execute_input":"2021-01-05T08:36:07.322585Z","iopub.status.busy":"2021-01-05T08:36:07.321894Z","iopub.status.idle":"2021-01-05T08:36:07.325071Z","shell.execute_reply":"2021-01-05T08:36:07.324572Z"},"papermill":{"duration":0.040713,"end_time":"2021-01-05T08:36:07.325185","exception":false,"start_time":"2021-01-05T08:36:07.284472","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nsaint_model = create_model()\ntry:\n    saint_model.load_state_dict(torch.load(SAINT_MODEL_PATH))\nexcept:\n    saint_model.load_state_dict(torch.load(SAINT_MODEL_PATH, map_location='cpu'))\nsaint_model.to(device)\nsaint_model.eval()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-01-05T08:36:07.405769Z","iopub.status.busy":"2021-01-05T08:36:07.404657Z","iopub.status.idle":"2021-01-05T08:36:07.408204Z","shell.execute_reply":"2021-01-05T08:36:07.407651Z"},"papermill":{"duration":0.054452,"end_time":"2021-01-05T08:36:07.408328","exception":false,"start_time":"2021-01-05T08:36:07.353876","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"class PredictEnv:\n    def __init__(self, folds_path, folds):\n        self.conn = sqlite3.connect(':memory:')\n        self.c = self.conn.cursor()\n        self.setup_folds(folds_path, folds)\n\n    def setup_folds(self, folds_path, folds):        \n        self.c.executescript(f\"\"\"\n            ATTACH DATABASE \"{folds_path}\" AS folds_db;\n\n            DROP TABLE IF EXISTS b_records;\n\n            CREATE TABLE b_records AS\n            SELECT row_id, timestamp, user_id, content_id, content_type_id, task_container_id, prior_question_elapsed_time,\n                prior_question_had_explanation, answered_correctly, user_answer\n            FROM folds_db.train\n            WHERE fold in ({(', ').join(list(map(str,folds)))})\n            ORDER BY user_id, task_container_id, row_id;\n\n            CREATE INDEX user_id_task_container_id_index ON b_records (user_id, task_container_id);\n\n            DROP TABLE IF EXISTS b_users;\n\n            CREATE TABLE b_users AS\n            SELECT user_id, MIN(task_container_id) - 1 task_container_id_next, MAX(task_container_id) task_container_id_max\n            FROM b_records\n            GROUP BY user_id\n                ORDER BY user_id, task_container_id_next;\n\n            CREATE UNIQUE INDEX user_id_index ON b_users (user_id);\n\n            ALTER TABLE b_users\n                ADD COLUMN group_num INTEGER;\n\n        \"\"\").fetchone()\n\n        self.group_num = 0\n        self.records_remaining = self.c.execute('SELECT COUNT(*) FROM b_records').fetchone()[0]\n        self.df_users = pd.read_sql('SELECT * FROM b_users', self.conn)\n\n\n    def iter_test(self):\n        next_correct = '[]'\n        next_responses = '[]'\n\n        while self.records_remaining:\n            self.c.execute(f\"\"\"\n                INSERT INTO b_users (user_id)\n                SELECT user_id\n                FROM b_users\n                WHERE task_container_id_next <= task_container_id_max\n                LIMIT 1 + ABS(RANDOM() % 40) + ABS(RANDOM() % 1000) * (ABS(RANDOM() % 100) < 5)\n                ON CONFLICT (user_id) DO UPDATE SET\n                    task_container_id_next = task_container_id_next + 1,\n                    group_num = {self.group_num};\n            \"\"\").fetchone()\n            \n            self.conn.commit()\n\n            df_b = pd.read_sql(f\"\"\"\n                SELECT r.*\n                FROM b_records r\n                JOIN b_users u\n                ON group_num = {self.group_num}\n                    AND r.user_id = u.user_id\n                    AND r.task_container_id = u.task_container_id_next\n            \"\"\", self.conn)\n\n            if len(df_b):\n                df_b['group_num'] = self.group_num\n                df_b['prior_group_answers_correct'] = None\n                df_b.at[0, 'prior_group_answers_correct'] = next_correct\n\n                df_b['prior_group_responses'] = None\n                df_b.at[0, 'prior_group_responses'] = next_responses\n\n                next_correct = f'[{(\", \").join(df_b.answered_correctly.astype(str))}]'\n                next_responses = f'[{(\", \").join(df_b.user_answer.astype(str))}]'\n                del df_b['answered_correctly']\n                del df_b['user_answer']\n\n                df_b = df_b.set_index('group_num')\n\n                df_p = df_b[['row_id']].copy()\n                df_p['answered_correctly'] = 0.5\n                \n                self.records_remaining -= len(df_b)\n\n                yield df_b, df_p\n            \n            self.group_num += 1\n\n    def predict(self, df_pred):\n        if (df_pred.answered_correctly == -1).any():\n            raise\n        else:\n            df_pred.reset_index().to_sql('predictions', self.conn, if_exists='append', index=False)\n\n    def get_predictions(self):\n        df_preds = pd.read_sql(\"\"\"\n            SELECT p.row_id, b.answered_correctly y_true, p.answered_correctly y_pred\n            FROM predictions p\n            JOIN b_records b\n            ON p.row_id = b.row_id\n        \"\"\", self.conn)\n\n        self.score = roc_auc_score(df_preds.y_true, df_preds.y_pred)\n\n        print(f'ROC AUC Score: {self.score:0.4f}')\n\n        return df_preds","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-01-05T08:36:07.468514Z","iopub.status.busy":"2021-01-05T08:36:07.46784Z","iopub.status.idle":"2021-01-05T08:36:07.583147Z","shell.execute_reply":"2021-01-05T08:36:07.582524Z"},"papermill":{"duration":0.146293,"end_time":"2021-01-05T08:36:07.583256","exception":false,"start_time":"2021-01-05T08:36:07.436963","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"if MOCK_MODE:\n    FOLDS = Path('../input/riiid-folds/riiid.db')\n    env = PredictEnv(FOLDS, [0, 1])\n    iter_test = env.iter_test()\n\nelse:\n    env = riiideducation.make_env()\n    iter_test = env.iter_test()\n    set_predict = env.predict\n\n\nif TEST_MODE and type(iter_test) != list:\n    list_df = []\n    for itr, (df_test, sample_prediction_df) in enumerate(iter_test):\n        df_test.loc[:, 'answered_correctly'] = 0.5\n        list_df.append((df_test, None))\n        env.predict(df_test.loc[df_test['content_type_id'] == 0, ['row_id', 'answered_correctly']])\n    iter_test = list_df\n    print(\"TEST_MODE MODE ENABLED\")\nelse:\n    print(\"TEST_MODE MODE DISABLED\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TARGET = \"answered_correctly\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"QUESTION_FEATURES = [\"part\", \"question_id\", \"lsi_topic\"]\nquestion_file = \"../input/question-features-0102/questions.pickle\"\nquestions_df = pd.read_pickle(question_file)[QUESTION_FEATURES]\nquestions_df[\"lsi_topic\"] = questions_df[\"lsi_topic\"].fillna(-1)\nquestions_df[\"lsi_topic\"] = questions_df[\"lsi_topic\"].map(dict(map(lambda x: (x[1],x[0]), enumerate(questions_df[\"lsi_topic\"].fillna(-1).unique()))))\nquestions_df[\"lsi_topic\"] = questions_df[\"lsi_topic\"] + 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\n\nwarnings.filterwarnings(action=\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-01-05T08:36:07.684995Z","iopub.status.busy":"2021-01-05T08:36:07.649971Z","iopub.status.idle":"2021-01-05T08:36:07.689709Z","shell.execute_reply":"2021-01-05T08:36:07.690332Z"},"papermill":{"duration":0.078378,"end_time":"2021-01-05T08:36:07.690524","exception":false,"start_time":"2021-01-05T08:36:07.612146","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"last_row_file = \"../input/saint-final/last_row_states.pickle\"\nwith open(last_row_file, \"rb\") as f:\n    last_row_states = pickle.load(f)\n    \n# Using time series api that simulates production predictions\ndef inference(iter_test, TARGET, saint_model, questions_df):\n    previous_test_df = None\n    \n    for (test_df, sample_prediction_df) in tqdm(iter_test):\n        if previous_test_df is not None:\n            previous_test_df[TARGET] = eval(test_df[\"prior_group_answers_correct\"].iloc[0])\n            previous_test_df = previous_test_df[previous_test_df.content_type_id == False]\n            previous_test_df[\"user_to_remove\"] = False\n            previous_test_df[\"prior_question_elapsed_time\"] = (previous_test_df[\"prior_question_elapsed_time\"] // 100).clip(0,300)\n            previous_test_df['current_container_size'] = previous_test_df[['user_id', 'task_container_id']].groupby(['user_id', 'task_container_id'])['task_container_id'].transform('size')\n            \n            if TEST_MODE: previous_test_df.to_csv(\"./previous_test_df1.csv\")\n            \n            common_users = set(previous_test_df[\"user_id\"].unique()).intersection(set(last_row_states.keys()))\n            last_records = pd.DataFrame([\n                {**last_row_states[user_id], **{'user_id': user_id}} for user_id in common_users\n            ])\n#             print(previous_test_df.info())\n#             print(last_records)\n            if len(last_records) != 0:\n                previous_test_df = pd.concat([last_records, previous_test_df]).reset_index(drop=True)\n                previous_test_df = previous_test_df.sort_values(['user_id','timestamp'], ascending=True).reset_index(drop = True)\n#                 previous_test_df = previous_test_df.sort_values(['user_id','timestamp'], ascending=True).reset_index(drop = True)\n            \n            previous_test_df['last_timestamp'] = previous_test_df[['user_id', 'timestamp']].groupby(['user_id'])['timestamp'].shift(1, fill_value=0)\n            previous_test_df['last_timestamp'] = previous_test_df[['user_id', 'task_container_id', 'last_timestamp']].groupby(['user_id', 'task_container_id'])['last_timestamp'].transform('first')\n#             print(previous_test_df)\n#             print(previous_test_df)\n            previous_test_df['last_task_container_size'] = previous_test_df[['user_id', 'current_container_size']].groupby(['user_id'])['current_container_size'].shift(1, fill_value=0)\n            previous_test_df['last_task_container_size'] = previous_test_df[['user_id', 'task_container_id', 'last_task_container_size']].groupby(['user_id', 'task_container_id'])['last_task_container_size'].transform('first')\n\n            if TEST_MODE: previous_test_df.to_csv(\"./previous_test_df2.csv\")\n            previous_test_df['lag'] = previous_test_df['timestamp'] - previous_test_df['last_timestamp'] - (previous_test_df['prior_question_elapsed_time'] * previous_test_df['last_task_container_size'])\n            previous_test_df[\"lag\"] = (previous_test_df[\"lag\"]//(100*60)).clip(0, 1440)\n            if TEST_MODE: previous_test_df.to_csv(\"./previous_test_df3.csv\")\n#             print(previous_test_df)\n            if TEST_MODE: previous_test_df.to_csv(\"./previous_test_df4.csv\")\n            previous_test_df = previous_test_df[previous_test_df[\"user_to_remove\"] != True]\n            if TEST_MODE: previous_test_df.to_csv(\"./previous_test_df5.csv\")\n#             print(previous_test_df)\n            # Update SAINT\n            prev_group = previous_test_df[[\n                'user_id', \n                'content_id', \n                'answered_correctly', \n                'lag', \n                'prior_question_elapsed_time', \n                'part', \n                'lsi_topic'\n            ]].groupby('user_id').apply(lambda r: (\n                r['content_id'].values, \n                r['answered_correctly'].values, \n                r['lag'].values, \n                r['prior_question_elapsed_time'].values, \n                r['part'].values, \n                r['lsi_topic'].values))\n            for prev_user_id in prev_group.index:\n                if prev_user_id in group.index:\n                    group[prev_user_id] = (\n                        np.append(group[prev_user_id][0], prev_group[prev_user_id][0])[-MAX_SEQ:], \n                        np.append(group[prev_user_id][1], prev_group[prev_user_id][1])[-MAX_SEQ:],\n                        np.append(group[prev_user_id][2], prev_group[prev_user_id][2])[-MAX_SEQ:], \n                        np.append(group[prev_user_id][3], prev_group[prev_user_id][3])[-MAX_SEQ:], \n                        np.append(group[prev_user_id][4], prev_group[prev_user_id][4])[-MAX_SEQ:],\n                        np.append(group[prev_user_id][5], prev_group[prev_user_id][5])[-MAX_SEQ:],\n                    )\n\n                else:\n                    group[prev_user_id] = (\n                        prev_group[prev_user_id][0], \n                        prev_group[prev_user_id][1],\n                        prev_group[prev_user_id][2], \n                        prev_group[prev_user_id][3],\n                        prev_group[prev_user_id][4],\n                        prev_group[prev_user_id][5],\n                    )\n            users_to_cache = previous_test_df.groupby(\"user_id\").last()\n            user_ids = users_to_cache.index\n            timestamps = users_to_cache[\"timestamp\"].values\n            content_ids = users_to_cache[\"content_id\"].values\n            content_type_ids = users_to_cache[\"content_type_id\"].values\n            task_container_ids = users_to_cache[\"task_container_id\"].values\n            prior_question_elapsed_times = users_to_cache[\"prior_question_elapsed_time\"].values\n            prior_question_had_explanations = users_to_cache[\"prior_question_had_explanation\"].values\n            current_container_sizes = previous_test_df['current_container_size']\n            \n            for row in zip(\n                user_ids,\n                timestamps,\n                content_ids,\n                content_type_ids,\n                task_container_ids,\n                prior_question_elapsed_times,\n                prior_question_had_explanations,\n                current_container_sizes,\n            ):\n                user_id = row[0]\n                timestamp = row[1]\n                content_id = row[2]\n                content_type_id = row[3]\n                task_container_id = row[4]\n                prior_question_elapsed_time = row[5]\n                prior_question_had_explanation = row[6]\n                current_container_size = row[7]\n                row = {\n                    \"user_id\": user_id,\n                    \"timestamp\": timestamp,\n                    \"content_id\": content_id,\n                    \"content_type_id\": content_type_id,\n                    \"task_container_id\": task_container_id,\n                    \"prior_question_elapsed_time\": prior_question_elapsed_time,\n                    \"prior_question_had_explanation\": prior_question_had_explanation,\n                    \"current_container_size\": current_container_size,\n                }\n                row[\"user_to_remove\"] = True\n                last_row_states[row[\"user_id\"]] = row\n\n        test_df = pd.merge(test_df, questions_df[QUESTION_FEATURES], left_on = 'content_id', right_on = 'question_id', how = 'left')\n        test_df['prior_question_had_explanation'] = test_df.prior_question_had_explanation.fillna(False).astype('int8')\n        test_df['prior_question_elapsed_time'].fillna(0, inplace = True)\n                \n        previous_test_df = test_df.copy()\n        test_df = test_df[test_df['content_type_id'] == 0].reset_index(drop = True)\n        test_df[TARGET] = 0\n\n        # SAINT inference\n        test_df = test_df[test_df.content_type_id == False]\n        test_dataset = TestDataset(group, test_df)\n        test_dataloader = DataLoader(test_dataset, batch_size=51200, shuffle=False)\n        \n        outs = []\n        for item in test_dataloader:\n            x = item[0].to(device).long()\n            target_id = item[1].to(device).long()\n            pt_x = item[2].to(device).long()\n            lsi_x = item[3].to(device).long()\n            c = item[4].to(device).long()\n            et = item[5].to(device).long()\n            lt = item[6].to(device).long()\n            if TEST_MODE:\n                pd.DataFrame({\n                    \"x\": x[0],\n                    \"target_id\": target_id[0].detach().numpy(),\n                    \"pt_x\": pt_x[0].detach().numpy(),\n                    \"lsi_x\": lsi_x[0].detach().numpy(),\n                    \"c\": c[0].detach().numpy(),\n                    \"et\": et[0].detach().numpy(),\n                    \"lt\": lt[0].detach().numpy(),\n                }).to_csv(\"input.csv\")\n            \n            with torch.no_grad():\n                output = saint_model(x, target_id, pt_x, lsi_x, c, et, lt)\n            outs.extend(torch.sigmoid(output)[:, -1].view(-1).data.cpu().numpy())\n\n        pred = np.array(outs)\n\n        test_df[TARGET] =  pred\n        if not TEST_MODE and not MOCK_MODE:\n            set_predict(test_df[['row_id', TARGET]])\n        \n    print('Job Done')\n\ninference(iter_test, TARGET, saint_model, questions_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}