{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!pip install ../input/python-datatable/datatable-0.11.0-cp37-cp37m-manylinux2010_x86_64.whl > /dev/null 2>&1","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom collections import defaultdict\nimport datatable as dt\nimport lightgbm as lgb\nfrom matplotlib import pyplot as plt\nimport riiideducation\nimport torch\n\n\n# Error handling, ignore all\nnp.seterr(divide = 'ignore', invalid = 'ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_types_dict = {\n    'user_id': 'int32', \n    'timestamp': 'int64',\n    'content_id': 'int16', \n    'answered_correctly': 'int8', \n    'prior_question_elapsed_time': 'float32', \n    'prior_question_had_explanation': 'bool'\n}\ntarget = 'answered_correctly'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = dt.fread('../input/riiid-test-answer-prediction/train.csv', columns = set(data_types_dict.keys())).to_pandas()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_df = train_df.groupby('user_id').tail(24).reset_index(drop = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Training dataset detailed information')\nprint('*' * 50)\nprint('Columns:', train_df.columns)\nprint('*' * 50)\nprint('Shape:', train_df.shape)\nprint('*' * 50)\nprint('NA values in each column:', sum(train_df.isna().sum()))\nprint('*' * 50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_df[train_df[target] != -1].reset_index(drop = True, inplace = False)#获取target非-1的样本\n\ntrain_df['prior_question_had_explanation'].fillna(False, inplace = True)#用False填充nan\n\ntrain_df = train_df.astype(data_types_dict)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prior_question_elapsed_time_mean = train_df.prior_question_elapsed_time.dropna().values.mean()\ntrain_df['prior_question_elapsed_time_mean'] = train_df.prior_question_elapsed_time.fillna(prior_question_elapsed_time_mean)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['lag'] = train_df.groupby('user_id')[target].shift()\n\ncum = train_df.groupby('user_id')['lag'].agg(['cumsum', 'cumcount']) # 列方向上求累积和 和累计个数\n#  学习进步的增长率\ntrain_df['user_correctness'] = cum['cumsum'] / cum['cumcount']\n# \ntrain_df.drop(columns = ['lag'], inplace = True)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Overall correctness of users 用户回答问题正确的比例，数目和次数 sum是回答正确的次数，count是回答的xx题目的总次数\nuser_agg = train_df.groupby('user_id')[target].agg(['sum', 'count'])\n# Overall difficulty of questions每个content出现的次数和被回答正确的比例\ncontent_agg = train_df.groupby('content_id')[target].agg(['sum', 'count'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_df['Accuracy'] = train_df['user_id'].map(user_agg['sum']/user_agg['count'])#每个用户回答问题的准确率\ntrain_df['Accuracy_sum'] = train_df['user_id'].map(user_agg['sum'])#每个用户回答问题对的总数\ntrain_df['Questions_num'] = train_df['user_id'].map(user_agg['count'])#每个用户回答问题的总数","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Take only 24 last observations of each user\ntrain_df = train_df.groupby('user_id').tail(500).reset_index(drop = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train = train_df.groupby('user_id').tail(24).reset_index(drop = True)\n#valid_df = valid_df.groupby('user_id').tail(24).reset_index(drop = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"questions_df = pd.read_csv(\n    '../input/riiid-test-answer-prediction/questions.csv', \n    usecols = [0, 3],\n    dtype = {'question_id': 'int16', 'part': 'int8'}\n)\ntrain_df = pd.merge(train_df, questions_df, left_on = 'content_id', right_on = 'question_id', how = 'left')\n\ntrain_df.drop(columns = ['question_id'], inplace = True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['content_count'] = train_df['content_id'].map(content_agg['count']).astype('int32')#某讲座被回答的次数\n\ntrain_df['content_id'] = train_df['content_id'].map(content_agg['sum'] / content_agg['count'])#某讲座被回答正确的比例","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.prior_question_had_explanation=train_df.prior_question_had_explanation.astype('int8')\n\ntrain_df['lag'] = train_df.groupby('user_id')['prior_question_had_explanation'].shift()#用户是否\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_df['lag'] = train_df.groupby('user_id')['prior_question_had_explanation'].shift()#用户是否看到上一个问题的答案，第一个题目为null。通常前几个都为false，因为那是测试。\ncum = train_df.groupby('user_id')['lag'].agg(['cumsum', 'cumcount'])#看上一题解释的总数和列数\ntrain_df['explanation_mean'] = cum['cumsum'] / cum['cumcount']#解释的平均\ntrain_df['explanation_cumsum'] = cum['cumsum'] \n\ntrain_df.drop(columns=['lag'], inplace=True)\n\ntrain_df['explanation_mean'].fillna(0, inplace=True)\ntrain_df['explanation_cumsum'].fillna(0, inplace=True)\ntrain_df.explanation_mean=train_df.explanation_mean.astype('float16')\ntrain_df.explanation_cumsum=train_df.explanation_cumsum.astype('int16')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"explanation_agg = train_df.groupby('user_id')['prior_question_had_explanation'].agg(['sum', 'count'])#与上面cusum和cucount的区别\nexplanation_agg = explanation_agg.astype('int16')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_timestamp_u = train_df[['user_id','timestamp']].groupby(['user_id']).agg(['max']).reset_index()#取出timestamp的最大值\nmax_timestamp_u.columns = ['user_id', 'max_time_stamp']#重新设置columns\n\ntrain_df['lagtime'] = train_df.groupby('user_id')['timestamp'].shift()\ntrain_df['lagtime']=train_df['timestamp']-train_df['lagtime']#此用户交互与该用户完成第一个事件之间的时间（毫秒）。\ntrain_df['lagtime'].fillna(0, inplace=True)#用0填充空值\ntrain_df.lagtime=train_df.lagtime.astype('int32')#数据格式转换\n\nlagtime_agg = train_df.groupby('user_id')['lagtime'].agg(['mean'])#完成每一题的平均时间\ntrain_df['lagtime_mean'] = train_df['user_id'].map(lagtime_agg['mean'])#map映射\ntrain_df.lagtime_mean=train_df.lagtime_mean.astype('int32')#转换数据格式\n\n\ntrain_df['timestamp']=train_df['timestamp']/(1000*3600)#时间转换为小时\ntrain_df.timestamp=train_df.timestamp.astype('int16')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"提取验证集"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Ratio is 6/24 = 25%\nvalid_df = train_df.groupby('user_id').tail(125)\ntrain_df.drop(valid_df.index, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.shape,valid_df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"训练"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfeatures = [ 'timestamp','lagtime','lagtime_mean','Accuracy_sum','Questions_num','prior_question_elapsed_time', \n            'prior_question_had_explanation', 'user_correctness', 'prior_question_elapsed_time_mean',\n            'part', 'content_count','content_id','explanation_mean','explanation_cumsum']\n'''\nfeatures = [ 'timestamp','lagtime','lagtime_mean',\n    'content_id', 'prior_question_elapsed_time', \n            'prior_question_had_explanation', 'user_correctness', \n            'part', 'content_count']\n'''\n\n\nparams = {\n    'loss_function': 'Logloss',\n    'eval_metric': 'AUC',\n    'task_type': 'GPU' if torch.cuda.is_available() else 'CPU',\n    'grow_policy': 'Lossguide',\n    'iterations': 5000,\n    'learning_rate': 4e-2,\n    'random_seed': 0,\n    'l2_leaf_reg': 1e-1,\n    'depth': 15,\n    'max_leaves': 10,\n    'border_count': 128,\n    'verbose': 50,\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from catboost import CatBoostClassifier, Pool\n\n# Training and validating data\ntrain_set = Pool(train_df[features], label = train_df[target])\nval_set = Pool(valid_df[features], label = valid_df[target])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model definition\nmodel = CatBoostClassifier(**params)\n\n# Fitting\nmodel.fit(train_set, eval_set = val_set, use_best_model = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"接口，这里是相比于国内的比赛有很大的不同。"},{"metadata":{"trusted":true},"cell_type":"code","source":"user_sum_dict = user_agg['sum'].astype('int16').to_dict(defaultdict(int))\nuser_count_dict = user_agg['count'].astype('int16').to_dict(defaultdict(int))\ncontent_sum_dict = content_agg['sum'].astype('int32').to_dict(defaultdict(int))\ncontent_count_dict = content_agg['count'].astype('int32').to_dict(defaultdict(int))\n\nexplanation_sum_dict = explanation_agg['sum'].astype('int16').to_dict(defaultdict(int))\nexplanation_count_dict = explanation_agg['count'].astype('int16').to_dict(defaultdict(int))\n\n\nlagtime_mean_dict = lagtime_agg['mean'].astype('int32').to_dict(defaultdict(int))\nmax_timestamp_u_dict = max_timestamp_u.set_index('user_id').to_dict()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    env = riiideducation.make_env()\nexcept:\n    pass\niter_test = env.iter_test()\nprior_test_df = None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for (test_df, sample_prediction_df) in iter_test:\n    if prior_test_df is not None:\n        prior_test_df[target] = eval(test_df['prior_group_answers_correct'].iloc[0])\n        prior_test_df = prior_test_df[prior_test_df[target] != -1].reset_index(drop = True)\n        \n        user_ids = prior_test_df['user_id'].values\n        content_ids = prior_test_df['content_id'].values\n        targets = prior_test_df[target].values\n        \n        for user_id, content_id, answered_correctly in zip(user_ids, content_ids, targets):\n            user_sum_dict[user_id] += answered_correctly\n            user_count_dict[user_id] += 1\n            content_sum_dict[content_id] += answered_correctly\n            content_count_dict[content_id] += 1\n\n    prior_test_df = test_df.copy()\n    \n    test_df = test_df[test_df['content_type_id'] == 0].reset_index(drop = True)#测试数据\n    test_df = pd.merge(test_df, questions_df, left_on = 'content_id', right_on = 'question_id', how = 'left')\n    test_df['prior_question_had_explanation'] = test_df['prior_question_had_explanation'].fillna(False).astype('bool')  \n    #prior_question_elapsed_time_mean = test_df.prior_question_elapsed_time.dropna().values.mean()\n    test_df['prior_question_elapsed_time_mean'] = test_df.prior_question_elapsed_time.fillna(prior_question_elapsed_time_mean)\n    \n    user_sum = np.zeros(len(test_df), dtype = np.int16)\n    user_count = np.zeros(len(test_df), dtype = np.int16)\n    content_sum = np.zeros(len(test_df), dtype = np.int32)\n    content_count = np.zeros(len(test_df), dtype = np.int32)\n    explanation_sum = np.zeros(len(test_df), dtype=np.int32)\n    explanation_count = np.zeros(len(test_df), dtype=np.int32)\n    \n    lagtime = np.zeros(len(test_df), dtype=np.int32)\n    lagtime_mean = np.zeros(len(test_df), dtype=np.int32)\n    \n    for i, (user_id, content_id,timestamp) in enumerate(zip(test_df['user_id'].values, test_df['content_id'].values,test_df['timestamp'].values)):\n        user_sum[i] = user_sum_dict[user_id]\n        user_count[i] = user_count_dict[user_id]\n        content_sum[i] = content_sum_dict[content_id]\n        content_count[i] = content_count_dict[content_id]\n        explanation_sum[i] = explanation_sum_dict[user_id]\n        explanation_count[i] = explanation_count_dict[user_id]\n        \n        if user_id in max_timestamp_u_dict['max_time_stamp'].keys():\n            lagtime[i] = timestamp-max_timestamp_u_dict['max_time_stamp'][user_id]\n            max_timestamp_u_dict['max_time_stamp'][user_id]=timestamp\n            lagtime_mean[i] = (lagtime_mean_dict[user_id]+lagtime[i])/2           \n        else:\n            lagtime[i]=0\n            max_timestamp_u_dict['max_time_stamp'].update({user_id:timestamp})\n            lagtime_mean_dict.update({user_id:timestamp})\n            lagtime_mean[i]=(lagtime_mean_dict[user_id]+lagtime[i])/2\n\n    #test_df['Accuracy'] = user_sum / user_count#每个用户回答问题的准确率\n    test_df['user_correctness'] = user_sum / user_count\n    test_df['Accuracy_sum'] = user_sum\n    test_df['Questions_num'] = user_count\n    test_df['content_count'] = content_count\n    test_df['content_id'] = content_sum / content_count\n    test_df['explanation_mean'] = explanation_sum / explanation_count\n    test_df['explanation_cumsum'] = explanation_sum \n    test_df[\"lagtime\"] = lagtime\n    test_df[\"lagtime_mean\"] = lagtime_mean\n    test_df['timestamp']=test_df['timestamp']/(1000*3600)#时间转换为小时\n    test_df.timestamp=test_df.timestamp.astype('int16')\n        \n\n       \n    test_df[target] = model.predict_proba(test_df[features])[:,1]\n    env.predict(test_df[['row_id', target]])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}