{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\nimport psutil\nimport random\nimport pandas as pd\nimport numpy as np\nimport datatable as dt\nimport seaborn as sns\n\nfrom enum import IntEnum\nfrom tqdm.notebook import tqdm\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.init import xavier_uniform_\nfrom torch.nn.init import constant_\nfrom torch.nn.init import xavier_normal_\nimport torch.nn.utils.rnn as rnn_utils\nfrom torch.autograd import Variable\nfrom torch.utils.data import Dataset, DataLoader\nfrom pathlib import Path","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ntrain_df = pd.read_pickle('/kaggle/input/data-plus/train.pkl')\n\nlen(train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ntrain_df = train_df[train_df.content_type_id == False]\n\n#arrange by timestamp\ntrain_df = train_df.sort_values(['timestamp'], ascending=True).reset_index(drop = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train_df['timestamp']\ndel train_df['content_type_id']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ngroup = train_df[['user_id', 'content_id', 'answered_correctly']].groupby('user_id').apply(lambda r: (r['content_id'].values, r['answered_correctly'].values))\n\ndel train_df\n\ngroup","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Dim(IntEnum):\n    batch = 0\n    seq = 1\n    feature = 2\n\n\nclass AKT(nn.Module):\n    def __init__(self, n_question, n_pid, d_model, n_blocks,\n                 kq_same, dropout, model_type, final_fc_dim=512, n_heads=8, d_ff=2048,  l2=1e-5, separate_qa=False):\n        super().__init__()\n        \"\"\"\n        Input:\n            d_model: dimension of attention block\n            final_fc_dim: dimension of final fully connected net before prediction\n            n_heads: number of heads in multi-headed attention\n            d_ff : dimension for fully conntected net inside the basic block\n        \"\"\"\n        self.n_question = n_question\n        self.dropout = dropout\n        self.kq_same = kq_same\n        self.n_pid = n_pid\n        self.l2 = l2\n        self.model_type = model_type\n        self.separate_qa = separate_qa\n        embed_l = d_model\n        if self.n_pid > 0:\n            self.difficult_param = nn.Embedding(self.n_pid+1, 1)\n            self.q_embed_diff = nn.Embedding(self.n_question+1, embed_l)\n            self.qa_embed_diff = nn.Embedding(2 * self.n_question + 1, embed_l)\n        # n_question+1 ,d_model\n        self.q_embed = nn.Embedding(self.n_question+1, embed_l)\n        if self.separate_qa:\n            self.qa_embed = nn.Embedding(2*self.n_question+1, embed_l)\n        else:\n            self.qa_embed = nn.Embedding(2, embed_l)\n        # Architecture Object. It contains stack of attention block\n        self.model = Architecture(n_question=n_question, n_blocks=n_blocks, n_heads=n_heads, dropout=dropout,\n                                    d_model=d_model, d_feature=d_model / n_heads, d_ff=d_ff,  kq_same=self.kq_same, model_type=self.model_type)\n\n        self.out = nn.Sequential(\n            nn.Linear(d_model + embed_l,\n                      final_fc_dim), nn.ReLU(), nn.Dropout(self.dropout),\n            nn.Linear(final_fc_dim, 256), nn.ReLU(\n            ), nn.Dropout(self.dropout),\n            nn.Linear(256, 1)\n        )\n        self.reset()\n\n    def reset(self):\n        for p in self.parameters():\n            if p.size(0) == self.n_pid+1 and self.n_pid > 0:\n                torch.nn.init.constant_(p, 0.)\n\n    def forward(self, q_data, qa_data, target, pid_data=None):\n        # Batch First\n        q_embed_data = self.q_embed(q_data)  # BS, seqlen,  d_model# c_ct\n        if self.separate_qa:\n            # BS, seqlen, d_model #f_(ct,rt)\n            qa_embed_data = self.qa_embed(qa_data)\n        else:\n            qa_data = (qa_data-q_data)//self.n_question  # rt\n            # BS, seqlen, d_model # c_ct+ g_rt =e_(ct,rt)\n            qa_embed_data = self.qa_embed(qa_data)+q_embed_data\n\n        if self.n_pid > 0:\n            q_embed_diff_data = self.q_embed_diff(q_data)  # d_ct\n            pid_embed_data = self.difficult_param(pid_data)  # uq\n            q_embed_data = q_embed_data + pid_embed_data * \\\n                q_embed_diff_data  # uq *d_ct + c_ct\n            qa_embed_diff_data = self.qa_embed_diff(\n                qa_data)  # f_(ct,rt) or #h_rt\n            if self.separate_qa:\n                qa_embed_data = qa_embed_data + pid_embed_data * \\\n                    qa_embed_diff_data  # uq* f_(ct,rt) + e_(ct,rt)\n            else:\n                qa_embed_data = qa_embed_data + pid_embed_data * \\\n                    (qa_embed_diff_data+q_embed_diff_data)  # + uq *(h_rt+d_ct)\n            c_reg_loss = (pid_embed_data ** 2.).sum() * self.l2\n        else:\n            c_reg_loss = 0.\n\n        # BS.seqlen,d_model\n        # Pass to the decoder\n        # output shape BS,seqlen,d_model or d_model//2\n        d_output = self.model(q_embed_data, qa_embed_data)  # 211x512\n\n        concat_q = torch.cat([d_output, q_embed_data], dim=-1)\n        output = self.out(concat_q)\n        labels = target.reshape(-1)\n#         m = nn.Sigmoid()\n#         preds = (output.reshape(-1))  # logit\n#         print(f\"- output: {type(output)} {output.size()}\")\n#         mask = labels > -0.9\n#         masked_labels = labels[mask].float()\n#         masked_preds = preds[mask]\n#         loss = nn.BCEWithLogitsLoss(reduction='none')\n#         output = loss(masked_preds, masked_labels)\n#         return output.sum()+c_reg_loss, m(preds), mask.sum()\n        return output\n\n\nclass Architecture(nn.Module):\n    def __init__(self, n_question,  n_blocks, d_model, d_feature,\n                 d_ff, n_heads, dropout, kq_same, model_type):\n        super().__init__()\n        \"\"\"\n            n_block : number of stacked blocks in the attention\n            d_model : dimension of attention input/output\n            d_feature : dimension of input in each of the multi-head attention part.\n            n_head : number of heads. n_heads*d_feature = d_model\n        \"\"\"\n        self.d_model = d_model\n        self.model_type = model_type\n\n        if model_type in {'akt'}:\n            self.blocks_1 = nn.ModuleList([\n                TransformerLayer(d_model=d_model, d_feature=d_model // n_heads,\n                                 d_ff=d_ff, dropout=dropout, n_heads=n_heads, kq_same=kq_same)\n                for _ in range(n_blocks)\n            ])\n            self.blocks_2 = nn.ModuleList([\n                TransformerLayer(d_model=d_model, d_feature=d_model // n_heads,\n                                 d_ff=d_ff, dropout=dropout, n_heads=n_heads, kq_same=kq_same)\n                for _ in range(n_blocks*2)\n            ])\n\n    def forward(self, q_embed_data, qa_embed_data):\n        # target shape  bs, seqlen\n        seqlen, batch_size = q_embed_data.size(1), q_embed_data.size(0)\n\n        qa_pos_embed = qa_embed_data\n        q_pos_embed = q_embed_data\n\n        y = qa_pos_embed\n        seqlen, batch_size = y.size(1), y.size(0)\n        x = q_pos_embed\n\n        # encoder\n        for block in self.blocks_1:  # encode qas\n            y = block(mask=1, query=y, key=y, values=y)\n        flag_first = True\n        for block in self.blocks_2:\n            if flag_first:  # peek current question\n                x = block(mask=1, query=x, key=x,\n                          values=x, apply_pos=False)\n                flag_first = False\n            else:  # dont peek current response\n                x = block(mask=0, query=x, key=x, values=y, apply_pos=True)\n                flag_first = True\n        return x\n\n\nclass TransformerLayer(nn.Module):\n    def __init__(self, d_model, d_feature,\n                 d_ff, n_heads, dropout,  kq_same):\n        super().__init__()\n        \"\"\"\n            This is a Basic Block of Transformer paper. It containts one Multi-head attention object. Followed by layer norm and postion wise feedforward net and dropout layer.\n        \"\"\"\n        kq_same = kq_same == 1\n        # Multi-Head Attention Block\n        self.masked_attn_head = MultiHeadAttention(\n            d_model, d_feature, n_heads, dropout, kq_same=kq_same)\n\n        # Two layer norm layer and two droput layer\n        self.layer_norm1 = nn.LayerNorm(d_model)\n        self.dropout1 = nn.Dropout(dropout)\n\n        self.linear1 = nn.Linear(d_model, d_ff)\n        self.activation = nn.ReLU()\n        self.dropout = nn.Dropout(dropout)\n        self.linear2 = nn.Linear(d_ff, d_model)\n\n        self.layer_norm2 = nn.LayerNorm(d_model)\n        self.dropout2 = nn.Dropout(dropout)\n\n    def forward(self, mask, query, key, values, apply_pos=True):\n        \"\"\"\n        Input:\n            block : object of type BasicBlock(nn.Module). It contains masked_attn_head objects which is of type MultiHeadAttention(nn.Module).\n            mask : 0 means, it can peek only past values. 1 means, block can peek only current and pas values\n            query : Query. In transformer paper it is the input for both encoder and decoder\n            key : Keys. In transformer paper it is the input for both encoder and decoder\n            Values. In transformer paper it is the input for encoder and  encoded output for decoder (in masked attention part)\n\n        Output:\n            query: Input gets changed over the layer and returned.\n\n        \"\"\"\n\n        seqlen, batch_size = query.size(1), query.size(0)\n        nopeek_mask = np.triu(\n            np.ones((1, 1, seqlen, seqlen)), k=mask).astype('uint8')\n        src_mask = (torch.from_numpy(nopeek_mask) == 0).to(device)\n        if mask == 0:  # If 0, zero-padding is needed.\n            # Calls block.masked_attn_head.forward() method\n            query2 = self.masked_attn_head(\n                query, key, values, mask=src_mask, zero_pad=True)\n        else:\n            # Calls block.masked_attn_head.forward() method\n            query2 = self.masked_attn_head(\n                query, key, values, mask=src_mask, zero_pad=False)\n\n        query = query + self.dropout1((query2))\n        query = self.layer_norm1(query)\n        if apply_pos:\n            query2 = self.linear2(self.dropout(\n                self.activation(self.linear1(query))))\n            query = query + self.dropout2((query2))\n            query = self.layer_norm2(query)\n        return query\n\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, d_feature, n_heads, dropout, kq_same, bias=True):\n        super().__init__()\n        \"\"\"\n        It has projection layer for getting keys, queries and values. Followed by attention and a connected layer.\n        \"\"\"\n        self.d_model = d_model\n        self.d_k = d_feature\n        self.h = n_heads\n        self.kq_same = kq_same\n\n        self.v_linear = nn.Linear(d_model, d_model, bias=bias)\n        self.k_linear = nn.Linear(d_model, d_model, bias=bias)\n        if kq_same is False:\n            self.q_linear = nn.Linear(d_model, d_model, bias=bias)\n        self.dropout = nn.Dropout(dropout)\n        self.proj_bias = bias\n        self.out_proj = nn.Linear(d_model, d_model, bias=bias)\n        self.gammas = nn.Parameter(torch.zeros(n_heads, 1, 1))\n        torch.nn.init.xavier_uniform_(self.gammas)\n\n        self._reset_parameters()\n\n    def _reset_parameters(self):\n        xavier_uniform_(self.k_linear.weight)\n        xavier_uniform_(self.v_linear.weight)\n        if self.kq_same is False:\n            xavier_uniform_(self.q_linear.weight)\n\n        if self.proj_bias:\n            constant_(self.k_linear.bias, 0.)\n            constant_(self.v_linear.bias, 0.)\n            if self.kq_same is False:\n                constant_(self.q_linear.bias, 0.)\n            constant_(self.out_proj.bias, 0.)\n\n    def forward(self, q, k, v, mask, zero_pad):\n\n        bs = q.size(0)\n\n        # perform linear operation and split into h heads\n\n        k = self.k_linear(k).view(bs, -1, self.h, self.d_k)\n        if self.kq_same is False:\n            q = self.q_linear(q).view(bs, -1, self.h, self.d_k)\n        else:\n            q = self.k_linear(q).view(bs, -1, self.h, self.d_k)\n        v = self.v_linear(v).view(bs, -1, self.h, self.d_k)\n\n        # transpose to get dimensions bs * h * sl * d_model\n\n        k = k.transpose(1, 2)\n        q = q.transpose(1, 2)\n        v = v.transpose(1, 2)\n        # calculate attention using function we will define next\n        gammas = self.gammas\n        scores = attention(q, k, v, self.d_k,\n                           mask, self.dropout, zero_pad, gammas)\n\n        # concatenate heads and put through final linear layer\n        concat = scores.transpose(1, 2).contiguous()\\\n            .view(bs, -1, self.d_model)\n\n        output = self.out_proj(concat)\n\n        return output\n\n\ndef attention(q, k, v, d_k, mask, dropout, zero_pad, gamma=None):\n    \"\"\"\n    This is called by Multi-head atention object to find the values.\n    \"\"\"\n    scores = torch.matmul(q, k.transpose(-2, -1)) / \\\n        math.sqrt(d_k)  # BS, 8, seqlen, seqlen\n    bs, head, seqlen = scores.size(0), scores.size(1), scores.size(2)\n\n    x1 = torch.arange(seqlen).expand(seqlen, -1).to(device)\n    x2 = x1.transpose(0, 1).contiguous()\n\n    with torch.no_grad():\n        scores_ = scores.masked_fill(mask == 0, -1e32)\n        scores_ = F.softmax(scores_, dim=-1)  # BS,8,seqlen,seqlen\n        scores_ = scores_ * mask.float().to(device)\n        distcum_scores = torch.cumsum(scores_, dim=-1)  # bs, 8, sl, sl\n        disttotal_scores = torch.sum(\n            scores_, dim=-1, keepdim=True)  # bs, 8, sl, 1\n        position_effect = torch.abs(\n            x1-x2)[None, None, :, :].type(torch.FloatTensor).to(device)  # 1, 1, seqlen, seqlen\n        # bs, 8, sl, sl positive distance\n        dist_scores = torch.clamp(\n            (disttotal_scores-distcum_scores)*position_effect, min=0.)\n        dist_scores = dist_scores.sqrt().detach()\n    m = nn.Softplus()\n    gamma = -1. * m(gamma).unsqueeze(0)  # 1,8,1,1\n    # Now after do exp(gamma*distance) and then clamp to 1e-5 to 1e5\n    total_effect = torch.clamp(torch.clamp(\n        (dist_scores*gamma).exp(), min=1e-5), max=1e5)\n    scores = scores * total_effect\n\n    scores.masked_fill_(mask == 0, -1e32)\n    scores = F.softmax(scores, dim=-1)  # BS,8,seqlen,seqlen\n    if zero_pad:\n        pad_zero = torch.zeros(bs, head, 1, seqlen).to(device)\n        scores = torch.cat([pad_zero, scores[:, :, 1:, :]], dim=2)\n    scores = dropout(scores)\n    output = torch.matmul(scores, v)\n    return output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_question = 13523\nbatch_size = 36\nseqlen = 200\nn_pid = -1\nn_blocks = 1\nd_model = 256\ndropout = 0.05\nkq_same = 1\nn_heads = 8\nd_ff = 2048\nl2 = 1e-5\nfinal_fc_dim = 512","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = AKT(\n    n_question=n_question, n_pid=n_pid, n_blocks=n_blocks, \n    d_model=d_model, n_heads=n_heads, dropout=dropout, \n    kq_same=kq_same, model_type='akt', l2=l2, \n    final_fc_dim=final_fc_dim, d_ff=d_ff\n)\n\nmodel","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_path = '/kaggle/input/riiid-ktmodel-cid/riiid_model_cid'\n\ncheckpoint = torch.load(model_path)\nmodel.load_state_dict(checkpoint['model_state_dict'])\n\nmodel.to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TestDataset(Dataset):\n    def __init__(self, samples, test_df, n_questions, max_seq=200):\n        super(TestDataset, self).__init__()\n        \n        self.samples = samples\n        self.test_df = test_df\n        self.user_ids = [x for x in test_df.user_id.unique()]\n        self.n_questions = n_questions \n        self.max_seq = max_seq\n        \n    def __len__(self):\n        return self.test_df.shape[0]\n    \n    def __getitem__(self, index):\n        row = self.test_df.iloc[index]\n        \n        user_id = row.user_id\n        target_id = row.content_id\n        \n        q = np.zeros(self.max_seq, dtype=int)\n        res = np.zeros(self.max_seq, dtype=int)\n        qa = np.zeros(self.max_seq, dtype=int)\n        \n        if user_id in self.samples.index:\n            q_, res_ = self.samples[user_id]\n            \n            seq_len = len(q_)\n            \n            if seq_len > self.max_seq:\n                q = q_[-self.max_seq:]\n                res = res_[-self.max_seq:]\n            else:\n                q[-seq_len:] = q_\n                res[-seq_len:] = res_\n                \n        q = np.append(q[2:], [target_id])\n        res = np.append(res[2:], [1])\n            \n#         q = np.asarray(q, dtype=np.int)\n        qa = res.astype(int) * self.n_questions + q\n        \n#         print(f\"- Shape of qa: {qa.shape}; q: {q.shape}; res: {res.shape}\")\n#         print(f\"target_id: {target_id}\")\n        \n            \n        return q, qa, target_id","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import riiideducation\n\n# You can only call make_env() once, so don't lose it!\nenv = riiideducation.make_env()\n\niter_test = env.iter_test()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import psutil\n\nmodel.eval()\n\nprev_test_df = None\n\n# print(f\"STARTi\")\n\nfor (test_df, sample_prediction_df) in tqdm(iter_test):\n#     print(\"==============================================\")\n    \n    if (prev_test_df is not None) & (psutil.virtual_memory().percent<90):\n        \n        print(psutil.virtual_memory().percent)\n        \n        prev_test_df['answered_correctly'] = eval(test_df['prior_group_answers_correct'].iloc[0])\n        prev_test_df = prev_test_df[prev_test_df.content_type_id == False]\n        prev_group = prev_test_df[['user_id', 'content_id', 'answered_correctly']].groupby('user_id').apply(lambda r: (\n            r['content_id'].values,\n            r['answered_correctly'].values))\n        \n        for prev_user_id in prev_group.index:\n            if prev_user_id in group.index:\n                group[prev_user_id] = (\n                    np.append(group[prev_user_id][0], prev_group[prev_user_id][0])[-seqlen:], \n                    np.append(group[prev_user_id][1], prev_group[prev_user_id][1])[-seqlen:]\n                )\n \n            else:\n                group[prev_user_id] = (\n                    prev_group[prev_user_id][0], \n                    prev_group[prev_user_id][1]\n                )\n                \n    prev_test_df = test_df.copy()\n    test_df = test_df[test_df.content_type_id == False]\n    \n    test_dataset = TestDataset(group, test_df, n_questions=n_question, max_seq=seqlen)\n    test_dataloader = DataLoader(test_dataset, batch_size=len(test_df), shuffle=False)\n    \n    item = next(iter(test_dataloader))\n    \n    q_data = item[0].to(device).long().cpu().detach().numpy().T\n    qa_data = item[1].to(device).long().cpu().detach().numpy().T\n    \n    input_q = np.transpose(q_data[:, :])\n    input_qa = np.transpose(qa_data[:, :])\n    target = np.transpose(qa_data[:, :])\n    \n    target = (target - 1) / n_question\n    target_1 = np.floor(target)\n    \n    input_q = torch.from_numpy(input_q).to(device).long()\n    input_qa = torch.from_numpy(input_qa).to(device).long()\n    target = torch.from_numpy(target_1).to(device).float()\n\n    with torch.no_grad():\n        output = model(input_q, input_qa, target)\n    \n    output = torch.sigmoid(output)\n    output = output[:, -1]\n    output = output.cpu().numpy()\n#     output = np.reshape(output, (seqlen, -1))\n#     print(type(output), output.shape, output)\n    \n    out = [i[0] for i in output]\n    \n#     print(f\"- OUT: {type(out)} {len(out)} {out}\")\n    \n    test_df['answered_correctly'] = out\n#     print(f\"END\")\n    env.predict(test_df.loc[test_df['content_type_id'] == 0, ['row_id', 'answered_correctly']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}