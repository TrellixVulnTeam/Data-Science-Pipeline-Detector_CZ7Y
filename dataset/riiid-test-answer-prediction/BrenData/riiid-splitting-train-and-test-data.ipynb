{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Opening Notes\n\nFor this dataset, we are given a few things. We are told that it is sort of time-series tabular data. What I mean is that the data is organized by user_id, and in order of how the questions were viewed by that user. So the way I wanted to split the data was by taking features from the first questions each user saw, and create the training data on the later questions.\n\nThere was no easy way I found to do this, but I managed to group user_id's by the number of quesitions they had seen. I used these groups to set aside roughly 9-12% of the data to create my training dataset. the remaining 89-90% of the data was used to create features.\n\nIn this notebook I also created the lag_time variable which was the most prominent feature on the LGB Model I submitted!\n\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#basic\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom tqdm.notebook import tqdm\nimport gc\n\n\nimport os","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"%%time\ntrain_questions_only_df = pd.read_pickle('../input/riiid-train-df/train_df.pkl.gzip')\n\nused_data_types_dict = {\n    'question_id': 'int16',\n    'bundle_id': 'int16',\n    'correct_answer': 'int8',\n    'part': 'int8',\n    'tags': 'str',\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_lag_time(df):\n    \n    lag_dict = {}\n    lag_list = []\n    prev_timestamp = 123\n    \n    for pair in tqdm(df[['user_id','timestamp']].values):\n        if pair[0] in lag_dict:\n            if prev_timestamp == pair[1]:\n                lag_list.append(lag_list[-1])\n            else:\n                lag_list.append(pair[1] - lag_dict[pair[0]])\n                lag_dict[pair[0]] = pair[1]\n            \n        else:\n            lag_dict[pair[0]]= 0\n            lag_list.append(0)\n            \n        prev_timestamp=pair[1]\n            \n    df['lag_time']=lag_list\n    \n    return(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_questions_only_df = get_lag_time(train_questions_only_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"b=train_questions_only_df[train_questions_only_df.groupby('user_id')['user_id'].transform('size') >= 10000]\nvalid_split1 = b.groupby('user_id').tail(1000)\ntrain_split1 = b[~b.index.isin(valid_split1.index)]\n\nprint(valid_split1.shape[0]/train_split1.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"b=train_questions_only_df[train_questions_only_df.groupby('user_id')['user_id'].transform('size') < 10000]\nc=b[b.groupby('user_id')['user_id'].transform('size') >= 5000]\nvalid_split2 = c.groupby('user_id').tail(600)\ntrain_split2 = c[~c.index.isin(valid_split2.index)]\n\nprint(valid_split2.shape[0]/train_split2.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"b=train_questions_only_df[train_questions_only_df.groupby('user_id')['user_id'].transform('size') < 5000]\nc=b[b.groupby('user_id')['user_id'].transform('size') >= 1000]\nvalid_split3 = c.groupby('user_id').tail(185)\ntrain_split3 = c[~c.index.isin(valid_split3.index)]\n\nprint(valid_split3.shape[0]/train_split3.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"b=train_questions_only_df[train_questions_only_df.groupby('user_id')['user_id'].transform('size') < 1000]\nc=b[b.groupby('user_id')['user_id'].transform('size') >= 500]\nvalid_split4 = c.groupby('user_id').tail(65)\ntrain_split4 = c[~c.index.isin(valid_split4.index)]\n\nprint(valid_split4.shape[0]/train_split4.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"b=train_questions_only_df[train_questions_only_df.groupby('user_id')['user_id'].transform('size') < 500]\nc=b[b.groupby('user_id')['user_id'].transform('size') >= 200]\nvalid_split5 = c.groupby('user_id').tail(28)\ntrain_split5 = c[~c.index.isin(valid_split5.index)]\n\nprint(valid_split5.shape[0]/train_split5.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"b=train_questions_only_df[train_questions_only_df.groupby('user_id')['user_id'].transform('size') < 200]\nc=b[b.groupby('user_id')['user_id'].transform('size') >= 50]\nvalid_split6 = c.groupby('user_id').tail(8)\ntrain_split6 = c[~c.index.isin(valid_split6.index)]\n\nprint(valid_split6.shape[0]/train_split6.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"b=train_questions_only_df[train_questions_only_df.groupby('user_id')['user_id'].transform('size') < 50]\nc=b[b.groupby('user_id')['user_id'].transform('size') >= 2]\nvalid_split7 = c.groupby('user_id').tail(3)\ntrain_split7 = c[~c.index.isin(valid_split7.index)]\n\nprint(valid_split7.shape[0]/train_split7.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"b=train_questions_only_df[train_questions_only_df.groupby('user_id')['user_id'].transform('size') < 50]\nc=b[b.groupby('user_id')['user_id'].transform('size') < 2]\n\nfeatures_df, train_df = train_test_split(c, test_size=0.09, random_state=314)\n\nprint(train_df.shape[0]/features_df.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train_questions_only_df, b, c\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I grouped all the dataframes together in the next cell, and when looping through I made sure to delete each df from memory. This was the only way I could get this done and stay withing the CPU limits."},{"metadata":{"trusted":true},"cell_type":"code","source":"for df in (valid_split1, valid_split2, valid_split3, valid_split4, valid_split5, valid_split6, valid_split7):\n    train_df = pd.concat([train_df, df], axis=0)\n    del df\n    \nfor df in (train_split1, train_split2, train_split3, train_split4, train_split5, train_split6, train_split7):\n    features_df = pd.concat([features_df, df], axis=0)\n    del df\n    \nprint(train_df.shape[0]/features_df.shape[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I did not take the exact same percentage of rows from each group as if I did so the average number of questions correct were a little odd. I both datasets to have roughly the same ratio of number of questions correct to the number of questions incorrect."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"features_df - average correct:\",features_df.answered_correctly.mean())\nprint(\"train_df - average correct:\", train_df.answered_correctly.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I found it useful to sort the rows just to keep everything nice and neat once I read the pickle file in the main notebook. This aided in the transparency and readability of the two dataframes."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_df.sort_index()\nfeatures_df = features_df.sort_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#writing data to use in main notebook\ntrain_df.to_pickle('./train_q_only.pkl.zip')\nfeatures_df.to_pickle('./features_q_only.pkl.zip')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Closing Notes\n\nI do think I would have a stronger model if I was able to select a few key features and train on a larger portion of the data. If I were to have time to do this competition over again I would attempt to create a model which could be trained on all 100 million rows."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}