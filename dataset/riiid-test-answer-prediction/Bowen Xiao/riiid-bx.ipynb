{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install ../input/python-datatable/datatable-0.11.0-cp37-cp37m-manylinux2010_x86_64.whl > /dev/null","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Load Data"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import datatable as dt\nimport pandas as pd\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = dt.fread('../input/riiid-test-answer-prediction/train.csv').to_pandas()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train['user_id'].max())\nprint(train['user_id'].min())\nprint(train['task_container_id'].max())\nprint(train['task_container_id'].min())\nprint(len(train))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Take A Sample on User Id"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train[(train['content_type_id'] == 0) & (train['user_id'] <= 21474828)]\nprint(len(train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.sort_values(by=['user_id', 'timestamp'])\n\ntrain['u_avg'] = train.groupby('user_id')['answered_correctly'].transform(lambda x: x.mean())\ntrain['q_avg'] = train.groupby('task_container_id')['answered_correctly'].transform(lambda x: x.mean())\n\ncol = ['user_id','task_container_id','answered_correctly', 'u_avg', 'q_avg']\nfeatures = ['l_pred', 'u_avg', 'q_avg']\n\nfor i in range(100):\n    c = 'l' + str(i+1)\n    train[c] = train.groupby('user_id')['answered_correctly'].apply(lambda x: x.shift(i+1))\n    col.append(c)\n    features.append(c)\n\ntrain = train[col]\n\nfor i in range(100):\n    c = 'l' + str(i+1)\n    train[c] = train[c].fillna(0)\n\nfrom sklearn import model_selection as cv\n\ntrain_data, test_data = cv.train_test_split(train, test_size=0.2)\ndel train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Linear Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nl = LinearRegression().fit(train_data[['l1', 'u_avg', 'q_avg']], train_data.answered_correctly)\ntrain_data['l_pred'] = l.predict(train_data[['l1', 'u_avg', 'q_avg']])\ntest_data['l_pred'] = l.predict(test_data[['l1', 'u_avg', 'q_avg']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\n\nprint(metrics.roc_auc_score(test_data.answered_correctly, test_data.l_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f = lambda x: x if x >= 0 and x <= 1 else 0 if x < 0 else 1\nvf = np.vectorize(f)\ntest_data['l_pred_trunc'] = vf(test_data.l_pred)\nprint(metrics.roc_auc_score(test_data.answered_correctly, test_data.l_pred_trunc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"User Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"u_tbl = train_data.groupby(['user_id']).size().reset_index(name='counts')\nu_tbl = u_tbl[u_tbl['counts'] >= 500]\nq_tbl = train_data.groupby(['task_container_id']).size().reset_index(name='counts')\nq_tbl = q_tbl[q_tbl['counts'] >= 2000]\nu_list = list(u_tbl.user_id.unique())\nq_list = list(q_tbl.task_container_id.unique())\n\nl_list = []\n\nfrom sklearn.linear_model import Lasso\n\nfor u in u_list:\n    l_list.append(Lasso(alpha=0.01, max_iter=10e5).fit(train_data[train_data['user_id']==u][features], train_data[train_data['user_id']==u].answered_correctly - train_data[train_data['user_id']==u].l_pred))\n    \ntrain_data['l_err_pred'] = train_data.apply(lambda x: l_list[u_list.index(x.user_id)].predict(x[features].values.reshape(1, -1))[0] if x.user_id in u_list and np.sum(l_list[u_list.index(x.user_id)].coef_!=0) >= 2 and np.sum(l_list[u_list.index(x.user_id)].coef_!=0) <= 20 else 0, axis=1)\ntest_data['l_err_pred'] = test_data.apply(lambda x: l_list[u_list.index(x.user_id)].predict(x[features].values.reshape(1, -1))[0] if x.user_id in u_list and np.sum(l_list[u_list.index(x.user_id)].coef_!=0) >= 2 and np.sum(l_list[u_list.index(x.user_id)].coef_!=0) <= 20 else 0, axis=1)\ntrain_data['l_pred_multi'] = train_data.apply(lambda x: x.l_pred + x.l_err_pred if abs(x.l_err_pred) <= 0.5 else x.l_pred + 0.5 * np.sign(x.l_err_pred), axis=1)\ntest_data['l_pred_multi'] = test_data.apply(lambda x: x.l_pred + x.l_err_pred if abs(x.l_err_pred) <= 0.5 else x.l_pred + 0.5 * np.sign(x.l_err_pred), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(metrics.roc_auc_score(test_data.answered_correctly, test_data.l_pred_multi))\nprint(metrics.roc_auc_score(test_data.answered_correctly, vf(test_data.l_pred_multi)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Similarity of Bias"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['bias'] = train_data.answered_correctly - train_data.l_pred_multi\ntrain_bias = pd.DataFrame(train_data.groupby(['user_id', 'task_container_id'])['bias'].mean()).reset_index()\n\ntrain_bias['user_id'] = train_bias['user_id'].apply(lambda x: u_list.index(x) if x in u_list else -1)\ntrain_bias['task_container_id'] = train_bias['task_container_id'].apply(lambda x: q_list.index(x) if x in q_list else -1)\nbias_matrix = np.zeros((len(u_list), len(q_list)))\nfor line in train_bias.itertuples():\n    if line[1] != -1 and line[2] != -1:\n        if abs(line[3]) < 0.25:\n            bias_matrix[line[1], line[2]] = line[3]\n        else:\n            bias_matrix[line[1], line[2]] = 0.25 * np.sign(line[3])\n\nfrom sklearn.metrics.pairwise import cosine_similarity\nsimilarity = cosine_similarity(bias_matrix.T)\n#s = lambda x: x if abs(x) >= 0.01 else 0\n#vs = np.vectorize(s)\n#similarity = vs(similarity)\nbias_pred = bias_matrix.dot(similarity) / np.array([np.abs(similarity).sum(axis=1)])\n\ntest_data['b_pred'] = test_data.apply(lambda x: bias_pred[u_list.index(x.user_id)][q_list.index(x.task_container_id)] if x.user_id in u_list and x.task_container_id in q_list else 0, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data['l_pred_sim'] = test_data.apply(lambda x: x.l_pred_multi + x.b_pred if abs(x.b_pred) <= 0.1 else x.l_pred_multi + 0.1 * np.sign(x.b_pred), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(metrics.roc_auc_score(test_data.answered_correctly, test_data.l_pred_sim))\nprint(metrics.roc_auc_score(test_data.answered_correctly, vf(test_data.l_pred_sim)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(metrics.roc_auc_score(test_data[test_data['b_pred'] != 0].answered_correctly, vf(test_data.l_pred_multi[test_data['b_pred'] != 0])))\nprint(metrics.roc_auc_score(test_data[test_data['b_pred'] != 0].answered_correctly, vf(test_data.l_pred_sim[test_data['b_pred'] != 0])))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}