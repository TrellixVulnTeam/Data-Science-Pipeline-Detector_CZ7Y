{"cells":[{"metadata":{},"cell_type":"markdown","source":"A model basted on ELO \n\nKey Ideas\n- Give each student a score per tag and use that to see how well they perform for questions in that tag group\n- Give each question a rating to judge how hard a question it is\n- Some charts to show results\n\nSome code and ideas borrowed from this model : https://www.kaggle.com/stevemju/riiid-simple-elo-rating/log#ELO-functions, give `stevemju` some love"},{"metadata":{},"cell_type":"markdown","source":"# Import Modules"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Imports\nimport importlib\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading Data\nroot = '/kaggle/input/riiid-test-answer-prediction'\n\ndef get_data( nrows = 1000000) :\n    return {\n        \"lectures\" : pd.read_csv(f'{root}/lectures.csv'),\n        \"questions\" : pd.read_csv(f'{root}/questions.csv'),\n        \"train\" : pd.read_csv(f'{root}/train.csv', nrows=nrows)\n    }\n\n\ndata = get_data()\ndata.keys()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Processing Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# A bit sloppy, but creating a unique question id for each sub-tag of a question\nquestion_data = data['questions'].assign(tags=data['questions'].tags.str.split(\" \")).explode('tags')\nquestion_data['question_id'] = question_data['question_id'].apply(str)\nquestion_data['question'] = question_data['question_id'] + '_' + question_data['tags']\nquestion_data = question_data[['question_id', 'tags', 'question']].set_index('question')\nquestion_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Furthermore, creating a lookup table that gives these question ids for each question in the data set\nquestion_data_lookup = question_data.reset_index().dropna().groupby('question_id')[['question', 'tags']].apply(lambda x: x.values.tolist())\nquestion_data_lookup = question_data_lookup.to_dict()\n\nfor i in question_data_lookup:\n    for t in question_data_lookup[i]:\n        t[1] = int(t[1])\n    \nquestion_data_lookup['0']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Elo"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Class I wrote that tries to use ELO for the classification of each question segmented by tags\n\nclass Elo:\n\n    def __init__ (self):\n        self.left_asymptote = 0.25\n    \n    ## These Functions pulled from https://www.kaggle.com/stevemju/riiid-simple-elo-rating/log#ELO-functions\n    ## Credit to `stevemju`\n    ## -------------------\n\n    def get_delta_student_rating(self, correct, projected_score, samples):\n        return + self.learning_rate_student(samples) * ( correct - projected_score )\n\n    def get_delta_question_rating(self, correct, projected_score, samples):\n        return - self.learning_rate_question(samples) * ( correct - projected_score )\n\n    def learning_rate_student(self, samples):\n        return max(0.3 / (1 + 0.01 * samples), 0.04)\n\n    def learning_rate_question(self, samples):\n        return 1 / (1 + 0.05 * samples)\n\n    def probability_of_good_answer(self, student_rating, question_rating):\n        return self.left_asymptote + (1 - self.left_asymptote) * self.sigmoid(student_rating - question_rating)\n\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))\n\n    ## -------------------\n\n    # Give Each person a rating per category of question\n    def initialize ( self, student_ids, question_ids, categories ):\n\n        # Students\n        self.students = {\n            student : { \n                \"rating\" : [ 0 for _ in range(categories) ], \n                \"samples\" : 0\n            }\n            for student in student_ids\n        }\n\n        # Questions\n        self.questions = {\n            question : { \n                \"rating\" : 0, \n                \"samples\" : 0\n            }\n            for question in question_ids\n        }\n\n    def reset_students ( self, student_ids, categories ):\n        \n        # Students\n        self.students = {\n            student : { \n                \"rating\" : [ 0 for _ in range(categories) ], \n                \"samples\" : 0\n            }\n            for student in student_ids\n        }\n\n    \n    # Takes in a single example and trains on it\n    def process_training_example ( self, student_id, question_id, category_id, correct ) :\n\n        # Select targets\n        target_student = self.students[student_id]\n        target_question = self.questions[question_id]\n        \n        projected_score = self.probability_of_good_answer(target_student['rating'][category_id], target_question['rating'])\n\n        # Update Rating\n        \n        delta_student = self.get_delta_student_rating (\n            correct, \n            projected_score,\n            target_student['samples']\n        )\n\n        delta_question = self.get_delta_question_rating (\n            correct, \n            projected_score,\n            target_question['samples']\n        )\n\n\n        # Log outputs for analysis\n\n        logged_output = {\n            'question' : question_id,\n            'student' : student_id,\n            'category' : category_id,\n            'correct' : correct,\n            'prediction' : projected_score,\n            'student_rating' : target_student['rating'][category_id],\n            'student_samples' : target_student['samples'],\n            'delta_student_rating' : delta_student,\n            'question_rating' : target_question['rating'],\n            'question_samples': target_question['samples'],\n            'delta_question_rating' : delta_question,\n        }\n\n        # Update Counts & Ratings\n\n        target_student['rating'][category_id] += delta_student\n        target_question['rating'] += delta_question\n\n        target_student['samples'] += 1\n        target_question['samples'] += 1\n\n\n        # Return output\n\n        return logged_output","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"user_ids = data['train'].user_id.unique()\nquestion_ids = question_data.index.values\n\nElo_Model = Elo()\nElo_Model.initialize( user_ids, question_ids, 188 ) # There are 188 unique categories","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Will take a bit to run\n\nbatch = []\nfull = []\n\nfor i,row in data['train'].iterrows():\n    \n    student_id = row['user_id']\n    question_id = str(row['content_id'])\n    correct = row['answered_correctly']\n    \n    if correct < 0 : continue\n    \n    question_ids = question_data_lookup[question_id]\n    \n    for question,category in question_ids:\n        _log = Elo_Model.process_training_example( student_id, question, category, correct )\n        batch.append( _log )\n        \n    if ( len(batch) > 50000 ):\n        full.extend(batch)\n        batch = []\n        print( len(full), end='\\r')\n        \n    if ( len(full) > 1000000):\n        break","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"# The output results of the first million training examples\ntrain_data = pd.DataFrame(full)\ntrain_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Most questions got slightly negative ratings\nplt.hist( train_data['question_rating'] )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Most students got very little rating change\nplt.hist( train_data['student_rating'] )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#The Range of predictions by the model\nplt.hist(train_data['prediction'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Questions seem to migrate along paths, thats kindof interesting. Could explore more why that happens\nplt.scatter(train_data['question_samples'],train_data['question_rating'] )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# See how our predictions line up with the actual student scores\ntrain_data['prediction_bucket'] = train_data['prediction'].round(2)\ntrain_sample = train_data[['prediction_bucket', 'correct']].groupby('prediction_bucket').mean()\ntrain_sample","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We seem to predict actual student scores shockingly well\nplt.plot(np.arange(0.2,1,0.01),np.arange(0.2,1,0.01), color='r')\nplt.scatter(train_sample.index, train_sample['correct'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Do rolling averages to see what we are really predicting\ntrain_data['score'] = train_data['prediction'].round(0)\ntrain_data['accuracy'] = (train_data['score'] == train_data['correct']) * 1\ntrain_data['accuracy_rolling'] = train_data['accuracy'].rolling(window=5000).mean()\ntrain_data['correct_rolling'] = train_data['correct'].rolling(window=5000).mean()\ntrain_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# And the model ultimantly is just predicting the average student scores as the prediction. :( sad\nplt.figure(figsize=(20,10))\n\nplt.plot(train_data['accuracy_rolling'], label='model_prediction')\nplt.plot(train_data['correct_rolling'], label='student_correctness')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# How good are our predictions\ntrain_data['accuracy'].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# How good the students did\ntrain_data['correct'].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Overall, kinda bad, would love feedback tho","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}