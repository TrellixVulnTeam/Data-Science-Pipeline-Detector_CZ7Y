{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,20))  # 创建一个20 * 20 英寸的图像\nplt.subplots_adjust(bottom=.05,top=.9,left=.05,right=.95)\n \nplt.subplot(421)\nplt.title(\"One informative feature, one cluster per class\",fontsize='small')\nX1,Y1= make_classification(n_samples=1000,n_features=2,n_redundant=0,n_informative=1,n_clusters_per_class=1)\nplt.scatter(X1[:,0],X1[:,1],marker='o',c=Y1)\n \nplt.subplot(422)\nplt.title(\"Two informative features, one cluster per class\", fontsize='small')\nX2,Y2 = make_classification(n_samples=1000,n_features=2,n_redundant=0,n_informative=2)\nplt.scatter(X2[:,0],X2[:,1],marker='o',c=Y2)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.compat.v1.disable_eager_execution()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def init_weights(shape):\n    return tf.Variable(tf.compat.v1.random_normal(shape, stddev=0.01))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model(X, w):\n    return tf.matmul(X, w) # notice we use the same model as linear regression, this is because there is a baked in cost function which performs softmax and cross entropy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X =  tf.compat.v1.placeholder(tf.float32, [None, 2]) # create symbolic variables\nY =  tf.compat.v1.placeholder(tf.float32, [None])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w = init_weights([2, 1]) # like in logits regression, we need a shared variable weight matrix for logistic regression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"py_x = model(X, w)\npy_x = tf.squeeze(py_x,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"py_x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cost = tf.reduce_mean(\n    tf.nn.sigmoid_cross_entropy_with_logits(logits=py_x, labels=Y)\n) # compute mean cross entropy (softmax is applied internally)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_op = tf.compat.v1.train.GradientDescentOptimizer(0.05).minimize(cost) # construct optimizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"one = tf.ones_like(py_x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"zero = tf.zeros_like(py_x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_op =  tf.where(py_x < 0.5, x=zero, y=one) # at predict time, evaluate the argmax of the logistic regression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X1,Y1= make_classification(n_samples=1000,n_features=2,n_redundant=0,n_informative=1,n_clusters_per_class=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(X1[:,0],X1[:,1],marker='o',c=Y1)\n ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trX, teX, trY, teY = train_test_split(X1,Y1,test_size=0.2,random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trX.shape, teX.shape, trY.shape, teY.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Launch the graph in a session\nwith tf.compat.v1.Session() as sess:\n    # you need to initialize all variables\n    tf.compat.v1.global_variables_initializer().run()\n\n    for i in range(100):\n        for start, end in zip(range(0, len(trX), 128), range(128, len(trX)+1, 128)):\n            sess.run(train_op, feed_dict={X: trX[start:end], Y: trY[start:end]})\n        print(i, np.mean(np.where(teY < 0.5, 0, 1)  ==\n                         sess.run(predict_op, feed_dict={X: teX})))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!head -n 5 /kaggle/input/riiid-test-answer-prediction/questions.csv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport lightgbm as lgb\nfrom sklearn.metrics import roc_auc_score\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dir_path = '/kaggle/input/riiid-test-answer-prediction/'\nfile_train = 'train.csv'\nfile_questions = 'questions.csv'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nrows =  100 * 10000\n# nrows = None\ntrain = pd.read_csv(\n                    dir_path + file_train, \n                    nrows=nrows, \n                    usecols=['row_id', 'timestamp', 'user_id', 'content_id', \n                             'content_type_id', 'task_container_id', 'answered_correctly',\n                            'prior_question_elapsed_time','prior_question_had_explanation'],\n                    dtype={\n                            'row_id': 'int64',\n                            'timestamp': 'int64',\n                            'user_id': 'int32',\n                            'content_id': 'int16',\n                            'content_type_id': 'int8',\n                            'task_container_id': 'int8',\n                            'answered_correctly': 'int8',\n                            'prior_question_elapsed_time': 'float32',\n                            'prior_question_had_explanation': 'str'\n                        }\n                   )\nquestions = pd.read_csv(\n                        dir_path + file_questions, \n                        nrows=nrows,\n                        usecols=['question_id','bundle_id','part'], \n                        dtype={\n                           'question_id': 'int16',\n                           'bundle_id': 'int16',\n                           'part': 'int8',\n                       }\n                    )\ntrain['prior_question_had_explanation'] = train['prior_question_had_explanation'].map({'True':1,'False':0}).fillna(-1).astype(np.int8)\ntrain = train[train['content_type_id']==0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 压缩内存\nmax_num = 100\ntrain = train.groupby(['user_id']).tail(max_num)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.merge(\n        left=train,\n        right=questions,\n        how='left',\n        left_on='content_id',\n        right_on='question_id'\n        )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class cat_deal:\n    def __init__(self):\n        self.max_len = 0\n        self.dict_map = {}\n    \n    def fit(self, cat_list):\n        index = 1 \n        for cat_i in cat_list:\n            if cat_i not in self.dict_map:\n                self.dict_map[cat_i] = index\n                index += 1\n        self.max_len = index + 1\n        \n    def transform(self, cat_list):\n        cat_transform_list = []\n        for cat_i in cat_list:\n            if cat_i in self.dict_map:\n                cat_transform_list.append(self.dict_map[cat_i])\n            else:\n                cat_transform_list.append(0)\n        return cat_transform_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class float_deal:\n    def __init__(self):\n        self.max = 0\n        self.min = 0\n        self.max_min = 0 \n        \n    def fit(self, float_list):\n        for float_i in float_list:\n            if float_i < self.min:\n                self.min = float_i\n            if float_i > self.max:\n                self.max = float_i\n        self.max_min = self.max - self.min\n        \n    def transform(self, float_list):\n        float_transform_list = []\n        for float_i in float_list:\n            if float_i < self.min:\n                float_transform_list.append(0)\n            elif float_i > self.max:\n                float_transform_list.append(1)\n            else:\n                float_transform_list.append(float_i/self.max_min)\n        return float_transform_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dict_cat_class = {}\nfor columns in ['user_id','content_id',\\\n                'task_container_id','prior_question_had_explanation',\\\n                'bundle_id','part']:\n    dict_cat_class[columns] = cat_deal()\n    dict_cat_class[columns].fit(train[columns])\n\n    train[columns] = dict_cat_class[columns].transform(train[columns])\n    print(columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dict_float_class = {}\nfor columns in ['timestamp','prior_question_elapsed_time']:\n    dict_float_class[columns] = float_deal()\n    dict_float_class[columns].fit(train[columns])\n    \n    train[columns] = dict_float_class[columns].transform(train[columns])\n    print(columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def squeeze(embedding):\n    embedding = tf.squeeze(embedding,axis=1)\n    return embedding\ndef concat(embedding_list):\n    embedding = tf.concat(embedding_list, axis=1)\n    return embedding\ndef multiply(multi_x_y):\n    multi_x = multi_x_y[0]\n    multi_y = multi_x_y[1]\n    multi_x_y = tf.multiply(multi_x, multi_y)\n    return multi_x_y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_timestamp = tf.keras.Input(shape=(1,))\ninput_prior_question_elapsed_time = tf.keras.Input(shape=(1,))\n\n# input int\ninput_user = tf.keras.Input(shape=(1,))\ninput_content = tf.keras.Input(shape=(1,))\ninput_task_container = tf.keras.Input(shape=(1,))\ninput_prior_question_had_explanation = tf.keras.Input(shape=(1,))\ninput_bundle = tf.keras.Input(shape=(1,))\ninput_part = tf.keras.Input(shape=(1,))\n\ninputs = [input_timestamp,input_prior_question_elapsed_time,\\\n         input_user,input_content,\\\n         input_task_container,input_prior_question_had_explanation,\\\n         input_bundle,input_part]\n# inputs = tf.keras.layers.Lambda(concat)(inputs)\n\n# input session\n# input_tags = Input(shape=(1))\n\n# embedding float\nembedding_timestamp = tf.keras.layers.Dense(64, activation=tf.nn.sigmoid)(input_timestamp)\nembedding_prior_question_elapsed_time = tf.keras.layers.Dense(64, activation=tf.nn.sigmoid)(input_prior_question_elapsed_time)\n\n# embedding int \nembedding_user = tf.keras.layers.Embedding(dict_cat_class['user_id'].max_len,\n                                           64, input_length=1)(input_user)\nembedding_user = tf.keras.layers.Lambda(squeeze)(embedding_user)\n\nembedding_content = tf.keras.layers.Embedding(dict_cat_class['content_id'].max_len,\n                                              64, input_length=1)(input_content)\nembedding_content = tf.keras.layers.Lambda(squeeze)(embedding_content)\n\nembedding_task_container = tf.keras.layers.Embedding(dict_cat_class['task_container_id'].max_len,\n                                                     64, input_length=1)(input_task_container)\nembedding_task_container = tf.keras.layers.Lambda(squeeze)(embedding_task_container)\n\nembedding_prior_question_had_explanation = tf.keras.layers.Embedding(dict_cat_class['prior_question_had_explanation'].max_len, \n                                                                     64, input_length=1)(input_prior_question_had_explanation)\nembedding_prior_question_had_explanation = tf.keras.layers.Lambda(squeeze)(embedding_prior_question_had_explanation)\n\nembedding_bundle = tf.keras.layers.Embedding(dict_cat_class['bundle_id'].max_len,\n                                             64, input_length=1)(input_bundle)\nembedding_bundle = tf.keras.layers.Lambda(squeeze)(embedding_bundle)\n\nembedding_part = tf.keras.layers.Embedding(dict_cat_class['part'].max_len,\n                                           64, input_length=1)(input_part)\nembedding_part = tf.keras.layers.Lambda(squeeze)(embedding_part)\n\nembedding_all = [embedding_timestamp,embedding_prior_question_elapsed_time,\\\n                embedding_user, embedding_content, embedding_task_container,\\\n                embedding_prior_question_had_explanation, embedding_bundle, embedding_part]\n\n\nnffm1, nffm2 = [], []\nfor i, embedding_i in enumerate(embedding_all):\n    for j, embedding_j in enumerate(embedding_all):\n        if i > j:\n            nffm1.append(embedding_i), nffm2.append(embedding_j)\nnffm1_layer = tf.keras.layers.Lambda(concat)(nffm1)\nnffm2_layer = tf.keras.layers.Lambda(concat)(nffm2)     \n\nnffm_all = tf.keras.layers.Lambda(multiply)([nffm1_layer,nffm2_layer])\n    \nlogit = tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)(nffm_all)\n\n\nmodel = tf.keras.models.Model(inputs=inputs, outputs=logit)\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['binary_crossentropy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nffm1, nffm2 = [], []\nfor i, embedding_i in enumerate(embedding_all):\n    for j, embedding_j in enumerate(embedding_all):\n        if i > j:\n            nffm1.append(embedding_i), nffm2.append(embedding_j)\nnffm1_layer = tf.keras.layers.Lambda(concat)(nffm1)\nnffm2_layer = tf.keras.layers.Lambda(concat)(nffm2) \n\nnffm_all = tf.keras.layers.Lambda(multiply)([nffm1_layer,nffm2_layer])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nffm1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nffm2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nffm1, nffm2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nffm1_layer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plateau = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n                            verbose=0,\n                            mode='min',\n                            factor=0.1,\n                            patience=6)\n\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n                               verbose=0,\n                               mode='min',\n                               patience=10)\n\n# 保存\ncheckpoint = tf.keras.callbacks.ModelCheckpoint(f'fold.h5',\n                             monitor='val_loss',\n                             verbose=0,\n                             mode='min',\n                             save_best_only=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid = pd.DataFrame()\nfor i in range(6):\n    \n    # 获取训练标签数据\n    last_records = train.drop_duplicates('user_id', keep='last')\n    \n    # 获取训练标签以前的数据\n    map__last_records__user_row = dict(zip(last_records['user_id'],last_records['row_id']))\n    train['filter_row'] = train['user_id'].map(map__last_records__user_row)\n    train = train[train['row_id']<train['filter_row']]\n\n    # 特征加入训练集\n    valid = valid.append(last_records)\n    print(len(valid))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features_columns = ['timestamp','prior_question_elapsed_time',\\\n                    'user_id','content_id',\\\n                    'task_container_id','prior_question_had_explanation',\\\n                    'bundle_id','part']\n\nX_test, y_test = [valid[columns].values for columns in features_columns], valid['answered_correctly'].values\n# del valid\n\nX_train, y_train = [train[columns].values for columns in features_columns], train['answered_correctly'].values\n# del train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('ok')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train, y_train,\n          epochs=10,\n          batch_size=512 * 500 * 2,\n          verbose=1,\n          shuffle=True,\n          validation_data=(X_test, y_test),\n          callbacks=[plateau, early_stopping, checkpoint])\n\ny_test_proba = model.predict(X_test, verbose=0, batch_size=512)\nauc = roc_auc_score(y_test, y_test_proba)\nprint(auc)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_proba = model.predict(X_test, verbose=0, batch_size=512)\nauc = roc_auc_score(y_test, y_test_proba)\nprint(auc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train, y_train,\n          epochs=1,\n          batch_size=512 * 500,\n          verbose=1,\n          shuffle=True,\n          validation_data=(X_test, y_test),\n          callbacks=[plateau, early_stopping, checkpoint])\n\ny_test_proba = model.predict(X_test, verbose=0, batch_size=512)\nauc = roc_auc_score(y_test, y_test_proba)\nprint(auc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_proba = model.predict(X_test, verbose=0, batch_size=512)\nauc = roc_auc_score(y_test, y_test_proba)\nprint(auc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train, y_train,\n          epochs=1,\n          batch_size=512 * 100,\n          verbose=1,\n          shuffle=True,\n          validation_data=(X_test, y_test),\n          callbacks=[plateau, early_stopping, checkpoint])\n\ny_test_proba = model.predict(X_test, verbose=0, batch_size=512)\nauc = roc_auc_score(y_test, y_test_proba)\nprint(auc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train, y_train,\n          epochs=1,\n          batch_size=512 * 500,\n          verbose=1,\n          shuffle=True,\n          validation_data=(X_test, y_test),\n          callbacks=[plateau, early_stopping, checkpoint])\n\ny_test_proba = model.predict(X_test, verbose=0, batch_size=512)\nauc = roc_auc_score(y_test, y_test_proba)\nprint(auc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train, y_train,\n          epochs=2,\n          batch_size=512 * 500,\n          verbose=1,\n          shuffle=True,\n          validation_data=(X_test, y_test),\n          callbacks=[plateau, early_stopping, checkpoint])\n\ny_test_proba = model.predict(X_test, verbose=0, batch_size=512)\nauc = roc_auc_score(y_test, y_test_proba)\nprint(auc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train, y_train,\n          epochs=2,\n          batch_size=512 * 500,\n          verbose=1,\n          shuffle=True,\n          validation_data=(X_test, y_test),\n          callbacks=[plateau, early_stopping, checkpoint])\n\ny_test_proba = model.predict(X_test, verbose=0, batch_size=512)\nauc = roc_auc_score(y_test, y_test_proba)\nprint(auc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train, y_train,\n          epochs=1,\n          batch_size=512 ,\n          verbose=1,\n          shuffle=True,\n          validation_data=(X_test, y_test),\n          callbacks=[plateau, early_stopping, checkpoint])\n\ny_test_proba = model.predict(X_test, verbose=0, batch_size=512)\nauc = roc_auc_score(y_test, y_test_proba)\nprint(auc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import riiideducation\nenv = riiideducation.make_env()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\niter_test = env.iter_test()\n\nfor (test_df, sample_prediction_df) in iter_test:\n\n    test_df['prior_question_had_explanation'] = test_df['prior_question_had_explanation'].map({'True':1,'False':0}).fillna(-1).astype(np.int8)\n\n    test_df = pd.merge(\n        left=test_df,\n        right=questions,\n        how='left',\n        left_on='content_id',\n        right_on='question_id'\n        )\n\n    test_df = test_df.fillna(0)\n\n\n    for columns in ['user_id','content_id',\\\n                    'task_container_id','prior_question_had_explanation',\\\n                    'bundle_id','part']:\n\n        test_df[columns] = dict_cat_class[columns].transform(test_df[columns])\n        print(columns)\n\n\n    for columns in ['timestamp','prior_question_elapsed_time']:\n\n        test_df[columns] = dict_float_class[columns].transform(test_df[columns])\n        print(columns)\n\n    X_test = [test_df[columns].values for columns in features_columns]\n\n    test_df['answered_correctly'] =  model.predict(X_test, verbose=0, batch_size=512)\n    env.predict(test_df.loc[test_df['content_type_id'] == 0, ['row_id', 'answered_correctly']])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}