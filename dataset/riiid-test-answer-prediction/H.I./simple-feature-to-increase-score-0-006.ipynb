{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Slightly change \"user_correctness\""},{"metadata":{},"cell_type":"markdown","source":"Here, 'user_correctness' is a feature that measures the smartness of each user. Probably in the published notebook, it was decided as follows."},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"train_df['user_correctness'] = train_df.groupby('user_id').agg({'answered_correctly':'mean'})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"but the following minor change will increase the score significantly.\n\n"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"content_agg = train_df.groupby('content_id').agg({'answered_correctly':'mean'})\ntrain_df['content_correctness'] = train_df['content_id'].map(content_agg['mean'])\ntrain_df['score_diff'] = train_df['answered_correctly'] - train_df['content_correctness'] \ntrain_df['user_correctness'] = train_df.groupby('user_id').agg({'score_diff':'mean'})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This feature is not evaluated so much when a question with a high correct answer rate is answered correctly, but is highly evaluated when a question with a low correct answer rate is answered correctly.\n\n\ne.g.) In case the user answered correctly\n* content_correctness : 0.8 -> score_diff : 0.2\n \n* content_correctness : 0.4 -> score_diff : 0.6\n\n\nThis makes it possible to create an average that takes into account the difficulty of the questions, rather than a simple binary average."},{"metadata":{},"cell_type":"markdown","source":"Let's take a quick look at the difference between traditional \"user_correctness\" and proposed \"user_correctness\".\n\nNote that other features that I was using are included, but I am using almost same except for user_correctness."},{"metadata":{},"cell_type":"markdown","source":"# Preparation"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Import the Rapids suite here - takes abot 1.5 mins\n\nimport sys\n!cp ../input/rapids/rapids.0.17.0 /opt/conda/envs/rapids.tar.gz\n!cd /opt/conda/envs/ && tar -xzvf rapids.tar.gz > /dev/null\nsys.path = [\"/opt/conda/envs/rapids/lib/python3.7/site-packages\"] + sys.path\nsys.path = [\"/opt/conda/envs/rapids/lib/python3.7\"] + sys.path\nsys.path = [\"/opt/conda/envs/rapids/lib\"] + sys.path \n!cp /opt/conda/envs/rapids/lib/libxgboost.so /opt/conda/lib/","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.execute_input":"2020-10-08T17:18:58.89824Z","iopub.status.busy":"2020-10-08T17:18:58.897497Z","iopub.status.idle":"2020-10-08T17:19:40.844954Z","shell.execute_reply":"2020-10-08T17:19:40.845812Z"},"papermill":{"duration":42.032589,"end_time":"2020-10-08T17:19:40.846019","exception":false,"start_time":"2020-10-08T17:18:58.81343","status":"completed"},"tags":[],"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import riiideducation\nimport pandas as pd\nimport numpy as np\nimport cudf\nimport cupy\nimport gc\nimport pickle\nimport xgboost\nfrom cuml.metrics import roc_auc_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"features = [\n    'lagtime',\n    'lagtime2',\n    'lagtime3',\n    'content_eqet',\n    'user_correctness',\n    'user_correct_cumsum',\n    'part_user_correctness',\n    'part_user_correct_cumcount',\n    'part_user_correct_cumsum',\n    'content_correctness',\n    'content_count',\n    'content_sum',\n    'attempt_no',\n    'part',\n    'part_correctness_mean',\n    'tags1',\n    'tags1_correctness_mean',\n    'bundle_id',\n    'explanation_mean', \n]\n\ntarget = 'answered_correctly'\n\nparams = {\n    'max_depth' : 8,\n    'max_leaves' : 350,\n    'max_bin':800,\n    'eta':0.1,\n    'min_child_weight':0.03,\n    'lambda':0.6,\n    'alpha':0.4,\n    'eval_metric': 'auc',\n    'tree_method' : 'gpu_hist',\n    'objective' : 'binary:logistic',\n    'grow_policy' : 'lossguide'\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Traditional user_correctness"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train_df = cudf.read_csv('../input/riiiddata2/data.csv')\nvalid_df = cudf.read_csv('../input/riiiddata2/valid.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"dtrain = xgboost.DMatrix(train_df[features], label=train_df[target])\ndvalid = xgboost.DMatrix(valid_df[features], label=valid_df[target])\n\n# Create & Train the model\nmodel = xgboost.train(params,\n                      dtrain = dtrain,\n                      evals = [(dtrain, 'train'),(dvalid, 'eval')],\n                      verbose_eval = 100,\n                      num_boost_round = 10000,\n                      early_stopping_rounds = 10,\n                     )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_auc_score(valid_df[target].astype('int32'),model.predict(dvalid))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### eval-auc:0.783..."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"del train_df\ndel valid_df\ndel dtrain\ndel dvalid\n_=gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Proposed user_correctness"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train_df = cudf.read_csv('../input/riiidlastdata/data.csv')\nvalid_df = cudf.read_csv('../input/riiidlastdata/valid.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"dtrain = xgboost.DMatrix(train_df[features], label=train_df[target])\ndvalid = xgboost.DMatrix(valid_df[features], label=valid_df[target])\n\n# Create & Train the model\nmodel = xgboost.train(params,\n                      dtrain = dtrain,\n                      evals = [(dtrain, 'train'),(dvalid, 'eval')],\n                      verbose_eval = 100,\n                      num_boost_round = 10000,\n                      early_stopping_rounds = 10,\n                     )\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### eval-auc:0.789..."},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_auc_score(valid_df[target].astype('int32'),model.predict(dvalid))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## As a result, increased score +0.006! (0.783 -> 0.789)\n\nthank you for watching."},{"metadata":{},"cell_type":"markdown","source":"NOTE: I wrote the proposed feature with an emphasis on readability, but be careful of leaks in practice."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}