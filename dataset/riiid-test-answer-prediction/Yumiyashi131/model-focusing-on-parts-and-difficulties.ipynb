{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Welcome!** Here is a baseline model for the Riiid challenge explained:"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import riiideducation\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_auc_score\nimport lightgbm as lgb\nenv = riiideducation.make_env()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dataset for training exceeds the RAM, if you do not use Google Cloud Storage. The dataset for testing, on the other hand, cannot be accessed directly, but the organisers of this competition provide a module for handling the data in batches. It's explained in this [Notebook](https://www.kaggle.com/sohier/competition-api-detailed-introduction). However, there are also more efficient ways to download and store the training data than csv to pandas(See this [Notebook](https://www.kaggle.com/rohanrao/riiid-with-blazing-fast-rid)). Still, we simply resort to using csv to pandas: We load the dataset that contains statistics on one specific answer given by a user to a question. Unfortunately, there are users in the test set for which we do not have data in this dataset:"},{"metadata":{"trusted":true},"cell_type":"code","source":"group3 = pd.read_csv(\"../input/flatten/group3.csv\")\nquestion2 = pd.read_csv(\"../input/flatten/question2.csv\",usecols=[1,2,3,4,5])\nresults_u2_final = pd.read_csv(\"../input/flatten/results_u2_final.csv\",usecols = [1,2])\nresults_u_final = pd.read_csv(\"../input/flatten/results_u_final.csv\",usecols = [1,2,3])\n#diff_score = pd.read_csv(\"../input/for-predicton-6/diff_score.csv\",usecols = [1,2,3])\n#part_score = pd.read_csv(\"../input/for-predicton-6/part_score.csv\",usecols = [1,2,3])\ndp = pd.read_csv(\"../input/flatten/dp.csv\",usecols = [1,2,3,4,5,6,7,8,9,10,11,12])\ncolumns = dp.columns\ndp = dp.drop(columns = \"part_score_7\")\ndp.columns = columns[1:]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_u_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#question2 = question2.astype({\"quest_pct\":\"float16\",'content_id':\"category\"})\n#group3 = group3.astype({\"avg_questions\":\"float32\",\"avg_questions_seen\":\"float32\",'task_container_id':\"category\"})\n#results_u_final = results_u_final.astype({\"answered_correctly_user\":\"float32\",\"answered_user\":\"float32\",\"user_id\":\"category\"})\n#results_u2_final = results_u2_final.astype({\"user_id\":\"category\"})\n#part_score = part_score.astype({\"user_id\":\"category\",\"part\":\"category\",\"part_score\":\"float32\"})\n#diff_score = diff_score.astype({\"user_id\":\"category\",\"difficulty\":\"category\",\"diff_score\":\"float32\"})\n\n#question2 = question2.astype({\"quest_pct\":\"float16\",'question_id':\"category\"})\n#group3 = group3.astype({\"avg_questions\":\"float32\",\"avg_questions_seen\":\"float32\",'task_container_id':\"category\"})\n#results_u_final = results_u_final.astype({\"answered_correctly_user\":\"float32\",\"answered_user\":\"float32\",\"user_id\":\"category\"})\n#results_u2_final = results_u2_final.astype({\"user_id\":\"category\"})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prior_mean_user = 0.5664264045515732\ncontent_mean = 0.7094600658138017\nelapsed_mean = 13238.587890625","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#import pickle\n \n#lb_make = pickle.load(open('../input/for-predicton-6/lb_make.pickle', 'rb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = lgb.Booster(model_file='../input/flatten/lgb_classifier_1.txt')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#diffs = [0.165512,0.341864,0.544930,0.741148,0.898677]\n#parts = [0.815105,0.745296,0.744598,0.724918,0.666118,0.699596,0.717958]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_u3_final = pd.merge(results_u_final,results_u2_final,how=\"outer\",on=\"user_id\")\nresults_u3_final = pd.merge(results_u3_final,dp,how=\"outer\",on=\"user_id\")\nmedians = results_u3_final.median()\nmedians[\"user_id\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predict"},{"metadata":{},"cell_type":"markdown","source":"We create an iterator of the test set using the function provided by the compition organiser. For each element in this iterator, we do the following: 1 We add the features that we computed, 2 We replace missing data in the same way that we did it in the training set, 3 We predict the target, and 4 We submit the predicitions with the function that is provided by the compition organisers:"},{"metadata":{"trusted":true},"cell_type":"code","source":"iter_test = env.iter_test()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"def fillpart(data):\n    def getpartaverage(i):\n        return parts[i-1]\n\n    _data = data.copy()\n    _data.loc['part_score',:] = _data.part.map(getpartaverage)\n    return _data"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"def filldiff(data):\n    def getdiffaverage(i):\n        return diffs[i-1]\n\n    _data = data.copy()\n    _data.loc['part_score',:] = _data.part.map(getdiffaverage)\n    return _data"},{"metadata":{"trusted":true},"cell_type":"code","source":"for (test_df, sample_prediction_df) in iter_test:\n    \n\n    test_df['task_container_id'] = test_df.task_container_id.mask(test_df.task_container_id > 9999, 9999)\n    test_df = pd.merge(test_df, group3, left_on=['task_container_id'], right_index= True, how=\"left\")\n    test_df = pd.merge(test_df, question2, left_on = 'content_id', right_on = 'question_id', how = 'left')\n    test_df = pd.merge(test_df, results_u3_final, on = 'user_id', how = 'left')\n\n\n    test_df['explanation_mean_user'].fillna(prior_mean_user,  inplace=True)\n    test_df['answered_correctly_user'].fillna(0.65,  inplace=True)\n    test_df['quest_pct'].fillna(content_mean,  inplace=True)\n\n    test_df['part'].fillna(5, inplace = True)\n    test_df['avg_questions_seen'].fillna(1, inplace = True)\n    test_df['prior_question_elapsed_time'].fillna(elapsed_mean, inplace = True)\n    test_df['difficulty'].fillna(3,inplace=True)\n    test_df[columns[1:]].fillna(medians,inplace=True)\n\n    test_df['answered_correctly'] =  model.predict(test_df[['answered_correctly_user', 'explanation_mean_user', 'quest_pct', 'avg_questions_seen',\n       'prior_question_elapsed_time', 'part', 'difficulty', \"diff_score_0\",\"diff_score_1\",\"diff_score_2\",\"part_score_1\",\"part_score_2\",\"part_score_3\",\n              \"part_score_4\",\"part_score_5\",\"part_score_6\",\"part_score_7\"]])\n    env.predict(test_df.loc[test_df['content_type_id'] == 0, ['row_id', 'answered_correctly']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(a)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Acknowledgement\nI am grateful to Takamotoki for inspiring me with this notebook: https://www.kaggle.com/takamotoki/lgbm-iii-part2"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}