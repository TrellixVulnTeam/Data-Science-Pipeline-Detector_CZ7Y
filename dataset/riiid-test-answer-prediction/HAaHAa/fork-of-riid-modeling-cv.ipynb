{"cells":[{"metadata":{},"cell_type":"markdown","source":"V5:FEATURE_COLS = ['row_id', 'performance','question_average','user_id','content_id', 'content_type_id', 'prior_question_elapsed_time', 'prior_question_had_explanation']\n"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Imports\nimport numpy as np       #Numpy for numerical computations\nimport pandas as pd      #Pandas for data manipulations\nimport riiideducation    #Package for the competition API\nimport seaborn as sns    #Seaborn for data vizualisation\nimport os\nimport gc\nfrom tqdm.notebook import tqdm\n\n\n#Import data\nfor dirname, _, filenames in os.walk('/kaggle/input/riiid-test-answer-prediction'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Reading data\nfull_train = pd.read_pickle(\"../input/riiid-train-data-multiple-formats/riiid_train.pkl.gzip\")\nquestions = pd.read_csv('/kaggle/input/riiid-test-answer-prediction/questions.csv')\nlectures = pd.read_csv('/kaggle/input/riiid-test-answer-prediction/lectures.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# A first glance at the data\n\n## Train set\n\nThe columns in the train file are described as:\n* row_id: (int64) ID code for the row.\n* timestamp: (int64) the time in milliseconds between this user interaction and the first event completion from that user.\n* user_id: (int32) ID code for the user.\n* content_id: (int16) ID code for the user interaction\n* content_type_id: (int8) 0 if the event was a question being posed to the user, 1 if the event was the user watching a lecture.\n* task_container_id: (int16) Id code for the batch of questions or lectures. For example, a user might see three questions in a row before seeing the explanations for any of them. Those three would all share a task_container_id.\n* user_answer: (int8) the user's answer to the question, if any. Read -1 as null, for lectures.\n* answered_correctly: (int8) if the user responded correctly. Read -1 as null, for lectures.\n* prior_question_elapsed_time: (float32) The average time in milliseconds it took a user to answer each question in the previous question bundle, ignoring any lectures in between. Is null for a user's first question bundle or lecture. Note that the time is the average time a user took to solve each question in the previous bundle.\n* prior_question_had_explanation: (bool) Whether or not the user saw an explanation and the correct response(s) after answering the previous question bundle, ignoring any lectures in between. The value is shared across a single question bundle, and is null for a user's first question bundle or lecture. Typically the first several questions a user sees were part of an onboarding diagnostic test where they did not get any feedback"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_pickle('../input/riidcv/cv1_train.pickle')\ntest = pd.read_pickle('../input/riidcv/cv1_valid.pickle')\ntrain.shape,test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dictionnary for questions average\nquestion_average = pd.DataFrame(full_train.loc[full_train['content_type_id'] == 0].groupby(['content_id'])['answered_correctly'].mean()).rename(columns={'answered_correctly':'question_average'})\n#Dictionnary for questions count\nquestion_count = pd.DataFrame(full_train.loc[full_train['content_type_id'] == 0].groupby(['content_id']).size(),columns=['question_count'])\n#Joining average and count\nquestion_df = question_average.join(question_count)\n#Computing sum as product of average and count\nquestion_df['question_sum'] = question_df['question_average'] * question_df['question_count']\n#Joining the new dataframe with questions data, getting more columns\nquestion_df = question_df.join(questions,how='outer')[['question_average','question_count','question_sum']]\n#Filling with default value\nquestion_df['question_average'].fillna(0,inplace=True)\nquestion_df['question_count'].fillna(0,inplace=True)\nquestion_df['question_sum'].fillna(0,inplace=True)\n#Cleaning for memory management\ndel question_average,question_count\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.join(question_df,on=['content_id'], rsuffix='_question')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Cleaning full_train to keep only train and test set (as it is too big)\ndel full_train\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Joining the average mark for the question to the train data\ntrain = train.join(question_df,on=['content_id'], rsuffix='_question')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_memory_usage(train_data):\n    #Converting row_id to int32\n#     train_data['row_id'] = train_data['row_id'].astype('int32')\n    #Converting prior_question_had_explanation to boolean value and filling NA with False\n    train_data['prior_question_had_explanation'] = train_data['prior_question_had_explanation'].fillna(False).astype('bool')\n    #Converting timestamps to minutes\n#     train_data['timestamp'] = (train_data['timestamp'] / (1000 * 60)).astype('float32')\n    #Converting elapsed time to minutes\n#     train_data['prior_question_elapsed_time'] = (train_data['prior_question_elapsed_time'] / (1000 * 60)).astype('float32')\n    return train_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What we need is :\n\n1) A dictionnary for the users with :\n    a) The number of questions answered\n    b) The number of correct answers\n    c) The sum of the average correctness of answers for the questions answered\n    \n2) A dictionnary for the questions with :\n    a) The number of times the question has been asked\n    b) The average percentage of correct answers for the question\n\nThen we can compute the user performance as the average of the user - the average correctness for the question she/he was asked\n\nWe can update the user performance after a batch of questions in the following way :\n\n1) For each question, update the number of times the question is asked the average percentage of correct answers for the question\n\n2) For each user, add the average correctness of each question to the sum of average correctness, add the number of correct answers and the number of questions.\n\nRecompute the user performance : average of answers - average of others on same questions"},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_user = train.loc[train['content_type_id'] == False].groupby(['user_id'])['answered_correctly'].mean().mean()\nmean_question = train.loc[train['content_type_id'] == False].groupby(['content_id'])['answered_correctly'].mean().mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_user","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_question","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['user_shift_question'] = train.loc[train['content_type_id'] == False].groupby(['user_id'])['question_average'].shift()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['user_shift_question'] = train.loc[train['content_type_id'] == False].groupby(['user_id'])['question_average'].shift()\ncumulated_question = train.loc[train['content_type_id'] == False].groupby(['user_id'])['user_shift_question'].agg(['cumsum','cumcount'])\ntrain.loc[train['content_type_id'] == False,'average_past_questions'] = cumulated_question['cumsum'] / (cumulated_question['cumcount'] + 1)\ntrain.drop(columns=['user_shift_question'],inplace=True)\ntrain['average_past_questions'].fillna(mean_question,inplace=True)\ndel cumulated_question","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Computing the average for each user depending only on past events\ntrain['user_shift'] = train.loc[train['content_type_id'] == False].groupby(['user_id'])['answered_correctly'].shift()\ncumulated = train.loc[train['content_type_id'] == False].groupby(['user_id'])['user_shift'].agg(['cumsum', 'cumcount'])\ntrain.loc[train['content_type_id'] == False,'answered_correctly_user_average'] = cumulated['cumsum'] / cumulated['cumcount']\ntrain['answered_correctly_user_average'].fillna(mean_user,inplace=True)\ntrain.drop(columns=['user_shift'], inplace=True)\ndel cumulated","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dictionnary for user average\nuser_average = pd.DataFrame(train.loc[train['content_type_id'] == 0].groupby(['user_id'])['answered_correctly_user_average'].last()).rename(columns={'answered_correctly_user_average':'user_average'})\n#Dictionnary for user count\nuser_count = pd.DataFrame(train.loc[train['content_type_id'] == 0].groupby(['user_id']).size() - 1,columns=['user_count'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tmp = train.loc[train['content_type_id'] == False].groupby(['user_id']).mean()\n#Ici, on prend la moyenne globale vs la performance globale sur toutes les questions\n#Je voudrais faire la performance avant ces questions (= answered_correctly_user_average) et moyenne sur les questions déjà faites\ntrain['performance_before'] = train['answered_correctly_user_average'] - train['average_past_questions']\nuser_performance = pd.DataFrame(train.loc[train['content_type_id'] == 0].groupby(['user_id'])['performance_before'].last()).rename(columns={'performance_before':'performance'})\n# user_performance = pd.DataFrame(tmp['answered_correctly'] - tmp['question_average']).rename(columns={0:'performance'})\n# user_performance2 = pd.DataFrame(train['answered_correctly_user_average'] - train['average_past_questions']).rename(columns={0:'performance'})\ndel tmp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"user_df = user_performance.join(user_average).join(user_count)\nuser_df['user_sum'] = user_df['user_average'] * user_df['user_count']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A student had a lecture for the given question if the tag of the lecture is one of the tags of the question\n\ntag[lect] in tags[question]"},{"metadata":{"trusted":true},"cell_type":"code","source":"def question_average_sum_by_user(df,question_df):\n    my_dict = {}\n    group = df.groupby(['user_id'])\n    for user, val in group:\n        average_sum = 0.0\n        for row_index, row in val.iterrows():\n            if (row['content_type_id'] == False):\n                question_id = row['content_id']\n                question_average = question_df.at[question_id,'question_average']\n                average_sum += question_average\n    #         print(f'user = {user}, id = {question_id}, average = {question_average}, average_sum={average_sum}')\n        my_dict[user] = [average_sum]\n    return pd.DataFrame.from_dict(my_dict,orient='index',columns=['question_average_sum'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_answers_to_prior_df(current_df,prior_df):\n    prior_df_ = prior_df.copy()\n    if (prior_df.shape[0] > 0):\n        val = eval(current_df.iloc[0]['prior_group_answers_correct'])\n        if (len(val) == prior_df.shape[0]):\n            prior_df_['answered_correctly_response'] = val\n    return prior_df_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_question_df(prior_df,question_df):\n    \n    \n    if (prior_df.shape[0] == 0):\n        return question_df\n    \n    #Dictionnary for questions average\n    question_sum_prior = pd.DataFrame(prior_df.loc[prior_df['content_type_id'] == 0]\\\n                           .groupby(['content_id'])['answered_correctly_response'].sum())\\\n                           .rename(columns={'answered_correctly_response':'question_sum'})\n    \n    #Dictionnary for questions count\n    question_count_prior = pd.DataFrame(prior_df.loc[prior_df['content_type_id'] == 0]\\\n                             .groupby(['content_id']).size(),columns=['question_count'])\n    \n    #Joining the two previous dataframes in one\n    question_df = question_df.join(question_sum_prior,rsuffix='_previous').join(question_count_prior,rsuffix='_previous')\n    \n    #Filling null values\n    question_df['question_average'].fillna(0,inplace=True)\n    question_df['question_count'].fillna(0,inplace=True)\n    question_df['question_sum'].fillna(0,inplace=True)\n    question_df['question_sum_previous'].fillna(0,inplace=True)\n    question_df['question_count_previous'].fillna(0,inplace=True)\n\n    #Updating values\n    question_df['question_sum'] = question_df['question_sum'] + question_df['question_sum_previous']\n    question_df['question_count'] = question_df['question_count'] + question_df['question_count_previous']\n    question_df['question_average'] = question_df['question_sum'] / question_df['question_count']\n    question_df.drop(['question_count_previous','question_sum_previous'],inplace=True,axis=1)\n    \n    return question_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_user_df(prior_df,user_df,question_df):\n    \n    if (prior_df.shape[0] == 0):\n        return user_df\n    \n    #Dictionnary for user average\n    user_sum_prior = pd.DataFrame(prior_df.loc[prior_df['content_type_id'] == 0]\\\n                       .groupby(['user_id'])['answered_correctly_response'].sum())\\\n                       .rename(columns={'answered_correctly_response':'user_sum'})\n    \n    #Dictionnary for user count\n    user_count_prior = pd.DataFrame(prior_df.loc[prior_df['content_type_id'] == 0]\\\n                         .groupby(['user_id']).size(),columns=['user_count'])\n\n    #Joining the df with preexisting one\n    user_df = user_df.join(user_sum_prior,how='outer',rsuffix='_previous').join(user_count_prior,rsuffix='_previous')\n    \n    #Filling null values\n    user_df['performance'].fillna(0,inplace=True)\n    user_df['user_average'].fillna(0,inplace=True)\n    user_df['user_count'].fillna(0,inplace=True)\n    user_df['user_sum'].fillna(0,inplace=True)\n    user_df['user_count_previous'].fillna(0,inplace=True)\n    user_df['user_sum_previous'].fillna(0,inplace=True)\n    \n    #Computing the average of correct answers for the list of questions each user head in prior\n    user_df = user_df.join(question_average_sum_by_user(prior_df,question_df))\n    user_df['question_average_sum'].fillna(0,inplace=True)\n    \n    #Updating values\n    user_df['user_mean_performance'] = (user_df['user_sum'] - user_df['performance'] * user_df['user_count'] + user_df['question_average_sum']) / (user_df['user_count'] + user_df['user_count_previous'])\n    user_df['user_sum'] = user_df['user_sum'] + user_df['user_sum_previous']\n    user_df['user_count'] = user_df['user_count'] + user_df['user_count_previous']\n    user_df['user_average'] = user_df['user_sum'] / user_df['user_count']\n    user_df['performance'] = user_df['user_average'] - user_df['user_mean_performance']\n    user_df.drop(['user_sum_previous','user_count_previous','question_average_sum','user_mean_performance'],axis=1,inplace=True)\n    \n    return user_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Main code\nprior_df = pd.DataFrame()\ncurrent_df = pd.DataFrame()\nprior_df = add_answers_to_prior_df(current_df,prior_df)\nquestion_df = build_question_df(prior_df,question_df)\nuser_df = build_user_df(prior_df,user_df,question_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TARGET_COL = ['answered_correctly']\n#FEATURE_COLS = ['performance', 'question_average','answered_correctly_avg_u', 'answered_correctly_sum_u', 'count_u', 'answered_correctly_avg_c',  'prior_question_had_explanation', 'prior_question_elapsed_time']\n#FEATURE_COLS = ['row_id', 'performance', 'question_average']\nFEATURE_COLS = ['row_id', 'question_average', 'answered_correctly_sum_u', 'count_u', 'answered_correctly_avg_c',  'prior_question_had_explanation', 'prior_question_elapsed_time']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_user_feats(df, answered_correctly_sum_u_dict, count_u_dict):\n    acsu = np.zeros(len(df), dtype=np.int32)\n    cu = np.zeros(len(df), dtype=np.int32)\n    for cnt,row in enumerate(tqdm(df[['user_id','answered_correctly']].values)):\n        acsu[cnt] = answered_correctly_sum_u_dict[row[0]]\n        cu[cnt] = count_u_dict[row[0]]\n        answered_correctly_sum_u_dict[row[0]] += row[1]\n        count_u_dict[row[0]] += 1\n    user_feats_df = pd.DataFrame({'answered_correctly_sum_u':acsu, 'count_u':cu})\n    user_feats_df['answered_correctly_avg_u'] = user_feats_df['answered_correctly_sum_u'] / user_feats_df['count_u']\n    df = pd.concat([df, user_feats_df], axis=1)\n    return df\n\ndef add_user_feats_without_update(df, answered_correctly_sum_u_dict, count_u_dict):\n    acsu = np.zeros(len(df), dtype=np.int32)\n    cu = np.zeros(len(df), dtype=np.int32)\n    for cnt,row in enumerate(df[['user_id']].values):\n        acsu[cnt] = answered_correctly_sum_u_dict[row[0]]\n        cu[cnt] = count_u_dict[row[0]]\n    user_feats_df = pd.DataFrame({'answered_correctly_sum_u':acsu, 'count_u':cu})\n    user_feats_df['answered_correctly_avg_u'] = user_feats_df['answered_correctly_sum_u'] / user_feats_df['count_u']\n    df = pd.concat([df, user_feats_df], axis=1)\n    return df\n\ndef update_user_feats(df, answered_correctly_sum_u_dict, count_u_dict):\n    for row in df[['user_id','answered_correctly','content_type_id']].values:\n        if row[2] == 0:\n            answered_correctly_sum_u_dict[row[0]] += row[1]\n            count_u_dict[row[0]] += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import defaultdict\ndef data_transform(df, is_training = True, is_validation = True): \n    #Casting types to reduce memory usage\n#     df = reduce_memory_usage(df)\n    \n    #Dropping columns from the beginning to accelerate further computations\n#     df.drop(['task_container_id'],axis=1,inplace=True)\n\n    #Joining average marks for questions with the main dataframe\n    df = df.join(question_df['question_average'],on=['content_id'],rsuffix='_question_average')\n    \n    df = df.join(user_df[['performance','user_average', 'user_count']],on=['user_id'],rsuffix='_right')\n    \n    df['is_beginning'] = df['user_count'] < 20\n    df = df.loc[df.content_type_id == False].reset_index(drop=True)\n\n# answered correctly average for each content\n    content_df = train_out[['content_id','answered_correctly']].groupby(['content_id']).agg(['mean']).reset_index()\n    content_df.columns = ['content_id', 'answered_correctly_avg_c']\n    df = pd.merge(df, content_df, on=['content_id'], how=\"left\")\n    answered_correctly_sum_u_dict = defaultdict(int)\n    count_u_dict = defaultdict(int)\n    df = add_user_feats(df, answered_correctly_sum_u_dict, count_u_dict)\n\n    #Recasting after join\n#     df['prior_question_had_explanation'] = df['prior_question_had_explanation'].astype('bool')\n    prior_question_elapsed_time_mean = df.prior_question_elapsed_time.dropna().values.mean()\n    df['prior_question_elapsed_time_mean'] = df.prior_question_elapsed_time.fillna(prior_question_elapsed_time_mean)\n    df = df.loc[df['content_type_id'] == False]\n    \n    df['prior_question_had_explanation'] = df.prior_question_had_explanation.fillna(False).astype('int8')\n    prior_question_elapsed_time_mean = df.prior_question_elapsed_time.dropna().values.mean()\n    df['prior_question_elapsed_time_mean'] = df.prior_question_elapsed_time.fillna(prior_question_elapsed_time_mean)\n    if is_training or is_validation:\n        df = df[FEATURE_COLS + TARGET_COL]\n    else:\n        df = df[FEATURE_COLS]\n    return df\ndef data_testtransform(df, is_training = True, is_validation = True): \n    #Casting types to reduce memory usage\n#     df = reduce_memory_usage(df)\n    \n    #Dropping columns from the beginning to accelerate further computations\n#     df.drop(['task_container_id'],axis=1,inplace=True)\n\n    #Joining average marks for questions with the main dataframe\n    df = df.join(question_df['question_average'],on=['content_id'],rsuffix='_question_average')\n    \n    df = df.join(user_df[['performance','user_average', 'user_count']],on=['user_id'],rsuffix='_right')\n    \n    df['is_beginning'] = df['user_count'] < 20\n    df = df.loc[df.content_type_id == False].reset_index(drop=True)\n\n# answered correctly average for each content\n    content_df = train_out[['content_id','answered_correctly']].groupby(['content_id']).agg(['mean']).reset_index()\n    content_df.columns = ['content_id', 'answered_correctly_avg_c']\n    df = pd.merge(df, content_df, on=['content_id'], how=\"left\")\n    answered_correctly_sum_u_dict = defaultdict(int)\n    count_u_dict = defaultdict(int)\n    df = add_user_feats_without_update(df, answered_correctly_sum_u_dict, count_u_dict)\n\n    #Recasting after join\n#     df['prior_question_had_explanation'] = df['prior_question_had_explanation'].astype('bool')\n    prior_question_elapsed_time_mean = df.prior_question_elapsed_time.dropna().values.mean()\n    df['prior_question_elapsed_time_mean'] = df.prior_question_elapsed_time.fillna(prior_question_elapsed_time_mean)\n    df = df.loc[df['content_type_id'] == False]\n    \n    df['prior_question_had_explanation'] = df.prior_question_had_explanation.fillna(False).astype('int8')\n    prior_question_elapsed_time_mean = df.prior_question_elapsed_time.dropna().values.mean()\n    df['prior_question_elapsed_time_mean'] = df.prior_question_elapsed_time.fillna(prior_question_elapsed_time_mean)\n    if is_training or is_validation:\n        df = df[FEATURE_COLS + TARGET_COL]\n    else:\n        df = df[FEATURE_COLS]\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_out = train\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#%%time\ntrain = data_transform(train)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_rows',100)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"current_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = data_transform(test,False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nX_train = train[FEATURE_COLS]\ny_train = train[TARGET_COL]\nX_test = test[FEATURE_COLS]\ny_test = test[TARGET_COL]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape,X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\nparams = {\n    'objective': 'binary',\n    'seed': 42,\n    'metric': 'auc',\n    'learning_rate': 0.05,\n    'max_bin': 1000,\n    'num_leaves': 80,\n    'num_iterations' : 1#50\n}\n#lgb_train = lgb.Dataset(X_train,y_train)\n#lgb_val = lgb.Dataset(X_test,y_test)\nlgb_train = lgb.Dataset(X_train.iloc[:,1:],y_train)\nlgb_val = lgb.Dataset(X_test.iloc[:,1:],y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del X_train,y_train\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = lgb.train(\n    {'objective': 'binary',\n#     'num_iterations' : 50\n    }, \n    lgb_train,\n    valid_sets=[lgb_train,lgb_val],\n    verbose_eval=10,\n    num_boost_round=10000,\n    early_stopping_rounds=10\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\npredictions = pd.DataFrame(model.predict(X_test.iloc[:,1:]),index=X_test.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nroc_auc_score(y_test,predictions[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env = riiideducation.make_env()\niter_test = env.iter_test()\niter_nb = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for (current_df, sample_prediction_df) in iter_test:\n    print(current_df)\n    if (iter_nb != 0):\n        prior_df = add_answers_to_prior_df(current_df,prior_df)\n        question_df = build_question_df(prior_df,question_df)\n        user_df = build_user_df(prior_df,user_df,question_df)\n        \n    prior_df = current_df.copy()\n    current_df = data_testtransform(current_df,False,False)\n    current_df['answered_correctly'] = model.predict(current_df.iloc[:,1:])\n    iter_nb = 1\n    env.predict(current_df.loc[:, ['row_id', 'answered_correctly']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib import pyplot as plt\nlgb.plot_importance(model)\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}