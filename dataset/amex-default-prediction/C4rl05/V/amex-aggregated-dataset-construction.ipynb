{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# AMEX - Aggregated Dataset ü¶ù\n## Predict if a customer will default in the future...\n\n#### Notebook Goal (Work in Progress...)\nThe objective of this Notebook is to create a more complex aggregated dataset to train models on it; **so far has been quite challenging to not run out of memory**...\nI will keep trying new ways to optimize the memory utilization...\n\n\n#### Dataset\nThe objective of this competition is to predict the probability that a customer does not pay back their credit card balance amount in the future based on their monthly customer profile. The target binary variable is calculated by observing 18 months performance window after the latest credit card statement, and if the customer does not pay due amount in 120 days after their latest statement date it is considered a default event.\n\n\n\n\n\n\n\n#### Resources\n* https://www.kaggle.com/code/huseyincot/amex-agg-data-how-it-created\n* https://waylonwalker.com/reset-ipython/ ","metadata":{}},{"cell_type":"markdown","source":"# 1.0 Loading Model Libraries...","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport gc as gc # garbage collector\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# 2.0 Setting the Notebook Parameters and Default Configuration...","metadata":{}},{"cell_type":"code","source":"%%time\n# I like to disable my Notebook Warnings.\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Notebook Configuration...\n\n# Amount of data we want to load into the Model...\nDATA_ROWS = None\n# Dataframe, the amount of rows and cols to visualize...\nNROWS = 50\nNCOLS = 15\n# Main data location path...\nBASE_PATH = '...'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Configure notebook display settings to only use 2 decimal places, tables look nicer.\npd.options.display.float_format = '{:,.5f}'.format\npd.set_option('display.max_columns', NCOLS) \npd.set_option('display.max_rows', NROWS)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# 3.0 Loading the Train Dataset Information (Using Feather)...","metadata":{}},{"cell_type":"code","source":"%%time\n# Load the CSV information into a Pandas DataFrame...\ntrn_data = pd.read_feather('../input/parquet-files-amexdefault-prediction/train_data.ftr')\ntrn_lbls = pd.read_csv('/kaggle/input/amex-default-prediction/train_labels.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# 4.0 Exploring the Dataset, Quick EDA...","metadata":{}},{"cell_type":"code","source":"%%time\n# Explore the shape of the DataFrame...\n# trn_data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Display simple information of the variables in the dataset...\n# trn_data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Display the first few rows of the DataFrame...\n# trn_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Display the Min Date...\n# trn_data['S_2'].min()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Display the Max Date...\n# trn_data['S_2'].max()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Generate a simple statistical summary of the DataFrame, Only Numerical...\n# trn_data.describe() # I believe it consume significant memory","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Calculates the total number of missing values...\n# trn_data.isnull().sum().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Display the number of missing values by variable...\n# trn_data.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Display the number of unique values for each variable...\n# trn_data.nunique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# 6.0 Structuring data for the Model (Aggreations and More)","metadata":{}},{"cell_type":"code","source":"%%time\nremove = ['customer_ID', 'S_2', 'target']\nfeatures = [f for f in trn_data.columns if f not in remove]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Based on the dataset Descriptions this are Categorical Variables...\ncat_variables_dataset = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Extract the name of categorical variables based in the number of unique values...\ncutoff = 64\ncat_variables_calc = [f for f in trn_data.columns if trn_data[f].nunique() < cutoff] ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nnum_features_calc = [col for col in features if col not in cat_variables_calc]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Print the number of categorical varibles identified\nprint(f'Categorical Variables Dataset:{len(cat_variables_dataset)}')\nprint(f'Categorical Variables Base on Calculations: {len(cat_variables_calc)}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# 7.0 Defining an Aggregation function.","metadata":{}},{"cell_type":"code","source":"%%time\ndef agg_data(df, features, agg_calcs = ['mean']):\n    '''\n    \n    '''\n    \n    tmp = df.groupby(\"customer_ID\")[features].agg(agg_calcs)\n    tmp.columns = ['_'.join(x) for x in tmp.columns]\n    tmp = tmp.reset_index()\n    \n    return tmp","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# 8.0 Aggregating the Train Dataset.","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"## 8.1 Aggregating Numerical Variables","metadata":{}},{"cell_type":"code","source":"%%time\nagg_calculations = ['mean', 'std', 'min', 'max', 'last',]\ntrn_num_agg = agg_data(trn_data, num_features_calc, agg_calculations)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrn_agg_dataset = pd.concat([trn_num_agg, trn_lbls], axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ndel trn_num_agg, trn_lbls\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 8.2 Aggregating Categorical Variables","metadata":{}},{"cell_type":"code","source":"%%time\nagg_calculations = ['count', 'last', 'nunique']\ntrn_cat_agg = agg_data(trn_data, cat_variables_dataset, agg_calculations)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrn_agg_dataset = pd.concat([trn_agg_dataset, trn_cat_agg], axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ndel trn_cat_agg\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 8.3 Reviewing the Agg Dataset","metadata":{}},{"cell_type":"code","source":"%%time\ntrn_agg_dataset.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 8.4 Destroying some of the datasets to release some memory","metadata":{}},{"cell_type":"code","source":"%%time\ndel trn_data\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 8.5 Creating a Pickle file Backup","metadata":{}},{"cell_type":"code","source":"%%time\ntrn_agg_dataset.to_pickle('trn_agg_dataset.pkl', compression = 'gzip')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ndel trn_agg_dataset\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%reset -f","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# 9.0 Loading Model Libraries One More Time (Due to Reset)...","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport gc as gc # garbage collector","metadata":{"execution":{"iopub.status.busy":"2022-05-30T06:01:38.998473Z","iopub.execute_input":"2022-05-30T06:01:38.999023Z","iopub.status.idle":"2022-05-30T06:01:39.031453Z","shell.execute_reply.started":"2022-05-30T06:01:38.998915Z","shell.execute_reply":"2022-05-30T06:01:39.030654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 10.0 Loading the Test Dataset Information (Using Feather)...","metadata":{}},{"cell_type":"code","source":"%%time\n#tst_data = pd.read_feather('../input/parquet-files-amexdefault-prediction/test_data.ftr')","metadata":{"execution":{"iopub.status.busy":"2022-05-30T06:01:43.119217Z","iopub.execute_input":"2022-05-30T06:01:43.119737Z","iopub.status.idle":"2022-05-30T06:02:20.826456Z","shell.execute_reply.started":"2022-05-30T06:01:43.119704Z","shell.execute_reply":"2022-05-30T06:02:20.825159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 11.0 Structuring Data for the Model (Aggreations and More)","metadata":{}},{"cell_type":"code","source":"%%time\nremove = ['customer_ID', 'S_2', 'target']\nfeatures = [f for f in tst_data.columns if f not in remove]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Based on the dataset Descriptions this are Categorical Variables...\ncat_variables_dataset = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Extract the name of categorical variables based in the number of unique values...\ncutoff = 64\ncat_variables_calc = [f for f in tst_data.columns if tst_data[f].nunique() < cutoff] ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nnum_features_calc = [col for col in features if col not in cat_variables_calc]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Print the number of categorical varibles identified\nprint(f'Categorical Variables Dataset:{len(cat_variables_dataset)}')\nprint(f'Categorical Variables Base on Calculations: {len(cat_variables_calc)}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 12.0 Defining an Aggregation function.","metadata":{}},{"cell_type":"code","source":"%%time\ndef agg_data(df, features, agg_calcs = ['mean']):\n    '''\n    \n    '''\n    \n    tmp = df.groupby(\"customer_ID\")[features].agg(agg_calcs)\n    tmp.columns = ['_'.join(x) for x in tmp.columns]\n    tmp = tmp.reset_index()\n    \n    return tmp","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# 13.0 Aggregating the Test Dataset.","metadata":{}},{"cell_type":"markdown","source":"## 13.1 Aggregating Numerical Variables","metadata":{}},{"cell_type":"code","source":"%%time\n#agg_calculations = ['mean', 'std', 'min', 'max', 'last',]\n#tst_num_agg = agg_data(tst_data, num_features_calc, agg_calculations)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n#tst_agg_dataset = pd.concat([tst_num_agg, trn_lbls], axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n#del tst_num_agg\n#gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 13.2 Aggregating Categorical Variables","metadata":{}},{"cell_type":"code","source":"%%time\n#agg_calculations = ['count', 'last', 'nunique']\n#tst_cat_agg = agg_data(tst_data, cat_variables_dataset, agg_calculations)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n#tst_agg_dataset = pd.concat([tst_agg_dataset, tst_cat_agg], axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n#del tst_cat_agg\n#gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 13.3 Reviewing the Agg Dataset","metadata":{}},{"cell_type":"code","source":"%%time\n#tst_agg_dataset.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 13.4 Destroying some of the datasets to release some memory","metadata":{}},{"cell_type":"code","source":"#del tst_data\n#gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 13.5 Creating a Pickle file Backup","metadata":{}},{"cell_type":"code","source":"%%time\n#tst_agg_dataset.to_pickle('tst_agg_dataset.pkl', compression = 'gzip')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#del tst_agg_dataset\n#gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%reset -f","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}