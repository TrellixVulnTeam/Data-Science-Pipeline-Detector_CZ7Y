{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd \nimport numpy as np\nimport pymc3 as pm\n\n\nimport dask.dataframe as dd\nfrom dask.diagnostics import ProgressBar\nimport arviz as az\nfrom matplotlib import pyplot as plt \nProgressBar().register()","metadata":{"execution":{"iopub.status.busy":"2022-06-29T13:55:00.724846Z","iopub.execute_input":"2022-06-29T13:55:00.725295Z","iopub.status.idle":"2022-06-29T13:55:00.732977Z","shell.execute_reply.started":"2022-06-29T13:55:00.725261Z","shell.execute_reply":"2022-06-29T13:55:00.731625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on https://www.kaggle.com/code/raddar/the-data-has-random-uniform-noise-added/notebook.  ","metadata":{}},{"cell_type":"code","source":"df = dd.read_csv('/kaggle/input/amex-default-prediction/train_data.csv')","metadata":{"execution":{"iopub.status.busy":"2022-06-29T13:55:01.000931Z","iopub.execute_input":"2022-06-29T13:55:01.001885Z","iopub.status.idle":"2022-06-29T13:55:01.097141Z","shell.execute_reply.started":"2022-06-29T13:55:01.001844Z","shell.execute_reply":"2022-06-29T13:55:01.095907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have a very large dataset. In fact, it is not so important whether we have 1 million data or 200 thousand (for our model).     \nThe distribution roughly remains the same.\n\nTo speed up the calculations of our model, we will make a sample. I chose the P_3 field because its distribution looks good.     \nUsing this field as an example, I will try to find out something about the noise that we have (Thanks https://www.kaggle.com/raddar for finding him for us).      ","metadata":{"execution":{"iopub.status.busy":"2022-06-29T13:21:13.400639Z","iopub.execute_input":"2022-06-29T13:21:13.401061Z","iopub.status.idle":"2022-06-29T13:21:13.411768Z","shell.execute_reply.started":"2022-06-29T13:21:13.401025Z","shell.execute_reply":"2022-06-29T13:21:13.410567Z"}}},{"cell_type":"code","source":"p3data = df['P_3'].compute()\np3data_sample = p3data.sample(100000).dropna()","metadata":{"execution":{"iopub.status.busy":"2022-06-29T13:55:02.157919Z","iopub.execute_input":"2022-06-29T13:55:02.158372Z","iopub.status.idle":"2022-06-29T13:56:42.377694Z","shell.execute_reply.started":"2022-06-29T13:55:02.158332Z","shell.execute_reply":"2022-06-29T13:56:42.376332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axs = plt.subplots(nrows=1, ncols=2)\nfig.set_size_inches(12.5, 5.5)\n\naxs[0].hist(p3data_sample, bins=500);\naxs[1].hist(p3data, bins=500);","metadata":{"execution":{"iopub.status.busy":"2022-06-29T13:56:42.379684Z","iopub.execute_input":"2022-06-29T13:56:42.380077Z","iopub.status.idle":"2022-06-29T13:56:44.726806Z","shell.execute_reply.started":"2022-06-29T13:56:42.380041Z","shell.execute_reply":"2022-06-29T13:56:44.725282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The form of distribution remained the same.Our sample is ok.     \nNow let's try to model our distribution. Our shape is similar to the Laplace distribution. As raddar said, we assume that our noise is uniformly distributed.     \nWe also assume that the upper and lower bounds of our noise are taken from a normal distribution, where the upper bound cannot be less than the lower.    ","metadata":{}},{"cell_type":"code","source":"with pm.Model() as noise_model:\n\n    lower_bound_noise = pm.Normal('lower_bound_noise', mu=0, sigma=.02)# as raddar said, lets assume that our lowwer bound is close to 0 \n    BoundedNormal = pm.Bound(pm.Normal, lower=lower_bound_noise) # upper bound cannot be less than lower \n    upper_bound_noise = BoundedNormal('upper_bound_noise', mu=0.01, sigma=.02) # upper bound is close to 0.01\n    \n    noise = pm.Uniform('noise', lower=lower_bound_noise, upper=upper_bound_noise)\n    \n    mu = pm.Uniform('mu', lower=.3, upper=.8) # empirical choice of parameters, approximately can be understood from the sampling plot\n    b = pm.Uniform('sigma', lower=.0, upper=.5)\n    \n    \n    P_3 = pm.Laplace('P_3', mu = mu, b = b, observed = p3data_sample) + noise ","metadata":{"execution":{"iopub.status.busy":"2022-06-29T13:56:44.728424Z","iopub.execute_input":"2022-06-29T13:56:44.728815Z","iopub.status.idle":"2022-06-29T13:56:48.501957Z","shell.execute_reply.started":"2022-06-29T13:56:44.728783Z","shell.execute_reply":"2022-06-29T13:56:48.500781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Okey, now we want to sample from posterior.    \nOur first goal is to see that the model works. How ? From the first, we want to see that our \"chains\", plus or minus, worked out the same way.    \nAlso, we want to see that our model, when sampled from it, gives a distribution that is approximately similar to what we have.      \n\nNOTE:    \n\nOn my PC, i have large then 16 gb ram. This allows me use more that 2000 sample. Kaggle notebooks blows up when you try to plot posterior predictive or something else which sample size more than 2000. ","metadata":{}},{"cell_type":"code","source":"with noise_model:\n    posterior = pm.sample(draws=2000, tune=10000, cores=2, progressbar=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-29T13:56:48.504336Z","iopub.execute_input":"2022-06-29T13:56:48.505596Z","iopub.status.idle":"2022-06-29T13:59:04.535543Z","shell.execute_reply.started":"2022-06-29T13:56:48.505537Z","shell.execute_reply":"2022-06-29T13:59:04.533754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with noise_model:\n    az.plot_trace(posterior, var_names=[\"lower_bound_noise\", \"upper_bound_noise\"]);","metadata":{"execution":{"iopub.status.busy":"2022-06-29T13:59:38.36587Z","iopub.execute_input":"2022-06-29T13:59:38.367076Z","iopub.status.idle":"2022-06-29T13:59:57.413792Z","shell.execute_reply.started":"2022-06-29T13:59:38.367029Z","shell.execute_reply":"2022-06-29T13:59:57.412899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our lower bound sample looks not so similar, but, trust me. It is okay. If you increase sample size and chains, it will look much better.     \nWhat about sampling?","metadata":{}},{"cell_type":"code","source":"with noise_model:  \n    posterior_pred = pm.sample_posterior_predictive(posterior)","metadata":{"execution":{"iopub.status.busy":"2022-06-29T14:00:15.58756Z","iopub.execute_input":"2022-06-29T14:00:15.588012Z","iopub.status.idle":"2022-06-29T14:00:34.957743Z","shell.execute_reply.started":"2022-06-29T14:00:15.587974Z","shell.execute_reply":"2022-06-29T14:00:34.956226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a = az.from_pymc3(posterior_predictive=posterior_pred, model=noise_model)\naz.plot_ppc(a);    ","metadata":{"execution":{"iopub.status.busy":"2022-06-29T14:00:40.314298Z","iopub.execute_input":"2022-06-29T14:00:40.31477Z","iopub.status.idle":"2022-06-29T14:01:37.937689Z","shell.execute_reply.started":"2022-06-29T14:00:40.314732Z","shell.execute_reply":"2022-06-29T14:01:37.936299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Nice, our model gives a distribution very very close to ours. Final step!","metadata":{}},{"cell_type":"code","source":"a = az.from_pymc3(posterior, model=noise_model)\naz.plot_posterior(a, var_names=['lower_bound_noise', 'upper_bound_noise'], ref_val= [0, 0.01]);","metadata":{"execution":{"iopub.status.busy":"2022-06-29T14:21:47.497884Z","iopub.execute_input":"2022-06-29T14:21:47.49845Z","iopub.status.idle":"2022-06-29T14:21:54.652523Z","shell.execute_reply.started":"2022-06-29T14:21:47.498406Z","shell.execute_reply":"2022-06-29T14:21:54.651342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that Raddar was right and we can accept the hypothesis that there is artificial noise in our data. If you try the same method to test other data whose distributions look reasonable, the result will be similar. As we increase the number of data in our sample, and as we increase the sample in our model, the HDI environment will tend to 0 on one side of the interval, and to 0.1 on the other. Tnx for reading!","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}