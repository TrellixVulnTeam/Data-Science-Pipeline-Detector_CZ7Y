{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Keras Quickstart for the AMEX Competition: Training and Inference\n\nThis notebook shows\n- how to do space-efficient feature engineering\n- how to implement a simple Keras model\n- how to train and cross-validate the model\n- how to understand the competition metric graphically\n\nThe notebook is based on insights of the [EDA which makes sense ⭐️⭐️⭐️⭐️⭐️](https://www.kaggle.com/code/ambrosm/amex-eda-which-makes-sense).","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"#tpu\nimport tensorflow as tf\nresolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\ntf.config.experimental_connect_to_cluster(resolver)\n# This is the TPU initialization code that has to be at the beginning.\ntf.tpu.experimental.initialize_tpu_system(resolver)\nprint(\"All devices: \", tf.config.list_logical_devices('TPU'))\nstrategy = tf.distribute.TPUStrategy(resolver)","metadata":{"execution":{"iopub.status.busy":"2022-06-13T17:37:56.067116Z","iopub.execute_input":"2022-06-13T17:37:56.067563Z","iopub.status.idle":"2022-06-13T17:38:08.734878Z","shell.execute_reply.started":"2022-06-13T17:37:56.067452Z","shell.execute_reply":"2022-06-13T17:38:08.733717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport pickle\nfrom matplotlib import pyplot as plt\nimport random\nimport datetime\nimport math\nimport gc\nimport warnings\nimport seaborn as sns\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nfrom matplotlib.ticker import MaxNLocator\nfrom colorama import Fore, Back, Style\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler, QuantileTransformer, OneHotEncoder, PowerTransformer\nfrom sklearn.metrics import roc_curve, roc_auc_score, average_precision_score\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.utils import class_weight \nfrom sklearn.utils.class_weight import compute_class_weight\n\n# tf.config.threading.set_inter_op_parallelism_threads(4)\nimport tensorflow_addons as tfa\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, LearningRateScheduler, EarlyStopping\nfrom tensorflow.keras.layers import Dense, Input, InputLayer, Add, Concatenate, Dropout, BatchNormalization\nfrom tensorflow.keras.utils import plot_model\nimport tensorflow.keras.backend as K","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-13T17:38:08.736811Z","iopub.execute_input":"2022-06-13T17:38:08.737131Z","iopub.status.idle":"2022-06-13T17:38:10.21517Z","shell.execute_reply.started":"2022-06-13T17:38:08.737098Z","shell.execute_reply":"2022-06-13T17:38:10.214158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot training history\ndef plot_history(history, *, n_epochs=None, plot_lr=False, title=None, bottom=None, top=None):\n    \"\"\"Plot (the last n_epochs epochs of) the training history\n    \n    Plots loss and optionally val_loss and lr.\"\"\"\n    plt.figure(figsize=(15, 6))\n    from_epoch = 0 if n_epochs is None else max(len(history['loss']) - n_epochs, 0)\n    \n    # Plot training and validation losses\n    plt.plot(np.arange(from_epoch, len(history['loss'])), history['loss'][from_epoch:], label='Training loss')\n    try:\n        plt.plot(np.arange(from_epoch, len(history['loss'])), history['val_loss'][from_epoch:], label='Validation loss')\n        best_epoch = np.argmin(np.array(history['val_loss']))\n        best_val_loss = history['val_loss'][best_epoch]\n        if best_epoch >= from_epoch:\n            plt.scatter([best_epoch], [best_val_loss], c='r', label=f'Best val_loss = {best_val_loss:.5f}')\n        if best_epoch > 0:\n            almost_epoch = np.argmin(np.array(history['val_loss'])[:best_epoch])\n            almost_val_loss = history['val_loss'][almost_epoch]\n            if almost_epoch >= from_epoch:\n                plt.scatter([almost_epoch], [almost_val_loss], c='orange', label='Second best val_loss')\n    except KeyError:\n        pass\n    if bottom is not None: plt.ylim(bottom=bottom)\n    if top is not None: plt.ylim(top=top)\n    plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend(loc='lower left')\n    if title is not None: plt.title(title)\n        \n    # Plot learning rate\n    if plot_lr and 'lr' in history:\n        ax2 = plt.gca().twinx()\n        ax2.plot(np.arange(from_epoch, len(history['lr'])), np.array(history['lr'][from_epoch:]), color='g', label='Learning rate')\n        ax2.set_ylabel('Learning rate')\n        ax2.legend(loc='upper right')\n        \n    plt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-13T17:38:10.216679Z","iopub.execute_input":"2022-06-13T17:38:10.216972Z","iopub.status.idle":"2022-06-13T17:38:10.235203Z","shell.execute_reply.started":"2022-06-13T17:38:10.216942Z","shell.execute_reply":"2022-06-13T17:38:10.234141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def amex_metric(y_true, y_pred, return_components=False) -> float:\n    \"\"\"Amex metric for ndarrays\"\"\"\n    def top_four_percent_captured(df) -> float:\n        \"\"\"Corresponds to the recall for a threshold of 4 %\"\"\"\n        df['weight'] = df['target'].apply(lambda x: 20 if x==0 else 1)\n        four_pct_cutoff = int(0.04 * df['weight'].sum())\n        df['weight_cumsum'] = df['weight'].cumsum()\n        df_cutoff = df.loc[df['weight_cumsum'] <= four_pct_cutoff]\n        return (df_cutoff['target'] == 1).sum() / (df['target'] == 1).sum()\n        \n    def weighted_gini(df) -> float:\n        df['weight'] = df['target'].apply(lambda x: 20 if x==0 else 1)\n        df['random'] = (df['weight'] / df['weight'].sum()).cumsum()\n        total_pos = (df['target'] * df['weight']).sum()\n        df['cum_pos_found'] = (df['target'] * df['weight']).cumsum()\n        df['lorentz'] = df['cum_pos_found'] / total_pos\n        df['gini'] = (df['lorentz'] - df['random']) * df['weight']\n        return df['gini'].sum()\n\n    def normalized_weighted_gini(df) -> float:\n        \"\"\"Corresponds to 2 * AUC - 1\"\"\"\n        df2 = pd.DataFrame({'target': df.target, 'prediction': df.target})\n        df2.sort_values('prediction', ascending=False, inplace=True)\n        return weighted_gini(df) / weighted_gini(df2)\n\n    df = pd.DataFrame({'target': y_true.ravel(), 'prediction': y_pred.ravel()})\n    df.sort_values('prediction', ascending=False, inplace=True)\n    g = normalized_weighted_gini(df)\n    d = top_four_percent_captured(df)\n\n    if return_components: return g, d, 0.5 * (g + d)\n    return 0.5 * (g + d)\ndef create_weighted_binary_crossentropy(zero_weight, one_weight):\n\n    def weighted_cross_entropy_fn(y_true, y_pred):\n        tf_y_true = tf.cast(y_true, dtype=y_pred.dtype)\n        tf_y_pred = tf.cast(y_pred, dtype=y_pred.dtype)\n\n        weights_v = tf.where(tf.equal(tf_y_true, 1), 2*one_weight, 2*zero_weight)\n        ce = K.binary_crossentropy(tf_y_true, tf_y_pred)\n        loss = K.mean(tf.multiply(ce, weights_v))\n        return loss\n\n    return weighted_cross_entropy_fn","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-13T17:38:10.237381Z","iopub.execute_input":"2022-06-13T17:38:10.237696Z","iopub.status.idle":"2022-06-13T17:38:10.258592Z","shell.execute_reply.started":"2022-06-13T17:38:10.237664Z","shell.execute_reply":"2022-06-13T17:38:10.257186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reading and preprocessing the training data\n\nWe read the data from @munumbutt's [AMEX-Feather-Dataset](https://www.kaggle.com/datasets/munumbutt/amexfeather). Then we create some groups of features:\n- Selected features taken as minimums, maximums, averages over all statements of a customer\n- Selected features taken from the last statement of a customer\n- Other featurers including the number of unique statements a customer gets and the time between their statements\n\nWe one-hot encode the categorical features and fill all missing values with 0.\n\nThe code has been optimized for memory efficiency rather than readability. In particular, `.iloc[mask_array, columns]` needs much less RAM than the groupby construction used in previous versions of the notebook.\n\nWe process test data first since it takes more memory\n","metadata":{}},{"cell_type":"code","source":"train = pd.read_feather('../input/amex-aggreation-dataset/train_processed.ftr')\ntrain.sort_values(by = 'customer_ID', inplace = True)\ntrain.reset_index(drop = True, inplace = True)\ncid = train['customer_ID']\ntarget = train['target']\ntrain = train.drop(['target','customer_ID'], axis = 1)\ntrain = pd.DataFrame(StandardScaler().fit_transform(train),columns = train.columns).astype('float16')","metadata":{"execution":{"iopub.status.busy":"2022-06-13T17:38:10.26015Z","iopub.execute_input":"2022-06-13T17:38:10.260415Z","iopub.status.idle":"2022-06-13T17:39:01.445027Z","shell.execute_reply.started":"2022-06-13T17:38:10.260383Z","shell.execute_reply":"2022-06-13T17:39:01.443284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-13T17:39:01.447569Z","iopub.execute_input":"2022-06-13T17:39:01.447907Z","iopub.status.idle":"2022-06-13T17:39:01.490847Z","shell.execute_reply.started":"2022-06-13T17:39:01.447858Z","shell.execute_reply":"2022-06-13T17:39:01.490095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-13T17:39:01.492405Z","iopub.execute_input":"2022-06-13T17:39:01.493242Z","iopub.status.idle":"2022-06-13T17:39:01.502333Z","shell.execute_reply.started":"2022-06-13T17:39:01.493182Z","shell.execute_reply":"2022-06-13T17:39:01.501323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The model\n\nOur model has four hidden layers, enriched by a skip connection and a Dropout layer.","metadata":{}},{"cell_type":"code","source":"def my_model(n_inputs=len(train.columns)):\n    \"\"\"Sequential neural network with a skip connection.\n    \n    Returns a compiled instance of tensorflow.keras.models.Model.\n    \"\"\"\n    activation = 'swish'\n    l1 = 1e-7\n    l2 = 4e-4\n    inputs = Input(shape=(n_inputs, ))\n    x0 = BatchNormalization()(inputs)\n    x0 = Dense(256, \n               kernel_regularizer=tf.keras.regularizers.L1L2(l1=l1,l2=l2),\n#                activity_regularizer=tf.keras.regularizers.L1L2(l1=l1,l2=l2),\n              activation=activation,\n             )(x0)\n    x0 = Dropout(0.1)(x0)\n    x = Dense(64, \n              kernel_regularizer=tf.keras.regularizers.L1L2(l1=l1,l2=l2),\n#               activity_regularizer=tf.keras.regularizers.L1L2(l1=l1,l2=l2),\n              activation=activation,\n             )(x0)\n    x = Dense(64, \n              kernel_regularizer=tf.keras.regularizers.L1L2(l1=l1,l2=l2),\n#               activity_regularizer=tf.keras.regularizers.L1L2(l1=l1,l2=l2),\n              activation=activation,\n             )(x)\n    x = Concatenate()([x, x0])\n    x = Dropout(0.1)(x)\n    x = Dense(16, \n              kernel_regularizer=tf.keras.regularizers.L1L2(l1=l1,l2=l2),\n#               activity_regularizer=tf.keras.regularizers.L1L2(l1=l1,l2=l2),\n              activation=activation,\n             )(x)\n    x = Dense(1,\n              activation='sigmoid',\n             )(x)\n    return Model(inputs, x)","metadata":{"execution":{"iopub.status.busy":"2022-06-13T17:39:01.504185Z","iopub.execute_input":"2022-06-13T17:39:01.504763Z","iopub.status.idle":"2022-06-13T17:39:01.516728Z","shell.execute_reply.started":"2022-06-13T17:39:01.504719Z","shell.execute_reply":"2022-06-13T17:39:01.515486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cross-validation\n\nWe use a standard cross-validation loop. In the loop, we scale the data and train a model. We use a StratifiedKFold because the data is imbalanced.\n","metadata":{}},{"cell_type":"code","source":"# %%time\n# Cross-validation of the classifier\n\nFOLDS = 11\nEPOCHS_EXPONENTIALDECAY = 125\nVERBOSE = 0 # set to 0 for less output, or to 2 for more output\nLR_START = 0.012\nLR_END = 1e-5 # learning rate at the end of training\nCYCLES = 1\nEPOCHS = 300\nDIAGRAMS = False\nUSE_PLATEAU = True # set to True for plateau, or to False for exponential learning rate decay\nBATCH_SIZE = 2048\none_weight = 0.55\n\nnp.random.seed(1)\nrandom.seed(1)\n\ndef fit_model(X_tr, y_tr, X_va=None, y_va=None, fold=0):\n    \"\"\"Scale the data, fit a model, plot the training history and optionally validate the model\n    Saves a trained instance of tensorflow.keras.models.Model.\n    As a side effect, updates oof_pred, y_pred_list and score_list.\n    \"\"\"\n    global oof_pred\n    start_time = datetime.datetime.now()\n    \n    if USE_PLATEAU and X_va is not None: # use plateau\n        epochs = EPOCHS\n        lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.80, \n                               patience=5, verbose=VERBOSE)\n        es = EarlyStopping(monitor=\"val_loss\",\n                           patience=20, \n                           verbose=1,\n                           mode=\"min\", \n                           restore_best_weights=True)\n        callbacks = [lr, es, tf.keras.callbacks.TerminateOnNaN()]\n\n    else: # use exponential learning \n        epochs = EPOCHS_EXPONENTIALDECAY\n\n        def exponential_decay(epoch):\n            # v decays from e^a to 1 in every cycle\n            # w decays from 1 to 0 in every cycle\n            # epoch == 0                  -> w = 1 (first epoch of cycle)\n            # epoch == epochs_per_cycle-1 -> w = 0 (last epoch of cycle)\n            # higher a -> decay starts with a steeper decline\n            a = 4\n            epochs_per_cycle = epochs // CYCLES\n            epoch_in_cycle = epoch % epochs_per_cycle\n            if epochs_per_cycle > 1:\n                v = math.exp(a * (1 - epoch_in_cycle / (epochs_per_cycle-1)))\n                w = (v - 1) / (math.exp(a) - 1)\n            else:\n                w = 1\n            return w * LR_START + (1 - w) * LR_END\n\n        lr = LearningRateScheduler(exponential_decay, verbose=0)\n        \n        es = EarlyStopping(monitor=\"val_loss\",\n                           patience=100, \n                           verbose=1,\n                           mode=\"min\", \n                           restore_best_weights=True)\n        \n        callbacks = [lr, es, tf.keras.callbacks.TerminateOnNaN()]\n            \n    #reset model\n    with strategy.scope():\n        loss = create_weighted_binary_crossentropy(1-one_weight, one_weight)\n        model = my_model(train.shape[1])\n        model.compile(optimizer=tf.keras.optimizers.Nadam(learning_rate=LR_START,\n                                                      clipvalue= 0.5,\n                                                      clipnorm = 1.0 # prevent gradient explosion\n                                                     ),\n                  loss=loss,\n                 )\n            \n    # Train the model\n    history = model.fit(X_tr, y_tr, \n                        validation_data=(X_va, y_va),\n                        epochs=epochs,\n                        verbose=VERBOSE,\n                        batch_size=BATCH_SIZE,\n                        shuffle=True,\n                        callbacks=callbacks).history\n        \n    \n    X_tr, y_tr, callbacks, es, lr = None, None, None, None, None\n    \n    lastloss = f\"Training loss: {history['loss'][-1]:.4f} | Val loss: {history['val_loss'][-1]:.4f}\"\n\n    # Inference for validation\n    oof_pred = model.predict(X_va, batch_size=len(X_va), verbose=0).ravel()\n\n    # Evaluation: Execution time, loss and metrics\n    score = amex_metric(y_va.values, oof_pred)\n    print(f\"{Fore.GREEN}{Style.BRIGHT}Fold {fold} | {str(datetime.datetime.now() - start_time)[-12:-7]}\"\n          f\" | {len(history['loss']):3} ep\"\n          f\" | {lastloss} | Score: {score:.5f}{Style.RESET_ALL}\")\n\n    if DIAGRAMS:\n        # Plot training history\n        plot_history(history, \n                     title=f\"Learning curve\",\n                     plot_lr=True)\n        \n    return score, model\n\n\nprint(f\"{len(train.columns)} features\")\n# history_list = []\noof_pred_list = []\nkf = StratifiedKFold(n_splits=FOLDS, shuffle= True, random_state= 42)\nfor fold, (idx_tr, idx_va) in enumerate(kf.split(train, target)):\n    print('-' * 15 + f'fold {fold}' + '-'*15)\n    gc.collect()\n    b_score = 0\n    b_oofpred = None\n    b_model = None\n    trys = 0\n    while trys < 4:\n        print('seed:', trys)\n        tf.random.set_seed(1+trys)\n        score, model = fit_model(train.iloc[idx_tr], target.iloc[idx_tr], train.iloc[idx_va], target.iloc[idx_va], fold=fold)\n        if score > b_score:\n            b_model = model\n            b_score = score\n            b_oofpred = pd.DataFrame({'customer_ID': cid.iloc[idx_va],\n                            'prediction': oof_pred})\n        trys+=1\n        \n    oof_pred_list.append(b_oofpred)\n    #save model\n    import h5py\n    b_model.save_weights(f\"model_{fold}.h5\")\n    b_model = b_model.to_json()\n    with open(f\"model_{fold}.json\", \"w\") as json_file:\n        json_file.write(b_model)\n\n    \n    del b_score, b_oofpred, b_model, score, model\n    gc.collect()\n\noof_pred = pd.concat(oof_pred_list, axis = 0).sort_values(by = 'customer_ID')\nprint(f\"{Fore.GREEN}{Style.BRIGHT}OOF Score: {amex_metric(target, oof_pred['prediction'].ravel())}{Style.RESET_ALL}\")\n# Fold 0 | 17:40 | 125 ep | Training loss: 0.2172 | Val loss: 0.2289 | Score: 0.78622\n# Fold 1 | 17:17 | 125 ep | Training loss: 0.2152 | Val loss: 0.2288 | Score: 0.79317\n# Fold 2 | 16:47 | 125 ep | Training loss: 0.2156 | Val loss: 0.2304 | Score: 0.78242\n# Fold 3 | 17:26 | 125 ep | Training loss: 0.2152 | Val loss: 0.2312 | Score: 0.78592\n# Fold 4 | 17:21 | 125 ep | Training loss: 0.2158 | Val loss: 0.2266 | Score: 0.78993\n# Fold 5 | 17:17 | 125 ep | Training loss: 0.2154 | Val loss: 0.2282 | Score: 0.78551\n\n# Fold 0 | 18:40 | 132 ep | Training loss: 0.2172 | Val loss: 0.2323 | Score: 0.78299\n# Fold 1 | 21:40 | 159 ep | Training loss: 0.2167 | Val loss: 0.2258 | Score: 0.79349","metadata":{"execution":{"iopub.status.busy":"2022-06-13T17:39:01.51887Z","iopub.execute_input":"2022-06-13T17:39:01.519589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# write to files and clear memory \ndel oof_pred_list, kf, train, target\ngc.collect()\n\noof_pred.to_csv('oof_keras.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission\n\nWe submit the mean of the five predictions.","metadata":{}},{"cell_type":"code","source":"test = pd.read_feather('../input/amex-aggreation-dataset/test_processed.ftr')\ntest.drop(['customer_ID'], axis = 1, inplace = True)\ntest.reset_index(drop = True, inplace = True)\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#scale test and predict\nSPLITS = 10\ndef split(a, n):\n    k, m = divmod(len(a), n)\n    return (a[i*k+min(i, m):(i+1)*k+min(i+1, m)] for i in range(n))\n\n\n\ny_pred_avg = np.zeros(len(test))\n\nfor fold in range(FOLDS):\n#     model = load_model(f\"model_{fold}\", custom_objects={\"weighted_cross_entropy_fn\": loss})\n    \n    from tensorflow.keras.models import model_from_json\n    json_file = open(f'model_{fold}.json', 'r')\n    model = json_file.read()\n    json_file.close()\n    model = model_from_json(model, custom_objects={\"weighted_cross_entropy_fn\": loss})\n    model.load_weights(f\"model_{fold}.h5\")\n    \n    \n    pred = np.array([])\n    split_ids = split(test.index, SPLITS)\n    for (j,ids) in enumerate(split_ids):\n        df = StandardScaler.fit_transform(test.iloc[ids])\n        pred = np.append(pred, model.predict(df).reshape(1, -1)[0])\n    del model\n    gc.collect()\n    y_pred_avg += pred / FOLDS\n    \ndel scaler, test\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.read_csv('../input/amex-default-prediction/sample_submission.csv')\nsub['prediction'] = y_pred_avg\nsub.to_csv('keras_submission.csv', index=False)\nsub","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As a plausibility test, we plot a histogram of the predictions. The histogram should resemble the OOF histogram (see above), and the majority of the predictions should be near 0 (because the classes are imbalanced).","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(16, 5))\nplt.hist(sub.prediction, bins=np.linspace(0, 1, 21), density=True)\nplt.title(\"Plausibility check\", fontsize=20)\nplt.xlabel('Prediction')\nplt.ylabel('Density')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}