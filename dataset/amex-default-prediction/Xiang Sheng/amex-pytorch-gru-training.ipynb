{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Time Series GRU PyTorch Training Notebook\nThis notebook is based on implementation of the [TensorFlow GRU Starter - [0.790]](https://www.kaggle.com/code/cdeotte/tensorflow-gru-starter-0-790/)\n\nIn this notebook we present starter code for a time series GRU model and starter code for processing Kaggle's 50GB CSV files into multiple saved NumPy files. Using a time series GRU allows us to use all the provided customer data and not just the customer's last data point. We published plots of time series data [here][1]. In this notebook we\n* Processes the train data from dataframes into 3D NumPy array of dimensions `num_of_customers x 13 x 188`\n* Save processed arrays as multiple NumPy files on disk\n* Next we build and train a GRU from the multiple files on disk\n* We compute validation score\n* Finally we process and save test data, infer test, and create a submission\n\n\nTo view time series EDA which can help give you intuition about feature engineering and improving model architecture, see the notebook [here][1].\n\n[1]: https://www.kaggle.com/cdeotte/time-series-eda","metadata":{}},{"cell_type":"code","source":"import os, copy, gc\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import MultiStepLR\nfrom sklearn.metrics import roc_auc_score, average_precision_score\nimport matplotlib.pyplot as plt\nprint('Using PyTorch version',torch.__version__)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-30T17:40:14.013912Z","iopub.execute_input":"2022-05-30T17:40:14.014267Z","iopub.status.idle":"2022-05-30T17:40:14.022693Z","shell.execute_reply.started":"2022-05-30T17:40:14.014238Z","shell.execute_reply":"2022-05-30T17:40:14.021923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Process Train Data\nWe process both train and test data in chunks. We split train data into 10 parts and process each part separately and save to disk. We split test into 20 parts. This allows us to avoid memory errors during processing. We can also perform processing on GPU which is faster than CPU. Discussions about data preprocessing are [here][1] and [here][2]\n\n[1]: https://www.kaggle.com/competitions/amex-default-prediction/discussion/327828\n[2]: https://www.kaggle.com/competitions/amex-default-prediction/discussion/328054","metadata":{}},{"cell_type":"code","source":"# LOADING JUST FIRST COLUMN OF TRAIN OR TEST IS SLOW\n# INSTEAD YOU CAN LOAD FIRST COLUMN FROM MY DATASET\n# OTHERWISE SET VARIABLE TO NONE TO LOAD FROM KAGGLE'S ORIGINAL DATAFRAME\nPATH_TO_CUSTOMER_HASHES = '../input/amex-data-files/'\n\n# AFTER PROCESSING DATA ONCE, UPLOAD TO KAGGLE DATASET\n# THEN SET VARIABLE BELOW TO FALSE\n# AND ATTACH DATASET TO NOTEBOOK AND PUT PATH TO DATASET BELOW\nPROCESS_DATA = False\n#PATH_TO_DATA = './data/'\nPATH_TO_DATA = '../input/amex-data-for-transformers-and-rnns/data/'\n\n# AFTER TRAINING MODEL, UPLOAD TO KAGGLE DATASET\n# THEN SET VARIABLE BELOW TO FALSE\n# AND ATTACH DATASET TO NOTEBOOK AND PUT PATH TO DATASET BELOW\nTRAIN_MODEL = True\nPATH_TO_MODEL = './model/'\n#PATH_TO_MODEL = '../input/amex-data-for-transformers-and-rnns/model/'\n\nINFER_TEST = True","metadata":{"execution":{"iopub.status.busy":"2022-05-30T17:40:14.024539Z","iopub.execute_input":"2022-05-30T17:40:14.025089Z","iopub.status.idle":"2022-05-30T17:40:14.036773Z","shell.execute_reply.started":"2022-05-30T17:40:14.02505Z","shell.execute_reply":"2022-05-30T17:40:14.035875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cupy, cudf # GPU LIBRARIES\nimport numpy as np, pandas as pd # CPU LIBRARIES\nimport matplotlib.pyplot as plt, gc\n\nif PROCESS_DATA:\n    # LOAD TARGETS\n    targets = cudf.read_csv('../input/amex-default-prediction/train_labels.csv')\n    targets['customer_ID'] = targets['customer_ID'].str[-16:].str.hex_to_int().astype('int64')\n    print(f'There are {targets.shape[0]} train targets')\n    \n    # GET TRAIN COLUMN NAMES\n    train = cudf.read_csv('../input/amex-default-prediction/train_data.csv', nrows=1)\n    T_COLS = train.columns\n    print(f'There are {len(T_COLS)} train dataframe columns')\n    \n    # GET TRAIN CUSTOMER NAMES (use pandas to avoid memory error)\n    if PATH_TO_CUSTOMER_HASHES:\n        train = cudf.read_parquet(f'{PATH_TO_CUSTOMER_HASHES}train_customer_hashes.pqt')\n    else:\n        train = pd.read_csv('/raid/Kaggle/amex/train_data.csv', usecols=['customer_ID'])\n        train['customer_ID'] = train['customer_ID'].apply(lambda x: int(x[-16:],16) ).astype('int64')\n    customers = train.drop_duplicates().sort_index().values.flatten()\n    print(f'There are {len(customers)} unique customers in train.')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-30T17:40:14.038178Z","iopub.execute_input":"2022-05-30T17:40:14.038825Z","iopub.status.idle":"2022-05-30T17:40:16.022228Z","shell.execute_reply.started":"2022-05-30T17:40:14.038712Z","shell.execute_reply":"2022-05-30T17:40:16.021339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# CALCULATE SIZE OF EACH SEPARATE FILE\ndef get_rows(customers, train, NUM_FILES = 10, verbose = ''):\n    chunk = len(customers)//NUM_FILES\n    if verbose != '':\n        print(f'We will split {verbose} data into {NUM_FILES} separate files.')\n        print(f'There will be {chunk} customers in each file (except the last file).')\n        print('Below are number of rows in each file:')\n    rows = []\n\n    for k in range(NUM_FILES):\n        if k==NUM_FILES-1: cc = customers[k*chunk:]\n        else: cc = customers[k*chunk:(k+1)*chunk]\n        s = train.loc[train.customer_ID.isin(cc)].shape[0]\n        rows.append(s)\n    if verbose != '': print( rows )\n    return rows\n\nif PROCESS_DATA:\n    NUM_FILES = 10\n    rows = get_rows(customers, train, NUM_FILES = NUM_FILES, verbose = 'train')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-30T17:40:16.024041Z","iopub.execute_input":"2022-05-30T17:40:16.024916Z","iopub.status.idle":"2022-05-30T17:40:16.581835Z","shell.execute_reply.started":"2022-05-30T17:40:16.024876Z","shell.execute_reply":"2022-05-30T17:40:16.58099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocess and Feature Engineering\nThe function below processes the data. Discussions describing the process are [here][1] and [here][2]. Currently the code below uses [RAPIDS][3] and GPU to\n* Reduces memory usage of customer_ID column by converting to int64\n* Reduces memory usage of date time column (then deletes the column).\n* We fill NANs\n* Label encodes the categorical columns\n* We reduce memory usage dtypes of columns\n* Converts every customer into a 3D array with sequence length 13 and feature length 188\n\nTo improve this model, try adding new feautures. The columns have been rearanged to have the 11 categorical features first. This makes building the TensorFlow model later easier. We can also try adding Standard Scaler. Currently the data is being used without scaling from the original Kaggle train data. \n\n[1]: https://www.kaggle.com/competitions/amex-default-prediction/discussion/327828\n[2]: https://www.kaggle.com/competitions/amex-default-prediction/discussion/328054\n[3]: https://rapids.ai/","metadata":{}},{"cell_type":"code","source":"def feature_engineer(train, PAD_CUSTOMER_TO_13_ROWS = True, targets = None):\n        \n    # REDUCE STRING COLUMNS \n    # from 64 bytes to 8 bytes, and 10 bytes to 3 bytes respectively\n    train['customer_ID'] = train['customer_ID'].str[-16:].str.hex_to_int().astype('int64')\n    train.S_2 = cudf.to_datetime( train.S_2 )\n    train['year'] = (train.S_2.dt.year-2000).astype('int8')\n    train['month'] = (train.S_2.dt.month).astype('int8')\n    train['day'] = (train.S_2.dt.day).astype('int8')\n    del train['S_2']\n        \n    # LABEL ENCODE CAT COLUMNS (and reduce to 1 byte)\n    # with 0: padding, 1: nan, 2,3,4,etc: values\n    d_63_map = {'CL':2, 'CO':3, 'CR':4, 'XL':5, 'XM':6, 'XZ':7}\n    train['D_63'] = train.D_63.map(d_63_map).fillna(1).astype('int8')\n\n    d_64_map = {'-1':2,'O':3, 'R':4, 'U':5}\n    train['D_64'] = train.D_64.map(d_64_map).fillna(1).astype('int8')\n    \n    CATS = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_66', 'D_68']\n    OFFSETS = [2,1,2,2,3,2,3,2,2] #2 minus minimal value in full train csv\n    # then 0 will be padding, 1 will be NAN, 2,3,4,etc will be values\n    for c,s in zip(CATS,OFFSETS):\n        train[c] = train[c] + s\n        train[c] = train[c].fillna(1).astype('int8')\n    CATS += ['D_63','D_64']\n    \n    # ADD NEW FEATURES HERE\n    # EXAMPLE: train['feature_189'] = etc etc etc\n    # EXAMPLE: train['feature_190'] = etc etc etc\n    # IF CATEGORICAL, THEN ADD TO CATS WITH: CATS += ['feaure_190'] etc etc etc\n    \n    # REDUCE MEMORY DTYPE\n    SKIP = ['customer_ID','year','month','day']\n    for c in train.columns:\n        if c in SKIP: continue\n        if str( train[c].dtype )=='int64':\n            train[c] = train[c].astype('int32')\n        if str( train[c].dtype )=='float64':\n            train[c] = train[c].astype('float32')\n            \n    # PAD ROWS SO EACH CUSTOMER HAS 13 ROWS\n    if PAD_CUSTOMER_TO_13_ROWS:\n        tmp = train[['customer_ID']].groupby('customer_ID').customer_ID.agg('count')\n        more = cupy.array([],dtype='int64') \n        for j in range(1,13):\n            i = tmp.loc[tmp==j].index.values\n            more = cupy.concatenate([more,cupy.repeat(i,13-j)])\n        df = train.iloc[:len(more)].copy().fillna(0)\n        df = df * 0 - 1 #pad numerical columns with -1\n        df[CATS] = (df[CATS] * 0).astype('int8') #pad categorical columns with 0\n        df['customer_ID'] = more\n        train = cudf.concat([train,df],axis=0,ignore_index=True)\n        \n    # ADD TARGETS (and reduce to 1 byte)\n    if targets is not None:\n        train = train.merge(targets,on='customer_ID',how='left')\n        train.target = train.target.astype('int8')\n        \n    # FILL NAN\n    train = train.fillna(-0.5) #this applies to numerical columns\n    \n    # SORT BY CUSTOMER THEN DATE\n    train = train.sort_values(['customer_ID','year','month','day']).reset_index(drop=True)\n    train = train.drop(['year','month','day'],axis=1)\n    \n    # REARRANGE COLUMNS WITH 11 CATS FIRST\n    COLS = list(train.columns[1:])\n    COLS = ['customer_ID'] + CATS + [c for c in COLS if c not in CATS]\n    train = train[COLS]\n    \n    return train","metadata":{"execution":{"iopub.status.busy":"2022-05-30T17:40:16.582958Z","iopub.execute_input":"2022-05-30T17:40:16.583956Z","iopub.status.idle":"2022-05-30T17:40:16.605847Z","shell.execute_reply.started":"2022-05-30T17:40:16.583918Z","shell.execute_reply":"2022-05-30T17:40:16.605059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if PROCESS_DATA:\n    # CREATE PROCESSED TRAIN FILES AND SAVE TO DISK        \n    for k in range(NUM_FILES):\n\n        # READ CHUNK OF TRAIN CSV FILE\n        skip = int(np.sum( rows[:k] ) + 1) #the plus one is for skipping header\n        train = cudf.read_csv('../input/amex-default-prediction/train_data.csv', nrows=rows[k], \n                              skiprows=skip, header=None, names=T_COLS)\n\n        # FEATURE ENGINEER DATAFRAME\n        train = feature_engineer(train, targets = targets)\n\n        # SAVE FILES\n        print(f'Train_File_{k+1} has {train.customer_ID.nunique()} customers and shape',train.shape)\n        tar = train[['customer_ID','target']].drop_duplicates().sort_index()\n        if not os.path.exists(PATH_TO_DATA): os.makedirs(PATH_TO_DATA)\n        tar.to_parquet(f'{PATH_TO_DATA}targets_{k+1}.pqt',index=False)\n        data = train.iloc[:,1:-1].values.reshape((-1,13,188))\n        cupy.save(f'{PATH_TO_DATA}data_{k+1}',data.astype('float32'))\n\n    # CLEAN MEMORY\n    del train, tar, data\n    del targets\n    gc.collect()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-30T17:40:16.606884Z","iopub.execute_input":"2022-05-30T17:40:16.607143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Build Model\nWe will just input the sequence data into a basic GRU. We will follow that we two dense layers and finally a sigmoid output to predict default. Try improving the model architecture.","metadata":{}},{"cell_type":"code","source":"# SIMPLE GRU MODEL\nclass gru_model(nn.Module):\n    def __init__(self, in_feats, hid_dim=256, activation=nn.ReLU()):\n        super(gru_model, self).__init__()\n        self.num_layers = 1\n        self.hid_dim = hid_dim\n        self.activation = activation\n        self.hidden_state = None\n        self.encode = nn.GRU(input_size=in_feats,\n                             hidden_size=hid_dim,\n                             num_layers=self.num_layers,\n                             batch_first=True,\n                             bidirectional=False)\n        self.hidden = nn.Sequential(nn.Linear(hid_dim, 64),\n                                    self.activation,\n                                    nn.Linear(64, 32),\n                                    self.activation)\n        self.predict = nn.Linear(32, 2)\n        \n    def init_hidden(self, batch_size, device=\"cpu\"):\n        return torch.autograd.Variable(torch.zeros(self.num_layers, batch_size, self.hid_dim)).to(device)\n    \n    def forward(self, x):\n        _, h = self.encode(x, self.hidden_state)\n        h = self.hidden(torch.squeeze(h))\n        return self.predict(h)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Competition Metric Code\nThe code below is from Konstantin Yakovlev's discussion post [here][1]\n\n[1]: https://www.kaggle.com/competitions/amex-default-prediction/discussion/327534","metadata":{}},{"cell_type":"code","source":"# COMPETITION METRIC FROM Konstantin Yakovlev\n# https://www.kaggle.com/kyakovlev\n# https://www.kaggle.com/competitions/amex-default-prediction/discussion/327534\ndef amex_metric(y_true, y_pred):\n\n    labels     = np.transpose(np.array([y_true, y_pred]))\n    labels     = labels[labels[:, 1].argsort()[::-1]]\n    weights    = np.where(labels[:,0]==0, 20, 1)\n    cut_vals   = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n    top_four   = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n\n    gini = [0,0]\n    for i in [1,0]:\n        labels         = np.transpose(np.array([y_true, y_pred]))\n        labels         = labels[labels[:, i].argsort()[::-1]]\n        weight         = np.where(labels[:,0]==0, 20, 1)\n        weight_random  = np.cumsum(weight / np.sum(weight))\n        total_pos      = np.sum(labels[:, 0] *  weight)\n        cum_pos_found  = np.cumsum(labels[:, 0] * weight)\n        lorentz        = cum_pos_found / total_pos\n        gini[i]        = np.sum((lorentz - weight_random) * weight)\n    print(\"G: {:.6f}, D: {:.6f}, ALL: {:6f}\".format(gini[1]/gini[0], top_four, 0.5*(gini[1]/gini[0] + top_four)))\n    return 0.5 * (gini[1]/gini[0] + top_four)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Early Stopper","metadata":{}},{"cell_type":"code","source":"class early_stopper(object):\n    def __init__(self, patience=12, verbose=False, delta=0):\n        self.patience = patience\n        self.verbose = verbose\n        self.delta = delta\n        self.best_value = None\n        self.best_cv = None\n        self.is_earlystop = False\n        self.count = 0\n        self.best_model = None\n        #self.val_preds = []\n        #self.val_logits = []\n\n    def earlystop(self, loss, value, model=None):#, preds, logits):\n        \"\"\"\n        value: evaluation value on valiation dataset\n        \"\"\"\n        cv = value\n        if self.best_value is None:\n            self.best_value = value\n            self.best_cv = cv\n            self.best_model = copy.deepcopy(model).to('cpu')\n            #self.val_preds = preds\n            #self.val_logits = logits\n        elif value < self.best_value + self.delta:\n            self.count += 1\n            if self.verbose:\n                print('EarlyStoper count: {:02d}'.format(self.count))\n            if self.count >= self.patience:\n                self.is_earlystop = True\n        else:\n            self.best_value = value\n            self.best_cv = cv\n            self.best_model = copy.deepcopy(model).to('cpu')\n            #self.val_preds = preds\n            #self.val_logits = logits\n            self.count = 0\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train Model\nWe train 5 folds for 8 epochs each. We save the 5 fold models for test inference later. If you only want to infer without training, then set variable `TRAIN_MODEL = False` in the beginning of this notebook.","metadata":{}},{"cell_type":"code","source":"params = {\n    'model': 'gru_model',\n    'batch_size': 512,\n    'lr': 0.002,\n    'wd': 1e-5,\n    #'device': 'cpu',\n    'device': 'cuda:0',\n    'early_stopping': 4,\n    'n_fold': 5,\n    'seed': 2021,\n    'max_epochs': 20,\n}\n\nif TRAIN_MODEL:\n    # SAVE TRUE AND OOF\n    device = params['device']\n    true = np.array([])\n    oof = np.array([])\n\n    for fold in range(5):\n\n        # INDICES OF TRAIN AND VALID FOLDS\n        valid_idx = [2*fold+1, 2*fold+2]\n        train_idx = [x for x in [1,2,3,4,5,6,7,8,9,10] if x not in valid_idx]\n\n        print('#'*25)\n        print(f'### Fold {fold+1} with valid files', valid_idx)\n\n        # READ TRAIN DATA FROM DISK\n        X_train = []; y_train = []\n        for k in train_idx:\n            X_train.append( np.load(f'{PATH_TO_DATA}data_{k}.npy'))\n            y_train.append( pd.read_parquet(f'{PATH_TO_DATA}targets_{k}.pqt') )\n        X_train = np.concatenate(X_train,axis=0)\n        y_train = pd.concat(y_train).target.values\n        print('### Training data shapes', X_train.shape, y_train.shape)\n\n        # READ VALID DATA FROM DISK\n        X_valid = []; y_valid = []\n        for k in valid_idx:\n            X_valid.append( np.load(f'{PATH_TO_DATA}data_{k}.npy'))\n            y_valid.append( pd.read_parquet(f'{PATH_TO_DATA}targets_{k}.pqt') )\n        X_valid = np.concatenate(X_valid,axis=0)\n        y_valid = pd.concat(y_valid).target.values\n        print('### Validation data shapes', X_valid.shape, y_valid.shape)\n        print('#'*25)\n\n        # TRAIN MODEL\n        # loss_fn = nn.CrossEntropyLoss(weight=torch.from_numpy(np.array([118828, 340085])).float()).to(device)\n        loss_fn = nn.CrossEntropyLoss().to(device)\n        train_sample_strategy = torch.utils.data.sampler.WeightedRandomSampler(np.ones(X_train.shape[0]),\n                                                                               num_samples=X_train.shape[0], replacement=False)\n        train_dataloader = torch.utils.data.DataLoader(np.array(range(X_train.shape[0])), batch_size=params['batch_size'], num_workers=0,\n                                                       sampler=train_sample_strategy, drop_last=False)\n        val_sample_strategy = torch.utils.data.sampler.WeightedRandomSampler(np.ones(X_valid.shape[0]),\n                                                                             num_samples=X_valid.shape[0], replacement=False)\n        val_dataloader = torch.utils.data.DataLoader(np.array(range(X_valid.shape[0])), batch_size=params['batch_size'], num_workers=0,\n                                                     sampler=val_sample_strategy, drop_last=False)\n        oof_predictions = torch.zeros(X_valid.shape[0], 2).float().to(device)\n        model = eval(params['model'])(X_train.shape[-1]).to(device)\n        lr = params['lr'] * np.sqrt(params['batch_size']/2048)\n        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=params['wd'])\n        lr_scheduler = MultiStepLR(optimizer=optimizer, milestones=[3600, 5000, 6000], gamma=0.1)\n        earlystoper = early_stopper(patience=params['early_stopping'], verbose=True)\n        start_epoch = 0\n        for epoch in range(start_epoch, params['max_epochs']):\n            train_loss_list = []\n            # train_acc_list = []\n            model.train()\n            for step, input_seeds in enumerate(train_dataloader):\n                batch_inputs = torch.from_numpy(X_train[input_seeds]).to(device)\n                batch_labels = torch.from_numpy(y_train[input_seeds]).to(device).long()\n                model.hidden_state = model.init_hidden(len(input_seeds), device)\n                train_batch_logits = model(batch_inputs)\n                train_loss = loss_fn(train_batch_logits, batch_labels)\n                # backward\n                optimizer.zero_grad()\n                train_loss.backward()\n                optimizer.step()\n                lr_scheduler.step()\n                train_loss_list.append(train_loss.cpu().detach().numpy())\n                \n                #tr_batch_pred = None\n\n                if step % 50 == 0:\n                    tr_batch_pred = torch.sum(torch.argmax(train_batch_logits.clone().detach(), dim=1) == batch_labels) / batch_labels.shape[0]\n                    score = torch.softmax(train_batch_logits.clone().detach(), dim=1)[:, 1].cpu().numpy()\n                    print('In epoch:{:03d}|batch:{:04d}, train_loss:{:4f}, '\n                          'train_ap:{:.4f}, train_acc:{:.4f}, train_auc:{:.4f}'.format(epoch,step,\n                                                                                       np.mean(train_loss_list),\n                                                                                       average_precision_score(batch_labels.cpu().numpy(), score), \n                                                                                       tr_batch_pred.detach(),\n                                                                                       roc_auc_score(batch_labels.cpu().numpy(), score)))\n        \n            # mini-batch for validation\n            val_loss_list = 0\n            val_acc_list = 0\n            #val_correct_list = 0\n            val_all_list = 0\n            model.eval()\n            with torch.no_grad():\n                for step, input_seeds in enumerate(val_dataloader):\n                    batch_inputs = torch.from_numpy(X_valid[input_seeds]).to(device)\n                    batch_labels = torch.from_numpy(y_valid[input_seeds]).to(device).long()\n                    model.hidden_state = model.init_hidden(len(input_seeds), device)\n                    val_batch_logits = model(batch_inputs)\n                    oof_predictions[input_seeds] = val_batch_logits\n                    val_loss_list = val_loss_list + loss_fn(val_batch_logits, batch_labels)\n                    val_batch_pred = torch.sum(torch.argmax(val_batch_logits, dim=1) == batch_labels) / torch.tensor(batch_labels.shape[0])\n                    val_acc_list = val_acc_list + val_batch_pred * torch.tensor(batch_labels.shape[0])\n                    val_all_list = val_all_list + batch_labels.shape[0]\n                    if step % 50 == 0:\n                        score = torch.softmax(val_batch_logits.clone().detach(), dim=1)[:, 1].cpu().numpy()\n                        print('In epoch:{:03d}|batch:{:04d}, val_loss:{:4f}, val_ap:{:.4f}, '\n                              'val_acc:{:.4f}, val_auc:{:.4f}'.format(epoch,\n                                                                      step,\n                                                                      val_loss_list/val_all_list,\n                                                                      average_precision_score(batch_labels.cpu().numpy(), score), \n                                                                      val_batch_pred.detach(),\n                                                                      roc_auc_score(batch_labels.cpu().numpy(), score)))\n                #tmp_predictions = model(test_feature).cpu().numpy()\n            #infold_preds[fold] = tmp_predictions\n            #test_predictions += tmp_predictions / params['n_fold']\n            val_predictions = torch.softmax(oof_predictions.detach(), dim=-1)[:, 1].cpu().numpy()\n            earlystoper.earlystop(val_loss_list, amex_metric(y_valid, val_predictions), model)\n            if earlystoper.is_earlystop:\n                print(\"Early Stopping!\")\n                break\n        print(\"Best val_metric is: {:.7f}\".format(earlystoper.best_cv))\n        if not os.path.exists(PATH_TO_MODEL): os.makedirs(PATH_TO_MODEL)\n        torch.save(earlystoper.best_model.to('cpu').state_dict(), f'{PATH_TO_MODEL}gru_fold_{fold+1}.h5')\n\n        # INFER VALID DATA\n        print('Inferring validation data...')\n        # mini-batch for validation\n        val_loss_list = 0\n        val_acc_list = 0\n        #val_correct_list = 0\n        val_all_list = 0\n        model.load_state_dict(torch.load(f'{PATH_TO_MODEL}gru_fold_{fold+1}.h5'))\n        model.eval()\n        with torch.no_grad():\n            for step, input_seeds in enumerate(val_dataloader):\n                batch_inputs = torch.from_numpy(X_valid[input_seeds]).to(device)\n                batch_labels = torch.from_numpy(y_valid[input_seeds]).to(device).long()\n                model.hidden_state = model.init_hidden(len(input_seeds), device)\n                val_batch_logits = model(batch_inputs)\n                oof_predictions[input_seeds] = val_batch_logits\n                val_loss_list = val_loss_list + loss_fn(val_batch_logits, batch_labels)\n                val_batch_pred = torch.sum(torch.argmax(val_batch_logits, dim=1) == batch_labels) / torch.tensor(batch_labels.shape[0])\n                val_acc_list = val_acc_list + val_batch_pred * torch.tensor(batch_labels.shape[0])\n                val_all_list = val_all_list + batch_labels.shape[0]\n                if step % 50 == 0:\n                    score = torch.softmax(val_batch_logits.clone().detach(), dim=1)[:, 1].cpu().numpy()\n                    print('In epoch:{:03d}|batch:{:04d}, val_loss:{:4f}, val_ap:{:.4f}, '\n                          'val_acc:{:.4f}, val_auc:{:.4f}'.format(epoch,\n                                                                  step,\n                                                                  val_loss_list/val_all_list,\n                                                                  average_precision_score(batch_labels.cpu().numpy(), score), \n                                                                  val_batch_pred.detach(),\n                                                                  roc_auc_score(batch_labels.cpu().numpy(), score)))\n        val_predictions = torch.softmax(oof_predictions.detach(), dim=-1)[:, 1].cpu().numpy()\n        print()\n        print(f'Fold {fold+1} CV=', amex_metric(y_valid, val_predictions) )\n        print()\n        true = np.concatenate([true, y_valid])\n        oof = np.concatenate([oof, val_predictions])\n        \n        # CLEAN MEMORY\n        del model, X_train, y_train, X_valid, y_valid\n        gc.collect()\n\n    # PRINT OVERALL RESULTS\n    print('#'*25)\n    print(f'Overall CV =', amex_metric(true, oof) )","metadata":{"scrolled":true,"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Process Test Data\nWe process the test data in the same way as train data.","metadata":{}},{"cell_type":"code","source":"if PROCESS_DATA:\n    # GET TEST COLUMN NAMES\n    test = cudf.read_csv('../input/amex-default-prediction/test_data.csv', nrows=1)\n    T_COLS = test.columns\n    print(f'There are {len(T_COLS)} test dataframe columns')\n    \n    # GET TEST CUSTOMER NAMES (use pandas to avoid memory error)\n    if PATH_TO_CUSTOMER_HASHES:\n        test = cudf.read_parquet(f'{PATH_TO_CUSTOMER_HASHES}test_customer_hashes.pqt')\n    else:\n        test = pd.read_csv('/raid/Kaggle/amex/test_data.csv', usecols=['customer_ID'])\n        test['customer_ID'] = test['customer_ID'].apply(lambda x: int(x[-16:],16) ).astype('int64')\n    customers = test.drop_duplicates().sort_index().values.flatten()\n    print(f'There are {len(customers)} unique customers in test.')","metadata":{"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NUM_FILES = 20\nif PROCESS_DATA:\n    # CALCULATE SIZE OF EACH SEPARATE FILE\n    rows = get_rows(customers, test, NUM_FILES = NUM_FILES, verbose = 'test')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if PROCESS_DATA:\n    # SAVE TEST CUSTOMERS INDEX\n    test_customer_hashes = cupy.array([],dtype='int64')\n    \n    # CREATE PROCESSED TEST FILES AND SAVE TO DISK\n    for k in range(NUM_FILES):\n\n        # READ CHUNK OF TEST CSV FILE\n        skip = int(np.sum( rows[:k] ) + 1) #the plus one is for skipping header\n        test = cudf.read_csv('../input/amex-default-prediction/test_data.csv', nrows=rows[k], \n                              skiprows=skip, header=None, names=T_COLS)\n\n        # FEATURE ENGINEER DATAFRAME\n        test = feature_engineer(test, targets = None)\n        \n        # SAVE TEST CUSTOMERS INDEX\n        cust = test[['customer_ID']].drop_duplicates().sort_index().values.flatten()\n        test_customer_hashes = cupy.concatenate([test_customer_hashes,cust])\n\n        # SAVE FILES\n        print(f'Test_File_{k+1} has {test.customer_ID.nunique()} customers and shape',test.shape)\n        data = test.iloc[:,1:].values.reshape((-1,13,188))\n        cupy.save(f'{PATH_TO_DATA}test_data_{k+1}',data.astype('float32'))\n        \n    # SAVE CUSTOMER INDEX OF ALL TEST FILES\n    cupy.save(f'{PATH_TO_DATA}test_hashes_data', test_customer_hashes)\n\n    # CLEAN MEMORY\n    del test, data\n    gc.collect()","metadata":{"scrolled":true,"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Infer Test Data\nWe infer the test data from our saved fold models. If you don't wish to infer test but you only want your notebook to compute a validation score to evaluate model changes, then set variable `INFER_TEST = False` in the beginning of this notebook. Also if you wish to infer from previously trained models, then add the path to the Kaggle dataset in the variable `PATH_TO_MODEL` in the beginning of this notebook.","metadata":{}},{"cell_type":"code","source":"if INFER_TEST:\n    # INFER TEST DATA\n    start = 0; end = 0\n    sub = pd.read_csv('../input/amex-default-prediction/sample_submission.csv')\n    NUM_FILES = 20\n    # REARANGE SUB ROWS TO MATCH PROCESSED TEST FILES\n    sub['hash'] = sub['customer_ID'].str[-16:].apply(lambda x: int(x, 16)).astype('int64')\n    test_hash_index = np.load(f'{PATH_TO_DATA}test_hashes_data.npy')\n    sub = sub.set_index('hash').loc[test_hash_index].reset_index(drop=True)\n    \n    for k in range(NUM_FILES):\n        # LOAD TEST DATA\n        print(f'Inferring Test_File_{k+1}')\n        X_test = np.load(f'{PATH_TO_DATA}test_data_{k+1}.npy')\n        end = start + X_test.shape[0]\n\n        # BUILD MODEL\n        model = eval(params['model'])(X_test.shape[-1]).to(device)\n        \n        # INFER 5 FOLD MODELS\n        model.load_state_dict(torch.load(f'{PATH_TO_MODEL}gru_fold_1.h5'))\n        test_predictions = torch.zeros(X_test.shape[0], 2).to(device).float()\n        test_sample_strategy = torch.utils.data.sampler.WeightedRandomSampler(np.ones(X_test.shape[0]),\n                                                                              num_samples=X_test.shape[0], replacement=False)\n        test_dataloader = torch.utils.data.DataLoader(np.array(range(X_test.shape[0])), batch_size=params['batch_size'], num_workers=0,\n                                                      sampler=test_sample_strategy, drop_last=False)\n        \n        for j in range(1,5):\n            model.load_state_dict(torch.load(f'{PATH_TO_MODEL}gru_fold_{j+1}.h5'))\n            with torch.no_grad():\n                for step, input_seeds in enumerate(test_dataloader):\n                    batch_inputs = torch.from_numpy(X_test[input_seeds]).to(device).float()\n                    model.hidden_state = model.init_hidden(len(input_seeds), device)\n                    test_batch_logits = model(batch_inputs)\n                    test_predictions[input_seeds] = test_predictions[input_seeds] + torch.softmax(test_batch_logits, dim=-1)\n                    #test_batch_pred = torch.sum(torch.argmax(test_batch_logits, dim=1) == batch_labels) / torch.tensor(batch_labels.shape[0])\n                    if step % 50 == 0:\n                        print('In fold {} test batch:{:04d}'.format(j+1, step))\n        test_predictions /= 5.0\n\n        # SAVE TEST PREDICTIONS\n        sub.loc[start:end-1,'prediction'] = test_predictions[:, 1].cpu().numpy()\n        start = end\n        \n        # CLEAN MEMORY\n        del model, X_test\n        gc.collect()\n","metadata":{"scrolled":true,"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create Submission","metadata":{}},{"cell_type":"code","source":"if INFER_TEST:\n    sub.to_csv('submission.csv',index=False)\n    print('Submission file shape is', sub.shape )\n    display( sub.head() )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if INFER_TEST:\n    # DISPLAY SUBMISSION PREDICTIONS\n    plt.hist(sub.prediction, bins=100)\n    plt.title('Test Predictions')\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}