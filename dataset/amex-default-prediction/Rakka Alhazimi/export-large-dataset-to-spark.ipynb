{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Export Large Dataset to Spark\nThe current competition data file size is about 30GB+. We can't use `pandas` because it will take a long time to even read it. You can switch to `dask` or `pyspark` for this kind of problem. In this notebook I will choose `pyspark` because I want to apply what have I learned about data engineering.\n\nThe real reason we use `pyspark` is because it runs operations using multiple machine while `pandas` only use single machine. `pyspark` can perform lazy operation so that we have no to wait every operations to be finished. If you try to read the data using `pandas` it will take a long long time to even finished the read operations, that isn't a good practice. Some may use `cudf` and that is quite a good idea, but we will stick to `pyspark`.\n\nThe downside is `pyspark` have less algorithms than `pandas`, it might restraining our flexibility.","metadata":{}},{"cell_type":"markdown","source":"## Install pyspark","metadata":{}},{"cell_type":"code","source":"!pip install -q pyspark","metadata":{"execution":{"iopub.status.busy":"2022-06-17T22:38:19.452611Z","iopub.execute_input":"2022-06-17T22:38:19.453067Z","iopub.status.idle":"2022-06-17T22:39:19.291998Z","shell.execute_reply.started":"2022-06-17T22:38:19.452977Z","shell.execute_reply":"2022-06-17T22:39:19.290673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Import Libraries","metadata":{}},{"cell_type":"code","source":"import os\nfrom pprint import pprint\n\nimport pandas as pd\nfrom pyspark.sql import SparkSession, types","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-17T22:39:19.295023Z","iopub.execute_input":"2022-06-17T22:39:19.295516Z","iopub.status.idle":"2022-06-17T22:39:19.373824Z","shell.execute_reply.started":"2022-06-17T22:39:19.295462Z","shell.execute_reply":"2022-06-17T22:39:19.372855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Spark Session\nIn order to use `pyspark` we need to create or get `spark` instance.","metadata":{}},{"cell_type":"code","source":"spark = SparkSession.builder.master(\"local[*]\").getOrCreate()","metadata":{"execution":{"iopub.status.busy":"2022-06-17T22:39:19.37501Z","iopub.execute_input":"2022-06-17T22:39:19.375789Z","iopub.status.idle":"2022-06-17T22:39:24.843989Z","shell.execute_reply.started":"2022-06-17T22:39:19.375754Z","shell.execute_reply":"2022-06-17T22:39:24.842747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Infer Data Types\nWhen we read the data directly with `pyspark` it regards all the data types as string. While for `pandas`, it tries to infer what the data types by the value. We will use `pandas` for this task, but we can't afford to read all the data by whole. However, we only read the first n rows of the data and fetch all the data types.","metadata":{}},{"cell_type":"code","source":"# Define data paths\ntest_path = \"../input/amex-default-prediction/test_data.csv\"\ntrain_path = \"../input/amex-default-prediction/train_data.csv\"\nlabel_path = \"../input/amex-default-prediction/train_labels.csv\"","metadata":{"execution":{"iopub.status.busy":"2022-06-17T22:39:24.851046Z","iopub.execute_input":"2022-06-17T22:39:24.851549Z","iopub.status.idle":"2022-06-17T22:39:24.856483Z","shell.execute_reply.started":"2022-06-17T22:39:24.851495Z","shell.execute_reply":"2022-06-17T22:39:24.855792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read data\ntrain_df = pd.read_csv(train_path, nrows=100)\ntest_df = pd.read_csv(test_path, nrows=100)\nlabel_df = pd.read_csv(label_path, nrows=100)","metadata":{"execution":{"iopub.status.busy":"2022-06-17T22:39:24.8578Z","iopub.execute_input":"2022-06-17T22:39:24.858357Z","iopub.status.idle":"2022-06-17T22:39:24.962718Z","shell.execute_reply.started":"2022-06-17T22:39:24.858313Z","shell.execute_reply":"2022-06-17T22:39:24.961958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get all data types\n\n## Train types\ntrain_types = train_df.dtypes\ntrain_types_count = train_types.value_counts()\n\n## Test types\ntest_types = test_df.dtypes\ntest_types_count = test_types.value_counts()\n\n## Label types\nlabel_types = label_df.dtypes\nlabel_types_count = label_types.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-06-17T22:39:24.964046Z","iopub.execute_input":"2022-06-17T22:39:24.965022Z","iopub.status.idle":"2022-06-17T22:39:24.979398Z","shell.execute_reply.started":"2022-06-17T22:39:24.964975Z","shell.execute_reply":"2022-06-17T22:39:24.978543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def print_splits(*msg):\n    for m in msg:\n        print(m)\n        print()","metadata":{"execution":{"iopub.status.busy":"2022-06-17T22:39:24.981305Z","iopub.execute_input":"2022-06-17T22:39:24.982171Z","iopub.status.idle":"2022-06-17T22:39:24.986638Z","shell.execute_reply.started":"2022-06-17T22:39:24.982139Z","shell.execute_reply":"2022-06-17T22:39:24.9856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We get all the data types","metadata":{}},{"cell_type":"code","source":"print_splits(train_types_count, test_types_count, label_types_count)","metadata":{"execution":{"iopub.status.busy":"2022-06-17T22:39:24.988376Z","iopub.execute_input":"2022-06-17T22:39:24.988859Z","iopub.status.idle":"2022-06-17T22:39:24.999201Z","shell.execute_reply.started":"2022-06-17T22:39:24.988814Z","shell.execute_reply":"2022-06-17T22:39:24.997924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create Schemas\nAfter we have retrieved the data types, we can create spark schema by creating `StructType` instances with every column as the argument. Every column is defined using `StructField` instance, it receives 3 arguments `colname`, `data types` and `nullable`.","metadata":{}},{"cell_type":"code","source":"# Types mapper\ntypes_map = {\n    \"object\": types.StringType(),\n    \"float64\": types.FloatType(),\n    \"int64\": types.IntegerType(),\n}\n\n# Known dtypes\nstring_dtypes = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']\ndate_dtypes = ['S_2']","metadata":{"execution":{"iopub.status.busy":"2022-06-17T22:39:25.000568Z","iopub.execute_input":"2022-06-17T22:39:25.001143Z","iopub.status.idle":"2022-06-17T22:39:25.009783Z","shell.execute_reply.started":"2022-06-17T22:39:25.001085Z","shell.execute_reply":"2022-06-17T22:39:25.008988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_spark_schema(series):\n    fields = []\n    \n    for index, value in series.items():\n        if index in string_dtypes:\n            field = types.StructField(index, types.StringType(), True)\n            \n        elif index in date_dtypes:\n            field = types.StructField(index, types.DateType(), True)\n        \n        else:\n            field = types.StructField(index, types_map.get(str(value)), True)\n            \n        fields.append(field)\n    return types.StructType(fields)","metadata":{"execution":{"iopub.status.busy":"2022-06-17T22:39:25.010838Z","iopub.execute_input":"2022-06-17T22:39:25.011729Z","iopub.status.idle":"2022-06-17T22:39:25.021136Z","shell.execute_reply.started":"2022-06-17T22:39:25.011698Z","shell.execute_reply":"2022-06-17T22:39:25.020201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_schema = create_spark_schema(train_types) \ntest_schema = create_spark_schema(test_types)\nlabel_schema = create_spark_schema(label_types)","metadata":{"execution":{"iopub.status.busy":"2022-06-17T22:39:25.023163Z","iopub.execute_input":"2022-06-17T22:39:25.023845Z","iopub.status.idle":"2022-06-17T22:39:25.038942Z","shell.execute_reply.started":"2022-06-17T22:39:25.023815Z","shell.execute_reply":"2022-06-17T22:39:25.037889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Read with Pyspark","metadata":{}},{"cell_type":"code","source":"# Set header to True or else it will be included as row\ntrain_psdf = spark.read.option(\"header\", \"true\").csv(train_path, schema=train_schema)\ntest_psdf = spark.read.option(\"header\", \"true\").csv(test_path, schema=test_schema)\nlabel_psdf = spark.read.option(\"header\", \"true\").csv(label_path, schema=label_schema)","metadata":{"execution":{"iopub.status.busy":"2022-06-17T22:39:25.040336Z","iopub.execute_input":"2022-06-17T22:39:25.040971Z","iopub.status.idle":"2022-06-17T22:39:28.852731Z","shell.execute_reply.started":"2022-06-17T22:39:25.040938Z","shell.execute_reply":"2022-06-17T22:39:28.851614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check schema\nprint_splits(train_psdf.schema[:3], test_psdf.schema[:3], label_psdf.schema[:3])","metadata":{"execution":{"iopub.status.busy":"2022-06-17T22:39:28.85402Z","iopub.execute_input":"2022-06-17T22:39:28.85447Z","iopub.status.idle":"2022-06-17T22:39:29.00729Z","shell.execute_reply.started":"2022-06-17T22:39:28.854424Z","shell.execute_reply":"2022-06-17T22:39:29.00612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Counts Data\nThe data is about 5 millions rows with 100+ columns","metadata":{}},{"cell_type":"code","source":"train_psdf.count()","metadata":{"execution":{"iopub.status.busy":"2022-06-17T22:39:29.009333Z","iopub.execute_input":"2022-06-17T22:39:29.009834Z","iopub.status.idle":"2022-06-17T22:40:34.533321Z","shell.execute_reply.started":"2022-06-17T22:39:29.009785Z","shell.execute_reply":"2022-06-17T22:40:34.532219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Save as Parquet\nWe choose `.parquet` as the file extension because it uses less disk memory.","metadata":{}},{"cell_type":"code","source":"(train_psdf.write\n           .partitionBy(\"S_2\")\n           .parquet(\"train_amex\"))\n\n(test_psdf.write\n          .partitionBy(\"S_2\")\n          .parquet(\"test_amex\"))\n\nlabel_psdf.write.parquet(\"label_amex\")","metadata":{"execution":{"iopub.status.busy":"2022-06-17T22:43:22.0427Z","iopub.execute_input":"2022-06-17T22:43:22.04312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## What to do next?\n1. You can read the exported data with pyspark API it can be `pyspark.sql`, `pyspark.pandas`, or `pyspark.rdd`\n2. Then perform preprocesing, you can see the related notebook from [Michal Slapek](https://www.kaggle.com/capslock): [here](https://www.kaggle.com/code/capslock/amex-export-to-parquet-with-apache-spark)\n\nMy coverage notebook about doing ML in pyspark for this competitions is still in progress. If anybody have done it I would like to know :D how it is done.","metadata":{}},{"cell_type":"markdown","source":"## Update\n\n**Changes:**\n- v.4 Added `.option(\"header\", \"true\")` to not include header as row\n- v.5 Update schema with known dtypes\n- v.6 Failed Run\n- v.7 Change the title to avoid misleading (the preprocessing notebook is still in progress)\n- v.8 PartitionBy column \"S_2\"\n\n\nGood luck for the competitions :D.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}