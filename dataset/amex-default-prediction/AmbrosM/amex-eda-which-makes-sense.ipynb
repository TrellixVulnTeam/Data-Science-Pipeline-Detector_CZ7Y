{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# AMEX EDA which makes sense ⭐️⭐️⭐️⭐️⭐️\n\nThis EDA analyzes the data and gives some insight which is useful for designing a machine learning pipeline and selecting a model.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport pickle, gc\nfrom matplotlib import pyplot as plt","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-02T19:43:10.135118Z","iopub.execute_input":"2022-06-02T19:43:10.13568Z","iopub.status.idle":"2022-06-02T19:43:10.151289Z","shell.execute_reply.started":"2022-06-02T19:43:10.135558Z","shell.execute_reply":"2022-06-02T19:43:10.150271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The labels\n\nWe start by reading the labels for the training data. There are neither missing values nor duplicated customer_IDs. Of the 458913 customer_IDs, 340000 (74 %) have a label of 0 (good customer, no default) and 119000 (26 %) have a label of 1 (bad customer, default).\n\nWe know that the good customers have been subsampled by a factor of 20; this means that in reality there are 6.8 million good customers. 98 % of the customers are good; 2 % are bad.\n\n**Insight:**\n- The classes are imbalanced. A StratifiedKFold for cross-validation is recommended.\n- Because the classes are imbalanced, accuracy would be a bad metric to evaluate a classifier. The [competition metric](https://www.kaggle.com/competitions/amex-default-prediction/discussion/327464) is a mix of area under the roc curve (auc) and recall.","metadata":{}},{"cell_type":"code","source":"train_labels = pd.read_csv('../input/amex-default-prediction/train_labels.csv')\ntrain_labels.head(2)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T19:43:10.178083Z","iopub.execute_input":"2022-06-02T19:43:10.178779Z","iopub.status.idle":"2022-06-02T19:43:11.421722Z","shell.execute_reply.started":"2022-06-02T19:43:10.178734Z","shell.execute_reply":"2022-06-02T19:43:11.420356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check for missing data and duplicated customer_IDs\ntrain_labels.isna().any().any(), train_labels.customer_ID.duplicated().any()","metadata":{"execution":{"iopub.status.busy":"2022-06-02T19:43:11.423928Z","iopub.execute_input":"2022-06-02T19:43:11.42458Z","iopub.status.idle":"2022-06-02T19:43:11.596Z","shell.execute_reply.started":"2022-06-02T19:43:11.424497Z","shell.execute_reply":"2022-06-02T19:43:11.594762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_stats = pd.DataFrame({'absolute': train_labels.target.value_counts(),\n              'relative': train_labels.target.value_counts() / len(train_labels)})\nlabel_stats['absolute upsampled'] =  label_stats.absolute * np.array([20, 1])\nlabel_stats['relative upsampled'] = label_stats['absolute upsampled'] / label_stats['absolute upsampled'].sum()\nlabel_stats","metadata":{"execution":{"iopub.status.busy":"2022-06-02T19:43:11.59769Z","iopub.execute_input":"2022-06-02T19:43:11.598103Z","iopub.status.idle":"2022-06-02T19:43:11.621986Z","shell.execute_reply.started":"2022-06-02T19:43:11.598066Z","shell.execute_reply":"2022-06-02T19:43:11.620917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The data\n\nThe dataset of this competition has a considerable size. If you read the original csv files, the data barely fits into memory. That's why we read the data from @munumbutt's [AMEX-Feather-Dataset](https://www.kaggle.com/datasets/munumbutt/amexfeather). In this [Feather](https://arrow.apache.org/docs/python/feather.html) file, the floating point precision has been reduced from 64 bit to 16 bit. And reading a Feather file is faster than reading a csv file because the Feather file format is binary.\n\nThere are 5.5 million rows for training and 11 million rows of test data.","metadata":{}},{"cell_type":"code","source":"%%time\ntrain = pd.read_feather('../input/amexfeather/train_data.ftr')\ntest = pd.read_feather('../input/amexfeather/test_data.ftr')\nwith pd.option_context(\"display.min_rows\", 6):\n    display(train)\n    display(test)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T19:43:11.624623Z","iopub.execute_input":"2022-06-02T19:43:11.625141Z","iopub.status.idle":"2022-06-02T19:44:03.622522Z","shell.execute_reply.started":"2022-06-02T19:43:11.625091Z","shell.execute_reply":"2022-06-02T19:44:03.620678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The target column of the train dataframe corresponds to the target column of train_labels.csv. In the csv file of the train data, there is no target column; it has been joined into the Feather file as a convenience.\n\nS_2 is the statement date. All train statement dates are between March of 2017 and March of 2018 (13 months), and no statement dates are missing. All test statement dates are between April of 2018 and October of 2019. This means that the statement dates of train and test don't overlap:","metadata":{}},{"cell_type":"code","source":"print('Train statement dates: ', train.S_2.min(), train.S_2.max(), train.S_2.isna().any())\nprint('Test statement dates: ',  test.S_2.min(), test.S_2.max(), test.S_2.isna().any())\n","metadata":{"execution":{"iopub.status.busy":"2022-06-02T19:44:03.624506Z","iopub.execute_input":"2022-06-02T19:44:03.625127Z","iopub.status.idle":"2022-06-02T19:44:03.791136Z","shell.execute_reply.started":"2022-06-02T19:44:03.625066Z","shell.execute_reply":"2022-06-02T19:44:03.78984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\n**Insight:**\n- The test data come from a different phase in the economic cycle than the training data. Our models have no way of learning the effect of the economic cycle.","metadata":{}},{"cell_type":"code","source":"print(f'Train data memory usage: {train.memory_usage().sum() / 1e9} GBytes')\nprint(f'Test data memory usage:  {test.memory_usage().sum() / 1e9} GBytes')\n","metadata":{"execution":{"iopub.status.busy":"2022-06-02T19:44:03.792949Z","iopub.execute_input":"2022-06-02T19:44:03.793425Z","iopub.status.idle":"2022-06-02T19:44:03.838788Z","shell.execute_reply.started":"2022-06-02T19:44:03.79338Z","shell.execute_reply":"2022-06-02T19:44:03.837017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The training data takes 2.2 GBytes of RAM. The test data is twice the size of the training data.\n\n**Insight:**\n- With that much data, we need to have an eye on memory efficiency. Avoid keeping unnecessary copies of the data in memory, and avoid keeping unnecessary copies of models!\n- Whereas most machine learning algorithms expect the whole training data to be in memory, we don't need to load all the test data at once. The test data can be processed in batches.\n- You may want to separate training and inference code into two notebooks so that you never have training and test data in memory at the same time.","metadata":{}},{"cell_type":"markdown","source":"The info function shows that most other features have missing values:\n","metadata":{}},{"cell_type":"code","source":"train.info(max_cols=200, show_counts=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T19:44:03.840726Z","iopub.execute_input":"2022-06-02T19:44:03.841631Z","iopub.status.idle":"2022-06-02T19:44:11.008171Z","shell.execute_reply.started":"2022-06-02T19:44:03.841566Z","shell.execute_reply":"2022-06-02T19:44:11.00666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Insight:**\n- There are many columns with missing values: Dropping all columns which have missing values is not a sensible strategy.\n- There are many rows with missing values: Dropping all rows which have missing values is not a sensible strategy.\n- Many decision-tree based algorithms can deal with missing values. If we choose such a model, we don't need to change the missing values.\n- Neural networks and other estimators cannot deal with missing values. If we choose such a model, we need to impute values. See [this guide](https://www.kaggle.com/code/parulpandey/a-guide-to-handling-missing-values-in-python) for an overview of the many imputation options.\n- Most features are 16-bit floats. The original data (in the csv file) has higher precision. By rounding it to 16-bit precision, some information is lost. To make this information loss more tangible: Every float16 number between 1 and 2 is a multiple of 1/1024. These numbers have only three digits behind the decimal point! This precision is enough to start the competition; maybe we'll have to switch to higher precision towards the end.","metadata":{}},{"cell_type":"markdown","source":"# Counting the statements per customer","metadata":{}},{"cell_type":"markdown","source":"Now we can count how many rows (credit card statements) there are per customer. We see that 80 % of the customers have 13 statements; the other 20 % of the customers have between 1 and 12 statements.\n\n**Insight:** Our model will have to deal with a variable-sized input per customer (unless we simplify our life and look only at the most recent statement as @inversion suggests [here](https://www.kaggle.com/competitions/amex-default-prediction/discussion/327094) or at the average over all statements).","metadata":{}},{"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\ntrain_sc = train.customer_ID.value_counts().value_counts().sort_index(ascending=False).rename('Train statements per customer')\nax1.pie(train_sc, labels=train_sc.index)\nax1.set_title(train_sc.name)\ntest_sc = test.customer_ID.value_counts().value_counts().sort_index(ascending=False).rename('Test statements per customer')\nax2.pie(test_sc, labels=test_sc.index)\nax2.set_title(test_sc.name)\nplt.show()\n\n# display(train.customer_ID.value_counts().value_counts().sort_index(ascending=False).rename('Train statements per customer'))\n# display(train.customer_ID.value_counts().value_counts().sort_index(ascending=False).rename('Test statements per customer'))\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-02T19:44:11.009849Z","iopub.execute_input":"2022-06-02T19:44:11.01042Z","iopub.status.idle":"2022-06-02T19:44:15.079693Z","shell.execute_reply.started":"2022-06-02T19:44:11.010378Z","shell.execute_reply":"2022-06-02T19:44:15.078394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's find out when these customers got their last statement. The histogram of the last statement dates shows that every train customer got his last statement in March of 2018. The first four Saturdays (March 3, 10, 17, 24) have more statements than an average day.\n\nThe test customers are split in two: half of them got their last statement in April of 2019 and half in October of 2019. As was [discussed here](https://www.kaggle.com/competitions/amex-default-prediction/discussion/327602), the April 2019 data is used for the public leaderboard and the October 2019 data is used for the private leaderboard.","metadata":{}},{"cell_type":"code","source":"temp = train.S_2.groupby(train.customer_ID).max()\nplt.figure(figsize=(16, 4))\nplt.hist(temp, bins=pd.date_range(\"2018-03-01\", \"2018-04-01\", freq=\"d\"),\n         rwidth=0.8, color='#ffd700')\nplt.title('When did the train customers get their last statements?', fontsize=20)\nplt.xlabel('Last statement date per customer')\nplt.ylabel('Count')\nplt.gca().set_facecolor('#0057b8')\nplt.show()\ndel temp\n\ntemp = test.S_2.groupby(test.customer_ID).max()\nplt.figure(figsize=(16, 4))\nplt.hist(temp, bins=pd.date_range(\"2019-04-01\", \"2019-11-01\", freq=\"d\"),\n         rwidth=0.74, color='#ffd700')\nplt.title('When did the test customers get their last statements?', fontsize=20)\nplt.xlabel('Last statement date per customer')\nplt.ylabel('Count')\nplt.gca().set_facecolor('#0057b8')\nplt.show()\ndel temp","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-02T19:44:15.081204Z","iopub.execute_input":"2022-06-02T19:44:15.081658Z","iopub.status.idle":"2022-06-02T19:44:21.455791Z","shell.execute_reply.started":"2022-06-02T19:44:15.081615Z","shell.execute_reply":"2022-06-02T19:44:21.453797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Insight:** Although the data are a kind of time series, we cannot cross-validate with a TimeSeriesSplit because all training happens in the same month.\n\nFor most customers, the first and last statement is about a year apart. Together with the fact that we typically have 13 statements per customer, this indicates that the customers get one credit card statement every month.","metadata":{}},{"cell_type":"code","source":"temp = train.S_2.groupby(train.customer_ID).agg(['max', 'min'])\nplt.figure(figsize=(16, 3))\nplt.hist((temp['max'] - temp['min']).dt.days, bins=400, color='#ffd700')\nplt.xlabel('days')\nplt.ylabel('count')\nplt.title('Number of days between first and last statement of customer (train)', fontsize=20)\nplt.gca().set_facecolor('#0057b8')\nplt.show()\n\ntemp = test.S_2.groupby(test.customer_ID).agg(['max', 'min'])\nplt.figure(figsize=(16, 3))\nplt.hist((temp['max'] - temp['min']).dt.days, bins=400, color='#ffd700')\nplt.xlabel('days')\nplt.ylabel('count')\nplt.title('Number of days between first and last statement of customer (test)', fontsize=20)\nplt.gca().set_facecolor('#0057b8')\nplt.show()\ndel temp","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-02T19:44:21.460072Z","iopub.execute_input":"2022-06-02T19:44:21.460711Z","iopub.status.idle":"2022-06-02T19:44:28.841217Z","shell.execute_reply.started":"2022-06-02T19:44:21.460662Z","shell.execute_reply":"2022-06-02T19:44:28.839841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If we color every statement (i.e. row of train or test) according to the dataset it belongs (training, public lb, and private lb), we see that every dataset covers thirteen months. Train and test don't overlap, but public and private lb periods overlap.","metadata":{}},{"cell_type":"code","source":"temp = pd.concat([train[['customer_ID', 'S_2']], test[['customer_ID', 'S_2']]], axis=0)\ntemp.set_index('customer_ID', inplace=True)\ntemp['last_month'] = temp.groupby('customer_ID').S_2.max().dt.month\nlast_month = temp['last_month'].values\n\nplt.figure(figsize=(16, 4))\nplt.hist([temp.S_2[temp.last_month == 3],   # ending 03/18 -> training\n          temp.S_2[temp.last_month == 4],   # ending 04/19 -> public lb\n          temp.S_2[temp.last_month == 10]], # ending 10/19 -> private lb\n         bins=pd.date_range(\"2017-03-01\", \"2019-11-01\", freq=\"MS\"),\n         label=['Training', 'Public leaderboard', 'Private leaderboard'],\n         stacked=True)\nplt.xticks(pd.date_range(\"2017-03-01\", \"2019-11-01\", freq=\"QS\"))\nplt.xlabel('Statement date')\nplt.ylabel('Count')\nplt.title('The three datasets over time', fontsize=20)\nplt.legend()\nplt.show()\n","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-06-02T19:44:28.842728Z","iopub.execute_input":"2022-06-02T19:44:28.843169Z","iopub.status.idle":"2022-06-02T19:44:42.485935Z","shell.execute_reply.started":"2022-06-02T19:44:28.843129Z","shell.execute_reply":"2022-06-02T19:44:42.484865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we'll look at the distribution of missing values over time. B_29 is most interesting. Given the each of the three datasets has almost half a million customers, we see that until May of 2019 fewer than a tenth of the customers have a value for B_29. The other nine tenths are missing. Starting in June of 2019, we have B_29 data for almost every customer. \n\n**Insight:** The distribution of the missing B_29 differs between train and test datasets. Whereas in the training and public leaderboard data >90 % are missing, during the last five months of private leaderboard, we have B_29 data for almost every customer. If we use this feature, we should be prepared for surprises in the private leaderboard. Is it better to drop the feature?","metadata":{}},{"cell_type":"code","source":"for f in [ 'B_29', 'S_9','D_87']:#, 'D_88', 'R_26', 'R_27', 'D_108', 'D_110', 'D_111', 'B_39', 'B_42']:\n    temp = pd.concat([train[[f, 'S_2']], test[[f, 'S_2']]], axis=0)\n    temp['last_month'] = last_month\n    temp['has_f'] = ~temp[f].isna() \n\n    plt.figure(figsize=(16, 4))\n    plt.hist([temp.S_2[temp.has_f & (temp.last_month == 3)],   # ending 03/18 -> training\n              temp.S_2[temp.has_f & (temp.last_month == 4)],   # ending 04/19 -> public lb\n              temp.S_2[temp.has_f & (temp.last_month == 10)]], # ending 10/19 -> private lb\n             bins=pd.date_range(\"2017-03-01\", \"2019-11-01\", freq=\"MS\"),\n             label=['Training', 'Public leaderboard', 'Private leaderboard'],\n             stacked=True)\n    plt.xticks(pd.date_range(\"2017-03-01\", \"2019-11-01\", freq=\"QS\"))\n    plt.xlabel('Statement date')\n    plt.ylabel(f'Count of {f} non-null values')\n    plt.title(f'{f} non-null values over time', fontsize=20)\n    plt.legend()\n    plt.show()\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-02T19:44:42.487462Z","iopub.execute_input":"2022-06-02T19:44:42.487936Z","iopub.status.idle":"2022-06-02T19:44:47.74617Z","shell.execute_reply.started":"2022-06-02T19:44:42.487896Z","shell.execute_reply":"2022-06-02T19:44:47.744983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The categorical features\n\nAccording to the [data description](https://www.kaggle.com/competitions/amex-default-prediction/data), there are eleven categorical features. We plot histograms for target=0 and target=1. For the ten features which have missing values, the missing values are represented by the rightmost bar of the histogram.\n","metadata":{}},{"cell_type":"code","source":"cat_features = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']\nplt.figure(figsize=(16, 16))\nfor i, f in enumerate(cat_features):\n    plt.subplot(4, 3, i+1)\n    temp = pd.DataFrame(train[f][train.target == 0].value_counts(dropna=False, normalize=True).sort_index().rename('count'))\n    temp.index.name = 'value'\n    temp.reset_index(inplace=True)\n    plt.bar(temp.index, temp['count'], alpha=0.5, label='target=0')\n    temp = pd.DataFrame(train[f][train.target == 1].value_counts(dropna=False, normalize=True).sort_index().rename('count'))\n    temp.index.name = 'value'\n    temp.reset_index(inplace=True)\n    plt.bar(temp.index, temp['count'], alpha=0.5, label='target=1')\n    plt.xlabel(f)\n    plt.ylabel('frequency')\n    plt.legend()\n    plt.xticks(temp.index, temp.value)\nplt.suptitle('Categorical features', fontsize=20, y=0.93)\nplt.show()\ndel temp\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-02T19:44:47.747819Z","iopub.execute_input":"2022-06-02T19:44:47.74866Z","iopub.status.idle":"2022-06-02T19:44:51.043859Z","shell.execute_reply.started":"2022-06-02T19:44:47.748604Z","shell.execute_reply":"2022-06-02T19:44:51.042653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Insight:**\n- Every feature has at most eight categories (including a nan category). One-hot encodings are feasible.\n- The distributions for target=0 and target=1 differ. This means that every feature gives some information about the target.\n","metadata":{}},{"cell_type":"markdown","source":"# The binary features\n\nTwo features are binary:\n- B_31 is always 0 or 1.\n- D_87 is always 1 or missing.","metadata":{}},{"cell_type":"code","source":"bin_features = ['B_31', 'D_87']\nplt.figure(figsize=(16, 4))\nfor i, f in enumerate(bin_features):\n    plt.subplot(1, 2, i+1)\n    temp = pd.DataFrame(train[f][train.target == 0].value_counts(dropna=False, normalize=True).sort_index().rename('count'))\n    temp.index.name = 'value'\n    temp.reset_index(inplace=True)\n    plt.bar(temp.index, temp['count'], alpha=0.5, label='target=0')\n    temp = pd.DataFrame(train[f][train.target == 1].value_counts(dropna=False, normalize=True).sort_index().rename('count'))\n    temp.index.name = 'value'\n    temp.reset_index(inplace=True)\n    plt.bar(temp.index, temp['count'], alpha=0.5, label='target=1')\n    plt.xlabel(f)\n    plt.ylabel('frequency')\n    plt.legend()\n    plt.xticks(temp.index, temp.value)\nplt.suptitle('Binary features', fontsize=20)\nplt.show()\ndel temp","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-02T19:44:51.045522Z","iopub.execute_input":"2022-06-02T19:44:51.046533Z","iopub.status.idle":"2022-06-02T19:44:51.68708Z","shell.execute_reply.started":"2022-06-02T19:44:51.046463Z","shell.execute_reply":"2022-06-02T19:44:51.685745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Insight:** If you impute missing values for D_87, don't fall into the trap of imputing the mean - the feature would become useless...","metadata":{}},{"cell_type":"markdown","source":"# The numerical features\n\nIf we plot histograms of the 175 numerical features, we see that they have all kinds of distributions:","metadata":{}},{"cell_type":"code","source":"cont_features = sorted([f for f in train.columns if f not in cat_features + bin_features + ['customer_ID', 'target', 'S_2']])\nprint(len(cont_features))\n# print(cont_features)\nncols = 4\nfor i, f in enumerate(cont_features):\n    if i % ncols == 0: \n        if i > 0: plt.show()\n        plt.figure(figsize=(16, 3))\n        if i == 0: plt.suptitle('Continuous features', fontsize=20, y=1.02)\n    plt.subplot(1, ncols, i % ncols + 1)\n    plt.hist(train[f], bins=200)\n    plt.xlabel(f)\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-02T19:44:51.689198Z","iopub.execute_input":"2022-06-02T19:44:51.689678Z","iopub.status.idle":"2022-06-02T19:48:21.122144Z","shell.execute_reply.started":"2022-06-02T19:44:51.689634Z","shell.execute_reply":"2022-06-02T19:48:21.120764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Insight:** Histograms with white space at the left or right end can indicate that the data contain outliers. We will have to deal with these outliers. But are these data really outliers? Maybe they are, but they could as well be legitimate traces of rare events. We do not know...\n","metadata":{}},{"cell_type":"markdown","source":"# The artificial noise in the data\n\nThe data contain artificial noise. To understand the noise, we need to analyze the data at full precision. Because we cannot load the whole dataset at full precision into memory, we load only two interesting columns. For a complete analysis, see @raddar's [discussion post](https://www.kaggle.com/competitions/amex-default-prediction/discussion/328514).","metadata":{}},{"cell_type":"code","source":"def read_columns(name, features):\n    \"\"\"Read the specified columns of the train/test csv at full precision\"\"\"\n    chunksize = 1000000\n    chunklist = []\n    with pd.read_csv(f\"../input/amex-default-prediction/{name}_data.csv\", chunksize=chunksize) as reader:\n        for i, chunk in enumerate(reader):\n            chunk = chunk[features] # keep only selected columns\n            chunklist.append(chunk)\n            print(i, end=' ')\n            if i == 5: break\n        print()\n    df = pd.concat(chunklist, axis=0)\n    return df\n\ndf = read_columns('train', ['B_19', 'S_13'])\ndf.info()","metadata":{"execution":{"iopub.status.busy":"2022-06-02T19:48:21.124131Z","iopub.execute_input":"2022-06-02T19:48:21.124633Z","iopub.status.idle":"2022-06-02T19:55:35.354214Z","shell.execute_reply.started":"2022-06-02T19:48:21.12459Z","shell.execute_reply":"2022-06-02T19:55:35.352224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's look at B_19. All values are between 0 and 1.01. To get a high-resolution histogram, we spread it over eleven diagrams. The histogram show only rectangles of width 0.01, which indicate that the values are uniformly distributed in intervals of width 0.01, but every interval has another probability. This means that B_19 originally had some other range, but was scaled, rounded and got added some uniform noise by applying the following function:\n\n```\ndef anonymize(data):\n    data -= data.min()\n    data /= data.max()\n    data = data.round(2)\n    rng = np.random.default_rng()\n    data += rng.uniform(0, 0.01, len(data))\n    return data\n```\n","metadata":{}},{"cell_type":"code","source":"y = df.B_19\nfor i in np.linspace(0, 1, 11):\n    plt.figure(figsize=(16, 3))\n    plt.hist(y, bins=np.linspace(i, i+0.1, 101), rwidth=0.8, color='m')\n    plt.xticks(np.linspace(i, i+0.1, 11))\n    plt.title(f\"B_19 histogram, part {int(i*10+1)}\")\n    plt.show()\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-02T19:55:35.35652Z","iopub.execute_input":"2022-06-02T19:55:35.357183Z","iopub.status.idle":"2022-06-02T19:55:44.751775Z","shell.execute_reply.started":"2022-06-02T19:55:35.357121Z","shell.execute_reply":"2022-06-02T19:55:44.750341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Insight:** We don't care about the scaling, we cannot do anything against the rounding, but we should remove the artificial noise, e.g. by applying a function such as `x['B_19'] = x['B_19'].apply(lambda t: np.floor(t*100))`","metadata":{}},{"cell_type":"markdown","source":"S_13 looks similar, the rectangles again have width 0.01, but they start at multiples of 1/1034. This means that S_13 originally had integer values in the range 0..1034 and was treated with\n```\ndef anonymize(data):\n    data -= data.min()\n    data /= data.max() # divides by 1034\n    rng = np.random.default_rng()\n    data += rng.uniform(0, 0.01, len(data))\n    return data\n```\nUnfortunately some rectangles overlap (e.g. between 0.68 and 0.7) so that we cannot remove the noise.","metadata":{}},{"cell_type":"code","source":"y = df.S_13\nfor i in np.linspace(0, 1, 11):\n    plt.figure(figsize=(16, 3))\n    plt.hist(y, bins=np.linspace(i, i+0.1, 101), rwidth=0.8, color='c')\n    plt.xticks(np.linspace(i, i+0.1, 11))\n    plt.title(f\"S_13 histogram, part {int(i*10+1)}\")\n    plt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-02T19:55:44.753278Z","iopub.execute_input":"2022-06-02T19:55:44.753812Z","iopub.status.idle":"2022-06-02T19:55:54.633844Z","shell.execute_reply.started":"2022-06-02T19:55:44.753763Z","shell.execute_reply":"2022-06-02T19:55:54.632612Z"},"trusted":true},"execution_count":null,"outputs":[]}]}