{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy as sp\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_log_error\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-27T04:45:31.117758Z","iopub.execute_input":"2021-07-27T04:45:31.118071Z","iopub.status.idle":"2021-07-27T04:45:31.126943Z","shell.execute_reply.started":"2021-07-27T04:45:31.118042Z","shell.execute_reply":"2021-07-27T04:45:31.125904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/tabular-playground-series-jul-2021/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/tabular-playground-series-jul-2021/test.csv\")\nsample_df = pd.read_csv(\"/kaggle/input/tabular-playground-series-jul-2021/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-07-27T04:45:31.128775Z","iopub.execute_input":"2021-07-27T04:45:31.129305Z","iopub.status.idle":"2021-07-27T04:45:31.168921Z","shell.execute_reply.started":"2021-07-27T04:45:31.129265Z","shell.execute_reply":"2021-07-27T04:45:31.168021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['date_time'] = pd.to_datetime(train_df['date_time'])\ntest_df['date_time'] = pd.to_datetime(test_df['date_time'])\nprint(train_df.head())","metadata":{"execution":{"iopub.status.busy":"2021-07-27T04:45:31.171688Z","iopub.execute_input":"2021-07-27T04:45:31.171964Z","iopub.status.idle":"2021-07-27T04:45:31.191679Z","shell.execute_reply.started":"2021-07-27T04:45:31.171936Z","shell.execute_reply":"2021-07-27T04:45:31.190794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Feature engineering\n# train_df['year'] = train_df['date_time'].dt.year\n# train_df['month'] = train_df['date_time'].dt.month\n# train_df['hour'] = train_df['date_time'].dt.hour\n# train_df['day'] = train_df['date_time'].dt.day\n\n# test_df['year'] = test_df['date_time'].dt.year\n# test_df['month'] = test_df['date_time'].dt.month\n# test_df['hour'] = test_df['date_time'].dt.hour\n# test_df['day'] = test_df['date_time'].dt.day\n# print(train_df.head())","metadata":{"execution":{"iopub.status.busy":"2021-07-27T04:45:31.193597Z","iopub.execute_input":"2021-07-27T04:45:31.194258Z","iopub.status.idle":"2021-07-27T04:45:31.202164Z","shell.execute_reply.started":"2021-07-27T04:45:31.194201Z","shell.execute_reply":"2021-07-27T04:45:31.201381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set data_time column as index as it is needed for RNN\ntrain = train_df.set_index(\"date_time\").copy()\ntest = test_df.set_index(\"date_time\").copy()\nprint(train.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T04:45:31.205604Z","iopub.execute_input":"2021-07-27T04:45:31.205909Z","iopub.status.idle":"2021-07-27T04:45:31.219942Z","shell.execute_reply.started":"2021-07-27T04:45:31.205882Z","shell.execute_reply":"2021-07-27T04:45:31.218934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_cols = [col for col in train.columns if col.startswith('target')]\nfeat_cols = [col for col in train.columns if col not in target_cols]\nprint(target_cols)\ntest = pd.concat([train.drop(target_cols[:], axis=1).iloc[-10:-1], test])\nprint(test.head())","metadata":{"execution":{"iopub.status.busy":"2021-07-27T04:45:31.221385Z","iopub.execute_input":"2021-07-27T04:45:31.221953Z","iopub.status.idle":"2021-07-27T04:45:31.240014Z","shell.execute_reply.started":"2021-07-27T04:45:31.221909Z","shell.execute_reply":"2021-07-27T04:45:31.238973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train, val = train_test_split(train, test_size=0.2, random_state=42)\nprint(test)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T04:45:31.241441Z","iopub.execute_input":"2021-07-27T04:45:31.241998Z","iopub.status.idle":"2021-07-27T04:45:31.263069Z","shell.execute_reply.started":"2021-07-27T04:45:31.241955Z","shell.execute_reply":"2021-07-27T04:45:31.261973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\nfea_scaler = MinMaxScaler()\nlab_scaler = MinMaxScaler()\n\nXtrain_scaled = fea_scaler.fit_transform(train.drop(target_cols[:],axis=1))\nXval_scaled = fea_scaler.transform(val.drop(target_cols[:],axis=1))\nYtrain_scaled =lab_scaler.fit_transform(train[target_cols[:]])\nYval_scaled =lab_scaler.transform(val[target_cols[:]])\nXtest_scaled = fea_scaler.transform(test)\nprint(Xtrain_scaled)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T04:45:31.264539Z","iopub.execute_input":"2021-07-27T04:45:31.265189Z","iopub.status.idle":"2021-07-27T04:45:31.631816Z","shell.execute_reply.started":"2021-07-27T04:45:31.265141Z","shell.execute_reply":"2021-07-27T04:45:31.630943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"length = 10 # use 50 observation to test_generator 51\nbatch_size = 1 # usually this batch size works well\n\n# train_generator = TimeseriesGenerator(data=Xtrain_scaled,\n#                                       targets=train[target_cols[:]],\n#                                       length=length,\n#                                       batch_size=batch_size)\n# test_generator = TimeseriesGenerator(data=Xtest_scaled,\n#                                      targets=test[target_cols[:]],\n#                                      length=length,\n#                                      batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T04:45:31.635697Z","iopub.execute_input":"2021-07-27T04:45:31.635971Z","iopub.status.idle":"2021-07-27T04:45:31.640515Z","shell.execute_reply.started":"2021-07-27T04:45:31.635943Z","shell.execute_reply":"2021-07-27T04:45:31.639365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = tf.keras.preprocessing.timeseries_dataset_from_array(\n    Xtrain_scaled,\n    targets=Ytrain_scaled,\n    sequence_length=length,\n    batch_size=batch_size,\n    shuffle=True,\n    start_index=0).prefetch(64)\nval_dataset = tf.keras.preprocessing.timeseries_dataset_from_array(\n    Xval_scaled,\n    targets=Yval_scaled,\n    sequence_length=length,\n    batch_size=batch_size,\n    shuffle=True,\n    start_index=0).prefetch(64)\ntest[target_cols[:]] = 0\ntest_dataset = tf.keras.preprocessing.timeseries_dataset_from_array(\n    Xtest_scaled,\n    targets=test[target_cols[:]],\n    sequence_length=length,\n    batch_size=batch_size,\n    shuffle=True,\n    start_index=0).prefetch(64)\nprint(train[target_cols[:]].shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T04:45:31.642165Z","iopub.execute_input":"2021-07-27T04:45:31.642602Z","iopub.status.idle":"2021-07-27T04:45:31.76781Z","shell.execute_reply.started":"2021-07-27T04:45:31.64252Z","shell.execute_reply":"2021-07-27T04:45:31.766853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_features = Xtrain_scaled.shape[1]\nprint(n_features)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T04:45:31.768993Z","iopub.execute_input":"2021-07-27T04:45:31.769461Z","iopub.status.idle":"2021-07-27T04:45:31.774788Z","shell.execute_reply.started":"2021-07-27T04:45:31.769424Z","shell.execute_reply":"2021-07-27T04:45:31.773788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rmsle_custom(y_true, y_pred):\n    msle = tf.keras.losses.MeanSquaredLogarithmicError()\n    return K.sqrt(msle(y_true, y_pred))\n\n\nes = tf.keras.callbacks.EarlyStopping(monitor='val_rmsle_custom', \n                                      mode='min',patience=4, \n                                      restore_best_weights=True)\n\nplateau = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\",\n                                               factor=0.5,\n                                               patience=3,\n                                               verbose=1,\n                                               mode=\"auto\",\n                                               min_delta=0.0001,\n                                               cooldown=0,\n                                               min_lr=0.000001,)\n\nweights_initializer = tf.keras.initializers.RandomNormal(mean=0., stddev=1., seed=45)\n\ndef lstm_autoencoder():\n    \n    model = tf.keras.Sequential([\n        tf.keras.layers.LSTM(100, return_sequences=True,\n                             dropout=0.3, \n                             activation = \"tanh\", \n                             stateful=True,\n                             batch_input_shape=(batch_size,length, n_features),\n                             kernel_initializer='LecunUniform'),\n        tf.keras.layers.LSTM(50, \n                             dropout=0.3, \n                             stateful=True,\n                             activation = \"tanh\", \n                             kernel_initializer='LecunUniform'),\n        tf.keras.layers.RepeatVector(1),\n        tf.keras.layers.LSTM(50, return_sequences=True, \n                             dropout=0.3, \n                             stateful=True,\n                             activation = \"sigmoid\", \n                             kernel_initializer='LecunUniform'),\n        tf.keras.layers.LSTM(100, return_sequences=True, \n                             dropout=0.3,\n                             stateful=True,\n                             activation = \"sigmoid\", \n                             kernel_initializer='LecunUniform'),\n        tf.keras.layers.TimeDistributed(Dense(n_features)), \n        tf.keras.layers.Dense(20, kernel_initializer='LecunUniform',\n                              activation = tf.keras.layers.PReLU()), \n        tf.keras.layers.Dropout(0.3), \n        tf.keras.layers.Dense(3, kernel_initializer='LecunUniform',\n                              activation = tf.keras.layers.PReLU())\n        ])\n    \n    model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.01),\n                  loss=tf.keras.losses.mean_squared_error,\n                  metrics=[rmsle_custom])\n    \n    history = model.fit(train_dataset,\n                        validation_data = val_dataset, \n                        epochs=50,\n                        callbacks=[es,plateau],\n                        verbose=1)\n    \n    return history, model\n\nlstm_2_history, lstm_model = lstm_autoencoder()","metadata":{"execution":{"iopub.status.busy":"2021-07-27T04:46:17.489118Z","iopub.execute_input":"2021-07-27T04:46:17.489494Z","iopub.status.idle":"2021-07-27T04:46:17.927307Z","shell.execute_reply.started":"2021-07-27T04:46:17.489462Z","shell.execute_reply":"2021-07-27T04:46:17.925235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = lstm_model.predict(test_dataset)\npreds = preds.reshape(2247, 3)\npreds = lab_scaler.inverse_transform(preds)\nsample_df[target_cols[:]] = preds\nsample_df.to_csv('sample_submission.csv', index=0)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T04:45:32.261126Z","iopub.status.idle":"2021-07-27T04:45:32.261834Z"},"trusted":true},"execution_count":null,"outputs":[]}]}