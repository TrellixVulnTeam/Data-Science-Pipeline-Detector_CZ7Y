{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import glob\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport copy\nimport warnings\nimport joblib\nfrom sklearn.model_selection import train_test_split, KFold\nimport lightgbm as lgb\nimport datatable as dt\nimport gc\nfrom tqdm import tqdm\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import OneHotEncoder, RobustScaler, StandardScaler\nimport os\nimport time\nimport math\ndef my_metrics(y_true, y_pred):\n    terms_to_sum = [(math.log(y_pred[i] + 1) - math.log(y_true[i] + 1)) ** 2.0 for i,pred in enumerate(y_pred)]\n    return (sum(terms_to_sum) * (1.0/len(y_true))) ** 0.5    \ndef rmsle(y_true, y_pred):     \n    output = my_metrics(y_true, y_pred)\n    return 'rmsle', output, False\ndef rmse(y_true, y_pred):\n    return mean_squared_error(y_true, y_pred, squared = False)\nwarnings.filterwarnings(action='ignore', category=UserWarning)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-09T00:42:26.496928Z","iopub.execute_input":"2021-07-09T00:42:26.497443Z","iopub.status.idle":"2021-07-09T00:42:29.611024Z","shell.execute_reply.started":"2021-07-09T00:42:26.497308Z","shell.execute_reply":"2021-07-09T00:42:29.610099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"folds = 5\nseed_list = [i for i in range(14, 15)]\nearly_stopping = 200","metadata":{"execution":{"iopub.status.busy":"2021-07-09T00:42:30.756941Z","iopub.execute_input":"2021-07-09T00:42:30.757262Z","iopub.status.idle":"2021-07-09T00:42:30.76282Z","shell.execute_reply.started":"2021-07-09T00:42:30.75723Z","shell.execute_reply":"2021-07-09T00:42:30.761767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/tabular-playground-series-jul-2021/train.csv')\ntest = pd.read_csv('../input/tabular-playground-series-jul-2021/test.csv')\nall_data = pd.concat([train, test])\nall_data['date_time'] = pd.to_datetime(all_data['date_time'])\nall_data['date_time'] = (all_data['date_time'].dt.year * 365 + all_data['date_time'].dt.month * 30 +\\\n                all_data['date_time'].dt.day + all_data['date_time'].dt.hour / 24) * 24\nall_data[\"date_time\"] = (2021 * 365 + 7 * 30 + 1) * 24 - all_data.date_time\ntrain, test = all_data.iloc[:len(train),], all_data.iloc[len(train):,]\ntarget = ['target_carbon_monoxide', 'target_benzene', 'target_nitrogen_oxides']\nto_test = test.drop(target, axis = 1)\ny = train[target]\nX = train.drop(target, axis = 1)","metadata":{"execution":{"iopub.status.busy":"2021-07-09T00:42:32.49248Z","iopub.execute_input":"2021-07-09T00:42:32.4928Z","iopub.status.idle":"2021-07-09T00:42:32.561634Z","shell.execute_reply.started":"2021-07-09T00:42:32.492769Z","shell.execute_reply":"2021-07-09T00:42:32.560724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_saved = y.copy()","metadata":{"execution":{"iopub.status.busy":"2021-07-09T00:42:34.170226Z","iopub.execute_input":"2021-07-09T00:42:34.170611Z","iopub.status.idle":"2021-07-09T00:42:34.176996Z","shell.execute_reply.started":"2021-07-09T00:42:34.170576Z","shell.execute_reply":"2021-07-09T00:42:34.176198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def objective(trial , X = X , y = y_saved.iloc[:,0]):\n    params = {\n        'reg_alpha' : trial.suggest_loguniform('reg_alpha' , 0.04 , 1),\n        'reg_lambda' : trial.suggest_loguniform('reg_lambda' , 3 , 9),\n        'num_leaves' : trial.suggest_int('num_leaves' , 40 , 80),\n        'learning_rate' : trial.suggest_uniform('learning_rate' , 0.01 , 0.05),\n        'max_depth' : trial.suggest_int('max_depth', 30 , 60),\n        'n_estimators' : trial.suggest_int('n_estimators', 3000 , 8000),  #  4000 , 5600\n        'min_child_weight' : trial.suggest_loguniform('min_child_weight', 0.04 , 0.1),\n        'subsample' : trial.suggest_uniform('subsample' , 0.7 , 1.0), # 0.7 , 1.0\n        'colsample_bytree' : trial.suggest_loguniform('colsample_bytree', 0.5 , 1), # 0.5 , 1\n        'min_child_samples' : trial.suggest_int('min_child_samples', 30, 60),\n        'metric' : 'rmse', #'rmse'\n        'device_type' : 'gpu',\n    }\n  #  pruning_callback = optuna.integration.LightGBMPruningCallback(trial, 'rmspe', valid_name = 'valid_0')  \n    score = 0\n    for seed in seed_list: \n        kf = KFold(n_splits = folds ,random_state= seed, shuffle=True)\n        for idx_train,idx_test in kf.split(X, y):\n            X_train,X_test=X.iloc[idx_train],X.iloc[idx_test]\n            y_train,y_test=y.iloc[idx_train],y.iloc[idx_test]\n            model = lgb.LGBMRegressor(**params, random_state = seed, n_jobs = -1)\n            model.fit(X_train, y_train.values.ravel(), eval_set = [(X_test , y_test.values.ravel())] ,eval_metric = rmsle, early_stopping_rounds = early_stopping, \\\n             verbose = 300\n                   #    ,callbacks = [pruning_callback]\n                     ) \n            y_pred = model.predict(X_test)  \n            score += (my_metrics(y_test.values.ravel(), y_pred) / folds) / len(seed_list)                 \n    del model\n    return score\nimport optuna\nstudy = optuna.create_study(direction = 'minimize' , study_name = 'lgbm'\n                           # , pruner = optuna.pruners.HyperbandPruner()\n                           )\nstudy.optimize(objective , n_trials = 100)\nprint('numbers of the finished trials:' , len(study.trials))\nprint('the best params:' , study.best_trial.params)\nprint('the best value:' , study.best_value)\nprint(\"done\")\n#time.sleep(60)","metadata":{"execution":{"iopub.status.busy":"2021-07-08T10:35:23.038652Z","iopub.execute_input":"2021-07-08T10:35:23.039137Z","iopub.status.idle":"2021-07-08T11:32:57.206147Z","shell.execute_reply.started":"2021-07-08T10:35:23.0391Z","shell.execute_reply":"2021-07-08T11:32:57.203765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = {'reg_alpha': 0.9983348199602602, 'reg_lambda': 3.7977035151026133,\n 'num_leaves': 43, \n 'learning_rate': 0.018110216441437378, 'max_depth': 43,\n 'n_estimators': 4465, 'min_child_weight': 0.08971736720140458, \n 'subsample': 0.8077831485462772, 'colsample_bytree': 0.5101790470372234, \n 'min_child_samples': 33} # Best is trial 26 with value: 0.10700507123161773.","metadata":{"execution":{"iopub.status.busy":"2021-07-08T11:33:00.566141Z","iopub.execute_input":"2021-07-08T11:33:00.566487Z","iopub.status.idle":"2021-07-08T11:33:00.573556Z","shell.execute_reply.started":"2021-07-08T11:33:00.566454Z","shell.execute_reply":"2021-07-08T11:33:00.572512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = y_saved.iloc[:,0]\nscore = 0\nfor seed in seed_list: \n    kf = KFold(n_splits = folds ,random_state= seed, shuffle=True)\n    count = 1\n    for idx_train,idx_test in kf.split(X, y):\n        print(\"=\" * 40)\n        print(\"seed\", seed)\n        print(\"fold\", count)\n        print(\"=\" * 30)\n        start_time = time.time()\n        X_train, X_test = X.iloc[idx_train], X.iloc[idx_test]\n        y_train, y_test = y.iloc[idx_train], y.iloc[idx_test]\n        model = lgb.LGBMRegressor(**params, random_state = seed, n_jobs = -1, metric = 'rmse', device_type = 'gpu')\n        model.fit(X_train, y_train, eval_set = [(X_test , y_test.values.ravel())], eval_metric = rmsle,\\\n                  early_stopping_rounds = early_stopping, verbose = False)\n        cv_score = my_metrics(y_test.values.ravel(), model.predict(X_test))\n        score += (cv_score / folds) / len(seed_list)\n        joblib.dump(model, f'target 0 seed_{seed}_fold_{count}_cv_score_{round(cv_score, 3)}.pkl') # save model\n        end_time = time.time()\n        run_time = round(end_time - start_time)\n        print (\"fold\", count, \"took\", run_time , \"seconds to run\")\n        count += 1\n        print (\"The estimated remaining training time in the current seed\", seed, \"are\",\\\n               round(((folds - count) * run_time) / 60, 3), \"minuets\")\n        print(\"Validation score\", cv_score)\nprint(\"Mean RMSPE validation score of\", folds, \"folds\", score)","metadata":{"execution":{"iopub.status.busy":"2021-07-08T11:33:47.118605Z","iopub.execute_input":"2021-07-08T11:33:47.118952Z","iopub.status.idle":"2021-07-08T11:35:41.940266Z","shell.execute_reply.started":"2021-07-08T11:33:47.118915Z","shell.execute_reply":"2021-07-08T11:35:41.939451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def objective(trial , X = X , y = y_saved.iloc[:, 1]):\n    params = {\n        'reg_alpha' : trial.suggest_loguniform('reg_alpha' , 0.04 , 1),\n        'reg_lambda' : trial.suggest_loguniform('reg_lambda' , 3 , 9),\n        'num_leaves' : trial.suggest_int('num_leaves' , 40 , 80),\n        'learning_rate' : trial.suggest_uniform('learning_rate' , 0.01 , 0.05),\n        'max_depth' : trial.suggest_int('max_depth', 30 , 60),\n        'n_estimators' : trial.suggest_int('n_estimators', 3000 , 8000),  #  4000 , 5600\n        'min_child_weight' : trial.suggest_loguniform('min_child_weight', 0.04 , 0.1),\n        'subsample' : trial.suggest_uniform('subsample' , 0.7 , 1.0), # 0.7 , 1.0\n        'colsample_bytree' : trial.suggest_loguniform('colsample_bytree', 0.5 , 1), # 0.5 , 1\n        'min_child_samples' : trial.suggest_int('min_child_samples', 30, 60),\n        'metric' : 'rmse', #'rmse'\n        'device_type' : 'gpu',\n    }\n  #  pruning_callback = optuna.integration.LightGBMPruningCallback(trial, 'rmspe', valid_name = 'valid_0')  \n    score = 0\n    for seed in seed_list: \n        kf = KFold(n_splits = folds ,random_state= seed, shuffle=True)\n        for idx_train,idx_test in kf.split(X, y):\n            X_train,X_test=X.iloc[idx_train],X.iloc[idx_test]\n            y_train,y_test=y.iloc[idx_train],y.iloc[idx_test]\n            model = lgb.LGBMRegressor(**params, random_state = seed, n_jobs = -1)\n            model.fit(X_train, y_train.values.ravel(), eval_set = [(X_test , y_test.values.ravel())] ,eval_metric = rmsle, early_stopping_rounds = early_stopping, \\\n             verbose = 300\n                   #    ,callbacks = [pruning_callback]\n                     ) \n            y_pred = model.predict(X_test)  \n            score += (my_metrics(y_test.values.ravel(), y_pred) / folds) / len(seed_list)                 \n    del model\n    return score\nimport optuna\nstudy = optuna.create_study(direction = 'minimize' , study_name = 'lgbm'\n                           # , pruner = optuna.pruners.HyperbandPruner()\n                           )\nstudy.optimize(objective , n_trials = 33)\nprint('numbers of the finished trials:' , len(study.trials))\nprint('the best params:' , study.best_trial.params)\nprint('the best value:' , study.best_value)\nprint(\"done\")\n#time.sleep(60)","metadata":{"execution":{"iopub.status.busy":"2021-07-08T11:36:13.185335Z","iopub.execute_input":"2021-07-08T11:36:13.185665Z","iopub.status.idle":"2021-07-08T12:12:43.20521Z","shell.execute_reply.started":"2021-07-08T11:36:13.185637Z","shell.execute_reply":"2021-07-08T12:12:43.204439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params2 = {'reg_alpha': 0.08954418283105867, \n           'reg_lambda': 4.34037322003328, 'num_leaves': 69, \n           'learning_rate': 0.01605276385426573, 'max_depth': 42, \n           'n_estimators': 6484, 'min_child_weight': 0.05758104491837273,\n           'subsample': 0.769111408198851, 'colsample_bytree': 0.6678960189167248, \n           'min_child_samples': 30} #. Best is trial 29 with value: 0.08656920519236028.","metadata":{"execution":{"iopub.status.busy":"2021-07-08T12:14:13.951076Z","iopub.execute_input":"2021-07-08T12:14:13.951388Z","iopub.status.idle":"2021-07-08T12:14:13.956183Z","shell.execute_reply.started":"2021-07-08T12:14:13.951358Z","shell.execute_reply":"2021-07-08T12:14:13.955239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = y_saved.iloc[:, 1]\nscore = 0\nfor seed in seed_list: \n    kf = KFold(n_splits = folds ,random_state= seed, shuffle=True)\n    count = 1\n    for idx_train,idx_test in kf.split(X, y):\n        print(\"=\" * 40)\n        print(\"seed\", seed)\n        print(\"fold\", count)\n        print(\"=\" * 30)\n        start_time = time.time()\n        X_train, X_test = X.iloc[idx_train], X.iloc[idx_test]\n        y_train, y_test = y.iloc[idx_train], y.iloc[idx_test]\n        model = lgb.LGBMRegressor(**params2, random_state = seed, n_jobs = -1, metric = 'rmse', device_type = 'gpu')\n        model.fit(X_train, y_train, eval_set = [(X_test , y_test.values.ravel())], eval_metric = rmsle,\\\n                  early_stopping_rounds = early_stopping, verbose = False)\n        cv_score = my_metrics(y_test.values.ravel(), model.predict(X_test))\n        score += (cv_score / folds) / len(seed_list)\n        joblib.dump(model, f'target 1 seed_{seed}_fold_{count}_cv_score_{round(cv_score, 3)}.pkl') # save model\n        end_time = time.time()\n        run_time = round(end_time - start_time)\n        print (\"fold\", count, \"took\", run_time , \"seconds to run\")\n        count += 1\n        print (\"The estimated remaining training time in the current seed\", seed, \"are\",\\\n               round(((folds - count) * run_time) / 60, 3), \"minuets\")\n        print(\"Validation score\", cv_score)\nprint(\"Mean RMSPE validation score of\", folds, \"folds\", score)","metadata":{"execution":{"iopub.status.busy":"2021-07-08T12:14:28.459287Z","iopub.execute_input":"2021-07-08T12:14:28.4596Z","iopub.status.idle":"2021-07-08T12:16:02.54265Z","shell.execute_reply.started":"2021-07-08T12:14:28.45957Z","shell.execute_reply":"2021-07-08T12:16:02.54067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def objective(trial , X = X , y = y_saved.iloc[:, 2]):\n    params = {\n        'reg_alpha' : trial.suggest_loguniform('reg_alpha' , 0.04 , 1),\n        'reg_lambda' : trial.suggest_loguniform('reg_lambda' , 3 , 9),\n        'num_leaves' : trial.suggest_int('num_leaves' , 40 , 80),\n        'learning_rate' : trial.suggest_uniform('learning_rate' , 0.01 , 0.05),\n        'max_depth' : trial.suggest_int('max_depth', 30 , 60),\n        'n_estimators' : trial.suggest_int('n_estimators', 3000 , 8000),  #  4000 , 5600\n        'min_child_weight' : trial.suggest_loguniform('min_child_weight', 0.04 , 0.1),\n        'subsample' : trial.suggest_uniform('subsample' , 0.7 , 1.0), # 0.7 , 1.0\n        'colsample_bytree' : trial.suggest_loguniform('colsample_bytree', 0.5 , 1), # 0.5 , 1\n        'min_child_samples' : trial.suggest_int('min_child_samples', 30, 60),\n        'metric' : 'rmse', #'rmse'\n        'device_type' : 'gpu',\n    }\n  #  pruning_callback = optuna.integration.LightGBMPruningCallback(trial, 'rmspe', valid_name = 'valid_0')  \n    score = 0\n    for seed in seed_list: \n        kf = KFold(n_splits = folds ,random_state= seed, shuffle=True)\n        for idx_train,idx_test in kf.split(X, y):\n            X_train,X_test=X.iloc[idx_train],X.iloc[idx_test]\n            y_train,y_test=y.iloc[idx_train],y.iloc[idx_test]\n            model = lgb.LGBMRegressor(**params, random_state = seed, n_jobs = -1)\n            model.fit(X_train, y_train.values.ravel(), eval_set = [(X_test , y_test.values.ravel())] ,eval_metric = 'rmse', early_stopping_rounds = early_stopping, \\\n             verbose = 3000\n                   #    ,callbacks = [pruning_callback]\n                     ) \n            y_pred = model.predict(X_test)  \n            score += (rmse(y_test.values.ravel(), y_pred) / folds) / len(seed_list)                 \n    del model\n    return score\nimport optuna\nstudy = optuna.create_study(direction = 'minimize' , study_name = 'lgbm'\n                           # , pruner = optuna.pruners.HyperbandPruner()\n                           )\nstudy.optimize(objective , n_trials = 33)\nprint('numbers of the finished trials:' , len(study.trials))\nprint('the best params:' , study.best_trial.params)\nprint('the best value:' , study.best_value)\nprint(\"done\")\n#time.sleep(60)","metadata":{"execution":{"iopub.status.busy":"2021-07-09T00:12:07.199273Z","iopub.execute_input":"2021-07-09T00:12:07.199651Z","iopub.status.idle":"2021-07-09T00:31:01.417791Z","shell.execute_reply.started":"2021-07-09T00:12:07.199616Z","shell.execute_reply":"2021-07-09T00:31:01.416959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params3 = {'reg_alpha': 0.05950215618354974, 'reg_lambda': 4.9053234921173186, \n           'num_leaves': 53, 'learning_rate': 0.013992638769108515, \n           'max_depth': 41, 'n_estimators': 4509, 'min_child_weight': 0.08516375010639021,\n           'subsample': 0.8804548923956849, 'colsample_bytree': 0.6436210260217696, 'min_child_samples': 31}\n#the best value: 53.542532685728304   rmse","metadata":{"execution":{"iopub.status.busy":"2021-07-09T00:35:03.764282Z","iopub.execute_input":"2021-07-09T00:35:03.764635Z","iopub.status.idle":"2021-07-09T00:35:03.771586Z","shell.execute_reply.started":"2021-07-09T00:35:03.764605Z","shell.execute_reply":"2021-07-09T00:35:03.770794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = y_saved.iloc[:, 2]\nscore = 0\nfor seed in seed_list: \n    kf = KFold(n_splits = folds, random_state= seed, shuffle=True)\n    count = 1\n    for idx_train,idx_test in kf.split(X, y):\n        print(\"=\" * 40)\n        print(\"seed\", seed)\n        print(\"fold\", count)\n        print(\"=\" * 30)\n        start_time = time.time()\n        X_train, X_test = X.iloc[idx_train], X.iloc[idx_test]\n        y_train, y_test = y.iloc[idx_train], y.iloc[idx_test]\n        model = lgb.LGBMRegressor(**params3, random_state = seed, n_jobs = -1, metric = 'rmse', device_type = 'gpu')\n        model.fit(X_train, y_train, eval_set = [(X_test , y_test.values.ravel())], eval_metric = 'rmse',\\\n                  early_stopping_rounds = early_stopping, verbose = False)\n        cv_score = rmse(y_test.values.ravel(), model.predict(X_test))\n        score += (cv_score / folds) / len(seed_list)\n        joblib.dump(model, f'target 2 seed_{seed}_fold_{count}_cv_score_{round(cv_score, 3)}.pkl') # save model\n        end_time = time.time()\n        run_time = round(end_time - start_time)\n        print (\"fold\", count, \"took\", run_time , \"seconds to run\")\n        count += 1\n        print (\"The estimated remaining training time in the current seed\", seed, \"are\",\\\n               round(((folds - count) * run_time) / 60, 3), \"minuets\")\n        print(\"Validation score\", cv_score)\nprint(\"Mean RMSPE validation score of\", folds, \"folds\", score)","metadata":{"execution":{"iopub.status.busy":"2021-07-09T00:35:07.822512Z","iopub.execute_input":"2021-07-09T00:35:07.822843Z","iopub.status.idle":"2021-07-09T00:35:53.740439Z","shell.execute_reply.started":"2021-07-09T00:35:07.822813Z","shell.execute_reply":"2021-07-09T00:35:53.739615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output = []\nfor filepath in glob.iglob('../input/tps-july-models-target-0/*.pkl'):\n    model = joblib.load(filepath)\n    pred = model.predict(to_test, num_iteration = model.best_iteration_)\n    output.append(pred)\n    del model\n    del pred\np0 = sum(output) / len(output)\noutput = []\nfor filepath in glob.iglob('../input/tps-july-model-target-1/*.pkl'):\n    model = joblib.load(filepath)\n    pred = model.predict(to_test, num_iteration = model.best_iteration_)\n    output.append(pred)\n    del model\n    del pred\np1 = sum(output) / len(output)\nfor filepath in glob.iglob('../input/tps-july-model-target-2/*.pkl'):\n    model = joblib.load(filepath)\n    pred = model.predict(to_test, num_iteration = model.best_iteration_)\n    output.append(pred)\n    del model\n    del pred\np2 = sum(output) / len(output)","metadata":{"execution":{"iopub.status.busy":"2021-07-09T00:42:44.183321Z","iopub.execute_input":"2021-07-09T00:42:44.183661Z","iopub.status.idle":"2021-07-09T00:42:49.642101Z","shell.execute_reply.started":"2021-07-09T00:42:44.18363Z","shell.execute_reply":"2021-07-09T00:42:49.641159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output = pd.read_csv(\"../input/tabular-playground-series-jul-2021/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-07-09T00:43:48.09892Z","iopub.execute_input":"2021-07-09T00:43:48.099268Z","iopub.status.idle":"2021-07-09T00:43:48.110082Z","shell.execute_reply.started":"2021-07-09T00:43:48.099233Z","shell.execute_reply":"2021-07-09T00:43:48.109268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label = ['target_carbon_monoxide', 'target_benzene', 'target_nitrogen_oxides']\noutput[label[0]] = p0\noutput[label[1]] = p1\noutput[label[2]] = p2\noutput.loc[0, label] = [1.4, 4.1, 186.5]","metadata":{"execution":{"iopub.status.busy":"2021-07-09T00:43:51.389787Z","iopub.execute_input":"2021-07-09T00:43:51.390108Z","iopub.status.idle":"2021-07-09T00:43:51.397274Z","shell.execute_reply.started":"2021-07-09T00:43:51.390076Z","shell.execute_reply":"2021-07-09T00:43:51.395724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output.to_csv('submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-09T00:44:31.451908Z","iopub.execute_input":"2021-07-09T00:44:31.452226Z","iopub.status.idle":"2021-07-09T00:44:31.476925Z","shell.execute_reply.started":"2021-07-09T00:44:31.452195Z","shell.execute_reply":"2021-07-09T00:44:31.476164Z"},"trusted":true},"execution_count":null,"outputs":[]}]}