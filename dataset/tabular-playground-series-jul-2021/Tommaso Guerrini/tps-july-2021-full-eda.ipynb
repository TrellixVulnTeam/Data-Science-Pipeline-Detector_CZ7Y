{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h2 style=\"background-color:#e6f7ff;\" align = 'center' > Tabular Playground Series July 2021 </h2>\n\nIn this notebook I'll perform some exploratory data analysis, trying to update it often. \n\n<h4 style=\"background-color:#e6f7ff;\" align = 'center'><i>Table of Contents</i></h4>\n\n- [Data Description](#files)\n- [First Exploration](#first_eda):\n    - general info about data\n    - train vs test date_time\n    \n- [Exploratory Data Analysis](#eda)\n\n    - features distributions (train, train vs test)\n    - correlation analysis\n    - auto-cross correlation analysis\n    - cross validation strategies\n\n**Under Construction**\n\n- [TimeSeriesSplit and Sample_Submission](#sub)\n\n\n*Versioning:*\n\nCheck Version 16 for just outputs.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport itertools\nimport tqdm\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom lightgbm import LGBMRegressor\nimport PIL\nimport urllib\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom matplotlib.offsetbox import (TextArea, DrawingArea, OffsetImage,\n                                  AnnotationBbox)\nfrom matplotlib.patches import Patch\nimport seaborn as sns\nimport os\n\ndef crosscorr(datax, datay, lag=0):\n    \"\"\" Lag-N cross correlation. \n    Parameters\n    ----------\n    lag : int, default 0\n    datax, datay : pandas.Series objects of equal length\n\n    Returns\n    ----------\n    crosscorr : float\n    \"\"\"\n    return datax.corr(datay.shift(lag))\n\nfrom statsmodels.tsa.stattools import acf, pacf\nfrom statsmodels.graphics.tsaplots import plot_pacf, plot_acf\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nroot_path = '/kaggle/input/tabular-playground-series-jul-2021/'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-20T08:45:17.758807Z","iopub.execute_input":"2021-07-20T08:45:17.759445Z","iopub.status.idle":"2021-07-20T08:45:20.0761Z","shell.execute_reply.started":"2021-07-20T08:45:17.759358Z","shell.execute_reply":"2021-07-20T08:45:20.074058Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"files\"></a>\n<h4>Data Description</h4>\n\nIn this competition you are predicting the values of air pollution measurements over time, based on basic weather information (temperature and humidity) and the input values of 5 sensors.\n\nThe three target values to you to predict are: *target_carbon_monoxide, target_benzene, and target_nitrogen_oxides*\n\n<h4>Files</h4>\n\n`train.csv` - the training data, including the weather data, sensor data, and values for the 3 targets\n\n`test.csv` - the same format as train.csv, but without the target value; your task is to predict the value for each of these targets.\n\n`sample_submission.csv` - a sample submission file in the correct format.","metadata":{}},{"cell_type":"markdown","source":"<a id = \"first_eda\"></a>","metadata":{}},{"cell_type":"markdown","source":"<h4 style=\"background-color:#e6f7ff;\" align = 'center'><i>First Exploration</i></h4>","metadata":{}},{"cell_type":"markdown","source":"<h6> Read Data </h6>","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(root_path + 'train.csv')\ntest = pd.read_csv(root_path + 'test.csv')","metadata":{"execution":{"iopub.status.busy":"2021-07-20T08:45:20.077334Z","iopub.execute_input":"2021-07-20T08:45:20.077576Z","iopub.status.idle":"2021-07-20T08:45:20.125541Z","shell.execute_reply.started":"2021-07-20T08:45:20.077553Z","shell.execute_reply":"2021-07-20T08:45:20.124617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h6> Show some info about `train` and `test` </h6>","metadata":{}},{"cell_type":"code","source":"train.info(), test.info()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T08:45:20.128516Z","iopub.execute_input":"2021-07-20T08:45:20.128897Z","iopub.status.idle":"2021-07-20T08:45:20.165642Z","shell.execute_reply.started":"2021-07-20T08:45:20.128867Z","shell.execute_reply":"2021-07-20T08:45:20.164514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Small datasets memory wise and no null present. All columns except `date_time` are `float64`","metadata":{}},{"cell_type":"markdown","source":"<a id = 'train'></a>","metadata":{}},{"cell_type":"markdown","source":"Let's check whether there are gaps in our dates: ","metadata":{}},{"cell_type":"code","source":"MIN_DATETIME_TRAIN = train.date_time.min()\nMAX_DATETIME_TRAIN = train.date_time.max()\n\nassert len(train.date_time.unique()) == len(pd.date_range(MIN_DATETIME_TRAIN, MAX_DATETIME_TRAIN, freq='1H')), \"There are gaps in train dates\"\n\nMIN_DATETIME_TEST = test.date_time.min()\nMAX_DATETIME_TEST = test.date_time.max()\n\nassert len(test.date_time.unique()) == len(pd.date_range(MIN_DATETIME_TEST, MAX_DATETIME_TEST, freq = '1H')), \"There are gaps in test dates\"","metadata":{"execution":{"iopub.status.busy":"2021-07-20T08:45:20.167545Z","iopub.execute_input":"2021-07-20T08:45:20.167932Z","iopub.status.idle":"2021-07-20T08:45:20.181164Z","shell.execute_reply.started":"2021-07-20T08:45:20.167892Z","shell.execute_reply":"2021-07-20T08:45:20.18004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"no gaps! \n\nLet's see it graphically ","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize = (11, 7))\ncmap_cv = plt.cm.coolwarm\n# Generate the training/testing visualizations for each CV split\n\n# Fill in indices with the training/test groups\ndates = pd.concat([train[['date_time']].assign(data='train'), test[['date_time']].assign(data = 'test')], axis = 0)\n\nindices = np.array([1] * len(dates))\nindices[dates.data == 'train'] = 1\nindices[dates.data == 'test'] = 0\n\n# Visualize the results\nax.scatter(range(len(train)), [.5] * len(train),\n           c=indices[indices==1], marker='_', lw=15, cmap=cmap_cv,\n           vmin=-.2, vmax=1.2)\n\nax.scatter(range(len(train), len(train)+len(test)), [1.] * len(test),\n           c=indices[indices==0], marker='_', lw=15, cmap=cmap_cv,\n           vmin=-.2, vmax=1.2)\n\ndate_col = dates['date_time']\n\nif date_col is not None:\n    tick_locations  = ax.get_xticks()\n    for i in (tick_locations)[1:-1]:\n        ax.vlines(i, 0, 2,linestyles='dotted', colors = 'grey')\n    tick_dates = [\" \"] + date_col.iloc[list(tick_locations[1:-1])].astype(str).tolist() + [\" \"]\n    \n    tick_locations_str = [str(int(i)) for i in tick_locations]\n    ax.set_xticks(tick_locations)\n    ax.set_xticklabels(tick_dates, rotation = 35)\n    ax.grid()\n    #ax.set_yticklabels([])\n    ax.set(yticks=np.arange(2) + .5, yticklabels=[],\n           xlabel='date_time', ylabel=\"set\",\n           ylim=[0., 1.5])\n    ax.legend([Patch(color=cmap_cv(.8)), Patch(color=cmap_cv(.02))],\n              ['Training set', 'Testing set'], loc=(1.02, .8))\nplt.suptitle(\"train.csv and test.csv date_time division\", fontsize = 20, fontweight = 'bold')\nplt.title(\"train: {}-{}\\t test: {} - {}\".format(MIN_DATETIME_TRAIN, MAX_DATETIME_TRAIN, MIN_DATETIME_TEST, MAX_DATETIME_TEST), fontsize = 10)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T08:45:20.182316Z","iopub.execute_input":"2021-07-20T08:45:20.182586Z","iopub.status.idle":"2021-07-20T08:45:20.48725Z","shell.execute_reply.started":"2021-07-20T08:45:20.18256Z","shell.execute_reply":"2021-07-20T08:45:20.486223Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**There is one timestamp of overlap between train and test: 2011-01-01 00:00:00**\n\nLet's check whether values correspond for that timestamp between train and test.","metadata":{}},{"cell_type":"code","source":"feature_cols = [i for i in train.columns if all(x not in i for x in ['target', 'date_time'])]\ntarget_cols = ['target_carbon_monoxide', 'target_benzene', 'target_nitrogen_oxides']","metadata":{"execution":{"iopub.status.busy":"2021-07-20T08:45:20.488784Z","iopub.execute_input":"2021-07-20T08:45:20.489215Z","iopub.status.idle":"2021-07-20T08:45:20.494281Z","shell.execute_reply.started":"2021-07-20T08:45:20.489172Z","shell.execute_reply":"2021-07-20T08:45:20.493328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[feature_cols + ['date_time']].merge(test[feature_cols + ['date_time']], on = 'date_time', suffixes = ('_train', '_test'))","metadata":{"execution":{"iopub.status.busy":"2021-07-20T08:45:20.495626Z","iopub.execute_input":"2021-07-20T08:45:20.496025Z","iopub.status.idle":"2021-07-20T08:45:20.539424Z","shell.execute_reply.started":"2021-07-20T08:45:20.495952Z","shell.execute_reply":"2021-07-20T08:45:20.538564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**You can see they are exactly the same. So for the first test point we may also have the target values.** ","metadata":{}},{"cell_type":"markdown","source":"Let's check whether there are nan values in our columns (we know from `.info()` there are none):","metadata":{}},{"cell_type":"code","source":"train.isna().sum(axis = 0).rename('Number_of_Nans').to_frame().transpose()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T08:45:20.54235Z","iopub.execute_input":"2021-07-20T08:45:20.542758Z","iopub.status.idle":"2021-07-20T08:45:20.55831Z","shell.execute_reply.started":"2021-07-20T08:45:20.542714Z","shell.execute_reply":"2021-07-20T08:45:20.557308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4 style=\"background-color:#e6f7ff;\" align = 'center'><i>Exploratory Data Analysis</i></h4>","metadata":{}},{"cell_type":"markdown","source":"<h6> Train Feature distributions: </h6>","metadata":{}},{"cell_type":"code","source":"index_col = 'date_time'\nN_BINS = 20\nplt.style.use('fivethirtyeight')\nfor feature in feature_cols[:1]:\n    \n    mean = round(train[feature].mean(), 2)\n    median = round(train[feature].median(),2)\n    st_dev = round(train[feature].std(), 2)\n    \n    fig,ax = plt.subplots(1, 1, figsize = (20, 6))\n    (train[[index_col, feature]].sample(500).sort_values(index_col, ignore_index = True).set_index(index_col)\n     .plot(lw = 2, linestyle = \"-.\", ax=ax, color = (0.31883238319215684, 0.4266050511215686, 0.8598574482039216)))\n    fig.suptitle('{} through time'.format(feature), fontsize = 20, color ='black', fontweight = 'bold')\n\n    Ys = ax.get_yticks()\n    y_min = Ys.min()\n\n\n    if y_min < 0:\n        new_miny = y_min*0.93\n    else:\n        new_miny = y_min*1.07\n    y_max = Ys.max()\n    new_maxy = y_max*1.07\n\n    Xs = ax.get_xticks()\n    x_min = Xs.min()\n    if x_min < 0:\n        new_minx = x_min*0.8\n    else:\n        new_minx = x_min*1.2\n    x_max = Xs.max()\n    new_maxx = x_max*1.01\n\n    ax.set_ylim(new_miny, new_maxy)\n    ax.set_xlim(new_minx, new_maxx)\n\n    ax.hlines(y = mean, xmin = x_min, xmax = x_max, colors='crimson',\n              linestyles='dashdot', label='mean', alpha = 0.3, linewidth = 3)\n    ax.text(x = x_max*0.9, y = train[feature].mean(), s = 'mean: {}'.format(mean))\n\n    fig.show()\n\n    fig,ax = plt.subplots(1, 1, figsize = (18, 6))\n    \n    if feature == 'deg_C':\n        hot = 'https://gilmour.com/gilmour_map/images/256/hot.png'\n        cold = 'https://cdn1.iconfinder.com/data/icons/winter-37/32/thermometer_cold_snow_winter_weather_forecast_temperature-128.png'\n        \n        arr_img = PIL.Image.open(urllib.request.urlopen(hot))\n        arr_img = plt.imread(hot, format='png')\n\n        imagebox = OffsetImage(arr_img, zoom=0.25)\n        imagebox.image.axes = ax\n\n        ab = AnnotationBbox(imagebox, xy = [45, 0.05],\n                        xybox=(30, 5),\n                        frameon = False,\n                        xycoords='data',\n                        boxcoords=\"offset points\",\n                        pad=0.5,\n                        arrowprops=dict(\n                            arrowstyle=\"->\",\n                            connectionstyle=\"angle,angleA=0,angleB=90,rad=3\")\n                        )\n\n        ax.add_artist(ab)\n        \n        arr_img = PIL.Image.open(urllib.request.urlopen(hot))\n        arr_img = plt.imread(cold, format='png')\n\n        imagebox = OffsetImage(arr_img, zoom=0.35)\n        imagebox.image.axes = ax\n\n        ab = AnnotationBbox(imagebox, xy = [0, 0.05],\n                        xybox=(30, 5),\n                        frameon = False,\n                        xycoords='data',\n                        boxcoords=\"offset points\",\n                        pad=0.5,\n                        arrowprops=dict(\n                            arrowstyle=\"->\",\n                            connectionstyle=\"angle,angleA=0,angleB=90,rad=3\")\n                        )\n\n        ax.add_artist(ab)\n    \n    fig.suptitle('{} distribution'.format(feature), fontsize = 20, color ='black', fontweight = 'bold')\n\n    percentiles_asked = [0.1, 0.25, 0.5, 0.75, 0.9]\n    percentiles = train[feature].quantile(percentiles_asked).tolist()\n\n    ax.grid()\n    \n    sns.histplot(data = train, x = feature, ax = ax, kde=False, bins = N_BINS, stat = 'density', \n                 alpha = 0.5, fill = True, linewidth = 3, edgecolor='black', color = 'red')\n    sns.kdeplot(data = train, x = feature, ax = ax, alpha = 0.01, fill = True, \n                linewidth = 3, color = 'blue')\n\n    Ys = ax.get_yticks()\n    y_min = Ys.min()\n    if y_min < 0:\n        new_miny = y_min*0.93\n    else:\n        new_miny = y_min*1.07\n        \n    y_max = Ys.max()\n    new_maxy = y_max*1.1\n    \n    Xs = ax.get_xticks()\n    x_min = Xs.min()\n    \n    if x_min < 0:\n        new_xmin = x_min*0.7\n    else:\n        new_xmin = x_min*1.2\n        \n    x_max = Xs.max()\n    new_maxx = x_max*1.07\n    \n    ax.grid()\n    \n    ax.set_ylim(new_miny, new_maxy)\n    ax.set_xlim(new_xmin-0.05, new_maxx)\n    ax.text(new_xmin, new_maxy*0.2, \"mean: {}\".format(mean), size = 12, alpha = 1)\n    ax.text(new_xmin, new_maxy*0.35, \"median: {}\".format(median), size = 12, alpha = 1)\n    ax.text(new_xmin, new_maxy*0.5, \"std deviation: {}\".format(st_dev), size = 12, alpha = 1)\n    \n    \n    percentiles_asked = [0.25, 0.5, 0.75]\n    percentiles = train[feature].quantile(percentiles_asked).tolist()\n    for m, percentile in enumerate(percentiles):\n        ax.axvline(percentile, alpha = 0.5, ymin = 0, ymax = 1, linestyle = \":\", color = '#FFC30B')\n        ax.text(percentile-0.16, new_maxy, \"{}\".format(percentiles_asked[m]), size = 12, alpha = 1)\n    \n    fig.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T08:45:20.560469Z","iopub.execute_input":"2021-07-20T08:45:20.560852Z","iopub.status.idle":"2021-07-20T08:45:21.603451Z","shell.execute_reply.started":"2021-07-20T08:45:20.560811Z","shell.execute_reply":"2021-07-20T08:45:21.602411Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Unhide to see all:","metadata":{}},{"cell_type":"code","source":"index_col = 'date_time'\nN_BINS = 20\nplt.style.use('fivethirtyeight')\nfor feature in feature_cols[1:]:\n    \n    mean = round(train[feature].mean(), 2)\n    median = round(train[feature].median(),2)\n    st_dev = round(train[feature].std(), 2)\n    \n    fig,ax = plt.subplots(1, 1, figsize = (20, 6))\n    (train[[index_col, feature]].sample(500).sort_values(index_col, ignore_index = True).set_index(index_col)\n     .plot(lw = 2, linestyle = \"-.\", ax=ax, color = (0.31883238319215684, 0.4266050511215686, 0.8598574482039216)))\n    fig.suptitle('{} through time'.format(feature), fontsize = 20, color ='black', fontweight = 'bold')\n\n    Ys = ax.get_yticks()\n    y_min = Ys.min()\n\n\n    if y_min < 0:\n        new_miny = y_min*0.93\n    else:\n        new_miny = y_min*1.07\n    y_max = Ys.max()\n    new_maxy = y_max*1.07\n\n    Xs = ax.get_xticks()\n    x_min = Xs.min()\n    if x_min < 0:\n        new_minx = x_min*0.8\n    else:\n        new_minx = x_min*1.2\n    x_max = Xs.max()\n    new_maxx = x_max*1.01\n\n    ax.set_ylim(new_miny, new_maxy)\n    ax.set_xlim(new_minx, new_maxx)\n\n    ax.hlines(y = mean, xmin = x_min, xmax = x_max, colors='crimson',\n              linestyles='dashdot', label='mean', alpha = 0.3, linewidth = 3)\n    ax.text(x = x_max*0.9, y = train[feature].mean(), s = 'mean: {}'.format(mean))\n\n    fig.show()\n\n    fig,ax = plt.subplots(1, 1, figsize = (18, 6))\n    \n    if feature == 'deg_C':\n        hot = 'https://gilmour.com/gilmour_map/images/256/hot.png'\n        cold = 'https://cdn1.iconfinder.com/data/icons/winter-37/32/thermometer_cold_snow_winter_weather_forecast_temperature-128.png'\n        \n        arr_img = PIL.Image.open(urllib.request.urlopen(hot))\n        arr_img = plt.imread(hot, format='png')\n\n        imagebox = OffsetImage(arr_img, zoom=0.25)\n        imagebox.image.axes = ax\n\n        ab = AnnotationBbox(imagebox, xy = [45, 0.05],\n                        xybox=(30, 5),\n                        frameon = False,\n                        xycoords='data',\n                        boxcoords=\"offset points\",\n                        pad=0.5,\n                        arrowprops=dict(\n                            arrowstyle=\"->\",\n                            connectionstyle=\"angle,angleA=0,angleB=90,rad=3\")\n                        )\n\n        ax.add_artist(ab)\n        \n        arr_img = PIL.Image.open(urllib.request.urlopen(hot))\n        arr_img = plt.imread(cold, format='png')\n\n        imagebox = OffsetImage(arr_img, zoom=0.35)\n        imagebox.image.axes = ax\n\n        ab = AnnotationBbox(imagebox, xy = [0, 0.05],\n                        xybox=(30, 5),\n                        frameon = False,\n                        xycoords='data',\n                        boxcoords=\"offset points\",\n                        pad=0.5,\n                        arrowprops=dict(\n                            arrowstyle=\"->\",\n                            connectionstyle=\"angle,angleA=0,angleB=90,rad=3\")\n                        )\n\n        ax.add_artist(ab)\n    \n    fig.suptitle('{} distribution'.format(feature), fontsize = 20, color ='black', fontweight = 'bold')\n\n    percentiles_asked = [0.1, 0.25, 0.5, 0.75, 0.9]\n    percentiles = train[feature].quantile(percentiles_asked).tolist()\n\n    ax.grid()\n    \n    sns.histplot(data = train, x = feature, ax = ax, kde=False, bins = N_BINS, stat = 'density', \n                 alpha = 0.5, fill = True, linewidth = 3, edgecolor='black', color = 'red')\n    sns.kdeplot(data = train, x = feature, ax = ax, alpha = 0.01, fill = True, \n                linewidth = 3, color = 'blue')\n\n    Ys = ax.get_yticks()\n    y_min = Ys.min()\n    if y_min < 0:\n        new_miny = y_min*0.93\n    else:\n        new_miny = y_min*1.07\n        \n    y_max = Ys.max()\n    new_maxy = y_max*1.1\n    \n    Xs = ax.get_xticks()\n    x_min = Xs.min()\n    \n    if x_min < 0:\n        new_xmin = x_min*0.7\n    else:\n        new_xmin = x_min*1.2\n        \n    x_max = Xs.max()\n    new_maxx = x_max*1.07\n    \n    ax.grid()\n    \n    ax.set_ylim(new_miny, new_maxy)\n    ax.set_xlim(new_xmin-0.05, new_maxx)\n    ax.text(new_xmin, new_maxy*0.2, \"mean: {}\".format(mean), size = 12, alpha = 1)\n    ax.text(new_xmin, new_maxy*0.35, \"median: {}\".format(median), size = 12, alpha = 1)\n    ax.text(new_xmin, new_maxy*0.5, \"std deviation: {}\".format(st_dev), size = 12, alpha = 1)\n    \n    \n    percentiles_asked = [0.25, 0.5, 0.75]\n    percentiles = train[feature].quantile(percentiles_asked).tolist()\n    for m, percentile in enumerate(percentiles):\n        ax.axvline(percentile, alpha = 0.5, ymin = 0, ymax = 1, linestyle = \":\", color = '#FFC30B')\n        ax.text(percentile-0.16, new_maxy, \"{}\".format(percentiles_asked[m]), size = 12, alpha = 1)\n    \n    fig.show()","metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"execution":{"iopub.status.busy":"2021-07-20T08:45:21.605211Z","iopub.execute_input":"2021-07-20T08:45:21.605605Z","iopub.status.idle":"2021-07-20T08:45:25.579262Z","shell.execute_reply.started":"2021-07-20T08:45:21.605564Z","shell.execute_reply":"2021-07-20T08:45:25.578084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Train target distributions","metadata":{}},{"cell_type":"code","source":"index_col = 'date_time'\nN_BINS = 20\nplt.style.use('fivethirtyeight')\nfor feature in target_cols:\n    \n    mean = round(train[feature].mean(), 2)\n    median = round(train[feature].median(),2)\n    st_dev = round(train[feature].std(), 2)\n    \n    fig,ax = plt.subplots(1, 1, figsize = (18, 6))\n    (train[[index_col, feature]].sample(500).sort_values(index_col, ignore_index = True).set_index(index_col)\n     .plot(lw = 2, linestyle = \"-.\", ax=ax, color = (0.31883238319215684, 0.4266050511215686, 0.8598574482039216)))\n    fig.suptitle('{} through time'.format(feature), fontsize = 20, color ='black', fontweight = 'bold')\n\n    Ys = ax.get_yticks()\n    y_min = Ys.min()\n\n\n    if y_min < 0:\n        new_miny = y_min*0.93\n    else:\n        new_miny = y_min*1.07\n    y_max = Ys.max()\n    new_maxy = y_max*1.07\n\n    Xs = ax.get_xticks()\n    x_min = Xs.min()\n    if x_min < 0:\n        new_minx = x_min*0.8\n    else:\n        new_minx = x_min*1.2\n    x_max = Xs.max()\n    new_maxx = x_max*1.01\n\n    ax.set_ylim(new_miny, new_maxy)\n    ax.set_xlim(new_minx, new_maxx)\n\n    ax.hlines(y = mean, xmin = x_min, xmax = x_max, colors='crimson',\n              linestyles='dashdot', label='mean', alpha = 0.3, linewidth = 3)\n    ax.text(x = x_max*0.9, y = train[feature].mean(), s = 'mean: {}'.format(mean))\n\n    fig.show()\n\n    fig,ax = plt.subplots(1, 1, figsize = (16, 6))\n    \n    fig.suptitle('{} distribution'.format(feature), fontsize = 20, color ='black', fontweight = 'bold')\n\n    percentiles_asked = [0.1, 0.25, 0.5, 0.75, 0.9]\n    percentiles = train[feature].quantile(percentiles_asked).tolist()\n\n    ax.grid()\n    \n    sns.histplot(data = train, x = feature, ax = ax, kde=False, bins = N_BINS, stat = 'density', \n                 alpha = 0.5, fill = True, linewidth = 3, edgecolor='black', color = 'red')\n    sns.kdeplot(data = train, x = feature, ax = ax, alpha = 0.01, fill = True, \n                linewidth = 3, color = 'blue')\n\n    Ys = ax.get_yticks()\n    y_min = Ys.min()\n    if y_min < 0:\n        new_miny = y_min*0.93\n    else:\n        new_miny = y_min*1.07\n        \n    y_max = Ys.max()\n    new_maxy = y_max*1.1\n    \n    Xs = ax.get_xticks()\n    x_min = Xs.min()\n    \n    if x_min < 0:\n        new_xmin = x_min*0.7\n    else:\n        new_xmin = x_min*1.2\n        \n    x_max = Xs.max()\n    new_maxx = x_max*1.07\n    \n    ax.grid()\n    \n    ax.set_ylim(new_miny, new_maxy)\n    ax.set_xlim(new_xmin-0.05, new_maxx)\n    ax.text(new_xmin, new_maxy*0.2, \"mean: {}\".format(mean), size = 12, alpha = 1)\n    ax.text(new_xmin, new_maxy*0.35, \"median: {}\".format(median), size = 12, alpha = 1)\n    ax.text(new_xmin, new_maxy*0.5, \"std deviation: {}\".format(st_dev), size = 12, alpha = 1)\n    \n    \n    percentiles_asked = [0.25, 0.5, 0.75]\n    percentiles = train[feature].quantile(percentiles_asked).tolist()\n    for m, percentile in enumerate(percentiles):\n        ax.axvline(percentile, alpha = 0.5, ymin = 0, ymax = 1, linestyle = \":\", color = '#FFC30B')\n        ax.text(percentile-0.16, new_maxy, \"{}\".format(percentiles_asked[m]), size = 12, alpha = 1)\n    \n    fig.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T08:45:25.580772Z","iopub.execute_input":"2021-07-20T08:45:25.581173Z","iopub.status.idle":"2021-07-20T08:45:27.17353Z","shell.execute_reply.started":"2021-07-20T08:45:25.581132Z","shell.execute_reply":"2021-07-20T08:45:27.172691Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h6> Train vs Test Feature distributions </h6>","metadata":{}},{"cell_type":"code","source":"for idx, feature in enumerate(feature_cols):\n    \n    if idx%4 == 0:\n        fig, axes = plt.subplots(2, 2, figsize = (20, 14))\n        ax = axes.ravel()\n\n    sns.kdeplot(x = train[feature], \n            ax = ax[idx%4], alpha = 0.25, fill = True, label = 'train', \n            linewidth = 3, color = 'blue')\n\n    sns.kdeplot(x = test[feature], \n            ax = ax[idx%4], alpha = 0.25, fill = True, label = 'test', \n            linewidth = 3, color = 'red')\n    \n    if idx%4 ==0:\n        ax[idx%4].legend(fontsize = 20, loc = 'upper right')\n\n    ax[idx%4].set_ylabel('Density', fontsize = 15)\n    ax[idx%4].set_title('')\n    fig.suptitle('Train vs Test Distribution comparison {}'.format(feature), \n             fontsize = 20, fontweight = 'bold')\n\n    plt.subplots_adjust(hspace = 0.6)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T08:45:27.174586Z","iopub.execute_input":"2021-07-20T08:45:27.174995Z","iopub.status.idle":"2021-07-20T08:45:28.937484Z","shell.execute_reply.started":"2021-07-20T08:45:27.174954Z","shell.execute_reply":"2021-07-20T08:45:28.936821Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h5> Correlation Matrix </h5> ","metadata":{"execution":{"iopub.status.busy":"2021-07-01T10:11:47.077882Z","iopub.execute_input":"2021-07-01T10:11:47.078244Z","iopub.status.idle":"2021-07-01T10:11:47.085086Z","shell.execute_reply.started":"2021-07-01T10:11:47.078212Z","shell.execute_reply":"2021-07-01T10:11:47.083415Z"}}},{"cell_type":"code","source":"corr_df = train.drop('date_time', axis = 1).copy()\ncorr_matrix = round(corr_df.corr(), 2)\nmask = np.triu(np.ones_like(corr_matrix, dtype=bool))\ncolors = sns.color_palette('coolwarm', 16)\nlevels = np.linspace(-1, 1, 16)\ncmap_plot, norm = matplotlib.colors.from_levels_and_colors(levels, colors, extend=\"max\")\n\nfig, axes = plt.subplots(1, 2, figsize = (18, 10), gridspec_kw={'width_ratios': [2, 1]})\nax = axes.ravel()\nmask_feature = np.triu(np.ones_like(corr_matrix[feature_cols].loc[feature_cols], dtype=bool))\nsns.heatmap(corr_matrix[feature_cols].loc[feature_cols], \n            mask = mask_feature,\n            annot=True, ax = ax[0], cbar=False,\n            cmap = cmap_plot, \n            norm = norm, annot_kws={\"size\": 15, \"color\": 'black', 'fontweight' : 'bold'})\nax[0].hlines(range(len(feature_cols)), *ax[0].get_xlim(), color = 'black')\nax[0].vlines(range(len(feature_cols)), *ax[0].get_ylim(), color = 'black')\n\nmask_target = np.triu(np.ones_like(corr_matrix[target_cols].loc[target_cols], dtype=bool))\nsns.heatmap(corr_matrix[target_cols].loc[target_cols], \n            mask = mask_target,\n            annot=True, ax = ax[1], \n            cmap = cmap_plot, \n            norm = norm, annot_kws={\"size\": 15, \"color\": 'black', 'fontweight' : 'bold'})\nax[1].hlines(range(len(target_cols)), *ax[0].get_xlim(), color = 'black')\nax[1].vlines(range(len(target_cols)), *ax[0].get_ylim(), color = 'black')\nax[0].set_title('Features', fontsize = 20)\nax[1].set_title('Targets', fontsize = 20)\n\nfig.suptitle('Correlation Matrix', \n             fontsize = 20, color = 'black', fontweight = 'bold')\nfig, ax = plt.subplots(1, 1, figsize = (20, 20))\nsns.heatmap(corr_matrix, mask=mask, annot=True, ax = ax, \n            cmap = cmap_plot, \n            norm = norm, annot_kws={\"size\": 15, \"color\": 'black', 'fontweight' : 'bold'})\nax.hlines(range(len(corr_matrix.columns)), *ax.get_xlim(), color = 'black')\nax.vlines(range(len(corr_matrix.columns)), *ax.get_ylim(), color = 'black')\nax.xaxis.set_ticks_position('bottom')\nax.set_title('Distinct values for each variable', fontsize = 20)\nax.tick_params(axis='both', which='major', labelsize=14)\nax.tick_params(axis='both', which='minor', labelsize=14)\nax.set_xticklabels(ax.get_xticklabels(), rotation = 'vertical', fontsize = 15, color = 'black')\nax.set_yticklabels(ax.get_yticklabels(), rotation = 35, fontsize = 15, color = 'black')\nax.xaxis.label.set_size(14)\n\ncircle_rad = 25  # This is the radius, in points\nax.plot(4.5, 9.5, 'o',\n        ms=circle_rad * 2, mec='red', mfc='none', mew=4)\n\ncircle_rad = 25  # This is the radius, in points\nax.plot(4.5, 5.5, 'o',\n        ms=circle_rad * 2, mec='blue', mfc='none', mew=4)\n\nfig.suptitle('Correlation Matrix for {}'.format('train.csv'), \n             fontsize = 20, color = 'black', fontweight = 'bold')\nplt.title(\"Circled highest and lowest correlation values\", fontsize = 12)\nfig.show()\n    ","metadata":{"execution":{"iopub.status.busy":"2021-07-20T08:45:28.938414Z","iopub.execute_input":"2021-07-20T08:45:28.938756Z","iopub.status.idle":"2021-07-20T08:45:30.577083Z","shell.execute_reply.started":"2021-07-20T08:45:28.938729Z","shell.execute_reply":"2021-07-20T08:45:30.575884Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h5> AutoCorrelation Analysis </h5>","metadata":{}},{"cell_type":"code","source":"colors = [(0.31883238319215684, 0.4266050511215686, 0.8598574482039216), \n          (0.810615674827451, 0.26879706171764706, 0.23542761153333333)]\n\nfor enum, feature in enumerate(feature_cols):\n    \n    if enum % 2 == 0:\n        fig, axes = plt.subplots(2, 3, figsize = (20, 12))\n        ax = axes.ravel()\n        \n    if enum == 0:\n        plt.suptitle(\"Autocorrelation Analysis for feature columns\", size = 20, fontweight='bold')\n    \n    series = pd.concat([train[['date_time', feature]], test[['date_time', feature]]], axis = 0).drop_duplicates(ignore_index=True)\n    \n    series = series.set_index('date_time')\n    \n    series.sample(1000).sort_index().plot(lw = 3, ax = ax[enum%2*3], title = feature, color = colors[enum%2], legend = False)\n    ax[0].set_xlabel('date_time')\n    ax[0].set_xticks([])\n    ax[3].set_xlabel('date_time')\n    ax[3].set_xticks([])\n    \n    #myFmt = matplotlib.dates.DateFormatter(\"%Y-%m-%d\")\n    #ax[enum*3].xaxis.set_major_formatter(myFmt)\n\n    acf_stat = acf(series.fillna(method = 'ffill'), nlags = 24)\n    pd.Series(acf_stat[1:]).plot(ax = ax[enum%2*3+1], color = colors[enum%2],\n                                 title = feature +' acf', linestyle = '--', alpha = None, lw = 2, ylim=(-1,1))\n\n    pacf_stat = pacf(series.fillna(method = 'ffill'), nlags = 24, method = 'ols')\n    pd.Series(pacf_stat[1:]).plot(ax = ax[enum%2*3+2], title = feature +' pacf', alpha = None, lw = 2, \n                                  linestyle = '-.', ylim=(-1,1), color = colors[enum%2],)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T08:45:30.578704Z","iopub.execute_input":"2021-07-20T08:45:30.579109Z","iopub.status.idle":"2021-07-20T08:45:34.633191Z","shell.execute_reply.started":"2021-07-20T08:45:30.579069Z","shell.execute_reply":"2021-07-20T08:45:34.632039Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"colors = [(0.31883238319215684, 0.4266050511215686, 0.8598574482039216), \n          (0.810615674827451, 0.26879706171764706, 0.23542761153333333),\n          '#FFCD00']\n\nfor enum, feature in enumerate(target_cols):\n    \n    if enum % 3 == 0:\n        fig, axes = plt.subplots(3, 3, figsize = (20, 12))\n        ax = axes.ravel()\n    \n    plt.suptitle(\"Autocorrelation Analysis for target columns\", size = 20, fontweight='bold')\n    \n    series = train.set_index('date_time')[feature].copy()\n    \n    series.sample(1000).sort_index().plot(lw = 3, ax = ax[enum%3*3], title = feature, color = colors[enum%3], legend = False)\n    ax[0].set_xlabel('date_time')\n    ax[0].set_xticks([])\n    ax[3].set_xlabel('date_time')\n    ax[6].set_xlabel('date_time')\n    ax[3].set_xticks([])\n    ax[6].set_xticks([])\n\n    acf_stat = acf(series.fillna(method = 'ffill'), nlags = 24)\n    pd.Series(acf_stat[1:]).plot(ax = ax[enum%3*3+1], color = colors[enum%3],\n                                 title = feature +' acf', linestyle = '--', alpha = None, lw = 2, ylim=(-1,1))\n\n    pacf_stat = pacf(series.fillna(method = 'ffill'), nlags = 24, method = 'ols')\n    pd.Series(pacf_stat[1:]).plot(ax = ax[enum%3*3+2], title = feature +' pacf', alpha = None, lw = 2, \n                                  linestyle = '-.', ylim=(-1,1), color = colors[enum%3],)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T08:45:34.634571Z","iopub.execute_input":"2021-07-20T08:45:34.634851Z","iopub.status.idle":"2021-07-20T08:45:35.868471Z","shell.execute_reply.started":"2021-07-20T08:45:34.634821Z","shell.execute_reply":"2021-07-20T08:45:35.867512Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Personal take**: It seems that each feature/target is correlated with himself at the previous timestamp (1 hour before), as we can see directly from the partial autocorrelation plot. Train and test were concatenated for feature columns (while target columns are populated just in train, of course). ","metadata":{}},{"cell_type":"code","source":"feature_df = (pd.concat([train[['date_time']+ feature_cols], test[['date_time']+ feature_cols]], axis = 0)\n              .drop_duplicates(ignore_index = True))\n\nautocorr_df_features = (pd.DataFrame(feature_df.drop('date_time', axis = 1).apply(lambda x: x.autocorr(), 0))\n                    .reset_index().rename(columns = {'index': 'column', 0: 'autocorrelation'})\n                    .sort_values('autocorrelation', ascending = False))\n\nautocorr_df_target = (pd.DataFrame(train[target_cols].apply(lambda x: x.autocorr(), 0))\n                    .reset_index().rename(columns = {'index': 'column', 0: 'autocorrelation'})\n                    .sort_values('autocorrelation', ascending = False))\n\nautocorr_df = (pd.concat([autocorr_df_features, autocorr_df_target], axis = 0)\n                    .sort_values('autocorrelation', ignore_index = True, ascending = False))\nautocorr_df['autocorrelation'] = autocorr_df['autocorrelation'].round(4)\n\ndel autocorr_df_features, autocorr_df_target\n\nfig, ax = plt.subplots(1, 2, figsize = (16, 8), gridspec_kw={'width_ratios': [2, 1]})\nfig.suptitle('Autocorrelation lag 1 for each feature and target')\nsns.barplot(x='autocorrelation', y='column', data=(autocorr_df), ax = ax[0], palette = 'coolwarm')\ny_labels = autocorr_df.column.tolist()\nax[0].set_yticklabels([])\nax[0].set_xticklabels([])\nt=0\nfor p in ax[0].patches:\n    width = p.get_width() \n    if width < 0.01:\n        ax[0].text(width,\n        p.get_y() + p.get_height() / 2, \n        '{:1.4f}'.format(width),\n        ha = 'left', \n        va = 'center')\n    else:\n        ax[0].text(width/4, \n\n        p.get_y() + p.get_height() / 2, \n        '{} {:1.4f}'.format(y_labels[t], width),\n        ha = 'left',  \n        va = 'center',\n        color = 'black',\n        fontsize = 12)\n    t+=1\n    \nbbox=[-0.2, 0, 1.2, 0.9]\nax[1].axis('off')\nax[1].title.set_text('')\nccolors = plt.cm.BuPu(np.full(len(autocorr_df.columns), 0.1))\n\nmpl_table = ax[1].table(cellText = autocorr_df.values, bbox=bbox, colLabels=autocorr_df.columns, colColours=ccolors)\nmpl_table.auto_set_font_size(False)\nmpl_table.auto_set_column_width(col=list(range(len(autocorr_df.columns))))\nmpl_table.set_fontsize(14)\nplt.subplots_adjust(hspace = 0.6)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-20T08:45:35.869822Z","iopub.execute_input":"2021-07-20T08:45:35.870378Z","iopub.status.idle":"2021-07-20T08:45:36.416409Z","shell.execute_reply.started":"2021-07-20T08:45:35.870338Z","shell.execute_reply":"2021-07-20T08:45:36.415397Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h5> CrossCorrelation Analysis </h5>","metadata":{}},{"cell_type":"code","source":"TOTAL_LAGS = 25\ntotal_lags = range(1, TOTAL_LAGS)\nfeatures = list(set(train.columns) - set(['date_time']))\ncombinations = list(itertools.product(features, features))\nCROSS_THRESHOLD = 0.5\n\ncross_corr = {}\n\nfor j in total_lags:\n    cross_corr[j] = []\n    for k in tqdm.tqdm(combinations):\n        cross_corr[j].append(crosscorr(train[k[0]], train[k[1]], lag = j))\n\ncross_corr = pd.DataFrame(cross_corr)\ncross_corr.columns = ['cross_correlation_lag_{}'.format(i) for i in range(1, TOTAL_LAGS)]\n\ncross_correlations = (pd.concat([pd.DataFrame(combinations).rename(columns = {0: 'first_feature', 1: 'second_feature'}),\n                                 pd.DataFrame(cross_corr)], 1))\n\ncross_correlations_melt = (pd.melt(cross_correlations, id_vars=['first_feature', 'second_feature'], \n                           value_vars=['cross_correlation_lag_{}'.format(i) for i in range(1, TOTAL_LAGS)],\n                           var_name = 'lag',\n                           value_name = 'cross_correlation')\n                          .assign(lag=lambda x: x.lag.str.replace('cross_correlation_lag_', \"\")))\n\n\ndef sort_features(x, y):\n\n    return tuple(sorted([x,y]))\n\ncross_correlations_melt[['pair_of_features']] = (cross_correlations_melt.apply(lambda x:sort_features(x.first_feature,\n                                                                                                          x.second_feature), 1))\n\ncross_correlations_melt['first_feature'] = cross_correlations_melt['pair_of_features'].apply(lambda x: x[0])\ncross_correlations_melt['second_feature'] = cross_correlations_melt['pair_of_features'].apply(lambda x: x[1])\n\ncross_correlations_melt['pair_of_features'] = (cross_correlations_melt['first_feature'].str.replace(\"feature_\", \"\") + \n                                          \"__\"  + cross_correlations_melt['second_feature'].str.replace(\"feature_\", \"\") + \"__lag\" +\n                                               cross_correlations_melt['lag']\n                                              ).astype(str)\n\ncross_correlations_melt = cross_correlations_melt.drop_duplicates(['pair_of_features'], ignore_index=True)\n\ncross_correlations_melt = (cross_correlations_melt.loc[(abs(cross_correlations_melt.cross_correlation) > CROSS_THRESHOLD) & \n                                  (cross_correlations_melt.first_feature!= cross_correlations_melt.second_feature)]\n                              .reset_index(drop = True))","metadata":{"execution":{"iopub.status.busy":"2021-07-20T08:45:36.417595Z","iopub.execute_input":"2021-07-20T08:45:36.417908Z","iopub.status.idle":"2021-07-20T08:45:38.015459Z","shell.execute_reply.started":"2021-07-20T08:45:36.417878Z","shell.execute_reply":"2021-07-20T08:45:38.014391Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h6> Most positively correlated features </h6>","metadata":{}},{"cell_type":"code","source":"display(cross_correlations_melt.sort_values('cross_correlation', ascending = False, ignore_index = True).head(5))","metadata":{"execution":{"iopub.status.busy":"2021-07-20T08:45:38.018871Z","iopub.execute_input":"2021-07-20T08:45:38.019185Z","iopub.status.idle":"2021-07-20T08:45:38.034029Z","shell.execute_reply.started":"2021-07-20T08:45:38.019155Z","shell.execute_reply":"2021-07-20T08:45:38.032962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h6> Most negatively correlated features </h6>","metadata":{}},{"cell_type":"code","source":"display(cross_correlations_melt.sort_values('cross_correlation', ascending = True, ignore_index = True).head(5))","metadata":{"execution":{"iopub.status.busy":"2021-07-20T08:45:38.036765Z","iopub.execute_input":"2021-07-20T08:45:38.037071Z","iopub.status.idle":"2021-07-20T08:45:38.054104Z","shell.execute_reply.started":"2021-07-20T08:45:38.03704Z","shell.execute_reply":"2021-07-20T08:45:38.053086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h6> An example of positive and negatively cross-correlated features </h6>","metadata":{}},{"cell_type":"code","source":"cols = ['sensor_2', 'target_benzene']\ndf_cross = train.copy()\ndf_cross['target_benzene__lag1'] = df_cross['target_benzene'].shift(1)\ndf_cross['target_benzene__lag1'] = (df_cross['target_benzene__lag1'] - df_cross['target_benzene__lag1'].mean())/df_cross['target_benzene__lag1'].std()\n\ndf_cross['sensor_2'] = (df_cross['sensor_2'] - df_cross['sensor_2'].mean())/df_cross['sensor_2'].std()\ndf_cross['sensor_3__lag1'] = df_cross['sensor_3'].shift(1)\ndf_cross['sensor_3__lag1'] = (df_cross['sensor_3__lag1'] - df_cross['sensor_3__lag1'].mean())/df_cross['sensor_3__lag1'].std()\n\ndf_cross = df_cross.set_index('date_time')\n\nfig, axes = plt.subplots(2, 1, figsize = (14, 10))\nax = axes.ravel()\n\ndf_cross[['sensor_2', 'target_benzene__lag1']].plot(ax = ax[0], lw = 2, alpha = 0.5, linestyle = \"-.\")\n\n\n(df_cross[['sensor_2', 'sensor_3__lag1']].plot(ax = ax[1], lw = 2, \n                                                 linestyle = \"-.\",  alpha = 0.5, sharex=True))\n\nfig.suptitle('Positive (Above) vs Negative (Below) crosscorrelation')\n\nmyFmt = matplotlib.dates.DateFormatter(\"%Y-%m\")\nax[1].xaxis.set_ticks([])\nax[0].set_title('sensor_2 vs target_benzene__lag1: 0.824438 Correlation')\nax[1].set_title('sensor_2 vs sensor_3__lag1: -0.72352 Correlation')\nax[1].legend(loc=\"upper right\", bbox_to_anchor=(1.1,1.1))","metadata":{"execution":{"iopub.status.busy":"2021-07-20T08:45:38.055439Z","iopub.execute_input":"2021-07-20T08:45:38.055827Z","iopub.status.idle":"2021-07-20T08:45:38.513381Z","shell.execute_reply.started":"2021-07-20T08:45:38.05579Z","shell.execute_reply":"2021-07-20T08:45:38.512435Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h5> Types of CrossValidation Techniques </h5>\n\nLet's see different `sklearn.model_selection` Split generators and choose the one most suitable for Time Series data.  ","metadata":{"execution":{"iopub.status.busy":"2021-07-01T13:29:44.942676Z","iopub.execute_input":"2021-07-01T13:29:44.943076Z","iopub.status.idle":"2021-07-01T13:29:44.955783Z","shell.execute_reply.started":"2021-07-01T13:29:44.94304Z","shell.execute_reply":"2021-07-01T13:29:44.954954Z"},"_kg_hide-output":true}},{"cell_type":"code","source":"def plot_cv_indices(cv, n_splits, X, y, date_col = None):\n    \"\"\"Create a sample plot for indices of a cross-validation object.\"\"\"\n    \n    fig, ax = plt.subplots(1, 1, figsize = (11, 7))\n    \n    # Generate the training/testing visualizations for each CV split\n    for ii, (tr, tt) in enumerate(cv.split(X=X, y=y)):\n        # Fill in indices with the training/test groups\n        indices = np.array([np.nan] * len(X))\n        indices[tt] = 1\n        indices[tr] = 0\n\n        # Visualize the results\n        ax.scatter(range(len(indices)), [ii+1] * len(indices),\n                   c=indices, marker='_', lw=10, cmap=cmap_cv,\n                   vmin=-.2, vmax=1.2)\n\n\n    # Formatting\n    yticklabels = list(range(n_splits))\n    \n    if date_col is not None:\n        tick_locations  = ax.get_xticks()\n        tick_dates = [\" \"] + date_col.iloc[list(tick_locations[1:-1])].astype(str).tolist() + [\" \"]\n\n        tick_locations_str = [str(int(i)) for i in tick_locations]\n        new_labels = ['\\n\\n'.join(x) for x in zip(list(tick_locations_str), tick_dates) ]\n        ax.set_xticks(tick_locations)\n        ax.set_xticklabels(new_labels)\n    \n    ax.set(yticks=np.arange(n_splits+2), #yticklabels=yticklabels,\n           xlabel='Sample index', ylabel=\"CV iteration\",\n           ylim=[5.5, 0]\n          )\n    #ax.set_yticklabels([\"\"] +range(1, n_splits+1)+[])\n    ax.legend([Patch(color=cmap_cv(.8)), Patch(color=cmap_cv(.02))],\n              ['Testing set', 'Training set'], loc=(1.02, .8))\n    ax.set_title('{}'.format(type(cv).__name__), fontsize=15)\n    \nfrom sklearn.model_selection import KFold, ShuffleSplit, StratifiedKFold, StratifiedShuffleSplit, TimeSeriesSplit\ncvs = [KFold, ShuffleSplit, StratifiedKFold, StratifiedShuffleSplit, TimeSeriesSplit]\nn_points = 100\nn_splits = 5\nX = np.random.randn(100, 10)\npercentiles_classes = [.1, .3, .6]\ny = np.hstack([[ii] * int(100 * perc) for ii, perc in enumerate(percentiles_classes)])\n\nfor i, cv in enumerate(cvs):\n    this_cv = cv(n_splits=n_splits)\n    plot_cv_indices(this_cv, n_splits, X, y, date_col=None)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T08:45:38.515185Z","iopub.execute_input":"2021-07-20T08:45:38.515624Z","iopub.status.idle":"2021-07-20T08:45:39.334149Z","shell.execute_reply.started":"2021-07-20T08:45:38.515581Z","shell.execute_reply":"2021-07-20T08:45:39.333158Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see Time Series Split is the most appropriate one when dealing with time series data, like in this case. ","metadata":{}},{"cell_type":"markdown","source":"<a id = 'sub'></a>\n<h5> Example usage of Time Series Split for Sample Submission </h5>","metadata":{}},{"cell_type":"code","source":"sample_submission = pd.read_csv(root_path +\"/sample_submission.csv\")\nassert len(sample_submission) == len(test), 'Different number of values between sample_submission.csv and test.csv'","metadata":{"execution":{"iopub.status.busy":"2021-07-20T08:45:39.335391Z","iopub.execute_input":"2021-07-20T08:45:39.335676Z","iopub.status.idle":"2021-07-20T08:45:39.348851Z","shell.execute_reply.started":"2021-07-20T08:45:39.33565Z","shell.execute_reply":"2021-07-20T08:45:39.347655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's build a training set, creating some lag 1 columns for our features (since autocorrelation is most at lag 1). ","metadata":{}},{"cell_type":"code","source":"train_length = len(train)-1 #I drop the last row, since already present in test\n\nfull_df = pd.concat([train[['date_time']+feature_cols].iloc[:-1], test[['date_time']+feature_cols]], axis = 0, ignore_index = True)\nassert len(full_df) == train_length + len(test)\n\nnew_feature_cols=[]\n\nfor feature_col in feature_cols:\n    new_feature = feature_col+\"_lag\"\n    full_df[new_feature] = full_df[feature_col].shift()\n    new_feature_cols.append(new_feature)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T08:45:39.351446Z","iopub.execute_input":"2021-07-20T08:45:39.351715Z","iopub.status.idle":"2021-07-20T08:45:39.368041Z","shell.execute_reply.started":"2021-07-20T08:45:39.351689Z","shell.execute_reply":"2021-07-20T08:45:39.366942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ts_fold = TimeSeriesSplit(n_splits = 5)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T08:45:39.369401Z","iopub.execute_input":"2021-07-20T08:45:39.369655Z","iopub.status.idle":"2021-07-20T08:45:39.381173Z","shell.execute_reply.started":"2021-07-20T08:45:39.36963Z","shell.execute_reply":"2021-07-20T08:45:39.380177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = []\n\nfor train_idx, val_idx in tqdm.tqdm(ts_fold.split(X=full_df.iloc[:train_length], y=train.iloc[:train_length][target_cols])):\n    \n    preds = []\n    for target in target_cols:\n        lgbm_model = LGBMRegressor(max_depth = 10)\n        lgbm_model.fit(full_df.loc[train_idx, feature_cols+new_feature_cols], train.loc[train_idx, target].values)\n        preds.append(lgbm_model.predict(full_df.iloc[train_length:][feature_cols+new_feature_cols]))\n    \n    predictions.append(preds)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T08:45:39.382498Z","iopub.execute_input":"2021-07-20T08:45:39.382942Z","iopub.status.idle":"2021-07-20T08:45:41.96674Z","shell.execute_reply.started":"2021-07-20T08:45:39.382896Z","shell.execute_reply":"2021-07-20T08:45:41.965954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = np.mean(predictions, axis = 0)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T08:45:41.968462Z","iopub.execute_input":"2021-07-20T08:45:41.968724Z","iopub.status.idle":"2021-07-20T08:45:41.972261Z","shell.execute_reply.started":"2021-07-20T08:45:41.968698Z","shell.execute_reply":"2021-07-20T08:45:41.971586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission[target_cols] = preds.transpose()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T08:45:41.973092Z","iopub.execute_input":"2021-07-20T08:45:41.97345Z","iopub.status.idle":"2021-07-20T08:45:41.98847Z","shell.execute_reply.started":"2021-07-20T08:45:41.973424Z","shell.execute_reply":"2021-07-20T08:45:41.987541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission.to_csv('sample_submission.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T08:45:41.989584Z","iopub.execute_input":"2021-07-20T08:45:41.98985Z","iopub.status.idle":"2021-07-20T08:45:42.01724Z","shell.execute_reply.started":"2021-07-20T08:45:41.989825Z","shell.execute_reply":"2021-07-20T08:45:42.016329Z"},"trusted":true},"execution_count":null,"outputs":[]}]}