{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<big>For classical machine learning algorithms, we often use the most popular Python library, Scikit-learn. With Scikit-learn you can fit models and search for optimal parameters, but it sometimes works for hours.</big><br><br>\n\n<big>I want to show you how to use Scikit-learn library and get the results faster without changing the code. To do this, we will make use of another Python library,  <a href='https://github.com/intel/scikit-learn-intelex'>Intel® Extension for Scikit-learn*</a>.</big><br><br>\n\n<big>I will show you how to <strong>speed up your kernel more than 2 times</strong> without changing your code!</big>","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2021-07-13T13:15:14.1205Z","iopub.execute_input":"2021-07-13T13:15:14.121327Z","iopub.status.idle":"2021-07-13T13:15:14.127067Z","shell.execute_reply.started":"2021-07-13T13:15:14.121254Z","shell.execute_reply":"2021-07-13T13:15:14.125694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>Importing data</h2>","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('../input/tabular-playground-series-jul-2021/train.csv', parse_dates=True)\ntest_data = pd.read_csv('../input/tabular-playground-series-jul-2021/test.csv')\nsemp_sub = pd.read_csv('../input/tabular-playground-series-jul-2021/sample_submission.csv')\npseudolabels = pd.read_csv('../input/psd-sub/submission_psd.csv')\n","metadata":{"execution":{"iopub.status.busy":"2021-07-13T13:15:14.128933Z","iopub.execute_input":"2021-07-13T13:15:14.129271Z","iopub.status.idle":"2021-07-13T13:15:14.194704Z","shell.execute_reply.started":"2021-07-13T13:15:14.129231Z","shell.execute_reply":"2021-07-13T13:15:14.193525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-13T13:15:14.196613Z","iopub.execute_input":"2021-07-13T13:15:14.196921Z","iopub.status.idle":"2021-07-13T13:15:14.220223Z","shell.execute_reply.started":"2021-07-13T13:15:14.196891Z","shell.execute_reply":"2021-07-13T13:15:14.218965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>Preprocessing</h2>\n\n<big><strong>Pseudodating</strong></big><br><br>\n<big>I took the previously predicted labels and added them to the test dataset.</big>","metadata":{}},{"cell_type":"code","source":"for col in ['target_carbon_monoxide', 'target_benzene', 'target_nitrogen_oxides']:\n    test_data[col] = pseudolabels[col]","metadata":{"execution":{"iopub.status.busy":"2021-07-13T13:15:14.222239Z","iopub.execute_input":"2021-07-13T13:15:14.222769Z","iopub.status.idle":"2021-07-13T13:15:14.234855Z","shell.execute_reply.started":"2021-07-13T13:15:14.222727Z","shell.execute_reply":"2021-07-13T13:15:14.233999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<big>Now let's combine the test and train datasets.</big>","metadata":{}},{"cell_type":"code","source":"full_data = pd.concat([data, test_data]).reset_index(drop = True)","metadata":{"execution":{"iopub.status.busy":"2021-07-13T13:15:14.235999Z","iopub.execute_input":"2021-07-13T13:15:14.236448Z","iopub.status.idle":"2021-07-13T13:15:14.257658Z","shell.execute_reply.started":"2021-07-13T13:15:14.236413Z","shell.execute_reply":"2021-07-13T13:15:14.256424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<big>I added new features to the dataset.</big> \n<big>They were obtained by researching combinations of original features using <code>feature_importances_</code>.</big>","metadata":{}},{"cell_type":"code","source":"test_data = test_data.drop(['target_carbon_monoxide', 'target_benzene', 'target_nitrogen_oxides'], axis=1)\nall_data = [full_data, test_data]\n\nfor df in all_data:\n    df['date_time'] = df['date_time'].astype('datetime64[ns]').astype(np.int64)/10**9\n    df['S1xS2'] = df['sensor_1'] * df['sensor_2']\n    df['S2xS5'] = df['sensor_2'] * df['sensor_5']\n    df['S2^2'] = df['sensor_2']**2\ndata = data.sample(frac=1)","metadata":{"execution":{"iopub.status.busy":"2021-07-13T13:15:14.25941Z","iopub.execute_input":"2021-07-13T13:15:14.259763Z","iopub.status.idle":"2021-07-13T13:15:14.285493Z","shell.execute_reply.started":"2021-07-13T13:15:14.259728Z","shell.execute_reply":"2021-07-13T13:15:14.284391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<big>Next step is split the data into features and targets.</big>","metadata":{}},{"cell_type":"code","source":"x_data = full_data.drop(['target_carbon_monoxide', 'target_benzene', 'target_nitrogen_oxides'], axis=1)\ny_data = full_data[['target_carbon_monoxide', 'target_benzene', 'target_nitrogen_oxides']]\nx_data.shape, y_data.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-13T13:15:14.28804Z","iopub.execute_input":"2021-07-13T13:15:14.288491Z","iopub.status.idle":"2021-07-13T13:15:14.30509Z","shell.execute_reply.started":"2021-07-13T13:15:14.288454Z","shell.execute_reply":"2021-07-13T13:15:14.304272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<big>Now split the data into training and validation sets.</big>","metadata":{}},{"cell_type":"code","source":"x_train, x_val, y_train, y_val = train_test_split(x_data, y_data, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2021-07-13T13:15:14.307313Z","iopub.execute_input":"2021-07-13T13:15:14.308003Z","iopub.status.idle":"2021-07-13T13:15:14.318309Z","shell.execute_reply.started":"2021-07-13T13:15:14.307951Z","shell.execute_reply":"2021-07-13T13:15:14.317226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>Installing Intel(R) Extension for Scikit-learn</h2>\n\n<big>Use Intel® Extension for Scikit-learn* for fast compute Scikit-learn estimators.</big>","metadata":{}},{"cell_type":"code","source":"!pip install scikit-learn-intelex --progress-bar off >> /tmp/pip_sklearnex.log","metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-07-13T13:15:14.319935Z","iopub.execute_input":"2021-07-13T13:15:14.320611Z","iopub.status.idle":"2021-07-13T13:15:44.156498Z","shell.execute_reply.started":"2021-07-13T13:15:14.320561Z","shell.execute_reply":"2021-07-13T13:15:44.152839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<big>Patch original scikit-learn.</big>","metadata":{}},{"cell_type":"code","source":"from sklearnex import patch_sklearn\npatch_sklearn()","metadata":{"execution":{"iopub.status.busy":"2021-07-13T13:15:44.160443Z","iopub.execute_input":"2021-07-13T13:15:44.160883Z","iopub.status.idle":"2021-07-13T13:15:44.916774Z","shell.execute_reply.started":"2021-07-13T13:15:44.160841Z","shell.execute_reply":"2021-07-13T13:15:44.915413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>Using optuna to select parameters for Random Forest Regressor</h2><br><br>\n<big>Random Forest is an ensemble of Decision Trees. The work of this algorithm can be represented as a collective decision made by some expert committee.</big><br><br>\n<big>We adjust hyperparameters for the best result.</big><br><br>\n<big>The parameters that we select:</big><br>\n<big>1. <code>n_estimators</code> -  the number of trees to be used in the algorithm.<br></big>\n<big>2. <code>max_depth</code> -  the depth of each tree.<br></big>\n<big>3. <code>min_samples_split</code> - the minimum number of samples in a leaf to split.<br> </big>","metadata":{}},{"cell_type":"code","source":"from sklearn.multioutput import RegressorChain\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_log_error\nimport numpy as np\nimport optuna\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2021-07-13T13:15:44.91858Z","iopub.execute_input":"2021-07-13T13:15:44.919053Z","iopub.status.idle":"2021-07-13T13:15:45.948645Z","shell.execute_reply.started":"2021-07-13T13:15:44.919006Z","shell.execute_reply":"2021-07-13T13:15:45.947384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def objective_rf(trial):\n    params ={\n        'n_estimators': trial.suggest_int('n_estimators', 100, 2000),\n        'max_depth': trial.suggest_int('max_depth', 3, 70),\n        'min_samples_split': trial.suggest_int('min_samples_split', 2, 50),\n        'criterion': trial.suggest_categorical('criterion', ['mse']),\n        'n_jobs': -1 \n        \n    }\n    model = RegressorChain(RandomForestRegressor(**params), random_state=47).fit(x_train, y_train)\n    y_pred = model.predict(x_val)\n    loss = np.sqrt(mean_squared_log_error(y_val, y_pred))\n    return loss\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-13T13:15:45.950005Z","iopub.execute_input":"2021-07-13T13:15:45.950373Z","iopub.status.idle":"2021-07-13T13:15:45.960813Z","shell.execute_reply.started":"2021-07-13T13:15:45.950337Z","shell.execute_reply":"2021-07-13T13:15:45.959039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<big><strong>Select parameters</strong></big>","metadata":{}},{"cell_type":"code","source":"study = optuna.create_study(sampler=optuna.samplers.TPESampler(seed=123),\n                            direction=\"minimize\",\n                            pruner=optuna.pruners.HyperbandPruner())","metadata":{"execution":{"iopub.status.busy":"2021-07-13T13:15:45.962083Z","iopub.execute_input":"2021-07-13T13:15:45.962582Z","iopub.status.idle":"2021-07-13T13:15:45.986648Z","shell.execute_reply.started":"2021-07-13T13:15:45.962548Z","shell.execute_reply":"2021-07-13T13:15:45.985681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<big>Let's see the execution time.</big>","metadata":{}},{"cell_type":"code","source":"%%time\nstudy.optimize(objective_rf, n_trials=40)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-07-13T13:15:45.989325Z","iopub.execute_input":"2021-07-13T13:15:45.989905Z","iopub.status.idle":"2021-07-13T13:36:37.489195Z","shell.execute_reply.started":"2021-07-13T13:15:45.989854Z","shell.execute_reply":"2021-07-13T13:36:37.488116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>Training the model with the selected parameters</h2>","metadata":{}},{"cell_type":"code","source":"%%time\nnew_model_rf = RegressorChain(RandomForestRegressor(**study.best_params, n_jobs=-1)).fit(x_data, y_data)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-13T13:36:37.49114Z","iopub.execute_input":"2021-07-13T13:36:37.49181Z","iopub.status.idle":"2021-07-13T13:37:42.254273Z","shell.execute_reply.started":"2021-07-13T13:36:37.491764Z","shell.execute_reply":"2021-07-13T13:37:42.252988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<big>Let's look at the importance of features in training.</big>","metadata":{}},{"cell_type":"code","source":"fet0 = new_model_rf.estimators_[0].feature_importances_\nfet1 = new_model_rf.estimators_[1].feature_importances_\nfet2 = new_model_rf.estimators_[2].feature_importances_\nfets = [fet0, fet1, fet2]","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-07-13T13:37:42.256049Z","iopub.execute_input":"2021-07-13T13:37:42.256491Z","iopub.status.idle":"2021-07-13T13:37:43.180864Z","shell.execute_reply.started":"2021-07-13T13:37:42.256446Z","shell.execute_reply":"2021-07-13T13:37:43.179831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i, _ in enumerate(fets):\n    fets[i] = np.sort(fets[i])\n\nfor fet in fets:\n    plt.figure()\n    plt.barh(full_data.drop(['target_carbon_monoxide', 'target_benzene', 'target_nitrogen_oxides'], axis=1).columns, fet[:12])","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-07-13T13:37:43.182275Z","iopub.execute_input":"2021-07-13T13:37:43.182582Z","iopub.status.idle":"2021-07-13T13:37:43.825617Z","shell.execute_reply.started":"2021-07-13T13:37:43.182552Z","shell.execute_reply":"2021-07-13T13:37:43.824509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>Prediction</h2>","metadata":{}},{"cell_type":"code","source":"%%time\ny_pred = new_model_rf.predict(test_data)","metadata":{"execution":{"iopub.status.busy":"2021-07-13T13:37:43.827031Z","iopub.execute_input":"2021-07-13T13:37:43.827543Z","iopub.status.idle":"2021-07-13T13:37:45.040183Z","shell.execute_reply.started":"2021-07-13T13:37:43.827505Z","shell.execute_reply":"2021-07-13T13:37:45.038965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<big>Save the results in 'submission.csv'.</big>","metadata":{}},{"cell_type":"code","source":"semp_sub['target_carbon_monoxide'] = y_pred[:, 0]\nsemp_sub['target_benzene'] = y_pred[:, 1]\nsemp_sub['target_nitrogen_oxides'] = y_pred[:, 2]\nsemp_sub.to_csv('submission.csv', index=False)\nsemp_sub.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-13T13:37:45.043954Z","iopub.execute_input":"2021-07-13T13:37:45.044285Z","iopub.status.idle":"2021-07-13T13:37:45.076798Z","shell.execute_reply.started":"2021-07-13T13:37:45.044253Z","shell.execute_reply":"2021-07-13T13:37:45.076043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>Now we use the same algorithms with original scikit-learn<h2>","metadata":{}},{"cell_type":"markdown","source":"<big>Let’s run the same Scikit-learn code without the patching offered by Intel® Extension for Scikit-learn and compare its execution time with the execution time of the patched Scikit-learn.</big>","metadata":{}},{"cell_type":"code","source":"from sklearnex import unpatch_sklearn\nunpatch_sklearn()","metadata":{"execution":{"iopub.status.busy":"2021-07-13T13:37:45.077974Z","iopub.execute_input":"2021-07-13T13:37:45.078379Z","iopub.status.idle":"2021-07-13T13:37:45.081985Z","shell.execute_reply.started":"2021-07-13T13:37:45.078349Z","shell.execute_reply":"2021-07-13T13:37:45.080912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor","metadata":{"execution":{"iopub.status.busy":"2021-07-13T13:37:45.083454Z","iopub.execute_input":"2021-07-13T13:37:45.083722Z","iopub.status.idle":"2021-07-13T13:37:45.094685Z","shell.execute_reply.started":"2021-07-13T13:37:45.083696Z","shell.execute_reply":"2021-07-13T13:37:45.09368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<big>Select parameters for Random Forest Regressor.</big>","metadata":{}},{"cell_type":"code","source":"study = optuna.create_study(sampler=optuna.samplers.TPESampler(seed=123),\n                            direction=\"minimize\",\n                            pruner=optuna.pruners.HyperbandPruner())","metadata":{"execution":{"iopub.status.busy":"2021-07-13T13:37:45.096076Z","iopub.execute_input":"2021-07-13T13:37:45.096452Z","iopub.status.idle":"2021-07-13T13:37:45.109193Z","shell.execute_reply.started":"2021-07-13T13:37:45.096417Z","shell.execute_reply":"2021-07-13T13:37:45.108008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<big>Let's see the execution time without patch.</big>","metadata":{}},{"cell_type":"code","source":"%%time\nstudy.optimize(objective_rf, n_trials=40)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-07-13T13:37:45.1108Z","iopub.execute_input":"2021-07-13T13:37:45.111251Z","iopub.status.idle":"2021-07-13T14:23:02.95326Z","shell.execute_reply.started":"2021-07-13T13:37:45.111188Z","shell.execute_reply":"2021-07-13T14:23:02.952144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nnew_model_rf = RegressorChain(RandomForestRegressor(**study.best_params, n_jobs=-1)).fit(x_data, y_data)","metadata":{"execution":{"iopub.status.busy":"2021-07-13T14:23:02.954915Z","iopub.execute_input":"2021-07-13T14:23:02.955237Z","iopub.status.idle":"2021-07-13T14:24:23.493193Z","shell.execute_reply.started":"2021-07-13T14:23:02.955192Z","shell.execute_reply":"2021-07-13T14:24:23.491488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>Conclusions</h2>\n<big>We can see that using only one classical machine learning algorithm may give you a pretty hight accuracy score. We also use well-known libraries Scikit-learn and Optuna, as well as the increasingly popular library Intel® Extension for Scikit-learn. Noted that Intel® Extension for Scikit-learn gives you opportunities to:</big>\n\n* <big>Use your Scikit-learn code for training and inference without modification.</big>\n* <big>Speed up selection of parameters <strong>from 45 minutes to 20 minutes.</strong></big>\n* <big>Get predictions of the similar quality.</big>\n","metadata":{}}]}