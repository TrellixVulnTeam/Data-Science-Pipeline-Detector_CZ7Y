{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Hi Kagglers!\n\nIn this month competition we are dealing with dataset which has time series, therefore the obvious choice would be an algorithm which can handle such data. In this notebook I'd like to create a model using LSTM, often referred as fancy RNN (Recursive Neural Network). Unlike ARIMA, RNNs are capable of learning nonlinearities, and specialized nodes like LSTM nodes are even better at this. First, in my EDA I'd like to show how to get insights using statistical methods from the dataset to help us choosing right algorythm and also what can be done (e.g. feature extraction, outliers) to improve a model.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nimport scipy as sp\n\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nprint(f\"Tensorflow version {tf.__version__}\")","metadata":{"execution":{"iopub.status.busy":"2021-07-15T19:06:44.545381Z","iopub.execute_input":"2021-07-15T19:06:44.545836Z","iopub.status.idle":"2021-07-15T19:06:51.459001Z","shell.execute_reply.started":"2021-07-15T19:06:44.545743Z","shell.execute_reply":"2021-07-15T19:06:51.457811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/tabular-playground-series-jul-2021/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/tabular-playground-series-jul-2021/test.csv\")\nsample_df = pd.read_csv(\"/kaggle/input/tabular-playground-series-jul-2021/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-07-15T19:06:51.460827Z","iopub.execute_input":"2021-07-15T19:06:51.461246Z","iopub.status.idle":"2021-07-15T19:06:51.53061Z","shell.execute_reply.started":"2021-07-15T19:06:51.461203Z","shell.execute_reply":"2021-07-15T19:06:51.529624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Datasets Overview ","metadata":{}},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-15T19:06:51.53273Z","iopub.execute_input":"2021-07-15T19:06:51.533058Z","iopub.status.idle":"2021-07-15T19:06:51.57664Z","shell.execute_reply.started":"2021-07-15T19:06:51.533018Z","shell.execute_reply":"2021-07-15T19:06:51.575646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.info()","metadata":{"execution":{"iopub.status.busy":"2021-07-15T19:06:51.578612Z","iopub.execute_input":"2021-07-15T19:06:51.578944Z","iopub.status.idle":"2021-07-15T19:06:51.602834Z","shell.execute_reply.started":"2021-07-15T19:06:51.578913Z","shell.execute_reply":"2021-07-15T19:06:51.601659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.info()","metadata":{"execution":{"iopub.status.busy":"2021-07-15T19:06:51.60426Z","iopub.execute_input":"2021-07-15T19:06:51.604643Z","iopub.status.idle":"2021-07-15T19:06:51.621734Z","shell.execute_reply.started":"2021-07-15T19:06:51.604606Z","shell.execute_reply":"2021-07-15T19:06:51.620629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.describe().T","metadata":{"execution":{"iopub.status.busy":"2021-07-15T19:06:51.62299Z","iopub.execute_input":"2021-07-15T19:06:51.623395Z","iopub.status.idle":"2021-07-15T19:06:51.680458Z","shell.execute_reply.started":"2021-07-15T19:06:51.623353Z","shell.execute_reply":"2021-07-15T19:06:51.679634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols = ['deg_C', 'relative_humidity', 'absolute_humidity',\n       'sensor_1', 'sensor_2', 'sensor_3', 'sensor_4', 'sensor_5',\n       'target_carbon_monoxide', 'target_benzene', 'target_nitrogen_oxides']","metadata":{"execution":{"iopub.status.busy":"2021-07-15T19:06:51.681862Z","iopub.execute_input":"2021-07-15T19:06:51.682273Z","iopub.status.idle":"2021-07-15T19:06:51.687804Z","shell.execute_reply.started":"2021-07-15T19:06:51.682218Z","shell.execute_reply":"2021-07-15T19:06:51.686647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.pairplot(data=train_df[cols])","metadata":{"execution":{"iopub.status.busy":"2021-07-15T19:06:52.607464Z","iopub.execute_input":"2021-07-15T19:06:52.607824Z","iopub.status.idle":"2021-07-15T19:07:18.372691Z","shell.execute_reply.started":"2021-07-15T19:06:52.607792Z","shell.execute_reply":"2021-07-15T19:07:18.371612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Correlation ","metadata":{}},{"cell_type":"code","source":"feat_cols_list = cols[:-3]\ntarget_cols_list = cols[-3:] \n\nplt.figure(figsize=(16,15),dpi=300)\nfor i, target in enumerate(target_cols_list):\n    temp_df = train_df[feat_cols_list].copy()\n    temp_df[target] = train_df[target]\n    cm = temp_df.corr()\n    mask = np.zeros_like(cm, dtype=np.bool)\n    mask[np.triu_indices_from(mask)] = True    \n    plt.subplot(2,2, i+1)\n    sns.heatmap(cm,mask=mask,square=True,cmap='coolwarm',linewidths=0.1, annot=True, cbar=False)\n    plt.title(f\"Features correlation to {target}\")\n    plt.tight_layout()\n","metadata":{"execution":{"iopub.status.busy":"2021-07-15T19:07:18.374317Z","iopub.execute_input":"2021-07-15T19:07:18.374677Z","iopub.status.idle":"2021-07-15T19:07:21.668308Z","shell.execute_reply.started":"2021-07-15T19:07:18.374635Z","shell.execute_reply":"2021-07-15T19:07:21.667088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install joypy\nimport joypy","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-07-15T19:07:21.670469Z","iopub.execute_input":"2021-07-15T19:07:21.670798Z","iopub.status.idle":"2021-07-15T19:07:31.928881Z","shell.execute_reply.started":"2021-07-15T19:07:21.670769Z","shell.execute_reply":"2021-07-15T19:07:31.927653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Distribution of target labels","metadata":{}},{"cell_type":"code","source":"color_list = ['b','g','r']\nfor i, target in enumerate(target_cols_list):\n    joypy.joyplot(train_df,\n                  column = target_cols_list[i],\n                  figsize=(6,4),\n                  legend=True,\n                  color=color_list[i],\n                 fade=0.3)\n    plt.title(f\"Distribution of {target}\", fontsize=22)","metadata":{"execution":{"iopub.status.busy":"2021-07-15T19:07:31.93096Z","iopub.execute_input":"2021-07-15T19:07:31.931424Z","iopub.status.idle":"2021-07-15T19:07:33.148601Z","shell.execute_reply.started":"2021-07-15T19:07:31.93137Z","shell.execute_reply":"2021-07-15T19:07:33.147351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,4))\nfor target in target_cols_list:\n    sns.kdeplot(x=train_df[target],shade=True)\n    plt.legend(target_cols_list)\n    #plt.xlim(-10,250)","metadata":{"execution":{"iopub.status.busy":"2021-07-15T19:07:33.150062Z","iopub.execute_input":"2021-07-15T19:07:33.150433Z","iopub.status.idle":"2021-07-15T19:07:33.498481Z","shell.execute_reply.started":"2021-07-15T19:07:33.1504Z","shell.execute_reply":"2021-07-15T19:07:33.497296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create fig and gridspec\nfig = plt.figure(figsize=(16,10),dpi=80)\ngrid = plt.GridSpec(4,4, hspace=0.5,wspace=0.2)\n\n# Define the axes\nax_main = fig.add_subplot(grid[:-1,:-1])\nax_right = fig.add_subplot(grid[:-1,-1], xticklabels=[],yticklabels=[])\nax_bottom = fig.add_subplot(grid[-1,0:-1],xticklabels=[],yticklabels=[])\n\n# Scatterplot on main ax\nax_main.scatter(x='sensor_2', y='target_nitrogen_oxides',data=train_df,alpha=.4,cmap=\"coolwarm\")\n\n# Boxplot on the right\nax_right.boxplot(x=train_df['sensor_2'])\nplt.xlabel(\"Sensor_2\")\n\n# boxplot on the bottom\nax_bottom.boxplot(x=train_df['target_nitrogen_oxides'],vert=False,)\n\n# Decorations\nax_main.set(title='Scatterplot with Boxplot \\n sensor_2 vs. target_nitrogen_oxides', ylabel='target_nitrogen_oxides');","metadata":{"execution":{"iopub.status.busy":"2021-07-15T19:07:33.499761Z","iopub.execute_input":"2021-07-15T19:07:33.500165Z","iopub.status.idle":"2021-07-15T19:07:33.960207Z","shell.execute_reply.started":"2021-07-15T19:07:33.500114Z","shell.execute_reply":"2021-07-15T19:07:33.959168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create fig and gridspec\nfig = plt.figure(figsize=(16,12),dpi=200)\ngrid = plt.GridSpec(4,4, hspace=0.5,wspace=0.2)\n\n# Define the axes\nax_main = fig.add_subplot(grid[:-1,:-1])\nax_right = fig.add_subplot(grid[:,-1])\nax_bottom = fig.add_subplot(grid[-1,0:-1])\n\n# Boxplot on main ax\nsensors = ['sensor_1','sensor_2','sensor_3','sensor_4','sensor_5']\nsns.boxplot(data=train_df[sensors],ax=ax_main, palette='coolwarm')\n\n\n# Boxplot on the right\nsns.boxplot(data=train_df['absolute_humidity'],palette='coolwarm',ax=ax_right)\nax_right.set_xlabel(\"absolute_humidity\")\n# boxplot on the bottom\nsns.boxplot(data=train_df[['deg_C','relative_humidity']],ax=ax_bottom,palette='coolwarm')\n\n# Decorations\nax_main.set(title='Sensors features boxplot');","metadata":{"execution":{"iopub.status.busy":"2021-07-15T19:07:33.96162Z","iopub.execute_input":"2021-07-15T19:07:33.961938Z","iopub.status.idle":"2021-07-15T19:07:34.798164Z","shell.execute_reply.started":"2021-07-15T19:07:33.961907Z","shell.execute_reply":"2021-07-15T19:07:34.797082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We also have datatime column, so we can check behavior of our features over time.Let's check our target columns first.In order to do this we need to convert datetime column form string into datetime object.\n","metadata":{}},{"cell_type":"code","source":"train_df['date_time'] = pd.to_datetime(train_df['date_time'])\ntest_df['date_time'] = pd.to_datetime(test_df['date_time'])","metadata":{"execution":{"iopub.status.busy":"2021-07-15T19:07:34.800597Z","iopub.execute_input":"2021-07-15T19:07:34.800932Z","iopub.status.idle":"2021-07-15T19:07:34.814393Z","shell.execute_reply.started":"2021-07-15T19:07:34.800903Z","shell.execute_reply":"2021-07-15T19:07:34.813083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"color_list = ['blue','red','green']\nplt.figure(figsize=(16,10), dpi=150)\nfor i,target in enumerate(target_cols_list):\n    plt.subplot(3,1,i+1)\n    sns.lineplot(x=train_df['date_time'], y=train_df[target_cols_list[i]], color=color_list[i])","metadata":{"execution":{"iopub.status.busy":"2021-07-15T19:07:34.817423Z","iopub.execute_input":"2021-07-15T19:07:34.817964Z","iopub.status.idle":"2021-07-15T19:07:36.919029Z","shell.execute_reply.started":"2021-07-15T19:07:34.817879Z","shell.execute_reply":"2021-07-15T19:07:36.91776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\"\"\"\nThis plot was copied from TPS July 2021 EDA created by Sharlto Cope.\n\"\"\"\nplt.rcParams['figure.dpi'] = 600\nfig = plt.figure(figsize=(10,8), facecolor='#f6f5f5')\ngs = fig.add_gridspec(8, 1)\ngs.update(wspace=0, hspace=1.5)\n\nbackground_color = \"#f6f5f5\"\n\nrun_no = 0\nfor row in range(0, 8):\n    for col in range(0, 1):\n        locals()[\"ax\"+str(run_no)] = fig.add_subplot(gs[row, col])\n        locals()[\"ax\"+str(run_no)].set_facecolor(background_color)\n        for s in [\"top\",\"right\"]:\n            locals()[\"ax\"+str(run_no)].spines[s].set_visible(False)\n        run_no += 1\n\nrun_no = 0\nfor col in feat_cols_list:\n    sns.lineplot(ax=locals()[\"ax\"+str(run_no)], y=train_df[col], x=pd.to_datetime(train_df['date_time']), color='#fcd12a')\n    sns.lineplot(ax=locals()[\"ax\"+str(run_no)], y=test_df[col], x=pd.to_datetime(test_df['date_time']), color='#287094')\n    locals()[\"ax\"+str(run_no)].set_ylabel('')\n    locals()[\"ax\"+str(run_no)].set_xlabel(col, fontsize=5, fontweight='bold')\n    locals()[\"ax\"+str(run_no)].tick_params(labelsize=5, width=0.5, length=1.5)\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.7)\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.7)\n    spring = np.arange(np.datetime64(\"2010-03-10\"), np.datetime64(\"2010-06-02\"))\n    locals()[\"ax\"+str(run_no)].fill_between(spring, np.max(train_df[col]), color='#ff69b4', alpha=0.2, zorder=2, linewidth=0)\n    summer = np.arange(np.datetime64(\"2010-06-01\"), np.datetime64(\"2010-09-02\"))\n    locals()[\"ax\"+str(run_no)].fill_between(summer, np.max(train_df[col]), color='#fcd12a', alpha=0.2, zorder=2, linewidth=0)\n    autumn = np.arange(np.datetime64(\"2010-09-01\"), np.datetime64(\"2010-12-02\"))\n    locals()[\"ax\"+str(run_no)].fill_between(autumn, np.max(train_df[col]), color='#ff9200', alpha=0.2, zorder=2, linewidth=0)\n    winter = np.arange(np.datetime64(\"2010-12-01\"), np.datetime64(\"2011-03-02\"))\n    locals()[\"ax\"+str(run_no)].fill_between(winter, np.max(train_df[col]), color='#287094', alpha=0.2, zorder=2, linewidth=0)\n    spring_2 = np.arange(np.datetime64(\"2011-03-01\"), np.datetime64(\"2011-04-05\"))\n    locals()[\"ax\"+str(run_no)].fill_between(spring_2, np.max(train_df[col]), color='#ff69b4', alpha=0.2, zorder=2, linewidth=0)\n    run_no += 1\n    \nax0.text(14660, 80, 'Time Series', fontsize=8, fontweight='bold')\nax0.text(14660, 65, 'Showing time series data starting from train dataset followed by test dataset', fontsize=5)\nfig.legend(['test', 'train'], ncol=2, facecolor=background_color, edgecolor=background_color, fontsize=4, bbox_to_anchor=(0.2, 0.895))\n\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-15T19:07:36.9209Z","iopub.execute_input":"2021-07-15T19:07:36.921303Z","iopub.status.idle":"2021-07-15T19:07:45.237592Z","shell.execute_reply.started":"2021-07-15T19:07:36.921264Z","shell.execute_reply":"2021-07-15T19:07:45.236518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Skewness and Kurtosis","metadata":{}},{"cell_type":"markdown","source":"![](https://www.researchgate.net/profile/Attila-Bonyar/publication/298415862/figure/fig1/AS:340236723867648@1458130164255/Illustration-of-the-skewness-and-kurtosis-values-and-how-they-correlate-with-the-shape-of.png)","metadata":{}},{"cell_type":"code","source":"from scipy.stats import kurtosis, skew\n\ndef skew_and_kurtosis_table(df):\n    sk_dict = {\"Skewness\": skew(df),\n               \"Kurtosis\": kurtosis(df)}\n    \n    sk_df = pd.DataFrame(sk_dict, index=df.columns).style.background_gradient(subset=[\"Skewness\", \"Kurtosis\"])\n    return sk_df\n\n# Create a table for training dataset\nskew_and_kurtosis_table(train_df.iloc[:,1:])","metadata":{"execution":{"iopub.status.busy":"2021-07-15T19:07:45.239118Z","iopub.execute_input":"2021-07-15T19:07:45.239464Z","iopub.status.idle":"2021-07-15T19:07:45.29556Z","shell.execute_reply.started":"2021-07-15T19:07:45.239432Z","shell.execute_reply":"2021-07-15T19:07:45.294497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a table for test dataset\nskew_and_kurtosis_table(test_df.iloc[:,1:])","metadata":{"execution":{"iopub.status.busy":"2021-07-15T19:07:45.297064Z","iopub.execute_input":"2021-07-15T19:07:45.297517Z","iopub.status.idle":"2021-07-15T19:07:45.316443Z","shell.execute_reply.started":"2021-07-15T19:07:45.297473Z","shell.execute_reply":"2021-07-15T19:07:45.315492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Variance Inflation Factor\n\nA variance inflation factor(VIF) detects multicollinearity in regression analysis. Multicollinearity is when there’s correlation between predictors (i.e. independent variables) in a model.The VIF estimates how much the variance of a regression coefficient is inflated due to multicollinearity in the model.VIFs are calculated by taking a predictor, and regressing it against every other predictor in the model. ","metadata":{"execution":{"iopub.status.busy":"2021-07-03T14:53:10.785012Z","iopub.execute_input":"2021-07-03T14:53:10.785389Z","iopub.status.idle":"2021-07-03T14:53:10.788731Z","shell.execute_reply.started":"2021-07-03T14:53:10.785357Z","shell.execute_reply":"2021-07-03T14:53:10.788102Z"}}},{"cell_type":"code","source":"from statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.preprocessing import StandardScaler\n\nvif = pd.DataFrame()\n\n# Normalize data first\nsc = StandardScaler()\nscaled_train_df = sc.fit_transform(train_df[feat_cols_list])\nscaled_test_df = sc.transform(test_df[feat_cols_list])\n\nvif['variables'] = feat_cols_list\nvif['vif_train'] = [variance_inflation_factor(scaled_train_df,i) for i in range(train_df[feat_cols_list].shape[1])]\nvif['vif_test'] = [variance_inflation_factor(scaled_test_df,i) for i in range(test_df[feat_cols_list].shape[1])]\nvif.style.background_gradient(cmap='magma')","metadata":{"execution":{"iopub.status.busy":"2021-07-15T19:07:45.317743Z","iopub.execute_input":"2021-07-15T19:07:45.31805Z","iopub.status.idle":"2021-07-15T19:07:45.701624Z","shell.execute_reply.started":"2021-07-15T19:07:45.318021Z","shell.execute_reply":"2021-07-15T19:07:45.700449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In general, a VIF above 10 indicates high correlation and is cause for concern. Some authors suggest a more conservative level of 2.5 or above. Sometimes a high VIF is no cause for concern at all. In this case our vif's for every features is in most cases lower than 10. There is one feature 'sensor_2' in training set which have 12 and two features ('absolute_humidity', 'sensor_4') in test dataset which have 45 and 46 respectively. Feature 'deg_C' and 'sensor_2' in test dataset are also high.One way of dealing with multicollinearity is using PCA to reduce number of features or simple delete one of the features.","metadata":{}},{"cell_type":"code","source":"!pip install regressors","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-07-15T19:07:45.703246Z","iopub.execute_input":"2021-07-15T19:07:45.70392Z","iopub.status.idle":"2021-07-15T19:07:56.515168Z","shell.execute_reply.started":"2021-07-15T19:07:45.703871Z","shell.execute_reply":"2021-07-15T19:07:56.513849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### P-value and t-value","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom regressors import stats\n\ndef stats_summary(df):\n    ols_dict = {}\n    X = train_df[feat_cols_list]\n    for i, target in enumerate(target_cols_list):\n        ols = LinearRegression()\n        ols.fit(X,df[target_cols_list[i]])\n        ols_dict[target] = ols\n        # to print summary table:\n        print(f\"\\n========== SUMMARY STATISTICS of TRAIN DATASET to {target_cols_list[i].upper()} ==============\")\n        stats.summary(ols, X, train_df[target_cols_list[i]], feat_cols_list)\n    return ols_dict\n        \nols_models_dict = stats_summary(train_df)","metadata":{"execution":{"iopub.status.busy":"2021-07-15T19:07:56.517183Z","iopub.execute_input":"2021-07-15T19:07:56.517562Z","iopub.status.idle":"2021-07-15T19:07:57.035718Z","shell.execute_reply.started":"2021-07-15T19:07:56.517525Z","shell.execute_reply":"2021-07-15T19:07:57.034654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"======= {target_cols_list[2]}=========\")\nols = LinearRegression()\nols.fit(train_df[feat_cols_list], train_df[target_cols_list[2]])\npd.DataFrame(stats.coef_pval(ols, train_df[feat_cols_list], train_df[target_cols_list[2]])[:-1],\n                 index=feat_cols_list,columns=['P-value']).style.background_gradient(cmap='magma_r')","metadata":{"execution":{"iopub.status.busy":"2021-07-15T19:07:57.037433Z","iopub.execute_input":"2021-07-15T19:07:57.038177Z","iopub.status.idle":"2021-07-15T19:07:57.081756Z","shell.execute_reply.started":"2021-07-15T19:07:57.038127Z","shell.execute_reply":"2021-07-15T19:07:57.080667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"🔑**Note:** The p-value for each independent variable tests the null hypothesis that the variable has no correlation with the dependent variable.A small p-value is an indication that the null hypothesis is false. It is good practice to decide in advance of the test how small a p-value is required to reject the test. P-value and t-value are inextricably linked. T-value measures the size of the difference relative to the variation in your sample data. The greater the magnitude of T, the greater the evidence against the null hypothesis. This means there is greater evidence that there is a significant difference. The closer T is to 0, the more likely there isn't a significant difference.\n\n\nThe regression output for target_nitrogen_oxides shows that variables are statistically significant because their p-values equal 0.000. On the other hand 'sensor_1' is not statistically significant because its p-value (0.99) is greater than the usual significance level of 0.05 and we might to consider to drop this column.","metadata":{}},{"cell_type":"markdown","source":"### Residuals","metadata":{}},{"cell_type":"code","source":"def plot_residuals(models_dict, df, feats, targets):\n    plt.figure(figsize=(18,5))\n    for i in range(len(target_cols_list)):\n        # Calculate predicions\n        model = models_dict[targets[i]]\n        y_pred = model.predict(df[feats])\n        residuals =  pd.Series(df[targets[i]] - y_pred, name=f\"residuals_1\")\n        # Plot scatterplot\n        plt.subplot(1, len(target_cols_list), i+1)\n        sns.scatterplot(x=df[targets[i]], y=residuals)\n        plt.axhline(y=0,color='red',linestyle='--')","metadata":{"execution":{"iopub.status.busy":"2021-07-15T19:07:57.083574Z","iopub.execute_input":"2021-07-15T19:07:57.084359Z","iopub.status.idle":"2021-07-15T19:07:57.099837Z","shell.execute_reply.started":"2021-07-15T19:07:57.084306Z","shell.execute_reply":"2021-07-15T19:07:57.098637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_residuals(models_dict= ols_models_dict, \n               df= train_df,\n               feats= feat_cols_list,\n               targets= target_cols_list)","metadata":{"execution":{"iopub.status.busy":"2021-07-15T19:07:57.102154Z","iopub.execute_input":"2021-07-15T19:07:57.104347Z","iopub.status.idle":"2021-07-15T19:08:00.205499Z","shell.execute_reply.started":"2021-07-15T19:07:57.10429Z","shell.execute_reply":"2021-07-15T19:08:00.204216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Regression residuals are actually estimates of the true error( y_true - y_prediction), just like the regression coefficients are estimates of the true population coefficients.Using residual plots, you can assess whether the observed error (residuals) is consistent with stochastic error (a fancy word for random):\n 1. The residuals should fall in a symmetrical pattern and have a constant spread throughout the range. \n 2. The non-random pattern in the residuals indicates that the deterministic portion (predictor variables) of the model is not capturing some explanatory information that is “leaking” into the residuals. Possibilities include:\n     - A missing variable\n     - A missing higher-order term of a variable in the model to explain the curvature\n     - A missing interaction between terms already in the model\n     \nIdentifying and fixing the problem so that the predictors now explain the information that they missed before should produce a good-looking set of residuals!\n\nIn addition to the above, here are two more specific ways that predictive information can sneak into the residuals:\n- The residuals should not be correlated with another variable. \n- Adjacent residuals should not be correlated with each other (autocorrelation). If you can use one residual to predict the next residual, there is some predictive information present that is not captured by the predictors. Typically, this situation involves time-ordered observations. For example, if a residual is more likely to be followed by another residual that has the same sign, adjacent residuals are positively correlated. You can include a variable that captures the relevant time-related information, or use a time series analysis. In regression, you can perform the Durbin-Watson test to test for autocorrelation.","metadata":{}},{"cell_type":"code","source":"def plot_residuals_dist(models_dict, df, feats, targets):\n    plt.figure(figsize=(18,4))\n    for i in range(len(target_cols_list)):\n        model = models_dict[targets[i]]\n        y_pred = model.predict(df[feats])\n        residuals =  pd.Series(df[targets[i]] - y_pred, name=\"residuals\")\n        plt.subplot(1, len(target_cols_list), i+1)\n        sns.histplot(x=residuals,bins=40, color='red')\n        plt.title(f\"{targets[i]}\")\n        \nplot_residuals_dist(models_dict=ols_models_dict,\n                    df=train_df,\n                    feats=feat_cols_list, \n                    targets=target_cols_list)","metadata":{"execution":{"iopub.status.busy":"2021-07-15T19:08:00.207119Z","iopub.execute_input":"2021-07-15T19:08:00.207535Z","iopub.status.idle":"2021-07-15T19:08:02.595941Z","shell.execute_reply.started":"2021-07-15T19:08:00.207492Z","shell.execute_reply":"2021-07-15T19:08:02.594854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def probability_plot(models_dict,df,feats,targets):\n    plt.figure(figsize=(20,4))\n    for i,target in enumerate(targets):\n        model = models_dict[targets[i]]\n        y_pred = model.predict(df[feats])\n        residuals =  pd.Series(df[targets[i]] - y_pred, name=\"residuals_1\")\n        ax = plt.subplot(1,3,i+1)\n        _ = sp.stats.probplot(residuals, plot=ax);\n        plt.title(f\"{targets[i]} probability plot\")\n        \nprobability_plot(models_dict=ols_models_dict,\n                 df = train_df,\n                 feats=feat_cols_list,\n                 targets=target_cols_list)","metadata":{"execution":{"iopub.status.busy":"2021-07-15T19:08:02.597351Z","iopub.execute_input":"2021-07-15T19:08:02.597665Z","iopub.status.idle":"2021-07-15T19:08:05.418734Z","shell.execute_reply.started":"2021-07-15T19:08:02.597634Z","shell.execute_reply":"2021-07-15T19:08:05.417385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The probability plot is a graphical technique for assessing whether or not a data set follows a given distribution such as the normal. The data are plotted against a theoretical distribution in such a way that the points should form approximately a straight line. Departures from this straight line indicate departures from the specified distribution.\nIt indicates that your distribution has:\n- Right Skew - If the plotted points appear to bend up and to the left of the normal line that indicates a long tail to the right. \n- Left Skew - If the plotted points bend down and to the right of the normal line that indicates a long tail to the left.","metadata":{}},{"cell_type":"markdown","source":"### Check stationarity of a Time Series\n\nI fill like there is one more thing I need go add to complete my EDA. Most of the Time Series Models(let's call it TS) work on assumption that TS is stationary. TS said to be stationary if its statistical properties such mean, variance remain constant over time.Intuitively, we can said that if a TS has a particular behaviour over time, there is a very high probability that it will follow the same in the future. Also, the theories related to stationary series are more mature and easier to implement as compared to non-stationary series.\n\nStationarity is defined using very strict criterion. However, for practical purposes we can assume the series to be stationary if it has constant statistical properties over time, ie. the following:\n\n- constant mean\n- constant variance\n- an autocovariance that does not depend on time.\n\nMore formally, we can check stationarity using the following:\n\n1. Plotting Rolling Statistics: We can plot the moving average or moving variance and see if it varies with time, this is more of a visual technique.\n2. Dickey-Fuller Test: This is one of the statistical tests for checking stationarity. Here the null hypothesis is that the TS is non-stationary. The test results comprise of a Test Statistic and some Critical Values for difference confidence levels. If the ‘Test Statistic’ is less than the ‘Critical Value’, we can reject the null hypothesis and say that the series is stationary.\n\nFor more information and the ways to make series stationary visit: https://www.analyticsvidhya.com/blog/2016/02/time-series-forecasting-codes-python/\n\nIn principle we do not need to check for stationarity nor correct for it when we are using an LSTM.The LSTM method is preferable over other existing algorithms as LSTM network is able to learn non-linear and non-stationary nature of a time series which reduces error in forecasting. However, if the data is stationary, it will help with better performance and make it easier for the neural network to learn. So to finished my EDA I will performe this test.","metadata":{}},{"cell_type":"code","source":"# Feature engineering\ntrain_df['year'] = train_df['date_time'].dt.year\ntrain_df['month'] = train_df['date_time'].dt.month\ntrain_df['hour'] = train_df['date_time'].dt.hour\ntrain_df['day'] = train_df['date_time'].dt.day\n\ntest_df['year'] = test_df['date_time'].dt.year\ntest_df['month'] = test_df['date_time'].dt.month\ntest_df['hour'] = test_df['date_time'].dt.hour\ntest_df['day'] = test_df['date_time'].dt.day","metadata":{"execution":{"iopub.status.busy":"2021-07-15T19:08:05.420481Z","iopub.execute_input":"2021-07-15T19:08:05.420956Z","iopub.status.idle":"2021-07-15T19:08:05.44233Z","shell.execute_reply.started":"2021-07-15T19:08:05.420911Z","shell.execute_reply":"2021-07-15T19:08:05.4414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from statsmodels.tsa.stattools import adfuller\n\n#Perform Dickey-Fuller test:\nprint('Results of Dickey-Fuller Test:')\ndftest = adfuller(train_df['sensor_2'], autolag='AIC')\ndfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\nfor key,value in dftest[4].items():\n    dfoutput['Critical Value (%s)'%key] = value\ndfoutput","metadata":{"execution":{"iopub.status.busy":"2021-07-15T19:08:05.443495Z","iopub.execute_input":"2021-07-15T19:08:05.443778Z","iopub.status.idle":"2021-07-15T19:08:05.968445Z","shell.execute_reply.started":"2021-07-15T19:08:05.443751Z","shell.execute_reply":"2021-07-15T19:08:05.967289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"How  we can interpret these results?\n\nThe **null hypothesis (H0)** of the test is that the time series can be represented by a unit root, that it **is not stationary** (has some time-dependent structure). The **alternate hypothesis (H1)** (rejecting the null hypothesis) is that the time series **is stationary**.\n\nWe interpret this result using the p-value from the test. A p-value below a threshold (such as 5% or 1%) suggests we reject the null hypothesis (stationary), otherwise a p-value above the threshold suggests we fail to reject the null hypothesis (non-stationary).\n\nThe more negative this statistic, the more likely we are to reject the null hypothesis (we have a stationary dataset).We can see that our statistic value of -9 is less than the value of -3.431 at 1% .This suggests that we can reject the null hypothesis with a significance level of less than 1% (i.e. a low probability that the result is a statistical fluke).Rejecting the null hypothesis means that the process has no unit root, and in turn that the time series is stationary or does not have time-dependent structure.","metadata":{}},{"cell_type":"markdown","source":"### Rolling Statistics for Training and Test Dataset","metadata":{}},{"cell_type":"code","source":"def rolling_statistics(df, f_names):\n    \"\"\"\n    This function plots roilling statistics for a given feature.\n    \"\"\"\n    \n    if f_names == target_cols_list:\n        size = (25,15)\n    else:\n        size = (25,25)\n        \n    for i,col in enumerate(f_names):\n        roll_mean = df[col].rolling(window=24).mean()\n        roll_std = df[col].rolling(window=24).std()\n        \n        plt.figure(figsize=size)\n        plt.subplot(len(f_names),1, i+1)\n        sns.lineplot(x='date_time', y=col, data=df, label=col)\n        sns.lineplot(x='date_time', y=roll_mean, data=df, label='roll_mean')\n        sns.lineplot(x='date_time', y=roll_std, data=df, label='roll_std')\n        plt.title(f\"Rolling statistics for '{col}' feature\")\n        plt.legend(loc='center right', bbox_to_anchor=(1.10,0.5));","metadata":{"execution":{"iopub.status.busy":"2021-07-15T19:08:05.973293Z","iopub.execute_input":"2021-07-15T19:08:05.97448Z","iopub.status.idle":"2021-07-15T19:08:05.988901Z","shell.execute_reply.started":"2021-07-15T19:08:05.97441Z","shell.execute_reply":"2021-07-15T19:08:05.987332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Training datasets rolling statistics","metadata":{}},{"cell_type":"code","source":"rolling_statistics(df=train_df, \n                   f_names=feat_cols_list)","metadata":{"execution":{"iopub.status.busy":"2021-07-15T19:08:36.528308Z","iopub.execute_input":"2021-07-15T19:08:36.528704Z","iopub.status.idle":"2021-07-15T19:09:09.534718Z","shell.execute_reply.started":"2021-07-15T19:08:36.528669Z","shell.execute_reply":"2021-07-15T19:09:09.533657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Test dataset rollinig statistics","metadata":{}},{"cell_type":"code","source":"rolling_statistics(df=test_df,\n                   f_names=feat_cols_list)","metadata":{"execution":{"iopub.status.busy":"2021-07-15T19:09:39.765128Z","iopub.execute_input":"2021-07-15T19:09:39.765492Z","iopub.status.idle":"2021-07-15T19:10:04.516157Z","shell.execute_reply.started":"2021-07-15T19:09:39.765463Z","shell.execute_reply":"2021-07-15T19:10:04.515248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Rolling statistics for targets columns","metadata":{}},{"cell_type":"code","source":"rolling_statistics(df=train_df,\n                   f_names=target_cols_list)","metadata":{"execution":{"iopub.status.busy":"2021-07-15T19:10:58.670423Z","iopub.execute_input":"2021-07-15T19:10:58.671039Z","iopub.status.idle":"2021-07-15T19:11:13.111188Z","shell.execute_reply.started":"2021-07-15T19:10:58.671001Z","shell.execute_reply":"2021-07-15T19:11:13.109561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modeling Neural Network with LTSM","metadata":{}},{"cell_type":"markdown","source":" Because of the technical problems with runnning this notebook (I think this is RAM issue) I can not finish everything what I intended to do in one notebook. I had to start a new notebook where I will complete modelling part. This is the link to part II of this notebook : https://www.kaggle.com/godzill22/tps-07-simply-rnn-and-lstm","metadata":{}}]}