{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install -U lightautoml","metadata":{"execution":{"iopub.status.busy":"2021-07-24T04:55:34.794719Z","iopub.execute_input":"2021-07-24T04:55:34.795328Z","iopub.status.idle":"2021-07-24T04:55:54.939564Z","shell.execute_reply.started":"2021-07-24T04:55:34.795178Z","shell.execute_reply":"2021-07-24T04:55:54.938494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.lines import Line2D\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\nimport time\nimport random\nimport torch\n\nfrom lightautoml.automl.presets.tabular_presets import TabularAutoML, TabularUtilizedAutoML\nfrom lightautoml.tasks import Task\nfrom lightautoml.dataset.roles import CategoryRole, DatetimeRole\n\n# Pandas setting to display more dataset rows and columns\npd.set_option('display.max_rows', 100)\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_colwidth', None)\npd.set_option('display.float_format', lambda x: '%.5f' % x)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-24T04:55:54.941194Z","iopub.execute_input":"2021-07-24T04:55:54.941464Z","iopub.status.idle":"2021-07-24T04:56:01.111473Z","shell.execute_reply.started":"2021-07-24T04:55:54.941436Z","shell.execute_reply":"2021-07-24T04:56:01.110536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Data import**","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/tabular-playground-series-jul-2021/train.csv\", low_memory=False)#, nrows=10000)\ntrain[\"date_time\"] = pd.to_datetime(train[\"date_time\"], format=\"%Y-%m-%d %H:%M:%S\")\ntest = pd.read_csv(\"/kaggle/input/tabular-playground-series-jul-2021/test.csv\", low_memory=False)\ntest[\"date_time\"] = pd.to_datetime(test[\"date_time\"], format=\"%Y-%m-%d %H:%M:%S\")\ntrain.info(memory_usage=\"deep\")","metadata":{"execution":{"iopub.status.busy":"2021-07-24T04:56:01.112981Z","iopub.execute_input":"2021-07-24T04:56:01.113316Z","iopub.status.idle":"2021-07-24T04:56:01.191937Z","shell.execute_reply.started":"2021-07-24T04:56:01.113286Z","shell.execute_reply":"2021-07-24T04:56:01.190911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.info(memory_usage=\"deep\")","metadata":{"execution":{"iopub.status.busy":"2021-07-24T04:56:01.193473Z","iopub.execute_input":"2021-07-24T04:56:01.193755Z","iopub.status.idle":"2021-07-24T04:56:01.205572Z","shell.execute_reply.started":"2021-07-24T04:56:01.193729Z","shell.execute_reply":"2021-07-24T04:56:01.204226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T04:56:01.206928Z","iopub.execute_input":"2021-07-24T04:56:01.20738Z","iopub.status.idle":"2021-07-24T04:56:01.233586Z","shell.execute_reply.started":"2021-07-24T04:56:01.207329Z","shell.execute_reply":"2021-07-24T04:56:01.232766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **EDA**","metadata":{}},{"cell_type":"code","source":"targets = [\"target_carbon_monoxide\", \"target_benzene\", \"target_nitrogen_oxides\"]\ntarget_names = [\"Carbon monoxide\", \"Benzene\", \"Nitrogen oxides\"]","metadata":{"execution":{"iopub.status.busy":"2021-07-24T04:56:01.234627Z","iopub.execute_input":"2021-07-24T04:56:01.234877Z","iopub.status.idle":"2021-07-24T04:56:01.238393Z","shell.execute_reply.started":"2021-07-24T04:56:01.234852Z","shell.execute_reply":"2021-07-24T04:56:01.237485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The idea of SMC feature below was taken from this [notebook](https://www.kaggle.com/junhyeok99/automl-pycaret).\n\nTaking into account temperature changes was suggested by [@lukaszborecki](https://www.kaggle.com/lukaszborecki) [here](https://www.kaggle.com/c/tabular-playground-series-jul-2021/discussion/250931#1380107).","metadata":{}},{"cell_type":"code","source":"def add_new_plot_features(df):\n    \"\"\"\n    Adds new features to a given dataset for plotting\n    \"\"\"\n    df[\"month\"] = df[\"date_time\"].dt.month\n    df[\"day_of_week\"] = df[\"date_time\"].dt.dayofweek\n    df[\"day_of_year\"] = df[\"date_time\"].dt.dayofyear\n    df[\"hour\"] = df[\"date_time\"].dt.hour\n    df[\"quarter\"] = df[\"date_time\"].dt.quarter\n    df[\"week_of_year\"] = df[\"date_time\"].dt.isocalendar().week.astype(\"int\")\n#     df[\"is_winter\"] = df[\"month\"].isin([1, 2, 12])\n#     df[\"is_sprint\"] = df[\"month\"].isin([3, 4, 5])\n#     df[\"is_summer\"] = df[\"month\"].isin([6, 7, 8])\n#     df[\"is_autumn\"] = df[\"month\"].isin([9, 10, 11])\n    df[\"working_hours\"] =  df[\"hour\"].isin(np.arange(8, 21, 1)).astype(\"int\")\n    df[\"is_weekend\"] = (df[\"date_time\"].dt.dayofweek >= 5).astype(\"int\")\n    return df\n\ndef add_new_ml_features(df, i=3): # i=3 is for heatmap plot\n    \"\"\"\n    Adds new features to a given dataset for training\n    \"\"\"\n    # Features to be added to every target dataset\n    df[\"hour\"] = df[\"date_time\"].dt.hour\n    df[\"working_hours\"] =  df[\"hour\"].isin(np.arange(8, 21, 1)).astype(\"int\")\n    df[\"maximum_hours\"] =  df[\"hour\"].isin([8, 9, 17, 18, 19, 20]).astype(\"int\")\n    # Marking weekends because they usually have lower target values\n    df[\"is_weekend\"] = (df[\"date_time\"].dt.dayofweek >= 5).astype(\"int\")\n    df[\"SMC\"] = (df[\"absolute_humidity\"] * 100) / df[\"relative_humidity\"]\n    \n    # A list of features to generate shifted and lagged values\n    shift_features = [[\"SMC\", \"absolute_humidity\", \"deg_C\",\n                      \"sensor_1\", \"sensor_2\", \"sensor_3\", \"sensor_4\", \"sensor_5\"],\n                      [\"SMC\", \"absolute_humidity\", \"target_carbon_monoxide_preds\",\n                      \"sensor_1\", \"sensor_2\", \"sensor_3\", \"sensor_4\", \"sensor_5\"],\n                      [\"SMC\", \"absolute_humidity\", \"target_carbon_monoxide_preds\", \"target_benzene_preds\",\n                      \"sensor_1\", \"sensor_2\", \"sensor_3\", \"sensor_5\"],\n                      # Features for heatmap plot\n                      [\"SMC\", \"absolute_humidity\", \"deg_C\",\n                      \"sensor_1\", \"sensor_2\", \"sensor_3\", \"sensor_4\", \"sensor_5\"]]\n    \n    # Amounts of hour shifts and lags\n    shifts = [1, 2, 3, 4, 5, 6, 12, 24]\n#     shifts = [1, 2, 3, 6, 12, 24]\n    \n    for feature in shift_features[i]:\n        for shift in shifts:\n            df[feature+\"-\"+str(shift)+\"abs_shift\"] = df[feature] - df[feature].shift(periods=shift, fill_value=0)\n            df[feature+\"+\"+str(shift)+\"abs_shift\"] = df[feature] - df[feature].shift(periods=-shift, fill_value=0)\n#             df[feature+\"-\"+str(shift)+\"prc_shift\"] = (df[feature] / df[feature].shift(periods=shift, fill_value=0)) - 1\n#             df[feature+\"+\"+str(shift)+\"prc_shift\"] = (df[feature] / df[feature].shift(periods=-shift, fill_value=0)) - 1\n    \n#     # Dropping the least important features as per previous runs\n#     to_drop = [ [\"sensor_2+2abs_shift\", \"sensor_1-2abs_shift\", \"deg_C-4abs_shift\", \"sensor_2-3abs_shift\", \"sensor_1+3abs_shift\",\n#                  \"deg_C-5abs_shift\", \"sensor_1+2abs_shift\", \"sensor_1-4abs_shift\", \"sensor_2-4abs_shift\", \"sensor_1-3abs_shift\"],\n#                 [\"sensor_5-12abs_shift\", \"sensor_3-5abs_shift\", \"sensor_5-3abs_shift\", \"sensor_5-4abs_shift\", \"sensor_5-5abs_shift\",\n#                  \"sensor_4-5abs_shift\", \"absolute_humidity-3abs_shift\", \"sensor_5+6abs_shift\", \"sensor_5+3abs_shift\", \"sensor_1-5abs_shift\"],\n#                 [\"sensor_3+2abs_shift\", \"sensor_1+4abs_shift\", \"sensor_1+3abs_shift\", \"sensor_2+3abs_shift\", \"maximum_hours\",\n#                  \"SMC+5abs_shift\", \"sensor_3+1abs_shift\", \"sensor_3+3abs_shift\", \"sensor_5+5abs_shift\", \"sensor_1+2abs_shift\"]\n#                 ]\n#     if i <= 2:\n#         df.drop(to_drop[i], axis=1, inplace=True)\n#     # Replacing infinity values as a result of devision by zero at the end of a dataset\n#     df.replace(to_replace=np.inf, value=0, inplace=True)\n    \n#     return df.drop([\"hour\", \"week_of_year\"], axis=1)\n    return df.drop([\"hour\"], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T04:56:01.239459Z","iopub.execute_input":"2021-07-24T04:56:01.239885Z","iopub.status.idle":"2021-07-24T04:56:01.255898Z","shell.execute_reply.started":"2021-07-24T04:56:01.239852Z","shell.execute_reply":"2021-07-24T04:56:01.254946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_copy = train.copy()\ntest_copy = test.copy()\ntrain = add_new_plot_features(train)\ntest = add_new_plot_features(test)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T04:56:01.258253Z","iopub.execute_input":"2021-07-24T04:56:01.258785Z","iopub.status.idle":"2021-07-24T04:56:01.310456Z","shell.execute_reply.started":"2021-07-24T04:56:01.258755Z","shell.execute_reply":"2021-07-24T04:56:01.309444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The datasets have timestamps. Let's compare which dates are in each dataset.","metadata":{}},{"cell_type":"code","source":"# Plot dataframe\ndf = pd.concat([train[\"date_time\"], test[\"date_time\"]], axis=0).reset_index(drop=True)\n\nfig, ax = plt.subplots(figsize=(16, 1.5))\nbar1 =  ax.barh(0, 7111+2247, color=\"salmon\", height=0.2)\nbar2 =  ax.barh(0, 7111, color=\"teal\", height=0.2)\nax.set_title(\"Train and test datasets size comparison\", fontsize=20, pad=5)\nax.bar_label(bar1, [\"Test dataset\"], label_type=\"edge\", padding=-170,\n             fontsize=20, color=\"white\", weight=\"bold\")\nax.bar_label(bar2, [\"Train dataset\"], label_type=\"center\",\n             fontsize=20, color=\"white\", weight=\"bold\")\nax.set_xticks([0, 7111, 7111+2247])\nax.set_xticklabels([\"2010-03-10\", \"2011-01-01\", \"2011-04-04\"])\nax.set_yticks([])\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-07-24T04:56:01.312123Z","iopub.execute_input":"2021-07-24T04:56:01.312419Z","iopub.status.idle":"2021-07-24T04:56:01.451058Z","shell.execute_reply.started":"2021-07-24T04:56:01.312393Z","shell.execute_reply":"2021-07-24T04:56:01.448503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The datasets also have three target columns that the model have to predict. Let's see how each target is changing in time.","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(figsize=(16, 18), ncols=1, nrows=3, sharex=False)\n\nplt.subplots_adjust(hspace = 0.3)\n\ncolors = [\"palevioletred\", \"deepskyblue\", \"mediumseagreen\"]\n\nfor i in [0, 1, 2]:\n    axs[i].plot(train[\"date_time\"], train[targets[i]], color=colors[i])\n    axs[i].set_title(f\"{target_names[i]} (target #{i+1}) levels across time\", fontsize=20, pad=5)\n    axs[i].set_ylabel(f\"{target_names[i]} level\", fontsize=14, labelpad=5)\n    axs[i].set_xlabel(\"Date\", fontsize=14, labelpad=5)\n    axs[i].grid(axis=\"both\")\n\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-07-24T04:56:01.452454Z","iopub.execute_input":"2021-07-24T04:56:01.452771Z","iopub.status.idle":"2021-07-24T04:56:02.385056Z","shell.execute_reply.started":"2021-07-24T04:56:01.452738Z","shell.execute_reply":"2021-07-24T04:56:02.383994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's see mean target values per day of year.","metadata":{}},{"cell_type":"code","source":"# Dataframe copy excluding the last row which is the only one representing January\ndf = train.drop([7110], axis=0).copy()\ndf[\"day\"] = df[\"date_time\"].dt.dayofyear\ndf[\"weekday\"] = df[\"date_time\"].dt.dayofweek\n\ncolors = [\"palevioletred\", \"deepskyblue\", \"mediumseagreen\"]\n\n# An array of number of days of year (i.e. from 1 to 365) which are mondays to mark week starts\nmondays = df.loc[df[\"weekday\"] == 0][\"day\"].value_counts(sort=False).index\n# An array of number of weeks of year to be used as label ticks\nweeks = df[\"date_time\"].dt.isocalendar().week.unique()[1:]\n\nfig, axs = plt.subplots(figsize=(16, 18), ncols=1, nrows=3, sharex=False)\n\nplt.subplots_adjust(hspace = 0.3)\n\nfor i in [0, 1, 2]:\n    axs[i].plot(df.groupby(\"day\")[targets[i]].mean().index,\n                df.groupby(\"day\")[targets[i]].mean().values, color=colors[i])\n    axs[i].set_title(f\"{target_names[i]} (target #{i+1}) mean levels across time\", fontsize=20, pad=5)\n    axs[i].set_ylabel(f\"{target_names[i]} level\", fontsize=14, labelpad=5)\n    axs[i].set_xlabel(\"Week starts\", fontsize=14, labelpad=5)\n    axs[i].set_xticks(mondays)\n    axs[i].set_xticklabels(weeks)\n    axs[i].grid(axis=\"both\")\n\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-07-24T04:56:02.386411Z","iopub.execute_input":"2021-07-24T04:56:02.386708Z","iopub.status.idle":"2021-07-24T04:56:03.576405Z","shell.execute_reply.started":"2021-07-24T04:56:02.386678Z","shell.execute_reply":"2021-07-24T04:56:03.575415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see, all target values usually go down at the end of each week (i.e. during weekends). \n\nLet's check targets distribution along each month.","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(ncols=2, nrows=5, figsize=(16, 20))\nplt.subplots_adjust(hspace = 0.3)\nfig.suptitle(target_names[0], fontsize=20)\n\ni=3\nfor r in np.arange(5):\n    for c in [0, 1]:\n        axs[r, c].plot(train.loc[train[\"month\"]==i, targets[0]], color=\"steelblue\")\n        axs[r, c].set_title(f\"Month #{i}\", fontsize=15)\n        axs[r, c].legend(fontsize=13)\n        i+=1","metadata":{"execution":{"iopub.status.busy":"2021-07-24T04:56:03.577713Z","iopub.execute_input":"2021-07-24T04:56:03.578014Z","iopub.status.idle":"2021-07-24T04:56:05.14502Z","shell.execute_reply.started":"2021-07-24T04:56:03.577984Z","shell.execute_reply":"2021-07-24T04:56:05.14395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axs = plt.subplots(ncols=2, nrows=5, figsize=(16, 20))\nplt.subplots_adjust(hspace = 0.3)\nfig.suptitle(target_names[1], fontsize=20)\n\ni=3\nfor r in np.arange(5):\n    for c in [0, 1]:\n        axs[r, c].plot(train.loc[train[\"month\"]==i, targets[1]], color=\"palevioletred\")\n        axs[r, c].set_title(f\"Month #{i}\", fontsize=15)\n        axs[r, c].legend(fontsize=13)\n        i+=1","metadata":{"execution":{"iopub.status.busy":"2021-07-24T04:56:05.146539Z","iopub.execute_input":"2021-07-24T04:56:05.146901Z","iopub.status.idle":"2021-07-24T04:56:06.671756Z","shell.execute_reply.started":"2021-07-24T04:56:05.146864Z","shell.execute_reply":"2021-07-24T04:56:06.670764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are some near zer flat areas at 4th, 6th, 8th, 12th month plots. Need to figure out what is so special about these days. It also may be a garbage data which sould be deleted before machine learning.","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(ncols=2, nrows=5, figsize=(16, 20))\nplt.set_cmap(\"Set2\")\nplt.subplots_adjust(hspace = 0.3)\nfig.suptitle(target_names[2], fontsize=20)\n\ni=3\nfor r in np.arange(5):\n    for c in [0, 1]:\n        axs[r, c].plot(train.loc[train[\"month\"]==i, targets[2]], color=\"goldenrod\")\n        axs[r, c].set_title(f\"Month #{i}\", fontsize=15)\n        axs[r, c].legend(fontsize=13)\n        i+=1","metadata":{"execution":{"iopub.status.busy":"2021-07-24T04:56:06.673439Z","iopub.execute_input":"2021-07-24T04:56:06.673838Z","iopub.status.idle":"2021-07-24T04:56:08.434161Z","shell.execute_reply.started":"2021-07-24T04:56:06.673797Z","shell.execute_reply":"2021-07-24T04:56:08.433233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check each target value distribution.","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(figsize=(15, 6), ncols=3, nrows=1, sharey=False)\n\nfig.suptitle(\"Target values distribution\", fontsize=20)\n\ncolors = [\"mediumorchid\", \"lightseagreen\", \"cornflowerblue\"]\n\nfor i in [0, 1, 2]:\n    axs[i].hist(train[targets[i]], bins=60, edgecolor=\"black\", color=colors[i])\n    axs[i].set_title(f\"{target_names[i]} (target #{i+1})\", fontsize=15, pad=5)\n    axs[i].set_ylabel(\"Amount of values\", fontsize=13, labelpad=5)\n    axs[i].set_xlabel(f\"{target_names[i]} level\", fontsize=13, labelpad=5)\n    axs[i].grid(axis=\"y\")\n\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-07-24T04:56:08.43567Z","iopub.execute_input":"2021-07-24T04:56:08.436117Z","iopub.status.idle":"2021-07-24T04:56:09.195743Z","shell.execute_reply.started":"2021-07-24T04:56:08.436075Z","shell.execute_reply":"2021-07-24T04:56:09.195019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check how each target value chenges depending on the time of day, day of week, and month.","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(figsize=(16, 18), ncols=1, nrows=3, sharex=False)\n\nplt.subplots_adjust(hspace = 0.3)\nwidth=0.35\nx = train.groupby(\"hour\")[\"target_carbon_monoxide\"].mean().index\n\nfor i in np.arange(3):\n    bars1 = axs[i].bar(x-width/2, train.groupby(\"hour\")[targets[i]].mean(),\n                        width=width, edgecolor=\"black\", label=\"Mean\", color=\"cornflowerblue\")\n    bars2 = axs[i].bar(x+width/2, train.groupby(\"hour\")[targets[i]].median(),\n                        width=width, edgecolor=\"black\", label=\"Median\", color=\"palevioletred\")\n    axs[i].set_title(f\"{target_names[i]} (target #{i+1})\", fontsize=15, pad=10)\n    axs[i].set_ylabel(\"Target value\", fontsize=13, labelpad=5)\n    axs[i].set_xlabel(\"Day hours\", fontsize=13, labelpad=5)\n    axs[i].set_xticks(x)\n    axs[i].grid(axis=\"y\")\n    axs[i].legend(fontsize=13)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T04:56:09.196735Z","iopub.execute_input":"2021-07-24T04:56:09.19699Z","iopub.status.idle":"2021-07-24T04:56:10.341506Z","shell.execute_reply.started":"2021-07-24T04:56:09.196964Z","shell.execute_reply":"2021-07-24T04:56:10.340498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dataframe copy excluding the last row which is the only one representing January\ndf = train.drop([7110], axis=0).copy()\n\nfig, axs = plt.subplots(figsize=(16, 19), ncols=2, nrows=3, sharex=False,\n                        gridspec_kw={'width_ratios': [1, 1.5]})\n\nfig.suptitle(\"Target values distribution per month and day of week\", fontsize=20)\n\nplt.subplots_adjust(hspace = 0.25)\nwidth=0.35\nx = df.groupby(\"day_of_week\")[\"target_carbon_monoxide\"].mean().index + 1\n\nfor i in np.arange(3):\n    bars1 = axs[i, 0].bar(x-width/2, df.groupby(\"day_of_week\")[targets[i]].mean(),\n                        width=width, edgecolor=\"black\", label=\"Mean\", color=\"salmon\")\n    bars2 = axs[i, 0].bar(x+width/2, df.groupby(\"day_of_week\")[targets[i]].median(),\n                        width=width, edgecolor=\"black\", label=\"Median\", color=\"teal\")\n    axs[i, 0].set_title(f\"{target_names[i]} (target #{i+1})\", fontsize=15, pad=10)\n    axs[i, 0].set_ylabel(\"Target value\", fontsize=13, labelpad=5)\n    axs[i, 0].set_xlabel(\"Day of week\", fontsize=13, labelpad=5)\n    axs[i, 0].set_xticks(x)\n    axs[i, 0].grid(axis=\"y\")\n    axs[i, 0].legend(fontsize=13)\n\nx = df.groupby(\"month\")[\"target_carbon_monoxide\"].mean().index\nfor i in np.arange(3):\n    bars1 = axs[i, 1].bar(x-width/2, df.groupby(\"month\")[targets[i]].mean(),\n                        width=width, edgecolor=\"black\", label=\"Mean\", color=\"salmon\")\n    bars2 = axs[i, 1].bar(x+width/2, df.groupby(\"month\")[targets[i]].median(),\n                        width=width, edgecolor=\"black\", label=\"Median\", color=\"teal\")\n    axs[i, 1].set_title(f\"{target_names[i]} (target #{i+1})\", fontsize=15, pad=10)\n    axs[i, 1].set_ylabel(\"Target value\", fontsize=13, labelpad=5)\n    axs[i, 1].set_xlabel(\"Month\", fontsize=13, labelpad=5)\n    axs[i, 1].set_xticks(x)\n    axs[i, 1].grid(axis=\"y\")\n    axs[i, 1].legend(fontsize=13)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T04:56:10.34302Z","iopub.execute_input":"2021-07-24T04:56:10.343451Z","iopub.status.idle":"2021-07-24T04:56:11.600797Z","shell.execute_reply.started":"2021-07-24T04:56:10.343408Z","shell.execute_reply":"2021-07-24T04:56:11.599799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Day hours which will be used for plotting data\nhours = [0, 5, 8, 14, 19]\n# Dataframe copy excluding the last row which is the only one representing January\ndf = train.loc[train[\"hour\"].isin(hours)].drop([7110], axis=0).copy()\n\nfig, axs = plt.subplots(figsize=(16, 18), ncols=2, nrows=3, sharex=False,\n                        gridspec_kw={'width_ratios': [1, 1.5]})\n\nfig.suptitle(\"Target values distribution per month and day of week at given hours\", fontsize=20)\n\nplt.subplots_adjust(hspace = 0.3)\nwidth=0.15\nx = np.sort(df[\"day_of_week\"].unique()) + 1\n\nfor i in np.arange(3):\n    bars1 = axs[i, 0].bar(x-width*2, df.loc[df[\"hour\"] == 0].groupby(\"day_of_week\")[targets[i]].mean(),\n                        width=width, edgecolor=\"black\", label=\"00:00\", color=\"salmon\")\n    bars2 = axs[i, 0].bar(x-width, df.loc[df[\"hour\"] == 5].groupby(\"day_of_week\")[targets[i]].mean(),\n                        width=width, edgecolor=\"black\", label=\"05:00\", color=\"sandybrown\")\n    bars3 = axs[i, 0].bar(x, df.loc[df[\"hour\"] == 8].groupby(\"day_of_week\")[targets[i]].mean(),\n                        width=width, edgecolor=\"black\", label=\"08:00\", color=\"teal\")\n    bars4 = axs[i, 0].bar(x+width, df.loc[df[\"hour\"] == 14].groupby(\"day_of_week\")[targets[i]].mean(),\n                        width=width, edgecolor=\"black\", label=\"14:00\", color=\"palevioletred\")\n    bars5 = axs[i, 0].bar(x+width*2, df.loc[df[\"hour\"] == 19].groupby(\"day_of_week\")[targets[i]].mean(),\n                        width=width, edgecolor=\"black\", label=\"19:00\", color=\"mediumslateblue\")\n    axs[i, 0].set_title(f\"{target_names[i]} (target #{i+1})\", fontsize=15, pad=10)\n    axs[i, 0].set_ylabel(\"Target value\", fontsize=13, labelpad=5)\n    axs[i, 0].set_xlabel(\"Day of week\", fontsize=13, labelpad=5)\n    axs[i, 0].set_xticks(x)\n    axs[i, 0].grid(axis=\"y\")\n    axs[i, 0].legend(fontsize=10)\n\nx = df[\"month\"].unique()\nfor i in np.arange(3):\n    bars1 = axs[i, 1].bar(x-width*2, df.loc[df[\"hour\"] == 0].groupby(\"month\")[targets[i]].mean(),\n                        width=width, edgecolor=\"black\", label=\"00:00\", color=\"salmon\")\n    bars2 = axs[i, 1].bar(x-width, df.loc[df[\"hour\"] == 5].groupby(\"month\")[targets[i]].mean(),\n                        width=width, edgecolor=\"black\", label=\"05:00\", color=\"sandybrown\")\n    bars3 = axs[i, 1].bar(x, df.loc[df[\"hour\"] == 8].groupby(\"month\")[targets[i]].mean(),\n                        width=width, edgecolor=\"black\", label=\"08:00\", color=\"teal\")\n    bars4 = axs[i, 1].bar(x+width, df.loc[df[\"hour\"] == 14].groupby(\"month\")[targets[i]].mean(),\n                        width=width, edgecolor=\"black\", label=\"14:00\", color=\"palevioletred\")\n    bars5 = axs[i, 1].bar(x+width*2, df.loc[df[\"hour\"] == 19].groupby(\"month\")[targets[i]].mean(),\n                        width=width, edgecolor=\"black\", label=\"19:00\", color=\"mediumslateblue\")\n    axs[i, 1].set_title(f\"{target_names[i]} (target #{i+1})\", fontsize=15, pad=10)\n    axs[i, 1].set_ylabel(\"Target value\", fontsize=13, labelpad=5)\n    axs[i, 1].set_xlabel(\"Month\", fontsize=13, labelpad=5)\n    axs[i, 1].set_xticks(x)\n    axs[i, 1].grid(axis=\"y\")\n    axs[i, 1].legend(fontsize=10)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T04:56:11.602296Z","iopub.execute_input":"2021-07-24T04:56:11.602702Z","iopub.status.idle":"2021-07-24T04:56:13.680865Z","shell.execute_reply.started":"2021-07-24T04:56:11.602659Z","shell.execute_reply":"2021-07-24T04:56:13.679855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature plots","metadata":{}},{"cell_type":"code","source":"# Lists of feature names to be used for plots below\nall_features = [\"deg_C\", \"relative_humidity\", \"absolute_humidity\", \"sensor_1\", \"sensor_2\", \"sensor_3\", \"sensor_4\", \"sensor_5\"]\nall_feature_names = [\"Temperature (deg. C)\", \"Relative humidity\", \"Absolute humidity\", \"Sensor 1\", \"Sensor_2\", \"Sensor 3\", \"Sensor 4\", \"Sensor 5\"]\n\nweather_features = [\"deg_C\", \"relative_humidity\", \"absolute_humidity\"]\nweather_feature_names = [\"Temperature (deg. C)\", \"Relative humidity\", \"Absolute humidity\"]\n\nsensor_features = [\"sensor_1\", \"sensor_2\", \"sensor_3\", \"sensor_4\", \"sensor_5\"]\nsensor_feature_names = [\"Sensor 1\", \"Sensor_2\", \"Sensor 3\", \"Sensor 4\", \"Sensor 5\"]","metadata":{"execution":{"iopub.status.busy":"2021-07-24T04:56:13.682144Z","iopub.execute_input":"2021-07-24T04:56:13.68244Z","iopub.status.idle":"2021-07-24T04:56:13.688075Z","shell.execute_reply.started":"2021-07-24T04:56:13.682412Z","shell.execute_reply":"2021-07-24T04:56:13.687197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's compare our train and test feature data.","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(figsize=(16, 30), ncols=1, nrows=8, sharex=False)\n\nplt.subplots_adjust(hspace = 0.4)\n\ncolors = [\"lightcoral\", \"sandybrown\", \"darkorange\", \"mediumseagreen\",\n          \"lightseagreen\", \"cornflowerblue\", \"mediumpurple\", \"palevioletred\",\n          \"lightskyblue\", \"sandybrown\", \"yellowgreen\", \"indianred\",\n          \"lightsteelblue\", \"mediumorchid\", \"deepskyblue\"]\n\nfor i in np.arange(8):\n    legend_lines = [Line2D([0], [0], color=colors[i], lw=10),\n                    Line2D([0], [0], color=\"black\", lw=10)]\n    axs[i].plot(train[\"date_time\"], train[all_features[i]], color=colors[i], label=\"Train data\")\n    axs[i].plot(test[\"date_time\"], test[all_features[i]], color=\"black\", label=\"Test data\")\n    axs[i].set_title(f\"{all_feature_names[i]} levels across time\", fontsize=20, pad=5)\n    axs[i].set_ylabel(f\"{all_feature_names[i]} level\", fontsize=14, labelpad=5)\n    axs[i].set_xlabel(\"Date\", fontsize=14, labelpad=5)\n    axs[i].legend(legend_lines, [\"Train data\", \"Test data\"], fontsize=12, loc=1)\n    axs[i].grid(axis=\"both\")","metadata":{"execution":{"iopub.status.busy":"2021-07-24T04:56:13.689283Z","iopub.execute_input":"2021-07-24T04:56:13.689545Z","iopub.status.idle":"2021-07-24T04:56:15.400934Z","shell.execute_reply.started":"2021-07-24T04:56:13.689519Z","shell.execute_reply":"2021-07-24T04:56:15.399989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot dataframe creation\ndf = pd.concat([train_copy, test_copy], axis=0)\ndf.reset_index(drop=True, inplace=True)\ndf[\"week_of_year\"] = df[\"date_time\"].dt.isocalendar().week.astype(\"int\")\ndf[\"day_of_year\"] = df[\"date_time\"].dt.dayofyear\n\nfig, axs = plt.subplots(figsize=(16, 18), ncols=2, nrows=3, sharex=False)\n\nplt.subplots_adjust(hspace = 0.4)\n\ncolors = [\"palevioletred\", \"deepskyblue\", \"mediumseagreen\"]\n\nfor i in [0, 1, 2]:\n    # New year days start from 7110th row\n    data = df.iloc[:7110].groupby(\"day_of_year\")[weather_features[i]].mean()\n    axs[i, 0].plot(data.index, data.values, color=colors[i], label=\"Train data\")\n    data = df.iloc[7110:].groupby(\"day_of_year\")[weather_features[i]].mean()\n    axs[i, 0].plot(data.index, data.values, color=\"black\", alpha=0.7, label=\"Test data\")\n    axs[i, 0].set_title(f\"Mean dayly {weather_feature_names[i]} levels\", fontsize=20, pad=5)\n    axs[i, 0].set_ylabel(f\"{weather_feature_names[i]} level\", fontsize=14, labelpad=5)\n    axs[i, 0].set_xlabel(\"Day of year\", fontsize=14, labelpad=5)\n    axs[i, 0].grid(axis=\"both\")\n    axs[i, 0].legend(fontsize=12)\n\n\nfor i in [0, 1, 2]:\n    # New year weeks start from 7159th row. \n    # Because of Jan 1st and 2nd from the test dataset are counted as 52nd week of 2010,\n    # the colored plotline contains some test data. \n    data = df.iloc[:7159].groupby(\"week_of_year\")[weather_features[i]].mean()\n    axs[i, 1].plot(data.index, data.values, color=colors[i], label=\"Train data\")\n    data = df.iloc[7159:].groupby(\"week_of_year\")[weather_features[i]].mean()\n    axs[i, 1].plot(data.index, data.values, color=\"black\", alpha=0.7, label=\"Test data\")\n    axs[i, 1].set_title(f\"Mean weekly {weather_feature_names[i]} levels\", fontsize=20, pad=5)\n    axs[i, 1].set_ylabel(f\"{weather_feature_names[i]} level\", fontsize=14, labelpad=5)\n    axs[i, 1].set_xlabel(\"Week of year\", fontsize=14, labelpad=5)\n    axs[i, 1].grid(axis=\"both\")\n    axs[i, 1].legend(fontsize=12)\n\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-07-24T04:56:15.402309Z","iopub.execute_input":"2021-07-24T04:56:15.402584Z","iopub.status.idle":"2021-07-24T04:56:16.641318Z","shell.execute_reply.started":"2021-07-24T04:56:15.402557Z","shell.execute_reply":"2021-07-24T04:56:16.640313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot dataframe creation\ndf = pd.concat([train_copy, test_copy], axis=0)\ndf.reset_index(drop=True, inplace=True)\ndf[\"week_of_year\"] = df[\"date_time\"].dt.isocalendar().week.astype(\"int\")\ndf[\"day_of_year\"] = df[\"date_time\"].dt.dayofyear\n\nfig, axs = plt.subplots(figsize=(16, 30), ncols=2, nrows=5, sharex=False)\n\nplt.subplots_adjust(hspace = 0.4)\n\ncolors = [\"palevioletred\", \"deepskyblue\", \"mediumseagreen\", \"goldenrod\", \"indianred\"]\n\nfor i in np.arange(5):\n    data = df.iloc[:7110].groupby(\"day_of_year\")[sensor_features[i]].mean()\n    axs[i, 0].plot(data.index, data.values, color=colors[i], label=\"Train data\")\n    data = df.iloc[7110:].groupby(\"day_of_year\")[sensor_features[i]].mean()\n    axs[i, 0].plot(data.index, data.values, color=\"black\", alpha=0.7, label=\"Test data\")\n    axs[i, 0].set_title(f\"Mean dayly {sensor_feature_names[i]} levels\", fontsize=20, pad=5)\n    axs[i, 0].set_ylabel(f\"{sensor_feature_names[i]} level\", fontsize=14, labelpad=5)\n    axs[i, 0].set_xlabel(\"Day of year\", fontsize=14, labelpad=5)\n    axs[i, 0].grid(axis=\"both\")\n    axs[i, 0].legend(fontsize=12)\n\n\nfor i in np.arange(5):\n    data = df.iloc[:7159].groupby(\"week_of_year\")[sensor_features[i]].mean()\n    axs[i, 1].plot(data.index, data.values, color=colors[i], label=\"Train data\")\n    data = df.iloc[7159:].groupby(\"week_of_year\")[sensor_features[i]].mean()\n    axs[i, 1].plot(data.index, data.values, color=\"black\", alpha=0.7, label=\"Test data\")\n    axs[i, 1].set_title(f\"Mean dayly {sensor_feature_names[i]} levels\", fontsize=20, pad=5)\n    axs[i, 1].set_ylabel(f\"{sensor_feature_names[i]} level\", fontsize=14, labelpad=5)\n    axs[i, 1].set_xlabel(\"Week of year\", fontsize=14, labelpad=5)\n    axs[i, 1].grid(axis=\"both\")\n    axs[i, 1].legend(fontsize=12)\n\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-07-24T04:56:16.642593Z","iopub.execute_input":"2021-07-24T04:56:16.642882Z","iopub.status.idle":"2021-07-24T04:56:18.43957Z","shell.execute_reply.started":"2021-07-24T04:56:16.642855Z","shell.execute_reply":"2021-07-24T04:56:18.438528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check feature correlation.","metadata":{}},{"cell_type":"code","source":"# Plot dataframe\ndf = train_copy.copy()\ndf = pd.concat([df[targets], df.drop(targets, axis=1)], axis=1).corr().round(2)\n\n# Mask to hide upper-right part of plot as it is a duplicate\nmask = np.zeros_like(df)\nmask[np.triu_indices_from(mask)] = True\n\n# Making a plot\nplt.figure(figsize=(12,12))\nax = sns.heatmap(df, annot=True, mask=mask, cmap=\"RdBu\", linewidths=1,\n                 annot_kws={\"weight\": \"bold\", \"fontsize\":13})\nax.set_title(\"Original dataset correlation heatmap\", fontsize=17)\nplt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n         rotation_mode=\"anchor\", weight=\"bold\")\nplt.setp(ax.get_yticklabels(), weight=\"bold\")\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-07-24T04:56:18.443996Z","iopub.execute_input":"2021-07-24T04:56:18.444402Z","iopub.status.idle":"2021-07-24T04:56:19.375095Z","shell.execute_reply.started":"2021-07-24T04:56:18.444364Z","shell.execute_reply":"2021-07-24T04:56:19.373986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot dataframe\ndf = add_new_ml_features(train_copy.copy())\ndf = pd.concat([df[targets], df.drop(targets, axis=1)], axis=1).corr().round(2)\n\n# Mask to hide upper-right part of plot as it is a duplicate\nmask = np.zeros_like(df)\nmask[np.triu_indices_from(mask)] = True\n\n# Making a plot\nplt.figure(figsize=(16,16))\nax = sns.heatmap(df, annot=False, mask=mask, cmap=\"RdBu\", annot_kws={\"weight\": \"bold\", \"fontsize\": 7})\nax.set_title(\"Original and engineered features correlation heatmap\", fontsize=17)\nplt.setp(ax.get_xticklabels(), rotation=90, ha=\"right\",\n         rotation_mode=\"anchor\", weight=\"normal\")\nplt.setp(ax.get_yticklabels(), weight=\"normal\")\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-07-24T04:56:19.377071Z","iopub.execute_input":"2021-07-24T04:56:19.377487Z","iopub.status.idle":"2021-07-24T04:56:22.44427Z","shell.execute_reply.started":"2021-07-24T04:56:19.377447Z","shell.execute_reply":"2021-07-24T04:56:22.442013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking the most correlated features \nfor col in df.columns:\n    for index in df[col].index:\n        if df[col][index] != 1:\n            if (df[col][index] >= 0.93) | (df[col][index] <=-0.93):\n                print(f\"Correlation of {index} and {col} is {df[col][index]}\")","metadata":{"execution":{"iopub.status.busy":"2021-07-24T04:56:22.445736Z","iopub.execute_input":"2021-07-24T04:56:22.446091Z","iopub.status.idle":"2021-07-24T04:56:23.033265Z","shell.execute_reply.started":"2021-07-24T04:56:22.446053Z","shell.execute_reply":"2021-07-24T04:56:23.032322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Model training**","metadata":{}},{"cell_type":"markdown","source":"The datetime conversion shown below was found in this [notebook](https://www.kaggle.com/jarupula/eda-rf-model-tps-july-21). It gives a significant score boost.","metadata":{}},{"cell_type":"code","source":"def prepare_dataset(train_copy, test_copy, i):\n    \n    X = add_new_ml_features(train_copy.copy(), i)\n\n    # Dropping the last row which is 2011-01-01 00:00:00\n    if X.index[-1] == 7110:\n        X.drop([7110], axis=0, inplace=True)\n\n    # Resetting dataframe index\n    X.reset_index(drop=True, inplace=True)\n\n    # The months will be used for folds split\n    months = X[\"date_time\"].dt.month\n\n    # Adding 72 last train set rows to the head of test set in order to get shifting feature values\n    X_test_temp = pd.concat([train_copy.iloc[-25:-1].drop([\"target_carbon_monoxide\", \"target_benzene\", \"target_nitrogen_oxides\"], axis=1), test_copy], axis=0)\n    X_test_temp.reset_index(inplace=True, drop=True)\n    X_test = add_new_ml_features(X_test_temp.copy(), i)\n    # Deleting added train set rows\n    X_test.drop(X_test.loc[:23].index, axis=0, inplace=True)\n    X_test.reset_index(inplace=True, drop=True)\n\n    y = np.log1p(X[[\"target_carbon_monoxide\", \"target_benzene\", \"target_nitrogen_oxides\"]])\n    X.drop([\"target_carbon_monoxide\", \"target_benzene\", \"target_nitrogen_oxides\"], axis=1, inplace=True)\n    \n    X['date_time'] = X['date_time'].astype('datetime64[ns]').astype(np.int64)/10**9\n    X_test['date_time'] = X_test['date_time'].astype('datetime64[ns]').astype(np.int64)/10**9\n    \n#     print(X.shape, y.shape, X_test.shape)\n#     display(X.head())\n#     display(y.head())\n    \n    return X, X_test, y","metadata":{"execution":{"iopub.status.busy":"2021-07-24T04:56:23.035109Z","iopub.execute_input":"2021-07-24T04:56:23.035428Z","iopub.status.idle":"2021-07-24T04:56:23.046226Z","shell.execute_reply.started":"2021-07-24T04:56:23.035398Z","shell.execute_reply":"2021-07-24T04:56:23.045378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# LightAutoML parameters\nN_THREADS = 4 # threads cnt for lgbm and linear models\nN_FOLDS = 5 # folds cnt for AutoML\nRANDOM_STATE = 42 # fixed random state for various reasons\nTEST_SIZE = 0.2 # Test size for metric check\nTIMEOUT = 2.9 * 3600 # Time in seconds for automl run\n\n# Fixing parameters for better repeatability \nnp.random.seed(RANDOM_STATE)\ntorch.set_num_threads(N_THREADS)\n\n# Initializing and filling predictions dataframe before datetime conversion\npreds = pd.DataFrame()\npreds[\"date_time\"] = test_copy[\"date_time\"].copy()\n\ntrain_preds_df = pd.DataFrame(index=np.arange(7110))\ntest_preds_df = pd.DataFrame(index=test.index)\n\nfeature_importances = []\nfor i, target in enumerate(targets):\n    \n    ROLES = {CategoryRole(force_input=True, ordinal=True): [\"working_hours\", \"maximum_hours\", \"is_weekend\"],\n#              DatetimeRole(base_date=False, base_feats=True, seasonality=(\"d\", \"wd\", \"hour\")): \"date_time\",\n             \"target\": target}    \n    X, X_test, y = prepare_dataset(pd.concat([train_copy, train_preds_df], axis=1), pd.concat([test_copy, test_preds_df], axis=1), i)\n    display(X)\n    \n    model = TabularUtilizedAutoML(task = Task(\"reg\", loss=\"rmsle\", metric=\"rmsle\"),\n                                  verbose=1,\n                                  timeout = TIMEOUT,\n                                  cpu_limit = N_THREADS,\n                                  reader_params = {'n_jobs': N_THREADS, 'cv': N_FOLDS, 'random_state': RANDOM_STATE},\n#                                   general_params = {'use_algos': [['lgb_tuned', 'cb_tuned'], ['lgb', 'cb_tuned']]}\n                                  general_params = {'use_algos': [['lgb_tuned', 'cb_tuned', 'lgb', 'cb'], ['cb']]}\n#                                   general_params = {'use_algos': [['cb']]}\n                                 )\n\n    oof_preds = model.fit_predict(pd.concat([X, y[target]], axis=1), roles = ROLES)\n\n    print(f\"{target} oof_score score is {np.sqrt(mean_squared_log_error(np.expm1(y[target].values), np.expm1(oof_preds.data)))}\")\n    preds[target] = np.expm1(model.predict(X_test).data)\n    feature_importances.append(model.get_feature_scores('fast', silent=False))\n    \n    train_preds_df[target+\"_preds\"] = oof_preds.data\n    test_preds_df[target+\"_preds\"] = np.log1p(preds[target])","metadata":{"execution":{"iopub.status.busy":"2021-07-24T04:56:23.047594Z","iopub.execute_input":"2021-07-24T04:56:23.047873Z","iopub.status.idle":"2021-07-24T05:37:19.01957Z","shell.execute_reply.started":"2021-07-24T04:56:23.047846Z","shell.execute_reply":"2021-07-24T05:37:19.018104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Feature importances**","metadata":{}},{"cell_type":"code","source":"# Creating feature list from feature importance dataframes in case there are diffrent dataset used for each target\nfeature_list = set()\nfor i in np.arange(len(feature_importances)):\n    feature_list = set.union(feature_list, set(feature_importances[i][\"Feature\"]))\nprint(f\"There are {len(feature_list)} unique features used for training: {feature_list}\")","metadata":{"execution":{"iopub.status.busy":"2021-07-24T05:37:19.020864Z","iopub.execute_input":"2021-07-24T05:37:19.021118Z","iopub.status.idle":"2021-07-24T05:37:19.028163Z","shell.execute_reply.started":"2021-07-24T05:37:19.021092Z","shell.execute_reply":"2021-07-24T05:37:19.027064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating a sorted dataframe with all feature importances\nfi_df = pd.DataFrame(columns=[\"Feature\", \"Target1_imp\", \"Target2_imp\", \"Target3_imp\"])\nfi_df[\"Feature\"] = list(feature_list)\nfi_df.sort_values(\"Feature\", inplace=True)\nfi_df.reset_index(drop=True, inplace=True)\nfor i, fi in enumerate(feature_importances):\n    for feature in fi[\"Feature\"]:\n        fi_df.loc[fi_df[\"Feature\"]==feature, \"Target\"+str(i+1)+\"_imp\"] = fi.loc[fi[\"Feature\"]==feature, \"Importance\"].values / fi[\"Importance\"].sum()\n\nfi_df.fillna(0, inplace=True)\nfi_df[\"Overall_importance\"] = fi_df[\"Target1_imp\"] + fi_df[\"Target2_imp\"] + fi_df[\"Target3_imp\"]\nfi_df.sort_values(\"Overall_importance\", ascending=False, inplace=True)\nfi_df.reset_index(drop=True, inplace=True)\n\n# Displaying original feature importance dataframes to quickly check target specific feature performance\ndisplay(feature_importances[0].T)\nprint(\"Importance sum\", feature_importances[0][\"Importance\"].sum())\ndisplay(feature_importances[1].T)\nprint(\"Importance sum\", feature_importances[1][\"Importance\"].sum())\ndisplay(feature_importances[2].T)\nprint(\"Importance sum\", feature_importances[2][\"Importance\"].sum())","metadata":{"execution":{"iopub.status.busy":"2021-07-24T05:37:19.030002Z","iopub.execute_input":"2021-07-24T05:37:19.030564Z","iopub.status.idle":"2021-07-24T05:37:19.7407Z","shell.execute_reply.started":"2021-07-24T05:37:19.030521Z","shell.execute_reply":"2021-07-24T05:37:19.739674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fi_df.T","metadata":{"execution":{"iopub.status.busy":"2021-07-24T05:37:19.742302Z","iopub.execute_input":"2021-07-24T05:37:19.742712Z","iopub.status.idle":"2021-07-24T05:37:19.890511Z","shell.execute_reply.started":"2021-07-24T05:37:19.742669Z","shell.execute_reply":"2021-07-24T05:37:19.889512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Displaying 10 least important features for each target\ndisplay(feature_importances[0].tail(10).T)\ndisplay(feature_importances[1].tail(10).T)\ndisplay(feature_importances[2].tail(10).T)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T05:37:19.891715Z","iopub.execute_input":"2021-07-24T05:37:19.892022Z","iopub.status.idle":"2021-07-24T05:37:19.934652Z","shell.execute_reply.started":"2021-07-24T05:37:19.891993Z","shell.execute_reply":"2021-07-24T05:37:19.933761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df= fi_df\nx = np.arange(0, len(df[\"Feature\"]))\nheight = 0.3\n\nfig, ax = plt.subplots(figsize=(12, 80))\nbars1 = ax.barh(x-height, df[\"Target1_imp\"], height=height,\n                color=\"cornflowerblue\",\n                edgecolor=\"black\",\n                label=target_names[0])\nbars2 = ax.barh(x, df[\"Target2_imp\"], height=height,\n                color=\"palevioletred\",\n                edgecolor=\"black\",\n                label=target_names[1])\nbars3 = ax.barh(x+height, df[\"Target3_imp\"], height=height,\n                color=\"mediumseagreen\",\n                edgecolor=\"black\",\n                label=target_names[2])\nax.set_title(\"Feature importances\", fontsize=20, pad=5)\nax.set_ylabel(\"Feature names\", fontsize=15, labelpad=5)\nax.set_xlabel(\"Feature importance\", fontsize=15, labelpad=5)\nax.set_yticks(x)\nax.set_yticklabels(df[\"Feature\"], fontsize=12)\nax.tick_params(axis=\"x\", labelsize=12)\nax.grid(axis=\"x\")\nax.legend(fontsize=13, loc=\"upper right\", bbox_to_anchor=(0, 0, 1, 0.92))\nplt.margins(0.04, 0.01)\nplt.gca().invert_yaxis()","metadata":{"execution":{"iopub.status.busy":"2021-07-24T05:37:19.935941Z","iopub.execute_input":"2021-07-24T05:37:19.936227Z","iopub.status.idle":"2021-07-24T05:37:24.653028Z","shell.execute_reply.started":"2021-07-24T05:37:19.936199Z","shell.execute_reply":"2021-07-24T05:37:24.65224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Predictions submission and comparison**","metadata":{}},{"cell_type":"code","source":"preds.to_csv('submission.csv', index=False)\npreds.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-24T05:37:24.654042Z","iopub.execute_input":"2021-07-24T05:37:24.654318Z","iopub.status.idle":"2021-07-24T05:37:24.685803Z","shell.execute_reply.started":"2021-07-24T05:37:24.654291Z","shell.execute_reply":"2021-07-24T05:37:24.684924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's compare predictions with the closest months from the train datasets.","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(ncols=1, nrows=3, figsize=(16, 8))\nplt.set_cmap(\"Set2\")\nplt.subplots_adjust(hspace = 0.3)\n\nfor i, target in enumerate(y.columns):\n    axs[i].plot(np.arange(0, 744, 1), train.loc[train[\"month\"]==12, target], label=\"Train, 12th month\")\n    axs[i].plot(np.arange(0, 744, 1), preds.loc[preds[\"date_time\"].dt.month==1, target],\n                label=\"Test, 1th month\")\n    axs[i].set_title(target_names[i], fontsize=15)\n    axs[i].legend(fontsize=13)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T05:37:24.687252Z","iopub.execute_input":"2021-07-24T05:37:24.687691Z","iopub.status.idle":"2021-07-24T05:37:25.205011Z","shell.execute_reply.started":"2021-07-24T05:37:24.687647Z","shell.execute_reply":"2021-07-24T05:37:25.204084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axs = plt.subplots(ncols=1, nrows=3, figsize=(16, 8))\nplt.set_cmap(\"Set2\")\nplt.subplots_adjust(hspace = 0.3)\n\nfor i, target in enumerate(y.columns):\n    axs[i].plot(np.arange(0, 720, 1), train.loc[train[\"month\"]==11, target], label=\"Train, 11th month\")\n    axs[i].plot(np.arange(0, 744, 1), preds.loc[preds[\"date_time\"].dt.month==1, target],\n                label=\"Test, 1th month\")\n    axs[i].set_title(target_names[i], fontsize=15)\n    axs[i].legend(fontsize=13)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T05:37:25.206289Z","iopub.execute_input":"2021-07-24T05:37:25.206578Z","iopub.status.idle":"2021-07-24T05:37:25.715693Z","shell.execute_reply.started":"2021-07-24T05:37:25.206548Z","shell.execute_reply":"2021-07-24T05:37:25.714749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axs = plt.subplots(ncols=1, nrows=3, figsize=(16, 8))\nplt.set_cmap(\"Set2\")\nplt.subplots_adjust(hspace = 0.3)\n\nfor i, target in enumerate(y.columns):\n    axs[i].plot(np.arange(0, 598, 1), train.loc[:597, target], label=\"Train, from 10.3 to 4.4\")\n    axs[i].plot(np.arange(0, 596, 1), preds.loc[1651: , target],\n                label=\"Test, from 10.3 to 4.4\")\n    axs[i].set_title(target_names[i], fontsize=15)\n    axs[i].legend(fontsize=13)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T05:37:25.716966Z","iopub.execute_input":"2021-07-24T05:37:25.717275Z","iopub.status.idle":"2021-07-24T05:37:26.234571Z","shell.execute_reply.started":"2021-07-24T05:37:25.717244Z","shell.execute_reply":"2021-07-24T05:37:26.233555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see, the predictions are the closest to the training set in the overlapping months (from March 10 to April 4). ","metadata":{}}]}