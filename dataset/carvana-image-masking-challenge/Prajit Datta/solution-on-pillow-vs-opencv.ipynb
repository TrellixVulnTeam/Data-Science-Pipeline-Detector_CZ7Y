{"metadata":{"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"},"language_info":{"mimetype":"text/x-python","nbconvert_exporter":"python","codemirror_mode":{"name":"ipython","version":3},"name":"python","version":"3.6.1","pygments_lexer":"ipython3","file_extension":".py"}},"nbformat":4,"cells":[{"cell_type":"markdown","metadata":{"_uuid":"10b24b50e4a18c18eba46934d8245217bb41c5b4","_cell_guid":"aca3928b-ed3d-4a9a-88fc-02bc3eea1750"},"source":"**Fast benchmark: Pillow vs OpenCV**\nBackground: when we deal with images in image-based problems and deploy a deep learning solution, it is better to have a fast image reading and transforming library. Let's compare Pillow and OpenCV python libraries on image loading and some basic transformations on source images from Carvana competition.\nOpenCV: C++, python-wrapper\nPillow: Python, C\nIntuition says that Opencv should be a little faster, let's see this by examples\nThis question I asked myself after reading the PyTorch documentation on image transformation. Most of transformations take as input a PIL image."},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"collapsed":true},"source":"import PIL\nimport cv2"},{"cell_type":"markdown","metadata":{},"source":"At first, let's get packages versions, specs and some info on the machine"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{},"source":"print(cv2.__version__, cv2.__spec__)\nprint(cv2.getBuildInformation())"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{},"source":"PIL.__version__, PIL.__spec__"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{},"source":"!cat /proc/cpuinfo | egrep \"model name\""},{"cell_type":"markdown","metadata":{},"source":"Data storage info: ROTA 1 means rotational device"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{},"source":"!lsblk -o name,rota,type,mountpoint"},{"cell_type":"markdown","metadata":{},"source":"Now let's setup the input data"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{},"source":"import os\nthis_path = os.path.dirname('.')\n\nINPUT_PATH = os.path.abspath(os.path.join(this_path, '..', 'input'))\nTRAIN_DATA = os.path.join(INPUT_PATH, \"train\")\nfrom glob import glob\nfilenames = glob(os.path.join(TRAIN_DATA, \"*.jpg\"))\nlen(filenames)"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"collapsed":true},"source":"import matplotlib.pylab as plt\n%matplotlib inline"},{"cell_type":"markdown","metadata":{},"source":"1 stage: 100 images, load image + blur + flip"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"collapsed":true},"source":"import numpy as np\nfrom PIL import Image, ImageOps\n\ndef stage_1_PIL(filename):\n    img_pil = Image.open(filename)\n    img_pil = ImageOps.box_blur(img_pil, radius=3)\n    img_pil = img_pil.transpose(Image.FLIP_LEFT_RIGHT)\n    return np.asarray(img_pil)\n\ndef stage_1_cv2(filename):\n    img = cv2.imread(filename)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = cv2.blur(img, ksize=(3, 3))\n    img = cv2.flip(img, flipCode=1)\n    return img"},{"cell_type":"markdown","metadata":{},"source":"Let's compare briefly results of transformations on the first image. Results are not perfectly the same, but it is not important for the benchmark"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{},"source":"f = filenames[0]\nr1 = stage_1_PIL(f) \nr2 = stage_1_cv2(f)\n\nplt.figure(figsize=(16, 16))\nplt.subplot(131)\nplt.imshow(r1)\nplt.subplot(132)\nplt.imshow(r2)\nplt.subplot(133)\nplt.imshow(np.abs(r1 - r2))"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"collapsed":true},"source":"%timeit -n5 -r3 [stage_1_PIL(f) for f in filenames[:100]]"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"collapsed":true},"source":"%timeit -n5 -r3 [stage_1_cv2(f) for f in filenames[:100]]"}],"nbformat_minor":1}