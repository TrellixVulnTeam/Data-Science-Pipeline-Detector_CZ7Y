{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport torchvision\nfrom torchvision import datasets,transforms\nfrom tqdm import tqdm\nimport cv2\nfrom torch.utils.data import Dataset,DataLoader\nimport torch.optim as optim\nfrom PIL import Image\nimport torchvision.transforms as transforms\nimport os\nimport torch.nn.functional as F","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-11T08:57:46.586415Z","iopub.execute_input":"2021-10-11T08:57:46.586679Z","iopub.status.idle":"2021-10-11T08:57:51.016109Z","shell.execute_reply.started":"2021-10-11T08:57:46.586649Z","shell.execute_reply":"2021-10-11T08:57:51.015302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import zipfile\n# 'test.zip'\ndirs = ['train.zip','train_masks.zip']\nfor x in dirs:\n    with zipfile.ZipFile(\"../input/carvana-image-masking-challenge/\"+ x,'r') as z:\n        z.extractall(\".\")","metadata":{"execution":{"iopub.status.busy":"2021-10-06T16:30:03.116084Z","iopub.execute_input":"2021-10-06T16:30:03.11641Z","iopub.status.idle":"2021-10-06T16:30:12.297049Z","shell.execute_reply.started":"2021-10-06T16:30:03.11637Z","shell.execute_reply":"2021-10-06T16:30:12.295881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls ","metadata":{"execution":{"iopub.status.busy":"2021-10-06T16:30:12.300142Z","iopub.execute_input":"2021-10-06T16:30:12.300817Z","iopub.status.idle":"2021-10-06T16:30:13.100039Z","shell.execute_reply.started":"2021-10-06T16:30:12.300767Z","shell.execute_reply":"2021-10-06T16:30:13.098914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# config\nLEARNING_RATE = 1e-4\nSPLIT=0.2\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nBATCH_SIZE = 4\nEPOCHS = 4\nNUM_WORKERS = 4\nIMAGE_HEIGHT = 572\nIMAGE_WIDTH = 572\nORIG_WIDTH = 1918\nORIG_HEIGHT = 1280\nthreshold = 0.5\nPIN_MEMORY = True\nDATAPATH = \"../input/carvana-image-masking-challenge/\"\nTRAIN_IMG_DIR = './train'\nTRAIN_MASK_DIR = './train_masks'\ntest_dir = './test'\n\n# VAL_IMG_DIR = \n# VAL_MASK_DIR = \n\n","metadata":{"execution":{"iopub.status.busy":"2021-10-11T08:57:52.173007Z","iopub.execute_input":"2021-10-11T08:57:52.173267Z","iopub.status.idle":"2021-10-11T08:57:52.21907Z","shell.execute_reply.started":"2021-10-11T08:57:52.173239Z","shell.execute_reply":"2021-10-11T08:57:52.218246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\n\ndef double_conv(in_c, out_c):\n    conv = nn.Sequential(\n        nn.Conv2d(in_c, out_c, kernel_size=3, stride=1, padding=1),\n        nn.BatchNorm2d(out_c),\n        nn.ReLU(inplace=True),\n        nn.Conv2d(out_c, out_c, kernel_size=3, stride=1, padding=1),\n        nn.BatchNorm2d(out_c),\n        nn.ReLU(inplace=True)\n    )\n    return conv.to(DEVICE)\n\n\n# def crop_img(tensor, target_tensor):\n#     target_size = target_tensor.size()[2]\n#     tensor_size = tensor.size()[2]\n#     delta = tensor_size-target_size\n#     delta = delta//2\n#     # all batch, all channels, heightModified,widthModified\n\n#     return tensor[:, :, delta:tensor_size-delta, delta:tensor_size-delta]\n\ndef addPadding(srcShapeTensor, tensor_whose_shape_isTobechanged):\n\n    if(srcShapeTensor.shape != tensor_whose_shape_isTobechanged.shape):\n        target = torch.zeros(srcShapeTensor.shape)\n        target[:, :, :tensor_whose_shape_isTobechanged.shape[2],\n               :tensor_whose_shape_isTobechanged.shape[3]] = tensor_whose_shape_isTobechanged\n        return target.to(DEVICE)\n    return tensor_whose_shape_isTobechanged.to(DEVICE)\n\nclass UNet(nn.Module):\n    def __init__(self):\n        super(UNet, self).__init__()\n        self.max_pool_2x2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.down_conv_1 = double_conv(3, 64)\n        self.down_conv_2 = double_conv(64, 128)\n        self.down_conv_3 = double_conv(128, 256)\n        self.down_conv_4 = double_conv(256, 512)\n        self.down_conv_5 = double_conv(512, 1024)\n\n        self.up_trans_1 = nn.ConvTranspose2d(\n            in_channels=1024,\n            out_channels=512,\n            kernel_size=2,\n            stride=2\n        )\n        self.up_conv_1 = double_conv(1024, 512)\n\n        self.up_trans_2 = nn.ConvTranspose2d(\n            in_channels=512,\n            out_channels=256,\n            kernel_size=2,\n            stride=2\n        )\n        self.up_conv_2 = double_conv(512, 256)\n\n        self.up_trans_3 = nn.ConvTranspose2d(\n            in_channels=256,\n            out_channels=128,\n            kernel_size=2,\n            stride=2\n        )\n        self.up_conv_3 = double_conv(256, 128)\n\n        self.up_trans_4 = nn.ConvTranspose2d(\n            in_channels=128,\n            out_channels=64,\n            kernel_size=2,\n            stride=2\n        )\n        self.up_conv_4 = double_conv(128, 64)\n\n        self.out = nn.Conv2d(\n            in_channels=64,\n            out_channels=1,\n            kernel_size=1\n        )\n\n    def forward(self, image):\n        # expected size\n        # encoder (Normal convolutions decrease the size)\n        x1 = self.down_conv_1(image)\n        # print(\"x1 \"+str(x1.shape))\n        x2 = self.max_pool_2x2(x1)\n        # print(\"x2 \"+str(x2.shape))\n        x3 = self.down_conv_2(x2)\n        # print(\"x3 \"+str(x3.shape))\n        x4 = self.max_pool_2x2(x3)\n        # print(\"x4 \"+str(x4.shape))\n        x5 = self.down_conv_3(x4)\n        # print(\"x5 \"+str(x5.shape))\n        x6 = self.max_pool_2x2(x5)\n        # print(\"x6 \"+str(x6.shape))\n        x7 = self.down_conv_4(x6)\n        # print(\"x7 \"+str(x7.shape))\n        x8 = self.max_pool_2x2(x7)\n        # print(\"x8 \"+str(x8.shape))\n        x9 = self.down_conv_5(x8)\n        # print(\"x9 \"+str(x9.shape))\n\n        # decoder (transposed convolutions increase the size)\n        x = self.up_trans_1(x9)\n        x = addPadding(x7, x)\n        x = self.up_conv_1(torch.cat([x7, x], 1))\n\n        x = self.up_trans_2(x)\n        x = addPadding(x5, x)\n        x = self.up_conv_2(torch.cat([x5, x], 1))\n\n        x = self.up_trans_3(x)\n        x = addPadding(x3, x)\n        x = self.up_conv_3(torch.cat([x3, x], 1))\n\n        x = self.up_trans_4(x)\n        x = addPadding(x1, x)\n        x = self.up_conv_4(torch.cat([x1, x], 1))\n\n        x = self.out(x)\n        # print(x.shape)\n        return x.to(DEVICE)\n\n\n# if __name__ == \"__main__\":\n#     image = torch.rand((3, 3, 572, 572))\n#     model = UNet()\n#     print(image.shape)\n#     model(image)\n","metadata":{"execution":{"iopub.status.busy":"2021-10-06T16:30:13.158366Z","iopub.execute_input":"2021-10-06T16:30:13.158681Z","iopub.status.idle":"2021-10-06T16:30:13.183907Z","shell.execute_reply.started":"2021-10-06T16:30:13.15865Z","shell.execute_reply":"2021-10-06T16:30:13.182484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CarvanaDataset(Dataset):\n    def __init__(self, image_dir, mask_dir, transform=None, train=True,image_names=None):\n        self.isTrain = train\n        if(self.isTrain):\n            self.image_dir = image_dir\n            self.mask_dir = mask_dir\n            self.transform = transform\n            self.images = os.listdir(self.image_dir)\n        else:\n            self.image_dir = image_dir\n            self.transform = transform\n#             self.images = os.listdir(self.image_dir)\n            self.images = image_names\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, index):\n        if(self.isTrain):\n            img_path = os.path.join(self.image_dir, self.images[index])\n            mask_path = os.path.join(\n                self.mask_dir, self.images[index].replace(\".jpg\", \"_mask.gif\"))\n            image = np.array(Image.open(img_path).convert(\"RGB\"))\n            mask = np.array(Image.open(mask_path).convert(\"L\"),\n                            dtype=np.float32)\n            mask[mask == 255.0] = 1.0\n\n            if self.transform is not None:\n                augmentations = self.transform(image=image, mask=mask)\n                image = augmentations['image']\n                mask = augmentations['mask']\n            return {\"image\": image, \"mask\": mask}\n\n        else:\n            img_path = os.path.join(self.image_dir, self.images[index])\n            image = np.array(Image.open(img_path).convert(\"RGB\"))\n            if self.transform is not None:\n                augmentations = self.transform(image=image)\n                image = augmentations['image']\n            return {'image': image,\"name\":self.images[index]}","metadata":{"execution":{"iopub.status.busy":"2021-10-06T16:30:13.186683Z","iopub.execute_input":"2021-10-06T16:30:13.188276Z","iopub.status.idle":"2021-10-06T16:30:13.20765Z","shell.execute_reply.started":"2021-10-06T16:30:13.188245Z","shell.execute_reply":"2021-10-06T16:30:13.206685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images = os.listdir(TRAIN_IMG_DIR)\nmasks = os.listdir(TRAIN_MASK_DIR)","metadata":{"execution":{"iopub.status.busy":"2021-10-06T16:30:13.209335Z","iopub.execute_input":"2021-10-06T16:30:13.209729Z","iopub.status.idle":"2021-10-06T16:30:13.296745Z","shell.execute_reply.started":"2021-10-06T16:30:13.209687Z","shell.execute_reply":"2021-10-06T16:30:13.295776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img = np.array(Image.open(TRAIN_IMG_DIR+\"/\"+images[0]).convert(\"RGB\"))\nplt.imshow(img,cmap=\"gray\")\nprint(img.shape)","metadata":{"execution":{"iopub.status.busy":"2021-10-06T16:30:13.299189Z","iopub.execute_input":"2021-10-06T16:30:13.29943Z","iopub.status.idle":"2021-10-06T16:30:14.124336Z","shell.execute_reply.started":"2021-10-06T16:30:13.299401Z","shell.execute_reply":"2021-10-06T16:30:14.123316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"msk = np.array(Image.open(TRAIN_MASK_DIR+\"/\"+images[0].replace(\".jpg\",\"_mask.gif\")).convert(\"L\"))\nplt.imshow(msk,cmap=\"gray\")\nprint(msk.shape)","metadata":{"execution":{"iopub.status.busy":"2021-10-06T16:30:14.125624Z","iopub.execute_input":"2021-10-06T16:30:14.126356Z","iopub.status.idle":"2021-10-06T16:30:14.634417Z","shell.execute_reply.started":"2021-10-06T16:30:14.12631Z","shell.execute_reply":"2021-10-06T16:30:14.633285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fit(model,dataloader,data,optimizer,criterion):\n    print('-------------Training---------------')\n    model.train()\n    train_running_loss = 0.0\n    counter=0\n    \n    # num of batches\n    num_batches = int(len(data)/dataloader.batch_size)\n    for i,data in tqdm(enumerate(dataloader),total=num_batches):\n        counter+=1\n        image,mask = data[\"image\"].to(DEVICE),data[\"mask\"].to(DEVICE)\n        optimizer.zero_grad()\n        outputs = model(image)\n        outputs =outputs.squeeze(1)\n        loss = criterion(outputs,mask)\n        train_running_loss += loss.item()\n        loss.backward()\n        optimizer.step()\n    train_loss = train_running_loss/counter\n    return train_loss\ndef validate(model,dataloader,data,criterion):\n    print(\"\\n--------Validating---------\\n\")\n    model.eval()\n    valid_running_loss = 0.0\n    counter = 0\n    # number of batches\n    num_batches = int(len(data)/dataloader.batch_size)\n    with torch.no_grad():\n        for i,data in tqdm(enumerate(dataloader),total=num_batches):\n            counter+=1\n            image,mask = data[\"image\"].to(DEVICE),data[\"mask\"].to(DEVICE)\n            outputs = model(image)\n            outputs =outputs.squeeze(1)\n            loss = criterion(outputs,mask)\n            valid_running_loss += loss.item()\n    valid_loss = valid_running_loss/counter\n    return valid_loss\n    ","metadata":{"execution":{"iopub.status.busy":"2021-10-06T16:30:14.636526Z","iopub.execute_input":"2021-10-06T16:30:14.636925Z","iopub.status.idle":"2021-10-06T16:30:14.649723Z","shell.execute_reply.started":"2021-10-06T16:30:14.636856Z","shell.execute_reply":"2021-10-06T16:30:14.6485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import albumentations as A\nfrom albumentations.pytorch import ToTensorV2 ","metadata":{"execution":{"iopub.status.busy":"2021-10-06T16:30:14.651794Z","iopub.execute_input":"2021-10-06T16:30:14.652144Z","iopub.status.idle":"2021-10-06T16:30:15.367641Z","shell.execute_reply.started":"2021-10-06T16:30:14.652105Z","shell.execute_reply":"2021-10-06T16:30:15.366566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_transform = A.Compose([\n    A.Resize(IMAGE_HEIGHT,IMAGE_WIDTH),\n    A.Rotate(limit=35,p=1.0),\n    A.HorizontalFlip(p=0.5),\n    A.VerticalFlip(p=0.1),\n    A.Normalize(\n        mean=[0.0,0.0,0.0],\n        std = [1.0,1.0,1.0],\n        max_pixel_value=255.0\n    ),\n    ToTensorV2()  \n])\nvalidation_transform = A.Compose([\n    A.Resize(IMAGE_HEIGHT,IMAGE_WIDTH),\n    A.Normalize(\n        mean = [0.0,0.0,0.0],\n        std = [1.0,1.0,1.0],\n        max_pixel_value=255.0,\n    ),\n    ToTensorV2()\n])","metadata":{"execution":{"iopub.status.busy":"2021-10-06T16:30:15.369406Z","iopub.execute_input":"2021-10-06T16:30:15.369971Z","iopub.status.idle":"2021-10-06T16:30:15.378857Z","shell.execute_reply.started":"2021-10-06T16:30:15.369926Z","shell.execute_reply":"2021-10-06T16:30:15.377837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_test_split(images,splitSize):\n    imageLen = len(images)\n    val_len = int(splitSize*imageLen)\n    train_len = imageLen - val_len\n    train_images,val_images = images[:train_len],images[train_len:]\n    return train_images,val_images","metadata":{"execution":{"iopub.status.busy":"2021-10-06T16:30:15.380587Z","iopub.execute_input":"2021-10-06T16:30:15.381285Z","iopub.status.idle":"2021-10-06T16:30:15.391369Z","shell.execute_reply.started":"2021-10-06T16:30:15.381232Z","shell.execute_reply":"2021-10-06T16:30:15.390186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_images_path,val_images_path = train_test_split(images,SPLIT)\ntrain_data = CarvanaDataset(train_images_path,TRAIN_IMG_DIR,TRAIN_MASK_DIR,train_transform,True)\nvalid_data = CarvanaDataset(val_images_path,TRAIN_IMG_DIR,TRAIN_MASK_DIR,validation_transform,True)\ntrain_dataloader = DataLoader(train_data,batch_size=BATCH_SIZE,shuffle=True)\nvalid_dataloader = DataLoader(valid_data,batch_size=BATCH_SIZE,shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2021-10-06T16:30:15.396789Z","iopub.execute_input":"2021-10-06T16:30:15.397073Z","iopub.status.idle":"2021-10-06T16:30:15.406274Z","shell.execute_reply.started":"2021-10-06T16:30:15.397035Z","shell.execute_reply":"2021-10-06T16:30:15.405093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loss = []\nval_loss =[]\nmodel = UNet().to(DEVICE)\noptimizer = optim.Adam(model.parameters(),lr=LEARNING_RATE)\ncriterion = nn.BCEWithLogitsLoss()\nfor epoch in range(EPOCHS):\n    print(f\"Epoch {epoch+1} of {EPOCHS}\")\n    train_epoch_loss = fit(model, train_dataloader, train_data,optimizer,criterion)\n    val_epoch_loss = validate(model, valid_dataloader, valid_data, criterion)\n    train_loss.append(train_epoch_loss)\n    val_loss.append(val_epoch_loss)\n    print(f\"Train Loss: {train_epoch_loss:.4f}\")\n    print(f'Val Loss: {val_epoch_loss:.4f}')\n\n# loss plots\nplt.figure(figsize=(10, 7))\nplt.plot(train_loss, color=\"orange\", label='train loss')\nplt.plot(val_loss, color=\"red\", label='validation loss')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend()\n# plt.savefig(f\"../input/loss.png\")\nplt.show()\ntorch.save({\n    'epoch': EPOCHS,\n    'model_state_dict': model.state_dict(),\n    'optimizer_state_dict': optimizer.state_dict(),\n    'loss': criterion,\n}, \"./model.pth\")\n\nprint(\"\\n---------DONE TRAINING----------\\n\")\n    ","metadata":{"execution":{"iopub.status.busy":"2021-10-06T16:30:15.810365Z","iopub.execute_input":"2021-10-06T16:30:15.810774Z","iopub.status.idle":"2021-10-06T18:12:16.803773Z","shell.execute_reply.started":"2021-10-06T16:30:15.810718Z","shell.execute_reply":"2021-10-06T18:12:16.80234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# torch.save({\n#     'epoch': EPOCHS,\n#     'model_state_dict': model.state_dict(),\n#     'optimizer_state_dict': optimizer.state_dict(),\n#     'loss': criterion,\n# }, \"./model.pth\")","metadata":{"execution":{"iopub.status.busy":"2021-10-06T18:15:15.875071Z","iopub.execute_input":"2021-10-06T18:15:15.875347Z","iopub.status.idle":"2021-10-06T18:15:16.697432Z","shell.execute_reply.started":"2021-10-06T18:15:15.875319Z","shell.execute_reply":"2021-10-06T18:15:16.696348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = train_data.__getitem__(100)\nplt.imshow(data['mask'],cmap=\"gray\")\nprint(train_data.__getitem__(0)['mask'].shape)","metadata":{"execution":{"iopub.status.busy":"2021-10-06T18:31:18.566015Z","iopub.execute_input":"2021-10-06T18:31:18.567159Z","iopub.status.idle":"2021-10-06T18:31:19.114724Z","shell.execute_reply.started":"2021-10-06T18:31:18.567111Z","shell.execute_reply":"2021-10-06T18:31:19.11375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for Testing on Single datapoint after training\n# plt.imshow(np.transpose(np.array(data['image']),(1,2,0)),cmap=\"gray\")\nprint(data['image'].shape)\nimg = data['image'].unsqueeze(0).to(device=\"cuda\")\n# model = UNet()\noutput = model(img)\noutput = torch.squeeze(output)\noutput[output>0.0] = 1.0\noutput[output<=0.0]=0\nprint(torch.max(output))\nprint(output.shape)\ndisp = output.detach().cpu()\nplt.imshow(disp,cmap=\"gray\")","metadata":{"execution":{"iopub.status.busy":"2021-10-06T18:31:23.897492Z","iopub.execute_input":"2021-10-06T18:31:23.897838Z","iopub.status.idle":"2021-10-06T18:31:24.251529Z","shell.execute_reply.started":"2021-10-06T18:31:23.897807Z","shell.execute_reply":"2021-10-06T18:31:24.250478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generating submission file\nimport zipfile\n# 'test.zip'\ndirs = ['test.zip']\nfor x in dirs:\n    with zipfile.ZipFile(\"../input/carvana-image-masking-challenge/\"+ x,'r') as z:\n        z.extractall(\".\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run_length_encode(mask):\n    inds = mask.flatten()\n#     print(inds)\n    runs = np.where(inds[1:]!=inds[:-1])[0]+2\n    runs[1::2] = runs[1::2]-runs[:-1:2]\n    rle = ' '.join([str(r) for r bin runs])\n    return rle","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_names_df = pd.read_csv(DATAPATH + 'sample_submission.csv.zip')\nimage_names = image_names_df.iloc[:,0].values","metadata":{"execution":{"iopub.status.busy":"2021-10-11T08:58:54.701604Z","iopub.execute_input":"2021-10-11T08:58:54.702146Z","iopub.status.idle":"2021-10-11T08:58:54.772958Z","shell.execute_reply.started":"2021-10-11T08:58:54.702109Z","shell.execute_reply":"2021-10-11T08:58:54.77208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_transform = A.Compose([\n    A.Resize(IMAGE_HEIGHT,IMAGE_WIDTH),\n    A.Normalize(\n        mean = [0.0,0.0,0.0],\n        std = [1.0,1.0,1.0],\n        max_pixel_value=255.0,\n    ),\n    ToTensorV2()\n])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data = CarvanaDataset(image_dir=test_dir,mask_dir=None,transform=test_transform,train=False,image_names=image_names)\ntest_dataloader = DataLoader(test_data,batch_size=1,shuffle=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_submission_file(model,dataloader,data):\n    print('----------Generating Preds-----------')\n    model.eval()\n    rles = []\n    names=[]\n    num_batches = int(len(data)/dataloader.batch_size)\n    with torch.no_grad():\n        for i,data in tqdm(enumerate(dataloader),total=num_batches):\n            image,name = data['image'].to(DEVICE),data[\"name\"]\n            out = model(image)\n            out = out.squeeze()\n            out = np.array(out.detach().cpu())\n            out = cv2.resize(out,(ORIG_WIDTH,ORIG_HEIGHT))\n            out[out > 0.0] = 1.0\n            out[out <= 0.0] = 0.0\n            rle = run_length_encode(out)\n            rles.append(rle)\n            names.append(name)\n        print(\"------------Generating Submission File---------\")\n        df = pd.DataFrame({\"img\":names,\"rle_mask\":rles})\n        df.to_csv('./submission.csv.gz',index=False,compression='gzip')\n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_submission_file(model,test_dataloader,test_data)","metadata":{},"execution_count":null,"outputs":[]}]}