{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os , glob\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport cv2\nfrom tqdm import tqdm\n\nimport torch\nfrom torch import Tensor\nfrom torch.autograd import Function\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nfrom keras.preprocessing import image\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nfrom torch.utils.data import Dataset , DataLoader\nfrom torchvision import transforms , utils , datasets\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-12T11:42:16.053715Z","iopub.execute_input":"2021-08-12T11:42:16.054027Z","iopub.status.idle":"2021-08-12T11:42:23.754467Z","shell.execute_reply.started":"2021-08-12T11:42:16.053952Z","shell.execute_reply":"2021-08-12T11:42:23.753634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Unzipping files","metadata":{}},{"cell_type":"code","source":"!unzip '/kaggle/input/carvana-image-masking-challenge/train_hq.zip' -d './'\n!unzip '/kaggle/input/carvana-image-masking-challenge/train_masks.zip' -d './'\n!unzip '/kaggle/input/carvana-image-masking-challenge/train.zip' -d './'\n!unzip '/kaggle/input/carvana-image-masking-challenge/train_masks.csv.zip' -d './'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"pth = './train'\n# os.listdir('./train_hq')\n# len(os.listdir(pth))\ndata = pd.read_csv('train_masks.csv')\ndata","metadata":{"execution":{"iopub.status.busy":"2021-08-12T11:42:51.94751Z","iopub.execute_input":"2021-08-12T11:42:51.947806Z","iopub.status.idle":"2021-08-12T11:42:52.331394Z","shell.execute_reply.started":"2021-08-12T11:42:51.947776Z","shell.execute_reply":"2021-08-12T11:42:52.330407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig=plt.figure(figsize=(15, 15))\nimg=cv2.imread('./train/fff9b3a5373f_12.jpg')\nmask=Image.open('./train_masks/fff9b3a5373f_12_mask.gif') #Image from PIL \nprint(img.shape)\nfiles=[img,mask]\nfor i in range(len(files)):\n    plt.subplot(1, 2 , i+1)\n    plt.imshow(files[i])","metadata":{"execution":{"iopub.status.busy":"2021-08-12T11:42:52.333112Z","iopub.execute_input":"2021-08-12T11:42:52.333454Z","iopub.status.idle":"2021-08-12T11:42:53.483002Z","shell.execute_reply.started":"2021-08-12T11:42:52.333418Z","shell.execute_reply":"2021-08-12T11:42:53.482099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"root_dir = './train'\nimages = pd.read_csv('./train_masks.csv')['img']\nlen(images) # contains all images\nimg_list = images","metadata":{"execution":{"iopub.status.busy":"2021-08-12T11:42:53.484152Z","iopub.execute_input":"2021-08-12T11:42:53.484458Z","iopub.status.idle":"2021-08-12T11:42:53.836281Z","shell.execute_reply.started":"2021-08-12T11:42:53.484426Z","shell.execute_reply":"2021-08-12T11:42:53.835424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# testing some path and data\nimg_name = img_list[2]\nimg_path = root_dir + '/' + str(img_name)\nimg2 = os.path.join(root_dir,img_name)\nprint(img_path)\nprint(img2)\n\nmask_name = img_name.split('.')[0] + '_mask.gif'\nmask_path = root_dir + '_masks/' + str(mask_name)\nprint(mask_path)\n\nimg = cv2.imread(img_path)\n# print(plt.imshow(img))\n\nmask = plt.imread(mask_path) # for masks\n# plt.imshow(mask)\n\nfig=plt.figure(figsize=(15, 15))\nfiles=[img,mask]\nfor i in range(len(files)):\n    plt.subplot(1, 2 , i+1)\n    plt.imshow(files[i])","metadata":{"execution":{"iopub.status.busy":"2021-08-12T11:42:53.837614Z","iopub.execute_input":"2021-08-12T11:42:53.837993Z","iopub.status.idle":"2021-08-12T11:42:54.709893Z","shell.execute_reply.started":"2021-08-12T11:42:53.837947Z","shell.execute_reply":"2021-08-12T11:42:54.708921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Custom Dataset Class\nclass CaravanDataset(Dataset) :\n    \n    def __init__(self,root_dir,img_list,transform_img=None,transform_m=None) :\n        \n        self.root_dir = root_dir\n        self.image_list = img_list\n        self.transform_img = transform_img\n        self.transform_m = transform_m\n    \n    def __len__(self) :\n        return len(self.image_list)\n    \n    def __getitem__(self,idx) :\n        \n        img_name = self.image_list[idx]\n        img_path = self.root_dir +'/'+ str(img_name)\n        \n        mask_name = img_name.split('.')[0] + '_mask.gif'\n        mask_path = self.root_dir +'_masks/'+ str(mask_name)\n        \n        img = cv2.imread(img_path)\n        mask = Image.open(mask_path)    #Object of PIL \n        mask = image.img_to_array(mask)\n        \n\n        \n        if self.transform_img :\n            img = self.transform_img(image=img)['image']\n            \n        if self.transform_m :\n            mask = self.transform_m(image=mask)['image']\n        \n        return img,mask","metadata":{"execution":{"iopub.status.busy":"2021-08-12T11:42:54.711394Z","iopub.execute_input":"2021-08-12T11:42:54.711752Z","iopub.status.idle":"2021-08-12T11:42:54.721043Z","shell.execute_reply.started":"2021-08-12T11:42:54.711715Z","shell.execute_reply":"2021-08-12T11:42:54.72013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Transforms\n\ntransform_img = A.Compose([\n    A.Resize(600,600),\n    A.Normalize(\n    mean=[0.485, 0.456, 0.406],\n    std=[0.229, 0.224, 0.225]),\n    ToTensorV2()])\n\ntransform_mask = A.Compose([\n    A.Resize(600,600),ToTensorV2()])\n\nimages = (pd.read_csv('./train_masks.csv')['img'])\ndataset = CaravanDataset('./train',images,transform_img,transform_mask)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T11:42:54.723848Z","iopub.execute_input":"2021-08-12T11:42:54.724199Z","iopub.status.idle":"2021-08-12T11:42:55.066512Z","shell.execute_reply.started":"2021-08-12T11:42:54.724164Z","shell.execute_reply":"2021-08-12T11:42:55.0657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# testing dataset\ni = 0\nfor x,y in dataset:\n    if i == 5: break\n    print(x.shape,y.shape)\n    i+=1","metadata":{"execution":{"iopub.status.busy":"2021-08-12T11:42:55.068181Z","iopub.execute_input":"2021-08-12T11:42:55.068537Z","iopub.status.idle":"2021-08-12T11:42:55.361274Z","shell.execute_reply.started":"2021-08-12T11:42:55.068501Z","shell.execute_reply":"2021-08-12T11:42:55.360469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split into Train and Validation Set\ntrain_size = int(len(images)*0.8)\nval_size = len(images)-train_size\nprint(train_size,val_size)\n\ntrain_set, val_set = torch.utils.data.random_split(dataset, [train_size, val_size]) # We sample 10% of the images as a validation dataset\n\ntrain_dataloader = torch.utils.data.DataLoader(train_set,\n                                          batch_size=4,\n                                          shuffle=True)\n\nvalid_dataloader = torch.utils.data.DataLoader(val_set,\n                                          batch_size=4,\n                                          shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T11:42:55.362519Z","iopub.execute_input":"2021-08-12T11:42:55.362883Z","iopub.status.idle":"2021-08-12T11:42:56.464536Z","shell.execute_reply.started":"2021-08-12T11:42:55.362834Z","shell.execute_reply":"2021-08-12T11:42:56.463636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Testing TrainLoader\nexamples = enumerate(train_dataloader)\nbatch_idx, (example_data, example_targets) = next(examples)\nexample_data.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-12T11:42:56.46579Z","iopub.execute_input":"2021-08-12T11:42:56.466278Z","iopub.status.idle":"2021-08-12T11:42:56.862407Z","shell.execute_reply.started":"2021-08-12T11:42:56.466239Z","shell.execute_reply":"2021-08-12T11:42:56.861645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modified UNet Network","metadata":{}},{"cell_type":"code","source":"class DoubleConv(nn.Module):\n\n    def __init__(self, in_channels, out_channels, mid_channels=None):\n        \n        super().__init__()\n        \n        if not mid_channels:\n            mid_channels = out_channels\n        \n        self.double_conv = nn.Sequential(\n            \n            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(mid_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        return self.double_conv(x)\n    \n    \n    \nclass Down(nn.Module):\n\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        \n        self.maxpool_conv = nn.Sequential(\n            nn.MaxPool2d(2),\n            DoubleConv(in_channels, out_channels)\n        )\n\n    def forward(self, x):\n        return self.maxpool_conv(x)\n    \n    \nclass Up(nn.Module):\n\n    def __init__(self, in_channels, out_channels, bilinear=True):\n        super().__init__()\n\n        # if bilinear, use the normal convolutions to reduce the number of channels\n        if bilinear:\n            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n        else:\n            self.up = nn.ConvTranspose2d(in_channels , in_channels // 2, kernel_size=2, stride=2)\n            self.conv = DoubleConv(in_channels, out_channels)\n\n\n    def forward(self, x1, x2):\n        x1 = self.up(x1)\n        # input is CHW\n        diffY = x2.size()[2] - x1.size()[2]\n        diffX = x2.size()[3] - x1.size()[3]\n\n        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n                        diffY // 2, diffY - diffY // 2])\n        # if you have padding issues, see\n        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n        x = torch.cat([x2, x1], dim=1)\n        return self.conv(x)\n\n\nclass OutConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(OutConv, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n\n    def forward(self, x):\n        return self.conv(x)\n    \n    \n    \nclass UNet(nn.Module):\n    def __init__(self, n_channels, n_classes, bilinear=True):\n        super(UNet, self).__init__()\n        self.n_channels = n_channels\n        self.n_classes = n_classes\n        self.bilinear = bilinear\n\n        self.inc = DoubleConv(n_channels, 64)\n        self.down1 = Down(64, 128)\n        self.down2 = Down(128, 256)\n        self.down3 = Down(256, 512)\n        factor = 2 if bilinear else 1\n        self.down4 = Down(512, 1024 // factor)\n        self.up1 = Up(1024, 512 // factor, bilinear)\n        self.up2 = Up(512, 256 // factor, bilinear)\n        self.up3 = Up(256, 128 // factor, bilinear)\n        self.up4 = Up(128, 64, bilinear)\n        self.outc = OutConv(64, n_classes)\n\n    def forward(self, x):\n        x1 = self.inc(x)\n        x2 = self.down1(x1)\n        x3 = self.down2(x2)\n        x4 = self.down3(x3)\n        x5 = self.down4(x4)\n        x = self.up1(x5, x4)\n        x = self.up2(x, x3)\n        x = self.up3(x, x2)\n        x = self.up4(x, x1)\n        logits = self.outc(x)\n        return logits\n","metadata":{"execution":{"iopub.status.busy":"2021-08-12T11:42:56.863761Z","iopub.execute_input":"2021-08-12T11:42:56.8641Z","iopub.status.idle":"2021-08-12T11:42:56.884824Z","shell.execute_reply.started":"2021-08-12T11:42:56.864064Z","shell.execute_reply":"2021-08-12T11:42:56.883666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import torch\n# import torch.nn as nn  \n\n# # print(torch.cuda.is_available())\n\n# def double_conv(in_c,out_c):\n#     conv = nn.Sequential(\n#         nn.Conv2d(in_c, out_c,kernel_size = 3),\n#         nn.ReLU(inplace=True),\n\n#         nn.Conv2d(out_c, out_c,kernel_size = 3),\n#         nn.ReLU(inplace=True)\n#     )\n#     return conv\n\n# def crop_image(tensor,target):\n#     target_size = target.size()[2]\n#     tensor_size = tensor.size()[2]\n\n#     diff = tensor_size - target_size\n#     diff = diff // 2\n\n#     return tensor[:,:,diff:tensor_size-diff,diff:tensor_size-diff]\n\n\n\n# class UNet(nn.Module):\n#     def __init__(self):\n#         super(UNet, self).__init__()\n\n#         self.max_pool_2x2 = nn.MaxPool2d(kernel_size=2, stride=2)\n\n#         self.down_conv1 = double_conv(3,64)\n#         self.down_conv2 = double_conv(64,128)\n#         self.down_conv3 = double_conv(128,256)\n#         self.down_conv4 = double_conv(256,512)\n#         self.down_conv5 = double_conv(512,1024)\n\n#         self.up_trans1 = nn.ConvTranspose2d(in_channels=1024, out_channels=512, kernel_size=2, stride=2)\n#         self.up_conv1 = double_conv(1024,512)\n\n#         self.up_trans2 = nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=2, stride=2)\n#         self.up_conv2 = double_conv(512,256)\n\n#         self.up_trans3 = nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=2, stride=2)\n#         self.up_conv3= double_conv(256,128)\n\n#         self.up_trans4 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=2, stride=2)\n#         self.up_conv4 = double_conv(128,64)\n\n#         self.out = nn.Conv2d(in_channels=64, out_channels=2, kernel_size=1)\n\n\n\n\n#     def forward(self, image):\n#         #encoder\n#         x1 = self.down_conv1(image) #op\n#         x2 = self.max_pool_2x2(x1)\n\n#         x3 = self.down_conv2(x2) #op\n#         x4 = self.max_pool_2x2(x3)\n\n#         x5 = self.down_conv3(x4) #op\n#         x6 = self.max_pool_2x2(x5)\n\n#         x7 = self.down_conv4(x6) #op\n#         x8 = self.max_pool_2x2(x7)\n\n#         x9 = self.down_conv5(x8) #op\n\n#         #decoder\n#         x = self.up_trans1(x9)\n#         y = crop_image(x7, x)\n#         x =  self.up_conv1(torch.cat([x,y],1))\n\n#         x = self.up_trans2(x)\n#         y = crop_image(x5, x)\n#         x =  self.up_conv2(torch.cat([x,y],1))\n\n#         x = self.up_trans3(x)\n#         y = crop_image(x3, x)\n#         x =  self.up_conv3(torch.cat([x,y],1))\n\n#         x = self.up_trans4(x)\n#         y = crop_image(x1, x)\n#         x =  self.up_conv4(torch.cat([x,y],1))\n\n#         x = self.out(x)\n\n\n#         print(x1.size())\n#         print(x3.size())\n#         print(x5.size())\n#         print(x7.size())\n#         print(x9.size())\n\n#         # print(x.size())\n#         print(y.size())\n\n#         print(x.size())\n        \n#         return x\n#         # check x7 and x size. so either 1.crop 2.padding\n\n#         # batch size, channel, height, width\n# if __name__ == \"__main__\":\n#     image = torch.rand((1,1,572,572))\n#     model = UNet()\n# #     print(model)\n# #     print(model(image)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T11:42:56.886296Z","iopub.execute_input":"2021-08-12T11:42:56.886696Z","iopub.status.idle":"2021-08-12T11:42:56.896314Z","shell.execute_reply.started":"2021-08-12T11:42:56.886646Z","shell.execute_reply":"2021-08-12T11:42:56.895516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def dice_calc(gt,pred) :\n    pred = torch.sigmoid(pred)\n    pred = ((pred) >= .5).float()\n    dice_score = (2 * (pred * gt).sum()) / ((pred + gt).sum() + 1e-8)\n    \n    return dice_score","metadata":{"execution":{"iopub.status.busy":"2021-08-12T11:42:56.897326Z","iopub.execute_input":"2021-08-12T11:42:56.897615Z","iopub.status.idle":"2021-08-12T11:42:56.908527Z","shell.execute_reply.started":"2021-08-12T11:42:56.897591Z","shell.execute_reply":"2021-08-12T11:42:56.907752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# net = UNet()\n# net.to(device=device)\n\n# optimizer = optim.RMSprop(net.parameters(), lr=0.0001, weight_decay=1e-8, momentum=0.9)\n\n# criterion = nn.CrossEntropyLoss()","metadata":{"execution":{"iopub.status.busy":"2021-08-12T11:42:56.909838Z","iopub.execute_input":"2021-08-12T11:42:56.910299Z","iopub.status.idle":"2021-08-12T11:42:56.917167Z","shell.execute_reply.started":"2021-08-12T11:42:56.910171Z","shell.execute_reply":"2021-08-12T11:42:56.916324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"net = UNet(n_channels=3, n_classes=1, bilinear=True)\nnet.to(device=device)\n\noptimizer = optim.RMSprop(net.parameters(), lr=0.0001, weight_decay=1e-8, momentum=0.9)\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min' if net.n_classes > 1 else 'max', patience=2)\n\nif net.n_classes > 1:\n    criterion = nn.CrossEntropyLoss()\nelse:\n    criterion = nn.BCEWithLogitsLoss()","metadata":{"execution":{"iopub.status.busy":"2021-08-12T11:42:56.918107Z","iopub.execute_input":"2021-08-12T11:42:56.918366Z","iopub.status.idle":"2021-08-12T11:43:02.624936Z","shell.execute_reply.started":"2021-08-12T11:42:56.918344Z","shell.execute_reply":"2021-08-12T11:43:02.624083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train\ndef train(epoch,epochs,tloader) :\n    net.train()\n    \n    tloader.set_description(f'EPOCH {epoch}')\n    epoch_loss = 0\n    dice_score = 0\n    num_correct = 0    \n    \n    for images , masks in tloader :\n \n        optimizer.zero_grad()\n        images = images.to(device, dtype=torch.float32)\n        masks  = masks.to(device, dtype=torch.float32)\n        mask_pred = net(images)\n\n        loss = criterion(mask_pred,masks)\n        epoch_loss += loss.item()      \n        loss.backward()\n        optimizer.step()\n    \n        running_DS = dice_calc(masks,mask_pred)\n        dice_score += running_DS\n                \n        tloader.set_postfix(loss=loss.item(),accuracy=(running_DS.item()))\n    print(' Train Dice Score Epoch : ',dice_score/len(train_dataloader))\n    \ndef validation(vloader) :\n    net.eval()\n    vloader.set_description('Validation')\n    \n    n_val = len(valid_dataloader)\n    total = 0\n    dice_score = 0\n    num_correct = 0\n    \n    with torch.no_grad():\n        for images ,masks in vloader :\n\n            images = images.to(device)\n            masks  = masks.to(device)\n                \n            mask_pred = net(images)\n            \n            loss = criterion(mask_pred,masks)\n            \n            running_DS = dice_calc(masks,mask_pred)\n            dice_score += running_DS\n            \n            vloader.set_postfix(loss=loss.item(),accuracy=(running_DS.item()))\n        \n    print('Validation Dice Score Epoch : ',dice_score/len(valid_dataloader))","metadata":{"execution":{"iopub.status.busy":"2021-08-12T11:43:02.626166Z","iopub.execute_input":"2021-08-12T11:43:02.626504Z","iopub.status.idle":"2021-08-12T11:43:02.639595Z","shell.execute_reply.started":"2021-08-12T11:43:02.62647Z","shell.execute_reply":"2021-08-12T11:43:02.638816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 3\n\nfor epoch in range(epochs) :\n    print(epoch+1,'/',epochs)\n    with tqdm(train_dataloader,unit='batch') as tloader : \n        train(epoch,epochs,tloader)\n    \n    with tqdm(valid_dataloader,unit='batch') as vloader:\n        validation(vloader)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T11:48:22.301028Z","iopub.execute_input":"2021-08-12T11:48:22.301349Z","iopub.status.idle":"2021-08-12T12:55:54.316414Z","shell.execute_reply.started":"2021-08-12T11:48:22.301318Z","shell.execute_reply":"2021-08-12T12:55:54.313209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import torch\n# torch.save(net.state_dict(), \"net_model.pt\")","metadata":{"execution":{"iopub.status.busy":"2021-08-12T13:50:27.740871Z","iopub.execute_input":"2021-08-12T13:50:27.741233Z","iopub.status.idle":"2021-08-12T13:50:27.745079Z","shell.execute_reply.started":"2021-08-12T13:50:27.741201Z","shell.execute_reply":"2021-08-12T13:50:27.744128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing","metadata":{}},{"cell_type":"code","source":"# Tests\nwith torch.no_grad():\n    for images ,masks in valid_dataloader :\n        images = images.to(device)\n        masks  = masks.to(device)\n\n        mask_pred = net(images)\n\n        img = mask_pred.cpu().numpy()\n        masks = masks.cpu().numpy()\n        masks_2 = (masks > 0.5).astype(int)\n        \n        fig, axes = plt.subplots(1, 3, figsize=(15, 15))\n        \n        axes[0].imshow(masks[0][0])\n        axes[0].set_title('Ground Truth Mask')\n        \n        axes[1].imshow(img[0][0])\n        axes[1].set_title('Probabilistic Mask')\n        \n        axes[2].imshow(masks_2[0][0])\n        axes[2].set_title('Probabilistic Mask threshold')\n        break\n\n","metadata":{"execution":{"iopub.status.busy":"2021-08-12T12:56:51.102864Z","iopub.execute_input":"2021-08-12T12:56:51.103194Z","iopub.status.idle":"2021-08-12T12:56:52.084174Z","shell.execute_reply.started":"2021-08-12T12:56:51.103163Z","shell.execute_reply":"2021-08-12T12:56:52.083363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tests\n\nfor i in range(5):\n    with torch.no_grad():\n        for images ,masks in valid_dataloader :\n            images = images.to(device)\n            masks  = masks.to(device)\n\n            mask_pred = net(images)\n\n            img = mask_pred.cpu().numpy()\n            masks = masks.cpu().numpy()\n            masks_2 = (masks > 0.5).astype(int)\n\n            fig, axes = plt.subplots(1, 3, figsize=(15, 15))\n\n            axes[0].imshow(masks[0][0])\n            axes[0].set_title('Ground Truth Mask')\n\n            axes[1].imshow(img[0][0])\n            axes[1].set_title('Probabilistic Mask')\n\n            axes[2].imshow(masks_2[0][0])\n            axes[2].set_title('Probabilistic Mask threshold')\n            \n            break\n\n","metadata":{"execution":{"iopub.status.busy":"2021-08-12T13:06:43.762718Z","iopub.execute_input":"2021-08-12T13:06:43.763075Z","iopub.status.idle":"2021-08-12T13:06:48.96255Z","shell.execute_reply.started":"2021-08-12T13:06:43.763043Z","shell.execute_reply":"2021-08-12T13:06:48.961561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}