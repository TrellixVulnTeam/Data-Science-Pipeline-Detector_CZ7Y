{"cells":[{"metadata":{},"cell_type":"markdown","source":"In this notebook, we are trying to predict 30 keypoints positions for facial images.\n\nWe have 4 files: \n\n        * 'training.zip', used for training the model\n        * 'test.zip' used to predict the keypoints\n        * 'IdLookupTable.csv' which helps to create submission file\n        * 'SampleSubmission.csv' an example of a submission file\n\nThe main challenge in this competition is to deal with a big amount of missing keypoints, which is crucial to be solved before to start with the training.\n\nTo overcome this issue, we used a convolutional neural network (CNN) trained on the data set with data augmentation techniques."},{"metadata":{},"cell_type":"markdown","source":"# Imports"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport skimage.transform as sk\nfrom sklearn.model_selection import train_test_split\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom keras.models import Sequential, load_model \nfrom keras.layers import Activation, Convolution2D, MaxPooling2D, BatchNormalization, Flatten, Dense, Dropout, SeparableConv2D, MaxPool2D\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, Callback\nfrom keras.optimizers import Adam\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prepare functions and paths"},{"metadata":{"trusted":true},"cell_type":"code","source":"# set up path to files\ndir_path = os.path.join('../input', os.listdir('../input')[0])\nprint(\"Files are :\", os.listdir(dir_path))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def load_data(dir_path, filename):#, cols=None):\n\n    df = pd.read_csv(os.path.join(dir_path, filename))\n\n    # The Image column has pixel values saved as strings separated by a space\n    if filename not in ['IdLookupTable.csv','SampleSubmission.csv']:\n        df['Image'] = df['Image'].apply(lambda img: np.fromstring(img, sep=' '))\n\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def summary(df, test=False):\n    \n    dataname = 'test' if test else 'train'  \n    \n    print(\"Length of %s data %d..\\n\" % (dataname, len(df)))\n    print(\"Count values for each variable:\\n \")\n    print(df.count(), '\\n')\n    print(\"How many variables has missing values ?\\n \")\n    print( df.isnull().any().value_counts())\n    print()\n    print(\"Pourcentage of missing values for each variable: \\n\")\n    summary_list =[100 - df[c].count()/len(df)*100 for c in df.columns]\n    var_list = [var for var in df.columns]\n\n    for i in range(len(summary_list)):\n        print(\"{} : {}\".format(var_list[i], np.round(summary_list[i]),2))\n        print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_mean_cols(df):\n    \"\"\"\n    return mean over means of variables for a df\n    \"\"\"\n    df_mean_cols = df[df.columns[:-1]].mean(axis = 0, skipna = True).reset_index()\n    mean_cols = df_mean_cols[0].mean(axis = 0, skipna = True)\n    \n    return round(mean_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def scale_data(df, test=False, random_state=42):\n    \"\"\"\n    Scale pixel values to [0,1] for train and test set\n    Scale keypoints to [-1, 1], only for train set, as there is no target values for test set\n    test: True, if test, otherwise train\n    \"\"\"\n    img = np.vstack(df['Image'].values) / 255. # scale Image range from [0, 255] to [0, 1]\n    img = img.astype(np.float32) # change data type to float\n    mean_cols = get_mean_cols(df) # get mean over variables\n    \n    if not test:\n        keypoints = df[df.columns[:-1]].values # scale keypoints to be centered around 0 with a range of [-1, 1]\n        keypoints = (keypoints - mean_cols) / mean_cols  # scale keypoints to [-1, 1]\n        keypoints = keypoints.astype(np.float32) # change keypoints type to float\n    else:\n        keypoints = None\n        \n    return img, keypoints, mean_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def reshape_data(df,width=96, height=96):\n    \"\"\"\n    Reshape data to (len_df, 96, 96, ?) by default\n    \"\"\"\n    return df.reshape(df.shape[0], width, height, -1) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def _random_indices(inputs, ratio, random_state=1234):\n    \"\"\"Generate random unique indices according to ratio\"\"\"\n    np.random.seed(random_state);\n    actual_batchsize = inputs.shape[0]\n    size = int(actual_batchsize * ratio)\n    indices = np.random.choice(actual_batchsize, size, replace=False)\n    return indices","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def rotate(y, inputs, targets, rotate_ratio, angle= None, right_left = 0):\n    \"\"\"Rotate slighly the image and the targets. Works only with one channel\"\"\"\n    if angle is None:\n        angle = np.random.randint(10)\n    if right_left != 0:\n        angle =  360 - angle\n    for i in range(inputs.shape[0]):\n        inputs[i, :, :, 0] = sk.rotate(inputs[i, :, :, 0], angle)\n    angle = np.radians(angle)\n    indices = np.arange(targets.shape[0])\n    R = np.array([[np.cos(angle), -np.sin(angle)], [np.sin(angle), np.cos(angle)]])\n    targets = targets.reshape(len(targets), y.shape[1] // 2, 2)\n    targets[indices] = np.dot(targets[indices], R)\n    targets = targets.reshape(len(targets), y.shape[1])\n    \n    return inputs, targets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def flipp(inputs, targets, flip_ratio, flip_indices= None, random_seed=123):\n    \"\"\"Flip image\"\"\"\n    if flip_indices is None:\n        flip_indices = [ (0, 2), (1, 3), (4, 8), (5, 9), (6, 10), (7, 11),\n                        (12, 16), (13, 17), (14, 18), (15, 19), (22, 24),\n                        (23, 25) ]\n    for i in range(inputs.shape[0]):\n        inputs[i, :, :, :] = inputs[i, :, ::-1, :]\n    indices = np.arange(inputs.shape[0])\n    targets[indices, ::2] = targets[indices, ::2] * -1\n    for a, b in flip_indices:\n        targets[indices, a], targets[indices, b] = targets[indices, b], targets[indices, a]\n    return inputs, targets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# idea from : https://www.kaggle.com/balraj98/data-augmentation-for-facial-keypoint-detection\ndef brightness(inputs):\n    \"\"\" Contrast jittering (reduction)\"\"\"\n    in_brightness_images = np.clip(inputs*1.6, 0.0, 1.0)\n    \n    return in_brightness_images","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def random_shift(shift_range,n=96):\n    '''\n    shift_range: \n    The maximum number of columns/rows to shift\n    return: \n    keep(0):   minimum row/column index to keep\n    keep(1):   maximum row/column index to keep\n    assign(0): minimum row/column index to assign\n    assign(1): maximum row/column index to assign\n    shift:     amount to shift the landmark\n\n    assign(1) - assign(0) == keep(1) - keep(0)\n    '''\n    shift = np.random.randint(-shift_range, shift_range)\n    def shift_left(n,shift):\n        shift = np.abs(shift)\n        return(0,n - shift)\n    def shift_right(n,shift):\n        shift = np.abs(shift)\n        return(shift,n)\n\n    if shift < 0:\n        keep = shift_left(n,shift) \n        assign = shift_right(n,shift)\n    else:\n        assign = shift_left(n,shift) ## less than 96\n        keep = shift_right(n,shift)\n\n    return((keep,  assign, shift))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def shift_single_image(x_,y_,w=96, h=96, prop=0.1):\n    '''\n    x_: a single picture array (96, 96, 1)\n    y_: 15 landmark locations \n               [0::2] contains x axis values\n               [1::2] contains y axis values \n    prop: proportion of random horizontal and vertical shift\n          relative to the number of columns\n    '''\n    w_shift_max = int(w * prop)\n    h_shift_max = int(h * prop)\n\n    w_keep,w_assign,w_shift = random_shift(w_shift_max)\n    h_keep,h_assign,h_shift = random_shift(h_shift_max)\n    \n    x_new = np.ones(x_.shape)\n    y_new = np.ones(y_.shape)\n    \n    x_new[w_assign[0]:w_assign[1], h_assign[0]:h_assign[1]] = x_[w_keep[0]:w_keep[1], h_keep[0]:h_keep[1]]\n\n    y_new[0::2] = y_[0::2] - h_shift/float(w/2.)\n    y_new[1::2] = y_[1::2] - w_shift/float(h/2.)\n    return(x_new,y_new)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def shift_image(X,y,prop=0.1):\n    X = X.reshape(-1,96,96)\n    y = y.reshape(-1,30)\n    for i in range(X.shape[0]):\n        x_ = X[i]\n        y_ = y[i]\n        X[i],y[i] = shift_single_image(x_,y_,prop=prop)\n    return(X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_noise(inputs, noise_ratio=0.001):\n    noisy_img = np.zeros(inputs.shape)\n    for i in range(inputs.shape[0]):\n        noise = np.random.randn(96,96,1)\n        noisy_img[i] = inputs[i] + noise_ratio*noise\n    return noisy_img","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_img(df, keypoints, mean_cols, title=\"\", add_keypoints=False ,num_img=None, rand_img=False):\n    \"\"\"\n    Plots different images randomly selected from df\n    num_img : numbre of images to select\n    add_keypoints = True if you want to add keypoints to each image\n    \"\"\"\n    if num_img is None:\n        num_img = df.shape[0]\n        \n    if rand_img:\n        list_img = list(np.random.choice(np.arange(0,df.shape[0]), num_img, replace=False)) # select random num_img index\n    else:\n        list_img = np.arange(num_img)\n    \n    fig = plt.figure(figsize=(12, 12))\n    fig.suptitle(title, fontsize='x-large')\n    fig.subplots_adjust(left=0, right=1, bottom=0, top=0.95, hspace=0.05, wspace=0.05)  \n    j = 0\n    for i in list_img: # i index for image selected\n        axis = fig.add_subplot(4, 4, j + 1, xticks=[], yticks=[])\n        img = plt.imshow(df[i].reshape(96,96), cmap='gray')\n        # as we scaled keypoints to [-1,1], we have to retrieve the real values by *mean_cols*\n        if add_keypoints:\n            axis.scatter(keypoints[i][0::2]*mean_cols + mean_cols, keypoints[i][1::2]*mean_cols + mean_cols, marker='x', s=10)\n        j += 1\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def split_train_validation(X, y, size=0.3, random_state = 69):\n    \n    X_train, X_validation, y_train, y_validation = train_test_split(X, y,  test_size=size, random_state=random_state)\n    print(\"Splitting data into train {} and validation {}\".format(X_train.shape, X_validation.shape))\n    return X_train, X_validation, y_train, y_validation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_loss(hist,name,plt):\n\n    loss = hist['loss']\n    val_loss = hist['val_loss']\n        \n    plt.plot(loss,\"--\", linewidth=3,label=\"Train:\" + name)\n    plt.plot(val_loss, linewidth=3,label=\"Validation:\" + name)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = load_data(dir_path, 'training.zip')\ntest_data = load_data(dir_path, 'test.zip')\nidlookup = load_data(dir_path, 'IdLookupTable.csv')\nsample_submission = load_data(dir_path,'SampleSubmission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Explore data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head().T # transpose is a simpler way to see how our training set looks like","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can observe that our 30 first variables represent the key points, and the last column contains the image. It would be better to save these seperatly."},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"summary(train_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"summary(test_data, test=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"summary(idlookup)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that in the training set there is 68% of missing data for 28 keypoints."},{"metadata":{},"cell_type":"markdown","source":"## How missing data can be handled ?\n\nMissing values can be handled with several techniques:\n\n* Drop each row which has a considerable percentage of missing values\n* Fill missing values\n* Drop these rows and use data augmentation to retrieve the same data size\n\nWe will treat each technique independently, because each one has its advantages and drawbacks. In fact, in our case, it is not appropriate to drop all rows that have missing values because we will loss approximately 70% of the information, which would make the training harder.\n\nThe second technique is filling missing values that have the drawback of creating a biased data set.\n\nThe third technique, data augmentation was proven to be robust and effective, but it still doesn’t work with all neural nets.\n\nWe will create 3 new datasets, each one is associated with one of the techniques detailed above."},{"metadata":{"trusted":true},"cell_type":"code","source":"# first of all we will create a copy of our current dataset so that nothing is lost\ntrain_data_copy = train_data.copy()\ntest_data_copy = test_data.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Handling Missing Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dropna = train_data_copy.dropna()\nprint(\"Tech: drop NaNs, X train shape: {}\\n\".format(train_dropna.shape))\n\n# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html\ntrain_fill_nan = train_data_copy.fillna(method = 'ffill')\nprint(\"Tech: fill NaNs with forward values, X train shape: {}\\n\".format(train_fill_nan.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check if there still missing values in each of our datasets\nprint(\"Missing values for the train set where simply NaNs were dropped:\\n\")\nprint(train_dropna.isnull().any().value_counts())\nprint()\nprint(\"Missing values for the train set where NaNs filled using the forward technique:\\n\")\nprint(train_fill_nan.isnull().any().value_counts())\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that there are not missing values we can now separate the labels and the features. \nThe image is our feature and other values are the keypoints that we have to predict later. \nSince image column values are in string format, we have to split the string by space and append it, and then handle missing values."},{"metadata":{},"cell_type":"markdown","source":"# Preparing Data for our training"},{"metadata":{},"cell_type":"markdown","source":"The given input image is a list of pixels (ordered by row), as integers in (0,255), which I transformed into values within the interval [0,1] (referred as X_train). The keypoints are also transformed into a range of [-1,1]."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_dropna, y_train_dropna, mean_cols = scale_data(train_dropna) # x_train == image, y_train == keypoints\nX_test_dropna, y_test_dropna, _ = scale_data(train_dropna, test=True)\nX_train_fill_nan, y_train_fill_nan, mean_cols = scale_data(train_fill_nan)\nX_test_fill_nan, y_test_fill_nan, _ = scale_data(train_fill_nan, test=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"1. Drop missing values:\")\nprint()\nprint(\"X train shape: {}, y train shape: {}, X test shape: {}\".format(X_train_dropna.shape, y_train_dropna.shape, X_test_dropna.shape))\nprint()\nprint(\"2. Fill missing values:\")\nprint()\nprint(\"X train shape: {}, y train shape: {}, X test shape: {}\".format(X_train_fill_nan.shape, y_train_fill_nan.shape, X_test_fill_nan.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" The images are 96x96 pixels, so the data needs to be reshaped."},{"metadata":{"trusted":true},"cell_type":"code","source":"# reshape X train and X test\nX_train_dropna = reshape_data(X_train_dropna)\nX_test_dropna = reshape_data(X_test_dropna)\nX_train_fill_nan = reshape_data(X_train_fill_nan)\nX_test_fill_nan = reshape_data(X_test_fill_nan)\nprint(\"X train shape: {}, X test shape: {}\".format(X_train_dropna.shape, X_test_dropna.shape))\nprint(\"X train shape: {}, X test shape: {}\".format(X_train_fill_nan.shape, X_test_fill_nan.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"title=\"Example of images in our train set \"\nplot_img(X_train_dropna, y_train_dropna, mean_cols, title, True, 4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data augmentation"},{"metadata":{},"cell_type":"markdown","source":"Data augmentation is used in order to avoid overfitting since 70% of data is missing. Thus, if images are transformed (rotation, flip, contrast, ..) the keypoints' new positions should be re-calculated. Then, we should modify our generator so that data augmentation would not affect our coordinates.\n\nSo the **ImageDataGenerator()** function from keras cannot be used in this case."},{"metadata":{},"cell_type":"markdown","source":"### Explaining data augmentation transformation[1]\n\n1. **Horizontal Reflection (flip)** :  The first data augmentation technique is fairly straight forward. We just need to reflect the image and its keypoint labels horizontally and then remap the keypoint labels to their new representations (left center eye becomes right center eye, and vice versa).\n\n\n2. **Rotation** : rotate the image clockwise or counterclockwise each with probability 0.5. The image pixel matrix (X) and labels are rotated using X•R, where R is the rotation matrix. The images are padded with their mean pixel value, along the edges where parts of the image were rotated out of bounds.\n\n\\begin{equation} R = \\begin{bmatrix}  cos(\\theta) & -sin(\\theta)  \\\\  sin(\\theta) & cos(\\theta) \\end{bmatrix} \\end{equation}\n\n3. **Contrast reduction** : reduce the contrast of the greyscale image. The idea is that pixel values are shifted slightly towards the images mean pixel value. \n\n\n4. **Shift** : By shifting the images, we can change the position of the object in the image and hence give more variety to the model. This will eventually lead to a more generalized model. After the shift operation, an object present at a location (x,y) in the input image is shifted to a new position (X, Y):\n\n\\begin{equation} X = x + dx \\end{equation}\n\\begin{equation} Y = y + dy\\end{equation}\n\n\n5. **Adding noise** : Add some random gaussien noise to the image. In fact, noise helps to regularize training and prevents the model from overfitting.\n"},{"metadata":{},"cell_type":"markdown","source":"### Remark:\n\nData augmentation will be applied on the dataset where we dropped missing values."},{"metadata":{"trusted":true},"cell_type":"code","source":"flip_ratio = 0.8\nrotate_ratio = 0.8\ncontrast_ratio = 1.2\nrandom_seed = 342\nangle = 9\nuse_flip_transf = True\nuse_rotation_transf = False\nuse_brightness_transf = False","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data augmentation results:"},{"metadata":{},"cell_type":"markdown","source":"## 1. Flip"},{"metadata":{"trusted":true},"cell_type":"code","source":"aug_x_train = X_train_dropna.copy()\naug_y_train = y_train_dropna.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if use_flip_transf:\n    flipped_img, flipped_kepoints = flipp(aug_x_train, aug_y_train, flip_ratio, None, random_seed)\n    print(\"Shape of flipped images {} and keypoints {}\".format(flipped_img.shape, flipped_kepoints.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"title = \"Flipped images\"\nplot_img(df=flipped_img, keypoints=flipped_kepoints, mean_cols=mean_cols, title=title, add_keypoints=True, num_img=4)\ntitle = \"Original images\"\nplot_img(df=X_train_dropna, keypoints=y_train_dropna, mean_cols=mean_cols, title=title, add_keypoints=True, num_img=4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Rotation"},{"metadata":{"trusted":true},"cell_type":"code","source":"aug_x_train = X_train_dropna.copy()\naug_y_train = y_train_dropna.copy()\nuse_rotation_transf = True\nif use_rotation_transf:\n    rotated_img_l, rotated_keypoints_l = rotate(y_train_dropna, aug_x_train, aug_y_train, rotate_ratio, 9, 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aug_x_train = X_train_dropna.copy()\naug_y_train = y_train_dropna.copy()\nif use_rotation_transf:\n    rotated_img_r, rotated_keypoints_r = rotate(y_train_dropna, aug_x_train, aug_y_train, rotate_ratio, 9, 1)\nprint(\"Shape of rotated images {} and keypoints {}\".format(rotated_img_r.shape, rotated_keypoints_r.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"title = \"Rotated images to the left\"\nplot_img(df=rotated_img_l, keypoints=rotated_keypoints_l, mean_cols=mean_cols, title=title, add_keypoints=True, num_img=4)\ntitle = \"Rotated images to the right\"\nplot_img(df=rotated_img_r, keypoints=rotated_keypoints_r, mean_cols=mean_cols, title=title, add_keypoints=True, num_img=4)\ntitle = \"Original images\"\nplot_img(df=X_train_dropna, keypoints=y_train_dropna, mean_cols=mean_cols, title=title, add_keypoints=True, num_img=4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Brightness"},{"metadata":{"trusted":true},"cell_type":"code","source":"aug_x_train = X_train_dropna.copy()\naug_y_train = y_train_dropna.copy()\nuse_brightness_transf = True\nif use_brightness_transf:\n    inc_brightness_images = brightness(aug_x_train)\n    print(\"Shape of brightned images {} \".format(inc_brightness_images.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"title = \"Increase brightness images\"\nplot_img(df=inc_brightness_images, keypoints=y_train_dropna, mean_cols=mean_cols, title=title, add_keypoints=True, num_img=4)\ntitle = \"Original images\"\nplot_img(df=X_train_dropna, keypoints=y_train_dropna, mean_cols=mean_cols, title=title, add_keypoints=True, num_img=4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Shift augmentation\nSource : https://fairyonice.github.io/achieving-top-23-in-kaggles-facial-keypoints-detection-with-keras-tensorflow.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"aug_x_train = X_train_dropna.copy()\naug_y_train = y_train_dropna.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shifted_img, shifted_keypoints = shift_image(aug_x_train, aug_y_train, prop=0.1)\nshifted_img = shifted_img[:,:,:,np.newaxis]\nprint(\"Shape of shifted images {} \".format(shifted_img.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"title = 'Shifted images'\nplot_img(df=shifted_img, keypoints=shifted_keypoints, mean_cols=mean_cols, title=title, add_keypoints=True, num_img=4)\ntitle = 'Original images'\nplot_img(df=X_train_dropna, keypoints=y_train_dropna, mean_cols=mean_cols, title=title, add_keypoints=True, num_img=4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Add noise :"},{"metadata":{"trusted":true},"cell_type":"code","source":"aug_x_train = X_train_dropna.copy()\nnoisy_img = add_noise(aug_x_train)\nprint(\"Shape of noisy images {} \".format(noisy_img.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"title = 'Noisy images'\nplot_img(df=noisy_img, keypoints=y_train_dropna, mean_cols=mean_cols, title=title, add_keypoints=True, num_img=4)\ntitle = 'Original images'\nplot_img(df=X_train_dropna, keypoints=y_train_dropna, mean_cols=mean_cols, title=title, add_keypoints=True, num_img=4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create Dataset for training :\n\nNow that we've done all our data transformation we create our new dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"aug_x_train_ffill = X_train_fill_nan.copy().reshape((-1, 96,96,1))\naug_y_train_ffill = y_train_fill_nan.copy()\naug_x_train = X_train_dropna.copy().reshape((-1, 96,96,1))\naug_y_train = y_train_dropna.copy()\naug_x_train = np.concatenate((aug_x_train, flipped_img, rotated_img_r, rotated_img_l, inc_brightness_images, shifted_img, noisy_img))\naug_y_train = np.concatenate((aug_y_train, flipped_kepoints, rotated_keypoints_r, rotated_keypoints_l, aug_y_train, shifted_keypoints, aug_y_train))\nprint(\"Number of images in the new train dataset using data augmentation :{} {} \".format(aug_x_train.shape, aug_y_train.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Building a CNN model !"},{"metadata":{},"cell_type":"markdown","source":"## Split datasets\n\nNow that we have finished data augmentation, we will assess its performance on different models. As we explained earlier in this notebook, we will use three datasets and compare them: dataset without missing values, with missing values imputed and with data augmentation."},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop NaN values\nx_train_dna, x_validation_dna, y_train_dna, y_validation_dna = split_train_validation(X_train_dropna, y_train_dropna)\n# impute NaN values\nx_train_ffill, x_validation_ffill, y_train_ffill, y_validation_ffill = split_train_validation(X_train_fill_nan, y_train_fill_nan)\n# Data augmentation\nx_train_da, x_validation_da, y_train_da, y_validation_da = split_train_validation(aug_x_train, aug_y_train, 0.1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training a model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# model_03_s = Sequential()\n\n# model_03_s.add(Convolution2D(32, (3,3), input_shape=(96,96,1)))\n# model_03_s.add(Activation('relu'))\n\n# model_03_s.add(Convolution2D(64, (3,3)))\n# model_03_s.add(Activation('relu'))\n# model_03_s.add(MaxPool2D(pool_size=(2, 2)))\n# model_03_s.add(Dropout(0.5))\n\n# model_03_s.add(Convolution2D(128, (3,3)))\n# model_03_s.add(Activation('relu'))\n\n# model_03_s.add(Convolution2D(256, (3,3)))\n# model_03_s.add(Activation('relu'))\n# model_03_s.add(MaxPool2D(pool_size=(2, 2)))\n# model_03_s.add(Dropout(0.5))\n\n# model_03_s.add(Convolution2D(512, (3,3)))\n# model_03_s.add(Activation('relu'))\n# model_03_s.add(Dropout(0.5))\n\n# model_03_s.add(Flatten())\n# model_03_s.add(Dense(512,activation='relu'))\n\n# model_03_s.add(Dense(30))\n# model_03_s.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# callbacks = [\n#         EarlyStopping(monitor='val_loss', patience=15, mode='min',restore_best_weights=True, verbose=1),\n#         ModelCheckpoint(filepath = 'best_model_03_s.hdf5', monitor='val_mae', verbose=1, save_best_only=True, mode='min')\n#     ]\n\n# hist_03_s = model_03_s.fit(x_train_da, y_train_da,\n#             epochs= 100, batch_size=64,\n#             validation_data=(x_validation_da, y_validation_da),\n#             callbacks=callbacks,\n#             verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot_loss(hist_03_s.history,\"Best model\",plt)\n# plt.legend()\n# plt.grid()\n# plt.yscale(\"log\")\n# plt.xlabel(\"epoch\")\n# plt.ylabel(\"log loss\")\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_06_01 = Sequential()\n\nmodel_06_01.add(Convolution2D(filters=32, kernel_size=(3,3), padding='same', input_shape=(96,96,1)))\nmodel_06_01.add(Activation('relu'))\n# model_06_01.add(BatchNormalization())\nmodel_06_01.add(Dropout(0.1))\n\n# model_06_01.add(Convolution2D(filters=32, kernel_size=(3,3), padding='same', input_shape=(96,96,1)))\n# model_06_01.add(Activation('relu'))\n# model_06_01.add(BatchNormalization())\n# model_06_01.add(MaxPooling2D(pool_size=(2, 2)))\n# model_06_01.add(Dropout(0.1))\n\n# model_06_01.add(Convolution2D(filters=64, kernel_size=(3,3), padding='same'))\n# model_06_01.add(Activation('relu'))\n\n\nmodel_06_01.add(SeparableConv2D(filters=256, kernel_size=(3,3), padding='same', use_bias=False))\nmodel_06_01.add(Activation('relu'))\n# model_06_01.add(BatchNormalization())\nmodel_06_01.add(MaxPooling2D(pool_size=(2, 2)))\n# model_06_01.add(Dropout(0.25))\n\n# model_06_01.add(Convolution2D(filters=64, kernel_size=(3,3), padding='same'))\n# model_06_01.add(Activation('relu'))\n\nmodel_06_01.add(SeparableConv2D(filters=256, kernel_size=(3,3), padding='same', use_bias=False))\nmodel_06_01.add(Activation('relu'))\nmodel_06_01.add(BatchNormalization())\nmodel_06_01.add(MaxPooling2D(pool_size=(2, 2)))\nmodel_06_01.add(Dropout(0.25))\n\nmodel_06_01.add(SeparableConv2D(filters=256, kernel_size=(3,3), padding='same', use_bias=False))\nmodel_06_01.add(Activation('relu'))\n# model_06_01.add(BatchNormalization())\nmodel_06_01.add(MaxPooling2D(pool_size=(2, 2)))\nmodel_06_01.add(Dropout(0.25))\n\n\nmodel_06_01.add(Flatten())\nmodel_06_01.add(Dense(512,activation='relu'))\nmodel_06_01.add(Dropout(0.5))\nmodel_06_01.add(Dense(30))\nmodel_06_01.summary()\nmodel_06_01.compile(optimizer = 'adam',loss = 'mean_squared_error', metrics=['mae', 'acc'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"callbacks = [\n        EarlyStopping(monitor='val_loss', patience=15, mode='min',restore_best_weights=True, verbose=1),\n        ModelCheckpoint(filepath = 'best_model_06_001.hdf5', monitor='val_mae', verbose=1, save_best_only=True, mode='min')\n    ]\n\nhist_06_01 = model_06_01.fit(x_train_da, y_train_da,\n            epochs= 80, batch_size=128,\n            validation_data=(x_validation_da, y_validation_da),\n            callbacks=callbacks,\n            verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_loss(hist_06_01.history,\"Best model\",plt)\nplt.legend()\nplt.grid()\nplt.yscale(\"Log\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Log loss\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model_06_02 = Sequential()\n\n# model_06_02.add(Convolution2D(filters=32, kernel_size=(3,3), padding='same', input_shape=(96,96,1)))\n# model_06_02.add(LeakyReLU(alpha = 0.1))\n# model_06_02.add(Dropout(0.1))\n\n\n# model_06_02.add(SeparableConv2D(filters=256, kernel_size=(3,3), padding='same'))\n# model_06_02.add(LeakyReLU(alpha = 0.1))\n# model_06_02.add(MaxPooling2D(pool_size=(2, 2)))\n\n\n# model_06_02.add(SeparableConv2D(filters=256, kernel_size=(3,3), padding='same'))\n# model_06_02.add(LeakyReLU(alpha = 0.1))\n# model_06_02.add(BatchNormalization())\n# model_06_02.add(MaxPooling2D(pool_size=(2, 2)))\n# model_06_02.add(Dropout(0.25))\n\n# model_06_02.add(SeparableConv2D(filters=256, kernel_size=(3,3), padding='same'))\n# model_06_02.add(LeakyReLU(alpha = 0.1))\n# model_06_02.add(MaxPooling2D(pool_size=(2, 2)))\n# model_06_02.add(Dropout(0.25))\n\n\n# model_06_02.add(Flatten())\n# model_06_02.add(Dense(512,activation='relu'))\n# model_06_02.add(Dropout(0.5))\n# model_06_02.add(Dense(30))\n# model_06_02.summary()\n# model_06_02.compile(optimizer = 'adam',loss = 'mean_squared_error', metrics=['mae', 'acc'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# callbacks = [\n#         EarlyStopping(monitor='val_loss', patience=15, mode='min',restore_best_weights=True, verbose=1),\n#         ModelCheckpoint(filepath = 'best_model_06_02.hdf5', monitor='val_mae', verbose=1, save_best_only=True, mode='min')\n#     ]\n\n# hist_06_02 = model_06_02.fit(x_train_da, y_train_da, batch_size=64, epochs=100,validation_data=(x_validation_da,y_validation_da),callbacks=callbacks, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot_loss(hist_06_02.history,\"Best model\",plt)\n# plt.legend()\n# plt.grid()\n# plt.yscale(\"Log\")\n# plt.xlabel(\"Epoch\")\n# plt.ylabel(\"Log loss\")\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model_06_03 = Sequential()\n\n# model_06_03.add(Convolution2D(filters=32, kernel_size=(3,3), padding='same', input_shape=(96,96,1)))\n# model_06_03.add(LeakyReLU(alpha = 0.1))\n# # model_06_03.add(BatchNormalization())\n# model_06_03.add(Dropout(0.1))\n\n\n# model_06_03.add(SeparableConv2D(filters=256, kernel_size=(3,3), padding='same'))\n# model_06_03.add(LeakyReLU(alpha = 0.1))\n# # model_06_03.add(BatchNormalization())\n# model_06_03.add(MaxPooling2D(pool_size=(2, 2)))\n# model_06_03.add(Dropout(0.25))\n\n\n# model_06_03.add(SeparableConv2D(filters=256, kernel_size=(3,3), padding='same'))\n# model_06_03.add(LeakyReLU(alpha = 0.1))\n# model_06_03.add(BatchNormalization())\n# model_06_03.add(MaxPooling2D(pool_size=(2, 2)))\n# model_06_03.add(Dropout(0.5))\n\n# model_06_03.add(SeparableConv2D(filters=256, kernel_size=(3,3), padding='same'))\n# model_06_03.add(LeakyReLU(alpha = 0.1))\n# model_06_03.add(MaxPooling2D(pool_size=(2, 2)))\n# model_06_03.add(Dropout(0.5))\n\n\n# model_06_03.add(Flatten())\n# model_06_03.add(Dense(512,activation='relu'))\n# model_06_03.add(Dropout(0.5))\n# model_06_03.add(Dense(30))\n# model_06_03.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# callbacks = [\n#         EarlyStopping(monitor='val_loss', patience=15, mode='min',restore_best_weights=True, verbose=1),\n#         ModelCheckpoint(filepath = 'best_model_06_03.hdf5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n#     ]\n\n# hist_06_03 = model_06_03.fit(x_train_da, y_train_da, batch_size=64, epochs=100,validation_data=(x_validation_da,y_validation_da),callbacks=callbacks, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot_loss(hist_06_03.history,\"Best model\",plt)\n# plt.legend()\n# plt.grid()\n# plt.yscale(\"Log\")\n# plt.xlabel(\"Epoch\")\n# plt.ylabel(\"Log loss\")\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predicting on Test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_,_, _ = scale_data(test_data_copy, True)\ntest_img = reshape_data(test_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_model = load_model('best_model_06_001.hdf5')\npred = best_model.predict(test_img)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Get Submission File"},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_name = list(idlookup['FeatureName'])\nimage_id = list(idlookup['ImageId']-1)\nrow_id = list(idlookup['RowId'])\n\nfeature_list = []\nfor feature in feature_name:\n    feature_list.append(feature_name.index(feature))\n    \npredictions = []\nfor x,y in zip(image_id, feature_list):\n    predictions.append(pred[x][y])\n    \nrow_id = pd.Series(row_id, name = 'RowId')\nlocations = pd.Series(predictions, name = 'Location')\nlocations = locations*mean_cols +mean_cols\nsubmission_result = pd.concat([row_id,locations],axis = 1)\nsubmission_result.to_csv('best_perf_15_1600.csv',index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"References:\n\n*[1] Longpre, S., & Sohmshetty, A. (2016). Facial keypoint detection.*"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}