{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Data used can be found at:\n# https://www.kaggle.com/c/facial-keypoints-detection/data\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport seaborn as sns\nimport keras as K # importing this just in case\n\nfrom keras import backend\n\n#from VGG16 import VGG16_Obj # contains the keras models I made, for local testing\n#from TrainImageObj import TrainImage # contains some viewing and V&V data \n\nimport tensorflow as tf\nimport os \nimport sys\nfrom datetime import datetime","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# process the image in the TF/Keras pretrained format\ndef formatImage(dataInput):\n    masterTemp = []\n    for i in dataInput:\n        tp = np.reshape(i.split(' '),(96,96))\n        imagePIL = Image.fromarray(np.array(tp, dtype='int32'))\n        imageResized = imagePIL.resize((224,224), Image.BILINEAR) # nearest neighbor resolution increase\n        imageResized = np.array(imageResized)\n        temp = []\n        for n in range(3):\n            temp.append(imageResized)\n        masterTemp.append(temp)\n    dataInput = np.array(masterTemp)\n    dataInput = np.swapaxes(dataInput,1,2)\n    dataInput = np.swapaxes(dataInput,2,3)\n    return dataInput\n\n# use class to make the system more modular \nclass VGG16_Obj:\n    def __init__ (self, projectDirectory = os.getcwd()):\n        # home computer has weights that can be leveraged for this, but right now jupyter can handle it\n        # interesting experiment: running this model with pretrained weights from the imagenet challenge and then again here without \n        pass\n        #self.projectDirectory = projectDirectory\n        #self.modelFC   = self.VGG16()\n        #self.modelNoFC = self.VGG16(FC_Include = False) # add additional method to call this out\n        #self.modelReducedFC = self.AddFCtoVGG16FeatureExtractor()\n\n    def VGG16(self,ClassicVGG16=True, \n                  FC_Include = True,\n                  l2_weight = 5e-04,\n                  like_its_hot = 0.4, # drop regulation\n                  FeatureExtractorTraining = False, \n                  FCTraining = True,\n                  weights= 'imagenet', \n                  input_tensor=None): \n        ''' \n        # Inputs\n        FC_Include = using the network as a feature extractor based on the convolutional layers\n        FullyConnected = if training is needed for the fully connected layer\n        classificationNumber = number of components \n        FeatureExtractorTraining = if you need to train the middle layers\n        weights = 'imagenet' means using the weights from a network pretrained by imagenet challenge\n        \n        Rules:\n        CANNOT have MORE than 1000 outputs (can't really see a case where they're would be >1k but eat your heart out.\n        Use the excess and keep training on the FC layers true\n        \n        # Returns\n            VGG16 Network\n        '''\n        if weights not in {'imagenet', None}:\n            raise ValueError('The `weights` argument should be either '\n                             '`None` (random initialization) or `imagenet` '\n                             '(pre-training on ImageNet).')\n        # Determine proper input shape\n        if ClassicVGG16:\n            inputShape = (224, 224, 3)\n        else:\n            inputShape = (None, None, 3)\n        \n        img_input = K.Input(inputShape)\n        \n        # Block 1     \n        b1_1 = K.layers.Conv2D(64, (3, 3), \n                      activation='relu', \n                      padding='same', # border_mode is now padding\n                      name='block1_conv1')\n        b1_1.trainable = FeatureExtractorTraining\n        x = b1_1(img_input)\n        \n        b1_2 = K.layers.Conv2D(64, (3, 3), \n                      activation='relu', \n                      padding='same', \n                      name='block1_conv2')\n        b1_2.trainable = FeatureExtractorTraining\n        x = b1_2(x)#_normalized)\n        \n        x = K.layers.MaxPooling2D((2,2), strides=(2,2), name='block1_pool')(x) \n        \n        # Block 2\n        b2_1 = K.layers.Conv2D(128, \n                      (3, 3), \n                      activation='relu', \n                      padding='same', \n                      name='block2_conv1')\n        b2_1.trainable = FeatureExtractorTraining\n        x = b2_1(x)\n        \n        b2_2 = K.layers.Conv2D(128, (3, 3), \n                               activation='relu', \n                               padding='same', \n                               name='block2_conv2')\n        b2_2.trainable = FeatureExtractorTraining\n        x = b2_2(x)#_normalized)\n        \n        x = K.layers.MaxPooling2D((2,2), strides=(2,2) , name='block2_pool')(x)#_normalized) # decrease the amout of data points with no rounding loss\n        \n        # Block 3\n        # convolution block\n        \n        b3_1 = K.layers.Conv2D(256, (3, 3), \n                      activation='relu', \n                      padding='same', \n                      name='block3_conv1')\n        b3_1.trainable = FeatureExtractorTraining\n        x = b3_1(x)\n        \n        b3_2 = K.layers.Conv2D(256, (3, 3), \n                      activation='relu', \n                      padding='same', \n                      name='block3_conv2')\n        b3_2.trainable = FeatureExtractorTraining\n        x = b3_2(x)#_normalized)\n\n        b3_3 = K.layers.Conv2D(256, (3, 3), \n                      activation='relu', \n                      padding='same', \n                      name='block3_conv3')\n        b3_3.trainable = FeatureExtractorTraining\n        x = b3_3(x)#_normalized)\n        \n        x = K.layers.MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n        \n        # Block 4 identity doc\n        \n        b4_1 = K.layers.Conv2D(512, (3, 3), \n                      activation='relu', \n                      padding='same', \n                      name='block4_conv1')\n        b4_1.trainable = FeatureExtractorTraining\n        x = b4_1(x)\n        \n        b4_2 = K.layers.Conv2D(512, (3, 3), \n                      activation='relu', \n                      padding='same', \n                      name='block4_conv2')\n        b4_2.trainable = FeatureExtractorTraining\n        x = b4_2(x)#_normalized)\n\n        b4_3 = K.layers.Conv2D(512, (3, 3), \n                      activation='relu', \n                      padding='same', \n                      name='block4_conv3')\n        b4_3.trainable = FeatureExtractorTraining\n        x = b4_3(x)#_normalized)\n        \n        x = K.layers.MaxPooling2D((2,2), strides=(2,2), name='block4_pool')(x)\n        \n        #Block 5\n        b5_1 = K.layers.Conv2D(512, (3, 3), \n                      activation='relu', \n                      padding='same', \n                      name='block5_conv1')\n        b5_1.trainable = FeatureExtractorTraining\n        x = b5_1(x)\n        \n        b5_2 = K.layers.Conv2D(512, (3, 3), \n                      activation='relu', \n                      padding='same', \n                      name='block5_conv2')\n        b5_2.trainable = FeatureExtractorTraining\n        x = b5_2(x)\n        \n        b5_3 = K.layers.Conv2D(512, (3, 3), \n                      activation='relu', \n                      padding='same', \n                      name='block5_conv3')\n        b5_3.trainable = FeatureExtractorTraining\n        x = b5_3(x)\n        \n        x = K.layers.MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x)\n\n        x = K.layers.Flatten(name='flatten')(x)\n\n        if FC_Include:\n            # Classification block\n            #x = K.layers.Flatten(name='flatten')(x) # moved this inside the lines\n            \n            #x = Dropout(like_its_hot, name = 'regulator_0')(x)\n            fc1 = K.layers.Dense(4096, activation='relu',\n                        kernel_regularizer=K.regularizers.l2(l2_weight),\n                        name='fc1')\n            fc1.trainable = FCTraining\n            x = fc1(x)\n            \n            x = K.layers.Dropout(like_its_hot, name = 'regulator_1')(x)\n            \n            fc2 = K.layers.Dense(4096, \n                        activation='relu', \n                        kernel_regularizer=K.regularizers.l2(l2_weight),\n                        name='fc2')\n            fc2.trainable = FCTraining\n            x = fc2(x)\n            \n            x = K.layers.Dropout(like_its_hot, name = 'regulator_2')(x)\n            \n            pred  = K.layers.Dense(1000, \n                          activation='softmax', \n                          kernel_regularizer=K.regularizers.l2(l2_weight),\n                          name='pred')(x)\n            \n            model = K.Model(img_input, pred)\n            \n        else: ########################################################################################\n            #print (\"You got no legs Lieutenant Dan!!!\")\n            model = K.Model(img_input,x)\n            \n        # load weights\n        if weights == 'imagenet':\n            currentCwd = os.getcwd()\n            os.chdir(self.projectDirectory) # hard coded for my directory\n            if FC_Include == False:\n                modelWeights = model.load_weights('vgg16Weights_noFC.h5')            \n            elif FC_Include == True: # only include the top if has \n                modelWeights = model.load_weights('vgg16Weights_FCincluded.h5')\n            os.chdir(currentCwd)\n        return model\n\n\n    def AddFCtoVGG16FeatureExtractor(self, fc1Variable = 500, \n                                    fc2Variable = 200, predVariable = 30, \n                                    l2_weight = 1e-03, like_its_hot = 0.4):\n        '''\n        Purpose: utalize Transfer Learning and slam a vgg16 network together with different FC layers\n        Output: model with just that (if you do a model.summary it misses the new model but data is there)\n        '''\n        \n        #TK modified here 11/03 \n        model1 = self.VGG16(ClassicVGG16=True, # use the 224x224x3 base to make it the most apples to apples comparison \n                    FC_Include = False,# remove the additional layers\n                    l2_weight = 5e-04,# Ridge Regression weights\n                    like_its_hot = 0.25, # drop regulation\n                    FeatureExtractorTraining = True,# go ahead and train \n                    FCTraining = True,\n                    weights= None,# not using transfer learnign\n                    input_tensor=None) # this is where I import the Transfer Learning Model\n\n        #model1 = self.modelNoFC # get the pretrained network with no FC layer\n        # end modifications here 11/03\n        \n        #make the second model we slam together\n        model2 = K.models.Sequential(name=\"FC_Layers_Model\") # MUST use Keras API to add layers together\n        model2.add(K.layers.Dense(fc1Variable, #define amount of variables in function header\n                      activation='relu',\n                      kernel_regularizer=K.regularizers.l2(l2_weight),\n                      name  =\"fc1\"))\n        \n        model2.add(K.layers.Dropout(like_its_hot, name = 'regulator_1')) # add regulation\n        model2.add(K.layers.Dense(fc2Variable, #define amount of variables in function header\n                     activation='relu',\n                     kernel_regularizer=K.regularizers.l2(l2_weight),\n                     name =\"fc2\"))\n        model2.add(K.layers.Dropout(like_its_hot, name = 'regulator_2')) #add a little more regulation\n        model2.add(K.layers.Dense(predVariable, #define amount of variables in function header\n                      #activation='softmax', # don't want an activator function here\n                      kernel_regularizer=K.regularizers.l2(l2_weight),\n                      name  =\"pred\"))\n\n        linkingOutput = model2(model1.output) #link the two models here\n        finalModel = K.Model(model1.input, linkingOutput)\n        return finalModel\nprint(\"Model Created\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# Any results you write to the current directory are saved as output.\n\n####################################### definitions for data import#######################################\ncwd = os.getcwd()\ntest = False # just incase there is something we need to test in this\nstart = datetime.now()\nmasterStart = datetime.now()\n\nprojectDirectory = \"/home/ted/Python_Projects/FacialKeypointsDetection\"#\"C:\\\\Users\\\\Ted\\\\Projects\\\\Python\\\\FacialKeypointsDetection\"\ndataDir = \"/media/ted/Elements/TK_PracticeDatabases/facial-keypoints-detection\"#\"E:\\\\TK_PracticeDatabases\\\\facial-keypoints-detection\"\n\ndirname = projectDirectory\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\nos.chdir(\"/kaggle/input/facial-keypoints-detection/test\")\ntestData = pd.read_csv(\"test.csv\")\n\nos.chdir(\"/kaggle/input/facial-keypoints-detection/training\")\ntrainingData = pd.read_csv(\"training.csv\")\n\nos.chdir(\"/kaggle/input/facial-keypoints-detection\")\nidLookupData = pd.read_csv(\"IdLookupTable.csv\")\nsampleSubData = pd.read_csv(\"SampleSubmission.csv\")\n#os.chdir(projectDirectory)\n\n# borrowed these line from Karan Jakhar's post\nb4NanFill = trainingData.isnull().any().value_counts() # only present for the print statement\ntrainingData.fillna(0, inplace=True) # replace nans with '0' \nafterNanFill = trainingData.isnull().any().value_counts() # only present for the print statement\n#\"Big thank you to Karan Jakhar's post for these lines \\n\\nB4Nan Handling :  \\n\"\nprint(str(b4NanFill) + \" \\n\\nHandling After: \\n\" + str(afterNanFill) +\"\\n\")\ndel b4NanFill\ndel afterNanFill # don't need to delete these to free up memory but doesn't hurt\n\n\n################################################################################################################","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataFromFiles = trainingData['Image']\ndataInput = formatImage(dataFromFiles)\n\nvalidationData = trainingData.drop('Image', axis=1)\n\nos.chdir(cwd)\n\nif test: # in the begining take a look at the training data\n    print(trainingData)\n\nimportComplete = datetime.now()\nprint(\"Import Finished: \" + str(dataInput.shape)+\" \"+ str(importComplete - start))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################################## Generate the model ##########################################\nstart = datetime.now()\n\n#os.chdir(projectDirectory)\nvgg16 = VGG16_Obj()\n\nmodel = vgg16.AddFCtoVGG16FeatureExtractor(fc1Variable = 1000, # use the VGG to then swap out the fully connected levels\n                                           fc2Variable = 200, # gradual is typically good\n                                           predVariable = 30, \n                                           l2_weight = 1e-03, \n                                           like_its_hot = 0.4)\n\nmodel.summary() # this will give us the VGG16 without the fully connected layers and label the new FCs as sequentional_1\n# we are using the VGG16 as a feature extractor and not changing them!!!\n\n# check if tensorflow sees my GPU\nprint(\"\\nDoes the computer recognize a GPU: \" + str(len(backend.tensorflow_backend._get_available_gpus())))\n\nimportComplete = datetime.now()\nprint(\"\\nModified Vgg16 Generated: \" + str(importComplete - start))\nplt.close() # close the figure for non-jupyter notebook applications","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################################## fit to model ##########################################\nstart = datetime.now()\n##log all the loss data\n#csvLogging = K.callbacks.CSVLogger(\"FacialPointExtractor_Loss.log\", append=True)\n\nmodel.compile(optimizer = 'adam', # adam seems to be the best optimizer I've used (mostly for classification tho) \n              loss = 'mse', # mse because we want a continuous number!!!\n              metrics = ['mae', 'accuracy']) # print/log some extra info\n#'''\nmodel.fit(dataInput, validationData,  #need data to insert \n          batch_size=128, # make this a power of 2 for best performance\n          epochs=20, # more epochs better the prediction until gain is saturated, start small and go bigger if needed\n          shuffle=True, # shuffling is never a bad idea...\n          verbose=True, # show me progress per epoch\n          #callbacks=[csv_logger] # log the data?\n          validation_split=0.2) # take a 1/5 of the data for validation purposes '''\n\n\ndoneFitting = datetime.now()\nprint(\"\\nFitting Complete: \" + str(doneFitting-start))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test data Prep, same as before, should make this a function...\ndataInput = testData['Image']\nmasterTemp = []\nfor i in dataInput:\n    tp = np.reshape(i.split(' '),(96,96))\n    imagePIL = Image.fromarray(np.array(tp, dtype='int32'))\n    imageResized = imagePIL.resize((224,224), Image.BILINEAR) # nearest neighbor resolution increase\n    imageResized = np.array(imageResized)\n    temp = []\n    for n in range(3):\n        temp.append(imageResized)\n    masterTemp.append(temp)\ndataInput = np.array(masterTemp)\ndataInput = np.swapaxes(dataInput,1,2)\ndataInput = np.swapaxes(dataInput,2,3)\n\npred = model.predict(dataInput)\nprint(\"Pred Complete\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = trainingData\ntest_data = testData\nlookid_data = idLookupData\nsampleSubData\n\nlookid_list = list(lookid_data['FeatureName'])\nimageID = list(lookid_data['ImageId']-1)\npre_list = list(pred)\n\nrowid = lookid_data['RowId']\nrowid=list(rowid)\n\nfeature = []\nfor f in list(lookid_data['FeatureName']):\n    feature.append(lookid_list.index(f))\npreded = []\nfor x,y in zip(imageID,feature):\n    preded.append(pre_list[x][y])\n\nrowid = pd.Series(rowid,name = 'RowId')\nloc = pd.Series(preded,name = 'Location')\nsubmission = pd.concat([rowid,loc],axis = 1)\nsubmission.to_csv('face_key_detection_submission.csv',index = False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.getcwd()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}