{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Facial key points"},{"metadata":{},"cell_type":"markdown","source":"The training of the facial key points dataset is performed here, we select a basic CNN model and consider two basic metric changes - \n* 1 The preoprocessing, there are NaN's in the dataset - an investigation of three choices is employed\n    * Forward fill\n    * Fill with average of column (constant)\n    * Fill with a simple fitted guassian distribution\n\n* 2 The consideration of a non-CNN model and a full CNN model.\n    * It makes sense to benchmark with a non-convoluted model, since this is a simpler model"},{"metadata":{},"cell_type":"markdown","source":"# 1 . Imports"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport torch.optim as optim\nimport logging\n\nimport torch\nimport torch.nn.functional as F\nimport torch.nn as nn\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nimport gc\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Opening data and preprocessing"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/facial-keypoints-detection/training/training.csv')\n#null values:\nnull = df.isnull().sum()\nnull = null[null!=0]\nnull","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So there a NaN values in the dataset, we now consider the different preoprocessing operations:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def fillNaN(df_series, method):\n    \"\"\"\n    NaN filling method.\n    Input:\n        df_series [pandas.core.series.Series] : A pandas series type with NaN to be filled in.\n        method [string] : A string valued method to identify how the NaN handling should take place.\n    Output:\n        None, the NaN handling is performed in-place\n    \"\"\"\n    if method == \"forwardFill\":\n        df_series = df_series.fillna(method='ffill')\n    elif method == \"constantMean\":\n        mean = df_series.mean()\n        df_series = df_series.fillna(mean)\n    elif method == \"sampleDist\":\n        mean = df_series.mean()\n        std = df_series.std()\n        df_series = df_series.fillna(pd.Series(np.random.normal(mean, std, len(df_series))))\n    else:\n        raise ValueError(\"Method not identified : must be in forwardFill, constantMean or sampleDist\")\n    \n    return df_series","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create a preprocess dictionary encapsulating all the data\ndata_processing = {}\ndata_processing['forwardFill'] = df.copy()\ndata_processing['constantMean'] = df.copy()\ndata_processing['sampleDist'] = df.copy()\n\nfor key in data_processing.keys():\n    temp_df = data_processing[key]\n    for null_keys in null.keys():\n        data_processing[key][null_keys] = fillNaN(data_processing[key][null_keys], key)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We process the images"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_images = df['Image'].map(lambda x : list(map(int,x.split(' '))))\n\n#Normalise the image data\ndf_images = df_images.map(lambda x: list((np.array(x) - np.array(x).mean())/ np.array(x).std()))\n\n#delete the image columns from the output data\nfor key in data_processing.keys():\n    del data_processing[key]['Image']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So so far we have the following data:\n* A dictionary of dataframes dictated by the NaN handling method : data_processing\n* The corresponding images (dataframe) which have been normalised : df_images"},{"metadata":{},"cell_type":"markdown","source":"# 3. Set up train-validation sets"},{"metadata":{},"cell_type":"markdown","source":"We define two classes :\n* processTrainValidation : provides some data preprocessings and creates the training/validation sets\n* sampler : a utlity class used for sampling stochastically"},{"metadata":{"trusted":true},"cell_type":"code","source":"class processTrainValidation:\n    def __init__(self, k, image_data, keypoint_data):\n        self.k = k\n        \n        self.image_data = image_data\n        self.keypoint_data = keypoint_data\n        self._process()\n        \n        self.N = len(image_data)\n        self.splitter = lambda i : int((i/self.k)*self.N)\n        \n    def _process(self):\n        self.X = self.image_data.values\n        self.X = np.array([np.array(x) for x in self.X])\n        \n        self.Y = self.keypoint_data[self.keypoint_data.columns].values\n        \n        del self.image_data\n        del self.keypoint_data\n        gc.collect()\n        \n    def create_set(self, I):\n        X_val, X_train = self.X[self.splitter(I):self.splitter(I+1)], \\\n        np.concatenate([self.X[self.splitter(0):self.splitter(I)],self.X[self.splitter(I+1):]])\n        y_val, y_train = self.Y[self.splitter(I):self.splitter(I+1)], \\\n        np.concatenate([self.Y[self.splitter(0):self.splitter(I)],self.Y[self.splitter(I+1):]])\n        \n        return X_train, y_train, X_val, y_val\n    \nclass sampler:\n    def __init__(self, indices, sample_size):\n        self.indices = indices\n        self.sample_size = sample_size\n        self.dontStop = True\n        \n    def sample(self):\n        self.dontStop = len(self.indices) > self.sample_size\n        \n        if not self.dontStop:\n            return self.indices\n        sample = np.random.choice(self.indices,self.sample_size,False)\n        self.indices = list(set(self.indices) - set(sample))\n        return sample\n    \n    def getDontStop(self):\n        return self.dontStop","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Define the models"},{"metadata":{},"cell_type":"markdown","source":"We now define two models:\n* A model completely built from linear models (called lin_mod), which is our benchmark model.\n* A model which includes a CNN (call cnn_mod)."},{"metadata":{"trusted":true},"cell_type":"code","source":"class c_unit(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size):\n        super(c_unit, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size)\n        self.conv1_bn = nn.BatchNorm2d(out_channels)\n        \n    def forward(self, x, with_max_pool = True):\n        x = self.conv1(x)\n        x = self.conv1_bn(x)\n        x = F.relu(x)\n        if with_max_pool:\n            x = F.max_pool2d(x, kernel_size=2)\n        return x\n    \nclass r_unit(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super(r_unit, self).__init__()\n        self.fc1 = nn.Linear(in_dim, out_dim)\n        self.bm = nn.BatchNorm1d(out_dim)\n        \n    def forward(self, x, with_batch_norm = False):\n        x = self.fc1(x)\n        if with_batch_norm:\n            x = self.bm(x)\n        x = F.relu(x)\n        return x\n    \nclass lin_mod(nn.Module):\n    def __init__(self):\n        super(lin_mod, self).__init__()        \n        self.r_unit_1 = r_unit(9216, 30)\n        self.dp1 = nn.Dropout(p=0.4)\n    \n    def forward(self, x, verbose=False):\n        x = x.view(-1, 9216)\n        \n        x = self.r_unit_1(x)\n        x = F.relu(x)\n        return x\n\nclass cnn_mod(nn.Module):\n    def __init__(self):\n        super(cnn_mod, self).__init__()\n        self.c_unit_1 = c_unit(1,12,2)\n        self.c_unit_2 = c_unit(12,64,2)\n        self.c_unit_3 = c_unit(64,128,2)\n        self.c_unit_4 = c_unit(128,256,2)\n        self.c_unit_5 = c_unit(256,512,2)\n        \n        self.r_unit_1 = r_unit(512*2*2, 1024)\n        self.r_unit_2 = r_unit(1024,256)\n        self.r_unit_3 = r_unit(256,30)\n        self.dp1 = nn.Dropout(p=0.4)\n    \n    def forward(self, x, verbose=False):\n        x = self.c_unit_1(x)\n        x = self.dp1(x)\n    \n        x = self.c_unit_2(x)\n        x = self.dp1(x)\n        \n        x = self.c_unit_3(x)\n        x = self.dp1(x)\n        \n        x = self.c_unit_4(x)\n        x = self.dp1(x)\n        \n        x = self.c_unit_5(x)\n        x = self.dp1(x)\n\n        x = x.view(-1, 512*2*2)\n        \n        # now use FC layer with relu\n        x = self.r_unit_1(x, True)\n        x = self.dp1(x)\n        \n        x = self.r_unit_2(x, True)\n        x = self.dp1(x)\n        \n        x = self.r_unit_3(x)\n        x = F.relu(x)\n        return x","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Define a training function"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(data_sampler, number_of_epochs, X_train, X_val, y_train, y_val, model, optimiser,\\\n         criterion,metric, validate_every, l,v,m,_):\n    N = len(X_train)\n    i = 0\n    while data_sampler.getDontStop():\n        sample = data_sampler.sample()\n        XX = torch.tensor(X_train[sample]).view(-1,1,96,96).cuda().float()\n        YY = torch.tensor(y_train[sample]).cuda().float()\n        \n        loss = criterion(model(XX),YY)\n        optimiser.zero_grad()\n        loss.backward()\n            \n        optimiser.step()\n        l.append(loss.item())\n        Pyloss = loss.item()\n        del loss\n        del XX\n        del YY\n        gc.collect()\n            \n        print('\\r', 'Epoch', _, 'Iteration',i,'of',int(N/data_sampler.sample_size), 'current_loss',Pyloss,end='')\n        i+=1\n            \n    if _%validate_every==0:\n        XX = torch.tensor(X_val).view(-1,1,96,96).cuda().float()\n        YY = torch.tensor(y_val).cuda().float()\n        val_loss = criterion(model(XX),YY)\n        v.append(val_loss.item())\n        PyVal_loss = val_loss.item()\n        mae = metric(model(XX),YY)\n        PyMae = mae.item()\n        m.append(PyMae)\n        del val_loss\n        del mae\n        del XX\n        del YY\n        print(' ')\n        print('loss : ', Pyloss)\n        print('validation loss : ', PyVal_loss)\n        print('MAE : ', PyMae)\n        print(' ')\n\n    if (len(v) >2 ) and v[-1] - v[-2] < 0:\n        optimiser.param_groups[0]['lr'] = optimiser.param_groups[0]['lr']/10\n        print('Optimiser learning rate reduced to '+str(optimiser.param_groups[0]['lr']))\n        \n    return l,v,m","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Train and validate"},{"metadata":{},"cell_type":"markdown","source":"We now perform a grid search along two axis: \n* preprocessing : forwadFill, constantMean and sampleDist\n* Model used : lin_mod and cnn_mod\n\nWe do this over 5 validation sets, over 15 epochs and average over the validation sets score to gain a result."},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_and_validate(criterion, metric, process_train_validation,model, number_of_epochs,\\\n                      sample_size, weight_init):\n    Loss, Validation, MeanAbsError = [],[],[]\n    for I in range(0,process_train_validation.k):\n        print('TRAIN/VAL SET NUMBER : ',I)\n        X_train, y_train, X_val, y_val = pTV.create_set(I)    \n        model.load_state_dict(weight_init)\n        model.cuda()\n        optimiser = optim.Adam(model.parameters(), lr=0.01,amsgrad=True)\n\n        l, v, m = [], [], []\n        clip = 0.1\n        torch.nn.utils.clip_grad_norm_(model.parameters(),clip)\n        loss = None\n\n        idxs = np.arange(X_train.shape[0])\n        N = len(idxs)\n\n        for _ in range(number_of_epochs):\n            data_sampler = sampler(idxs, sample_size)\n            l,v,m = train(data_sampler, number_of_epochs, X_train, X_val, y_train, y_val, model, optimiser,\\\n             criterion,metric,1,l,v,m,_)\n            \n        Loss.append(l)\n        Validation.append(v)\n        MeanAbsError.append(m)\n        \n    return Loss, Validation, MeanAbsError","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"criterion = nn.MSELoss()\nmetric = nn.L1Loss()\nsample_size = 300\nnumber_of_epochs = 5\n\n\nresults = {'lin_mod' : {'forwardFill' : 0 ,'constantMean' : 0, 'sampleDist' : 0},\\\n           'cnn_mod' :{'forwardFill' : 0 ,'constantMean' : 0, 'sampleDist' : 0}}\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#linear model\n\nfor key in data_processing.keys():\n    print('========= process : '+key+' =========')\n    pTV = processTrainValidation(5,df_images,data_processing[key])\n    model = lin_mod().train()\n    weight_inits = model.state_dict()\n\n    Loss, Validation, Mae = train_and_validate(criterion, metric, pTV, model, number_of_epochs, sample_size,\\\n                                               weight_inits)\n    results['lin_mod'][key] = Validation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#cnn model\n\nfor key in data_processing.keys():\n    print('========= process : '+key+' =========')\n    pTV = processTrainValidation(5,df_images,data_processing[key])\n    model = cnn_mod().train()\n    weight_inits = model.state_dict()\n\n    Loss, Validation, Mae = train_and_validate(criterion, metric, pTV, model, number_of_epochs, sample_size,\\\n                                               weight_inits)\n    results['cnn_mod'][key] = Validation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def save_obj(obj, name ):\n    with open(name + '.pkl', 'wb') as f:\n        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n\ndef load_obj(name ):\n    with open(name + '.pkl', 'rb') as f:\n        return pickle.load(f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\nsave_obj(results,'validation_results')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6. Pick a model and train on full data"},{"metadata":{},"cell_type":"markdown","source":"The best performing processing/model is constantMean with cnn_mod, so we know train on the full dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(data_sampler, number_of_epochs, X_train, y_train, model, optimiser, criterion,metric, l, m, _):\n    N = len(X_train)\n    i = 0\n    while data_sampler.getDontStop():\n        sample = data_sampler.sample()\n        XX = torch.tensor(X_train[sample]).view(-1,1,96,96).cuda().float()\n        YY = torch.tensor(y_train[sample]).cuda().float()\n        \n        predict = model(XX)\n        \n        loss = criterion(predict,YY)\n        optimiser.zero_grad()\n        loss.backward()\n            \n        optimiser.step()\n        l.append(loss.item())\n        Pyloss = loss.item()\n        mae = metric(predict, YY)\n        PyMae = mae.item()\n        m.append(PyMae)\n        del loss\n        del XX\n        del YY\n        del mae\n        gc.collect()\n            \n        print('\\r', 'Epoch', _, 'Iteration',i,'of',int(N/data_sampler.sample_size), 'current_loss',Pyloss,end='')\n        i+=1\n    return l,m","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = data_processing['constantMean'].values\nX_train = df_images.values\nX_train = np.array([np.array(x) for x in X_train])\n\ncriterion = nn.MSELoss()\nmetric = nn.L1Loss()\n\nmodel = cnn_mod().train()\n\nnumber_of_epochs = 50\nsample_size = 300\nmodel.cuda()\noptimiser = optim.Adam(model.parameters(), lr=0.01,amsgrad=True)\n\nl, m = [], [],\nclip = 0.1\ntorch.nn.utils.clip_grad_norm_(model.parameters(),clip)\nloss = None\n\nidxs = np.arange(X_train.shape[0])\nN = len(idxs)\n\nepoch_level_mae = []\n\nfor _ in range(number_of_epochs):\n    data_sampler = sampler(idxs, sample_size)\n    l,m = train(data_sampler, number_of_epochs, X_train, y_train, model, optimiser,criterion, metric, l,m,_)\n    epoch_level_mae.append(m[-1])\n    print('')\n    print('Epoch',_)\n    print('current loss : ',l[-1])\n    print('current mae : ',m[-1])\n    if (len(epoch_level_mae) >2 ) and epoch_level_mae[-1] - epoch_level_mae[-2] < 0:\n        optimiser.param_groups[0]['lr'] = optimiser.param_groups[0]['lr']/10\n        print('Optimiser learning rate reduced to '+str(optimiser.param_groups[0]['lr']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.save(model.state_dict(), 'cnn_mod.pth')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}