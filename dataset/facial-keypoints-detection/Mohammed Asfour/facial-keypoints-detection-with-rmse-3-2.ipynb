{"cells":[{"metadata":{"id":"KN6zLQbKP_vT","outputId":"53da9d5e-2bb3-43e3-a3f9-5a6df2e5e54c","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.layers import Input, Dense, Conv2D, MaxPooling2D, Dropout, Activation, BatchNormalization, Flatten, Lambda\nfrom keras.optimizers import Adam\nfrom keras.models import Model\nfrom keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau\nfrom keras.initializers import he_normal\nfrom keras import backend as K\n\nfrom IPython.display import clear_output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# first let's make a directory to extract the data\n!mkdir /root/data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# extract the data in the created directory\n!unzip ../input/training.zip -d /root/data\n!unzip ../input/test.zip -d /root/data","execution_count":null,"outputs":[]},{"metadata":{"id":"IOheiascQZH0","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('/root/data/training.csv')","execution_count":null,"outputs":[]},{"metadata":{"id":"cUaFqcbwZgck","outputId":"ef0aa3da-0ae4-4dd6-9360-a1cf11c6db42","trusted":true},"cell_type":"code","source":"train_df.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"rvyhoTWlY557","outputId":"28bb6a46-2aee-4e96-cba2-9d16891d9efa","trusted":true},"cell_type":"code","source":"# let's see what we are working with here\ntrain_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The image is stored in a list as the final column of each entry. To be able to use it as an image we need to convert it to a proper ndarray and to do that we'll use this next function"},{"metadata":{"id":"QIrNjQHPRlEp","outputId":"7e0912a3-5753-4a4d-93c7-41226cb50f3b","trusted":true},"cell_type":"code","source":"def list2img(df_data):\n    \"\"\"\n    converts img from the provided list type to a proper ndarray\n    :param df_data: the image in its list type\n    :return: the image in a proped ndarray shape\n    \"\"\"\n    int_arr = np.array(df_data.split(' ')).astype(int)\n    img = int_arr.reshape(96, 96)\n    return img\n\n# let's check a random image from the dataframe to check that it works as needed\nrnd_idx = np.random.randint(0, train_df.shape[0])  # select a random idx\nsample = train_df.Image.iloc[rnd_idx]  # select the corresponding row and get its image\nsample_img = list2img(sample)  # convert it to an ndarray\n\nplt.imshow(sample_img, cmap='gray')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since the function works as wanted, let's apply it to all the rows of the dataframe so that we don't have to worry about the images shape later on"},{"metadata":{"id":"fbLpVfsmRYp3","trusted":true},"cell_type":"code","source":"train_df.Image = train_df.Image.apply(lambda x: list2img(x));  # apply the function to all image entries","execution_count":null,"outputs":[]},{"metadata":{"id":"7Yj718FRUlgd","outputId":"b8cd4eae-e4ee-4fb1-f9a8-a9f2a685facf","trusted":true},"cell_type":"code","source":"# let's check that the dataframe images column is now converted to the required shape\nplt.imshow(train_df.Image.iloc[rnd_idx], cmap='gray')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we have converted the images in the dataframe to ndarrays, we'll then plot the keypoints from rows onto the image to understand the data better"},{"metadata":{"id":"UKLSCcfWVUDU","outputId":"427b8b6d-c673-4380-c9d3-99eadf8e52aa","trusted":true},"cell_type":"code","source":"def plot_keypoints(df_row, plot_img=True, ax=None, color='blue'):\n    \"\"\"\n    :param df_row: a row of the dataframe with the image entrey converted\n    :param plot_img: boolean for plotting the image as well or just the keypoints\n    :param ax: provided axes to plot on - the function will create a new one if not provided\n    :param color: color of th plotted keypoints\n    :return: the axes that was used in plotting \n    \"\"\"\n    if not ax:\n        fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n    if plot_img:\n        ax.imshow(df_row[-1], cmap='gray')\n    \n    for i in range(0, (df_row.size-1), 2):  # loop on every two entries since data is stored as (x, y)\n        # get the x and y coordinates for each of the 15 features\n        x_coord = df_row[i]\n        y_coord = df_row[i+1]\n        if not np.isnan(x_coord) and not np.isnan(y_coord):  # check if th coordinates aren't missing\n            circle = plt.Circle((x_coord, y_coord), 1, color=color)  # create circle\n            ax.add_artist(circle)  # add the circle to the axes\n    return ax\n\nrnd_idx = np.random.randint(0, train_df.shape[0])\nprint(train_df.iloc[rnd_idx][:-1])\nplot_keypoints(train_df.iloc[rnd_idx])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we have the functions we need to preprocess and visualize the data, let's check the null valus or missing coordinates in the dataset"},{"metadata":{"id":"G6ppZxwtbSam","outputId":"acb550cd-5e74-4cdf-9197-d027c0267c22","trusted":true},"cell_type":"code","source":"train_df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### That's a lot of missing data\n### we can try to handle them by:\n* forward fill or back fill but both would result into a highly mislabelled dataset and the model won't have any chance to predict the test data correctly\n* Use the average of each column/coordinate to fill the missing value which will make the dataset very biased towards certain values, hence the model will just to learn to predict the average most times with slight adjustments and that would actually give an OK loss on the test set since the faces are mostly cropped and centered but we'd be lying to ourselves thinking that it has learnt latent features from the data\n\n### So we'll just have to eleminate the entries with missing values and work with very small dataset, then we can just use transfer learning and train the last few dense layers or choose a very small model so that it gives a good validation error and doesn't overfit"},{"metadata":{"id":"4xef71Q5oxia","trusted":true},"cell_type":"code","source":"# fill_vals = train_df.iloc[:, :-1].mean().to_dict()\n# fill_vals","execution_count":null,"outputs":[]},{"metadata":{"id":"ztQ6eMU6oK2q","trusted":true},"cell_type":"code","source":"# train_df.fillna(value=fill_vals, axis=0, inplace=True)\n# train_df.fillna(method='ffill', axis=0, inplace=True)\ntrain_df.dropna(axis=0, how='any', inplace=True)  # drop the rows with missing values","execution_count":null,"outputs":[]},{"metadata":{"id":"3fR5y4Nn95AZ","outputId":"6298380a-0b56-4df7-c46e-74bedb936abb","trusted":true},"cell_type":"code","source":"train_df.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"I6L6bBaspB-9","outputId":"9f0e9ddd-f883-4476-efc0-495b4d297178","trusted":true},"cell_type":"code","source":"train_df.isna().sum()  # we no longer have missing vlues","execution_count":null,"outputs":[]},{"metadata":{"id":"1NiuIHJXsaiO","outputId":"b8f56609-2313-4505-f715-8cae55631262","trusted":true},"cell_type":"code","source":"faces = train_df.Image.values  # let's extract the images on their own outside the dataframe\nkeypoints = np.array(train_df.iloc[:, :-1])  # extract the keypoints\nprint(faces.shape, keypoints.shape)","execution_count":null,"outputs":[]},{"metadata":{"id":"VOdShNeCsfB4","outputId":"0bba62d9-392f-4d08-e247-e25bc630804b","trusted":true},"cell_type":"code","source":"# faces are now an array of array we'll convert it to a 2d array\nfaces = np.apply_along_axis(lambda x: x.tolist(), axis=0, arr=faces)  # convert to 2d array \nfaces = faces.reshape(*faces.shape, 1)  # add a dimension for color channels because convolution layers require it\nfaces.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"CCvZMr7h9gkY","trusted":true},"cell_type":"code","source":"faces = faces.astype(float) / 255  # normalize the images","execution_count":null,"outputs":[]},{"metadata":{"id":"eD6L7bUothr4","outputId":"085096e9-73fa-4ae3-9146-744086f48a40","trusted":true},"cell_type":"code","source":"fit_x, test_x, fit_y, test_y = train_test_split(faces, keypoints, test_size=0.1)\ntrain_x, valid_x, train_y, valid_y = train_test_split(fit_x, fit_y, test_size=0.2)\nprint(f'training set shape (x, y): {train_x.shape, train_y.shape}')\nprint(f'validation set shape (x, y): {valid_x.shape, valid_y.shape}')\nprint(f'test set shape (x, y): {test_x.shape, test_y.shape}')","execution_count":null,"outputs":[]},{"metadata":{"id":"nlM9yB7ILZ83","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"A41lNwDDOugM","trusted":true},"cell_type":"code","source":"# because we defined plot_keypoints to take a row of the dataframe as an input we use this function to combine the\n# image with its keypoints as a row before sending it to the function\ndef wrap_as_row(keypoints, img):\n    return np.array(list(keypoints) + [img])","execution_count":null,"outputs":[]},{"metadata":{"id":"cwoLtmOC2cm2","trusted":true},"cell_type":"code","source":"# used to plot training curves (accuracy, loss) while model is training\nclass Plotter(Callback):\n    def plot(self):  # Updates the graph\n        clear_output(wait=True)\n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n        \n        # plot the losses\n        ax1.plot(self.epochs, self.losses, label='train_loss')\n        ax1.plot(self.epochs, self.val_losses, label='val_loss')\n        \n        # plot the accuracies\n        ax2.plot(self.epochs, self.acc, label='train_acc')\n        ax2.plot(self.epochs, self.val_acc, label='val_acc')\n    \n        ax1.set_title(f'Loss vs Epochs')\n        ax1.set_xlabel(\"Epochs\")\n        ax1.set_ylabel(\"Loss\")\n        \n        ax2.set_title(f'Accuracy vs Epochs')\n        ax2.set_xlabel(\"Epoches\")\n        ax2.set_ylabel(\"Accuracy\")\n        \n        ax1.legend()\n        ax2.legend()\n        plt.show()\n        \n        # print out the accuracies at each epoch\n        print(f'Epoch #{self.epochs[-1]+1} >> train_acc={self.acc[-1]*100:.3f}%, train_loss={self.losses[-1]:.5f}')\n        print(f'Epoch #{self.epochs[-1]+1} >> val_acc={self.val_acc[-1]*100:.3f}%, val_loss={self.val_losses[-1]:.5f}')\n        \n    def on_train_begin(self, logs={}):\n        # initialize lists to store values from training\n        self.losses = []\n        self.val_losses = []\n        self.epochs = []\n        self.batch_no = []\n        self.acc = []\n        self.val_acc = []\n    \n    def on_epoch_end(self, epoch, logs={}):\n        # append values from the last epoch\n        self.losses.append(logs.get('loss'))\n        self.val_losses.append(logs.get('val_loss'))\n        self.acc.append(logs.get('acc'))\n        self.val_acc.append(logs.get('val_acc'))\n        self.epochs.append(epoch)\n        self.plot()  # update the graph\n        \n    def on_train_end(self, logs={}):\n        self.plot()\n               \nplotter = Plotter()","execution_count":null,"outputs":[]},{"metadata":{"id":"FOn03UIa6Pkb","trusted":true},"cell_type":"code","source":"# used to decrease the learning rate if val_acc doesn't enhance\nplateau_reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n                              patience=1, min_lr=1e-25)","execution_count":null,"outputs":[]},{"metadata":{"id":"6gkHd1wo6ZI3","trusted":true},"cell_type":"code","source":"# not used to stop the training but just to rollback to the best weights encountered during training\ne_stop = EarlyStopping(monitor='val_loss', patience=15, mode='min', restore_best_weights=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"oMk0mOS96aLn","trusted":true},"cell_type":"code","source":"callbacks = [plotter, plateau_reduce, e_stop]","execution_count":null,"outputs":[]},{"metadata":{"id":"yKYhUMih6SCf","trusted":true},"cell_type":"code","source":"# a Convolution block with optional pooling\ndef conv_block(x, filters, kernel_size, strides, layer_no, add_pool=False):\n    x = Conv2D(filters=filters, kernel_size=kernel_size, strides=strides, name=f'conv{layer_no}', \n               padding='same', kernel_initializer=he_normal(layer_no))(x)\n    x = Activation('relu', name=f'activation{layer_no}')(x)\n    x = BatchNormalization(name=f'bn{layer_no}')(x)\n    if add_pool:\n        x = MaxPooling2D(pool_size=[2, 2], strides=[2, 2], padding='same')(x)\n    return x","execution_count":null,"outputs":[]},{"metadata":{"id":"rxWAN3VB6T6v","trusted":true},"cell_type":"code","source":"# a Fully connected layer with activation, batchnorm and dropout\ndef dense_block(x, neurons, layer_no):\n    x = Dense(neurons, kernel_initializer=he_normal(layer_no), name=f'topDense{layer_no}')(x)\n    x = Activation('relu', name=f'Relu{layer_no}')(x)\n    x = BatchNormalization(name=f'BatchNorm{layer_no}')(x)\n    x = Dropout(0.5, name=f'Dropout{layer_no}')(x)\n    return x","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## I found that using an extremely small model performs a bit better that using pre-trained VGG16 on imagenet with a couple of fully connected layers on top so that is what we'll be using"},{"metadata":{"id":"arq2doK26WsP","trusted":true},"cell_type":"code","source":"def create_model(shape):\n    input_layer = Input(shape, name='input_layer')  # input layer with given shape\n    \n    conv1 = conv_block(input_layer, filters=32, kernel_size=[6, 6], strides=[2, 2], layer_no=1)\n    conv2 = conv_block(conv1, filters=64, kernel_size=[6, 6], strides=[3, 3], layer_no=2)\n    conv3 = conv_block(conv2, filters=128, kernel_size=[6, 6], strides=[3, 3], layer_no=3)\n   \n    flat1 = Flatten(name='Flatten1')(conv3)\n     \n    output_layer = Dense(30, name='Dense1')(flat1)\n    \n    model = Model(inputs=[input_layer], outputs=[output_layer])\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"id":"T8yyaMD26fAv","trusted":true},"cell_type":"code","source":"# hyperparameters\nheight, width, channels_num = 96, 96, 1\nlearning_rate = 0.01\nepochs = 40\nbatch_size = 32","execution_count":null,"outputs":[]},{"metadata":{"id":"bIq0UO1SFeyI","trusted":true},"cell_type":"code","source":"# define the proper loss function - same function used for evaluation\ndef rmse(y_true, y_pred):\n    return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1))","execution_count":null,"outputs":[]},{"metadata":{"id":"t7lWPgiE6lC4","outputId":"108d1c87-68b7-402b-92b3-c3e9b4c4ee2b","trusted":true},"cell_type":"code","source":"model = create_model((height, width, channels_num))\noptimizer = Adam(learning_rate)\n\nmodel.compile(optimizer=optimizer, loss=rmse, metrics=['acc'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"id":"vd1ay8ca6moe","outputId":"9dc25b34-856e-4295-897a-520377f717d0","trusted":true},"cell_type":"code","source":"model.fit(x=train_x, y=train_y, batch_size=batch_size, epochs=epochs, verbose=1, \n          callbacks=callbacks, validation_data=(valid_x, valid_y), shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"-zHLJFu16y_e","outputId":"ac96605a-a5ce-4175-ce4d-21629a175753","trusted":true},"cell_type":"code","source":"metrics = model.evaluate(test_x, test_y)  # evaluate on the labelled test set we kept aside early on","execution_count":null,"outputs":[]},{"metadata":{"id":"epeqDv4w61Cm","outputId":"fe628c2d-9cf5-465b-f665-6eaf606f4068","trusted":true},"cell_type":"code","source":"m_names = model.metrics_names\nprint(f'{m_names[0]} = {metrics[0]}\\n{m_names[1]} = {metrics[1]}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Not bad we did next to nothing in terms of preprocessing, we could have done a lot more, and ended up with pretty decent accuray on a very dirty dataset with mislabelled, missing values, and image sharpeness variant dataset. Not bad at all!"},{"metadata":{"id":"LFiq1mkwbClp","trusted":true},"cell_type":"code","source":"preds = model.predict(test_x)  # let's predict the same labelled test data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"let's plot the model predictions against the real keypoins and see how it holds up"},{"metadata":{"id":"YL5TyqkcO__9","outputId":"26cb1bb6-c70f-402e-ecde-2ec359333089","trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(2, 4, figsize=(20, 10))\naxs = axs.ravel()  # flatten the axs array from 2d to 1d \n\nfor ax in axs:\n    rnd_idx = np.random.randint(0, test_x.shape[0])  # select a random image\n    img = test_x[rnd_idx].reshape(96, 96)\n    pred = preds[rnd_idx]  # get the corresponding prediction\n    true_label = test_y[rnd_idx]  # get the corresponding label\n    \n    pred_row = wrap_as_row(pred, img)  # wrap them together as if they are a row from the dataframe\n    true_row = wrap_as_row(true_label, img) \n    ax = plot_keypoints(pred_row, plot_img=True, ax=ax, color='red')  # plot the prediction as red\n    plot_keypoints(true_row, plot_img=False, ax=ax, color='blue')  # plot the true keypoints as blue","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Given the nature of the datsaset and that we didn't really preprocess the data that much, the model is performing well, but could have been better."},{"metadata":{},"cell_type":"markdown","source":"Now let's load the csv files for submission and see their format and how to format our submission in the same way"},{"metadata":{"id":"7b9_cIMqdNgX","trusted":true},"cell_type":"code","source":"test_df = pd.read_csv('/root/data/test.csv')\nid_df = pd.read_csv('../input/IdLookupTable.csv')\nsample_df = pd.read_csv('../input/SampleSubmission.csv')","execution_count":null,"outputs":[]},{"metadata":{"id":"gjEBmaLDgwFU","outputId":"af360669-5469-44a2-814d-debae742169f","trusted":true},"cell_type":"code","source":"test_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"id":"OiOHOkaLg3up","outputId":"8120c77a-c09a-4cc4-ae23-2f5f2198dc1d","trusted":true},"cell_type":"code","source":"id_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"id":"r5zFQ65OhWZh","outputId":"662b41c3-fa9c-4a1c-bf18-a668a55beae2","trusted":true},"cell_type":"code","source":"sample_df.head(5) ","execution_count":null,"outputs":[]},{"metadata":{"id":"lVgOmZaZk8u3","outputId":"e2a69fc0-cd3b-426a-93e4-505e37fc198f","trusted":true},"cell_type":"code","source":"test_df.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"GQJkUeEBj6eH","outputId":"b39ca810-f0d2-43bd-9eed-25bc0e109e5a","trusted":true},"cell_type":"code","source":"# let's transform the test images into an ndarray just like we did with the training images\ntest_df.Image = test_df.Image.apply(list2img)\ntest_img = test_df.Image.values\ntest_img = np.apply_along_axis(lambda x: x.tolist(), axis=0, arr=test_img)\ntest_img.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"NGsAlrYz4g02","trusted":true},"cell_type":"code","source":"# normalize the test images just like the training images used in training the model\ntest_img = test_img.astype(float) / 255","execution_count":null,"outputs":[]},{"metadata":{"id":"ffeXjkLSlj6a","trusted":true},"cell_type":"code","source":"test_preds = model.predict(test_img.reshape(*test_img.shape, 1))  # our predictions","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### let's see some of the model predictions on the unlabelled data"},{"metadata":{"id":"nuHj2CPJmGEa","outputId":"5982e31a-1ed0-439c-fea6-2f14a38ce9f4","trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(2, 4, figsize=(20, 10))\naxs = axs.ravel()\n\nfor i in range(axs.size):\n    rnd_idx = np.random.randint(0, test_img.shape[0])\n\n    img = test_img[rnd_idx].reshape(96, 96)\n    pred = test_preds[rnd_idx]\n    pred_row = wrap_as_row(pred, img)\n\n    plot_keypoints(pred_row, plot_img=True, ax=axs[i], color='red')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now let's save the predictions in the required format"},{"metadata":{"id":"4aoHmIdTmgRc","trusted":true},"cell_type":"code","source":"# create a dataframe using the prediction because we'll be accessing certain features by feature/column name to\n# make our submission file so this way it'll be easier\npreds_df = pd.DataFrame(columns=train_df.columns[:-1], data=test_preds)","execution_count":null,"outputs":[]},{"metadata":{"id":"oRy5AgjGo2Ov","outputId":"e7c2086f-1591-4378-a2b4-521c3e00a9df","trusted":true},"cell_type":"code","source":"preds_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"id":"7_iMr-N1o421","outputId":"e5d8a8f0-c538-4c99-c8ae-2aa722faf4c8","trusted":true},"cell_type":"code","source":"id_df.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"v3S9G8tSpI99","outputId":"61ac64ce-1055-4108-fc5a-e6c027c1e991","trusted":true},"cell_type":"code","source":"# create an empty dataframe with the required columns\nsubmission = pd.DataFrame(columns=sample_df.columns, data=[])\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"MqhcfCoPpZi0","trusted":true},"cell_type":"code","source":"# iterate over the ids and get the name of the required feature name at each row\nfor columns, (row_id, img_id, feature, location) in id_df.iterrows():\n    submission.loc[row_id-1] = [row_id, preds_df[feature].iloc[img_id-1]]","execution_count":null,"outputs":[]},{"metadata":{"id":"as4hTeHOqkzH","outputId":"d6b5dfc8-e83e-44e5-ac7c-ee223036b1b8","trusted":true},"cell_type":"code","source":"submission.RowId = submission.RowId.astype(int)\nsubmission.head(5)","execution_count":null,"outputs":[]},{"metadata":{"id":"MVeC0xF-uWLC","outputId":"ae989712-e090-4f5d-f4e0-6a3635239695","trusted":true},"cell_type":"code","source":"submission.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"5LTb0zBIqwkC","outputId":"83d0184b-f893-4dc8-9ae2-94a4ad0c3f8f","trusted":true},"cell_type":"code","source":"id_df.head(5)  # check the format before saving","execution_count":null,"outputs":[]},{"metadata":{"id":"Uj8oIjVtsJyb","trusted":false},"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)  # save to csv file","execution_count":0,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We could have done so much better by preproessing the data more.\n### One way is by using other pre-trained models to to extract useful information from the image as bounding boxes for faces or segmentation results and give it as an input to the model.\n\n### But since the dataset contatin a lot of missing and mislabelled keypoints, and its images are variant in sharpness and have very low resolution, I would perfer to save the time of doing that on larger RGB images dataset and deploy them outside of the competition."},{"metadata":{"id":"uMxv8lDtuJiZ","trusted":false},"cell_type":"code","source":"","execution_count":0,"outputs":[]}],"metadata":{"colab":{"name":"facial_keypoints.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":4}