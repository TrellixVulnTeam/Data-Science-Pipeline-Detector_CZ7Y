{"cells":[{"metadata":{"id":"qiLp2OtHA0R-"},"cell_type":"markdown","source":"# Introduction and Set Up"},{"metadata":{"id":"hhz8o2UuLGEG"},"cell_type":"markdown","source":"Facial recognition is rapidly becoming a key topic in the field of machine learning. Facial recognition systems can be used to identify people in photos, video, or in real-time. Certain agencies are using this technology to identify people from photographs and many technological tools are using this technology to add security to user devices. \n\nThe first step in building something like this is to identify the facial keypoints from a image. Facial Keypoints are the areas of the nose, eyes, and mouth, that can be classified by a series of points, with coordinates (x, y), for that face. Every image of a human face has some amount of facial keypoints. With facial keypoints, we can obtain a plethora of information, including the identity of the person in the photo, their emotions, and even whether they are real or not. \n\nThis will allow future data scientists to do things such as develop new face filters, for a service like Snapchat, and research user emotions and poses. It will also medical experts to detect dysmorphic facial signs for medical diagnosis. Ultimately though, detecting facial keypoints is a difficult problem because facial features vary greatly from one image of an individual to another. There can be a large amount of variance between the poses, sizes, angles and positions of an image.\n\nFor our project, we will tackle some of the challenges head-on. Using 7049 images from Kaggleâ€™s Facial Keypoints Detection challenge, we have developed a framework for identifying these key points from specific photographs.\n\nFirst, we'll import the necessary Python packages used later in the document."},{"metadata":{"id":"peeXy2iv4RjB","outputId":"619914e8-182d-42d3-a903-0970bb509957","trusted":false},"cell_type":"code","source":"import json\nimport os\nimport zipfile\nfrom time import sleep\nfrom random import randint\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom IPython.display import clear_output\nfrom keras.callbacks import EarlyStopping\nfrom keras.layers import Activation, Convolution2D, MaxPooling2D, BatchNormalization, \\\n    Flatten, Dense, Dropout, Conv2D, MaxPool2D, ZeroPadding2D\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom keras.models import Sequential, Model, clone_model\nfrom keras.preprocessing.image import ImageDataGenerator\n!pip install keras_sequential_ascii\nfrom keras_sequential_ascii import keras2ascii\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"id":"eAT2UxM8C9Vs"},"cell_type":"markdown","source":"Next, the raw competition data is downloaded into the `/content/competitions` directory."},{"metadata":{"id":"tNZlCKW_4ZhI","outputId":"ef08f09d-2065-4b7f-c59d-039081e17091","trusted":false},"cell_type":"code","source":"# raw api token\napi_token = {\"username\":\"derektopper\",\n             \"key\":\"e2dc6654f1c2c5062f7349ca6c83ec5e\"}\n\n# create dir and write to readable json\nif not os.path.exists('/root/.kaggle'):\n  os.makedirs('/root/.kaggle')\nwith open('/root/.kaggle/kaggle.json', 'w') as file:\n    json.dump(api_token, file)\n!chmod 600 /root/.kaggle/kaggle.json\n\n# Download raw kaggle data\n!kaggle config set -n path -v /content\n!kaggle competitions download -c facial-keypoints-detection\n\n\n# unzip training and test data\nRoot_Dir = '/content/competitions/facial-keypoints-detection'\nfor file in os.listdir(Root_Dir):\n  if file.endswith('.zip'):\n    zip_ref = zipfile.ZipFile(os.path.join(Root_Dir, file), 'r')\n    zip_ref.extractall(Root_Dir)\n    zip_ref.close()\n  \n# read in data\ntrain_data = pd.read_csv(os.path.join(Root_Dir, 'training.csv'))  \ntest_data = pd.read_csv(os.path.join(Root_Dir, 'test.csv'))\nlookid_data = pd.read_csv(os.path.join(Root_Dir, 'IdLookupTable.csv'))\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"nd0n1H8g-Bxb"},"cell_type":"markdown","source":"\n# Exploratory Data Analysis (EDA)\n\n"},{"metadata":{"id":"TpW0WDmkDvGp"},"cell_type":"markdown","source":"Let's begin by taking a look at our training data. We'll print out the first 3 rows, columnwise for readability. We can see that there are xy coordinates for 15 different keypoints, alongside the actual image data stored as a vector in the `Image` column.\n\nAs we mentioned earlier, there are 15 different keypoints that we'll be identifying in this project. Those are:\n\n\n\n*   Left Eye (Center, Inner Corner, Outer Corner)\n*   Right Eye (Center, Inner Corner, Outer Corner)\n*   Left Eyebrow (Inner Corner, Outer Corner)\n*   Right Eyebrow (Inner Corner, Outer Corner)\n*   Nose (Tip)\n*   Mouth (Left Corner, Right Corner)\n*   Top Lip (Center)\n*   Bottom Lip (Center)\n\nThese are among the most important things that a computer vision system will look for when identifying people from images.\n"},{"metadata":{"id":"sME9n0I97WXz","outputId":"440e05fc-16e5-434b-baef-0827bb893087","trusted":false},"cell_type":"code","source":"train_data.head(3).T","execution_count":null,"outputs":[]},{"metadata":{"id":"zTOSevT-EySg"},"cell_type":"markdown","source":"To check the integrity of the data, let's see if there are any null values. Using the `Seaborn` package, we plot a simple heatmap. In the visual below, we can see that all samples have image data, there are a small set of samples with only a couple keypoints missing, and a large percentage of samples where it appears keypoints were stripped out. For these stripped-samples, the 4 keypoints available are:\n- centers of eyes (2)\n- tip of nose\n- center of bottom lip"},{"metadata":{"id":"UeFIGBrTPOkD","outputId":"cd3c59d7-ea93-40ae-cd49-fce48effe58e","trusted":false},"cell_type":"code","source":"# heatmap of null values\nplt.rcParams[\"figure.figsize\"]=(18, 6)\nsns.heatmap(~train_data.isnull().T,\n            xticklabels = False,\n            cbar =False)\nplt.suptitle('Heatmap of Missing Keypoints', size=18)\nplt.title('Black bars represent null values for sample features')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"NiOafcq9OFpV"},"cell_type":"markdown","source":"We'll split our training data into two sets:\n\n1.   \"Complete\" training set, containing only samples with no missing keypoints\n2.   \"Incomplete\" training set, containing only samples with at least 1 missing keypoint\n\n"},{"metadata":{"id":"dKA3lleBbVVM","outputId":"e4d7b3ea-7205-4f6a-f462-87199339e375","trusted":false},"cell_type":"code","source":"# subset into samples will complete keypoints or not\nincomplete_train_data = train_data[train_data.isnull().any(axis=1)]\ncomplete_train_data = train_data[~train_data.isnull().any(axis=1)]\n\nprint(f\"Samples with no missing keypoints: {len(complete_train_data)}\")\nprint(f\"Samples with missing keypoints: {len(incomplete_train_data)}\")","execution_count":null,"outputs":[]},{"metadata":{"id":"TqRLjzNyR-5m"},"cell_type":"markdown","source":"Next, let's split our features (image data) and labels (keypoints) into different arrays. We'll also go ahead and reshape our images into the appropriate 96x96 pixel image and normalize the 8-bit grayscale by dividing by 255. "},{"metadata":{"id":"QzfdaCYTPawR","trusted":false},"cell_type":"code","source":"# transform images and labels\ndef process_training_data(data):\n    images = []\n    keypoints = []\n    for idx, sample in data.iterrows():\n      # extract and reshape image\n      image = np.reshape(np.array(sample['Image'].split(' '),\n                                  dtype=int),\n                         (96,96,1))\n      images.append(image)\n        # extract keypoints\n      keypoints.append(sample.drop('Image'))\n    # convert keypoints to array\n    keypoints = np.array(keypoints, dtype = 'float')\n\n    # normalize grayscale images and convert to array\n    images = np.array(images)/255.\n    return images, keypoints\n\n# get complete training data\nX_train, y_train = process_training_data(train_data)\n\n# get complete training data\ncomplete_X_train, complete_y_train = process_training_data(complete_train_data)\n\n# get incomplete training data\nincomplete_X_train, incomplete_y_train = process_training_data(incomplete_train_data)","execution_count":null,"outputs":[]},{"metadata":{"id":"KCruaUkO1Uqw"},"cell_type":"markdown","source":"Let's see if we have any duplicate values in our \"Complete\" training set:"},{"metadata":{"id":"I0ZJABuF1TVN","outputId":"1c7df8f0-a66a-41b7-a2cf-4b00971f0e33","trusted":false},"cell_type":"code","source":"print(\"Size of complete X set: \")\nprint(complete_X_train.shape)\nprint(\"\\nSize of unique complete X set: \")\nprint(np.unique(complete_X_train, axis=0).shape)\nprint(\"\\nSize of complete y set: \")\nprint(complete_y_train.shape)\nprint(\"\\nSize of unique complete X set: \")\nprint(np.unique(complete_y_train, axis=0).shape)","execution_count":null,"outputs":[]},{"metadata":{"id":"6jgnwDd7TjHt"},"cell_type":"markdown","source":"It looks like there are no duplicates. Let's take a look at some of our \"complete\" samples that contain all 15 keypoints:\n\n\n"},{"metadata":{"id":"_jN6ccAFJs-n","outputId":"8c9d8c64-b936-4b04-da91-829cd31e1d7d","trusted":false},"cell_type":"code","source":"# plot images with keypoints\nfig, axs = plt.subplots(3, 3, figsize=(14, 14), sharex=True, sharey=True)\nfor i, ax in enumerate(axs.flatten()):\n    ax.imshow(np.reshape(complete_X_train[i]*(-1), [96, 96]), cmap='Greys')\n    ax.axis('off')\n    x_coords = []\n    y_coords = []\n    for col in train_data.columns.to_list():\n      if col.endswith('_x'):\n        x_coords.append(train_data[col][i])\n      if col.endswith('_y'):\n        y_coords.append(train_data[col][i])\n    ax.scatter(x_coords, y_coords, marker='+', c='red')\n    ax.scatter(x_coords, y_coords, s=100, c='blue', alpha=0.25)\nplt.suptitle('Images with complete keypoint set', size=18)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"id":"4xq7BwGNSapj"},"cell_type":"markdown","source":"As you can see from the above images, many of the items in our training data set are very accurate. Even despite features such as glasses, and different facial expressions, we have a good bit of data to work from. Not all of our images are like the ones above, however.\n\nIn the images below, we can see that our dataset has plenty of images that play around with these issues. For example, in the images below, there are faces that are looking upwards, are turned to the side, and are displaying varied facial expressions, such as smiles and sadness.\n\nUltimately, however, the images below are also missing one or more of the keypoints that I mentioned earlier. This is generally because of the way that the faces are turned. Let's take a look at some of our \"incomplete\" samples that do not contain all 15 keypoints:\n"},{"metadata":{"id":"VSFLFEAoea6q","outputId":"619c3c90-4587-422f-d0a7-153ba18c91c7","trusted":false},"cell_type":"code","source":"# plot images with keypoints\nfig, axs = plt.subplots(3, 3, figsize=(14, 14), sharex=True, sharey=True)\nfor i, ax in enumerate(axs.flatten()):\n    ax.imshow(np.reshape(incomplete_X_train[i+17]*(-1), [96, 96]), cmap='Greys')\n    ax.axis('off')\n    x_coords = []\n    y_coords = []\n    for j, key in enumerate(incomplete_y_train[i+17]):\n      if j%2 == 0:\n        x_coords.append(key)\n      else:\n        y_coords.append(key)\n    ax.scatter(x_coords, y_coords, marker='+', c='red')\n    ax.scatter(x_coords, y_coords, s=100, c='blue', alpha=0.25)\nplt.suptitle('Images with some missing keypoints', size=18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"NDt5Dg6dVqYt"},"cell_type":"markdown","source":"As you can see from the above images, many of the items in our training data set are missing important keypoints. Even despite having most of the relavant keypoints, and despite each keypoint being mostly present in the image. This is something we will need to account for. Some images have keypoints blokced by things like hair or glasses. This is also something we will need to account for. Not all of our \"incomplete\" images are even as complete like the ones above, however.\n\nIn the images below, we can see that our dataset has plenty of images that have very few keypoints. For example, in the images below, there are faces that appear like they should have all the features but only have 4 key points. Some are also distorted in some way, such as their angle.\n\nUltimately, for whatever reason, we have images with only 4 keypoints. We will need to come up with a way to account for this type of data, in our solution.\n\nLet's take a look at some of our \"incomplete\" samples that only contain 4 keypoints:\n"},{"metadata":{"id":"_tsRJ2VZUGEM","outputId":"7b674b90-e6b3-46b5-aa62-f18e20931e9d","trusted":false},"cell_type":"code","source":"# plot images with keypoints\nfig, axs = plt.subplots(3, 3, figsize=(14, 14), sharex=True, sharey=True)\nfor i, ax in enumerate(axs.flatten()):\n    ax.imshow(np.reshape(incomplete_X_train[i+150]*(-1), [96, 96]), cmap='Greys')\n    ax.axis('off')\n    x_coords = []\n    y_coords = []\n    for j, key in enumerate(incomplete_y_train[i+150]):\n      if j%2 == 0:\n        x_coords.append(key)\n      else:\n        y_coords.append(key)\n    ax.scatter(x_coords, y_coords, marker='+', c='red')\n    ax.scatter(x_coords, y_coords, s=100, c='blue', alpha=0.25)\nplt.suptitle('Stripped images with only 4 keypoints', size=18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"4GzPbybUXqnd"},"cell_type":"markdown","source":"Now that we have seen the quality of our dataset, we are going to need to fix the images, like the above. We need to have a strategy to handle the missing keypoint data. \n\nClearly, there are problems with the above images that we will need to account for."},{"metadata":{"id":"TmAjedbuKieH"},"cell_type":"markdown","source":"# Handling of Missing Keypoints\n\n"},{"metadata":{"id":"mpJ0gIhNlkLO"},"cell_type":"markdown","source":"There are quite a few options and considerations when considering how to handle the missing keypoints. We need our solution to be automatic and scalable, so that we don't have to manually add keypoints everytime we add new data. \n\nOne option would be to forward fill our training data. Forward filling is used to fill missing values in our dataframe. It will propagate the last valid observation forward. This is problematic for a number of reasons. As you will see when we plot this on images, it works for some of our data but does not work for all of our data."},{"metadata":{"id":"kmwBw2yKKzhH","trusted":false},"cell_type":"code","source":"# forward fill training data\nffilled_train_data = train_data.fillna(method = 'ffill')\n\n# subset to sample that were originally missing data\nincomplete_ffilled_train_data = ffilled_train_data[train_data.isnull().any(axis=1)]\n\n# process incomplete ffilled training data\nincomplete_ffilled_X_train, incomplete_ffilled_y_train = process_training_data(\n    incomplete_ffilled_train_data)","execution_count":null,"outputs":[]},{"metadata":{"id":"RYahurIvonNl","outputId":"ec1b950d-6646-4b90-ffa8-de9b2e6f33ad","trusted":false},"cell_type":"code","source":"# plot images with keypoints\nfig, axs = plt.subplots(4, 2, figsize=(8, 16), sharex=True, sharey=True)\nfor i, ax in enumerate(axs.flatten()):\n  if i%2 == 0:\n    i_orig = i\n    img = incomplete_X_train[i+150]\n    keys = incomplete_y_train[i+150]\n    ax.title.set_text('Original missing keypoints')\n  else:\n    img = incomplete_ffilled_X_train[i_orig+150]\n    keys = incomplete_ffilled_y_train[i_orig+150]\n    ax.title.set_text('Ffilled keypoints')\n  it = iter(keys)\n  xy_coords = [(x, y) for x, y in zip(it, it)]\n  ax.imshow(np.reshape(img*(-1), [96, 96]), cmap='Greys')\n  ax.axis('off')\n  ax.scatter(*zip(*xy_coords), marker='+', c='red')\n  ax.scatter(*zip(*xy_coords), s=100, c='blue', alpha=0.25)\nplt.suptitle('Images with forward filled missing keypoints')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"yQwaPbRdd2y1"},"cell_type":"markdown","source":"Looking at the images above, there are clear problems. While the first and third images appear to line up pretty well, the second and fourth images do not. For the first image, the right mouth corner and the left eye outer corner, appear to be slightly off, but the rest of the data appears to be good. For the second image, none of the keypoints are where they should be. For the third image, this approach appears to be a near perfect match. Finally, for the fourth image, the data is again very off. \n\nUltimately, from this, we determined that this was not a truly viable result. The keypoints will not be accurate if we are simply pasting the points from the last full facial image onto every image missing data. Every image is different so forward filling is not appropriate for this problem. Thus, we explored a new solution.\n"},{"metadata":{"id":"5vhwg_S8LEF2"},"cell_type":"markdown","source":"# Data Augmentation"},{"metadata":{"id":"YyHVJ69BZ3_i"},"cell_type":"markdown","source":"For now, we will move forward using only images that have all keypoints available. We will return to the images missing keypoints in later sections.\n\n\nIn order to make our model robust, we will augment our dataset with additional images. We use the `ImageDataGenerator` from `keras` to shift, shear, scale, and adjust brightness on some of the provided images. Then, we use a module that applies the same transformations to the keypoints, so that they map accurately onto the transformed images ([link to this module here](https://github.com/keras-team/keras-preprocessing/pull/132#issuecomment-537836519)). \n\nWe did not write this module, though we stepped through each method to ensure that the code was sound. This module takes the transform parameters generated by `ImageDataGenerator`, then applies them as an affline transform to the keypoint coordinates. It also checks if the transformed keypoints are within the new image bounding box. This means that if the transform moves the points outside of the image area, that is detected so the sample can be skipped."},{"metadata":{"id":"nRNixY7wWSgr","trusted":false},"cell_type":"code","source":"class ImageDataGeneratorLandmarks(ImageDataGenerator):\n    \"\"\"\n    Generator to transform the landmarks in the same way as the augmented (transformed) images.\n\n    This generator assumes all landmarks belong to images of the same size (width x height).\n    If you have annotated landmarks in images of different size but resize them later on,\n    for example using the target_size parameter of the ImageDataGenerator, then you must\n    first normalize your landmark coordinates to the target image size.\n    You can do so through the helper function normalize_landmarks in this module.\n\n    After transforming (rotating, scaling, shifting, ...) landmarks can end up outside visible part of the image.\n    This generator automatically calculates if the landmark is still visible. If not, the landmark-present\n    indicator is automatically set to 0.0.\n\n    Landmarks must be placed in the 'x' of the dataset and be encoded as follows:\n        Shape:\n            (batch_index, landmark, 3)\n\n        Where each landmark consists of 3 elements:\n            - X coordinate\n            - Y coordinate\n            - Present indicator (0.0 if not present, 1.0 if present)\n\n        Example:\n            [\n                [   [landmark_1_x, landmark_1_y, landmark_1_present],\n                    [landmark_2_x, landmark_2_y, landmark_2_present] ],\n                [   [landmark_1_x, landmark_1_y, landmark_1_present],\n                    [landmark_2_x, landmark_2_y, landmark_2_present] ]\n            ]\n\n    Code based on:\n    https://github.com/keras-team/keras-preprocessing/pull/132/commits\n    \"\"\"\n\n    def __init__(self, width, height,\n                 featurewise_center=False, samplewise_center=False, featurewise_std_normalization=False,\n                 samplewise_std_normalization=False, zca_whitening=False, zca_epsilon=1e-6, rotation_range=0,\n                 width_shift_range=0., height_shift_range=0., brightness_range=None, shear_range=0., zoom_range=0.,\n                 channel_shift_range=0., fill_mode='nearest', cval=0., horizontal_flip=False, vertical_flip=False,\n                 rescale=None, preprocessing_function=None, data_format=None, validation_split=0.0, dtype=None):\n        \"\"\"\n        :type width: Integer. Width of the images.\n        :type height: Integer. Height of the images.\n        \"\"\"\n        super().__init__(featurewise_center, samplewise_center, featurewise_std_normalization,\n                         samplewise_std_normalization, zca_whitening, zca_epsilon, rotation_range, width_shift_range,\n                         height_shift_range, brightness_range, shear_range, zoom_range, channel_shift_range, fill_mode,\n                         cval, horizontal_flip, vertical_flip, rescale, preprocessing_function, data_format,\n                         validation_split, dtype)\n        self.image_width = width\n        self.image_height = height\n\n    def flow_landmarks(self, landmarks, batch_size=32, shuffle=True, seed=None, subset=None):\n        # Convert landmarks to 4-rank to remain compatible with downstream Keras code\n        s = landmarks.shape\n        x = landmarks.reshape((s[0], s[1], s[2], 1))\n        return super().flow(x, None, batch_size, shuffle, None, seed, None, '', 'png', subset)\n\n    def apply_transform(self, x, transform_parameters):\n        # Fix translation!\n        # It is miscalculated because is it based the tensor shape is taken as image width/height.\n        wrong_img_shape = x.shape\n        img_row_axis = self.row_axis - 1\n        img_col_axis = self.col_axis - 1\n        tx_factor = self.image_height / wrong_img_shape[img_row_axis]\n        ty_factor = self.image_width / wrong_img_shape[img_col_axis]\n        transform_parameters['tx'] = transform_parameters['tx'] * tx_factor\n        transform_parameters['ty'] = transform_parameters['ty'] * ty_factor\n\n        # Landmarks are of a single sample. So a 3-rank array.\n\n        # Convert to 2-rank\n        landmarks = x.reshape((x.shape[0], x.shape[1]))\n\n        # Extract only the X/Y for transformations and transform\n        landmarks_xy = landmarks[:, 0:2]\n        landmarks_xy_transformed = _transform_landmarks(\n            self.image_width,\n            self.image_height,\n            landmarks_xy,\n            transform_parameters\n        )\n\n        # Determine if the landmarks are still inside the image\n        landmarks_visibilities = (landmarks_xy_transformed[:, 0] >= 0) & \\\n                                 (landmarks_xy_transformed[:, 0] < self.image_width) & \\\n                                 (landmarks_xy_transformed[:, 1] >= 0) & \\\n                                 (landmarks_xy_transformed[:, 1] < self.image_height)\n\n        # Pack result back in original landmarks (they are a copy anyway)\n        landmarks[:, 0:2] = landmarks_xy_transformed\n        landmarks[:, 2] = landmarks_visibilities\n\n        # Restore the 3-rank array\n        return landmarks.reshape(x.shape)\n\n\ndef derank_landmarks_output(landmarks_output):\n    \"\"\"\n    Removes the last dimension from the landmarks output by the ImageDataGeneratorLandmarks.\n\n    :param landmarks_output:\n    :return:\n    \"\"\"\n    return landmarks_output.reshape(landmarks_output.shape[:-1])\n\n\ndef normalize_landmarks(landmarks, image_widths, image_heights, target_width=1.0, target_height=1.0):\n    \"\"\"\n    Normalize image landmarks between 0.0 and 1.0 or between 0.0 and target image width/height.\n\n    Input:\n        Numpy array of landmarks of shape (n, 2+) where:\n            - n is the number of landmarks\n            - 2+ are the landmark x, landmark y and optionally other data that is not touched.\n\n    :param landmarks: Numpy array containing landmarks\n    :param image_widths: Numpy array of widths of images in same order as the landmarks\n    :param image_heights: Numpy array of heights of images in same order as the landmarks\n    :param target_width: Float. Optional. Width of target image, default 1.0\n    :param target_height: Float. Optional. Height of target image, default 1.0\n    :return: Copy of landmarks array with the x and y normalized.\n    \"\"\"\n    widths_correction = target_width / image_widths\n    heights_correction = target_height / image_heights\n\n    landmarks_normalized = landmarks.copy()\n    for i in range(landmarks.shape[1]):\n        landmarks_normalized[:, i, 0] = landmarks[:, i, 0] * widths_correction\n        landmarks_normalized[:, i, 1] = landmarks[:, i, 1] * heights_correction\n\n    return landmarks_normalized\n\n\ndef _transform_landmarks(width, height, landmarks, transform_parameters):\n    \"\"\"Applies a transformation to an tensor of landmarks on an image according to given parameters.\n    # Arguments\n        x: 3D tensor, single image. Required only to analyze image dimensions.\n        landmarks: 3D tensor, containing all the 2D points to be transformed.\n        transform_parameters: Dictionary with string - parameter pairs\n            describing the transformation.\n            Currently, the following parameters\n            from the dictionary are used:\n            - `'theta'`: Float. Rotation angle in degrees.\n            - `'tx'`: Float. Shift in the x direction.\n            - `'ty'`: Float. Shift in the y direction.\n            - `'shear'`: Float. Shear angle in degrees.\n            - `'zx'`: Float. Zoom in the x direction.\n            - `'zy'`: Float. Zoom in the y direction.\n            - `'flip_horizontal'`: Boolean. Horizontal flip.\n            - `'flip_vertical'`: Boolean. Vertical flip.\n    # Returns\n        A transformed version of the landmarks.\n    \"\"\"\n    landmarks = _affine_transform_points(landmarks, height, width,\n                                         transform_parameters.get('theta', 0),\n                                         transform_parameters.get('tx', 0),\n                                         transform_parameters.get('ty', 0),\n                                         transform_parameters.get('shear', 0),\n                                         transform_parameters.get('zx', 1),\n                                         transform_parameters.get('zy', 1)\n                                         )\n\n    landmarks = _flip_points(\n        landmarks,\n        height,\n        width,\n        flip_horizontal=transform_parameters.get('flip_horizontal', 0),\n        flip_vertical=transform_parameters.get('flip_vertical', 0)\n    )\n\n    return landmarks\n\n\ndef _affine_transform_points(points, height, width, theta=0, tx=0, ty=0, shear=0, zx=1, zy=1):\n    \"\"\"Applies an affine transformation of the points specified\n     by the parameters given.\n    # Arguments\n        points: 3D tensor, containing all the 2D points to be transformed.\n        height: Height of the image the points are part of\n        width: Width of the image the points are part of\n        theta: Rotation angle in degrees.\n        tx: Width shift.\n        ty: Height shift.\n        shear: Shear angle in degrees.\n        zx: Zoom in x direction.\n        zy: Zoom in y direction\n    # Returns\n        The transformed version of the points.\n    \"\"\"\n    transform_matrix = _get_affine_transform_matrix(\n        height, width,\n        theta, tx, ty, shear, zx, zy)\n\n    if transform_matrix is not None:\n        homogeneous_points = np.transpose(points)\n        homogeneous_points = np.insert(homogeneous_points[[1, 0]], 2, 1, axis=0)\n        inverse = np.linalg.inv(transform_matrix)\n        homogeneous_points = np.dot(inverse, homogeneous_points)\n        points = homogeneous_points[[1, 0]]\n        points = np.transpose(points)\n\n    return points\n\n\ndef _flip_points(points, height, width, flip_horizontal=False, flip_vertical=False):\n    \"\"\"Flips the coordinates of points in a frame with dimensions height and width\n    horizontally and/or vertically.\n    # Arguments\n        x: Point tensor. Must be 3D.\n        height: Height of the frame.\n        width: Width of the frame.\n        flip_horizontal: Boolean if coordinates shall be flipped horizontally.\n        flip_vertical: Boolean if coordinates shall be flipped vertically.\n    # Returns\n        Flipped Numpy point tensor.\n    \"\"\"\n    if flip_horizontal or flip_vertical:\n        points = points * np.array(\n            [1 - 2 * flip_horizontal, 1 - 2 * flip_vertical]) + np.array(\n            [width * flip_horizontal, height * flip_vertical])\n    return points\n\n\ndef _get_affine_transform_matrix(height, width, theta=0, tx=0, ty=0, shear=0, zx=1, zy=1):\n    \"\"\"Compute the affine transformation specified by the parameters given.\n\n    # Arguments\n        height: Height of the image to transform\n        width: Width of the image to transform\n        theta: Rotation angle in degrees.\n        tx: Width shift.\n        ty: Height shift.\n        shear: Shear angle in degrees.\n        zx: Zoom in x direction.\n        zy: Zoom in y direction\n\n    # Returns\n        The affine transformation matrix for the parameters\n        or None if no transformation is needed.\n    \"\"\"\n\n    transform_matrix = None\n    if theta != 0:\n        theta = np.deg2rad(theta)\n        rotation_matrix = np.array([[np.cos(theta), -np.sin(theta), 0],\n                                    [np.sin(theta), np.cos(theta), 0],\n                                    [0, 0, 1]])\n        transform_matrix = rotation_matrix\n\n    if tx != 0 or ty != 0:\n        shift_matrix = np.array([[1, 0, tx],\n                                 [0, 1, ty],\n                                 [0, 0, 1]])\n        if transform_matrix is None:\n            transform_matrix = shift_matrix\n        else:\n            transform_matrix = np.dot(transform_matrix, shift_matrix)\n\n    if shear != 0:\n        shear = np.deg2rad(shear)\n        shear_matrix = np.array([[1, -np.sin(shear), 0],\n                                 [0, np.cos(shear), 0],\n                                 [0, 0, 1]])\n        if transform_matrix is None:\n            transform_matrix = shear_matrix\n        else:\n            transform_matrix = np.dot(transform_matrix, shear_matrix)\n\n    if zx != 1 or zy != 1:\n        zoom_matrix = np.array([[zx, 0, 0],\n                                [0, zy, 0],\n                                [0, 0, 1]])\n        if transform_matrix is None:\n            transform_matrix = zoom_matrix\n        else:\n            transform_matrix = np.dot(transform_matrix, zoom_matrix)\n\n    if transform_matrix is not None:\n        transform_matrix = _transform_matrix_offset_center(\n            transform_matrix, height, width)\n\n    return transform_matrix\n\n\ndef _transform_matrix_offset_center(matrix, x, y):\n    o_x = float(x) / 2\n    o_y = float(y) / 2\n    offset_matrix = np.array([[1, 0, o_x], [0, 1, o_y], [0, 0, 1]])\n    reset_matrix = np.array([[1, 0, -o_x], [0, 1, -o_y], [0, 0, 1]])\n    transform_matrix = np.dot(np.dot(offset_matrix, matrix), reset_matrix)\n    return transform_matrix","execution_count":null,"outputs":[]},{"metadata":{"id":"8mPp__nMkirM"},"cell_type":"markdown","source":"In order to use the method above, we transform our keypoint coordinates into a array of xy keypoint pairs with an indicator of whether the keypoints are within the image bounding box. Since this is pre-transformation, all keypoints are within the image, so they are all set to 1."},{"metadata":{"id":"uZjmu3WpboDr","outputId":"ccdf0124-142c-46e2-deab-378f5857ef36","trusted":false},"cell_type":"code","source":"# function that reshapes keypoint\ndef reshape_keypoints(m):\n  \"\"\"\n  Reshapes keypoints and adds indicator that keypoint exists (1)\n\n  m: array of keypoints in shape [x1 y1 x2 y2 x3 y3 ...]\n  return: array of coordinate pairs [[x1 y1 1] [x2 y2 1] ...]\n  \"\"\"\n  it = iter(m)\n  return [(x, y, 1) for x, y in zip(it, it)]\n\n# function that unpacks keypoints\ndef unpack_keypoints(m):\n  \"\"\"\n  Reshapes keypoints and adds indicator that keypoint exists (1)\n\n  m: array of coordinate pairs [[x1 y1 1] [x2 y2 1] ...]\n  return: array of keypoints in shape [x1 y1 x2 y2 x3 y3 ...]\n  \"\"\"\n  unpacked = []\n  for i in m:\n    unpacked.append(i[0])\n    unpacked.append(i[1])\n  return np.array(unpacked)\n\n# reshape keypoints\nxy_keypoints = np.apply_along_axis(reshape_keypoints, 1, complete_y_train)\n\nprint(\"Example of keypoints before being reshaped:\")\nprint(\"[x1 y1 x2 y2 x3 y3 ...]\")\nprint(complete_y_train[0])\nprint(\"\\nExample of keypoints after being reshaped:\")\nprint(\"[[x1 y1 1] [x2 y2 1] [x3 y3 1] ...]\")\nprint(xy_keypoints[0])\nprint(\"\\nExample of keypoints after being unpacked:\")\nprint(\"[x1 y1 x2 y2 x3 y3 ...]\")\nprint(unpack_keypoints(xy_keypoints[0]))","execution_count":null,"outputs":[]},{"metadata":{"id":"T3_dbN0okoz_"},"cell_type":"markdown","source":"We then defined our transformation arguments as ranges and initialize generators for the images and keypoints with the same seed. The generators will apply transformations from the provided ranges randomly, and the seed will ensure that the keypoints and images are transformed the same way."},{"metadata":{"id":"YUtC5fyJWSgr","trusted":false},"cell_type":"code","source":"# define transformation arguments\ndata_gen_args = dict(\n    zoom_range=[0.8, 1.2],\n    shear_range=5,\n    rotation_range=10,\n    brightness_range=[0.8, 1.2],\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    horizontal_flip=False,\n)\n\n# initialize generators\nimage_datagen = ImageDataGenerator(**data_gen_args)\nlandmark_datagen = ImageDataGeneratorLandmarks(width=96,\n                                               height=96,\n                                               **data_gen_args)\n\n# set seed so transformations are the same for keypoints and images\nseed = 123\n\n# Combine generators \ntrain_generator = zip(image_datagen.flow(x=complete_X_train, seed=seed),\n                      landmark_datagen.flow_landmarks(xy_keypoints, seed=seed))","execution_count":null,"outputs":[]},{"metadata":{"id":"LdBkZtlPcN1z"},"cell_type":"markdown","source":"Let's test out our workflow. We plot 9 images with random transformations below, and are able to confirm that the keypoints were appropriately mapped."},{"metadata":{"id":"fhojbXbVWSgr","outputId":"eaf58413-700e-44f5-c579-d729ef5132ec","trusted":false},"cell_type":"code","source":"# plot images with keypoints\nfig, axs = plt.subplots(3, 3, figsize=(14, 14), sharex=True, sharey=True)\nfor i, ax in enumerate(axs.flatten()):\n  for img, landmarks in train_generator:\n    # skip if landmarks are outside of boundary\n    if np.any(landmarks[0,:,2] == 0):\n      continue\n    ax.imshow(img[0,:,:,0].reshape(96,96)*(-1), cmap='Greys')\n    ax.scatter(landmarks[0,:,0], landmarks[0,:,1], marker='+', c='red')\n    ax.scatter(landmarks[0,:,0], landmarks[0,:,1], s=100, c='blue', alpha=0.25)\n    ax.axis('off')\n    break\nplt.suptitle('Images with random transformations', size=18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"VEpND9mkkuj5"},"cell_type":"markdown","source":"Now that we have confirmed the method works appropriately, we will generate a large set of transformed images to augment our data with."},{"metadata":{"id":"keJNR_TBkyYk","trusted":false},"cell_type":"code","source":"# define function to generate images and keypoints\ndef augment_data(X, y, size):\n  i = 0\n  augmented_X_train = np.copy(X)\n  augmented_y_train = np.copy(y)\n  # generate batches of images and keypoints\n  for imgs, landmarks in train_generator:\n    # iterate over keypoints in batch\n    for ind, landmark in enumerate(landmarks):\n      # skip where any keypoint is outside boundary\n      if np.any(landmark[:, 2]==0):\n        continue\n      # append image\n      augmented_X_train = np.append(augmented_X_train,\n                                    np.expand_dims(imgs[ind], axis=0),\n                                    axis=0)\n      augmented_y_train = np.append(augmented_y_train,\n                                    unpack_keypoints(landmark).T,\n                                    axis=0)\n      i += 1\n      if i == size:\n        return augmented_X_train, augmented_y_train\n\naugmented_X_train, augmented_y_train = augment_data(complete_X_train,\n                                                    complete_y_train,\n                                                    4000)","execution_count":null,"outputs":[]},{"metadata":{"id":"ECO_fmxYLL-d"},"cell_type":"markdown","source":"# Training a CNN Model"},{"metadata":{"id":"4mRIZd1TD7Xb"},"cell_type":"markdown","source":"For the model portion to actually predict new keypoints, we are using a Convolutional Neural Network, also known as a CNN. A CNN is a type of neural network that uses Convolutional layers, a.k.a. Conv layers, which are based on the mathematical operation of convolution. A Conv layer works by effectively overlaying a filter on top of an image at some location, then preforming element-wise multiplication between the values in the filter and their corresponding values in the image. It then sums the element-wise products and outputs the value as a single pixel, before repeating this for every pixel. This is useful to help better denote the edges of images. \n\nBefore creating our final complex model, we will first take a look at how a simple model behaves. Below we explore a model with only 5 layers:"},{"metadata":{"id":"y_xKfls7Neeo","trusted":false},"cell_type":"code","source":"#Basic Model\nmodel = Sequential([Flatten(input_shape=(96,96)),\n                    Dense(128, activation=\"relu\"),\n                    Dropout(0.1),\n                    Dense(64, activation=\"relu\"),\n                    Dense(30)])\n","execution_count":null,"outputs":[]},{"metadata":{"id":"dqNe_l4SR0CF"},"cell_type":"markdown","source":"This is a summary of how our base model works. We see that the `Dense` layer adds over one million trainable parameters."},{"metadata":{"id":"cUsOBw-xNeeo","outputId":"8e7e4675-fa77-4d31-9093-2d03e8b70749","trusted":false},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"id":"ljknsb5nR6FJ"},"cell_type":"markdown","source":"The `keras2ascii` module lets us create a visualization of our base model.\n\n"},{"metadata":{"id":"tiQOfKKukfeM","outputId":"282956a1-4cd4-4392-a613-75bc1a253a9c","trusted":false},"cell_type":"code","source":"keras2ascii(model)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"lDbQOVjKQ8rZ"},"cell_type":"markdown","source":"### Model\n\nMany Keras models are built using the Sequential class, which represents a linear stack of layers. We will be progressively adding layers to our sequence.\n\nFirstly, we start by adding a conv layer, as described earlier. The first convolution layer has an input_shape of 96 x 96, which is our image shape. It has 32 filters and a filter size of 3x3, meaning that the convolution approach from earlier will use a 3x3 grid. Finally, the padding level of 'same' means that our data will try to pad a single layer evenly on the left and right, to make the convolution work correctly.\n \nNext, we apply a leaky ReLU, which an activation function that is added to layers in neural networks to add nonlinearity, which is important to handle modern complex and nonlinear datasets. In it, each neuron computes a dot product and adds a bias value before the value is output to the neurons in the subsequent layer.\n\nThen we apply batch normalization, which applies a transformation that maintains the mean output close to 0 and the output standard deviation close to 1. We then run each of those things again to maximize the efficiency of each piece.\n\nFinally, before moving on to the number of filters in a new conv layer, we add in a max pooling layer to tie it all together. A max pooling layer is a type of operation that reduces the dimensionality of each image by reducing the number of pixels in the output from the previous convolutional layer. THis helps to reduce our computational load and reduce overfitting the data.\n\nWe then do this for a variety of different filter inputs, before implementing a flatten layer. Flatten is used to flatten the input, which is how we pass the data into the dense layers we'll need. We then use a dense layer, which is a fully connected layer where every neuron is connected to every neuron in the next layer. We also use a ReLU activation function, similar to the Leaky ReLU described above.\n\nNext, we use a dropout level of 0.5 to cut down on excess association among features by dropping the weights (edges) at a probability. \n\nFinally, we take this and apply to a dense layer of 30 to produce our 30 keypoint estimations. We also use an adam optimizer, which is an optimization algorithm that can be used instead of the classical stochastic gradient descent procedure to update network weights iterative based in training data.\n\nAll of this comprises what became our CNN prediction model.\n\n\n\n"},{"metadata":{"id":"B5339woONeeo","outputId":"7981da83-66a0-45e6-c723-e514722c1067","trusted":false},"cell_type":"code","source":"model = Sequential()\n\n\nmodel.add(Convolution2D(32, (3,3), padding='same', use_bias=False, input_shape=(96,96,1)))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\n\nmodel.add(Convolution2D(32, (3,3), padding='same', use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\nmodel.add(Convolution2D(64, (3,3), padding='same', use_bias=False, input_shape=(96,96,1)))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\n\nmodel.add(Convolution2D(64, (3,3), padding='same', use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\nmodel.add(Convolution2D(96, (3,3), padding='same', use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\n\nmodel.add(Convolution2D(96, (3,3), padding='same', use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\nmodel.add(Convolution2D(128, (3,3),padding='same', use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\n\nmodel.add(Convolution2D(128, (3,3),padding='same', use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\nmodel.add(Convolution2D(256, (3,3),padding='same',use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\n\nmodel.add(Convolution2D(256, (3,3),padding='same',use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\nmodel.add(Convolution2D(512, (3,3), padding='same', use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\n\nmodel.add(Convolution2D(512, (3,3), padding='same', use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\n\n\n\nmodel.add(Flatten())\nmodel.add(Dense(512,activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(30))\n\nmodel.compile(optimizer='adam', \n              loss='mean_squared_error',\n              metrics=['mae', 'acc'])\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"id":"56H7jBJ0cfuJ"},"cell_type":"markdown","source":"This is a visualization of our final model."},{"metadata":{"id":"J60-fn26Neeo","outputId":"eb246f51-74f7-499e-8256-ca232ec07329","trusted":false},"cell_type":"code","source":"keras2ascii(model)","execution_count":null,"outputs":[]},{"metadata":{"id":"Q3oMJU5XckHi"},"cell_type":"markdown","source":"Let's compare the performance of this CNN design using 3 different sets of data to train.\n\n1.   Full data set including missing keypoints\n2.   \"Complete\" data set with only images that have no missing keypoints\n3.   \"Augmented\" data set consisting of the complete dataset and 4000 additional random transformations\n\n\n"},{"metadata":{"id":"nsVQCDqRx2-z","trusted":false},"cell_type":"code","source":"# create copies of untrained model\nbase_model = clone_model(model)\ncomplete_model = clone_model(model)\naugmented_model = clone_model(model)\npred_keypoints_model = clone_model(model)\n\n# compile new models\nbase_model.compile(optimizer='adam', \n                   loss='mean_squared_error',\n                   metrics=['mae', 'acc'])\ncomplete_model.compile(optimizer='adam', \n                       loss='mean_squared_error',\n                       metrics=['mae', 'acc'])\naugmented_model.compile(optimizer='adam', \n                        loss='mean_squared_error',\n                        metrics=['mae', 'acc'])\npred_keypoints_model.compile(optimizer='adam',\n                             loss='mean_squared_error',\n                             metrics=['mae', 'acc'])\n","execution_count":null,"outputs":[]},{"metadata":{"id":"GuUb0_GHTnk6","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"iimKlkoV1ZtW"},"cell_type":"markdown","source":"We'll start by fitting the a model with the full data, including missing keypoints:"},{"metadata":{"id":"u-LE2qicNeeo","outputId":"a5b89927-60a9-4f3f-f206-1423c7f965ae","trusted":false},"cell_type":"code","source":"history = base_model.fit(X_train,\n                        y_train,\n                        epochs = 100,\n                        batch_size = 128,\n                        validation_split=0.05)","execution_count":null,"outputs":[]},{"metadata":{"id":"Cu2yzFzq7npQ"},"cell_type":"markdown","source":"Next we'll fit a model to the provided images with no missing keypoints."},{"metadata":{"id":"dZ941UnAvVyr","outputId":"c84890e6-b302-4275-87ff-9653b749f5df","trusted":false},"cell_type":"code","source":"complete_history = complete_model.fit(complete_X_train,\n                    complete_y_train,\n                    epochs = 100,\n                    batch_size = 128,\n                    validation_split=0.05)","execution_count":null,"outputs":[]},{"metadata":{"id":"VstZncJp7wE_"},"cell_type":"markdown","source":"Finally, we'll fit a model to our augmented dataset, containing images with no missing keypoints alongside the additional generated random transformations of those images:"},{"metadata":{"id":"L78PLQcBvWzL","outputId":"8dad3921-f938-4185-be6b-96307889131c","trusted":false},"cell_type":"code","source":"augmented_history = augmented_model.fit(augmented_X_train,\n                    augmented_y_train,\n                    epochs = 100,\n                    batch_size = 128,\n                    validation_split=0.05)","execution_count":null,"outputs":[]},{"metadata":{"id":"EMUnNd3tNeep"},"cell_type":"markdown","source":"In training our model, I have kept a running tally of some of the early attempts of our base CNN. Below are the number of filters in each layer, and then the two dense layer inputs are in parentheses. For each of these attempts, I have shown the attempts that had higher accuracies and lower MAE values.\n\nPre Data Augmentation:\n* 32-48-64-96-128-256 (256-30): Acc: 0.7224, Mae: 3.9815\n* 32-64-128-256-512 (128-30): Acc: 0.6229, Mae: 5.9423\n* 48-64-96-128-192-256 (128-30): Acc: 0.6186, MAE: 5.8305\n* 32-64-96-128-256-512 (128-30): Acc: 0.6474, MAE: 5.4123\n* 32-48-64-96-128-256 (128-30): Acc: 0.6519, MAE: 5.377\n* 32-64-96-128-192-256 (256-30): Acc: 0.7119, MAE: 4.0641\n* 32-64-96-128-256-512 (512-30): Acc: 0.7525, MAE: 3.2551\n* 32-48-64-96-128-256-512 (512-30): Acc: 0.6007, MAE: 3.2089\n* 64-128-256-512 (512-30): Acc: 0.6266, MAE: 5.1721\n* 32-64-96-128-256-512-1024 (512-30): Acc: 0.6160, MAE: 3.3356\n* 32-64-96-128-256-512-1024 (1024-30): Acc: 0.6056, MAE: 2.8476\n"},{"metadata":{"id":"90T9BNDIhl03"},"cell_type":"markdown","source":"Below is the visualizations of the MAE vs Epoch, Accuracy vs Epoch and Loss vs Epoch for each version of the model. Recall that we split off 5% of our samples as a validation set, retaining 95% of samples as the training set."},{"metadata":{"id":"1vLDdFh6yTKK","outputId":"e64d5c50-ef18-482b-dc86-ec771541f54b","trusted":false},"cell_type":"code","source":"# plot to show training MAE over time\nplt.plot(history.history['mae'])\nplt.plot(complete_history.history['mae'])\nplt.plot(augmented_history.history['mae'])\nplt.title('Mean Absolute Error vs Epoch (training data)')\nplt.ylabel('Mean Absolute Error')\nplt.xlabel('Epochs')\nplt.legend(['Full data set',\n            '\"Complete\" data set',\n            '\"Augmented\" data set'], loc='upper right')\nplt.show()\n\n# plot to show validation MAE over time\nplt.plot(history.history['val_mae'])\nplt.plot(complete_history.history['val_mae'])\nplt.plot(augmented_history.history['val_mae'])\nplt.title('Mean Absolute Error vs Epoch (validation data)')\nplt.ylabel('Mean Absolute Error')\nplt.xlabel('Epochs')\nplt.legend(['Full data set',\n            '\"Complete\" data set',\n            '\"Augmented\" data set'], loc='upper right')\nplt.show()\n\n# plot to show training accuracy over time\nplt.plot(history.history['acc'])\nplt.plot(complete_history.history['acc'])\nplt.plot(augmented_history.history['acc'])\nplt.title('Accuracy vs Epoch (training data)')\nplt.ylabel('Accuracy')\nplt.xlabel('Epochs')\nplt.legend(['Full data set',\n            '\"Complete\" data set',\n            '\"Augmented\" data set'], loc='upper left')\nplt.show()\n\n# plot to show validation accuracy over time\nplt.plot(history.history['val_acc'])\nplt.plot(complete_history.history['val_acc'])\nplt.plot(augmented_history.history['val_acc'])\nplt.title('Accuracy vs Epoch (validation data)')\nplt.ylabel('Accuracy')\nplt.xlabel('Epochs')\nplt.legend(['Full data set',\n            '\"Complete\" data set',\n            '\"Augmented\" data set'], loc='upper left')\nplt.show()\n\n# plot to show training loss over time\nplt.plot(history.history['loss'])\nplt.plot(complete_history.history['loss'])\nplt.plot(augmented_history.history['loss'])\nplt.title('Loss vs Epoch (training data)')\nplt.ylabel('Loss')\nplt.xlabel('Epochs')\nplt.legend(['Full data set',\n            '\"Complete\" data set',\n            '\"Augmented\" data set'], loc='upper left')\nplt.show()\n\n# plot to show validation loss over time\nplt.plot(history.history['val_loss'])\nplt.plot(complete_history.history['val_loss'])\nplt.plot(augmented_history.history['val_loss'])\nplt.title('Loss vs Epoch (validation data)')\nplt.ylabel('Loss')\nplt.xlabel('Epochs')\nplt.legend(['Full data set',\n            '\"Complete\" data set',\n            '\"Augmented\" data set'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"CMF-DsINJB_c"},"cell_type":"markdown","source":"Above, it is clear that the Full data set model was unsuccessful. This is unfortunately due to the missing keypoint values. While the Complete model and Augmented model ended up with comparable results, the Augmented model appears to train in fewer epochs. At first glance, it appears that the Augmented model performs significantly better in the validation set, but this is likely due to the fact that the additional transformed images are modifications of other samples within the same set. This means that it is quite likely that in the 5% validation split, many of the samples are transformed versions of samples in the 95% training split. With all this in mind, let's move forward using our Augmented data set model."},{"metadata":{"id":"x-5xJAWe_Ojc"},"cell_type":"markdown","source":"# Evaluating Predictions"},{"metadata":{"id":"aYidjhq-_SVr"},"cell_type":"markdown","source":"Let's take a look at some predictions from our models"},{"metadata":{"id":"mxGWiuzjDNAy","trusted":false},"cell_type":"code","source":"# generate predictions\nbase_pred = base_model.predict(incomplete_X_train)\ncomplete_pred = complete_model.predict(incomplete_X_train)\naugmented_pred = augmented_model.predict(incomplete_X_train)\n\n# reshape predicted keypoints\nbase_pred_xy = np.apply_along_axis(reshape_keypoints, 1, base_pred)\ncomplete_pred_xy = np.apply_along_axis(reshape_keypoints, 1, complete_pred)\naugmented_pred_xy = np.apply_along_axis(reshape_keypoints, 1, augmented_pred)","execution_count":null,"outputs":[]},{"metadata":{"id":"KukaZ7IqCuMC","outputId":"e5e2a391-6c67-4622-97d8-afe770496e69","trusted":false},"cell_type":"code","source":"# plot images with keypoints\nfig, axs = plt.subplots(9, 3, figsize=(12, 36), sharex=True, sharey=True)\nfor i, ax in enumerate(axs.flatten()):\n  if i in range(0, 27, 3):\n    i_orig = i\n    img = incomplete_X_train[i+200]\n    true_keys = incomplete_y_train[i+200]\n    keys = base_pred_xy[i+200]\n    ax.title.set_text('Base model')\n  elif i in range(1, 27, 3):\n    keys = complete_pred_xy[i_orig+200]\n    ax.title.set_text('Complete model')\n  else:\n    keys = augmented_pred_xy[i_orig+200]\n    ax.title.set_text('Augmented model')\n  ax.imshow(np.reshape(img*(-1), [96, 96]), cmap='Greys')\n  ax.axis('off')\n  ax.scatter(keys[:,0],\n             keys[:,1],\n             marker='+', c='red')\n  ax.scatter(keys[:,0],\n             keys[:,1],\n             s=100, c='blue', alpha=0.25)\n  it = iter(true_keys)\n  xy_coords = [(x, y) for x, y in zip(it, it)]\n  ax.scatter(*zip(*xy_coords), marker='x', c='red')\nplt.suptitle('Predicted keypoints\\n(Actual keypoints marked with red x)', size=18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"xOZJE2TNLSZf"},"cell_type":"markdown","source":"Visually, both the Augmented and Complete models have similar accuracy. They both appear quite accurate for some samples, while clearly inaccurate for others.  There does not appear to be enough evidence here to suggest that one model is substantially better than the other. We will continue with our Augmented model."},{"metadata":{"id":"MMVmUx1DinXy"},"cell_type":"markdown","source":"# Back to the Missing Keypoints"},{"metadata":{"id":"KSma9RrOirb0"},"cell_type":"markdown","source":"Now we'll use our predicted keypoints from the Augmented model to fill the missing keypoints in the \"incomplete\" set. We'll take our predicted keypoints and replace only the null values in the incomplete training keypoint set for the sample."},{"metadata":{"id":"iA5tvHl1iyt0","outputId":"dde72af7-11bc-4bd0-8979-9594527511d1","trusted":false},"cell_type":"code","source":"def merge_keypoints(incomplete, predicted):\n  return np.where(np.isnan(incomplete), predicted, incomplete)\n\n\nprint(\"Original incomplete:\")\nprint(incomplete_y_train[-1])\nprint(\"\\nPredicted:\")\nprint(augmented_pred[-1])\nprint(\"\\nMerged:\")\nprint(np.where(\n    np.isnan(incomplete_y_train[-1]),\n    augmented_pred[-1],\n    incomplete_y_train[-1]\n))\n\n# plot images with keypoints\nfig, axs = plt.subplots(4, 2, figsize=(8, 16), sharex=True, sharey=True)\nfor i, ax in enumerate(axs.flatten()):\n  if i in range(0, 16, 2):\n    i_orig = i\n    img = incomplete_X_train[i+310]\n    keys = incomplete_y_train[i+310]\n    ax.title.set_text('\"Incomplete\" keypoints')\n  elif i in range(1, 16, 2):\n    keys = merge_keypoints(incomplete_y_train[i_orig+310],\n                           augmented_pred[i_orig+310])\n    ax.title.set_text('Predicted missing')\n  ax.imshow(np.reshape(img*(-1), [96, 96]), cmap='Greys')\n  ax.axis('off')\n  it = iter(keys)\n  xy_coords = [(x, y) for x, y in zip(it, it)]\n  ax.scatter(*zip(*xy_coords), marker='x', c='red')\n  ax.scatter(*zip(*xy_coords), s=100, c='blue', alpha=0.25)\nplt.suptitle('\\nFilling missing keypoints with predicted values', size=18)\nplt.show()\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"8JEHROG7sXFh"},"cell_type":"markdown","source":"While the above method was not perfect, it does appear to perform much better than forward-filling the missing points. Let's take our Augmented data set, and append this set of predicted points."},{"metadata":{"id":"tchCzjh-s8Oa","trusted":false},"cell_type":"code","source":"# merge missing points\npred_keypoints_y_train = np.where(\n    np.isnan(incomplete_y_train),\n    augmented_pred,\n    incomplete_y_train\n)\npred_keypoints_X_train = incomplete_X_train\n# append to augmented data\npred_keypoints_X_train = np.append(augmented_X_train, pred_keypoints_X_train, axis=0)\npred_keypoints_y_train = np.append(augmented_y_train, pred_keypoints_y_train, axis=0)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"IIVHrCy1u3nj"},"cell_type":"markdown","source":"# Additional CNN models"},{"metadata":{"id":"q9ySVgNzMdRx"},"cell_type":"markdown","source":"We'll reproduce the steps we followed before, now using the largest possible dataset. To recap, we will train a model with the same configuration as before on the following data:\n1.   All provided images that had no missing values (15 keypoints total) \n2.   4000 random transformations of the aforementioned images\n3.   All provided images that had missing values, with the missing values populated via predictions from our Augmented model.\n\n"},{"metadata":{"id":"dIqH4QSauzd5","outputId":"665b1c0f-ba76-487d-c7a7-55cfa9b2228d","trusted":false},"cell_type":"code","source":"pred_keypoints_history = pred_keypoints_model.fit(pred_keypoints_X_train,\n                    pred_keypoints_y_train,\n                    epochs = 100,\n                    batch_size = 128,\n                    validation_split=0.05)","execution_count":null,"outputs":[]},{"metadata":{"id":"XXI-8wJ6Ne6R"},"cell_type":"markdown","source":"Let's compare our model metrics for all 3 versions:\n---\n\n"},{"metadata":{"id":"p_t7FveGvU7v","outputId":"c03b5846-5a48-48f3-827a-25b3eb171220","trusted":false},"cell_type":"code","source":"# plot to show training MAE over time\nplt.plot(pred_keypoints_history.history['mae'])\nplt.plot(complete_history.history['mae'])\nplt.plot(augmented_history.history['mae'])\nplt.title('Mean Absolute Error vs Epoch (training data)')\nplt.ylabel('Mean Absolute Error')\nplt.xlabel('Epochs')\nplt.legend(['\"Predicted missing\" data set',\n            '\"Complete\" data set',\n            '\"Augmented\" data set'], loc='upper right')\nplt.show()\n\n# plot to show validation MAE over time\nplt.plot(pred_keypoints_history.history['val_mae'])\nplt.plot(complete_history.history['val_mae'])\nplt.plot(augmented_history.history['val_mae'])\nplt.title('Mean Absolute Error vs Epoch (validation data)')\nplt.ylabel('Mean Absolute Error')\nplt.xlabel('Epochs')\nplt.legend(['\"Predicted missing\" data set',\n            '\"Complete\" data set',\n            '\"Augmented\" data set'], loc='upper right')\nplt.show()\n\n# plot to show training accuracy over time\nplt.plot(pred_keypoints_history.history['acc'])\nplt.plot(complete_history.history['acc'])\nplt.plot(augmented_history.history['acc'])\nplt.title('Accuracy vs Epoch (training data)')\nplt.ylabel('Accuracy')\nplt.xlabel('Epochs')\nplt.legend(['\"Predicted missing\" data set',\n            '\"Complete\" data set',\n            '\"Augmented\" data set'], loc='upper left')\nplt.show()\n\n# plot to show validation accuracy over time\nplt.plot(pred_keypoints_history.history['val_acc'])\nplt.plot(complete_history.history['val_acc'])\nplt.plot(augmented_history.history['val_acc'])\nplt.title('Accuracy vs Epoch (validation data)')\nplt.ylabel('Accuracy')\nplt.xlabel('Epochs')\nplt.legend(['\"Predicted missing\" data set',\n            '\"Complete\" data set',\n            '\"Augmented\" data set'], loc='upper left')\nplt.show()\n\n# plot to show training loss over time\nplt.plot(pred_keypoints_history.history['loss'])\nplt.plot(complete_history.history['loss'])\nplt.plot(augmented_history.history['loss'])\nplt.title('Loss vs Epoch (training data)')\nplt.ylabel('Loss')\nplt.xlabel('Epochs')\nplt.legend(['\"Predicted missing\" data set',\n            '\"Complete\" data set',\n            '\"Augmented\" data set'], loc='upper left')\nplt.show()\n\n# plot to show validation loss over time\nplt.plot(pred_keypoints_history.history['val_loss'])\nplt.plot(complete_history.history['val_loss'])\nplt.plot(augmented_history.history['val_loss'])\nplt.title('Loss vs Epoch (validation data)')\nplt.ylabel('Loss')\nplt.xlabel('Epochs')\nplt.legend(['\"Predicted missing\" data set',\n            '\"Complete\" data set',\n            '\"Augmented\" data set'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"lX-rOP9fPHTd"},"cell_type":"markdown","source":"It would appear that our model built with predicted missing data is unstable, based on the validation accuracy. Let's fit one last time, with early stopping to attempt to stop the model at a high-accuracy epoch.\n\n"},{"metadata":{"id":"S0iIJZclPcM2","outputId":"e9b49344-97f0-4dc5-fd8e-4d1ecab88881","trusted":false},"cell_type":"code","source":"# define ES\nes = EarlyStopping(monitor='val_acc', patience=75)\n# predict again\npred_keypoints_history = pred_keypoints_model.fit(pred_keypoints_X_train,\n                    pred_keypoints_y_train,\n                    epochs = 75,\n                    batch_size = 128,\n                    validation_split=0.05,\n                    callbacks=[es])","execution_count":null,"outputs":[]},{"metadata":{"id":"3pITekb3Pa8k"},"cell_type":"markdown","source":"Now that we've stopped our fitting process at a more accurate point, we can continue. Below, we take a look at some predictions from our 3 functional models:"},{"metadata":{"id":"6ZQ1nDMBPV7c","outputId":"859ec5af-abe2-4d94-b7e2-d216e55e32e4","trusted":false},"cell_type":"code","source":"# predict keypoints\npred_keypoints_pred = pred_keypoints_model.predict(incomplete_X_train)\n\n# reshape keypoints\npred_keypoints_pred_xy = np.apply_along_axis(reshape_keypoints,\n                                             1,\n                                             pred_keypoints_pred)\n\n# plot images with keypoints\nfig, axs = plt.subplots(9, 3, figsize=(12, 36), sharex=True, sharey=True)\nfor i, ax in enumerate(axs.flatten()):\n  if i in range(0, 27, 3):\n    i_orig = i\n    img = incomplete_X_train[i+60]\n    keys = pred_keypoints_pred_xy[i+60]\n    ax.title.set_text('Predicted keypoint model')\n  elif i in range(1, 27, 3):\n    keys = complete_pred_xy[i_orig+60]\n    ax.title.set_text('Complete model')\n  elif i in range(2, 27, 3):\n    keys = augmented_pred_xy[i_orig+60]\n    ax.title.set_text('Augmented model')\n  ax.imshow(np.reshape(img*(-1), [96, 96]), cmap='Greys')\n  ax.axis('off')\n  ax.scatter(keys[:,0],\n             keys[:,1],\n             marker='+', c='red')\n  ax.scatter(keys[:,0],\n             keys[:,1],\n             s=100, c='blue', alpha=0.25)\nplt.suptitle('Predicted keypoints', size=18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"7lue-lvMhse_"},"cell_type":"markdown","source":"# Final Model Building Process"},{"metadata":{"id":"d_3NdZAWV29m"},"cell_type":"markdown","source":"Overall, the predicted keypoint model does not appear to perform particularly well. Visually, it seems as though the complete model tends to have keypoints that match up better than any other model. \n\nIt does appear as though regardless of the submission type, the predicted keypoints are quite rigid across predictions. We hypothesize that this may be caused by the fact that a large portion of faces have keypoints in very similar positions within the image. We'd like to explore further on if this \"rigid\" effect can be minimized by training a model on ONLY the data that has undergone transformations. We will progress as follows:\n\n1. Transform each image in the \"Complete\" data set\n2. Train intermediate model on this\n3. Predict the missing keypoints from the \"Incomplete\" data set\n4. Transform each of the predicted keypoint images\n5. Train our final model on the combined \"complete\" and \"predicted incomplete\" data sets\n\nFinal model training data set:\n- Transformed images with all existing keypoints provided\n- Transformed images with some keypoints provided and missing keypoints predicted\n\n\n"},{"metadata":{"id":"Qu-Wl3DwbIyr","trusted":false},"cell_type":"code","source":"# define function to generate images and keypoints\ndef transform_data(X, y):\n  xy = np.apply_along_axis(reshape_keypoints, 1, y)\n  for i, keypoints in enumerate(y):\n    success = False\n    while not success:\n      # generator for only this image\n      seed = randint(1, 10000) \n      generator = zip(image_datagen.flow(x=np.expand_dims(X[i], axis=0), seed=seed),\n                      landmark_datagen.flow_landmarks(np.expand_dims(xy[i], axis=0), seed=seed))\n      # generate batches of images and keypoints\n      for imgs, landmarks in generator:\n        # iterate over keypoints in batch\n        for ind, landmark in enumerate(landmarks):\n          # skip where any keypoint is outside boundary\n          if np.any(landmark[:, 2]==0):\n            continue\n          success = True\n          if i == 0:\n            XX = np.expand_dims(imgs[ind], axis=0)\n            yy = unpack_keypoints(landmark).T\n            break\n          else:\n            # append image\n            XX = np.append(XX, np.expand_dims(imgs[ind], axis=0), axis=0)\n            yy = np.append(yy, unpack_keypoints(landmark).T, axis=0)\n            break\n        break\n  return XX, yy\n\n# transform all samples\nt_complete_X_train, t_complete_y_train = transform_data(complete_X_train,\n                                                        complete_y_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"5-X9ECqC122Z"},"cell_type":"markdown","source":"In the code above, we transformed all of our samples to obtain the data for the complete data set. That was Part 1 from the above recipe. Now below, we will take a look at what this transformed data actually looks like.\n\nThis allows us to dramatically increase our training set amount, while allowing us to obtain new data."},{"metadata":{"id":"9FjV0aSPmLId","outputId":"30422e11-524f-493e-d7ca-2e26b89af45e","trusted":false},"cell_type":"code","source":"# plot images with keypoints\nfig, axs = plt.subplots(4, 4, figsize=(14, 14), sharex=True, sharey=True)\nfor ind, ax in enumerate(axs.flatten()):\n  i = ind+1000\n  ax.imshow(t_complete_X_train[i].reshape(96,96)*(-1), cmap='Greys')\n  ax.axis('off')\nplt.suptitle('Transformed images', size=18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"G-LNnlyF2gr1"},"cell_type":"markdown","source":"Next, we're going to train an intermediate model. This is part 2 of the above recipe. By doing this now, we are going to be able to predict the values for the data where all of the keypoints were not given. This will allow us to further expand our dataset.\n\n"},{"metadata":{"id":"SeGH4WNFfn-2","outputId":"09da4ca3-b64a-4a37-b9b5-fb697f47a4e0","trusted":false},"cell_type":"code","source":"# copy model\nt_complete_model = clone_model(model)\n\n# compile model\nt_complete_model.compile(optimizer='adam', \n                   loss='mean_squared_error',\n                   metrics=['mae', 'acc'])\n\n# predict again\nt_complete_history = t_complete_model.fit(t_complete_X_train,\n                    t_complete_y_train,\n                    epochs = 100,\n                    batch_size = 128,\n                    validation_split=0.05,\n                    callbacks=[es])","execution_count":null,"outputs":[]},{"metadata":{"id":"uaznaumygJwH"},"cell_type":"markdown","source":"Ultimately, that intermediate model produced an MAE of 3.76, an accuracy of 0.77, a validation MAE of 2.17 and a validation accuracy of 0.68. This is a fairly accurate model that will be a good predictor for our missing keypoints.\n\nNow let's fill in the missing keypoints, transform the images, append these transformations to our transformed complete data set, and train one final model. This is Part 3 of the above workflow."},{"metadata":{"id":"0M722lkUgIA1","trusted":false},"cell_type":"code","source":"# predict missing points\nt_complete_pred = t_complete_model.predict(incomplete_X_train)\n\n# merge missing points\npred_incomplete_y_train = np.where(\n    np.isnan(incomplete_y_train),\n    t_complete_pred,\n    incomplete_y_train\n)\n\n# transform\nt_incomplete_X_train, t_incomplete_y_train = transform_data(\n    incomplete_X_train, pred_incomplete_y_train)\n\n# append to augmented data\nfinal_X_train = np.append(t_complete_X_train, t_incomplete_X_train, axis=0)\nfinal_y_train = np.append(t_complete_y_train, t_incomplete_y_train, axis=0)","execution_count":null,"outputs":[]},{"metadata":{"id":"5YJXOPOWp00p","outputId":"b557c088-1548-4980-a10c-8051ebe25973","trusted":false},"cell_type":"code","source":"# copy model\nfinal_model = clone_model(model)\n\n# compile model\nfinal_model.compile(optimizer='adam', \n                   loss='mean_squared_error',\n                   metrics=['mae', 'acc'])\n\n# predict again\nes = EarlyStopping(monitor='val_loss', patience=75)\nfinal_history = final_model.fit(final_X_train,\n                    final_y_train,\n                    epochs = 100,\n                    batch_size = 128,\n                    validation_split=0.05,\n                    callbacks=[es])","execution_count":null,"outputs":[]},{"metadata":{"id":"JKjLhwVu3WeS"},"cell_type":"markdown","source":"Now, we have a model that combines the given \"complete\" data, the augmented predictions from the \"complete\" data and our new \"incomplete\" data with the incompleteness filled in by predictions from that intermediate model. This new model produced an MAE of 3.53, an accuracy of 0.85, a validation MAE of 2.89 and a validation accuracy of 0.92. This is a much more accurate model that will be a great predictor for our testing data.\n\nNext, we can explore this accuracy through plotting these MAE, accuracy and loss values."},{"metadata":{"id":"aTSU_yj_iIl_","outputId":"89e21e17-dd46-49f6-f37c-acf3f976d52c","trusted":false},"cell_type":"code","source":"# plot to show training MAE over time\nplt.plot(complete_history.history['mae'])\nplt.plot(t_complete_history.history['mae'])\nplt.plot(final_history.history['mae'])\nplt.title('Mean Absolute Error vs Epoch (training data)')\nplt.ylabel('Mean Absolute Error')\nplt.xlabel('Epochs')\nplt.legend(['\"Complete\" data set provided',\n            '\"Complete\" with transformations',\n            '\"Complete/Predicted\" with transformations'], loc='upper right')\nplt.show()\n\n# plot to show validation MAE over time\nplt.plot(complete_history.history['val_mae'])\nplt.plot(t_complete_history.history['val_mae'])\nplt.plot(final_history.history['val_mae'])\nplt.title('Mean Absolute Error vs Epoch (validation data)')\nplt.ylabel('Mean Absolute Error')\nplt.xlabel('Epochs')\nplt.legend(['\"Complete\" data set provided',\n            '\"Complete\" with transformations',\n            '\"Complete/Predicted\" with transformations'], loc='upper right')\nplt.show()\n\n# plot to show training accuracy over time\nplt.plot(complete_history.history['acc'])\nplt.plot(t_complete_history.history['acc'])\nplt.plot(final_history.history['acc'])\nplt.title('Accuracy vs Epoch (training data)')\nplt.ylabel('Accuracy')\nplt.xlabel('Epochs')\nplt.legend(['\"Complete\" data set provided',\n            '\"Complete\" with transformations',\n            '\"Complete/Predicted\" with transformations'], loc='lower right')\nplt.show()\n\n# plot to show validation accuracy over time\nplt.plot(complete_history.history['val_acc'])\nplt.plot(t_complete_history.history['val_acc'])\nplt.plot(final_history.history['val_acc'])\nplt.title('Accuracy vs Epoch (validation data)')\nplt.ylabel('Accuracy')\nplt.xlabel('Epochs')\nplt.legend(['\"Complete\" data set provided',\n            '\"Complete\" with transformations',\n            '\"Complete/Predicted\" with transformations'], loc='lower right')\nplt.show()\n\n# plot to show training loss over time\nplt.plot(complete_history.history['loss'])\nplt.plot(t_complete_history.history['loss'])\nplt.plot(final_history.history['loss'])\nplt.title('Loss vs Epoch (training data)')\nplt.ylabel('Loss')\nplt.xlabel('Epochs')\nplt.legend(['\"Complete\" data set provided',\n            '\"Complete\" with transformations',\n            '\"Complete/Predicted\" with transformations'], loc='upper right')\nplt.show()\n\n# plot to show validation loss over time\nplt.plot(complete_history.history['val_loss'])\nplt.plot(t_complete_history.history['val_loss'])\nplt.plot(final_history.history['val_loss'])\nplt.title('Loss vs Epoch (validation data)')\nplt.ylabel('Loss')\nplt.xlabel('Epochs')\nplt.legend(['\"Complete\" data set provided',\n            '\"Complete\" with transformations',\n            '\"Complete/Predicted\" with transformations'], loc='upper right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"d817Xdqvj3Et"},"cell_type":"markdown","source":"The complete/predicted with transformations model performs the best on the validation set of data, though appears to cap out around 70% accuracy against the training set. Both of the transformed models appear to perform better than the untransformed model in every regard.\n\nWe can now take a look at how this model works when predicting other data. Let's look at some predictions:"},{"metadata":{"id":"J5NbI0qbj5bQ","trusted":false},"cell_type":"code","source":"# predict\nfinal_pred = final_model.predict(incomplete_X_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"9eHJrhur4vQE","trusted":false},"cell_type":"code","source":"# reshape keypoints\nt_complete_pred_xy = np.apply_along_axis(reshape_keypoints, 1, t_complete_pred)\nfinal_pred_xy = np.apply_along_axis(reshape_keypoints, 1, final_pred)","execution_count":null,"outputs":[]},{"metadata":{"id":"3NT-lS4gkOar","outputId":"fd5be372-5cf3-4ffe-c99e-0aa7d468feac","trusted":false},"cell_type":"code","source":"# plot images with keypoints\nfig, axs = plt.subplots(9, 3, figsize=(12, 36), sharex=True, sharey=True)\nfor i, ax in enumerate(axs.flatten()):\n  if i in range(0, 27, 3):\n    i_orig = i\n    img = incomplete_X_train[i+60]\n    keys = complete_pred_xy[i+60]\n    ax.title.set_text('Complete model')\n  elif i in range(1, 27, 3):\n    img = incomplete_X_train[i_orig+60]\n    keys = t_complete_pred_xy[i_orig+60]\n    ax.title.set_text('Transformed complete model')\n  elif i in range(2, 27, 3):\n    keys = final_pred_xy[i_orig+60]\n    ax.title.set_text('Transformed/Predicted model')\n  ax.imshow(np.reshape(img*(-1), [96, 96]), cmap='Greys')\n  ax.axis('off')\n  ax.scatter(keys[:,0],\n             keys[:,1],\n             marker='+', c='red')\n  ax.scatter(keys[:,0],\n             keys[:,1],\n             s=100, c='blue', alpha=0.25)\nplt.suptitle('Predicted keypoints', size=18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"KefS6Opm50W2"},"cell_type":"markdown","source":"After all of the experiments, transformations, and augmenting of data the model trained on simply the subset of data that contained full set of 15 keypoints appears to visually perform the best. This appears to move best with the data, however, let's take a closer look and confirm our suspicions."},{"metadata":{"id":"5zXVK0TjiKG6"},"cell_type":"markdown","source":"# Final Test and Submission\n\nBased on our suspicions, from the last two pieces of information, we can see that our lowest MAE value occurred from the complete model, with all 15 existing keypoints. Despite all of the transformations and different features we implemented, we found that this simpler solution was more elegant and worked at a more accurate rate for our purposes.\n\nThe transformed model, with the transformations of the completed data, also worked well. Our augmented model, predicted keypoints model and transformed/predicted model also preformed well and all had MAEs under 3 and accuracies above 0.70."},{"metadata":{"id":"zGGseIZv6FEe","outputId":"9ae7cfe3-e25c-4f03-ce3e-5c2617b4062a","trusted":false},"cell_type":"code","source":"print(\"Base model (including nulls)\")\nb = base_model.evaluate(X_train, y_train)\nprint(\"Complete model (only images with all 15 existing keypoints)\")\nc = complete_model.evaluate(complete_X_train, complete_y_train)\nprint(\"Augmented model (Complete + transforms of complete)\")\na = augmented_model.evaluate(augmented_X_train, augmented_y_train)\nprint(\"Predicted Keypoints model (Complete + transforms of complete + incomplete with predicted keypoints)\")\np = pred_keypoints_model.evaluate(pred_keypoints_X_train, pred_keypoints_y_train)\nprint(\"Transformed model (Transforms of complete ONLY)\")\nt = t_complete_model.evaluate(t_complete_X_train, t_complete_y_train)\nprint(\"Transformed/Predicted model (Transforms of complete and transforms of predicted incomplete)\")\nf = final_model.evaluate(final_X_train, final_y_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"nTUCC3UzUoOH"},"cell_type":"markdown","source":"With this information in mind, let's begin the process of making predictions on the test data.\n\nWe'll first have to prepare the test data, by transforming the images and converting them in the same way we did for training. "},{"metadata":{"id":"Z8vJC5SyNeep","trusted":false},"cell_type":"code","source":"# transform images\ndef process_test_data(data):\n    images = []\n    for idx, sample in data.iterrows():\n      # extract and reshape image\n      image = np.reshape(np.array(sample['Image'].split(' '),\n                                  dtype=int),\n                         (96,96,1))\n      images.append(image)\n\n    # normalize grayscale images and convert to array\n    images = np.array(images)/255.\n    return images\n\n#preparing test data\nX_test = process_test_data(test_data)","execution_count":null,"outputs":[]},{"metadata":{"id":"IohrbBK47HhN"},"cell_type":"markdown","source":"We'll then predict on the testing data using the complete model."},{"metadata":{"id":"mfI2HCTGNeep","trusted":false},"cell_type":"code","source":"# predict\ntest_pred = complete_model.predict(X_test)\n\n# reshape keypoints\ntest_pred_xy = np.apply_along_axis(reshape_keypoints, 1, test_pred)","execution_count":null,"outputs":[]},{"metadata":{"id":"KK_3lSo37LCt"},"cell_type":"markdown","source":"Next, let's visualize these test images with predicted keypoints."},{"metadata":{"id":"J2YuqRw24TGO","outputId":"9d62f66b-ddff-4f5d-f4c0-90c88e917f2e","trusted":false},"cell_type":"code","source":"# plot images with keypoints\nfig, axs = plt.subplots(4, 4, figsize=(14, 14), sharex=True, sharey=True)\nfor ind, ax in enumerate(axs.flatten()):\n  i = ind+1000\n  ax.imshow(X_test[i].reshape(96,96)*(-1), cmap='Greys')\n  ax.scatter(test_pred_xy[i,:,0],\n             test_pred_xy[i,:,1], marker='+', c='red')\n  ax.scatter(test_pred_xy[i,:,0],\n             test_pred_xy[i,:,1], s=100, c='blue', alpha=0.25)\n  ax.axis('off')\nplt.suptitle('Test images with predicted keypoints\\n(Predicted with the \"Complete\" model)', size=18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"y2fR-kDgiCbQ"},"cell_type":"markdown","source":"So these images generally look good! Below is the code used to output the final model predictions for submission to Kaggle. Our submission was good enough for a 3.1 MAE in the Kaggle competition. This would've been good enough for 65th place in the event, if we had competed."},{"metadata":{"id":"K_udNqbbNeep","outputId":"a40f1454-47f8-4747-cfba-a83673aa57c2","trusted":false},"cell_type":"code","source":"# Take test predictions data and convert it into Kaggle's desired format.\nlookid_list = list(lookid_data['FeatureName'])\nimageID = list(lookid_data['ImageId']-1)\npre_list = list(test_pred)\nrowid = lookid_data['RowId']\nrowid=list(rowid)\nfeature = []\nfor f in list(lookid_data['FeatureName']):\n    feature.append(lookid_list.index(f))\npreded = []\nfor x,y in zip(imageID,feature):\n    preded.append(pre_list[x][y])\n\n# Obtain desired rows and predicted locations\nrowid = pd.Series(rowid,name = 'RowId')\nloc = pd.Series(preded,name = 'Location')\nsubmission = pd.concat([rowid,loc],axis = 1)\nsubmission.to_csv('face_key_detection_submission_finalth_attempt.csv',index = False)\n\n# save models to load later if necessary\ncomplete_model.save('complete_model')\nfinal_model.save('final_model')","execution_count":null,"outputs":[]},{"metadata":{"id":"dgLrtbiD8Ybt"},"cell_type":"markdown","source":"# Conclusion\n\nUltimately, given that this was our first foray into facial recognition, this project was successful for us. We were able to detect the 15 facial keypoints from static imagesWith a fairly good level of accuracy. Using the data provided by Kaggle allowed us to make relevant predictions and obtain reasonable results. As a result of our efforts, this project could be expanded to have many modern applications in the facial recognition space. \n\nThere are certainly some growth areas as well. If we were to continue to work on this, we would certainly want to get a better sense for why our augmented and transformed data did not yield better results than simply using the raw images. Furthermore, we may even want to try to develop features from these images as a result. We would also want to explore the concept of facial geometry and potentially could develop some sort of clustering algorithm to classify similar types of faces to make better predictions. We may also want to consider using other data sources to help bolster our training data.\n\nUltimately, the approach we took was one rooted in our machine learning knowledge and backed by the various forms of research, we did for this analysis. We are confident that the decisions we made lead to better outcomes, such as our decision not to use forward filling and focus on predictions instead. These decisions lead us to a result that we are happy with and a successful model. "},{"metadata":{"id":"TY-l0s4K-8C1","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}