{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        break\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-05-30T03:57:11.82796Z","iopub.execute_input":"2021-05-30T03:57:11.828383Z","iopub.status.idle":"2021-05-30T03:57:11.837882Z","shell.execute_reply.started":"2021-05-30T03:57:11.828312Z","shell.execute_reply":"2021-05-30T03:57:11.836857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Import libraries and check versions","metadata":{}},{"cell_type":"code","source":"from fastai.vision.all import *\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2021-05-30T03:57:11.840271Z","iopub.execute_input":"2021-05-30T03:57:11.84069Z","iopub.status.idle":"2021-05-30T03:57:11.852015Z","shell.execute_reply.started":"2021-05-30T03:57:11.840605Z","shell.execute_reply":"2021-05-30T03:57:11.851162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(torch.cuda.is_available())\nprint(torch.version.cuda)\nprint(torch.__version__)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T03:57:11.855371Z","iopub.execute_input":"2021-05-30T03:57:11.855638Z","iopub.status.idle":"2021-05-30T03:57:11.865188Z","shell.execute_reply.started":"2021-05-30T03:57:11.855613Z","shell.execute_reply":"2021-05-30T03:57:11.864232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Explore the input data","metadata":{}},{"cell_type":"code","source":"id_lookup_table_csv = pd.read_csv('../input/facial-keypoints-detection/IdLookupTable.csv', index_col='RowId')\nprint(id_lookup_table_csv.head())\nprint(id_lookup_table_csv.describe())","metadata":{"execution":{"iopub.status.busy":"2021-05-30T03:57:11.869033Z","iopub.execute_input":"2021-05-30T03:57:11.869295Z","iopub.status.idle":"2021-05-30T03:57:11.906033Z","shell.execute_reply.started":"2021-05-30T03:57:11.869269Z","shell.execute_reply":"2021-05-30T03:57:11.905358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_csv = pd.read_csv('../input/facial-keypoints-detection/training.zip')\ntrain_csv_images = train_csv['Image']\ntrain_csv = train_csv.drop(columns='Image')\ntrain_csv.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-30T03:57:11.909082Z","iopub.execute_input":"2021-05-30T03:57:11.909336Z","iopub.status.idle":"2021-05-30T03:57:15.080392Z","shell.execute_reply.started":"2021-05-30T03:57:11.909309Z","shell.execute_reply":"2021-05-30T03:57:15.079614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The data frame info() function tells us that we have lots of NaNs in our data.\nPandas provides some functions to replace NaNs with other plausible values (e.g. median),\nbut that's not a really satisfying solution.\nWe can however choose to ignore coordinates with NaNs during loss value computation.","metadata":{}},{"cell_type":"code","source":"train_csv.info()","metadata":{"execution":{"iopub.status.busy":"2021-05-30T03:57:15.081641Z","iopub.execute_input":"2021-05-30T03:57:15.081982Z","iopub.status.idle":"2021-05-30T03:57:15.098002Z","shell.execute_reply.started":"2021-05-30T03:57:15.081946Z","shell.execute_reply":"2021-05-30T03:57:15.097097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's convert the training image pixel values to actual image data:","metadata":{}},{"cell_type":"code","source":"train_images = [np.fromstring(train_csv_images.iloc[i], sep=' ').reshape([96,96]) for i in range(train_csv_images.size)]\ntrain_points = [train_csv.iloc[k].values.reshape([15,2]) for k in range(train_csv.shape[0]) ]","metadata":{"execution":{"iopub.status.busy":"2021-05-30T03:57:15.099374Z","iopub.execute_input":"2021-05-30T03:57:15.09975Z","iopub.status.idle":"2021-05-30T03:57:29.895812Z","shell.execute_reply.started":"2021-05-30T03:57:15.099711Z","shell.execute_reply":"2021-05-30T03:57:29.89498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Have a look at a sample training image and its feature points:","metadata":{}},{"cell_type":"code","source":"print(train_points[0])\nplt.imshow(train_images[0], cmap='gray')\nplt.plot(train_points[0][:,0], train_points[0][:,1], 'gx')\n","metadata":{"execution":{"iopub.status.busy":"2021-05-30T03:57:29.899032Z","iopub.execute_input":"2021-05-30T03:57:29.899295Z","iopub.status.idle":"2021-05-30T03:57:30.037574Z","shell.execute_reply.started":"2021-05-30T03:57:29.899267Z","shell.execute_reply":"2021-05-30T03:57:30.036497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Just like the training data, we convert the test data for easier access and display. ","metadata":{}},{"cell_type":"code","source":"test_csv = pd.read_csv('../input/facial-keypoints-detection/test.zip')\ntest_csv_images = test_csv['Image']\ntest_csv = test_csv.drop(columns='Image')\nprint(test_csv.head())","metadata":{"execution":{"iopub.status.busy":"2021-05-30T03:57:30.040645Z","iopub.execute_input":"2021-05-30T03:57:30.040997Z","iopub.status.idle":"2021-05-30T03:57:30.751685Z","shell.execute_reply.started":"2021-05-30T03:57:30.040959Z","shell.execute_reply":"2021-05-30T03:57:30.750776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_images = [np.fromstring(test_csv_images[i], sep=' ').reshape([96,96]) for i in range(test_csv_images.size)]","metadata":{"execution":{"iopub.status.busy":"2021-05-30T03:57:30.75491Z","iopub.execute_input":"2021-05-30T03:57:30.755265Z","iopub.status.idle":"2021-05-30T03:57:34.269478Z","shell.execute_reply.started":"2021-05-30T03:57:30.755228Z","shell.execute_reply":"2021-05-30T03:57:34.268652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(test_images[0], cmap='gray')","metadata":{"execution":{"iopub.status.busy":"2021-05-30T03:57:34.273642Z","iopub.execute_input":"2021-05-30T03:57:34.273911Z","iopub.status.idle":"2021-05-30T03:57:34.40876Z","shell.execute_reply.started":"2021-05-30T03:57:34.273881Z","shell.execute_reply":"2021-05-30T03:57:34.407773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For convenience, we define a display function that can plot training and test data, and additionally predicted feature points (if given):","metadata":{}},{"cell_type":"code","source":"def show_image_and_points(img, true_pnts=None, pred_pnts=None):\n    ax = plt.imshow(img, cmap='gray')\n    if true_pnts is not None:\n        plt.plot(true_pnts[:,0], true_pnts[:,1], 'gx')\n    if pred_pnts is not None:\n        plt.plot(pred_pnts[:,0], pred_pnts[:,1], 'r+')\ndef show_test(i):\n    pred,_,_ = learner.predict(test_images[i])\n    show_image_and_points(test_images[i], pred_pnts=pred)\ndef show_train(i, learner=None):\n    fully_decoded = None\n    if learner is not None:\n        fully_decoded, loss_func_decoded, probabilities = learner.predict(train_images[i])\n    show_image_and_points(train_images[i], true_pnts=train_points[i], pred_pnts=fully_decoded)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T03:57:34.409929Z","iopub.execute_input":"2021-05-30T03:57:34.410265Z","iopub.status.idle":"2021-05-30T03:57:34.418886Z","shell.execute_reply.started":"2021-05-30T03:57:34.410228Z","shell.execute_reply":"2021-05-30T03:57:34.417756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Just check some sample image:","metadata":{}},{"cell_type":"code","source":"show_train(2010)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T03:57:34.420595Z","iopub.execute_input":"2021-05-30T03:57:34.421013Z","iopub.status.idle":"2021-05-30T03:57:34.554445Z","shell.execute_reply.started":"2021-05-30T03:57:34.420976Z","shell.execute_reply":"2021-05-30T03:57:34.553333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Augment the training data\nIn addition to FastAI's on-the-fly augmentation, we deliberately augment our training set with shift, rotation and scaling augmentations. The intention here is to add augmentation in a non-probabilistic way that pushes our model to make good predictions for the original image _and_ a shifted/rotated/scaled version of the same image during each epoch.","metadata":{}},{"cell_type":"markdown","source":"First, we define our augmentation function that takes a training image and training points and returns\na tuple with the transformed image and training points.  \nAfter the transformation, point coordinates may be outside the valid range of [0,96], so we make sure we\nset these points' coordinates to NaN.","metadata":{}},{"cell_type":"code","source":"def augment(img, pnts, rot_deg, zoom_factor, x_shift_pix, y_shift_pix):\n    sz = img.shape[-2:]\n    def get_rotation(x):\n        mysz = x.new_ones(x.shape[0])\n        rot_rad = torch.ones_like(mysz)*(rot_deg / 180.0 * np.pi)\n        m11 = rot_rad.cos() / zoom_factor\n        m12 = rot_rad.sin() / zoom_factor\n        t0 = torch.ones_like(mysz)*(x_shift_pix/48.0)\n        t1 = torch.ones_like(mysz)*(y_shift_pix/48.0)\n        return affine_mat(m11, m12, t0, -m12, m11, t1)\n    t1 = AffineCoordTfm(aff_fs=get_rotation, size=sz)\n    p1 = Pipeline(funcs=t1)\n    x = TensorImage(img).view([1,1,96,96])\n    y = TensorPoint(pnts, img_size=[96,96]).view([1,15,2])\n    x,y = p1((x,y/48.0-1.0))\n    y = y.view([15,2])\n    coord_ok = (y[:,0] > -1.0) & (y[:,0] < 1.0) & (y[:,1] > -1.0) & (y[:,1] < 1.0)\n    coord_ok = torch.stack([coord_ok, coord_ok], dim=1)\n    y = y.where(coord_ok, tensor(np.nan))\n    y = y*48.0+48.0\n    return np.array(x.view([96,96])), np.array(y)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T03:57:34.555921Z","iopub.execute_input":"2021-05-30T03:57:34.556264Z","iopub.status.idle":"2021-05-30T03:57:34.567336Z","shell.execute_reply.started":"2021-05-30T03:57:34.556225Z","shell.execute_reply":"2021-05-30T03:57:34.566525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check visually that our function works. Note that the augmented points have been correctly transformed so that they align with the actual facial features of the transformed image.","metadata":{}},{"cell_type":"code","source":"aug_img, aug_pnts = augment(train_images[0], train_points[0], 10.0, 1.0, 16.0, -16.0)\nshow_image_and_points(aug_img, true_pnts=aug_pnts)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T03:57:34.568945Z","iopub.execute_input":"2021-05-30T03:57:34.569455Z","iopub.status.idle":"2021-05-30T03:57:34.711263Z","shell.execute_reply.started":"2021-05-30T03:57:34.569418Z","shell.execute_reply":"2021-05-30T03:57:34.710557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We define augmentation with x-shift from -10...+10 pixels, y-shift from -10...+10 pixels, rotations from -10...+10 degrees, and scale factor from 90% to 110%.","metadata":{}},{"cell_type":"code","source":"augs = []\none_pixel = 2.0/96.0\nfor dx in range(21):\n    for dy in range(21):\n        if dx==10 and dy==10:\n            continue\n        augs.append([0.0, 1.0, dx-10.0, dy-10.0])\nfor rot_deg in range(21):\n    if rot_deg==10:\n        continue\n    augs.append([rot_deg-10, 1.0, 0.0, 0.0])\nfor scale in range(21):\n    if scale==10:\n        continue\n    augs.append([0.0, 0.9 + 0.01*scale, 0.0, 0.0])\nprint(len(augs))","metadata":{"execution":{"iopub.status.busy":"2021-05-30T03:57:34.712753Z","iopub.execute_input":"2021-05-30T03:57:34.713182Z","iopub.status.idle":"2021-05-30T03:57:34.723367Z","shell.execute_reply.started":"2021-05-30T03:57:34.713144Z","shell.execute_reply":"2021-05-30T03:57:34.722337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So we have a total of 480 augmentation transformations.  \nNow create the augmented training data set, interleaving original training data with augmented training data.","metadata":{}},{"cell_type":"code","source":"aug_images = []\naug_points = []\naug_ind = 0\nfor k1 in range(len(train_images)):\n    img,pnt = augment(train_images[k1], train_points[k1], *augs[aug_ind])\n    aug_images.append(train_images[k1])\n    aug_points.append(train_points[k1])\n    aug_images.append(img)\n    aug_points.append(pnt)\n    aug_ind = (aug_ind + 1) % len(augs)\nprint(len(aug_images))","metadata":{"execution":{"iopub.status.busy":"2021-05-30T03:57:34.724825Z","iopub.execute_input":"2021-05-30T03:57:34.725219Z","iopub.status.idle":"2021-05-30T03:57:57.192368Z","shell.execute_reply.started":"2021-05-30T03:57:34.725168Z","shell.execute_reply":"2021-05-30T03:57:57.191273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This gives us 14098 augmented training images (+ their feature points).  \nFinally, replace the training data set with the augmented data set.","metadata":{}},{"cell_type":"code","source":"train_images = aug_images\ntrain_points = aug_points","metadata":{"execution":{"iopub.status.busy":"2021-05-30T03:57:57.193734Z","iopub.execute_input":"2021-05-30T03:57:57.194097Z","iopub.status.idle":"2021-05-30T03:57:57.198227Z","shell.execute_reply.started":"2021-05-30T03:57:57.194046Z","shell.execute_reply":"2021-05-30T03:57:57.197419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visually check some training images and their augmented versions:\nIn particular, we verify that the feature points of our augmented images align with the actual augmented image data.","metadata":{}},{"cell_type":"code","source":"print(\"Original\")\nshow_train(0)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T03:57:57.19992Z","iopub.execute_input":"2021-05-30T03:57:57.20027Z","iopub.status.idle":"2021-05-30T03:57:57.337231Z","shell.execute_reply.started":"2021-05-30T03:57:57.200235Z","shell.execute_reply":"2021-05-30T03:57:57.33639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Augmented (shifted right+down)\")\nshow_train(1)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T03:57:57.338829Z","iopub.execute_input":"2021-05-30T03:57:57.339196Z","iopub.status.idle":"2021-05-30T03:57:57.470738Z","shell.execute_reply.started":"2021-05-30T03:57:57.339156Z","shell.execute_reply":"2021-05-30T03:57:57.469852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define Loss function for use with NANs\nThe original input data as well as our augmented training points may contain NaNs.\nIn order to enable training with NaN coordinates, let's define a loss function that computes a meaningful loss value even if some points cannot be used.  \nThe below definition might look a little complicated at first glance but is actually a copy\nof the implementation of BaseLoss in Lib\\site-packages\\fastai\\loss.py, with only a small modification in the call() function.\nThis modification simply sets the loss value to zero for those coordinates that are NaN.","metadata":{}},{"cell_type":"code","source":"class MyBaseLoss():\n    \"Same as my `loss_cls`, but flattens input and target.\"\n    activation=decodes=noops\n    def __init__(self, loss_cls, *args, axis=-1, flatten=True, floatify=False, is_2d=True, **kwargs):\n        store_attr(\"axis,flatten,floatify,is_2d\")\n        self.func = loss_cls(*args,**kwargs)\n        functools.update_wrapper(self, self.func)\n\n    def __repr__(self): return f\"MyFlattenedLoss of {self.func}\"\n    @property\n    def reduction(self): return self.func.reduction\n    @reduction.setter\n    def reduction(self, v): self.func.reduction = v\n\n    def _contiguous(self,x):\n        return TensorBase(x.transpose(self.axis,-1).contiguous()) if isinstance(x,torch.Tensor) else x\n\n    def __call__(self, inp, targ, **kwargs):\n        inp,targ  = map(self._contiguous, (inp,targ))\n        if self.floatify and targ.dtype!=torch.float16: targ = targ.float()\n        if targ.dtype in [torch.int8, torch.int16, torch.int32]: targ = targ.long()\n        if self.flatten: inp = inp.view(-1,inp.shape[-1]) if self.is_2d else inp.view(-1)\n        tmptarg2 = targ.view(inp.shape)\n        tmptarg3 = torch.where(torch.isnan(tmptarg2), inp, tmptarg2)\n        tmptarg4 = tmptarg3.view(-1) if self.flatten else tmptarg3\n        return self.func.__call__(inp, tmptarg4, **kwargs)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T03:57:57.472414Z","iopub.execute_input":"2021-05-30T03:57:57.472743Z","iopub.status.idle":"2021-05-30T03:57:57.483923Z","shell.execute_reply.started":"2021-05-30T03:57:57.472706Z","shell.execute_reply":"2021-05-30T03:57:57.48281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Similarly to BaseLoss, we define our version of MSELossFlat based on the original implementation, returning our customized loss from above: ","metadata":{}},{"cell_type":"code","source":"@use_kwargs_dict(reduction='mean')\ndef MyMSELossFlat(*args, axis=-1, floatify=True, **kwargs):\n    \"Same as MY `nn.MSELoss`, but flattens input and target.\"\n    return MyBaseLoss(nn.MSELoss, *args, axis=axis, floatify=floatify, is_2d=False, **kwargs)\n","metadata":{"execution":{"iopub.status.busy":"2021-05-30T03:57:57.485693Z","iopub.execute_input":"2021-05-30T03:57:57.486052Z","iopub.status.idle":"2021-05-30T03:57:57.496994Z","shell.execute_reply.started":"2021-05-30T03:57:57.486015Z","shell.execute_reply":"2021-05-30T03:57:57.496345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training/Validation data split\nHaving inserted an augmented image after each training image, we choose to split our data at a predefined index (without randomization). This way we can avoid that accidentally an augmented training image slips into our validation data set. ","metadata":{}},{"cell_type":"code","source":"def MySplitter(valid_pcts=0.2):\n    def _inner(item_range):\n        cut = int(len(item_range) * (1.0-valid_pcts))\n        mylist = list(item_range)\n        l1 = mylist[:cut]\n        l2 = mylist[cut:]\n        return L(l1), L(l2)\n    return _inner\n","metadata":{"execution":{"iopub.status.busy":"2021-05-30T03:57:57.498641Z","iopub.execute_input":"2021-05-30T03:57:57.499013Z","iopub.status.idle":"2021-05-30T03:57:57.508794Z","shell.execute_reply.started":"2021-05-30T03:57:57.498962Z","shell.execute_reply":"2021-05-30T03:57:57.507927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Defining our DataLoaders\nWe use the DataBlock API to define images (ImageBlock) as input and 2D points (PointBlock) as output.\n\nAlso, we enable FastAI's on-the-fly augmentation transformations for increased robustness.\nNote that it is necessary to pass \"do_flip=False\", otherwise the model will get confused because\nthe image and its associated training points will get horizontally flipped, but the model will still\ninterpret e.g. the \"left\" eye on the left side of the flipped image,\nwhich would cause a very large loss and thus disturb the model's weights.","metadata":{}},{"cell_type":"code","source":"def get_x(ind):\n    return train_images[ind]\ndef get_y(ind):\n    return train_points[ind]\ndef get_items(i): return i\ndb = DataBlock(blocks=[ImageBlock, PointBlock],\n               get_items=get_items,\n               get_x=get_x, get_y=get_y,\n               item_tfms=Resize([96,96]),\n               splitter=MySplitter(0.2),\n               batch_tfms=aug_transforms(size=[96,96],\n                                         mult=1.0, max_rotate=8.0, \n                                         flip_vert=False, \n                                         do_flip=False, \n                                         pad_mode='border', # 'border' or 'reflection'\n                                         max_zoom=1.0, min_zoom=0.9, max_lighting=0.1, max_warp=0.0)\n              )","metadata":{"execution":{"iopub.status.busy":"2021-05-30T03:57:57.510205Z","iopub.execute_input":"2021-05-30T03:57:57.510617Z","iopub.status.idle":"2021-05-30T03:57:57.52088Z","shell.execute_reply.started":"2021-05-30T03:57:57.51058Z","shell.execute_reply":"2021-05-30T03:57:57.520213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can use the summary() function to check whether the DataBlock definition works:","metadata":{}},{"cell_type":"code","source":"db.summary(range(len(train_images)))","metadata":{"execution":{"iopub.status.busy":"2021-05-30T03:57:57.522462Z","iopub.execute_input":"2021-05-30T03:57:57.522808Z","iopub.status.idle":"2021-05-30T03:57:57.775446Z","shell.execute_reply.started":"2021-05-30T03:57:57.522772Z","shell.execute_reply":"2021-05-30T03:57:57.77451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally create the DataLoaders from the DataBlock, using a batch size of 64.  \nAlso, we override the standard loss with our custom loss function.","metadata":{}},{"cell_type":"code","source":"dls = db.dataloaders(range(len(train_images)), bs=64)\ndls.train_ds.loss_func = MyMSELossFlat()","metadata":{"execution":{"iopub.status.busy":"2021-05-30T03:57:57.776834Z","iopub.execute_input":"2021-05-30T03:57:57.777197Z","iopub.status.idle":"2021-05-30T03:57:57.854196Z","shell.execute_reply.started":"2021-05-30T03:57:57.777158Z","shell.execute_reply":"2021-05-30T03:57:57.853343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dls.show_batch(cmap='gray', unique=False)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T03:57:57.855479Z","iopub.execute_input":"2021-05-30T03:57:57.855848Z","iopub.status.idle":"2021-05-30T03:57:58.487002Z","shell.execute_reply.started":"2021-05-30T03:57:57.855808Z","shell.execute_reply":"2021-05-30T03:57:58.486082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define Learner\nCreate a convolutional neural network learner from the DataLoaders.\nWe use transfer learning based on resnet18.  \nUsing deeper resnets does not improve accuracy.\nAlso, accuracy get worse if setting limits with y_range=(-1,1).\n","metadata":{}},{"cell_type":"code","source":"learner = cnn_learner(dls, resnet18)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T03:57:58.488235Z","iopub.execute_input":"2021-05-30T03:57:58.488759Z","iopub.status.idle":"2021-05-30T03:57:58.824399Z","shell.execute_reply.started":"2021-05-30T03:57:58.488719Z","shell.execute_reply":"2021-05-30T03:57:58.823612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can have a look the model architecture using the summary() function, which also tells us the activations' shape at several points in the model.","metadata":{}},{"cell_type":"code","source":"learner.summary()","metadata":{"execution":{"iopub.status.busy":"2021-05-30T03:57:58.825656Z","iopub.execute_input":"2021-05-30T03:57:58.826022Z","iopub.status.idle":"2021-05-30T03:57:59.009075Z","shell.execute_reply.started":"2021-05-30T03:57:58.825979Z","shell.execute_reply":"2021-05-30T03:57:59.008251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the above output, we note that our custom loss function is indeed used.\n\nAnother way to look inside our model is to directly print the model, which gives more details about the PyTorch model layers, but does not tell us about FastAI's added functionality:","metadata":{}},{"cell_type":"code","source":"learner.model","metadata":{"execution":{"iopub.status.busy":"2021-05-30T03:57:59.010703Z","iopub.execute_input":"2021-05-30T03:57:59.011062Z","iopub.status.idle":"2021-05-30T03:57:59.01858Z","shell.execute_reply.started":"2021-05-30T03:57:59.011024Z","shell.execute_reply":"2021-05-30T03:57:59.017473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training loop","metadata":{}},{"cell_type":"markdown","source":"Let's use the learning rate finder to find out what learning rate makes sense:","metadata":{}},{"cell_type":"code","source":"learner.lr_find()","metadata":{"execution":{"iopub.status.busy":"2021-05-30T03:57:59.02038Z","iopub.execute_input":"2021-05-30T03:57:59.020776Z","iopub.status.idle":"2021-05-30T03:58:10.634161Z","shell.execute_reply.started":"2021-05-30T03:57:59.020737Z","shell.execute_reply":"2021-05-30T03:58:10.633423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We pick 1e-2 as our learning rate and start training for \"one cycle\":","metadata":{}},{"cell_type":"code","source":"print('Starting fit one cycle')\nlearner.fit_one_cycle(15, lr_max=1e-2)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T03:58:10.635797Z","iopub.execute_input":"2021-05-30T03:58:10.636155Z","iopub.status.idle":"2021-05-30T04:04:51.931184Z","shell.execute_reply.started":"2021-05-30T03:58:10.636115Z","shell.execute_reply":"2021-05-30T04:04:51.930212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Plot training and validation loss:","metadata":{}},{"cell_type":"code","source":"learner.recorder.plot_loss()","metadata":{"execution":{"iopub.status.busy":"2021-05-30T04:04:51.932795Z","iopub.execute_input":"2021-05-30T04:04:51.933163Z","iopub.status.idle":"2021-05-30T04:04:52.131139Z","shell.execute_reply.started":"2021-05-30T04:04:51.933123Z","shell.execute_reply":"2021-05-30T04:04:52.130419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's look at some sample predictions on our training data:","metadata":{}},{"cell_type":"code","source":"learner.show_results(ds_idx=0, shuffle=False, nrows=2, ncols=4)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T04:04:52.132558Z","iopub.execute_input":"2021-05-30T04:04:52.132877Z","iopub.status.idle":"2021-05-30T04:04:53.125875Z","shell.execute_reply.started":"2021-05-30T04:04:52.132841Z","shell.execute_reply":"2021-05-30T04:04:53.124808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now look at some sample predictions on the validation data:","metadata":{}},{"cell_type":"code","source":"learner.show_results(nrows=3, ncols=4, max_n=16)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T04:04:53.127319Z","iopub.execute_input":"2021-05-30T04:04:53.127701Z","iopub.status.idle":"2021-05-30T04:04:54.912531Z","shell.execute_reply.started":"2021-05-30T04:04:53.127657Z","shell.execute_reply":"2021-05-30T04:04:54.911491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We try to improve our model using fine tuning: ","metadata":{}},{"cell_type":"code","source":"print('Starting fine tuning')\nlearner.fine_tune(75)\nprint('Fine tuning finished')","metadata":{"execution":{"iopub.status.busy":"2021-05-30T04:04:54.917281Z","iopub.execute_input":"2021-05-30T04:04:54.91765Z","iopub.status.idle":"2021-05-30T04:40:13.611518Z","shell.execute_reply.started":"2021-05-30T04:04:54.917611Z","shell.execute_reply":"2021-05-30T04:40:13.61045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Again, plot training and validation loss:","metadata":{}},{"cell_type":"code","source":"learner.recorder.plot_loss()","metadata":{"execution":{"iopub.status.busy":"2021-05-30T04:40:13.613766Z","iopub.execute_input":"2021-05-30T04:40:13.614121Z","iopub.status.idle":"2021-05-30T04:40:14.051138Z","shell.execute_reply.started":"2021-05-30T04:40:13.61408Z","shell.execute_reply":"2021-05-30T04:40:14.05005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Validation loss has improved, but reached a plateau.  \nShow some validation images with predictions:","metadata":{}},{"cell_type":"code","source":"learner.show_results(nrows=3, ncols=4, max_n=16)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T04:54:20.871874Z","iopub.execute_input":"2021-05-30T04:54:20.872218Z","iopub.status.idle":"2021-05-30T04:54:22.65396Z","shell.execute_reply.started":"2021-05-30T04:54:20.872187Z","shell.execute_reply":"2021-05-30T04:54:22.652845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Generate predictions for the test data","metadata":{}},{"cell_type":"markdown","source":"Depending on the input data, our model might predict coordinates outside of the range (0,96).  \nSince a submission is only valid if all its coordinates are in (0,96),\nwe compute our predictions for all test images and then clamp the coordinates to (0,96):","metadata":{}},{"cell_type":"code","source":"test_predictions = np.zeros([len(test_images), 30])\nfor k in range(len(test_images)):\n    pred, pred_loss, pred_prob = learner.predict(test_images[k])\n    pred = torch.where(pred>=96, torch.ones(pred.shape)*95.99, pred)\n    pred = torch.where(pred<=0, torch.ones(pred.shape)*0.01, pred)\n    test_predictions[k] = pred.view([30])","metadata":{"execution":{"iopub.status.busy":"2021-05-30T04:40:15.491186Z","iopub.execute_input":"2021-05-30T04:40:15.491641Z","iopub.status.idle":"2021-05-30T04:41:11.334623Z","shell.execute_reply.started":"2021-05-30T04:40:15.491604Z","shell.execute_reply":"2021-05-30T04:41:11.333679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prepare submission file\n","metadata":{}},{"cell_type":"markdown","source":"The IdLookupTable.csv file tells us which feature of which image to output, for each row of the submission file.","metadata":{}},{"cell_type":"code","source":"lut = pd.read_csv('../input/facial-keypoints-detection/IdLookupTable.csv', index_col='RowId')\nlut.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-30T04:41:11.335911Z","iopub.execute_input":"2021-05-30T04:41:11.336273Z","iopub.status.idle":"2021-05-30T04:41:11.37146Z","shell.execute_reply.started":"2021-05-30T04:41:11.336236Z","shell.execute_reply":"2021-05-30T04:41:11.370279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the above output, we note that ImageIDs are starting with 1 (i.e. are 1-based), so we need to subtract 1 later when accessing our predictions array.\n\nLet's read the sample submission file.  \nWe will prepare the submission by simply replacing the Location entry in each row with our own predictions","metadata":{}},{"cell_type":"code","source":"sample = pd.read_csv('../input/facial-keypoints-detection/SampleSubmission.csv', index_col='RowId')\nsample['Location'] = sample['Location'].astype(np.float)\nsample.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-30T04:41:11.373056Z","iopub.execute_input":"2021-05-30T04:41:11.373441Z","iopub.status.idle":"2021-05-30T04:41:11.398036Z","shell.execute_reply.started":"2021-05-30T04:41:11.373402Z","shell.execute_reply":"2021-05-30T04:41:11.397282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For easier mapping of column names to prediction array indices, we create a dictionary:","metadata":{}},{"cell_type":"code","source":"namedict = {train_csv.columns[k1]: k1 for k1 in range(len(train_csv.columns))}\nprint(namedict)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T04:41:11.400366Z","iopub.execute_input":"2021-05-30T04:41:11.40086Z","iopub.status.idle":"2021-05-30T04:41:11.407186Z","shell.execute_reply.started":"2021-05-30T04:41:11.400821Z","shell.execute_reply":"2021-05-30T04:41:11.406104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we can access the predicted facial features with their column names:","metadata":{}},{"cell_type":"code","source":"print(test_predictions.shape)\nprint(test_predictions[0][:])\nprint(namedict['right_eye_center_y'])\nprint(test_predictions[0][namedict['right_eye_center_y']])","metadata":{"execution":{"iopub.status.busy":"2021-05-30T04:41:11.4088Z","iopub.execute_input":"2021-05-30T04:41:11.409298Z","iopub.status.idle":"2021-05-30T04:41:11.419783Z","shell.execute_reply.started":"2021-05-30T04:41:11.409254Z","shell.execute_reply":"2021-05-30T04:41:11.418091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now all we have to do is to go through the sample submission file, extract the image id and feature name from each row,\nand then insert the predicted coordinate into the sample DataFrame.","metadata":{}},{"cell_type":"code","source":"for k1 in range(sample.shape[0]):\n    imageid = lut.iloc[k1]['ImageId']-1\n    featurename = lut.iloc[k1]['FeatureName']\n    featurecol = namedict[featurename]\n    sample.iloc[k1]['Location'] = test_predictions[imageid,featurecol]","metadata":{"execution":{"iopub.status.busy":"2021-05-30T04:41:11.422474Z","iopub.execute_input":"2021-05-30T04:41:11.422767Z","iopub.status.idle":"2021-05-30T04:41:20.318381Z","shell.execute_reply.started":"2021-05-30T04:41:11.422738Z","shell.execute_reply":"2021-05-30T04:41:20.317593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check that the sample dataframe now contains our predictions:","metadata":{}},{"cell_type":"code","source":"sample.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-30T04:41:20.319765Z","iopub.execute_input":"2021-05-30T04:41:20.320105Z","iopub.status.idle":"2021-05-30T04:41:20.329888Z","shell.execute_reply.started":"2021-05-30T04:41:20.32007Z","shell.execute_reply":"2021-05-30T04:41:20.328829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally output the sample DataFrame as 'submission.csv' file: ","metadata":{}},{"cell_type":"code","source":"sample.to_csv('submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-05-30T04:41:20.331294Z","iopub.execute_input":"2021-05-30T04:41:20.331895Z","iopub.status.idle":"2021-05-30T04:41:20.441457Z","shell.execute_reply.started":"2021-05-30T04:41:20.33183Z","shell.execute_reply":"2021-05-30T04:41:20.440625Z"},"trusted":true},"execution_count":null,"outputs":[]}]}