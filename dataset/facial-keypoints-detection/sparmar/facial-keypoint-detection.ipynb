{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Facial Keypoint Detection"},{"metadata":{"_uuid":"4dbced162440c393a0a5b7e5aee344711a30e994"},"cell_type":"markdown","source":"The objective of this task is to predict keypoint positions on face images.\n"},{"metadata":{},"cell_type":"markdown","source":"## Introduction/Business Problem"},{"metadata":{},"cell_type":"markdown","source":"This can be used as a building block in several applications, such as:\n\ntracking faces in images and video analysing facial expressions detecting dysmorphic facial signs for medical diagnosis biometrics / face recognition Detecing facial keypoints is a very challenging problem. Facial features vary greatly from one individual to another, and even for a single individual, there is a large amount of variation due to 3D pose, size, position, viewing angle, and illumination conditions. Computer vision research has come a long way in addressing these difficulties, but there remain many opportunities for improvement."},{"metadata":{},"cell_type":"markdown","source":"## Import all useful libraries"},{"metadata":{"trusted":true,"_uuid":"369fa247a546e39d82bdfdc5b7d4ed58baa40e4f"},"cell_type":"code","source":"import types\nimport pandas as pd\nimport numpy as np\nfrom botocore.client import Config\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom IPython.display import clear_output\nfrom time import sleep\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom keras.models import Sequential, Model\nfrom keras.layers import Activation, Convolution2D, MaxPooling2D, BatchNormalization, Flatten, Dense, Dropout, Conv2D,MaxPool2D, ZeroPadding2D\nfrom keras.layers import Conv2D,Dropout,Dense,Flatten\nfrom keras.models import Sequential\nimport os","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Section"},{"metadata":{},"cell_type":"markdown","source":"### Load Data From CSV File"},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls ../input/training.zip","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Contents of input/facial-keypoints-detection directory: \")\n!ls ../input/\n\nprint(\"\\nExtracting .zip dataset files to working directory ...\")\n!unzip -u ../input/test.zip\n!unzip -u ../input/training.zip\n\nprint(\"\\nCurrent working directory:\")\n!pwd\nprint(\"\\nContents of working directory:\")\n!ls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls ../input","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Load Data From CSV File"},{"metadata":{"trusted":true,"_uuid":"fa1b76273d02502e3fd668dddf74ecf522044524"},"cell_type":"code","source":"lookid_dir = '../input/IdLookupTable.csv'\ntrain_data = pd.read_csv('training.csv')  \ntest_data = pd.read_csv('test.csv')\nlookid_data = pd.read_csv(lookid_dir)\nos.listdir('../input')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### About dataset"},{"metadata":{},"cell_type":"markdown","source":"Each predicted keypoint is specified by an (x,y) real-valued pair in the space of pixel indices. There are 15 keypoints, which represent the following elements of the face:\n\nleft_eye_center, right_eye_center, left_eye_inner_corner, left_eye_outer_corner, right_eye_inner_corner, right_eye_outer_corner, left_eyebrow_inner_end, left_eyebrow_outer_end, right_eyebrow_inner_end, right_eyebrow_outer_end, nose_tip, mouth_left_corner, mouth_right_corner, mouth_center_top_lip, mouth_center_bottom_lip\n\nLeft and right here refers to the point of view of the subject.\n\nIn some examples, some of the target keypoint positions are misssing (encoded as missing entries in the csv, i.e., with nothing between two commas).\n\nThe input image is given in the last field of the data files, and consists of a list of pixels (ordered by row), as integers in (0,255). The images are 96x96 pixels.\n\nData files\n\ntraining.csv: list of training 7049 images. Each row contains the (x,y) coordinates for 15 keypoints, and image data as row-ordered list of pixels.\ntest.csv: list of 1783 test images. Each row contains ImageId and image data as row-ordered list of pixels\nsubmissionFileFormat.csv: list of 27124 keypoints to predict. Each row contains a RowId, ImageId, FeatureName, Location. FeatureName are \"left_eye_center_x,\" \"right_eyebrow_outer_end_y,\" etc. Location is what you need to predict. "},{"metadata":{},"cell_type":"markdown","source":"### Data Understanding"},{"metadata":{"trusted":true,"_uuid":"cfd045f7166f9cce2e2075b3ead83813d07012c8"},"cell_type":"code","source":"train_data.head().T","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"14ae8eb84c1ed40db949d6dddeba331b2ed84487"},"cell_type":"markdown","source":"Lets check for missing values"},{"metadata":{"trusted":true,"_uuid":"67368f645afe618d6fc717552a6847de7e5ec66a"},"cell_type":"code","source":"train_data.isnull().any().value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f55b89f149e6bbc79d0e9c636c64c5c6fc7192b2"},"cell_type":"markdown","source":"So there are missing values in 28 columns. We can do two things here one remove the rows having missing values and another is the fill missing values with something. I used two option as removing rows will reduce our dataset. \nI filled the missing values with the previous values in that row."},{"metadata":{"trusted":true,"_uuid":"69165acb462a9b47f22fe81a8fe8eaaca4f518d2"},"cell_type":"code","source":"\ntrain_data.fillna(method = 'ffill',inplace = True)\n#train_data.reset_index(drop = True,inplace = True)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e6cd1f4c243b44ecc0530bf416d761930a4f94d1"},"cell_type":"markdown","source":"Lets check for missing values now"},{"metadata":{"trusted":true,"_uuid":"29ca4e12ec805d837ec4eac91cb91d5a133f8740"},"cell_type":"code","source":"train_data.isnull().any().value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data visualization"},{"metadata":{"_uuid":"e1b88f1528838c0a8fec61f9a02a70b5077312e9"},"cell_type":"markdown","source":"As there is no missing values we can now separate the labels and features.\nThe image is our feature and other values are labes that we have to predict later.\nAs image column values are in string format and there is also some missing values so we have to split the string by space and append it and also handling missing values"},{"metadata":{"trusted":true,"_uuid":"e78ca4523425835f1b584f3e30e5c9dcc8014253"},"cell_type":"code","source":"imag = []\nfor i in range(0,7049):\n    img = train_data['Image'][i].split(' ')\n    img = ['0' if x == '' else x for x in img]\n    imag.append(img)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e112a93d30f86687f80fb01d9f916633d87682db"},"cell_type":"markdown","source":"Lets reshape and convert it into float value."},{"metadata":{"trusted":true,"_uuid":"da09436050dc2df5da7cb49ad90125b1b756f1a4"},"cell_type":"code","source":"image_list = np.array(imag,dtype = 'float')\nX_train = image_list.reshape(-1,96,96,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"58a352bb3b38dadf37da1a6b62798fe1e3df8614","collapsed":true},"cell_type":"markdown","source":"Lets see what is the second image."},{"metadata":{"trusted":true,"_uuid":"3e953b9fa753d6f7c1092a2d8d2faf6cff5a4f93"},"cell_type":"code","source":"plt.imshow(X_train[1].reshape(96,96),cmap='gray')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Pre-processing"},{"metadata":{"trusted":true},"cell_type":"code","source":"training = train_data.drop('Image',axis = 1)\ntraining.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now lets separate labels."},{"metadata":{"trusted":true,"_uuid":"e9d804a035809cdf8ffda19f41ce3feb278a38fb"},"cell_type":"code","source":"training = train_data.drop('Image',axis = 1)\n\ny_train = []\nfor i in range(0,7049):\n    y = training.iloc[i,:]\n\n    y_train.append(y)\ny_train = np.array(y_train,dtype = 'float')\ny_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modeling / Methodology and Analysis"},{"metadata":{"_uuid":"d1ade36c2a9ffc30411987384691c49a8859818b"},"cell_type":"markdown","source":"As our data is ready for training , lets define our model. I am using keras and simple dense layers. For loss function I am using 'mse' ( mean squared error ) as we have to predict new values. Our result evaluted on the basics of 'mae' ( mean absolute error ) . "},{"metadata":{"trusted":true,"_uuid":"182dbbf5e211249cd5087f2e1218c7000c15e8d7"},"cell_type":"code","source":"model = Sequential([Flatten(input_shape=(96,96)),\n                         Dense(128, activation=\"relu\"),\n                         Dropout(0.1),\n                         Dense(64, activation=\"relu\"),\n                         Dense(30)\n                         ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\n\nmodel.add(Convolution2D(32, (3,3), padding='same', use_bias=False, input_shape=(96,96,1)))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\n\nmodel.add(Convolution2D(32, (3,3), padding='same', use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\nmodel.add(Convolution2D(64, (3,3), padding='same', use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\n\nmodel.add(Convolution2D(64, (3,3), padding='same', use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\nmodel.add(Convolution2D(96, (3,3), padding='same', use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\n\nmodel.add(Convolution2D(96, (3,3), padding='same', use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\nmodel.add(Convolution2D(128, (3,3),padding='same', use_bias=False))\n# model.add(BatchNormalization())\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\n\nmodel.add(Convolution2D(128, (3,3),padding='same', use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\nmodel.add(Convolution2D(256, (3,3),padding='same',use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\n\nmodel.add(Convolution2D(256, (3,3),padding='same',use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\nmodel.add(Convolution2D(512, (3,3), padding='same', use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\n\nmodel.add(Convolution2D(512, (3,3), padding='same', use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\n\n\nmodel.add(Flatten())\nmodel.add(Dense(512,activation='relu'))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(30))\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer='adam', \n              loss='mean_squared_error',\n              metrics=['mae'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e4cf4686b410841f2e34dbb081f3429d1b0f67e9"},"cell_type":"markdown","source":"Now our model is defined and we will train it by calling fit method. I ran it for 500 iteration keeping batch size and validtion set size as 20% ( 20% of the training data will be kept for validating the model )."},{"metadata":{"trusted":true,"_uuid":"894af9cbfcf2dca50e7407946cad318157b77d0a"},"cell_type":"code","source":"model.fit(X_train,y_train,epochs = 50,batch_size = 256,validation_split = 0.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Prepare testing data"},{"metadata":{"_uuid":"cf7e21d1b2e1282636b8bca23d1297ec43642179"},"cell_type":"markdown","source":"Now lets prepare our testing data"},{"metadata":{"trusted":true,"_uuid":"587e6f2a158cccc7b8de4df99885c9878c6a5683"},"cell_type":"code","source":"#preparing test data\ntimag = []\nfor i in range(0,1783):\n    timg = test_data['Image'][i].split(' ')\n    timg = ['0' if x == '' else x for x in timg]\n    \n    timag.append(timg)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"88db8ebf9a5e18eb120ed1808ec361ff53bdc7be"},"cell_type":"markdown","source":"Reshaping and converting "},{"metadata":{"trusted":true,"_uuid":"1203ed3b00d70e52de0ac457facfb774a11f5816"},"cell_type":"code","source":"timage_list = np.array(timag,dtype = 'float')\nX_test = timage_list.reshape(-1,96,96,1) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d6733e0e2d3fa1384f84bdfaa143b6278e07e736"},"cell_type":"markdown","source":"Lets see first image in out test data"},{"metadata":{"trusted":true,"_uuid":"bd3f2367733d37be74f26a8a53a8b33808eaf64b"},"cell_type":"code","source":"plt.imshow(X_test[0].reshape(96,96),cmap = 'gray')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c41bd777b9055ed782a475d88f9e7c0ac7d12a2b"},"cell_type":"markdown","source":"Lets predict our results"},{"metadata":{"trusted":true,"_uuid":"ecaf24956f805de32614c5476528102bdf56b329"},"cell_type":"code","source":"pred = model.predict(X_test)\npred","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Result"},{"metadata":{},"cell_type":"markdown","source":"## Discussion"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}