{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#import all library required\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom pathlib import Path\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\n\nmpl.style.use(\"seaborn-darkgrid\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# switch to GPU","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#import all keras library component \nfrom tensorflow.keras.layers import (\n    Input,\n    Conv2D, \n    MaxPool2D, \n    Dense, \n    BatchNormalization, \n    ReLU, \n    Dropout, \n    Flatten,\n    Dropout,\n    Concatenate,\n    GlobalAvgPool2D\n)\n\nfrom tensorflow.keras.regularizers import L2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#read Dataset\ndf_train=pd.read_csv(\"../input/facial-keypoints-detection/training.zip\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head(1)\n#dataset consist of 30 feature (x and y coordinate ) and image array","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define feature and target columns\n# \nfeature_col,target_col=\"Image\",list(df_train.drop(\"Image\",axis=1).columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# replace NaN with mean  \n# if found Nan replace\ndf_train[target_col] = df_train[target_col].fillna(df_train[target_col].mean())\ndf_train.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_col # total 30 target (x,y coordinate)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define width and height of image\nIMG_WIDTH=96\nIMG_HEIGHT=96\nIMG_CANNELS=1 # grayscale image\n\n## split image and label  #image\nimages=np.array(df_train[feature_col].str.split().tolist(),dtype=\"float\").reshape(-1,IMG_HEIGHT,IMG_WIDTH,IMG_CANNELS)\nlabels=df_train[target_col].to_numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# scale image from 0 -255 to 0-1 for better training\nscaled_images=images/255.0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's make a function to show training dataset visual\n# \ndef show_examples(images, landmarks):\n  # make a figure consist of 16 images\n    fig, axes = plt.subplots(nrows=4, ncols=4, figsize=(16, 16))\n    # looping through each image\n    for img, marks, ax in zip(images, landmarks, axes.ravel()):\n        # Keypoints\n        # every odd values are x coordinate\n        # every even values are y coodinate\n        x_points = marks[:: 2]\n        y_points = marks[1::2]\n        \n        # display on figure \n        ax.imshow(img.squeeze(), cmap='gray')\n        ax.scatter(x_points, y_points, s=10, color='red')\n    \n    plt.show()\n    \n\nidx = np.random.choice(16, 16)\nshow_examples(images[idx], labels[idx])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# spliting training dataset for training and validation\ntrain_image,valid_images,train_labels,valid_labels=train_test_split(scaled_images,labels,test_size=0.1,random_state=7)\n# 10 % of data is for validation \n# 90 % of data is for traiing \n# shuffle dataset before spliting with randam state of 7\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid_labels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's start with model\n# We will create CNN model that uses Inception Architecture\ndef inception_module(inputs,f1,f2):\n    # defining convolution 2d layer \n    x1=Conv2D(f1,3,padding=\"same\")(inputs)\n    # batchnormalize the x1\n    x1=BatchNormalization()(x1)\n    x1=ReLU()(x1)\n    \n    # it was f1 change it to f2 \n    x2=Conv2D(f2,5,padding=\"same\")(inputs)\n    # batchnormalize the x1\n    x2=BatchNormalization()(x2)\n    x2=ReLU()(x2)\n    # combine x1 and x2\n    return Concatenate()([x1,x2])\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_model():\n    inputs = Input((96, 96, 1))\n\n    x = inception_module(inputs, 64,  32)\n    x = MaxPool2D()(x)\n    \n    x = inception_module(x, 64,  32)\n    x = MaxPool2D()(x)\n    \n    x = inception_module(x, 128, 32)\n    x = MaxPool2D()(x)\n    \n    x = inception_module(x, 128, 32)\n    x = MaxPool2D()(x)\n    \n    x = inception_module(x, 256, 64)\n    x = MaxPool2D()(x)\n    \n    x = Flatten()(x)\n    x = Dense(1024, kernel_regularizer=L2(l2=0.05))(x)\n    x = BatchNormalization()(x)\n    x = ReLU()(x)\n    \n    x = Dense(512, kernel_regularizer=L2(l2=0.02))(x)\n    x = BatchNormalization()(x)\n    x = ReLU()(x)\n    \n    x = Dense(128, kernel_regularizer=L2(l2=0.01))(x)\n    x = BatchNormalization()(x)\n    x = ReLU()(x)\n    \n    x = Dense(30)(x)\n    \n    model = tf.keras.Model(inputs, outputs=x)\n    return model\n\nmodel = build_model()\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#compile model \n# you can change this value to get better accuracy\nmodel.compile(optimizer=\"adam\",loss=\"mean_squared_error\",metrics=[\"mae\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating a chechpoint to save model at best accuarcy\n\nckp_path=\"trained_model/model\"\nmodel_checkpoint=tf.keras.callbacks.ModelCheckpoint(filepath=ckp_path,\n                                                   monitor=\"val_mae\",\n                                                   mode=\"auto\",\n                                                   save_best_only=True,\n                                                   save_weights_only=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create a lr reducer which decrease learning rate when accuarcy does not increase\nreduce_lr=tf.keras.callbacks.ReduceLROnPlateau(factor=0.9,monitor=\"val_mae\",\n                                             mode=\"auto\",cooldown=0,\n                                             patience=5,verbose=1,min_lr=1e-5)\n# patience : wait till 5 epoch\n# verbose : show accuracy every 1 epoch\n# min_lr=minimum learning rate\n#","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_labels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training model\nEPOCHS=300\nBATCH_SIZE=256\n\nhistory=model.fit(train_image,\n                 train_labels,\n                 validation_data=(valid_images,valid_labels),\n                 batch_size=BATCH_SIZE,\n                 epochs=EPOCHS,\n                 callbacks=[model_checkpoint,reduce_lr]\n                 )\n# found the error\n# we are getting NaN value in training because there is NaN in train set","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# now  training is working properly","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# I will see you when training is finish","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#now the training is finished load best model from check point\nmodel.load_weights(ckp_path)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# now save the model in tflite formate\n#converter that convert tf to tflite\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\ntflite_model = converter.convert()\n\n# Save the model.\nwith open('model.tflite', 'wb') as f:\n  f.write(tflite_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for testing if model prediction is correct or not \n# use test dataset\ndf_test=pd.read_csv(\"../input/facial-keypoints-detection/test.zip\")\ndf_test.head(1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_images=np.array(df_test[\"Image\"].str.split().tolist(),dtype=\"float\").reshape(-1,96,96,1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scale_test_images=test_images/255.0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction_val=model.predict(scale_test_images,batch_size=BATCH_SIZE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# draw figure using matplot\nidx=np.random.choice(16,16)\nshow_examples(test_images[idx],prediction_val[idx])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# perfect \n# now download model \n# or you can save version for future use\n# download model after creating save is done \n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Link of model and training will be in description \n# you can copy code from that link \n#","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's make a function to show training dataset visual\n# \ndef show_examples(images, landmarks):\n  # make a figure consist of 16 images\n    fig, axes = plt.subplots(nrows=4, ncols=4, figsize=(16, 16))\n    # looping through each image\n    for img, marks, ax in zip(images, landmarks, axes.ravel()):\n        # Keypoints\n        # every odd values are x coordinate\n        # every even values are y coodinate\n        x_points = marks[:: 2]\n        y_points = marks[1::2]\n        \n        # display on figure \n        ax.imshow(img.squeeze(), cmap='gray')\n    #    ax.scatter(x_points, y_points, s=10, color='red')\n    \n    plt.show()\n    \n\nidx = np.random.choice(16, 16)\nshow_examples(images[idx], labels[idx])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}