{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!unzip -u /kaggle/input/facial-keypoints-detection/training.zip\n!unzip -u /kaggle/input/facial-keypoints-detection/test.zip","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"idlookup_df = pd.read_csv(\"/kaggle/input/facial-keypoints-detection/IdLookupTable.csv\")\ntrain_df = pd.read_csv(\"/kaggle/working/training.csv\")\ntest_df = pd.read_csv(\"/kaggle/working/test.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"idlookup_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.duplicated().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unclean_train_df = train_df.fillna(method = 'ffill')\nprint(\"unclean_train_data shape: {}\\n\".format(np.shape(unclean_train_df)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.dropna(inplace=True)\ntrain_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Number of missing pixel values: {}\".format(len(unclean_train_df) - int(train_df.Image.apply(lambda x: len(x.split())).value_counts().values)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_height = 96\nimg_width = 96","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_sample(image, keypoint, axis, title):\n    image = image.reshape(img_height, img_width)\n    axis.imshow(image, cmap=\"gray\")\n    axis.scatter(keypoint[::2], keypoint[1::2], marker='x', s=20)\n    plt.title(title)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convert_data_to_image(image_data):\n    images = []\n    for _, sample in image_data.iterrows():\n        image = np.array(sample[\"Image\"].split(' '), dtype=int)\n        image = np.reshape(image, (img_height,img_width,1))\n        images.append(image)\n    images = np.array(images)/255\n    return images","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_keypoints_features(keypoint_data):\n    keypoint_data = keypoint_data.drop(\"Image\", axis=1)\n    keypoint_features = []\n    for _, sample_keypoints in keypoint_data.iterrows():\n        keypoint_features.append(sample_keypoints)\n    \n    keypoint_features = np.array(keypoint_features, dtype=\"float\")\n    return keypoint_features","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_image_index = 10\n\ntrain_images = convert_data_to_image(train_df)\n\ntrain_keypoints = get_keypoints_features(train_df)\n\nprint(\"Shape of train_images: {}\".format(np.shape(train_images)))\nprint(\"Shape of train_keypoints: {}\".format(np.shape(train_keypoints)))\n\nfig, axis = plt.subplots()\nplot_sample(train_images[sample_image_index], train_keypoints[sample_image_index], axis, \"Sample image & keypoints\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_image_index = 10\n\nunclean_train_images = convert_data_to_image(unclean_train_df)\n\nunclean_train_keypoints = get_keypoints_features(unclean_train_df)\n\nprint(\"Shape of unclean_train_images: {}\".format(np.shape(unclean_train_images)))\nprint(\"Shape of unclean_train_keypoints: {}\".format(np.shape(unclean_train_keypoints)))\n\nfig, axis = plt.subplots()\nplot_sample(unclean_train_images[sample_image_index], unclean_train_keypoints[sample_image_index], axis, \"Sample image & keypoints\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_images = convert_data_to_image(test_df)\n\ntest_keypoints = get_keypoints_features(test_df)\n\nprint(\"Shape of test_images: {}\".format(np.shape(test_images)))\nprint(\"Shape of test_keypoints: {}\".format(np.shape(test_keypoints)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"full_train_images = train_images\nfull_train_keypoints = train_keypoints\n\nfull_train_images = np.concatenate((full_train_images, unclean_train_images))\nfull_train_keypoints = np.concatenate((full_train_keypoints, unclean_train_keypoints))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train_images = unclean_train_images\n#train_keypoints = unclean_train_keypoints","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def left_right_flip(images, keypoints_features):\n    flipped_keypoints = []\n    flipped_images = np.flip(images, axis=2)\n    for index, sample_keypoints in enumerate(keypoints_features):\n        flipped_keypoints.append([96.-coord if index%2==0 else coord for index,coord in enumerate(sample_keypoints)])\n        \n    return flipped_images, flipped_keypoints","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"horizontal_flip = True\n\nflipped_train_images = np.array([])\nflipped_train_keypoints = np.array([])\n\nif horizontal_flip:\n    flipped_train_images, flipped_train_keypoints = left_right_flip(train_images, train_keypoints)\n    print(\"Shape of flipped_train_images: {}\".format(np.shape(flipped_train_images)))\n    print(\"Shape of flipped_train_keypoints: {}\".format(np.shape(flipped_train_keypoints)))\n    \n    full_train_images = np.concatenate((full_train_images, flipped_train_images))\n    full_train_keypoints = np.concatenate((full_train_keypoints, flipped_train_keypoints))\n    \n    fig, axis = plt.subplots()\n    plot_sample(flipped_train_images[sample_image_index], flipped_train_keypoints[sample_image_index], axis, \"Horizontally Flipped\")\n    \n    print(full_train_images.shape)\n    print(full_train_keypoints.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nimport math","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rotate_augmentation(images, keypoints_features, rotation_angles):\n    rotated_images = []\n    rotated_keypoints_features = []\n    for angle in rotation_angles:    \n        for angle in [angle,-angle]:\n            M = cv2.getRotationMatrix2D((img_height/2,img_width/2), angle, 1.0)\n            angle_rad = -angle*math.pi/180\n            \n            \n            for image in images:\n                rotated_image = cv2.warpAffine(image, M, (img_height,img_width), flags=cv2.INTER_CUBIC)\n                rotated_images.append(rotated_image)\n\n            for keypoint in keypoints_features:\n                rotated_keypoint = keypoint - img_height/2.    \n                \n                for idx in range(0,len(rotated_keypoint),2):\n                    rotated_keypoint[idx] = rotated_keypoint[idx]*math.cos(angle_rad)-rotated_keypoint[idx+1]*math.sin(angle_rad)\n                    rotated_keypoint[idx+1] = rotated_keypoint[idx]*math.sin(angle_rad)+rotated_keypoint[idx+1]*math.cos(angle_rad)\n                    \n                rotated_keypoint += img_height/2\n                rotated_keypoints_features.append(rotated_keypoint)\n            \n    return np.reshape(rotated_images,(-1,img_height,img_width,1)), rotated_keypoints_features","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rotation_augmentation = True\nrotation_angles = [15] \n\nrotated_train_images = np.array([])\nrotated_train_keypoints_features = np.array([])\n\nif rotation_augmentation:\n    rotated_train_images, rotated_train_keypoints_features = rotate_augmentation(train_images, train_keypoints, rotation_angles)\n    print(\"Shape of rotated_train_images: {}\".format(np.shape(rotated_train_images)))\n    print(\"Shape of rotated_train_keypoints: {}\".format(np.shape(rotated_train_keypoints_features)))\n    \n    full_train_images = np.concatenate((full_train_images, rotated_train_images))\n    full_train_keypoints = np.concatenate((full_train_keypoints, rotated_train_keypoints_features))\n    \n    fig, axis = plt.subplots()\n    plot_sample(rotated_train_images[sample_image_index], rotated_train_keypoints_features[sample_image_index], axis, \"Rotation Augmentation\")\n    \n    print(full_train_images.shape)\n    print(full_train_keypoints.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def alter_brightness(images, keypoints):    \n    altered_brightness_images = []\n    inc_brightness_images = np.clip(images*1.2, 0.0, 1.0)    \n    dec_brightness_images = np.clip(images*0.6, 0.0, 1.0)   \n    \n    altered_brightness_images.extend(inc_brightness_images)\n    altered_brightness_images.extend(dec_brightness_images)\n    \n    return altered_brightness_images, np.concatenate((keypoints, keypoints))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"brightness_augmentation = True\n\naltered_brightness_train_images = np.array([])\naltered_brightness_train_keypoints = np.array([])\n\nif brightness_augmentation:\n    altered_brightness_train_images, altered_brightness_train_keypoints = alter_brightness(train_images, train_keypoints)\n    print(f\"Shape of altered_brightness_train_images: {np.shape(altered_brightness_train_images)}\")\n    print(f\"Shape of altered_brightness_train_keypoints: {np.shape(altered_brightness_train_keypoints)}\")\n    \n    \n    full_train_images = np.concatenate((full_train_images, altered_brightness_train_images))\n    full_train_keypoints = np.concatenate((full_train_keypoints, altered_brightness_train_keypoints))\n    \n    fig, axis = plt.subplots()\n    plot_sample(altered_brightness_train_images[sample_image_index], altered_brightness_train_keypoints[sample_image_index], axis, \"Increased Brightness\") \n    \n    fig, axis = plt.subplots()\n    image_plot =  altered_brightness_train_images[len(altered_brightness_train_images)//2+sample_image_index]\n    keypoints_plot = altered_brightness_train_keypoints[len(altered_brightness_train_images)//2+sample_image_index]\n    plot_sample(image_plot, keypoints_plot, axis, \"Decreased Brightness\") \n    \n    print(full_train_images.shape)\n    print(full_train_keypoints.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def shift_images(images, keypoints, pixel_shifts):\n    shifted_images = []\n    shifted_keypoints = []\n    for shift in pixel_shifts:    \n        for (shift_x,shift_y) in [(-shift,-shift),(-shift,shift),(shift,-shift),(shift,shift)]:\n            matrix = np.float32([[1,0,shift_x],[0,1,shift_y]])\n            \n            for image, keypoint in zip(images, keypoints):\n                shifted_image = cv2.warpAffine(image, matrix, (img_height,img_width), flags=cv2.INTER_CUBIC)\n                shifted_keypoint = np.array([(point+shift_x) if idx%2==0 else (point+shift_y) for idx, point in enumerate(keypoint)])\n                \n                if np.all(0.0<shifted_keypoint) and np.all(shifted_keypoint<img_height):\n                    shifted_images.append(shifted_image.reshape(img_height,img_width,1))\n                    shifted_keypoints.append(shifted_keypoint)\n                    \n    shifted_keypoints = np.clip(shifted_keypoints,0.0,img_height)\n    return shifted_images, shifted_keypoints","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shift_augmentation = True\npixel_shifts = [12] \n\nif shift_augmentation:\n    shifted_train_images, shifted_train_keypoints = shift_images(train_images, train_keypoints, pixel_shifts)\n    print(f\"Shape of shifted_train_images: {np.shape(shifted_train_images)}\")\n    print(f\"Shape of shifted_train_keypoints: {np.shape(shifted_train_keypoints)}\")\n    \n    full_train_images = np.concatenate((full_train_images, shifted_train_images))\n    full_train_keypoints = np.concatenate((full_train_keypoints, shifted_train_keypoints))\n\n    fig, axis = plt.subplots()\n    plot_sample(shifted_train_images[sample_image_index], shifted_train_keypoints[sample_image_index], axis, \"Shift Augmentation\")\n    \n    print(full_train_images.shape)\n    print(full_train_keypoints.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def add_noise(images):\n    noisy_images = []\n    for image in images:\n        noisy_image = cv2.add(image, 0.008*np.random.randn(img_height, img_width, 1))\n        noisy_images.append(noisy_image.reshape(img_height, img_width, 1))\n        \n    return noisy_images","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random_noise_augmentation = True\n\nif random_noise_augmentation:\n    noisy_train_images = add_noise(train_images)\n    \n    print(f\"Shape of noisy_train_images: {np.shape(noisy_train_images)}\")\n    \n    full_train_images = np.concatenate((full_train_images, noisy_train_images))\n    full_train_keypoints = np.concatenate((full_train_keypoints, train_keypoints))\n   \n    fig, axis = plt.subplots()\n    plot_sample(noisy_train_images[sample_image_index], train_keypoints[sample_image_index], axis, \"Random Noise Augmentation\")\n    \n    print(full_train_images.shape)\n    print(full_train_keypoints.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(20,8))\nfor i in range(10):\n    axis = fig.add_subplot(2, 5, i+1, xticks=[], yticks=[])\n    plot_sample(train_images[i], train_keypoints[i], axis, \"\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if horizontal_flip:\n    print(\"Horizontal Flip Augmentation: \")\n    fig = plt.figure(figsize=(20,8))\n    for i in range(10):\n        axis = fig.add_subplot(2,5,i+1,xticks=[],yticks=[])\n        plot_sample(flipped_train_images[i], flipped_train_keypoints[i], axis, \"\")\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if rotation_augmentation:\n    print(\"Rotation Augmentation: \")\n    fig = plt.figure(figsize=(20,8))\n    for i in range(10):\n        axis = fig.add_subplot(2, 5, i+1, xticks=[], yticks=[])\n        plot_sample(rotated_train_images[i], rotated_train_keypoints_features[i], axis, \"\")\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if brightness_augmentation:\n    print(\"Brightness Augmentation: \")\n    fig = plt.figure(figsize=(20,8))\n    for i in range(10):\n        axis = fig.add_subplot(2, 5, i+1, xticks=[], yticks=[])\n        plot_sample(altered_brightness_train_images[i], altered_brightness_train_keypoints[i], axis, \"\")\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if shift_augmentation:\n    print(\"Shift Augmentation: \")\n    fig = plt.figure(figsize=(20,8))\n    for i in range(10):\n        axis = fig.add_subplot(2, 5, i+1, xticks=[], yticks=[])\n        plot_sample(shifted_train_images[i], shifted_train_keypoints[i], axis, \"\")\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if random_noise_augmentation:\n    print(\"Random Noise Augmentation: \")\n    fig = plt.figure(figsize=(20,8))\n    for i in range(10):\n        axis = fig.add_subplot(2, 5, i+1, xticks=[], yticks=[])\n        plot_sample(noisy_train_images[i], train_keypoints[i], axis, \"\")\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.utils import shuffle\nfull_train_images, full_train_keypoints = shuffle(full_train_images, full_train_keypoints, random_state=0)\nprint(full_train_images.shape)\nprint(full_train_keypoints.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import Sequential, Model, load_model\nfrom keras.layers import Activation, Convolution2D, MaxPooling2D, BatchNormalization, Flatten, Dense, Dropout, Conv2D, MaxPool2D, ZeroPadding2D\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\nfrom keras.optimizers import Adam","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Sequential()\n\n# ----------------------------------------\nmodel.add(Convolution2D(32, (3,3), padding='same', use_bias=False, input_shape=(96,96,1)))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\n\nmodel.add(Convolution2D(32, (3,3), padding='same', use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\n\n# ----------------------------------------\nmodel.add(Convolution2D(64, (3,3), padding='same', use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\n\nmodel.add(Convolution2D(64, (3,3), padding='same', use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\n\n# ----------------------------------------\nmodel.add(Convolution2D(96, (3,3), padding='same', use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\n\nmodel.add(Convolution2D(96, (3,3), padding='same', use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\n\n# ----------------------------------------\nmodel.add(Convolution2D(128, (3,3),padding='same', use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\n\nmodel.add(Convolution2D(128, (3,3),padding='same', use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\n\n# ----------------------------------------\nmodel.add(Convolution2D(256, (3,3),padding='same',use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\n\nmodel.add(Convolution2D(256, (3,3),padding='same',use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\n\n# ----------------------------------------\nmodel.add(Convolution2D(512, (3,3), padding='same', use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\n\nmodel.add(Convolution2D(512, (3,3), padding='same', use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\n\nmodel.add(Flatten())\nmodel.add(Dense(512,activation='relu'))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(30))\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\nmodel_file_path = \"best_model.hdf5\"\nnum_epochs = 100\nbatch_size = 64\nvalidation_ratio = 0.05\nif False:\n#if os.path.exists(model_file_path):\n    model = load_model(model_file_path)\n\nelse:\n    checkpoint = ModelCheckpoint(filepath=model_file_path, monitor=\"val_mea\", verbose=1, save_best_only=True, mode=\"min\")\n    es = EarlyStopping(monitor=\"val_mea\", mode=\"min\", verbose=1, patience=10)\n    #lr_reduce = ReduceLROnPlateau(monitor=\"val_mea\", factor=0, min_delta=0.001, patience=1, verbose=1)\n    \n    model.compile(optimizer=\"adam\", loss=\"mean_squared_error\", metrics=['mae', 'acc'])\n    \n    history = model.fit(full_train_images, full_train_keypoints, epochs=num_epochs, batch_size=batch_size, validation_split=validation_ratio, callbacks=[checkpoint, es], verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['mae'])\nplt.plot(history.history['val_mae'])\nplt.title('Mean Absolute Error vs Epoch')\nplt.ylabel('Mean Absolute Error')\nplt.xlabel('Epochs')\nplt.legend(['train', 'validation'], loc='upper right')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('Accuracy vs Epoch')\nplt.ylabel('Accuracy')\nplt.xlabel('Epochs')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Loss vs Epoch')\nplt.ylabel('Loss')\nplt.xlabel('Epochs')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n \n#model = load_model(model_file_path)\ntest_preds = model.predict(test_images)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(20,16))\nfor i in range(20):\n    axis = fig.add_subplot(4, 5, i+1, xticks=[], yticks=[])\n    plot_sample(test_images[i], test_preds[i], axis, \"\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_names = list(idlookup_df['FeatureName'])\nimage_ids = list(idlookup_df['ImageId']-1)\nrow_ids = list(idlookup_df['RowId'])\n\nfeature_list = []\nfor feature in feature_names:\n    feature_list.append(feature_names.index(feature))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = []\nfor x,y in zip(image_ids, feature_list):\n    predictions.append(test_preds[x][y])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"row_ids = pd.Series(row_ids, name = 'RowId')\nlocations = pd.Series(predictions, name = 'Location')\nlocations = locations.clip(0.0,96.0)\nsubmission_result = pd.concat([row_ids,locations],axis = 1)\nsubmission_result.to_csv('submission.csv',index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}