{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#原始数据将image的数据变成96*96的形状，缺失数据删除，用DNN模型\n#将Image缩进[0, 1]范围内\n#采用CNN，参数为这个Case种，Best Score的参数, 不适用BatchNormalization\n#Convolution2D改为Conv2D, LeakyRelu改进Conv2D里面\n#使用adam算法\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom keras.layers import Dense, Dropout, Flatten, Convolution2D, MaxPool2D, BatchNormalization, LeakyReLU\nfrom keras.models import Sequential\nfrom keras.optimizers import RMSprop\nfrom keras.callbacks import ReduceLROnPlateau\n\nclass Data_Clean(object):\n    def __init__(self, trainPath, testPath):\n        self.train = pd.read_csv(trainPath)\n        self.test = pd.read_csv(testPath)\n\n    def check_null(self):\n        print(self.train.isnull().any().value_counts())\n        print(len(self.train['left_eye_center_x']))\n\n    def drop_na_row(self):\n        self.train = self.train.dropna()\n\n    def reshape_image(self):\n        self.train = self.train.apply(lambda im: np.fromstring(im, sep=' ', dtype=float))\n        # scaler the data to [0, 1]\n        self.train = self.train.values / 255.0\n        self.train = np.vstack(self.train).reshape(-1, 96, 96, 1)\n        self.test = self.test.apply(lambda img: np.fromstring(img, sep=' ', dtype=float))\n        # scaler the data to [0, 1]\n        self.test = self.test.values / 255.0\n        self.test = np.vstack(self.test).reshape(-1, 96, 96, 1)\n\n    def split_train_label(self):\n        self.label = self.train.drop(['Image'], axis=1)\n        y_label = []\n        for i in range(len(self.label)):\n            y_val = self.label.iloc[i, :]\n            y_label.append(y_val)\n        self.label = np.array(y_label, dtype='float')\n        self.train = self.train['Image']\n        self.test = self.test['Image']\n\n    def show_a_picture(self):\n        feature = np.split(self.label[9], 30)\n        x, y = [], []\n        for i in range(0, 30, 2):\n            x.append(feature[i])\n            y.append(feature[i+1])\n        plt.imshow(self.train[9].reshape(96, 96))\n        #plt.imshow(self.train[9].values)\n        plt.plot(x, y, 'o', color='red')\n        plt.show()\n\n    def run(self):\n        # Delete the Null data\n        self.drop_na_row()\n        # Split the train and label\n        self.split_train_label()\n        #reshape\n        self.reshape_image()\n        # up to here, we finished cleaning our data\n        # show a picture to see if it is okay\n        #self.show_a_picture()\n        return self.train, self.label, self.test\n\nclass CNN(object):\n    def __init__(self, xtrain, xtest, ytrain, ytest, test):\n        self.xtrain = xtrain\n        self.xtest = xtest\n        self.ytrain = ytrain\n        self.ytest = ytest\n        self.test = test\n\n    def define_CNN(self):\n        model = Sequential()\n        model.add(Convolution2D(filters=32, kernel_size=(3, 3), padding='same', use_bias=False, input_shape=(96, 96, 1)))\n        model.add(LeakyReLU(alpha=0.1))\n        model.add(BatchNormalization())\n\n        model.add(Convolution2D(filters=32, kernel_size=(3, 3), padding='same', use_bias=False))\n        model.add(LeakyReLU(alpha=0.1))\n        model.add(BatchNormalization())\n        model.add(MaxPool2D(pool_size=(2, 2)))\n\n        model.add(Convolution2D(filters=64, kernel_size=(3, 3), padding='same', use_bias=False))\n        model.add(LeakyReLU(alpha=0.1))\n        model.add(BatchNormalization())\n\n        model.add(Convolution2D(filters=64, kernel_size=(3, 3), padding='same', use_bias=False))\n        model.add(LeakyReLU(alpha=0.1))\n        model.add(BatchNormalization())\n        model.add(MaxPool2D(pool_size=(2, 2)))\n\n        model.add(Convolution2D(filters=96, kernel_size=(3, 3), padding='same', use_bias=False))\n        model.add(LeakyReLU(alpha=0.1))\n        model.add(BatchNormalization())\n\n        model.add(Convolution2D(filters=96, kernel_size=(3, 3), padding='same', use_bias=False))\n        model.add(LeakyReLU(alpha=0.1))\n        model.add(BatchNormalization())\n        model.add(MaxPool2D(pool_size=(2, 2)))\n\n        model.add(Convolution2D(filters=128, kernel_size=(3, 3), padding='same', use_bias=False))\n        model.add(LeakyReLU(alpha=0.1))\n        model.add(BatchNormalization())\n\n        model.add(Convolution2D(filters=128, kernel_size=(3, 3), padding='same', use_bias=False))\n        model.add(LeakyReLU(alpha=0.1))\n        model.add(BatchNormalization())\n        model.add(MaxPool2D(pool_size=(2, 2)))\n\n        model.add(Convolution2D(filters=256, kernel_size=(3, 3), padding='same', use_bias=False))\n        model.add(LeakyReLU(alpha=0.1))\n        model.add(BatchNormalization())\n\n        model.add(Convolution2D(filters=256, kernel_size=(3, 3), padding='same', use_bias=False))\n        model.add(LeakyReLU(alpha=0.1))\n        model.add(BatchNormalization())\n        model.add(MaxPool2D(pool_size=(2, 2)))\n\n        model.add(Convolution2D(filters=512, kernel_size=(3, 3), padding='same', use_bias=False))\n        model.add(LeakyReLU(alpha=0.1))\n        model.add(BatchNormalization())\n\n        model.add(Convolution2D(filters=512, kernel_size=(3, 3), padding='same', use_bias=False))\n        model.add(LeakyReLU(alpha=0.1))\n        model.add(BatchNormalization())\n\n        model.add(Flatten())\n        model.add(Dense(512, activation='relu'))\n        model.add(Dropout(0.1))\n        model.add(Dense(30))\n        self.model = model\n\n    def Adam(self, epochs, batchSize):\n        self.model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n        self.model.fit(self.xtrain, self.ytrain, batch_size=batchSize, epochs=epochs, validation_data=(self.xtest, self.ytest),\n                       verbose=2)\n\n    def prediction(self):\n        pred = self.model.predict(self.test)\n        lookid_data = pd.read_csv('../input/IdLookupTable.csv')\n        lookid_list = list(lookid_data['FeatureName'])\n        imageID = list(lookid_data['ImageId'] - 1)\n        pred_list = list(pred)\n        rowid = lookid_data['RowId']\n        rowid = list(rowid)\n        feature = []\n        for f in list(lookid_data['FeatureName']):\n            feature.append(lookid_list.index(f))\n    \n        preded = []\n        for x, y in zip(imageID, feature):\n            preded.append(pred_list[x][y])\n        rowid = pd.Series(rowid, name='RowId')\n        loc = pd.Series(preded, name='Location')\n        submission = pd.concat([rowid, loc], axis=1)\n        submission.to_csv('Utkarsh.csv', index=False)\n\n    def show_any_result(self):\n        self.pred = self.model.predict(self.xtest)\n        import copy\n        xtest, ypred, ytest = self.xtest[37], copy.deepcopy(self.pred[37]), copy.deepcopy(self.ytest[37])\n        predx, predy, testx, testy = [], [], [], []\n        xypred = np.split(ypred, 30)\n        xytest = np.split(ytest, 30)\n        for i in range(0, 30, 2):\n            predx.append(xypred[i])\n            predy.append(xypred[i+1])\n            testx.append(xytest[i])\n            testy.append(xytest[i + 1])\n        plt.imshow(xtest.reshape(96, 96))\n        #plt.imshow(self.train[9].values)\n        plt.plot(predx, predy, 'o', color='red')\n        plt.plot(testx, testy, 'o', color='blue')\n        plt.show()\n\n\n    def run(self):\n        self.define_CNN()\n        self.Adam(epochs=200, batchSize= 128)\n        self.show_any_result()\n        self.prediction()\n\nimport os\nif __name__ == '__main__':\n    dataPath =  '../input/training/training.csv'\n    testPath = '../input/test/test.csv'\n    data_clean = Data_Clean(dataPath, testPath)\n    train, label, test = data_clean.run()\n\n    global random_seed\n    random_seed = 9\n    xtrain, xtest, ytrain, ytest = train_test_split(train, label, test_size=0.1, random_state=random_seed)\n\n    cnn = CNN(xtrain, xtest, ytrain, ytest, test)\n    model = cnn.run()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}