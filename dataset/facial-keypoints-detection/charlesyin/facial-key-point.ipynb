{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Prepare data\n# Read the data from csv and save them to npy\n\ntrain_dir = '../input/training/training.csv'\ndata = pd.read_csv(train_dir)\n\nnum_images = data.shape[0]\nimages = np.zeros((num_images, 96, 96))\nlandmarks = np.zeros((num_images, data.shape[1] - 1))\n\nfor i in range(num_images):\n    img = data['Image'][i].split(' ')\n    img = np.array(img, dtype='float32').reshape(96, 96)\n    images[i, :, :] = img\n\n    ldmk = np.array(data.iloc[i, 0:-1], dtype='float32')\n    landmarks[i, :] = ldmk\n\nnp.save('train_data.npy', images)\nnp.save('train_ldmk.npy', landmarks)\n\n# Read and save test data\n\ndata = pd.read_csv('../input/test/test.csv')\nnum_images = data.shape[0]\nimages = np.zeros((num_images, 96, 96))\n\nfor i in range(num_images):\n    img = data['Image'][i].split(' ')\n    img = np.array(img, dtype='float32').reshape(96, 96)\n    images[i, :, :] = img\n\nnp.save('test_data.npy', images)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"51513fc5f625bc882dae15e40bbe8fb380d3290a"},"cell_type":"code","source":"# load data\nimport numpy as np\nfrom torch.utils.data import Dataset\nimport torch.utils.data as Data\nimport matplotlib.pyplot as plt\nfrom torchvision import transforms\nfrom skimage import transform\nimport torch\nimport cv2\n\n\nclass Rescale(object):\n    def __init__(self, output_size):\n        assert isinstance(output_size, (int, tuple))\n        self.output_size = output_size\n\n    def __call__(self, sample):\n        image, landmarks = sample['image'], sample['landmarks']\n        h, w = image.shape[:2]\n        if isinstance(self.output_size, int):\n            if h > w:\n                new_h, new_w = self.output_size * h / w, self.output_size\n            else:\n                new_h, new_w = self.output_size, self.output_size * w / h\n        else:\n            new_h, new_w = self.output_size\n\n        new_h, new_w = int(new_h), int(new_w)\n        img = transform.resize(image, (new_h, new_w))\n        landmarks = landmarks * [new_w * 1.0 / w, new_h * 1.0 / h]\n\n        return {'image': img, 'landmarks': landmarks}\n\n\nclass RandomCrop(object):\n    def __init__(self, output_size):\n        assert isinstance(output_size, (int, tuple))\n        if isinstance(output_size, int):\n            self.output_size = (output_size, output_size)\n        else:\n            assert len(output_size) == 2\n            self.output_size = output_size\n\n    def __call__(self, sample):\n        image, landmarks = sample['image'], sample['landmarks']\n        h, w = image.shape[:2]\n        new_h, new_w = self.output_size\n        if h > new_h:\n            top = np.random.randint(0, h - new_h)\n        else:\n            top = 0\n        if w > new_w:\n            left = np.random.randint(0, w - new_w)\n        else:\n            left = 0\n        image = image[top:top + new_h,\n                left:left + new_w]\n        landmarks = landmarks - [left, top]\n        return {'image': image, 'landmarks': landmarks}\n\n\nclass RandomFlip(object):\n    def __init__(self, p=0.5):\n        self.p = 0.5\n\n    def __call__(self, sample):\n        image, landmarks = sample['image'], sample['landmarks']\n        p = np.random.rand()\n        if p > self.p:\n            image = np.fliplr(image)\n            landmarks = landmarks * [-1, 1] + [96, 0]\n\n        return {'image': image, 'landmarks': landmarks}\n\n\ndef get_rotation_mat(img, angle_degree):\n    height, width = img.shape[0:2]\n    center = (width / 2.0, height / 2.0)\n    angle_radians = np.radians(angle_degree)\n    rot_mat = cv2.getRotationMatrix2D(center, angle_degree, scale=1)\n    new_height = height * np.abs(np.cos(angle_radians)) + width * np.abs(np.sin(angle_radians))\n    new_width = height * np.abs(np.sin(angle_radians)) + width * np.abs(np.cos(angle_radians))\n    new_center = (new_width / 2.0, new_height / 2.0)\n    dx, dy = (new_center[0] - center[0], new_center[1] - center[1])\n    rot_mat[0, 2] += dx\n    rot_mat[1, 2] += dy\n    # img = cv2.warpAffine(img, rot_mat,(int(new_width), int(new_height)))\n    return rot_mat, (int(new_width), int(new_height))\n\n\ndef show_sample(sample):\n    img = sample['image']\n    lm = sample['landmarks']\n    plt.imshow(img, cmap='gray')\n    plt.scatter(lm[:, 0], lm[:, 1], s=10, c='r')\n    plt.show()\n\n\ndef test_rotation(img, lm, angle_degree):\n    rot_mat, new_size = get_rotation_mat(img, angle_degree)\n    new_img = cv2.warpAffine(img, rot_mat, new_size)\n    lm = np.hstack((lm, np.ones((lm.shape[0], 1))))\n    new_lm = np.dot(lm, rot_mat.T)\n    plt.imshow(new_img, cmap='gray')\n    plt.scatter(new_lm[:, 0], new_lm[:, 1], s=10, c='r')\n    plt.show()\n\n\nclass RandomRotation(object):\n    def __init__(self, angle_degree=10):\n        self.angle = angle_degree\n\n    def __call__(self, sample):\n        image, landmarks = sample['image'], sample['landmarks']\n        angle = np.random.uniform(-self.angle, self.angle, 1)\n        rot_mat, new_size = get_rotation_mat(image, angle)\n\n        image = cv2.warpAffine(image, rot_mat, new_size)\n        landmarks = np.hstack((landmarks, np.ones((landmarks.shape[0], 1))))\n        landmarks = np.dot(landmarks, rot_mat.T)\n\n        return {'image': image, 'landmarks': landmarks}\n\n\nclass Normalize(object):\n    def __call__(self, sample):\n        image, landmarks = sample['image'], sample['landmarks']\n        image = (image / 255.0 - 0.5) / 0.5\n        image = image[np.newaxis, ...]\n        landmarks = (landmarks / 96.0 - 0.5) / 0.5\n        return {'image': image, 'landmarks': landmarks}\n\n\nclass ToTensor(object):\n    def __call__(self, sample):\n        image, landmarks = sample['image'], sample['landmarks']\n        return {'image': torch.Tensor(image), 'landmarks': torch.Tensor(landmarks)}\n\n\nclass FaceDataset(Dataset):\n    def __init__(self, pth_img, transform=None):\n        self.images = np.load(pth_img)\n        self.transform = transform\n        self.num_imgs = len(self.images)\n\n    def __len__(self):\n        return self.num_imgs\n\n    def __getitem__(self, idx):\n        img = self.images[idx]\n        lms = np.random.rand(30)\n        sample = {'image': img, 'landmarks': lms.reshape(-1, 2)}\n        if self.transform:\n            sample = self.transform(sample)\n        return sample['image']\n\n\nclass FaceLandmarkDataset(Dataset):\n    def __init__(self, pth_img, pth_ldmk, transform=None):\n        self.images = np.load(pth_img)\n        self.transform = transform\n        self.landmarks = np.load(pth_ldmk)\n        self.num_imgs = len(self.images)\n\n    def __len__(self):\n        return self.num_imgs\n\n    def __getitem__(self, idx):\n        img = self.images[idx]\n        lm = self.landmarks[idx]\n        sample = {'image': img, 'landmarks': lm.reshape(-1, 2)}\n        if self.transform:\n            sample = self.transform(sample)\n        return sample\n\n\ndef get_train_loader():\n    scale = Rescale(110)\n    crop = RandomCrop(110)\n    flip = RandomFlip(0.5)\n    rot = RandomRotation(15)\n    norm = Normalize()\n    totensor = ToTensor()\n    composed = transforms.Compose([flip, scale, rot, crop, norm, totensor])\n\n    train_data = FaceLandmarkDataset('train_data.npy', 'train_ldmk.npy', transform=composed)\n    train_loader = Data.DataLoader(train_data, batch_size=32, shuffle=True, num_workers=4)\n    return train_loader\n\ndef get_test_loader():\n    scale = Rescale(110)\n    norm = Normalize()\n    totensor = ToTensor()\n    composed = transforms.Compose([scale, norm, totensor])\n    test_data = FaceDataset('test_data.npy', transform=composed)\n    test_loader = Data.DataLoader(test_data, batch_size=32, shuffle=False, num_workers=4)\n    return test_loader\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d3f03368999f35aea2b17b1654a25b3b9d96ef2c"},"cell_type":"code","source":"# construct models\nimport torch.nn as nn\n\n\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(1, 16, 3),\n            nn.ReLU(),\n            nn.MaxPool2d(2, stride=2),\n\n            nn.Conv2d(16, 32, 3),\n            nn.ReLU(),\n            nn.MaxPool2d(2, stride=2),\n\n            nn.Conv2d(32, 64, 3),\n            nn.ReLU(),\n            nn.MaxPool2d(2, stride=2),\n\n            nn.Conv2d(64, 128, 3),\n            nn.ReLU(),\n            nn.MaxPool2d(2, stride=2),\n        )\n\n        self.fc = nn.Sequential(\n            nn.Linear(128 * 5 * 5, 512),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n\n            nn.Linear(512, 30)\n        )\n\n    def forward(self, x):\n        conv_feat = self.conv(x)\n        out = self.fc(conv_feat.view(-1, 128 * 5 * 5))\n        return out\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3c6956a49300ec5b009af2851f8a2a6127560aed"},"cell_type":"code","source":"train_loader = get_train_loader()\ntest_loader = get_test_loader()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1fe9cf2c35850a7bd3537ab4e581c30e5c0e9d07"},"cell_type":"code","source":"# test loader\nfrom torchvision.utils import make_grid\nimport cv2\nimport torch\nimport math\n\ndef get_grid(batch_img, batch_lm):\n    processed = []\n    for idx in range(batch_img.shape[0]):\n        img = (batch_img[idx, 0].numpy() * 0.5 + 0.5) * 255.0\n        img = cv2.cvtColor(img.astype('uint8'), cv2.COLOR_GRAY2BGR)\n        lm = (batch_lm[idx].numpy().reshape((-1, 2)) * 0.5 + 0.5) * 96\n        for lm_idx in range(lm.shape[0]):\n            if not math.isnan(lm[lm_idx][0]):\n                cv2.circle(img, (int(lm[lm_idx][0]), int(lm[lm_idx][1])), 3, (0, 255, 0), -1)\n        processed.append(torch.from_numpy(img).permute(2, 0, 1))\n    processed = torch.stack(processed, dim=0)\n    I = make_grid(processed).permute(1, 2, 0)\n\n    return I\n\nsample = iter(train_loader).next()\nimgs, lms = sample['image'], sample['landmarks']\nI = get_grid(imgs, lms)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fe053516708aeeaf043aae9c3eefb5814eee5c08"},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.imshow(I.numpy().astype('uint8'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"139caa54d97d433ce696a19a9e824e34c33b3384"},"cell_type":"code","source":"# define loss function\ndef loss_func(pred, gt):\n    batch_sz = pred.shape[0]\n    diff = pred - gt.view(batch_sz, -1)\n    nan_ind = (diff!=diff)\n    diff[nan_ind] = 0\n    loss = diff.pow(2).sum() / (diff.numel()-nan_ind.sum())\n    return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8aa00a49b186bb19ec81780016cd44517345e3ce"},"cell_type":"code","source":"from torch import optim\n\n# initialize the model\nmodel = Model()\nmodel.cuda()\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\n\ndef train(epoch):\n    model.train()\n    train_loss = 0\n    for batch_idx, samples in enumerate(train_loader):\n        imgs, lms = samples['image'], samples['landmarks']\n        imgs = imgs.cuda()\n        lms = lms.cuda()\n\n        optimizer.zero_grad()\n        pred = model(imgs)\n        loss = loss_func(pred, lms)\n        loss.backward()\n        train_loss += loss.item()\n        optimizer.step()\n        \n    train_loss /= len(train_loader)\n    if (epoch+1) % 10 == 0:\n        print('======> Epoch: {}\\t Average Train Loss: {}'.format(epoch, train_loss))\n    return train_loss\n\ntotal_train_loss = []\nfor epoch in range(150):\n    loss_epoch = train(epoch)\n    total_train_loss.append(loss_epoch)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d02ada320f70e7b724b05448c35a0d6eda1822e0"},"cell_type":"code","source":"def test():\n    imgs = iter(test_loader).next()\n    imgs = imgs.cuda()\n    pred = model(imgs)\n    I = get_grid(imgs.detach().cpu(), pred.detach().cpu())\n    return I\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2486efb48840d1ff2b789660fc7782f57c6dd3c1"},"cell_type":"code","source":"I = test()\nplt.imshow(I)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"85e7a10b5b4c1445b06fe6acfdc20f573dceb777"},"cell_type":"code","source":"test_data = test_loader.dataset\npred = []\nfor idx in range(len(test_data)):\n    img = test_data[idx].unsqueeze(0)\n    pt = model(img.cuda())\n    pred.append(pt)\npred = torch.stack(pred, dim=0).detach().squeeze().cpu().numpy()\n\nlookid_data = pd.read_csv('../input/IdLookupTable.csv')\nlookid_list = list(lookid_data['FeatureName'])\nimageID = list(lookid_data['ImageId'] - 1)\nrowid = lookid_data['RowId']\nrowid = list(rowid)\nfeature = []\nfor f in list(lookid_list):\n    feature.append(lookid_list.index(f))\n\npre_list = list(pred)\npreded = []\nfor x, y in zip(imageID, feature):\n    preded.append(pre_list[x][y])\n\nrowid = pd.Series(rowid, name='RowId')\nloc = pd.Series(preded, name='Location')\nsubmission = pd.concat([rowid, loc], axis=1)\nsubmission.to_csv('face_key_detection_submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"089a74d7a527806a3cbfedc7e9b9b249e71d3a1d"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}