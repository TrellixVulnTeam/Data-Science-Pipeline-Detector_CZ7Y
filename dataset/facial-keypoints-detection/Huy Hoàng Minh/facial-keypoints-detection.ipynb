{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n# Assume that we are on a CUDA machine, then this should print a CUDA device:\n\nprint(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom math import sin, cos, pi\nimport cv2\nfrom tqdm.notebook import tqdm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!unzip -u /kaggle/input/facial-keypoints-detection/training.zip","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!unzip -u /kaggle/input/facial-keypoints-detection/test.zip","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_csv=pd.read_csv('training.csv')\ntest_csv=pd.read_csv('test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_info(data):\n    images_=[]\n    results_=[]\n    for idx, sample in data.iterrows():\n        image=sample['Image'].split(' ')\n        image=np.array(image,dtype=int)\n        image=image.reshape(96,96,1)\n        images_.append(image)\n    images_=np.array(images_)/255\n\n    data=data.drop('Image',axis=1)\n    for idx, sample in data.iterrows():\n        results_.append(np.array(sample))\n    results_=np.array(results_)\n\n    return(images_,results_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_train=train_csv.dropna()\nunclean_train=train_csv.fillna(method='ffill')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(clean_image_train,clean_results_train)=get_info(clean_train)\n(unclean_image_train,unclean_results_train)=get_info(unclean_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_sample(image, keypoint, axis, title):\n    image = image.reshape(96,96)\n    axis.imshow(image, cmap='gray')\n    axis.scatter(keypoint[0::2], keypoint[1::2], marker='x', s=20)\n    plt.title(title)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axis = plt.subplots()\nplot_sample(unclean_image_train[150], unclean_results_train[150], axis, \"Sample image & keypoints\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_images=np.concatenate((clean_image_train,unclean_image_train))\ntrain_results=np.concatenate((clean_results_train,unclean_results_train))\nprint(train_images.shape,train_results.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_results[5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def swap_col(keypoints,n1,n2):\n    keypoints[:,[n1,n2]]=keypoints[:,[n2,n1]]\n    return keypoints","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def flip(images, keypoints):\n    flipped_keypoints = []\n    flipped_images = np.flip(images, axis=2) \n    for idx, sample_keypoints in enumerate(keypoints):\n        flipped_keypoints.append([96.-coor if idx%2==0 else coor for idx,coor in enumerate(sample_keypoints)]) \n    flipped_keypoints=np.array(flipped_keypoints)\n    flipped_keypoints=swap_col(flipped_keypoints,0,2)\n    flipped_keypoints=swap_col(flipped_keypoints,1,3)\n    flipped_keypoints=swap_col(flipped_keypoints,4,8)\n    flipped_keypoints=swap_col(flipped_keypoints,5,9)\n    flipped_keypoints=swap_col(flipped_keypoints,6,10)\n    flipped_keypoints=swap_col(flipped_keypoints,7,11)\n    flipped_keypoints=swap_col(flipped_keypoints,12,16)\n    flipped_keypoints=swap_col(flipped_keypoints,13,17)\n    flipped_keypoints=swap_col(flipped_keypoints,14,18)\n    flipped_keypoints=swap_col(flipped_keypoints,15,19)\n    flipped_keypoints=swap_col(flipped_keypoints,22,24)\n    flipped_keypoints=swap_col(flipped_keypoints,23,25)\n    return flipped_images, flipped_keypoints","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"flip_clean_images,flip_clean_keypoints=flip(clean_image_train,clean_results_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_images=np.concatenate((train_images,flip_clean_images))\ntrain_keypoints=np.concatenate((train_results,flip_clean_keypoints))\nprint(train_images.shape,train_keypoints.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def alter_brightness(images, keypoints):\n    altered_brightness_images = []\n    inc_brightness_images = np.clip(images*1.2, 0.0, 1.0)    # Increased brightness by a factor of 1.2 & clip any values outside the range of [-1,1]\n    dec_brightness_images = np.clip(images*0.6, 0.0, 1.0)    # Decreased brightness by a factor of 0.6 & clip any values outside the range of [-1,1]\n    altered_brightness_images.extend(inc_brightness_images)\n    altered_brightness_images.extend(dec_brightness_images)\n    return altered_brightness_images, np.concatenate((keypoints, keypoints))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(altered_brightness_images,altered_brightness_keypoints)=alter_brightness(clean_image_train,clean_results_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_images=np.concatenate((train_images,altered_brightness_images))\ntrain_keypoints=np.concatenate((train_keypoints,altered_brightness_keypoints))\nprint(train_images.shape,train_keypoints.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_valid, y_train, y_valid = train_test_split(train_images, train_keypoints, test_size=0.2)\nprint(X_train.shape,X_valid.shape,y_train.shape,y_valid.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train=np.transpose(X_train,[0,3,1,2])\nX_valid=np.transpose(X_valid,[0,3,1,2])\nprint(X_train.shape,X_valid.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train=torch.from_numpy(X_train).type(torch.FloatTensor)\nX_valid=torch.from_numpy(X_valid).type(torch.FloatTensor)\ny_train=torch.from_numpy(y_train).type(torch.FloatTensor)\ny_valid=torch.from_numpy(y_valid).type(torch.FloatTensor)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size=8\nn_iter=100000\nn_epoch=int(n_iter/(y_train.shape[0]/batch_size))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntrain=torch.utils.data.TensorDataset(X_train,y_train)\nvalid=torch.utils.data.TensorDataset(X_valid,y_valid)\ntrain_loader=DataLoader(train,batch_size=batch_size,shuffle=True)\nvalid_loader=DataLoader(valid,batch_size=batch_size,shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net,self).__init__()\n        self.pool=nn.MaxPool2d(2)\n        \n        self.conv1=nn.Conv2d(1,32,kernel_size=3,padding=1)\n        self.BN1=nn.BatchNorm2d(32)\n        self.conv2=nn.Conv2d(32,32,kernel_size=3,padding=1)\n        self.BN2=nn.BatchNorm2d(32)\n        self.drop1=nn.Dropout(0.4)\n        \n        self.conv3=nn.Conv2d(32,64,kernel_size=3,padding=1)\n        self.BN3=nn.BatchNorm2d(64)\n        self.conv4=nn.Conv2d(64,64,kernel_size=3,padding=1)\n        self.BN4=nn.BatchNorm2d(64)\n        self.drop2=nn.Dropout(0.4)\n        \n        self.conv5=nn.Conv2d(64,96,kernel_size=3,padding=1)\n        self.BN5=nn.BatchNorm2d(96)\n        self.conv6=nn.Conv2d(96,96,kernel_size=3,padding=1)\n        self.BN6=nn.BatchNorm2d(96)\n        self.drop3=nn.Dropout(0.4)\n        \n        self.conv7=nn.Conv2d(96,128,kernel_size=3,padding=1)\n        self.BN7=nn.BatchNorm2d(128)\n        self.conv8=nn.Conv2d(128,128,kernel_size=3,padding=1)\n        self.BN8=nn.BatchNorm2d(128)\n        self.drop4=nn.Dropout(0.4)\n        \n        self.conv9=nn.Conv2d(128,256,kernel_size=3,padding=1)\n        self.BN9=nn.BatchNorm2d(256)\n        self.conv10=nn.Conv2d(256,256,kernel_size=3,padding=1)\n        self.BN10=nn.BatchNorm2d(256)\n        self.drop5=nn.Dropout(0.4)\n        \n        self.conv11=nn.Conv2d(256,512,kernel_size=3,padding=1)\n        self.BN11=nn.BatchNorm2d(512)\n        self.conv12=nn.Conv2d(512,512,kernel_size=3,padding=1)\n        self.BN12=nn.BatchNorm2d(512)\n        self.drop6=nn.Dropout(0.4)\n        \n        self.LN1=nn.Linear(512*3*3,512)\n        self.BN13=nn.BatchNorm1d(512)\n        self.drop7=nn.Dropout(0.4)\n        self.LN2=nn.Linear(512,256)\n        self.LN3=nn.Linear(256,30)\n        \n    def forward(self,x):\n        \n        x=F.relu(self.conv1(x))\n        x=self.BN1(x)\n        x=F.relu(self.conv2(x))\n        x=self.BN2(x)\n        x=self.pool(x)\n        x=self.drop1(x)\n        \n        x=F.relu(self.conv3(x))\n        x=self.BN3(x)\n        x=F.relu(self.conv4(x))\n        x=self.BN4(x)\n        x=self.pool(x)\n        x=self.drop2(x)\n        \n        x=F.relu(self.conv5(x))\n        x=self.BN5(x)\n        x=F.relu(self.conv6(x))\n        x=self.BN6(x)\n        x=self.pool(x)\n        x=self.drop3(x)\n        \n        x=F.relu(self.conv7(x))\n        x=self.BN7(x)\n        x=F.relu(self.conv8(x))\n        x=self.BN8(x)\n        x=self.pool(x)\n        x=self.drop4(x)\n        \n        x=F.relu(self.conv9(x))\n        x=self.BN9(x)\n        x=F.relu(self.conv10(x))\n        x=self.BN10(x)\n        x=self.pool(x)\n        x=self.drop5(x)\n        \n        x=F.relu(self.conv11(x))\n        x=self.BN11(x)\n        x=F.relu(self.conv12(x))\n        x=self.BN12(x)\n        x=self.drop6(x)\n        \n        x=x.view(-1,512*3*3)\n        x=F.relu(self.LN1(x))\n        x=self.BN13(x)\n        x=self.drop7(x)\n        x=F.relu(self.LN2(x))\n        x=self.LN3(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def weight_init(m):\n    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n        nn.init.xavier_uniform_(m.weight, gain=nn.init.calculate_gain('relu'))\n        nn.init.zeros_(m.bias)\n\nmodel=Net()\nmodel.apply(weight_init)\nmodel.to(device)\nimport torch.optim as optim\n\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.0001)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"his_train=[]\nhis_val=[]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_iter=0\n\nfor epoch in range(n_epoch):  # loop over the dataset multiple times\n    train_cnt=0\n    running_loss = 0.0\n    for i, data in enumerate(train_loader, 0):\n        # get the inputs\n        train_cnt+=1\n        inputs, labels = data\n        #print(inputs.shape,labels.shape)\n        inputs, labels = inputs.to(device),labels.to(device)\n        num_iter+=1\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = model(inputs)\n        print(type(outputs),type(labels))\n        loss = torch.sqrt(criterion(outputs, labels))\n        loss.backward()\n        optimizer.step()\n\n        # print statistics\n        running_loss += loss.item()\n#if num_iter %100== 99:    # print every 100 mini-batches\n    print('train loss[%6d]: %.6f' %(num_iter,running_loss / train_cnt))\n    \n    cnt=0\n    valid_loss = 0\n    for i, data in enumerate(valid_loader, 0):\n        inputs, labels = data\n        inputs, labels = inputs.to(device),labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = torch.sqrt(criterion(outputs, labels))\n        valid_loss += loss.item()\n        cnt+=1\n    print('valid loss: %.6f'%(valid_loss/cnt))\n    his_train.append((running_loss / train_cnt))\n    his_val.append((valid_loss/cnt))\nprint('Finished Training')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}