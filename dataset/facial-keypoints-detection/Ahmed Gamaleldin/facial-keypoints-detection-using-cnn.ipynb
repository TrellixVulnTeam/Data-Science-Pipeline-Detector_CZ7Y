{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport os\n\n# Python ≥3.5 is required\nimport sys\nassert sys.version_info >= (3, 5)\n\n# Scikit-Learn ≥0.20 is required\nimport sklearn\nassert sklearn.__version__ >= \"0.20\"\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import MinMaxScaler\n\n# TensorFlow ≥2.0 is required\nimport tensorflow as tf\nfrom tensorflow import keras\nassert tf.__version__ >= \"2.0\"\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, BatchNormalization\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline \n\n# to make this notebook's output stable across runs\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n# To plot pretty figures\n%matplotlib inline\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nmpl.rc('axes', labelsize=14)\nmpl.rc('xtick', labelsize=12)\nmpl.rc('ytick', labelsize=12)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# read train and test dataset\ntrain_df = pd.read_csv(\"/kaggle/input/facial-keypoints-detection/training.zip\")\ntest_df = pd.read_csv(\"/kaggle/input/facial-keypoints-detection/test.zip\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_df.shape)\nprint(test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check the train dataset information. You see there are lots of missing label values\ntrain_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.describe().loc['count'].plot.bar()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# there is 4909 examples from training set with missing values\ntrain_df.T.isnull().any().value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we have 2 options: removing any example with missing value | filling the missed values with keypoints average\n# removing will decrease our dataset | Filling will produce random error, but it can act as a regularizer\n\n#train_df = train_df.dropna()\ntrain_df.fillna(train_df.drop(labels=['Image'], axis=1).mean(), inplace=True)\ntrain_df.T.isnull().any().value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# the images are stored in the last column in string format.\n\nX_train_full = np.vstack(train_df['Image'].apply(lambda img: np.fromstring(img, dtype=int, sep=' ')))\nX_test = np.vstack(test_df['Image'].apply(lambda img: np.fromstring(img, sep=' ')))\nY_train_full = train_df.drop(labels=['Image'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# it is a good habit to delete unnecessary variables to free up some space\ndel train_df, test_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train_full.shape)\nprint(X_test.shape)\nprint(Y_train_full.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#reshape, convert to float32, and normalize the input and output\nX_train_full = X_train_full.reshape(-1, 96, 96, 1).astype('float32') / 255.0\nX_test = X_test.reshape(-1, 96, 96, 1).astype('float32') / 255.0\nY_train_full = Y_train_full.values.astype('float32')\n\noutput_pipe = make_pipeline(\n                MinMaxScaler(feature_range=(-1,1))\n                )\nY_train_full = output_pipe.fit_transform(Y_train_full)\n\nX_train, x_val, Y_train, y_val = train_test_split(X_train_full, Y_train_full,\n                                                  test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train.shape)\nprint(x_val.shape)\nprint(Y_train.shape)\nprint(y_val.shape)\nprint(X_test.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot random samples with keypoints from training\ndef plot_img_with_keypoints(nrows=4, ncols=4):\n    selection = np.random.choice(len(X_train), size=(nrows*ncols), replace=False)\n    images = X_train[selection]\n    keypoints = output_pipe.inverse_transform(Y_train[selection])\n    fig, axes = plt.subplots(figsize=(nrows*2, ncols*2), nrows=nrows, ncols=ncols)\n    for img, keypoint, ax in zip(images, keypoints, axes.ravel()):\n        keypoint = keypoint.reshape(15,2)\n        ax.imshow(img.reshape(96,96), cmap='gray')\n        ax.scatter(keypoint[:,0], keypoint[:,1], marker='o', s=15)\n        ax.axis('off')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_img_with_keypoints(4,4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# CNN model architecture (all these parameters might be tuned to achieve better results)\nfrom functools import partial\n\nDefaultConv2D = partial(Conv2D, activation='relu', padding='SAME')\n\nmodel = Sequential([\n    # input layer\n    BatchNormalization(input_shape=(96, 96, 1)),\n    DefaultConv2D(24, (5, 5), kernel_initializer='he_normal'),\n    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n    Dropout(0.2),\n    # layer 2\n    DefaultConv2D(36, (5, 5)),\n    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n    Dropout(0.2),\n    # layer 3\n    DefaultConv2D(48, (5, 5)),\n    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n    Dropout(0.2),\n    # layer 4\n    DefaultConv2D(64, (3, 3)),\n    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n    Dropout(0.2),\n    # layer 5\n    DefaultConv2D(64, (3, 3)),\n    Flatten(),\n    # layer 6\n    Dense(500, activation=\"relu\"),\n    # layer 7\n    Dense(90, activation=\"relu\"),\n    # layer 8\n    Dense(30),\n])\n\n# this model acheive much worse RMSE. Do you know why?\n'''\nmodel = Sequential([\n    BatchNormalization(input_shape=(96, 96, 1)),\n    DefaultConv2D(filters=24, kernel_size=7, kernel_initializer='he_normal'),\n    BatchNormalization(),\n    MaxPooling2D(pool_size=2),\n    DefaultConv2D(filters=36, kernel_size=5),\n    BatchNormalization(),\n    DefaultConv2D(filters=48, kernel_size=5),\n    BatchNormalization(),\n    MaxPooling2D(pool_size=2),\n    DefaultConv2D(filters=64, kernel_size=3),\n    BatchNormalization(),\n    DefaultConv2D(filters=64, kernel_size=3),\n    BatchNormalization(),\n    MaxPooling2D(pool_size=2),\n    DefaultConv2D(filters=128, kernel_size=3),\n    BatchNormalization(),\n    MaxPooling2D(pool_size=2),\n    Flatten(),\n    Dense(units=500, activation='relu'),\n    Dropout(0.2),\n    Dense(units=90, activation='relu'),\n    Dropout(0.2),\n    Dense(units=30),\n])\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# show model architecture\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer='adam', loss='mse',\n             metrics=['mae'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# another method is to use LearningRateScheduler, reduce the learning rate by 10% every epoch\n# annealer = LearningRateScheduler(lambda x: 1e-3 * 0.9 ** x)\nK = keras.callbacks\nreduce_lr = K.ReduceLROnPlateau(monitor='val_accuracy', patience=7,\n                                             verbose=1, factor=0.1, min_lr=0.00001)\n\nearly_stopping = K.EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True,\n                                 verbose=1, mode='auto')\n#checkpointer = K.ModelCheckpoint(filepath = 'best_model.hdf5', monitor='val_mae',\n                                 #verbose=1, save_weights_only=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# it is better to increase the batch size when the dataset is small\nepochs = 600\nbatch_size = 512\nhistory = model.fit(X_train, Y_train, validation_data=(x_val, y_val),\n                   batch_size=batch_size, epochs=epochs, shuffle=True,\n                   callbacks=[reduce_lr, early_stopping])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_loss, final_mae = model.evaluate(x_val, y_val, verbose=0)\nprint(\"Final loss: {0:.4f}, final mae: {1:.4f}\".format(final_loss * 48, final_mae * 48))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the loss and accuracy curves for training and validation \nfig, ax = plt.subplots(2,1)\nax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\nax[0].plot(history.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\nlegend = ax[0].legend(loc='best', shadow=True)\n\nax[1].plot(history.history['mae'], color='b', label=\"Training mae\")\nax[1].plot(history.history['val_mae'], color='r',label=\"Validation mae\")\nlegend = ax[1].legend(loc='best', shadow=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_img_with_keypoints_after_training(nrows=4, ncols=4):\n    selection = np.random.choice(len(X_test), size=(nrows*ncols), replace=False)\n    images = X_test[selection]\n    keypoints = output_pipe.inverse_transform(model.predict(images))\n    fig, axes = plt.subplots(figsize=(nrows*2, ncols*2), nrows=nrows, ncols=ncols)\n    for img, keypoint, ax in zip(images, keypoints, axes.ravel()):\n        keypoint = keypoint.reshape(15,2)\n        ax.imshow(img.reshape(96,96), cmap='gray')\n        ax.scatter(keypoint[:,0], keypoint[:,1], marker='o', s=15)\n        ax.axis('off')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_img_with_keypoints_after_training()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save the weights to prevent training every time you open the kernel\nmodel.save_weights(\"model.h5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# after loading, you have to compile the model\n#model.load_weights('/kaggle/input/facial-keypoints-detection2/model.h5')\n#model.compile(optimizer='adam', loss='mean_squared_error',\n #            metrics=['mae'])\n\n#final_loss, final_mae = model.evaluate(x_val, y_val, verbose=0)\n#print(\"Final loss: {0:.4f}, final mae: {1:.4f}\".format(final_loss * 48, final_mae * 48))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = model.predict(X_test) \nresults = output_pipe.inverse_transform(results)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(results.shape)\nprint(type(results))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lookup_data = pd.read_csv(\"/kaggle/input/facial-keypoints-detection/IdLookupTable.csv\")\nrow_ids = list(lookup_data['RowId'])\nimage_ids = list(lookup_data['ImageId'] - 1)\nfeature_names = list(lookup_data['FeatureName'])\n\nfeature_list = []\nfor feature in feature_names:\n    feature_list.append(feature_names.index(feature))\n    \npredictions = []\nfor x,y in zip(image_ids, feature_list):\n    predictions.append(results[x][y])\n    \nrow_ids = pd.Series(row_ids, name = 'RowId')\nlocations = pd.Series(predictions, name = 'Location')\nlocations = locations.clip(0.0,96.0)\nsubmission_result = pd.concat([row_ids,locations],axis = 1)\nsubmission_result.to_csv('facial_keypoints.csv',index = False)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_result.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}