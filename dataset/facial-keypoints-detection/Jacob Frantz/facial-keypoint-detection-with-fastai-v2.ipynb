{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Facial Keypoint Detection with Fastai V2\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"%reload_ext autoreload\n%autoreload 2\n%matplotlib inline\n\nfrom PIL import Image\nfrom fastai.vision.all import *\nfrom pathlib import Path\nimport pandas as pd\nimport tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pwd\n!cd\n!unzip -o /kaggle/input/facial-keypoints-detection/test.zip -d .\n!unzip -o /kaggle/input/facial-keypoints-detection/training.zip -d .\n!ls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get files \n\npath = Path.cwd()\ntrain = path/'training.csv'\ntest = path/'test.csv'\n\ntrain_df = pd.read_csv(train, header='infer')\ntest_df = pd.read_csv(test)\n\n\n#test_df.head() \n#train_df.describe() \ntrain_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# how to get an image from a row of the dataset\ndef str2img(row):\n  imarr = np.fromstring(row.Image, dtype='int32', sep=' ').astype(np.int32)\n  i = Image.fromarray(imarr.reshape(-1, 96)).convert('P')\n  return PILImage(i)\n\n# how to get the keypoints from a row of the dataset\ndef row2points(r): \n  a = np.reshape(r[0:30].values, (15,2)).astype(np.float64)\n  return a\n\n#type(PILImage(str2img(train_df.iloc[5298])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# investigate which columns are null\nlabel_df = train_df[train_df.columns[:-1]] # -1 bc last row is img\nnulls_by_row = label_df.isnull().sum(axis=1)\nnulls_by_row.plot()\nnulls_by_row.value_counts() # most common nulls are 22 (4755 rows) and 0 (2140 rows)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##  deal with missing values\n\n## Option A: only take \"full\" rows\n#label_df = train_df[train_df.columns[:-1]] #\n#train_df = train_df.loc[label_df.notnull().sum(axis=1) == 30]\n\n## Option B: fill everything with prev value\ntrain_df = train_df.fillna(method='ffill')\n\n#train_df[train_df.columns[:-1]].describe()\n#train_df[train_df.columns[:-1]] = train_df[train_df.columns[:-3]].fillna(train_df[train_df.columns[:-3]].mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"db = DataBlock(\n    blocks = (ImageBlock, PointBlock),\n    get_x = str2img,\n    get_y = row2points,\n    splitter = RandomSplitter(valid_pct=0.15, seed=42),\n    batch_tfms = aug_transforms(do_flip=False, max_zoom=1.0), # should prob adjust these params    \n)\ndls = db.dataloaders(train_df)\ndls.show_batch()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dls.train.show_batch()\ndls.valid.show_batch()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn = cnn_learner(dls, resnet152)\nlearn.lr_find()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fine_tune(10) # should really do like 50\nlearn.show_results()\nlearn.save('after-first-finetune')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# function to sanity check and play around with results\ndef show_pred(i, df):\n    pic = PILImage(str2img(df.iloc[i]))\n    (pred, t1, t2) = learn.predict(pic) #don't know what t1 and t2 are\n    pic = TensorImage(pic)\n    screen = pic.show()\n    pred.show(ctx=screen)\n    \nshow_pred(1000, test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = [learn.predict(PILImage(str2img(test_df.iloc[idx])))[0] for idx in range(len(test_df))]\nresults = [x.reshape(30).numpy() for x in preds]\nresults = pd.DataFrame(results)\nresults.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results.shape\n\nnames = [\n         'left_eye_center_x','left_eye_center_y',\n         'right_eye_center_x','right_eye_center_y',\n         'left_eye_inner_corner_x','left_eye_inner_corner_y',\n         'left_eye_outer_corner_x','left_eye_outer_corner_y',\n         'right_eye_inner_corner_x','right_eye_inner_corner_y',\n         'right_eye_outer_corner_x','right_eye_outer_corner_y',\n         'left_eyebrow_inner_end_x', 'left_eyebrow_inner_end_y',\n         'left_eyebrow_outer_end_x', 'left_eyebrow_outer_end_y',\n         'right_eyebrow_inner_end_x', 'right_eyebrow_inner_end_y',\n         'right_eyebrow_outer_end_x', 'right_eyebrow_outer_end_y',\n         'nose_tip_x','nose_tip_y',\n         'mouth_left_corner_x','mouth_left_corner_y',\n         'mouth_right_corner_x','mouth_right_corner_y',\n         'mouth_center_top_lip_x','mouth_center_top_lip_y',\n         'mouth_center_bottom_lip_x','mouth_center_bottom_lip_y'\n]\ndicty = {}\nfor x in range(30):\n  dicty[x] = names[x]\nprint(dicty)\nresults.rename(dicty, axis='columns', inplace=True)\nresults['ImageId'] = range(1, 1783+1)\n#results.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# need to go from point values as COLUMNS to point values as ROWS\nsorted_results = results.melt(id_vars='ImageId', value_vars=names).sort_values(by=['ImageId'])\nsorted_results['variable'] = pd.CategoricalIndex(sorted_results['variable'], names)\nsorted_results.rename(columns={'variable':'FeatureName', 'value':'Location'}, inplace=True)\nsorted_results.sort_values(by=['ImageId','FeatureName'], inplace=True)\nsorted_results.set_index(['ImageId','FeatureName'], inplace=True)\n#sortrez.head(30)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# and then put predictions the way the submission file wants it\n\nlook = pd.read_csv('/kaggle/input/facial-keypoints-detection/IdLookupTable.csv')\nlook.set_index(['ImageId','FeatureName'], inplace=True)\nlook.head()\n\ncombo = look.join(sorted_results, on=['ImageId','FeatureName'], lsuffix='remove')\ncombo.drop(columns='Locationremove', inplace=True)\ncombo.reset_index(inplace=True)\ncombo[['RowId','ImageId','FeatureName','Location']]\ncombo['Location'] = combo['Location'].clip(lower=0, upper=96)\ncombo.describe()\n#combo","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combo[['RowId','Location']].to_csv('submission.csv', index=False)\n# ran with like 70 epochs and got ~top third of leaderboard","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}