{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_data = pd.read_csv('/kaggle/input/facial-keypoints-detection/training/training.csv')  \ntest_data = pd.read_csv('/kaggle/input/facial-keypoints-detection/test/test.csv')\nlookid_data = pd.read_csv('/kaggle/input/facial-keypoints-detection/IdLookupTable.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef show_img(image,loc,y_min,y_max):\n    plt.imshow(image.reshape(96,96),cmap='gray')\n    plt.scatter((loc[0::2]*(y_max-y_min))+y_min, (loc[1::2]*(y_max-y_min))+y_min , marker='x', s=10)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def data_preprocess(train_data,is_test):\n    train_data.isnull().any().value_counts()\n    train_data=train_data.fillna(method = 'ffill')\n    train_data.isnull().any().value_counts()#removing None\n    imgs=[]\n    for i in range(len(train_data)):#preparing X \n       img=train_data['Image'][i].split(' ')\n       img=[0 if x=='' else int(x) for x in img ]\n       imgs.append(img)\n    imgs=np.array(imgs,dtype = 'float')\n    images=imgs.reshape([-1,96,96,1])\n    X_train=images/255\n    if is_test==True:\n        return X_train\n    else:\n       training=train_data.drop('Image',axis=1)#prepearing y\n       y_train=[]\n       for i in range(0,len(train_data)):\n          y=training.iloc[i,:]\n          y_train.append(y)\n       y_train=np.array(y_train,dtype = 'float')\n       y_min=y_train.min()\n       y_max=y_train.max()\n       y_train=(y_train-y_train.min())/(y_train.max()-y_train.min())\n       #print(y_train.min(),y_train.max())\n       return X_train,y_train ,y_min,y_max\n    \nX_train,y_train,y_min,y_max=data_preprocess(train_data,False)\nX_test=data_preprocess(test_data,is_test=True)\nshow_img(X_train[1],y_train[1],y_min,y_max)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\ntf.reset_default_graph()\n\ntrain_graph = tf.Graph()\nwith train_graph.as_default():\n    \n    Weights = {\n    \n    'Wc1' : tf.get_variable('W0', shape = (3, 3, 1, 32), initializer = tf.contrib.layers.xavier_initializer()),\n    'Wc2' : tf.get_variable('W1', shape = (3, 3, 32, 64), initializer = tf.contrib.layers.xavier_initializer()),\n    'Wc3' : tf.get_variable('W2', shape = (3, 3, 64, 128), initializer = tf.contrib.layers.xavier_initializer()),\n    'Wc4' : tf.get_variable('W3', shape = (3, 3, 128, 256), initializer = tf.contrib.layers.xavier_initializer()),\n    'Wc5' : tf.get_variable('W4', shape = (3, 3, 256, 512), initializer = tf.contrib.layers.xavier_initializer()),\n\n    #'Wc6' : tf.get_variable('W5', shape = (3, 3, 512, 1024), initializer = tf.contrib.layers.xavier_initializer()),\n    #'Wc7' : tf.get_variable('W6', shape = (3, 3, 1024, 2048), initializer = tf.contrib.layers.xavier_initializer()),        \n        \n    'Wd1' : tf.get_variable('W7', shape = (3 * 3 * 512, 256), initializer = tf.contrib.layers.xavier_initializer()),\n    'Wd2' : tf.get_variable('W8', shape = (256, 128), initializer = tf.contrib.layers.xavier_initializer()),\n\n    'Wd3' : tf.get_variable('W9', shape = (128, 68), initializer = tf.contrib.layers.xavier_initializer()),\n\n                            \n    'Wd4' : tf.get_variable('W10', shape = (68, 30), initializer = tf.contrib.layers.xavier_initializer())\n\n    }\n\n    Biases = {\n    'bc1': tf.get_variable('B0', shape = (32), initializer = tf.contrib.layers.xavier_initializer()),\n    'bc2': tf.get_variable('B1', shape = (64), initializer = tf.contrib.layers.xavier_initializer()),\n    'bc3': tf.get_variable('B2', shape = (128), initializer = tf.contrib.layers.xavier_initializer()),\n    'bc4': tf.get_variable('B3', shape = (256), initializer = tf.contrib.layers.xavier_initializer()),\n    'bc5': tf.get_variable('B4', shape = (512), initializer = tf.contrib.layers.xavier_initializer()),\n    \n    #'bc6': tf.get_variable('B5', shape = (1024), initializer = tf.contrib.layers.xavier_initializer()),\n    #'bc7': tf.get_variable('B6', shape = (2048), initializer = tf.contrib.layers.xavier_initializer()),    \n        \n    'bd1': tf.get_variable('B7', shape = (256), initializer = tf.contrib.layers.xavier_initializer()),\n    'bd2': tf.get_variable('B8', shape = (128), initializer = tf.contrib.layers.xavier_initializer()),\n\n    'bd3': tf.get_variable('B9', shape = (68), initializer = tf.contrib.layers.xavier_initializer()),\n        \n    \n    'bd4': tf.get_variable('B10', shape = (30), initializer = tf.contrib.layers.xavier_initializer()) \n    }\n\n    '''\n    \n    \n    Weights = {   \n    \n    'Wc1' : tf.Variable(tf.random_normal([3,3,1,32])),\n    'Wc2' : tf.Variable(tf.random_normal([3,3,32,64])),\n    'Wc3' :tf.Variable(tf.random_normal([3,3,64,128])),\n    'Wc4' :tf.Variable(tf.random_normal([3,3,128,256])),\n    'Wc5' :tf.Variable(tf.random_normal([3,3,256,512])),\n\n    #'Wc6' : tf.get_variable('W5', shape = (3, 3, 512, 1024), initializer = tf.contrib.layers.xavier_initializer()),\n    #'Wc7' : tf.get_variable('W6', shape = (3, 3, 1024, 2048), initializer = tf.contrib.layers.xavier_initializer()),        \n            \n    'Wd1' : tf.Variable(tf.random_normal([3*3*512,256])),\n    'Wd2' : tf.Variable(tf.random_normal([256,128])),\n\n    'Wd3' :tf.Variable(tf.random_normal([128,68])),\n\n                            \n    'Wd4' : tf.Variable(tf.random_normal([68,30]))\n\n    }\n\n    Biases = {\n    'bc1': tf.Variable(tf.random_normal([32])),\n    'bc2':tf.Variable(tf.random_normal([64])),\n    'bc3': tf.Variable(tf.random_normal([128])),\n    'bc4': tf.Variable(tf.random_normal([256])),\n    'bc5': tf.Variable(tf.random_normal([512])),\n    \n    #'bc6': tf.get_variable('B5', shape = (1024), initializer = tf.contrib.layers.xavier_initializer()),\n    #'bc7': tf.get_variable('B6', shape = (2048), initializer = tf.contrib.layers.xavier_initializer()),    \n        \n    'bd1': tf.Variable(tf.random_normal([256])),\n    'bd2': tf.Variable(tf.random_normal([128])),\n\n    'bd3': tf.Variable(tf.random_normal([68])),\n        \n    \n    'bd4': tf.Variable(tf.random_normal([30])) \n    }\n    '''\n    \n    \n    \n    \n    \n    \n    \ndef conv2d(input_,W,b,stride=1):\n    with train_graph.as_default():\n\n       out=tf.nn.conv2d(input_,W,[1,stride,stride,1],'SAME')\n       out=tf.nn.bias_add(out, b)\n       out=tf.nn.relu(out)\n    \n    return out\ndef maxPooling2d(input_,k=2):\n    with train_graph.as_default():\n\n     return tf.nn.max_pool(input_,ksize=[1, k, k, 1],strides=[1, k, k, 1],padding='SAME')\n\ndef batch_normalization(input_,is_training):\n   with train_graph.as_default():\n\n    return tf.compat.v1.layers.batch_normalization(input_, training=is_training)\n\ndef conv_net(input_,W,b,stride,k,dropout,is_training):\n   with train_graph.as_default():\n\n    conv1=conv2d(input_,W['Wc1'],b['bc1'],stride)\n    conv1=tf.nn.dropout(conv1,dropout)\n    conv1=batch_normalization(conv1,is_training) \n    conv1=tf.nn.leaky_relu(conv1,alpha=0.1)\n    conv1=maxPooling2d(conv1,k)\n    conv2=conv2d(conv1,W['Wc2'],b['bc2'],stride)\n    conv2=tf.nn.dropout(conv2,dropout)\n    conv2=batch_normalization(conv2,is_training) \n    conv2=tf.nn.leaky_relu(conv2,alpha=0.1)\n    conv2=maxPooling2d(conv2,k)\n    conv3=conv2d(conv2,W['Wc3'],b['bc3'],stride)\n    conv3=tf.nn.dropout(conv3,dropout)\n    conv3=batch_normalization(conv3,is_training) \n    conv3=tf.nn.leaky_relu(conv3,alpha=0.1)\n    conv3=maxPooling2d(conv3,k)\n    conv4=conv2d(conv3,W['Wc4'],b['bc4'],stride)\n    conv4=tf.nn.dropout(conv4,dropout)\n    conv4=batch_normalization(conv4,is_training) \n    conv4=tf.nn.leaky_relu(conv4,alpha=0.1)\n    conv4=maxPooling2d(conv4,k)\n    conv5=conv2d(conv4,W['Wc5'],b['bc5'],stride)\n    conv5=tf.nn.dropout(conv5,dropout)\n    conv5=batch_normalization(conv5,is_training) \n    conv5=tf.nn.leaky_relu(conv5,alpha=0.1)\n    conv5=maxPooling2d(conv5,k)\n    '''\n    conv6=conv2d(conv5,W['Wc6'],b['bc6'],stride)\n    conv6=tf.nn.dropout(conv6,dropout)\n    conv6=batch_normalization(conv6,is_training) \n    \n    conv7=conv2d(conv6,W['Wc7'],b['bc7'],stride)\n    conv7=tf.nn.dropout(conv7,dropout)\n    conv7=batch_normalization(conv7,is_training) \n    '''\n    fc1_input=tf.reshape(conv5,[-1,W['Wd1'].shape.as_list()[0]])\n    fc1=tf.add(tf.matmul(fc1_input,W['Wd1']),b['bd1'])\n    fc1=batch_normalization(fc1,is_training) \n    fc1 = tf.nn.relu(fc1)\n    fc1=tf.nn.dropout(fc1,dropout)\n    fc2=tf.add(tf.matmul(fc1,W['Wd2']),b['bd2'])\n    fc2=batch_normalization(fc2,is_training) \n    fc2 = tf.nn.relu(fc2)\n    fc2=tf.nn.dropout(fc2,dropout)\n    \n    fc3=tf.add(tf.matmul(fc2,W['Wd3']),b['bd3'])\n    fc3= tf.nn.relu(fc3)\n    fc3=tf.nn.dropout(fc3,dropout)\n    \n    fc4=tf.add(tf.matmul(fc3,W['Wd4']),b['bd4'])\n    \n    \n    return fc4\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_batches(X_train,y_train,batch_size):\n    if X_train.shape[0]%batch_size==0:\n        no_batches=X_train.shape[0]/batch_size\n    else :\n        no_batches=X_train.shape[0]//batch_size+1\n    for batch in range(int(no_batches)):\n        \n       if batch<no_batches-1:\n          yield X_train[batch*batch_size:(batch*batch_size)+batch_size,:],\\\n                              y_train[batch*batch_size:(batch*batch_size)+batch_size,:]\n       elif batch==no_batches-1 :\n           yield X_train[batch*batch_size:,:,:,:],\\\n                              y_train[batch*batch_size:,:]  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# parameters\nlearning_rate = 0.001\nepochs = 300\nbatch_size = 256\n\n# network Parameters\nno_output = 30  # number of the output neurons\ndropout = 0.90 # dropout (probability to keep units)   \n#train_graph = tf.Graph()\nwith train_graph.as_default():\n     X_=tf.placeholder(tf.float32,[None,96,96,1],name='input_') \n     y_=tf.placeholder(tf.float32,[None,30],name='out')\n     keep_prob = tf.placeholder(tf.float32,name='drop')\n     is_training = tf.placeholder(tf.bool,name='training')\n     logits=conv_net(X_,Weights,Biases,1,2,keep_prob,is_training)\n     cost = tf.reduce_mean(tf.losses.mean_squared_error(predictions=logits, labels=y_))\n     with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n        train_opt = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n     init=tf.global_variables_initializer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with train_graph.as_default():\n    saver = tf.train.Saver()\n    with tf.Session() as sess:\n        sess.run(init)\n        train_losses=[]\n        val_losses=[]\n        \n        for e in range(epochs):\n          #batch_no=0\n          train_epoch_loss=0\n          val_epoch_loss=0\n            \n          for X,y in generate_batches(X_train,y_train,batch_size):\n              #print(X.shape,y.shape)\n              dict_= { X_:np.array(X[:-5,:],dtype=np.float32).reshape([-1,96,96,1]),\n                       y_:np.array(y[:-5,:],dtype=np.float32).reshape([-1,30]),\n                       keep_prob: dropout,\n                       is_training:True\n                      }\n              sess.run(train_opt, feed_dict=dict_)\n              train_loss = sess.run(cost, feed_dict=dict_)\n            \n              dict_= { X_:np.array(X[-5:,:],dtype=np.float32).reshape([-1,96,96,1]),\n                       y_:np.array(y[-5:,:],dtype=np.float32).reshape([-1,30]),\n\n                       keep_prob: dropout,\n                       is_training:False\n                      }\n              val_loss = sess.run(cost, feed_dict=dict_)            \n            \n              train_epoch_loss+=train_loss\n              val_epoch_loss+=val_loss\n                \n          train_losses.append(train_epoch_loss)\n          val_losses.append(val_epoch_loss)\n\n          print('Epoch {:>2} - '\n              'train Loss: {:>10.4f} '\n              'val Loss: {:>10.4f} '.format(\n                e + 1,\n                train_epoch_loss,\n                val_epoch_loss  \n                ))\n              #batch_no=batch_no+1\n        save_path = saver.save(sess, \"model_\")        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n      \nplt.plot(train_losses,linewidth=1,label=\"train:\")\nplt.legend()\nplt.grid()\nplt.yscale(\"log\")\nplt.xlabel(\"epoch\")\nplt.ylabel(\"log loss\")\nplt.show()\n\nplt.plot(val_losses,linewidth=1,label=\"train:\")\nplt.legend()\nplt.grid()\nplt.yscale(\"log\")\nplt.xlabel(\"epoch\")\nplt.ylabel(\"log loss\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with train_graph.as_default():\n    saver = tf.train.Saver()\n\n    with tf.Session() as sess:\n       saver.restore(sess, tf.train.latest_checkpoint('./'))\n       dict_= { X_:X_test.reshape([-1,96,96,1]),\n                    keep_prob: 1.0,\n                    is_training: False\n                   }\n       pred=sess.run(logits, feed_dict=dict_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_img(X_test[0],pred[0],y_min,y_max)\nshow_img(X_test[1],pred[1],y_min,y_max)\nshow_img(X_test[2],pred[2],y_min,y_max)\nshow_img(X_test[3],pred[3],y_min,y_max)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"landmark_dict={}\nlookid_list = list(lookid_data['FeatureName'])\n\nfor f in list(lookid_data['FeatureName']):\n    landmark_dict.update({f:lookid_list.index(f)})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ImageId = lookid_data[\"ImageId\"]\nFeatureName = lookid_data[\"FeatureName\"]\nRowId = lookid_data[\"RowId\"]\npred=(pred*(y_max-y_min))+y_min\n\n\nfor i in range(pred.shape[0]):\n    for j in range(pred.shape[1]):\n         if pred[i][j]>96 :\n            pred[i][j]=96\n         elif pred[i][j]<0:\n            pred[i][j]=-pred[i][j]            \n\n            \nsubmit = []\nfor rowId,irow,landmark in zip(RowId,ImageId,FeatureName):\n    submit.append([rowId,pred[irow-1][landmark_dict[landmark]]])\n    \nsubmit = pd.DataFrame(submit,columns=[\"RowId\",\"Location\"])\n    ## adjust the scale \nprint(submit.shape)\n\nsubmit.to_csv(\"submision1.csv\",index=False) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'submision1.csv')\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}