{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import argparse\nfrom itertools import islice\nimport json\nfrom pathlib import Path\nimport shutil\nimport warnings\nfrom typing import Dict\nfrom torch.utils.data import DataLoader\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import fbeta_score\nfrom sklearn.exceptions import UndefinedMetricWarning\nimport torch\nfrom torch import nn, cuda\nfrom multiprocessing.pool import ThreadPool\nfrom torch.optim import Adam\nfrom tqdm import tqdm\nfrom pathlib import Path\nfrom typing import Callable, List\nfrom torch.nn.parallel.data_parallel import data_parallel\nimport cv2\nimport pandas as pd\nfrom PIL import Image\nimport torch\nimport torch.utils.data.sampler as torchSampler\nfrom torch.utils.data import Dataset\nimport random\nimport math\nimport torch.nn.functional\nimport torch.functional as F\nimport torchvision.models as M\nfrom torchvision.transforms import (\n    ToTensor, Normalize, Compose, Resize, CenterCrop, RandomCrop,\n    RandomHorizontalFlip,ColorJitter,RandomRotation)\nfrom scipy import ndimage\nfrom __future__ import print_function, division, absolute_import\nfrom collections import OrderedDict\n\nimport torch.nn as nn\nfrom torch.utils import model_zoo\nfrom functools import partial\ndef multi_apply(func, *args, **kwargs):\n    pfunc = partial(func, **kwargs) if kwargs else func\n    map_results = map(pfunc, *args)\n    return tuple(map(list, zip(*map_results)))\nimport os","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random.seed(233)\nSIZE = 352\n\ndef random_scale(image, size_=-1):\n    if size_ == -1:\n        sizes = [round(SIZE * 0.8), SIZE, round(SIZE * 1.2)]\n        size_ = random.choice(sizes)\n    image = cv2.resize(image, (size_, size_))\n\n    if size_ == SIZE:\n        return image\n    x_start = random.randint(0, abs(SIZE - size_))\n    y_start = random.randint(0, abs(SIZE - size_))\n    if size_ > SIZE:\n        return image[x_start:x_start + SIZE, y_start:y_start + SIZE]\n    else:\n        image_zero = np.zeros((SIZE, SIZE, 3))\n        image_zero[x_start:x_start + size_, y_start:y_start + size_] = image\n        return image_zero.astype(np.uint8)\n\ntensor_transform = Compose([\n    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ndef transform_test(image):\n    raw_image = image.copy()\n    images = []\n    for size in [SIZE]:\n        image = raw_image.copy()\n\n        image_changed = random_scale(image, size)\n        image = np.transpose(image_changed, (2, 0, 1)).astype(np.float)\n        image = torch.from_numpy(image).div(255).float()\n        images.append(tensor_transform(image))\n\n        image = np.fliplr(image_changed)\n        image = np.transpose(image, (2, 0, 1)).astype(np.float)\n        image = torch.from_numpy(image).div(255).float()\n        images.append(tensor_transform(image))\n\n    return images","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"ON_KAGGLE = True\nN_CLASSES = 1103\nDATA_ROOT = Path('../input/imet-2019-fgvc6')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def gmean_df(df: pd.DataFrame) -> pd.DataFrame:\n    return df.groupby(level=0).agg(lambda x: gmean(list(x)))\n\n\ndef mean_df(df: pd.DataFrame) -> pd.DataFrame:\n    return df.groupby(level=0).mean()\n\n\ndef load_model(model: nn.Module, path: Path) -> Dict:\n    state = torch.load(str(path))\n    model.load_state_dict(state['model'])\n    print('Loaded model from epoch {epoch}, step {step:,}'.format(**state))\n    return state\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef load_model(model: nn.Module, path: Path) -> Dict:\n    state = torch.load(str(path))\n    model.load_state_dict(state['model'])\n    print('Loaded model from epoch {epoch}, step {step:,}'.format(**state))\n    return state\n__all__ = ['SENet', 'senet154', 'se_resnet50', 'se_resnet101', 'se_resnet152',\n           'se_resnext50_32x4d', 'se_resnext101_32x4d']\n\npretrained_settings = {\n   \n    'se_resnext101_32x4d': {\n        'imagenet': {\n            'url': 'http://data.lip6.fr/cadene/pretrainedmodels/se_resnext101_32x4d-3b2fe3d8.pth',\n            'input_space': 'RGB',\n            'input_size': [3, 224, 224],\n            'input_range': [0, 1],\n            'mean': [0.485, 0.456, 0.406],\n            'std': [0.229, 0.224, 0.225],\n            'num_classes': 1000\n        }\n    },\n}\n\n\nclass SEModule(nn.Module):\n\n    def __init__(self, channels, reduction):\n        super(SEModule, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc1 = nn.Conv2d(channels, channels // reduction, kernel_size=1,\n                             padding=0)\n        self.relu = nn.ReLU(inplace=True)\n        self.fc2 = nn.Conv2d(channels // reduction, channels, kernel_size=1,\n                             padding=0)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        module_input = x\n        x = self.avg_pool(x)\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.sigmoid(x)\n        return module_input * x\n\n\nclass Bottleneck(nn.Module):\n    \"\"\"\n    Base class for bottlenecks that implements `forward()` method.\n    \"\"\"\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out = self.se_module(out) + residual\n        out = self.relu(out)\n\n        return out\n\n\nclass SEBottleneck(Bottleneck):\n    \"\"\"\n    Bottleneck for SENet154.\n    \"\"\"\n    expansion = 4\n\n    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n                 downsample=None):\n        super(SEBottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes * 2, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes * 2)\n        self.conv2 = nn.Conv2d(planes * 2, planes * 4, kernel_size=3,\n                               stride=stride, padding=1, groups=groups,\n                               bias=False)\n        self.bn2 = nn.BatchNorm2d(planes * 4)\n        self.conv3 = nn.Conv2d(planes * 4, planes * 4, kernel_size=1,\n                               bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes * 4, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n\nclass SEResNetBottleneck(Bottleneck):\n    \"\"\"\n    ResNet bottleneck with a Squeeze-and-Excitation module. It follows Caffe\n    implementation and uses `stride=stride` in `conv1` and not in `conv2`\n    (the latter is used in the torchvision implementation of ResNet).\n    \"\"\"\n    expansion = 4\n\n    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n                 downsample=None):\n        super(SEResNetBottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False,\n                               stride=stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1,\n                               groups=groups, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes * 4, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n\nclass SEResNeXtBottleneck(Bottleneck):\n    \"\"\"\n    ResNeXt bottleneck type C with a Squeeze-and-Excitation module.\n    \"\"\"\n    expansion = 4\n\n    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n                 downsample=None, base_width=4):\n        super(SEResNeXtBottleneck, self).__init__()\n        width = math.floor(planes * (base_width / 64)) * groups\n        self.conv1 = nn.Conv2d(inplanes, width, kernel_size=1, bias=False,\n                               stride=1)\n        self.bn1 = nn.BatchNorm2d(width)\n        self.conv2 = nn.Conv2d(width, width, kernel_size=3, stride=stride,\n                               padding=1, groups=groups, bias=False)\n        self.bn2 = nn.BatchNorm2d(width)\n        self.conv3 = nn.Conv2d(width, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes * 4, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n\nclass SENet(nn.Module):\n\n    def __init__(self, block, layers, groups, reduction, dropout_p=0.2,\n                 inplanes=128, input_3x3=True, downsample_kernel_size=3,\n                 downsample_padding=1, num_classes=1000):\n        \"\"\"\n        Parameters\n        ----------\n        block (nn.Module): Bottleneck class.\n            - For SENet154: SEBottleneck\n            - For SE-ResNet models: SEResNetBottleneck\n            - For SE-ResNeXt models:  SEResNeXtBottleneck\n        layers (list of ints): Number of residual blocks for 4 layers of the\n            network (layer1...layer4).\n        groups (int): Number of groups for the 3x3 convolution in each\n            bottleneck block.\n            - For SENet154: 64\n            - For SE-ResNet models: 1\n            - For SE-ResNeXt models:  32\n        reduction (int): Reduction ratio for Squeeze-and-Excitation modules.\n            - For all models: 16\n        dropout_p (float or None): Drop probability for the Dropout layer.\n            If `None` the Dropout layer is not used.\n            - For SENet154: 0.2\n            - For SE-ResNet models: None\n            - For SE-ResNeXt models: None\n        inplanes (int):  Number of input channels for layer1.\n            - For SENet154: 128\n            - For SE-ResNet models: 64\n            - For SE-ResNeXt models: 64\n        input_3x3 (bool): If `True`, use three 3x3 convolutions instead of\n            a single 7x7 convolution in layer0.\n            - For SENet154: True\n            - For SE-ResNet models: False\n            - For SE-ResNeXt models: False\n        downsample_kernel_size (int): Kernel size for downsampling convolutions\n            in layer2, layer3 and layer4.\n            - For SENet154: 3\n            - For SE-ResNet models: 1\n            - For SE-ResNeXt models: 1\n        downsample_padding (int): Padding for downsampling convolutions in\n            layer2, layer3 and layer4.\n            - For SENet154: 1\n            - For SE-ResNet models: 0\n            - For SE-ResNeXt models: 0\n        num_classes (int): Number of outputs in `last_linear` layer.\n            - For all models: 1000\n        \"\"\"\n        super(SENet, self).__init__()\n        self.inplanes = inplanes\n        if input_3x3:\n            layer0_modules = [\n                ('conv1', nn.Conv2d(3, 64, 3, stride=2, padding=1,\n                                    bias=False)),\n                ('bn1', nn.BatchNorm2d(64)),\n                ('relu1', nn.ReLU(inplace=True)),\n                ('conv2', nn.Conv2d(64, 64, 3, stride=1, padding=1,\n                                    bias=False)),\n                ('bn2', nn.BatchNorm2d(64)),\n                ('relu2', nn.ReLU(inplace=True)),\n                ('conv3', nn.Conv2d(64, inplanes, 3, stride=1, padding=1,\n                                    bias=False)),\n                ('bn3', nn.BatchNorm2d(inplanes)),\n                ('relu3', nn.ReLU(inplace=True)),\n            ]\n        else:\n            layer0_modules = [\n                ('conv1', nn.Conv2d(3, inplanes, kernel_size=7, stride=2,\n                                    padding=3, bias=False)),\n                ('bn1', nn.BatchNorm2d(inplanes)),\n                ('relu1', nn.ReLU(inplace=True)),\n            ]\n        # To preserve compatibility with Caffe weights `ceil_mode=True`\n        # is used instead of `padding=1`.\n        layer0_modules.append(('pool', nn.MaxPool2d(3, stride=2,\n                                                    ceil_mode=True)))\n        self.layer0 = nn.Sequential(OrderedDict(layer0_modules))\n        self.layer1 = self._make_layer(\n            block,\n            planes=64,\n            blocks=layers[0],\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=1,\n            downsample_padding=0\n        )\n        self.layer2 = self._make_layer(\n            block,\n            planes=128,\n            blocks=layers[1],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding\n        )\n        self.layer3 = self._make_layer(\n            block,\n            planes=256,\n            blocks=layers[2],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding\n        )\n        self.layer4 = self._make_layer(\n            block,\n            planes=512,\n            blocks=layers[3],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding\n        )\n        # self.avg_pool = nn.AvgPool2d(7, stride=1)\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.dropout = nn.Dropout(dropout_p) if dropout_p is not None else None\n        self.last_linear = nn.Linear(512 * block.expansion, num_classes)\n\n    def _make_layer(self, block, planes, blocks, groups, reduction, stride=1,\n                    downsample_kernel_size=1, downsample_padding=0):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=downsample_kernel_size, stride=stride,\n                          padding=downsample_padding, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, groups, reduction, stride,\n                            downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, groups, reduction))\n\n        return nn.Sequential(*layers)\n\n    def features(self, x):\n        x = self.layer0(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        return x\n\n    def logits(self, x):\n        x = self.avg_pool(x)\n        if self.dropout is not None:\n            x = self.dropout(x)\n        x = x.view(x.size(0), -1)\n        x = self.last_linear(x)\n        return x\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.logits(x)\n        return x\n\n\ndef initialize_pretrained_model(model, num_classes, settings):\n    assert num_classes == settings['num_classes'], \\\n        'num_classes should be {}, but is {}'.format(\n            settings['num_classes'], num_classes)\n#     model.load_state_dict(model_zoo.load_url(settings['url']))\n    model.last_linear = nn.Linear(model.last_linear.in_features, 1103)\n    model.input_space = settings['input_space']\n    model.input_size = settings['input_size']\n    model.input_range = settings['input_range']\n    model.mean = settings['mean']\n    model.std = settings['std']\ndef senet154(num_classes=1000, pretrained='imagenet'):\n    model = SENet(SEBottleneck, [3, 8, 36, 3], groups=64, reduction=16,\n                  dropout_p=0.2, num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings['senet154'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\ndef se_resnext101_32x4d(num_classes=1000, pretrained='imagenet'):\n    model = SENet(SEResNeXtBottleneck, [3, 4, 23, 3], groups=32, reduction=16,\n                  dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings['se_resnext101_32x4d'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\ndef se_resnet152(num_classes=1000, pretrained='imagenet'):\n    model = SENet(SEResNetBottleneck, [3, 8, 36, 3], groups=1, reduction=16,\n                  dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings['se_resnet152'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\nclass SEResNet152(nn.Module):\n    def __init__(self, num_classes,\n                 pretrained=True, net_cls=M.resnet50, dropout=True):\n        super().__init__()\n        self.net = se_resnet152(num_classes=1000,pretrained='imagenet')\n        # self.net.avgpool = AvgPool()\n        if dropout:\n            self.net.last_linear = nn.Sequential(\n                nn.Dropout(0.2),\n                nn.Linear(self.net.last_linear.in_features, num_classes),\n            )\n        else:\n            self.net.last_linear = nn.Linear(self.net.last_linear.in_features, 1103)\n\n    def fresh_params(self):\n        return self.net.parameters()\n\n    def forward(self, x):\n        return self.net(x)\nclass SERNet101_32x4d(nn.Module):\n    def __init__(self, num_classes,\n                 pretrained=True, net_cls=M.resnet50, dropout=False):\n        super().__init__()\n        self.net = se_resnext101_32x4d(num_classes=1000,pretrained='imagenet')\n        # self.net.avgpool = AvgPool()\n        if dropout:\n            self.net.fc = nn.Sequential(\n                nn.Dropout(),\n                nn.Linear(self.net.fc.in_features, num_classes),\n            )\n        else:\n            self.net.last_linear = nn.Linear(self.net.last_linear.in_features, 1103)\n\n    def fresh_params(self):\n        return self.net.parameters()\n\n    def forward(self, x):\n        return self.net(x)\nclass AvgPool(nn.Module):\n    def forward(self, x):\n        return torch.nn.functional.avg_pool2d(x, x.shape[2:])\n\n\ndef create_net(net_cls, pretrained: bool):\n    if ON_KAGGLE and pretrained:\n        net = net_cls()\n        model_name = net_cls.__name__\n        weights_path = f'../input/{model_name}/{model_name}.pth'\n        net.load_state_dict(torch.load(weights_path))\n    else:\n        net = net_cls(pretrained=pretrained)\n    return net\nclass ResNet(nn.Module):\n    def __init__(self, num_classes,\n                 pretrained=True, net_cls=M.resnet152, dropout=False):\n        super().__init__()\n        self.net = create_net(net_cls, pretrained=False)\n        self.net.avgpool = AvgPool()\n        if dropout:\n            self.net.fc = nn.Sequential(\n                nn.Dropout(),\n                nn.Linear(self.net.fc.in_features, num_classes),\n            )\n        else:\n            self.net.fc = nn.Linear(self.net.fc.in_features, num_classes)\n            # self.net.fc = nn.Sequential(\n            #     nn.BatchNorm1d(self.net.fc.in_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True),\n            #     nn.Dropout(p=0.5),\n            #     nn.Linear(in_features=self.net.fc.in_features,out_features=1500,bias=True),\n            #     nn.ReLU(),\n            #     nn.BatchNorm1d(1500,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True),\n            #     nn.Dropout(p=0.5),\n            #     nn.Linear(in_features=1500,out_features=num_classes,bias=True)\n            # )\n\n    def fresh_params(self):\n        return self.net.parameters()\n\n    def forward(self, x):\n        return self.net(x)\ndef se_resnext50_32x4d(num_classes=1000, pretrained='imagenet'):\n    model = SENet(SEResNeXtBottleneck, [3, 4, 6, 3], groups=32, reduction=16,\n                  dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings['se_resnext50_32x4d'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n# class SERNext50_32x4d(nn.Module):\n#     def __init__(self, num_classes,\n#                  pretrained=True, net_cls=M.resnet50, dropout=False):\n#         super().__init__()\n#         self.net = se_resnext50_32x4d(num_classes=1000, pretrained='imagenet')\n#         # self.net.avgpool = AvgPool()\n#         self.net.last_linear = nn.Linear(self.net.last_linear.in_features, 1103)\n\n#     def fresh_params(self):\n#         return self.net.parameters()\n\n#     def forward(self, x):\n#         return self.net(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = []\n\nmodel = se_resnext101_32x4d()\nmodel.cuda()\nload_model(model,Path('../input/0518-serx101-fd1/0518-serx101-fd1.pt'))\nmodel.eval()\nmodels.append(model)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Dataset_met(Dataset):\n    def __init__(self, names, labels=None, mode='train', transform=None):\n        super(Dataset_met, self).__init__()\n        self.names = names\n        self.labels = labels\n        self.mode = mode\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.names)\n\n    def load_data(self, name, m='train'):\n        image=cv2.imread('../input/imet-2019-fgvc6/{}/{}.png'.format(m, name))\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        return image\n    def __getitem__(self, index):\n        if self.mode in ['train', 'valid']:\n            name = self.names[index]\n            label = self.labels[index]\n            image = self.load_data(name)\n            image = self.transform(image)\n            label_zero = np.zeros(NUM_CLASSES)\n            for l in label.split(' '):\n                label_zero[int(l)] = 1\n            return image, label_zero\n        else:\n            name = self.names[index].replace('.png', '')\n            image = self.load_data(name, self.mode)\n            image = self.transform(image)\n            return image, name","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NUM_TTA = 2\ndef test_collate(batch):\n    batch_size = len(batch)\n    images = []\n    names = []\n    for b in range(batch_size):\n        images.extend(batch[b][0])\n        names.append(batch[b][1])\n    images = torch.stack(images, 0)\n    return images, names\ntest_names = os.listdir('../input/imet-2019-fgvc6/test')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weights = [1]*1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dst_test = Dataset_met(test_names, mode='test', transform=transform_test)\ndataloader_test = DataLoader(dst_test, shuffle=False, batch_size=64, num_workers=2, collate_fn=test_collate)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with torch.no_grad():\n    all_names = []\n    all_outs = []\n    for data in tqdm(dataloader_test):\n        images, names = data\n        images = images.cuda()\n        outs = 0\n        for w, model in zip(weights, models):\n            out = model(images)\n            out = torch.sigmoid(out).data.cpu().numpy()\n            outs += out * w\n        outs /= sum(weights)\n        outs_zero = 0\n        for i in range(NUM_TTA):\n            outs_zero += outs[i::NUM_TTA]\n        outs = outs_zero/NUM_TTA\n  \n        for out, name in zip(outs, names):\n            out_indexs = np.nonzero(out > 0.105)[0]\n            str_out = ''\n            for o in out_indexs:\n                str_out += '{} '.format(int(o))\n            if len(out_indexs) == 0:\n                str_out = str(np.argmax(out))\n            else:\n                str_out = str_out[:-1]\n            print(str_out)\n            all_outs.append(str_out)\n            all_names.append(name)\n# id,attribute_ids\n    pd.DataFrame({'id':all_names, 'attribute_ids':all_outs}).to_csv(\n        'submission.csv', index=None)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}