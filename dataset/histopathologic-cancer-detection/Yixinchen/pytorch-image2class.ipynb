{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import TensorDataset,DataLoader,Dataset","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"## parameters for mmodel\n# Hyper parameters\nnum_epochs = 8\nnum_classes = 2\nbatch_size = 128\nlearning_rate = 0.02\n\n# Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('GPU or CPU:{}'.format(device))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load the data\nlabels = pd.read_csv('../input/histopathologic-cancer-detection/train_labels.csv')\nsub = pd.read_csv('../input/histopathologic-cancer-detection/sample_submission.csv')\ntrain_path = '../input/histopathologic-cancer-detection/train/'\ntest_path = '../input/histopathologic-cancer-detection/test/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# splitting data into train and val\ntrain, val = train_test_split(labels, stratify = labels.label, test_size = 0.1)\nlen(train),len(val)\nprint(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create data pipe\nclass MyDataset(Dataset):\n    def __init__(self,df_data,data_dir = './', transform=None):\n        super().__init__()\n        self.df = df_data.values\n        self.data_dir = data_dir\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.df)\n    \n    #index is relative with __len__, like \"for index in range(__len__)\"\n    def __getitem__(self,index):\n        img_name,label = self.df[index]\n        img_path = os.path.join(self.data_dir, img_name+'.tif')\n        image = cv2.imread(img_path)\n        if self.transform is not None:\n            image = self.transform(image)\n        return image ,label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define transform, which is preprocess of image\ntrans_train = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.Pad(64,padding_mode='reflect'),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomVerticalFlip(),\n    transforms.RandomRotation(20),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.5,0.5,0.5],\n        std =[0.5,0.5,0.5])    \n])\ntrans_valid = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.Pad(64,padding_mode='reflect'),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.5,0.5,0.5],\n        std =[0.5,0.5,0.5])\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_train = MyDataset(df_data=train,data_dir = train_path,transform = trans_train)\ndataset_valid = MyDataset(df_data=val,data_dir=train_path,transform=trans_valid)\n\nloader_train = DataLoader(dataset = dataset_train,batch_size = batch_size,shuffle=True, num_workers=0)\nloader_valid = DataLoader(dataset = dataset_valid,batch_size = batch_size//2,shuffle=False,num_workers=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define CNN model\nclass SimpleCNN(nn.Module):\n    def __init__(self):\n        super(SimpleCNN,self).__init__()\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32,\n                               kernel_size=3,padding=2)\n        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64,\n                               kernel_size=3,padding=2)\n        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128,\n                               kernel_size=3,padding=2)\n        self.conv4 = nn.Conv2d(in_channels=128, out_channels=256,\n                               kernel_size=3,padding=2)\n        self.conv5 = nn.Conv2d(in_channels=256, out_channels=512,\n                               kernel_size=3,padding=2)\n        self.bn1 = nn.BatchNorm2d(32)\n        self.bn2 = nn.BatchNorm2d(64)        \n        self.bn3 = nn.BatchNorm2d(128)        \n        self.bn4 = nn.BatchNorm2d(256)        \n        self.bn5 = nn.BatchNorm2d(512)        \n        self.pool = nn.MaxPool2d(kernel_size=2,stride=2)\n        self.avg = nn.AvgPool2d(8)\n        self.fc = nn.Linear(512*1*1,2) #!!! 2 classes, dense layer\n    \n    def forward(self,x):\n        #convolutional layer, then batchnorm then activation and pooling\n        x = self.pool(F.leaky_relu(self.bn1(self.conv1(x))))\n        x = self.pool(F.leaky_relu(self.bn2(self.conv2(x))))\n        x = self.pool(F.leaky_relu(self.bn3(self.conv3(x))))\n        x = self.pool(F.leaky_relu(self.bn4(self.conv4(x))))\n        x = self.pool(F.leaky_relu(self.bn5(self.conv5(x))))\n        x = self.avg(x)\n        x = x.view(-1,512*1*1)\n        x = self.fc(x)\n        return x\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = SimpleCNN().to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loss and optimizer\ncriterion = nn.BCELoss(reduction=\"mean\")\noptimizer = torch.optim.Adamax(model.parameters(), lr=learning_rate)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(outputs[0])\n# m = nn.Sigmoid()\n# x = m(outputs.view(256)).view(128,2)\n# print(x)\n\n# x = nn.Sigmoid(x)\n# print(x[0])\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train the model,total_step is the number of batch\ntotal_step = len(loader_train)\nm = nn.Sigmoid()\nfor epoch in range(num_epochs):\n    for i, (images,labels) in enumerate(loader_train):\n        images = images.to(device)\n#         labels = labels.to(device)\n        # one hot label\n        class_num = 2\n        batch_size = len(labels)\n        labels = labels.view(len(labels),1)\n        one_hot = torch.zeros(batch_size, class_num).scatter_(1, labels, 1)\n        one_hot = one_hot.to(device)\n        # Forward pass\n        outputs = model(images)\n        outputs = m(outputs.view(2*len(outputs))).view(len(outputs),2)\n        loss = criterion(outputs,one_hot)\n        \n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        if(i+1)%100 == 0:\n            print('Epoch[{}/{}],step[{}/{},Loss:{}'\n                  .format(epoch+1,num_epochs,i+1,total_step,loss.item()))\n            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test the model\n# model.eval()\n\nwith torch.no_grad():\n    correct = 0\n    total = 0\n    for images, labels in loader_valid:\n        images = images.to(device)\n        labels = labels.to(device)\n        outputs = model(images)\n        print(torch.max(outputs.data,1))\n        _, predicted = torch.max(outputs.data,1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n        \n    print('Test Accuracy of the model on the 22003 test images:{}%'\n         .format(100*correct/total))\n\n# save the model checkpoint\ntorch.save(model.state_dict(), 'model.ckpt')\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}