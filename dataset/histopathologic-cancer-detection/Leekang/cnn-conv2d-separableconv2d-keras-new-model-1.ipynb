{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nfrom glob import glob\nfrom random import shuffle\nimport cv2\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.image import ImageDataGenerator\n\nfrom keras import layers,Input\n\nfrom keras.losses import mae, sparse_categorical_crossentropy, binary_crossentropy\nfrom keras.models import Model\nfrom keras.applications.nasnet import  preprocess_input\nfrom keras.optimizers import Adam, RMSprop\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nfrom imgaug import augmenters as iaa\nimport imgaug as ia\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df_train = pd.read_csv(\"../input/histopathologic-cancer-detection/train_labels.csv\")\nid_label_map = {k:v for k,v in zip(df_train.id.values, df_train.label.values)}\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4c1169ec9a84704cff822b6e8ba90729d0ee383e"},"cell_type":"code","source":"def get_id_from_file_path(file_path):\n    return file_path.split(os.path.sep)[-1].replace('.tif', '')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4839c33e47619dfaf0f63d29bcf01ba50a96dfec"},"cell_type":"code","source":"labeled_files = glob('../input/histopathologic-cancer-detection/train/*.tif')\ntest_files = glob('../input/histopathologic-cancer-detection/test/*.tif')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4d3168a806b716023cef1d798b3ede847f7546ee"},"cell_type":"code","source":"print(\"labeled_files size :\", len(labeled_files))\nprint(\"test_files size :\", len(test_files))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"af19d597affefcbb1a14e5dd0d591bb3294efb61"},"cell_type":"code","source":"train, val = train_test_split(labeled_files, test_size=0.1, random_state=101010)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bd67e144f327114229b410e0af43b71f45ce0d72"},"cell_type":"code","source":"def chunker(seq, size):\n    return (seq[pos:pos + size] for pos in range(0, len(seq), size))\ndef get_seq():\n    sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n    seq = iaa.Sequential(\n        [\n            # apply the following augmenters to most images\n            iaa.Fliplr(0.5), # horizontally flip 50% of all images\n            iaa.Flipud(0.2), # vertically flip 20% of all images\n            sometimes(iaa.Affine(\n                scale={\"x\": (0.9, 1.1), \"y\": (0.9, 1.1)}, # scale images to 80-120% of their size, individually per axis\n                translate_percent={\"x\": (-0.1, 0.1), \"y\": (-0.1, 0.1)}, # translate by -20 to +20 percent (per axis)\n                rotate=(-10, 10), # rotate by -45 to +45 degrees\n                shear=(-5, 5), # shear by -16 to +16 degrees\n                order=[0, 1], # use nearest neighbour or bilinear interpolation (fast)\n                cval=(0, 255), # if mode is constant, use a cval between 0 and 255\n                mode=ia.ALL # use any of scikit-image's warping modes (see 2nd image from the top for examples)\n            )),\n            # execute 0 to 5 of the following (less important) augmenters per image\n            # don't execute all of them, as that would often be way too strong\n            iaa.SomeOf((0, 5),\n                [\n                    sometimes(iaa.Superpixels(p_replace=(0, 1.0), n_segments=(20, 200))), # convert images into their superpixel representation\n                    iaa.OneOf([\n                        iaa.GaussianBlur((0, 1.0)), # blur images with a sigma between 0 and 3.0\n                        iaa.AverageBlur(k=(3, 5)), # blur image using local means with kernel sizes between 2 and 7\n                        iaa.MedianBlur(k=(3, 5)), # blur image using local medians with kernel sizes between 2 and 7\n                    ]),\n                    iaa.Sharpen(alpha=(0, 1.0), lightness=(0.9, 1.1)), # sharpen images\n                    iaa.Emboss(alpha=(0, 1.0), strength=(0, 2.0)), # emboss images\n                    # search either for all edges or for directed edges,\n                    # blend the result with the original image using a blobby mask\n                    iaa.SimplexNoiseAlpha(iaa.OneOf([\n                        iaa.EdgeDetect(alpha=(0.5, 1.0)),\n                        iaa.DirectedEdgeDetect(alpha=(0.5, 1.0), direction=(0.0, 1.0)),\n                    ])),\n                    iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.01*255), per_channel=0.5), # add gaussian noise to images\n                    iaa.OneOf([\n                        iaa.Dropout((0.01, 0.05), per_channel=0.5), # randomly remove up to 10% of the pixels\n                        iaa.CoarseDropout((0.01, 0.03), size_percent=(0.01, 0.02), per_channel=0.2),\n                    ]),\n                    iaa.Invert(0.01, per_channel=True), # invert color channels\n                    iaa.Add((-2, 2), per_channel=0.5), # change brightness of images (by -10 to 10 of original value)\n                    iaa.AddToHueAndSaturation((-1, 1)), # change hue and saturation\n                    # either change the brightness of the whole image (sometimes\n                    # per channel) or change the brightness of subareas\n                    iaa.OneOf([\n                        iaa.Multiply((0.9, 1.1), per_channel=0.5),\n                        iaa.FrequencyNoiseAlpha(\n                            exponent=(-1, 0),\n                            first=iaa.Multiply((0.9, 1.1), per_channel=True),\n                            second=iaa.ContrastNormalization((0.9, 1.1))\n                        )\n                    ]),\n                    sometimes(iaa.ElasticTransformation(alpha=(0.5, 3.5), sigma=0.25)), # move pixels locally around (with random strengths)\n                    sometimes(iaa.PiecewiseAffine(scale=(0.01, 0.05))), # sometimes move parts of the image around\n                    sometimes(iaa.PerspectiveTransform(scale=(0.01, 0.1)))\n                ],\n                random_order=True\n            )\n        ],\n        random_order=True\n    )\n    return seq\n\ndef data_gen(list_files, id_label_map, batch_size, augment=False):\n    seq = get_seq()\n    while True:\n        shuffle(list_files)\n        for batch in chunker(list_files, batch_size):\n            X = [cv2.imread(x) for x in batch]\n            Y = [id_label_map[get_id_from_file_path(x)] for x in batch]\n            if augment:\n                X = seq.augment_images(X)\n            X = [preprocess_input(x) for x in X]\n                \n            yield np.array(X), np.array(Y)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ad80bc3b50a313d8fd09637cc40c390e0042c765"},"cell_type":"code","source":"def get_model_classif_nasnet():  \n    # epoch = 20 ---- 95.87%\n    inputs = Input((96, 96, 3))\n\n    x1 = layers.Conv2D(32,3,padding='same')(inputs)\n    x1 = layers.BatchNormalization()(x1)\n    x1 = layers.Activation('relu')(x1)\n    x1 = layers.Conv2D(32,3,padding='same')(x1)\n    x1 = layers.BatchNormalization()(x1)\n    x1 = layers.Activation('relu')(x1)\n    x1 = layers.Conv2D(32,3,padding='same')(x1)\n    x1 = layers.BatchNormalization()(x1)\n    x1 = layers.Activation('relu')(x1)\n    x1 = layers.MaxPool2D(2,2)(x1)  \n\n\n    x2 = layers.Conv2D(64,3,padding='same')(x1)\n    x2 = layers.BatchNormalization()(x2)\n    x2 = layers.Activation('relu')(x2)\n    x2 = layers.Conv2D(64,3,padding='same')(x2)\n    x2 = layers.BatchNormalization()(x2)\n    x2 = layers.Activation('relu')(x2)\n    x2 = layers.Conv2D(64,3,padding='same')(x2)\n    x2 = layers.BatchNormalization()(x2)\n    x2 = layers.Activation('relu')(x2)\n    residual_x1 = layers.Conv2D(64,1,strides=1,padding='same')(x1)\n    x2 = layers.add([x2,residual_x1])\n    x2 = layers.MaxPool2D(2,2)(x2)\n    \n    \n    x2_s = layers.SeparableConv2D(64,3,padding='same')(x2)\n    x2_s = layers.BatchNormalization()(x2_s)\n    x2_s = layers.Activation('relu')(x2_s)\n    x2_s = layers.SeparableConv2D(64,3,padding='same')(x2_s)\n    x2_s= layers.BatchNormalization()(x2_s)\n    x2_s= layers.Activation('relu')(x2_s)\n    x2_s = layers.SeparableConv2D(64,3,padding='same')(x2_s)\n    x2_s= layers.BatchNormalization()(x2_s)\n    x2_s= layers.Activation('relu')(x2_s)\n    x2_s = layers.add([x2_s,x2]) \n    x2_s = layers.MaxPool2D(2,2)(x2_s)\n    \n    x3 = layers.Conv2D(128,3,padding='same')(x2)\n    x3 = layers.BatchNormalization()(x3)\n    x3 = layers.Activation('relu')(x3)\n    x3 = layers.Conv2D(128,3,padding='same')(x3)\n    x3 = layers.BatchNormalization()(x3)\n    x3 = layers.Activation('relu')(x3)\n    x3 = layers.Conv2D(128,3,padding='same')(x3)\n    x3 = layers.BatchNormalization()(x3)\n    x3 = layers.Activation('relu')(x3)\n    residual_x2 = layers.Conv2D(128,1,strides=1,padding='same')(x2)\n    x3 = layers.add([residual_x2,x3]) \n    \n    x3_x3 = layers.Conv2D(128,3,padding='same')(x3)\n    x3_x3 = layers.BatchNormalization()(x3_x3)\n    x3_x3 = layers.Activation('relu')(x3_x3)\n    x3_x3 = layers.Conv2D(128,3,padding='same')(x3_x3)\n    x3_x3 = layers.BatchNormalization()(x3_x3)\n    x3_x3 = layers.Activation('relu')(x3_x3)\n    x3_x3 = layers.Conv2D(128,3,padding='same')(x3_x3)\n    x3_x3 = layers.BatchNormalization()(x3_x3)\n    x3_x3 = layers.Activation('relu')(x3_x3)\n    x3_x3 = layers.add([x3,x3_x3]) \n    x3_x3 = layers.MaxPool2D(2,2)(x3_x3)\n    \n    \n    concetenated_1 = layers.concatenate([x3_x3,x2_s])\n    x3_s = layers.SeparableConv2D(128,3,padding='same')(concetenated_1)\n    x3_s = layers.BatchNormalization()(x3_s)\n    x3_s = layers.Activation('relu')(x3_s)\n    x3_s = layers.SeparableConv2D(128,3,padding='same')(x3_s)\n    x3_s= layers.BatchNormalization()(x3_s)\n    x3_s= layers.Activation('relu')(x3_s)\n    x3_s = layers.SeparableConv2D(128,3,padding='same')(x3_s)\n    x3_s= layers.BatchNormalization()(x3_s)\n    x3_s= layers.Activation('relu')(x3_s)\n    x3_s = layers.add([x3_s,x3_x3]) \n    x3_s = layers.MaxPool2D(2,2)(x3_s)\n    \n    x4 = layers.Conv2D(256,3,padding='same')(x3_x3)\n    x4 = layers.BatchNormalization()(x4)\n    x4 = layers.Activation('relu')(x4)\n    x4 = layers.Conv2D(256,3,padding='same')(x4)\n    x4 = layers.BatchNormalization()(x4)\n    x4 = layers.Activation('relu')(x4)\n    x4 = layers.Conv2D(256,3,padding='same')(x4)\n    x4 = layers.BatchNormalization()(x4)\n    x4 = layers.Activation('relu')(x4)\n    residual_x3 = layers.Conv2D(256,1,strides=1,padding='same')(x3_x3)\n    x4 = layers.add([residual_x3,x4]) \n    \n    x4_x4 = layers.Conv2D(256,3,padding='same')(x4)\n    x4_x4 = layers.BatchNormalization()(x4_x4)\n    x4_x4 = layers.Activation('relu')(x4_x4)\n    x4_x4 = layers.Conv2D(256,3,padding='same')(x4_x4)\n    x4_x4 = layers.BatchNormalization()(x4_x4)\n    x4_x4 = layers.Activation('relu')(x4_x4)\n    x4_x4 = layers.Conv2D(256,3,padding='same')(x4_x4)\n    x4_x4 = layers.BatchNormalization()(x4_x4)\n    x4_x4 = layers.Activation('relu')(x4_x4)\n    x4_x4 = layers.add([x4,x4_x4]) \n    x4_x4 = layers.MaxPool2D(2,2)(x4_x4)\n    \n\n    concetenated_2 = layers.concatenate([x4_x4,x3_s])\n    x4_s = layers.SeparableConv2D(256,3,padding='same')(concetenated_2)\n    x4_s = layers.BatchNormalization()(x4_s)\n    x4_s = layers.Activation('relu')(x4_s)\n    x4_s = layers.SeparableConv2D(256,3,padding='same')(x4_s)\n    x4_s= layers.BatchNormalization()(x4_s)\n    x4_s= layers.Activation('relu')(x4_s)\n    x4_s = layers.SeparableConv2D(256,3,padding='same')(x4_s)\n    x4_s= layers.BatchNormalization()(x4_s)\n    x4_s= layers.Activation('relu')(x4_s)\n    x4_s = layers.add([x4_s,x4_x4]) \n    x4_s = layers.MaxPool2D(2,2)(x4_s)\n    \n    x5 = layers.Conv2D(512,3,padding='same')(x4_x4)\n    x5 = layers.BatchNormalization()(x5)\n    x5 = layers.Activation('relu')(x5)\n    x5 = layers.Conv2D(512,3,padding='same')(x5)\n    x5 = layers.BatchNormalization()(x5)\n    x5 = layers.Activation('relu')(x5)\n    x5 = layers.Conv2D(512,3,padding='same')(x5)\n    x5 = layers.BatchNormalization()(x5)\n    x5 = layers.Activation('relu')(x5)\n    residual_x4 = layers.Conv2D(512,1,strides=1,padding='same')(x4_x4)\n    x5 = layers.add([residual_x4,x5])\n\n    x5_x5 = layers.Conv2D(512,3,padding='same')(x5)\n    x5_x5 = layers.BatchNormalization()(x5_x5)\n    x5_x5 = layers.Activation('relu')(x5_x5)\n    x5_x5 = layers.Conv2D(512,3,padding='same')(x5_x5)\n    x5_x5 = layers.BatchNormalization()(x5_x5)\n    x5_x5 = layers.Activation('relu')(x5_x5)\n    x5_x5 = layers.Conv2D(512,3,padding='same')(x5_x5)\n    x5_x5 = layers.BatchNormalization()(x5_x5)\n    x5_x5 = layers.Activation('relu')(x5_x5)\n    x5_x5 = layers.add([x5,x5_x5])\n    x5_x5 = layers.MaxPool2D(2,2)(x5_x5)\n    \n    concetenated_3 = layers.concatenate([x5_x5,x4_s])\n    x5_s = layers.SeparableConv2D(512,3,padding='same')(concetenated_3)\n    x5_s = layers.BatchNormalization()(x5_s)\n    x5_s = layers.Activation('relu')(x5_s)\n    x5_s = layers.SeparableConv2D(512,3,padding='same')(x5_s)\n    x5_s= layers.BatchNormalization()(x5_s)\n    x5_s= layers.Activation('relu')(x5_s)\n    x5_s = layers.SeparableConv2D(512,3,padding='same')(x5_s)\n    x5_s= layers.BatchNormalization()(x5_s)\n    x5_s= layers.Activation('relu')(x5_s)\n    x5_s = layers.add([x5_s,x5_x5]) \n\n    x = layers.GlobalAveragePooling2D()(x5_s)\n\n    x = layers.Dense(64)(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation('relu')(x)\n    x = layers.Dropout(0.2)(x)\n    \n    output_tensor = layers.Dense(1,activation='sigmoid')(x)\n\n    model = Model(inputs,output_tensor)\n    \n    model.compile(optimizer=Adam(0.01), loss=binary_crossentropy, metrics=['acc'])\n    model.summary()\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_model_classif_nasnet_1():  \n    \n    #epoch = 15 --- 96.31%\n    \n    inputs = Input((96, 96, 3))\n\n    x1 = layers.Conv2D(32,3,padding='same')(inputs)\n    x1 = layers.BatchNormalization()(x1)\n    x1 = layers.Activation('relu')(x1)\n    x1 = layers.Conv2D(32,3,padding='same')(x1)\n    x1 = layers.BatchNormalization()(x1)\n    x1 = layers.Activation('relu')(x1)\n    x1 = layers.Conv2D(32,3,padding='same')(x1)\n    x1 = layers.BatchNormalization()(x1)\n    x1 = layers.Activation('relu')(x1)\n#     x1 = layers.MaxPool2D(2,2)(x1)  \n\n    \n    x1_s = layers.SeparableConv2D(32,3,padding='same')(inputs)\n    x1_s = layers.BatchNormalization()(x1_s)\n    x1_s = layers.Activation('relu')(x1_s)\n    x1_s = layers.SeparableConv2D(32,3,padding='same')(x1_s)\n    x1_s = layers.BatchNormalization()(x1_s)\n    x1_s = layers.Activation('relu')(x1_s)    \n    x1_s = layers.SeparableConv2D(32,3,padding='same')(x1_s)\n    x1_s = layers.BatchNormalization()(x1_s)\n    x1_s = layers.Activation('relu')(x1_s)\n    concetenated_0 = layers.concatenate([x1,x1_s])\n\n    x2 = layers.Conv2D(64,3,padding='same')(concetenated_0)\n    x2 = layers.BatchNormalization()(x2)\n    x2 = layers.Activation('relu')(x2)\n    x2 = layers.Conv2D(64,3,padding='same')(x2)\n    x2 = layers.BatchNormalization()(x2)\n    x2 = layers.Activation('relu')(x2)\n    x2 = layers.Conv2D(64,3,padding='same')(x2)\n    x2 = layers.BatchNormalization()(x2)\n    x2 = layers.Activation('relu')(x2)\n    residual_concetenated_0 = layers.Conv2D(64,1,strides=1,padding='same')(concetenated_0)\n    x2 = layers.add([x2,residual_concetenated_0])\n    concetenates_x2_x1_s = layers.concatenate([x2,x1_s])\n    x2 = layers.MaxPool2D(2,2)(concetenates_x2_x1_s)\n    \n    \n\n    x2_s = layers.SeparableConv2D(64,3,padding='same')(x2)\n    x2_s = layers.BatchNormalization()(x2_s)\n    x2_s = layers.Activation('relu')(x2_s)\n    x2_s = layers.SeparableConv2D(64,3,padding='same')(x2_s)\n    x2_s= layers.BatchNormalization()(x2_s)\n    x2_s= layers.Activation('relu')(x2_s)\n    x2_s = layers.SeparableConv2D(64,3,padding='same')(x2_s)\n    x2_s= layers.BatchNormalization()(x2_s)\n    x2_s= layers.Activation('relu')(x2_s)\n    x2_s = layers.Conv2D(96,1,strides=1,padding='same')(x2_s)\n    x2_s = layers.add([x2_s,x2]) \n    x2_s = layers.MaxPool2D(2,2)(x2_s)\n    \n    x3 = layers.Conv2D(128,3,padding='same')(x2)\n    x3 = layers.BatchNormalization()(x3)\n    x3 = layers.Activation('relu')(x3)\n    x3 = layers.Conv2D(128,3,padding='same')(x3)\n    x3 = layers.BatchNormalization()(x3)\n    x3 = layers.Activation('relu')(x3)\n    x3 = layers.Conv2D(128,3,padding='same')(x3)\n    x3 = layers.BatchNormalization()(x3)\n    x3 = layers.Activation('relu')(x3)\n    residual_x2 = layers.Conv2D(128,1,strides=1,padding='same')(x2)\n    x3 = layers.add([residual_x2,x3]) \n    \n    x3_x3 = layers.Conv2D(128,3,padding='same')(x3)\n    x3_x3 = layers.BatchNormalization()(x3_x3)\n    x3_x3 = layers.Activation('relu')(x3_x3)\n    x3_x3 = layers.Conv2D(128,3,padding='same')(x3_x3)\n    x3_x3 = layers.BatchNormalization()(x3_x3)\n    x3_x3 = layers.Activation('relu')(x3_x3)\n    x3_x3 = layers.Conv2D(128,3,padding='same')(x3_x3)\n    x3_x3 = layers.BatchNormalization()(x3_x3)\n    x3_x3 = layers.Activation('relu')(x3_x3)\n    x3_x3 = layers.add([x3,x3_x3]) \n    x3_x3 = layers.MaxPool2D(2,2)(x3_x3)\n    \n    \n    concetenated_1 = layers.concatenate([x3_x3,x2_s])\n    x3_s = layers.SeparableConv2D(128,3,padding='same')(concetenated_1)\n    x3_s = layers.BatchNormalization()(x3_s)\n    x3_s = layers.Activation('relu')(x3_s)\n    x3_s = layers.SeparableConv2D(128,3,padding='same')(x3_s)\n    x3_s= layers.BatchNormalization()(x3_s)\n    x3_s= layers.Activation('relu')(x3_s)\n    x3_s = layers.SeparableConv2D(128,3,padding='same')(x3_s)\n    x3_s= layers.BatchNormalization()(x3_s)\n    x3_s= layers.Activation('relu')(x3_s)\n    x3_s = layers.add([x3_s,x3_x3]) \n    x3_s = layers.MaxPool2D(2,2)(x3_s)\n    \n    x4 = layers.Conv2D(256,3,padding='same')(x3_x3)\n    x4 = layers.BatchNormalization()(x4)\n    x4 = layers.Activation('relu')(x4)\n    x4 = layers.Conv2D(256,3,padding='same')(x4)\n    x4 = layers.BatchNormalization()(x4)\n    x4 = layers.Activation('relu')(x4)\n    x4 = layers.Conv2D(256,3,padding='same')(x4)\n    x4 = layers.BatchNormalization()(x4)\n    x4 = layers.Activation('relu')(x4)\n    residual_x3 = layers.Conv2D(256,1,strides=1,padding='same')(x3_x3)\n    x4 = layers.add([residual_x3,x4]) \n    \n    x4_x4 = layers.Conv2D(256,3,padding='same')(x4)\n    x4_x4 = layers.BatchNormalization()(x4_x4)\n    x4_x4 = layers.Activation('relu')(x4_x4)\n    x4_x4 = layers.Conv2D(256,3,padding='same')(x4_x4)\n    x4_x4 = layers.BatchNormalization()(x4_x4)\n    x4_x4 = layers.Activation('relu')(x4_x4)\n    x4_x4 = layers.Conv2D(256,3,padding='same')(x4_x4)\n    x4_x4 = layers.BatchNormalization()(x4_x4)\n    x4_x4 = layers.Activation('relu')(x4_x4)\n    x4_x4 = layers.add([x4,x4_x4]) \n    x4_x4 = layers.MaxPool2D(2,2)(x4_x4)\n    \n\n    concetenated_2 = layers.concatenate([x4_x4,x3_s])\n    x4_s = layers.SeparableConv2D(256,3,padding='same')(concetenated_2)\n    x4_s = layers.BatchNormalization()(x4_s)\n    x4_s = layers.Activation('relu')(x4_s)\n    x4_s = layers.SeparableConv2D(256,3,padding='same')(x4_s)\n    x4_s= layers.BatchNormalization()(x4_s)\n    x4_s= layers.Activation('relu')(x4_s)\n    x4_s = layers.SeparableConv2D(256,3,padding='same')(x4_s)\n    x4_s= layers.BatchNormalization()(x4_s)\n    x4_s= layers.Activation('relu')(x4_s)\n    x4_s = layers.add([x4_s,x4_x4]) \n    x4_s = layers.MaxPool2D(2,2)(x4_s)\n    \n    x5 = layers.Conv2D(512,3,padding='same')(x4_x4)\n    x5 = layers.BatchNormalization()(x5)\n    x5 = layers.Activation('relu')(x5)\n    x5 = layers.Conv2D(512,3,padding='same')(x5)\n    x5 = layers.BatchNormalization()(x5)\n    x5 = layers.Activation('relu')(x5)\n    x5 = layers.Conv2D(512,3,padding='same')(x5)\n    x5 = layers.BatchNormalization()(x5)\n    x5 = layers.Activation('relu')(x5)\n    residual_x4 = layers.Conv2D(512,1,strides=1,padding='same')(x4_x4)\n    x5 = layers.add([residual_x4,x5])\n\n    x5_x5 = layers.Conv2D(512,3,padding='same')(x5)\n    x5_x5 = layers.BatchNormalization()(x5_x5)\n    x5_x5 = layers.Activation('relu')(x5_x5)\n    x5_x5 = layers.Conv2D(512,3,padding='same')(x5_x5)\n    x5_x5 = layers.BatchNormalization()(x5_x5)\n    x5_x5 = layers.Activation('relu')(x5_x5)\n    x5_x5 = layers.Conv2D(512,3,padding='same')(x5_x5)\n    x5_x5 = layers.BatchNormalization()(x5_x5)\n    x5_x5 = layers.Activation('relu')(x5_x5)\n    x5_x5 = layers.add([x5,x5_x5])\n    x5_x5 = layers.MaxPool2D(2,2)(x5_x5)\n    \n    concetenated_3 = layers.concatenate([x5_x5,x4_s])\n    x5_s = layers.SeparableConv2D(512,3,padding='same')(concetenated_3)\n    x5_s = layers.BatchNormalization()(x5_s)\n    x5_s = layers.Activation('relu')(x5_s)\n    x5_s = layers.SeparableConv2D(512,3,padding='same')(x5_s)\n    x5_s= layers.BatchNormalization()(x5_s)\n    x5_s= layers.Activation('relu')(x5_s)\n    x5_s = layers.SeparableConv2D(512,3,padding='same')(x5_s)\n    x5_s= layers.BatchNormalization()(x5_s)\n    x5_s= layers.Activation('relu')(x5_s)\n    x5_s = layers.add([x5_s,x5_x5]) \n\n    x = layers.GlobalAveragePooling2D()(x5_s)\n\n    x = layers.Dense(64)(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation('relu')(x)\n    x = layers.Dropout(0.3)(x)\n    \n    output_tensor = layers.Dense(1,activation='sigmoid')(x)\n\n    model = Model(inputs,output_tensor)\n    \n    model.compile(optimizer=Adam(0.001), loss='binary_crossentropy', metrics=['acc'])\n    model.summary()\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0872139964e7b1e7a25b327941b68fc3f90daa46"},"cell_type":"code","source":"model = get_model_classif_nasnet_1()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"db0b06f165a011723aef4266bf1be50b977268c5"},"cell_type":"code","source":"## first round\n\n# batch_size=128\n# h5_path = \"model_2.h5\"\n# checkpoint = ModelCheckpoint(h5_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n\n# history = model.fit_generator(\n#     data_gen(train, id_label_map, batch_size, augment=True),\n#     validation_data=data_gen(val, id_label_map, batch_size),\n#     epochs=15, verbose=1,\n#     callbacks=[checkpoint],\n#     steps_per_epoch=len(train) // batch_size,\n#     validation_steps=len(val) // batch_size)\n\n# model.load_weights(h5_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir('../input/')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # second round \n\n# model.load_weights('../input/wwwkagglecomleekingmodel-1h5/model_1.h5')\n# batch_size=128\n# h5_path = \"model_2.h5\"\n# checkpoint = ModelCheckpoint(h5_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n\n\n# history = model.fit_generator(\n#     data_gen(train, id_label_map, batch_size, augment=True),\n#     validation_data=data_gen(val, id_label_map, batch_size),\n#     epochs=15, verbose=1,\n#     callbacks=[checkpoint],\n#     steps_per_epoch=len(train) // batch_size,\n#     validation_steps=len(val) // batch_size)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Third round\n\nmodel.load_weights('../input/model-3/model_3.h5')\nbatch_size=128\nh5_path = \"model_4.h5\"\ncheckpoint = ModelCheckpoint(h5_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n\n\nhistory = model.fit_generator(\n    data_gen(train, id_label_map, batch_size, augment=True),\n    validation_data=data_gen(val, id_label_map, batch_size),\n    epochs=5, verbose=1,\n    callbacks=[checkpoint],\n    steps_per_epoch=len(train) // batch_size,\n    validation_steps=len(val) // batch_size)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1,len(acc)+1)\n\nplt.plot(epochs,acc,'bo',label='Train acc')\nplt.plot(epochs,val_acc,'b',label='Validation acc')\nplt.title('Training and validatoin accuracy')\nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs,loss,'bo',label='Train loss')\nplt.plot(epochs,val_loss,'b',label='Validation loss')\nplt.title('Training and validatoin loss')\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d399c51b099a414ef83e9b45a8bafab7206b1c24"},"cell_type":"code","source":"preds = []\nids = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"787cec513417ebb2c703d48b0ba97e1c4344c8d7"},"cell_type":"code","source":"model.load_weights(h5_path)\nfor batch in chunker(test_files, batch_size):\n    X = [preprocess_input(cv2.imread(x)) for x in batch]\n    ids_batch = [get_id_from_file_path(x) for x in batch]\n    X = np.array(X)\n    preds_batch = ((model.predict(X).ravel()*model.predict(X[:, ::-1, :, :]).ravel()*model.predict(X[:, ::-1, ::-1, :]).ravel()*model.predict(X[:, :, ::-1, :]).ravel())**0.25).tolist()\n    preds += preds_batch\n    ids += ids_batch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1b531bfe1d28a7bae83b74d9f857ae0f7029fdd2"},"cell_type":"code","source":"df = pd.DataFrame({'id':ids, 'label':preds})\ndf.to_csv(\"baseline_nasnet.csv\", index=False)\ndf.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}