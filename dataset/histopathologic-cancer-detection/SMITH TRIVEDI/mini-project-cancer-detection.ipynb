{"cells":[{"metadata":{"_uuid":"21b70685-cc36-4128-a3f8-802b205286a7","_cell_guid":"64d51ed1-f376-4a75-a938-304b888b9f8e","trusted":true},"cell_type":"code","source":"\n# **Image file descriptors**\n# \n# Description | \n# :--------:|:-------:\n# Format | TIF\n# Size | 96 x 96\n# Channels | 3\n# Bits per channel | 8\n# Data type | Unsigned \n\n# # Data visualization\n\nimport numpy as np\nimport pandas as pd\nimport os\nimport cv2\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport random\nfrom sklearn.utils import shuffle\nfrom tqdm import tqdm_notebook\n\ndata = pd.read_csv('/kaggle/input/train_labels.csv')\ntrain_path = '/kaggle/input/train/'\ntest_path = '/kaggle/input/test/'\n# label statistics\ndata['label'].value_counts()\n\n\n# Plot some images with and without cancer tissue for comparison\ndef readImage(path):\n    # OpenCV reads the image in bgr format by default\n    bgr_img = cv2.imread(path)\n    # We flip it to rgb for visualization purposes\n    b,g,r = cv2.split(bgr_img)\n    rgb_img = cv2.merge([r,g,b])\n    return rgb_img\n\n# random sampling\nshuffled_data = shuffle(data)\n\nfig, ax = plt.subplots(2,5, figsize=(20,8))\nfig.suptitle('Histopathologic scans of lymph node sections',fontsize=20)\n# Negatives\nfor i, idx in enumerate(shuffled_data[shuffled_data['label'] == 0]['id'][:5]):\n    path = os.path.join(train_path, idx)\n    ax[0,i].imshow(readImage(path + '.tif'))\n    # Create a Rectangle patch\n    box = patches.Rectangle((32,32),32,32,linewidth=4,edgecolor='b',facecolor='none', linestyle=':', capstyle='round')\n    ax[0,i].add_patch(box)\nax[0,0].set_ylabel('Negative samples', size='large')\n# Positives\nfor i, idx in enumerate(shuffled_data[shuffled_data['label'] == 1]['id'][:5]):\n    path = os.path.join(train_path, idx)\n    ax[1,i].imshow(readImage(path + '.tif'))\n    # Create a Rectangle patch\n    box = patches.Rectangle((32,32),32,32,linewidth=4,edgecolor='r',facecolor='none', linestyle=':', capstyle='round')\n    ax[1,i].add_patch(box)\nax[1,0].set_ylabel('Tumor tissue samples', size='large')\n\n\n# \n# label of the image is influenced only by the center region (32 x 32px) \n#crop  data to that region only. \n\n# ### Preprocessing and augmentation\n# The augmentations used for this data:\n# - random rotation\n# - random crop\n# - random flip (horizontal and vertical both)\n# - random lighting\n# We use OpenCV with image operations\n\nimport random\nORIGINAL_SIZE = 96      # original size of the images\n\n# AUGMENTATION VARIABLES\nCROP_SIZE = 90          # final size after crop\nRANDOM_ROTATION = 3    # range (0-180), 180 allows all rotation variations, 0=no change\nRANDOM_SHIFT = 2        # center crop shift in x and y axes, 0=no change. This cannot be more than (ORIGINAL_SIZE - CROP_SIZE)//2 \nRANDOM_BRIGHTNESS = 7  # range (0-100), 0=no change\nRANDOM_CONTRAST = 5    # range (0-100), 0=no change\nRANDOM_90_DEG_TURN = 1  # 0 or 1= random turn to left or right\n\ndef readCroppedImage(path, augmentations = True):\n    \n    # OpenCV reads the image in bgr \n    bgr_img = cv2.imread(path)\n    # We flip it to rgb for visualization purposes\n    b,g,r = cv2.split(bgr_img)\n    rgb_img = cv2.merge([r,g,b])\n    \n    if(not augmentations):\n        return rgb_img / 255\n    \n    #random rotation\n    rotation = random.randint(-RANDOM_ROTATION,RANDOM_ROTATION)\n    if(RANDOM_90_DEG_TURN == 1):\n        rotation += random.randint(-1,1) * 90\n    M = cv2.getRotationMatrix2D((48,48),rotation,1)   # the center point is the rotation anchor\n    rgb_img = cv2.warpAffine(rgb_img,M,(96,96))\n    \n    #random x,y-shift\n    x = random.randint(-RANDOM_SHIFT, RANDOM_SHIFT)\n    y = random.randint(-RANDOM_SHIFT, RANDOM_SHIFT)\n    \n    # crop to center and normalize to 0-1 range\n    start_crop = (ORIGINAL_SIZE - CROP_SIZE) // 2\n    end_crop = start_crop + CROP_SIZE\n    rgb_img = rgb_img[(start_crop + x):(end_crop + x), (start_crop + y):(end_crop + y)] / 255\n    \n    # Random flip\n    flip_hor = bool(random.getrandbits(1))\n    flip_ver = bool(random.getrandbits(1))\n    if(flip_hor):\n        rgb_img = rgb_img[:, ::-1]\n    if(flip_ver):\n        rgb_img = rgb_img[::-1, :]\n        \n    # Random brightness\n    br = random.randint(-RANDOM_BRIGHTNESS, RANDOM_BRIGHTNESS) / 100.\n    rgb_img = rgb_img + br\n    \n    # Random contrast\n    cr = 1.0 + random.randint(-RANDOM_CONTRAST, RANDOM_CONTRAST) / 100.\n    rgb_img = rgb_img * cr\n    \n    # clip values to 0-1 range\n    rgb_img = np.clip(rgb_img, 0, 1.0)\n    \n    return rgb_img\n\n\nfig, ax = plt.subplots(2,5, figsize=(20,8))\nfig.suptitle('Cropped histopathologic scans of lymph node sections',fontsize=20)\n# Negatives\nfor i, idx in enumerate(shuffled_data[shuffled_data['label'] == 0]['id'][:5]):\n    path = os.path.join(train_path, idx)\n    ax[0,i].imshow(readCroppedImage(path + '.tif'))\nax[0,0].set_ylabel('Negative samples', size='large')\n# Positives\nfor i, idx in enumerate(shuffled_data[shuffled_data['label'] == 1]['id'][:5]):\n    path = os.path.join(train_path, idx)\n    ax[1,i].imshow(readCroppedImage(path + '.tif'))\nax[1,0].set_ylabel('Tumor tissue samples', size='large')\n\n\n# **To see the effects of our augmentation, we plot one image multiple times.\n\nfig, ax = plt.subplots(1,5, figsize=(20,4))\nfig.suptitle('Random augmentations to the same image',fontsize=20)\n# Negatives\nfor i, idx in enumerate(shuffled_data[shuffled_data['label'] == 0]['id'][:1]):\n    for j in range(5):\n        path = os.path.join(train_path, idx)\n        ax[j].imshow(readCroppedImage(path + '.tif'))\n\n\n\n\n# Baseline model (Fastai v1)\n#min viable model (the simplest accepted model)\n\n# Prepare the data and split train\n# Split train data to 90% training and 10% validation parts.\n\nfrom sklearn.model_selection import train_test_split\n\ntrain_df = data.set_index('id')\ntrain_names = train_df.index.values\ntrain_labels = np.asarray(train_df['label'].values)\n\ntr_n, tr_idx, val_n, val_idx = train_test_split(train_names, range(len(train_names)), test_size=0.1, stratify=train_labels, random_state=123)\n\n\n# fastai 1.0\nfrom fastai import *\nfrom fastai.vision import *\nfrom torchvision.models import *    # import *=all the models from torchvision  \n\narch = densenet169                  # specify model\nBATCH_SIZE = 128                    \nsz = CROP_SIZE                      # input size is the crop size\nMODEL_PATH = str(arch).split()[1]   # this will extrat the model name as the model file name \n\n\ntrain_dict = {'name': train_path + train_names, 'label': train_labels}\ndf = pd.DataFrame(data=train_dict)\n# create test dataframe\ntest_names = []\nfor f in os.listdir(test_path):\n    test_names.append(test_path + f)\ndf_test = pd.DataFrame(np.asarray(test_names), columns=['name'])\n\n\nclass MyImageItemList(ImageList):\n    def open(self, fn:PathOrStr)->Image:\n        img = readCroppedImage(fn.replace('/./','').replace('//','/'))\n        # This ndarray image has to be converted to tensor before passing on as fastai Image, we can use pil2tensor\n        return vision.Image(px=pil2tensor(img, np.float32))\n\n\n# Create ImageDataBunch using fastai data block API\nimgDataBunch = (MyImageItemList.from_df(path='/', df=df, suffix='.tif')\n        #Where to find the data?\n        .split_by_idx(val_idx)\n        #How to split in train/valid?\n        .label_from_df(cols='label')\n        #Where are the labels?\n        .add_test(MyImageItemList.from_df(path='/', df=df_test))\n        #dataframe pointing to the test set?\n        .transform(tfms=[[],[]], size=sz)\n        # We have our custom transformations implemented in the image loader but we could apply transformations also here\n        # Even though we don't apply transformations here, we set two empty lists to tfms. Train and Validation augmentations\n        .databunch(bs=BATCH_SIZE)\n        # convert to databunch\n        .normalize([tensor([0.702447, 0.546243, 0.696453]), tensor([0.238893, 0.282094, 0.216251])])\n        # Normalize with training set stats. These are means and std's of each three channel and we calculated these previously in the stats step.\n       )\n\n\nimgDataBunch.show_batch(rows=2, figsize=(4,4))\n\ndef getLearner():\n    return create_cnn(imgDataBunch, arch, pretrained=True, path='.', metrics=accuracy, ps=0.5, callback_fns=ShowGraph)\n\nlearner = getLearner()\n\nlrs = []\nlosses = []\nwds = []\niter_count = 600\n\n\nlearner.lr_find(wd=1e-6, num_it=iter_count)\nlrs.append(learner.recorder.lrs)\nlosses.append(learner.recorder.losses)\nwds.append('1e-6')\nlearner = getLearner() \n\n\nlearner.lr_find(wd=1e-4, num_it=iter_count)\nlrs.append(learner.recorder.lrs)\nlosses.append(learner.recorder.losses)\nwds.append('1e-4')\nlearner = getLearner() \n\nlearner.lr_find(wd=1e-2, num_it=iter_count)\nlrs.append(learner.recorder.lrs)\nlosses.append(learner.recorder.losses)\nwds.append('1e-2')\nlearner = getLearner() #reset learner\n\n\n# Plot weight decays\n_, ax = plt.subplots(1,1) \nmin_y = 0.5\nmax_y = 0.55\nfor i in range(len(losses)):\n    ax.plot(lrs[i], losses[i])\n    min_y = min(np.asarray(losses[i]).min(), min_y)\nax.set_ylabel(\"Loss\")\nax.set_xlabel(\"Learning Rate\")\nax.set_xscale('log')\n#ax ranges may need some tuning with different model architectures \nax.set_xlim((1e-3,3e-1))\nax.set_ylim((min_y - 0.02,max_y))\nax.legend(wds)\nax.xaxis.set_major_formatter(plt.FormatStrFormatter('%.0e'))\n\n\n\nmax_lr = 2e-2\nwd = 1e-4\n\nlearner.fit_one_cycle(cyc_len=8, max_lr=max_lr, wd=wd)\n\n\nlearner.recorder.plot_lr()\n\nlearner.recorder.plot_losses()\n\n\n# predict the validation set with our model\ninterp = ClassificationInterpretation.from_learner(learner)\ninterp.plot_confusion_matrix(title='Confusion matrix')\n\n#save the model at this stage\nlearner.save(MODEL_PATH + '_stage1')\n\n\nlearner.load(MODEL_PATH + '_stage1')\n\n\nlearner.unfreeze()\nlearner.lr_find(wd=wd)\n\n\nlearner.recorder.plot()\n\n\nlearner.fit_one_cycle(cyc_len=12, max_lr=slice(4e-5,4e-4))\n\n\nlearner.recorder.plot_losses()\n\n\ninterp = ClassificationInterpretation.from_learner(learner)\ninterp.plot_confusion_matrix(title='Confusion matrix')\n\n\n# Save the finetuned model\nlearner.save(MODEL_PATH + '_stage2')\n\n\npreds,y, loss = learner.get_preds(with_loss=True)\n# get accuracy\nacc = accuracy(preds, y)\nprint('The accuracy is {0} %.'.format(acc))\n\n\nfrom random import randint\n\ndef plot_overview(interp:ClassificationInterpretation, classes=['Negative','Tumor']):\n    # top losses will return all validation losses and indexes sorted by the largest first\n    tl_val,tl_idx = interp.top_losses()\n    #classes = interp.data.classes\n    fig, ax = plt.subplots(3,4, figsize=(16,12))\n    fig.suptitle('Predicted / Actual / Loss / Probability',fontsize=20)\n    # Random\n    for i in range(4):\n        random_index = randint(0,len(tl_idx))\n        idx = tl_idx[random_index]\n        im,cl = interp.data.dl(DatasetType.Valid).dataset[idx]\n        im = image2np(im.data)\n        cl = int(cl)\n        ax[0,i].imshow(im)\n        ax[0,i].set_xticks([])\n        ax[0,i].set_yticks([])\n        ax[0,i].set_title(f'{classes[interp.pred_class[idx]]} / {classes[cl]} / {interp.losses[idx]:.2f} / {interp.probs[idx][cl]:.2f}')\n    ax[0,0].set_ylabel('Random samples', fontsize=16, rotation=0, labelpad=80)\n    # Most incorrect or top losses\n    for i in range(4):\n        idx = tl_idx[i]\n        im,cl = interp.data.dl(DatasetType.Valid).dataset[idx]\n        cl = int(cl)\n        im = image2np(im.data)\n        ax[1,i].imshow(im)\n        ax[1,i].set_xticks([])\n        ax[1,i].set_yticks([])\n        ax[1,i].set_title(f'{classes[interp.pred_class[idx]]} / {classes[cl]} / {interp.losses[idx]:.2f} / {interp.probs[idx][cl]:.2f}')\n    ax[1,0].set_ylabel('Most incorrect\\nsamples', fontsize=16, rotation=0, labelpad=80)\n    # Most correct or least losses\n    for i in range(4):\n        idx = tl_idx[len(tl_idx) - i - 1]\n        im,cl = interp.data.dl(DatasetType.Valid).dataset[idx]\n        cl = int(cl)\n        im = image2np(im.data)\n        ax[2,i].imshow(im)\n        ax[2,i].set_xticks([])\n        ax[2,i].set_yticks([])\n        ax[2,i].set_title(f'{classes[interp.pred_class[idx]]} / {classes[cl]} / {interp.losses[idx]:.2f} / {interp.probs[idx][cl]:.2f}')\n    ax[2,0].set_ylabel('Most correct\\nsamples', fontsize=16, rotation=0, labelpad=80)\n\n\nplot_overview(interp, ['Negative','Tumor'])\n\n\nfrom fastai.callbacks.hooks import *\n\n# hook into forward pass\ndef hooked_backward(m, oneBatch, cat):\n    # we hook into the convolutional part = m[0] of the model\n    with hook_output(m[0]) as hook_a: \n        with hook_output(m[0], grad=True) as hook_g:\n            preds = m(oneBatch)\n            preds[0,int(cat)].backward()\n    return hook_a,hook_g","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1952a1c5-357d-44dd-97be-dc5e4991b954","_cell_guid":"bf3fb7b5-20ce-4d38-ab04-5f7e258c9d0d","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# We can create a utility function for getting a validation image with an activation map\ndef getHeatmap(val_index):\n    \"\"\"Returns the validation set image and the activation map\"\"\"\n    # this gets the model\n    m = learner.model.eval()\n    tensorImg,cl = imgDataBunch.valid_ds[val_index]\n    # create a batch from the one image\n    oneBatch,_ = imgDataBunch.one_item(tensorImg)\n    oneBatch_im = vision.Image(imgDataBunch.denorm(oneBatch)[0])\n    # convert batch tensor image to grayscale image with opencv\n    cvIm = cv2.cvtColor(image2np(oneBatch_im.data), cv2.COLOR_RGB2GRAY)\n    # attach hooks\n    hook_a,hook_g = hooked_backward(m, oneBatch, cl)\n    # get convolutional activations and average from channels\n    acts = hook_a.stored[0].cpu()\n    #avg_acts = acts.mean(0)\n\n    # Grad-CAM\n    grad = hook_g.stored[0][0].cpu()\n    grad_chan = grad.mean(1).mean(1)\n    grad.shape,grad_chan.shape\n    mult = (acts*grad_chan[...,None,None]).mean(0)\n    return mult, cvIm","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ae6b7c69-e56d-4cb6-84da-69985fbcc94d","_cell_guid":"37ee3ffb-22fe-4790-a080-62116dc3c7cc","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Then, modify our plotting func a bit\ndef plot_heatmap_overview(interp:ClassificationInterpretation, classes=['Negative','Tumor']):\n    # top losses will return all validation losses and indexes sorted by the largest first\n    tl_val,tl_idx = interp.top_losses()\n    #classes = interp.data.classes\n    fig, ax = plt.subplots(3,4, figsize=(16,12))\n    fig.suptitle('Grad-CAM\\nPredicted / Actual / Loss / Probability',fontsize=20)\n    # Random\n    for i in range(4):\n        random_index = randint(0,len(tl_idx))\n        idx = tl_idx[random_index]\n        act, im = getHeatmap(idx)\n        H,W = im.shape\n        _,cl = interp.data.dl(DatasetType.Valid).dataset[idx]\n        cl = int(cl)\n        ax[0,i].imshow(im)\n        ax[0,i].imshow(im, cmap=plt.cm.gray)\n        ax[0,i].imshow(act, alpha=0.5, extent=(0,H,W,0),\n              interpolation='bilinear', cmap='inferno')\n        ax[0,i].set_xticks([])\n        ax[0,i].set_yticks([])\n        ax[0,i].set_title(f'{classes[interp.pred_class[idx]]} / {classes[cl]} / {interp.losses[idx]:.2f} / {interp.probs[idx][cl]:.2f}')\n    ax[0,0].set_ylabel('Random samples', fontsize=16, rotation=0, labelpad=80)\n    # Most incorrect or top losses\n    for i in range(4):\n        idx = tl_idx[i]\n        act, im = getHeatmap(idx)\n        H,W = im.shape\n        _,cl = interp.data.dl(DatasetType.Valid).dataset[idx]\n        cl = int(cl)\n        ax[1,i].imshow(im)\n        ax[1,i].imshow(im, cmap=plt.cm.gray)\n        ax[1,i].imshow(act, alpha=0.5, extent=(0,H,W,0),\n              interpolation='bilinear', cmap='inferno')\n        ax[1,i].set_xticks([])\n        ax[1,i].set_yticks([])\n        ax[1,i].set_title(f'{classes[interp.pred_class[idx]]} / {classes[cl]} / {interp.losses[idx]:.2f} / {interp.probs[idx][cl]:.2f}')\n    ax[1,0].set_ylabel('Most incorrect\\nsamples', fontsize=16, rotation=0, labelpad=80)\n    # Most correct or least losses\n    for i in range(4):\n        idx = tl_idx[len(tl_idx) - i - 1]\n        act, im = getHeatmap(idx)\n        H,W = im.shape\n        _,cl = interp.data.dl(DatasetType.Valid).dataset[idx]\n        cl = int(cl)\n        ax[2,i].imshow(im)\n        ax[2,i].imshow(im, cmap=plt.cm.gray)\n        ax[2,i].imshow(act, alpha=0.5, extent=(0,H,W,0),\n              interpolation='bilinear', cmap='inferno')\n        ax[2,i].set_xticks([])\n        ax[2,i].set_yticks([])\n        ax[2,i].set_title(f'{classes[interp.pred_class[idx]]} / {classes[cl]} / {interp.losses[idx]:.2f} / {interp.probs[idx][cl]:.2f}')\n    ax[2,0].set_ylabel('Most correct\\nsamples', fontsize=16, rotation=0, labelpad=80)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c6eaf478-1337-445b-b109-2483351a6994","_cell_guid":"da171e3a-2a16-4c8a-aeba-e175b48bce0b","trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"plot_heatmap_overview(interp, ['Negative','Tumor'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6df5eede-c5de-47f2-a2c6-6f7a800e0073","_cell_guid":"c206b9e9-7175-4b27-b692-b57cab080fcb","trusted":true},"cell_type":"markdown","source":"This shows the activation maps of the predicted category so if the label is ```tumor```, the visualization shows all the places where the model thinks the tumor patterns are."},{"metadata":{"_uuid":"70c6592f-aabe-4224-97a4-577969d6d87b","_cell_guid":"b6bfd8ec-c93f-4b7e-af04-acf65e314de3","trusted":true},"cell_type":"markdown","source":"### ROC curve and AUC\nRemember, AUC is the metric that is used for evaluating submissions. We can calculate it here for ou validation set but it will most likely differ from the final score."},{"metadata":{"_uuid":"91e54c19-9c91-446e-8346-2bccdc1053ec","_cell_guid":"ab7020b0-4e28-4a9a-88d0-7ce858964e7f","trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\n# probs from log preds\nprobs = np.exp(preds[:,1])\n# Compute ROC curve\nfpr, tpr, thresholds = roc_curve(y, probs, pos_label=1)\n\n# Compute ROC area\nroc_auc = auc(fpr, tpr)\nprint('ROC area is {0}'.format(roc_auc))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"06f56548-e91a-4a31-ae30-ec29ae78c419","_cell_guid":"8ca44905-9e66-4da8-b35a-86ccb54e10a6","trusted":true},"cell_type":"code","source":"plt.figure()\nplt.plot(fpr, tpr, color='darkorange', label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', linestyle='--')\nplt.xlim([-0.01, 1.0])\nplt.ylim([0.0, 1.01])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eb36ef0f-7fc1-4ea7-8b43-5d1363f7f186","_cell_guid":"2508e03f-e14d-420e-864e-2a7f35792960","trusted":true},"cell_type":"markdown","source":"----------------\n\n# Submit predictions\n### TTA\nTo evaluate the model, we run inference on all test images. As we have test time augmentation, our results will probably improve if we do predictions multiple times per image and average out the results."},{"metadata":{"_uuid":"8f8745e4-2883-4d58-990c-0716007c3a76","_cell_guid":"cc493e9f-b001-4ad3-9f03-fdbc8a91086f","trusted":true},"cell_type":"code","source":"# make sure we have the best performing model stage loaded\nlearner.load(MODEL_PATH + '_stage2')\n\n# Fastai has a function for this but we don't want the additional augmentations it does (our image loader has augmentations) so we just use the get_preds\n#preds_test,y_test=learner.TTA(ds_type=DatasetType.Test)\n\n# We do a fair number of iterations to cover different combinations of flips and rotations.\n# The predictions are then averaged.\nn_aug = 12\npreds_n_avg = np.zeros((len(learner.data.test_ds.items),2))\nfor n in tqdm_notebook(range(n_aug), 'Running TTA...'):\n    preds,y = learner.get_preds(ds_type=DatasetType.Test, with_loss=False)\n    preds_n_avg = np.sum([preds_n_avg, preds.numpy()], axis=0)\npreds_n_avg = preds_n_avg / n_aug","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6cfa450c-e631-476b-97b7-3601a682ec5b","_cell_guid":"0dead92b-5413-4ef5-9ca4-c05a3cab46a5","trusted":true},"cell_type":"code","source":"# Next, we will transform class probabilities to just tumor class probabilities\nprint('Negative and Tumor Probabilities: ' + str(preds_n_avg[0]))\ntumor_preds = preds_n_avg[:, 1]\nprint('Tumor probability: ' + str(tumor_preds[0]))\n# If we wanted to get the predicted class, argmax would get the index of the max\nclass_preds = np.argmax(preds_n_avg, axis=1)\nclasses = ['Negative','Tumor']\nprint('Class prediction: ' + classes[class_preds[0]])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a264b056-20b4-4649-a1ea-999cece98ffb","_cell_guid":"b2c85221-5267-4ae1-b958-aa7b9d3536da","trusted":true},"cell_type":"markdown","source":"### Submit the model for evaluation\nWe need to submit the tumor probability for each test sample in this competition. The probability ranges from 0 to 1."},{"metadata":{"_uuid":"fc581299-06ca-48fb-90b3-629c81ac9920","_cell_guid":"d54cc15d-6454-4543-ac9d-653fb1d4b2b2","trusted":true},"cell_type":"code","source":"# get test id's from the sample_submission.csv and keep their original order\nSAMPLE_SUB = '/kaggle/input/sample_submission.csv'\nsample_df = pd.read_csv(SAMPLE_SUB)\nsample_list = list(sample_df.id)\n\n# List of tumor preds. \n# These are in the order of our test dataset and not necessarily in the same order as in sample_submission\npred_list = [p for p in tumor_preds]\n\n# To know the id's, we create a dict of id:pred\npred_dic = dict((key, value) for (key, value) in zip(learner.data.test_ds.items, pred_list))\n\n# Now, we can create a new list with the same order as in sample_submission\npred_list_cor = [pred_dic['///kaggle/input/test/' + id + '.tif'] for id in sample_list]\n\n# Next, a Pandas dataframe with id and label columns.\ndf_sub = pd.DataFrame({'id':sample_list,'label':pred_list_cor})\n\n# Export to csv\ndf_sub.to_csv('{0}_submission.csv'.format(MODEL_PATH), header=True, index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"40129c7c-2297-4f6d-b6c7-87c22548225b","_cell_guid":"df01e450-f530-4216-82de-8a21fd89e41b","trusted":true},"cell_type":"code","source":"# This is what the first 10 items of submission look like\ndf_sub.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a49fc521-f6ad-45dc-86b0-9398310010bc","_cell_guid":"1c3f2637-ae84-42d0-9c79-1cee625096b6","trusted":true},"cell_type":"markdown","source":"--------------------------------------------------------------\n\n# Deploy (example)\nNow that we have a working model. We could deploy this for inference to another machine, a web server for example.\nFor this, we need our saved model, and then we need to export the  DataBunch.\n\n**Note**. I haven't tried these so I cannot guarantee these will work straight away. You may need to do some debugging. [Official documentation](https://docs.fast.ai/tutorial.inference.html)."},{"metadata":{"_uuid":"74361faa-fc00-4193-96c7-ee866e44ad26","_cell_guid":"a334da5a-9a1d-4c4c-8e35-70e6296f32ba","trusted":true},"cell_type":"code","source":"# This will create an export.pkl file that you'll need to copy with your model file if you want to deploy it on another device.\n# This saves the internal information (classes, etc) need for inference in a file named 'export.pkl'. \nimgDataBunch.export(fname='./export.pkl')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d38681b9-428f-433c-9c61-a08eecd70e37","_cell_guid":"4ef8e828-117d-4cd0-a64b-4fcf3e3dad86","trusted":true},"cell_type":"markdown","source":"**On a new machine**\nWe need to create an empty DataBunch and load it to a learner."},{"metadata":{"_uuid":"65830781-9e7d-4275-8c52-1c4ba8cc171d","_cell_guid":"489f76ea-4a95-4932-8a89-c5e1c6659345","trusted":true},"cell_type":"code","source":"######## RUN THIS ON A NEW MACHINE ##########\n#from fastai.vision import * # fastai 1.0\n#from fastai import *\n#from torchvision.models import *\n#arch = densenet169       # specify model architecture\n#MODEL_PATH = str(arch).split()[1] + '_stage2'\n#empty_data = ImageDataBunch.load_empty('./') #this will look for a file named export.pkl in the specified path\n#learner = create_cnn(empty_data, arch).load(MODEL_PATH)","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"f22a6af9-b3e7-4399-a387-caadab18a7a6","_cell_guid":"7ea5e5c4-928f-4a61-a3ac-33d04cce4d1a","trusted":true},"cell_type":"code","source":"## And then we are ready to do predictions\nimport cv2\nsz = 68\n\n# This function will convert image to the prediction format\ndef imageToTensorImage(path):\n    bgr_img = cv2.imread(path)\n    b,g,r = cv2.split(bgr_img)\n    rgb_img = cv2.merge([r,g,b])\n    # crop to center to the correct size and convert from 0-255 range to 0-1 range\n    H,W,C = rgb_img.shape\n    rgb_img = rgb_img[(H-sz)//2:(sz +(H-sz)//2),(H-sz)//2:(sz +(H-sz)//2),:] / 256\n    return vision.Image(px=pil2tensor(rgb_img, np.float32))\n\nimg = imageToTensorImage('/kaggle/input/test/0eb051700fb6b1bf96188f36c8e4889598c6a157.tif')\n\n## predict and visualize\nimg.show(y=learner.predict(img)[0])\nclasses = ['negative', 'tumor']\nprint('This is a ' + classes[int(learner.predict(img)[0])] + ' tissue image.')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4f6c4f39-93e3-43f3-8b3a-7a78b5c528f2","_cell_guid":"731c37b9-5f40-40fc-803c-908479c4c671","trusted":true},"cell_type":"code","source":"img = imageToTensorImage('/kaggle/input/test/00040095a4a671280aeb66cb0c9231e6216633b5.tif')\n\n## predict and visualize\nimg.show(y=learner.predict(img)[0])\nclasses = ['negative', 'tumor']\nprint('This is a ' + classes[int(learner.predict(img)[0])] + ' tissue image.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img = imageToTensorImage('/kaggle/input/test/000de14191f3bab4d2d6a7384ca0e5aa5dc0dffe.tif')\n\n## predict and visualize\nimg.show(y=learner.predict(img)[0])\nclasses = ['negative', 'tumor']\nprint('This is a ' + classes[int(learner.predict(img)[0])] + ' tissue image.')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}