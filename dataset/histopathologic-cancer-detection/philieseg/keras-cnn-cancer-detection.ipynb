{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport cv2\nfrom tensorflow.python import keras\nfrom tensorflow.python.keras.models import Sequential\nfrom tensorflow.python.keras.layers import Dense, Flatten, Conv2D, Dropout, MaxPooling2D\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Load Data\nsample_submission = pd.read_csv(\"../input/histopathologic-cancer-detection/sample_submission.csv\")\ntrain_labels      = pd.read_csv(\"../input/histopathologic-cancer-detection/train_labels.csv\")\n\ntrain_dir         = \"../input/histopathologic-cancer-detection/train/\"\ntrain_files       = os.listdir(train_dir)\n\ntest_dir          = \"../input/histopathologic-cancer-detection/test/\"\ntest_files        = os.listdir(test_dir)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"#Inspect Labels\nprint(train_labels.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Preview Images\n\nnr_images = 10\n    \nfig, axs = plt.subplots(1, len(train_labels[:nr_images]), figsize = (20, 2))\nfor ii, ax in enumerate(axs):\n    ax.imshow(cv2.imread(train_dir + train_labels.id[ii] + \".tif\"))\n    ax.set_title(\"Label: \" + str(train_labels.label[ii]))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Descriptive Analytics for given Dataset\n\nprint(train_labels.label.value_counts())\n\nimport plotly.graph_objects as go\n\nlabels = [\"No Cancer - 0\", \"Cancer - 1\"]\nvalues = train_labels.label.value_counts()\n\ndonut = go.Figure(data=[go.Pie(labels=labels, values=values, hole=.5, marker_colors=[\"rgb(217, 140, 217)\", \"rgb(96, 32, 96)\"])])\ndonut.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def img_prep(directory, files, start = 0, end = -1, test=False):\n    if end == -1:\n        end = len(files)\n    X = []\n    if test:\n        for image in files:\n            img = cv2.imread( directory + image)\n            #img = cv2.resize(img,(img_size,img_size))\n            img = cv2.resize(img, (96, 96))\n            X.append(img)\n        print(\"Image shape: \",X[0].shape)\n        X = np.array(X)\n        return X\n    else:\n        for image in files.id[start:end]:\n            img = cv2.imread( directory + image + \".tif\")\n            #img = cv2.resize(img,(img_size,img_size))\n            img = cv2.resize(img, (96, 96))\n            X.append(img)\n        print(\"Image shape: \",X[0].shape)\n        X = np.array(X)\n    \n    \n        y = files.label[start:end]\n        return X, y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nX_train, y_train = img_prep(train_dir, train_labels, start=10_000, end = 120_000)\n#X_test, y_test  = img_prep(train_dir, train_labels, start= 200000, end=203000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Build CNN Model (3 * 3 w/ ascending filters)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Build CNN\n\nmodel = Sequential()\nmodel.add(Conv2D(filters=32, kernel_size = 3, padding='same', activation = 'relu', input_shape = (96, 96, 3)))\nmodel.add(Conv2D(filters=32, kernel_size = 3, padding='same', activation = 'relu'))\nmodel.add(Conv2D(filters=32, kernel_size = 3, padding='same', activation = 'relu'))\nmodel.add(Dropout(0.25))\nmodel.add(MaxPooling2D(pool_size = 3))\n\nmodel.add(Conv2D(filters=64, kernel_size = 3, padding='same', activation = 'relu'))\nmodel.add(Conv2D(filters=64, kernel_size = 3, padding='same', activation = 'relu'))\nmodel.add(Conv2D(filters=64, kernel_size = 3, padding='same', activation = 'relu'))\nmodel.add(Dropout(0.25))\nmodel.add(MaxPooling2D(pool_size = 3)) \n\nmodel.add(Conv2D(filters=128, kernel_size = 3, padding='same', activation = 'relu'))\nmodel.add(Conv2D(filters=256, kernel_size = 3, padding='same', activation = 'relu'))\nmodel.add(Conv2D(filters=512, kernel_size = 3, padding='same', activation = 'relu'))\nmodel.add(Flatten())\nmodel.add(Dense(256, activation=\"relu\"))\nmodel.add(Dense(1, activation = \"sigmoid\"))\n\nmodel.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n#Model Callbacks to reduce computation time\nearly_stopping = EarlyStopping(monitor = \"val_loss\", patience = 3)\ncheckpoint = ModelCheckpoint(filepath = \"weights.hdf5\", verbose=1, save_best_only=True)\n\n#Model Training\nhistory = model.fit(X_train, y_train, batch_size = 100, epochs = 20, validation_split=.2, callbacks = [early_stopping, checkpoint])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot training & validation accuracy values\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='best')\nplt.show()\n\n# Plot training & validation loss values\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='best')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Image Augmentations"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Define Image Augmentations (horizontal and vertical flip)\n#Fit model on randomly augmented images\n\n# from keras.preprocessing.image import ImageDataGenerator\n# import gc\n\n# sample_size = 10_000\n# batch_size = 35\n# epochs = 15\n\n# early_stopping = EarlyStopping(monitor = \"val_loss\", patience = 3)\n# checkpoint_aug = ModelCheckpoint(filepath = \"aug_weights.hdf5\", verbose=1, save_best_only=True)\n\n# for x in range((len(train_labels)//sample_size) + 1):\n    \n#     gc.collect()\n    \n#     if x * sample_size > len(train_labels):\n#         end = len(train_labels)\n#     else:\n#         start = x * sample_size\n#         end = start + sample_size\n    \n#     X_train, y_train = img_prep(train_dir, train_labels, start = start, end = end)\n    \n#     full = len(X_train)\n\n#     split_start = int(full * 0.8)\n#     split_end = split_start + int(full * 0.2)\n#     steps_per_ep = int(split_start / batch_size / epochs)\n\n#     print(\"Train Samples:\",split_start)\n#     print(\"Validation Samples:\",split_end - split_start)\n\n#     # define data preparation\n#     datagen_train = ImageDataGenerator(horizontal_flip=True,\n#                                        vertical_flip=True)\n#     datagen_train.fit(X_train)\n#     gen_train = datagen_train.flow(X_train[:split_start], y_train[:split_start] , batch_size=batch_size)\n\n#     datagen_val = ImageDataGenerator()\n#     datagen_val.fit(X_train)\n#     gen_val = datagen_val.flow(X_train[split_start:split_end], y_train[split_start:split_end])\n    \n#     if start == 0:\n#         model.load_weights('../working/aug_weights.hdf5')\n\n#     history_aug = model.fit_generator(\n#         gen_train,\n#         steps_per_epoch = steps_per_ep,\n#         epochs = epochs,\n#         validation_data = gen_val,\n#         validation_steps = steps_per_ep,\n#         callbacks = [early_stopping, checkpoint_aug])\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Plot training & validation accuracy values\n# plt.plot(history_aug.history['accuracy'])\n# plt.plot(history_aug.history['val_accuracy'])\n# plt.title('Model accuracy')\n# plt.ylabel('Accuracy')\n# plt.xlabel('Epoch')\n# plt.legend(['Train', 'Validation'], loc='best')\n# plt.show()\n\n# # Plot training & validation loss values\n# plt.plot(history_aug.history['loss'])\n# plt.plot(history_aug.history['val_loss'])\n# plt.title('Model loss')\n# plt.ylabel('Loss')\n# plt.xlabel('Epoch')\n# plt.legend(['Train', 'Validation'], loc='best')\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Load best Model weights and evaluate on unseen data\n#model.load_weights('../input/model-weights/weights (3).hdf5')\n#model.evaluate(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Load Test Files and predict labels (used during model building and optimization)\ntest = img_prep(test_dir, test_files, test=True)\npred_test = model.predict(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create Submission File"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Prepare Submission.csv file\nlst = []\nfor item in test_files:\n    lst.append(item[:-4])\n    \ntest_df = pd.DataFrame(lst)\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create Submission.csv file\npredictions = np.array(pred_test)\ntest_df[\"label\"] = predictions\ntest_df.columns = [\"id\", \"label\"]\nsubmission = test_df\n\nprint(submission.head())\nsubmission.to_csv(\"submission.csv\", index = False, header = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}