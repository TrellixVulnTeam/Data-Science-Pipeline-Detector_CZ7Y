{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Summary Notebook\n### Taylor Kern","metadata":{}},{"cell_type":"markdown","source":"# Original Model","metadata":{}},{"cell_type":"markdown","source":"This model, was my original model which I created from scratch. I trained it for three training sets. The results of this was shown in a loss, accuracy, and AUC graph. We can see that by the end of the three training sets, the validation line was not only staggery in all graphs, but also had a higher loss, and a lower accuracy and AUC than the training line. ","metadata":{}},{"cell_type":"markdown","source":"# Model 1\nhttps://www.kaggle.com/taylorkern/tk-hcd-v011\n","metadata":{}},{"cell_type":"markdown","source":"Essentially the same architecture, but with images resized from 96x96 to 32x32 and with no cropping.\nI trained this model for three training sets. The results of this was shown in a loss, accuracy, and AUC graph. We can see that by the end of the three training sets, the validation line was around training line on each graph, although it was very staggery.\n","metadata":{}},{"cell_type":"markdown","source":"# Model 2\nhttps://www.kaggle.com/code/taylorkern/tk-hcd-v022","metadata":{}},{"cell_type":"markdown","source":"Essentially the same architecture, but with images left at 96x96 and with no cropping. \n","metadata":{}},{"cell_type":"markdown","source":"# Model 3\nhttps://www.kaggle.com/code/taylorkern/tk-hcd-v033","metadata":{}},{"cell_type":"markdown","source":"Train a slightly larger model, but with images cropping down to 32x32.\n\nI trained this model for three training sets. The results of this was shown in a loss, accuracy, and AUC graph. We can see that by the end of the three training sets, the validation line was not only staggery in all graphs, but also had a lower loss, and a higher accuracy and AUC than the training line. ","metadata":{}},{"cell_type":"markdown","source":"# Model 4\nhttps://www.kaggle.com/code/taylorkern/tk-hcd-v044","metadata":{}},{"cell_type":"markdown","source":"Train a slightly larger model, but with images resized down to 32x32 (without cropping).\n\nI trained this model for three training sets. The results of this was shown in a loss, accuracy, and AUC graph. We can see that by the end of the three training sets, the validation line was not only staggery in all graphs, but also had a loss, accuracy and AUC on the training line. ","metadata":{}},{"cell_type":"markdown","source":"# Model 5\nhttps://www.kaggle.com/code/taylorkern/tk-hcd-v0055","metadata":{}},{"cell_type":"markdown","source":"Train a slightly larger model, using 96x96 images (without cropping).\n\nI trained this model for three training sets. The results of this was shown in a loss, accuracy, and AUC graph. We can see that by the end of the three training sets, the validation line was not only staggery in all graphs, but also had a lower loss, and a higher accuracy and AUC than the training line.","metadata":{}},{"cell_type":"markdown","source":"# VGG16 Model\nhttps://www.kaggle.com/code/taylorkern/tk-hcd-vgg16-v01","metadata":{}},{"cell_type":"markdown","source":"In this model, I created a base model which had 96x96 images. I trained for three training sets, the first for 40 epochs with a learning rate of 0.001\n\nI then conducted fine tuning on the base model setting trainable equal to true, with a learning rate of 0.00001. From here I obtained the accuracy, and AUC. \n\nI then continued training for 30 more epochs, in the second training set, and then for 20 epochs.\n\nAfter the third training set, we are able to see the large jump due to the finr tuning, as well as the staggered validation line on the loss, accuracy, and AUC graphs. We can also see that the validation lline has a slightly lower loss, and slightly larger accuracy, and AUC score. ","metadata":{}},{"cell_type":"markdown","source":"# VGG19 Model \nhttps://www.kaggle.com/code/taylorkern/tk-hcd-vgg19","metadata":{}},{"cell_type":"markdown","source":"In this model, I created a base model which had 96x96 images. I also use VGG19 I trained for three training sets, the first for 40 epochs with a learning rate of 0.001\n\nI then conducted fine tuning on the base model setting trainable equal to true, with a learning rate of 0.00001. From here I obtained the loss, accuracy, and AUC. \n\nI then continued training for 30 more epochs, in the second training set, and then for 20 epochs\n\nAfter the third training set, through the graph regarding the vlidation and training loss, accuracy, and AUC, we can see that the validation line is staggery. We can also see when the fine tuning kicked in by the large stagger. We also may notice that the validation loss is lower, and has a higher accuracy and AUC than the training. We also can see that the training lines are smoother than the validation.","metadata":{}},{"cell_type":"markdown","source":"# Efficient Net Model \nhttps://www.kaggle.com/code/taylorkern/tk-efficientnet","metadata":{}},{"cell_type":"markdown","source":"In this model, I created a base model which had 96x96 images. I also use EfficientNet. I trained for three training sets, the first for 40 epochs with a learning rate of 0.001\n\nI then conducted fine tuning on the base model setting trainable equal to true, with a learning rate of 0.00001. From here I obtained the accuracy, and AUC. \n\nI then continued training for 30 more epochs, in the second training set, and then for 20 epochs\n\nAfter the third training set, we can see that while the training line is very smooth, the validation line is quite staggery in the loss, accuracy, and AUC graphs. While this is true, the validation lines are mostly around the same area as the training lines.","metadata":{}},{"cell_type":"markdown","source":"# ResNet 50 \n### (Best Model)\nhttps://www.kaggle.com/code/taylorkern/tk-hcd-resnet50-v01","metadata":{}},{"cell_type":"markdown","source":"In this model, I created a base model which had 96x96 images. I also use ResNet50. I trained for three training sets, the first for 40 epochs with a learning rate of 0.001\n\nI then conducted fine tuning on the base model setting trainable equal to true, with a learning rate of 0.00001. From here I obtained the accuracy, and AUC. \n\nI then continued training for 30 more epochs, in the second training set, and then for 20 epochs.\n\nAfter the third training set, we are able to see in the graphs provided that the training loss is very low while the validation is even lower. In the graph we can see there is not a lot of staggering. \n\nWe can also see that the validation accuracy is higher than the training accuracy. Again, the output of the line is not staggery, and is very smooth.\n\nFinally, in the AUC graph, we can see that the validation and training is about the same, again with a smooth line. ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}