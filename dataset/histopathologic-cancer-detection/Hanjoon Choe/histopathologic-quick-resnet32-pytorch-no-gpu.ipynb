{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n'''\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n'''\n\n# Any results you write to the current directory are saved as output.\nimport torch\nimport cv2\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F\nfrom functools import partial\nfrom sklearn.model_selection import train_test_split\nimport torch.optim as optim\nfrom tqdm import tqdm_notebook as tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport gc","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"path = '/kaggle/input/histopathologic-cancer-detection'\ntrain = pd.read_csv(os.path.join(path,'train_labels.csv'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TumorDataset(Dataset):\n    def __init__(self,df, kind = 'train'):\n        self.df = df\n        self.kind = kind\n        self.path = '/kaggle/input/histopathologic-cancer-detection'\n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self,idx):\n        if (self.kind == 'train') or (self.kind =='test'):\n            label = self.df.label.values[idx]\n            fname = self.df.id.values[idx]\n            #Image = cv2.resize(cv2.imread(os.path.join(self.path,self.kind,fname+'.tif')),(224,224))\n            Image = cv2.imread(os.path.join(self.path,self.kind,fname+'.tif'))\n            return torch.tensor(Image/255.0).permute(2,1,0),label\n        else:\n            fname = self.df.id.values[idx]\n            Image = cv2.imread(os.path.join(self.path,'test',fname+'.tif'))\n            return torch.tensor(Image/255.0).permute(2,1,0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set = TumorDataset(train)\ntrain_loader = torch.utils.data.DataLoader(train_set,batch_size=16,shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a = next(iter(train_loader))\nfig,ax = plt.subplots(4,4, figsize=(10,10))\n\nfor i in range(16):\n    j = i//4\n    k = i%4\n    ax[j,k].imshow(a[0][i].permute(2,1,0))\n    ax[j,k].set_title(f'label : {a[1][i]}')\nfig.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Source : https://github.com/FrancescoSaverioZuppichini/ResNet"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Conv2dAuto(nn.Conv2d):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.padding =  (self.kernel_size[0] // 2, self.kernel_size[1] // 2) # dynamic add padding based on the kernel_size\n        \nconv3x3 = partial(Conv2dAuto, kernel_size=3, bias=False) \ndef activation_func(activation):\n    return  nn.ModuleDict([\n        ['relu', nn.ReLU(inplace=True)],\n        ['leaky_relu', nn.LeakyReLU(negative_slope=0.01, inplace=True)],\n        ['selu', nn.SELU(inplace=True)],\n        ['none', nn.Identity()]\n    ])[activation]\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, activation='relu'):\n        super().__init__()\n        self.in_channels, self.out_channels, self.activation = in_channels, out_channels, activation\n        self.blocks = nn.Identity()\n        self.activate = activation_func(activation)\n        self.shortcut = nn.Identity()   \n    \n    def forward(self, x):\n        residual = x\n        if self.should_apply_shortcut: residual = self.shortcut(x)\n        x = self.blocks(x)\n        x += residual\n        x = self.activate(x)\n        return x\n    \n    @property\n    def should_apply_shortcut(self):\n        return self.in_channels != self.out_channels\n    \nclass ResNetResidualBlock(ResidualBlock):\n    def __init__(self, in_channels, out_channels, expansion=1, downsampling=1, conv=conv3x3, *args, **kwargs):\n        super().__init__(in_channels, out_channels, *args, **kwargs)\n        self.expansion, self.downsampling, self.conv = expansion, downsampling, conv\n        self.shortcut = nn.Sequential(\n            nn.Conv2d(self.in_channels, self.expanded_channels, kernel_size=1,\n                      stride=self.downsampling, bias=False),\n            nn.BatchNorm2d(self.expanded_channels)) if self.should_apply_shortcut else None\n        \n        \n    @property\n    def expanded_channels(self):\n        return self.out_channels * self.expansion\n    \n    @property\n    def should_apply_shortcut(self):\n        return self.in_channels != self.expanded_channels\ndef conv_bn(in_channels, out_channels, conv, *args, **kwargs):\n    return nn.Sequential(conv(in_channels, out_channels, *args, **kwargs), nn.BatchNorm2d(out_channels))\n\nclass ResNetBasicBlock(ResNetResidualBlock):\n    \"\"\"\n    Basic ResNet block composed by two layers of 3x3conv/batchnorm/activation\n    \"\"\"\n    expansion = 1\n    def __init__(self, in_channels, out_channels, *args, **kwargs):\n        super().__init__(in_channels, out_channels, *args, **kwargs)\n        self.blocks = nn.Sequential(\n            conv_bn(self.in_channels, self.out_channels, conv=self.conv, bias=False, stride=self.downsampling),\n            activation_func(self.activation),\n            conv_bn(self.out_channels, self.expanded_channels, conv=self.conv, bias=False),\n        )\nclass ResNetBottleNeckBlock(ResNetResidualBlock):\n    expansion = 4\n    def __init__(self, in_channels, out_channels, *args, **kwargs):\n        super().__init__(in_channels, out_channels, expansion=4, *args, **kwargs)\n        self.blocks = nn.Sequential(\n           conv_bn(self.in_channels, self.out_channels, self.conv, kernel_size=1),\n             activation_func(self.activation),\n             conv_bn(self.out_channels, self.out_channels, self.conv, kernel_size=3, stride=self.downsampling),\n             activation_func(self.activation),\n             conv_bn(self.out_channels, self.expanded_channels, self.conv, kernel_size=1),\n        )\nclass ResNetLayer(nn.Module):\n    \"\"\"\n    A ResNet layer composed by `n` blocks stacked one after the other\n    \"\"\"\n    def __init__(self, in_channels, out_channels, block=ResNetBasicBlock, n=1, *args, **kwargs):\n        super().__init__()\n        # 'We perform downsampling directly by convolutional layers that have a stride of 2.'\n        downsampling = 2 if in_channels != out_channels else 1\n        self.blocks = nn.Sequential(\n            block(in_channels , out_channels, *args, **kwargs, downsampling=downsampling),\n            *[block(out_channels * block.expansion, \n                    out_channels, downsampling=1, *args, **kwargs) for _ in range(n - 1)]\n        )\n\n    def forward(self, x):\n        x = self.blocks(x)\n        return x\n    \nclass ResNetEncoder(nn.Module):\n    \"\"\"\n    ResNet encoder composed by layers with increasing features.\n    \"\"\"\n    def __init__(self, in_channels=3, blocks_sizes=[64, 128, 256, 512], deepths=[2,2,2,2], \n                 activation='relu', block=ResNetBasicBlock, *args, **kwargs):\n        super().__init__()\n        self.blocks_sizes = blocks_sizes\n        \n        self.gate = nn.Sequential(\n            nn.Conv2d(in_channels, self.blocks_sizes[0], kernel_size=7, stride=2, padding=3, bias=False),\n            nn.BatchNorm2d(self.blocks_sizes[0]),\n            activation_func(activation),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        )\n        \n        self.in_out_block_sizes = list(zip(blocks_sizes, blocks_sizes[1:]))\n        self.blocks = nn.ModuleList([ \n            ResNetLayer(blocks_sizes[0], blocks_sizes[0], n=deepths[0], activation=activation, \n                        block=block,*args, **kwargs),\n            *[ResNetLayer(in_channels * block.expansion, \n                          out_channels, n=n, activation=activation, \n                          block=block, *args, **kwargs) \n              for (in_channels, out_channels), n in zip(self.in_out_block_sizes, deepths[1:])]       \n        ])\n        \n        \n    def forward(self, x):\n        x = self.gate(x)\n        for block in self.blocks:\n            x = block(x)\n        return x\nclass ResnetDecoder(nn.Module):\n    \"\"\"\n    This class represents the tail of ResNet. It performs a global pooling and maps the output to the\n    correct class by using a fully connected layer.\n    \"\"\"\n    def __init__(self, in_features, n_classes):\n        super().__init__()\n        self.avg = nn.AdaptiveAvgPool2d((1, 1))\n        self.decoder = nn.Linear(in_features, n_classes)\n\n    def forward(self, x):\n        x = self.avg(x)\n        x = x.view(x.size(0), -1)\n        x = self.decoder(x)\n        return x\nclass ResNet(nn.Module):\n    \n    def __init__(self, in_channels, n_classes, *args, **kwargs):\n        super().__init__()\n        self.encoder = ResNetEncoder(in_channels, *args, **kwargs)\n        self.decoder = ResnetDecoder(self.encoder.blocks[-1].blocks[-1].expanded_channels, n_classes)\n        self.sigmoid = nn.Sigmoid()\n        \n    def forward(self, x):\n        x = self.encoder(x)\n        x = self.decoder(x)\n        x = self.sigmoid(x)\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def resnet34(in_channels, n_classes, block=ResNetBasicBlock, *args, **kwargs):\n    return ResNet(in_channels, n_classes, block=block, deepths=[3, 4, 6, 3], *args, **kwargs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"!pip install torchsummary\nfrom torchsummary import summary\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel = resnet34(3, 1).to(device)\nsummary(model, (3, 32, 32))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_loss = []\nvalid_loss = []\ntrain_acc = []\nvalid_acc = []\ntrain_auc = []\nvalid_auc = []\n'''\ndef criterion(input, target, size_average=True):\n    \"\"\"Categorical cross-entropy with logits input and one-hot target\"\"\"\n    l = -(target * torch.log(F.softmax(input, dim=1) + 1e-10)).sum(1)\n    if size_average:\n        l = l.mean()\n    else:\n        l = l.sum()\n    return l\n'''\nclass Learner(object):\n    def __init__(self,df):\n        self.df = df\n    def fit(self,epochs,batch_size,shuffle):\n\n        self.train(epochs,model,batch_size,shuffle)\n    def train(self,epochs,model,batch_size,shuffle):\n        train_df, valid_df, _, _ = train_test_split(self.df, self.df['label'],\n                                                    stratify=self.df['label'], \n                                                    test_size=0.1)\n        train_set = TumorDataset(train_df)\n        valid_set = TumorDataset(valid_df)\n        train_loader = torch.utils.data.DataLoader(train_set,batch_size=batch_size,shuffle=shuffle)\n        valid_loader = torch.utils.data.DataLoader(valid_set,batch_size=500,shuffle=shuffle)\n        \n        for epoch in range(epochs):\n            model.train()\n            running_loss = 0.0\n            running_acc = 0.0\n            running_auc = 0.0\n            print(f'epoch {epoch+1}/{epochs}')\n            for idx, (inputs,targets) in tqdm(enumerate(train_loader),total=len(train_loader)):\n                optimizer.zero_grad()\n                inputs.to(device)\n                targets.to(device)\n                outputs = model(inputs.float())#.cuda())\n                loss = criterion(outputs.cpu().squeeze(),targets.float())\n                loss.backward()\n                optimizer.step()\n                running_loss += loss\n                running_acc += (outputs.cpu().round() == targets).float().mean()\n                running_auc += roc_auc_score_FIXED(targets.detach().numpy(),outputs.detach().squeeze().numpy())\n                if idx%20 == 19:\n                    gc.collect()\n            train_loss.append(running_loss/len(train_loader))\n            train_acc.append(running_acc/len(train_loader))\n            train_auc.append(running_auc/len(train_loader))\n            print('train loss {:.2f} train acc {:.2f} train auc {:.3f}'.format(running_loss/len(train_loader),running_acc/len(train_loader),running_auc/len(train_loader)))\n            if epoch%2 == 1:\n                self.valid(valid_loader,model)\n\n    def valid(self,data_loader,model):\n        model.eval()\n        running_loss = 0.0\n        running_acc = 0.0\n        running_auc = 0.0\n        for idx, (inputs,targets) in enumerate(data_loader):\n            with torch.no_grad():\n                inputs.to(device)\n                targets.to(device)\n                outputs = model(inputs.float())#.cuda())\n                loss = criterion(outputs.cpu().squeeze(),targets.float())\n                running_loss += loss\n                running_acc += (outputs.cpu().round() == targets).float().mean()\n                running_auc += roc_auc_score_FIXED(targets.detach().numpy(),outputs.detach().squeeze().numpy())\n        valid_loss.append(running_loss/len(data_loader))\n        valid_acc.append(running_acc/len(data_loader))\n        valid_auc.append(running_auc/len(data_loader))\n        print('valid loss {:.2f} valid acc {:.2f}'.format(running_loss/len(data_loader),running_acc/len(data_loader)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It fails to commit as I increase epochs without using gpu,\nI downsampled the train_set to 300(150 samples from each classes) for each epochs(set to around 20).\nThis is an experiment if this also works, not hope it with high auc score. :-)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score, accuracy_score\ndef roc_auc_score_FIXED(y_true, y_pred):\n    if len(np.unique(y_true)) == 1: # bug in roc_auc_score\n        return accuracy_score(y_true, np.rint(y_pred))\n    return roc_auc_score(y_true, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_zero = train.loc[train.label==0].sample(350)\ntrain_one = train.loc[train.label==1].sample(350)\ntrain_downsample = pd.concat([train_zero,train_one],ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel = resnet34(3, 1).to(device)\ncriterion = nn.BCELoss(reduce=True)\noptimizer = optim.Adam(model.parameters(),lr = 1e-5)\nlearner = Learner(train_downsample)\nlearner.fit(epochs=20,batch_size=15,shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(2, 3, figsize=(15, 5))\naxs[0,0].plot(train_loss)\naxs[0,0].set_title('Train Loss')\naxs[0,1].plot(train_acc)\naxs[0,1].set_title('Train acc')\naxs[0,2].plot(train_auc)\naxs[0,2].set_title('Train auc')\naxs[1,0].plot(valid_loss)\naxs[1,0].set_title('Valid Loss')\naxs[1,1].plot(valid_acc)\naxs[1,1].set_title('Valid acc')\naxs[1,2].plot(valid_auc)\naxs[1,2].set_title('Valid auc')\nfig.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv(os.path.join(path,'sample_submission.csv'))\ntest_set = TumorDataset(test,kind='test_no_label')\ntest_loader = torch.utils.data.DataLoader(test_set,batch_size=300,shuffle=False)\nprediction = []\nmodel.eval()\nwith torch.no_grad():\n    for idx, (inputs) in tqdm(enumerate(test_loader),total=len(test_loader)):\n        inputs.to(device)\n        outputs = model(inputs.float())#.cuda())\n        preds = outputs.detach().cpu().numpy()\n        prediction.append(preds)\n        \ndel model\nprediction = np.hstack(np.vstack(prediction))\n\ntest['label'] = prediction\ntest.to_csv('submission.csv', index=False)\n\ntest_set = TumorDataset(test,kind='test')\ntest_loader = torch.utils.data.DataLoader(test_set,batch_size=16,shuffle=True)\n\na = next(iter(test_loader))\nfig,ax = plt.subplots(4,4, figsize=(10,10))\n\nfor i in range(16):\n    j = i//4\n    k = i%4\n    ax[j,k].imshow(a[0][i].permute(2,1,0))\n    ax[j,k].set_title('label : {:.4f}'.format(a[1][i]))\nfig.tight_layout()\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}