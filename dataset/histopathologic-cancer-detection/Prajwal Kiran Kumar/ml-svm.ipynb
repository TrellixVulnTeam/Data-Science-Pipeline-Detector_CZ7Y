{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import cv2\nimport numpy as np\nimport pandas as pd\nimport mahotas\nimport h5py\nfrom sklearn.preprocessing import MinMaxScaler\ntraindf=pd.read_csv(\"../input/train_labels.csv\",dtype=str)\ntestdf=pd.read_csv(\"../input/sample_submission.csv\",dtype=str)\ncombineddf = pd.concat([traindf,testdf])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"def fd_hu_moments(image):\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    feature = cv2.HuMoments(cv2.moments(image)).flatten()\n    return feature\ndef fd_haralick(image):    # convert the image to grayscale\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    # compute the haralick texture feature vector\n    haralick = mahotas.features.haralick(gray).mean(axis=0)\n    return haralick\n \ndef fd_histogram(image, mask=None):\n    # convert the image to HSV color-space\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n    bins = 256\n    # compute the color histogram\n    hist  = cv2.calcHist([image], [0, 1, 2], None, [bins, bins, bins], [0, 256, 0, 256, 0, 256])\n    # normalize the histogram\n    cv2.normalize(hist, hist)\n    hist= hist.flatten()\n    return hist","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"global_features = []\nfor i in range(len(traindf)):\n    image = cv2.imread(\"../input/train/\"+traindf.id[i]+\".tif\")\n    global_feature = np.hstack([fd_histogram(image), fd_haralick(image), fd_hu_moments(image)])\n    scaler = MinMaxScaler(feature_range=(0, 1))\n    global_features.append(global_feature)\nfor i in range(len(testdf)):\n    image = cv2.imread(\"../input/test/\"+testdf.id[i]+\".tif\")\n    global_feature = np.hstack([fd_histogram(image), fd_haralick(image), fd_hu_moments(image)])\n    scaler = MinMaxScaler(feature_range=(0, 1))\n    global_features.append(global_feature)\n#Normalize The feature vectors...\nrescaled_features = scaler.fit_transform(global_features)\ntarget = list(traindf.label)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"h5f_data = h5py.File('data.h5', 'w')\nh5f_data.create_dataset('dataset_1', data=np.array(rescaled_features))\n\nh5f_label = h5py.File('labels.h5', 'w')\nh5f_label.create_dataset('dataset_1', data=np.array(target))\n\nh5f_data.close()\nh5f_label.close()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"models = []\nmodels.append(('SVM', SVC(random_state=9)))\n# variables to hold the results and names\nresults = []\nnames = []\nscoring = \"accuracy\"\n\n# import the feature vector and trained labels\nh5f_data = h5py.File('data.h5', 'r')\nh5f_label = h5py.File('labels.h5', 'r')\n\nglobal_features_string = h5f_data['dataset_1']\nglobal_labels_string = h5f_label['dataset_1']\n\nglobal_features = np.array(global_features_string)\nglobal_labels = np.array(global_labels_string)\n\nh5f_data.close()\nh5f_label.close()\n"},{"metadata":{},"cell_type":"markdown","source":"from sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nfrom sklearn.svm import SVC"},{"metadata":{},"cell_type":"markdown","source":"test_size = 0.1\n# split the training and testing data\n(trainDataGlobal, testDataGlobal, trainLabelsGlobal, testLabelsGlobal) = train_test_split(np.array(global_features),\n                                                                                          np.array(global_labels),\n                                                                                          test_size=test_size,\n                                                                                          random_state=seed)"},{"metadata":{},"cell_type":"markdown","source":"# 10-fold cross validation\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=7)\n    cv_results = cross_val_score(model, trainDataGlobal, trainLabelsGlobal, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}