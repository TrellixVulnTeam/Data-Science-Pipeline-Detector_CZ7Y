{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#beluga\n#!/usr/bin/env python\n# coding: utf-8\n\n# # COVID-19 W2: A few charts and a simple baseline\n# \n# \n# # Summary\n# \n# **Disclaimer** We still have limited data to predict or understand what will happen in the next few weeks (months).\n# \n# At this point I see more value in collecting data and monitoring the outbreak than trying to predict the future.\n# \n# [Please don't kill yourself because I published a notebook](https://www.reddit.com/r/datascience/comments/fsfdn2/the_best_thing_you_can_do_to_fight_covid19_is/)\n# \n# \n# \n# ### Challenges\n#  * The outbreak patterns vary a lot among countries\n#  * Most countries have only 2 weeks data\n#  * Only a handful countries managed to succesfuly slow down the outbreak\n#  * Almost every country had several serious regulations in recent weeks\n#  * Increasing testing capacity could have serious impact on confirmed cases\n# \n# \n# \n#  ### Assumptions\n#   * As we are still in the early period, we will see exponential growth in the next few weeks\n#   * Thanks to the panic/awareness/regulations/social distancing the exponential increase will slow down\n# \n# As the process is not stationary at all I decided to use a simple heuristic approach. Maybe I will import sklearn next week.\n#   \n#   ### TIL\n#  * Namibia's country code is NA. Now I remember I heard it in joke before, but I had to investigate a bug learn it again :)\n#  * I haven't used plotly recently, I quite enjoyed the \"new\" Plotly Express interface\n# \n\n# In[1]:\n\n\nget_ipython().run_line_magic('matplotlib', 'inline')\nimport pandas as pd\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\npd.set_option('display.max_columns', 99)\npd.set_option('display.max_rows', 99)\nimport os\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom tqdm import tqdm\nimport datetime as dt\n\n\n# In[2]:\n\n\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = [16, 10]\nplt.rcParams['font.size'] = 14\nimport seaborn as sns\nsns.set_palette(sns.color_palette('tab20', 20))\n\nimport plotly.express as px\nimport plotly.graph_objects as go\n\n\n# In[3]:\n\n\nCOMP = '../input/covid19-global-forecasting-week-3'\nDATEFORMAT = '%Y-%m-%d'\n\n\ndef get_comp_data(COMP):\n    train = pd.read_csv(f'{COMP}/train.csv')\n    test = pd.read_csv(f'{COMP}/test.csv')\n    submission = pd.read_csv(f'{COMP}/submission.csv')\n    print(train.shape, test.shape, submission.shape)\n    train['Country_Region'] = train['Country_Region'].str.replace(',', '')\n    test['Country_Region'] = test['Country_Region'].str.replace(',', '')\n\n    train['Location'] = train['Country_Region'] + '-' + train['Province_State'].fillna('')\n\n    test['Location'] = test['Country_Region'] + '-' + test['Province_State'].fillna('')\n\n    train['LogConfirmed'] = to_log(train.ConfirmedCases)\n    train['LogFatalities'] = to_log(train.Fatalities)\n    train = train.drop(columns=['Province_State'])\n    test = test.drop(columns=['Province_State'])\n\n    country_codes = pd.read_csv('../input/covid19-metadata/country_codes.csv', keep_default_na=False)\n    train = train.merge(country_codes, on='Country_Region', how='left')\n    test = test.merge(country_codes, on='Country_Region', how='left')\n\n    train['DateTime'] = pd.to_datetime(train['Date'])\n    test['DateTime'] = pd.to_datetime(test['Date'])\n    \n    return train, test, submission\n\n\ndef process_each_location(df):\n    dfs = []\n    for loc, df in tqdm(df.groupby('Location')):\n        df = df.sort_values(by='Date')\n        df['Fatalities'] = df['Fatalities'].cummax()\n        df['ConfirmedCases'] = df['ConfirmedCases'].cummax()\n        df['LogFatalities'] = df['LogFatalities'].cummax()\n        df['LogConfirmed'] = df['LogConfirmed'].cummax()\n        df['LogConfirmedNextDay'] = df['LogConfirmed'].shift(-1)\n        df['ConfirmedNextDay'] = df['ConfirmedCases'].shift(-1)\n        df['DateNextDay'] = df['Date'].shift(-1)\n        df['LogFatalitiesNextDay'] = df['LogFatalities'].shift(-1)\n        df['FatalitiesNextDay'] = df['Fatalities'].shift(-1)\n        df['LogConfirmedDelta'] = df['LogConfirmedNextDay'] - df['LogConfirmed']\n        df['ConfirmedDelta'] = df['ConfirmedNextDay'] - df['ConfirmedCases']\n        df['LogFatalitiesDelta'] = df['LogFatalitiesNextDay'] - df['LogFatalities']\n        df['FatalitiesDelta'] = df['FatalitiesNextDay'] - df['Fatalities']\n        dfs.append(df)\n    return pd.concat(dfs)\n\n\ndef add_days(d, k):\n    return dt.datetime.strptime(d, DATEFORMAT) + dt.timedelta(days=k)\n\n\ndef to_log(x):\n    return np.log(x + 1)\n\n\ndef to_exp(x):\n    return np.exp(x) - 1\n\n\n# In[4]:\n\n\nstart = dt.datetime.now()\ntrain, test, submission = get_comp_data(COMP)\ntrain.shape, test.shape, submission.shape\ntrain.head(2)\ntest.head(2)\n\n\n# In[5]:\n\n\ntrain.describe()\ntrain.nunique()\ntrain.dtypes\ntrain.count()\n\nTRAIN_START = train.Date.min()\nTEST_START = test.Date.min()\nTRAIN_END = train.Date.max()\nTEST_END = test.Date.max()\nTRAIN_START, TRAIN_END, TEST_START, TEST_END\n\n\n# # Worldwide\n\n# In[6]:\n\n\ntrain = train.sort_values(by='Date')\ncountries_latest_state = train[train['Date'] == TRAIN_END].groupby([\n    'Country_Region', 'continent', 'geo_region', 'country_iso_code_3']).sum()[[\n    'ConfirmedCases', 'Fatalities']].reset_index()\ncountries_latest_state['Log10Confirmed'] = np.log10(countries_latest_state.ConfirmedCases + 1)\ncountries_latest_state['Log10Fatalities'] = np.log10(countries_latest_state.Fatalities + 1)\ncountries_latest_state = countries_latest_state.sort_values(by='Fatalities', ascending=False)\ncountries_latest_state.to_csv('countries_latest_state.csv', index=False)\n\ncountries_latest_state.shape\ncountries_latest_state.head()\n\n\n# In[7]:\n\n\nfig = go.Figure(data=go.Choropleth(\n    locations = countries_latest_state['country_iso_code_3'],\n    z = countries_latest_state['Log10Confirmed'],\n    text = countries_latest_state['Country_Region'],\n    colorscale = 'viridis_r',\n    autocolorscale=False,\n    reversescale=False,\n    marker_line_color='darkgray',\n    marker_line_width=0.5,\n    colorbar_tickprefix = '10^',\n    colorbar_title = 'Confirmed cases <br>(log10 scale)',\n))\n\n_ = fig.update_layout(\n    title_text=f'COVID-19 Global Cases [Updated: {TRAIN_END}]',\n    geo=dict(\n        showframe=False,\n        showcoastlines=False,\n        projection_type='equirectangular'\n    )\n)\n\nfig.show()\n\n\n# In[8]:\n\n\nfig = go.Figure(data=go.Choropleth(\n    locations = countries_latest_state['country_iso_code_3'],\n    z = countries_latest_state['Log10Fatalities'],\n    text = countries_latest_state['Country_Region'],\n    colorscale = 'viridis_r',\n    autocolorscale=False,\n    reversescale=False,\n    marker_line_color='darkgray',\n    marker_line_width=0.5,\n    colorbar_tickprefix = '10^',\n    colorbar_title = 'Deaths <br>(log10 scale)',\n))\n\n_ = fig.update_layout(\n    title_text=f'COVID-19 Global Deaths [Updated: {TRAIN_END}]',\n    geo=dict(\n        showframe=False,\n        showcoastlines=False,\n        projection_type='equirectangular'\n    )\n)\n\nfig.show()\n\n\n# In[9]:\n\n\ncountries_latest_state['DeathConfirmedRatio'] = (countries_latest_state.Fatalities + 1) / (countries_latest_state.ConfirmedCases + 1)\ncountries_latest_state['DeathConfirmedRatio'] = countries_latest_state['DeathConfirmedRatio'].clip(0, 0.1) \nfig = px.scatter(countries_latest_state,\n                 x='ConfirmedCases',\n                 y='Fatalities',\n                 color='DeathConfirmedRatio',\n                 size='Log10Fatalities',\n                 size_max=20,\n                 hover_name='Country_Region',\n                 color_continuous_scale='viridis_r'\n)\n_ = fig.update_layout(\n    title_text=f'COVID-19 Deaths vs Confirmed Cases by Country [Updated: {TRAIN_END}]',\n    xaxis_type=\"log\",\n    yaxis_type=\"log\"\n)\nfig.show()\n\n\n# In[10]:\n\n\n# The source dataset is not necessary cumulative we will force it\nlatest_loc = train[train['Date'] == TRAIN_END][['Location', 'ConfirmedCases', 'Fatalities']]\nmax_loc = train.groupby(['Location'])[['ConfirmedCases', 'Fatalities']].max().reset_index()\ncheck = pd.merge(latest_loc, max_loc, on='Location')\nnp.mean(check.ConfirmedCases_x == check.ConfirmedCases_y)\nnp.mean(check.Fatalities_x == check.Fatalities_y)\ncheck[check.Fatalities_x != check.Fatalities_y]\ncheck[check.ConfirmedCases_x != check.ConfirmedCases_y]\n\n\n# In[11]:\n\n\ntrain_clean = process_each_location(train)\n\ntrain_clean.shape\ntrain_clean.tail()\n\n\n# # Continents\n\n# In[12]:\n\n\nregional_progress = train_clean.groupby(['DateTime', 'continent']).sum()[['ConfirmedCases', 'Fatalities']].reset_index()\nregional_progress['Log10Confirmed'] = np.log10(regional_progress.ConfirmedCases + 1)\nregional_progress['Log10Fatalities'] = np.log10(regional_progress.Fatalities + 1)\nregional_progress = regional_progress[regional_progress.continent != '#N/A']\n\n\n# In[13]:\n\n\nfig = px.area(regional_progress, x=\"DateTime\", y=\"ConfirmedCases\", color=\"continent\")\n_ = fig.update_layout(\n    title_text=f'COVID-19 Cumulative Confirmed Cases by Continent [Updated: {TRAIN_END}]'\n)\nfig.show()\nfig2 = px.line(regional_progress, x='DateTime', y='ConfirmedCases', color='continent')\n_ = fig2.update_layout(\n    yaxis_type=\"log\",\n    title_text=f'COVID-19 Cumulative Confirmed Cases by Continent [Updated: {TRAIN_END}]'\n)\nfig2.show()\n\n\n# In[14]:\n\n\nfig = px.area(regional_progress, x=\"DateTime\", y=\"Fatalities\", color=\"continent\")\n_ = fig.update_layout(\n    title_text=f'COVID-19 Cumulative Confirmed Deaths by Continent [Updated: {TRAIN_END}]'\n)\nfig.show()\nfig2 = px.line(regional_progress, x='DateTime', y='Fatalities', color='continent')\n_ = fig2.update_layout(\n    yaxis_type=\"log\",\n    title_text=f'COVID-19 Cumulative Confirmed Deaths by Continent [Updated: {TRAIN_END}]'\n)\nfig2.show()\n\n\n# In[15]:\n\n\nchina = train_clean[train_clean.Location.str.startswith('China')]\ntop10_locations = china.groupby('Location')[['ConfirmedCases']].max().sort_values(\n    by='ConfirmedCases', ascending=False).reset_index().Location.values[:10]\nfig2 = px.line(china[china.Location.isin(top10_locations)], x='DateTime', y='ConfirmedCases', color='Location')\n_ = fig2.update_layout(\n    yaxis_type=\"log\",\n    title_text=f'COVID-19 Cumulative Confirmed Cases in China [Updated: {TRAIN_END}]'\n)\nfig2.show()\n\n\n# In[16]:\n\n\neurope = train_clean[train_clean.continent == 'Europe']\ntop10_locations = europe.groupby('Location')[['ConfirmedCases']].max().sort_values(\n    by='ConfirmedCases', ascending=False).reset_index().Location.values[:10]\nfig2 = px.line(europe[europe.Location.isin(top10_locations)], x='DateTime', y='ConfirmedCases', color='Location')\n_ = fig2.update_layout(\n    yaxis_type=\"log\",\n    title_text=f'COVID-19 Cumulative Confirmed Cases in Europe [Updated: {TRAIN_END}]'\n)\nfig2.show()\n\n\n# In[17]:\n\n\nus = train_clean[train_clean.Country_Region == 'US']\ntop10_locations = us.groupby('Location')[['ConfirmedCases']].max().sort_values(\n    by='ConfirmedCases', ascending=False).reset_index().Location.values[:10]\nfig2 = px.line(us[us.Location.isin(top10_locations)], x='DateTime', y='ConfirmedCases', color='Location')\n_ = fig2.update_layout(\n    yaxis_type=\"log\",\n    title_text=f'COVID-19 Cumulative Confirmed Cases in the USA [Updated: {TRAIN_END}]'\n)\nfig2.show()\n\n\n# In[18]:\n\n\nafrica = train_clean[train_clean.continent == 'Africa']\ntop10_locations = africa.groupby('Location')[['ConfirmedCases']].max().sort_values(\n    by='ConfirmedCases', ascending=False).reset_index().Location.values[:10]\nfig2 = px.line(africa[africa.Location.isin(top10_locations)], x='DateTime', y='ConfirmedCases', color='Location')\n_ = fig2.update_layout(\n    yaxis_type=\"log\",\n    title_text=f'COVID-19 Cumulative Confirmed Cases in Africa [Updated: {TRAIN_END}]'\n)\nfig2.show()\n\n\n# # Countries\n\n# In[19]:\n\n\ncountry_progress = train_clean.groupby(['Date', 'DateTime', 'Country_Region']).sum()[[\n    'ConfirmedCases', 'Fatalities', 'ConfirmedDelta', 'FatalitiesDelta']].reset_index()\ntop10_countries = country_progress.groupby('Country_Region')[['Fatalities']].max().sort_values(\n    by='Fatalities', ascending=False).reset_index().Country_Region.values[:10]\n\nfig2 = px.line(country_progress[country_progress.Country_Region.isin(top10_countries)],\n               x='DateTime', y='ConfirmedCases', color='Country_Region')\n_ = fig2.update_layout(\n    yaxis_type=\"log\",\n    title_text=f'COVID-19 Cumulative Confirmed Cases by Country [Updated: {TRAIN_END}]'\n)\nfig2.show()\nfig3 = px.line(country_progress[country_progress.Country_Region.isin(top10_countries)],\n               x='DateTime', y='Fatalities', color='Country_Region')\n_ = fig3.update_layout(\n    yaxis_type=\"log\",\n    title_text=f'COVID-19 Cumulative Deaths by Country [Updated: {TRAIN_END}]'\n)\nfig3.show()\n\n\n# # Outbreak during March\n\n# In[20]:\n\n\ncountries_0301 = country_progress[country_progress.Date == '2020-03-01'][[\n    'Country_Region', 'ConfirmedCases', 'Fatalities']]\ncountries_0331 = country_progress[country_progress.Date == '2020-03-31'][[\n    'Country_Region', 'ConfirmedCases', 'Fatalities']]\ncountries_in_march = pd.merge(countries_0301, countries_0331, on='Country_Region', suffixes=['_0301', '_0331'])\ncountries_in_march['IncreaseInMarch'] = countries_in_march.ConfirmedCases_0331 / (countries_in_march.ConfirmedCases_0301 + 1)\ncountries_in_march = countries_in_march[countries_in_march.ConfirmedCases_0331 > 200].sort_values(\n    by='IncreaseInMarch', ascending=False)\ncountries_in_march.tail(15)\n\n\n# In[21]:\n\n\nselected_countries = [\n    'Italy', 'Vietnam', 'Bahrain', 'Singapore', 'Taiwan*', 'Japan', 'Kuwait', 'Korea, South', 'China']\nfig2 = px.line(country_progress[country_progress.Country_Region.isin(selected_countries)],\n               x='DateTime', y='ConfirmedCases', color='Country_Region')\n_ = fig2.update_layout(\n    yaxis_type=\"log\",\n    title_text=f'COVID-19 Cumulative Confirmed Cases by Country [Updated: {TRAIN_END}]'\n)\nfig2.show()\nfig3 = px.line(country_progress[country_progress.Country_Region.isin(selected_countries)],\n               x='DateTime', y='Fatalities', color='Country_Region')\n_ = fig3.update_layout(\n    yaxis_type=\"log\",\n    title_text=f'COVID-19 Cumulative Deaths by Country [Updated: {TRAIN_END}]'\n)\nfig3.show()\n\n\n# In[22]:\n\n\nlist(train_clean.Date)\n\n\n# In[23]:\n\n\ntrain_clean['Geo#Country#Contintent'] = train_clean.Location + '#' + train_clean.Country_Region + '#' + train_clean.continent\nlatest = train_clean[train_clean.Date == '2020-04-06'][[\n    'Geo#Country#Contintent', 'ConfirmedCases', 'Fatalities', 'LogConfirmed', 'LogFatalities']]\n# daily_confirmed_deltas = train_clean[train_clean.Date >= '2020-03-17'].pivot(\n#     'Geo#Country#Contintent', 'Date', 'LogConfirmedDelta').round(3).reset_index()\n# daily_confirmed_deltas = latest.merge(daily_confirmed_deltas, on='Geo#Country#Contintent')\n# daily_confirmed_deltas.shape\n# daily_confirmed_deltas.head()\n# daily_confirmed_deltas.to_csv('daily_confirmed_deltas.csv', index=False)\n\n\n# In[24]:\n\n\ndeltas = train_clean[np.logical_and(\n        train_clean.LogConfirmed > 2,\n        ~train_clean.Location.str.startswith('China')\n)].dropna().sort_values(by='LogConfirmedDelta', ascending=False)\n\ndeltas['start'] = deltas['LogConfirmed'].round(0)\nconfirmed_deltas = pd.concat([\n    deltas.groupby('start')[['LogConfirmedDelta']].mean(),\n    deltas.groupby('start')[['LogConfirmedDelta']].std(),\n    deltas.groupby('start')[['LogConfirmedDelta']].count()\n], axis=1)\n\ndeltas.mean()\n\nconfirmed_deltas.columns = ['avg', 'std', 'cnt']\nconfirmed_deltas\nconfirmed_deltas.to_csv('confirmed_deltas.csv')\n\n\n# In[25]:\n\n\nfig = px.box(deltas,  x=\"start\", y=\"LogConfirmedDelta\", range_y=[0, 0.35])\nfig.show()\n\n\n# In[26]:\n\n\nfig = px.box(deltas[deltas.Date >= '2020-03-01'],  x=\"DateTime\", y=\"LogConfirmedDelta\", range_y=[0, 0.6])\nfig.show()\n\n\n# In[27]:\n\n\ndeltas = train_clean[np.logical_and(\n        train_clean.LogConfirmed > 0,\n        ~train_clean.Location.str.startswith('China')\n)].dropna().sort_values(by='LogConfirmedDelta', ascending=False)\ndeltas = deltas[deltas['Date'] >= '2020-03-12']\n\nconfirmed_deltas = pd.concat([\n    deltas.groupby('Location')[['LogConfirmedDelta']].mean(),\n    deltas.groupby('Location')[['LogConfirmedDelta']].std(),\n    deltas.groupby('Location')[['LogConfirmedDelta']].count(),\n    deltas.groupby('Location')[['LogConfirmed']].max()\n], axis=1)\nconfirmed_deltas.columns = ['avg', 'std', 'cnt', 'max']\n\nconfirmed_deltas.sort_values(by='avg').head(10)\nconfirmed_deltas.sort_values(by='avg').tail(10)\nconfirmed_deltas.to_csv('confirmed_deltas.csv')\n\n\n# # Create prediction\n\n# In[28]:\n\n\nDECAY = 0.93\nDECAY ** 7, DECAY ** 14, DECAY ** 21, DECAY ** 28\n\nconfirmed_deltas = train.groupby(['Location', 'Country_Region', 'continent'])[[\n    'Id']].count().reset_index()\n\nGLOBAL_DELTA = 0.11\nconfirmed_deltas['DELTA'] = GLOBAL_DELTA\n\nconfirmed_deltas.loc[confirmed_deltas.continent=='Africa', 'DELTA'] = 0.14\nconfirmed_deltas.loc[confirmed_deltas.continent=='Oceania', 'DELTA'] = 0.06\nconfirmed_deltas.loc[confirmed_deltas.Country_Region=='Korea South', 'DELTA'] = 0.011\nconfirmed_deltas.loc[confirmed_deltas.Country_Region=='US', 'DELTA'] = 0.15\nconfirmed_deltas.loc[confirmed_deltas.Country_Region=='China', 'DELTA'] = 0.01\nconfirmed_deltas.loc[confirmed_deltas.Country_Region=='Japan', 'DELTA'] = 0.05\nconfirmed_deltas.loc[confirmed_deltas.Country_Region=='Singapore', 'DELTA'] = 0.05\nconfirmed_deltas.loc[confirmed_deltas.Country_Region=='Taiwan*', 'DELTA'] = 0.05\nconfirmed_deltas.loc[confirmed_deltas.Country_Region=='Switzerland', 'DELTA'] = 0.05\nconfirmed_deltas.loc[confirmed_deltas.Country_Region=='Norway', 'DELTA'] = 0.05\nconfirmed_deltas.loc[confirmed_deltas.Country_Region=='Iceland', 'DELTA'] = 0.05\nconfirmed_deltas.loc[confirmed_deltas.Country_Region=='Austria', 'DELTA'] = 0.06\nconfirmed_deltas.loc[confirmed_deltas.Country_Region=='Italy', 'DELTA'] = 0.04\nconfirmed_deltas.loc[confirmed_deltas.Country_Region=='Spain', 'DELTA'] = 0.08\nconfirmed_deltas.loc[confirmed_deltas.Country_Region=='Portugal', 'DELTA'] = 0.12\nconfirmed_deltas.loc[confirmed_deltas.Country_Region=='Israel', 'DELTA'] = 0.12\nconfirmed_deltas.loc[confirmed_deltas.Country_Region=='Iran', 'DELTA'] = 0.08\nconfirmed_deltas.loc[confirmed_deltas.Country_Region=='Germany', 'DELTA'] = 0.07\nconfirmed_deltas.loc[confirmed_deltas.Country_Region=='Malaysia', 'DELTA'] = 0.06\nconfirmed_deltas.loc[confirmed_deltas.Country_Region=='Russia', 'DELTA'] = 0.18\nconfirmed_deltas.loc[confirmed_deltas.Country_Region=='Ukraine', 'DELTA'] = 0.18\nconfirmed_deltas.loc[confirmed_deltas.Country_Region=='Brazil', 'DELTA'] = 0.12\nconfirmed_deltas.loc[confirmed_deltas.Country_Region=='Turkey', 'DELTA'] = 0.18\nconfirmed_deltas.loc[confirmed_deltas.Country_Region=='Philippines', 'DELTA'] = 0.18\nconfirmed_deltas.loc[confirmed_deltas.Location=='France-', 'DELTA'] = 0.1\nconfirmed_deltas.loc[confirmed_deltas.Location=='United Kingdom-', 'DELTA'] = 0.12\nconfirmed_deltas.loc[confirmed_deltas.Location=='Diamond Princess-', 'DELTA'] = 0.00\nconfirmed_deltas.loc[confirmed_deltas.Location=='China-Hong Kong', 'DELTA'] = 0.08\nconfirmed_deltas.loc[confirmed_deltas.Location=='San Marino-', 'DELTA'] = 0.03\n\n\nconfirmed_deltas.shape, confirmed_deltas.DELTA.mean()\n\nconfirmed_deltas[confirmed_deltas.DELTA != GLOBAL_DELTA].shape, confirmed_deltas[confirmed_deltas.DELTA != GLOBAL_DELTA].DELTA.mean()\nconfirmed_deltas[confirmed_deltas.DELTA != GLOBAL_DELTA]\nconfirmed_deltas.describe()\n\n\n# In[29]:\n\n\ndaily_log_confirmed = train_clean.pivot('Location', 'Date', 'LogConfirmed').reset_index()\ndaily_log_confirmed = daily_log_confirmed.sort_values(TRAIN_END, ascending=False)\ndaily_log_confirmed.to_csv('daily_log_confirmed.csv', index=False)\n\nfor i, d in tqdm(enumerate(pd.date_range(add_days(TRAIN_END, 1), add_days(TEST_END, 1)))):\n    new_day = str(d).split(' ')[0]\n    last_day = dt.datetime.strptime(new_day, DATEFORMAT) - dt.timedelta(days=1)\n    last_day = last_day.strftime(DATEFORMAT)\n    for loc in confirmed_deltas.Location.values:\n        confirmed_delta = confirmed_deltas.loc[confirmed_deltas.Location == loc, 'DELTA'].values[0]\n        daily_log_confirmed.loc[daily_log_confirmed.Location == loc, new_day] = daily_log_confirmed.loc[daily_log_confirmed.Location == loc, last_day] +             confirmed_delta * DECAY ** i\n\n\n# In[30]:\n\n\ndaily_log_confirmed.head()\n\n\n# In[31]:\n\n\nconfirmed_prediciton = pd.melt(daily_log_confirmed[:25], id_vars='Location')\nconfirmed_prediciton['ConfirmedCases'] = to_exp(confirmed_prediciton['value'])\nfig2 = px.line(confirmed_prediciton,\n               x='Date', y='ConfirmedCases', color='Location')\n_ = fig2.update_layout(\n    yaxis_type=\"log\",\n    title_text=f'COVID-19 Cumulative Confirmed Cases Prediction [Updated: {TRAIN_END}]'\n)\nfig2.show()\n\n\n# # Fatalities\n\n# In[32]:\n\n\ntrain_clean['Geo#Country#Contintent'] = train_clean.Location + '#' + train_clean.Country_Region + '#' + train_clean.continent\nlatest = train_clean[train_clean.Date == TRAIN_END][[\n    'Geo#Country#Contintent', 'ConfirmedCases', 'Fatalities', 'LogConfirmed', 'LogFatalities']]\n# daily_death_deltas = train_clean[train_clean.Date >= '2020-03-24'].pivot(\n#     'Geo#Country#Contintent', 'Date', 'LogFatalitiesDelta').round(3).reset_index()\n# daily_death_deltas = latest.merge(daily_death_deltas, on='Geo#Country#Contintent')\n# daily_death_deltas.shape\n# daily_death_deltas.head()\n# daily_death_deltas.to_csv('daily_death_deltas.csv', index=False)\n\n\n# In[33]:\n\n\ndeath_deltas = train.groupby(['Location', 'Country_Region', 'continent'])[[\n    'Id']].count().reset_index()\n\nGLOBAL_DELTA = 0.11\ndeath_deltas['DELTA'] = GLOBAL_DELTA\n\ndeath_deltas.loc[death_deltas.Country_Region=='China', 'DELTA'] = 0.005\ndeath_deltas.loc[death_deltas.continent=='Oceania', 'DELTA'] = 0.08\ndeath_deltas.loc[death_deltas.Country_Region=='Korea South', 'DELTA'] = 0.04\ndeath_deltas.loc[death_deltas.Country_Region=='Japan', 'DELTA'] = 0.04\ndeath_deltas.loc[death_deltas.Country_Region=='Singapore', 'DELTA'] = 0.05\ndeath_deltas.loc[death_deltas.Country_Region=='Taiwan*', 'DELTA'] = 0.06\n\n\n\ndeath_deltas.loc[death_deltas.Country_Region=='US', 'DELTA'] = 0.17\n\ndeath_deltas.loc[death_deltas.Country_Region=='Switzerland', 'DELTA'] = 0.15\ndeath_deltas.loc[death_deltas.Country_Region=='Norway', 'DELTA'] = 0.15\ndeath_deltas.loc[death_deltas.Country_Region=='Iceland', 'DELTA'] = 0.01\ndeath_deltas.loc[death_deltas.Country_Region=='Austria', 'DELTA'] = 0.14\ndeath_deltas.loc[death_deltas.Country_Region=='Italy', 'DELTA'] = 0.07\ndeath_deltas.loc[death_deltas.Country_Region=='Spain', 'DELTA'] = 0.1\ndeath_deltas.loc[death_deltas.Country_Region=='Portugal', 'DELTA'] = 0.13\ndeath_deltas.loc[death_deltas.Country_Region=='Israel', 'DELTA'] = 0.16\ndeath_deltas.loc[death_deltas.Country_Region=='Iran', 'DELTA'] = 0.06\ndeath_deltas.loc[death_deltas.Country_Region=='Germany', 'DELTA'] = 0.14\ndeath_deltas.loc[death_deltas.Country_Region=='Malaysia', 'DELTA'] = 0.14\ndeath_deltas.loc[death_deltas.Country_Region=='Russia', 'DELTA'] = 0.2\ndeath_deltas.loc[death_deltas.Country_Region=='Ukraine', 'DELTA'] = 0.2\ndeath_deltas.loc[death_deltas.Country_Region=='Brazil', 'DELTA'] = 0.2\ndeath_deltas.loc[death_deltas.Country_Region=='Turkey', 'DELTA'] = 0.22\ndeath_deltas.loc[death_deltas.Country_Region=='Philippines', 'DELTA'] = 0.12\ndeath_deltas.loc[death_deltas.Location=='France-', 'DELTA'] = 0.14\ndeath_deltas.loc[death_deltas.Location=='United Kingdom-', 'DELTA'] = 0.14\ndeath_deltas.loc[death_deltas.Location=='Diamond Princess-', 'DELTA'] = 0.00\n\ndeath_deltas.loc[death_deltas.Location=='China-Hong Kong', 'DELTA'] = 0.01\ndeath_deltas.loc[death_deltas.Location=='San Marino-', 'DELTA'] = 0.05\n\n\ndeath_deltas.shape\ndeath_deltas.DELTA.mean()\n\ndeath_deltas[death_deltas.DELTA != GLOBAL_DELTA].shape\ndeath_deltas[death_deltas.DELTA != GLOBAL_DELTA].DELTA.mean()\ndeath_deltas[death_deltas.DELTA != GLOBAL_DELTA]\ndeath_deltas.describe()\n\n\n# In[34]:\n\n\ndaily_log_deaths = train_clean.pivot('Location', 'Date', 'LogFatalities').reset_index()\ndaily_log_deaths = daily_log_deaths.sort_values(TRAIN_END, ascending=False)\ndaily_log_deaths.to_csv('daily_log_deaths.csv', index=False)\n\nfor i, d in tqdm(enumerate(pd.date_range(add_days(TRAIN_END, 1), add_days(TEST_END, 1)))):\n    new_day = str(d).split(' ')[0]\n    last_day = dt.datetime.strptime(new_day, DATEFORMAT) - dt.timedelta(days=1)\n    last_day = last_day.strftime(DATEFORMAT)\n    for loc in death_deltas.Location:\n        death_delta = death_deltas.loc[death_deltas.Location == loc, 'DELTA'].values[0]\n        daily_log_deaths.loc[daily_log_deaths.Location == loc, new_day] = daily_log_deaths.loc[daily_log_deaths.Location == loc, last_day] +             death_delta * DECAY ** i\n\n\n# In[35]:\n\n\nconfirmed_prediciton = pd.melt(daily_log_deaths[:25], id_vars='Location')\nconfirmed_prediciton['Fatalities'] = to_exp(confirmed_prediciton['value'])\nfig2 = px.line(confirmed_prediciton,\n               x='Date', y='Fatalities', color='Location')\n_ = fig2.update_layout(\n    yaxis_type=\"log\",\n    title_text=f'COVID-19 Cumulative Fatalities Prediction [Updated: {TRAIN_END}]'\n)\nfig2.show()\n\n\n# # Create submission file\n\n# In[36]:\n\n\nsubmission.head(2)\n\n\n# In[37]:\n\n\nconfirmed = []\nfatalities = []\nfor id, d, loc in tqdm(test[['ForecastId', 'Date', 'Location']].values):\n    c = to_exp(daily_log_confirmed.loc[daily_log_confirmed.Location == loc, d].values[0])\n    f = to_exp(daily_log_deaths.loc[daily_log_deaths.Location == loc, d].values[0])\n    confirmed.append(c)\n    fatalities.append(f)\n\n\n# In[38]:\n\n\nmy_submission = test.copy()\nmy_submission['ConfirmedCases'] = confirmed\nmy_submission['Fatalities'] = fatalities\n\nmy_submission['ConfirmedCases']=my_submission['ConfirmedCases'].fillna(1)\nmy_submission['Fatalities']=my_submission['Fatalities'].fillna(1)\nmy_submission.shape\nmy_submission.head()\n\n\n# In[39]:\n\n\nmy_submission.groupby('Date').sum().tail()\n\n\n# # Sanity check\n\n# In[40]:\n\n\ntotal = my_submission.groupby('Date')[['ConfirmedCases', 'Fatalities']].sum().reset_index()\n\nfig2 = px.line(pd.melt(total, id_vars=['Date']), x='Date', y='value', color='variable')\n_ = fig2.update_layout(\n    yaxis_type=\"log\",\n    title_text=f'COVID-19 Cumulative Prediction Total [Updated: {TRAIN_END}]'\n)\nfig2.show()\n\n\n# In[41]:\n\n\nmy_submission[[\n    'ForecastId', 'ConfirmedCases', 'Fatalities'\n]].to_csv('sub1.csv', index=False)\nprint(DECAY)\nmy_submission.head()\nmy_submission.tail()\nmy_submission.shape\n\n\n# In[42]:\n\n\nend = dt.datetime.now()\nprint('Finished', end, (end - start).seconds, 's')\nsub1=my_submission[['ForecastId', 'ConfirmedCases', 'Fatalities']].copy()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#vopani\n#!/usr/bin/env python\n# coding: utf-8\n\n# ## Covid-19 Forecasting (Week 2)\n# ![Coronavirusm.jpg](attachment:Coronavirusm.jpg)\n# \n# The [covid-19 virus](https://en.wikipedia.org/wiki/Coronavirus_disease_2019) has created a pandemic in the early months of 2020. Kaggle has hosted a series of forecasting competitions for predict the number the spread of the virus. This is my solution for Week 2 of the series.\n# \n# I am compiling external datasets in the competition format here: https://www.kaggle.com/rohanrao/covid19-forecasting-metadata\n# They are structured such that it can directly be merged with the train and test datasets on Kaggle.\n# \n\n# In[1]:\n\n\n## importing packages\nimport lightgbm as lgb\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n# In[2]:\n\n\n## defining constants\nPATH_TRAIN = \"/kaggle/input/covid19-global-forecasting-week-3/train.csv\"\nPATH_TEST = \"/kaggle/input/covid19-global-forecasting-week-3/test.csv\"\n\nPATH_SUBMISSION = \"submission.csv\"\nPATH_OUTPUT = \"output.csv\"\n\nPATH_REGION_METADATA = \"/kaggle/input/covid19-forecasting-metadata/region_metadata.csv\"\nPATH_REGION_DATE_METADATA = \"/kaggle/input/covid19-forecasting-metadata/region_date_metadata.csv\"\n\nVAL_DAYS = 7\nMAD_FACTOR = 0.5\nDAYS_SINCE_CASES = [1, 10, 50, 100, 500, 1000, 5000, 10000]\n\nSEED = 2357\n\nLGB_PARAMS = {\"objective\": \"regression\",\n              \"num_leaves\": 5,\n              \"learning_rate\": 0.013,\n              \"bagging_fraction\": 0.91,\n              \"feature_fraction\": 0.81,\n              \"reg_alpha\": 0.13,\n              \"reg_lambda\": 0.13,\n              \"metric\": \"rmse\",\n              \"seed\": SEED\n             }\n\n\n# ## Prepare Data\n# * Unlike many competitions, there are overlapping dates across train and test data. So it is important to be careful while handling the dates.\n# \n# * Some data issues are fixed using previous max values in case of discrepancy.\n# \n# * External data from [this dataset](https://www.kaggle.com/rohanrao/covid19-forecasting-metadata) is merged.\n\n# In[3]:\n\n\n## reading data\ntrain = pd.read_csv(PATH_TRAIN)\ntest = pd.read_csv(PATH_TEST)\n\nregion_metadata = pd.read_csv(PATH_REGION_METADATA)\nregion_date_metadata = pd.read_csv(PATH_REGION_DATE_METADATA)\n\n\n# In[4]:\n\n\n## preparing data\ntrain = train.merge(test[[\"ForecastId\", \"Province_State\", \"Country_Region\", \"Date\"]], on = [\"Province_State\", \"Country_Region\", \"Date\"], how = \"left\")\ntest = test[~test.Date.isin(train.Date.unique())]\n\ndf_panel = pd.concat([train, test], sort = False)\n\n# combining state and country into 'geography'\ndf_panel[\"geography\"] = df_panel.Country_Region.astype(str) + \": \" + df_panel.Province_State.astype(str)\ndf_panel.loc[df_panel.Province_State.isna(), \"geography\"] = df_panel[df_panel.Province_State.isna()].Country_Region\n\n# fixing data issues with cummax\ndf_panel.ConfirmedCases = df_panel.groupby(\"geography\")[\"ConfirmedCases\"].cummax()\ndf_panel.Fatalities = df_panel.groupby(\"geography\")[\"Fatalities\"].cummax()\n\n# merging external metadata\ndf_panel = df_panel.merge(region_metadata, on = [\"Country_Region\", \"Province_State\"])\ndf_panel = df_panel.merge(region_date_metadata, on = [\"Country_Region\", \"Province_State\", \"Date\"], how = \"left\")\n\n# label encoding continent\ndf_panel.continent = LabelEncoder().fit_transform(df_panel.continent)\ndf_panel.Date = pd.to_datetime(df_panel.Date, format = \"%Y-%m-%d\")\n\ndf_panel.sort_values([\"geography\", \"Date\"], inplace = True)\n\n\n# ## Feature Engineering\n# * Lag features are created.\n# \n# * Several differences, ratios and averages of historic values are created and used.\n\n# In[5]:\n\n\n## feature engineering\nmin_date_train = np.min(df_panel[~df_panel.Id.isna()].Date)\nmax_date_train = np.max(df_panel[~df_panel.Id.isna()].Date)\n\nmin_date_test = np.min(df_panel[~df_panel.ForecastId.isna()].Date)\nmax_date_test = np.max(df_panel[~df_panel.ForecastId.isna()].Date)\n\nn_dates_test = len(df_panel[~df_panel.ForecastId.isna()].Date.unique())\n\nprint(\"Train date range:\", str(min_date_train), \" - \", str(max_date_train))\nprint(\"Test date range:\", str(min_date_test), \" - \", str(max_date_test))\n\n# creating lag features\nfor lag in range(1, 41):\n    df_panel[f\"lag_{lag}_cc\"] = df_panel.groupby(\"geography\")[\"ConfirmedCases\"].shift(lag)\n    df_panel[f\"lag_{lag}_ft\"] = df_panel.groupby(\"geography\")[\"Fatalities\"].shift(lag)\n    df_panel[f\"lag_{lag}_rc\"] = df_panel.groupby(\"geography\")[\"Recoveries\"].shift(lag)\n\nfor case in DAYS_SINCE_CASES:\n    df_panel = df_panel.merge(df_panel[df_panel.ConfirmedCases >= case].groupby(\"geography\")[\"Date\"].min().reset_index().rename(columns = {\"Date\": f\"case_{case}_date\"}), on = \"geography\", how = \"left\")\n\n\n# In[6]:\n\n\n## function for preparing features\ndef prepare_features(df, gap):\n    \n    df[\"perc_1_ac\"] = (df[f\"lag_{gap}_cc\"] - df[f\"lag_{gap}_ft\"] - df[f\"lag_{gap}_rc\"]) / df[f\"lag_{gap}_cc\"]\n    df[\"perc_1_cc\"] = df[f\"lag_{gap}_cc\"] / df.population\n    \n    df[\"diff_1_cc\"] = df[f\"lag_{gap}_cc\"] - df[f\"lag_{gap + 1}_cc\"]\n    df[\"diff_2_cc\"] = df[f\"lag_{gap + 1}_cc\"] - df[f\"lag_{gap + 2}_cc\"]\n    df[\"diff_3_cc\"] = df[f\"lag_{gap + 2}_cc\"] - df[f\"lag_{gap + 3}_cc\"]\n    \n    df[\"diff_1_ft\"] = df[f\"lag_{gap}_ft\"] - df[f\"lag_{gap + 1}_ft\"]\n    df[\"diff_2_ft\"] = df[f\"lag_{gap + 1}_ft\"] - df[f\"lag_{gap + 2}_ft\"]\n    df[\"diff_3_ft\"] = df[f\"lag_{gap + 2}_ft\"] - df[f\"lag_{gap + 3}_ft\"]\n    \n    df[\"diff_123_cc\"] = (df[f\"lag_{gap}_cc\"] - df[f\"lag_{gap + 3}_cc\"]) / 3\n    df[\"diff_123_ft\"] = (df[f\"lag_{gap}_ft\"] - df[f\"lag_{gap + 3}_ft\"]) / 3\n\n    df[\"diff_change_1_cc\"] = df.diff_1_cc / df.diff_2_cc\n    df[\"diff_change_2_cc\"] = df.diff_2_cc / df.diff_3_cc\n    \n    df[\"diff_change_1_ft\"] = df.diff_1_ft / df.diff_2_ft\n    df[\"diff_change_2_ft\"] = df.diff_2_ft / df.diff_3_ft\n\n    df[\"diff_change_12_cc\"] = (df.diff_change_1_cc + df.diff_change_2_cc) / 2\n    df[\"diff_change_12_ft\"] = (df.diff_change_1_ft + df.diff_change_2_ft) / 2\n    \n    df[\"change_1_cc\"] = df[f\"lag_{gap}_cc\"] / df[f\"lag_{gap + 1}_cc\"]\n    df[\"change_2_cc\"] = df[f\"lag_{gap + 1}_cc\"] / df[f\"lag_{gap + 2}_cc\"]\n    df[\"change_3_cc\"] = df[f\"lag_{gap + 2}_cc\"] / df[f\"lag_{gap + 3}_cc\"]\n\n    df[\"change_1_ft\"] = df[f\"lag_{gap}_ft\"] / df[f\"lag_{gap + 1}_ft\"]\n    df[\"change_2_ft\"] = df[f\"lag_{gap + 1}_ft\"] / df[f\"lag_{gap + 2}_ft\"]\n    df[\"change_3_ft\"] = df[f\"lag_{gap + 2}_ft\"] / df[f\"lag_{gap + 3}_ft\"]\n\n    df[\"change_123_cc\"] = df[f\"lag_{gap}_cc\"] / df[f\"lag_{gap + 3}_cc\"]\n    df[\"change_123_ft\"] = df[f\"lag_{gap}_ft\"] / df[f\"lag_{gap + 3}_ft\"]\n    \n    for case in DAYS_SINCE_CASES:\n        df[f\"days_since_{case}_case\"] = (df[f\"case_{case}_date\"] - df.Date).astype(\"timedelta64[D]\")\n        df.loc[df[f\"days_since_{case}_case\"] < gap, f\"days_since_{case}_case\"] = np.nan\n\n    df[\"country_flag\"] = df.Province_State.isna().astype(int)\n    df[\"density\"] = df.population / df.area\n    \n    # target variable is log of change from last known value\n    df[\"target_cc\"] = np.log1p(df.ConfirmedCases) - np.log1p(df[f\"lag_{gap}_cc\"])\n    df[\"target_ft\"] = np.log1p(df.Fatalities) - np.log1p(df[f\"lag_{gap}_ft\"])\n    \n    features = [\n        f\"lag_{gap}_cc\",\n        f\"lag_{gap}_ft\",\n        f\"lag_{gap}_rc\",\n        \"perc_1_ac\",\n        \"perc_1_cc\",\n        \"diff_1_cc\",\n        \"diff_2_cc\",\n        \"diff_3_cc\",\n        \"diff_1_ft\",\n        \"diff_2_ft\",\n        \"diff_3_ft\",\n        \"diff_123_cc\",\n        \"diff_123_ft\",\n        \"diff_change_1_cc\",\n        \"diff_change_2_cc\",\n        \"diff_change_1_ft\",\n        \"diff_change_2_ft\",\n        \"diff_change_12_cc\",\n        \"diff_change_12_ft\",\n        \"change_1_cc\",\n        \"change_2_cc\",\n        \"change_3_cc\",\n        \"change_1_ft\",\n        \"change_2_ft\",\n        \"change_3_ft\",\n        \"change_123_cc\",\n        \"change_123_ft\",\n        \"days_since_1_case\",\n        \"days_since_10_case\",\n        \"days_since_50_case\",\n        \"days_since_100_case\",\n        \"days_since_500_case\",\n        \"days_since_1000_case\",\n        \"days_since_5000_case\",\n        \"days_since_10000_case\",\n        \"country_flag\",\n        \"lat\",\n        \"lon\",\n        \"continent\",\n        \"population\",\n        \"area\",\n        \"density\",\n        \"target_cc\",\n        \"target_ft\"\n    ]\n    \n    return df[features]\n\n\n# ## LGB Model\n# * Note that the target variable used is the change (or difference) in log values of ConfirmedCases / Fatalities from the last known value.\n# \n# * One model is built for each day in the private test data. So that's a total of 28 models.\n# \n# * A single set of parameters with less number of leaves along with regularization is used to control overfitting.\n\n# In[7]:\n\n\n## function for building and predicting using LGBM model\ndef build_predict_lgbm(df_train, df_test, gap):\n    \n    df_train.dropna(subset = [\"target_cc\", \"target_ft\", f\"lag_{gap}_cc\", f\"lag_{gap}_ft\"], inplace = True)\n    \n    target_cc = df_train.target_cc\n    target_ft = df_train.target_ft\n    \n    test_lag_cc = df_test[f\"lag_{gap}_cc\"].values\n    test_lag_ft = df_test[f\"lag_{gap}_ft\"].values\n    \n    df_train.drop([\"target_cc\", \"target_ft\"], axis = 1, inplace = True)\n    df_test.drop([\"target_cc\", \"target_ft\"], axis = 1, inplace = True)\n    \n    categorical_features = [\"continent\"]\n    \n    dtrain_cc = lgb.Dataset(df_train, label = target_cc, categorical_feature = categorical_features)\n    dtrain_ft = lgb.Dataset(df_train, label = target_ft, categorical_feature = categorical_features)\n\n    model_cc = lgb.train(LGB_PARAMS, train_set = dtrain_cc, num_boost_round = 200)\n    model_ft = lgb.train(LGB_PARAMS, train_set = dtrain_ft, num_boost_round = 200)\n    \n    # inverse transform from log of change from last known value\n    y_pred_cc = np.expm1(model_cc.predict(df_test, num_boost_round = 200) + np.log1p(test_lag_cc))\n    y_pred_ft = np.expm1(model_ft.predict(df_test, num_boost_round = 200) + np.log1p(test_lag_ft))\n    \n    return y_pred_cc, y_pred_ft, model_cc, model_ft\n\n\n# ## MAD Model\n# * This Moving Average with Decay (MAD) model is a simple heuristic using historic values that decays with time.\n# \n# * It is structured based on my EDA and intuitive feeling of how the Covid-19 trend is likely to move.\n\n# In[8]:\n\n\n## function for predicting moving average decay model\ndef predict_mad(df_test, gap, val = False):\n    \n    df_test[\"avg_diff_cc\"] = (df_test[f\"lag_{gap}_cc\"] - df_test[f\"lag_{gap + 3}_cc\"]) / 3\n    df_test[\"avg_diff_ft\"] = (df_test[f\"lag_{gap}_ft\"] - df_test[f\"lag_{gap + 3}_ft\"]) / 3\n\n    if val:\n        y_pred_cc = df_test[f\"lag_{gap}_cc\"] + gap * df_test.avg_diff_cc - (1 - MAD_FACTOR) * df_test.avg_diff_cc * np.sum([x for x in range(gap)]) / VAL_DAYS\n        y_pred_ft = df_test[f\"lag_{gap}_ft\"] + gap * df_test.avg_diff_ft - (1 - MAD_FACTOR) * df_test.avg_diff_ft * np.sum([x for x in range(gap)]) / VAL_DAYS\n    else:\n        y_pred_cc = df_test[f\"lag_{gap}_cc\"] + gap * df_test.avg_diff_cc - (1 - MAD_FACTOR) * df_test.avg_diff_cc * np.sum([x for x in range(gap)]) / n_dates_test\n        y_pred_ft = df_test[f\"lag_{gap}_ft\"] + gap * df_test.avg_diff_ft - (1 - MAD_FACTOR) * df_test.avg_diff_ft * np.sum([x for x in range(gap)]) / n_dates_test\n\n    return y_pred_cc, y_pred_ft\n\n\n# ## Modelling\n# * One model is built for each date.\n# \n# * For dates already present in train data due to the overlap, no model is built.\n# \n# * Models are validated using the same framework.\n\n# In[9]:\n\n\n## building lag x-days models\ndf_train = df_panel[~df_panel.Id.isna()]\ndf_test_full = df_panel[~df_panel.ForecastId.isna()]\n\ndf_preds_val = []\ndf_preds_test = []\n\nfor date in df_test_full.Date.unique():\n    \n    print(\"Processing date:\", date)\n    \n    # ignore date already present in train data\n    if date in df_train.Date.values:\n        df_pred_test = df_test_full.loc[df_test_full.Date == date, [\"ForecastId\", \"ConfirmedCases\", \"Fatalities\"]].rename(columns = {\"ConfirmedCases\": \"ConfirmedCases_test\", \"Fatalities\": \"Fatalities_test\"})\n        \n        # multiplying predictions by 41 to not look cool on public LB\n        df_pred_test.ConfirmedCases_test = df_pred_test.ConfirmedCases_test * 41\n        df_pred_test.Fatalities_test = df_pred_test.Fatalities_test * 41\n    else:\n        df_test = df_test_full[df_test_full.Date == date]\n        \n        gap = (pd.Timestamp(date) - max_date_train).days\n        \n        if gap <= VAL_DAYS:\n            val_date = max_date_train - pd.Timedelta(VAL_DAYS, \"D\") + pd.Timedelta(gap, \"D\")\n\n            df_build = df_train[df_train.Date < val_date]\n            df_val = df_train[df_train.Date == val_date]\n            \n            X_build = prepare_features(df_build, gap)\n            X_val = prepare_features(df_val, gap)\n            \n            y_val_cc_lgb, y_val_ft_lgb, _, _ = build_predict_lgbm(X_build, X_val, gap)\n            y_val_cc_mad, y_val_ft_mad = predict_mad(df_val, gap, val = True)\n            \n            df_pred_val = pd.DataFrame({\"Id\": df_val.Id.values,\n                                        \"ConfirmedCases_val_lgb\": y_val_cc_lgb,\n                                        \"Fatalities_val_lgb\": y_val_ft_lgb,\n                                        \"ConfirmedCases_val_mad\": y_val_cc_mad,\n                                        \"Fatalities_val_mad\": y_val_ft_mad,\n                                       })\n\n            df_preds_val.append(df_pred_val)\n\n        X_train = prepare_features(df_train, gap)\n        X_test = prepare_features(df_test, gap)\n\n        y_test_cc_lgb, y_test_ft_lgb, model_cc, model_ft = build_predict_lgbm(X_train, X_test, gap)\n        y_test_cc_mad, y_test_ft_mad = predict_mad(df_test, gap)\n        \n        if gap == 1:\n            model_1_cc = model_cc\n            model_1_ft = model_ft\n            features_1 = X_train.columns.values\n        elif gap == 14:\n            model_14_cc = model_cc\n            model_14_ft = model_ft\n            features_14 = X_train.columns.values\n        elif gap == 28:\n            model_28_cc = model_cc\n            model_28_ft = model_ft\n            features_28 = X_train.columns.values\n\n        df_pred_test = pd.DataFrame({\"ForecastId\": df_test.ForecastId.values,\n                                     \"ConfirmedCases_test_lgb\": y_test_cc_lgb,\n                                     \"Fatalities_test_lgb\": y_test_ft_lgb,\n                                     \"ConfirmedCases_test_mad\": y_test_cc_mad,\n                                     \"Fatalities_test_mad\": y_test_ft_mad,\n                                    })\n    \n    df_preds_test.append(df_pred_test)\n\n\n# ## Validation\n# * Validating LGB and MAD models using RMSLE metric.\n# \n# * Visualizing feature importance of 1st, 14th and 28th models to see which features are crucial to predict short-term, medium-term and long-term respectively.\n\n# In[10]:\n\n\n## validation score\ndf_panel = df_panel.merge(pd.concat(df_preds_val, sort = False), on = \"Id\", how = \"left\")\ndf_panel = df_panel.merge(pd.concat(df_preds_test, sort = False), on = \"ForecastId\", how = \"left\")\n\nrmsle_cc_lgb = np.sqrt(mean_squared_error(np.log1p(df_panel[~df_panel.ConfirmedCases_val_lgb.isna()].ConfirmedCases), np.log1p(df_panel[~df_panel.ConfirmedCases_val_lgb.isna()].ConfirmedCases_val_lgb)))\nrmsle_ft_lgb = np.sqrt(mean_squared_error(np.log1p(df_panel[~df_panel.Fatalities_val_lgb.isna()].Fatalities), np.log1p(df_panel[~df_panel.Fatalities_val_lgb.isna()].Fatalities_val_lgb)))\n\nrmsle_cc_mad = np.sqrt(mean_squared_error(np.log1p(df_panel[~df_panel.ConfirmedCases_val_mad.isna()].ConfirmedCases), np.log1p(df_panel[~df_panel.ConfirmedCases_val_mad.isna()].ConfirmedCases_val_mad)))\nrmsle_ft_mad = np.sqrt(mean_squared_error(np.log1p(df_panel[~df_panel.Fatalities_val_mad.isna()].Fatalities), np.log1p(df_panel[~df_panel.Fatalities_val_mad.isna()].Fatalities_val_mad)))\n\nprint(\"LGB CC RMSLE Val of\", VAL_DAYS, \"days for CC:\", round(rmsle_cc_lgb, 2))\nprint(\"LGB FT RMSLE Val of\", VAL_DAYS, \"days for FT:\", round(rmsle_ft_lgb, 2))\nprint(\"LGB Overall RMSLE Val of\", VAL_DAYS, \"days:\", round((rmsle_cc_lgb + rmsle_ft_lgb) / 2, 2))\nprint(\"\\n\")\nprint(\"MAD CC RMSLE Val of\", VAL_DAYS, \"days for CC:\", round(rmsle_cc_mad, 2))\nprint(\"MAD FT RMSLE Val of\", VAL_DAYS, \"days for FT:\", round(rmsle_ft_mad, 2))\nprint(\"MAD Overall RMSLE Val of\", VAL_DAYS, \"days:\", round((rmsle_cc_mad + rmsle_ft_mad) / 2, 2))\n\n\n# In[11]:\n\n\n## feature importance\nfrom bokeh.io import output_notebook, show\nfrom bokeh.layouts import column\nfrom bokeh.palettes import Spectral3\nfrom bokeh.plotting import figure\n\noutput_notebook()\n\ndf_fimp_1_cc = pd.DataFrame({\"feature\": features_1, \"importance\": model_1_cc.feature_importance(), \"model\": \"m01\"})\ndf_fimp_14_cc = pd.DataFrame({\"feature\": features_14, \"importance\": model_14_cc.feature_importance(), \"model\": \"m14\"})\ndf_fimp_28_cc = pd.DataFrame({\"feature\": features_28, \"importance\": model_28_cc.feature_importance(), \"model\": \"m28\"})\n\ndf_fimp_1_cc.sort_values(\"importance\", ascending = False, inplace = True)\ndf_fimp_14_cc.sort_values(\"importance\", ascending = False, inplace = True)\ndf_fimp_28_cc.sort_values(\"importance\", ascending = False, inplace = True)\n\nv1 = figure(plot_width = 800, plot_height = 400, x_range = df_fimp_1_cc.feature, title = \"Feature Importance of LGB Model 1\")\nv1.vbar(x = df_fimp_1_cc.feature, top = df_fimp_1_cc.importance, width = 1)\nv1.xaxis.major_label_orientation = 1.3\n\nv14 = figure(plot_width = 800, plot_height = 400, x_range = df_fimp_14_cc.feature, title = \"Feature Importance of LGB Model 14\")\nv14.vbar(x = df_fimp_14_cc.feature, top = df_fimp_14_cc.importance, width = 1)\nv14.xaxis.major_label_orientation = 1.3\n\nv28 = figure(plot_width = 800, plot_height = 400, x_range = df_fimp_28_cc.feature, title = \"Feature Importance of LGB Model 28\")\nv28.vbar(x = df_fimp_28_cc.feature, top = df_fimp_28_cc.importance, width = 1)\nv28.xaxis.major_label_orientation = 1.3\n\nv = column(v1, v14, v28)\n\nshow(v)\n\n\n# ## Visualizing Predictions\n# * Viewing the actual, validation and test values together for each geography for ConfirmedCases as well as Fatalities.\n\n# In[12]:\n\n\n## visualizing ConfirmedCases\nfrom bokeh.models import Panel, Tabs\nfrom bokeh.io import output_notebook, show\nfrom bokeh.plotting import figure\n\noutput_notebook()\n\ntab_list = []\n\nfor geography in df_panel.geography.unique():\n    df_geography = df_panel[df_panel.geography == geography]\n    v = figure(plot_width = 800, plot_height = 400, x_axis_type = \"datetime\", title = \"Covid-19 ConfirmedCases over time\")\n    v.line(df_geography.Date, df_geography.ConfirmedCases, color = \"green\", legend_label = \"CC (Train)\")\n    v.line(df_geography.Date, df_geography.ConfirmedCases_val_lgb, color = \"blue\", legend_label = \"CC LGB (Val)\")\n    v.line(df_geography.Date, df_geography.ConfirmedCases_val_mad, color = \"purple\", legend_label = \"CC MAD (Val)\")\n    v.line(df_geography.Date[df_geography.Date > max_date_train], df_geography.ConfirmedCases_test_lgb[df_geography.Date > max_date_train], color = \"red\", legend_label = \"CC LGB (Test)\")\n    v.line(df_geography.Date[df_geography.Date > max_date_train], df_geography.ConfirmedCases_test_mad[df_geography.Date > max_date_train], color = \"orange\", legend_label = \"CC MAD (Test)\")\n    v.legend.location = \"top_left\"\n    tab = Panel(child = v, title = geography)\n    tab_list.append(tab)\n\ntabs = Tabs(tabs=tab_list)\nshow(tabs)\n\n\n# In[13]:\n\n\n## visualizing Fatalities\nfrom bokeh.models import Panel, Tabs\nfrom bokeh.io import output_notebook, show\nfrom bokeh.plotting import figure\n\noutput_notebook()\n\ntab_list = []\n\nfor geography in df_panel.geography.unique():\n    df_geography = df_panel[df_panel.geography == geography]\n    v = figure(plot_width = 800, plot_height = 400, x_axis_type = \"datetime\", title = \"Covid-19 Fatalities over time\")\n    v.line(df_geography.Date, df_geography.Fatalities, color = \"green\", legend_label = \"FT (Train)\")\n    v.line(df_geography.Date, df_geography.Fatalities_val_lgb, color = \"blue\", legend_label = \"FT LGB (Val)\")\n    v.line(df_geography.Date, df_geography.Fatalities_val_mad, color = \"purple\", legend_label = \"FT MAD (Val)\")\n    v.line(df_geography.Date[df_geography.Date > max_date_train], df_geography.Fatalities_test_lgb[df_geography.Date > max_date_train], color = \"red\", legend_label = \"FT LGB (Test)\")\n    v.line(df_geography.Date[df_geography.Date > max_date_train], df_geography.Fatalities_test_mad[df_geography.Date > max_date_train], color = \"orange\", legend_label = \"FT MAD (Test)\")\n    v.legend.location = \"top_left\"\n    tab = Panel(child = v, title = geography)\n    tab_list.append(tab)\n\ntabs = Tabs(tabs=tab_list)\nshow(tabs)\n\n\n# ## Submission\n# * Combining the two aapproaches using weights for final submission.\n# \n# * LGB models don't seem to perform well for certain geographies and are replaced by MAD predictions.\n\n# In[14]:\n\n\n## preparing submission file\ndf_test = df_panel.loc[~df_panel.ForecastId.isna(), [\"ForecastId\", \"Country_Region\", \"Province_State\", \"Date\",\n                                                     \"ConfirmedCases_test\", \"ConfirmedCases_test_lgb\", \"ConfirmedCases_test_mad\",\n                                                     \"Fatalities_test\", \"Fatalities_test_lgb\", \"Fatalities_test_mad\"]].reset_index()\n\ndf_test[\"ConfirmedCases\"] = 0.41 * df_test.ConfirmedCases_test_lgb + 0.59 * df_test.ConfirmedCases_test_mad\ndf_test[\"Fatalities\"] = 0.05 * df_test.Fatalities_test_lgb + 0.95 * df_test.Fatalities_test_mad\n\n# Since LGB models don't predict these countries well\ndf_test.loc[df_test.Country_Region.isin([\"China\", \"US\", \"Diamond Princess\"]), \"ConfirmedCases\"] = df_test[df_test.Country_Region.isin([\"China\", \"US\", \"Diamond Princess\"])].ConfirmedCases_test_mad.values\ndf_test.loc[df_test.Country_Region.isin([\"China\", \"US\", \"Diamond Princess\"]), \"Fatalities\"] = df_test[df_test.Country_Region.isin([\"China\", \"US\", \"Diamond Princess\"])].Fatalities_test_mad.values\n\ndf_test.loc[df_test.Date.isin(df_train.Date.values), \"ConfirmedCases\"] = df_test[df_test.Date.isin(df_train.Date.values)].ConfirmedCases_test.values\ndf_test.loc[df_test.Date.isin(df_train.Date.values), \"Fatalities\"] = df_test[df_test.Date.isin(df_train.Date.values)].Fatalities_test.values\n\ndf_submission = df_test[[\"ForecastId\", \"ConfirmedCases\", \"Fatalities\"]]\ndf_submission.ForecastId = df_submission.ForecastId.astype(int)\n\ndf_submission\n\n\n# In[15]:\n\n\n## writing final submission and complete output\ndf_submission.to_csv(\"sub2.csv\", index = False)\ndf_test.to_csv(PATH_OUTPUT, index = False)\nsub2=df_submission.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#zoo\n#!/usr/bin/env python\n# coding: utf-8\n\n# In[1]:\n\n\nimport os\nimport datetime\nimport numpy as np\nimport pandas as pd\nimport time\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error\n\npath = '../input/covid19-global-forecasting-week-3/'\ntrain = pd.read_csv(path + 'train.csv')\ntest  = pd.read_csv(path + 'test.csv')\nsub   = pd.read_csv(path + 'submission.csv')\n\ntrain['Date'] = train['Date'].apply(lambda x: (datetime.datetime.strptime(x, '%Y-%m-%d')))\ntest['Date'] = test['Date'].apply(lambda x: (datetime.datetime.strptime(x, '%Y-%m-%d')))\n#path_ext = '../input/novel-corona-virus-2019-dataset/'\n#ext_rec = pd.read_csv(path_ext + 'time_series_covid_19_recovered.csv').\\\n#        melt(id_vars=[\"Province/State\", \"Country/Region\", \"Lat\", \"Long\"], \n#            var_name=\"Date\", \n#            value_name=\"Recoveries\")\n#ext_rec['Date'] = ext_rec['Date'].apply(lambda x: (datetime.datetime.strptime(x+\"20\", '%m/%d/%Y')))\n#train = train.merge(ext_rec[['Province/State', 'Country/Region', 'Date', 'Recoveries']], how='left',\n#           left_on=['Province/State', 'Country/Region', 'Date'],\n#           right_on=['Province/State', 'Country/Region', 'Date'])\n\ntrain['days'] = (train['Date'].dt.date - train['Date'].dt.date.min()).dt.days\ntest['days'] = (test['Date'].dt.date - train['Date'].dt.date.min()).dt.days\n#train['isTest'] = train['Date'].dt.date >= datetime.date(2020, 3, 12)\n#train['isVal'] = np.logical_and(train['Date'].dt.date >= datetime.date(2020, 3, 11), train['Date'].dt.date <= datetime.date(9999, 3, 18))\ntrain.loc[train['Province_State'].isnull(), 'Province_State'] = 'N/A'\ntest.loc[test['Province_State'].isnull(), 'Province_State'] = 'N/A'\n\ntrain['Area'] = train['Country_Region'] + '_' + train['Province_State']\ntest['Area'] = test['Country_Region'] + '_' + test['Province_State']\n\nprint(train['Date'].max())\nprint(test['Date'].min())\nprint(train['days'].max())\nN_AREAS = train['Area'].nunique()\nAREAS = np.sort(train['Area'].unique())\n#TRAIN_N = 50 + 7\nTRAIN_N = 70\n\n\nprint(train[train['days'] < TRAIN_N]['Date'].max())\ntrain.head()\n\n\n# In[2]:\n\n\ntrain_p_c_raw = train.pivot(index='Area', columns='days', values='ConfirmedCases').sort_index()\ntrain_p_f_raw = train.pivot(index='Area', columns='days', values='Fatalities').sort_index()\n\ntrain_p_c = np.maximum.accumulate(train_p_c_raw, axis=1)\ntrain_p_f = np.maximum.accumulate(train_p_f_raw, axis=1)\n\nf_rate = (train_p_f / train_p_c).fillna(0)\n\nX_c = np.log(1+train_p_c.values)[:,:TRAIN_N]\nX_f = train_p_f.values[:,:TRAIN_N]\n\n\n# In[3]:\n\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_squared_error\n\ndef eval1(y, p):\n    val_len = y.shape[1] - TRAIN_N\n    return np.sqrt(mean_squared_error(y[:, TRAIN_N:TRAIN_N+val_len].flatten(), p[:, TRAIN_N:TRAIN_N+val_len].flatten()))\n\ndef run_c(params, X, test_size=50):\n    \n    gr_base = []\n    gr_base_factor = []\n    \n    x_min = np.ma.MaskedArray(X, X<1)\n    x_min = x_min.argmin(axis=1) \n    \n    for i in range(X.shape[0]):\n        temp = X[i,:]\n        threshold = np.log(1+params['min cases for growth rate'])\n        num_days = params['last N days']\n        if (temp > threshold).sum() > num_days:\n            gr_base.append(np.clip(np.diff(temp[temp > threshold])[-num_days:].mean(), 0, params['growth rate max']))\n            gr_base_factor.append(np.clip(np.diff(np.diff(temp[temp > threshold]))[-num_days:].mean(), -0.2, params[\"growth rate factor max\"]))\n        else:\n            gr_base.append(params['growth rate default'])\n            gr_base_factor.append(params['growth rate factor'])\n\n    gr_base = np.array(gr_base)\n    gr_base_factor = np.array(gr_base_factor)\n    #print(gr_base_factor)\n    #gr_base = np.clip(gr_base, 0.02, 0.8)\n    preds = X.copy()\n\n    for i in range(test_size):\n        delta = np.clip(preds[:, -1], np.log(2), None) + gr_base * (1 + params['growth rate factor']*(1 + params['growth rate factor factor'])**(i))**(i)\n        #delta = np.clip(preds[:, -1], np.log(2), None) + gr_base * (1 + params['growth rate factor']*(1 + params['growth rate factor factor'])**(i+X.shape[1]-x_min))**(i+X.shape[1]-x_min) \n        preds = np.hstack((preds, delta.reshape(-1,1)))\n\n    return preds\n\nparams = {\n    \"min cases for growth rate\": 30,\n    \"last N days\": 5,\n    \"growth rate default\": 0.25,\n    \"growth rate max\": 0.3,\n    \"growth rate factor max\": -0.01,\n    \"growth rate factor\": -0.05,\n    \"growth rate factor factor\": 0.005,\n}\n\n#x = train_p_c[train_p_c.index==\"Austria_N/A\"]\n\nx = train_p_c\n\npreds_c = run_c(params, np.log(1+x.values)[:,:TRAIN_N])\n#eval1(np.log(1+x).values, preds_c)\n\n\n# In[4]:\n\n\n\nfor i in range(N_AREAS):\n    if 'China' in AREAS[i] and preds_c[i, TRAIN_N-1] < np.log(31):\n        preds_c[i, TRAIN_N:] = preds_c[i, TRAIN_N-1]\n\n\n# In[5]:\n\n\ndef lin_w(sz):\n    res = np.linspace(0, 1, sz+1, endpoint=False)[1:]\n    return np.append(res, np.append([1], res[::-1]))\n\n\ndef run_f(params, X_c, X_f, X_f_r, test_size=50):\n\n\n    \n    X_f_r = np.array(np.ma.mean(np.ma.masked_outside(X_f_r, 0.06, 0.4)[:,:], axis=1))\n    X_f_r = np.clip(X_f_r, params['fatality_rate_lower'], params['fatality_rate_upper'])\n    #print(X_f_r)\n    \n    X_c = np.clip(np.exp(X_c)-1, 0, None)\n    preds = X_f.copy()\n    #print(preds.shape)\n    \n    train_size = X_f.shape[1] - 1\n    for i in range(test_size):\n        \n        t_lag = train_size+i-params['length']\n        t_wsize = 3\n        delta = np.average(np.diff(X_c, axis=1)[:, t_lag-t_wsize:t_lag+1+t_wsize], axis=1)\n        #delta = np.average(np.diff(X_c, axis=1)[:, t_lag-t_wsize:t_lag+1+t_wsize], axis=1, weights=lin_w(t_wsize))\n        \n        delta = params['absolute growth'] + delta * X_f_r\n        \n        preds = np.hstack((preds, preds[:, -1].reshape(-1,1) + delta.reshape(-1,1)))\n\n    return preds\n\nparams = {\n    \"length\": 6,\n    \"absolute growth\": 0.02,\n    \"fatality_rate_lower\": 0.035,\n    \"fatality_rate_upper\": 0.40,\n}\n\npreds_f = run_f(params, preds_c, X_f, f_rate.values[:,:TRAIN_N])\npreds_f = np.log(1+preds_f)\n#eval1(np.log(1+train_p_f).values, preds_f)\n\n\n# In[6]:\n\n\nfrom sklearn.metrics import mean_squared_error\n\nif False:\n    val_len = train_p_c.values.shape[1] - TRAIN_N\n\n    for i in range(val_len):\n        d = i + TRAIN_N\n        m1 = np.sqrt(mean_squared_error(np.log(1 + train_p_c_raw.values[:, d]), preds_c[:, d]))\n        m2 = np.sqrt(mean_squared_error(np.log(1 + train_p_f_raw.values[:, d]), preds_f[:, d]))\n        print(f\"{d}: {(m1 + m2)/2:8.5f} [{m1:8.5f} {m2:8.5f}]\")\n\n    print()\n\n    m1 = np.sqrt(mean_squared_error(np.log(1 + train_p_c_raw.values[:, TRAIN_N:TRAIN_N+val_len]).flatten(), preds_c[:, TRAIN_N:TRAIN_N+val_len].flatten()))\n    m2 = np.sqrt(mean_squared_error(np.log(1 + train_p_f_raw.values[:, TRAIN_N:TRAIN_N+val_len]).flatten(), preds_f[:, TRAIN_N:TRAIN_N+val_len].flatten()))\n    print(f\"{(m1 + m2)/2:8.5f} [{m1:8.5f} {m2:8.5f}]\")\n\n\n# In[7]:\n\n\nimport matplotlib.pyplot as plt\n\nplt.style.use(['default'])\nfig = plt.figure(figsize = (15, 5))\n\n#idx = worst_idx\n#print(AREAS[idx])\n\nidx = np.where(AREAS == 'Austria_N/A')[0][0]\nplt.plot(np.log(1+train_p_c.values[idx]), label=AREAS[idx], color='darkblue')\nplt.plot(preds_c[idx], linestyle='--', color='darkblue')\n\nidx = np.where(AREAS == 'Germany_N/A')[0][0]\nplt.plot(np.log(1+train_p_c.values[idx]), label=AREAS[idx], color='red')\nplt.plot(preds_c[idx], linestyle='--', color='red')\n\n\nidx = np.where(AREAS == 'China_Hubei')[0][0]\nplt.plot(np.log(1+train_p_c.values[idx]), label=AREAS[idx], color='grey')\nplt.plot(preds_c[idx], linestyle='--', color='grey')\n\n\nidx = np.where(AREAS == 'Iran_N/A')[0][0]\nplt.plot(np.log(1+train_p_c.values[idx]), label=AREAS[idx], color='green')\nplt.plot(preds_c[idx], linestyle='--', color='green')\n\n\nidx = np.where(AREAS == 'Japan_N/A')[0][0]\nplt.plot(np.log(1+train_p_c.values[idx]), label=AREAS[idx], color='purple')\nplt.plot(preds_c[idx], linestyle='--', color='purple')\n\n\nidx = np.where(AREAS == 'Brazil_N/A')[0][0]\nplt.plot(np.log(1+train_p_c.values[idx]), label=AREAS[idx], color='black')\nplt.plot(preds_c[idx], linestyle='--', color='black')\n\n\nidx = np.where(AREAS == 'Denmark_N/A')[0][0]\nplt.plot(np.log(1+train_p_c.values[idx]), label=AREAS[idx], color='yellow')\nplt.plot(preds_c[idx], linestyle='--', color='yellow')\n\nidx = np.where(AREAS == 'Italy_N/A')[0][0]\nplt.plot(np.log(1+train_p_c.values[idx]), label=AREAS[idx], color='blue')\nplt.plot(preds_c[idx], linestyle='--', color='blue')\n\n\nplt.legend()\nplt.show()\n\n\n# In[8]:\n\n\nimport matplotlib.pyplot as plt\n\nplt.style.use(['default'])\nfig = plt.figure(figsize = (15, 5))\n\n#idx = worst_idx\n#print(AREAS[idx])\n\nidx = np.where(AREAS == 'Austria_N/A')[0][0]\nplt.plot(np.log(1+train_p_f.values[idx]), label=AREAS[idx], color='darkblue')\nplt.plot(preds_f[idx], linestyle='--', color='darkblue')\n\nidx = np.where(AREAS == 'Germany_N/A')[0][0]\nplt.plot(np.log(1+train_p_f.values[idx]), label=AREAS[idx], color='red')\nplt.plot(preds_f[idx], linestyle='--', color='red')\n\n\nidx = np.where(AREAS == 'China_Hubei')[0][0]\nplt.plot(np.log(1+train_p_f.values[idx]), label=AREAS[idx], color='grey')\nplt.plot(preds_f[idx], linestyle='--', color='grey')\n\n\nidx = np.where(AREAS == 'Iran_N/A')[0][0]\nplt.plot(np.log(1+train_p_f.values[idx]), label=AREAS[idx], color='green')\nplt.plot(preds_f[idx], linestyle='--', color='green')\n\n\nidx = np.where(AREAS == 'Japan_N/A')[0][0]\nplt.plot(np.log(1+train_p_f.values[idx]), label=AREAS[idx], color='purple')\nplt.plot(preds_f[idx], linestyle='--', color='purple')\n\n\nidx = np.where(AREAS == 'Brazil_N/A')[0][0]\nplt.plot(np.log(1+train_p_f.values[idx]), label=AREAS[idx], color='black')\nplt.plot(preds_f[idx], linestyle='--', color='black')\n\n\nidx = np.where(AREAS == 'Denmark_N/A')[0][0]\nplt.plot(np.log(1+train_p_f.values[idx]), label=AREAS[idx], color='yellow')\nplt.plot(preds_f[idx], linestyle='--', color='yellow')\n\nidx = np.where(AREAS == 'Italy_N/A')[0][0]\nplt.plot(np.log(1+train_p_f.values[idx]), label=AREAS[idx], color='blue')\nplt.plot(preds_f[idx], linestyle='--', color='blue')\n\nplt.legend()\nplt.show()\n\n\n# In[9]:\n\n\nimport matplotlib.pyplot as plt\n\nplt.style.use(['default'])\nfig = plt.figure(figsize = (15, 5))\n\nidx = np.random.choice(N_AREAS)\nprint(AREAS[idx])\n\nplt.plot(np.log(1+train_p_c.values[idx]), label=AREAS[idx], color='darkblue')\nplt.plot(preds_c[idx], linestyle='--', color='darkblue')\n\nplt.show()\n\n\n# In[ ]:\n\n\n\n\n\n# In[10]:\n\n\nEU_COUNTRIES = ['Austria', 'Italy', 'Belgium', 'Latvia', 'Bulgaria', 'Lithuania', 'Croatia', 'Luxembourg', 'Cyprus', 'Malta', 'Czechia', \n                'Netherlands', 'Denmark', 'Poland', 'Estonia', 'Portugal', 'Finland', 'Romania', 'France', 'Slovakia', 'Germany', 'Slovenia', \n                'Greece', 'Spain', 'Hungary', 'Sweden', 'Ireland']\nEUROPE_OTHER = ['Albania', 'Andorra', 'Bosnia and Herzegovina', 'Liechtenstein', 'Monaco', 'Montenegro', 'North Macedonia',\n                'Norway', 'San Marino', 'Serbia', 'Switzerland', 'Turkey', 'United Kingdom']\nAFRICA = ['Algeria', 'Burkina Faso', 'Cameroon', 'Congo (Kinshasa)', \"Cote d'Ivoire\", 'Egypt', 'Ghana', 'Kenya', 'Madagascar',\n                'Morocco', 'Nigeria', 'Rwanda', 'Senegal', 'South Africa', 'Togo', 'Tunisia', 'Uganda', 'Zambia']\nNORTH_AMERICA = ['US', 'Canada', 'Mexico']\nSOUTH_AMERICA = ['Argentina', 'Bolivia', 'Brazil', 'Chile', 'Colombia', 'Ecuador', 'Paraguay', 'Peru', 'Uruguay', 'Venezuela']\nMIDDLE_EAST = ['Afghanistan', 'Bahrain', 'Iran', 'Iraq', 'Israel', 'Jordan', 'Kuwait', 'Lebanon', 'Oman', 'Qatar', 'Saudi Arabia', 'United Arab Emirates']\nASIA = ['Bangladesh', 'Brunei', 'Cambodia', 'India', 'Indonesia', 'Japan', 'Kazakhstan', 'Korea, South', 'Kyrgyzstan', 'Malaysia',\n                'Pakistan', 'Singapore', 'Sri Lanka', 'Taiwan*', 'Thailand', 'Uzbekistan', 'Vietnam']\n\n\n# In[11]:\n\n\nimport matplotlib.pyplot as plt\n\ndef plt1(ar, ar2, ax, col='darkblue', linew=0.2):\n    ax.plot(ar2, linestyle='--', linewidth=linew/2, color=col)\n    ax.plot(np.log(1+ar), linewidth=linew, color=col)\n\nplt.style.use(['default'])\nfig, axs = plt.subplots(3, 2, figsize=(18, 15), sharey=True)\n\nX = train_p_c.values\n#X = train_p_f.values\n\nfor ar in range(X.shape[0]):\n    \n    temp = X[ar]\n    temp2 = preds_c[ar]\n    if 'China' in AREAS[ar]:\n        plt1(temp, temp2, axs[0,0])\n    elif AREAS[ar].split('_')[0] in NORTH_AMERICA:\n        plt1(temp, temp2, axs[0,1])\n    elif AREAS[ar].split('_')[0] in EU_COUNTRIES + EUROPE_OTHER:\n        plt1(temp, temp2, axs[1,0])\n    elif AREAS[ar].split('_')[0] in SOUTH_AMERICA + AFRICA:\n        plt1(temp, temp2, axs[1,1])\n    elif AREAS[ar].split('_')[0] in MIDDLE_EAST + ASIA:\n        plt1(temp, temp2, axs[2,0])\n    else:\n        plt1(temp, temp2, axs[2,1])\n\nprint(\"Confirmed Cases\")\naxs[0,0].set_title('China')\naxs[0,1].set_title('North America')\naxs[1,0].set_title('Europe')\naxs[1,1].set_title('Africa + South America')\naxs[2,0].set_title('Asia + Middle East')\naxs[2,1].set_title('Other')\nplt.show()\n\n\n# In[12]:\n\n\nimport matplotlib.pyplot as plt\n\ndef plt1(ar, ar2, ax, col='darkblue', linew=0.2):\n    ax.plot(ar2, linestyle='--', linewidth=linew/2, color=col)\n    ax.plot(np.log(1+ar), linewidth=linew, color=col)\n\nplt.style.use(['default'])\nfig, axs = plt.subplots(3, 2, figsize=(18, 15), sharey=True)\n\n#X = train_p_c.values\nX = train_p_f.values\n\nfor ar in range(X.shape[0]):\n    \n    temp = X[ar]\n    temp2 = preds_f[ar]\n    if 'China' in AREAS[ar]:\n        plt1(temp, temp2, axs[0,0])\n    elif AREAS[ar].split('_')[0] in NORTH_AMERICA:\n        plt1(temp, temp2, axs[0,1])\n    elif AREAS[ar].split('_')[0] in EU_COUNTRIES + EUROPE_OTHER:\n        plt1(temp, temp2, axs[1,0])\n    elif AREAS[ar].split('_')[0] in SOUTH_AMERICA + AFRICA:\n        plt1(temp, temp2, axs[1,1])\n    elif AREAS[ar].split('_')[0] in MIDDLE_EAST + ASIA:\n        plt1(temp, temp2, axs[2,0])\n    else:\n        plt1(temp, temp2, axs[2,1])\n\nprint(\"Fatalities\")\naxs[0,0].set_title('China')\naxs[0,1].set_title('North America')\naxs[1,0].set_title('Europe')\naxs[1,1].set_title('Africa + South America')\naxs[2,0].set_title('Asia + Middle East')\naxs[2,1].set_title('Other')\nplt.show()\n\n\n# In[ ]:\n\n\n\n\n\n# In[13]:\n\n\ntemp = pd.DataFrame(np.clip(np.exp(preds_c) - 1, 0, None))\ntemp['Area'] = AREAS\ntemp = temp.melt(id_vars='Area', var_name='days', value_name=\"ConfirmedCases\")\n\ntest = test.merge(temp, how='left', left_on=['Area', 'days'], right_on=['Area', 'days'])\n\ntemp = pd.DataFrame(np.clip(np.exp(preds_f) - 1, 0, None))\ntemp['Area'] = AREAS\ntemp = temp.melt(id_vars='Area', var_name='days', value_name=\"Fatalities\")\n\ntest = test.merge(temp, how='left', left_on=['Area', 'days'], right_on=['Area', 'days'])\ntest.head()\n\n\n# In[14]:\n\n\ntest.to_csv(\"sub3.csv\", index=False, columns=[\"ForecastId\", \"ConfirmedCases\", \"Fatalities\"])\nsub3=test[[\"ForecastId\", \"ConfirmedCases\", \"Fatalities\"]].copy()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub1.sort_values(['ForecastId'],inplace=True)\nsub2.sort_values(['ForecastId'],inplace=True)\nsub3.sort_values(['ForecastId'],inplace=True)\nsub=sub1.copy()\nsub['ConfirmedCases']=sub1['ConfirmedCases']*0.33+sub2['ConfirmedCases']*0.34+sub3['ConfirmedCases']*0.33\nsub['Fatalities']=sub1['Fatalities']*0.33+sub2['Fatalities']*0.34+sub3['Fatalities']*0.33\nsub.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"sub.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}