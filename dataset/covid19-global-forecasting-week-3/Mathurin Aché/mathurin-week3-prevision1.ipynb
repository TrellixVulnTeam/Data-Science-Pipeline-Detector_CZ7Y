{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#Kazanova\n#!/usr/bin/env python\n# coding: utf-8\n\n# In[1]:\n\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\n\n# In[2]:\n\n\nimport math\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import mean_absolute_error\n\ndef rmsle(y, y_pred):\n        assert len(y) == len(y_pred)\n        terms_to_sum = [(math.log(y_pred[i] + 1) - math.log(y[i] + 1)) ** 2.0 for i,pred in enumerate(y_pred)]\n        return (sum(terms_to_sum) * (1.0/len(y))) ** 0.5\n    \n\ndef fix_target(frame, key, target, new_target_name=\"target\"):\n    import numpy as np\n\n    corrections = 0\n    group_keys = frame[ key].values.tolist()\n    target = frame[target].values.tolist()\n\n    for i in range(1, len(group_keys) - 1):\n        previous_group = group_keys[i - 1]\n        current_group = group_keys[i]\n\n        previous_value = target[i - 1]\n        current_value = target[i]\n        if current_group == previous_group:\n                if current_value<previous_value:\n                    current_value=previous_value\n                    target[i] =current_value\n\n\n        target[i] =max(0,target[i] )#correct negative values\n\n    frame[new_target_name] = np.array(target)\n    \n    \ndef rate(frame, key, target, new_target_name=\"rate\"):\n    import numpy as np\n\n\n    corrections = 0\n    group_keys = frame[ key].values.tolist()\n    target = frame[target].values.tolist()\n    rate=[1.0 for k in range (len(target))]\n\n    for i in range(1, len(group_keys) - 1):\n        previous_group = group_keys[i - 1]\n        current_group = group_keys[i]\n\n        previous_value = target[i - 1]\n        current_value = target[i]\n         \n        if current_group == previous_group:\n                if previous_value!=0.0:\n                     rate[i]=current_value/previous_value\n\n                 \n        rate[i] =max(1,rate[i] )#correct negative values\n\n    frame[new_target_name] = np.array(rate)\n    \ndef get_data_by_key(dataframe, key, key_value, fields=None):\n    mini_frame=dataframe[dataframe[key]==key_value]\n    if not fields is None:                \n        mini_frame=mini_frame[fields].values\n        \n    return mini_frame\n\ndirectory=\"/kaggle/input/covid19-global-forecasting-week-3/\"\nmodel_directory=\"/kaggle/input/model-dir/model\"\n\ntrain=pd.read_csv(directory + \"train.csv\", parse_dates=[\"Date\"] , engine=\"python\")\ntest=pd.read_csv(directory + \"test.csv\", parse_dates=[\"Date\"], engine=\"python\")\n\ntrain[\"key\"]=train[[\"Province_State\",\"Country_Region\"]].apply(lambda row: str(row[0]) + \"_\" + str(row[1]),axis=1)\ntest[\"key\"]=test[[\"Province_State\",\"Country_Region\"]].apply(lambda row: str(row[0]) + \"_\" + str(row[1]),axis=1)\n\n#last day in train\nmax_train_date=train[\"Date\"].max()\nmax_test_date=test[\"Date\"].max()\nhorizon=  (max_test_date-max_train_date).days\nprint (\"horizon\", int(horizon))\n\n\n#test_new=pd.merge(test,train, how=\"left\", left_on=[\"key\",\"Date\"], right_on=[\"key\",\"Date\"] )\n#train.to_csv(directory + \"transfomed.csv\")\n\ntarget1=\"ConfirmedCases\"\ntarget2=\"Fatalities\"\n\nkey=\"key\"\n\n\n# In[3]:\n\n\nfix_target(train, key, target1, new_target_name=target1)\n#fix_target(train, key, target2, new_target_name=target2)\n\nrate(train, key, target1, new_target_name=\"rate_\" +target1 )\nrate(train, key, target2, new_target_name=\"rate_\" +target2 )\nunique_keys=train[key].unique()\nprint(len(unique_keys))\n\n\ntrain\n\n\n# In[4]:\n\n\ndef get_lags(rate_array, current_index, size=20):\n    lag_confirmed_rate=[-1 for k in range(size)]\n    for j in range (0, size):\n        if current_index-j>=0:\n            lag_confirmed_rate[j]=rate_array[current_index-j]\n        else :\n            break\n    return lag_confirmed_rate\n\ndef days_ago_thresold_hit(full_array, indx, thresold):\n        days_ago_confirmed_count_10=-1\n        if full_array[indx]>thresold: # if currently the count of confirmed is more than 10\n            for j in range (indx,-1,-1):\n                entered=False\n                if full_array[j]<=thresold:\n                    days_ago_confirmed_count_10=abs(j-indx)\n                    entered=True\n                    break\n                if entered==False:\n                    days_ago_confirmed_count_10=100 #this value would we don;t know it cross 0      \n        return days_ago_confirmed_count_10 \n    \n    \ndef ewma_vectorized(data, alpha):\n    sums=sum([ (alpha**(k+1))*data[k] for  k in range(len(data)) ])\n    counts=sum([ (alpha**(k+1)) for  k in range(len(data)) ])\n    return sums/counts\n\ndef generate_ma_std_window(rate_array, current_index, size=20, window=3):\n    ma_rate_confirmed=[-1 for k in range(size)]\n    std_rate_confirmed=[-1 for k in range(size)] \n    \n    for j in range (0, size):\n        if current_index-j>=0:\n            ma_rate_confirmed[j]=np.mean(rate_array[max(0,current_index-j-window+1 ):current_index-j+1])\n            std_rate_confirmed[j]=np.std(rate_array[max(0,current_index-j-window+1 ):current_index-j+1])           \n        else :\n            break\n    return ma_rate_confirmed, std_rate_confirmed\n\ndef generate_ewma_window(rate_array, current_index, size=20, window=3, alpha=0.05):\n    ewma_rate_confirmed=[-1 for k in range(size)]\n\n    \n    for j in range (0, size):\n        if current_index-j>=0:\n            ewma_rate_confirmed[j]=ewma_vectorized(rate_array[max(0,current_index-j-window+1 ):current_index-j+1, ], alpha)           \n        else :\n            break\n    \n    #print(ewma_rate_confirmed)\n    return ewma_rate_confirmed\n\n\ndef get_target(rate_col, indx, horizon=33, average=3, use_hard_rule=False):\n    target_values=[-1 for k in range(horizon)]\n    cou=0\n    for j in range(indx+1, indx+1+horizon):\n        if j<len(rate_col):\n            if average==1:\n                target_values[cou]=rate_col[j]\n            else :\n                if use_hard_rule and j +average <=len(rate_col) :\n                     target_values[cou]=np.mean(rate_col[j:j +average])\n                else :\n                    target_values[cou]=np.mean(rate_col[j:min(len(rate_col),j +average)])\n                   \n            cou+=1\n        else :\n            break\n    return target_values\n\n\ndef dereive_features(frame, confirmed, fatalities, rate_confirmed, rate_fatalities, \n                     horizon ,size=20, windows=[3,7], days_back_confimed=[1,10,100], days_back_fatalities=[1,2,10]):\n    targets=[]\n    \n    names=[\"lag_confirmed_rate\" + str(k+1) for k in range (size)]\n    for day in days_back_confimed:\n        names+=[\"days_ago_confirmed_count_\" + str(day) ]\n    for window in windows:        \n        names+=[\"ma\" + str(window) + \"_rate_confirmed\" + str(k+1) for k in range (size)]\n        names+=[\"std\" + str(window) + \"_rate_confirmed\" + str(k+1) for k in range (size)] \n        names+=[\"ewma\" + str(window) + \"_rate_confirmed\" + str(k+1) for k in range (size)]         \n        \n        \n    names+=[\"lag_fatalities_rate\" + str(k+1) for k in range (size)]\n    for day in days_back_fatalities:\n        names+=[\"days_ago_fatalitiescount_\" + str(day) ]    \n    for window in windows:        \n        names+=[\"ma\" + str(window) + \"_rate_fatalities\" + str(k+1) for k in range (size)]\n        names+=[\"std\" + str(window) + \"_rate_fatalities\" + str(k+1) for k in range (size)]  \n        names+=[\"ewma\" + str(window) + \"_rate_fatalities\" + str(k+1) for k in range (size)]        \n    names+=[\"confirmed_level\"]\n    names+=[\"fatalities_level\"]    \n    \n    names+=[\"confirmed_plus\" + str(k+1) for k in range (horizon)]    \n    names+=[\"fatalities_plus\" + str(k+1) for k in range (horizon)]  \n    \n    #names+=[\"current_confirmed\"]\n    #names+=[\"current_fatalities\"]    \n    \n    features=[]\n    for i in range (len(confirmed)):\n        row_features=[]\n        #####################lag_confirmed_rate       \n        lag_confirmed_rate=get_lags(rate_confirmed, i, size=size)\n        row_features+=lag_confirmed_rate\n        #####################days_ago_confirmed_count_10\n        for day in days_back_confimed:\n            days_ago_confirmed_count_10=days_ago_thresold_hit(confirmed, i, day)               \n            row_features+=[days_ago_confirmed_count_10] \n        #####################ma_rate_confirmed       \n        #####################std_rate_confirmed \n        for window in windows:\n            ma3_rate_confirmed,std3_rate_confirmed= generate_ma_std_window(rate_confirmed, i, size=size, window=window)\n            row_features+= ma3_rate_confirmed   \n            row_features+= std3_rate_confirmed          \n            ewma3_rate_confirmed=generate_ewma_window(rate_confirmed, i, size=size, window=window, alpha=0.05)\n            row_features+= ewma3_rate_confirmed              \n        #####################lag_fatalities_rate   \n        lag_fatalities_rate=get_lags(rate_fatalities, i, size=size)\n        row_features+=lag_fatalities_rate\n        #####################days_ago_confirmed_count_10\n        for day in days_back_fatalities:\n            days_ago_fatalitiescount_2=days_ago_thresold_hit(fatalities, i, day)               \n            row_features+=[days_ago_fatalitiescount_2]     \n        #####################ma_rate_fatalities       \n        #####################std_rate_fatalities \n        for window in windows:        \n            ma3_rate_fatalities,std3_rate_fatalities= generate_ma_std_window(rate_fatalities, i, size=size, window=window)\n            row_features+= ma3_rate_fatalities   \n            row_features+= std3_rate_fatalities  \n            ewma3_rate_fatalities=generate_ewma_window(rate_fatalities, i, size=size, window=window, alpha=0.05)\n            row_features+= ewma3_rate_fatalities                  \n        ##################confirmed_level\n        confirmed_level=0\n        \n        \"\"\"\n        if confirmed[i]>0 and confirmed[i]<1000:\n            confirmed_level= confirmed[i]\n        else :\n            confirmed_level=2000\n        \"\"\"   \n        confirmed_level= confirmed[i]\n        row_features+=[confirmed_level]\n        ##################fatalities_is_level\n        fatalities_is_level=0\n        \"\"\"\n        if fatalities[i]>0 and fatalities[i]<100:\n            fatalities_is_level= fatalities[i]\n        else :\n            fatalities_is_level=200            \n        \"\"\"\n        fatalities_is_level= fatalities[i]\n        \n        row_features+=[fatalities_is_level]              \n            \n        #######################confirmed_plus target\n        confirmed_plus=get_target(rate_confirmed, i, horizon=horizon)\n        row_features+= confirmed_plus          \n        #######################fatalities_plus target\n        fatalities_plus=get_target(rate_fatalities, i, horizon=horizon)\n        row_features+= fatalities_plus \n        ##################current_confirmed\n        #row_features+=[confirmed[i]]\n        ##################current_fatalities\n        #row_features+=[fatalities[i]]        \n        \n          \n\n        \n        features.append(row_features)\n        \n    new_frame=pd.DataFrame(data=features, columns=names).reset_index(drop=True)\n    frame=frame.reset_index(drop=True)\n    frame=pd.concat([frame, new_frame], axis=1)\n    #print(frame.shape)\n    return frame\n    \n    \ndef feature_engineering_for_single_key(frame, group, key, horizon=33, size=20, windows=[3,7], \n                                       days_back_confimed=[1,10,100], days_back_fatalities=[1,2,10]):\n    mini_frame=get_data_by_key(frame, group, key, fields=None)\n    \n    mini_frame_with_features=dereive_features(mini_frame, mini_frame[\"ConfirmedCases\"].values,\n                                              mini_frame[\"Fatalities\"].values, mini_frame[\"rate_ConfirmedCases\"].values, \n                                               mini_frame[\"rate_Fatalities\"].values, horizon ,size=size, windows=windows,\n                                              days_back_confimed=days_back_confimed, days_back_fatalities=days_back_fatalities)\n    #print (mini_frame_with_features.shape[0])\n    return mini_frame_with_features\n\n\n# In[5]:\n\n\nfrom tqdm import tqdm\ntrain_frame=[]\nsize=20\nwindows=[3,5,7]\ndays_back_confimed=[1,10,100]\ndays_back_fatalities=[1,2,10]\n#print (len(train['key'].unique()))\nfor unique_k in tqdm(unique_keys):\n    mini_frame=feature_engineering_for_single_key(train, key, unique_k, horizon=horizon, size=size, \n                                                  windows=windows, days_back_confimed=days_back_confimed,\n                                                  days_back_fatalities=days_back_fatalities).reset_index(drop=True) \n    #print (mini_frame.shape[0])\n    train_frame.append(mini_frame)\n    \ntrain_frame = pd.concat(train_frame, axis=0).reset_index(drop=True)\n#train_frame.to_csv(directory +\"all\" + \".csv\", index=False)\nnew_unique_keys=train_frame['key'].unique()\nfor kee in new_unique_keys:\n    if kee not in unique_keys:\n        print (kee , \" is not there \")\n\n\n# In[6]:\n\n\nimport lightgbm as lgb\nfrom sklearn.linear_model import Ridge\nfrom sklearn.externals import joblib\n\ndef predict(xtest,input_name=None):\n   #print (type(yt))\n   # create array object to hold predictions \n  \n   baggedpred=np.array([ 0.0 for d in range(0, xtest.shape[0])]) \n   model=  joblib.load( input_name) \n   preds=model.predict(xtest)               \n   baggedpred+=preds\n\n   return baggedpred\n\n\n# In[7]:\n\n\nnames=[\"lag_confirmed_rate\" + str(k+1) for k in range (size)]\nfor day in days_back_confimed:\n    names+=[\"days_ago_confirmed_count_\" + str(day) ]\nfor window in windows:        \n    names+=[\"ma\" + str(window) + \"_rate_confirmed\" + str(k+1) for k in range (size)]\n    names+=[\"std\" + str(window) + \"_rate_confirmed\" + str(k+1) for k in range (size)] \n    names+=[\"ewma\" + str(window) + \"_rate_confirmed\" + str(k+1) for k in range (size)]         \n\n\nnames+=[\"lag_fatalities_rate\" + str(k+1) for k in range (size)]\nfor day in days_back_fatalities:\n    names+=[\"days_ago_fatalitiescount_\" + str(day) ]    \nfor window in windows:        \n    names+=[\"ma\" + str(window) + \"_rate_fatalities\" + str(k+1) for k in range (size)]\n    names+=[\"std\" + str(window) + \"_rate_fatalities\" + str(k+1) for k in range (size)]  \n    names+=[\"ewma\" + str(window) + \"_rate_fatalities\" + str(k+1) for k in range (size)]        \nnames+=[\"confirmed_level\"]\nnames+=[\"fatalities_level\"]      \n\n\n# In[8]:\n\n\n#### scoring \ndef decay_4_first_10_then_1_f(array):\n    arr=[1.0 for k in range(len(array))]\n    for j in range(len(array)):\n        if j<10:\n            arr[j]=1. + (max(1,array[j])-1.)/4.\n        else :\n            arr[j]=1.\n    return arr\n            \ndef decay_2_f(array):\n    arr=[1.0 for k in range(len(array))]    \n    for j in range(len(array)):\n            arr[j]=1. + (max(1,array[j])-1.)/2.\n    return arr \n\ndef acceleratorx2_f(array):\n    arr=[1.0 for k in range(len(array))]    \n    for j in range(len(array)):\n            arr[j]=1. + (max(1,array[j])-1.)*2.\n    return arr \n\n\n\ndef decay_1_5_f(array):\n    arr=[1.0 for k in range(len(array))]    \n    for j in range(len(array)):\n            arr[j]=1. + (max(1,array[j])-1.)/1.5\n    return arr            \n         \n         \ndef stay_same_f(array):\n    arr=[1.0 for k in range(len(array))]      \n    for j in range(len(array)):\n        arr[j]=1.\n    return arr   \n\ndef decay_2_last_12_linear_inter_f(array):\n    arr=[1.0 for k in range(len(array))]\n    for j in range(len(array)):\n        arr[j]=1. + (max(1,array[j])-1.)/2.\n    arr12= (max(1,arr[-12])-1.)/12. \n\n    for j in range(0, 12):\n        arr[len(arr)-12 +j]= max(1, 1 + ( (arr12*12) - (j+1)*arr12 ))\n    return arr\n\ndef linear_last_12_f(array):\n    arr=[1.0 for k in range(len(array))]\n    for j in range(len(array)):\n        arr[j]=max(1,array[j])\n    arr12= (max(1,arr[-12])-1.)/12. \n    \n    for j in range(0, 12):\n        arr[len(arr)-12 +j]= max(1, 1 + ( (arr12*12) - (j+1)*arr12 ))\n    return arr\n    \ndecay_4_first_10_then_1 =[\"Beijing_China\",\"Fujian_China\",\"Guangdong_China\", \"Hong Kong_China\",\n\"Inner Mongolia_China\",\"Jiangsu_China\",\"Liaoning_China\",\"Macau_China\",\"Shandong_China\",\"Tianjin_China\",\n\"Yunnan_China\",\"Zhejiang_China\",\"Northern Territory_Australia\",\n\"nan_Belize\",\"nan_Benin\",\"nan_Bhutan\",\"nan_Seychelles\",\"nan_Cabo Verde\"]\n\ndecay_2 =[\"Shanghai_China\" , \"nan_Afghanistan\",\"nan_Andorra\",\"Australian Capital Territory_Australia\",\n\"South Australia_Australia\",\"Tasmania_Australia\",\"nan_Bahrain\",\"nan_Belarus\"\n\"nan_Belgium\",\"nan_Bolivia\",\"Manitoba_Canada\",\"New Brunswick_Canada\",\"Newfoundland and Labrador_Canada\",\n\"Saskatchewan_Canada\",\"nan_Central African Republic\",\"nan_Congo (Kinshasa)\",\"nan_Cote d'Ivoire\",\"Mayotte_France\",\"nan_Ukraine\"]\n\nstay_same=[\"China\", \"nan_Antigua and Barbuda\",\"nan_Diamond Princess\",\"nan_Saint Vincent and the Grenadines\",\n           \"nan_Timor-Leste\",\"Montserrat_United Kingdom\"]\n\ndecay_2_last_12_linear_inter =[\"nan_Angola\" , \"nan_Barbados\" ,\"Prince Edward Island_Canada\",\"nan_Chad\",\n\"nan_Congo (Brazzaville)\",\"Greenland_Denmark\",\"nan_Djibouti\",\"nan_Dominica\",\"nan_El Salvador\",\n\"nan_Eritrea\",\"nan_Eswatini\",\"nan_Fiji\",\"French Guiana_France\",\"French Polynesia_France\",\"New Caledonia_France\",\n\"Saint Barthelemy_France\",\"St Martin_France\",\"nan_Gabon\",\"nan_Gambia\",\"nan_Grenada\",\"nan_Guinea-Bissau\",\n\"nan_Guyana\",\"nan_Haiti\",\"nan_Holy See\",\"nan_Kyrgyzstan\",\"nan_Laos\",\"nan_Libya\",\"nan_Madagascar\",\n\"nan_Maldives\",\"nan_Mali\",\"nan_Mauritania\",\"nan_Mauritius\",\"nan_Mozambique\",\"nan_Nepal\",\n\"Aruba_Netherlands\",\"Curacao_Netherlands\",\"Sint Maarten_Netherlands\",\"nan_Nicaragua\",\"nan_Niger\",\"nan_Papua New Guinea\",\n\"nan_Saint Kitts and Nevis\",\"nan_Saint Lucia\",\"nan_Somalia\",\"nan_Sudan\",\"nan_Suriname\",\"nan_Syria\",\"nan_Tanzania\",\n\"nan_Togo\",\"Virgin Islands_US\",\"Bermuda_United Kingdom\",\"Cayman Islands_United Kingdom\",\"Channel Islands_United Kingdom\",\n\"Gibraltar_United Kingdom\",\"Isle of Man_United Kingdom\",\"nan_Zimbabwe\",\"nan_Bahamas\",\"nan_Zambia\"]\n\nacceleratorx2=[\"nan_Kenya\",\"nan_Moldova\"]\n\ndecay_1_5 =[\"nan_Kazakhstan\",\"nan_Tunisia\", \"Alabama_US\", \"Alaska_US\",\n\t\"Arizona_US\",\"Colorado_US\",\"Florida_US\",\"Montana_US\",\"Nebraska_US\",\"Nevada_US\",\"New Hampshire_US\",\"New Mexico_US\",\n\t\"Puerto Rico_US\",\"nan_Uzbekistan\",\"nan_Azerbaijan\",\"nan_Bangladesh\",\"nan_Bosnia and Herzegovina\",\n\t\"nan_Cameroon\",\"nan_Cuba\",\"nan_Guatemala\",\"nan_Jamaica\",\"nan_Morocco\",\"nan_New Zealand\",\"nan_Philippines\",\"nan_Romania\",\n\t\"nan_Trinidad and Tobago\"]\n\nlinear_last_12=[\"nan_Uganda\",\"nan_Equatorial Guinea\",\"nan_Guinea\",\"nan_Honduras\",\"nan_Liberia\",\"nan_Mongolia\",\"nan_Namibia\"]\n\nstay_same=[ \"nan_Antigua and Barbuda\",\"nan_Diamond Princess\",\"nan_Saint Vincent and the Grenadines\",\"nan_Timor-Leste\",\"Montserrat_United Kingdom\"]\n\n#\"China\",\n\ntr_frame=train_frame\n\nfeatures_train=tr_frame[names].values   \n\nstandard_confirmed_train=tr_frame[\"ConfirmedCases\"].values\nstandard_fatalities_train=tr_frame[\"Fatalities\"].values\ncurrent_confirmed_train=tr_frame[\"ConfirmedCases\"].values\n\n     \n\nfeatures_cv=[]\nname_cv=[]\nstandard_confirmed_cv=[]\nstandard_fatalities_cv=[]\nnames_=tr_frame[\"key\"].values\ntraining_horizon=int(features_train.shape[0]/len(unique_keys)) \nprint(\"training horizon = \",training_horizon)\nfor dd in range(training_horizon-1,features_train.shape[0],training_horizon):\n    features_cv.append(features_train[dd])\n    name_cv.append(names_[dd])\n    standard_confirmed_cv.append(standard_confirmed_train[dd])\n    standard_fatalities_cv.append(standard_fatalities_train[dd])\n    print (name_cv[-1], standard_confirmed_cv[-1], standard_fatalities_cv[-1])\n    \n \n\nfeatures_cv=np.array(features_cv)\npreds_confirmed_cv=np.zeros((features_cv.shape[0],horizon))\npreds_confirmed_standard_cv=np.zeros((features_cv.shape[0],horizon))\n\npreds_fatalities_cv=np.zeros((features_cv.shape[0],horizon))\npreds_fatalities_standard_cv=np.zeros((features_cv.shape[0],horizon))\n\noveral_rmsle_metric_confirmed=0.0\nprint(\"preds_confirmed_cv.shape[1]\",preds_confirmed_cv.shape[1])\nfor j in range (27):\n\n    this_features_cv=features_cv                          \n\n    preds=predict(features_cv, input_name=model_directory +\"confirmed\"+ str(j))\n    preds_confirmed_cv[:,j]=preds\n    print (\" modelling confirmed, case %d, , original cv %d and after %d \"%(j,this_features_cv.shape[0],this_features_cv.shape[0])) \n\npredictions=[] \nfor ii in range (preds_confirmed_cv.shape[0]):\n    current_prediction=standard_confirmed_cv[ii]\n    if current_prediction==0 :\n        current_prediction=0.1   \n    this_preds=preds_confirmed_cv[ii].tolist()\n    name=name_cv[ii]\n    #overrides\n    if name in decay_4_first_10_then_1:\n        this_preds=decay_4_first_10_then_1_f(this_preds)\n        \n    elif name in decay_2:\n        this_preds=decay_2_f(this_preds)\n        \n    elif name in decay_2_last_12_linear_inter:\n        this_preds=decay_2_last_12_linear_inter_f(this_preds)\n        \n    elif name in decay_1_5:\n        this_preds=decay_1_5_f(this_preds)        \n        \n    elif name in linear_last_12:\n        this_preds=linear_last_12_f(this_preds)\n        \n    elif name in acceleratorx2:\n        this_preds=acceleratorx2_f(this_preds)         \n\n        \n    elif name in stay_same or  \"China\" in name:\n        this_preds=stay_same_f(this_preds)      \n\n    for j in range (preds_confirmed_cv.shape[1]):\n                current_prediction*=max(1,this_preds[j])\n                preds_confirmed_standard_cv[ii][j]=current_prediction\n\n\nfor j in range (27):\n\n    this_features_cv=features_cv\n                             \n    preds=predict(features_cv, input_name=model_directory +\"fatal\"+ str(j))\n    preds_fatalities_cv[:,j]=preds\n    print (\" modelling fatalities, case %d, original cv %d and after %d \"%( j,this_features_cv.shape[0],this_features_cv.shape[0])) \n\npredictions=[]\nfor ii in range (preds_fatalities_cv.shape[0]):\n    current_prediction=standard_fatalities_cv[ii]\n    if current_prediction==0 and standard_confirmed_cv[ii]>400:\n        current_prediction=0.1\n        \n    this_preds=preds_fatalities_cv[ii].tolist()\n    name=name_cv[ii]\n    #overrides\n    if name in decay_4_first_10_then_1:\n        this_preds=decay_4_first_10_then_1_f(this_preds)\n        \n    elif name in decay_2:\n        this_preds=decay_2_f(this_preds)\n        \n    elif name in decay_2_last_12_linear_inter:\n        this_preds=decay_2_last_12_linear_inter_f(this_preds)\n        \n    elif name in decay_1_5:\n        this_preds=decay_1_5_f(this_preds)        \n        \n    elif name in linear_last_12:\n        this_preds=linear_last_12_f(this_preds) \n        \n    elif name in acceleratorx2:\n        this_preds=acceleratorx2_f(this_preds)                 \n        \n    elif name in stay_same or  \"China\" in name:\n        this_preds=stay_same_f(this_preds)         \n        \n    for j in range (preds_fatalities_cv.shape[1]):\n                if current_prediction==0 and  preds_confirmed_standard_cv[ii][j]>400:\n                    current_prediction=1.\n                current_prediction*=max(1,this_preds[j])\n                preds_fatalities_standard_cv[ii][j]=current_prediction\n\n\n\n\n\n\n\n\n\n\n\n# In[9]:\n\n\nkey_to_confirmed_rate={}\nkey_to_fatality_rate={}\nkey_to_confirmed={}\nkey_to_fatality={}\nprint(len(features_cv), len(name_cv),len(standard_confirmed_cv),len(standard_fatalities_cv)) \nprint(preds_confirmed_cv.shape,preds_confirmed_standard_cv.shape,preds_fatalities_cv.shape,preds_fatalities_standard_cv.shape) \n\nfor j in range (len(name_cv)):\n    \n    key_to_confirmed_rate[name_cv[j]]=preds_confirmed_cv[j,:].tolist()\n    #print(key_to_confirmed_rate[name_cv[j]])\n    key_to_fatality_rate[name_cv[j]]=preds_fatalities_cv[j,:].tolist()\n    key_to_confirmed[name_cv[j]]  =preds_confirmed_standard_cv[j,:].tolist()  \n    key_to_fatality[name_cv[j]]=preds_fatalities_standard_cv[j,:].tolist()  \n    \n\n\n# In[10]:\n\n\ntrain_new=train[[\"Date\",\"ConfirmedCases\",\"Fatalities\",\"key\",\"rate_ConfirmedCases\",\"rate_Fatalities\"]]\n\ntest_new=pd.merge(test,train_new, how=\"left\", left_on=[\"key\",\"Date\"], right_on=[\"key\",\"Date\"] ).reset_index(drop=True)\ntest_new\n\n\n# In[11]:\n\n\ndef fillin_columns(frame,key_column, original_name, training_horizon, test_horizon, unique_values, key_to_values):\n    keys=frame[key_column].values\n    original_values=frame[original_name].values.tolist()\n    print(len(keys), len(original_values), training_horizon ,test_horizon,len(key_to_values))\n    \n    for j in range(unique_values):\n        current_index=(j * (training_horizon +test_horizon )) +training_horizon \n        current_key=keys[current_index]\n        values=key_to_values[current_key]\n        co=0\n        for g in range(current_index, current_index + test_horizon):\n            original_values[g]=values[co]\n            co+=1\n    \n    frame[original_name]=original_values\n \n\nall_days=int(test_new.shape[0]/len(unique_keys))\n\ntr_horizon=all_days-horizon\nprint(all_days,tr_horizon, horizon )\n\nfillin_columns(test_new,\"key\", 'ConfirmedCases', tr_horizon, horizon, len(unique_keys), key_to_confirmed)    \nfillin_columns(test_new,\"key\", 'Fatalities', tr_horizon, horizon, len(unique_keys), key_to_fatality)   \nsubmission=test_new[[\"ForecastId\",\"ConfirmedCases\",\"Fatalities\"]]\n\nsubmission.to_csv( \"sub1.csv\", index=False)\nsub1=submission.copy()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#CPMP\n#!/usr/bin/env python\n# coding: utf-8\n\n# In[1]:\n\n\nimport pandas as pd\nimport numpy as np\nfrom scipy.optimize import curve_fit\n\nget_ipython().run_line_magic('matplotlib', 'inline')\nget_ipython().run_line_magic('config', \"InlineBackend.figure_format = 'retina'\")\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import OneHotEncoder\n\nimport xgboost as xgb\n\nfrom tensorflow.keras.optimizers import Nadam\nfrom sklearn.metrics import mean_squared_error\nimport tensorflow as tf\nimport tensorflow.keras.layers as KL\nfrom datetime import timedelta\nimport numpy as np\nimport pandas as pd\n\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression, Ridge\n\nimport datetime\nimport gc\nfrom tqdm import tqdm\n\n\n# In[2]:\n\n\ndef get_cpmp_sub(save_oof=False, save_public_test=False):\n    train = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-3/train.csv')\n    train['Province_State'].fillna('', inplace=True)\n    train['Date'] = pd.to_datetime(train['Date'])\n    train['day'] = train.Date.dt.dayofyear\n    #train = train[train.day <= 85]\n    train['geo'] = ['_'.join(x) for x in zip(train['Country_Region'], train['Province_State'])]\n    train\n\n    test = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-3/test.csv')\n    test['Province_State'].fillna('', inplace=True)\n    test['Date'] = pd.to_datetime(test['Date'])\n    test['day'] = test.Date.dt.dayofyear\n    test['geo'] = ['_'.join(x) for x in zip(test['Country_Region'], test['Province_State'])]\n    test\n\n    day_min = train['day'].min()\n    train['day'] -= day_min\n    test['day'] -= day_min\n\n    min_test_val_day = test.day.min()\n    max_test_val_day = train.day.max()\n    max_test_day = test.day.max()\n    num_days = max_test_day + 1\n\n    min_test_val_day, max_test_val_day, num_days\n\n    train['ForecastId'] = -1\n    test['Id'] = -1\n    test['ConfirmedCases'] = 0\n    test['Fatalities'] = 0\n\n    debug = False\n\n    data = pd.concat([train,\n                      test[test.day > max_test_val_day][train.columns]\n                     ]).reset_index(drop=True)\n    if debug:\n        data = data[data['geo'] >= 'France_'].reset_index(drop=True)\n    #del train, test\n    gc.collect()\n\n    dates = data[data['geo'] == 'France_'].Date.values\n\n    if 0:\n        gr = data.groupby('geo')\n        data['ConfirmedCases'] = gr.ConfirmedCases.transform('cummax')\n        data['Fatalities'] = gr.Fatalities.transform('cummax')\n\n    geo_data = data.pivot(index='geo', columns='day', values='ForecastId')\n    num_geo = geo_data.shape[0]\n    geo_data\n\n    geo_id = {}\n    for i,g in enumerate(geo_data.index):\n        geo_id[g] = i\n\n\n    ConfirmedCases = data.pivot(index='geo', columns='day', values='ConfirmedCases')\n    Fatalities = data.pivot(index='geo', columns='day', values='Fatalities')\n\n    if debug:\n        cases = ConfirmedCases.values\n        deaths = Fatalities.values\n    else:\n        cases = np.log1p(ConfirmedCases.values)\n        deaths = np.log1p(Fatalities.values)\n\n\n    def get_dataset(start_pred, num_train, lag_period):\n        days = np.arange( start_pred - num_train + 1, start_pred + 1)\n        lag_cases = np.vstack([cases[:, d - lag_period : d] for d in days])\n        lag_deaths = np.vstack([deaths[:, d - lag_period : d] for d in days])\n        target_cases = np.vstack([cases[:, d : d + 1] for d in days])\n        target_deaths = np.vstack([deaths[:, d : d + 1] for d in days])\n        geo_ids = np.vstack([geo_ids_base for d in days])\n        country_ids = np.vstack([country_ids_base for d in days])\n        return lag_cases, lag_deaths, target_cases, target_deaths, geo_ids, country_ids, days\n\n    def update_valid_dataset(data, pred_death, pred_case):\n        lag_cases, lag_deaths, target_cases, target_deaths, geo_ids, country_ids, days = data\n        day = days[-1] + 1\n        new_lag_cases = np.hstack([lag_cases[:, 1:], pred_case])\n        new_lag_deaths = np.hstack([lag_deaths[:, 1:], pred_death]) \n        new_target_cases = cases[:, day:day+1]\n        new_target_deaths = deaths[:, day:day+1] \n        new_geo_ids = geo_ids  \n        new_country_ids = country_ids  \n        new_days = 1 + days\n        return new_lag_cases, new_lag_deaths, new_target_cases, new_target_deaths, new_geo_ids, new_country_ids, new_days\n\n    def fit_eval(lr_death, lr_case, data, start_lag_death, end_lag_death, num_lag_case, fit, score):\n        lag_cases, lag_deaths, target_cases, target_deaths, geo_ids, country_ids, days = data\n\n        X_death = np.hstack([lag_cases[:, -start_lag_death:-end_lag_death], country_ids])\n        X_death = np.hstack([lag_deaths[:, -num_lag_case:], country_ids])\n        X_death = np.hstack([lag_cases[:, -start_lag_death:-end_lag_death], lag_deaths[:, -num_lag_case:], country_ids])\n        y_death = target_deaths\n        y_death_prev = lag_deaths[:, -1:]\n        if fit:\n            if 0:\n                keep = (y_death > 0).ravel()\n                X_death = X_death[keep]\n                y_death = y_death[keep]\n                y_death_prev = y_death_prev[keep]\n            lr_death.fit(X_death, y_death)\n        y_pred_death = lr_death.predict(X_death)\n        y_pred_death = np.maximum(y_pred_death, y_death_prev)\n\n        X_case = np.hstack([lag_cases[:, -num_lag_case:], geo_ids])\n        X_case = lag_cases[:, -num_lag_case:]\n        y_case = target_cases\n        y_case_prev = lag_cases[:, -1:]\n        if fit:\n            lr_case.fit(X_case, y_case)\n        y_pred_case = lr_case.predict(X_case)\n        y_pred_case = np.maximum(y_pred_case, y_case_prev)\n\n        if score:\n            death_score = val_score(y_death, y_pred_death)\n            case_score = val_score(y_case, y_pred_case)\n        else:\n            death_score = 0\n            case_score = 0\n\n        return death_score, case_score, y_pred_death, y_pred_case\n\n    def train_model(train, valid, start_lag_death, end_lag_death, num_lag_case, num_val, score=True):\n        alpha = 2#3\n        lr_death = Ridge(alpha=alpha, fit_intercept=False)\n        lr_case = Ridge(alpha=alpha, fit_intercept=True)\n\n        (train_death_score, train_case_score, train_pred_death, train_pred_case,\n        ) = fit_eval(lr_death, lr_case, train, start_lag_death, end_lag_death, num_lag_case, fit=True, score=score)\n\n        death_scores = []\n        case_scores = []\n\n        death_pred = []\n        case_pred = []\n\n        for i in range(num_val):\n\n            (valid_death_score, valid_case_score, valid_pred_death, valid_pred_case,\n            ) = fit_eval(lr_death, lr_case, valid, start_lag_death, end_lag_death, num_lag_case, fit=False, score=score)\n\n            death_scores.append(valid_death_score)\n            case_scores.append(valid_case_score)\n            death_pred.append(valid_pred_death)\n            case_pred.append(valid_pred_case)\n\n            if 0:\n                print('val death: %0.3f' %  valid_death_score,\n                      'val case: %0.3f' %  valid_case_score,\n                      'val : %0.3f' %  np.mean([valid_death_score, valid_case_score]),\n                      flush=True)\n            valid = update_valid_dataset(valid, valid_pred_death, valid_pred_case)\n\n        if score:\n            death_scores = np.sqrt(np.mean([s**2 for s in death_scores]))\n            case_scores = np.sqrt(np.mean([s**2 for s in case_scores]))\n            if 0:\n                print('train death: %0.3f' %  train_death_score,\n                      'train case: %0.3f' %  train_case_score,\n                      'val death: %0.3f' %  death_scores,\n                      'val case: %0.3f' %  case_scores,\n                      'val : %0.3f' % ( (death_scores + case_scores) / 2),\n                      flush=True)\n            else:\n                print('%0.4f' %  case_scores,\n                      ', %0.4f' %  death_scores,\n                      '= %0.4f' % ( (death_scores + case_scores) / 2),\n                      flush=True)\n        death_pred = np.hstack(death_pred)\n        case_pred = np.hstack(case_pred)\n        return death_scores, case_scores, death_pred, case_pred\n\n    countries = [g.split('_')[0] for g in geo_data.index]\n    countries = pd.factorize(countries)[0]\n\n    country_ids_base = countries.reshape((-1, 1))\n    ohe = OneHotEncoder(sparse=False)\n    country_ids_base = 0.2 * ohe.fit_transform(country_ids_base)\n    country_ids_base.shape\n\n    geo_ids_base = np.arange(num_geo).reshape((-1, 1))\n    ohe = OneHotEncoder(sparse=False)\n    geo_ids_base = 0.1 * ohe.fit_transform(geo_ids_base)\n    geo_ids_base.shape\n\n    def val_score(true, pred):\n        pred = np.log1p(np.round(np.expm1(pred) - 0.2))\n        return np.sqrt(mean_squared_error(true.ravel(), pred.ravel()))\n\n    def val_score(true, pred):\n        return np.sqrt(mean_squared_error(true.ravel(), pred.ravel()))\n\n\n\n    start_lag_death, end_lag_death = 14, 6,\n    num_train = 10#5\n    num_lag_case = 28#14\n    lag_period = max(start_lag_death, num_lag_case)\n\n    def get_oof(start_val_delta=0):   \n        start_val = min_test_val_day + start_val_delta\n        last_train = start_val - 1\n        num_val = max_test_val_day - start_val + 1\n        print(dates[start_val], start_val, num_val)\n        train_data = get_dataset(last_train, num_train, lag_period)\n        valid_data = get_dataset(start_val, 1, lag_period)\n        _, _, val_death_preds, val_case_preds = train_model(train_data, valid_data, \n                                                            start_lag_death, end_lag_death, num_lag_case, num_val)\n\n        pred_deaths = Fatalities.iloc[:, start_val:start_val+num_val].copy()\n        pred_deaths.iloc[:, :] = np.expm1(val_death_preds)\n        pred_deaths = pred_deaths.stack().reset_index()\n        pred_deaths.columns = ['geo', 'day', 'Fatalities']\n        pred_deaths\n\n        pred_cases = ConfirmedCases.iloc[:, start_val:start_val+num_val].copy()\n        pred_cases.iloc[:, :] = np.expm1(val_case_preds)\n        pred_cases = pred_cases.stack().reset_index()\n        pred_cases.columns = ['geo', 'day', 'ConfirmedCases']\n        pred_cases\n\n        sub = train[['Date', 'Id', 'geo', 'day']]\n        sub = sub.merge(pred_cases, how='left', on=['geo', 'day'])\n        sub = sub.merge(pred_deaths, how='left', on=['geo', 'day'])\n        #sub = sub.fillna(0)\n        sub = sub[sub.day >= start_val]\n        sub = sub[['Id', 'ConfirmedCases', 'Fatalities']].copy()\n        return sub\n\n\n    if save_oof:\n        for start_val_delta, date in zip(range(3, -8, -3),\n                                  ['2020-03-22', '2020-03-19', '2020-03-16', '2020-03-13']):\n            print(date, end=' ')\n            oof = get_oof(start_val_delta)\n            oof.to_csv('../submissions/cpmp-%s.csv' % date, index=None)\n\n    def get_sub(start_val_delta=0):   \n        start_val = min_test_val_day + start_val_delta\n        last_train = start_val - 1\n        num_val = max_test_val_day - start_val + 1\n        print(dates[last_train], start_val, num_val)\n        num_lag_case = 28#14\n        train_data = get_dataset(last_train, num_train, lag_period)\n        valid_data = get_dataset(start_val, 1, lag_period)\n        _, _, val_death_preds, val_case_preds = train_model(train_data, valid_data, \n                                                            start_lag_death, end_lag_death, num_lag_case, num_val)\n\n        pred_deaths = Fatalities.iloc[:, start_val:start_val+num_val].copy()\n        pred_deaths.iloc[:, :] = np.expm1(val_death_preds)\n        pred_deaths = pred_deaths.stack().reset_index()\n        pred_deaths.columns = ['geo', 'day', 'Fatalities']\n        pred_deaths\n\n        pred_cases = ConfirmedCases.iloc[:, start_val:start_val+num_val].copy()\n        pred_cases.iloc[:, :] = np.expm1(val_case_preds)\n        pred_cases = pred_cases.stack().reset_index()\n        pred_cases.columns = ['geo', 'day', 'ConfirmedCases']\n        pred_cases\n\n        sub = test[['Date', 'ForecastId', 'geo', 'day']]\n        sub = sub.merge(pred_cases, how='left', on=['geo', 'day'])\n        sub = sub.merge(pred_deaths, how='left', on=['geo', 'day'])\n        sub = sub.fillna(0)\n        sub = sub[['ForecastId', 'ConfirmedCases', 'Fatalities']]\n        return sub\n        return sub\n\n\n    known_test = train[['geo', 'day', 'ConfirmedCases', 'Fatalities']\n              ].merge(test[['geo', 'day', 'ForecastId']], how='left', on=['geo', 'day'])\n    known_test = known_test[['ForecastId', 'ConfirmedCases', 'Fatalities']][known_test.ForecastId.notnull()].copy()\n    known_test\n\n    unknow_test = test[test.day > max_test_val_day]\n    unknow_test\n\n    def get_final_sub():   \n        start_val = max_test_val_day + 1\n        last_train = start_val - 1\n        num_val = max_test_day - start_val + 1\n        print(dates[last_train], start_val, num_val)\n        num_lag_case = num_val + 3\n        train_data = get_dataset(last_train, num_train, lag_period)\n        valid_data = get_dataset(start_val, 1, lag_period)\n        (_, _, val_death_preds, val_case_preds\n        ) = train_model(train_data, valid_data, start_lag_death, end_lag_death, num_lag_case, num_val, score=False)\n\n        pred_deaths = Fatalities.iloc[:, start_val:start_val+num_val].copy()\n        pred_deaths.iloc[:, :] = np.expm1(val_death_preds)\n        pred_deaths = pred_deaths.stack().reset_index()\n        pred_deaths.columns = ['geo', 'day', 'Fatalities']\n        pred_deaths\n\n        pred_cases = ConfirmedCases.iloc[:, start_val:start_val+num_val].copy()\n        pred_cases.iloc[:, :] = np.expm1(val_case_preds)\n        pred_cases = pred_cases.stack().reset_index()\n        pred_cases.columns = ['geo', 'day', 'ConfirmedCases']\n        pred_cases\n        print(unknow_test.shape, pred_deaths.shape, pred_cases.shape)\n\n        sub = unknow_test[['Date', 'ForecastId', 'geo', 'day']]\n        sub = sub.merge(pred_cases, how='left', on=['geo', 'day'])\n        sub = sub.merge(pred_deaths, how='left', on=['geo', 'day'])\n        #sub = sub.fillna(0)\n        sub = sub[['ForecastId', 'ConfirmedCases', 'Fatalities']]\n        sub = pd.concat([known_test, sub])\n        return sub\n\n    if save_public_test:\n        sub = get_sub()\n    else:\n        sub = get_final_sub()\n    return sub\n\n\n# In[3]:\n\n\ndef get_nn_sub():\n    df = pd.read_csv(\"../input/covid19-global-forecasting-week-3/train.csv\")\n    sub_df = pd.read_csv(\"../input/covid19-global-forecasting-week-3/test.csv\")\n\n    coo_df = pd.read_csv(\"../input/covidweek1/train.csv\").rename(columns={\"Country/Region\": \"Country_Region\"})\n    coo_df = coo_df.groupby(\"Country_Region\")[[\"Lat\", \"Long\"]].mean().reset_index()\n    coo_df = coo_df[coo_df[\"Country_Region\"].notnull()]\n\n    loc_group = [\"Province_State\", \"Country_Region\"]\n\n\n    def preprocess(df):\n        df[\"Date\"] = df[\"Date\"].astype(\"datetime64[ms]\")\n        df[\"days\"] = (df[\"Date\"] - pd.to_datetime(\"2020-01-01\")).dt.days\n        df[\"weekend\"] = df[\"Date\"].dt.dayofweek//5\n\n        df = df.merge(coo_df, how=\"left\", on=\"Country_Region\")\n        df[\"Lat\"] = (df[\"Lat\"] // 30).astype(np.float32).fillna(0)\n        df[\"Long\"] = (df[\"Long\"] // 60).astype(np.float32).fillna(0)\n\n        for col in loc_group:\n            df[col].fillna(\"none\", inplace=True)\n        return df\n\n    df = preprocess(df)\n    sub_df = preprocess(sub_df)\n\n    print(df.shape)\n\n    TARGETS = [\"ConfirmedCases\", \"Fatalities\"]\n\n    for col in TARGETS:\n        df[col] = np.log1p(df[col])\n\n    NUM_SHIFT = 10#5\n\n    features = [\"Lat\", \"Long\"]\n\n    for s in range(1, NUM_SHIFT+1):\n        for col in TARGETS:\n            df[\"prev_{}_{}\".format(col, s)] = df.groupby(loc_group)[col].shift(s)\n            features.append(\"prev_{}_{}\".format(col, s))\n\n    df = df[df[\"Date\"] >= df[\"Date\"].min() + timedelta(days=NUM_SHIFT)].copy()\n\n    TEST_FIRST = sub_df[\"Date\"].min() # pd.to_datetime(\"2020-03-13\") #\n    TEST_DAYS = (df[\"Date\"].max() - TEST_FIRST).days + 1\n\n    dev_df, test_df = df[df[\"Date\"] < TEST_FIRST].copy(), df[df[\"Date\"] >= TEST_FIRST].copy()\n\n    def nn_block(input_layer, size, dropout_rate, activation):\n        out_layer = KL.Dense(size, activation=None)(input_layer)\n        #out_layer = KL.BatchNormalization()(out_layer)\n        out_layer = KL.Activation(activation)(out_layer)\n        out_layer = KL.Dropout(dropout_rate)(out_layer)\n        return out_layer\n\n\n    def get_model():\n        inp = KL.Input(shape=(len(features),))\n\n        hidden_layer = nn_block(inp, 128, 0.0, \"relu\")\n        gate_layer = nn_block(hidden_layer, 64, 0.0, \"sigmoid\")\n        hidden_layer = nn_block(hidden_layer, 64, 0.0, \"relu\")\n        hidden_layer = KL.multiply([hidden_layer, gate_layer])\n\n        out = KL.Dense(len(TARGETS), activation=\"linear\")(hidden_layer)\n\n        model = tf.keras.models.Model(inputs=[inp], outputs=out)\n        return model\n\n    get_model().summary()\n\n    def get_input(df):\n        return [df[features]]\n\n    NUM_MODELS = 10\n\n\n    def train_models(df, save=False):\n        models = []\n        for i in range(NUM_MODELS):\n            model = get_model()\n            model.compile(loss=\"mean_squared_error\", optimizer=Nadam(lr=1e-4))\n            hist = model.fit(get_input(df), df[TARGETS],\n                             batch_size=2048, epochs=500, verbose=0, shuffle=True)\n            if save:\n                model.save_weights(\"model{}.h5\".format(i))\n            models.append(model)\n        return models\n\n    models = train_models(dev_df)\n\n\n    prev_targets = ['prev_ConfirmedCases_1', 'prev_Fatalities_1']\n\n    def predict_one(df, models):\n        pred = np.zeros((df.shape[0], 2))\n        for model in models:\n            pred += model.predict(get_input(df))/len(models)\n        pred = np.maximum(pred, df[prev_targets].values)\n        pred[:, 0] = np.log1p(np.expm1(pred[:, 0]) + 0.1)\n        pred[:, 1] = np.log1p(np.expm1(pred[:, 1]) + 0.01)\n        return np.clip(pred, None, 15)\n\n    print([mean_squared_error(dev_df[TARGETS[i]], predict_one(dev_df, models)[:, i]) for i in range(len(TARGETS))])\n\n\n    def rmse(y_true, y_pred):\n        return np.sqrt(mean_squared_error(y_true, y_pred))\n\n    def evaluate(df):\n        error = 0\n        for col in TARGETS:\n            error += rmse(df[col].values, df[\"pred_{}\".format(col)].values)\n        return np.round(error/len(TARGETS), 5)\n\n\n    def predict(test_df, first_day, num_days, models, val=False):\n        temp_df = test_df.loc[test_df[\"Date\"] == first_day].copy()\n        y_pred = predict_one(temp_df, models)\n\n        for i, col in enumerate(TARGETS):\n            test_df[\"pred_{}\".format(col)] = 0\n            test_df.loc[test_df[\"Date\"] == first_day, \"pred_{}\".format(col)] = y_pred[:, i]\n\n        print(first_day, np.isnan(y_pred).sum(), y_pred.min(), y_pred.max())\n        if val:\n            print(evaluate(test_df[test_df[\"Date\"] == first_day]))\n\n\n        y_prevs = [None]*NUM_SHIFT\n\n        for i in range(1, NUM_SHIFT):\n            y_prevs[i] = temp_df[['prev_ConfirmedCases_{}'.format(i), 'prev_Fatalities_{}'.format(i)]].values\n\n        for d in range(1, num_days):\n            date = first_day + timedelta(days=d)\n            print(date, np.isnan(y_pred).sum(), y_pred.min(), y_pred.max())\n\n            temp_df = test_df.loc[test_df[\"Date\"] == date].copy()\n            temp_df[prev_targets] = y_pred\n            for i in range(2, NUM_SHIFT+1):\n                temp_df[['prev_ConfirmedCases_{}'.format(i), 'prev_Fatalities_{}'.format(i)]] = y_prevs[i-1]\n\n            y_pred, y_prevs = predict_one(temp_df, models), [None, y_pred] + y_prevs[1:-1]\n\n\n            for i, col in enumerate(TARGETS):\n                test_df.loc[test_df[\"Date\"] == date, \"pred_{}\".format(col)] = y_pred[:, i]\n\n            if val:\n                print(evaluate(test_df[test_df[\"Date\"] == date]))\n\n        return test_df\n\n    test_df = predict(test_df, TEST_FIRST, TEST_DAYS, models, val=True)\n    print(evaluate(test_df))\n\n    for col in TARGETS:\n        test_df[col] = np.expm1(test_df[col])\n        test_df[\"pred_{}\".format(col)] = np.expm1(test_df[\"pred_{}\".format(col)])\n\n    models = train_models(df, save=True)\n\n    sub_df_public = sub_df[sub_df[\"Date\"] <= df[\"Date\"].max()].copy()\n    sub_df_private = sub_df[sub_df[\"Date\"] > df[\"Date\"].max()].copy()\n\n    pred_cols = [\"pred_{}\".format(col) for col in TARGETS]\n    #sub_df_public = sub_df_public.merge(test_df[[\"Date\"] + loc_group + pred_cols].rename(columns={col: col[5:] for col in pred_cols}), \n    #                                    how=\"left\", on=[\"Date\"] + loc_group)\n    sub_df_public = sub_df_public.merge(test_df[[\"Date\"] + loc_group + TARGETS], how=\"left\", on=[\"Date\"] + loc_group)\n\n    SUB_FIRST = sub_df_private[\"Date\"].min()\n    SUB_DAYS = (sub_df_private[\"Date\"].max() - sub_df_private[\"Date\"].min()).days + 1\n\n    sub_df_private = df.append(sub_df_private, sort=False)\n\n    for s in range(1, NUM_SHIFT+1):\n        for col in TARGETS:\n            sub_df_private[\"prev_{}_{}\".format(col, s)] = sub_df_private.groupby(loc_group)[col].shift(s)\n\n    sub_df_private = sub_df_private[sub_df_private[\"Date\"] >= SUB_FIRST].copy()\n\n    sub_df_private = predict(sub_df_private, SUB_FIRST, SUB_DAYS, models)\n\n    for col in TARGETS:\n        sub_df_private[col] = np.expm1(sub_df_private[\"pred_{}\".format(col)])\n\n    sub_df = sub_df_public.append(sub_df_private, sort=False)\n    sub_df[\"ForecastId\"] = sub_df[\"ForecastId\"].astype(np.int16)\n\n    return sub_df[[\"ForecastId\"] + TARGETS]\n\n\n# In[4]:\n\n\nsub1 = get_cpmp_sub()\nsub1['ForecastId'] = sub1['ForecastId'].astype('int')\n\n\n# In[5]:\n\n\nsub2 = get_nn_sub()\n\n\n# In[6]:\n\n\nsub1.sort_values(\"ForecastId\", inplace=True)\nsub2.sort_values(\"ForecastId\", inplace=True)\n\n\n# In[7]:\n\n\nsub1.to_csv(\"sub1.csv\",index=False)\nsub2.to_csv(\"sub2.csv\",index=False)\n\n\n# In[8]:\n\n\nfrom sklearn.metrics import mean_squared_error\n\nTARGETS = [\"ConfirmedCases\", \"Fatalities\"]\n\n[np.sqrt(mean_squared_error(np.log1p(sub1[t].values), np.log1p(sub2[t].values))) for t in TARGETS]\n\n\n# In[9]:\n\n\nsub_df = sub1.copy()\nfor t in TARGETS:\n    sub_df[t] = np.expm1(np.log1p(sub1[t].values)*0.5 + np.log1p(sub2[t].values)*0.5)\n    \nsub_df.to_csv(\"sub2.csv\", index=False)\nsub2=sub_df.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ROSS\n#!/usr/bin/env python\n# coding: utf-8\n\n# In[1]:\n\n\nimport numpy as np\nimport pandas as pd\nimport os, gc\nimport lightgbm as lgb\nimport xgboost as xgb\nimport catboost as ctb\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nget_ipython().run_line_magic('matplotlib', 'inline')\nfrom datetime import date, datetime, timedelta\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.neighbors import KNeighborsRegressor, NearestNeighbors\nfrom scipy.optimize import nnls\npd.set_option('display.max_columns', 10)\npd.set_option('display.max_rows', 10)\nnp.set_printoptions(precision=6, suppress=True)\n\n\n# In[2]:\n\n\n# note: all data update 2020-04-01, china rule, smooth weights\nmname = 'gbt1u'\npath = '/kaggle/input/gbt1n-external/'\npathk = '/kaggle/input/covid19-global-forecasting-week-3/'\nnhorizon = 30\nskip = 0\n# kv = [3]\n# kv = [6]\nkv = [6,11]\n# kv = [13]\ntrain_full = True\nsave_data = False\n\n# booster = ['lgb','xgb']\nbooster = ['lgb','xgb','ctb']\n# booster = ['cas']\n\n# if using updated daily data, also update time-varying external data\n# in COVID-19 and covid-19-data, git pull origin master \n# ecdc wget https://opendata.ecdc.europa.eu/covid19/casedistribution/csv\n# weather: https://www.kaggle.com/davidbnn92/weather-data/output?scriptVersionId=31103959\n# google trends: pytrends0b.ipynb\n\n\n# In[3]:\n\n\ntrain = pd.read_csv(pathk+'train.csv')\ntest = pd.read_csv(pathk+'test.csv')\nss = pd.read_csv(pathk+'submission.csv')\n\n\n# In[4]:\n\n\ntrain\n\n\n# In[5]:\n\n\n# tmax and dmax are the last day of training\ntmax = train.Date.max()\ndmax = datetime.strptime(tmax,'%Y-%m-%d').date()\nprint(tmax, dmax)\n\n\n# In[6]:\n\n\n# ddate is the last day of validation training\nddate = dmax - timedelta(days=nhorizon)\nddate\n\n\n# In[7]:\n\n\ntest\n\n\n# In[8]:\n\n\nfmax = test.Date.max()\nfdate = datetime.strptime(fmax,'%Y-%m-%d').date()\nfdate\n\n\n# In[9]:\n\n\ntmin = train.Date.min()\nfmin = test.Date.min()\ntmin, fmin\n\n\n# In[10]:\n\n\ndmin = datetime.strptime(tmin,'%Y-%m-%d').date()\nprint(dmin)\n\n\n# In[11]:\n\n\n# train['ForecastId'] = train.Id - train.Id.max()\ncp = ['Country_Region','Province_State']\ncpd = cp + ['Date']\ntrain = train.merge(test[cpd+['ForecastId']], how='left', on=cpd)\ntrain['ForecastId'] = train['ForecastId'].fillna(0).astype(int)\ntrain['y0_pred'] = np.nan\ntrain['y1_pred'] = np.nan\n\ntest['Id'] = test.ForecastId + train.Id.max()\ntest['ConfirmedCases'] = np.nan\ntest['Fatalities'] = np.nan\n# use zeros here instead of nans so monotonic adjustment fills final dates if necessary\ntest['y0_pred'] = 0.0\ntest['y1_pred'] = 0.0\n\n\n# In[12]:\n\n\n# concat non-overlapping part of test to train for feature engineering\nd = pd.concat((train,test[test.Date > train.Date.max()])).reset_index(drop=True)\nd\n\n\n# In[13]:\n\n\n(dmin + timedelta(30)).isoformat()\n\n\n# In[14]:\n\n\nd['Date'].value_counts().std()\n\n\n# In[15]:\n\n\n# fill missing province with blank, must also do this with external data before merging\nd[cp] = d[cp].fillna('')\n\n# create single location variable\nd['Loc'] = d['Country_Region'] + ' ' + d['Province_State']\nd['Loc'] = d['Loc'].str.strip()\nd['Loc'].value_counts()\n\n\n# In[16]:\n\n\n# sort by location then date\nd = d.sort_values(['Loc','Date']).reset_index(drop=True)\n\n\n# In[17]:\n\n\nd['Country_Region'].value_counts(dropna=False)\n\n\n# In[18]:\n\n\nd['Province_State'].value_counts(dropna=False)\n\n\n# In[19]:\n\n\ngt = pd.read_csv(path+'google_trends.csv')\ngt[cp] = gt[cp].fillna('')\ngt\n\n\n# In[20]:\n\n\n# since trends data lags behind a day or two, shift the date to make it contemporaneous\ngmax = gt.Date.max()\ngmax = datetime.strptime(gmax,'%Y-%m-%d').date()\ngoff = (dmax - gmax).days\nprint(dmax, gmax, goff)\ngt['Date'] = (pd.to_datetime(gt.Date) + timedelta(goff)).dt.strftime('%Y-%m-%d')\ngt['google_covid'] = gt['coronavirus'] + gt['covid-19'] + gt['covid19']\ngt.drop(['coronavirus','covid-19','covid19'], axis=1, inplace=True)\ngoogle = ['google_covid']\ngt\n\n\n# In[21]:\n\n\nd = d.merge(gt, how='left', on=['Country_Region','Province_State','Date'])\nd\n\n\n# In[22]:\n\n\nd['google_covid'].describe()\n\n\n# In[23]:\n\n\n# merge country info\ncountry = pd.read_csv(path+'covid19countryinfo1.csv')\n# country[\"pop\"] = country[\"pop\"].str.replace(\",\",\"\").astype(float)\ncountry\n\n\n# In[24]:\n\n\ncountry.columns\n\n\n# In[25]:\n\n\nd.shape\n\n\n# In[26]:\n\n\n# first merge by country\nd = d.merge(country.loc[country.medianage.notnull(),['country','pop','testpop','medianage']],\n            how='left', left_on='Country_Region', right_on='country')\nd\n\n\n# In[27]:\n\n\n# then merge by province\nc1 = country.loc[country.medianage.isnull(),['country','pop','testpop']]\nprint(c1.shape)\nc1.columns = ['Province_State','pop1','testpop1']\n# d.update(c1)\nd = d.merge(c1,how='left',on='Province_State')\nd.loc[d.pop1.notnull(),'pop'] = d.loc[d.pop1.notnull(),'pop1']\nd.loc[d.testpop1.notnull(),'testpop'] = d.loc[d.testpop1.notnull(),'testpop1']\nd.drop(['pop1','testpop1'], axis=1, inplace=True)\nprint(d.shape)\nprint(d.loc[(d.Date=='2020-03-25') & (d['Province_State']=='New York')])\n\n\n# In[28]:\n\n\n# testing data time series, us states only, would love to have this for all countries\nct = pd.read_csv(path+'states_daily_4pm_et.csv')\nsi = pd.read_csv(path+'states_info.csv')\nsi = si.rename(columns={'name':'Province_State'})\nct = ct.merge(si[['state','Province_State']], how='left', on='state')\nct['Date'] = ct['date'].apply(str).transform(lambda x: '-'.join([x[:4], x[4:6], x[6:]]))\nct.loc[ct.Province_State=='US Virgin Islands','Province_State'] = 'Virgin Islands'\nct.loc[ct.Province_State=='District Of Columbia','Province_State'] = 'District of Columbia'\npd.set_option('display.max_rows', 20)\nct\n# ct = ct['Date','state','total']\n\n\n# In[29]:\n\n\nckeep = ['positive','negative','totalTestResults']\nfor c in ckeep: ct[c] = np.log1p(ct[c])\n\n\n# In[30]:\n\n\nd = d.merge(ct[['Province_State','Date']+ckeep], how='left',\n            on=['Province_State','Date'])\nd\n\n\n# In[31]:\n\n\nw = pd.read_csv(path+'training_data_with_weather_info_week_2.csv')\nw.drop(['Id','Id.1','ConfirmedCases','Fatalities','country+province','day_from_jan_first'], axis=1, inplace=True)\nw[cp] = w[cp].fillna('')\nwf = list(w.columns[5:])\nw\n\n\n# In[32]:\n\n\n# since weather data lags behind a day or two, adjust the date to make it contemporaneous\nwmax = w.Date.max()\nwmax = datetime.strptime(wmax,'%Y-%m-%d').date()\nwoff = (dmax - wmax).days\nprint(dmax, wmax, woff)\nw['Date'] = (pd.to_datetime(w.Date) + timedelta(woff)).dt.strftime('%Y-%m-%d')\nw\n\n\n# In[33]:\n\n\n# merge Lat and Long for all times and the time-varying weather data based on date\nd = d.merge(w[cp+['Lat','Long']].drop_duplicates(), how='left', on=cp)\nw.drop(['Lat','Long'],axis=1,inplace=True)\nd = d.merge(w, how='left', on=cpd)\nd\n\n\n# In[34]:\n\n\n# combine ecdc and nytimes data\necdc = pd.read_csv(path+'ecdc.csv', encoding = 'latin')\necdc\n\n\n# In[35]:\n\n\n# combine ecdc and nytimes data as extra y0 and y1\n# https://opendata.ecdc.europa.eu/covid19/casedistribution/csv\necdc['Date'] = pd.to_datetime(ecdc[['year','month','day']]).dt.strftime('%Y-%m-%d')\necdc = ecdc.rename(mapper={'countriesAndTerritories':'Country_Region'}, axis=1)\necdc['Country_Region'] = ecdc['Country_Region'].replace('_',' ',regex=True)\necdc['Province_State'] = ''\necdc['cc'] = ecdc.groupby(cp)['cases'].cummax()\necdc['extra_y0'] = np.log1p(ecdc.cc)\necdc['cd'] = ecdc.groupby(cp)['deaths'].cummax()\necdc['extra_y1'] = np.log1p(ecdc.cd)\necdc = ecdc[cpd + ['extra_y0','extra_y1']]\necdc[::63]\n\n\n# In[36]:\n\n\necdc = ecdc[ecdc.Date >= '2020-01-22']\necdc\n\n\n# In[37]:\n\n\n# https://github.com/nytimes/covid-19-data\nnyt = pd.read_csv(path+'us-states.csv')\nnyt['extra_y0'] = np.log1p(nyt.cases)\nnyt['extra_y1'] = np.log1p(nyt.deaths)\nnyt['Country_Region'] = 'US'\nnyt = nyt.rename(mapper={'date':'Date','state':'Province_State'},axis=1)\nnyt.drop(['fips','cases','deaths'],axis=1,inplace=True)\nnyt\n\n\n# In[38]:\n\n\nextra = pd.concat([ecdc,nyt], sort=True)\nextra\n\n\n# In[39]:\n\n\nd = d.merge(extra, how='left', on=cpd)\nd\n\n\n# In[40]:\n\n\nd[['extra_y0','extra_y1']].describe()\n\n\n# In[41]:\n\n\n# recovered data from hopkins, https://github.com/CSSEGISandData/COVID-19\nrecovered = pd.read_csv(path+'time_series_covid19_recovered_global.csv')\nrecovered = recovered.rename(mapper={'Country/Region':'Country_Region','Province/State':'Province_State'}, axis=1)\nrecovered[cp] = recovered[cp].fillna('')\nrecovered = recovered.drop(['Lat','Long'], axis=1)\nrecovered\n\n\n# In[42]:\n\n\n# replace US row with identical rows for every US state\nusp = d.loc[d.Country_Region=='US','Province_State'].unique()\nprint(usp, len(usp))\nrus = recovered[recovered.Country_Region=='US']\nrus\n\n\n# In[43]:\n\n\nrus = rus.reindex(np.repeat(rus.index.values,len(usp)))\nrus.loc[:,'Province_State'] = usp\nrus\n\n\n# In[44]:\n\n\nrecovered =  recovered[recovered.Country_Region!='US']\nrecovered = pd.concat([recovered,rus]).reset_index(drop=True)\nrecovered\n\n\n# In[45]:\n\n\n# melt and merge\nrm = pd.melt(recovered, id_vars=cp, var_name='d', value_name='recov')\nrm\n\n\n# In[46]:\n\n\nrm['Date'] = pd.to_datetime(rm.d)\nrm.drop('d',axis=1,inplace=True)\nrm['Date'] = rm['Date'].dt.strftime('%Y-%m-%d')\nrm\n\n\n# In[47]:\n\n\nd = d.merge(rm, how='left', on=['Country_Region','Province_State','Date'])\nd\n\n\n# In[48]:\n\n\nd['recov'].describe()\n\n\n# In[49]:\n\n\n# approximate US state recovery via proportion of confirmed cases\nd['ccsum'] = d.groupby(['Country_Region','Date'])['ConfirmedCases'].transform(lambda x: x.sum())\nd.loc[d.Country_Region=='US','recov'] = d.loc[d.Country_Region=='US','recov'] *                                         d.loc[d.Country_Region=='US','ConfirmedCases'] /                                         (d.loc[d.Country_Region=='US','ccsum'] + 1)\n\n\n# In[50]:\n\n\nd.loc[:,'recov'] = np.log1p(d.recov)\n# d.loc[:,'recov'] = d['recov'].fillna(0)\n\n\n# In[51]:\n\n\nd.loc[d.Province_State=='North Carolina','recov'][45:55]\n\n\n# In[52]:\n\n\n# log1p transform both targets\nynames = ['ConfirmedCases','Fatalities']\nny = len(ynames)\nyv = []\nfor i in range(ny):\n    v = 'y'+str(i)\n    d[v] = np.log1p(d[ynames[i]])\n    yv.append(v)\nprint(d[yv].describe())\n\n\n# In[53]:\n\n\nd['rate0'] = d.y0 - np.log(d['pop'])\nd['rate1'] = d.y1 - np.log(d['pop'])\n\n\n# In[54]:\n\n\nd = d.sort_values(['Loc','Date']).reset_index(drop=True)\nd.shape\n\n\n# In[55]:\n\n\n# compute nearest neighbors\nregions = d[['Loc','Lat','Long']].drop_duplicates('Loc').reset_index(drop=True)\nregions\n\n\n# In[56]:\n\n\n# regions.to_csv('regions.csv', index=False)\n\n\n# In[57]:\n\n\n# knn max features\nk = kv[0]\nnn = NearestNeighbors(k)\nregions[['Lat','Long']]=regions[['Lat','Long']].fillna(-1)\nnn.fit(regions[['Lat','Long']])\n\n\n# In[58]:\n\n\n# first matrix is distances, second indices to nearest neighbors including self\n# note two cruise ships are replicated and have identical lat, long values\nknn = nn.kneighbors(regions[['Lat','Long']])\nknn\n\n\n# In[59]:\n\n\nns = d['Loc'].nunique()\n\n\n# In[60]:\n\n\n# time series matrix\nky = d['y0'].values.reshape(ns,-1)\nprint(ky.shape)\n\nprint(ky[0])\n\n# use knn indices to create neighbors\nknny = ky[knn[1]]\nprint(knny.shape)\n\nknny = knny.transpose((0,2,1)).reshape(-1,k)\nprint(knny.shape)\n\n\n# In[61]:\n\n\n# knn max features\nnk = len(kv)\nkp = []\nkd = []\nns = regions.shape[0]\nfor k in kv:\n    nn = NearestNeighbors(k)\n    nn.fit(regions[['Lat','Long']])\n    knn = nn.kneighbors(regions[['Lat','Long']])\n    kp.append('knn'+str(k)+'_')\n    kd.append('kd'+str(k)+'_')\n    for i in range(ny):\n        yi = 'y'+str(i)\n        kc = kp[-1]+yi\n        # time series matrix\n        ky = d[yi].values.reshape(ns,-1)\n        # use knn indices to create neighbor matrix\n        km = ky[knn[1]].transpose((0,2,1)).reshape(-1,k)\n        \n        # take maximum value over all neighbors to approximate spreading\n        d[kc] = np.amax(km, axis=1)\n        print(d[kc].describe())\n        print()\n        \n        # distance to max\n        kc = kd[-1]+yi\n        ki = np.argmax(km, axis=1).reshape(ns,-1)\n        kw = np.zeros_like(ki).astype(float)\n        # inefficient indexing, surely some way to do it faster\n        for j in range(ns): \n            kw[j] = knn[0][j,ki[j]]\n        d[kc] = kw.flatten()\n        print(d[kc].describe())\n        print()\n\n\n# In[62]:\n\n\nki[j]\n\n\n# In[63]:\n\n\n# range of dates for training\n# dates = d[~d.y0.isnull()]['Date'].drop_duplicates()\ndates = d[d.y0.notnull()]['Date'].drop_duplicates()\ndates\n\n\n# In[64]:\n\n\n# correlations\ncols = []\nfor i in range(ny):\n    yi = yv[i]\n    cols.append(yi)\n    for k in kp:\n        cols.append(k+yi)\nd.loc[:,cols].corr()\n\n\n# In[65]:\n\n\nd['Date'] = pd.to_datetime(d['Date'])\nd['Date'].describe()\n\n\n# In[66]:\n\n\n# days since beginning\n# basedate = train['Date'].min()\n# train['dint'] = train.apply(lambda x: (x.name.to_datetime() - basedate).days, axis=1)\nd['dint'] = (d['Date'] - d['Date'].min()).dt.days\nd['dint'].describe()\n\n\n# In[67]:\n\n\nd.shape\n\n\n# In[68]:\n\n\n# reference days since exp(j)th occurrence\nfor i in range(ny):\n    \n    for j in range(3):\n\n        ij = str(i)+'_'+str(j)\n        \n        cut = 2**j if i==0 else j\n        \n        qd1 = (d[yv[i]] > cut) & (d[yv[i]].notnull())\n        d1 = d.loc[qd1,['Loc','dint']]\n        # d1.shape\n        # d1.head()\n\n        # get min for each location\n        d1['dmin'] = d1.groupby('Loc')['dint'].transform(lambda x: x.min())\n        # dintmax = d1['dint'].max()\n        # print(i,j,'dintmax',dintmax)\n        # d1.head()\n\n        d1.drop('dint',axis=1,inplace=True)\n        d1 = d1.drop_duplicates()\n        d = d.merge(d1,how='left',on=['Loc'])\n \n        # if dmin is missing then the series had no occurrences in the training set\n        # go ahead and assume there will be one at the beginning of the test period\n        # the average time between first occurrence and first death is 14 days\n        # if j==0: d[dmi] = d[dmi].fillna(dintmax + 1 + i*14)\n\n        # ref day is days since dmin, must clip at zero to avoid leakage\n        d['ref_day'+ij] = np.clip(d.dint - d.dmin, 0, 100000)\n        d.drop('dmin',axis=1,inplace=True)\n\n        # asymptotic curve may bin differently\n        d['recip_day'+ij] = 1 / (1 + (1 + d['ref_day'+ij])**(-1.0))\n    \n\ngc.collect()\n\n\n# In[69]:\n\n\nd['dint'].value_counts().std()\n\n\n# In[70]:\n\n\n# diffs and rolling means\ne = 1\nr = 5\nfor i in range(ny):\n    yi = 'y'+str(i)\n    dd = '_d'+str(e)\n    rr = '_r'+str(r)\n    \n    d[yi+dd] = d.groupby('Loc')[yi].transform(lambda x: x.diff(e))\n    d[yi+rr] = d.groupby('Loc')[yi].transform(lambda x: x.rolling(r).mean())\n    d['rate'+str(i)+dd] = d.groupby('Loc')['rate'+str(i)].transform(lambda x: x.diff(e))\n    d['rate'+str(i)+rr] = d.groupby('Loc')['rate'+str(i)].transform(lambda x: x.rolling(r).mean())\n    d['extra_y'+str(i)+dd] = d.groupby('Loc')['extra_y'+str(i)].transform(lambda x: x.diff(e))\n    d['extra_y'+str(i)+rr] = d.groupby('Loc')['extra_y'+str(i)].transform(lambda x: x.rolling(r).mean())\n\n    for k in kp:\n        d[k+yi+dd] = d.groupby('Loc')[k+yi].transform(lambda x: x.diff(e))\n        d[k+yi+rr] = d.groupby('Loc')[k+yi].transform(lambda x: x.rolling(r).mean())\n\n    for k in kd:\n        d[k+yi+dd] = d.groupby('Loc')[k+yi].transform(lambda x: x.diff(e))\n        d[k+yi+rr] = d.groupby('Loc')[k+yi].transform(lambda x: x.rolling(r).mean())\n        \nlaglist = ['recov'] + google + wf\n\nfor v in laglist:\n    d[v+dd] = d.groupby('Loc')[v].transform(lambda x: x.diff(e))\n    d[v+rr] = d.groupby('Loc')[v].transform(lambda x: x.rolling(r).mean())\n\n\n# In[71]:\n\n\n# final sort before training\nd = d.sort_values(['Loc','dint']).reset_index(drop=True)\nd.shape\n\n\n# In[72]:\n\n\n# initial continuous and categorical features\ndogs = []\nfor i in range(ny):\n    for j in range(3):\n        dogs.append('ref_day'+str(i)+'_'+str(j))\ncats = ['Loc']\nprint(dogs, len(dogs))\nprint(cats, len(cats))\n\n\n# In[73]:\n\n\n# one-hot encode categorical features\nohef = []\nfor i,c in enumerate(cats):\n    print(c, d[c].nunique())\n    ohe = pd.get_dummies(d[c], prefix=c)\n    ohec = [f.translate({ord(c): \"_\" for c in \" !@#$%^&*()[]{};:,./<>?\\|`~-=_+\"}) for f in list(ohe.columns)]\n    ohe.columns = ohec\n    d = pd.concat([d,ohe],axis=1)\n    ohef = ohef + ohec\n\n\n# In[74]:\n\n\nd['Loc_US_North_Carolina'].describe()\n\n\n# In[75]:\n\n\nd['Loc_US_Colorado'].describe()\n\n\n# In[76]:\n\n\n# boosting hyperparameters\nparams = {}\n\nparams[('lgb','y0')] = {'lambda_l2': 1.9079933811271934, 'max_depth': 5}\nparams[('lgb','y1')] = {'lambda_l2': 1.690407455211948, 'max_depth': 3}\nparams[('xgb','y0')] = {'lambda_l2': 1.9079933811271934, 'max_depth': 5}\nparams[('xgb','y1')] = {'lambda_l2': 1.690407455211948, 'max_depth': 3}\nparams[('ctb','y0')] = {'l2_leaf_reg': 1.9079933811271934, 'max_depth': 5}\nparams[('ctb','y1')] = {'l2_leaf_reg': 1.690407455211948, 'max_depth': 3}\n\n\n# In[77]:\n\n\n# must start cas server before running this cell\nif 'cas' in booster:\n    from swat import *\n    s = CAS('server', 1)\n\n\n# In[78]:\n\n\n# single horizon validation using one day at a time for 28 days\nnb = len(booster)\nnls = np.zeros((nhorizon,ny,nb))\nrallv = np.zeros((nhorizon,ny,nb))\niallv = np.zeros((nhorizon,ny,nb)).astype(int)\nyallv = []\npallv = []\nimps = []\n \n# loop over horizons\nfor horizon in range(1+skip,nhorizon+1):\n# for horizon in range(4,5):\n    \n    print()\n#     print('*'*20)\n#     print(f'horizon {horizon}')\n#     print('*'*20)\n    \n    gc.collect()\n    \n    hs = str(horizon)\n    if horizon < 10: hs = '0' + hs\n    \n    # build lists of features\n    lags = []\n    diffs = []\n    for i in range(ny):\n        yi = 'y'+str(i)\n        lags.append(yi)\n        lags.append('extra_'+yi)\n        lags.append('rate'+str(i))\n        lags.append(yi+dd)\n        lags.append('extra_'+yi+dd)\n        lags.append('rate'+str(i)+dd)\n        lags.append(yi+rr)\n        lags.append('extra_'+yi+rr)\n        lags.append('rate'+str(i)+rr)\n        for k in kp:\n            lags.append(k+yi)\n            lags.append(k+yi+dd)\n            lags.append(k+yi+rr)\n        for k in kd:\n            lags.append(k+yi)\n            lags.append(k+yi+dd)\n            lags.append(k+yi+rr)\n       \n    lags.append('recov')\n    \n    lags = lags + google + wf + ckeep\n    \n#     cinfo = ['pop', 'tests', 'testpop', 'density', 'medianage',\n#        'urbanpop', 'hospibed', 'smokers']\n    cinfo0 = ['testpop']\n    cinfo1 = ['testpop','medianage']\n    \n    f0 = dogs + lags + cinfo0 + ohef\n    f1 = dogs + lags + cinfo1 + ohef\n    \n    # remove some features based on validation experiments\n    f0 = [f for f in f0 if not f.startswith('knn11') and not f.startswith('kd')          and not f.startswith('rate') and not f.endswith(dd) and not f.endswith(rr)]\n    f1 = [f for f in f1 if not f.startswith('knn6') and not f.startswith('kd6')]\n    \n    # remove any duplicates\n    # f0 = list(set(f0))\n    # f1 = list(set(f1))\n    \n    features = []\n    features.append(f0)\n    features.append(f1)\n    \n    nf = []\n    for i in range(ny):\n        nf.append(len(features[i]))\n        # print(nf[i], features[i][:10])\n        \n    qtrain = d['Date'] <= ddate.isoformat()\n\n    vdate = ddate + timedelta(days=horizon)\n    qval = d['Date'] == vdate.isoformat()\n    qvallag = d['Date'] == ddate.isoformat()\n    \n    x_train = d[qtrain].copy()\n    # make y training data monotonic nondecreasing\n    y_train = []\n    for i in range(ny):\n        y_train.append(pd.Series(d.loc[qtrain,['Loc',yv[i]]].groupby('Loc')[yv[i]].cummax()))\n\n    x_val = d[qval].copy()\n    y_val = [d.loc[qval,'y0'].copy(), d.loc[qval,'y1'].copy()]\n    yallv.append(y_val)\n    \n    # lag features\n    x_train.loc[:,lags] = x_train.groupby('Loc')[lags].transform(lambda x: x.shift(horizon))\n    x_val.loc[:,lags] = d.loc[qvallag,lags].values\n\n    print()\n    print(horizon, 'x_train', x_train.shape)\n    print(horizon, 'x_val', x_val.shape)\n    \n    if train_full:\n        \n        qfull = (d['Date'] <= tmax)\n        \n        tdate = dmax + timedelta(days=horizon)\n        qtest = d['Date'] == tdate.isoformat()\n        qtestlag = d['Date'] == dmax.isoformat()\n    \n        x_full = d[qfull].copy()\n        \n        # make y training data monotonic nondecreasing\n        y_full = []\n        for i in range(ny):\n            y_full.append(pd.Series(d.loc[qfull,['Loc',yv[i]]].groupby('Loc')[yv[i]].cummax()))\n        \n        x_test = d[qtest].copy()\n        \n        # lag features\n        x_full.loc[:,lags] = x_full.groupby('Loc')[lags].transform(lambda x: x.shift(horizon))\n        x_test.loc[:,lags] = d.loc[qtestlag,lags].values\n\n        print(horizon, 'x_full', x_full.shape)\n        print(horizon, 'x_test', x_test.shape)\n\n    train_set = []\n    val_set = []\n    ny = len(y_train)\n\n#     for i in range(ny):\n#         train_set.append(xgb.DMatrix(x_train[features[i]], y_train[i]))\n#         val_set.append(xgb.DMatrix(x_val[features[i]], y_val[i]))\n\n    gc.collect()\n\n    # loop over multiple targets\n    mod = []\n    pred = []\n    rez = []\n    iters = []\n    \n    for i in range(ny):\n#     for i in range(1):\n        print()\n        print('*'*40)\n        print(f'horizon {horizon} {yv[i]} {ynames[i]} {vdate}')\n        print('*'*40)\n        \n        # use catboost only for y1\n        # nb = 2 if i==0 else 3\n       \n        # matrices to store predictions\n        vpm = np.zeros((x_val.shape[0],nb))\n        tpm = np.zeros((x_test.shape[0],nb))\n        \n        for b in range(nb):\n            \n            if booster[b] == 'cas':\n                \n                x_train['Partition'] = 1\n                x_val['Partition'] = 0\n                x_cas_all = pd.concat([x_train, x_val], axis=0)\n                # make copy of target since it is also used for lags\n                x_cas_all['target'] = pd.concat([y_train[i], y_val[i]], axis=0).values\n                s.upload(x_cas_all, casout=\"x_cas_val\")\n\n                target = 'target'\n                inputs = features[i]\n                inputs.append(target)\n\n                s.loadactionset(\"autotune\")\n                res=s.autotune.tuneGradientBoostTree (\n                    trainOptions = {\n                        \"table\":{\"name\":'x_cas_val',\"where\":\"Partition=1\"},\n                        \"target\":target,\n                        \"inputs\":inputs,\n                        \"casOut\":{\"name\":\"model\", \"replace\":True}\n                    },\n                    scoreOptions = {\n                        \"table\":{\"name\":'x_cas_val', \"where\":\"Partition=0\"},\n                        \"model\":{\"name\":'model'},\n                        \"casout\":{\"name\":\"x_valid_preds\",\"replace\":True},\n                        \"copyvars\": ['Id','Loc','Date']\n                    },\n                    tunerOptions = {\n                        \"seed\":54321,  \n                        \"objective\":\"RASE\", \n                        \"userDefinedPartition\":True \n                    }\n                )\n                print()\n                print(res.TunerSummary)\n                print()\n                print(res.BestConfiguration)        \n\n                TunerSummary=pd.DataFrame(res['TunerSummary'])\n                TunerSummary[\"Value\"]=pd.to_numeric(TunerSummary[\"Value\"])\n                BestConf=pd.DataFrame(res['BestConfiguration'])\n                BestConf[\"Value\"]=pd.to_numeric(BestConf[\"Value\"])\n                vpt = s.CASTable(\"x_valid_preds\").to_frame()\n                #FG: resort the CAS predictions by Id\n                vpt = vpt.sort_values(['Loc','Date']).reset_index(drop=True)\n                vp = vpt['P_target'].values\n\n                s.dropTable(\"x_cas_val\")\n                s.dropTable(\"x_valid_preds\")\n                \n            else:\n                # scikit interface automatically uses best model for predictions\n                params[(booster[b],yv[i])]['n_estimators'] = 5000\n                if booster[b]=='lgb':\n                    model = lgb.LGBMRegressor(**params[(booster[b],yv[i])]) \n                elif booster[b]=='xgb':\n                    model = xgb.XGBRegressor(**params[(booster[b],yv[i])])\n                else:\n                    # hack for categorical features, ctb must be last in booster list\n                    features[i] = features[i][:-294] + ['Loc']\n                    params[(booster[b],yv[i])]['cat_features'] = ['Loc']\n                    model = ctb.CatBoostRegressor(**params[(booster[b],yv[i])])\n                    \n                model.fit(x_train[features[i]], y_train[i],\n                                  eval_set=[(x_train[features[i]], y_train[i]),\n                                            (x_val[features[i]], y_val[i])],\n                                  early_stopping_rounds=30,\n                                  verbose=False)\n\n                vp = model.predict(x_val[features[i]])\n                \n                iallv[horizon-1,i,b] = model._best_iteration if booster[b]=='lgb' else                                        model.best_iteration if booster[b]=='xgb' else                                        model.best_iteration_\n\n                gain = model.feature_importances_\n        #         gain = model.get_score(importance_type='gain')\n        #         split = model.get_score(importance_type='weight')   \n            #     gain = model.feature_importance(importance_type='gain')\n            #     split = model.feature_importance(importance_type='split').astype(float)  \n            #     imp = pd.DataFrame({'feature':features,'gain':gain,'split':split})\n                imp = pd.DataFrame({'feature':features[i],'gain':gain})\n        #         imp = pd.DataFrame({'feature':features[i]})\n        #         imp['gain'] = imp['feature'].map(gain)\n        #         imp['split'] = imp['feature'].map(split)\n\n                imp.set_index(['feature'],inplace=True)\n\n                imp.gain /= np.sum(imp.gain)\n        #         imp.split /= np.sum(imp.split)\n\n                imp.sort_values(['gain'], ascending=False, inplace=True)\n\n                print()\n                print(imp.head(n=10))\n                # print(imp.shape)\n\n                imp.reset_index(inplace=True)\n                imp['horizon'] = horizon\n                imp['target'] = yv[i]\n                imp['set'] = 'valid'\n                imp['booster'] = booster[b]\n\n                mod.append(model)\n                imps.append(imp)\n                \n            # china rule, last observation carried forward, set to zero here\n            qcv = (x_val['Country_Region'] == 'China') &                   (x_val['Province_State'] != 'Hong Kong') &                   (x_val['Province_State'] != 'Macau')\n            vp[qcv] = 0.0\n\n            # make sure horizon 1 prediction is not smaller than first lag\n            # because we know series is monotonic\n            # if horizon==1+skip:\n            if True:\n                a = np.zeros((len(vp),2))\n                a[:,0] = vp\n                a[:,1] = x_val[yv[i]].values\n                vp = np.nanmax(a,axis=1)\n            \n            val_score = np.sqrt(mean_squared_error(vp, y_val[i]))\n            vpm[:,b] = vp\n            \n            print()\n            print(f'{booster[b]} validation rmse {val_score:.6f}')\n            rallv[horizon-1,i,b] = val_score\n\n            gc.collect()\n    \n#             break\n\n            if train_full:\n                \n                print()\n                print(f'{booster[b]} training with full data and predicting', tdate.isoformat())\n                    \n                if booster[b] == 'cas':\n                    \n                    x_full['target'] = y_full[i].values\n                    s.upload(x_full, casout=\"x_full\")\n                    # use hyperparameters from validation fit\n                    s.loadactionset(\"decisionTree\")\n                    result = s.gbtreetrain(\n                        table={\"name\":'x_full'},\n                        target=target,\n                        inputs= inputs,\n                        varimp=True,\n                        ntree=BestConf.iat[0,2], \n                        m=BestConf.iat[1,2],\n                        learningRate=BestConf.iat[2,2],\n                        subSampleRate=BestConf.iat[3,2],\n                        lasso=BestConf.iat[4,2],\n                        ridge=BestConf.iat[5,2],\n                        nbins=BestConf.iat[6,2],\n                        maxLevel=BestConf.iat[7,2],\n                        #quantileBin=True,\n                        seed=326146718,\n                        #savestate={\"name\":\"aStore\",\"replace\":True}\n                        casOut={\"name\":'fullmodel', \"replace\":True}\n                        ) \n\n                    s.upload(x_test, casout=\"x_test_cas\")\n\n                    s.decisionTree.gbtreeScore(\n                        modelTable={\"name\":\"fullmodel\"},        \n                        table={\"name\":\"x_test_cas\"},\n                        casout={\"name\":\"x_test_preds\",\"replace\":True},\n                        copyvars= ['Loc','Date']\n                        ) \n                    # save test predictions back into main table\n                    forecast = s.CASTable(\"x_test_preds\").to_frame()\n                    forecast = forecast.sort_values(['Loc','Date']).reset_index(drop=True)\n                    tp = forecast['_GBT_PredMean_'].values\n                    \n                    s.dropTable(\"x_full\")\n                    s.dropTable(\"x_test_cas\")\n                     \n                else:\n                \n                    # use number of iterations from validation fit\n                    params[(booster[b],yv[i])]['n_estimators'] = iallv[horizon-1,i,b]\n                    if booster[b]=='lgb':\n                        model = lgb.LGBMRegressor(**params[(booster[b],yv[i])])\n                    elif booster[b]=='xgb':\n                        model = xgb.XGBRegressor(**params[(booster[b],yv[i])])\n                    else:\n                        model = ctb.CatBoostRegressor(**params[(booster[b],yv[i])])\n                    \n                    model.fit(x_full[features[i]], y_full[i], verbose=False)\n                    \n                    params[(booster[b],yv[i])]['n_estimators'] = 5000\n\n                    tp = model.predict(x_test[features[i]])\n                \n                    gain = model.feature_importances_\n            #         gain = model.get_score(importance_type='gain')\n            #         split = model.get_score(importance_type='weight')   \n                #     gain = model.feature_importance(importance_type='gain')\n                #     split = model.feature_importance(importance_type='split').astype(float)  \n                #     imp = pd.DataFrame({'feature':features,'gain':gain,'split':split})\n                    imp = pd.DataFrame({'feature':features[i],'gain':gain})\n            #         imp = pd.DataFrame({'feature':features[i]})\n            #         imp['gain'] = imp['feature'].map(gain)\n            #         imp['split'] = imp['feature'].map(split)\n\n                    imp.set_index(['feature'],inplace=True)\n\n                    imp.gain /= np.sum(imp.gain)\n            #         imp.split /= np.sum(imp.split)\n\n                    imp.sort_values(['gain'], ascending=False, inplace=True)\n\n                    print()\n                    print(imp.head(n=10))\n                    # print(imp.shape)\n\n                    imp.reset_index(inplace=True)\n                    imp['horizon'] = horizon\n                    imp['target'] = yv[i]\n                    imp['set'] = 'full'\n                    imp['booster'] = booster[b]\n\n                    imps.append(imp)\n\n                # china rule, last observation carried forward, set to zero here\n                qct = (x_test['Country_Region'] == 'China') &                       (x_test['Province_State'] != 'Hong Kong') &                       (x_test['Province_State'] != 'Macau')\n                tp[qct] = 0.0\n\n                # make sure first horizon prediction is not smaller than first lag\n                # because we know series is monotonic\n                # if horizon==1+skip:\n                if True:\n                    a = np.zeros((len(tp),2))\n                    a[:,0] = tp\n                    a[:,1] = x_test[yv[i]].values\n                    tp = np.nanmax(a,axis=1)\n\n                tpm[:,b] = tp\n                \n                gc.collect()\n                \n        # nonnegative least squares to estimate ensemble weights\n        x, rnorm = nnls(vpm, y_val[i])\n        \n        # smooth weights by shrinking towards all equal\n        # x = (x + np.ones(3)/3.)/2\n\n        # smooth weights with rolling mean, ewma\n        # if horizon+skip > 1: x = (x + nls[horizon+skip-2,i])/2\n        alpha = 0.1\n        if horizon+skip > 1: x = alpha * x + (1 - alpha) * nls[horizon+skip-2,i]\n\n        nls[horizon+skip-1,i] = x\n        \n        val_pred = np.matmul(vpm, x)\n        test_pred = np.matmul(tpm, x)\n        \n        # china rule in case weights do not sum to 1\n        # val_pred[qcv] = vpm[:,0][qcv]\n        # test_pred[qcv] = tpm[:,0][qct]\n        \n        # save validation and test predictions back into main table\n        d.loc[qval,yv[i]+'_pred'] = val_pred\n        d.loc[qtest,yv[i]+'_pred'] = test_pred\n\n        # ensemble validation score\n        # val_score = np.sqrt(rnorm/vpm.shape[0])\n        val_score = np.sqrt(mean_squared_error(val_pred, y_val[i]))\n        \n        rez.append(val_score)\n        pred.append(val_pred)\n\n    pallv.append(pred)\n    \n    # nnls weights\n    w0 = ''\n    w1 = ''\n    for b in range(nb):\n        w0 = w0 + f' {nls[horizon-1,0,b]:.2f}'\n        w1 = w1 + f' {nls[horizon-1,1,b]:.2f}'\n        \n    print()\n    print('Validation RMSLE')\n    print(f'{ynames[0]} \\t {rez[0]:.6f}  ' + w0)\n    print(f'{ynames[1]} \\t {rez[1]:.6f}  ' + w1)\n    print(f'Mean \\t \\t {np.mean(rez):.6f}')\n\n#     # break down RMSLE by day\n#     rp = np.zeros((2,7))\n#     for i in range(ny):\n#         for di in range(50,57):\n#             j = di - 50\n#             qf = x_val.dint == di\n#             rp[i,j] = np.sqrt(mean_squared_error(pred[i][qf], y_val[i][qf]))\n#             print(i,di,f'{rp[i,j]:.6f}')\n#         print(i,f'{np.mean(rp[i,:]):.6f}')\n#         plt.plot(rp[i])\n#         plt.title(ynames[i] + ' RMSLE')\n#         plt.show()\n        \n    # plot actual vs predicted\n    plt.figure(figsize=(10, 5))\n    for i in range(ny):\n        plt.subplot(1,2,i+1)\n        plt.plot([0, 12], [0, 12], 'black')\n        plt.plot(pred[i], y_val[i], '.')\n        plt.xlabel('Predicted')\n        plt.ylabel('Actual')\n        plt.title(ynames[i])\n        plt.grid()\n    plt.show()\n    \n# save one big table of importances\nimpall = pd.concat(imps)\n\n# remove number suffixes from lag names to aid in analysis\n# impall['feature1'] = impall['feature'].replace(to_replace='lag..', value='lag', regex=True)\n\nos.makedirs('imp', exist_ok=True)\nfname = 'imp/' + mname + '_imp.csv'\nimpall.to_csv(fname, index=False)\nprint()\nprint(fname, impall.shape)\n\n# save scores and weights\nos.makedirs('rez', exist_ok=True)\nfname = 'rez/' + mname+'_rallv.npy'\nnp.save(fname, rallv)\nprint(fname, rallv.shape)\n\nfname = 'rez/' + mname+'_nnls.npy'\nnp.save(fname, nls)\nprint(fname, nls.shape)\n\n\n# In[79]:\n\n\nif 'cas' in booster: s.shutdown()\n\n\n# In[80]:\n\n\ntdate.isoformat()\n\n\n# In[81]:\n\n\nrf = [f for f in features[0] if f.startswith('ref')]\nd[rf].describe()\n\n\n# In[82]:\n\n\nnp.mean(iallv, axis=0)\n\n\n# In[83]:\n\n\nplt.figure(figsize=(10, 8))\nfor i in range(ny):\n    plt.subplot(2,2,1+i)\n    plt.plot(rallv[:,i])\n    plt.title(ynames[i] + ' RMSLE vs Horizon')\n    plt.grid()\n    \n    plt.subplot(2,2,3+i)\n    plt.plot(nls[:,i])\n    plt.title(ynames[i] + ' Ensemble Weights')\n    plt.grid()\nplt.show()\n\n\n# In[84]:\n\n\n# compute validation rmsle\nm = 0\nlocs = d.loc[:,['Loc','Country_Region','Province_State']].drop_duplicates().reset_index(drop=True)\n# locs = x_val.copy().reset_index(drop=True)\n# print(locs.shape)\ny_truea = []\ny_preda = []\n\nprint(f'# {mname}')\nfor i in range(ny):\n    y_true = []\n    y_pred = []\n    for j in range(nhorizon-skip):\n        y_true.append(yallv[j][i])\n        y_pred.append(pallv[j][i])\n    y_true = np.stack(y_true)\n    y_pred = np.stack(y_pred)\n    # print(y_pred.shape)\n    # make each series monotonic increasing\n    for j in range(y_pred.shape[1]): \n        y_pred[:,j] = np.maximum.accumulate(y_pred[:,j])\n    # copy updated predictions into main table\n    for horizon in range(1+skip,nhorizon+1):\n        vdate = ddate + timedelta(days=horizon)\n        qval = d['Date'] == vdate.isoformat()\n        d.loc[qval,yv[i]+'_pred'] = y_pred[horizon-1-skip]\n    rmse = np.sqrt(mean_squared_error(y_pred, y_true))\n    print(f'# {rmse:.6f}')\n    m += rmse/2\n    locs['rmse'+str(i)] = np.sqrt(np.mean((y_true-y_pred)**2, axis=0))\n    y_truea.append(y_true)\n    y_preda.append(y_pred)\nprint(f'# {m:.6f}')\n\n\n# In[85]:\n\n\n# gbt1u\n# 0.626124\n# 0.404578\n# 0.515351\n\n\n# In[86]:\n\n\n# sort to find worst predictions of y0\nlocs = locs.sort_values('rmse0', ascending=False)\nlocs[:10]\n\n\n# In[87]:\n\n\n# plot worst fits of y0\nfor i in range(5):\n    li = locs.index[i]\n    plt.plot(y_truea[0][:,li])\n    plt.plot(y_preda[0][:,li])\n    plt.title(locs.loc[li,'Loc'])\n    plt.show()\n\n\n# In[88]:\n\n\n# plt.plot(d.loc[d.Loc=='Belgium','y0'][39:])\n\n\n# In[89]:\n\n\n# sort to find worst predictions of y1\nlocs = locs.sort_values('rmse1', ascending=False)\nlocs[:10]\n\n\n# In[90]:\n\n\n# plot worst fits of y1\nfor i in range(5):\n    li = locs.index[i]\n    plt.plot(y_truea[1][:,li])\n    plt.plot(y_preda[1][:,li])\n    plt.title(locs.loc[li,'Loc'])\n    plt.show()\n\n\n# In[91]:\n\n\ntmax\n\n\n# In[92]:\n\n\n# enforce monotonicity of forecasts in test set after last date in training\nloc = d['Loc'].unique()\nfor l in loc:\n    # q = (d.Loc==l) & (d.ForecastId > 0)\n    q = (d.Loc==l) & (d.Date > tmax)\n    for yi in yv:\n        yp = yi+'_pred'\n        d.loc[q,yp] = np.maximum.accumulate(d.loc[q,yp])\n\n\n# In[93]:\n\n\n# plot actual and predicted curves over time for specific locations\nlocs = ['China Tibet','China Xinjiang','China Hong Kong', 'China Macau',\n        'Spain','Italy','India',\n        'US Washington','US New York','US California',\n        'US North Carolina','US Ohio']\nxlab = ['03-12','03-18','03-25','04-01','04-08','04-15','04-22']\nfor loc in locs:\n    plt.figure(figsize=(14,2))\n    \n    # fig, ax = plt.subplots()\n    # fig.autofmt_xdate()\n    \n    for i in range(ny):\n    \n        plt.subplot(1,2,i+1)\n        plt.plot(d.loc[d.Loc==loc,[yv[i],'Date']].set_index('Date'))\n        plt.plot(d.loc[d.Loc==loc,[yv[i]+'_pred','Date']].set_index('Date'))\n        # plt.plot(d.loc[d.Loc==loc,[yv[i]]])\n        # plt.plot(d.loc[d.Loc==loc,[yv[i]+'_pred']])\n        # plt.xticks(np.arange(len(xlab)), xlab, rotation=-45)\n        # plt.xticks(np.arange(12), calendar.month_name[3:5], rotation=20)\n        # plt.xticks(rotation=-45)\n        plt.xticks([])\n        plt.title(loc + ' ' + ynames[i])\n       \n    plt.show()\n\n\n# In[94]:\n\n\npd.set_option('display.max_rows', 100)\nloc = 'China Xinjiang'\nd.loc[d.Loc==loc,['Date',yv[0],yv[0]+'_pred']]\n\n\n# In[95]:\n\n\ntmax\n\n\n# In[96]:\n\n\nfmin\n\n\n# In[97]:\n\n\n# compute public lb score\nq = (d.Date >= fmin) & (d.Date <= tmax)\nprint(f'# {tmax} {sum(q)/ns} {mname}')\ns0 = np.sqrt(mean_squared_error(d.loc[q,'y0'],d.loc[q,'y0_pred']))\ns1 = np.sqrt(mean_squared_error(d.loc[q,'y1'],d.loc[q,'y1_pred']))\nprint(f'# CC \\t {s0:.6f}')\nprint(f'# Fa \\t {s1:.6f}')\nprint(f'# Mean \\t {(s0+s1)/2:.6f}')\n\n\n# In[98]:\n\n\n# 2020-03-31 13.0 gbt1u\n# CC \t 0.744772\n# Fa \t 0.567829\n# Mean \t 0.656301\n\n\n# In[99]:\n\n\n# create submission\nsub = d.loc[d.ForecastId > 0, ['ForecastId','y0_pred','y1_pred']]\nsub['ConfirmedCases'] = np.expm1(sub['y0_pred'])\nsub['Fatalities'] = np.expm1(sub['y1_pred'])\nsub.drop(['y0_pred','y1_pred'],axis=1,inplace=True)\nos.makedirs('sub',exist_ok=True)\n# fname = 'sub/' + mname + '.csv'\nfname = 'sub3.csv'\nsub.to_csv(fname, index=False)\nprint(fname, sub.shape)\nsub3=sub.copy()\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub1.reset_index(drop=True,inplace=True)\nsub2.reset_index(inplace=True)\nsub3.reset_index(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub1.shape,sub2.shape,sub3.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub1=sub1[[\"ForecastId\",\"ConfirmedCases\",\"Fatalities\"]]\nsub2=sub2[[\"ForecastId\",\"ConfirmedCases\",\"Fatalities\"]]\nsub3=sub3[[\"ForecastId\",\"ConfirmedCases\",\"Fatalities\"]]\nsub1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub1.sort_values(['ForecastId'],inplace=True)\nsub2.sort_values(['ForecastId'],inplace=True)\nsub3.sort_values(['ForecastId'],inplace=True)\nsub=sub1.copy()\nsub['ConfirmedCases']=sub1['ConfirmedCases']*0.33+sub2['ConfirmedCases']*0.34+sub3['ConfirmedCases']*0.33\nsub['Fatalities']=sub1['Fatalities']*0.33+sub2['Fatalities']*0.34+sub3['Fatalities']*0.33\nsub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}