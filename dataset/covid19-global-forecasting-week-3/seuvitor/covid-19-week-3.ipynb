{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"TEST_TRAIN_AND_PREDICT = False\nPLOT_LEARNING_CURVES = False\nGENERATE_SUBMISSION = True\n\nCASES_REG_ALPHA = 5.0\nCASES_NUM_LEAVES = 8\nFATALITIES_REG_ALPHA = 5.0\nFATALITIES_NUM_LEAVES = 4\n\nSEED = 654321\n\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nPUBLIC_LEADERBOARD_FROM_DAY = pd.Timestamp('2020-03-26')\n\nPUBLIC_LEADERBOARD_TO_DAY = pd.Timestamp('2020-04-08')\nPUBLIC_LEADERBOARD_USE_REAL_DATA_UP_TO = pd.Timestamp('2020-03-25')\n\nPRIVATE_LEADERBOARD_FROM_DAY = pd.Timestamp('2020-04-09')\nPRIVATE_LEADERBOARD_TO_DAY = pd.Timestamp('2020-05-07')\nPRIVATE_LEADERBOARD_USE_REAL_DATA_UP_TO = pd.Timestamp('2020-04-07')\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\nrandom.seed(SEED)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-3/train.csv\")\ntest_data = pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-3/test.csv\")\n\nhealth_systems = pd.read_csv('/kaggle/input/world-bank-wdi-212-health-systems/2.12_Health_systems.csv')\n\nregion_metadata = pd.read_csv('/kaggle/input/covid19-forecasting-metadata/region_metadata.csv')\n\n#Fix Diamond Princess cruise ship area from source \nregion_metadata.loc[region_metadata.Country_Region == 'Diamond Princess', 'area'] = 0.37\nregion_metadata.loc[region_metadata.Country_Region == 'Holy See', 'area'] = 0.44","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"merged_data = (train_data\n               .merge(region_metadata\n                      .rename(columns={'population': 'Population',\n                                       'area': 'Area'})\n                      .drop(columns=['lat',\n                                     'lon',\n                                     'continent']),\n                      how='left')\n               .merge(health_systems\n                      .rename(columns={'Health_exp_per_capita_USD_2016': 'HealthExpPerCapita',\n                                       'Physicians_per_1000_2009-18': 'PhysiciansPer1000',\n                                       'Nurse_midwife_per_1000_2009-18': 'NursesPer1000'})\n                      .drop(columns=['World_Bank_Name',\n                                     'Health_exp_pct_GDP_2016',\n                                     'Health_exp_public_pct_2016',\n                                     'Health_exp_out_of_pocket_pct_2016',\n                                     'per_capita_exp_PPP_2016',\n                                     'External_health_exp_pct_2016',\n                                     'Specialist_surgical_per_1000_2008-18',\n                                     'Completeness_of_birth_reg_2009-18',\n                                     'Completeness_of_death_reg_2008-16']),\n                      how='left'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"merged_data.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_cases_per_population(dataset):\n    dataset.loc[:, 'CasesPerPopulation'] = dataset.loc[:, 'ConfirmedCases'] / dataset.loc[:, 'Population']\n\ndef create_population_per_area(dataset):\n    dataset.loc[:, 'PopulationPerArea'] = dataset.loc[:, 'Population'] / dataset.loc[:, 'Area']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def lastNDaysMask(n, days):\n    mask = np.zeros((n, n))\n    for i in range(days):\n        mask += np.eye(n, k=(-1 - i))\n    return mask\n\ndef buildLastDaysMasks(dataset_length):\n    return {2: lastNDaysMask(dataset_length, 2),\n                4: lastNDaysMask(dataset_length, 4),\n                8: lastNDaysMask(dataset_length, 8),\n                16: lastNDaysMask(dataset_length, 16)}\n\ndef addGrowthFromLastDaysForProvince(dataset, provinceMask, featureName):\n    provinceSubset = dataset.loc[provinceMask]\n    lastDaysMasks = buildLastDaysMasks(len(provinceSubset))\n    featureValues = provinceSubset.loc[:, featureName].values\n    for numDays in [2, 4, 8, 16]:\n        lastDaysAvg = ((lastDaysMasks[numDays] * featureValues).sum(axis=1) / numDays)\n        dataset.loc[provinceMask, 'Growth' + featureName + 'From' + str(numDays) + 'DaysAvg'] = np.nan_to_num((featureValues / lastDaysAvg), nan=0, posinf=0, neginf=0)\n\ndef addGrowthToNextDayForProvince(dataset, provinceMask, featureName):\n    provinceSubset = dataset.loc[provinceMask]\n    nextDayMask = np.eye(len(provinceSubset), k=1)\n    featureValues = provinceSubset.loc[:, featureName].values\n    nextDayValues = (nextDayMask * featureValues).sum(axis=1)\n    dataset.loc[provinceMask, 'Growth' + featureName + 'ToNextDay'] = np.nan_to_num((nextDayValues / featureValues), nan=0, posinf=0, neginf=0)\n\ndef create_growth_features(dataset):\n    for (province, country) in dataset[['Province_State', 'Country_Region']].drop_duplicates().values:\n        provinceMask = (dataset.Province_State == province) & (dataset.Country_Region == country)\n        addGrowthFromLastDaysForProvince(dataset, provinceMask, 'ConfirmedCases')\n        addGrowthFromLastDaysForProvince(dataset, provinceMask, 'Fatalities')\n        addGrowthToNextDayForProvince(dataset, provinceMask, 'ConfirmedCases')\n        addGrowthToNextDayForProvince(dataset, provinceMask, 'Fatalities')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_features(dataset):\n    create_cases_per_population(dataset)\n    create_population_per_area(dataset)\n    create_growth_features(dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_defaults_for_missing(dataset):\n    defaults_for_missing = {\n        'Province_State': '',\n        'Population':  dataset.loc[:, 'Population'].quantile(0.1),\n        'Area':  dataset.loc[:, 'Area'].quantile(0.1),\n        'HealthExpPerCapita': dataset['HealthExpPerCapita'].median(),\n        'PhysiciansPer1000': dataset['PhysiciansPer1000'].median(),\n        'NursesPer1000': dataset['NursesPer1000'].median()\n    }\n    return defaults_for_missing","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def imput_missing_defaults(dataset, default_features):\n    for feature_name in default_features:\n        if feature_name in dataset.columns:\n            default_value = default_features[feature_name]\n            dataset[feature_name].fillna(default_value, inplace=True)\n            \ndef convert_datatypes(dataset):\n    if 'Date' in dataset.columns:\n        dataset.loc[:, ['Date']] = pd.to_datetime(dataset['Date'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"defaults_for_missing = extract_defaults_for_missing(merged_data)\nimput_missing_defaults(merged_data, defaults_for_missing)\nconvert_datatypes(merged_data)\n\ncreate_features(merged_data)\n\nmerged_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"merged_data.loc[(merged_data.Country_Region=='Brazil')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def filter_for_training(dataset):\n    return dataset.loc[(dataset.Date >= '2020-02-07') & (dataset.Date <= '2020-04-02')]\n\ndef features_for_training(targetFeature):\n    features = ['HealthExpPerCapita','PhysiciansPer1000', 'NursesPer1000', 'CasesPerPopulation', 'PopulationPerArea']\n    for numDays in [2, 4, 8, 16]:\n        features = features + ['Growth' + targetFeature + 'From' + str(numDays) + 'DaysAvg']\n    return features\n    \ndef growth_feature_to_predict(targetFeature):\n    return 'Growth' + targetFeature + 'ToNextDay'\n\ndef project_for_training(dataset, targetFeature):\n    return dataset.loc[:, features_for_training(targetFeature) + [growth_feature_to_predict(targetFeature)]]\n\ndef project_for_predicting(dataset, targetFeature):\n    return dataset.loc[:, features_for_training(targetFeature)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\ndef extract_data_to_preprocess(dataset):\n    features_to_scale = set([item\n                             for featureName in ['ConfirmedCases', 'Fatalities']\n                             for item in (features_for_training(featureName) + [growth_feature_to_predict(featureName)])])\n    scalers = {}\n    for feature in features_to_scale:\n        scalers[feature] = StandardScaler().fit(dataset[[feature]])\n    return (scalers)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"merged_data[np.isinf(merged_data.PopulationPerArea)]\n\n(scalers) = extract_data_to_preprocess(merged_data)\n\nfor feature_name in scalers:\n    print('scaler', feature_name, ', mean:', scalers[feature_name].mean_, ', var:', scalers[feature_name].var_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def scale_features(dataset, scalers):\n    for feature_name in scalers:\n        if feature_name in dataset.columns:\n            dataset[[feature_name]] = scalers[feature_name].transform(dataset[[feature_name]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_preprocessed_dataset(original_dataset, scalers):\n    dataset = original_dataset.copy(deep = True)\n    scale_features(dataset, scalers)\n    return dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_preprocessed_dataset(project_for_training(filter_for_training(merged_data), 'ConfirmedCases'), scalers).sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_preprocessed_dataset(project_for_training(filter_for_training(merged_data), 'Fatalities'), scalers).sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import svm\n\ndef get_svr_predictor_for_feature(train_dataset, target_column, c_value, gamma_value):\n    alg = svm.SVR(gamma=gamma_value, kernel='rbf', C=c_value)\n    alg.fit(train_dataset.loc[:, train_dataset.columns != target_column], train_dataset[target_column])\n    return alg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\n\ndef get_gbm_predictor_for_feature(train_dataset, target_column, reg_alpha, num_leaves):\n    \n    (training_set, validation_set) = split_training_and_validation_sets(train_dataset, 0.2)\n    \n    params = {\"bagging_fraction\": 0.9,\n              \"device\": \"gpu\",\n              \"feature_fraction\": 0.8,\n              \"learning_rate\": 0.015,\n              \"metric\": \"rmse\",\n              \"num_leaves\": num_leaves,\n              \"objective\": \"regression\",\n              \"reg_alpha\": reg_alpha,\n              \"reg_lambda\": 0.1,\n              \"seed\": SEED}\n    \n    #tr = lgb.Dataset(train_dataset.loc[:, train_dataset.columns != target_column], label=train_dataset[target_column])\n    tr = lgb.Dataset(training_set.loc[:, training_set.columns != target_column], label=training_set[target_column])\n    val = lgb.Dataset(validation_set.loc[:, validation_set.columns != target_column], label=validation_set[target_column])\n    #bst = lgb.train(params, tr, num_boost_round=200)\n    bst = lgb.train(params, tr, num_boost_round=400, valid_sets=[val])\n    \n    return bst","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_log_error\n\ndef calculate_error(predictions, actual):\n    return np.sqrt(mean_squared_log_error(actual, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_and_train_for_feature(dataset, scalers, featureName, reg_alpha, num_leaves):\n    complete_data_for_training = project_for_training(filter_for_training(dataset), featureName)\n    train = get_preprocessed_dataset(complete_data_for_training, scalers)\n    \n    target_column = growth_feature_to_predict(featureName)\n    alg = get_gbm_predictor_for_feature(train, target_column, reg_alpha, num_leaves)\n    return alg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_and_predict_for_feature(dataset, scalers, featureName, alg):\n    complete_data_for_predicting = project_for_predicting(dataset, featureName)\n    test = get_preprocessed_dataset(complete_data_for_predicting, scalers)\n    \n    predicted_growth_scaled = alg.predict(test)\n    \n    growth_feature_name = growth_feature_to_predict(featureName)\n    predicted_growth = scalers[growth_feature_name].inverse_transform(predicted_growth_scaled)\n    \n    # Predicted growth must be non-negative\n    predicted_growth[predicted_growth < 0.0] = 0.0\n    \n    return predicted_growth","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import svm\n\ndef train_and_predict(target_column, training_set, validation_set, scalers, reg_alpha, num_leaves):\n    \n    alg = preprocess_and_train_for_feature(training_set, scalers, target_column, reg_alpha, num_leaves)\n    \n    predictions_train = preprocess_and_predict_for_feature(training_set, scalers, target_column, alg)\n    actual_train = training_set.loc[:, growth_feature_to_predict(target_column)]\n    error_train = calculate_error(predictions_train, actual_train)\n    \n    predictions_val = preprocess_and_predict_for_feature(validation_set, scalers, target_column, alg)\n    actual_val = validation_set.loc[:, growth_feature_to_predict(target_column)]\n    error_val = calculate_error(predictions_val, actual_val)\n\n    return (error_train, error_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if TEST_TRAIN_AND_PREDICT:\n    filtered_merged_data = filter_for_training(merged_data)\n\n    (error_train_cases, error_val_cases) = train_and_predict('ConfirmedCases', filtered_merged_data, filtered_merged_data, scalers, CASES_REG_ALPHA, CASES_NUM_LEAVES)\n    (error_train_fatalities, error_val_fatalities) = train_and_predict('Fatalities', filtered_merged_data, filtered_merged_data, scalers, FATALITIES_REG_ALPHA, FATALITIES_NUM_LEAVES)\n\n    print('Training prediction error (ConfirmedCases):', error_train_cases)\n    print('Training prediction error (Fatalities):', error_train_fatalities)\n\n#SVR\n#Training prediction error (ConfirmedCases): 0.14009188606673692\n#Training prediction error (Fatalities): 0.10828247058967111\n\n#GBM\n#Training prediction error (ConfirmedCases): 0.10621665813527417\n#Training prediction error (Fatalities): 0.10720360641223572","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def split_training_and_validation_sets(dataset, validation_percent):\n    validation_mask = np.random.rand(len(dataset)) < validation_percent\n    training_set = dataset[~validation_mask]\n    validation_set = dataset[validation_mask]\n    return (training_set, validation_set)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef plot_decreasing_error(target_column, training_set, validation_set, scalers, reg_alpha, num_leaves, subplot):\n    plot_steps = 20\n    n = len(training_set)\n    sample_sizes = list(range(n // plot_steps, n, n // plot_steps))\n    \n    decreasing_error = [train_and_predict(target_column, training_set[0:size], validation_set, scalers, reg_alpha, num_leaves)\n                        for size in sample_sizes]\n    \n    (_, final_val_error) = decreasing_error[-1]\n    \n    subplot.plot([error_train for (error_train, _) in decreasing_error], label='training error')\n    subplot.plot([error_val for (_, error_val) in decreasing_error], label='validation error')\n    subplot.legend(loc='lower right')\n    subplot.set_title('num_leaves=' + str(round(gamma_value, 4)) + ', reg_alpha=' + str(round(c_value, 4)) + ', val_error=' + str(round(final_val_error, 4)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef plot_learning_curves(target_column, dataset, scalers, validation_percent):\n    (training_set, validation_set) = split_training_and_validation_sets(dataset, validation_percent)\n    \n    num_leaves_values = [4, 8, 16, 32, 64, 128] # num_leaves\n    reg_alpha_values = [0.04, 0.2, 1.0, 5.0, 25.0] # reg_alpha\n    \n    fig, axs = plt.subplots(len(reg_alpha_values), len(num_leaves_values), sharex=True, sharey=True)\n    fig.set_size_inches(75, 60)\n\n    for i in range(len(reg_alpha_values)):\n        for j in range(len(num_leaves_values)):\n            plot_decreasing_error(target_column, training_set, validation_set, scalers, reg_alpha=reg_alpha_values[i], num_leaves=num_leaves_values[j], subplot=axs[i, j])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if PLOT_LEARNING_CURVES:\n    filtered_merged_data = filter_for_training(merged_data)\n    plot_learning_curves('ConfirmedCases', filtered_merged_data, scalers, 0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if PLOT_LEARNING_CURVES:\n    filtered_merged_data = filter_for_training(merged_data)\n    plot_learning_curves('Fatalities', filtered_merged_data, scalers, 0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"alg = {}\nalg['ConfirmedCases'] = preprocess_and_train_for_feature(merged_data, scalers, 'ConfirmedCases', CASES_REG_ALPHA, CASES_NUM_LEAVES)\nalg['Fatalities'] = preprocess_and_train_for_feature(merged_data, scalers, 'Fatalities', FATALITIES_REG_ALPHA, FATALITIES_NUM_LEAVES)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_last_days_masks_for_dataset_of_length_17():\n    masks = buildLastDaysMasks(17)\n    return {2: masks[2][-1],\n                4: masks[4][-1],\n                8: masks[8][-1],\n                16: masks[16][-1]}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LAST_DAYS_MASKS_LENGTH_17 = build_last_days_masks_for_dataset_of_length_17()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict_next_day_cases(row):\n    predicted_cases_growth = preprocess_and_predict_for_feature(row, scalers, 'ConfirmedCases', alg['ConfirmedCases'])\n    predicted_fatalities_growth = preprocess_and_predict_for_feature(row, scalers, 'Fatalities', alg['Fatalities'])\n    \n    [predicted_next_day_cases] = row.ConfirmedCases * predicted_cases_growth\n    [predicted_next_day_fatalities] = row.Fatalities * predicted_fatalities_growth\n    \n    return (predicted_next_day_cases, predicted_next_day_fatalities)\n\ndef calculate_growths_from_last_days_for_province_day(dataset, feature_name):\n    value_day = dataset[feature_name].values[-1]\n    feature_values = dataset.loc[:, feature_name].values\n    \n    growths = {}\n    for num_days in [2, 4, 8, 16]:\n        last_days_avg = ((LAST_DAYS_MASKS_LENGTH_17[num_days] * feature_values).sum() / num_days)\n        growths['Growth' + feature_name + 'From' + str(num_days) + 'DaysAvg'] = (0 if last_days_avg == 0 else (value_day / last_days_avg))\n    return growths\n\ndef generate_predictions_for_province(real_province_data, from_day, to_day, last_day_of_available_real_data):\n    days_to_predict = pd.date_range(last_day_of_available_real_data + pd.Timedelta(days=1), to_day)\n    \n    extended_data = pd.concat([real_province_data.loc[real_province_data.Date <= last_day_of_available_real_data],\n                                pd.DataFrame({'Date': days_to_predict})],\n                                sort=False)\n    \n    convert_datatypes(extended_data)\n    \n    for day in days_to_predict:\n        previous_day_row = extended_data[extended_data.Date == (day - pd.Timedelta(days=1))]\n        (predicted_cases, predicted_fatalities) = predict_next_day_cases(previous_day_row)\n        \n        copy_columns = ['Population', 'HealthExpPerCapita', 'PhysiciansPer1000', 'NursesPer1000', 'PopulationPerArea']\n\n        extended_data.loc[extended_data.Date == day, copy_columns] = previous_day_row.loc[:, copy_columns].values\n        extended_data.loc[extended_data.Date == day, ['ConfirmedCases', 'Fatalities']] = (predicted_cases, predicted_fatalities)\n        extended_data.loc[extended_data.Date == day, 'CasesPerPopulation'] = predicted_cases / extended_data.loc[extended_data.Date == day, 'Population']\n        \n        dataset_to_calculate_growth_features = extended_data[(extended_data.Date >= (day - pd.Timedelta(days=16))) & (extended_data.Date <= day)]\n        \n        for feature_name in ['ConfirmedCases', 'Fatalities']:\n            for growth_feature_name, value in calculate_growths_from_last_days_for_province_day(dataset_to_calculate_growth_features, feature_name).items():\n                extended_data.loc[extended_data.Date == day, growth_feature_name] = value\n        \n    return extended_data.loc[(extended_data.Date >= from_day) & (extended_data.Date <= to_day), ['ConfirmedCases', 'Fatalities']]\n\ndef generate_predictions_for_all_provinces(real_data, predicted_data, from_day, to_day, use_real_data_up_to):\n    for (province, country) in real_data[['Province_State', 'Country_Region']].drop_duplicates().values:\n        provinceMask = (real_data.Province_State == province) & (real_data.Country_Region == country)\n        \n        predictions = generate_predictions_for_province(real_data[provinceMask], from_day, to_day, use_real_data_up_to)\n\n        predicted_data.loc[(predicted_data.Province_State == province) &\n                           (predicted_data.Country_Region == country) &\n                           (pd.to_datetime(predicted_data.Date) >= from_day) &\n                           (pd.to_datetime(predicted_data.Date) <= to_day), ['ConfirmedCases', 'Fatalities']] = predictions.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if GENERATE_SUBMISSION:\n    imput_missing_defaults(test_data, defaults_for_missing)\n    \n    predicted_data = pd.DataFrame(columns=train_data.columns)\n    predicted_data = predicted_data.append(test_data.rename(columns={'ForecastId': 'Id'}), sort=False)\n\n    generate_predictions_for_all_provinces(merged_data, predicted_data, PUBLIC_LEADERBOARD_FROM_DAY, PUBLIC_LEADERBOARD_TO_DAY, PUBLIC_LEADERBOARD_USE_REAL_DATA_UP_TO)\n    generate_predictions_for_all_provinces(merged_data, predicted_data, PRIVATE_LEADERBOARD_FROM_DAY, PRIVATE_LEADERBOARD_TO_DAY, PRIVATE_LEADERBOARD_USE_REAL_DATA_UP_TO)\n\n    submission_data = pd.DataFrame(columns=['ForecastId', 'ConfirmedCases', 'Fatalities'])\n    submission_data = submission_data.append(predicted_data.rename(columns={'Id': 'ForecastId'}).loc[:, ['ForecastId', 'ConfirmedCases', 'Fatalities']], sort=False)\n    \n    submission_data.loc[submission_data.ConfirmedCases.notnull(), 'ConfirmedCases'] = np.floor(submission_data.loc[submission_data.ConfirmedCases.notnull(), 'ConfirmedCases'])\n    submission_data.loc[submission_data.Fatalities.notnull(), 'Fatalities'] = np.floor(submission_data.loc[submission_data.Fatalities.notnull(), 'Fatalities'])\n\n    submission_data.to_csv('submission.csv', index=False)\n    print(\"Your submission was successfully saved!\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}