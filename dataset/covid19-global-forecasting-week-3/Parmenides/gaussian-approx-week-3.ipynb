{"cells":[{"metadata":{},"cell_type":"markdown","source":"In this notebook I will predict the fatalities and confirmed cases assuming a gaussian temporal evolution. The reason why a gaussian may be a good approximation can be found, for instance in the SIR model. A gaussian distribution is assumed for the daily *changes*.\nWe start with loading a few libraries."},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\nimport datetime\nfrom scipy.optimize import curve_fit\nfrom sklearn.metrics import mean_squared_error as mse\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, I define a gaussian distribution function."},{"metadata":{"trusted":true},"cell_type":"code","source":"def gaussian(x,a,mu,sigma):\n    return a*np.exp(-(x-mu)**2./(2.*sigma**2.))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A function for reading the train set. This function requires the region of interest (parameter: regional), and the type of statistics (either \"Fatalities\" or \"ConfirmedCases\"). It returns the daily changes (y), the first date data are available (x0), the difference in date from x0 (dx) and the max value of the data(max_y)."},{"metadata":{"trusted":true},"cell_type":"code","source":"def GetRegion(regional,f=\"Fatalities\"):\n    y=regional[f].values\n    y=np.diff(y,prepend=[0])\n    x=regional[\"Date\"].values.astype(np.datetime64)\n    x0=x[0].copy()\n    dx=x-x0\n    max_y=np.max(y)\n    y=y/np.max(y)\n    return dx,y,x0,max_y\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A function for reading the test set. It needs the regional data under scrutiny and the start data from the *train* data. It returns the difference in number of days from x0 and the id."},{"metadata":{"trusted":true},"cell_type":"code","source":"def SetRegion(regional,x0):\n    x=regional[\"Date\"].values.astype(np.datetime64)\n    f_id=regional[\"ForecastId\"].values.astype(np.int)\n    dx=x-x0\n    return dx,f_id\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A function for fitting the data with a gaussian function. It needs the region of interest and the type of data. It return the best fit for the data (popt) and some information about the fit itself(pcov). The latter is not really used here. In addition, it passes along x0 and max_y from GetRegion()"},{"metadata":{"trusted":true},"cell_type":"code","source":"def doFit(region,typef=\"ConfirmedCases\"):\n    dx,y,x0,max_y=GetRegion(region,f=typef)\n    n=dx.shape[0]\n    mu=np.sum(dx.astype(np.float)*y)/n\n    sigma=np.sum(y*np.square(dx.astype(np.float)-mu))/n\n    popt,pcov=curve_fit(gaussian,dx,y,p0=[1.,mu,sigma],maxfev=1000000)\n    return popt,pcov,x0,max_y\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"One of the problems with my approach is that some nations have limited data, leading to a poor or unstable fit. In order to avoid this, initially I only fit the regions with the highest number of cases and store them in a list."},{"metadata":{"trusted":true},"cell_type":"code","source":"def AppendPopt(list_par,regional,f=\"ConfirmedCases\",cutoff=1000):\n    if regional[f].max()>cutoff:\n        popt,*_,max_y=doFit(regional,f)\n        if popt[0]<100:\n            list_par.append(np.append(popt,max_y))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In order to fit the remaining data, I assume that the spreading of the disease is similar to one of the regions with a very high number of cases. The only difference would be the time at which the disease starts spreading. Thus, I use one of the fitted distribution and \"shift\" it until it fits this data set. Notice that all the distributions are normalized. In practice, I only vary the mean of the gaussian."},{"metadata":{"trusted":true},"cell_type":"code","source":"def ShiftDistr(region,distr,typef):\n    dx,y,x0,max_y=GetRegion(region,f=typef)\n    shift=dx.shape[0]\n    min_mse=1000000.\n    best_popt=distr[0]  #just to initialize, no real meaning\n    for i in distr:\n        #print(i)\n        for mu in np.arange(-shift,shift):\n            curve=gaussian(dx.astype(np.int),i[0],i[1]+mu,i[2])\n            m=mse(y,curve)\n            if m<min_mse:\n                min_mse=m\n                best_popt=i\n                best_popt[1]=i[1]+mu\n\n    return best_popt,x0,max_y\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, once the fitting is completed, I can predict the test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"def PredictDistr(regional,test_dx,par,f):\n    if regional[f].max()>0:                    \n        popt,_,max_y=ShiftDistr(regional,par,typef=f)            \n        curve=max_y*gaussian(test_dx.astype(np.int),*popt[0:3])\n        print(popt,end=' ')            \n    else:\n        curve=np.zeros(test_dx.shape[0])\n        print('[0,0,0]',end=' ') \n\n    return curve\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Once all the functions are defined, the code is as follows. The first loop fits the data for the datasets with the most cases, while the second loop fits all datasets and makes predicitions. As the model developed so far only predicts the changes on a daily basis, I have to integrate the prediction."},{"metadata":{"_uuid":"5ad29cfb-7d2c-4201-bd7c-20e1a52156dc","_cell_guid":"be190c84-1357-4a3e-85a5-0fd95840ff5d","trusted":true},"cell_type":"code","source":" \n\ndf=pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-3/train.csv\").fillna(0)\ncountries=df[\"Country_Region\"].unique()\n\nlist_par=[]\nlist_fat=[]\nfor i in countries: \n\n    tmp=df.loc[df[\"Country_Region\"]==i]\n    states=tmp[\"Province_State\"].unique()\n\n    if states.size<=1:\n        AppendPopt(list_par,tmp,f=\"ConfirmedCases\",cutoff=10000)\n        AppendPopt(list_fat,tmp,f=\"Fatalities\",cutoff=1000)\n    else:\n        for k in states:\n            tmpk=tmp.loc[tmp[\"Province_State\"]==k]\n            AppendPopt(list_par,tmpk,f=\"ConfirmedCases\",cutoff=10000)\n            AppendPopt(list_fat,tmpk,f=\"Fatalities\",cutoff=1000)\n\nprint(list_par)\nprint(list_fat)\n\n\ntest=pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-3/test.csv\").fillna(0)\nfout=open(\"submission.csv\",\"w\")\nprint(\"ForecastId,ConfirmedCases,Fatalities\",file=fout)\n\nstartDate=datetime.date(2020,3,26)\nfor i in countries: \n\n    tmp=df.loc[df[\"Country_Region\"]==i]\n    states=tmp[\"Province_State\"].unique()\n    x0=tmp[\"Date\"].values.astype(np.datetime64)[0]\n\n\n    if states.size<=1:\n        print(i,end=' ')\n        scc=tmp.loc[tmp[\"Date\"].values.astype(np.datetime64)==startDate][\"ConfirmedCases\"].values\n        sfat=tmp.loc[tmp[\"Date\"].values.astype(np.datetime64)==startDate][\"Fatalities\"].values\n        test_tmp=test.loc[test[\"Country_Region\"]==i] \n        test_dx,test_id=SetRegion(test_tmp,x0)\n        curve1=PredictDistr(tmp,test_dx,list_par,\"ConfirmedCases\")\n        curve2=PredictDistr(tmp,test_dx,list_fat,\"Fatalities\")\n        print('')\n\n        for i_id,i_c1,i_c2 in zip(test_id,scc+np.cumsum(curve1),sfat+np.cumsum(curve2)):\n            print(\"{:d},{:d},{:d}\".format(i_id,np.round(i_c1).astype(np.int),np.round(i_c2).astype(np.int)),file=fout)\n\n    else:\n        for k in states:\n            print(i,\",\",k,end=' ')\n            tmpk=tmp.loc[tmp[\"Province_State\"]==k]\n            test_tmp=test.loc[test[\"Country_Region\"]==i]\n            test_tmpk=test_tmp.loc[test_tmp[\"Province_State\"]==k]\n            scc=tmpk.loc[tmpk[\"Date\"].values.astype(np.datetime64)==startDate][\"ConfirmedCases\"].values\n            sfat=tmpk.loc[tmpk[\"Date\"].values.astype(np.datetime64)==startDate][\"Fatalities\"].values\n            test_dx,test_id=SetRegion(test_tmpk,x0)\n            curve1=PredictDistr(tmpk,test_dx,list_par,\"ConfirmedCases\")\n            curve2=PredictDistr(tmpk,test_dx,list_fat,\"Fatalities\")\n            print('')\n                \n            for i_id,i_c1,i_c2 in zip(test_id,scc+np.cumsum(curve1),sfat+np.cumsum(curve2)):\n                print(\"{:d},{:d},{:d}\".format(i_id,np.round(i_c1).astype(np.int),np.round(i_c2).astype(np.int)),file=fout)\n\nfout.close()","execution_count":0,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":4}