{"cells":[{"metadata":{},"cell_type":"markdown","source":"# SEIR-HCD Model\nThis is a working example of a [SIER](https://en.wikipedia.org/wiki/Compartmental_models_in_epidemiology#The_SEIR_model) model with added compartments for HCD. The letters stand for:\n* Susceptible\n* Exposed\n* Infected\n* Recovered\n* Hospitalized\n* Critical\n* Death\n\nI have adapted the equations from these great web apps:\n* http://gabgoh.github.io/COVID/index.html\n* https://neherlab.org/covid19\n\n**NOTE:** If you are looking for the SIER model, check commit 20 or earlier."},{"metadata":{"ExecuteTime":{"end_time":"2020-03-30T20:24:18.873179Z","start_time":"2020-03-30T20:24:18.871087Z"},"trusted":true},"cell_type":"code","source":"OPTIM_DAYS = 16 # Number of days to use for the optimisation evaluation\nRUN_VAL = False\nRUN_SUB = True\nDATE_BORDER = '2020-04-08'\n\nDEBUG = [\n    # 'Italy',\n    # 'France',\n    # 'Hubei'\n]","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-03-30T20:24:17.892422Z","start_time":"2020-03-30T20:24:17.364374Z"},"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"from joblib import Parallel, delayed\nimport multiprocessing\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nimport os\nfrom tqdm.auto import tqdm\nfrom scipy.integrate import solve_ivp\nfrom scipy.optimize import minimize\nfrom sklearn.metrics import mean_squared_log_error, mean_squared_error","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Parameters used in the model\n`R_t` = reproduction number at time t. Typical 3.6* at t=0\n\n**Transition times**\n* `T_inc` = average incubation period. Typical 5.6* days\n* `T_inf` = average infectious period. Typical 2.9 days\n* `T_hosp` = average time a patient is in hospital before either recovering or becoming critical. Typical 4 days\n* `T_crit` = average time a patient is in a critical state (either recover or die). Typical 14 days\n\n**Fractions**\nThese constants are likely to be age specific (hence the subscript a):\n* `m_a` = fraction of infections that are asymptomatic or mild. Assumed 80% (i.e. 20% severe)\n* `c_a` = fraction of severe cases that turn critical. Assumed 10%\n* `f_a` = fraction of critical cases that are fatal. Assumed 30%\n\n*Averages taken from https://www.kaggle.com/covid-19-contributions"},{"metadata":{"ExecuteTime":{"end_time":"2020-03-30T20:24:17.904171Z","start_time":"2020-03-30T20:24:17.893723Z"},"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Susceptible equation\ndef dS_dt(S, I, R_t, t_inf):\n    return -(R_t / t_inf) * I * S\n\n\n# Exposed equation\ndef dE_dt(S, E, I, R_t, t_inf, t_inc):\n    return (R_t / t_inf) * I * S - (E / t_inc)\n\n\n# Infected equation\ndef dI_dt(I, E, t_inc, t_inf):\n    return (E / t_inc) - (I / t_inf)\n\n\n# Hospialized equation\ndef dH_dt(I, C, H, t_inf, t_hosp, t_crit, m_a, f_a):\n    return ((1 - m_a) * (I / t_inf)) + ((1 - f_a) * C / t_crit) - (H / t_hosp)\n\n\n# Critical equation\ndef dC_dt(H, C, t_hosp, t_crit, c_a):\n    return (c_a * H / t_hosp) - (C / t_crit)\n\n\n# Recovered equation\ndef dR_dt(I, H, t_inf, t_hosp, m_a, c_a):\n    return (m_a * I / t_inf) + (1 - c_a) * (H / t_hosp)\n\n\n# Deaths equation\ndef dD_dt(C, t_crit, f_a):\n    return f_a * C / t_crit\n\n\ndef SEIR_HCD_model(t, y, R_t, t_inc=2.9, t_inf=5.2, t_hosp=4, t_crit=10, m_a=0.8, c_a=0.1, f_a=0.3):\n    \"\"\"\n\n    :param t: Time step for solve_ivp\n    :param y: Previous solution or initial values\n    :param R_t: Reproduction number\n    :param t_inc: Average incubation period. Default 5.2 days\n    :param t_inf: Average infectious period. Default 2.9 days\n    :param t_hosp: Average time a patient is in hospital before either recovering or becoming critical. Default 4 days\n    :param t_crit: Average time a patient is in a critical state (either recover or die). Default 14 days\n    :param m_a: Fraction of infections that are asymptomatic or mild. Default 0.8\n    :param c_a: Fraction of severe cases that turn critical. Default 0.1\n    :param f_a: Fraction of critical cases that are fatal. Default 0.3\n    :return:\n    \"\"\"\n    if callable(R_t):\n        reprod = R_t(t)\n    else:\n        reprod = R_t\n        \n    S, E, I, R, H, C, D = y\n    \n    S_out = dS_dt(S, I, reprod, t_inf)\n    E_out = dE_dt(S, E, I, reprod, t_inf, t_inc)\n    I_out = dI_dt(I, E, t_inc, t_inf)\n    R_out = dR_dt(I, H, t_inf, t_hosp, m_a, c_a)\n    H_out = dH_dt(I, C, H, t_inf, t_hosp, t_crit + t_hosp, m_a, f_a)\n    C_out = dC_dt(H, C, t_hosp, t_crit + t_hosp, c_a)\n    D_out = dD_dt(C, t_crit + t_hosp, f_a)\n    return [S_out, E_out, I_out, R_out, H_out, C_out, D_out]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Fitting the model to data\nThere are certain variables that we can play with to fit the model to real data:\n* Average incubation period, `t_inc`\n* Average infection period, `t_inf`\n* Average hospitalization period, `t_hosp`\n* Average critital period, `t_crit`\n* The fraction of mild/asymptomatic cases, `m_a`\n* The fraction of severe cases that turn critical, `c_a`\n* The fraction of critical cases that result in a fatality, `f_a`\n* Reproduction number, `R_0` or `R_t`\n\nThe some of these are likely to be constants specific to the virus and some are likely to be time dependent variables dependent on factors such as:\n* When a government intervened\n* Peoples behaviours (do people actively self-isolate, not visit religious shrines etc.)\n* Population demographic of a country (is a significant proportion of the population old?). This is the `a` subscript\n* Heathcare system capacity (hostpital beds per capita)\n* Number of testing kits available\n\nWe have already used two different reproduction numbers above. Let's see if we can derive a time-dependent `R_t` from the data. We will also try and optimize a handful of the parameters above to match the data.\n\nWe will also compare this to just using a single reproduction number. This might actaully be more suitable in countries where the outbreak has just started or they are struggling to limit the spread.\n\nThere are lots of ways to decay a parameter in epidemiology. I'm going to use a Hill decay, which has 2 parameters, `k` and `L` (the half decay constant):"},{"metadata":{"ExecuteTime":{"end_time":"2020-03-30T20:24:18.869654Z","start_time":"2020-03-30T20:24:18.799083Z"},"trusted":true},"cell_type":"code","source":"data_path = Path('/kaggle/input/covid19-global-forecasting-week-3/')\n\ntrain = pd.read_csv(data_path / 'train.csv', parse_dates=['Date'])\ntest = pd.read_csv(data_path /'test.csv', parse_dates=['Date'])\nsubmission = pd.read_csv(data_path /'submission.csv')\n\n# Load the population data into lookup dicts\npop_info = pd.read_csv('/kaggle/input/covid19-population-data/population_data.csv')\ncountry_pop = pop_info.query('Type == \"Country/Region\"')\nprovince_pop = pop_info.query('Type == \"Province/State\"')\ncountry_lookup = dict(zip(country_pop['Name'], country_pop['Population']))\nprovince_lookup = dict(zip(province_pop['Name'], province_pop['Population']))\n\n# Fix the Georgia State/Country confusion - probably a better was of doing this :)\ntrain['Province_State'] = train['Province_State'].replace('Georgia', 'Georgia (State)')\ntest['Province_State'] = test['Province_State'].replace('Georgia', 'Georgia (State)')\nprovince_lookup['Georgia (State)'] = province_lookup['Georgia']\n\ntrain['Area'] = train['Province_State'].fillna(train['Country_Region'])\ntest['Area'] = test['Province_State'].fillna(test['Country_Region'])\n\n# https://www.kaggle.com/c/covid19-global-forecasting-week-1/discussion/139172\ntrain['ConfirmedCases'] = train.groupby('Area')['ConfirmedCases'].cummax()\ntrain['Fatalities'] = train.groupby('Area')['Fatalities'].cummax()\n\n# Remove the leaking data\ntrain_full = train.copy()\nvalid = train[train['Date'] >= test['Date'].min()]\ntrain = train[train['Date'] < test['Date'].min()]\nVALID_START, VALID_END = valid['Date'].min(), valid['Date'].max()\n\n# Split the test into public & private\ntest_public = test[test['Date'] <= DATE_BORDER]\ntest_private = test[test['Date'] > DATE_BORDER]\nTEST_PUBLIC_START, TEST_PUBLIC_END = test_public['Date'].min(), test_public['Date'].max()\nTEST_PRIVATE_START, TEST_PRIVATE_END = test_private['Date'].min(), test_private['Date'].max()\n\n# Use a multi-index for easier slicing\ntrain_full.set_index(['Area', 'Date'], inplace=True)\ntrain.set_index(['Area', 'Date'], inplace=True)\nvalid.set_index(['Area', 'Date'], inplace=True)\ntest_public.set_index(['Area', 'Date'], inplace=True)\ntest_private.set_index(['Area', 'Date'], inplace=True)\n\n# submission['ConfirmedCases'] = 0\n# submission['Fatalities'] = 0\nsubmission = submission.drop(['ConfirmedCases', 'Fatalities'], axis=1)\n\ntrain_full.shape, train.shape, valid.shape, test_public.shape, test_private.shape, submission.shape\nprint(f'Public test: {TEST_PUBLIC_START} - {TEST_PUBLIC_END}')\nprint(f'Private test: {TEST_PRIVATE_START} - {TEST_PRIVATE_END}')\nprint(f'Validation: {VALID_START} - {VALID_END}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = submission.merge(test[['ForecastId', 'Area', 'Date']], on='ForecastId', how='left')\nsubmission.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"popu = pop_info.copy()\npopu.loc[(popu['Name'].str.contains('Georgia')) & (popu['Type'] == 'Province/State'), 'Name'] = 'Georgia (State)'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The function below evaluates a model with a constant `R` number as well as `t_hosp`, `t_crit`, `m`, `c`, `f`"},{"metadata":{"ExecuteTime":{"end_time":"2020-03-30T20:24:18.882722Z","start_time":"2020-03-30T20:24:18.874114Z"},"trusted":true},"cell_type":"code","source":"# Use a constant reproduction number\ndef eval_model_const(params, data, population, return_solution=False, forecast_days=0):\n    R_0, t_hosp, t_crit, m, c, f = params\n    N = population\n    n_infected = data['ConfirmedCases'].iloc[0]\n    max_days = len(data) + forecast_days\n    initial_state = [(N - n_infected)/ N, 0, n_infected / N, 0, 0, 0, 0]\n    args = (R_0, 5.6, 2.9, t_hosp, t_crit, m, c, f)\n               \n    sol = solve_ivp(SEIR_HCD_model, [0, max_days], initial_state, args=args, t_eval=np.arange(0, max_days))\n    \n    sus, exp, inf, rec, hosp, crit, deaths = sol.y\n    \n    y_pred_cases = np.clip(inf + rec + hosp + crit + deaths, 0, np.inf) * population\n    y_true_cases = data['ConfirmedCases'].values\n    y_pred_fat = np.clip(deaths, 0, np.inf) * population\n    y_true_fat = data['Fatalities'].values\n    \n    optim_days = min(OPTIM_DAYS, len(data))  # Days to optimise for\n    weights = 1 / np.arange(1, optim_days+1)[::-1]  # Recent data is more heavily weighted\n    msle_cases = mean_squared_log_error(y_true_cases[-optim_days:], y_pred_cases[-optim_days:], weights)\n    msle_fat = mean_squared_log_error(y_true_fat[-optim_days:], y_pred_fat[-optim_days:], weights)\n    msle_final = np.mean([msle_cases, msle_fat])\n    \n    if return_solution:\n        return msle_final, sol\n    else:\n        return msle_final","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The function below is essentially the same as above, by R is decayed using a Hill decay function. This model requires 2 additional parameters to be optimized, `k` & `L`"},{"metadata":{"ExecuteTime":{"end_time":"2020-03-30T20:24:18.892749Z","start_time":"2020-03-30T20:24:18.88376Z"},"trusted":true},"cell_type":"code","source":"# Use a Hill decayed reproduction number\ndef eval_model_decay(params, data, population, return_solution=False, forecast_days=0):\n    R_0, t_hosp, t_crit, m, c, f, k, L = params  \n    N = population\n    n_infected = data['ConfirmedCases'].iloc[0]\n    max_days = len(data) + forecast_days\n    \n    # https://github.com/SwissTPH/openmalaria/wiki/ModelDecayFunctions   \n    # Hill decay. Initial values: R_0=2.2, k=2, L=50\n    def time_varying_reproduction(t): \n        return R_0 / (1 + (t/L)**k)\n    \n    initial_state = [(N - n_infected)/ N, 0, n_infected / N, 0, 0, 0, 0]\n    args = (time_varying_reproduction, 5.6, 2.9, t_hosp, t_crit, m, c, f)\n            \n    sol = solve_ivp(SEIR_HCD_model, [0, max_days], initial_state, args=args, t_eval=np.arange(0, max_days))\n    \n    sus, exp, inf, rec, hosp, crit, deaths = sol.y\n    \n    y_pred_cases = np.clip(inf + rec + hosp + crit + deaths, 0, np.inf) * population\n    y_true_cases = data['ConfirmedCases'].values\n    y_pred_fat = np.clip(deaths, 0, np.inf) * population\n    y_true_fat = data['Fatalities'].values\n    \n    optim_days = min(OPTIM_DAYS, len(data))  # Days to optimise for\n    weights = 1 / np.arange(1, optim_days+1)[::-1]  # Recent data is more heavily weighted    \n    msle_cases = mean_squared_log_error(y_true_cases[-optim_days:], y_pred_cases[-optim_days:], weights)\n    msle_fat = mean_squared_log_error(y_true_fat[-optim_days:], y_pred_fat[-optim_days:], weights)\n    msle_final = np.mean([msle_cases, msle_fat])\n    \n    if return_solution:\n        return msle_final, sol\n    else:\n        return msle_final","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def use_last_value(train_data, test_data):\n    y_pred = train_data[['ConfirmedCases', 'Fatalities']].copy().reset_index()\n    y_pred['R'] = 0.\n\n    # Last value\n    lv = pd.DataFrame(\n        data={\n            'ConfirmedCases': [y_pred.iloc[-1]['ConfirmedCases']],\n            'Fatalities': [y_pred.iloc[-1]['Fatalities']],\n            'R': [0.],\n            'Date': [y_pred.iloc[-1]['Date']]\n        }\n    )\n\n    dates_train = train_data.index.tolist()\n    dates_test = [d for d in test_data.index.tolist() if d not in dates_train]\n    dates_all = sorted(dates_train + dates_test)\n    \n    # Fill the test data\n    for d in dates_test:\n        _lv = lv.copy()\n        _lv['Date'] = d\n        y_pred = pd.concat([y_pred, _lv], ignore_index=True, sort=True)\n    \n    y_pred['Date'] = dates_all\n    y_pred.set_index('Date', inplace=True)\n    \n    return y_pred","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-03-30T20:24:18.909281Z","start_time":"2020-03-30T20:24:18.89988Z"},"trusted":true},"cell_type":"code","source":"def plot_model_results(y_pred, train_data, valid_data=None, area=None):\n    if area is None:\n        area = 'unknown'\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16,5))\n    \n    ax1.set_title(f'{area} Confirmed Cases')\n    ax2.set_title(f'{area} Fatalities')\n    \n    train_data['ConfirmedCases'].plot(label='Confirmed Cases (train)', color='g', ax=ax1)\n    y_pred.loc[train_data.index, 'ConfirmedCases'].plot(label='Modeled Cases', color='r', ax=ax1)\n    ax3 = y_pred['R'].plot(label='Reproduction number', color='c', linestyle='-', secondary_y=True, ax=ax1)\n    ax3.set_ylabel(\"Reproduction number\", fontsize=10, color='c');\n        \n    train_data['Fatalities'].plot(label='Fatalities (train)', color='g', ax=ax2)\n    y_pred.loc[train_data.index, 'Fatalities'].plot(label='Modeled Fatalities', color='r', ax=ax2)\n    \n    if valid_data is not None:\n        valid_data['ConfirmedCases'].plot(label='Confirmed Cases (valid)', color='g', linestyle=':', ax=ax1)\n        valid_data['Fatalities'].plot(label='Fatalities (valid)', color='g', linestyle=':', ax=ax2)\n        y_pred.loc[valid_data.index, 'ConfirmedCases'].plot(label='Modeled Cases (forecast)', color='r', linestyle=':', ax=ax1)\n        y_pred.loc[valid_data.index, 'Fatalities'].plot(label='Modeled Fatalities (forecast)', color='r', linestyle=':', ax=ax2)\n    else:\n        y_pred.loc[:, 'ConfirmedCases'].plot(label='Modeled Cases (forecast)', color='r', linestyle=':', ax=ax1)\n        y_pred.loc[:, 'Fatalities'].plot(label='Modeled Fatalities (forecast)', color='r', linestyle=':', ax=ax2)\n        \n    ax1.legend(loc='best')\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The function below fits a SEIR-HCD model for each area, either using a constant R or a decayed R, whichever is better. If the total cases/1M pop is below 1, then the last value is used."},{"metadata":{"ExecuteTime":{"end_time":"2020-03-30T20:24:18.92308Z","start_time":"2020-03-30T20:24:18.91017Z"},"trusted":true},"cell_type":"code","source":"def fit_model_public(area_name, \n                     initial_guess=[3.6, 4, 10, 0.8, 0.1, 0.3, 2, 50],\n                     bounds=((1, 20), # R bounds\n                             (0.5, 10), (2, 20), # transition time param bounds\n                             (0.5, 1), (0, 1), (0, 1), (1, 5), (1, 100)), # fraction time param bounds\n                     make_plot=True):\n        \n    train_data = train.loc[area_name].query('ConfirmedCases > 0')\n    valid_data = valid.loc[area_name]\n    test_data = test_public.loc[area_name]  \n    \n    try:\n        population = province_lookup[area_name]\n    except KeyError:\n        population = country_lookup[area_name]\n        \n    cases_per_million = train_data['ConfirmedCases'].max() * 10**6 / population\n    n_infected = train_data['ConfirmedCases'].iloc[0]\n        \n#     if cases_per_million < 1:\n#         # print('Using last value')\n#         return use_last_value(train_data, test_data)\n                \n    res_const = minimize(eval_model_const, initial_guess[:-2], bounds=bounds[:-2],\n                         args=(train_data, population, False),\n                         method='L-BFGS-B')\n    \n    res_decay = minimize(eval_model_decay, initial_guess, bounds=bounds,\n                         args=(train_data, population, False),\n                         method='L-BFGS-B')\n    \n    dates_all = train_data.index.append(test_data.index)\n    dates_val = train_data.index.append(valid_data.index)\n    \n    \n    # If using a constant R number is better, use that model\n    if res_const.fun < res_decay.fun:\n        msle, sol = eval_model_const(res_const.x, train_data, population, True, len(test_data))\n        res = res_const\n        R_t = pd.Series([res_const.x[0]] * len(dates_val), dates_val)\n    else:\n        msle, sol = eval_model_decay(res_decay.x, train_data, population, True, len(test_data))\n        res = res_decay\n        \n        # Calculate the R_t values\n        t = np.arange(len(dates_val))\n        R_0, t_hosp, t_crit, m, c, f, k, L = res.x  \n        R_t = pd.Series(R_0 / (1 + (t/L)**k), dates_val)\n        \n    sus, exp, inf, rec, hosp, crit, deaths = sol.y\n    \n    y_pred = pd.DataFrame({\n        'ConfirmedCases': np.clip(inf + rec + hosp + crit + deaths, 0, np.inf) * population,\n        'Fatalities': np.clip(deaths, 0, np.inf) * population,\n        'R': R_t,\n    }, index=dates_all)\n    \n    # Sanity check\n    pred_max_cc, pred_max_f = y_pred['ConfirmedCases'].max(), y_pred['Fatalities'].max()\n    obs_max_cc, obs_max_f = train_data['ConfirmedCases'].max(), train_data['Fatalities'].max()\n\n    if (pred_max_cc < obs_max_cc) or (pred_max_f < obs_max_f):\n        lv = use_last_value(train_data, test_data)\n        if pred_max_cc < obs_max_cc:\n            y_pred['ConfirmedCases'] = lv['ConfirmedCases']\n        if pred_max_f < obs_max_f:\n            y_pred['Fatalities'] = lv['Fatalities']      \n    \n    y_pred_valid = y_pred.iloc[len(train_data): len(train_data)+len(valid_data)]\n    y_pred_test = y_pred.iloc[len(train_data):]\n    y_true_valid = valid_data[['ConfirmedCases', 'Fatalities']]\n        \n    valid_msle_cases = np.sqrt(mean_squared_log_error(y_true_valid['ConfirmedCases'], y_pred_valid['ConfirmedCases']))\n    valid_msle_fat = np.sqrt(mean_squared_log_error(y_true_valid['Fatalities'], y_pred_valid['Fatalities']))\n    valid_msle = np.mean([valid_msle_cases, valid_msle_fat])\n    \n    if make_plot:\n        print(f'Validation RMSLE: {valid_msle:0.5f}')\n        print(f'R: {res.x[0]:0.3f}, t_hosp: {res.x[1]:0.3f}, t_crit: {res.x[2]:0.3f}, '\n              f'm: {res.x[3]:0.3f}, c: {res.x[4]:0.3f}, f: {res.x[5]:0.3f}')\n        plot_model_results(y_pred, train_data, valid_data, area=area_name)\n        \n#     # Put the forecast in the submission\n#     forecast_ids = test_data['ForecastId']\n#     submission.loc[forecast_ids, ['ConfirmedCases', 'Fatalities']] = y_pred_test[['ConfirmedCases', 'Fatalities']].values\n    \n    return (valid_msle, valid_msle_cases, valid_msle_fat), y_pred\n            ","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-03-30T20:24:18.934763Z","start_time":"2020-03-30T20:24:18.923988Z"},"trusted":true},"cell_type":"code","source":"# Fit a model on the full dataset (i.e. no validation)\ndef fit_model_private(area_name, \n                      initial_guess=[3.6, 4, 10, 0.8, 0.1, 0.3, 2, 50],\n                      bounds=((1, 20), # R bounds\n                              (0.5, 10), (2, 20), # transition time param bounds\n                              (0.5, 1), (0, 1), (0, 1), (1, 5), (1, 100)), # fraction time param bounds\n                      make_plot=True):\n        \n    train_data = train_full.loc[area_name].query('ConfirmedCases > 0')\n    test_data = test_private.loc[area_name]\n    \n    try:\n        population = province_lookup[area_name]\n    except KeyError:\n        population = country_lookup[area_name]\n        \n    cases_per_million = train_data['ConfirmedCases'].max() * 10**6 / population\n    n_infected = train_data['ConfirmedCases'].iloc[0]\n        \n    if cases_per_million < 1:\n        return use_last_value(train_data, test_data)\n                \n    res_const = minimize(eval_model_const, initial_guess[:-2], bounds=bounds[:-2],\n                         args=(train_data, population, False),\n                         method='L-BFGS-B')\n    \n    res_decay = minimize(eval_model_decay, initial_guess, bounds=bounds,\n                         args=(train_data, population, False),\n                         method='L-BFGS-B')\n    \n    dates_all = train_data.index.append(test_data.index)\n    \n    \n    # If using a constant R number is better, use that model\n    if res_const.fun < res_decay.fun:\n        msle, sol = eval_model_const(res_const.x, train_data, population, True, len(test_data))\n        res = res_const\n        R_t = pd.Series([res_const.x[0]] * len(dates_all), dates_all)\n    else:\n        msle, sol = eval_model_decay(res_decay.x, train_data, population, True, len(test_data))\n        res = res_decay\n        \n        # Calculate the R_t values\n        t = np.arange(len(dates_all))\n        R_0, t_hosp, t_crit, m, c, f, k, L = res.x  \n        R_t = pd.Series(R_0 / (1 + (t/L)**k), dates_all)\n        \n    sus, exp, inf, rec, hosp, crit, deaths = sol.y\n    \n    y_pred = pd.DataFrame({\n        'ConfirmedCases': np.clip(inf + rec + hosp + crit + deaths, 0, np.inf) * population,\n        'Fatalities': np.clip(deaths, 0, np.inf) * population,\n        'R': R_t,\n    }, index=dates_all)\n    \n    # Sanity check\n    pred_max_cc, pred_max_f = y_pred['ConfirmedCases'].max(), y_pred['Fatalities'].max()\n    obs_max_cc, obs_max_f = train_data['ConfirmedCases'].max(), train_data['Fatalities'].max()\n    if (pred_max_cc < obs_max_cc) or (pred_max_f < obs_max_f):\n        lv = use_last_value(train_data, test_data)\n        if pred_max_cc < obs_max_cc:\n            y_pred['ConfirmedCases'] = lv['ConfirmedCases']\n        if pred_max_f < obs_max_f:\n            y_pred['Fatalities'] = lv['Fatalities']    \n    \n    y_pred_test = y_pred.iloc[len(train_data):]\n    \n    if make_plot:\n        print(f'R: {res.x[0]:0.3f}, t_hosp: {res.x[1]:0.3f}, t_crit: {res.x[2]:0.3f}, '\n              f'm: {res.x[3]:0.3f}, c: {res.x[4]:0.3f}, f: {res.x[5]:0.3f}')\n        plot_model_results(y_pred, train_data, area=area_name)\n        \n#     # Put the forecast in the submission\n#     forecast_ids = test_data['ForecastId']\n#     submission.loc[forecast_ids, ['ConfirmedCases', 'Fatalities']] = y_pred_test[['ConfirmedCases', 'Fatalities']].values\n\n    return y_pred\n            ","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-03-30T20:24:36.051856Z","start_time":"2020-03-30T20:24:18.935707Z"},"trusted":true},"cell_type":"code","source":"if DEBUG is None:\n    DEBUG = []\nfor c in DEBUG:\n    score, _ = fit_model_public(c)\n    _ = fit_model_private(c)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Calculate for all countries"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"def get_score(c):\n    try:\n        score, _ = fit_model_public(c, make_plot=False)\n    except Exception as e:\n        score = (np.nan, np.nan, np.nan)\n    return {'Country': c, 'RMSLE': score[0], 'RMSLE_CASES': score[1], 'RMSLE_FATALITIES': score[2]}\n\n\nif RUN_VAL:\n    validation_scores = Parallel(n_jobs=multiprocessing.cpu_count(), verbose=10)(delayed(get_score)(c) for c in test_public.index.levels[0].values)\n    validation_scores = pd.DataFrame(validation_scores)\n    print(f'Mean validation score: {validation_scores[\"RMSLE\"].mean():0.3f}')\n    print(f'Cases validation score: {validation_scores[\"RMSLE_CASES\"].mean():0.3f}')\n    print(f'Fatalities validation score: {validation_scores[\"RMSLE_FATALITIES\"].mean():0.3f}')          \n    n_nans = validation_scores['RMSLE'].isnull().sum()\n    print(f'NaNs = {n_nans}')\n    # Find which areas are not being predicted well\n    validation_scores = validation_scores.sort_values(by=['RMSLE'], ascending=False)\n    print(validation_scores.head(20))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Optim days = 210: Mean validation score: 0.826","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute_scores(cc_true, cc_pred, f_true, f_pred):\n    rmsle_cc = np.sqrt(mean_squared_log_error(cc_true, cc_pred))\n    rmsle_f = np.sqrt(mean_squared_log_error(f_true, f_pred))\n    rmsle_tot = np.mean([rmsle_cc, rmsle_f])\n    return rmsle_tot, rmsle_cc, rmsle_f","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_lb_preds(c):\n    try:\n        preds = fit_model_private(c, make_plot=False)\n        preds = preds.reset_index().drop('R', axis=1)\n        preds['Area'] = c\n        return preds.merge(submission[['Area', 'Date', 'ForecastId']], on=['Area', 'Date'], how='left')\n    except Exception as e:\n        print(str(e))\n        return pd.DataFrame()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"if RUN_SUB:\n    lb_preds = Parallel(n_jobs=multiprocessing.cpu_count(), verbose=10)(delayed(get_lb_preds)(c) for c in test_private.index.levels[0].values)\n#     lb_preds = []\n#     for c in tqdm(test_private.index.levels[0].values):\n#         lb_preds.append(get_lb_preds(c))    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if RUN_SUB:\n    df_preds = pd.concat(lb_preds)\n    df_preds['ForecastId'] = df_preds['ForecastId'].fillna(-1).astype(int)\n    # Prepare sub\n    df_preds_test = df_preds[df_preds['Date'] >= TEST_PUBLIC_START].copy()\n    df_preds_val = df_preds[(df_preds['Date'] >= VALID_START) & (df_preds['Date'] <= VALID_END)].copy()\n    df_preds_val = df_preds_val.rename({'ConfirmedCases': 'ConfirmedCases_pred', 'Fatalities': 'Fatalities_pred'}, axis=1)\n    df_preds_val = df_preds_val.merge(valid[['ConfirmedCases', 'Fatalities']], on=['Area', 'Date'], how='left')\n    df_preds_val = df_preds_val.sort_values(['Area', 'Date'])\n    print(VALID_START, VALID_END, df_preds['Date'].min(), df_preds['Date'].max())\n    rmsle_tot, rmsle_cc, rmsle_f = compute_scores(\n        df_preds_val['ConfirmedCases'], \n        df_preds_val['ConfirmedCases_pred'],\n        df_preds_val['Fatalities'],\n        df_preds_val['Fatalities_pred'],\n    )\n    print(rmsle_tot, rmsle_cc, rmsle_f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_preds_test.shape, df_preds_test.drop_duplicates(subset='ForecastId').shape)\ndf_preds_test = df_preds_test.merge(popu, left_on='Area', right_on='Name', how='left')\ndf_preds_test = df_preds_test.drop_duplicates(subset=['Area', 'Date'])\nprint(df_preds_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_preds_val.shape)\ndf_preds_val = df_preds_val.merge(popu, left_on='Area', right_on='Name', how='left')\ndf_preds_val = df_preds_val.drop_duplicates(subset=['Area', 'Date'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# THR = 0.12\n# best_thr, best_score = 1., rmsle_cc\n# for thr in np.arange(0.000001, 0.0001, 0.000001):\n#     sub = df_preds_val.copy()\n#     sub['ConfirmedCases_pred'] = sub.apply(\n#         lambda x: min(x['ConfirmedCases_pred'], x['Population'] * thr) if x['Population'] >= 100.E+06 else x['ConfirmedCases_pred'], axis=1\n#     )    \n#     score_tot, score_cc, score_f = compute_scores(\n#             sub['ConfirmedCases'], \n#             sub['ConfirmedCases_pred'],\n#             sub['Fatalities'],\n#             sub['Fatalities_pred'],\n#         )\n#     if score_cc < best_score:\n#         best_thr = float(thr)\n#         best_score = float(score_cc)\n#         print(thr, score_cc)\n# df_preds_val['ConfirmedCases_pred'] = df_preds_val.apply(\n#         lambda x: min(x['ConfirmedCases_pred'], x['Population'] * best_thr) if x['Population'] >= 100.E+06 else x['ConfirmedCases_pred'], axis=1\n# )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# THR = 0.12\n# best_thr = 1.\n# for thr in np.arange(0.0001, 0.01, 0.0001):\n#     sub = df_preds_val.copy()\n#     sub['ConfirmedCases_pred'] = sub.apply(\n#         lambda x: min(x['ConfirmedCases_pred'], x['Population'] * thr) if 10.E+06 <= x['Population'] < 100.E+06 else x['ConfirmedCases_pred'], axis=1\n#     )    \n#     score_tot, score_cc, score_f = compute_scores(\n#             sub['ConfirmedCases'], \n#             sub['ConfirmedCases_pred'],\n#             sub['Fatalities'],\n#             sub['Fatalities_pred'],\n#         )\n#     if score_cc < best_score:\n#         best_thr = float(thr)\n#         best_score = float(score_cc)\n#         print(thr, score_cc)\n# df_preds_val['ConfirmedCases_pred'] = df_preds_val.apply(\n#         lambda x: min(x['ConfirmedCases_pred'], x['Population'] * best_thr) if 10.E+06 <= x['Population'] < 100.E+06 else x['ConfirmedCases_pred'], axis=1\n# )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# THR = 0.12\n# best_thr = 1.\n# for thr in np.arange(0.001, 1., 0.001):\n#     sub = df_preds_val.copy()\n#     sub['ConfirmedCases_pred'] = sub.apply(\n#         lambda x: min(x['ConfirmedCases_pred'], x['Population'] * thr) if x['Population'] < 10.E+06 else x['ConfirmedCases_pred'], axis=1\n#     )    \n#     score_tot, score_cc, score_f = compute_scores(\n#             sub['ConfirmedCases'], \n#             sub['ConfirmedCases_pred'],\n#             sub['Fatalities'],\n#             sub['Fatalities_pred'],\n#         )\n#     if score_cc < best_score:\n#         best_thr = float(thr)\n#         best_score = float(score_cc)\n#         print(thr, score_cc)\n# # df_preds_val['ConfirmedCases_pred'] = df_preds_val.apply(\n# #         lambda x: min(x['ConfirmedCases_pred'], x['Population'] * best_thr) if x['Population'] < 10.E+06 else x['ConfirmedCases_pred'], axis=1\n# # )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# THR = 0.12\n# best_thr, best_score = 1., rmsle_f\n# for thr in np.arange(0.01, 0.3, 0.01):\n#     sub = df_preds_val.copy()\n#     sub['Fatalities_pred'] = sub.apply(\n#         lambda x: min(x['Fatalities_pred'], x['ConfirmedCases_pred'] * thr), axis=1\n#     )\n#     score_tot, score_cc, score_f = compute_scores(\n#             sub['ConfirmedCases'], \n#             sub['ConfirmedCases_pred'],\n#             sub['Fatalities'],\n#             sub['Fatalities_pred'],\n#         )\n#     if score_f < best_score:\n#         best_thr = thr\n#         best_score = score_f\n#         print(thr, score_f)\n# df_preds_val['Fatalities_pred'] = df_preds_val.apply(\n#         lambda x: min(x['Fatalities_pred'], x['ConfirmedCases_pred'] * best_thr), axis=1\n# )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cap_cases_val(x):\n    if x['Population'] >= 100.E+06: \n        return min(x['ConfirmedCases_pred'], x['Population'] * 0.001)\n    if x['Population'] >= 10.E+06: \n        return min(x['ConfirmedCases_pred'], x['Population'] * 0.0029)\n    return min(x['ConfirmedCases_pred'], x['Population'] * 0.19)\n\n\ndef cap_cases(x):\n    if x['Population'] >= 100.E+06: \n        return min(x['ConfirmedCases'], x['Population'] * 0.001)\n    if x['Population'] >= 10.E+06: \n        return min(x['ConfirmedCases'], x['Population'] * 0.0029)\n    return min(x['ConfirmedCases'], x['Population'] * 0.0029)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_preds_val['ConfirmedCases_pred'] = df_preds_val.apply(cap_cases_val, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_preds_val['Fatalities_pred'] = df_preds_val['Fatalities_pred'].clip(0., df_preds_val['ConfirmedCases_pred'] * 0.26)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score_tot, score_cc, score_f = compute_scores(\n        df_preds_val['ConfirmedCases'], \n        df_preds_val['ConfirmedCases_pred'],\n        df_preds_val['Fatalities'],\n        df_preds_val['Fatalities_pred'],\n    )\nprint(score_tot, score_cc, score_f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cap everything\ndf_preds_test['ConfirmedCases'] = df_preds_test.apply(cap_cases, axis=1)\ndf_preds_test['Fatalities'] = df_preds_test['Fatalities'].clip(0., df_preds_test['ConfirmedCases'] * 0.26)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if RUN_SUB:\n    for country in test_private.index.levels[0].values:\n        df_c = df_preds_val[df_preds_val['Area'] == country].copy()\n        try:\n            score_tot, score_cc, score_f = compute_scores(\n                df_c['ConfirmedCases'], \n                df_c['ConfirmedCases_pred'],\n                df_c['Fatalities'],\n                df_c['Fatalities_pred'],\n            )\n            if score_tot > 0.5:\n                print(f'{country}: {score_tot:.3f} {score_cc:.3f} {score_f:.3f}')\n        except:\n            print(f'No score for {country}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if RUN_SUB:    \n    submission = submission.merge(df_preds_test[['ForecastId', 'ConfirmedCases', 'Fatalities']], on='ForecastId', how='left')\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if RUN_SUB:\n    submission['ConfirmedCases'].isnull().sum(), submission['Fatalities'].isnull().sum()\n    submission['Fatalities'] = submission['Fatalities'].fillna(0.)\n    submission['ConfirmedCases'] = submission['ConfirmedCases'].fillna(0.)\n    submission = submission[['ForecastId', 'ConfirmedCases', 'Fatalities']]\n    submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Todo/ideas\n* Some of the search boundaries boundaries are dependent on each other (e.g. `t_hosp` should always be less than `t_crit`). Using `NonlinearConstraint` may solve this\n* Time dependence of other parameters, e.g. `f` as hospitals get overwhelmed\n* Mix in other sources of data (e.g. number of hospital beds, age demographics)\n* Global optmisation of virus specific parameters\n* Use as features into a different model"},{"metadata":{},"cell_type":"markdown","source":"# Thoughts on compartment models\nI think compartment models are very powerful toy models and add lots of value for scenario testing, e.g. when a healthcare system might be overwhelmed, the impact of various interventions etc.. They can also be easily bootstrapped using population demographics to generate uncertainty estimates on those scenarios. \n\nThe hospitalization compartment in this model is tricky to work with as I had to set the lower bounds for these transition times quite low (`t_hosp`, `t_crit`). I think the reason for this is that many countries are telling people to stay away from the hospitals unless they are experiencing severe symptoms, meaning that once a patient is in a hospital, there are likely to already be very sick. This means when I optimize the transition times to go from hospitalized > severe, the times are often very small.\n\nHowever, I think that for this particular forecasting exercise and metric, there are more accurate methods that can be used. One could also use a compartment model like this to generate features for another model.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"14bee22e5e224199bb86ed3e06ac4d3f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"33bd8d6e036042d68c3808f02e46b869":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3aa77069515147a89ec42c9c47aab9e3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"40a965310e694ab3819ccb71a199701f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"100%","description_tooltip":null,"layout":"IPY_MODEL_ef09ed561a8849169a904c0f7cd0ffb3","max":306,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c3c5c23a96cb419db7975b7d3c9d8be9","value":306}},"5533080c0c0c452d8ee24f1b713ccd3a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"100%","description_tooltip":null,"layout":"IPY_MODEL_96fd28fb95124cac9e27be1d5479ab81","max":306,"min":0,"orientation":"horizontal","style":"IPY_MODEL_14bee22e5e224199bb86ed3e06ac4d3f","value":306}},"96cfd434b85945e285c1499f0e94d9bc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b5f54992a84f4078b82c374803db03ab","placeholder":"​","style":"IPY_MODEL_b0ee6276c2ca4098b2aa2f8c12ce2716","value":" 306/306 [45:47&lt;00:00,  8.98s/it]"}},"96fd28fb95124cac9e27be1d5479ab81":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9923cdbcc95b416f930cc115f5f89668":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ecab8702fd474b51b78c04d16ead97b5","placeholder":"​","style":"IPY_MODEL_f725a2ae3c1043c0b1855840baf53096","value":" 306/306 [35:53&lt;00:00,  7.04s/it]"}},"ac80e9be4f9a401e849a08dbfcf7325e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_40a965310e694ab3819ccb71a199701f","IPY_MODEL_9923cdbcc95b416f930cc115f5f89668"],"layout":"IPY_MODEL_3aa77069515147a89ec42c9c47aab9e3"}},"b0ee6276c2ca4098b2aa2f8c12ce2716":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b5f54992a84f4078b82c374803db03ab":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c3c5c23a96cb419db7975b7d3c9d8be9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"ecab8702fd474b51b78c04d16ead97b5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef09ed561a8849169a904c0f7cd0ffb3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f725a2ae3c1043c0b1855840baf53096":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fdf25ec94c0d485aab47dad5560b8aaa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5533080c0c0c452d8ee24f1b713ccd3a","IPY_MODEL_96cfd434b85945e285c1499f0e94d9bc"],"layout":"IPY_MODEL_33bd8d6e036042d68c3808f02e46b869"}}},"version_major":2,"version_minor":0}}},"nbformat":4,"nbformat_minor":4}