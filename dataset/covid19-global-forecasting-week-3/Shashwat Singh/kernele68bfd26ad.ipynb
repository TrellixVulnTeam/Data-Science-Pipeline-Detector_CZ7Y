{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from datetime import datetime\nimport time\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\nimport xgboost as xgb\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\nimport xgboost\nimport math\n\nfrom scipy.stats import pearsonr\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import tree, linear_model\nfrom xgboost.sklearn import XGBRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import explained_variance_score\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom math import sqrt","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train= pd.read_csv(\"../input/covid19-global-forecasting-week-3/train.csv\")\ntest= pd.read_csv(\"../input/covid19-global-forecasting-week-3/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(train.groupby(['Date','Lat']).count().sort_values(by='ConfirmedCases', ascending=False)['ConfirmedCases'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(train.head(5))\ndisplay(train.describe())\nprint(\"Number of Country_Region: \", train['Country_Region'].nunique())\nprint(\"Number of Country_Region: \", train['Country_Region'].unique())\nprint(\"Dates go from day\", max(train['Date']), \"to day\", min(train['Date']), \", a total of\", train['Date'].nunique(), \"days\")\nprint(\"Countries with Province/State informed: \", train[train['Province_State'].isna()==False]['Country_Region'].unique())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Countries with Province/State informed: \", train[train['Province_State'].isna()==False]['Province_State'].unique())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.groupby(['Date']).mean().sort_values(by='ConfirmedCases', ascending=False)['ConfirmedCases'][[1,2]])\nprint(train.groupby('Date').mean().sort_values(by='ConfirmedCases', ascending=False)['ConfirmedCases'][[4,5,6]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.groupby(['Date','Country_Region']).mean().sort_values(by='ConfirmedCases', ascending=False)['ConfirmedCases'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_data=train.groupby('Date')['ConfirmedCases','Fatalities'].sum().reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_data=train.groupby('Date')['ConfirmedCases','Fatalities'].sum().reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = train[[\"ConfirmedCases\", \"Fatalities\"]]\ntrain = train[[\"Province_State\",\"Country_Region\",\"Date\"]]\nX_test_Id = test.loc[:, 'ForecastId']\ntest = test[[\"Province_State\",\"Country_Region\",\"Date\"]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(train_encoded.count())\nprint(y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#xyz = train.groupby(['Country_Region']).count().sort_values(by='ConfirmedCases', ascending=False)()\n#train.groupby(['Country_Region']).sort_values(by='Date', ascending=False)[:100]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train.groupby(['Country_Region','col2']).count()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"y_train.head()"},{"metadata":{"trusted":true},"cell_type":"code","source":"#train.groupby(['Date','Country_Region'])['count','ConfirmedCases']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train = train.sort_values(by=['Date','Country_Region'], ascending=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train[train.groupby(['Date','Country_Region'])'count','ConfirmedCases')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train['count']=1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train['count'] = train.groupby(['Date','Country_Region'])['ConfirmedCases'].apply()#","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"X_xTrain = xtrain.copy()\n\nX_xTrain['State'].fillna(EMPTY_VAL, inplace=True)\nX_xTrain['State'] = X_xTrain.loc[:, ['State', 'Country']].apply(lambda x : fillState(x['State'], x['Country']), axis=1)\n\nX_xTrain.loc[:, 'Date'] = X_xTrain.Date.dt.strftime(\"%m%d\")\nX_xTrain[\"Date\"]  = X_xTrain[\"Date\"].astype(int)\n\nX_xTrain.head()\n\n#X_Test = df_test.loc[:, ['State', 'Country', 'Date']]\nX_xTest = xtest.copy()\n\nX_xTest['State'].fillna(EMPTY_VAL, inplace=True)\nX_xTest['State'] = X_xTest.loc[:, ['State', 'Country']].apply(lambda x : fillState(x['State'], x['Country']), axis=1)\n\nX_xTest.loc[:, 'Date'] = X_xTest.Date.dt.strftime(\"%m%d\")\nX_xTest[\"Date\"]  = X_xTest[\"Date\"].astype(int)\n\nX_xTest.head()\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EMPTY_VAL = \"EMPTY_VAL\"\n\ndef fillState(state, country):\n    if state == EMPTY_VAL: return country\n    return state","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nprint(\"fill blanks and add region for counting\")\n#train.fillna(' ',inplace=True)\n#train['Lat']=train['Province_State']+train['Country_Region']\n#train.drop('Province_State',axis=1,inplace=True)\n#train.drop('Country_Region',axis=1,inplace=True)\n\n\ncols = ['ConfirmedCases', 'Fatalities']\nindex_split = train.shape[0]\n\nfull_df = pd.concat([train,test],sort=False)\nfull_df.fillna(' ',inplace=True)\n#full_df['Lat']=full_df['Province_State']+full_df['Country_Region']\n#X_xTrain['State'].fillna(EMPTY_VAL, inplace=True)\nfull_df['Province_State'] = full_df.loc[:, ['Province_State', 'Country_Region']].apply(lambda x : fillState(x['Province_State'], x['Country_Region']), axis=1)\n\n\n\n#full_df.drop('Province_State',axis=1,inplace=True)\n#full_df.drop('Country_Region',axis=1,inplace=True)\ndisplay(full_df.head())\n#full_df = pd.concat([train.drop(cols, axis=1))\n#full_df.Date = full_df.Date.astype('int64')\nfull_df['Mon'] = full_df['Date'].apply(lambda x: int(x.split('-')[1]))\nfull_df['Day'] = full_df['Date'].apply(lambda x: int(x.split('-')[2]))\nfull_df['serial'] = full_df['Mon'] * 30 + full_df['Day']\nfull_df['serial'] = full_df['serial'] - full_df['serial'].min()\nfull_df.Date = pd.to_datetime(full_df.Date)\nfull_df['Date'] = full_df['Date'].apply(pd.to_datetime)\n\nfull_df['day_of_week'] = full_df['Date'].apply(lambda ts: ts.weekday()).astype('int')\nfull_df['month'] = full_df['Date'].apply(lambda ts: ts.month)\nfull_df['day'] = full_df['Date'].apply(lambda ts: ts.day)\nfull_df.loc[:, 'Date'] = full_df.Date.dt.strftime(\"%m%d\")\nfull_df['Date']  = full_df['Date'].astype(int)\n\n#full_df.drop(['Province_State','Country_Region'],axis=1, inplace= True )\nfull_df.drop(['Mon','Day'],axis=1, inplace= True )\n#full_df.drop(['Date', 'Province_State','Country_Region','Mon','Day'],axis=1, inplace= True )\ndisplay(full_df.dtypes)\ndisplay(full_df.head())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"le = LabelEncoder()\ndef CustomLabelEncoder(df):\n    for c in df.columns:\n        if df.dtypes[c] == object:\n            le.fit(df[c].astype(str))\n            df[c] = le.transform(df[c].astype(str))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfull_df_encoded = CustomLabelEncoder(full_df)\n\ntrain_encoded = full_df[:index_split]\ntest_encoded= full_df[index_split:]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_encoded.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_encoded.iloc[250:350,:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_encoded.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_encoded.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_df_encoded.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nle = LabelEncoder()\ndef CustomLabelEncoder(df):\n    for c in df.columns:\n        if df.dtypes[c] == object:\n            le.fit(df[c].astype(str))\n            df[c] = le.transform(df[c].astype(str))\n    return df\n\nfull_df_encoded = CustomLabelEncoder(full_df)\n\ntrain_encoded = full_df[:index_split]\ntest_encoded= full_df[index_split:]\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_encoded.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_encoded","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train1, X_test1, y_train1, y_test1 = train_test_split(train_encoded, y_train[['ConfirmedCases']],test_size=0.2, random_state=48)\n\nX_train2, X_test2, y_train2, y_test2 = train_test_split(train_encoded, y_train[['Fatalities']] ,test_size=0.2, random_state=48)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train1,y_test1,y_train2,y_test2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# features that will be used in the model\ny1 = y_train[['ConfirmedCases']]\ny2 = y_train[['Fatalities']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor  \nregressor = DecisionTreeRegressor(random_state = 0) \nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"regressor.fit(X_train1,y_train1)\n#predict_dt1 = regressor.predict(X_test1)\n#predict_dt1 = pd.DataFrame(predict_dt1)\n#predict_dt1.columns = [\"ConfirmedCases\"]\n#print(predict_dt1)\ndisplay(regressor.score(X_test1,y_test1))\n\npredict_dt1 = regressor.predict(X_test1)\n#predict_dt1 = pd.DataFrame(predict_dt1)\n#predict_dt1.columns = [\"ConfirmedCases\"]\npredict_dt1 = predict_dt1.astype(int)\npredict_dt1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(regressor.score(X_test1,y_test1)) \nprint(explained_variance_score(predict_dt1,y_test1)) \nprint(mean_squared_error(y_test1,predict_dt1)) \nprint(r2_score(y_test1,predict_dt1)) \nmae = mean_absolute_error(y_test1, predict_dt1) \nprint('MAE: %f' % mae) \nmse = mean_squared_error(y_test1, predict_dt1) \nrmse = sqrt(mse) \nprint('RMSE: %f' % rmse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"regressor.fit(train_encoded, y_train[['ConfirmedCases']])\n#predict_dt1 = regressor.predict(X_test1)\n#predict_dt1 = pd.DataFrame(predict_dt1)\n#predict_dt1.columns = [\"ConfirmedCases\"]\n#print(predict_dt1)\nprint(regressor.score(X_test1,y_test1))\n\npredict_dt1 = regressor.predict(test_encoded)\n#predict_dt1 = pd.DataFrame(predict_dt1)\n#predict_dt1.columns = [\"ConfirmedCases\"]\npredict_dt1 = predict_dt1.astype(int)\npredict_dt1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"print(regressor.score(X_test1,y_test1))\nprint(explained_variance_score(predict_dt1,y_test1))\nprint(mean_squared_error(y_test1,predict_dt1))\nprint(r2_score(y_test1,predict_dt1))\nmae = mean_absolute_error(y_test1, predict_dt1)\nprint('MAE: %f' % mae)\nmse = mean_squared_error(y_test1, predict_dt1)\nrmse = sqrt(mse)\nprint('RMSE: %f' % rmse)"},{"metadata":{"trusted":true},"cell_type":"code","source":"regressor.fit(train_encoded, y_train[['Fatalities']])\n#predict_dt2 = regressor.predict(X_test2)\n#predict_dt2 =predict_dt1.astype(int)\n#predict_dt2 = pd.DataFrame(predict_dt2)\n#predict_dt2.columns = [\"Fatalities\"]\npredict_dt2 = regressor.predict(test_encoded)\n#predict_dt1 = pd.DataFrame(predict_dt1)\n#predict_dt1.columns = [\"ConfirmedCases\"]\npredict_dt2 = predict_dt2.astype(int)\npredict_dt2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_test_Id,predict_dt1,predict_dt2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.DataFrame({'ForecastId':X_test_Id,'ConfirmedCases': predict_dt1, 'Fatalities': predict_dt2})\n\nsub.ForecastId = sub.ForecastId.astype('int')\nsub.head()\nsub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED = 42\nparams = {'num_leaves': 8,\n          'min_data_in_leaf': 5,  # 42,\n          'objective': 'regression',\n          'max_depth': 8,\n          'learning_rate': 0.02,\n          'boosting': 'gbdt',\n          'bagging_freq': 5,  # 5\n          'bagging_fraction': 0.8,  # 0.5,\n          'feature_fraction': 0.8201,\n          'bagging_seed': SEED,\n          'reg_alpha': 1,  # 1.728910519108444,\n          'reg_lambda': 4.9847051755586085,\n          'random_state': SEED,\n          'metric': 'mse',\n          'verbosity': 100,\n          'min_gain_to_split': 0.02,  # 0.01077313523861969,\n          'min_child_weight': 5,  # 19.428902804238373,\n          'num_threads': 6,\n          }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_train = lgb.Dataset(X_train1, y_train1) \nlgb_eval = lgb.Dataset(X_test1, y_test1, reference=lgb_train)\n\n#specify your configurations as a dict\nparams = { 'boosting_type': 'gbdt', 'objective': 'regression', 'metric': {'l2', 'l1'}, 'num_leaves': 31, 'learning_rate': 0.05, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5, 'verbose': 0 }\n\nprint('Starting training...')\n\n#train\ngbm = lgb.train(params, lgb_train, num_boost_round=20, valid_sets=lgb_eval, early_stopping_rounds=5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"lgb_train = lgb.Dataset(X_train, y_train)\nlgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n\n# specify your configurations as a dict\nparams = {\n    'boosting_type': 'gbdt',\n    'objective': 'regression',\n    'metric': {'l2', 'l1'},\n    'num_leaves': 31,\n    'learning_rate': 0.05,\n    'feature_fraction': 0.9,\n    'bagging_fraction': 0.8,\n    'bagging_freq': 5,\n    'verbose': 0\n}\n\nprint('Starting training...')\n# train\ngbm = lgb.train(params,\n                lgb_train,\n                num_boost_round=20,\n                valid_sets=lgb_eval,\n                early_stopping_rounds=5)"},{"metadata":{},"cell_type":"markdown","source":"train_data = lgb.Dataset(X_train1, y_train1)\nvalid_data = lgb.Dataset(X_test1, y_test1, reference=train_data)\nnum_round = 15000\nmodel = lgb.train(params, train_data, num_round, valid_sets=valid_data,\n                  verbose_eval=100,\n                  early_stopping_rounds=150)\n\nbest_itr = model.best_iteration"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"train_data = lgb.Dataset(X_train1, y_train1)\nvalid_data = lgb.Dataset(X_test1, y_test1, reference=train_data)\nnum_round = 15000\nmodel = lgb.train(params, train_data, num_round, valid_sets=valid_data,\n                  verbose_eval=100,\n                  early_stopping_rounds=150)\n\nbest_itr = model.best_iteration"},{"metadata":{},"cell_type":"markdown","source":"model.predict(X_test1, num_iteration=best_itr)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#model.predict(X_test1, num_iteration=best_itr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nfrom sklearn import metrics\n\ndef calc_score(y_true, y_pred):\n    #y_true[y_true<0] = 0\n    score = metrics.mean_squared_error(np.log(np.nan_to_num(y_true.clip(0, 1e10)+1)), np.log(np.nan_to_num(y_pred[:]+1)))**0.5\n    return score"},{"metadata":{},"cell_type":"markdown","source":"#model.fit(train_encoded, y_train[['ConfirmedCases']])\n#model.fit(X_test1,y_test1)\n#predict_dt1 = regressor.predict(X_test1)\n#predict_dt1 = pd.DataFrame(predict_dt1)\n#predict_dt1.columns = [\"ConfirmedCases\"]\n#print(predict_dt1)\n#model.score(X_test1,y_test1)\n\npredict_dt1 = model.predict(X_test1)\n#predict_dt1 = pd.DataFrame(predict_dt1)\n#predict_dt1.columns = [\"ConfirmedCases\"]\npredict_dt1 = predict_dt1.astype(int)\npredict_dt1"},{"metadata":{},"cell_type":"markdown","source":"train_data = lgb.Dataset(X_train2, y_train2)\nvalid_data = lgb.Dataset(X_test2, y_test2, reference=train_data)\nnum_round = 15000\nmodel = lgb.train(params, train_data, num_round, valid_sets=valid_data,\n                  verbose_eval=100,\n                  early_stopping_rounds=150)\n\nbest_itr = model.best_iteration"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#subb.ForecastId = subb.ForecastId.astype('int')\nsub.tail()\nsub.to_csv('submission.csv', index=False)"},{"metadata":{},"cell_type":"markdown","source":"#xgb1=xgb.fit(X_train1,y_train1)\npredictions = regressor.predict(test_encoded)\n#print(explained_variance_score(predictions,y_test1))\nprint(predictions)"},{"metadata":{},"cell_type":"markdown","source":"param_grid = {\"criterion\": [\"mse\", \"mae\"],\n              \"min_samples_split\": [10, 20, 40],\n              \"min_samples_leaf\": [20, 40, 100],\n              \"max_leaf_nodes\": [5, 20, 100]\n              }\n\n## Comment in order to publish in kaggle.\n\n\ngrid1 = GridSearchCV(regressor, param_grid,n_jobs=-1, cv=5, verbose=3,refit='AUC', return_train_score=True)\ngrid1=grid1.fit(X_train1, y_train1)\n\n"},{"metadata":{},"cell_type":"markdown","source":"predict_dt3 = grid1.predict(test_encoded)\npredict_dt3 = pd.DataFrame(predict_dt3)\npredict_dt3.columns = [\"Fatalities\"]\nprint(predict_dt3)"},{"metadata":{},"cell_type":"markdown","source":"#xgb1=xgb.fit(X_train1,y_train1)\npredictions = grid1.predict(test_encoded)\n#print(explained_variance_score(predictions,y_test1))\nprint(predictions)"},{"metadata":{},"cell_type":"markdown","source":"predict_dt1 = grid1.predict(X_test1)\npredict_dt1 = pd.DataFrame(predict_dt1)\npredict_dt1.columns = [\"ConfirmedCases\"]\nprint(predict_dt1)\nprint(y_test1)"},{"metadata":{},"cell_type":"markdown","source":"grid1.best_params_"},{"metadata":{},"cell_type":"markdown","source":"grid1.best_params_['criterion']"},{"metadata":{},"cell_type":"markdown","source":"grid1.score(X_test2,y_test2)"},{"metadata":{},"cell_type":"markdown","source":"grid1.best_params_"},{"metadata":{},"cell_type":"markdown","source":"dtreg = DecisionTreeRegressor(criterion = grid1.best_params_['criterion'], max_leaf_nodes=grid1.best_params_['max_leaf_nodes'], min_samples_leaf=grid1.best_params_['min_samples_leaf'],min_samples_split=grid1.best_params_['min_samples_split'])\n\ndtreg.fit(X_train1, y_train1)"},{"metadata":{},"cell_type":"markdown","source":"dtreg.score(X_test1,y_test1)"},{"metadata":{},"cell_type":"markdown","source":"predict_dt1 = dtreg.predict(X_test1)\npredict_dt1 = pd.DataFrame(predict_dt1)\npredict_dt1.columns = [\"ConfirmedCases\"]\nprint(predict_dt1)"},{"metadata":{},"cell_type":"markdown","source":"print(explained_variance_score(predict_dt1,y_test1))\nprint(r2_score(y_test1,predict_dt1))\nmean_squared_error(y_test2,predict_dt1)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb = xgboost.XGBRegressor(n_estimators=1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb1=xgb.fit(X_train1,y_train1)\npredictions = xgb1.predict(X_test1)\nprint(explained_variance_score(predictions,y_test1))\n\npredict_xgb1 = xgb1.predict(test_encoded)\n#predict_xg2 = pd.DataFrame(predict_dt2)\n#predict_xg2.columns = [\"Fatalities\"]\nprint(xgb1.score(X_test1,y_test1))\nprint(explained_variance_score(predictions,y_test1))\nprint(mean_squared_error(y_test1,predictions))\nprint(r2_score(y_test1,predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#xgb1=xgb.fit(X_train1,y_train1)\npredictions_xg = xgb1.predict(test_encoded)\n#print(explained_variance_score(predictions,y_test1))\nprint(predictions_xg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(explained_variance_score(predictions_xg,predict_dt1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#xgb1=xgb.fit(X_train1,y_train1)\npredictions = xgb1.predict(test_encoded)\n#print(explained_variance_score(predictions,y_test1))\nprint(predictions)"},{"metadata":{},"cell_type":"markdown","source":"from sklearn.metrics import mean_squared_error, r2_score\nr2_score(y_test1,predictions)\nprint(explained_variance_score(predictions,y_test1))\nprint(r2_score(y_test1,predictions))\nmean_squared_error(y_test1,predictions)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb1.score(X_test1,y_test1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb2=xgb.fit(X_train2,y_train2)\npredictions = xgb2.predict(X_test2)\nprint(explained_variance_score(predictions,y_test2))\n\npredict_xgb2 = xgb2.predict(test_encoded)\n#predict_xg2 = pd.DataFrame(predict_dt2)\n#predict_xg2.columns = [\"Fatalities\"]\nprint(xgb2.score(X_test2,y_test2))\nprint(explained_variance_score(predictions,y_test2))\nprint(mean_squared_error(y_test2,predictions))\nprint(r2_score(y_test2,predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"params = {'min_child_weight':[1,4,5,8,10], 'gamma':[i/10.0 for i in range(1,10)],  'subsample':[i/10.0 for i in range(1,11)],\n'colsample_bytree':[i/10.0 for i in range(1,11)], 'max_depth': [1,2,4,5,8,10]}\n\n#scoring = {'AUC': 'roc_auc', 'Accuracy': make_scorer(accuracy_score)}\n\n# Initialize XGB and GridSearch\nxgb = XGBRegressor(nthread=-1,verbosity=3,n_estimators=100)\n\n#grid = GridSearchCV(xgb, params,n_jobs=-1, cv=10, verbose=3,refit='AUC', return_train_score=True)\n\ngrid = RandomizedSearchCV(xgb, \n                         param_distributions = params,\n                         cv = 10,  \n                         n_iter = 5, # you want 5 here not 25 if I understand you correctly \n                         error_score = 0, \n                         verbose = 3,\n                         n_jobs=-1)\n\n\n\ngrid.fit(X_train1, y_train1)\n\n\n"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"grid.best_params_"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"xgb3 = XGBRegressor(colsample_bytree = grid.best_params_['colsample_bytree'], min_child_weight = grid.best_params_['min_child_weight'], gamma=grid.best_params_['gamma'], subsample=grid.best_params_['subsample'],max_depth=grid.best_params_['max_depth'],nthread=-1)\n\n#xgb3.fit(X_train1, y_train1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb1=xgb.fit(X_train1,y_train1)\npredictions = xgb1.predict(X_test1)\nprint(explained_variance_score(predictions,y_test1))\n\npredict_xgb2 = xgb1.predict(test_encoded)\n#predict_xg2 = pd.DataFrame(predict_dt2)\n#predict_xg2.columns = [\"Fatalities\"]\nprint(xgb1.score(X_test1,y_test1))\nprint(explained_variance_score(predictions,y_test1))\nprint(mean_squared_error(y_test1,predictions))\nprint(r2_score(y_test1,predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#predict_dt1 = regressor.predict(X_test1)\n#predict_dt1 = pd.DataFrame(predict_dt1)\n#predict_dt1.columns = [\"ConfirmedCases\"]\n#print(predict_dt1)\ndisplay(xgb.score(X_test1,y_test1))\n\npredict_dt1 = xgb.predict(test_encoded)\n#predict_dt1 = pd.DataFrame(predict_dt1)\n#predict_dt1.columns = [\"ConfirmedCases\"]\npredict_dt1 = predict_dt1.astype(int)\npredict_dt1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb.fit(train_encoded, y_train[['ConfirmedCases']])\n#predict_dt1 = regressor.predict(X_test1)\n#predict_dt1 = pd.DataFrame(predict_dt1)\n#predict_dt1.columns = [\"ConfirmedCases\"]\n#print(predict_dt1)\nprint(xgb.score(X_test1,y_test1))\n\npredict_dt1 = xgb.predict(test_encoded)\n#predict_dt1 = pd.DataFrame(predict_dt1)\n#predict_dt1.columns = [\"ConfirmedCases\"]\npredict_dt1 = predict_dt1.astype(int)\npredict_dt1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"params = {'min_child_weight':[1,4,5,8,10], 'gamma':[i/10.0 for i in range(1,10)],  'subsample':[i/10.0 for i in range(1,11)],\n'colsample_bytree':[i/10.0 for i in range(1,11)], 'max_depth': [1,2,4,5,8,10]}\n\n#scoring = {'AUC': 'roc_auc', 'Accuracy': make_scorer(accuracy_score)}\n\n# Initialize XGB and GridSearch\nxgb = XGBRegressor(nthread=-1,verbosity=3,n_estimators=100)\n\n#grid = GridSearchCV(xgb, params,n_jobs=-1, cv=10, verbose=3,refit='AUC', return_train_score=True)\n\ngrid = RandomizedSearchCV(xgb, \n                         param_distributions = params,\n                         cv = 10,  \n                         n_iter = 5, # you want 5 here not 25 if I understand you correctly \n                         error_score = 0, \n                         verbose = 3,\n                         n_jobs=-1)\n\n\n\ngrid.fit(X_train2, y_train2)\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"params = {'min_child_weight':[1,4,5,8,10], 'gamma':[i/10.0 for i in range(1,10)], 'subsample':[i/10.0 for i in range(1,11)], 'colsample_bytree':[i/10.0 for i in range(1,11)], 'max_depth': [1,2,4,5,8,10]}\n\n#scoring = {'AUC': 'roc_auc', 'Accuracy': make_scorer(accuracy_score)}\n\nInitialize XGB and GridSearch\nxgb = XGBRegressor(nthread=-1,verbosity=3,n_estimators=1000)\n\n#grid = GridSearchCV(xgb, params,n_jobs=-1, cv=10, verbose=3,refit='AUC', return_train_score=True)\n\ngrid = RandomizedSearchCV(xgb, param_distributions = params, cv = 10,\nn_iter = 10, # you want 5 here not 25 if I understand you correctly error_score = 0, verbose = 3, n_jobs=-1)\n\ngrid.fit(X_train2, y_train2)"},{"metadata":{},"cell_type":"markdown","source":"grid.best_params_"},{"metadata":{},"cell_type":"markdown","source":"xgb3 = XGBRegressor(colsample_bytree = grid.best_params_['colsample_bytree'], min_child_weight = grid.best_params_['min_child_weight'], gamma=grid.best_params_['gamma'], subsample=grid.best_params_['subsample'],max_depth=grid.best_params_['max_depth'],nthread=-1)\n\nxgb3.fit(X_train1, y_train1)"},{"metadata":{},"cell_type":"markdown","source":"print(xgb3.score(X_test1,y_test1))"},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb1=xgb.fit(X_train2,y_train2)\npredictions = xgb1.predict(X_test2)\nprint(explained_variance_score(predictions,y_test2))\n\npredict_xgb2 = xgb1.predict(test_encoded)\n#predict_xg2 = pd.DataFrame(predict_dt2)\n#predict_xg2.columns = [\"Fatalities\"]\nprint(xgb1.score(X_test2,y_test2))\nprint(explained_variance_score(predictions,y_test2))\nprint(mean_squared_error(y_test2,predictions))\nprint(r2_score(y_test2,predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#predict_dt1 = regressor.predict(X_test1)\n#predict_dt1 = pd.DataFrame(predict_dt1)\n#predict_dt1.columns = [\"ConfirmedCases\"]\n#print(predict_dt1)\ndisplay(xgb.score(X_test2,y_test2))\n\npredict_dt2 = xgb.predict(test_encoded)\n#predict_dt1 = pd.DataFrame(predict_dt1)\n#predict_dt1.columns = [\"ConfirmedCases\"]\npredict_dt2 = predict_dt1.astype(int)\npredict_dt2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb.fit(train_encoded, y_train[['Fatalities']])\n#predict_dt2 = regressor.predict(X_test2)\n#predict_dt2 =predict_dt1.astype(int)\n#predict_dt2 = pd.DataFrame(predict_dt2)\n#predict_dt2.columns = [\"Fatalities\"]\npredict_dt2 = xgb.predict(test_encoded)\n#predict_dt1 = pd.DataFrame(predict_dt1)\n#predict_dt1.columns = [\"ConfirmedCases\"]\npredict_dt2 = predict_dt2.astype(int)\npredict_dt2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"sub = pd.DataFrame({'ForecastId':X_test_Id,'ConfirmedCases': predict_dt1, 'Fatalities': predict_dt2})\n\nsub.ForecastId = sub.ForecastId.astype('int')\nsub.head()\nsub.to_csv('submission.csv', index=False)"},{"metadata":{},"cell_type":"markdown","source":"rf = RandomForestRegressor()\nrf1 = rf.fit(X_train1,y_train1)\n\npredictions = rf1.predict(X_test1)\nprint(explained_variance_score(predictions,y_test1))\n\npredict_rf1 = rf1.predict(test_encoded)\n#predict_xg2 = pd.DataFrame(predict_dt2)\n#predict_xg2.columns = [\"Fatalities\"]\nprint(rf1.score(X_test1,y_test1))\nprint(explained_variance_score(predictions,y_test1))\nprint(mean_squared_error(y_test1,predictions))\nprint(r2_score(y_test1,predictions))\n"},{"metadata":{},"cell_type":"markdown","source":"rf_params = {'max_features':  [1, 2, 3], 'min_samples_leaf': [5, 10, 15, 20], 'max_depth': [8, 10, 20, 30]}\nrf = RandomForestRegressor(n_estimators=100, random_state=17, n_jobs= -1)\ngcv = GridSearchCV(rf, rf_params, n_jobs=-1, cv=5, verbose=1)\ngcv.fit(X_train1,y_train1)"},{"metadata":{},"cell_type":"markdown","source":"rf = RandomForestRegressor(max_depth = gcv.best_params_['max_depth'], max_features = gcv.best_params_['max_features'], min_samples_leaf=gcv.best_params_['min_samples_leaf'], random_state=17, n_estimators=100, n_jobs= -1)\n\nrf.fit(train_encoded,y1)"},{"metadata":{},"cell_type":"markdown","source":"rf.score(X_test1,y_test1)"},{"metadata":{},"cell_type":"markdown","source":"predictions = rf.predict(X_test1)\nprint(explained_variance_score(predictions,y_test1))\n"},{"metadata":{},"cell_type":"markdown","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\n\nregr = linear_model.LinearRegression()\nregr1 = regr.fit(X_train1, y_train1)\nprint(regr.predict(X_test1))\n\npredictions = regr1.predict(X_test1)\nprint(explained_variance_score(predictions,y_test1))\n\npredict_rf1 = regr1.predict(test_encoded)\n#predict_xg2 = pd.DataFrame(predict_dt2)\n#predict_xg2.columns = [\"Fatalities\"]\nprint(regr1.score(X_test1,y_test1))\nprint(explained_variance_score(predictions,y_test1))\nprint(mean_squared_error(y_test1,predictions))\nprint(r2_score(y_test1,predictions))\n"},{"metadata":{},"cell_type":"markdown","source":"print(\"RMSE: %.2f\"\n      % math.sqrt(np.mean((regr.predict(X_test1) - y_test1) ** 2)))\n"},{"metadata":{},"cell_type":"markdown","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\npipeline = Pipeline([('scaler', StandardScaler()), ('lr', LinearRegression())])\npipeline.fit(X_train1, y_train1)\npredicted = pipeline.predict(X_test1)"},{"metadata":{},"cell_type":"markdown","source":"from sklearn.neighbors import KNeighborsClassifier\n# Create KNN classifier\nknn = KNeighborsClassifier(n_neighbors = 3)\n# Fit the classifier to the data\nknn1 = knn.fit(X_train1,y_train1.values.ravel())\n\npredictions = knn1.predict(X_test1)\nprint(explained_variance_score(y_test1,predictions))\npredict_knn1 = knn1.predict(test_encoded)\n#predict_xg2 = pd.DataFrame(predict_dt2)\n#predict_xg2.columns = [\"Fatalities\"]\nprint(knn1.score(X_test1,y_test1))\nprint(explained_variance_score(y_test1,predictions))\nprint(mean_squared_error(y_test1,predictions))\nprint(r2_score(predictions,y_test1))"},{"metadata":{},"cell_type":"markdown","source":"from sklearn.neighbors import KNeighborsClassifier\n# Create KNN classifier\nknn = KNeighborsClassifier(n_neighbors = 3)\n# Fit the classifier to the data\nknn1 = knn.fit(X_train2,y_train2.values.ravel())\n\npredictions = knn1.predict(X_test2)\nprint(explained_variance_score(y_test2,predictions))\npredict_knn1 = knn1.predict(test_encoded)\n#predict_xg2 = pd.DataFrame(predict_dt2)\n#predict_xg2.columns = [\"Fatalities\"]\nprint(knn1.score(X_test2,y_test2))\nprint(explained_variance_score(y_test2,predictions))\nprint(mean_squared_error(y_test2,predictions))\nprint(r2_score(predictions,y_test2))"},{"metadata":{},"cell_type":"markdown","source":"knn.score(X_test2, y_test2)"},{"metadata":{},"cell_type":"markdown","source":"#xgb1=xgb.fit(X_train1,y_train1)\npredictions = knn1.predict(test_encoded)\npredictions1 = knn1.predict(X_test1)\nprint(explained_variance_score(predictions1,y_test1))\nprint(predictions1)"},{"metadata":{},"cell_type":"markdown","source":"predictions"},{"metadata":{},"cell_type":"markdown","source":"knn.fit(X_train1,y_train1)\n#predict_dt1 = regressor.predict(X_test1)\n#predict_dt1 = pd.DataFrame(predict_dt1)\n#predict_dt1.columns = [\"ConfirmedCases\"]\n#print(predict_dt1)\nknn.score(X_test1,y_test1)\n\npredict_dt1 = knn.predict(X_test1)\n#predict_dt1 = pd.DataFrame(predict_dt1)\n#predict_dt1.columns = [\"ConfirmedCases\"]\npredict_dt1 = predict_dt1.astype(int)\npredict_dt1"},{"metadata":{},"cell_type":"markdown","source":"knn.fit(train_encoded, y_train[['ConfirmedCases']])\n#predict_dt1 = regressor.predict(X_test1)\n#predict_dt1 = pd.DataFrame(predict_dt1)\n#predict_dt1.columns = [\"ConfirmedCases\"]\n#print(predict_dt1)\nknn.score(X_test1,y_test1)\n\npredict_dt1 = knn.predict(test_encoded)\n#predict_dt1 = pd.DataFrame(predict_dt1)\n#predict_dt1.columns = [\"ConfirmedCases\"]\npredict_dt1 = predict_dt1.astype(int)\npredict_dt1"},{"metadata":{},"cell_type":"markdown","source":"knn.fit(train_encoded, y_train[['Fatalities']])\n#predict_dt2 = regressor.predict(X_test2)\n#predict_dt2 =predict_dt1.astype(int)\n#predict_dt2 = pd.DataFrame(predict_dt2)\n#predict_dt2.columns = [\"Fatalities\"]\npredict_dt2 = knn.predict(test_encoded)\n#predict_dt1 = pd.DataFrame(predict_dt1)\n#predict_dt1.columns = [\"ConfirmedCases\"]\npredict_dt2 = predict_dt2.astype(int)\npredict_dt2"},{"metadata":{},"cell_type":"markdown","source":"print(X_test_Id,predict_dt1,predict_dt2)"},{"metadata":{},"cell_type":"markdown","source":"sub = pd.DataFrame({'ForecastId':X_test_Id,'ConfirmedCases': predict_dt1, 'Fatalities': predict_dt2})\n\nsub.ForecastId = sub.ForecastId.astype('int')\nsub.head()\nsub.to_csv('submission.csv', index=False)"},{"metadata":{},"cell_type":"markdown","source":"pd.DataFrame(predictions)"},{"metadata":{},"cell_type":"markdown","source":"from sklearn.model_selection import GridSearchCV\n#create new a knn model\nknn2 = KNeighborsClassifier()\n#create a dictionary of all values we want to test for n_neighbors\nparam_grid = {'n_neighbors': np.arange(1, 25)}\n#use gridsearch to test all values for n_neighbors\nknn_gscv = GridSearchCV(knn2, param_grid, cv=10,n_jobs=-1, verbose=3)\n#fit model to data\nknn_gscv.fit(X_train1, y_train1.values.ravel())\n"},{"metadata":{},"cell_type":"markdown","source":"#check top performing n_neighbors value\nknn_gscv.best_params_\n"},{"metadata":{},"cell_type":"markdown","source":"#check mean score for the top performing value of n_neighbors\nknn_gscv.best_score_"},{"metadata":{},"cell_type":"markdown","source":"#xgb1=xgb.fit(X_train1,y_train1)\npredictions = knn_gscv.predict(test_encoded)\npredictions1 = knn_gscv.predict(X_test2)\nprint(explained_variance_score(predictions1,y_test2))\nprint(predictions)"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":4}