{"cells":[{"metadata":{},"cell_type":"markdown","source":"> # Model 1 - SEIR"},{"metadata":{"ExecuteTime":{"end_time":"2020-03-23T05:24:24.576517Z","start_time":"2020-03-23T05:24:23.891697Z"},"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nimport os\nfrom tqdm.notebook import tqdm\nfrom scipy.integrate import solve_ivp\nimport numpy\nimport datetime\nfrom datetime import timedelta\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## SEIR Model function\n* Function From [SEIR Great APP](http://gabgoh.github.io/COVID/index.html)\n![image.png](https://upload.wikimedia.org/wikipedia/commons/3/3d/SEIR.PNG)\n* S ==> Susceptible : number of susceptible\n* E ==> Expose : number of expose\n* I ==> Infectious : number of infectious\n* R ==> Recovered or Removed : number recovered (or immune) individuals. \n* We have S + E + I + R = N, this is only constant because of the (degenerate) assumption that birth and death rates are equal, N is country population.\n\nSusceptible → Exposed → Infected → Removed, Differential Function as below (from [SEIR Great APP](http://gabgoh.github.io/COVID/index.html)): \n![image.png](attachment:image.png)\nWe need to solve the Differential equation to find the S,E,I,R, but what is **\"R_t\"**, **\"T_inf\"**, **\"T_inc\"** and how can we define those variable?\n* R_0 & R_t ==> [Reproduction number](https://en.wikipedia.org/wiki/Basic_reproduction_number), The definition describes the state where no other individuals are infected or immunized (naturally or through vaccination)\n* T_inf ==> Average duration of the infection, 1/T_inf can be treat as individual experiences one recovery in D units of time.\n* T_inc ==> Average incubation period, Many paper and article define as 5.1 ([reference](https://www.ncbi.nlm.nih.gov/pubmed/32150748), [reference2](https://www.worldometers.info/coronavirus/coronavirus-incubation-period/))\n\n### Assume there are some intervention will cause reproduction number (R_0) reduce (such as bed nets and vaccines,government, isolation ....), have an effectiveness which decays over time ","attachments":{"image.png":{"image/png":"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"}}},{"metadata":{"ExecuteTime":{"end_time":"2020-03-23T05:24:24.587962Z","start_time":"2020-03-23T05:24:24.577878Z"},"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Function code refernece from https://www.kaggle.com/anjum48/seir-model-with-intervention\n\n# Susceptible equation\ndef dS_dt(S, I, R_t, T_inf):\n    return -(R_t / T_inf) * I * S\n\n# Exposed equation\ndef dE_dt(S, E, I, R_t, T_inf, T_inc):\n    return (R_t / T_inf) * I * S - (T_inc**-1) * E\n\n# Infected equation\ndef dI_dt(I, E, T_inc, T_inf):\n    return (T_inc**-1) * E - (T_inf**-1) * I\n\n# Recovered/Remove/deceased equation\ndef dR_dt(I, T_inf):\n    return (T_inf**-1) * I\n\ndef SEIR_model(t, y, R_t, T_inf, T_inc):\n    \n    if callable(R_t):\n        reproduction = R_t(t)\n    else:\n        reproduction = R_t\n        \n    S, E, I, R = y\n    \n    S_out = dS_dt(S, I, reproduction, T_inf)\n    E_out = dE_dt(S, E, I, reproduction, T_inf, T_inc)\n    I_out = dI_dt(I, E, T_inc, T_inf)\n    R_out = dR_dt(I, T_inf)\n    \n    return [S_out, E_out, I_out, R_out]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Load dataset (Global ComfirmedCase of each country)"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/covid19-global-forecasting-week-3/train.csv')\ntest = pd.read_csv('../input/covid19-global-forecasting-week-3/test.csv')\ntrain['Date_datetime'] = train['Date'].apply(lambda x: (datetime.datetime.strptime(x, '%Y-%m-%d')))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Load populations of each country"},{"metadata":{"trusted":true},"cell_type":"code","source":"pop_info = pd.read_csv('/kaggle/input/covid19-population-data/population_data.csv')\ncountry_pop = pop_info.query('Type == \"Country/Region\"')\nprovince_pop = pop_info.query('Type == \"Province/State\"')\ncountry_lookup = dict(zip(country_pop['Name'], country_pop['Population']))\nprovince_lookup = dict(zip(province_pop['Name'], province_pop['Population']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plot SEIR model and predict"},{"metadata":{"ExecuteTime":{"end_time":"2020-03-23T05:24:24.619254Z","start_time":"2020-03-23T05:24:24.612497Z"},"trusted":true},"cell_type":"code","source":"def plot_model_and_predict(data, pop, solution, title='SEIR model'):\n    sus, exp, inf, rec = solution.y\n    \n    f = plt.figure(figsize=(16,5))\n    ax = f.add_subplot(1,2,1)\n    #ax.plot(sus, 'b', label='Susceptible');\n    ax.plot(exp, 'y', label='Exposed');\n    ax.plot(inf, 'r', label='Infected');\n    ax.plot(rec, 'c', label='Recovered/deceased');\n    plt.title(title)\n    plt.xlabel(\"Days\", fontsize=10);\n    plt.ylabel(\"Fraction of population\", fontsize=10);\n    plt.legend(loc='best');\n    \n    ax2 = f.add_subplot(1,2,2)\n    preds = np.clip((inf + rec) * pop ,0,np.inf)\n    ax2.plot(range(len(data)),preds[:len(data)],label = 'Predict ConfirmedCases')\n    ax2.plot(range(len(data)),data['ConfirmedCases'])\n    plt.title('Model predict and data')\n    plt.ylabel(\"Population\", fontsize=10);\n    plt.xlabel(\"Days\", fontsize=10);\n    plt.legend(loc='best');","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-03-23T05:24:27.519392Z","start_time":"2020-03-23T05:24:26.943489Z"},"trusted":true},"cell_type":"code","source":"from scipy.optimize import minimize\nfrom sklearn.metrics import mean_squared_log_error, mean_squared_error","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fit the SEIR model to real data\nFind the best variables of SEIR model to fit the real data\n* T_inf ==> Using average value 2.9 \n* T_inc ==> Using average value 5.2\n* **R_t** ==> find the best reproduction number by fitting the real data (if have decay function, find the paramater of decay function)\n* **cfr** ==> find the best Case fatality rate, this parater is for predict Fatalities"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define all variable of SEIR model \nT_inc = 5.2  # average incubation period\nT_inf = 2.9  # average infectious period\n\n# Define the intervention parameters (fit result, latter will show how to fit)\nR_0, cfr, k, L=[ 3.95469597 , 0.04593316 , 3.      ,   15.32328881]\n\ndef time_varying_reproduction(t): \n    return R_0 / (1 + (t/L)**k)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cumsum_signal(vec):\n    temp_val = 0\n    vec_new = []\n    for i in vec:\n        if i > temp_val:\n            vec_new.append(i)\n            temp_val = i\n        else:\n            vec_new.append(temp_val)\n    return vec_new","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-03-23T05:24:27.575293Z","start_time":"2020-03-23T05:24:27.530166Z"},"trusted":true},"cell_type":"code","source":"# Use a constant reproduction number\ndef eval_model_const(params, data, population, return_solution=False, forecast_days=0):\n    R_0, cfr = params # Paramaters, R0 and cfr \n    N = population # Population of each country\n    n_infected = data['ConfirmedCases'].iloc[0] # start from first comfirmedcase on dataset first date\n    max_days = len(data) + forecast_days # How many days want to predict\n    s, e, i, r = (N - n_infected)/ N, 0, n_infected / N, 0 #Initial stat for SEIR model\n    \n    # R0 become half after intervention days\n    def time_varying_reproduction(t):\n        if t > 80: # we set intervention days = 80\n            return R_0 * 0.5\n        else:\n            return R_0\n    \n    # Solve the SEIR differential equation.\n    sol = solve_ivp(SEIR_model, [0, max_days], [s, e, i, r], args=(time_varying_reproduction, T_inf, T_inc),\n                    t_eval=np.arange(0, max_days))\n    \n    sus, exp, inf, rec = sol.y\n    # Predict confirmedcase\n    y_pred_cases = np.clip((inf + rec) * N ,0,np.inf)\n    y_true_cases = data['ConfirmedCases'].values\n    \n    # Predict Fatalities by remove * fatality rate(cfr)\n    y_pred_fat = np.clip(rec*N* cfr, 0, np.inf)\n    y_true_fat = data['Fatalities'].values\n    \n    optim_days = min(20, len(data))  # Days to optimise for\n    weights = 1 / np.arange(1, optim_days+1)[::-1]  # Recent data is more heavily weighted\n    \n    # using mean squre log error to evaluate\n    msle_cases = mean_squared_log_error(y_true_cases[-optim_days:], y_pred_cases[-optim_days:], weights)\n    msle_fat = mean_squared_log_error(y_true_fat[-optim_days:], y_pred_fat[-optim_days:], weights)\n    msle_final = np.mean([msle_cases, msle_fat])\n    \n    if return_solution:\n        return msle_final, sol\n    else:\n        return msle_final","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Intervention by Hill function for SEIR model\n* https://github.com/SwissTPH/openmalaria/wiki/ModelDecayFunctions"},{"metadata":{"ExecuteTime":{"end_time":"2020-03-23T05:24:27.608292Z","start_time":"2020-03-23T05:24:27.578669Z"},"trusted":true},"cell_type":"code","source":"# Use a Hill decayed reproduction number\ndef eval_model_decay(params, data, population, return_solution=False, forecast_days=0):\n    R_0, cfr, k, L = params # Paramaters, R0 and cfr \n    N = population # Population of each country\n    n_infected = data['ConfirmedCases'].iloc[0] # start from first comfirmedcase on dataset first date\n    max_days = len(data) + forecast_days # How many days want to predict\n    s, e, i, r = (N - n_infected)/ N, 0, n_infected / N, 0 #Initial stat for SEIR model\n    \n    # https://github.com/SwissTPH/openmalaria/wiki/ModelDecayFunctions   \n    # Hill decay. Initial values: R_0=2.2, k=2, L=50\n    def time_varying_reproduction(t): \n        return R_0 / (1 + (t/L)**k)\n    \n    # Solve the SEIR differential equation.\n    sol = solve_ivp(SEIR_model, [0, max_days], [s, e, i, r], args=(time_varying_reproduction, T_inf, T_inc),\n                    t_eval=np.arange(0, max_days))\n    \n    sus, exp, inf, rec = sol.y\n    # Predict confirmedcase\n    y_pred_cases = np.clip((inf + rec) * N ,0,np.inf)\n    y_true_cases = data['ConfirmedCases'].values\n    \n    # Predict Fatalities by remove * fatality rate(cfr)\n    y_pred_fat = np.clip(rec*N* cfr, 0, np.inf)\n    y_true_fat = data['Fatalities'].values\n    \n    optim_days = min(20, len(data))  # Days to optimise for\n    weights = 1 / np.arange(1, optim_days+1)[::-1]  # Recent data is more heavily weighted\n    \n    # using mean squre log error to evaluate\n    msle_cases = mean_squared_log_error(y_true_cases[-optim_days:], y_pred_cases[-optim_days:], weights)\n    msle_fat = mean_squared_log_error(y_true_fat[-optim_days:], y_pred_fat[-optim_days:], weights)\n    msle_final = np.mean([msle_cases, msle_fat])\n    \n    if return_solution:\n        return msle_final, sol\n    else:\n        return msle_final","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train[-7:]),len(train[:-7]),len(train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Function of Fit the SEIR model to real data\n* Auto choose the best decay function of R_t (intervention days decay or Hill decay)\n* Total case/country population is below 1, reduce country population\n* If datset still no case, return 0 \n* Plot the fit result and forecast trends (Infect smooth decrease by what date)\n* Function being hide, there are describe in code."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from matplotlib import dates\nimport plotly.graph_objects as go\n\ndef fit_model_new(data, area_name, initial_guess=[2.2, 0.02, 2, 50], \n              bounds=((1, 20), (0, 0.15), (1, 3), (1, 100)), make_plot=True, decay_mode = None):\n    \n    if area_name in ['France']:# France last data looks weird, remove it\n        train = data.query('ConfirmedCases > 0').copy()[:-1]\n    else:\n        train = data.query('ConfirmedCases > 0').copy()\n    \n    ####### Split Train & Valid #######\n    valid_data = train[-7:].copy()\n    train_data = train[:-7].copy()\n    \n    ####### If this country have no ConfirmedCase, return 0 #######\n    if len(train_data) == 0:\n        result_zero = np.zeros((43))\n        return pd.DataFrame({'ConfirmedCases':result_zero,'Fatalities':result_zero}), 0 \n    \n    ####### Load the population of area #######\n    try:\n        #population = province_lookup[area_name]\n        population = pop_info[pop_info['Name']==area_name]['Population'].tolist()[0]\n    except IndexError:\n        print ('country not in population set, '+str(area_name))\n        population = 1000000 \n    \n    \n    if area_name == 'US':\n        population = 327200000\n        \n    if area_name == 'Global':\n        population = 7744240900\n        \n    cases_per_million = train_data['ConfirmedCases'].max() * 10**6 / population\n    n_infected = train_data['ConfirmedCases'].iloc[0]\n    \n    ####### Total case/popuplation below 1, reduce country population #######\n    if cases_per_million < 1:\n        #print ('reduce pop divide by 100')\n        population = population/100\n        \n    ####### Fit the real data by minimize the MSLE #######\n    res_const = minimize(eval_model_const, [2.2, 0.02], bounds=((1, 20), (0, 0.15)),\n                         args=(train_data, population, False),\n                         method='L-BFGS-B')\n\n    res_decay = minimize(eval_model_decay, initial_guess, bounds=bounds,\n                         args=(train_data, population, False),\n                         method='L-BFGS-B')\n    \n    ####### Align the date information #######\n    test_end = datetime.datetime.strptime('2020-05-07','%Y-%m-%d')\n    test_start = datetime.datetime.strptime('2020-03-26','%Y-%m-%d')\n    test_period = (test_end - test_start).days\n    train_max = train_data.Date_datetime.max()\n    train_all_max = train.Date_datetime.max()\n    train_min = train_data.Date_datetime.min()\n    add_date = 0\n    delta_days =(test_end - train_max).days\n    train_add_time=[]\n\n    if train_min > test_start:\n        add_date = (train_min-test_start).days\n        last = train_min-timedelta(add_date)\n        train_add_time = np.arange(last, train_min, dtype='datetime64[D]').tolist()\n        train_add_time = pd.to_datetime(train_add_time)\n        dates_all = train_add_time.append(pd.to_datetime(np.arange(train_min, test_end+timedelta(1), dtype='datetime64[D]')))\n    else:\n        dates_all = pd.to_datetime(np.arange(train_min, test_end+timedelta(1), dtype='datetime64[D]'))\n\n\n    ####### Auto find the best decay function ####### \n    if decay_mode is None:\n        if res_const.fun < res_decay.fun :\n            msle, sol = eval_model_const(res_const.x, train_data, population, True, delta_days+add_date)\n            res = res_const\n\n        else:\n            msle, sol = eval_model_decay(res_decay.x, train_data, population, True, delta_days+add_date)\n            res = res_decay\n            R_0, cfr, k, L = res.x\n    else:\n        if decay_mode =='day_decay':\n            msle, sol = eval_model_const(res_const.x, train_data, population, True, delta_days+add_date)\n            res = res_const\n        else:\n            msle, sol = eval_model_decay(res_decay.x, train_data, population, True, delta_days+add_date)\n            res = res_decay\n            R_0, cfr, k, L = res.x\n\n    ####### Predict the result by using best fit paramater of SEIR model ####### \n    sus, exp, inf, rec = sol.y\n    \n    y_pred = pd.DataFrame({\n        'ConfirmedCases': cumsum_signal(np.diff((inf + rec) * population, prepend=n_infected).cumsum()),\n       # 'ConfirmedCases': [inf[0]*population for i in range(add_date)]+(np.clip((inf + rec) * population,0,np.inf)).tolist(),\n       # 'Fatalities': [rec[0]*population for i in range(add_date)]+(np.clip(rec, 0, np.inf) * population * res.x[1]).tolist()\n        'Fatalities': cumsum_signal((np.clip(rec * population * res.x[1], 0, np.inf)).tolist())\n    })\n\n    y_pred_valid = y_pred.iloc[len(train_data):len(train_data)+len(valid_data)]\n    #y_pred_valid = y_pred.iloc[:len(train_data)]\n    y_pred_test = y_pred.iloc[-(test_period+1):]\n    #y_true_valid = train_data[['ConfirmedCases', 'Fatalities']]\n    y_true_valid = valid_data[['ConfirmedCases', 'Fatalities']]\n    #print (len(y_pred),train_min)\n    #print (y_true_valid['ConfirmedCases'])\n    #print (y_pred_valid['ConfirmedCases'])\n    ####### Calculate MSLE ####### \n    valid_msle_cases = mean_squared_log_error(y_true_valid['ConfirmedCases'], y_pred_valid['ConfirmedCases'])\n    valid_msle_fat = mean_squared_log_error(y_true_valid['Fatalities'], y_pred_valid['Fatalities'])\n    valid_msle = np.mean([valid_msle_cases, valid_msle_fat])\n    \n    ####### Plot the fit result of train data and forecast after 300 days ####### \n    if make_plot:\n        if len(res.x)<=2:\n            print(f'Validation MSLE: {valid_msle:0.5f}, using intervention days decay, Reproduction number(R0) : {res.x[0]:0.5f}, Fatal rate : {res.x[1]:0.5f}')\n        else:\n            print(f'Validation MSLE: {valid_msle:0.5f}, using Hill decay, Reproduction number(R0) : {res.x[0]:0.5f}, Fatal rate : {res.x[1]:0.5f}, K : {res.x[2]:0.5f}, L: {res.x[3]:0.5f}')\n        \n        ####### Plot the fit result of train data dna SEIR model trends #######\n\n        f = plt.figure(figsize=(16,5))\n        ax = f.add_subplot(1,2,1)\n        ax.plot(exp, 'y', label='Exposed');\n        ax.plot(inf, 'r', label='Infected');\n        ax.plot(rec, 'c', label='Recovered/deceased');\n        plt.title('SEIR Model Trends')\n        plt.xlabel(\"Days\", fontsize=10);\n        plt.ylabel(\"Fraction of population\", fontsize=10);\n        plt.legend(loc='best');\n        #train_date_remove_year = train_data['Date_datetime'].apply(lambda date:'{:%m-%d}'.format(date))\n        ax2 = f.add_subplot(1,2,2)\n        xaxis = train_data['Date_datetime'].tolist()\n        xaxis = dates.date2num(xaxis)\n        hfmt = dates.DateFormatter('%m\\n%d')\n        ax2.xaxis.set_major_formatter(hfmt)\n        ax2.plot(np.array(train_data['Date_datetime'], dtype='datetime64[D]'),train_data['ConfirmedCases'],label='Confirmed Cases (train)', c='g')\n        ax2.plot(np.array(train_data['Date_datetime'], dtype='datetime64[D]'), y_pred['ConfirmedCases'][:len(train_data)],label='Cumulative modeled infections', c='r')\n        ax2.plot(np.array(valid_data['Date_datetime'], dtype='datetime64[D]'), y_true_valid['ConfirmedCases'],label='Confirmed Cases (valid)', c='b')\n        ax2.plot(np.array(valid_data['Date_datetime'], dtype='datetime64[D]'),y_pred_valid['ConfirmedCases'],label='Cumulative modeled infections (valid)', c='y')\n        plt.title('Real ConfirmedCase and Predict ConfirmedCase')\n        plt.legend(loc='best');\n        plt.show()\n            \n        ####### Forecast 300 days after by using the best paramater of train data #######\n        if len(res.x)>2:\n            msle, sol = eval_model_decay(res.x, train_data, population, True, 300)\n        else:\n            msle, sol = eval_model_const(res.x, train_data, population, True, 300)\n        \n        sus, exp, inf, rec = sol.y\n        \n        y_pred = pd.DataFrame({\n            'ConfirmedCases': cumsum_signal(np.diff((inf + rec) * population, prepend=n_infected).cumsum()),\n            'Fatalities': cumsum_signal(np.clip(rec, 0, np.inf) * population * res.x[1])\n        })\n        \n        ####### Plot 300 days after of each country #######\n        start = train_min\n        end = start + timedelta(len(y_pred))\n        time_array = np.arange(start, end, dtype='datetime64[D]')\n\n        max_day = numpy.where(inf == numpy.amax(inf))[0][0]\n        where_time = time_array[max_day]\n        pred_max_day = y_pred['ConfirmedCases'][max_day]\n        xy_show_max_estimation = (where_time, max_day)\n        \n        con = y_pred['ConfirmedCases']\n        max_day_con = numpy.where(con == numpy.amax(con))[0][0] # Find the max confimed case of each country\n        max_con = numpy.amax(con)\n        where_time_con = time_array[len(time_array)-50]\n        xy_show_max_estimation_confirmed = (where_time_con, max_con)\n        \n        fig = go.Figure()\n        fig.add_trace(go.Scatter(x=time_array, y=y_pred['ConfirmedCases'].astype(int),\n                            mode='lines',\n                            line = dict(color='red'),\n                            name='Estimation Confirmed Case Start from '+ str(start.date())+ ' to ' +str(end.date())))\n        \n        fig.add_trace(go.Scatter(x=time_array[:len(train)], y=train['ConfirmedCases'],\n                            mode='lines',\n                            name='Confirmed case until '+ str(train_all_max.date()),line = dict(color='green', width=4)))\n        fig.add_annotation(\n            x=where_time_con,\n            y=max_con-(max_con/30),\n            showarrow=False,\n            text=\"Estimate Max Case around:\" +str(int(max_con)),\n            font=dict(\n                color=\"Blue\",\n                size=15\n            ))\n        fig.add_annotation(\n            x=time_array[len(train)-1],\n            y=train['ConfirmedCases'].tolist()[-1],\n            showarrow=True,\n            text=f\"Real Max ConfirmedCase: \" +str(int(train['ConfirmedCases'].tolist()[-1]))) \n        \n        fig.add_annotation(\n            x=where_time,\n            y=pred_max_day,\n            text='Infect start decrease from: ' + str(where_time))   \n        fig.update_layout(title='Estimate Confirmed Case ,'+area_name+' Total population ='+ str(int(population)), legend_orientation=\"h\")\n        fig.show()\n        \n        #df = pd.DataFrame({'Values': train_data['ConfirmedCases'].tolist()+y_pred['ConfirmedCases'].tolist(),'Date_datatime':time_array[:len(train_data)].tolist()+time_array.tolist(),\n        #           'Real/Predict': ['ConfirmedCase' for i in range(len(train_data))]+['PredictCase' for i in range(len(y_pred))]})\n        #fig = px.line(df, x=\"Date_datatime\", y=\"Values\",color = 'Real/Predict')\n        #fig.show()\n        #plt.figure(figsize = (16,7))\n        #plt.plot(time_array[:len(train_data)],train_data['ConfirmedCases'],label='Confirmed case until '+ str(train_max.date()),color='g', linewidth=3.0)\n        #plt.plot(time_array,y_pred['ConfirmedCases'],label='Estimation Confirmed Case Start from '+ str(start.date())+ ' to ' +str(end.date()),color='r', linewidth=1.0)\n        #plt.annotate('Infect start decrease from: ' + str(where_time), xy=xy_show_max_estimation, size=15, color=\"black\")\n        #plt.annotate('max Confirmedcase: ' + str(int(max_con)), xy=xy_show_max_estimation_confirmed, size=15, color=\"black\")\n        #plt.title('Estimate Confirmed Case '+area_name+' Total population ='+ str(int(population)))\n        #plt.legend(loc='lower right')\n        #plt.show()\n\n\n    return y_pred_test, valid_msle","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predict all Country/Region and Province/States\n* Counting all Country/Region MSLE & predict\n* If MSLE is lower than 1 , using PR model to retrain and check the performance"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy\nvalidation_scores = []\nvalidation_county = []\nvalidation_country = []\nfor country in tqdm(train['Country_Region'].unique()):\n    country_pd_train = train[train['Country_Region']==country]\n    #if country_pd_train['Province_State'].isna().unique()==True:\n    if len(country_pd_train['Province_State'].unique())<2:\n        predict_test, score = fit_model_new(country_pd_train,country,make_plot=False)\n        if score ==0:\n            print(f'{country} no case')\n        validation_scores.append(score)\n        validation_county.append(country)\n        validation_country.append(country)\n        test.loc[test['Country_Region']==country,'ConfirmedCases'] = predict_test['ConfirmedCases'].tolist()\n        test.loc[test['Country_Region']==country,'Fatalities'] = predict_test['Fatalities'].tolist()\n    else:\n        for state in country_pd_train['Province_State'].unique():\n            if state != state: # check nan\n                state_pd = country_pd_train[country_pd_train['Province_State'].isna()]\n                predict_test, score = fit_model_new(state_pd,state,make_plot=False)\n                if score ==0:\n                    print(f'{country} / {state} no case')\n                validation_scores.append(score)\n                validation_county.append(state)\n                validation_country.append(country)\n                test.loc[(test['Country_Region']==country)&(test['Province_State'].isna()),'ConfirmedCases'] = predict_test['ConfirmedCases'].tolist()\n                test.loc[(test['Country_Region']==country)&(test['Province_State'].isna()),'Fatalities'] = predict_test['Fatalities'].tolist()\n            else:\n                state_pd = country_pd_train[country_pd_train['Province_State']==state]\n                predict_test, score = fit_model_new(state_pd,state,make_plot=False)\n                if score ==0:\n                    print(f'{country} / {state} no case')\n                validation_scores.append(score)\n                validation_county.append(state)\n                validation_country.append(country)\n                test.loc[(test['Country_Region']==country)&(test['Province_State']==state),'ConfirmedCases'] = predict_test['ConfirmedCases'].tolist()\n                test.loc[(test['Country_Region']==country)&(test['Province_State']==state),'Fatalities'] = predict_test['Fatalities'].tolist()\n         #   print(f'{country} {state} {score:0.5f}')\n            \nprint(f'Mean validation score: {np.average(validation_scores):0.5f}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"validation_scores = pd.DataFrame({'country/state':validation_country,'country':validation_county,'MSLE':validation_scores})\nvalidation_scores.sort_values(by=['MSLE'], ascending=False).head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"large_msle = validation_scores[validation_scores['MSLE']>1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Retrain PR model for MSLE>1 countries\n* If MSLE of PR model lower than SEIR model, than use PR model as predict result"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import linear_model\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression\n\nfor country in large_msle['country'].unique():\n    if (country!= country)==False: # check None\n        #print ('training model for country ==>'+country)\n        country_pd_train = train[train['Country_Region']==country]\n        country_pd_test = test[test['Country_Region']==country]\n        if len(country_pd_train)==0:\n            country_pd_train = train[train['Province_State']==country]\n            country_pd_test = test[test['Province_State']==country]\n\n            x = np.array(range(len(country_pd_train))).reshape((-1,1))[:-7]\n            valid_x = np.array(range(len(country_pd_train))).reshape((-1,1))[-7:]\n            y = country_pd_train['ConfirmedCases'][:-7]\n            valid_y = country_pd_train['ConfirmedCases'][-7:]\n            y_fat = country_pd_train['Fatalities'][:-7]\n            valid_y_fat = country_pd_train['Fatalities'][-7:]\n            \n            model = Pipeline([('poly', PolynomialFeatures(degree=2)),\n                             ('linear', LinearRegression(fit_intercept=False))])\n            model = model.fit(x, y)\n\n            model_fat = Pipeline([('poly', PolynomialFeatures(degree=2)),\n                             ('linear', LinearRegression(fit_intercept=False))])\n            model_fat = model_fat.fit(x, y_fat)\n            \n            predict_y = model.predict(valid_x)\n            predict_yfat = model_fat.predict(valid_x)\n            score = mean_squared_log_error(np.clip(valid_y,0,np.inf), np.clip(predict_y,0,np.inf))\n            score_fat = mean_squared_log_error(np.clip(valid_y_fat,0,np.inf), np.clip(predict_yfat,0,np.inf))\n            score = (score+score_fat)/2\n\n            print(f'{country} {score:0.5f}')\n            if score < large_msle[large_msle['country']==country]['MSLE'].tolist()[0]:\n                validation_scores.loc[validation_scores['country']==country,'MSLE'] = score\n                predict_x = (np.array(range(len(country_pd_test)))+50).reshape((-1,1))\n                test.loc[test['Province_State']==country,'ConfirmedCases'] = model.predict(predict_x)\n                test.loc[test['Province_State']==country,'Fatalities'] = model_fat.predict(predict_x)\n        else:\n            x = np.array(range(len(country_pd_train))).reshape((-1,1))[:-7]\n            valid_x = np.array(range(len(country_pd_train))).reshape((-1,1))[-7:]\n            y = country_pd_train['ConfirmedCases'][:-7]\n            valid_y = country_pd_train['ConfirmedCases'][-7:]\n            y_fat = country_pd_train['Fatalities'][:-7]\n            valid_y_fat = country_pd_train['Fatalities'][-7:]\n            \n            model = Pipeline([('poly', PolynomialFeatures(degree=2)),\n                             ('linear', LinearRegression(fit_intercept=False))])\n            model = model.fit(x, y)\n\n            model_fat = Pipeline([('poly', PolynomialFeatures(degree=2)),\n                             ('linear', LinearRegression(fit_intercept=False))])\n            model_fat = model_fat.fit(x, y_fat)\n            \n            predict_y = model.predict(valid_x)\n            predict_yfat = model_fat.predict(valid_x)\n            score = mean_squared_log_error(np.clip(valid_y,0,np.inf), np.clip(predict_y,0,np.inf))\n            score_fat = mean_squared_log_error(np.clip(valid_y_fat,0,np.inf), np.clip(predict_yfat,0,np.inf))\n            score = (score+score_fat)/2\n\n            print(f'{country} {score:0.5f}')\n            if score < large_msle[large_msle['country']==country]['MSLE'].tolist()[0]:\n                validation_scores.loc[validation_scores['country']==country,'MSLE'] = score\n                predict_x = (np.array(range(len(country_pd_test)))+50).reshape((-1,1))\n                test.loc[test['Country_Region']==country,'ConfirmedCases'] = model.predict(predict_x)\n                test.loc[test['Country_Region']==country,'Fatalities'] = model_fat.predict(predict_x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submit result"},{"metadata":{"trusted":true},"cell_type":"code","source":"val_soces = validation_scores['MSLE'].tolist()\nprint(f'Mean validation score: {np.average(val_soces):0.5f}')","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-03-23T05:27:29.369906Z","start_time":"2020-03-23T05:27:29.257863Z"},"trusted":true},"cell_type":"code","source":"submit = pd.read_csv('../input/covid19-global-forecasting-week-3/submission.csv')\nsubmit['Fatalities'] = test['Fatalities'].astype('float')\nsubmit['ConfirmedCases'] = test['ConfirmedCases'].astype('float')\nsubmit.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model 2 - XGB CNN"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom scipy.optimize import curve_fit\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import OneHotEncoder\n\nimport xgboost as xgb\n\nfrom tensorflow.keras.optimizers import Nadam\nfrom sklearn.metrics import mean_squared_error\nimport tensorflow as tf\nimport tensorflow.keras.layers as KL\nfrom datetime import timedelta\nimport numpy as np\nimport pandas as pd\n\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression, Ridge\n\nimport datetime\nimport gc\nfrom tqdm import tqdm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_cpmp_sub(save_oof=False, save_public_test=False):\n    train = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-3/train.csv')\n    train['Province_State'].fillna('', inplace=True)\n    train['Date'] = pd.to_datetime(train['Date'])\n    train['day'] = train.Date.dt.dayofyear\n    #train = train[train.day <= 85]\n    train['geo'] = ['_'.join(x) for x in zip(train['Country_Region'], train['Province_State'])]\n    train\n\n    test = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-3/test.csv')\n    test['Province_State'].fillna('', inplace=True)\n    test['Date'] = pd.to_datetime(test['Date'])\n    test['day'] = test.Date.dt.dayofyear\n    test['geo'] = ['_'.join(x) for x in zip(test['Country_Region'], test['Province_State'])]\n    test\n\n    day_min = train['day'].min()\n    train['day'] -= day_min\n    test['day'] -= day_min\n\n    min_test_val_day = test.day.min()\n    max_test_val_day = train.day.max()\n    max_test_day = test.day.max()\n    num_days = max_test_day + 1\n\n    min_test_val_day, max_test_val_day, num_days\n\n    train['ForecastId'] = -1\n    test['Id'] = -1\n    test['ConfirmedCases'] = 0\n    test['Fatalities'] = 0\n\n    debug = False\n\n    data = pd.concat([train,\n                      test[test.day > max_test_val_day][train.columns]\n                     ]).reset_index(drop=True)\n    if debug:\n        data = data[data['geo'] >= 'France_'].reset_index(drop=True)\n    #del train, test\n    gc.collect()\n\n    dates = data[data['geo'] == 'France_'].Date.values\n\n    if 0:\n        gr = data.groupby('geo')\n        data['ConfirmedCases'] = gr.ConfirmedCases.transform('cummax')\n        data['Fatalities'] = gr.Fatalities.transform('cummax')\n\n    geo_data = data.pivot(index='geo', columns='day', values='ForecastId')\n    num_geo = geo_data.shape[0]\n    geo_data\n\n    geo_id = {}\n    for i,g in enumerate(geo_data.index):\n        geo_id[g] = i\n\n\n    ConfirmedCases = data.pivot(index='geo', columns='day', values='ConfirmedCases')\n    Fatalities = data.pivot(index='geo', columns='day', values='Fatalities')\n\n    if debug:\n        cases = ConfirmedCases.values\n        deaths = Fatalities.values\n    else:\n        cases = np.log1p(ConfirmedCases.values)\n        deaths = np.log1p(Fatalities.values)\n\n\n    def get_dataset(start_pred, num_train, lag_period):\n        days = np.arange( start_pred - num_train + 1, start_pred + 1)\n        lag_cases = np.vstack([cases[:, d - lag_period : d] for d in days])\n        lag_deaths = np.vstack([deaths[:, d - lag_period : d] for d in days])\n        target_cases = np.vstack([cases[:, d : d + 1] for d in days])\n        target_deaths = np.vstack([deaths[:, d : d + 1] for d in days])\n        geo_ids = np.vstack([geo_ids_base for d in days])\n        country_ids = np.vstack([country_ids_base for d in days])\n        return lag_cases, lag_deaths, target_cases, target_deaths, geo_ids, country_ids, days\n\n    def update_valid_dataset(data, pred_death, pred_case):\n        lag_cases, lag_deaths, target_cases, target_deaths, geo_ids, country_ids, days = data\n        day = days[-1] + 1\n        new_lag_cases = np.hstack([lag_cases[:, 1:], pred_case])\n        new_lag_deaths = np.hstack([lag_deaths[:, 1:], pred_death]) \n        new_target_cases = cases[:, day:day+1]\n        new_target_deaths = deaths[:, day:day+1] \n        new_geo_ids = geo_ids  \n        new_country_ids = country_ids  \n        new_days = 1 + days\n        return new_lag_cases, new_lag_deaths, new_target_cases, new_target_deaths, new_geo_ids, new_country_ids, new_days\n\n    def fit_eval(lr_death, lr_case, data, start_lag_death, end_lag_death, num_lag_case, fit, score):\n        lag_cases, lag_deaths, target_cases, target_deaths, geo_ids, country_ids, days = data\n\n        X_death = np.hstack([lag_cases[:, -start_lag_death:-end_lag_death], country_ids])\n        X_death = np.hstack([lag_deaths[:, -num_lag_case:], country_ids])\n        X_death = np.hstack([lag_cases[:, -start_lag_death:-end_lag_death], lag_deaths[:, -num_lag_case:], country_ids])\n        y_death = target_deaths\n        y_death_prev = lag_deaths[:, -1:]\n        if fit:\n            if 0:\n                keep = (y_death > 0).ravel()\n                X_death = X_death[keep]\n                y_death = y_death[keep]\n                y_death_prev = y_death_prev[keep]\n            lr_death.fit(X_death, y_death)\n        y_pred_death = lr_death.predict(X_death)\n        y_pred_death = np.maximum(y_pred_death, y_death_prev)\n\n        X_case = np.hstack([lag_cases[:, -num_lag_case:], geo_ids])\n        X_case = lag_cases[:, -num_lag_case:]\n        y_case = target_cases\n        y_case_prev = lag_cases[:, -1:]\n        if fit:\n            lr_case.fit(X_case, y_case)\n        y_pred_case = lr_case.predict(X_case)\n        y_pred_case = np.maximum(y_pred_case, y_case_prev)\n\n        if score:\n            death_score = val_score(y_death, y_pred_death)\n            case_score = val_score(y_case, y_pred_case)\n        else:\n            death_score = 0\n            case_score = 0\n\n        return death_score, case_score, y_pred_death, y_pred_case\n\n    def train_model(train, valid, start_lag_death, end_lag_death, num_lag_case, num_val, score=True):\n        alpha = 3\n        lr_death = Ridge(alpha=alpha, fit_intercept=False)\n        lr_case = Ridge(alpha=alpha, fit_intercept=True)\n\n        (train_death_score, train_case_score, train_pred_death, train_pred_case,\n        ) = fit_eval(lr_death, lr_case, train, start_lag_death, end_lag_death, num_lag_case, fit=True, score=score)\n\n        death_scores = []\n        case_scores = []\n\n        death_pred = []\n        case_pred = []\n\n        for i in range(num_val):\n\n            (valid_death_score, valid_case_score, valid_pred_death, valid_pred_case,\n            ) = fit_eval(lr_death, lr_case, valid, start_lag_death, end_lag_death, num_lag_case, fit=False, score=score)\n\n            death_scores.append(valid_death_score)\n            case_scores.append(valid_case_score)\n            death_pred.append(valid_pred_death)\n            case_pred.append(valid_pred_case)\n\n            if 0:\n                print('val death: %0.3f' %  valid_death_score,\n                      'val case: %0.3f' %  valid_case_score,\n                      'val : %0.3f' %  np.mean([valid_death_score, valid_case_score]),\n                      flush=True)\n            valid = update_valid_dataset(valid, valid_pred_death, valid_pred_case)\n\n        if score:\n            death_scores = np.sqrt(np.mean([s**2 for s in death_scores]))\n            case_scores = np.sqrt(np.mean([s**2 for s in case_scores]))\n            if 0:\n                print('train death: %0.3f' %  train_death_score,\n                      'train case: %0.3f' %  train_case_score,\n                      'val death: %0.3f' %  death_scores,\n                      'val case: %0.3f' %  case_scores,\n                      'val : %0.3f' % ( (death_scores + case_scores) / 2),\n                      flush=True)\n            else:\n                print('%0.4f' %  case_scores,\n                      ', %0.4f' %  death_scores,\n                      '= %0.4f' % ( (death_scores + case_scores) / 2),\n                      flush=True)\n        death_pred = np.hstack(death_pred)\n        case_pred = np.hstack(case_pred)\n        return death_scores, case_scores, death_pred, case_pred\n\n    countries = [g.split('_')[0] for g in geo_data.index]\n    countries = pd.factorize(countries)[0]\n\n    country_ids_base = countries.reshape((-1, 1))\n    ohe = OneHotEncoder(sparse=False)\n    country_ids_base = 0.2 * ohe.fit_transform(country_ids_base)\n    country_ids_base.shape\n\n    geo_ids_base = np.arange(num_geo).reshape((-1, 1))\n    ohe = OneHotEncoder(sparse=False)\n    geo_ids_base = 0.1 * ohe.fit_transform(geo_ids_base)\n    geo_ids_base.shape\n\n    def val_score(true, pred):\n        pred = np.log1p(np.round(np.expm1(pred) - 0.2))\n        return np.sqrt(mean_squared_error(true.ravel(), pred.ravel()))\n\n    def val_score(true, pred):\n        return np.sqrt(mean_squared_error(true.ravel(), pred.ravel()))\n\n\n\n    start_lag_death, end_lag_death = 14, 6,\n    num_train = 5\n    num_lag_case = 14\n    lag_period = max(start_lag_death, num_lag_case)\n\n    def get_oof(start_val_delta=0):   \n        start_val = min_test_val_day + start_val_delta\n        last_train = start_val - 1\n        num_val = max_test_val_day - start_val + 1\n        print(dates[start_val], start_val, num_val)\n        train_data = get_dataset(last_train, num_train, lag_period)\n        valid_data = get_dataset(start_val, 1, lag_period)\n        _, _, val_death_preds, val_case_preds = train_model(train_data, valid_data, \n                                                            start_lag_death, end_lag_death, num_lag_case, num_val)\n\n        pred_deaths = Fatalities.iloc[:, start_val:start_val+num_val].copy()\n        pred_deaths.iloc[:, :] = np.expm1(val_death_preds)\n        pred_deaths = pred_deaths.stack().reset_index()\n        pred_deaths.columns = ['geo', 'day', 'Fatalities']\n        pred_deaths\n\n        pred_cases = ConfirmedCases.iloc[:, start_val:start_val+num_val].copy()\n        pred_cases.iloc[:, :] = np.expm1(val_case_preds)\n        pred_cases = pred_cases.stack().reset_index()\n        pred_cases.columns = ['geo', 'day', 'ConfirmedCases']\n        pred_cases\n\n        sub = train[['Date', 'Id', 'geo', 'day']]\n        sub = sub.merge(pred_cases, how='left', on=['geo', 'day'])\n        sub = sub.merge(pred_deaths, how='left', on=['geo', 'day'])\n        #sub = sub.fillna(0)\n        sub = sub[sub.day >= start_val]\n        sub = sub[['Id', 'ConfirmedCases', 'Fatalities']].copy()\n        return sub\n\n\n    if save_oof:\n        for start_val_delta, date in zip(range(3, -8, -3),\n                                  ['2020-03-22', '2020-03-19', '2020-03-16', '2020-03-13']):\n            print(date, end=' ')\n            oof = get_oof(start_val_delta)\n            oof.to_csv('../submissions/cpmp-%s.csv' % date, index=None)\n\n    def get_sub(start_val_delta=0):   \n        start_val = min_test_val_day + start_val_delta\n        last_train = start_val - 1\n        num_val = max_test_val_day - start_val + 1\n        print(dates[last_train], start_val, num_val)\n        num_lag_case = 14\n        train_data = get_dataset(last_train, num_train, lag_period)\n        valid_data = get_dataset(start_val, 1, lag_period)\n        _, _, val_death_preds, val_case_preds = train_model(train_data, valid_data, \n                                                            start_lag_death, end_lag_death, num_lag_case, num_val)\n\n        pred_deaths = Fatalities.iloc[:, start_val:start_val+num_val].copy()\n        pred_deaths.iloc[:, :] = np.expm1(val_death_preds)\n        pred_deaths = pred_deaths.stack().reset_index()\n        pred_deaths.columns = ['geo', 'day', 'Fatalities']\n        pred_deaths\n\n        pred_cases = ConfirmedCases.iloc[:, start_val:start_val+num_val].copy()\n        pred_cases.iloc[:, :] = np.expm1(val_case_preds)\n        pred_cases = pred_cases.stack().reset_index()\n        pred_cases.columns = ['geo', 'day', 'ConfirmedCases']\n        pred_cases\n\n        sub = test[['Date', 'ForecastId', 'geo', 'day']]\n        sub = sub.merge(pred_cases, how='left', on=['geo', 'day'])\n        sub = sub.merge(pred_deaths, how='left', on=['geo', 'day'])\n        sub = sub.fillna(0)\n        sub = sub[['ForecastId', 'ConfirmedCases', 'Fatalities']]\n        return sub\n        return sub\n\n\n    known_test = train[['geo', 'day', 'ConfirmedCases', 'Fatalities']\n              ].merge(test[['geo', 'day', 'ForecastId']], how='left', on=['geo', 'day'])\n    known_test = known_test[['ForecastId', 'ConfirmedCases', 'Fatalities']][known_test.ForecastId.notnull()].copy()\n    known_test\n\n    unknow_test = test[test.day > max_test_val_day]\n    unknow_test\n\n    def get_final_sub():   \n        start_val = max_test_val_day + 1\n        last_train = start_val - 1\n        num_val = max_test_day - start_val + 1\n        print(dates[last_train], start_val, num_val)\n        num_lag_case = num_val + 3\n        train_data = get_dataset(last_train, num_train, lag_period)\n        valid_data = get_dataset(start_val, 1, lag_period)\n        (_, _, val_death_preds, val_case_preds\n        ) = train_model(train_data, valid_data, start_lag_death, end_lag_death, num_lag_case, num_val, score=False)\n\n        pred_deaths = Fatalities.iloc[:, start_val:start_val+num_val].copy()\n        pred_deaths.iloc[:, :] = np.expm1(val_death_preds)\n        pred_deaths = pred_deaths.stack().reset_index()\n        pred_deaths.columns = ['geo', 'day', 'Fatalities']\n        pred_deaths\n\n        pred_cases = ConfirmedCases.iloc[:, start_val:start_val+num_val].copy()\n        pred_cases.iloc[:, :] = np.expm1(val_case_preds)\n        pred_cases = pred_cases.stack().reset_index()\n        pred_cases.columns = ['geo', 'day', 'ConfirmedCases']\n        pred_cases\n        print(unknow_test.shape, pred_deaths.shape, pred_cases.shape)\n\n        sub = unknow_test[['Date', 'ForecastId', 'geo', 'day']]\n        sub = sub.merge(pred_cases, how='left', on=['geo', 'day'])\n        sub = sub.merge(pred_deaths, how='left', on=['geo', 'day'])\n        #sub = sub.fillna(0)\n        sub = sub[['ForecastId', 'ConfirmedCases', 'Fatalities']]\n        sub = pd.concat([known_test, sub])\n        return sub\n\n    if save_public_test:\n        sub = get_sub()\n    else:\n        sub = get_final_sub()\n    return sub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_nn_sub():\n    df = pd.read_csv(\"../input/covid19-global-forecasting-week-3/train.csv\")\n    sub_df = pd.read_csv(\"../input/covid19-global-forecasting-week-3/test.csv\")\n\n    coo_df = pd.read_csv(\"../input/covid19week1/train.csv\").rename(columns={\"Country/Region\": \"Country_Region\"})\n    coo_df = coo_df.groupby(\"Country_Region\")[[\"Lat\", \"Long\"]].mean().reset_index()\n    coo_df = coo_df[coo_df[\"Country_Region\"].notnull()]\n\n    loc_group = [\"Province_State\", \"Country_Region\"]\n\n    def preprocess(df):\n        df[\"Date\"] = df[\"Date\"].astype(\"datetime64[ms]\")\n        df[\"days\"] = (df[\"Date\"] - pd.to_datetime(\"2020-01-01\")).dt.days\n        df[\"weekend\"] = df[\"Date\"].dt.dayofweek//5\n\n        df = df.merge(coo_df, how=\"left\", on=\"Country_Region\")\n        df[\"Lat\"] = (df[\"Lat\"] // 30).astype(np.float32).fillna(0)\n        df[\"Long\"] = (df[\"Long\"] // 60).astype(np.float32).fillna(0)\n\n        for col in loc_group:\n            df[col].fillna(\"none\", inplace=True)\n        return df\n\n    df = preprocess(df)\n    sub_df = preprocess(sub_df)\n\n    print(df.shape)\n\n    TARGETS = [\"ConfirmedCases\", \"Fatalities\"]\n\n    for col in TARGETS:\n        df[col] = np.log1p(df[col])\n\n    NUM_SHIFT = 5\n\n    features = [\"Lat\", \"Long\"]\n\n    for s in range(1, NUM_SHIFT+1):\n        for col in TARGETS:\n            df[\"prev_{}_{}\".format(col, s)] = df.groupby(loc_group)[col].shift(s)\n            features.append(\"prev_{}_{}\".format(col, s))\n\n    df = df[df[\"Date\"] >= df[\"Date\"].min() + timedelta(days=NUM_SHIFT)].copy()\n\n    TEST_FIRST = sub_df[\"Date\"].min() # pd.to_datetime(\"2020-03-13\") #\n    TEST_DAYS = (df[\"Date\"].max() - TEST_FIRST).days + 1\n\n    dev_df, test_df = df[df[\"Date\"] < TEST_FIRST].copy(), df[df[\"Date\"] >= TEST_FIRST].copy()\n\n    def nn_block(input_layer, size, dropout_rate, activation):\n        out_layer = KL.Dense(size, activation=None)(input_layer)\n        #out_layer = KL.BatchNormalization()(out_layer)\n        out_layer = KL.Activation(activation)(out_layer)\n        out_layer = KL.Dropout(dropout_rate)(out_layer)\n        return out_layer\n\n\n    def get_model():\n        inp = KL.Input(shape=(len(features),))\n\n        hidden_layer = nn_block(inp, 64, 0.0, \"relu\")\n        gate_layer = nn_block(hidden_layer, 32, 0.0, \"sigmoid\")\n        hidden_layer = nn_block(hidden_layer, 32, 0.0, \"relu\")\n        hidden_layer = KL.multiply([hidden_layer, gate_layer])\n\n        out = KL.Dense(len(TARGETS), activation=\"linear\")(hidden_layer)\n\n        model = tf.keras.models.Model(inputs=[inp], outputs=out)\n        return model\n\n    get_model().summary()\n\n    def get_input(df):\n        return [df[features]]\n\n    NUM_MODELS = 10\n\n\n    def train_models(df, save=False):\n        models = []\n        for i in range(NUM_MODELS):\n            model = get_model()\n            model.compile(loss=\"mean_squared_error\", optimizer=Nadam(lr=1e-4))\n            hist = model.fit(get_input(df), df[TARGETS],\n                             batch_size=2048, epochs=500, verbose=0, shuffle=True)\n            if save:\n                model.save_weights(\"model{}.h5\".format(i))\n            models.append(model)\n        return models\n\n    models = train_models(dev_df)\n\n\n    prev_targets = ['prev_ConfirmedCases_1', 'prev_Fatalities_1']\n\n    def predict_one(df, models):\n        pred = np.zeros((df.shape[0], 2))\n        for model in models:\n            pred += model.predict(get_input(df))/len(models)\n        pred = np.maximum(pred, df[prev_targets].values)\n        pred[:, 0] = np.log1p(np.expm1(pred[:, 0]) + 0.1)\n        pred[:, 1] = np.log1p(np.expm1(pred[:, 1]) + 0.01)\n        return np.clip(pred, None, 15)\n\n    print([mean_squared_error(dev_df[TARGETS[i]], predict_one(dev_df, models)[:, i]) for i in range(len(TARGETS))])\n\n\n    def rmse(y_true, y_pred):\n        return np.sqrt(mean_squared_error(y_true, y_pred))\n\n    def evaluate(df):\n        error = 0\n        for col in TARGETS:\n            error += rmse(df[col].values, df[\"pred_{}\".format(col)].values)\n        return np.round(error/len(TARGETS), 5)\n\n\n    def predict(test_df, first_day, num_days, models, val=False):\n        temp_df = test_df.loc[test_df[\"Date\"] == first_day].copy()\n        y_pred = predict_one(temp_df, models)\n\n        for i, col in enumerate(TARGETS):\n            test_df[\"pred_{}\".format(col)] = 0\n            test_df.loc[test_df[\"Date\"] == first_day, \"pred_{}\".format(col)] = y_pred[:, i]\n\n        print(first_day, np.isnan(y_pred).sum(), y_pred.min(), y_pred.max())\n        if val:\n            print(evaluate(test_df[test_df[\"Date\"] == first_day]))\n\n\n        y_prevs = [None]*NUM_SHIFT\n\n        for i in range(1, NUM_SHIFT):\n            y_prevs[i] = temp_df[['prev_ConfirmedCases_{}'.format(i), 'prev_Fatalities_{}'.format(i)]].values\n\n        for d in range(1, num_days):\n            date = first_day + timedelta(days=d)\n            print(date, np.isnan(y_pred).sum(), y_pred.min(), y_pred.max())\n\n            temp_df = test_df.loc[test_df[\"Date\"] == date].copy()\n            temp_df[prev_targets] = y_pred\n            for i in range(2, NUM_SHIFT+1):\n                temp_df[['prev_ConfirmedCases_{}'.format(i), 'prev_Fatalities_{}'.format(i)]] = y_prevs[i-1]\n\n            y_pred, y_prevs = predict_one(temp_df, models), [None, y_pred] + y_prevs[1:-1]\n\n\n            for i, col in enumerate(TARGETS):\n                test_df.loc[test_df[\"Date\"] == date, \"pred_{}\".format(col)] = y_pred[:, i]\n\n            if val:\n                print(evaluate(test_df[test_df[\"Date\"] == date]))\n\n        return test_df\n\n    test_df = predict(test_df, TEST_FIRST, TEST_DAYS, models, val=True)\n    print(evaluate(test_df))\n\n    for col in TARGETS:\n        test_df[col] = np.expm1(test_df[col])\n        test_df[\"pred_{}\".format(col)] = np.expm1(test_df[\"pred_{}\".format(col)])\n\n    models = train_models(df, save=True)\n\n    sub_df_public = sub_df[sub_df[\"Date\"] <= df[\"Date\"].max()].copy()\n    sub_df_private = sub_df[sub_df[\"Date\"] > df[\"Date\"].max()].copy()\n\n    pred_cols = [\"pred_{}\".format(col) for col in TARGETS]\n    #sub_df_public = sub_df_public.merge(test_df[[\"Date\"] + loc_group + pred_cols].rename(columns={col: col[5:] for col in pred_cols}), \n    #                                    how=\"left\", on=[\"Date\"] + loc_group)\n    sub_df_public = sub_df_public.merge(test_df[[\"Date\"] + loc_group + TARGETS], how=\"left\", on=[\"Date\"] + loc_group)\n\n    SUB_FIRST = sub_df_private[\"Date\"].min()\n    SUB_DAYS = (sub_df_private[\"Date\"].max() - sub_df_private[\"Date\"].min()).days + 1\n\n    sub_df_private = df.append(sub_df_private, sort=False)\n\n    for s in range(1, NUM_SHIFT+1):\n        for col in TARGETS:\n            sub_df_private[\"prev_{}_{}\".format(col, s)] = sub_df_private.groupby(loc_group)[col].shift(s)\n\n    sub_df_private = sub_df_private[sub_df_private[\"Date\"] >= SUB_FIRST].copy()\n\n    sub_df_private = predict(sub_df_private, SUB_FIRST, SUB_DAYS, models)\n\n    for col in TARGETS:\n        sub_df_private[col] = np.expm1(sub_df_private[\"pred_{}\".format(col)])\n\n    sub_df = sub_df_public.append(sub_df_private, sort=False)\n    sub_df[\"ForecastId\"] = sub_df[\"ForecastId\"].astype(np.int16)\n\n    return sub_df[[\"ForecastId\"] + TARGETS]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub1 = get_cpmp_sub()\nsub1['ForecastId'] = sub1['ForecastId'].astype('int')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub2 = get_nn_sub()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub1.sort_values(\"ForecastId\", inplace=True)\nsub2.sort_values(\"ForecastId\", inplace=True)\nsubmit.sort_values(\"ForecastId\", inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\nTARGETS = [\"ConfirmedCases\", \"Fatalities\"]\n\n[np.sqrt(mean_squared_error(np.log1p(sub1[t].values), np.log1p(sub2[t].values))) for t in TARGETS]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df = sub1.copy()\nfor t in TARGETS:\n    sub_df[t] = np.expm1(np.log1p(submit[t].values)*0.2 + np.log1p(sub1[t].values)*0.3 + np.log1p(sub2[t].values)*0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.1"}},"nbformat":4,"nbformat_minor":4}