{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Logistic curve fit and XGBoost hybrid fit"},{"metadata":{},"cell_type":"markdown","source":"In previous weeks we found that a logistic curve fit works quite well on a per country level, and that adding a global XGBoost fit with [augmented data](https://www.kaggle.com/nxpnsv/country-health-indicators) is an improvement. The main idea for improvement in this notebook is to make an optimal interpolation between the two methods."},{"metadata":{},"cell_type":"markdown","source":"## Set up environment"},{"metadata":{"ExecuteTime":{"end_time":"2020-04-08T13:47:08.959572Z","start_time":"2020-04-08T13:47:07.584904Z"},"trusted":true},"cell_type":"code","source":"# Imports\nimport os\nfrom typing import Dict, List, Tuple\nfrom joblib import Parallel, delayed\nimport pandas as pd\nimport numpy as np\nfrom scipy.optimize.minpack import curve_fit\nfrom scipy.optimize import least_squares\nfrom xgboost import XGBRegressor","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-04-08T13:47:08.98239Z","start_time":"2020-04-08T13:47:08.962592Z"},"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Helper functions\ndef load_kaggle_csv(dataset: str, datadir: str) -> pd.DataFrame:\n    \"\"\"Load and clean kaggle csv input\"\"\"\n    df = pd.read_csv(\n        f\"{os.path.join(datadir,dataset)}.csv\", parse_dates=[\"Date\"]\n    )\n    df['country'] = df[\"Country_Region\"]\n    if \"Province_State\" in df:\n        df[\"Country_Region\"] = np.where(\n            df[\"Province_State\"].isnull(),\n            df[\"Country_Region\"],\n            df[\"Country_Region\"] + \"_\" + df[\"Province_State\"],\n        )\n        df.drop(columns=\"Province_State\", inplace=True)\n    if \"ConfirmedCases\" in df:\n        df[\"ConfirmedCases\"] = df.groupby(\"Country_Region\")[\n            \"ConfirmedCases\"\n        ].cummax()\n    if \"Fatalities\" in df:\n        df[\"Fatalities\"] = df.groupby(\"Country_Region\")[\"Fatalities\"].cummax()\n    if not \"DayOfYear\" in df:\n        df[\"DayOfYear\"] = df[\"Date\"].dt.dayofyear\n    df[\"Date\"] = df[\"Date\"].dt.date\n    return df\n\ndef RMSLE(actual: np.ndarray, prediction: np.ndarray) -> float:\n    \"\"\"Calculate RMSLE between actual and predicted values\"\"\"\n    return np.sqrt(\n        np.mean(\n            np.power(np.log1p(np.maximum(0, prediction)) - np.log1p(actual), 2)\n        )\n    )\n\ndef get_extra_features(df): \n    df['school_closure_status_daily'] = np.where(df['school_closure'] < df['Date'], 1, 0)\n    df['school_closure_first_fatality'] = np.where(df['school_closure'] < df['first_1Fatalities'], 1, 0)\n    df['school_closure_first_10cases'] = np.where( df['school_closure'] < df['first_10ConfirmedCases'], 1, 0)\n    #\n    df['case_delta1_10'] = (df['first_10ConfirmedCases'] - df['first_1ConfirmedCases']).dt.days\n    df['case_death_delta1'] = (df['first_1Fatalities'] - df['first_1ConfirmedCases']).dt.days\n    df['case_delta1_100'] = (df['first_100ConfirmedCases'] - df['first_1ConfirmedCases']).dt.days\n    df['days_since'] = df['DayOfYear']-df['case1_DayOfYear']\n    df['weekday'] = pd.to_datetime(df['Date']).dt.weekday\n    col = df.isnull().mean()\n    rm_null_col = col[col > 0.2].index.tolist()\n    return df#.drop(rm_null_col, axis=1)\n    \n\n    \ndef dateparse(x): \n    try:\n        return pd.datetime.strptime(x, '%Y-%m-%d')\n    except:\n        return pd.NaT\n\ndef prepare_lat_long(df):\n    df[\"Country_Region\"] = np.where(\n            df[\"Province/State\"].isnull(),\n            df[\"Country/Region\"],\n            df[\"Country/Region\"] + \"_\" + df[\"Province/State\"],\n        )\n    return df[['Country_Region', 'Lat', 'Long']].drop_duplicates()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load data"},{"metadata":{"ExecuteTime":{"end_time":"2020-04-08T13:59:47.60271Z","start_time":"2020-04-08T13:59:44.677041Z"},"trusted":true},"cell_type":"code","source":"df_lat = prepare_lat_long(pd.read_csv(\"/kaggle/input/inputlat-long/lat_long.csv\"))\n### TRAIN DATA\n# Kaggle input\ntrain = load_kaggle_csv(\"train\", \"/kaggle/input/covid19-global-forecasting-week-3\")\n# Augmentations\n\ncountry_health_indicators = (\n    (pd.read_csv(\"/kaggle/input/country-health-indicators/country_health_indicators_v3.csv\", \n        parse_dates=['first_1ConfirmedCases', 'first_10ConfirmedCases', \n                     'first_50ConfirmedCases', 'first_100ConfirmedCases',\n                     'first_1Fatalities', 'school_closure'], date_parser=dateparse)).rename(\n        columns ={'Country_Region':'country'}))\n# Merge augmentation to kaggle input\ntrain = (pd.merge(train, country_health_indicators,\n                  on=\"country\",\n                  how=\"left\")).merge(df_lat, on='Country_Region', how='left')\ntrain = get_extra_features(train)\n\n# train=train.fillna(0)\ntrain.head(3)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-04-08T13:59:52.303419Z","start_time":"2020-04-08T13:59:50.743626Z"},"trusted":true},"cell_type":"code","source":"### TEST DATA\ntest = load_kaggle_csv(\"test\", \"/kaggle/input/covid19-global-forecasting-week-3\")\ntest = (pd.merge(\n    test, country_health_indicators, on=\"country\", how=\"left\")).merge(\n    df_lat, on ='Country_Region', how='left')\ntest = get_extra_features(test)\n# test=test.fillna(0)\ndel country_health_indicators","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Logistic fit\n\nThe logistc fit uses `scipy.optimize.curvefit` to fit a [logistic function](https://en.wikipedia.org/wiki/Logistic_function):\n\n$$f(x) = \\frac{L}{1 + \\exp(-k(x - x0))}$$\n\nThe fit is done for each `Country_Region` separateley. Each fit is initialized with a first guess \n\n$$p_0(x_0, L, k)=(\\mathrm{median}(x), \\max(y), 0.1)*(U+0.5,U +1.0 ,U+0.5)$$\n\nwhere $U$ are uniform random numbers. The fits are repeated repeated $n_\\mathrm{samples}$ times and the fit producing the lowest RMSLE is used for prediction.  In addition, bounds are set as $x_0\\in[0, 200]$, $L\\in[\\max(y), 10^6]$, and $k\\in[0.1, 0.5]$. Furthermore, the error on $y$ is estimated to be $\\sigma_y=\\sqrt(y)(0.1+0.9U)$. This is the Poisson error with a random scaling to reduce assumptions on the optimal scaling with $\\sigma_y$. For speed these fits are done in parallel with `joblib`.\n\nFirst we define the required functions:"},{"metadata":{"ExecuteTime":{"end_time":"2020-04-08T14:00:22.642319Z","start_time":"2020-04-08T14:00:22.601542Z"},"trusted":true},"cell_type":"code","source":"def logistic(x: np.ndarray, x0: float, L: float, k: float) -> np.ndarray:\n    \"\"\"Simple logistic function\"\"\"\n    return L / (1 + np.exp(-k * (x - x0)))\n\n\ndef fit_single_logistic(x: np.ndarray, y: np.ndarray, maxfev: float) -> Tuple:\n    \"\"\"Fit with randopm jitter\"\"\"\n    # Fuzzy fitter\n    p0 = [np.median(x), y[-1], 0.1]\n    pn0 = p0 * (np.random.random(len(p0)) + [0.5, 1.0, 0.5])\n    try:\n        params, pcov = curve_fit(\n            logistic,\n            x,\n            y,\n            p0=pn0,\n            maxfev=maxfev,\n            sigma=np.maximum(1, np.sqrt(y)) * (0.1 + 0.9 * np.random.random()),\n            bounds=([0, y[-1], 0.01], [200, 1e6, 1.5]),\n        )\n        pcov = pcov[np.triu_indices_from(pcov)]\n    except (RuntimeError, ValueError):\n        params = p0\n        pcov = np.zeros(len(p0) * (len(p0) - 1))\n    y_hat = logistic(x, *params)\n    rmsle = RMSLE(y_hat, y)\n    return (params, pcov, rmsle, y_hat)\n\n\ndef fit_logistic(\n    df: pd.DataFrame,\n    n_jobs: int = 8,\n    n_samples: int = 80,\n    maxfev: int = 8000,\n    x_col: str = \"DayOfYear\",\n    y_cols: List[str] = [\"ConfirmedCases\", \"Fatalities\"],\n) -> pd.DataFrame:\n    def fit_one(df: pd.DataFrame, y_col: str) -> Dict:\n        best_rmsle = None\n        best_params = None\n        x = df[x_col].to_numpy()\n        y = df[y_col].to_numpy()\n        for (params, cov, rmsle, y_hat) in Parallel(n_jobs=n_jobs)(\n            delayed(fit_single_logistic)(x, y, maxfev=maxfev)\n            for i in range(n_samples)\n        ):\n            if rmsle >= (best_rmsle or rmsle):\n                best_rmsle = rmsle\n                best_params = params\n        result = {f\"{y_col}_rmsle\": best_rmsle}\n        result.update({f\"{y_col}_p_{i}\": p for i, p in enumerate(best_params)})\n        return result\n\n    result = {}\n    for y_col in y_cols:\n        result.update(fit_one(df, y_col))\n    return pd.DataFrame([result])\n\n\ndef predict_logistic(\n    df: pd.DataFrame,\n    x_col: str = \"DayOfYear\",\n    y_cols: List[str] = [\"ConfirmedCases\", \"Fatalities\"],\n):\n    def predict_one(col):\n        df[f\"yhat_logistic_{col}\"] = logistic(\n            df[x_col].to_numpy(),\n            df[f\"{col}_p_0\"].to_numpy(),\n            df[f\"{col}_p_1\"].to_numpy(),\n            df[f\"{col}_p_2\"].to_numpy(),\n        )\n\n    for y_col in y_cols:\n        predict_one(y_col)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now apply fit to each `Country_Region`. This takes a few minutes..."},{"metadata":{"ExecuteTime":{"end_time":"2020-04-08T14:27:39.491866Z","start_time":"2020-04-08T14:00:23.362765Z"},"trusted":true},"cell_type":"code","source":"train = pd.merge(\n    train, train.groupby(\n    [\"Country_Region\"], observed=True, sort=False\n).apply(lambda x: fit_logistic(x, n_jobs=8, n_samples=80, maxfev=16000)).reset_index(), \n    on=[\"Country_Region\"], how=\"left\")\npredict_logistic(train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# XGB boost regression"},{"metadata":{"ExecuteTime":{"end_time":"2020-04-08T14:27:39.497814Z","start_time":"2020-04-08T14:27:39.493732Z"},"trusted":true},"cell_type":"code","source":"def apply_xgb_model(train, x_columns, y_column, xgb_params):\n    X = train[x_columns].to_numpy()\n    y = train[y_column].to_numpy()\n    xgb_fit = XGBRegressor(**xgb_params).fit(X, y)\n    y_hat = xgb_fit.predict(X)\n    train[f\"yhat_xgb_{y_column}\"] = y_hat\n    return RMSLE(y, y_hat), xgb_fit","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-04-08T14:29:23.382729Z","start_time":"2020-04-08T14:27:39.50015Z"},"trusted":true},"cell_type":"code","source":"xgb_params_c = dict(\n    gamma=0.1,\n    learning_rate=0.35,\n    n_estimators=221,\n    max_depth=15,\n    min_child_weight=1,\n    nthread=8,\n    objective=\"reg:squarederror\")\n\nxgb_params_f = dict(\n    gamma=0.1022,\n    learning_rate=0.338,\n    n_estimators=292,\n    max_depth=14,\n    min_child_weight=1,\n    nthread=8,\n    objective=\"reg:squarederror\")\n\nx_columns = ['DayOfYear', \n       'Diabetes, blood, & endocrine diseases (%)', 'Respiratory diseases (%)',\n       'Diarrhea & common infectious diseases (%)',\n       'Nutritional deficiencies (%)',\n       'obesity - adult prevalence rate',\n       'pneumonia-death-rates', 'animal_fats', 'animal_products', 'eggs',\n       'offals', 'treenuts', 'vegetable_oils', 'nbr_surgeons',\n       'nbr_anaesthesiologists', 'population',\n       'school_shutdown_1case',\n       'school_shutdown_10case', 'school_shutdown_50case',\n       'school_shutdown_1death', 'case1_DayOfYear', 'case10_DayOfYear',\n       'case50_DayOfYear',\n    'school_closure_status_daily', 'case_delta1_10',\n       'case_death_delta1', 'case_delta1_100', 'days_since','Lat','Long','weekday',\n 'yhat_logistic_ConfirmedCases', 'yhat_logistic_Fatalities'\n]\nxgb_c_rmsle, xgb_c_fit = apply_xgb_model(train, x_columns, \"ConfirmedCases\", xgb_params_c)\nxgb_f_rmsle, xgb_f_fit = apply_xgb_model(train, x_columns, \"Fatalities\", xgb_params_f)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Hybrid fit\n\nFrom logistic curve fit we have $\\hat{y}_L$: `yhat_logistic_ConfirmedCases`,and from XGB boost regression $\\hat{y}_X$: `yhat_xgb_ConfirmedCases`.\nHere we make a hybrid predictor\n\n $\\hat{y}_H = \\alpha \\hat{y}_L + (1-\\alpha) \\hat{y}_X$ \n \n by fitting alpha with `scipy.optmize.least_squares`. Similarly for `Fatalities`. First we define a few functions to do the work:"},{"metadata":{"ExecuteTime":{"end_time":"2020-04-08T14:29:23.393647Z","start_time":"2020-04-08T14:29:23.384811Z"},"trusted":true},"cell_type":"code","source":"def interpolate(alpha, x0, x1):\n    return x0 * alpha + x1 * (1 - alpha)\n\n\ndef RMSLE_interpolate(alpha, y, x0, x1):\n    return RMSLE(y, interpolate(alpha, x0, x1))\n\n\ndef fit_hybrid(\n    train: pd.DataFrame, y_cols: List[str] = [\"ConfirmedCases\", \"Fatalities\"]\n) -> pd.DataFrame:\n    def fit_one(y_col: str):\n        opt = least_squares(\n            fun=RMSLE_interpolate,\n            args=(\n                train[y_col],\n                train[f\"yhat_logistic_{y_col}\"],\n                train[f\"yhat_xgb_{y_col}\"],\n            ),\n            x0=(0.5,),\n            bounds=((0.0), (1.0,)),\n        )\n        return {f\"{y_col}_alpha\": opt.x[0], f\"{y_col}_cost\": opt.cost}\n\n    result = {}\n    for y_col in y_cols:\n        result.update(fit_one(y_col))\n    return pd.DataFrame([result])\n\n\ndef predict_hybrid(\n    df: pd.DataFrame,\n    x_col: str = \"DayOfYear\",\n    y_cols: List[str] = [\"ConfirmedCases\", \"Fatalities\"],\n):\n    def predict_one(col):\n        df[f\"yhat_hybrid_{col}\"] = interpolate(\n            df[f\"{y_col}_alpha\"].to_numpy(),\n            df[f\"yhat_logistic_{y_col}\"].to_numpy(),\n            df[f\"yhat_xgb_{y_col}\"].to_numpy(),\n        )\n\n    for y_col in y_cols:\n        predict_one(y_col)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now apply to each `Country_Region`:"},{"metadata":{"ExecuteTime":{"end_time":"2020-04-08T14:29:34.432367Z","start_time":"2020-04-08T14:29:23.395231Z"},"trusted":true},"cell_type":"code","source":"train = pd.merge(\n    train,\n    train.groupby([\"Country_Region\"], observed=True, sort=False)\n    .apply(lambda x: fit_hybrid(x))\n    .reset_index(),\n    on=[\"Country_Region\"],\n    how=\"left\",\n)\n","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-04-08T14:29:34.442819Z","start_time":"2020-04-08T14:29:34.434058Z"},"trusted":true},"cell_type":"code","source":"predict_hybrid(train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Compare aproaches"},{"metadata":{"ExecuteTime":{"end_time":"2020-04-08T14:31:06.953659Z","start_time":"2020-04-08T14:31:06.870236Z"},"trusted":true},"cell_type":"code","source":"print(\n    \"Confirmed:\\n\"\n    f'Logistic\\t{RMSLE(train[\"ConfirmedCases\"], train[\"yhat_logistic_ConfirmedCases\"])}\\n'\n    f'XGBoost\\t{RMSLE(train[\"ConfirmedCases\"], train[\"yhat_xgb_ConfirmedCases\"])}\\n'\n    f'Hybrid\\t{RMSLE(train[\"ConfirmedCases\"], train[\"yhat_hybrid_ConfirmedCases\"])}\\n'\n    f\"Fatalities:\\n\"\n    f'Logistic\\t{RMSLE(train[\"Fatalities\"], train[\"yhat_logistic_Fatalities\"])}\\n'\n    f'XGBoost\\t{RMSLE(train[\"Fatalities\"], train[\"yhat_xgb_Fatalities\"])}\\n'\n    f'Hybrid\\t{RMSLE(train[\"Fatalities\"], train[\"yhat_hybrid_Fatalities\"])}\\n'\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predict test cases"},{"metadata":{"ExecuteTime":{"end_time":"2020-04-08T14:31:46.275774Z","start_time":"2020-04-08T14:31:45.64899Z"},"trusted":true},"cell_type":"code","source":"# Merge logistic and hybrid fit into test\ntest = pd.merge(\n    test, \n    train[[\"Country_Region\"] +\n          ['ConfirmedCases_p_0', 'ConfirmedCases_p_1', 'ConfirmedCases_p_2']+\n          ['Fatalities_p_0','Fatalities_p_1', 'Fatalities_p_2'] + \n          [\"Fatalities_alpha\"] + \n          [\"ConfirmedCases_alpha\"]].groupby(['Country_Region']).head(1), on=\"Country_Region\", how=\"left\")","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-04-08T14:31:47.146654Z","start_time":"2020-04-08T14:31:46.277713Z"},"trusted":true},"cell_type":"code","source":"# Test predictions\npredict_logistic(test)\ntest[\"yhat_xgb_ConfirmedCases\"] = xgb_c_fit.predict(test[x_columns].to_numpy())\ntest[\"yhat_xgb_Fatalities\"] = xgb_f_fit.predict(test[x_columns].to_numpy())\npredict_hybrid(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prepare submission"},{"metadata":{"ExecuteTime":{"end_time":"2020-04-08T14:32:15.22552Z","start_time":"2020-04-08T14:32:15.210518Z"},"trusted":true},"cell_type":"code","source":"submission = test[[\"ForecastId\", \"yhat_hybrid_ConfirmedCases\", \"yhat_hybrid_Fatalities\"]].round(2).rename(\n        columns={\n            \"yhat_hybrid_ConfirmedCases\": \"ConfirmedCases\",\n            \"yhat_hybrid_Fatalities\": \"Fatalities\",\n        }\n    )\nsubmission[\"ConfirmedCases\"] = np.maximum(0, submission[\"ConfirmedCases\"])\nsubmission[\"Fatalities\"] = np.maximum(0, submission[\"Fatalities\"])","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-04-08T14:32:15.875248Z","start_time":"2020-04-08T14:32:15.853676Z"},"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-04-08T13:54:50.359489Z","start_time":"2020-04-08T13:47:07.765Z"},"trusted":true},"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"nbformat":4,"nbformat_minor":4}