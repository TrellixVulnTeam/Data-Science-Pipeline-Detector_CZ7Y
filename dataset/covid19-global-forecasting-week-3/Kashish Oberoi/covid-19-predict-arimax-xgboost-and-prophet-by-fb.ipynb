{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        os.path.join(dirname, filename)\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Link to Competition\nhttps://www.kaggle.com/c/covid19-global-forecasting-week-3/data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Background\n> The White House Office of Science and Technology Policy (OSTP) pulled together a coalition research groups and companies (including Kaggle) to prepare the COVID-19 Open Research Dataset (CORD-19) to attempt to address key open scientific questions on COVID-19. Those questions are drawn from National Academies of Sciences, Engineering, and Medicine’s (NASEM) and the World Health Organization (WHO).\n\n# The Challenge\n> Kaggle is launching a companion COVID-19 forecasting challenges to help answer a subset of the NASEM/WHO questions. While the challenge involves forecasting confirmed cases and fatalities between April 1 and April 30 by region, the primary goal isn't only to produce accurate forecasts. It’s also to identify factors that appear to impact the transmission rate of COVID-19.\n\n> You are encouraged to pull in, curate and share data sources that might be helpful. If you find variables that look like they impact the transmission rate, please share your finding in a notebook.\n\n> As the data becomes available, we will update the leaderboard with live results based on data made available from the Johns Hopkins University Center for Systems Science and Engineering (JHU CSSE).\n\n> We have received support and guidance from health and policy organizations in launching these challenges. We're hopeful the Kaggle community can make valuable contributions to developing a better understanding of factors that impact the transmission of COVID-19.\n\n# Companies and Organizations\n> There is also a call to action for companies and other organizations: If you have datasets that might be useful, please upload them to Kaggle’s dataset platform and reference them in this forum thread. That will make them accessible to those participating in this challenge and a resource to the wider scientific community.\n\n# Acknowledgements\n> JHU CSSE for making the data available to the public. The White House OSTP for pulling together the key open questions. The image comes from the Center for Disease Control.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Data Description\n> In this challenge, you will be predicting the cumulative number of confirmed COVID19 cases in various locations across the world, as well as the number of resulting fatalities, for future dates.\n> \n> We understand this is a serious situation, and in no way want to trivialize the human impact this crisis is causing by predicting fatalities. Our goal is to provide better methods for estimates that can assist medical and governmental institutions to prepare and adjust as pandemics unfold.\n\n# Files\n> train.csv - the training data (you are encouraged to join in many more useful external datasets)\n> test.csv - the dates to predict; there is a week of overlap with the training data for the initial Public leaderboard. Once submissions are paused, the Public leaderboard will update based on last 28 days of predicted data.\n> submission.csv - a sample submission in the correct format; again, predictions should be cumulative\n\n# Data Source\n> This evaluation data for this competition comes from John Hopkins CSSE, which is uninvolved in the competition.\n> See their README for a description of how the data was collected.\n> They are currently updating the data daily.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Importing Modules","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport itertools\nimport statsmodels.api as sm\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom matplotlib import rcParams\nrcParams['figure.figsize'] = 18,8","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading and Preparing Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-3/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-3/test.csv\")\nsubmission = pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-3/submission.csv\")\nmax_val = len(train[train['Country_Region'] == 'Afghanistan'])\ncountry_dict= dict()\nfor itr in range(len(train)):\n    if train.loc[itr]['Country_Region'] not in country_dict.keys():\n        country_dict[train.loc[itr]['Country_Region']]= []\n    else:\n        if len(country_dict[train.loc[itr]['Country_Region']])>=max_val:\n            continue\n    country_dict[train.loc[itr]['Country_Region']].append([[train.loc[itr]['Date']],[train.loc[itr]['ConfirmedCases']],[train.loc[itr]['Fatalities']]])    \n    \ntime_series_dict = dict()\nfor country in country_dict.keys():\n    for case in ['ConfirmedCases','Fatalities']:\n        tsz=train.loc[(train['Country_Region']==country)]\n        tsz=tsz[['Date',case]]\n        x = []\n        for itr in tsz.index:\n            x.append([pd.to_datetime(tsz.loc[itr]['Date']),tsz.loc[itr][case]])\n        tsz = pd.DataFrame(x,columns = ['Date',case])\n        tsz=tsz.set_index('Date')\n        tsz\n        if country not in time_series_dict.keys():\n            time_series_dict[country] = dict()\n        time_series_dict[country][case] = tsz","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualizations","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rank_country = dict()\nfor country in country_dict.keys():\n    rank_country[country]=[max(time_series_dict[country]['ConfirmedCases']['ConfirmedCases']),max(time_series_dict[country]['Fatalities']['Fatalities'])]\nrank_country = sorted(rank_country.items(), key = lambda kv:(kv[1][0],kv[1][1], kv[0]),reverse = True)[:20]\n\nlabels = [y[0] for y in rank_country]\nConfirmedCases = [y[1][0] for y in rank_country]\nFatalities = [y[1][1] for y in rank_country]\n\nx = np.arange(len(labels))  # the label locations\nwidth = 0.35  # the width of the bars\n\nfig, ax = plt.subplots()\nrects1 = ax.bar(x - width/2, ConfirmedCases, width, label='ConfirmedCases',color = '#FFBF00')\nrects2 = ax.bar(x + width/2, Fatalities, width, label='Fatalities',color = 'red')\n\n# Add some text for labels, title and custom x-axis tick labels, etc.\nax.set_ylabel('Number of Cases',fontsize=30, fontweight=20)\nax.set_title('COVID-19',fontsize=30, fontweight=20)\nax.set_xticks(x)\nax.set_xticklabels(labels)\nax.legend()\n\n\ndef autolabel(rects):\n    for rect in rects:\n        height = rect.get_height()\n        ax.annotate('{}'.format(height),\n                    xy=(rect.get_x() + rect.get_width() / 2, height),\n                    xytext=(0, 3),  # 3 points vertical offset\n                    textcoords=\"offset points\",\n                    ha='center', va='bottom')\n\nautolabel(rects1)\nautolabel(rects2)\nfig.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"color_pallete = ['#FFBF00','red']\nfor country in ['India','China','US','Italy','Spain']:\n    case_number = 1\n    for case in ['ConfirmedCases','Fatalities']:\n        plt.subplot(1,2,case_number)\n        plt.title(case, loc='center', fontsize=20, fontweight=10)\n        if case_number==2:\n            plt.ylim(bottom,top)\n        plt.plot(time_series_dict[country][case][:max_val], color=color_pallete[case_number-1], linewidth=3, alpha=1)\n        plt.xlabel('Date', fontsize=20)\n        plt.ylabel('Number of Cases', fontsize=20)\n        if case_number==1:\n            bottom,top = plt.ylim()\n        case_number = case_number + 1\n    plt.suptitle(country, fontsize=30, fontweight=20)\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# SARIMAX Model ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"> Seasonal Autoregressive Integrated Moving Average, SARIMA or Seasonal ARIMA, is an extension of ARIMA that explicitly supports univariate time-series data with a seasonal component. It adds three new hyperparameters to specify the autoregression (AR), differencing (I) and moving average (MA) for the seasonal component of the series, as well as an additional parameter for the period of the seasonality.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction_country_list = dict()\ncount = 0\nfor country in country_dict.keys():\n    prediction_country_list[country] = dict()\n    for case in ['ConfirmedCases','Fatalities']:\n        start = 0\n        end = max_val\n        prediction_country_list[country][case] = []\n        len(time_series_dict[country][case])//max_val\n        for i in range(len(time_series_dict[country][case])//max_val):\n            mod = sm.tsa.statespace.SARIMAX(time_series_dict[country][case].iloc[start:end],\n                                                order=(1,1,1),\n                                                trend = np.flip(np.polyfit(range(0,7),time_series_dict[country][case].iloc[end-7:end],4),1),\n                                                enforce_stationarity=True,\n                                                enforce_invertibility=True)\n            results = mod.fit()\n            pred = results.get_prediction(start=pd.to_datetime('2020-03-26'),end=pd.to_datetime('2020-05-07'),dynamic=True )\n            prediction_country_list[country][case].append(pred.predicted_mean)\n            start = start + max_val\n            end = end + max_val\n    count = count+1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preparing Submission File","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"forecastid = 1\nsubmission_out = []\nfor country in country_dict.keys():\n    for itr in range(len(prediction_country_list[country]['ConfirmedCases'])):\n        for index in prediction_country_list[country]['ConfirmedCases'][itr].index:\n            submission_out.append([forecastid,prediction_country_list[country]['ConfirmedCases'][itr][index],prediction_country_list[country]['Fatalities'][itr][index]])\n            forecastid = forecastid +1\nfor i in range(len(submission_out)):\n    submission_out[i][1] = round(submission_out[i][1])\n    submission_out[i][2] = round(submission_out[i][2])\n# submission_file = pd.DataFrame(submission_out,columns=['ForecastId','ConfirmedCases','Fatalities'])\n# submission_file.to_csv('submission.csv',index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# XGBOOST Regressor","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"> **XGBoost stands for eXtreme Gradient Boosting.**\n> \n> The name xgboost, though, actually refers to the engineering goal to push the limit of computations resources for boosted tree algorithms. Which is the reason why many people use xgboost.\n> This algorithm goes by lots of different names such as gradient boosting, multiple additive regression trees, stochastic gradient boosting or gradient boosting machines.\n> \n> Boosting is an ensemble technique where new models are added to correct the errors made by existing models. Models are added sequentially until no further improvements can be made. A popular example is the AdaBoost algorithm that weights data points that are hard to predict.\n> \n> Gradient boosting is an approach where new models are created that predict the residuals or errors of prior models and then added together to make the final prediction. It is called gradient boosting because it uses a gradient descent algorithm to minimize the loss when adding new models.\n> \n> This approach supports both regression and classification predictive modeling problems.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\nfrom xgboost import plot_importance, plot_tree\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-3/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-3/test.csv\")\nsubmission = pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-3/submission.csv\")\ntrain.Date = pd.to_datetime(train.Date)\ntest.Date = pd.to_datetime(test.Date)\ntrain['Date'] = train['Date'].dt.strftime(\"%d%m\").astype(int)\ntest['Date'] = test['Date'].dt.strftime(\"%d%m\").astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"country_dict= dict()\nprovince_list = []\nfor itr in range(len(train)):\n    if train.loc[itr]['Country_Region'] not in country_dict.keys():\n        country_dict[train.loc[itr]['Country_Region']]= dict()\n    if str(train.iloc[itr]['Province_State']) != 'nan':\n        province_list.append(train.iloc[itr]['Province_State'])\n        if train.loc[itr]['Province_State'] not in country_dict[train.loc[itr]['Country_Region']].keys():\n            country_dict[train.loc[itr]['Country_Region']][train.loc[itr]['Province_State']] = dict()\n            country_dict[train.loc[itr]['Country_Region']][train.loc[itr]['Province_State']]['ConfirmedCases'] = []\n            country_dict[train.loc[itr]['Country_Region']][train.loc[itr]['Province_State']]['Fatalities'] = []\n        country_dict[train.loc[itr]['Country_Region']][train.loc[itr]['Province_State']]['ConfirmedCases'].append([train.loc[itr]['Date'],train.loc[itr]['ConfirmedCases']])\n        country_dict[train.loc[itr]['Country_Region']][train.loc[itr]['Province_State']]['Fatalities'].append([train.loc[itr]['Date'],train.loc[itr]['Fatalities']])\n    if str(train.loc[itr]['Province_State']) == 'nan':\n        province_list.append(train.iloc[itr]['Country_Region'])\n        if train.loc[itr]['Country_Region'] not in country_dict[train.loc[itr]['Country_Region']].keys():\n            country_dict[train.loc[itr]['Country_Region']][train.loc[itr]['Country_Region']] = dict()\n            country_dict[train.loc[itr]['Country_Region']][train.loc[itr]['Country_Region']]['ConfirmedCases'] = []\n            country_dict[train.loc[itr]['Country_Region']][train.loc[itr]['Country_Region']]['Fatalities'] = []\n        country_dict[train.loc[itr]['Country_Region']][train.loc[itr]['Country_Region']]['ConfirmedCases'].append([train.loc[itr]['Date'],train.loc[itr]['ConfirmedCases']])\n        country_dict[train.loc[itr]['Country_Region']][train.loc[itr]['Country_Region']]['Fatalities'].append([train.loc[itr]['Date'],train.loc[itr]['Fatalities']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dates = []\nfor itr in range(len(test)):\n    if test.iloc[itr]['Country_Region'] == 'Afghanistan':\n        test_dates.append(test.iloc[itr]['Date'])\ntest_dates = np.array(test_dates)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_dict = dict()\nfor country in country_dict.keys():\n    pred_dict[country] = dict()\n    for province in country_dict[country].keys():\n        train_x_ConfirmedCases = train_x_Fatalities = pd.DataFrame(country_dict[country][province]['ConfirmedCases'])[0].values.reshape(-1,1)\n        train_y_ConfirmedCases = pd.DataFrame(country_dict[country][province]['ConfirmedCases'])[1].values\n        train_y_Fatalities = pd.DataFrame(country_dict[country][province]['Fatalities'])[1].values\n        test_x_ConfirmedCases = test_x_Fatalities = test_dates.reshape(-1,1)\n        pred_y_ConfirmedCases = xgb.XGBRegressor(n_estimators=500).fit(train_x_ConfirmedCases, train_y_ConfirmedCases).predict(test_x_ConfirmedCases)\n        pred_y_Fatalities = xgb.XGBRegressor(n_estimators=500).fit(train_x_Fatalities, train_y_Fatalities).predict(test_x_Fatalities)\n        pred_dict[country][province] = dict()\n        pred_dict[country][province]['ConfirmedCases'] = pred_y_ConfirmedCases\n        pred_dict[country][province]['Fatalities'] = pred_y_Fatalities","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ForecastId = 1\nsubmission_out = []\nfor country in country_dict.keys():\n    for province in country_dict[country].keys():\n        for i in range(len(pred_dict[country][province]['ConfirmedCases'])):\n            submission_out.append([ForecastId,pred_dict[country][province]['ConfirmedCases'][i],pred_dict[country][province]['Fatalities'][i]])\n            ForecastId = ForecastId + 1\n# submission_file = pd.DataFrame(submission_out,columns=['ForecastId','ConfirmedCases','Fatalities'])\n# submission_file.to_csv('submission.csv',index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Improvement in XGBoost","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"> Adding Date as feature to XGBOOST as integer","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-3/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-3/test.csv\")\nsubmission = pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-3/submission.csv\")\ntrain.Date = pd.to_datetime(train.Date)\ntest.Date = pd.to_datetime(test.Date)\ntrain['Month'] = train['Date'].dt.strftime(\"%m\").astype(int)\ntrain['Date'] = train['Date'].dt.strftime(\"%d\").astype(int)\ntest['Month'] = test['Date'].dt.strftime(\"%m\").astype(int)\ntest['Date'] = test['Date'].dt.strftime(\"%d\").astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# use pd.concat to join the new columns with your original dataframe\ndf = pd.concat([train,pd.get_dummies(train['Country_Region'], prefix='Country_Region')],axis=1)\ndf2 = pd.concat([df,pd.get_dummies(df['Province_State'], prefix='Province_State')],axis=1)\ndf2.drop(['Country_Region','Province_State'],axis=1, inplace=True)\ntrain_y_ConfirmedCases,train_y_Fatalities = df2['ConfirmedCases'],df2['Fatalities']\ndf2.drop(['Id','ConfirmedCases','Fatalities'],axis=1, inplace=True)\ntrain_x = df2\ndf3 = pd.concat([test,pd.get_dummies(test['Country_Region'], prefix='Country_Region')],axis=1)\ndf4 = pd.concat([df3,pd.get_dummies(df3['Province_State'], prefix='Province_State')],axis=1)\ndf4.drop(['ForecastId','Country_Region','Province_State'],axis=1, inplace=True)\ntest_x = df4\npred_y_ConfirmedCases = xgb.XGBRegressor(n_estimators=1500).fit(train_x, train_y_ConfirmedCases).predict(test_x)\npred_y_Fatalities = xgb.XGBRegressor(n_estimators=1500).fit(train_x, train_y_Fatalities).predict(test_x)\nfor i in range(len(pred_y_Fatalities)):\n    pred_y_Fatalities[i] = round(pred_y_Fatalities[i])\n    pred_y_ConfirmedCases[i] = round(pred_y_ConfirmedCases[i])\n# submission_file = pd.DataFrame(list(zip(submission.ForecastId,pred_y_ConfirmedCases,pred_y_Fatalities)),columns=['ForecastId','ConfirmedCases','Fatalities'])\n# submission_file.to_csv('submission.csv',index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prophet by FACEBOOK\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"> Prophet is a procedure for forecasting time series data based on an additive model where non-linear trends are fit with yearly, weekly, and daily seasonality, plus holiday effects. It works best with time series that have strong seasonal effects and several seasons of historical data. Prophet is robust to missing data and shifts in the trend, and typically handles outliers well.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-3/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-3/test.csv\")\nsubmission = pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-3/submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"country_dict= dict()\nprovince_list = []\nfor itr in range(len(train)):\n    if train.loc[itr]['Country_Region'] not in country_dict.keys():\n        country_dict[train.loc[itr]['Country_Region']]= dict()\n    if str(train.iloc[itr]['Province_State']) != 'nan':\n        province_list.append(train.iloc[itr]['Province_State'])\n        if train.loc[itr]['Province_State'] not in country_dict[train.loc[itr]['Country_Region']].keys():\n            country_dict[train.loc[itr]['Country_Region']][train.loc[itr]['Province_State']] = dict()\n            country_dict[train.loc[itr]['Country_Region']][train.loc[itr]['Province_State']]['ConfirmedCases'] = []\n            country_dict[train.loc[itr]['Country_Region']][train.loc[itr]['Province_State']]['Fatalities'] = []\n        country_dict[train.loc[itr]['Country_Region']][train.loc[itr]['Province_State']]['ConfirmedCases'].append([train.loc[itr]['Date'],train.loc[itr]['ConfirmedCases']])\n        country_dict[train.loc[itr]['Country_Region']][train.loc[itr]['Province_State']]['Fatalities'].append([train.loc[itr]['Date'],train.loc[itr]['Fatalities']])\n    if str(train.loc[itr]['Province_State']) == 'nan':\n        province_list.append(train.iloc[itr]['Country_Region'])\n        if train.loc[itr]['Country_Region'] not in country_dict[train.loc[itr]['Country_Region']].keys():\n            country_dict[train.loc[itr]['Country_Region']][train.loc[itr]['Country_Region']] = dict()\n            country_dict[train.loc[itr]['Country_Region']][train.loc[itr]['Country_Region']]['ConfirmedCases'] = []\n            country_dict[train.loc[itr]['Country_Region']][train.loc[itr]['Country_Region']]['Fatalities'] = []\n        country_dict[train.loc[itr]['Country_Region']][train.loc[itr]['Country_Region']]['ConfirmedCases'].append([train.loc[itr]['Date'],train.loc[itr]['ConfirmedCases']])\n        country_dict[train.loc[itr]['Country_Region']][train.loc[itr]['Country_Region']]['Fatalities'].append([train.loc[itr]['Date'],train.loc[itr]['Fatalities']])\nfor country in country_dict.keys():\n    for province in country_dict[country].keys():\n        for case in country_dict[country][province].keys():\n            for itr in range(len(country_dict[country][province][case])):\n                country_dict[country][province][case][itr][0] = pd.to_datetime(country_dict[country][province][case][itr][0])\nfor country in country_dict.keys():\n    for province in country_dict[country].keys():\n        for case in country_dict[country][province].keys():\n            country_dict[country][province][case] = pd.DataFrame(country_dict[country][province][case],columns=['ds','y'])\ntest_dates = pd.DataFrame(set(test['Date']),columns=['ds'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom fbprophet import Prophet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for country in country_dict.keys():\n    for province in country_dict[country].keys():\n        for case in country_dict[country][province].keys():\n            m = Prophet()\n            m.fit(country_dict[country][province][case])\n            forecast = m.predict(test_dates)\n            country_dict[country][province][case] = forecast[['ds','yhat']]\nsubmission_list = []\nforecastId = 1\nfor country in country_dict.keys():\n    for province in country_dict[country].keys():\n        for itr in range(len(country_dict[country][province][case])):\n            submission_list.append([forecastId,round(country_dict[country][province]['ConfirmedCases'].iloc[itr]['yhat']),round(country_dict[country][province]['Fatalities'].iloc[itr]['yhat'])])\n            forecastId = forecastId+1\n# submission_file = pd.DataFrame(submission_list,columns =['ForecastId','ConfirmedCases','Fatalities'])\n# submission_file.to_csv('submission.csv',index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}