{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\ntrain = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-3/train.csv', parse_dates=['Date'])\ntest = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-3/test.csv',parse_dates=['Date'])\nsubmission = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-3/submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape\n#train.date.drop_duplicates()\n#test.date.drop_duplicates()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns = train.columns.str.lower()\ntest.columns = test.columns.str.lower()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.fillna(' ',inplace=True)\ntest.fillna(' ', inplace=True)\ntrain_id = train.pop('id')\ntest_id = test.pop('forecastid')\n\ntrain['cp'] = train['country_region'] + train['province_state']\ntest['cp'] = test['country_region'] + test['province_state']\n\ntrain.drop(['province_state','country_region'], axis=1, inplace=True)\ntest.drop(['province_state','country_region'], axis =1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.cp.nunique(), test.cp.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame()\ndef create_time_feat(data):\n    df['date']= data['date']\n    df['hour']=df['date'].dt.hour\n    df['weekofyear']=df['date'].dt.weekofyear\n    df['quarter'] =df['date'].dt.quarter\n    df['month'] = df['date'].dt.month\n    df['dayofyear']=df['date'].dt.dayofyear\n    \n    x=df[['hour','weekofyear','quarter','month','dayofyear']]\n    \n    return x\n\ncr_tr = create_time_feat(train)\ncr_te = create_time_feat(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.concat([train,cr_tr], axis=1)\ntest_df = pd.concat([test, cr_te], axis =1)\ntrain_df.shape, test_df.shape, train_df.cp.nunique(), test_df.cp.nunique(), test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nle=LabelEncoder()\ntrain_df['cp_le']=le.fit_transform(train_df['cp'])\ntest_df['cp_le']=le.transform(test_df['cp'])\n\ntrain_df.drop(['cp'], axis=1, inplace=True)\ntest_df.drop(['cp'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\ncl_new=[]\nfor i in train_df['cp_le'].drop_duplicates():\n    cl_new.append(train_df[train_df['cp_le']==i])\n    \nimport matplotlib.pyplot as plt\nplt.figure(figsize=(12,8))\n\nfor i in cl_new:\n    df=i.confirmedcases.astype('int64').tolist()\n\n    plt.plot(i.date, df)\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\n#  Use stepwise to find opt-model\nimport pmdarima as pm\n\nfor i in cl_new:\n    df=i.confirmedcases.astype('int64').tolist()\n    \n    scmodel = pm.auto_arima(df, start_p=1,start_q=1, test='adf', max_p=3, max_q=3, m=12, start_P=0, seasonal=True,\n                              D=1, trace=True, error_action='ignore', suppress_warnings=True, stepwise=True) \n   # scmodel.summary()\n\n        \n#    data = i.fatalities.astype('int64').tolist()\n#    sfmodel = pm.auto_arima(data,star_p=1,start_q=1, test='adf', max_p=3, max_q=3, m=12, start_P=0, seasonal=True,\n#                              d=None, D=1, trace=True, error_action='ignore', suppress_warnings=True, stepwise=True) \n#    sfmodel.summary()\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\n\n\nsubmit_confirmed = []\nsubmit_fatal = []\n\nfor i in cl_new:\n    #confirmed cases predict\n    data = i.confirmedcases.astype('int64').tolist()\n    try:\n        model_c = SARIMAX(data, order=(1,1,0), seasonal_order=(0,1,1,12),measurement_error=True )\n        model_c_fit = model_c.fit(disp=False)\n        predicted = model_c_fit.predict(len(data), len(data)+32)\n        new = np.concatenate((np.array(data), np.array([int(num) for num in predicted])), axis=0)\n        submit_confirmed.extend(list(new[-43:]))\n\n    except:\n        submit_confirmed.extend(list(data[-10:-1]))\n        for j in range(32):\n            submit_confirmed.append(data[-1]*2)\n    \n    #fatalities predict\n    data = i.fatalities.astype('int64').tolist()\n    try:\n        model_f = SARIMAX(data, order=(1,1,0), seasonal_order=(0,1,0,12),measurement_error=True )\n        model_f_fit = model_f.fit(disp=False)\n        predicted = model_f_fit.predict(len(data), len(data)+32)\n        new = np.concatenate((np.array(data), np.array([int(num) for num in predicted])), axis=0)\n        submit_fatal.extend(list(new[-43:]))\n\n    except:\n        submit_fatal.extend(list(data[-10:-1]))\n        for j in range(32):\n            submit_fatal.append(data[-1]*2)\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#df_submit = pd.concat([pd.Series(np.arange(1,1+len(submit_confirmed))),pd.Series(submit_confirmed), pd.Series(submit_fatal)], axis=1)\n#df_submit.rename(columns={0:'ForecastId', 1:'ConfirmedCases', 2:'Fatalities'}, inplace=True)\n#df_submit.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using LSTM to see if this brings better performance\ntrain_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_df.head()\ntest_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_date_feat(data, cf, ft):\n    for d in data['date'].drop_duplicates():\n        for i in data['cp_le'].drop_duplicates():\n            org_mask = (data['date']==d) & (data['cp_le']==i)\n            for lag in range(1,15):\n                mask_loc = (data['date']==(d-pd.Timedelta(days=lag))) & (data['cp_le']==i)\n                \n                try:\n                    data.loc[org_mask, 'cf_' + str(lag)]=data.loc[mask_loc, cf].values\n                    data.loc[org_mask, 'ft_' + str(lag)]=data.loc[mask_loc, ft].values\n                \n                except:\n                    data.loc[org_mask, 'cf_' + str(lag)]=0.0\n                    data.loc[org_mask, 'ft_' + str(lag)]=0.0\n\ncreate_date_feat(train_df,'confirmedcases','fatalities')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.tail(50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ncf_feat = ['cp_le', 'weekofyear','quarter','month','dayofyear','cf_1', 'cf_2', 'cf_3', \n          'cf_4', 'cf_5', 'cf_6', 'cf_7', 'cf_8', 'cf_9','cf_10', 'cf_11', 'cf_12', \n          'cf_13', 'cf_14']\nft_feat = ['cp_le', 'weekofyear','quarter','month','dayofyear','ft_1', 'ft_2', 'ft_3', \n          'ft_4', 'ft_5', 'ft_6', 'ft_7', 'ft_8', 'ft_9','ft_10', 'ft_11', 'ft_12', \n          'ft_13', 'ft_14']\n\ntrain_x_cf = train_df[cf_feat]\nprint(train_x_cf.shape)\ntrain_x_ft = train_df[ft_feat]\nprint(train_x_ft.shape)\ntrain_x_cf_reshape = train_x_cf.values.reshape(train_x_cf.shape[0],1,train_x_cf.shape[1])\ntrain_x_ft_reshape = train_x_ft.values.reshape(train_x_ft.shape[0],1,train_x_ft.shape[1])\n\ntrain_y_cf = train_df['confirmedcases']\ntrain_y_ft = train_df['fatalities']\n\ntrain_y_cf_reshape = train_y_cf.values.reshape(-1,1)\ntrain_y_ft_reshape = train_y_ft.values.reshape(-1,1)\n\ntr_x_cf, val_x_cf, tr_y_cf, val_y_cf = train_test_split(train_x_cf_reshape, train_y_cf_reshape, test_size=0.2, random_state=0)\ntr_x_ft, val_x_ft, tr_y_ft, val_y_ft = train_test_split(train_x_ft_reshape, train_y_ft_reshape, test_size=0.2, random_state=0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x_cf_reshape.shape, train_y_cf_reshape.shape, train_y_ft_reshape.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras.backend as K\n\ndef rmsle(pred,true):\n    assert pred.shape[0]==true.shape[0]\n    return K.sqrt(K.mean(K.square(K.log(pred+1) - K.log(true+1))))\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\n\nes = EarlyStopping(monitor='val_loss', min_delta = 0, verbose=0, patience=10, mode='auto')\nmc_cf = ModelCheckpoint('model_cf.h5', monitor='val_loss', verbose=0, save_best_only=True)\nmc_ft = ModelCheckpoint('model_ft.h5', monitor='val_loss', verbose=0, save_best_only=True)\n\ndef lstm_model(hidden_nodes, second_dim, third_dim):\n    model = Sequential([LSTM(hidden_nodes, input_shape=(second_dim, third_dim), activation='relu'),\n                        Dense(64, activation='relu'),\n                        Dense(32, activation='relu'),\n                        Dense(1, activation='relu')])\n    model.compile(loss=rmsle, optimizer = 'adam')\n    \n    return model\n\nmodel_cf = lstm_model(10, tr_x_cf.shape[1], tr_x_cf.shape[2])\nmodel_ft = lstm_model(10, tr_x_ft.shape[1], tr_x_ft.shape[2])\n\nhistory_cf = model_cf.fit(tr_x_cf, tr_y_cf, epochs=200, batch_size=512, validation_data=(val_x_cf,val_y_cf), callbacks=[es,mc_cf])\nhistory_ft = model_ft.fit(tr_x_ft, tr_y_ft, epochs=200, batch_size=512, validation_data=(val_x_ft,val_y_ft), callbacks=[es,mc_ft])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history_ft.history['loss']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(8,6))\nplt.plot(history_cf.history['loss'], label='Train')\nplt.plot(history_cf.history['val_loss'], label='Test')\nplt.title(\"CF Model Loss\")\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend(loc=\"upper left\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,6))\nplt.plot(history_ft.history['loss'], label='Train')\nplt.plot(history_ft.history['val_loss'], label='Test')\nplt.title('FT Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epochs')\nplt.legend(loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# formatting Test data & predicting\n\nfeat = ['confirmedcases','fatalities','cf_1', 'ft_1', 'cf_2', 'ft_2', 'cf_3', 'ft_3', \n        'cf_4', 'ft_4', 'cf_5', 'ft_5', 'cf_6', 'ft_6', 'cf_7', 'ft_7', 'cf_8', 'ft_8',\n        'cf_9', 'ft_9', 'cf_10', 'ft_10', 'cf_11', 'ft_11', 'cf_12', 'ft_12', 'cf_13', 'ft_13',\n        'cf_14', 'ft_14']\nc_feat = ['cp_le', 'weekofyear','quarter','month','dayofyear','cf_1', 'cf_2', 'cf_3', \n          'cf_4', 'cf_5', 'cf_6', 'cf_7', 'cf_8', 'cf_9','cf_10', 'cf_11', 'cf_12', \n          'cf_13', 'cf_14']\nf_feat =  ['cp_le', 'weekofyear','quarter','month','dayofyear','ft_1', 'ft_2', 'ft_3', \n          'ft_4', 'ft_5', 'ft_6', 'ft_7', 'ft_8', 'ft_9','ft_10', 'ft_11', 'ft_12', \n          'ft_13', 'ft_14']\ntot_feat = ['cp_le', 'weekofyear','quarter','month','dayofyear','cf_1', 'ft_1', 'cf_2', 'ft_2', 'cf_3', 'ft_3', \n        'cf_4', 'ft_4', 'cf_5', 'ft_5', 'cf_6', 'ft_6', 'cf_7', 'ft_7', 'cf_8', 'ft_8',\n        'cf_9', 'ft_9', 'cf_10', 'ft_10', 'cf_11', 'ft_11', 'cf_12', 'ft_12', 'cf_13', 'ft_13',\n        'cf_14', 'ft_14']\n\ntest_new = test_df.copy().join(pd.DataFrame(columns=feat))\ntest_mask = (test_df['date'] <= train_df['date'].max())\ntrain_mask = (train_df['date'] >= test_df['date'].min())\ntest_new.loc[test_mask,feat] = train_df.loc[train_mask, feat].values\nfuture_df = pd.date_range(start = train_df['date'].max()+pd.Timedelta(days=1),end=test_df['date'].max(), freq='1D')\n\ndef create_add_trend_pred(data, cf, ft):\n    for d in future_df:\n        for i in data['cp_le'].drop_duplicates():\n            org_mask = (data['date']==d) & (data['cp_le']==i)\n            for lag in range(1,15):\n                mask_loc = (data['date']==(d-pd.Timedelta(days=lag))) & (data['cp_le']==i)\n                \n                try:\n                    data.loc[org_mask, 'cf_' + str(lag)]=data.loc[mask_loc,cf].values\n                    data.loc[org_mask, 'ft_' + str(lag)]=data.loc[mask_loc,ft].values\n                    \n                except:\n                    data.loc[org_mask, 'cf_' + str(lag)]=0.0\n                    data.loc[org_mask, 'ft_' + str(lag)]=0.0\n            \n            test_x = data.loc[org_mask,tot_feat]\n            \n            test_x_cf = test_x[c_feat]\n            test_x_cf = test_x_cf.to_numpy().reshape(1,-1)\n            test_x_cf_reshape = test_x_cf.reshape(test_x_cf.shape[0],1,test_x_cf.shape[1])\n            \n            test_x_ft = test_x[f_feat]\n            test_x_ft = test_x_ft.to_numpy().reshape(1,-1)\n            test_x_ft_reshape = test_x_ft.reshape(test_x_ft.shape[0],1,test_x_ft.shape[1])\n            data.loc[org_mask, cf] = model_cf.predict(test_x_cf_reshape)\n            data.loc[org_mask, ft] = model_ft.predict(test_x_ft_reshape)\n\ncreate_add_trend_pred(test_new, 'confirmedcases', 'fatalities')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_new.head(60)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_pred = pd.DataFrame({'ForecastId': test_id, 'ConfirmedCases':test_new['confirmedcases'],'Fatalities':test_new['fatalities']})\nsub_pred.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_pred.head(60)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}