{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os, gc, pickle, copy, datetime, warnings\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nfrom sklearn import metrics\npd.set_option('display.max_columns', 100)\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df_train = pd.read_csv(\"../input/covid19-global-forecasting-week-3/train.csv\")\nprint(df_train.shape)\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = pd.read_csv(\"../input/covid19-global-forecasting-week-3/test.csv\")\nprint(df_test.shape)\ndf_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# concat train and test\ndf_traintest = pd.concat([df_train, df_test])\nprint(df_train.shape, df_test.shape, df_traintest.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# process date\ndf_traintest['Date'] = pd.to_datetime(df_traintest['Date'])\ndf_traintest['day'] = df_traintest['Date'].apply(lambda x: x.dayofyear).astype(np.int16)\ndf_traintest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"day_before_valid = 71+7 # 3-18 day  before of validation\nday_before_public = 78+7 # 3-25 last day of train\nday_before_private = df_traintest['day'][pd.isna(df_traintest['ForecastId'])].max() # last day of train\nprint(df_traintest['Date'][df_traintest['day']==day_before_valid].values[0])\nprint(df_traintest['Date'][df_traintest['day']==day_before_public].values[0])\nprint(df_traintest['Date'][df_traintest['day']==day_before_private].values[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# concat Country/Region and Province/State\ndef func(x):\n    try:\n        x_new = x['Country_Region'] + \"/\" + x['Province_State']\n    except:\n        x_new = x['Country_Region']\n    return x_new\n        \ndf_traintest['place_id'] = df_traintest.apply(lambda x: func(x), axis=1)\ndf_traintest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_traintest[(df_traintest['day']>=day_before_public-3) & (df_traintest['place_id']=='China/Hubei')].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# concat lat and long\ndf_latlong = pd.read_csv(\"../input/smokingstats/df_Latlong.csv\")\ndf_latlong.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# concat Country/Region and Province/State\ndef func(x):\n    try:\n        x_new = x['Country/Region'] + \"/\" + x['Province/State']\n    except:\n        x_new = x['Country/Region']\n    return x_new\n        \ndf_latlong['place_id'] = df_latlong.apply(lambda x: func(x), axis=1)\ndf_latlong = df_latlong[df_latlong['place_id'].duplicated()==False]\ndf_latlong.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_traintest = pd.merge(df_traintest, df_latlong[['place_id', 'Lat', 'Long']], on='place_id', how='left')\ndf_traintest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(pd.isna(df_traintest['Lat']).sum()) # count Nan\ndf_traintest[pd.isna(df_traintest['Lat'])].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get place list\nplaces = np.sort(df_traintest['place_id'].unique())\nprint(len(places))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calc cases, fatalities per day\ndf_traintest2 = copy.deepcopy(df_traintest)\ndf_traintest2['cases/day'] = 0\ndf_traintest2['fatal/day'] = 0\ntmp_list = np.zeros(len(df_traintest2))\nfor place in places:\n    tmp = df_traintest2['ConfirmedCases'][df_traintest2['place_id']==place].values\n    tmp[1:] -= tmp[:-1]\n    df_traintest2['cases/day'][df_traintest2['place_id']==place] = tmp\n    tmp = df_traintest2['Fatalities'][df_traintest2['place_id']==place].values\n    tmp[1:] -= tmp[:-1]\n    df_traintest2['fatal/day'][df_traintest2['place_id']==place] = tmp\nprint(df_traintest2.shape)\ndf_traintest2[df_traintest2['place_id']=='China/Hubei'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def do_aggregation(df, col, mean_range):\n    df_new = copy.deepcopy(df)\n    col_new = '{}_({}-{})'.format(col, mean_range[0], mean_range[1])\n    df_new[col_new] = 0\n    tmp = df_new[col].rolling(mean_range[1]-mean_range[0]+1).mean()\n    df_new[col_new][mean_range[0]:] = tmp[:-(mean_range[0])]\n    df_new[col_new][pd.isna(df_new[col_new])] = 0\n    return df_new[[col_new]].reset_index(drop=True)\n\ndef do_aggregations(df):\n    df = pd.concat([df, do_aggregation(df, 'cases/day', [1,1]).reset_index(drop=True)], axis=1)\n    df = pd.concat([df, do_aggregation(df, 'cases/day', [1,7]).reset_index(drop=True)], axis=1)\n    df = pd.concat([df, do_aggregation(df, 'cases/day', [8,14]).reset_index(drop=True)], axis=1)\n    df = pd.concat([df, do_aggregation(df, 'cases/day', [15,21]).reset_index(drop=True)], axis=1)\n    df = pd.concat([df, do_aggregation(df, 'fatal/day', [1,1]).reset_index(drop=True)], axis=1)\n    df = pd.concat([df, do_aggregation(df, 'fatal/day', [1,7]).reset_index(drop=True)], axis=1)\n    df = pd.concat([df, do_aggregation(df, 'fatal/day', [8,14]).reset_index(drop=True)], axis=1)\n    df = pd.concat([df, do_aggregation(df, 'fatal/day', [15,21]).reset_index(drop=True)], axis=1)\n    for threshold in [1, 10, 100]:\n        days_under_threshold = (df['ConfirmedCases']<threshold).sum()\n        tmp = df['day'].values - 22 - days_under_threshold\n        tmp[tmp<=0] = 0\n        df['days_since_{}cases'.format(threshold)] = tmp\n            \n    for threshold in [1, 10, 100]:\n        days_under_threshold = (df['Fatalities']<threshold).sum()\n        tmp = df['day'].values - 22 - days_under_threshold\n        tmp[tmp<=0] = 0\n        df['days_since_{}fatal'.format(threshold)] = tmp\n    \n    # process China/Hubei\n    if df['place_id'][0]=='China/Hubei':\n        df['days_since_1cases'] += 35 # 2019/12/8\n        df['days_since_10cases'] += 35-13 # 2019/12/8-2020/1/2 assume 2019/12/8+13\n        df['days_since_100cases'] += 4 # 2020/1/18\n        df['days_since_1fatal'] += 13 # 2020/1/9\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_traintest3 = []\nfor place in places[:]:\n    df_tmp = df_traintest2[df_traintest2['place_id']==place].reset_index(drop=True)\n    df_tmp = do_aggregations(df_tmp)\n    df_traintest3.append(df_tmp)\ndf_traintest3 = pd.concat(df_traintest3).reset_index(drop=True)\ndf_traintest3[df_traintest3['place_id']=='China/Hubei'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# add Smoking rate per country\n# data of smoking rate is obtained from https://ourworldindata.org/smoking\ndf_smoking = pd.read_csv(\"../input/smokingstats/share-of-adults-who-smoke.csv\")\nprint(np.sort(df_smoking['Entity'].unique())[:10])\ndf_smoking.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# extract newest data\ndf_smoking_recent = df_smoking.sort_values('Year', ascending=False).reset_index(drop=True)\ndf_smoking_recent = df_smoking_recent[df_smoking_recent['Entity'].duplicated()==False]\ndf_smoking_recent['Country_Region'] = df_smoking_recent['Entity']\ndf_smoking_recent['SmokingRate'] = df_smoking_recent['Smoking prevalence, total (ages 15+) (% of adults)']\ndf_smoking_recent.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# merge\ndf_traintest4 = pd.merge(df_traintest3, df_smoking_recent[['Country_Region', 'SmokingRate']], on='Country_Region', how='left')\nprint(df_traintest4.shape)\ndf_traintest4.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fill na with world smoking rate\nSmokingRate = df_smoking_recent['SmokingRate'][df_smoking_recent['Entity']=='World'].values[0]\nprint(\"Smoking rate of the world: {:.6f}\".format(SmokingRate))\ndf_traintest4['SmokingRate'][pd.isna(df_traintest4['SmokingRate'])] = SmokingRate\ndf_traintest4.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# add data from World Economic Outlook Database\n# https://www.imf.org/external/pubs/ft/weo/2017/01/weodata/index.aspx\ndf_weo = pd.read_csv(\"../input/smokingstats/WEO.csv\")\ndf_weo.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_weo['Subject Descriptor'].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subs  = df_weo['Subject Descriptor'].unique()[:-1]\ndf_weo_agg = df_weo[['Country']][df_weo['Country'].duplicated()==False].reset_index(drop=True)\nfor sub in subs[:]:\n    df_tmp = df_weo[['Country', '2019']][df_weo['Subject Descriptor']==sub].reset_index(drop=True)\n    df_tmp = df_tmp[df_tmp['Country'].duplicated()==False].reset_index(drop=True)\n    df_tmp.columns = ['Country', sub]\n    df_weo_agg = df_weo_agg.merge(df_tmp, on='Country', how='left')\ndf_weo_agg.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in df_weo_agg.columns]\ndf_weo_agg.columns\ndf_weo_agg['Country_Region'] = df_weo_agg['Country']\ndf_weo_agg.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# merge\ndf_traintest5 = pd.merge(df_traintest4, df_weo_agg, on='Country_Region', how='left')\nprint(df_traintest5.shape)\ndf_traintest5.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# add Life expectancy\n# Life expectancy at birth obtained from http://hdr.undp.org/en/data\ndf_life = pd.read_csv(\"../input/smokingstats/Life expectancy at birth.csv\")\ntmp = df_life.iloc[:,1].values.tolist()\ndf_life = df_life[['Country', '2018']]\ndef func(x):\n    x_new = 0\n    try:\n        x_new = float(x.replace(\",\", \"\"))\n    except:\n#         print(x)\n        x_new = np.nan\n    return x_new\n    \ndf_life['2018'] = df_life['2018'].apply(lambda x: func(x))\ndf_life.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_life = df_life[['Country', '2018']]\ndf_life.columns = ['Country_Region', 'LifeExpectancy']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# merge\ndf_traintest6 = pd.merge(df_traintest5, df_life, on='Country_Region', how='left')\nprint(len(df_traintest6))\ndf_traintest6.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# add additional info from countryinfo dataset\ndf_country = pd.read_csv(\"../input/countryinfo/covid19countryinfo.csv\")\ndf_country.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_country['Country_Region'] = df_country['country']\ndf_country = df_country[df_country['country'].duplicated()==False]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_country[df_country['country'].duplicated()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_traintest7 = pd.merge(df_traintest6, \n                         df_country.drop(['tests', 'testpop', 'country'], axis=1), \n                         on=['Country_Region',], how='left')\nprint(df_traintest7.shape)\ndf_traintest7.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode_label(df, col, freq_limit=0):\n    df[col][pd.isna(df[col])] = 'nan'\n    tmp = df[col].value_counts()\n    cols = tmp.index.values\n    freq = tmp.values\n    num_cols = (freq>=freq_limit).sum()\n    print(\"col: {}, num_cat: {}, num_reduced: {}\".format(col, len(cols), num_cols))\n\n    col_new = '{}_le'.format(col)\n    df_new = pd.DataFrame(np.ones(len(df), np.int16)*(num_cols-1), columns=[col_new])\n    for i, item in enumerate(cols[:num_cols]):\n        df_new[col_new][df[col]==item] = i\n\n    return df_new\n\ndef get_df_le(df, col_index, col_cat):\n    df_new = df[[col_index]]\n    for col in col_cat:\n        df_tmp = encode_label(df, col)\n        df_new = pd.concat([df_new, df_tmp], axis=1)\n    return df_new\n\ndf_traintest7['id'] = np.arange(len(df_traintest7))\ndf_le = get_df_le(df_traintest7, 'id', ['Country_Region', 'Province_State'])\ndf_traintest8 = pd.merge(df_traintest7, df_le, on='id', how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_traintest8['cases/day'] = df_traintest8['cases/day'].astype(np.float)\ndf_traintest8['fatal/day'] = df_traintest8['fatal/day'].astype(np.float)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# covert object type to float\ndef func(x):\n    x_new = 0\n    try:\n        x_new = float(x.replace(\",\", \"\"))\n    except:\n#         print(x)\n        x_new = np.nan\n    return x_new\ncols = [\n    'Gross_domestic_product__constant_prices', \n    'Gross_domestic_product__current_prices', \n    'Gross_domestic_product__deflator', \n    'Gross_domestic_product_per_capita__constant_prices', \n    'Gross_domestic_product_per_capita__current_prices', \n    'Output_gap_in_percent_of_potential_GDP', \n    'Gross_domestic_product_based_on_purchasing_power_parity__PPP__valuation_of_country_GDP', \n    'Gross_domestic_product_based_on_purchasing_power_parity__PPP__per_capita_GDP', \n    'Gross_domestic_product_based_on_purchasing_power_parity__PPP__share_of_world_total', \n    'Implied_PPP_conversion_rate', 'Total_investment', \n    'Gross_national_savings', 'Inflation__average_consumer_prices', \n    'Inflation__end_of_period_consumer_prices', \n    'Six_month_London_interbank_offered_rate__LIBOR_', \n    'Volume_of_imports_of_goods_and_services', \n    'Volume_of_Imports_of_goods', \n    'Volume_of_exports_of_goods_and_services', \n    'Volume_of_exports_of_goods', 'Unemployment_rate', 'Employment', 'Population', \n    'General_government_revenue', 'General_government_total_expenditure', \n    'General_government_net_lending_borrowing', 'General_government_structural_balance', \n    'General_government_primary_net_lending_borrowing', 'General_government_net_debt', \n    'General_government_gross_debt', 'Gross_domestic_product_corresponding_to_fiscal_year__current_prices', \n    'Current_account_balance', 'pop'\n]\nfor col in cols:\n    df_traintest8[col] = df_traintest8[col].apply(lambda x: func(x))  \nprint(df_traintest8['pop'].dtype)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_traintest8[df_traintest8['place_id']=='China/Hubei'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"day_before_valid = 71+7 # 3-18 day  before of validation\nday_before_public = 78+7 # 3-25 last day of train\nday_before_launch = 85+7 # 4-8 last day before launch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_traintest8['cases/day_(1-1)']/df_traintest8['cases/day_(1-7)']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_traintest8['cases/day_1/7_ratio'] = df_traintest8['cases/day_(1-1)']/df_traintest8['cases/day_(1-7)']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_traintest8['fatal/day_1/7_ratio'] = df_traintest8['fatal/day_(1-1)']/df_traintest8['fatal/day_(1-7)']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_traintest8['fatal/cases_1/1_ratio'] = df_traintest8['fatal/day_(1-1)']/df_traintest8['cases/day_(1-1)']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_traintest8['fatal/cases_7/7_ratio'] = df_traintest8['fatal/day_(1-7)']/df_traintest8['cases/day_(1-7)']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_traintest8.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def calc_score(y_true, y_pred):\n    y_true[y_true<0] = 0\n    score = metrics.mean_squared_error(np.log(y_true.clip(0, 1e10)+1), np.log(y_pred[:]+1))**0.5\n    return score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train model to predict fatalities/day\n# params\nSEED = 42\nparams = {'num_leaves': 8,\n          'min_data_in_leaf': 5,  # 42,\n          'objective': 'regression',\n          'max_depth': 8,\n          'learning_rate': 0.02,\n          'boosting': 'gbdt',\n          'bagging_freq': 5,  # 5\n          'bagging_fraction': 0.8,  # 0.5,\n          'feature_fraction': 0.8201,\n          'bagging_seed': SEED,\n          'reg_alpha': 1,  # 1.728910519108444,\n          'reg_lambda': 4.9847051755586085,\n          'random_state': SEED,\n          'metric': 'mse',\n          'verbosity': 100,\n          'min_gain_to_split': 0.02,  # 0.01077313523861969,\n          'min_child_weight': 5,  # 19.428902804238373,\n          'num_threads': 6,\n          }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train model to predict fatalities/day\n# features are selected manually based on valid score\ncol_target = 'fatal/day'\ncol_var = [\n    'Lat', 'Long',\n#     'days_since_1cases', \n#     'days_since_10cases', \n#     'days_since_100cases',\n#     'days_since_1fatal', \n#     'days_since_10fatal', 'days_since_100fatal',\n#     'days_since_1recov',\n#     'days_since_10recov', 'days_since_100recov', \n    'cases/day_(1-1)', \n    'cases/day_(1-7)',\n    'cases/day_1/7_ratio',\n    'fatal/day_1/7_ratio',\n    'fatal/cases_1/1_ratio',\n    'fatal/cases_7/7_ratio',\n    \n#     'cases/day_(8-14)',  \n#     'cases/day_(15-21)', \n    \n#     'fatal/day_(1-1)', \n    'fatal/day_(1-7)', \n    'fatal/day_(8-14)', \n    'fatal/day_(15-21)', \n    'SmokingRate',\n#     'Gross_domestic_product__constant_prices',\n#     'Gross_domestic_product__current_prices',\n#     'Gross_domestic_product__deflator',\n#     'Gross_domestic_product_per_capita__constant_prices',\n#     'Gross_domestic_product_per_capita__current_prices',\n#     'Output_gap_in_percent_of_potential_GDP',\n#     'Gross_domestic_product_based_on_purchasing_power_parity__PPP__valuation_of_country_GDP',\n#     'Gross_domestic_product_based_on_purchasing_power_parity__PPP__per_capita_GDP',\n#     'Gross_domestic_product_based_on_purchasing_power_parity__PPP__share_of_world_total',\n#     'Implied_PPP_conversion_rate', 'Total_investment',\n#     'Gross_national_savings', 'Inflation__average_consumer_prices',\n#     'Inflation__end_of_period_consumer_prices',\n#     'Six_month_London_interbank_offered_rate__LIBOR_',\n#     'Volume_of_imports_of_goods_and_services', 'Volume_of_Imports_of_goods',\n#     'Volume_of_exports_of_goods_and_services', 'Volume_of_exports_of_goods',\n#     'Unemployment_rate', \n#     'Employment', 'Population',\n#     'General_government_revenue', 'General_government_total_expenditure',\n#     'General_government_net_lending_borrowing',\n#     'General_government_structural_balance',\n#     'General_government_primary_net_lending_borrowing',\n#     'General_government_net_debt', 'General_government_gross_debt',\n#     'Gross_domestic_product_corresponding_to_fiscal_year__current_prices',\n#     'Current_account_balance', \n#     'LifeExpectancy',\n#     'pop',\n    'density', \n#     'medianage', \n#     'urbanpop', \n#     'hospibed', 'smokers', \n]\ncol_cat = []\ndf_train = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']<=day_before_valid)]\ndf_valid = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (day_before_valid<df_traintest8['day']) & (df_traintest8['day']<=day_before_public)]\ndf_test = df_traintest8[pd.isna(df_traintest8['ForecastId'])==False]\nX_train = df_train[col_var]\nX_valid = df_valid[col_var]\ny_train = np.log(df_train[col_target].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target].values.clip(0, 1e10)+1)\ntrain_data = lgb.Dataset(X_train, label=y_train, categorical_feature=col_cat)\nvalid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=col_cat)\nnum_round = 15000\nmodel = lgb.train(params, train_data, num_round, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=150,)\n\nbest_itr = model.best_iteration","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_true = df_valid['fatal/day'].values\ny_pred = np.exp(model.predict(X_valid))-1\nscore = calc_score(y_true, y_pred)\nprint(\"{:.6f}\".format(score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# display feature importance\ntmp = pd.DataFrame()\ntmp[\"feature\"] = col_var\ntmp[\"importance\"] = model.feature_importance()\ntmp = tmp.sort_values('importance', ascending=False)\ntmp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train with all data before public\ndf_train = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']<=day_before_public)]\ndf_valid = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']<=day_before_public)]\ndf_test = df_traintest8[pd.isna(df_traintest8['ForecastId'])==False]\nX_train = df_train[col_var]\nX_valid = df_valid[col_var]\ny_train = np.log(df_train[col_target].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target].values.clip(0, 1e10)+1)\ntrain_data = lgb.Dataset(X_train, label=y_train, categorical_feature=col_cat)\nvalid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=col_cat)\nmodel = lgb.train(params, train_data, best_itr, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=150,)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train model to predict fatalities/day\ncol_target2 = 'cases/day'\ncol_var2 = [\n    'Lat', 'Long',\n#     'days_since_1cases', \n    'days_since_10cases', #selected\n#     'days_since_100cases',\n#     'days_since_1fatal', \n#     'days_since_10fatal',\n#     'days_since_100fatal',\n#     'days_since_1recov',\n#     'days_since_10recov', 'days_since_100recov', \n    'cases/day_(1-1)', \n    'cases/day_(1-7)', \n    'cases/day_(8-14)',  \n    'cases/day_(15-21)', \n    'cases/day_1/7_ratio',\n    'fatal/day_1/7_ratio',\n    'fatal/cases_1/1_ratio',\n    'fatal/cases_7/7_ratio',\n#     'fatal/day_(1-1)', \n#     'fatal/day_(1-7)', \n#     'fatal/day_(8-14)', \n#     'fatal/day_(15-21)', \n#     'recov/day_(1-1)', 'recov/day_(1-7)', \n#     'recov/day_(8-14)',  'recov/day_(15-21)',\n#     'active_(1-1)', \n#     'active_(1-7)', \n#     'active_(8-14)',  'active_(15-21)', \n#     'SmokingRate',\n#     'Gross_domestic_product__constant_prices',\n#     'Gross_domestic_product__current_prices',\n#     'Gross_domestic_product__deflator',\n#     'Gross_domestic_product_per_capita__constant_prices',\n#     'Gross_domestic_product_per_capita__current_prices',\n#     'Output_gap_in_percent_of_potential_GDP',\n#     'Gross_domestic_product_based_on_purchasing_power_parity__PPP__valuation_of_country_GDP',\n#     'Gross_domestic_product_based_on_purchasing_power_parity__PPP__per_capita_GDP',\n#     'Gross_domestic_product_based_on_purchasing_power_parity__PPP__share_of_world_total',\n#     'Implied_PPP_conversion_rate', 'Total_investment',\n#     'Gross_national_savings', 'Inflation__average_consumer_prices',\n#     'Inflation__end_of_period_consumer_prices',\n#     'Six_month_London_interbank_offered_rate__LIBOR_',\n#     'Volume_of_imports_of_goods_and_services', 'Volume_of_Imports_of_goods',\n#     'Volume_of_exports_of_goods_and_services', 'Volume_of_exports_of_goods',\n#     'Unemployment_rate', \n#     'Employment', \n#     'Population',\n#     'General_government_revenue', 'General_government_total_expenditure',\n#     'General_government_net_lending_borrowing',\n#     'General_government_structural_balance',\n#     'General_government_primary_net_lending_borrowing',\n#     'General_government_net_debt', 'General_government_gross_debt',\n#     'Gross_domestic_product_corresponding_to_fiscal_year__current_prices',\n#     'Current_account_balance', \n#     'LifeExpectancy',\n#     'pop',\n#     'density', \n#     'medianage', \n#     'urbanpop', \n#     'hospibed', 'smokers', \n]\ncol_cat = []\ndf_train = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']<=day_before_valid)]\ndf_valid = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (day_before_valid<df_traintest8['day']) & (df_traintest8['day']<=day_before_public)]\nX_train = df_train[col_var2]\nX_valid = df_valid[col_var2]\ny_train = np.log(df_train[col_target2].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target2].values.clip(0, 1e10)+1)\ntrain_data = lgb.Dataset(X_train, label=y_train, categorical_feature=col_cat)\nvalid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=col_cat)\nmodel2 = lgb.train(params, train_data, num_round, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=150,)\nbest_itr2 = model2.best_iteration","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_true = df_valid['cases/day'].values\ny_pred = np.exp(model2.predict(X_valid))-1\nscore = calc_score(y_true, y_pred)\nprint(\"{:.6f}\".format(score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# display feature importance\ntmp = pd.DataFrame()\ntmp[\"feature\"] = col_var2\ntmp[\"importance\"] = model2.feature_importance()\ntmp = tmp.sort_values('importance', ascending=False)\ntmp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']<=day_before_public)]\ndf_valid = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']<=day_before_public)]\nX_train = df_train[col_var2]\nX_valid = df_valid[col_var2]\ny_train = np.log(df_train[col_target2].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target2].values.clip(0, 1e10)+1)\ntrain_data = lgb.Dataset(X_train, label=y_train, categorical_feature=col_cat)\nvalid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=col_cat)\nmodel2 = lgb.train(params, train_data, best_itr2, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=150,)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train model to predict fatalities/day\ndf_train = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']<=day_before_public)]\ndf_valid = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (day_before_public<df_traintest8['day'])]\ndf_test = df_traintest8[pd.isna(df_traintest8['ForecastId'])==False]\nX_train = df_train[col_var]\nX_valid = df_valid[col_var]\ny_train = np.log(df_train[col_target].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target].values.clip(0, 1e10)+1)\ntrain_data = lgb.Dataset(X_train, label=y_train, categorical_feature=col_cat)\nvalid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=col_cat)\nnum_round = 15000\nmodel = lgb.train(params, train_data, num_round, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=150,)\n\nbest_itr = model.best_iteration\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train with all data\ndf_train = df_traintest8[(pd.isna(df_traintest8['ForecastId']))]\ndf_valid = df_traintest8[(pd.isna(df_traintest8['ForecastId']))]\nX_train = df_train[col_var]\nX_valid = df_valid[col_var]\ny_train = np.log(df_train[col_target].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target].values.clip(0, 1e10)+1)\ntrain_data = lgb.Dataset(X_train, label=y_train, categorical_feature=col_cat)\nvalid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=col_cat)\nmodel_pri = lgb.train(params, train_data, best_itr, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=150,)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train model to predict cases/day\ndf_train = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']<=day_before_public)]\ndf_valid = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (day_before_public<df_traintest8['day'])]\nX_train = df_train[col_var2]\nX_valid = df_valid[col_var2]\ny_train = np.log(df_train[col_target2].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target2].values.clip(0, 1e10)+1)\ntrain_data = lgb.Dataset(X_train, label=y_train, categorical_feature=col_cat)\nvalid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=col_cat)\nmodel2 = lgb.train(params, train_data, num_round, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=150,)\nbest_itr2 = model2.best_iteration","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train with all data\ndf_train = df_traintest8[(pd.isna(df_traintest8['ForecastId']))]\ndf_valid = df_traintest8[(pd.isna(df_traintest8['ForecastId']))]\nX_train = df_train[col_var2]\nX_valid = df_valid[col_var2]\ny_train = np.log(df_train[col_target2].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target2].values.clip(0, 1e10)+1)\ntrain_data = lgb.Dataset(X_train, label=y_train, categorical_feature=col_cat)\nvalid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=col_cat)\nmodel2_pri = lgb.train(params, train_data, best_itr2, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=150,)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove overlap for public LB prediction\ndf_tmp = df_traintest8[\n    ((df_traintest8['day']<=day_before_public)  & (pd.isna(df_traintest8['ForecastId'])))\n    | ((day_before_public<df_traintest8['day']) & (pd.isna(df_traintest8['ForecastId'])==False))].reset_index(drop=True)\ndf_tmp = df_tmp.drop([\n    'cases/day_(1-1)', 'cases/day_(1-7)', 'cases/day_(8-14)', 'cases/day_(15-21)', \n    'fatal/day_(1-1)', 'fatal/day_(1-7)', 'fatal/day_(8-14)', 'fatal/day_(15-21)', 'cases/day_1/7_ratio', 'fatal/cases_1/1_ratio', 'fatal/cases_7/7_ratio', 'fatal/day_1/7_ratio',\n    'days_since_1cases', 'days_since_10cases', 'days_since_100cases',\n    'days_since_1fatal', 'days_since_10fatal', 'days_since_100fatal',\n                               ],  axis=1)\ndf_traintest9 = []\nfor i, place in enumerate(places[:]):\n    df_tmp2 = df_tmp[df_tmp['place_id']==place].reset_index(drop=True)\n    df_tmp2 = do_aggregations(df_tmp2)\n    df_tmp2['cases/day_1/7_ratio'] = df_tmp2['cases/day_(1-1)']/df_tmp2['cases/day_(1-7)']\n    df_tmp2['fatal/day_1/7_ratio'] = df_tmp2['fatal/day_(1-1)']/df_tmp2['fatal/day_(1-7)']\n    df_tmp2['fatal/cases_1/1_ratio'] = df_tmp2['fatal/day_(1-1)']/df_tmp2['cases/day_(1-1)']\n    df_tmp2['fatal/cases_7/7_ratio'] = df_tmp2['fatal/day_(1-7)']/df_tmp2['cases/day_(1-7)']\n    df_traintest9.append(df_tmp2)\ndf_traintest9 = pd.concat(df_traintest9).reset_index(drop=True)\ndf_traintest9[df_traintest9['day']>day_before_public-2].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove overlap for private LB prediction\ndf_tmp = df_traintest8[\n    ((df_traintest8['day']<=day_before_private)  & (pd.isna(df_traintest8['ForecastId'])))\n    | ((day_before_private<df_traintest8['day']) & (pd.isna(df_traintest8['ForecastId'])==False))].reset_index(drop=True)\ndf_tmp = df_tmp.drop([\n    'cases/day_(1-1)', 'cases/day_(1-7)', 'cases/day_(8-14)', 'cases/day_(15-21)', \n    'fatal/day_(1-1)', 'fatal/day_(1-7)', 'fatal/day_(8-14)', 'fatal/day_(15-21)', 'cases/day_1/7_ratio', 'fatal/cases_1/1_ratio', 'fatal/cases_7/7_ratio', 'fatal/day_1/7_ratio',\n    'days_since_1cases', 'days_since_10cases', 'days_since_100cases',\n    'days_since_1fatal', 'days_since_10fatal', 'days_since_100fatal',\n                               ],  axis=1)\ndf_traintest10 = []\nfor i, place in enumerate(places[:]):\n    df_tmp2 = df_tmp[df_tmp['place_id']==place].reset_index(drop=True)\n    df_tmp2 = do_aggregations(df_tmp2)\n    df_tmp2['cases/day_1/7_ratio'] = df_tmp2['cases/day_(1-1)']/df_tmp2['cases/day_(1-7)']\n    df_tmp2['fatal/day_1/7_ratio'] = df_tmp2['fatal/day_(1-1)']/df_tmp2['fatal/day_(1-7)']\n    df_tmp2['fatal/cases_1/1_ratio'] = df_tmp2['fatal/day_(1-1)']/df_tmp2['cases/day_(1-1)']\n    df_tmp2['fatal/cases_7/7_ratio'] = df_tmp2['fatal/day_(1-7)']/df_tmp2['cases/day_(1-7)']\n    df_traintest10.append(df_tmp2)\ndf_traintest10 = pd.concat(df_traintest10).reset_index(drop=True)\ndf_traintest10[df_traintest10['day']>day_before_private-2].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predict test data in public\n# predict the cases and fatatilites one day at a time and use the predicts as next day's feature recursively.\ndf_preds = []\nfor i, place in enumerate(places[:]):\n    df_interest = copy.deepcopy(df_traintest9[df_traintest9['place_id']==place].reset_index(drop=True))\n    df_interest['cases/day'][(pd.isna(df_interest['ForecastId']))==False] = -1\n    df_interest['fatal/day'][(pd.isna(df_interest['ForecastId']))==False] = -1\n    len_known = (df_interest['day']<=day_before_public).sum()\n    len_unknown = (day_before_public<df_interest['day']).sum()\n    for j in range(len_unknown): # use predicted cases and fatal for next days' prediction\n        X_valid = df_interest[col_var].iloc[j+len_known]\n        X_valid2 = df_interest[col_var2].iloc[j+len_known]\n        pred_f = model.predict(X_valid)\n        pred_c = model2.predict(X_valid2)\n        pred_c = (np.exp(pred_c)-1).clip(0, 1e10)\n        pred_f = (np.exp(pred_f)-1).clip(0, 1e10)\n        df_interest['fatal/day'][j+len_known] = pred_f\n        df_interest['cases/day'][j+len_known] = pred_c\n        df_interest['Fatalities'][j+len_known] = df_interest['Fatalities'][j+len_known-1] + pred_f\n        df_interest['ConfirmedCases'][j+len_known] = df_interest['ConfirmedCases'][j+len_known-1] + pred_c\n#         print(df_interest['ConfirmedCases'][j+len_known-1], df_interest['ConfirmedCases'][j+len_known], pred_c)\n        df_interest = df_interest.drop([\n            'cases/day_(1-1)', 'cases/day_(1-7)', 'cases/day_(8-14)', 'cases/day_(15-21)', \n            'fatal/day_(1-1)', 'fatal/day_(1-7)', 'fatal/day_(8-14)', 'fatal/day_(15-21)', 'cases/day_1/7_ratio', 'fatal/cases_1/1_ratio', 'fatal/cases_7/7_ratio', 'fatal/day_1/7_ratio',\n            'days_since_1cases', 'days_since_10cases', 'days_since_100cases',\n            'days_since_1fatal', 'days_since_10fatal', 'days_since_100fatal',\n\n                                       ],  axis=1)\n        df_interest = do_aggregations(df_interest)\n        df_interest['cases/day_1/7_ratio'] = df_interest['cases/day_(1-1)']/df_interest['cases/day_(1-7)']\n        df_interest['fatal/day_1/7_ratio'] = df_interest['fatal/day_(1-1)']/df_interest['fatal/day_(1-7)']\n        df_interest['fatal/cases_1/1_ratio'] = df_interest['fatal/day_(1-1)']/df_interest['cases/day_(1-1)']\n        df_interest['fatal/cases_7/7_ratio'] = df_interest['fatal/day_(1-7)']/df_interest['cases/day_(1-7)']\n    if (i+1)%10==0:\n        print(\"{:3d}/{}  {}, len known: {}, len unknown: {}\".format(i+1, len(places), place, len_known, len_unknown), df_interest.shape)\n    df_interest['fatal_pred'] = np.cumsum(df_interest['fatal/day'].values)\n    df_interest['cases_pred'] = np.cumsum(df_interest['cases/day'].values)\n    df_preds.append(df_interest)\ndf_preds = pd.concat(df_preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predict test data in public\ndf_preds_pri = []\nfor i, place in enumerate(places[:]):\n    df_interest = copy.deepcopy(df_traintest10[df_traintest10['place_id']==place].reset_index(drop=True))\n    df_interest['cases/day'][(pd.isna(df_interest['ForecastId']))==False] = -1\n    df_interest['fatal/day'][(pd.isna(df_interest['ForecastId']))==False] = -1\n    len_known = (df_interest['day']<=day_before_private).sum()\n    len_unknown = (day_before_private<df_interest['day']).sum()\n    for j in range(len_unknown): # use predicted cases and fatal for next days' prediction\n        X_valid = df_interest[col_var].iloc[j+len_known]\n        X_valid2 = df_interest[col_var2].iloc[j+len_known]\n        pred_f = model_pri.predict(X_valid)\n        pred_c = model2_pri.predict(X_valid2)\n        pred_c = (np.exp(pred_c)-1).clip(0, 1e10)\n        pred_f = (np.exp(pred_f)-1).clip(0, 1e10)\n        df_interest['fatal/day'][j+len_known] = pred_f\n        df_interest['cases/day'][j+len_known] = pred_c\n        df_interest['Fatalities'][j+len_known] = df_interest['Fatalities'][j+len_known-1] + pred_f\n        df_interest['ConfirmedCases'][j+len_known] = df_interest['ConfirmedCases'][j+len_known-1] + pred_c\n#         print(df_interest['ConfirmedCases'][j+len_known-1], df_interest['ConfirmedCases'][j+len_known], pred_c)\n        df_interest = df_interest.drop([\n            'cases/day_(1-1)', 'cases/day_(1-7)', 'cases/day_(8-14)', 'cases/day_(15-21)', \n            'fatal/day_(1-1)', 'fatal/day_(1-7)', 'fatal/day_(8-14)', 'fatal/day_(15-21)', 'cases/day_1/7_ratio', 'fatal/cases_1/1_ratio', 'fatal/cases_7/7_ratio', 'fatal/day_1/7_ratio',\n            'days_since_1cases', 'days_since_10cases', 'days_since_100cases',\n            'days_since_1fatal', 'days_since_10fatal', 'days_since_100fatal',\n\n                                       ],  axis=1)\n        df_interest = do_aggregations(df_interest)\n        df_interest['cases/day_1/7_ratio'] = df_interest['cases/day_(1-1)']/df_interest['cases/day_(1-7)']\n        df_interest['fatal/day_1/7_ratio'] = df_interest['fatal/day_(1-1)']/df_interest['fatal/day_(1-7)']\n        df_interest['fatal/cases_1/1_ratio'] = df_interest['fatal/day_(1-1)']/df_interest['cases/day_(1-1)']\n        df_interest['fatal/cases_7/7_ratio'] = df_interest['fatal/day_(1-7)']/df_interest['cases/day_(1-7)']\n    if (i+1)%10==0:\n        print(\"{:3d}/{}  {}, len known: {}, len unknown: {}\".format(i+1, len(places), place, len_known, len_unknown), df_interest.shape)\n    df_interest['fatal_pred'] = np.cumsum(df_interest['fatal/day'].values)\n    df_interest['cases_pred'] = np.cumsum(df_interest['cases/day'].values)\n    df_preds_pri.append(df_interest)\ndf_preds_pri = pd.concat(df_preds_pri)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"places_sort = df_traintest10[['place_id', 'ConfirmedCases']][df_traintest10['day']==day_before_private]\nplaces_sort = places_sort.sort_values('ConfirmedCases', ascending=False).reset_index(drop=True)['place_id'].values\nprint(len(places_sort))\nplaces_sort[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Fatalities / Public\")\nplt.figure(figsize=(30,30))\nfor i in range(30):\n    plt.subplot(5,6,i+1)\n    idx = i * 10\n    df_interest = df_preds[df_preds['place_id']==places_sort[idx]].reset_index(drop=True)\n    tmp = df_interest['fatal/day'].values\n    tmp = np.cumsum(tmp)\n    sns.lineplot(x=df_interest['day'], y=tmp, label='pred')\n    df_interest2 = df_traintest10[(df_traintest10['place_id']==places_sort[idx]) & (df_traintest10['day']<=day_before_private)].reset_index(drop=True)\n    sns.lineplot(x=df_interest2['day'].values, y=df_interest2['Fatalities'].values, label='true')\n    plt.title(places_sort[idx])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Confirmed Cases / Public\")\nplt.figure(figsize=(30,30))\nfor i in range(30):\n    plt.subplot(5,6,i+1)\n    idx = i * 10\n    df_interest = df_preds[df_preds['place_id']==places_sort[idx]].reset_index(drop=True)\n    tmp = df_interest['cases/day'].values\n    tmp = np.cumsum(tmp)\n    sns.lineplot(x=df_interest['day'], y=tmp, label='pred')\n    df_interest2 = df_traintest10[(df_traintest10['place_id']==places_sort[idx]) & (df_traintest10['day']<=day_before_private)].reset_index(drop=True)\n    sns.lineplot(x=df_interest2['day'].values, y=df_interest2['ConfirmedCases'].values, label='true')\n    plt.title(places_sort[idx])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Fatalities / Private\")\nplt.figure(figsize=(30,30))\nfor i in range(30):\n    plt.subplot(5,6,i+1)\n    idx = i * 10\n    df_interest = df_preds_pri[df_preds_pri['place_id']==places_sort[idx]].reset_index(drop=True)\n    tmp = df_interest['fatal/day'].values\n    tmp = np.cumsum(tmp)\n    sns.lineplot(x=df_interest['day'], y=tmp, label='pred')\n    df_interest2 = df_traintest10[(df_traintest10['place_id']==places_sort[idx]) & (df_traintest10['day']<=day_before_private)].reset_index(drop=True)\n    sns.lineplot(x=df_interest2['day'].values, y=df_interest2['Fatalities'].values, label='true')\n    plt.title(places_sort[idx])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"ConfirmedCases / Private\")\nplt.figure(figsize=(30,30))\nfor i in range(30):\n    plt.subplot(5,6,i+1)\n    idx = i * 10\n    df_interest = df_preds_pri[df_preds_pri['place_id']==places_sort[idx]].reset_index(drop=True)\n    tmp = df_interest['cases/day'].values\n    tmp = np.cumsum(tmp)\n    sns.lineplot(x=df_interest['day'], y=tmp, label='pred')\n    df_interest2 = df_traintest10[(df_traintest10['place_id']==places_sort[idx]) & (df_traintest10['day']<=day_before_private)].reset_index(drop=True)\n    sns.lineplot(x=df_interest2['day'].values, y=df_interest2['ConfirmedCases'].values, label='true')\n    plt.title(places_sort[idx])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# merge 2 preds\ndf_preds[df_preds['day']>day_before_private] = df_preds_pri[df_preds['day']>day_before_private]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_preds.to_csv(\"df_preds.csv\", index=None)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load sample submission\ndf_sub = pd.read_csv(\"../input/covid19-global-forecasting-week-3/submission.csv\")\nprint(len(df_sub))\ndf_sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# merge prediction with sub\ndf_sub = pd.merge(df_sub, df_traintest3[['ForecastId', 'place_id', 'day']])\ndf_sub = pd.merge(df_sub, df_preds[['place_id', 'day', 'cases_pred', 'fatal_pred']], on=['place_id', 'day',], how='left')\ndf_sub.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save\ndf_sub['ConfirmedCases'] = df_sub['cases_pred']\ndf_sub['Fatalities'] = df_sub['fatal_pred']\ndf_sub = df_sub[['ForecastId', 'ConfirmedCases', 'Fatalities']]\ndf_sub.to_csv(\"submission_0.csv\", index=None)\ndf_sub.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport pandas as pd\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\npd.set_option('display.max_columns', 99)\npd.set_option('display.max_rows', 99)\nimport os\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom tqdm import tqdm\nimport datetime as dt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = [16, 10]\nplt.rcParams['font.size'] = 14\nimport seaborn as sns\nsns.set_palette(sns.color_palette('tab20', 20))\n\nimport plotly.express as px\nimport plotly.graph_objects as go","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"COMP = '../input/covid19-global-forecasting-week-3'\nDATEFORMAT = '%Y-%m-%d'\n\n\ndef get_comp_data(COMP):\n    train = pd.read_csv(f'{COMP}/train.csv')\n    test = pd.read_csv(f'{COMP}/test.csv')\n    submission = pd.read_csv(f'{COMP}/submission.csv')\n    print(train.shape, test.shape, submission.shape)\n    train['Country_Region'] = train['Country_Region'].str.replace(',', '')\n    test['Country_Region'] = test['Country_Region'].str.replace(',', '')\n\n    train['Location'] = train['Country_Region'] + '-' + train['Province_State'].fillna('')\n\n    test['Location'] = test['Country_Region'] + '-' + test['Province_State'].fillna('')\n\n    train['LogConfirmed'] = to_log(train.ConfirmedCases)\n    train['LogFatalities'] = to_log(train.Fatalities)\n    train = train.drop(columns=['Province_State'])\n    test = test.drop(columns=['Province_State'])\n\n    country_codes = pd.read_csv('../input/covid19-metadata/country_codes.csv', keep_default_na=False)\n    train = train.merge(country_codes, on='Country_Region', how='left')\n    test = test.merge(country_codes, on='Country_Region', how='left')\n\n    train['DateTime'] = pd.to_datetime(train['Date'])\n    test['DateTime'] = pd.to_datetime(test['Date'])\n    \n    return train, test, submission\n\n\ndef process_each_location(df):\n    dfs = []\n    for loc, df in tqdm(df.groupby('Location')):\n        df = df.sort_values(by='Date')\n        df['Fatalities'] = df['Fatalities'].cummax()\n        df['ConfirmedCases'] = df['ConfirmedCases'].cummax()\n        df['LogFatalities'] = df['LogFatalities'].cummax()\n        df['LogConfirmed'] = df['LogConfirmed'].cummax()\n        df['LogConfirmedNextDay'] = df['LogConfirmed'].shift(-1)\n        df['ConfirmedNextDay'] = df['ConfirmedCases'].shift(-1)\n        df['DateNextDay'] = df['Date'].shift(-1)\n        df['LogFatalitiesNextDay'] = df['LogFatalities'].shift(-1)\n        df['FatalitiesNextDay'] = df['Fatalities'].shift(-1)\n        df['LogConfirmedDelta'] = df['LogConfirmedNextDay'] - df['LogConfirmed']\n        df['ConfirmedDelta'] = df['ConfirmedNextDay'] - df['ConfirmedCases']\n        df['LogFatalitiesDelta'] = df['LogFatalitiesNextDay'] - df['LogFatalities']\n        df['FatalitiesDelta'] = df['FatalitiesNextDay'] - df['Fatalities']\n        dfs.append(df)\n    return pd.concat(dfs)\n\n\ndef add_days(d, k):\n    return dt.datetime.strptime(d, DATEFORMAT) + dt.timedelta(days=k)\n\n\ndef to_log(x):\n    return np.log(x + 1)\n\n\ndef to_exp(x):\n    return np.exp(x) - 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = dt.datetime.now()\ntrain, test, submission = get_comp_data(COMP)\ntrain.shape, test.shape, submission.shape\ntrain.head(2)\ntest.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train.geo_region.isna()].Country_Region.unique()\ntrain = train.fillna('#N/A')\ntest = test.fillna('#N/A')\n\ntrain[train.duplicated(['Date', 'Location'])]\ntrain.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()\ntrain.nunique()\ntrain.dtypes\ntrain.count()\n\nTRAIN_START = train.Date.min()\nTEST_START = test.Date.min()\nTRAIN_END = train.Date.max()\nTEST_END = test.Date.max()\nTRAIN_START, TRAIN_END, TEST_START, TEST_END","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.sort_values(by='Date')\ncountries_latest_state = train[train['Date'] == TRAIN_END].groupby([\n    'Country_Region', 'continent', 'geo_region', 'country_iso_code_3']).sum()[[\n    'ConfirmedCases', 'Fatalities']].reset_index()\ncountries_latest_state['Log10Confirmed'] = np.log10(countries_latest_state.ConfirmedCases + 1)\ncountries_latest_state['Log10Fatalities'] = np.log10(countries_latest_state.Fatalities + 1)\ncountries_latest_state = countries_latest_state.sort_values(by='Fatalities', ascending=False)\ncountries_latest_state.to_csv('countries_latest_state.csv', index=False)\n\ncountries_latest_state.shape\ncountries_latest_state.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The source dataset is not necessary cumulative we will force it\nlatest_loc = train[train['Date'] == TRAIN_END][['Location', 'ConfirmedCases', 'Fatalities']]\nmax_loc = train.groupby(['Location'])[['ConfirmedCases', 'Fatalities']].max().reset_index()\ncheck = pd.merge(latest_loc, max_loc, on='Location')\nnp.mean(check.ConfirmedCases_x == check.ConfirmedCases_y)\nnp.mean(check.Fatalities_x == check.Fatalities_y)\ncheck[check.Fatalities_x != check.Fatalities_y]\ncheck[check.ConfirmedCases_x != check.ConfirmedCases_y]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_clean = process_each_location(train)\n\ntrain_clean.shape\ntrain_clean.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"regional_progress = train_clean.groupby(['DateTime', 'continent']).sum()[['ConfirmedCases', 'Fatalities']].reset_index()\nregional_progress['Log10Confirmed'] = np.log10(regional_progress.ConfirmedCases + 1)\nregional_progress['Log10Fatalities'] = np.log10(regional_progress.Fatalities + 1)\nregional_progress = regional_progress[regional_progress.continent != '#N/A']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"china = train_clean[train_clean.Location.str.startswith('China')]\ntop10_locations = china.groupby('Location')[['ConfirmedCases']].max().sort_values(\n    by='ConfirmedCases', ascending=False).reset_index().Location.values[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"country_progress = train_clean.groupby(['Date', 'DateTime', 'Country_Region']).sum()[[\n    'ConfirmedCases', 'Fatalities', 'ConfirmedDelta', 'FatalitiesDelta']].reset_index()\ntop10_countries = country_progress.groupby('Country_Region')[['Fatalities']].max().sort_values(\n    by='Fatalities', ascending=False).reset_index().Country_Region.values[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"countries_0301 = country_progress[country_progress.Date == '2020-03-01'][[\n    'Country_Region', 'ConfirmedCases', 'Fatalities']]\ncountries_0331 = country_progress[country_progress.Date == '2020-03-31'][[\n    'Country_Region', 'ConfirmedCases', 'Fatalities']]\ncountries_in_march = pd.merge(countries_0301, countries_0331, on='Country_Region', suffixes=['_0301', '_0331'])\ncountries_in_march['IncreaseInMarch'] = countries_in_march.ConfirmedCases_0331 / (countries_in_march.ConfirmedCases_0301 + 1)\ncountries_in_march = countries_in_march[countries_in_march.ConfirmedCases_0331 > 200].sort_values(\n    by='IncreaseInMarch', ascending=False)\ncountries_in_march.tail(15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"selected_countries = [\n    'Italy', 'Vietnam', 'Bahrain', 'Singapore', 'Taiwan*', 'Japan', 'Kuwait', 'Korea, South', 'China']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"deltas = train_clean[np.logical_and(\n        train_clean.LogConfirmed > 0,\n        ~train_clean.Location.str.startswith('China')\n)].dropna().sort_values(by='LogConfirmedDelta', ascending=False)\ndeltas = deltas[deltas['Date'] >= '2020-03-12']\n\nconfirmed_deltas = pd.concat([\n    deltas.groupby('Location')[['LogConfirmedDelta']].mean(),\n    deltas.groupby('Location')[['LogConfirmedDelta']].std(),\n    deltas.groupby('Location')[['LogConfirmedDelta']].count(),\n    deltas.groupby('Location')[['LogConfirmed']].max()\n], axis=1)\nconfirmed_deltas.columns = ['avg', 'std', 'cnt', 'max']\n\nconfirmed_deltas.sort_values(by='avg').head(10)\nconfirmed_deltas.sort_values(by='avg').tail(10)\nconfirmed_deltas.to_csv('confirmed_deltas.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DECAY = 0.93\nDECAY ** 7, DECAY ** 14, DECAY ** 21, DECAY ** 28\n\nconfirmed_deltas = train.groupby(['Location', 'Country_Region', 'continent'])[[\n    'Id']].count().reset_index()\n\nGLOBAL_DELTA = 0.11\nconfirmed_deltas['DELTA'] = GLOBAL_DELTA\n\nconfirmed_deltas.loc[confirmed_deltas.continent=='Africa', 'DELTA'] = 0.14\nconfirmed_deltas.loc[confirmed_deltas.continent=='Oceania', 'DELTA'] = 0.06\nconfirmed_deltas.loc[confirmed_deltas.Country_Region=='Korea South', 'DELTA'] = 0.011\nconfirmed_deltas.loc[confirmed_deltas.Country_Region=='US', 'DELTA'] = 0.15\nconfirmed_deltas.loc[confirmed_deltas.Country_Region=='China', 'DELTA'] = 0.01\nconfirmed_deltas.loc[confirmed_deltas.Country_Region=='Japan', 'DELTA'] = 0.05\nconfirmed_deltas.loc[confirmed_deltas.Country_Region=='Singapore', 'DELTA'] = 0.05\nconfirmed_deltas.loc[confirmed_deltas.Country_Region=='Taiwan*', 'DELTA'] = 0.05\nconfirmed_deltas.loc[confirmed_deltas.Country_Region=='Switzerland', 'DELTA'] = 0.05\nconfirmed_deltas.loc[confirmed_deltas.Country_Region=='Norway', 'DELTA'] = 0.05\nconfirmed_deltas.loc[confirmed_deltas.Country_Region=='Iceland', 'DELTA'] = 0.05\nconfirmed_deltas.loc[confirmed_deltas.Country_Region=='Austria', 'DELTA'] = 0.06\nconfirmed_deltas.loc[confirmed_deltas.Country_Region=='Italy', 'DELTA'] = 0.04\nconfirmed_deltas.loc[confirmed_deltas.Country_Region=='Spain', 'DELTA'] = 0.08\nconfirmed_deltas.loc[confirmed_deltas.Country_Region=='Portugal', 'DELTA'] = 0.12\nconfirmed_deltas.loc[confirmed_deltas.Country_Region=='Israel', 'DELTA'] = 0.12\nconfirmed_deltas.loc[confirmed_deltas.Country_Region=='Iran', 'DELTA'] = 0.08\nconfirmed_deltas.loc[confirmed_deltas.Country_Region=='Germany', 'DELTA'] = 0.07\nconfirmed_deltas.loc[confirmed_deltas.Country_Region=='Malaysia', 'DELTA'] = 0.06\nconfirmed_deltas.loc[confirmed_deltas.Country_Region=='Russia', 'DELTA'] = 0.18\nconfirmed_deltas.loc[confirmed_deltas.Country_Region=='Ukraine', 'DELTA'] = 0.18\nconfirmed_deltas.loc[confirmed_deltas.Country_Region=='Brazil', 'DELTA'] = 0.12\nconfirmed_deltas.loc[confirmed_deltas.Country_Region=='Turkey', 'DELTA'] = 0.18\nconfirmed_deltas.loc[confirmed_deltas.Country_Region=='Philippines', 'DELTA'] = 0.18\nconfirmed_deltas.loc[confirmed_deltas.Location=='France-', 'DELTA'] = 0.1\nconfirmed_deltas.loc[confirmed_deltas.Location=='United Kingdom-', 'DELTA'] = 0.12\nconfirmed_deltas.loc[confirmed_deltas.Location=='Diamond Princess-', 'DELTA'] = 0.00\nconfirmed_deltas.loc[confirmed_deltas.Location=='China-Hong Kong', 'DELTA'] = 0.08\nconfirmed_deltas.loc[confirmed_deltas.Location=='San Marino-', 'DELTA'] = 0.03\n\n\nconfirmed_deltas.shape, confirmed_deltas.DELTA.mean()\n\nconfirmed_deltas[confirmed_deltas.DELTA != GLOBAL_DELTA].shape, confirmed_deltas[confirmed_deltas.DELTA != GLOBAL_DELTA].DELTA.mean()\nconfirmed_deltas[confirmed_deltas.DELTA != GLOBAL_DELTA]\nconfirmed_deltas.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"daily_log_confirmed = train_clean.pivot('Location', 'Date', 'LogConfirmed').reset_index()\ndaily_log_confirmed = daily_log_confirmed.sort_values(TRAIN_END, ascending=False)\ndaily_log_confirmed.to_csv('daily_log_confirmed.csv', index=False)\n\nfor i, d in tqdm(enumerate(pd.date_range(add_days(TRAIN_END, 1), add_days(TEST_END, 1)))):\n    new_day = str(d).split(' ')[0]\n    last_day = dt.datetime.strptime(new_day, DATEFORMAT) - dt.timedelta(days=1)\n    last_day = last_day.strftime(DATEFORMAT)\n    for loc in confirmed_deltas.Location.values:\n        confirmed_delta = confirmed_deltas.loc[confirmed_deltas.Location == loc, 'DELTA'].values[0]\n        daily_log_confirmed.loc[daily_log_confirmed.Location == loc, new_day] = daily_log_confirmed.loc[daily_log_confirmed.Location == loc, last_day] + \\\n            confirmed_delta * DECAY ** i","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"daily_log_confirmed.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confirmed_prediciton = pd.melt(daily_log_confirmed[:25], id_vars='Location')\nconfirmed_prediciton['ConfirmedCases'] = to_exp(confirmed_prediciton['value'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"death_deltas = train.groupby(['Location', 'Country_Region', 'continent'])[[\n    'Id']].count().reset_index()\n\nGLOBAL_DELTA = 0.11\ndeath_deltas['DELTA'] = GLOBAL_DELTA\n\ndeath_deltas.loc[death_deltas.Country_Region=='China', 'DELTA'] = 0.005\ndeath_deltas.loc[death_deltas.continent=='Oceania', 'DELTA'] = 0.08\ndeath_deltas.loc[death_deltas.Country_Region=='Korea South', 'DELTA'] = 0.04\ndeath_deltas.loc[death_deltas.Country_Region=='Japan', 'DELTA'] = 0.04\ndeath_deltas.loc[death_deltas.Country_Region=='Singapore', 'DELTA'] = 0.05\ndeath_deltas.loc[death_deltas.Country_Region=='Taiwan*', 'DELTA'] = 0.06\n\n\n\ndeath_deltas.loc[death_deltas.Country_Region=='US', 'DELTA'] = 0.17\n\ndeath_deltas.loc[death_deltas.Country_Region=='Switzerland', 'DELTA'] = 0.15\ndeath_deltas.loc[death_deltas.Country_Region=='Norway', 'DELTA'] = 0.15\ndeath_deltas.loc[death_deltas.Country_Region=='Iceland', 'DELTA'] = 0.01\ndeath_deltas.loc[death_deltas.Country_Region=='Austria', 'DELTA'] = 0.14\ndeath_deltas.loc[death_deltas.Country_Region=='Italy', 'DELTA'] = 0.07\ndeath_deltas.loc[death_deltas.Country_Region=='Spain', 'DELTA'] = 0.1\ndeath_deltas.loc[death_deltas.Country_Region=='Portugal', 'DELTA'] = 0.13\ndeath_deltas.loc[death_deltas.Country_Region=='Israel', 'DELTA'] = 0.16\ndeath_deltas.loc[death_deltas.Country_Region=='Iran', 'DELTA'] = 0.06\ndeath_deltas.loc[death_deltas.Country_Region=='Germany', 'DELTA'] = 0.14\ndeath_deltas.loc[death_deltas.Country_Region=='Malaysia', 'DELTA'] = 0.14\ndeath_deltas.loc[death_deltas.Country_Region=='Russia', 'DELTA'] = 0.2\ndeath_deltas.loc[death_deltas.Country_Region=='Ukraine', 'DELTA'] = 0.2\ndeath_deltas.loc[death_deltas.Country_Region=='Brazil', 'DELTA'] = 0.2\ndeath_deltas.loc[death_deltas.Country_Region=='Turkey', 'DELTA'] = 0.22\ndeath_deltas.loc[death_deltas.Country_Region=='Philippines', 'DELTA'] = 0.12\ndeath_deltas.loc[death_deltas.Location=='France-', 'DELTA'] = 0.14\ndeath_deltas.loc[death_deltas.Location=='United Kingdom-', 'DELTA'] = 0.14\ndeath_deltas.loc[death_deltas.Location=='Diamond Princess-', 'DELTA'] = 0.00\n\ndeath_deltas.loc[death_deltas.Location=='China-Hong Kong', 'DELTA'] = 0.01\ndeath_deltas.loc[death_deltas.Location=='San Marino-', 'DELTA'] = 0.05\n\n\ndeath_deltas.shape\ndeath_deltas.DELTA.mean()\n\ndeath_deltas[death_deltas.DELTA != GLOBAL_DELTA].shape\ndeath_deltas[death_deltas.DELTA != GLOBAL_DELTA].DELTA.mean()\ndeath_deltas[death_deltas.DELTA != GLOBAL_DELTA]\ndeath_deltas.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"daily_log_deaths = train_clean.pivot('Location', 'Date', 'LogFatalities').reset_index()\ndaily_log_deaths = daily_log_deaths.sort_values(TRAIN_END, ascending=False)\ndaily_log_deaths.to_csv('daily_log_deaths.csv', index=False)\n\nfor i, d in tqdm(enumerate(pd.date_range(add_days(TRAIN_END, 1), add_days(TEST_END, 1)))):\n    new_day = str(d).split(' ')[0]\n    last_day = dt.datetime.strptime(new_day, DATEFORMAT) - dt.timedelta(days=1)\n    last_day = last_day.strftime(DATEFORMAT)\n    for loc in death_deltas.Location:\n        death_delta = death_deltas.loc[death_deltas.Location == loc, 'DELTA'].values[0]\n        daily_log_deaths.loc[daily_log_deaths.Location == loc, new_day] = daily_log_deaths.loc[daily_log_deaths.Location == loc, last_day] + \\\n            death_delta * DECAY ** i","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confirmed_prediciton = pd.melt(daily_log_deaths[:25], id_vars='Location')\nconfirmed_prediciton['Fatalities'] = to_exp(confirmed_prediciton['value'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confirmed = []\nfatalities = []\nfor id, d, loc in tqdm(test[['ForecastId', 'Date', 'Location']].values):\n    c = to_exp(daily_log_confirmed.loc[daily_log_confirmed.Location == loc, d].values[0])\n    f = to_exp(daily_log_deaths.loc[daily_log_deaths.Location == loc, d].values[0])\n    confirmed.append(c)\n    fatalities.append(f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_submission = test.copy()\nmy_submission['ConfirmedCases'] = confirmed\nmy_submission['Fatalities'] = fatalities\nmy_submission.shape\nmy_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_submission.groupby('Date').sum().tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total = my_submission.groupby('Date')[['ConfirmedCases', 'Fatalities']].sum().reset_index()\n\nfig2 = px.line(pd.melt(total, id_vars=['Date']), x='Date', y='value', color='variable')\n_ = fig2.update_layout(\n    yaxis_type=\"log\",\n    title_text=f'COVID-19 Cumulative Prediction Total [Updated: {TRAIN_END}]'\n)\nfig2.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_submission[[\n    'ForecastId', 'ConfirmedCases', 'Fatalities'\n]].to_csv('submission_1.csv', index=False)\nprint(DECAY)\nmy_submission.head()\nmy_submission.tail()\nmy_submission.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"end = dt.datetime.now()\nprint('Finished', end, (end - start).seconds, 's')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## importing packages\nimport lightgbm as lgb\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## defining constants\nPATH_TRAIN = \"/kaggle/input/covid19-global-forecasting-week-3/train.csv\"\nPATH_TEST = \"/kaggle/input/covid19-global-forecasting-week-3/test.csv\"\n\nPATH_SUBMISSION = \"submission.csv\"\nPATH_OUTPUT = \"output.csv\"\n\nPATH_REGION_METADATA = \"/kaggle/input/covid19-forecasting-metadata/region_metadata.csv\"\nPATH_REGION_DATE_METADATA = \"/kaggle/input/covid19-forecasting-metadata/region_date_metadata.csv\"\n\nVAL_DAYS = 7\nMAD_FACTOR = 0.5\nDAYS_SINCE_CASES = [1, 10, 50, 100, 500, 1000, 5000, 10000]\n\nSEED = 137\n\nLGB_PARAMS = {\"objective\": \"regression\",\n              \"num_leaves\": 5,\n              \"learning_rate\": 0.013,\n              \"bagging_fraction\": 0.91,\n              \"feature_fraction\": 0.81,\n              \"reg_alpha\": 0.13,\n              \"reg_lambda\": 0.13,\n              \"metric\": \"rmse\",\n              \"seed\": SEED\n             }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## reading data\ntrain = pd.read_csv(PATH_TRAIN)\ntest = pd.read_csv(PATH_TEST)\n\nregion_metadata = pd.read_csv(PATH_REGION_METADATA)\nregion_date_metadata = pd.read_csv(PATH_REGION_DATE_METADATA)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## preparing data\ntrain = train.merge(test[[\"ForecastId\", \"Province_State\", \"Country_Region\", \"Date\"]], on = [\"Province_State\", \"Country_Region\", \"Date\"], how = \"left\")\ntest = test[~test.Date.isin(train.Date.unique())]\n\ndf_panel = pd.concat([train, test], sort = False)\n\n# combining state and country into 'geography'\ndf_panel[\"geography\"] = df_panel.Country_Region.astype(str) + \": \" + df_panel.Province_State.astype(str)\ndf_panel.loc[df_panel.Province_State.isna(), \"geography\"] = df_panel[df_panel.Province_State.isna()].Country_Region\n\n# fixing data issues with cummax\ndf_panel.ConfirmedCases = df_panel.groupby(\"geography\")[\"ConfirmedCases\"].cummax()\ndf_panel.Fatalities = df_panel.groupby(\"geography\")[\"Fatalities\"].cummax()\n\n# merging external metadata\ndf_panel = df_panel.merge(region_metadata, on = [\"Country_Region\", \"Province_State\"])\ndf_panel = df_panel.merge(region_date_metadata, on = [\"Country_Region\", \"Province_State\", \"Date\"], how = \"left\")\n\n# label encoding continent\ndf_panel.continent = LabelEncoder().fit_transform(df_panel.continent)\ndf_panel.Date = pd.to_datetime(df_panel.Date, format = \"%Y-%m-%d\")\n\ndf_panel.sort_values([\"geography\", \"Date\"], inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## feature engineering\nmin_date_train = np.min(df_panel[~df_panel.Id.isna()].Date)\nmax_date_train = np.max(df_panel[~df_panel.Id.isna()].Date)\n\nmin_date_test = np.min(df_panel[~df_panel.ForecastId.isna()].Date)\nmax_date_test = np.max(df_panel[~df_panel.ForecastId.isna()].Date)\n\nn_dates_test = len(df_panel[~df_panel.ForecastId.isna()].Date.unique())\n\nprint(\"Train date range:\", str(min_date_train), \" - \", str(max_date_train))\nprint(\"Test date range:\", str(min_date_test), \" - \", str(max_date_test))\n\n# creating lag features\nfor lag in range(1, 41):\n    df_panel[f\"lag_{lag}_cc\"] = df_panel.groupby(\"geography\")[\"ConfirmedCases\"].shift(lag)\n    df_panel[f\"lag_{lag}_ft\"] = df_panel.groupby(\"geography\")[\"Fatalities\"].shift(lag)\n    df_panel[f\"lag_{lag}_rc\"] = df_panel.groupby(\"geography\")[\"Recoveries\"].shift(lag)\n\nfor case in DAYS_SINCE_CASES:\n    df_panel = df_panel.merge(df_panel[df_panel.ConfirmedCases >= case].groupby(\"geography\")[\"Date\"].min().reset_index().rename(columns = {\"Date\": f\"case_{case}_date\"}), on = \"geography\", how = \"left\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## function for preparing features\ndef prepare_features(df, gap):\n    \n    df[\"perc_1_ac\"] = (df[f\"lag_{gap}_cc\"] - df[f\"lag_{gap}_ft\"] - df[f\"lag_{gap}_rc\"]) / df[f\"lag_{gap}_cc\"]\n    df[\"perc_1_cc\"] = df[f\"lag_{gap}_cc\"] / df.population\n    \n    df[\"diff_1_cc\"] = df[f\"lag_{gap}_cc\"] - df[f\"lag_{gap + 1}_cc\"]\n    df[\"diff_2_cc\"] = df[f\"lag_{gap + 1}_cc\"] - df[f\"lag_{gap + 2}_cc\"]\n    df[\"diff_3_cc\"] = df[f\"lag_{gap + 2}_cc\"] - df[f\"lag_{gap + 3}_cc\"]\n    \n    df[\"diff_1_ft\"] = df[f\"lag_{gap}_ft\"] - df[f\"lag_{gap + 1}_ft\"]\n    df[\"diff_2_ft\"] = df[f\"lag_{gap + 1}_ft\"] - df[f\"lag_{gap + 2}_ft\"]\n    df[\"diff_3_ft\"] = df[f\"lag_{gap + 2}_ft\"] - df[f\"lag_{gap + 3}_ft\"]\n    \n    df[\"diff_123_cc\"] = (df[f\"lag_{gap}_cc\"] - df[f\"lag_{gap + 3}_cc\"]) / 3\n    df[\"diff_123_ft\"] = (df[f\"lag_{gap}_ft\"] - df[f\"lag_{gap + 3}_ft\"]) / 3\n\n    df[\"diff_change_1_cc\"] = df.diff_1_cc / df.diff_2_cc\n    df[\"diff_change_2_cc\"] = df.diff_2_cc / df.diff_3_cc\n    \n    df[\"diff_change_1_ft\"] = df.diff_1_ft / df.diff_2_ft\n    df[\"diff_change_2_ft\"] = df.diff_2_ft / df.diff_3_ft\n\n    df[\"diff_change_12_cc\"] = (df.diff_change_1_cc + df.diff_change_2_cc) / 2\n    df[\"diff_change_12_ft\"] = (df.diff_change_1_ft + df.diff_change_2_ft) / 2\n    \n    df[\"change_1_cc\"] = df[f\"lag_{gap}_cc\"] / df[f\"lag_{gap + 1}_cc\"]\n    df[\"change_2_cc\"] = df[f\"lag_{gap + 1}_cc\"] / df[f\"lag_{gap + 2}_cc\"]\n    df[\"change_3_cc\"] = df[f\"lag_{gap + 2}_cc\"] / df[f\"lag_{gap + 3}_cc\"]\n\n    df[\"change_1_ft\"] = df[f\"lag_{gap}_ft\"] / df[f\"lag_{gap + 1}_ft\"]\n    df[\"change_2_ft\"] = df[f\"lag_{gap + 1}_ft\"] / df[f\"lag_{gap + 2}_ft\"]\n    df[\"change_3_ft\"] = df[f\"lag_{gap + 2}_ft\"] / df[f\"lag_{gap + 3}_ft\"]\n\n    df[\"change_1_3_cc\"] = df[f\"lag_{gap}_cc\"] / df[f\"lag_{gap + 3}_cc\"]\n    df[\"change_1_3_ft\"] = df[f\"lag_{gap}_ft\"] / df[f\"lag_{gap + 3}_ft\"]\n    \n    df[\"change_1_7_cc\"] = df[f\"lag_{gap}_cc\"] / df[f\"lag_{gap + 7}_cc\"]\n    df[\"change_1_7_ft\"] = df[f\"lag_{gap}_ft\"] / df[f\"lag_{gap + 7}_ft\"]\n    \n    for case in DAYS_SINCE_CASES:\n        df[f\"days_since_{case}_case\"] = (df[f\"case_{case}_date\"] - df.Date).astype(\"timedelta64[D]\")\n        df.loc[df[f\"days_since_{case}_case\"] < gap, f\"days_since_{case}_case\"] = np.nan\n\n    df[\"country_flag\"] = df.Province_State.isna().astype(int)\n\n    # target variable is log of change from last known value\n    df[\"target_cc\"] = np.log1p(df.ConfirmedCases - df[f\"lag_{gap}_cc\"])\n    df[\"target_ft\"] = np.log1p(df.Fatalities - df[f\"lag_{gap}_ft\"])\n    \n    features = [\n        f\"lag_{gap}_cc\",\n        f\"lag_{gap}_ft\",\n        f\"lag_{gap}_rc\",\n        \"perc_1_ac\",\n        \"perc_1_cc\",\n        \"diff_1_cc\",\n        \"diff_2_cc\",\n        \"diff_3_cc\",\n        \"diff_1_ft\",\n        \"diff_2_ft\",\n        \"diff_3_ft\",\n        \"diff_123_cc\",\n        \"diff_123_ft\",\n        \"diff_change_1_cc\",\n        \"diff_change_2_cc\",\n        \"diff_change_1_ft\",\n        \"diff_change_2_ft\",\n        \"diff_change_12_cc\",\n        \"diff_change_12_ft\",\n        \"change_1_cc\",\n        \"change_2_cc\",\n        \"change_3_cc\",\n        \"change_1_ft\",\n        \"change_2_ft\",\n        \"change_3_ft\",\n        \"change_1_3_cc\",\n        \"change_1_3_ft\",\n        \"change_1_7_cc\",\n        \"change_1_7_ft\",\n        \"days_since_1_case\",\n        \"days_since_10_case\",\n        \"days_since_50_case\",\n        \"days_since_100_case\",\n        \"days_since_500_case\",\n        \"days_since_1000_case\",\n        \"days_since_5000_case\",\n        \"days_since_10000_case\",\n        \"country_flag\",\n        \"lat\",\n        \"lon\",\n        \"continent\",\n        \"population\",\n        \"area\",\n        \"density\",\n        \"target_cc\",\n        \"target_ft\"\n    ]\n    \n    return df[features]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## function for building and predicting using LGBM model\ndef build_predict_lgbm(df_train, df_test, gap):\n    \n    df_train.dropna(subset = [\"target_cc\", \"target_ft\", f\"lag_{gap}_cc\", f\"lag_{gap}_ft\"], inplace = True)\n    \n    target_cc = df_train.target_cc\n    target_ft = df_train.target_ft\n    \n    test_lag_cc = df_test[f\"lag_{gap}_cc\"].values\n    test_lag_ft = df_test[f\"lag_{gap}_ft\"].values\n    \n    df_train.drop([\"target_cc\", \"target_ft\"], axis = 1, inplace = True)\n    df_test.drop([\"target_cc\", \"target_ft\"], axis = 1, inplace = True)\n    \n    categorical_features = [\"continent\"]\n    \n    dtrain_cc = lgb.Dataset(df_train, label = target_cc, categorical_feature = categorical_features)\n    dtrain_ft = lgb.Dataset(df_train, label = target_ft, categorical_feature = categorical_features)\n\n    model_cc = lgb.train(LGB_PARAMS, train_set = dtrain_cc, num_boost_round = 200)\n    model_ft = lgb.train(LGB_PARAMS, train_set = dtrain_ft, num_boost_round = 200)\n    \n    # inverse transform from log of change from last known value\n    y_pred_cc = np.expm1(model_cc.predict(df_test, num_boost_round = 200)) + test_lag_cc\n    y_pred_ft = np.expm1(model_ft.predict(df_test, num_boost_round = 200)) + test_lag_ft\n    \n    return y_pred_cc, y_pred_ft, model_cc, model_ft","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## function for predicting moving average decay model\ndef predict_mad(df_test, gap, val = False):\n    \n    df_test[\"avg_diff_cc\"] = (df_test[f\"lag_{gap}_cc\"] - df_test[f\"lag_{gap + 3}_cc\"]) / 3\n    df_test[\"avg_diff_ft\"] = (df_test[f\"lag_{gap}_ft\"] - df_test[f\"lag_{gap + 3}_ft\"]) / 3\n\n    if val:\n        y_pred_cc = df_test[f\"lag_{gap}_cc\"] + gap * df_test.avg_diff_cc - (1 - MAD_FACTOR) * df_test.avg_diff_cc * np.sum([x for x in range(gap)]) / VAL_DAYS\n        y_pred_ft = df_test[f\"lag_{gap}_ft\"] + gap * df_test.avg_diff_ft - (1 - MAD_FACTOR) * df_test.avg_diff_ft * np.sum([x for x in range(gap)]) / VAL_DAYS\n    else:\n        y_pred_cc = df_test[f\"lag_{gap}_cc\"] + gap * df_test.avg_diff_cc - (1 - MAD_FACTOR) * df_test.avg_diff_cc * np.sum([x for x in range(gap)]) / n_dates_test\n        y_pred_ft = df_test[f\"lag_{gap}_ft\"] + gap * df_test.avg_diff_ft - (1 - MAD_FACTOR) * df_test.avg_diff_ft * np.sum([x for x in range(gap)]) / n_dates_test\n\n    return y_pred_cc, y_pred_ft","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## building lag x-days models\ndf_train = df_panel[~df_panel.Id.isna()]\ndf_test_full = df_panel[~df_panel.ForecastId.isna()]\n\ndf_preds_val = []\ndf_preds_test = []\n\nfor date in df_test_full.Date.unique():\n    \n    print(\"Processing date:\", date)\n    \n    # ignore date already present in train data\n    if date in df_train.Date.values:\n        df_pred_test = df_test_full.loc[df_test_full.Date == date, [\"ForecastId\", \"ConfirmedCases\", \"Fatalities\"]].rename(columns = {\"ConfirmedCases\": \"ConfirmedCases_test\", \"Fatalities\": \"Fatalities_test\"})\n        \n        # multiplying predictions by 41 to not look cool on public LB\n        #df_pred_test.ConfirmedCases_test = df_pred_test.ConfirmedCases_test * 41\n        #df_pred_test.Fatalities_test = df_pred_test.Fatalities_test * 41\n    else:\n        df_test = df_test_full[df_test_full.Date == date]\n        \n        gap = (pd.Timestamp(date) - max_date_train).days\n        \n        if gap <= VAL_DAYS:\n            val_date = max_date_train - pd.Timedelta(VAL_DAYS, \"D\") + pd.Timedelta(gap, \"D\")\n\n            df_build = df_train[df_train.Date < val_date]\n            df_val = df_train[df_train.Date == val_date]\n            \n            X_build = prepare_features(df_build, gap)\n            X_val = prepare_features(df_val, gap)\n            \n            y_val_cc_lgb, y_val_ft_lgb, _, _ = build_predict_lgbm(X_build, X_val, gap)\n            y_val_cc_mad, y_val_ft_mad = predict_mad(df_val, gap, val = True)\n            \n            df_pred_val = pd.DataFrame({\"Id\": df_val.Id.values,\n                                        \"ConfirmedCases_val_lgb\": y_val_cc_lgb,\n                                        \"Fatalities_val_lgb\": y_val_ft_lgb,\n                                        \"ConfirmedCases_val_mad\": y_val_cc_mad,\n                                        \"Fatalities_val_mad\": y_val_ft_mad,\n                                       })\n\n            df_preds_val.append(df_pred_val)\n\n        X_train = prepare_features(df_train, gap)\n        X_test = prepare_features(df_test, gap)\n\n        y_test_cc_lgb, y_test_ft_lgb, model_cc, model_ft = build_predict_lgbm(X_train, X_test, gap)\n        y_test_cc_mad, y_test_ft_mad = predict_mad(df_test, gap)\n        \n        if gap == 1:\n            model_1_cc = model_cc\n            model_1_ft = model_ft\n            features_1 = X_train.columns.values\n        elif gap == 14:\n            model_14_cc = model_cc\n            model_14_ft = model_ft\n            features_14 = X_train.columns.values\n        elif gap == 28:\n            model_28_cc = model_cc\n            model_28_ft = model_ft\n            features_28 = X_train.columns.values\n\n        df_pred_test = pd.DataFrame({\"ForecastId\": df_test.ForecastId.values,\n                                     \"ConfirmedCases_test_lgb\": y_test_cc_lgb,\n                                     \"Fatalities_test_lgb\": y_test_ft_lgb,\n                                     \"ConfirmedCases_test_mad\": y_test_cc_mad,\n                                     \"Fatalities_test_mad\": y_test_ft_mad,\n                                    })\n    \n    df_preds_test.append(df_pred_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## validation score\ndf_panel = df_panel.merge(pd.concat(df_preds_val, sort = False), on = \"Id\", how = \"left\")\ndf_panel = df_panel.merge(pd.concat(df_preds_test, sort = False), on = \"ForecastId\", how = \"left\")\n\nrmsle_cc_lgb = np.sqrt(mean_squared_error(np.log1p(df_panel[~df_panel.ConfirmedCases_val_lgb.isna()].ConfirmedCases), np.log1p(df_panel[~df_panel.ConfirmedCases_val_lgb.isna()].ConfirmedCases_val_lgb)))\nrmsle_ft_lgb = np.sqrt(mean_squared_error(np.log1p(df_panel[~df_panel.Fatalities_val_lgb.isna()].Fatalities), np.log1p(df_panel[~df_panel.Fatalities_val_lgb.isna()].Fatalities_val_lgb)))\n\nrmsle_cc_mad = np.sqrt(mean_squared_error(np.log1p(df_panel[~df_panel.ConfirmedCases_val_mad.isna()].ConfirmedCases), np.log1p(df_panel[~df_panel.ConfirmedCases_val_mad.isna()].ConfirmedCases_val_mad)))\nrmsle_ft_mad = np.sqrt(mean_squared_error(np.log1p(df_panel[~df_panel.Fatalities_val_mad.isna()].Fatalities), np.log1p(df_panel[~df_panel.Fatalities_val_mad.isna()].Fatalities_val_mad)))\n\nprint(\"LGB CC RMSLE Val of\", VAL_DAYS, \"days for CC:\", round(rmsle_cc_lgb, 2))\nprint(\"LGB FT RMSLE Val of\", VAL_DAYS, \"days for FT:\", round(rmsle_ft_lgb, 2))\nprint(\"LGB Overall RMSLE Val of\", VAL_DAYS, \"days:\", round((rmsle_cc_lgb + rmsle_ft_lgb) / 2, 2))\nprint(\"\\n\")\nprint(\"MAD CC RMSLE Val of\", VAL_DAYS, \"days for CC:\", round(rmsle_cc_mad, 2))\nprint(\"MAD FT RMSLE Val of\", VAL_DAYS, \"days for FT:\", round(rmsle_ft_mad, 2))\nprint(\"MAD Overall RMSLE Val of\", VAL_DAYS, \"days:\", round((rmsle_cc_mad + rmsle_ft_mad) / 2, 2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## preparing submission file\ndf_test = df_panel.loc[~df_panel.ForecastId.isna(), [\"ForecastId\", \"Country_Region\", \"Province_State\", \"Date\",\n                                                     \"ConfirmedCases_test\", \"ConfirmedCases_test_lgb\", \"ConfirmedCases_test_mad\",\n                                                     \"Fatalities_test\", \"Fatalities_test_lgb\", \"Fatalities_test_mad\"]].reset_index()\n\ndf_test[\"ConfirmedCases\"] = 0.13 * df_test.ConfirmedCases_test_lgb + 0.87 * df_test.ConfirmedCases_test_mad\ndf_test[\"Fatalities\"] = 0.13 * df_test.Fatalities_test_lgb + 0.87 * df_test.Fatalities_test_mad\n\n# Since LGB models don't predict these geographies well\ndf_test.loc[df_test.Country_Region.isin([\"Diamond Princess\", \"MS Zaandam\"]), \"ConfirmedCases\"] = df_test[df_test.Country_Region.isin([\"Diamond Princess\", \"MS Zaandam\"])].ConfirmedCases_test_mad.values\ndf_test.loc[df_test.Country_Region.isin([\"Diamond Princess\", \"MS Zaandam\"]), \"Fatalities\"] = df_test[df_test.Country_Region.isin([\"Diamond Princess\", \"MS Zaandam\"])].Fatalities_test_mad.values\n\ndf_test.loc[df_test.Date.isin(df_train.Date.values), \"ConfirmedCases\"] = df_test[df_test.Date.isin(df_train.Date.values)].ConfirmedCases_test.values\ndf_test.loc[df_test.Date.isin(df_train.Date.values), \"Fatalities\"] = df_test[df_test.Date.isin(df_train.Date.values)].Fatalities_test.values\n\ndf_submission = df_test[[\"ForecastId\", \"ConfirmedCases\", \"Fatalities\"]]\ndf_submission.ForecastId = df_submission.ForecastId.astype(int)\n\ndf_submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## writing final submission and complete output\ndf_submission.to_csv('submission_2.csv', index = False)\ndf_test.to_csv(PATH_OUTPUT, index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_0 = pd.read_csv('submission_0.csv')\nsubmission_1 = pd.read_csv('submission_1.csv')\nsubmission_2 = pd.read_csv('submission_2.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_1.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_2.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = submission_1.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission['ConfirmedCases'] = 0.8*(0.6*submission_1['ConfirmedCases'].values+0.4*submission_2['ConfirmedCases'].values)+0.2*submission_0['ConfirmedCases'].values\nsubmission['Fatalities'] = 0.8*(0.6*submission_1['Fatalities'].values+0.4*submission_2['Fatalities'].values)+0.2*submission_0['Fatalities'].values\nsubmission.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}