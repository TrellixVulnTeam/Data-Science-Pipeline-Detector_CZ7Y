{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import preprocessing\nimport time\nfrom datetime import datetime\nfrom scipy import integrate, optimize\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# ML libraries\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom xgboost import plot_importance, plot_tree\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn import linear_model\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\n\nfrom tensorflow.python.ops import variables\nfrom tensorflow.python.framework import ops\nimport tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()\n\nfrom fancyimpute import KNN \n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Load Data\n\n# Train is 22338 by 6. Each column is Id, Province_State, Country_Region, Date, ConfirmedCases, Fatalities\n# Test is 13158 by 4 ForecastId, Province_State, Country_Region, Date\ntest = pd.read_csv(\"../input/covid19-global-forecasting-week-3/test.csv\")\ntrain = pd.read_csv(\"../input/covid19-global-forecasting-week-3/train.csv\")\n\n# Separate by date\ntrain_df = train.query(\"Date<='2020-03-25'\")\nvalid_df = train.query(\"Date>'2020-03-25'\")\n\n# Convert dates to integers, starting from 0\ntest[\"Date\"] = (pd.to_datetime(test['Date']) - pd.to_datetime(min(train['Date']))).dt.days\nvalid_df[\"Date\"] = (pd.to_datetime(valid_df['Date']) - pd.to_datetime(min(train['Date']))).dt.days\ntrain_df[\"Date\"] = (pd.to_datetime(train_df['Date']) - pd.to_datetime(min(train['Date']))).dt.days\n\ntrain_data = train_df.to_numpy()\ntest_data = test.to_numpy()\nvalid_data = valid_df.to_numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Helper functions for managing the data\n\ndef get_place(row):\n    place = row[2]\n    if isinstance(row[1], str):\n        place = row[1]\n    return place\n\n# Returns a dictionary, keyed by places, of their data\ndef separate_by_place(data):\n    place_data = {}\n    for row in data:\n        place = get_place(row)\n        if place in place_data:\n            place_data[place].append(row)\n        else:\n            place_data[place] = [row]\n    return place_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate error\n\ndef rmsle(y_test, predictions):\n    return np.sqrt(mean_squared_log_error(y_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Linear Regression Test, this is the baseline\n# Optimal performance appears to be at 5th order death, 4th order case\n# Performance actually worsens if 0's are removed\n\ndef linear_reg(order, train, valid):\n    # Split by place\n    train_place = separate_by_place(train)\n    valid_place = separate_by_place(valid)\n    \n    case_predictions = []\n    case_actual = []\n    death_predictions = []\n    death_actual = []\n\n    poly = PolynomialFeatures(order)\n    \n    for place in train_place.keys():\n        train = np.asarray(train_place[place])\n        valid = np.asarray(valid_place[place])\n        \n        # Remove days where there were no cases reported yet\n        train_del = np.delete(train, np.where(train[:, 4] == 0), axis=0)\n        if len(train_del) > 0:\n            train = train_del\n\n        days = poly.fit_transform(train[:, [3]])\n        cases = train[:, [4, 5]]\n        days_predict = poly.fit_transform(valid[:, [3]])\n        cases_predict = valid[:, [4, 5]]\n\n        d_reg = LinearRegression().fit(days, cases[:, [1]])\n        death_predictions.extend(d_reg.predict(days_predict).flatten())\n        death_actual.extend(cases_predict[:, [1]])\n\n        c_reg = LinearRegression().fit(days, cases[:, [0]])\n        case_predictions.extend(c_reg.predict(days_predict).flatten())\n        case_actual.extend(cases_predict[:, [0]])\n\n    # Remove negatives\n    case_predictions = np.asarray(case_predictions)\n    case_predictions = np.where(case_predictions < 0, 0, case_predictions)\n    death_predictions = np.asarray(death_predictions)\n    death_predictions = np.where(death_predictions < 0, 0, death_predictions)\n\n    c_error = rmsle(case_actual, case_predictions)\n    d_error = rmsle(death_actual, death_predictions)\n    print(\"Death error:\", d_error, \"\\tCase error:\", c_error)\n    \nfor i in range(10):    \n    print(\"Order:\", i)\n    linear_reg(i, train_data, valid_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now try a neural network\n# The goal of the network is to compute the factor to multiply previous day by\nNUM_FEATURES = 92\n\ntf.reset_default_graph()\ngraph = tf.Graph()\n\nlearning_rate = 0.001\n\nwith graph.as_default():\n    X = tf.placeholder(tf.float32, (None, NUM_FEATURES))\n    labels = tf.placeholder(tf.float32, (None, 2))\n    with tf.name_scope(\"fcn1\"):\n        W1 = tf.get_variable('W1', shape=(NUM_FEATURES, 500), initializer=tf.keras.initializers.glorot_normal())\n        b1 = tf.Variable(tf.zeros((500,)), trainable=True)\n        X1 = tf.add(tf.matmul(X, W1), b1)\n        X1 = tf.layers.batch_normalization(X1)\n        X1 = tf.nn.leaky_relu(X1)\n    with tf.name_scope(\"fcn2\"):\n        W2 = tf.get_variable('W2', shape=(500, 500), initializer=tf.keras.initializers.glorot_normal())\n        b2 = tf.Variable(tf.zeros((500,)), trainable=True)\n        X2 = tf.add(tf.matmul(X1, W2), b2)\n        X2 = tf.layers.batch_normalization(X2)\n        X2 = tf.nn.leaky_relu(X2)\n    with tf.name_scope(\"fcn3\"):\n        W3 = tf.get_variable('W3', shape=(500, 100), initializer=tf.keras.initializers.glorot_normal())\n        b3 = tf.Variable(tf.zeros((100,)), trainable=True)\n        X3 = tf.add(tf.matmul(X2, W3), b3)\n        X3 = tf.layers.batch_normalization(X3)\n        X3 = tf.nn.leaky_relu(X3)\n    with tf.name_scope(\"fcn4\"):\n        W4 = tf.get_variable('W4', shape=(100, 10), initializer=tf.keras.initializers.glorot_normal())\n        b4 = tf.Variable(tf.zeros((10,)), trainable=True)\n        X4 = tf.add(tf.matmul(X3, W4), b4)\n        X4 = tf.layers.batch_normalization(X4)\n        X4 = tf.nn.leaky_relu(X4)\n    with tf.name_scope(\"fcn5\"):\n        W5 = tf.get_variable('W5', shape=(10, 2), initializer=tf.keras.initializers.glorot_normal())\n        b5 = tf.Variable(tf.zeros((2,)), trainable=True)\n        predictions = tf.add(tf.matmul(X4, W5), b5)\n    loss = tf.losses.mean_squared_error(labels, predictions)\n\n    optimizer = tf.train.AdagradOptimizer(learning_rate)\n    train_op = optimizer.minimize(loss)\n    init = tf.global_variables_initializer()\n    saver = tf.train.Saver()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_nn_train_data(country_data):\n    # Sort by date\n    country_data = np.asarray(country_data)\n    indices = country_data[:, 3].argsort()\n    country_data = country_data[indices]\n    \n    # Remove rows without new cases\n#     indices = [0]\n#     for i in range(len(country_data) - 1):\n#         train_y = country_data[i+1, [4, 5]] - country_data[i, [4, 5]]\n#         if np.sum(train_y) == 0.0:\n#             continue\n#         indices.append(i)\n#     country_data = country_data[indices]\n    \n    # Add two columns: one each for previous day total cases and deaths\n    added_data = np.c_[country_data, np.zeros(len(country_data))]\n    added_data = np.c_[added_data, np.zeros(len(country_data))]\n    for i in range(1, len(country_data)):\n        added_data[i, [-2, -1]] = country_data[i-1, [4, 5]]\n    x_indices = [3] + [i for i in range(6, len(added_data[0]))]\n    train_x = added_data[:, x_indices]\n    \n    # Y data is growth from previous day\n    train_y = np.zeros((len(country_data), 2))\n    for i in range(len(country_data) - 1):\n        train_y[i] = country_data[i+1, [4, 5]] - country_data[i, [4, 5]]\n    train_y[len(country_data) - 1] = train_y[len(country_data) - 2]\n    \n    return train_x, train_y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def debug_grads(sess, feed_dict):\n    var_list = (variables.trainable_variables() + ops.get_collection(\n        ops.GraphKeys.TRAINABLE_RESOURCE_VARIABLES))\n    print('variables')\n    for v in var_list:\n        print('  ', v.name)\n    # get all gradients\n    grads_and_vars = optimizer.compute_gradients(loss)\n    # train_op = optimizer.apply_gradients(grads_and_vars)\n\n    zipped_val = sess.run(grads_and_vars, feed_dict=feed_dict)\n\n    for rsl, tensor in zip(zipped_val, grads_and_vars):\n        print('-----------------------------------------')\n        print('name', tensor[0].name.replace('/tuple/control_dependency_1:0', '').replace('gradients/', ''))\n        print('gradient', rsl[0])\n        print('value', rsl[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# First, add some data (first WDI obtained here https://www.kaggle.com/sambitmukherjee/covid-19-data-adding-world-development-indicators/output)\ntrain_wdi = pd.read_csv(\"../input/combine-wdi-covid/train_with_WDI.csv\")\ntest_wdi = pd.read_csv(\"../input/combine-wdi-covid/test_with_WDI.csv\")\ntest_norm = pd.read_csv(\"../input/covid19-global-forecasting-week-3/test.csv\")\n\n# Add personality info\npersonality_info = pd.read_csv(\"../input/covid19-country-data-wk3-release/Data Join - RELEASE.csv\")\npersonality_info = personality_info.rename(columns={\"TRUE POPULATION\": \"TRUE_POPULATION\"})\npersonality_info.pct_in_largest_city = personality_info.pct_in_largest_city.apply(lambda x: x.replace('%', ''))\npersonality_info.TRUE_POPULATION = personality_info.TRUE_POPULATION.apply(lambda x: x.replace(',', ''))\ntrain_wdi = pd.merge(train_wdi, personality_info,  how='left', on=['Province_State','Country_Region'])\n\n# Add leader info https://www.kaggle.com/lunatics/global-politcs-and-governance-data-apr-2020\nleader_info = pd.read_csv(\"../input/politics/politics_apr2020.csv\")\ntrain_wdi = pd.merge(train_wdi, leader_info,  how='left', on=['Country_Region'])\n\n# Add immunization coverage https://www.kaggle.com/lsind18/who-immunization-coverage\nfor filename in os.listdir(\"../input/who-immunization-coverage\"):\n    immun_info = pd.read_csv(\"../input/who-immunization-coverage/\" + filename).iloc[:,0:2]\n    immun_info = immun_info.rename(columns={\"Country\": \"Country_Region\", \"2018\": filename})\n    train_wdi = pd.merge(train_wdi, immun_info,  how='left', on=['Country_Region'])\n\n# Replace bad data with nan\ntrain_wdi = train_wdi.apply(lambda x: x.replace('#NULL!', np.nan))\ntrain_wdi = train_wdi.apply(lambda x: x.replace('#DIV/0!', np.nan))\ntrain_wdi = train_wdi.apply(lambda x: x.replace('#N/A', np.nan))\ntrain_wdi = train_wdi.apply(lambda x: x.replace('N.A.', np.nan))\n\n# Separate by date \nvalid_wdi = train_wdi.query(\"Date>'2020-04-03'\")\n# Remove for public leaderboard\n# train_wdi = train_wdi.query(\"Date<='2020-04-03'\")\n\n# Convert dates to integers, starting from 0\ntest_wdi[\"Date\"] = (pd.to_datetime(test_wdi['Date']) - pd.to_datetime(min(train_wdi['Date']))).dt.days\nvalid_wdi[\"Date\"] = (pd.to_datetime(valid_wdi['Date']) - pd.to_datetime(min(train_wdi['Date']))).dt.days\ntest_norm[\"Date\"] = (pd.to_datetime(test_norm['Date']) - pd.to_datetime(min(train_wdi['Date']))).dt.days\ntrain_wdi[\"Date\"] = (pd.to_datetime(train_wdi['Date']) - pd.to_datetime(min(train_wdi['Date']))).dt.days\n\ntrain_wdi = train_wdi.to_numpy()\ntest_wdi = test_wdi.to_numpy()\nvalid_wdi = valid_wdi.to_numpy()\ntest_norm = test_norm.to_numpy()\n\n# Cast to float\nindices = [i for i in range(3, len(train_wdi[0]))]\ntrain_wdi[:, indices] = train_wdi[:, indices].astype('float64') \n\n# Apply k nearest neighbors to obtain data for nan\ntrain_wdi[:, indices] = KNN(k=10).fit_transform(train_wdi[:, indices])\n\ntrain_place = separate_by_place(train_wdi)\nvalid_place = separate_by_place(valid_wdi)\n\ntrain_x = {}\ntrain_y = {}\nvalid_x, valid_y = make_nn_train_data(valid_wdi)\nfor place in train_place.keys():\n    train_x[place], train_y[place] = make_nn_train_data(train_place[place])\n    \n# Normalize the data\nall_x = -1\nall_y = -1\n# Train set\nfor place in train_x.keys():\n    try:\n        all_x = np.vstack([all_x, train_x[place]])\n        all_y = np.vstack([all_y, train_y[place]])\n    except:\n        all_x = train_x[place]\n        all_y = train_y[place]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NUM_FEATURES = all_x.shape[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NUM_EPOCHS = 5000\nDEBUG = False\nlearning_rate = 0.007\ndays_to_extend = 60\nrestore = False\nsave = False\n\n# SKLearn scalers\nx_scaler = StandardScaler()\nx_scaler.fit(all_x)\ny_scaler = StandardScaler()\ny_scaler.fit(all_y)\n\nwith tf.Session(graph=graph) as sess:\n    if restore:\n        saver.restore(sess, \"tmp/model.ckpt\")\n        NUM_EPOCHS = 0\n    else:\n        init.run()\n    for epoch in range(NUM_EPOCHS):\n        avg_loss = 0\n        # Train\n        for place in train_x.keys():\n            batch_x = train_x[place]\n            batch_y = train_y[place]\n            standardized_x = x_scaler.transform(batch_x)\n            standardized_y = y_scaler.transform(batch_y)\n            feed_dict = {X: standardized_x, labels: standardized_y}\n            if DEBUG:\n                debug_grads(sess, feed_dict)\n            _, loss_val, outs = sess.run([train_op, loss, predictions], feed_dict=feed_dict)\n            avg_loss += loss_val\n        print(epoch, \"Total Loss\", avg_loss)\n            \n        # Test on CV set\n#         standardized_x = x_scaler.transform(valid_x)\n#         standardized_y = y_scaler.transform(valid_y)\n#         feed_dict = {X: standardized_x, labels: standardized_y}\n#         loss_validation = sess.run(loss, feed_dict=feed_dict)\n        \n    # Make the predictions\n    for place in train_x.keys():\n        for day in range(days_to_extend):\n            # Predict change from last day\n            old_row = [train_x[place][-1]]\n            standardized_x = x_scaler.transform(old_row)\n            feed_dict = {X: standardized_x}\n            outs = sess.run(predictions, feed_dict=feed_dict)\n            preds = y_scaler.inverse_transform(outs)\n            # Insert new row\n            # 3 (date) is index 0, last two indices are total cases as of yesterday\n            new_row = np.zeros(old_row[0].shape) + old_row[0]\n            new_row[0] += 1\n            # Check for 0\n            if preds[0][0] >= 0:\n                new_row[-2] += preds[0][0]\n            if preds[0][1] >= 0:\n                new_row[-1] += preds[0][1]\n            train_x[place] = np.vstack([train_x[place], new_row])\n            \n    # Save\n    if save:\n        save_path = saver.save(sess, \"tmp/model.ckpt\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make predictions\nmy_columns = [\"ForecastId\", \"ConfirmedCases\", \"Fatalities\"]\npredictions = []\nfor row in test_norm:\n    # ForecastId, cases, mortality\n    place = get_place(row)\n    date = row[3]\n    new_row = [row[0], train_x[place][date][-2], train_x[place][date][-1]]\n    predictions.append(new_row)\ndf = pd.DataFrame(predictions, columns=my_columns) \ndf.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}