{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# imports\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom scipy import stats, special\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import interpolate\nimport json\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import lognorm\nfrom scipy.optimize import curve_fit\nimport string\nfrom scipy.integrate import quad\n\nfrom sklearn import mixture\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PolynomialFeatures, StandardScaler\nfrom sklearn.linear_model import SGDRegressor, LinearRegression, Lasso, Ridge, LogisticRegression\nfrom sklearn.base import clone\nfrom sklearn.pipeline import Pipeline, make_pipeline\n\n\n# https://images.plot.ly/plotly-documentation/images/python_cheat_sheet.pdf\n# https://www.apsnet.org/edcenter/disimpactmngmnt/topc/EpidemiologyTemporal/Pages/ModellingProgress.aspx\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Motivation\nPredict and model the COVID-19 pandemic and compete in the [Global Forecasting (Week 3) Challenge](https://www.kaggle.com/c/covid19-global-forecasting-week-3)\n\n# Inspirations\n* https://www.kaggle.com/saga21/covid-global-forecast-sir-model-ml-regressions#4.-Predictions-for-the-early-stages-of-the-transmission-\n\n# Intuition\nIn this work, I assume that number of cases and fatalities per day follow a bell curve, ie. initially new cases start to rise as more people get infected. After some government intervention, various environmental factors, social distancing and people start to recover, the number of new cases and fatalities will eventually start to decline.\n\n# Algorithm\n1. Obtain daily new cases and new fatalities for each country / province so that we can capture the downward slope as well, just remember to translate this back to cumulative for submission\n2. Fit Gaussian curve for each country / province group. \n3. I used a reference Gaussian model of New York City, US as the index distribution for these scenarios:\n * For those instances when a Gaussian can't be fitted\n * For ConfirmedCases, if maximum number of daily cases is less than 50\n * For ConfirmedCases, if number of daily cases is less than 1000 but the predicted Gaussian amplitude is over 1000\n * For ConfirmedCases, if estimated mean is over 150, then use the NYC distribution\n \n I scale down the NYC model to match the highest number of case reported in a day for the country of interest. I arbitrarily chose New York City because it has a good number of cases so the model should be robust. \n \n Since DailyFatalities is underestimated, I scaled the predictions by a factor of 2. The sigma in the ConfirmedCases model have been adjusted by a factor of 2.5 as well to compensate for the underestimation.\n \n4. I observed that the Gaussian fit tend to underestimate the numbers so I scaled the fitted model to match the maximum observed daily case / fatality.\n5. Use the model to predict and calculate the cumulative cases and fatalities.\n\nI find that the initial gaussian_param to curve_fit() can play a big role. The numbers were determined empirically.\n\n# References\n* https://console.cloud.google.com/marketplace/details/bigquery-public-datasets/covid19-public-data-program?_ga=2.96592223.-1125899609.1585633504&pli=1\n* https://www.nytimes.com/interactive/2020/04/03/world/coronavirus-flatten-the-curve-countries.html\n* https://coronavirus.jhu.edu/map.html\n* http://phy224.ca/19-curvefit/index.html\n\n# Evaluation\nTo have a public leaderboard for this forecasting task, we will be using data from 7 days before to 7 days after competition launch. Only use data prior to 2020-03-26 for predictions on the public leaderboard period. Use up to and including the most recent data for predictions on the private leaderboard period.\n\nPublic Leaderboard Period - 2020-03-26 - 2020-04-08\nPrivate Leaderboard Period - 2020-04-09 - 2020-05-07\n\n# Submission\nForecastId,ConfirmedCases,Fatalities\n1,10,0\n2,10,0\n3,10,0\netc.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://stackoverflow.com/questions/56176657/how-to-fit-data-with-log-normal-cdf\n# y = a * log10(b * x + h) + k\n\ndef fgaussian(x,a,x0,sigma):\n    return a*np.exp(-(x-x0)**2/(2*sigma**2))\n\ndef ferf(x, a, b, c, d):\n    return d + 0.5*c*(1 + special.erf(a*(x-b)))\n\ndef ftriangle (z, a, b, c):\n    y = np.zeros(z.shape)\n    y[z <= a] = 0\n    y[z >= c] = 0\n    first_half = np.logical_and(a < z, z <= b)\n    y[first_half] = (z[first_half]-a) / (b-a)\n    second_half = np.logical_and(b < z, z < c)\n    y[second_half] = (c-z[second_half]) / (c-b)\n    return y\n\ndef flog10(x, a, b, h, k):\n    return a*np.log10(b*x + h) + k\n\ndef fsigmoid(x, a, b):\n    return 1.0 / (1.0 + np.exp(-a*(x-b)))\n\ndef fexp(t, a, b, alpha):\n    return a - b * np.exp(-alpha * t)\n\ndef fexp2(x, a, b, c, d):\n    return a * np.exp(b * x) + c * np.exp(d * x)\n\ndef fpow(x, a, b, c):\n    return c * pow(a, x) + b\n\ndef flog(x, a, b, c):\n    return c * np.log(x * a) + b\n\ndef poly_transform_to_ln(x):\n    \"\"\"polycyclic disease model transform x is within 0.11111 to 0.99999\"\"\"\n    return np.log( x / (1-x) )\n\ndef fline(x, a, b):\n    \"\"\"polycyclic disease model transform x is within 0.11111 to 0.99999\"\"\"\n    return a*x + b\n\ndef get_country_sum(df, country='Italy'):\n    df_country = df[df['Country_Region']==country]\n    df_country = df_country.groupby('Date').agg({'ConfirmedCases':['sum'], 'Fatalities':['sum']}).reset_index()\n    df_country.columns = ['Date','ConfirmedCases','Fatalities']\n    df_country['Country_Region'] = country\n    \n    return df_country\n\ndef fit(fun,x,y,params=None):\n#     params=[100,80,5]\n    return curve_fit(fgaussian,x,y,maxfev=2000,method='lm',p0=params)\n#     return curve_fit(fun,x,y,maxfev=2000,method='trf',loss='huber',p0=params)\n\ndef country_slice(df, country='China', province=None):\n    if province is None or pd.isna(province):\n        return df[(df['Country_Region']==country) & (pd.isna(df['Province_State']) == True) ]\n    else:\n        return df[(df['Country_Region']==country) & (df['Province_State']==province)]\n\n    \ndef do_fit_fun(x, y, fun=fgaussian, params=None):\n    x = np.array(x)\n    y = np.array(y)\n    \n    filter_nan_inf = lambda y: (np.isnan(y)==False) & (np.isinf(y)==False)\n    x = x[filter_nan_inf(y)]\n    y = y[filter_nan_inf(y)]\n    \n#     print(f'x={x}')\n#     print(f'y={y}')\n#     print(f'params={params}')\n#     print(f'fun={fun}')\n    \n    if params:\n#         popt, pcov = curve_fit(fun, x, y, p0=params)\n        popt, pcov = fit(fun, x, y, params=params)\n    else:\n#         popt, pcov = curve_fit(fun, x, y)\n        popt, pcov = fit(fun, x, y)\n    \n    perr = mean_squared_error(y, fun(x,*popt))\n\n    label = 'fit: ' + ' '.join([f'{a} = %.2f' for a in string.ascii_lowercase[:len(popt)]]) % tuple(popt)\n#     label = 'fit: a = %.2f b = %.2f' % tuple(popt) #locl = %.2f scale = %.2f\n                                    \n#     print(f'noparams! popt={popt}, pcov={pcov}, err={perr}, label={label}')\n\n    return popt, perr\n\ndef do_fit(df, y_label, gaussian_params=[1000, 60, 2]):\n    y = df[y_label]\n    x = list(range(1,len(y)+1))\n    \n    ret = {'Country_Region':df['Country_Region'].drop_duplicates().tolist()[0], \n           'Province_State':df['Province_State'].drop_duplicates().tolist()[0],\n           'total_cases':y.sum(), 'max_cases':y.max()\n          }\n    try:\n        popt, perr = do_fit_fun(x, y, fun=fgaussian, params=gaussian_params)\n        ret['fgaussian_popt_a'] = popt[0]\n        ret['fgaussian_popt_x0'] = popt[1]\n        ret['fgaussian_popt_sd'] = popt[2]\n        ret['fgaussian_perr'] = perr\n    except:\n        ret['fgaussian_popt_a'] = None\n        ret['fgaussian_popt_x0'] = None\n        ret['fgaussian_popt_sd'] = None\n        ret['fgaussian_perr'] = None\n        \n    return ret\n\n\ndef nyc_scale(df_fit_confirmed, country, province=None, ref_country='US', ref_province='New York', metric='DailyConfirmedCases'):\n# def nyc_scale(df_fit_confirmed, country, province=None, ref_country='France', ref_province=None, metric='DailyConfirmedCases'):\n# def nyc_scale(df_fit_confirmed, country, province=None, ref_country='Italy', ref_province=None, metric='DailyConfirmedCases'):\n    \"\"\"\n    Use the NYC gaussian distribution as reference and scale by max_cases\n    \n    Returns popt and scaling_factor\n    \"\"\"\n#     nyc = country_slice(df_fit_confirmed,'US','New York')  # NYC is the index\n    nyc = country_slice(df_fit_confirmed, ref_country, ref_province)\n    nyc_opts = np.array(nyc[['fgaussian_popt_a','fgaussian_popt_x0','fgaussian_popt_sd']])[0]\n    print(f\"nyc_opts={nyc_opts}\")\n\n#     country = 'Australia'\n#     province = 'Tasmania'\n    # province = 'Virginia'\n    # province = 'Massachusetts'\n    tmp = country_slice(df_fit_confirmed,country,province)\n    if metric == 'DailyConfirmedCases':\n        scaling_factor = np.array(tmp['max_cases']) /  np.array(nyc['max_cases'])\n    else:\n        scaling_factor = 2.  # DailyFatalities is small, so make it bigger\n        \n#     print(f\"province={province}, tmp.shape={tmp.shape}, province_max={tmp['max_cases']}, nyc_max={nyc['max_cases']}, scaling_factor={scaling_factor}\")\n\n    # new_york_popts = [5093.862731, 63.169327, 3.791986 ]\n\n    f = fgaussian\n    tmp = country_slice(train, country=country, province=province)\n    y = np.array(tmp[ metric ])\n    x = np.array(list(range(len(y))))\n    # popt, pcov = curve_fit(f, x, y, p0=[100, 60, 1])\n    popt = nyc_opts\n#     print(popt)\n#     plt.plot(x,y,'b.')\n\n#     plt.plot(x,scaling_factor*f(x,*popt),'r.')\n    \n    return popt, scaling_factor[0]\n\n\ndef get_popts_scaling_factory(fits, df_fit, country, province, x, fun=fgaussian, min_cases=50, max_x0=150, max_a=1000, metric='DailyConfirmedCases'):\n    \"\"\"\n    Handle countries with special cases.\n    \n    - Colombia was predicted to have a mean of over 130 which is too far out.\n    - Zimbabwe has fewer than 10 cases\n    \n    \"\"\"\n    scaling_factor = 1.\n    try:\n        popts = np.array(fits[['fgaussian_popt_a','fgaussian_popt_x0','fgaussian_popt_sd']])[0]\n#         if pd.isna(popts[0]) or (np.array(fits['max_cases']) < min_cases) or (popts[1] > 50 and popts[1] < 70):\n#             raise Exception\n#         if pd.isna(popts[0]) or (metric=='DailyConfirmedCases' and (np.array(fits['max_cases']) < min_cases) or (np.array(fits['fgaussian_popt_x0']) > max_x0)):\n        if pd.isna(popts[0]) or (metric=='DailyConfirmedCases' and (np.array(fits['max_cases']) < min_cases) ):\n            print(f'found no popts or cases less than {min_cases}, OLD scaling factor {scaling_factor}, OLD popts={popts}')\n            popts, scaling_factor = nyc_scale(df_fit, country, province, metric=metric)\n            print(f'NEW scaling factor {scaling_factor}, NEW popts={popts}')\n        elif (np.array(fits['fgaussian_popt_x0']) > max_x0):\n            print(f'mean is over {max_x0}, OLD scaling factor {scaling_factor}, OLD popts={popts}')\n            popts, scaling_factor = nyc_scale(df_fit, country, province, metric=metric)\n            print(f'NEW scaling factor {scaling_factor}, NEW popts={popts}')\n        elif (metric=='DailyConfirmedCases' and (np.array(fits['max_cases']) < max_a) and (np.array(fits['fgaussian_popt_a']) > max_a)):\n            print(f'cases less than {max_a} and amplitude is over {max_a}, OLD scaling factor {scaling_factor}, OLD popts={popts}')\n            popts, scaling_factor = nyc_scale(df_fit, country, province, metric=metric)\n            print(f'NEW scaling factor {scaling_factor}, NEW popts={popts}')\n        else:\n#             x = np.array(list(range(tmp.shape[0])))\n            if metric == 'DailyConfirmedCases':\n                scaling_factor = np.array(fits['max_cases']) / np.array(max(fun(x,*popts)))\n            else:\n                scaling_factor = 2.  # DailyFatalities is small, so make it bigger\n\n    except Exception as e:\n        print(f'Error {e}')\n        popts, scaling_factor = nyc_scale(df_fit, country, province)\n    \n    # make it wider\n    if metric=='DailyConfirmedCases':\n        scaling_factor = 1.1 * scaling_factor\n        sigma_scaling_factor = 2.5\n        popts[2] = sigma_scaling_factor * popts[2]\n        print(f'Scaling factors sigma={sigma_scaling_factor}, amplitude={scaling_factor})')\n    \n    return popts, scaling_factor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plotting functions\n\ndef plot_fit(x, y, params=None, fun=fsigmoid, title=''):\n#     params = [ 0.21766133, 18.87933821]\n    popt, perr = do_fit_fun(x, y, fun, params)         \n    \n    plt.plot(x,y,'r.')\n    plt.title(title)\n    plt.plot(x, fun(x, *popt), 'k--', label=label)\n    plt.legend(loc = 'lower right')\n    # plt.xscale('log')\n#     plt.show()\n\n    print(f'RMSE={mean_squared_error(y, fun(x,*popt))}')\n    \n    return plt\n\ndef plot_fit_daily(df, country='China', transform=None, fun=fpow, params=None, y_label='DailyConfirmedCases'):\n#     df = get_country_sum(df, country)\n#     df = df[df['Country_Region']==country]\n#     y = df[y_label] / max(df[y_label])\n    y = df[y_label]\n    x = list(range(1,len(y)+1))\n    # y = np.array(y)\n    # x = np.array(x)\n\n    if transform:\n        y = transform(y)\n        \n    plot_fit(x, y, fun=fun, title=country, params=params)\n    print(f'X={x}, y={y}')\n    \n    \ndef plot_fit_country(train, df_fit, country='China', province=None, fun=fgaussian, metric='DailyConfirmedCases', scaling_factor=None):\n    # plot gausian parameters\n    # country = 'Canada'\n    # province = 'Quebec'\n    # country = 'China'\n    # province = 'Hubei'\n#     country = 'US'\n#     province = 'New Mexico'\n    # country = 'Australia'\n    # province = 'South Australia'\n    # province = 'Washington'\n    # province = 'Louisiana'\n    # province = 'Florida'\n\n    # province = 'California'\n    tmp = country_slice(train, country, province)\n    # print(tmp)\n    fits = country_slice(df_fit, country, province)\n    \n#     try:\n#         scaling_factor = np.array(fits[['scaling_factor']])[0]\n#     except:\n#         scaling_factor = 1.\n        \n    x = np.array(list(range(tmp.shape[0])))\n    popts, _scaling_factor = get_popts_scaling_factory(fits, df_fit, country, province, x, metric=metric)\n    \n    if scaling_factor is None:\n        scaling_factor = _scaling_factor\n        \n    print(f'{province} {country} scaling_factor={scaling_factor} popts={popts}')\n    \n    y = np.array(tmp[ metric ])\n    x = np.array(list(range(len(y))))\n    \n    y_pred = scaling_factor * fun(x,*popts)\n    \n    plt.plot(x,y,'b-+', label='Actual')\n    # popts, pcov = curve_fit(fgausian,x,y, p0=[400,23,2])\n#     popts = fits[['fgaussian_popt_a','fgaussian_popt_x0','fgaussian_popt_sd']].transpose().iloc[:,0].tolist()\n    plt.plot(x,y_pred,'r.', label='Predicted')\n    plt.title(f'Actual vs fitted gaussian for {province} {country}')\n    plt.ylabel(metric)\n    plt.xlabel('Time')\n    ax = plt.gca()\n    ax.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load datasets\ntrain = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-3/train.csv')\ntrain['Date'] = pd.to_datetime(train['Date'])\ntrain = train.sort_values(by=['Country_Region','Province_State','Date'])\nprint(f'train min_date={min(train[\"Date\"])}, max_date={max(train[\"Date\"])}')\ntrain['DailyConfirmedCases'] = train['ConfirmedCases'].diff()\ntrain['DailyFatalities'] = train['Fatalities'].diff()\ntrain_bak = train  # make a backup\n\n# replace negatives with a 0\nfilter = train['DailyConfirmedCases']<0\ntrain.loc[filter,'DailyConfirmedCases'] = 0\ntrain.loc[filter,'DailyFatalities'] = 0\nfilter = np.isnan(train['DailyConfirmedCases'])\ntrain.loc[filter,'DailyConfirmedCases'] = 0\ntrain.loc[filter,'DailyFatalities'] = 0\n\ntrain.to_csv('train_daily.csv',index=False)\n\ntest = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-3/test.csv')\ntest['Date'] = pd.to_datetime(test['Date'])\nprint(f'test min_date={min(test[\"Date\"])}, max_date={max(test[\"Date\"])}')\ntest\n\n# filter training data upto the test date\ntrain = train[train['Date']<min(test['Date'])]\nprint(f\"max_train={max(train['Date'])}\")\n\nmin(test['Date'])\n\nsubmission = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-3/submission.csv')\nsubmission\n\n# countries with a Province\ntrain[train['Province_State'].isna()==False]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tmp = country_slice(train,'Korea, South')\ny = np.array(tmp['DailyConfirmedCases'])\nx = np.array(list(range(len(y))))\npopts, pcov = curve_fit(fgaussian,x,y,p0=[10,50,100])\nprint(popts)\nplt.plot(x,y)\nplt.plot(x,fgaussian(x,*popts)*1.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.leastsq.html\ntmp = country_slice(train,'Algeria')\ny = np.array(tmp['DailyConfirmedCases'])\nx = np.array(list(range(len(y))))\n# popts, pcov = curve_fit(fgaussian,x,y,absolute_sigma=False,maxfev=2000,method='trf',loss='huber',p0=[100,80,5])\npopts, pcov = fit(fgaussian,x,y,params=[100,80,5])\n# popts, pcov = curve_fit(fgaussian,x,y,absolute_sigma=False,maxfev=2000,method='lm',p0=[100,60,1])\nprint(popts)\nplt.plot(x,y)\nplt.plot(x,fgaussian(x,*popts))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.leastsq.html\ntmp = country_slice(train,'Algeria')\ny = np.array(tmp['DailyFatalities'])\nx = np.array(list(range(len(y))))\n# popts, pcov = curve_fit(fgaussian,x,y,absolute_sigma=False,maxfev=2000,method='trf',loss='huber',p0=[100,80,5])\n# popts, pcov = fit(fgaussian,x,y,params=[1,80,5])\npopts, pcov = curve_fit(fgaussian,x,y,absolute_sigma=False,maxfev=2000,method='lm',p0=[100,60,1])\nprint(popts)\nplt.plot(x,y)\nplt.plot(x,fgaussian(x,*popts))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tmp = country_slice(train,'Korea, South')\ny = np.array(tmp['DailyConfirmedCases'])\nx = np.array(list(range(len(y))))\npopts, pcov = curve_fit(fgaussian,x,y,absolute_sigma=True)\nprint(popts)\nplt.plot(x,y)\nplt.plot(x,fgaussian(x,*popts))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tmp = country_slice(train,'Korea, South')\ny = np.array(tmp['DailyFatalities'])\nx = np.array(list(range(len(y))))\npopts, pcov = curve_fit(fgaussian,x,y,p0=[1,60,1])\nprint(popts)\nplt.plot(x,y)\nplt.plot(x,fgaussian(x,*popts))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tmp = country_slice(train,'China', province='Hubei')\ny = np.array(tmp['DailyFatalities'])\nx = np.array(list(range(len(y))))\npopts, pcov = curve_fit(fgaussian,x,y,p0=[1,60,1])\nprint(popts)\nplt.plot(x,y)\nplt.plot(x,fgaussian(x,*popts)*2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot_fit_country(train, df_fit_confirmed, country='China', province='Shanghai')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot_fit_country(train, df_fit_confirmed, country='US', province='New Mexico')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # covid\n# import requests\n# import io\n\n# def get_df_from_url(url):\n#     s = requests.get(url).content\n#     return pd.read_csv(io.StringIO(s.decode('utf-8')))\n\n# covid_url_prefix = 'https://github.com/CSSEGISandData/COVID-19/raw/master/csse_covid_19_data/csse_covid_19_time_series/'\n# df_covid_confirmed = get_df_from_url(covid_url_prefix + 'time_series_covid19_confirmed_global.csv')\n# df_covid_deaths = get_df_from_url(covid_url_prefix + 'time_series_covid19_deaths_global.csv')\n# df_covid_recovered = get_df_from_url(covid_url_prefix + 'time_series_covid19_recovered_global.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tmp = train[train['Country_Region']=='Korea, South']\nplt.plot(list(range(tmp.shape[0])), tmp['DailyConfirmedCases'])\nplt.title('Korea, South daily confirmed cases')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tmp = train[(train['Country_Region']=='China')&(train['Province_State']=='Hubei')]\nplt.plot(list(range(tmp.shape[0])), tmp['DailyConfirmedCases'])\nplt.title('Hubei, China daily confirmed cases')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Fit gaussian for each country / province"},{"metadata":{"trusted":true},"cell_type":"code","source":"# perform curve fitting\ndf_country_state_max = train.groupby(['Country_Region','Province_State']).max().reset_index()\ndf_country_state_max\nfit_confirmed = []\nfit_fatality = []\n# confirmed_cases_gaussian_params = [1000, 70, 60] # initial params determined empirically \n# fatalities_gaussian_params = [1,60,40]\nconfirmed_cases_gaussian_params = [1000, 70, 40] # initial params determined empirically \nfatalities_gaussian_params = [10,70,20]\nprint(f'Confirmed cases Gaussian params={confirmed_cases_gaussian_params}, fatalities_gaussian_params={fatalities_gaussian_params}')\ncountry_state_df = train[['Country_Region','Province_State']].drop_duplicates()\n# for i, row in list(df_country_state_max.iterrows())[:20]:\nfor i, row in list(country_state_df.iterrows()):\n    country = row[0]\n    province = row[1]\n    if province is None or pd.isna(province):\n        tmp = train[(train['Country_Region']==country)&(pd.isna(train['Province_State']))]\n    else:\n        tmp = train[(train['Country_Region']==country)&(train['Province_State']==province)]\n#     print(f'tmp={tmp}')\n    # filter out cumulative days with 0 cases so that all the graphs are shifted to the left and start the same\n    # then compare the fitted means\n#     tmp = tmp[tmp['ConfirmedCases']>=5]\n    if tmp.shape[0] == 0:\n        print(f'No confirmed cases found for {country} {province}')\n        continue\n    ret = do_fit(tmp, y_label='DailyConfirmedCases', gaussian_params=confirmed_cases_gaussian_params)\n    fit_confirmed.append(ret)\n    ret = do_fit(tmp, y_label='DailyFatalities', gaussian_params=fatalities_gaussian_params)\n    fit_fatality.append(ret)\n\ndf_fit_confirmed = pd.DataFrame(fit_confirmed)\ndf_fit_fatality = pd.DataFrame(fit_fatality)\n\ndf_fit_confirmed.to_csv('fit_confirmed.csv')\ndf_fit_fatality.to_csv('fit_fatality.csv')\n\ndf_fit_confirmed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"df_fit_confirmed error={np.sum(df_fit_confirmed['fgaussian_perr'])}\")\nprint(f\"df_fit_fatality error={np.sum(df_fit_fatality['fgaussian_perr'])}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Countries that failed to fit a Gaussian"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"df_fit_confirmed={df_fit_confirmed.shape}\")\nprint(f\"nan={df_fit_confirmed[np.isnan(df_fit_confirmed['fgaussian_popt_x0'])].shape}\")\ndf_fit_confirmed[np.isnan(df_fit_confirmed['fgaussian_popt_x0'])].sort_values(by='total_cases', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Apply model correction using scaling factor"},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_correction(df_fit, country, province=None):\n    \"\"\"\n    Returns new popts (model parameter options) and scaling factor\n    \"\"\"\n    scaling_factor = 1.\n    \n#     try:\n#         popts = np.array(fits[['fgaussian_popt_a','fgaussian_popt_x0','fgaussian_popt_sd']])[0]\n#         if pd.isna(popts[0]) or (np.array(fits['max_cases']) < 50):\n#             raise Exception\n#     except:\n        \n    # because the means are bimodal\n    fits = country_slice(df_fit, country, province)\n    popts = np.array(fits[['fgaussian_popt_a','fgaussian_popt_x0','fgaussian_popt_sd']])[0]\n    print(f\"popts[1]={popts[1]}\")\n    if (popts[1] <= 30):\n        popts, scaling_factor = nyc_scale(df_fit, country, province, ref_country='China', ref_province='Hubei')\n    else:\n#               (popts[1] > 50 and popts[1] < 70)\n        popts, scaling_factor = nyc_scale(df_fit, country, province, ref_country='US', ref_province='New York')\n        \n        \n    return popts, scaling_factor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_fit_confirmed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_fit_fatality_old = df_fit_fatality\ndf_fit_confirmed_old = df_fit_confirmed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# apply correction cases\n# metric = 'DailyConfirmedCases'\n# df_all = df_fit_confirmed\n# df_all_out = []\n# for i, row in list(df_all.iterrows()):\n#     country = row[0]\n#     province = row[1]\n#     fits = row\n#     popts, scaling_factor = model_correction(df_all, country, province)\n#     row['scaling_factor'] = scaling_factor\n#     row['fgaussian_popt_a'] = popts[0]\n#     row['fgaussian_popt_x0'] = popts[1]\n#     row['fgaussian_popt_sd'] = popts[2]\n#     y = np.array(train[[metric]])\n#     x = np.array(list(range(len(y))))\n#     y_pred = np.array(fgaussian(x, *popts))\n#     row['fgaussian_perr'] = mean_squared_error(y, y_pred)\n#     df_all_out.append(row)\n# df_fit_confirmed = pd.concat(df_all_out,axis=1)\n# df_fit_confirmed = df_fit_confirmed.transpose()\n# df_fit_confirmed\n\n# # apply correction cases\n# metric = 'DailyFatalities'\n# df_all = df_fit_fatality\n# df_all_out = []\n# for i, row in list(df_all.iterrows())[:5]:\n#     country = row[0]\n#     province = row[1]\n#     fits = row\n#     popts, scaling_factor = model_correction(df_all, country, province)\n#     row['scaling_factor'] = scaling_factor\n#     row['fgaussian_popt_a'] = popts[0]\n#     row['fgaussian_popt_x0'] = popts[1]\n#     row['fgaussian_popt_sd'] = popts[2]\n#     y = np.array(train[[metric]])\n#     x = np.array(list(range(len(y))))\n#     y_pred = np.array(fgaussian(x, *popts))\n#     row['fgaussian_perr'] = mean_squared_error(y, y_pred)\n#     df_all_out.append(row)\n# df_fit_fatality = pd.concat(df_all_out,axis=1)\n# df_fit_fatality = df_fit_fatality.transpose()\n# df_fit_fatality","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_fit_fatality = df_fit_fatality.astype({'total_cases':'int64', 'max_cases':'int64', 'fgaussian_perr':'float64'})\ndf_fit_confirmed = df_fit_confirmed.astype({'total_cases':'int64', 'max_cases':'int64', 'fgaussian_perr':'float64' })","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_fit_fatality","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_fit_confirmed[df_fit_confirmed['Country_Region']=='Denmark']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluate fitted models"},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot model mean and sd\nplt.plot(df_fit_confirmed['fgaussian_popt_x0'],df_fit_confirmed['fgaussian_popt_sd'],'b.')\nplt.title('Confirmed Gaussian mean vs stdev')\nplt.xlabel('Mean')\nplt.ylabel('Stdev')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_fit_confirmed.sort_values(by='fgaussian_popt_sd',ascending=False).head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot model amplitude and sd\nplt.plot(df_fit_confirmed['fgaussian_popt_a'],df_fit_confirmed['total_cases'],'b.')\nplt.title('Confirmed Gaussian amplitude vs total_cases')\nplt.xlabel('amplitude (log)')\nplt.ylabel('total_cases')\nplt.xscale('log')\nplt.yscale('log')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_fit_confirmed.sort_values(by='fgaussian_popt_a',ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot fit against number of cases\nplt.plot(df_fit_fatality['fgaussian_popt_x0'],np.log10(df_fit_fatality['total_cases']+0.1),'b.')\nplt.title('Fatalities Gaussian mean vs total confirmed cases')\nplt.xlabel('Mean')\nplt.ylabel('Total cases (log10)')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot fit against number of cases\nplt.plot(df_fit_confirmed['fgaussian_popt_x0'],np.log10(df_fit_confirmed['total_cases']+0.1),'b.')\nplt.title('Gaussian mean vs total confirmed cases')\nplt.xlabel('Mean')\nplt.ylabel('Total cases (log10)')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# show which countries\nprint('Mean > 80')\nprint(df_fit_confirmed[df_fit_confirmed['fgaussian_popt_x0']>80])\n\n# mostly China, the index country, was the first country to flatten the curve\nprint('Mean < 20')\nprint(df_fit_confirmed[df_fit_confirmed['fgaussian_popt_x0']<20])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot fit against number of fatalities\nplt.plot(np.log10(df_fit_fatality['fgaussian_perr']+0.00001),np.log10(df_fit_fatality['total_cases']+0.1),'b.')\nplt.title('Fatalities Gaussian RMSE vs total confirmed cases')\nplt.xlabel('RMSE (log10)')\nplt.ylabel('Total cases (log10)')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot fit against number of cases\nplt.plot(np.log10(df_fit_confirmed['fgaussian_perr']+0.00001),np.log10(df_fit_confirmed['total_cases']+0.1),'b.')\nplt.title('Gaussian RMSE vs total confirmed cases')\nplt.xlabel('RMSE (log10)')\nplt.ylabel('Total cases (log10)')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hubei, China has the largest error\nprint(df_fit_confirmed[df_fit_confirmed['fgaussian_perr']>1e5])\nprint(country_slice(df_fit_confirmed, country='China', province='Shanghai'))\nplot_fit_country(train, df_fit_confirmed, country='China', province='Shanghai')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import linear_model\nclf = linear_model.Lasso(alpha=0.1)\nclf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])\n# Lasso(alpha=0.1)\nprint(clf.coef_)\nprint(clf.intercept_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_fit_country(train, df_fit_confirmed, country='Colombia')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_fit_country(train, df_fit_confirmed, country='Japan')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_fit_country(train, df_fit_fatality, country='Japan', metric='DailyFatalities')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_fit_country(train, df_fit_confirmed, country='US', province='New Mexico')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_fit_country(train, df_fit_confirmed, country='Australia', province='Queensland')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_fit_country(train, df_fit_confirmed, country='US', province='Texas')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_fit_country(train, df_fit_confirmed, country='US', province='New York')\n\n# Use New York as the reference distribution for NaNs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_fit_country(train, df_fit_confirmed, country='Korea, South')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_fit_country(train, df_fit_confirmed, country='Italy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_fit_country(train, df_fit_confirmed, country='Australia', province='Tasmania')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"country = 'France'\nprovince = None\ncountry_slice(df_fit_confirmed,country,province).sort_values(by='total_cases',ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"country = 'Netherlands'\nprovince = None\ncountry_slice(df_fit_confirmed,country,province).sort_values(by='total_cases',ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fix NaNs using New York as the reference because the fit looks reasonable and it has a good number of cases\ncountry = 'US'\nprovince = 'New York'\ncountry_slice(df_fit_confirmed,country,province)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Use the model to predict cases and fatalities"},{"metadata":{"trusted":true},"cell_type":"code","source":"train[['Country_Region','Province_State']].drop_duplicates()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def calc_cumsum_predict(train, df_fit, test, country,province,metric='DailyConfirmedCases', scaling_factor=None):\n    \"\"\"\n    Returns the cumulative sum for submission\n    \"\"\"\n    tmp_test = country_slice(test,country, province)\n    tmp = country_slice(train,country, province)\n    tmp_fit = country_slice(df_fit,country, province)\n   \n    x = np.array(list(range(tmp.shape[0])))\n    popts, _scaling_factor = get_popts_scaling_factory(tmp_fit, df_fit, country, province, x, metric=metric)\n    \n    if scaling_factor is None:\n        scaling_factor = _scaling_factor\n        \n    print(f'country={country}, province={province}, scaling_factor={scaling_factor}, popts={popts}')\n    y = np.array(tmp[metric])\n    x = np.array(list(range( len(y) )))\n\n    x_pred = np.array(list(range( tmp.shape[0], tmp.shape[0]+tmp_test.shape[0] )))\n    y_pred = np.array(scaling_factor * fgaussian(x_pred,*popts))\n    \n#     print(f'y_pred.shape={y_pred.shape}')\n\n    concat_x = np.concatenate([x,x_pred])\n    concat_y = np.ceil(np.concatenate([y,y_pred]))\n    ret = np.cumsum(concat_y)[tmp.shape[0]:]\n    \n#     print(f'ret.shape={ret.shape}')\n    \n    return ret","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prepare data for submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"out = []\ncountry = 'US'\nprovince = 'New York'\n\ncountry_state_df = train[['Country_Region','Province_State']].drop_duplicates()\n\nfor i, row in list(country_state_df.iterrows()):\n    country = row[0]\n    province = row[1]\n    \n    tmp_test = country_slice(test,country, province)\n    \n    y_submit = calc_cumsum_predict(train, df_fit_confirmed, test, country, province, metric='DailyConfirmedCases')\n    tmp_test['ConfirmedCases'] = y_submit\n    \n    y_submit = calc_cumsum_predict(train, df_fit_fatality, test, country, province, metric='DailyFatalities')\n    tmp_test['Fatalities'] = y_submit\n    \n    out.append(tmp_test)\n\nresults = pd.concat(out)\n\n# make sure there's no negative fatalities\nresults['Fatalities'] = results[['Fatalities','ConfirmedCases']].min(axis=1)  # Gambia","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Summary\n\nComparing with train.csv, the prediction results tend to underestimate the number of cases and fatalities"},{"metadata":{"trusted":true},"cell_type":"code","source":"results.to_csv('results.csv',index=False)\nresults[submission.columns].to_csv('submission.csv',index=False)\nprint(f'Results saved to results.csv {results.shape}, submission_shape={submission.shape}, total_cases={results[\"ConfirmedCases\"].sum()}, total_fatalities={results[\"Fatalities\"].sum()}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results.groupby(['Country_Region']).agg({'ConfirmedCases':'max'}).reset_index().sort_values(by='ConfirmedCases',ascending=False).head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results.groupby(['Country_Region']).agg({'Fatalities':'max'}).reset_index().sort_values(by='Fatalities',ascending=False).head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_fit_country(train, df_fit_confirmed, country='France')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_fit_country(train, df_fit_confirmed, country='Iran')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_fit_country(train, df_fit_confirmed, country='Spain')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# mean is too far\nplot_fit_country(train, df_fit_confirmed, country='US', province='Texas')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# mean is too far\nplot_fit_country(train, df_fit_confirmed, country='Colombia')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_fit_country(train, df_fit_confirmed, country='Italy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_fit_country(train, df_fit_confirmed, country='Korea, South')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_fit_country(train, df_fit_fatality, country='Korea, South', metric='DailyFatalities')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_fit_country(train, df_fit_fatality, country='China', province='Hubei', metric='DailyFatalities')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_fit_country(train, df_fit_confirmed, country='Canada', province='British Columbia')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_fit_country(train, df_fit_fatality, country='US', province='New York', metric='DailyFatalities')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_fit_country(train, df_fit_confirmed, country='US', province='New York', metric='DailyConfirmedCases')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_fit_country(train, df_fit_fatality, country='Canada', province='British Columbia', metric='DailyFatalities')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_fit_country(train, df_fit_confirmed, country='China', province='Hubei', metric='DailyConfirmedCases')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_fit_country(train, df_fit_confirmed, country='Diamond Princess', metric='DailyConfirmedCases')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_fit_country(train, df_fit_confirmed, country='Switzerland', metric='DailyConfirmedCases')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_fit_country(train, df_fit_confirmed, country='Algeria', metric='DailyConfirmedCases')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_fit_country(train, df_fit_confirmed, country='US', province='New York', metric='DailyConfirmedCases')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_fit_country(train, df_fit_confirmed, country='US', province='New Jersey', metric='DailyConfirmedCases')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compare results\ntrain_all = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-3/train.csv')\ntrain_all['Date'] = pd.to_datetime(train_all['Date'])\ndf_ypred = pd.merge(results, train_all, on=['Country_Region','Province_State','Date'])\ndf_ypred.columns = [c.replace('_x','_pred') for c in df_ypred.columns]\ndf_ypred['ConfirmedCases_diff'] = abs(df_ypred['ConfirmedCases_pred'] - df_ypred['ConfirmedCases_y'])\ndf_ypred['Fatalities_diff'] = abs(df_ypred['Fatalities_pred'] - df_ypred['Fatalities_y'])\ndf_ypred_stats = df_ypred.groupby(['Country_Region']).agg({'ConfirmedCases_diff':'sum', 'Fatalities_diff':'sum'}).reset_index()\ndf_ypred_stats.sort_values(by='ConfirmedCases_diff', ascending=False).head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = ['ForecastId','Province_State','Country_Region','ConfirmedCases_pred','ConfirmedCases_y','ConfirmedCases_diff','Fatalities_pred','Fatalities_y','Fatalities_diff']\ndf_ypred[df_ypred['Country_Region'].isin(['US'])].sort_values(by='ConfirmedCases_diff',ascending=False)[cols].head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_ypred[df_ypred['Country_Region'].isin(['Italy'])].sort_values(by='ConfirmedCases_diff',ascending=False)[cols].head(5)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}