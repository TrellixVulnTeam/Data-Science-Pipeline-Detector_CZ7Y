{"cells":[{"metadata":{},"cell_type":"markdown","source":"##### D.Tom (April, 2020)\n# <center>COVID19 Global Forecasting (Week 3):</center>\n<b>Task: </b>*Forecast daily COVID-19 spread in regions around the world.  Predict the cumulative number of confirmed COVID-19 cases in various locations across the world, as well as the number of resulting fatalaties, for future dates.*"},{"metadata":{},"cell_type":"markdown","source":"### Introduction"},{"metadata":{},"cell_type":"markdown","source":"The White House Office of Science and Technology Policy alongside research groups and companies have prepared a COVID-19 Open Research Dataset ('CORD-19').\n\nCORD-19 alongside the United Nation's dataset [('SYB62_1_201907')](https://data.un.org/_Docs/SYB/CSV/SYB62_1_201907_Population,%20Surface%20Area%20and%20Density.csv) on Population, Surface Area and Density will identify specific variables asociated to the infection and fatality rates based on location.\n\nProper Data Science techniques will afford governments around the world to better appropraite resources in an attempt to slow the infection, decrease mortality rates, and eradicate the virus.  \n\n<b>No prediction is guarenteed</b>:\n    - An appropriate predictive model will save time, money, and lives through proper planning.\n    - More location specific data should accurately predict future cases. (ie. City population)\n    - More social specific data should accurately predict future cases. (ie. People's movements)"},{"metadata":{},"cell_type":"markdown","source":"## Table of Contents\n\n<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n\n1. [Setting up the Environment](#0)<br>\n2. [Combine the *pandas* dataframes](#2)<br>\n3. [Analyze correlations and probability values](#4)<br>\n4. [Plot the training data analyzing 'ConfirmedCases'](#6)<br>\n5. [Build a Long-Term Predictive Model based on Hubei province (Polynomial Regression)](#8)<br>\n6. [Generate a Sigmoid Predictive Model based on Hubei Province](#10)<br>\n6. [Conclusion](#12) <br>\n</div>\n<hr>"},{"metadata":{},"cell_type":"markdown","source":"# Setting up the Environment<a id=\"0\"></a>"},{"metadata":{},"cell_type":"markdown","source":"The below environment will allow for most machine learning methods to be quickly loaded and incorporated into the project.  Such modules include loading datasets, analyzing and plotting values across various diagrams, optimizating dependencies, and prebuilt machine learning algorithms.  Additional optimization methods can be performed to remove and omit certain modules based on one's given project."},{"metadata":{},"cell_type":"markdown","source":"##### Download and import dependencies"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%capture\n!pip install -upgrade pip\n!pip install ipywidgets matplotlib==2.2.0 pyproj==1.9.6 numpy pandas proj pillow cython sklearn datetime seaborn pylab","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport pandas as pd\n#import pylab as pl\nimport numpy as np\nfrom scipy import stats\nimport seaborn as sns\nimport datetime as dt\n\nfrom mpl_toolkits.basemap import Basemap\nimport matplotlib.pyplot as plt\n#from pylab import rcParams\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.metrics import r2_score\nfrom scipy.stats import pearsonr\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n\nfrom scipy.optimize import curve_fit\n\nmpl.style.use('ggplot') # optional: for ggplot-like style\nprint ('Matplotlib version: ', mpl.__version__) # >= 2.0.0 # check for latest version of Matplotlib\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Download and load files\n\n##### It is suggested to download and unzip the files manually from:\n    1. https://www.kaggle.com/ \n          - Competition requires approvaland verification before downloads can be authorized\n          \n    2. https://data.un.org/_Docs/SYB/CSV/SYB62_1_201907_Population,%20Surface%20Area%20and%20Density.csv"},{"metadata":{"trusted":true},"cell_type":"code","source":"#fname = 'SYB62_1_201907_Population, Surface Area and Density.csv'\n#site = 'https://data.un.org/_Docs/SYB/CSV/SYB62_1_201907_Population,%20Surface%20Area%20and%20Density.csv'\n#!wget -O  'SYB62_1_201907_Population, Surface Area and Density.csv' 'https://data.un.org/_Docs/SYB/CSV/SYB62_1_201907_Population,%20Surface%20Area%20and%20Density.csv'\n#!unzip -o -j 'SYB62_1_201907_Population, Surface Area and Density.csv'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Load the Kaggle Competition training data 'train.csv'.  \nClean the dataset:\n     - parse dates\n     - create DateKey column"},{"metadata":{"trusted":true},"cell_type":"code","source":"path = '../input/covid19-global-forecasting-week-3/'\ncsv = 'train.csv'\nfilepath = path + csv\n\npdf = pd.read_csv(filepath, parse_dates=['Date'])\n\npdf['Year'] = pdf['Date'].dt.year\npdf['Month'] = pdf['Date'].dt.month\npdf['Week'] = pdf['Date'].dt.week\npdf['Day'] = pdf['Date'].dt.day\npdf['DateKey'] = (pdf['Date'] - pdf['Date'].min()).astype(int)\n\nbkdf = pdf\ntoday = pd.datetime.now()\n\nprint(' Filepath: ', filepath, '\\n', 'Shape: ', pdf.shape, '\\n', 'Date: ', today)\npdf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Load the United Nations Population, Surface Area and Density dataset\nClean the dataset: \n     - allign the dataset with the Kaggle dataset\n     - create Total Population column"},{"metadata":{"trusted":true},"cell_type":"code","source":"#path2 = '../output/kaggle/working/'\ncsv2 = 'SYB62_1_201907_Population, Surface Area and Density.csv'\nfilepath2 = csv2 #path2 + csv2\n\nasdf = pd.read_csv(filepath2, encoding='iso-8859-1', header=1)\nasdf = asdf.pivot_table(values='Value', index=['Region/Country/Area', 'Unnamed: 1', 'Year'], columns='Series').reset_index()\nasdf = asdf.rename(columns={'Unnamed: 1':'Country_Region'}, inplace=False)\nasdf['Population mid-year Total'] = ((asdf['Population mid-year estimates for females (millions)'] \n                                    + asdf['Population mid-year estimates for males (millions)'])\n                                    * 1000000)\nasdf = asdf.drop(['Year'], axis=1, inplace=False)\nasdf = asdf.dropna()\n\nprint(' Filepath: ', filepath2, '\\n', 'Shape: ', asdf.shape, '\\n', 'Date: ', today)\nasdf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Load the Kaggle Competition testing data 'test.csv'.  \nClean the dataset:\n     - parse dates"},{"metadata":{"trusted":true},"cell_type":"code","source":"tp = '../input/covid19-global-forecasting-week-3/'\ntf = 'test.csv'\nfilepath3 = tp + tf\n\ntest = pd.read_csv(filepath3, parse_dates=['Date'])\n\nprint(' Filepath: ', filepath3, '\\n', 'Shape: ', test.shape, '\\n', 'Date: ', today)\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### At this point there are three *pandas* dataframes loaded into the notebook:\n    \n    pdf = training data\n    asdf = additional data\n    test = testing data\n    \n*These dataframes will be merged into two separate dataframes:*\n    \n    tes = (test + asdf) 'Testing + Additional'\n    res = (pdf + asdf) 'Training' + Additional'"},{"metadata":{},"cell_type":"markdown","source":"# Combine the *pandas* dataframes:<a id=\"2\"></a>"},{"metadata":{},"cell_type":"markdown","source":"Create the testing dataframe:\n\n    tes = (test + asdf) 'Testing + Additional'\n    \n##### Cleanup the data by parsing dates and creating a 'DateKey' similar to the above dataframes.  \n\n    - Drop unassociated null values \n    - Create column for 'Cases per pop'\n    - Create column for 'Case Density'"},{"metadata":{"trusted":true},"cell_type":"code","source":"tes = pd.concat([pdf, test])\ntes = tes.reset_index(drop=True)\ntes = pd.merge(tes, asdf, on='Country_Region', how='left')\n\ntes['Year'] = tes['Date'].dt.year\ntes['Month'] = tes['Date'].dt.month\ntes['Week'] = tes['Date'].dt.week\ntes['Day'] = tes['Date'].dt.day\ntes['DateKey'] = (tes['Date'] - tes['Date'].min()).astype(int)\n\ntes['ConfirmedCases'] = tes['ConfirmedCases'].replace(np.nan, 0)\ntes['ConfirmedCases'] = tes['ConfirmedCases'].dropna()\n\ntes['Fatalities'] = tes['Fatalities'].replace(np.nan, 0)\ntes['Fatalities'] = tes['Fatalities'].dropna()\n\ntes['Cases per pop'] = (tes['Population mid-year Total'] / (tes['ConfirmedCases'].max()))\ncasp = (tes['Population mid-year Total'].mean() / (tes['ConfirmedCases'].max()))\ntes['Cases per pop'] = tes['Cases per pop'].fillna(casp).astype(int)\ntes['Case Density'] = tes['Cases per pop'] / tes['Population density']\n\nprint('Shape: ', tes.shape)\ntes.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create a training dataframe:\n\n    res = (pdf + asdf) 'Training' + Additional'\n    \n##### Cleanup the data by parsing dates and creating a 'DateKey' similar to the above dataframes.  \n\n    - Drop unassociated null values \n    - Create column for 'Cases per pop'\n    - Create column for 'Case Density'"},{"metadata":{"trusted":true},"cell_type":"code","source":"res = pd.merge(pdf, asdf, on='Country_Region', how='left')\nres = res.reset_index(drop=True)\n\n\nres['Cases per pop'] = (res['Population mid-year Total'] / (res['ConfirmedCases'].max()))\ncasp2 = (res['Population mid-year Total'].mean() / (res['ConfirmedCases'].max()))\nres['Cases per pop'] = res['Cases per pop'].fillna(casp).astype(int)\nres['Case Density'] = res['Cases per pop'] / res['Population density']\n\n\nprint('Shape: ', res.shape)\nres.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Analyze correlations and probability values:<a id=\"4\"></a>\n    tes = testing dataframe\n    res = training dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"tes['Country_Region'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res['Country_Region'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Identify any Pearson Correlation values greater than or equal to 0.3\n\nNotice the correlations between 'Case Density', 'Cases per pop', and 'Surface area (thousand km2)'"},{"metadata":{"trusted":true},"cell_type":"code","source":"name = res\nname.corr()[name.corr() >= 0.3]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Identify any probability values less than or equal to 0.2\n\nNotice the significance betwen 'ConfirmedCases', 'Week', and 'Month'."},{"metadata":{"trusted":true},"cell_type":"code","source":"name = res.dropna()\n\ndef calculate_pvalues(name):\n    name = name._get_numeric_data()\n    col = pd.DataFrame(columns=name.columns)\n    pval = col.transpose().join(col, how='outer')\n    for r in name.columns:\n        for c in name.columns:\n            pval[r][c] = round(pearsonr(name[r], name[c])[1], 4)\n    return pval\n\n# 0 SIGNIFICANT, >0.1 NO SIGNIFICANCE\ncalculate_pvalues(name)[calculate_pvalues(name) <= 0.2]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Initial analysis of the available datasets indicates a correlation between 'ConfirmedCases' and 'Fatalities' with regards to the following variables:\n\n    1. 'Case Density'\n    2. 'Cases per pop'\n    3. 'Month'\n    4. 'Week'\n    \n<b>A rough predictive model for 'Predicted Cases' and 'Predicted Fatalaties' can be built around the four above variables.</b>\n\n*It must be noted that additional data may indicate additional correlations, or lack there of.*"},{"metadata":{},"cell_type":"markdown","source":"# Plot the training data analyzing 'ConfirmedCases':<a id=\"6\"></a>"},{"metadata":{},"cell_type":"markdown","source":"##### Below is a diagram indicating the True Values of 'ConfirmedCases' in reference to 'Date'"},{"metadata":{"trusted":true},"cell_type":"code","source":"xydf = res\n\nxx = 'Date'\nyy = 'ConfirmedCases'\n\nxxdf = xydf[xx]\nyydf = xydf[yy]\n\nplt.scatter(xxdf, yydf,  color='blue')\nplt.xlabel(\"True Values: \" + xx)\nplt.ylabel(\"True Values: \" + yy)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*It should be noted that the above dataset contains outliers that will require additional data to properly correct for.  Such data may include the amount of tests with regards to testing, population, and location.*  \n*Without the exact confirmed case value it will be nearly impossible to accurately predict any values, however the available data will give insight as to what may occur globally over the course of the next few years.*\n\n#### It is well documented that China, the specific location being Wuhan within Hubei, was the first known carrier of the COVID-19 outbreak.  This location will be further analyzed to determine a long term correlation of Predicted Values to that of confirmed cases."},{"metadata":{},"cell_type":"markdown","source":"Create a specific testing China and Hubei dataframe:\n    - Identify/remove outliers (ie. Hubei Province)\n    - Identify a long term prediction for the ConfirmedCases and Fatalities based on the below variables: \n\n    1. 'Case Density'\n    2. 'Cases per pop'\n    3. 'Month'\n    4. 'Week'"},{"metadata":{"trusted":true},"cell_type":"code","source":"resch = res.Country_Region.str.contains('China')\nresch = res[resch]\nresch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hubeidf = resch.dropna()\nhubeidf = hubeidf.reset_index(drop=True)\nhubei2df = hubeidf.Province_State.str.contains('Hubei')\nhubei2df = hubeidf[hubei2df]\nhubei2df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Build a Long-Term Predictive Model based on Hubei province (Polynomial Regression)<a id=\"8\"></a>"},{"metadata":{},"cell_type":"markdown","source":"A predictive model will be analyzed based on the Hubei province using a multivariate polynomial regression plot.  The variables used in this predictive model will be based on all the available numerical data and will use sklearn for the automated machine learning pipeline."},{"metadata":{},"cell_type":"markdown","source":"##### ConfirmedCases"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = hubei2df['ConfirmedCases']\nZ = hubei2df.drop(['ConfirmedCases', 'Country_Region', 'Date', 'Province_State'], axis=1, inplace=False)\n\n\nInput=[('scale',StandardScaler()),('model',LinearRegression())]\n\npipe=Pipeline(Input)\n\npipe.fit(Z,y)\n\nypipe=pipe.predict(Z)\n\nhubei2df['Predicted Cases'] = ypipe\nhubei2df['Predicted Cases'] = hubei2df['Predicted Cases'].astype(int)\nr_squared = r2_score(y, ypipe)\nprint('The R-square value is: ', r_squared)\nhubei2df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Fatalities"},{"metadata":{"trusted":true},"cell_type":"code","source":"y2 = hubei2df['Fatalities']\nZ2 = hubei2df.drop(['Fatalities', 'Country_Region', 'Date', 'Province_State'], axis=1, inplace=False)\n\n\nInput=[('scale',StandardScaler()),('model',LinearRegression())]\n\npipe=Pipeline(Input)\n\npipe.fit(Z2,y2)\n\ny2pipe=pipe.predict(Z2)\n\nhubei2df['Predicted Fatalities'] = y2pipe\nhubei2df['Predicted Fatalities'] = hubei2df['Predicted Fatalities'].astype(int)\nr_squared2 = r2_score(y2, y2pipe)\nprint('The R-square value is: ', r_squared2)\nhubei2df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### To further analyze the polynomial regression, an equation will be based on the Confirmed Values and the Date.\n*Note: A 16-order polynomial was found to accurately represent the number of Confirmed Values in reference to the 'DateKey'.*"},{"metadata":{"trusted":true},"cell_type":"code","source":"x = hubei2df['DateKey']\ncr = hubei2df['ConfirmedCases']\n\nf = np.polyfit(cr, x, 16)\np = np.poly1d(f)\nprint(p)\n\nr_squared = r2_score(x, p(cr))\nmse = mean_squared_error(x, p(cr))\n\nplt.title('Polynomial fit')\nplt.xlabel('DateKey')\nplt.ylabel('ConfirmedCases')\nplt.plot(x, ypipe, '-', x, cr, '.')\nplt.show()\n\nprint('The R-square value is: ', r_squared)\nprint('The MSE value is: ', mse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def PlotPolly(model, independent_variable, dependent_variabble, Name):\n    x_new = np.linspace(15, 55, 100)\n    y_new = model(x_new)\n\n    plt.plot(independent_variable, dependent_variabble, '.', x_new, y_new, '-')\n    plt.title('Polynomial Fit with Matplotlib for Confirmed Cases ~ Predicted Cases')\n    ax = plt.gca()\n    ax.set_facecolor((0.898, 0.898, 0.898))\n    fig = plt.gcf()\n    plt.xlabel(Name)\n    plt.ylabel('ConfirmedCases')\n    plt.show()\n    plt.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pc = hubei2df['Predicted Cases']\nf = np.polyfit(y, pc, 16)\np = np.poly1d(f)\nprint(p)\nPlotPolly(p, y, pc, 'Predicted Cases')\nprint(np.polyfit(y, pc, 16))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Further analyze the polynomial model's 'Predicted Cases' and 'Predicted Fatalaties' by calculating their pearson correlation to that of 'ConfirmedCases' and 'Fatalities'."},{"metadata":{"trusted":true},"cell_type":"code","source":"hubei2df.corr()[hubei2df.corr() >= 0.1 ]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### It should be noted that the above analysis proves that a model can be generated to predict cases with relative accuracy, however, a comparison of the True Values Predicted Values shows that the polynomial regression does perform as intended and may require more data or a separate model."},{"metadata":{},"cell_type":"markdown","source":"# Generate a Sigmoid Predictive Model based on Hubei Province<a id=\"10\"></a>"},{"metadata":{},"cell_type":"markdown","source":"A new attempt will be made to generate a predictive model for the Hubei Province based on a Sigmoid function.  This will use scipy for the automation to better fit the beta_1 and beta_2 values.\n\n*It should be noted that ConfirmedCases should both rise and fall based on whether or not the infection is spreading or regressing.  A sigmoid function does not allow for such ebb and flow, but a sigmoid predictive model might accurately predict a long-term correlation associated to both 'ConfirmedCases' and 'Date' to that of the potential effects of possible quarentine measures.*\n\n##### Note: The more effective the quarentine, the lower the rate of infection.  The infection might spread to the entire population, but it might take a certain amount of time (such as [arbitrary value: 2 years] with effective quarentine measures) to reach that point.  If a vaccine is found before the entire population is infected then the sigmoid will reverse giving a bell curve of infected cases to that of the vaccinated population."},{"metadata":{"trusted":true},"cell_type":"code","source":"def sigmoid(x, Beta_1, Beta_2):\n     y = 1 / (1 + np.exp(-Beta_1*(x-Beta_2)))\n     return y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xdata = hubei2df['DateKey'].values/max(hubei2df['DateKey'].values)\nydata = hubei2df['ConfirmedCases'].values/max(hubei2df['ConfirmedCases'].values)\npopt, pcov = curve_fit(sigmoid, xdata, ydata)\n\nprint(\" beta_1 = %f, beta_2 = %f\" % (popt[0], popt[1]))\n\nx = np.linspace(0, 51, 52)\nx = x/max(x)\nplt.figure(figsize=(8,5))\ny = sigmoid(x, *popt)\n\nplt.plot(xdata, ydata, 'ro', label='data')\nplt.plot(x,y, linewidth=3.0, label='fit')\nplt.legend(loc='best')\nplt.ylabel('ConfirmedCases')\nplt.xlabel('Date')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split data into train/test\nmsk = np.random.rand(len(hubei2df)) < 0.8\ntrain_x = xdata[msk]\ntest_x = xdata[~msk]\ntrain_y = ydata[msk]\ntest_y = ydata[~msk]\n\n# build the model using train set\npopt, pcov = curve_fit(sigmoid, train_x, train_y)\n\n# predict using test set\ny_hat = sigmoid(test_x, *popt)\n\n# evaluation\nprint(\"Mean absolute error: %.2f\" % np.mean(np.absolute(y_hat - test_y)))\nprint(\"Residual sum of squares (MSE): %.2f\" % np.mean((y_hat - test_y) **2))\nfrom sklearn.metrics import r2_score\nprint(\"R2-Score: %.2f\" % r2_score(y_hat, test_y))\n\n#logistic function\nY_pred = sigmoid(xdata, popt[0], popt[1])\n\n#plot initial prediction against datapoints\nplt.plot(xdata, Y_pred)\nplt.plot(xdata, ydata, 'ro')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above sigmoid fits very well, it should be noted however that the number of confirmed cases may vary greatly based on testing per location (ie. Tests per 1000 population) and may require an additional dataset based on estimated 'Un-ConfirmedCases'.\n\n*Note: If the date of a vaccine is 2 years and a known rate of infection is calculated based on the above sigmoid model, then it can be reasonably assumed that a prediction can be built based on the Total Population.*\n\n*Note2: If the amount of tests performed is equal to that of the population, then the number of 'ConfirmedCases' would be completely accurate, however this value is not the case and may need a new variable to consititute the lack of testing.*"},{"metadata":{},"cell_type":"markdown","source":"### Create a column of Predicted Cases based on the Sigmoid model"},{"metadata":{"trusted":true},"cell_type":"code","source":"xdata2 = tes['ConfirmedCases'].values/max(tes['ConfirmedCases'].values)\nsigpred = sigmoid(xdata2, *popt)#[0], popt[1])\nprint(len(sigpred))\nprint(sigpred)\ntes['Predicted Cases (Sig)'] = (sigpred * popt[0]).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xdata2 = tes['Fatalities'].values/max(tes['Fatalities'].values)\nsigpred = sigmoid(xdata2, *popt)#[0], popt[1])\nprint(len(sigpred))\nprint(sigpred)\ntes['Predicted Fatalities (Sig)'] = (sigpred * popt[0]).astype(int)\ntes [32300:32370]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion:<a id=\"12\"></a>"},{"metadata":{},"cell_type":"markdown","source":"<b>A 16-order Polynomial Function will accurately predict COVID-19 cases and fatalaties across various locations.  Main factors associated to this conclusion are location specific and will require more data for to accurately predict the case count.  Datasets required for a more accurate analysis include:</b>\n\n    Datasets Required:\n    - More location specific data should accurately predict future cases. (ie. City population)\n    - More social specific data should accurately predict future cases. (ie. People's movements)\n    - More testing specific data should accurately predict future cases. (ie. Time and Volume of tests)\n    - More quarentine specific data should accurately predict future cases. (ie. Limiting People's movements)\n    \n    Minimum Necessary Variables:\n    1. 'Case Density'\n    2. 'Cases per pop'\n    3. 'Month'\n    4. 'Week'\n    \n\n<b>A sigmoid function based on China's Hubei province was successful in visually representing the appropriate values, however results for the predictive model cannot be accurately utilized in the project without further analysis of the values.  It must be directly stated that it appears an issue occured somewhere in the above prediction values, the error will be submitted based on the time constraints of the project.</b>"},{"metadata":{},"cell_type":"markdown","source":"##### Prediction based on Sigmoid"},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv(path + 'submission.csv')\nsub['ConfirmedCases'] = tes['Predicted Cases (Sig)']\nsub['Fatalities'] = tes['Predicted Fatalities (Sig)']\nsub.to_csv('submission.csv', index=False)\nsub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"}},"nbformat":4,"nbformat_minor":4}