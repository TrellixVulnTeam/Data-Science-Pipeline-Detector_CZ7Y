{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport math\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nfiles = [None]*3\ni = 0\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        files[i] = os.path.join(dirname, filename)\n        i = i + 1\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"for i in range(3):\n    f = files[i].find('train')\n    if f!=-1:\n        train_dt = pd.read_csv(files[i])\nfor i in range(3):\n    f = files[i].find('test')\n    if f!=-1:\n        test_dt = pd.read_csv(files[i])\nfor i in range(3):\n    f = files[i].find('submission')\n    if f!=-1:\n        submi_dt = pd.read_csv(files[i])\nend_train = train_dt.shape[0]\n\nprint('train inst., test inst')\nprint(train_dt.shape[0], test_dt.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas.api.types import CategoricalDtype\n\n# append train test sets\ntest_train_dt = train_dt.append(test_dt, sort=False)\n\n# make Date Categorical (ordered)\nuniqueDates = list(test_train_dt['Date'].unique())\nnrOfTrain_Test_dates = len(uniqueDates)\n\n# dates strating from 0\ncat_type_date = CategoricalDtype(categories = uniqueDates , ordered=True)\ntest_train_dt.Date = test_train_dt.Date.astype(cat_type_date).cat.codes.astype(float)\n# test_train_dt[test_train_dt.Country_Region=='Denmark']\ntest_train_dt.Province_State = test_train_dt.Province_State.astype(\"category\").cat.codes.astype(float)\n# Country_Region categorical\ntest_train_dt.Country_Region = test_train_dt.Country_Region.astype(\"category\").cat.codes.astype(float)\n# country starts with 0\nmax_country = test_train_dt.Country_Region.unique().max()\nmax_State = test_train_dt.Province_State.unique().max()\nprint('max_country, max_state')\nprint(max_country,max_State)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make COuntryIndx unique\ntest_train_dt['CountrIndx']=0\n\n# newCases_conf, neewCases_fat, mult_conf, mult_fat\n\ntest_train_dt['newCases_conf'] = 0.0\ntest_train_dt['newCases_fat'] = 0.0\n# test_train_dt['mult_conf']\n# test_train_dt['mult_fat']\nnrOfTrainTestInst = test_train_dt.shape[0]\n# for i in range(nrOfTrainTestInst):\n\nCountryIndx_Array = np.zeros((nrOfTrainTestInst,1))\n\n# give to the country that have not states the country id\nCountryIndx_Array[test_train_dt['Province_State']==-1,0] = test_train_dt[test_train_dt['Province_State']==-1].Country_Region.values\nCountryIndx_Array[test_train_dt['Province_State']!=-1,0] = test_train_dt[test_train_dt['Province_State']!=-1].Province_State.values\nCountryIndx_Array[test_train_dt['Province_State']!=-1,0] = CountryIndx_Array[test_train_dt['Province_State']!=-1,0] + (max_country + 1)\ntest_train_dt.CountrIndx = CountryIndx_Array","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train_set = test_train_dt[:end_train]\ntest_set = test_train_dt[end_train:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nrOfTrainSamples = train_set.shape[0]\n# train_set['ConfirmedCases'][0:10]\ntrain_set['DaySinceOutbr']=0\nmax_tr_date = train_set['Date'].unique().max()\nnrTrainInst = train_set.shape[0]\nsinceTheOutbr_array = np.zeros((nrTrainInst,1))\nfor i in range(nrTrainInst):\n    if train_set['Date'][i] == 0:\n        count = 0\n    if train_set['ConfirmedCases'][i] > 0 :\n        count = count + 1\n#         train_set['DaySinceOutbr'][i] = count\n        sinceTheOutbr_array[i] = count\n    if train_set['Date'][i] > 0:\n        tmp_11 = train_set['ConfirmedCases'][i] - train_set['ConfirmedCases'][i-1]\n        if tmp_11 < 0:\n            train_set.set_value(i,'ConfirmedCases', train_set['ConfirmedCases'][i-1])\n        tmp_11 = train_set['ConfirmedCases'][i] - train_set['ConfirmedCases'][i-1]\n        train_set.set_value(i,'newCases_conf', tmp_11)\n        \n        tmp_12 = train_set['Fatalities'][i] - train_set['Fatalities'][i-1]\n        if tmp_12 < 0:\n            train_set.set_value(i,'Fatalities', train_set['Fatalities'][i-1])\n        tmp_12 = train_set['Fatalities'][i] - train_set['Fatalities'][i-1]\n        train_set.set_value(i,'newCases_fat', tmp_12)\n#         tmp_12 = train_set['Fatalities'][i] - train_set['Fatalities'][i-1]\n#         train_set.set_value(i,'newCases_fat', tmp_12)\n        \ntrain_set['DaySinceOutbr'] = sinceTheOutbr_array\ntest_set['DaySinceOutbr']=0\nnrTestInst = test_set.shape[0]\nlastTrainDate = train_set['Date'].unique().max()\n\n# for i in range(nrTestInst):\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set[train_set['newCases_conf']<0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set[9280:9295]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"first_test_day = test_set['Date'].unique().min()\nnrOfTestInst = test_set.shape[0]\nsinceTheOutTest_arr = np.zeros((nrOfTestInst,1))\n\nfor i in range(nrOfTestInst):\n    if first_test_day>lastTrainDate:\n        print('ton ipiame')\n        if test_set['Date'][i] == first_test_day:\n            \n            # find the since the outbraek from train set\n            country_reg = test_set['CountrIndx'][i]\n            tmp1 = train_set[train_set['CountrIndx'] == country_reg]\n            # lastTrainDate\n            tmpSinceTheOut = tmp1[tmp1.Date==lastTrainDate].DaySinceOutbr.values\n            count = tmpSinceTheOut + 1\n            sinceTheOutTest_arr[i] = count\n        else:\n            count = count + 1\n            sinceTheOutTest_arr[i] = count\n    else:\n        if test_set['Date'][i] <= lastTrainDate:\n            courentDate = test_set['Date'][i]\n            #find the since the outbreak from train\n            country_reg = test_set['CountrIndx'][i]\n            tmp1 = train_set[train_set['CountrIndx'] == country_reg]\n            # lastTrainDate\n            tmpSinceTheOut = tmp1[tmp1.Date==courentDate].DaySinceOutbr.values\n            sinceTheOutTest_arr[i] = tmpSinceTheOut\n            count = tmpSinceTheOut\n        else:\n            count = count + 1\n            sinceTheOutTest_arr[i] = count\n            \ntest_set['DaySinceOutbr'] = sinceTheOutTest_arr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"time_span = 7;\n#confirmedCases = time_span, fatalities = time_span\n#DaySinceTheOutbreak = time_span, CountryIndx = 1, CourentDay = time_span\n#newCases_conf= time_span, newCases_fat= time_span\n# nuberOfFeatures = 6*time_span  + 1\nnuberOfFeatures = 7*time_span\n# train_set.shape[0]\n# nrOfTrainDates = train_set.Date.unique().shape[0]\n\nnrOfTrainData = train_set.shape[0]\n\n# find the unique countries/staes\n\ncountrUniques = train_set.CountrIndx.unique()\n\n# nrOfTrainData\nX = np.zeros((nrOfTrainData,nuberOfFeatures))\nY = np.zeros((nrOfTrainData,2))\n\n# calcuate the labels for the train set\n\ninst_count = 0;\n\n\nfor i in countrUniques:\n\n    country = train_set[train_set['CountrIndx']==i]\n    nrOfdaysPerCountry = country.shape[0]\n    \n    for j in range(nrOfdaysPerCountry-time_span):\n        \n        X[inst_count:inst_count+1, 0:time_span] = country.ConfirmedCases[j:time_span+j].values\n        X[inst_count:inst_count+1, time_span:2*time_span] = country.Fatalities[j:time_span+j].values\n        \n        X[inst_count:inst_count+1, 2*time_span:3*time_span] = country.DaySinceOutbr[j:time_span+j].values\n        \n        X[inst_count:inst_count+1,3*time_span:4*time_span] = country.Date[j:time_span+j].values\n        \n        X[inst_count:inst_count+1,4*time_span:5*time_span] = country.newCases_conf[j:time_span+j].values\n        X[inst_count:inst_count+1,5*time_span:6*time_span] = country.newCases_fat[j:time_span+j].values\n        \n#         X[inst_count:inst_count+1,6*time_span:7*time_span] = country.CountrIndx[j:j+1].values\n        X[inst_count:inst_count+1,6*time_span:7*time_span] = country.CountrIndx[j:time_span+j].values\n        \n#         X[inst_count:inst_count+1, 2*time_span+1:2*time_span+2] = country.DaySinceOutbr[j:j+1].values\n        \n        Y[inst_count:inst_count+1,0:1] = country.ConfirmedCases[time_span+j:time_span+j+1].values\n        Y[inst_count:inst_count+1,1:2] = country.Fatalities[time_span+j:time_span+j+1].values\n        \n        inst_count = inst_count + 1\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X[250,:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def log_10(x):\n#     return 1 / (1 + np.exp(x))\n    return np.log10(x+1)\ndef min_max_scale(x, min_val, max_val):\n    return (x - min_val) / (max_val - min_val)\nX_scale = np.zeros((nrOfTrainData,nuberOfFeatures))\nY_scale = np.zeros((nrOfTrainData,2))\nX_scale = X\nX_scale[:,0:2*time_span] = log_10(X[:,0:2*time_span])\nX_scale[:,2*time_span:4*time_span] = min_max_scale(X[:,2*time_span:4*time_span], 0, test_set['Date'].unique().max())\n\nX_scale[:,4*time_span:6*time_span] = log_10(X[:,4*time_span:6*time_span])\n\nX_scale[:,6*time_span:7*time_span] = min_max_scale(X[:,6*time_span:7*time_span], 0, train_set['CountrIndx'].unique().max())\n# X_scale[:,3*time_span+1:3*time_span+2] = min_max_scale(X[:,3*time_span:3*time_span+1], 0, test_set['Date'].unique().max())\n\nY_scale = log_10(Y)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X[200:200+1,6*time_span:7*time_span] = country.CountrIndx[j:time_span+j].values\ncountry.CountrIndx[j:time_span+j].values\nX[150,:].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from keras.datasets import mnist\nimport keras.utils.np_utils as ku\nimport keras.models as models\nimport keras.layers as layers\nfrom keras import regularizers\nfrom keras.optimizers import rmsprop, Adam\nimport numpy as np\nimport numpy.random as nr\n# from tensorflow import set_random_seed\nimport matplotlib.pyplot as plt\n\nfrom keras.layers import Dropout, LeakyReLU","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nrOfTrainSamples\n# countrUniques.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nn = models.Sequential()\nnn.add(layers.Dense(6*time_span, activation = 'softplus', \n                    input_shape = (nuberOfFeatures, ), \n                   kernel_regularizer=regularizers.l2(0.001)))\n\n# nn.add(layers.Dense(6*time_span, activation = 'softplus',  \n#                    kernel_regularizer=regularizers.l2(0.001)))\n\nnn.add(layers.Dense(5*time_span, activation = 'softplus',  \n                   kernel_regularizer=regularizers.l2(0.001)))\n\nnn.add(layers.Dense(4*time_span, activation = 'softplus',  \n                   kernel_regularizer=regularizers.l2(0.001)))\nnn.add(layers.Dense(3*time_span, activation = 'softplus',  \n                   kernel_regularizer=regularizers.l2(0.001)))\nnn.add(layers.Dense(2*time_span, activation = 'softplus',  \n                   kernel_regularizer=regularizers.l2(0.001)))\nnn.add(layers.Dense(time_span,activation = 'softplus', \n                   kernel_regularizer=regularizers.l2(0.001)))\n# nn.add(Dropout(rate = 0.2))\n# nn.add(layers.Dense(countrUniques.shape[0], activation = 'relu',\n#                    kernel_regularizer=regularizers.l2(0.01)))\n# nn.add(layers.Dense(countrUniques.shape[0], activation = 'softplus',\n#                    kernel_regularizer=regularizers.l2(0.001)))\nnn.add(layers.Dense(time_span, activation = 'softplus',  \n                   kernel_regularizer=regularizers.l2(0.001)))\nnn.add(layers.Dense(2*time_span, activation = 'softplus',  \n                   kernel_regularizer=regularizers.l2(0.001)))\n# nn.add(Dropout(rate = 0.2))\nnn.add(layers.Dense(3*time_span, activation = 'softplus', \n                   kernel_regularizer=regularizers.l2(0.001)))\nnn.add(layers.Dense(4*time_span, activation = 'softplus', \n                   kernel_regularizer=regularizers.l2(0.001)))\nnn.add(layers.Dense(5*time_span, activation = 'softplus', \n                   kernel_regularizer=regularizers.l2(0.001)))\nnn.add(layers.Dense(6*time_span, activation = 'softplus', \n                   kernel_regularizer=regularizers.l2(0.001)))\n# nn.add(layers.Dense(7*time_span, activation = 'softplus', \n#                    kernel_regularizer=regularizers.l2(0.001)))\n# nn.add(Dropout(rate = 0.2))\n\nnn.add(layers.Dense(2))\nnn.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_loss(history):\n    train_loss = history.history['loss']\n#     test_loss = history.history['val_loss']\n    x = list(range(1, len(train_loss) + 1))\n#     plt.plot(x, test_loss, color = 'red', label = 'test loss')\n    plt.plot(x, train_loss, label = 'traning loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Loss vs. Epoch')\n    plt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nn.compile(optimizer = 'Adam', loss = 'mean_squared_error', \n                metrics = ['mae'])\nhistory = nn.fit(X_scale, Y_scale,\n                  epochs = 800, batch_size = nrOfdaysPerCountry, verbose = 0, shuffle = False) #,validation_data = (X, Y))\n# history = nn.fit(X_scale, Y,\n#                   epochs = 600, batch_size = nrOfdaysPerCountry, verbose = 0) #,validation_data = (X, Y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history.history['loss'][-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_loss(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nrOfTestInst = test_set.shape[0]\n\n# X_test = np.zeros((nrOfTestInst, nuberOfFeatures))\nX_test_scale = np.zeros((1,nuberOfFeatures))\n# X_test_scale = np.zeros((nrOfTestInst, nuberOfFeatures))\n\n# the train dates\ntestDates = test_set.Date.unique()\nfirstTestDay = testDates.min()\n\ntest_set['Date'].unique().min()\n\nfor i in range(nrOfTestInst): #nrOfTestInst\n#     i=1\n    country_index = test_set.CountrIndx[i:i+1].values[0]\n\n    test_instance = test_set[i:i+1]\n    test_date = test_instance.Date.values[0]\n\n    start_previous_days = test_date - time_span\n    end_previous_days = test_date - 1\n    # how many days from test set\n\n\n    only_test_set_day = test_date-firstTestDay\n    if only_test_set_day>=time_span:\n        test_set_days = time_span\n    else:\n        test_set_days = test_date - firstTestDay   \n    # how many days from the train set\n    train_set_days = time_span - test_set_days\n\n    tmp_count = train_set[train_set['CountrIndx']==country_index]\n\n    tmp_count1 = tmp_count[tmp_count['Date']>=start_previous_days]\n    tmp_train_set_days = tmp_count1[tmp_count1['Date']<=start_previous_days+train_set_days-1]\n\n    test_set_start_day = test_date - test_set_days\n    test_set_end_day = test_date - 1\n\n    tmp_test = test_set[test_set.CountrIndx==country_index]\n\n    tmp_test1 = tmp_test[tmp_test.Date>=test_set_start_day]\n    tmp_test_set_days = tmp_test1[tmp_test1.Date<=test_set_end_day]\n\n    features_df = tmp_train_set_days.append(tmp_test_set_days)\n\n    #     X_test[i:i+1, 0:time_span] = features_df.ConfirmedCases[:].values\n    #     X_test[i:i+1, time_span:2*time_span] = features_df.Fatalities[:].values\n\n    #     X_test[i:i+1, 2*time_span:3*time_span] = features_df.DaySinceOutbr[:].values\n\n    #     X_test[i:i+1, 3*time_span:3*time_span+1] = features_df.CountrIndx[0:1].values\n\n    X_test_scale[0, 0:time_span] = log_10(features_df.ConfirmedCases[:].values)\n    X_test_scale[0, time_span:2*time_span] = log_10(features_df.Fatalities[:].values)\n\n    X_test_scale[0, 2*time_span:3*time_span] = min_max_scale(features_df.DaySinceOutbr[:].values,0, test_set['Date'].unique().max())\n    X_test_scale[0, 3*time_span:4*time_span] = min_max_scale(features_df.Date[:].values, 0, test_set['Date'].unique().max())\n\n    X_test_scale[0, 4*time_span:5*time_span] = log_10(features_df.newCases_conf[:].values)\n    X_test_scale[0, 5*time_span:6*time_span] = log_10(features_df.newCases_fat[:].values)\n\n    X_test_scale[0, 6*time_span:7*time_span] = min_max_scale(features_df.CountrIndx[:].values, 0, train_set['CountrIndx'].unique().max())\n\n    #     X_scale[:,2*time_span:3*time_span] = min_max_scale(X[:,2*time_span:3*time_span], 0, test_set['Date'].unique().max())\n    #     X_scale[:,3*time_span:3*time_span+1] = min_max_scale(X[:,3*time_span:3*time_span+1], 0, train_set['CountrIndx'].unique().max())\n    #     X_test_scale[i,:] = sigmoid(X_test[i,:])\n\n    #     prediction = nn.predict(X_test[i:i+1])\n    prediction = nn.predict(X_test_scale)\n    prediction = 10**prediction -1\n    #     print(prediction)\n    #     if prediction[0,0] < 0:\n    #         prediction[0,0] = 0\n    #     if prediction[0,1] < 0:\n    #         prediction[0,1] = 0\n    test_set.set_value(i, 'ConfirmedCases', round(prediction[0,0]))\n    test_set.set_value(i, 'Fatalities', round(prediction[0,1]))\n\n    # compute newConfirmed and newFat\n\n    if test_set['Date'][i] == firstTestDay:\n        # the first day of each country\n        # find the country\n        tmpCountry_ind = test_set['CountrIndx'][i]\n        tmp001 = train_set[train_set['CountrIndx']==tmpCountry_ind]\n        trainConf = tmp001[tmp001.Date==firstTestDay].ConfirmedCases.values[0]\n        trainFat = tmp001[tmp001.Date==firstTestDay].Fatalities.values[0]\n\n        # if newCases is negative give the previous value\n        newCASES = round(prediction[0,0]) - trainConf\n        newFAT = round(prediction[0,1]) - trainFat\n        if newCASES<0:\n#             test_set.set_value(i, 'ConfirmedCases', trainConf+1)\n            test_set.set_value(i, 'newCases_conf', 0)\n        else:\n            test_set.set_value(i, 'newCases_conf', newCASES)\n        if newFAT<0:\n#             test_set.set_value(i, 'Fatalities', trainFat)\n            test_set.set_value(i, 'newCases_fat', 0)\n        else:\n            test_set.set_value(i, 'newCases_fat', newFAT)\n    else:\n        newCASES = round(prediction[0,0]) - test_set['ConfirmedCases'][i-1]\n        newFAT = round(prediction[0,1]) - test_set['Fatalities'][i-1]\n        if newCASES<0:\n#             test_set.set_value(i, 'ConfirmedCases', test_set['ConfirmedCases'][i-1])\n            test_set.set_value(i, 'newCases_conf', 0)\n        else:\n            test_set.set_value(i, 'newCases_conf', newCASES)\n        if newFAT<0:\n#             test_set.set_value(i, 'Fatalities', test_set['Fatalities'][i-1])\n            test_set.set_value(i, 'newCases_fat', 0)\n        else:\n            test_set.set_value(i, 'newCases_fat', newFAT)\n#end for\n\n# find the overlap between test and train\ncountry_index = 0\ntrainDates = train_set[train_set['Country_Region']==0].Date\ntestDates = test_set[test_set['Country_Region']==0].Date\ncommon_Dates = np.intersect1d(trainDates, testDates)\n\nnrOfCommon_Dates = common_Dates.shape[0]\n\nS = 0\nn = 0\n\nfor i in range(nrOfCommon_Dates):\n    \n    tmp_train = train_set[train_set['Date']==common_Dates[i]].ConfirmedCases + 1\n    tmp_log_train = tmp_train.apply(np.log)\n    \n    tmp_test = test_set[test_set['Date']==common_Dates[i]].ConfirmedCases + 1\n    tmp_log_test = tmp_test.apply(np.log)\n    dif = (tmp_log_train.values - tmp_log_test.values)\n    squ = dif*dif\n    \n    tmp_train2 = train_set[train_set['Date']==common_Dates[i]].Fatalities + 1\n    tmp_log_train2 = tmp_train2.apply(np.log)\n    \n    tmp_test2 = test_set[test_set['Date']==common_Dates[i]].Fatalities + 1\n    tmp_log_test2 = tmp_test2.apply(np.log)\n    dif2 = (tmp_log_train2.values - tmp_log_test2.values)\n    squ2 = dif2*dif2\n    \n    S = S + squ.sum() + squ2.sum()\n    n = n + squ.shape[0] + squ2.shape[0]\n    \nRMSLE = np.sqrt((1/n)*S)\nprint('RMSLE:')\nprint(RMSLE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_scale[0,:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_set[test_set['ConfirmedCases']!=7]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_set[90:110]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ind = test_dt[test_dt['Country_Region']=='France'].ForecastId.index\n# ind = test_dt[test_dt['Province_State']=='Hubei'].ForecastId.index\nconfCases = test_set[ind.min():ind.max()]['ConfirmedCases']\nplt.plot(confCases, label = 'Confirmed Cases')\nplt.xlabel('Predictions Dates since 19/03')\nplt.ylabel('Confirmed Cases')\nplt.title('Confirmed Cases')\n# ConfirmedCases\nfatal = test_set[ind.min():ind.max()]['Fatalities']\nplt.figure()\nplt.plot(fatal, label = 'Fatalities')\nplt.xlabel('Predictions Dates since 19/03')\nplt.ylabel('Fatalities')\nplt.title('Fatalities')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submi_dt['ConfirmedCases'] = test_set.ConfirmedCases\nsubmi_dt['Fatalities'] = test_set.Fatalities\nsubmi_dt.to_csv('submission.csv', index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}