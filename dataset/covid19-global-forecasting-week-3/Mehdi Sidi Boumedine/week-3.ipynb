{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\n#submission = pd.read_csv(\"../input/covid19-global-forecasting-week-1/submission.csv\")\ntest = pd.read_csv(\"../input/covid19-global-forecasting-week-3/test.csv\", parse_dates=[\"Date\"])\ntrain = pd.read_csv(\"../input/covid19-global-forecasting-week-3/train.csv\", parse_dates=[\"Date\"])\nglobal_data = pd.read_csv(\"../input/external/Global_Data_by_Country_2019.csv\")\ncoordinates=pd.read_csv(\"../input/external/coordinates.csv\")\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"coordinates.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"coordinates=coordinates[[\"Country/Region\", \"Province/State\", \"Lat\", \"Long\"]]\ncoordinates.loc[coordinates[\"Province/State\"].isnull(), \"Province/State\"]=coordinates.loc[coordinates[\"Province/State\"].isnull(), \"Country/Region\"]\ncoordinates.drop_duplicates(keep='first', inplace=True)\n\n\nfrom math import radians, sin, cos, asin, sqrt\ndef haversine(lon1, lat1, lon2, lat2):\n    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n    dlon = lon2 - lon1\n    dlat = lat2 - lat1\n    a = sin(dlat / 2) ** 2 + cos(lat1) * cos(lat2) * sin(dlon / 2) ** 2\n    return 2 * 6371 * asin(sqrt(a))\n\n\ncoordinates.set_index(\"Province/State\")\nlatitude=coordinates[coordinates[\"Province/State\"]==\"Hubei\"].Lat\nlongitude=coordinates[coordinates[\"Province/State\"]==\"Hubei\"].Long\n\ncoordinates[\"Distance\"]=coordinates.apply(lambda x : haversine(lat1=x.Lat, lon1=x.Long, lat2=latitude, lon2=longitude), axis=1)\ncoordinates=coordinates[[\"Province/State\", \"Distance\"]]\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"coordinates.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### when there's no province replace it by country name\ntrain.loc[train[\"Province_State\"].isnull(), \"Province_State\"]=train.loc[train[\"Province_State\"].isnull(), \"Country_Region\"]\n###join train data with external data about country mainly : area, population, life expectancy, health spendings\ntrain.rename(columns = {'Country_Region':'Country', 'Province_State':'Province'}, inplace = True) \ntrain=train.merge(global_data, how='left', left_on=['Country', 'Province'], right_on=['CountryName', 'Province'])\ntrain=train.merge(coordinates, how='left', left_on=['Province'], right_on=['Province/State'])\ntrain=train.drop(\"Province/State\", axis=1)\ntrain=train.drop(\"Population\", axis=1)\ntrain=train.drop(\"CountryName\", axis=1)\ntrain=train.rename(columns={\"ExtraColumn\": \"Population\"})\nmindates = train[train[\"ConfirmedCases\"]>0].groupby(['Province'])[\"Date\"].min()\nmindates.reset_index()\nmindatesDF = mindates.to_frame()\nmindatesDF.rename(columns={\"Date\":\"MinDate\"}, inplace=True)\ntrain=train.merge(mindatesDF, how='left', left_on=\"Province\", right_on=\"Province\")\ntrain[\"DaysFrom1stCase\"]=(train[\"Date\"]-train[\"MinDate\"]).dt.days\ntrain.loc [train[\"DaysFrom1stCase\"]<0 , \"DaysFrom1stCase\"] =0\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.rename(columns = {'Country_Region':'Country', 'Province_State':'Province'}, inplace = True) \ntest.loc[test[\"Province\"].isnull(), \"Province\"]=test.loc[test[\"Province\"].isnull(), \"Country\"]\nX_test=test.merge(global_data, how='left', left_on=['Country', 'Province'], right_on=['CountryName', 'Province'])\nX_test=X_test.drop(\"Population\", axis=1)\nX_test=X_test.drop(\"CountryName\", axis=1)\nX_test=X_test.rename(columns={\"ExtraColumn\": \"Population\"})\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test=X_test.merge(coordinates, how='left', left_on=['Province'], right_on=['Province/State'])\nX_test=X_test.drop(\"Province/State\", axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder \n\nlabelencoder = LabelEncoder()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"province=labelencoder.fit_transform(train[\"Province\"])\ntrain=pd.concat([train, pd.DataFrame(province)], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntrain=train.drop([\"Country\",\"MinDate\", \"Province\"], axis=1)\n#train=train.drop([\"Country\",\"MinDate\"], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train.ConfirmedCases>0].head(100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA\n\n\ny_train_CC=train.loc[:,\"ConfirmedCases\"]\ny_train_F=train.loc[:, \"Fatalities\"]\nX_train=train.drop([\"ConfirmedCases\", \"Fatalities\", \"Id\", \"Date\"], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in X_train.columns:\n    X_train[col]=X_train[col].fillna(X_train[col].median())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"from sklearn.model_selection import train_test_split\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom sklearn.model_selection import GridSearchCV\n\nparams = {\n        'task': 'train',\n        'boosting_type': 'gbdt',\n        'objective': 'regression',\n        'metric': {'rmse'},\n        'learning_rate': 0.3,\n        'num_leaves': 30,\n        'min_data_in_leaf': 1,\n        'num_iteration': 100,\n        'verbose': 20\n}\n\nscore_store={}\nfor col1 in X_train.columns:\n    df1=X_train.drop(col1, axis=1)\n    second_col_score={}\n    for col2 in df1.columns:\n        df2=df1.drop(col2, axis=1)\n        X_train_real, X_test_val, y_train_real_CC, y_test_val_CC = train_test_split(df2, y_train_CC, test_size=0.3, random_state=0)\n        lgb_train_CC = lgb.Dataset(X_train_real, y_train_real_CC)\n        lgb_eval_CC = lgb.Dataset(X_test_val, y_test_val_CC, reference=lgb_train_CC)\n        X_train_real, X_test_val, y_train_real_F, y_test_val_F = train_test_split(df2, y_train_F, test_size=0.3, random_state=0)\n        lgb_train_F = lgb.Dataset(X_train_real, y_train_real_F)\n        lgb_eval_F = lgb.Dataset(X_test_val, y_test_val_F, reference=lgb_train_F)\n        print (\"XXXXX %s XXXXXXXXX %s\", col1 , col2)\n        gbm_CC = lgb.train(params,\n            lgb_train_CC,\n            num_boost_round=100,\n            valid_sets=lgb_eval_CC,\n            early_stopping_rounds=10, verbose_eval = -1)\n        gbm_F = lgb.train(params,\n            lgb_train_F,\n            num_boost_round=100,\n            valid_sets=lgb_eval_F,\n            early_stopping_rounds=10, verbose_eval = -1)\n        second_col_score[col2]=gbm_CC.best_score['valid_0']['rmse']\n        print (gbm_CC.best_score['valid_0']['rmse'])\n        print (gbm_F.best_score['valid_0']['rmse'])\n    score_store[col1]=second_col_score\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nfrom sklearn.model_selection import train_test_split\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom sklearn.model_selection import GridSearchCV\n\n#pca = PCA(n_components=5)\n#pca.fit(X_train)\n\n#X_train= pd.DataFrame(pca.transform(X_train))\n\nX_train_CC=X_train.drop([\"LifeExpectancy\", \"Population\"], axis=1)\n\nX_train_real, X_test_val, y_train_real_CC, y_test_val_CC = train_test_split(\n        X_train_CC, y_train_CC, test_size=0.3, random_state=0)\n\n\n\nlgb_train_CC = lgb.Dataset(X_train_real, y_train_real_CC)\nlgb_eval_CC = lgb.Dataset(X_test_val, y_test_val_CC, reference=lgb_train_CC)\n\nX_train_F=X_train.drop([\"LifeExpectancy\", \"Distance\"], axis=1)\n\nX_train_real, X_test_val, y_train_real_F, y_test_val_F = train_test_split(\n        X_train_F, y_train_F, test_size=0.3, random_state=0)\n\nlgb_train_F = lgb.Dataset(X_train_real, y_train_real_F)\nlgb_eval_F = lgb.Dataset(X_test_val, y_test_val_F, reference=lgb_train_F)\n\n\n\nparams = {\n        'task': 'train',\n        'boosting_type': 'gbdt',\n        'objective': 'regression',\n        'metric': {'rmse'},\n        'learning_rate': 0.3,\n        'num_leaves': 30,\n        'min_data_in_leaf': 1,\n        'num_iteration': 100,\n        'verbose': 20\n}\n\ngbm_CC = lgb.train(params,\n            lgb_train_CC,\n            num_boost_round=100,\n            valid_sets=lgb_eval_CC,\n            early_stopping_rounds=10)\n\ngbm_F = lgb.train(params,\n            lgb_train_F,\n            num_boost_round=100,\n            valid_sets=lgb_eval_F,\n            early_stopping_rounds=10)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Transform the Test Dataset before prediction**"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n## add days since first case column ##\nX_test=X_test.merge(mindatesDF, how='left', left_on=\"Province\", right_on=\"Province\")\nX_test[\"DaysFrom1stCase\"]=(X_test[\"Date\"]-X_test[\"MinDate\"]).dt.days\nX_test.loc [X_test[\"DaysFrom1stCase\"]<0 , \"DaysFrom1stCase\"] =0\n\n#X_test=pd.get_dummies(X_test, prefix='prov', prefix_sep='_', dummy_na=True, columns=\"Province\", sparse=False, drop_first=False, dtype=None)\n\nX_test=X_test.drop([\"Country\", \"MinDate\"], axis=1)\n\nX_test=X_test.drop([\"ForecastId\", \"Date\"], axis=1)\n\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"province=labelencoder.transform(X_test[\"Province\"])\nX_test=pd.concat([X_test,pd.DataFrame(province) ], axis=1)\nX_test=X_test.drop([\"Province\"], axis=1)\nfor col in X_test.columns:\n    X_test[col]=X_test[col].fillna(X_test[col].median())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#X_test= pca.transform(X_test)\nX_test_CC=X_test.drop([\"LifeExpectancy\", \"Population\"], axis=1)\nX_test_F=X_test.drop([\"LifeExpectancy\", \"Population\"], axis=1)\n#y_pred=regressor.predict(X_test)\ny_pred_CC = gbm_CC.predict(X_test_CC, num_iteration=gbm_CC.best_iteration)\ny_pred_F = gbm_F.predict(X_test_F, num_iteration=gbm_F.best_iteration)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#forecastId=test.ForecastId.to_numpy()\nsubmission_CC=pd.DataFrame(y_pred_CC)\nsubmission_CC=submission_CC.rename(columns={0:\"ConfirmedCases\"})\nsubmission_F=pd.DataFrame(y_pred_F)\nsubmission_F=submission_F.rename(columns={0:\"Fatalities\"})\n#submission[\"ForecastId\"]=test[\"ForecastId\"]\nsubmission=test.copy()\nsubmission=submission.drop(columns=[\"Province\", \"Country\", \"Date\"])\nsubmission[\"ConfirmedCases\"]=submission_CC[\"ConfirmedCases\"]\nsubmission[\"Fatalities\"]=submission_F[\"Fatalities\"]\nsubmission.head()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission=submission.dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test=pd.concat([test, submission ], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test[test[\"Country\"]==\"Algeria\"]","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":4}