{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.optimizers import Nadam\nfrom sklearn.metrics import mean_squared_error\nimport tensorflow as tf\nimport tensorflow.keras.layers as KL\nfrom datetime import timedelta\nimport numpy as np\nimport pandas as pd\nimport tensorflow.keras.backend as K\n\nimport os\npd.options.display.max_rows = 500\npd.options.display.max_columns = 500\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom scipy.signal import savgol_filter\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.isotonic import IsotonicRegression\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error\nfrom sklearn.linear_model import LinearRegression, Ridge\n\nimport datetime\nimport gc\nfrom tqdm import tqdm\n\nimport xgboost as xgb\n\ndef rmse( yt, yp ):\n    return np.sqrt( np.mean( (yt-yp)**2 ) )\n\n\n\n\nclass CovidModel:\n    def __init__(self):\n        pass\n    \n    def predict_first_day(self, date):\n        return None\n    \n    def predict_next_day(self, yesterday_pred_df):\n        return None\n\n\nclass CovidModelAhmet(CovidModel):\n    def preprocess(self, df, meta_df):\n        df[\"Date\"] = pd.to_datetime(df['Date'])\n\n        df = df.merge(meta_df, on=self.loc_group, how=\"left\")\n        df[\"lat\"] = (df[\"lat\"] // 30).astype(np.float32).fillna(0)\n        df[\"lon\"] = (df[\"lon\"] // 60).astype(np.float32).fillna(0)\n\n        df[\"population\"] = np.log1p(df[\"population\"]).fillna(-1)\n        df[\"area\"] = np.log1p(df[\"area\"]).fillna(-1)\n\n        for col in self.loc_group:\n            df[col].fillna(\"\", inplace=True)\n            \n        df['day'] = df.Date.dt.dayofyear\n        df['geo'] = ['_'.join(x) for x in zip(df['Country_Region'], df['Province_State'])]\n        return df\n\n    def get_model(self):\n        \n        def nn_block(input_layer, size, dropout_rate, activation):\n            out_layer = KL.Dense(size, activation=None)(input_layer)\n            out_layer = KL.Activation(activation)(out_layer)\n            out_layer = KL.Dropout(dropout_rate)(out_layer)\n            return out_layer\n    \n        ts_inp = KL.Input(shape=(len(self.ts_features),))\n        global_inp = KL.Input(shape=(len(self.global_features),))\n\n        inp = KL.concatenate([global_inp, ts_inp])\n        hidden_layer = nn_block(inp, 64, 0.0, \"relu\")\n        gate_layer = nn_block(hidden_layer, 32, 0.0, \"sigmoid\")\n        hidden_layer = nn_block(hidden_layer, 32, 0.0, \"relu\")\n        hidden_layer = KL.multiply([hidden_layer, gate_layer])\n\n        out = KL.Dense(len(self.TARGETS), activation=\"linear\")(hidden_layer)\n\n        model = tf.keras.models.Model(inputs=[global_inp, ts_inp], outputs=out)\n        return model\n    \n    def get_input(self, df):\n        return [df[self.global_features], df[self.ts_features]]\n        \n    def train_models(self, df, num_models=20, save=False):\n        \n        def custom_loss(y_true, y_pred):\n            return K.sum(K.sqrt(K.sum(K.square(y_true - y_pred), axis=0, keepdims=True)))/len(self.TARGETS)\n    \n        models = []\n        for i in range(num_models):\n            model = self.get_model()\n            model.compile(loss=custom_loss, optimizer=Nadam(lr=1e-4))\n            hist = model.fit(self.get_input(df), df[self.TARGETS],\n                             batch_size=2048, epochs=200, verbose=0, shuffle=True)\n            if save:\n                model.save_weights(\"model{}.h5\".format(i))\n            models.append(model)\n        return models\n    \n    \n    def predict_one(self, df):\n        \n        pred = np.zeros((df.shape[0], 2))\n        for model in self.models:\n            pred += model.predict(self.get_input(df))/len(self.models)\n        pred = np.maximum(pred, df[self.prev_targets].values)\n        pred[:, 0] = np.log1p(np.expm1(pred[:, 0]) + 0.1)\n        pred[:, 1] = np.log1p(np.expm1(pred[:, 1]) + 0.01)\n        return np.clip(pred, None, 15)\n    \n\n    def __init__(self):\n        df = pd.read_csv(\"../input/covid19-global-forecasting-week-3/train.csv\")\n        sub_df = pd.read_csv(\"../input/covid19-global-forecasting-week-3/test.csv\")\n\n        meta_df = pd.read_csv(\"../input/covid19-forecasting-metadata/region_metadata.csv\")\n\n        self.loc_group = [\"Province_State\", \"Country_Region\"]\n\n        df = self.preprocess(df, meta_df)\n        sub_df = self.preprocess(sub_df, meta_df)\n        \n        df = df.merge(sub_df[[\"ForecastId\", \"Date\", \"geo\"]], how=\"left\", on=[\"Date\", \"geo\"])\n        df = df.append(sub_df[sub_df[\"Date\"] > df[\"Date\"].max()], sort=False)\n        \n        df[\"day\"] = df[\"day\"] - df[\"day\"].min()\n\n        self.TARGETS = [\"ConfirmedCases\", \"Fatalities\"]\n        self.prev_targets = ['prev_ConfirmedCases_1', 'prev_Fatalities_1']\n\n        for col in self.TARGETS:\n            df[col] = np.log1p(df[col])\n\n        self.NUM_SHIFT = 7\n\n        self.global_features = [\"lat\", \"lon\", \"population\", \"area\"]\n        self.ts_features = []\n\n        for s in range(1, self.NUM_SHIFT+1):\n            for col in self.TARGETS:\n                df[\"prev_{}_{}\".format(col, s)] = df.groupby(self.loc_group)[col].shift(s)\n                self.ts_features.append(\"prev_{}_{}\".format(col, s))\n\n        self.df = df[df[\"Date\"] >= df[\"Date\"].min() + timedelta(days=self.NUM_SHIFT)].copy()\n\n        \n    def predict_first_day(self, day):\n        self.models = self.train_models(self.df[self.df[\"day\"] < day])\n        \n        temp_df = self.df.loc[self.df[\"day\"] == day].copy()\n        y_pred = self.predict_one(temp_df)\n            \n        self.y_prevs = [None]*self.NUM_SHIFT\n\n        for i in range(1, self.NUM_SHIFT):\n            self.y_prevs[i] = temp_df[['prev_ConfirmedCases_{}'.format(i), 'prev_Fatalities_{}'.format(i)]].values\n            \n        temp_df[self.TARGETS] = y_pred\n        self.day = day\n        return temp_df[[\"geo\", \"day\"] + self.TARGETS]\n    \n    \n    def predict_next_day(self, yesterday_pred_df):\n        self.day = self.day + 1\n\n        temp_df = self.df.loc[self.df[\"day\"] == self.day].copy()\n        \n        yesterday_pred_df = temp_df[[\"geo\"]].merge(yesterday_pred_df[[\"geo\"] + self.TARGETS], on=\"geo\", how=\"left\")\n        temp_df[self.prev_targets] = yesterday_pred_df[self.TARGETS].values\n\n        for i in range(2, self.NUM_SHIFT+1):\n            temp_df[['prev_ConfirmedCases_{}'.format(i), 'prev_Fatalities_{}'.format(i)]] = self.y_prevs[i-1]\n\n        y_pred, self.y_prevs = self.predict_one(temp_df), [None, temp_df[self.prev_targets].values] + self.y_prevs[1:-1]\n\n        temp_df[self.TARGETS] = y_pred\n        return temp_df[[\"geo\", \"day\"] + self.TARGETS]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CovidModelCPMP(CovidModel):\n    \n    def __init__(self):\n        train = pd.read_csv('../input/covid19-global-forecasting-week-3/train.csv')\n        train['Province_State'].fillna('', inplace=True)\n        train['Date'] = pd.to_datetime(train['Date'])\n        train['day'] = train.Date.dt.dayofyear\n        train['geo'] = ['_'.join(x) for x in zip(train['Country_Region'], train['Province_State'])]\n        test = pd.read_csv('../input/covid19-global-forecasting-week-3/test.csv')\n        test['Province_State'].fillna('', inplace=True)\n        test['Date'] = pd.to_datetime(test['Date'])\n        test['day'] = test.Date.dt.dayofyear\n        test['geo'] = ['_'.join(x) for x in zip(test['Country_Region'], test['Province_State'])]\n        day_min = train['day'].min()\n        train['day'] -= day_min\n        test['day'] -= day_min  \n        self.min_test_val_day = test.day.min()\n        self.max_test_val_day = train.day.max()\n        self.max_test_day = test.day.max()\n\n        train['ForecastId'] = -1\n        test['Id'] = -1\n        test['ConfirmedCases'] = 0\n        test['Fatalities'] = 0    \n        data = pd.concat([train,\n                  test[test.day > self.max_test_val_day][train.columns]\n                 ]).reset_index(drop=True)\n        self.data = data\n        self.train = train\n        self.test = test\n        self.dates = data[data['geo'] == 'France_'].Date.values\n        region_meta = pd.read_csv('../input/covid19-forecasting-metadata/region_metadata.csv')\n        region_meta['Province_State'].fillna('', inplace=True)\n        region_meta['geo'] = ['_'.join(x) for x in zip(region_meta['Country_Region'], region_meta['Province_State'], )]\n        population = data[['geo']].merge(region_meta, how='left', on='geo').fillna(0)\n        population = population.groupby('geo')[['population']].first()\n        population['population'] = np.log1p(population['population'])\n        self.population = population[['population']].values\n        continents = region_meta['continent']\n        continents = pd.factorize(continents)[0]\n        continents_ids_base = continents.reshape((-1, 1))\n        ohe = OneHotEncoder(sparse=False)\n        self.continents_ids_base = ohe.fit_transform(continents_ids_base)\n        \n        self.geo_data = data.pivot(index='geo', columns='day', values='ForecastId')\n        self.num_geo = self.geo_data.shape[0]\n        self.ConfirmedCases = data.pivot(index='geo', columns='day', values='ConfirmedCases')\n        self.Fatalities = data.pivot(index='geo', columns='day', values='Fatalities')\n        self.cases = np.log1p(self.ConfirmedCases.values)\n        self.deaths = np.log1p(self.Fatalities.values)\n        self.case_threshold = 30\n        \n        self.c_case = 10\n        self.t_case = 100\n        self.c_death = 10\n        self.t_death = 5\n\n        time_cases = self.c_case * (self.cases >= np.log1p(self.t_case)) \n        time_cases = np.cumsum(time_cases, axis=1)\n        self.time_cases = 1 * np.log1p(time_cases) \n\n        time_deaths = self.c_death * (self.deaths >= np.log1p(self.t_death))\n        time_deaths = np.cumsum(time_deaths, axis=1)\n        self.time_deaths = 1 *np.log1p(time_deaths) \n\n        countries = [g.split('_')[0] for g in self.geo_data.index]\n        countries = pd.factorize(countries)[0]\n        country_ids_base = countries.reshape((-1, 1))\n        ohe = OneHotEncoder(sparse=False)\n        self.country_ids_base = 0.2 * ohe.fit_transform(country_ids_base)\n\n        self.start_lag_death = 13\n        self.end_lag_death = 5\n        self.num_train = 5\n        self.num_lag_case = 14\n        self.lag_period = max(self.start_lag_death, self.num_lag_case)\n        \n        # For tetsing purpose       \n        self.df = train[['geo', 'day', 'ConfirmedCases', 'Fatalities']].copy()\n        self.df.ConfirmedCases = np.log1p(self.df.ConfirmedCases)\n        self.df.Fatalities = np.log1p(self.df.Fatalities)\n        \n    def get_country_ids(self):\n        countries = [g.split('_')[0] for g in self.geo_data.index]\n        countries = pd.factorize(countries)[0]\n        countries[self.cases[:, :self.last_train+1].max(axis=1) < np.log1p(self.case_threshold)] = -1\n        countries = pd.factorize(countries)[0]\n\n\n        country_ids_base = countries.reshape((-1, 1))\n        ohe = OneHotEncoder(sparse=False)\n        country_ids_base = 0.2 * ohe.fit_transform(country_ids_base)\n        return country_ids_base\n    \n    def val_score(self, true, pred):\n        return np.sqrt(mean_squared_error(true.ravel(), pred.ravel()))\n    \n    def get_dataset(self, start_pred, num_train):\n        days = np.arange( start_pred - num_train + 1, start_pred + 1)\n        lag_cases = np.vstack([self.cases[:, d - self.lag_period : d] for d in days])\n        lag_deaths = np.vstack([self.deaths[:, d - self.lag_period : d] for d in days])\n        target_cases = np.vstack([self.cases[:, d : d + 1] for d in days])\n        target_deaths = np.vstack([self.deaths[:, d : d + 1] for d in days])\n        continents_ids = np.vstack([self.continents_ids_base for d in days])\n        country_ids = np.vstack([self.country_ids_base for d in days])\n        population = np.vstack([self.population for d in days])\n        time_case = np.vstack([self.time_cases[:, d - 1: d ] for d in days])\n        time_death = np.vstack([self.time_deaths[:, d - 1 : d ] for d in days])\n        return (lag_cases, lag_deaths, target_cases, target_deaths, \n            continents_ids, country_ids, population, time_case, time_death, days)\n    \n    def update_time(self, time_death, time_case, pred_death, pred_case):\n        new_time_death = np.expm1(time_death) + self.c_death * (pred_death >= np.log1p(self.t_death))\n        new_time_death = 1 *np.log1p(new_time_death) \n        new_time_case = np.expm1(time_case) + self.c_case * (pred_case >= np.log1p(self.t_case))\n        new_time_case = 1 *np.log1p(new_time_case) \n        return new_time_death, new_time_case\n\n    def update_valid_dataset(self, dataset, pred_death, pred_case, pred_day):\n        (lag_cases, lag_deaths, target_cases, target_deaths, \n         continents_ids, country_ids, population, time_case, time_death, days) = dataset\n        if pred_day != days[-1]:\n            print('error', pred_day, days[-1])\n            return None\n        day = days[-1] + 1\n        new_lag_cases = np.hstack([lag_cases[:, 1:], pred_case])\n        new_lag_deaths = np.hstack([lag_deaths[:, 1:], pred_death]) \n        new_target_cases = self.cases[:, day:day+1]\n        new_target_deaths = self.deaths[:, day:day+1] \n        new_continents_ids = continents_ids  \n        new_country_ids = country_ids  \n        new_population = population  \n        new_time_death, new_time_case = self.update_time(time_death, time_case, pred_death, pred_case)\n        new_days = 1 + days\n        return (new_lag_cases, new_lag_deaths, new_target_cases, new_target_deaths, \n            new_continents_ids, new_country_ids, new_population, \n                new_time_case, new_time_death, new_days)\n        \n    def fit_eval(self, dataset, fit):\n        (lag_cases, lag_deaths, target_cases, target_deaths, \n         continents_ids, country_ids, population, \n         time_case, time_death, days) = dataset\n\n        X_death = np.hstack([lag_cases[:, -self.start_lag_death:-self.end_lag_death], \n                             lag_deaths[:, -self.num_lag_case:], \n                             country_ids,\n                             continents_ids,\n                              population,\n                             time_case,\n                             time_death,\n                            ])\n        y_death = target_deaths\n        y_death_prev = lag_deaths[:, -1:]\n        if fit:\n             self.lr_death.fit(X_death, y_death)\n        y_pred_death = self.lr_death.predict(X_death)\n        y_pred_death = np.maximum(y_pred_death, y_death_prev)\n\n        X_case = np.hstack([lag_cases[:, -self.num_lag_case:], \n                            country_ids, \n                            continents_ids,\n                            population,\n                             time_case,\n                             #time_death,\n                           ])\n        y_case = target_cases\n        y_case_prev = lag_cases[:, -1:]\n        if fit:\n            self.lr_case.fit(X_case, y_case)\n        y_pred_case = self.lr_case.predict(X_case)\n        y_pred_case = np.maximum(y_pred_case, y_case_prev)\n\n        return y_pred_death, y_pred_case\n     \n    def get_pred_df(self, val_death_preds, val_case_preds, ):\n        pred_deaths = self.Fatalities.iloc[:, self.start_val:self.start_val+self.num_val].copy()\n        #pred_deaths.iloc[:, :] = np.expm1(val_death_preds)\n        pred_deaths.iloc[:, :] = val_death_preds\n        pred_deaths = pred_deaths.stack().reset_index()\n        pred_deaths.columns = ['geo', 'day', 'Fatalities']\n        pred_deaths\n\n        pred_cases = self.ConfirmedCases.iloc[:, self.start_val:self.start_val+self.num_val].copy()\n        #pred_cases.iloc[:, :] = np.expm1(val_case_preds)\n        pred_cases.iloc[:, :] = val_case_preds\n        pred_cases = pred_cases.stack().reset_index()\n        pred_cases.columns = ['geo', 'day', 'ConfirmedCases']\n        pred_cases\n\n        sub = self.data[['geo', 'day']]\n        sub = sub[sub.day == self.start_val]\n        sub = sub.merge(pred_cases, how='left', on=['geo', 'day'])\n        sub = sub.merge(pred_deaths, how='left', on=['geo', 'day'])\n        sub = sub[(sub.day >= self.start_val) & (sub.day <= self.end_val)]\n        return sub\n    \n    def predict_first_day(self, day):\n        self.start_val = day\n        self.end_val = day + 1\n        self.num_val = self.end_val - self.start_val + 1\n        score = True\n        self.last_train = self.start_val - 1\n        print(self.dates[self.last_train], self.start_val, self.num_val)\n        self.country_ids_base = self.get_country_ids()\n        train_data = self.get_dataset(self.last_train, self.num_train)\n        alpha = 3\n        self.lr_death = Ridge(alpha=alpha, fit_intercept=True)\n        self.lr_case = Ridge(alpha=alpha, fit_intercept=True)\n        _ = self.fit_eval(train_data, fit=True)\n        \n        self.valid_data = self.get_dataset(self.start_val, 1)\n        val_death_preds, val_case_preds = self.fit_eval(self.valid_data, fit=False)\n        df = self.get_pred_df(val_death_preds, val_case_preds)\n        return df\n    \n    def predict_next_day(self, yesterday_pred_df):\n        yesterday_pred_df = yesterday_pred_df.sort_values(by='geo').reset_index(drop=True)\n        if yesterday_pred_df.day.nunique() != 1:\n            print('error', yesterday_pred_df.day.unique())\n            return None\n        pred_death = yesterday_pred_df[['Fatalities']].values\n        pred_case = yesterday_pred_df[['ConfirmedCases']].values\n        pred_day = yesterday_pred_df.day.unique()[0]\n        \n        new_valid_data = self. update_valid_dataset(self.valid_data, \n                                                    pred_death, pred_case, pred_day)\n        if len(new_valid_data) > 0:\n            self.valid_data = new_valid_data\n        self.start_val = pred_day + 1\n        self.end_val = pred_day + 2\n        val_death_preds, val_case_preds = self.fit_eval(self.valid_data, fit=False)\n        df = self.get_pred_df(val_death_preds, val_case_preds)\n         \n        return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CovidModelGIBA(CovidModel):\n    def __init__(self, lag=1, seed=1 ):\n\n        self.lag  = lag\n        self.seed = seed\n        print( 'Lag:', lag, 'Seed:', seed )\n        \n        train = pd.read_csv('../input/covid19-global-forecasting-week-3/train.csv')\n        train['Date'] = pd.to_datetime( train['Date'] )\n        self.maxdate  = str(train['Date'].max())[:10]\n        self.testdate = str( train['Date'].max() + pd.Timedelta(days=1) )[:10]\n        print( 'Last Date in Train:',self.maxdate, 'Test first Date:',self.testdate )\n        train['Province_State'].fillna('', inplace=True)\n        train['day'] = train.Date.dt.dayofyear\n        self.day_min = train['day'].min()\n        train['day'] -= self.day_min\n        train['geo'] = ['_'.join(x) for x in zip(train['Country_Region'], train['Province_State'])]\n\n        test  = pd.read_csv('../input/covid19-global-forecasting-week-3/test.csv')\n        test['Date'] = pd.to_datetime( test['Date'] )\n        test['Province_State'].fillna('', inplace=True)\n        test['day'] = test.Date.dt.dayofyear\n        test['day'] -= self.day_min\n        test['geo'] = ['_'.join(x) for x in zip(test['Country_Region'], test['Province_State'])]\n        test['Id'] = -1\n        test['ConfirmedCases'] = 0\n        test['Fatalities'] = 0\n\n        self.trainmaxday  = train['day'].max()\n        self.testday1 = train['day'].max() + 1\n        self.testdayN = test['day'].max()\n        \n        publictest = test.loc[ test.Date > train.Date.max() ].copy()\n        train = pd.concat( (train, publictest ), sort=False )\n        train.sort_values( ['Country_Region','Province_State','Date'], inplace=True )\n        train = train.reset_index(drop=True)\n\n        train['ForecastId'] = pd.merge( train, test, on=['Country_Region','Province_State','Date'], how='left' )['ForecastId_y'].values\n\n        train['cid'] = train['Country_Region'] + '_' + train['Province_State']\n\n        train['log0'] = np.log1p( train['ConfirmedCases'] )\n        train['log1'] = np.log1p( train['Fatalities'] )\n\n        train = train.loc[ (train.log0 > 0) | (train.ForecastId.notnull()) | (train.Date >= '2020-03-17') ].copy()\n        train = train.reset_index(drop=True)\n\n        train['days_since_1case'] = train.groupby('cid')['Id'].cumcount()\n\n        dt = pd.read_csv('../input/covid19-lockdown-dates-by-country/countryLockdowndates.csv')\n        dt.columns = ['Country_Region','Province_State','Date','Type','Reference']\n        dt = dt.loc[ dt.Date == dt.Date ]\n        dt['Province_State'] = dt['Province_State'].fillna('')\n        dt['Date'] = pd.to_datetime( dt['Date'] )\n        dt['Date'] = dt['Date'] + pd.Timedelta(days=8)\n        dt['Type'] = pd.factorize( dt['Type'] )[0]\n        dt['cid'] = dt['Country_Region'] + '_' + dt['Province_State']\n        del dt['Reference'], dt['Country_Region'], dt['Province_State']\n        train = pd.merge( train, dt, on=['cid','Date'], how='left' )\n        train['Type'] = train.groupby('cid')['Type'].fillna( method='ffill' )\n\n        train['target0'] = np.log1p( train['ConfirmedCases'] )\n        train['target1'] = np.log1p( train['Fatalities'] )\n        # dt = pd.read_csv('../input/covid19-country-data-wk3-release/Data Join - RELEASE.csv')\n        # dt['Province_State'] = dt['Province_State'].fillna('')\n        # dt['Country_Region'] = dt['Country_Region'].fillna('')\n        # train = pd.merge( train, dt, on=['Country_Region','Province_State'], how='left' )\n        # #Fix\n\n        #print( train.head(4) )\n        #print( train.shape ) \n        \n        self.train = train.copy()\n   \n    \n    def create_features( self, df, valid_day ):\n\n        df['lag0_1'] = df.groupby('cid')['target0'].shift(self.lag)\n        df['lag1_1'] = df.groupby('cid')['target1'].shift(self.lag)\n        df['lag0_1'] = df.groupby('cid')['lag0_1'].fillna( method='bfill' )\n        df['lag1_1'] = df.groupby('cid')['lag1_1'].fillna( method='bfill' )\n\n        df['m0'] = df.groupby('cid')['lag0_1'].rolling(2).mean().values\n        df['m1'] = df.groupby('cid')['lag0_1'].rolling(3).mean().values\n        df['m2'] = df.groupby('cid')['lag0_1'].rolling(4).mean().values\n        df['m3'] = df.groupby('cid')['lag0_1'].rolling(5).mean().values\n        df['m4'] = df.groupby('cid')['lag0_1'].rolling(7).mean().values\n        df['m5'] = df.groupby('cid')['lag0_1'].rolling(10).mean().values\n        df['m6'] = df.groupby('cid')['lag0_1'].rolling(12).mean().values\n        df['m7'] = df.groupby('cid')['lag0_1'].rolling(16).mean().values\n        df['m8'] = df.groupby('cid')['lag0_1'].rolling(20).mean().values\n\n        df['n0'] = df.groupby('cid')['lag1_1'].rolling(2).mean().values\n        df['n1'] = df.groupby('cid')['lag1_1'].rolling(3).mean().values\n        df['n2'] = df.groupby('cid')['lag1_1'].rolling(4).mean().values\n        df['n3'] = df.groupby('cid')['lag1_1'].rolling(5).mean().values\n        df['n4'] = df.groupby('cid')['lag1_1'].rolling(7).mean().values\n        df['n5'] = df.groupby('cid')['lag1_1'].rolling(10).mean().values\n        df['n6'] = df.groupby('cid')['lag1_1'].rolling(12).mean().values\n        df['n7'] = df.groupby('cid')['lag1_1'].rolling(16).mean().values\n        df['n8'] = df.groupby('cid')['lag1_1'].rolling(20).mean().values\n\n\n        df['m0'] = df.groupby('cid')['m0'].fillna( method='bfill' )\n        df['m1'] = df.groupby('cid')['m1'].fillna( method='bfill' )\n        df['m2'] = df.groupby('cid')['m2'].fillna( method='bfill' )\n        df['m3'] = df.groupby('cid')['m3'].fillna( method='bfill' )\n        df['m4'] = df.groupby('cid')['m4'].fillna( method='bfill' )\n        df['m5'] = df.groupby('cid')['m5'].fillna( method='bfill' )\n        df['m6'] = df.groupby('cid')['m6'].fillna( method='bfill' )\n        df['m7'] = df.groupby('cid')['m7'].fillna( method='bfill' )\n        df['m8'] = df.groupby('cid')['m8'].fillna( method='bfill' )\n\n        df['n0'] = df.groupby('cid')['n0'].fillna( method='bfill' )\n        df['n1'] = df.groupby('cid')['n1'].fillna( method='bfill' )\n        df['n2'] = df.groupby('cid')['n2'].fillna( method='bfill' )\n        df['n3'] = df.groupby('cid')['n3'].fillna( method='bfill' )\n        df['n4'] = df.groupby('cid')['n4'].fillna( method='bfill' )\n        df['n5'] = df.groupby('cid')['n5'].fillna( method='bfill' )\n        df['n6'] = df.groupby('cid')['n6'].fillna( method='bfill' )\n        df['n7'] = df.groupby('cid')['n7'].fillna( method='bfill' )\n        df['n8'] = df.groupby('cid')['n8'].fillna( method='bfill' )\n\n        df['flag_China'] = 1*(df['Country_Region'] == 'China')\n        #df['flag_Italy'] = 1*(df['Country_Region'] == 'Italy')\n        #df['flag_Spain'] = 1*(df['Country_Region'] == 'Spain')\n        df['flag_US']    = 1*(df['Country_Region'] == 'US')\n        #df['flag_Brazil']= 1*(df['Country_Region'] == 'Brazil')\n        \n        df['flag_Kosovo_']   = 1*(df['cid'] == 'Kosovo_')\n        df['flag_Korea']     = 1*(df['cid'] == 'Korea, South_')\n        df['flag_Nepal_']    = 1*(df['cid'] == 'Nepal_')\n        df['flag_Holy See_'] = 1*(df['cid'] == 'Holy See_')\n        df['flag_Suriname_'] = 1*(df['cid'] == 'Suriname_')\n        df['flag_Ghana_']    = 1*(df['cid'] == 'Ghana_')\n        df['flag_Togo_']     = 1*(df['cid'] == 'Togo_')\n        df['flag_Malaysia_'] = 1*(df['cid'] == 'Malaysia_')\n        df['flag_US_Rhode']  = 1*(df['cid'] == 'US_Rhode Island')\n        df['flag_Bolivia_']  = 1*(df['cid'] == 'Bolivia_')\n        df['flag_China_Tib'] = 1*(df['cid'] == 'China_Tibet')\n        df['flag_Bahrain_']  = 1*(df['cid'] == 'Bahrain_')\n        df['flag_Honduras_'] = 1*(df['cid'] == 'Honduras_')\n        df['flag_Bangladesh']= 1*(df['cid'] == 'Bangladesh_')\n        df['flag_Paraguay_'] = 1*(df['cid'] == 'Paraguay_')\n\n        tr = df.loc[ df.day  < valid_day ].copy()\n        vl = df.loc[ df.day == valid_day ].copy()\n\n        tr = tr.loc[ tr.lag0_1 > 0 ].copy()\n\n        maptarget0 = tr.groupby('cid')['target0'].agg( log0_max='max' ).reset_index()\n        maptarget1 = tr.groupby('cid')['target1'].agg( log1_max='max' ).reset_index()\n        vl['log0_max'] = pd.merge( vl, maptarget0, on='cid' , how='left' )['log0_max'].values\n        vl['log1_max'] = pd.merge( vl, maptarget1, on='cid' , how='left' )['log1_max'].values\n        vl['log0_max'] = vl['log0_max'].fillna(0)\n        vl['log1_max'] = vl['log1_max'].fillna(0)\n\n        return tr, vl\n    \n\n    def train_models(self, valid_day = 10 ):\n\n        train = self.train.copy()\n\n        #Fix some anomalities:\n        train.loc[ (train.cid=='China_Guizhou') & (train.Date=='2020-03-17') , 'target0' ] = np.log1p( 146 )\n        train.loc[ (train.cid=='Guyana_')&(train.Date>='2020-03-22')&(train.Date<='2020-03-30') , 'target0' ] = np.log1p( 12 )\n        train.loc[ (train.cid=='US_Virgin Islands')&(train.Date>='2020-03-29')&(train.Date<='2020-03-29') , 'target0' ] = np.log1p( 24 )\n        train.loc[ (train.cid=='US_Virgin Islands')&(train.Date>='2020-03-30')&(train.Date<='2020-03-30') , 'target0' ] = np.log1p( 27 )\n\n        train.loc[ (train.cid=='Iceland_')&(train.Date>='2020-03-15')&(train.Date<='2020-03-15') , 'target1' ] = np.log1p( 0 )\n        train.loc[ (train.cid=='Kazakhstan_')&(train.Date>='2020-03-20')&(train.Date<='2020-03-20') , 'target1' ] = np.log1p( 0 )\n        train.loc[ (train.cid=='Serbia_')&(train.Date>='2020-03-26')&(train.Date<='2020-03-26') , 'target1' ] = np.log1p( 5 )\n        train.loc[ (train.cid=='Serbia_')&(train.Date>='2020-03-27')&(train.Date<='2020-03-27') , 'target1' ] = np.log1p( 6 )\n        train.loc[ (train.cid=='Slovakia_')&(train.Date>='2020-03-22')&(train.Date<='2020-03-31') , 'target1' ] = np.log1p( 1 )\n        train.loc[ (train.cid=='US_Hawaii')&(train.Date>='2020-03-25')&(train.Date<='2020-03-31') , 'target1' ] = np.log1p( 1 )\n\n        param = {\n            'subsample': 0.9850,\n            'colsample_bytree': 0.850,\n            'max_depth': 6,\n            'gamma': 0.000,\n            'learning_rate': 0.010,\n            'min_child_weight': 5.00,\n            'reg_alpha': 0.000,\n            'reg_lambda': 0.400,\n            'silent':1,\n            'objective':'reg:squarederror',\n            #'booster':'dart',\n            #'tree_method': 'gpu_hist',\n            'nthread': 12,#-1,\n            'seed': self.seed\n            }    \n\n        tr, vl = self.create_features( train.copy(), valid_day )\n        \n        features = [f for f in tr.columns if f not in [\n            'Id','ConfirmedCases','Fatalities','log0','log1','target0','target1','ypred0','ypred1','Province_State','Country_Region','Date','ForecastId','cid','geo','day',\n            'GDP_region','TRUE POPULATION','pct_in_largest_city',' TFR ',' Avg_age ','latitude','longitude','abs_latitude','temperature', 'humidity',\n            'Personality_pdi','Personality_idv','Personality_mas','Personality_uai','Personality_ltowvs','Personality_assertive','personality_perform','personality_agreeableness',\n            'murder','High_rises','max_high_rises','AIR_CITIES','AIR_AVG','continent_gdp_pc','continent_happiness','continent_generosity','continent_corruption','continent_Life_expectancy'        \n        ] ]\n        self.features = features\n        #print( self.features )\n        #print( 'tr.shape, vl.shape:',tr[features].shape, vl[features].shape )\n        #print( tr['Date'].min(),tr['Date'].max(),vl['Date'].min(),vl['Date'].max() )\n        #print( tr['day'].min(),tr['day'].max(),vl['day'].min(),vl['day'].max() )\n\n        nrounds0 = 630\n        nrounds1 = 630\n         #lag 1###############################################################\n        dtrain = xgb.DMatrix( tr[features], tr['target0'] )\n        param['seed'] = self.seed\n        self.model0 = xgb.train( param, dtrain, nrounds0, verbose_eval=0 )\n        param['seed'] = self.seed+1\n        self.model1 = xgb.train( param, dtrain, nrounds0, verbose_eval=0 )\n        \n        dtrain = xgb.DMatrix( tr[features], tr['target1'] )\n        param['seed'] = self.seed\n        self.model2 = xgb.train( param, dtrain, nrounds1, verbose_eval=0 ) \n        param['seed'] = self.seed+1\n        self.model3 = xgb.train( param, dtrain, nrounds1, verbose_eval=0 )\n        \n        self.vl = vl\n        \n        return 1\n    \n        \n    def predict_first_day(self, day ):\n        \n        self.day = day\n        self.train_models( day )\n        \n        dvalid = xgb.DMatrix( self.vl[self.features] )\n        ypred0 = ( self.model0.predict( dvalid ) + self.model1.predict( dvalid )  ) / 2\n        ypred1 = ( self.model2.predict( dvalid ) + self.model3.predict( dvalid )  ) / 2\n        \n        self.vl['ypred0'] = ypred0\n        self.vl['ypred1'] = ypred1\n        self.vl.loc[ self.vl.ypred0<self.vl.log0_max, 'ypred0'] =  self.vl.loc[ self.vl.ypred0<self.vl.log0_max, 'log0_max']\n        self.vl.loc[ self.vl.ypred1<self.vl.log1_max, 'ypred1'] =  self.vl.loc[ self.vl.ypred1<self.vl.log1_max, 'log1_max']\n        \n        VALID = self.vl[[\"geo\", \"day\", 'ypred0', 'ypred1']].copy()\n        VALID.columns = [\"geo\", \"day\", 'ConfirmedCases', 'Fatalities']        \n        return VALID.reset_index(drop=True)\n    \n    \n    def predict_next_day(self, yesterday ):\n\n        self.day += 1\n        \n        feats = ['geo','day']        \n        self.train[ 'ypred0' ] = pd.merge( self.train[feats], yesterday[feats+['ConfirmedCases']], on=feats, how='left' )['ConfirmedCases'].values\n        self.train.loc[ self.train.ypred0.notnull(), 'target0'] = self.train.loc[ self.train.ypred0.notnull() , 'ypred0']\n\n        self.train[ 'ypred1' ] = pd.merge( self.train[feats], yesterday[feats+['Fatalities']], on=feats, how='left' )['Fatalities'].values\n        self.train.loc[ self.train.ypred1.notnull(), 'target1'] = self.train.loc[ self.train.ypred1.notnull() , 'ypred1']\n        del self.train['ypred0'], self.train['ypred1']\n        \n        tr, vl = self.create_features( self.train.copy(), self.day )        \n        dvalid = xgb.DMatrix( vl[self.features] )\n        \n        ypred0 = (self.model0.predict( dvalid ) + self.model1.predict( dvalid ) )/2\n        ypred1 = (self.model2.predict( dvalid ) + self.model3.predict( dvalid ) )/2\n    \n        vl['ypred0'] = ypred0\n        vl['ypred1'] = ypred1\n        vl.loc[ vl.ypred0<vl.log0_max, 'ypred0'] =  vl.loc[ vl.ypred0<vl.log0_max, 'log0_max']\n        vl.loc[ vl.ypred1<vl.log1_max, 'ypred1'] =  vl.loc[ vl.ypred1<vl.log1_max, 'log1_max']\n        \n        self.vl = vl\n        VALID = vl[[\"geo\", \"day\", 'ypred0', 'ypred1']].copy()\n        VALID.columns = [\"geo\", \"day\", 'ConfirmedCases', 'Fatalities']        \n        return VALID.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TARGETS = [\"ConfirmedCases\", \"Fatalities\"]\n\n\ndef rmse(y_true, y_pred):\n    return np.sqrt(mean_squared_error(y_true, y_pred))\n\ndf = pd.read_csv(\"../input/covid19-global-forecasting-week-3/train.csv\")\ndf[TARGETS] = np.log1p(df[TARGETS].values)\nsub_df = pd.read_csv(\"../input/covid19-global-forecasting-week-3/test.csv\")\n\ndef preprocess(df):\n    for col in [\"Country_Region\", \"Province_State\"]:\n        df[col].fillna(\"\", inplace=True)\n\n    df[\"Date\"] = pd.to_datetime(df['Date'])\n    df['day'] = df.Date.dt.dayofyear\n    df['geo'] = ['_'.join(x) for x in zip(df['Country_Region'], df['Province_State'])]\n    return df\n\ndf = preprocess(df)\nsub_df = preprocess(sub_df)\n\nsub_df[\"day\"] -= df[\"day\"].min()\ndf[\"day\"] -= df[\"day\"].min()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TEST_FIRST = sub_df[sub_df[\"Date\"] > df[\"Date\"].max()][\"Date\"].min()\nprint(TEST_FIRST)\nTEST_DAYS = (sub_df[\"Date\"].max() - TEST_FIRST).days + 1\nTEST_FIRST = (TEST_FIRST - df[\"Date\"].min()).days\n\nprint(TEST_FIRST, TEST_DAYS)\n\n\ndef get_blend(pred_dfs, weights, verbose=True):\n    if verbose:\n        for n1, n2 in [(\"cpmp\", \"giba\"), (\"cpmp\", \"ahmet\"), (\"giba\", \"ahmet\")]:\n            for t in TARGETS:\n                print(n1, n2, t, np.round(rmse(pred_dfs[n1][t], pred_dfs[n2][t]), 4))\n    \n    blend_df = pred_dfs[\"cpmp\"].copy()\n    blend_df[TARGETS] = 0\n    for name, pred_df in pred_dfs.items():\n        blend_df[TARGETS] += weights[name]*pred_df[TARGETS].values\n        \n    return blend_df\n\n\ncov_models = {\"ahmet\": CovidModelAhmet(), \"cpmp\": CovidModelCPMP(), 'giba': CovidModelGIBA()}\nweights = {\"ahmet\": 0.35, \"cpmp\": 0.30, \"giba\": 0.35}\npred_dfs = {name: cm.predict_first_day(TEST_FIRST).sort_values(\"geo\") for name, cm in cov_models.items()}\n\n\nblend_df = get_blend(pred_dfs, weights)\neval_df = blend_df.copy()\n\nfor d in range(1, TEST_DAYS):\n    pred_dfs = {name: cm.predict_next_day(blend_df).sort_values(\"geo\") for name, cm in cov_models.items()}\n    blend_df = get_blend(pred_dfs, weights)\n    eval_df = eval_df.append(blend_df)\n    print(d, eval_df.shape, flush=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eval_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(sub_df.shape)\nsub_df = sub_df.merge(df.append(eval_df, sort=False), on=[\"geo\", \"day\"], how=\"left\")\nprint(sub_df.shape)\nprint(sub_df[TARGETS].isnull().mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df[sub_df[\"geo\"] == \"France_\"][[\"day\"] +TARGETS].plot(x=\"day\")\nplt.axvline(TEST_FIRST, color='r', linestyle='--', lw=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df[sub_df[\"geo\"] == \"Brazil_\"][[\"day\"] +TARGETS].plot(x=\"day\")\nplt.axvline(TEST_FIRST, color='r', linestyle='--', lw=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df[sub_df[\"geo\"] == \"Turkey_\"][[\"day\"] +TARGETS].plot(x=\"day\")\nplt.axvline(TEST_FIRST, color='r', linestyle='--', lw=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df[TARGETS] = np.expm1(sub_df[TARGETS].values)\nsub_df.to_csv(\"submission.csv\", index=False, columns=[\"ForecastId\"] + TARGETS)\nsub_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":4}