{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5"},"cell_type":"markdown","source":"# Summary\n\nSo far QDA with pseudo labelling has returned the best results in this competition. Pseudo labelling is used as a technique to augment the data set and improve the quality of the QDA fit.\n\n**What will be the score if we slightly boost the accuracy of the pseudo labelled data with intermediate models (one or more)?**. \n\nHere I am using PCA-QDA as intermediate model (feel free to play with different models). My final prediction is 50 percent of PCA-QDA (pseudo label from variance-QDA) and 50 percent of Variance-QDA (pseudo label from PCA-QDA intermediate model).\n\n95% of the kernel is stolen from \n[Dieter](https://www.kaggle.com/christofhenkel/lets-implement-qda-by-ourself) and [Chris](https://www.kaggle.com/cdeotte/pseudo-labeling-qda-0-969). "},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true},"cell_type":"markdown","source":"# Intro\nLuckily Chris already put some illustrative pictures in his kernel, I can steal.\nHere is a pictorial explanation using sythetic 2D data. \n  \n## Step 1 - Build first model\nGiven 50 training observations (25 target=1 yellow points, 25 target=0 blue points) we can estimate the multivariate (approx 40 dimensions) normal distributions of each of the two target types (0 & 1) by calculating empiral covariance and mean (see np.cov and np.mean) and then calculate that a given datapoint belongs to distribution A or B using scipy.stats.multivariate.\n\n\n![image](http://playagricola.com/Kaggle/p16419.png)\n\nhttps://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.multivariate_normal.html#scipy.stats.multivariate_normal\nhttps://docs.scipy.org/doc/numpy/reference/generated/numpy.cov.html\n"},{"metadata":{},"cell_type":"markdown","source":"\n## Load Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np, pandas as pd, os\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.svm import NuSVC\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.decomposition import PCA\nfrom matplotlib import pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\n\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"#train['wheezy-copper-turtle-magic']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"# INITIALIZE VARIABLES\ncols = [c for c in train.columns if c not in ['id', 'target']]\ncols.remove('wheezy-copper-turtle-magic')\noof = np.zeros(len(train))\npreds = np.zeros(len(test))\n\n# BUILD 512 SEPARATE MODELS\nfor i in range(512):\n    # ONLY TRAIN WITH DATA WHERE WHEEZY EQUALS I\n    train2 = train[train['wheezy-copper-turtle-magic']==i]\n    test2 = test[test['wheezy-copper-turtle-magic']==i]\n    idx1 = train2.index; idx2 = test2.index\n    train2.reset_index(drop=True,inplace=True)\n    \n    data = pd.concat([pd.DataFrame(train2[cols]), pd.DataFrame(test2[cols])])\n    \n    # FEATURE SELECTION (USE APPROX 40 OF 255 FEATURES)\n    sel = VarianceThreshold(threshold=1.5).fit(data[cols])\n    data2 = sel.transform(data[cols])\n    \n    train3 = data2[:train2.shape[0]]; test3 = data2[train2.shape[0]:]\n    #sel = VarianceThreshold(threshold=1.5).fit(train2[cols])\n    #train3 = sel.transform(train2[cols])\n    #test3 = sel.transform(test2[cols])\n    \n    # STRATIFIED K-FOLD\n    skf = StratifiedKFold(n_splits=11, random_state=42, shuffle=True)\n    for train_index, test_index in skf.split(train3, train2['target']):\n        \n        # MODEL AND PREDICT WITH QDA\n        clf = QuadraticDiscriminantAnalysis(reg_param=0.5)\n        \n        \n        clf.fit(train3[train_index,:],train2.loc[train_index]['target'])\n        oof[idx1[test_index]] = clf.predict_proba(train3[test_index,:])[:,1]\n        preds[idx2] += clf.predict_proba(test3)[:,1] / skf.n_splits\n       \n    #if i%64==0: print(i)\n        \n# PRINT CV AUC\nauc = roc_auc_score(train['target'],oof)\nprint('QDA scores CV =',round(auc,5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  Add pseudo label data and run an intermediate model (PCA-QDA) "},{"metadata":{},"cell_type":"markdown","source":"I am using the variance threshold to estimate the number of components"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ncat_dict = dict()\n\n# INITIALIZE VARIABLES\ncols = [c for c in train.columns if c not in ['id', 'target']]\ncols.remove('wheezy-copper-turtle-magic')\n\nfor i in range(512):\n\n    \n    # ONLY TRAIN WITH DATA WHERE WHEEZY EQUALS I\n    train2 = train[train['wheezy-copper-turtle-magic']==i]\n    test2 = test[test['wheezy-copper-turtle-magic']==i]\n    idx1 = train2.index; idx2 = test2.index\n    train2.reset_index(drop=True,inplace=True)\n    \n    \n    \n    sel = VarianceThreshold(threshold=1.5).fit(train2[cols])\n    train3 = sel.transform(train2[cols])\n    test3 = sel.transform(test2[cols])\n        \n    cat_dict[i] = train3.shape[1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The number of variables generated from the variance based threshold is not uniform. Therefore  I have chose to adapt the number of components in PCA in consequence.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"\npd.DataFrame(list(cat_dict.items()))[1].value_counts().plot.barh()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Same as in the original kernel I use pseudlabelling ..."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Add pseudo label data from PCA-QDA and run a new model Variance-QDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# INITIALIZE VARIABLES\ntest['target'] = preds\noof_var = np.zeros(len(train))\npreds_var = np.zeros(len(test))\n\n# BUILD 512 SEPARATE MODELS\nfor k in range(512):\n    # ONLY TRAIN WITH DATA WHERE WHEEZY EQUALS I\n    train2 = train[train['wheezy-copper-turtle-magic']==k] \n    train2p = train2.copy(); idx1 = train2.index \n    test2 = test[test['wheezy-copper-turtle-magic']==k]\n    \n    # ADD PSEUDO LABELED DATA\n    test2p = test2[ (test2['target']<=0.01) | (test2['target']>=0.99) ].copy()\n    test2p.loc[ test2p['target']>=0.5, 'target' ] = 1\n    test2p.loc[ test2p['target']<0.5, 'target' ] = 0 \n    train2p = pd.concat([train2p,test2p],axis=0)\n    train2p.reset_index(drop=True,inplace=True)\n    \n    # FEATURE SELECTION (USE APPROX 40 OF 255 FEATURES)\n    \n  \n    \n    pca = PCA(n_components=cat_dict[k], random_state= 1234)\n    pca.fit(train2p[cols])\n    train3p = pca.transform(train2p[cols])\n    train3 = pca.transform(train2[cols])\n    test3 = pca.transform(test2[cols])\n\n           \n        \n    # STRATIFIED K FOLD\n    skf = StratifiedKFold(n_splits=11, random_state=42, shuffle=True)\n    for train_index, test_index in skf.split(train3p, train2p['target']):\n        test_index3 = test_index[ test_index<len(train3) ] # ignore pseudo in oof\n        \n        clf = QuadraticDiscriminantAnalysis(reg_param=0.5)\n        clf.fit(train3p[train_index,:],train2p.loc[train_index]['target'])\n        oof_var[idx1[test_index3]] += clf.predict_proba(train3[test_index3,:])[:,1]\n        preds_var[test2.index] += clf.predict_proba(test3)[:,1] / skf.n_splits\n       \n       \n    #if k%64==0: print(k)\n        \n# PRINT CV AUC\nauc = roc_auc_score(train['target'],oof_var)\nprint('Pseudo Labeled QDA scores CV =',round(auc,5)) #0.97035\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# INITIALIZE VARIABLES\ntest['target'] = preds_var  \noof_var2 = np.zeros(len(train))\npreds_var2 = np.zeros(len(test))\n\n# BUILD 512 SEPARATE MODELS\nfor k in range(512):\n    # ONLY TRAIN WITH DATA WHERE WHEEZY EQUALS I\n    train2 = train[train['wheezy-copper-turtle-magic']==k] \n    train2p = train2.copy(); idx1 = train2.index \n    test2 = test[test['wheezy-copper-turtle-magic']==k]\n    \n    # ADD PSEUDO LABELED DATA\n    test2p = test2[ (test2['target']<=0.01) | (test2['target']>=0.99) ].copy()\n    test2p.loc[ test2p['target']>=0.5, 'target' ] = 1\n    test2p.loc[ test2p['target']<0.5, 'target' ] = 0 \n    train2p = pd.concat([train2p,test2p],axis=0)\n    train2p.reset_index(drop=True,inplace=True)\n    \n    # FEATURE SELECTION (USE APPROX 40 OF 255 FEATURES)\n    \n    \n    \n       \n    sel = VarianceThreshold(threshold=1.5).fit(train2p[cols])     \n    train3p = sel.transform(train2p[cols])\n    train3 = sel.transform(train2[cols])\n    test3 = sel.transform(test2[cols])\n           \n        \n    # STRATIFIED K FOLD\n    skf = StratifiedKFold(n_splits=11, random_state=42, shuffle=True)\n    for train_index, test_index in skf.split(train3p, train2p['target']):\n        test_index3 = test_index[ test_index<len(train3) ] # ignore pseudo in oof\n        \n        clf = QuadraticDiscriminantAnalysis(reg_param=0.5)\n        clf.fit(train3p[train_index,:],train2p.loc[train_index]['target'])\n        oof_var2[idx1[test_index3]] += clf.predict_proba(train3[test_index3,:])[:,1]\n        preds_var2[test2.index] += clf.predict_proba(test3)[:,1] / skf.n_splits\n       \n       \n    #if k%64==0: print(k)\n        \n# PRINT CV AUC\nauc = roc_auc_score(train['target'],oof_var2)\nprint('Pseudo Labeled QDA scores CV =',round(auc,5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Final model accuracy"},{"metadata":{"trusted":true},"cell_type":"code","source":"auc = roc_auc_score(train['target'],0.5*(oof_var+ oof_var2) )\nprint('Pseudo Labeled QDA scores CV =',round(auc,5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submit Predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv('../input/sample_submission.csv')\nsub['target'] = 0.5* preds_var + 0.5*preds_var2\nsub.to_csv('submission.csv',index=False)\n\nimport matplotlib.pyplot as plt\nplt.hist(preds,bins=100)\nplt.title('Final Test.csv predictions')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\n**Let  try  intermediate models on pseudo labels?**"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}