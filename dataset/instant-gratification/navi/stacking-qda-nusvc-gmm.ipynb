{"cells":[{"metadata":{},"cell_type":"markdown","source":"### LB: 0.97018   Private Score: 0.97121"},{"metadata":{},"cell_type":"markdown","source":"As it was the first competition to work seriously, I will leave it in Kernel as a memorial...\n\nMy coding skill is poor.. So, I'm glad to point out the wrong codes.\n\nThank you."},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import os, gc, time, datetime, hashlib, random, pickle\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm_notebook as tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import make_classification \n\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures, MinMaxScaler\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier, Ridge\nfrom sklearn.svm import SVC, NuSVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.decomposition import PCA, TruncatedSVD, KernelPCA\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis, LinearDiscriminantAnalysis\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.tree import ExtraTreeClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.cluster import KMeans\n\nimport optuna\noptuna.logging.disable_default_handler()\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline\npd.set_option('max_columns', 9999)\npd.set_option('max_rows', 9999)\n\nstart = time.time()\nos.listdir('../input/')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_kg_hide-input":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"def load_data(nrows=None, verbose=True):\n    train = pd.read_csv('../input/train.csv', nrows=nrows)\n    test = pd.read_csv('../input/test.csv')\n            \n    if verbose:\n        print('Train Shape: {}'.format(train.shape))\n        print('Test Shape: {}'.format(test.shape))\n    \n    return train, test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# https://www.kaggle.com/christofhenkel/graphicallasso-gaussianmixture\nfrom sklearn.covariance import GraphicalLasso\n\ndef get_mean_cov(x,y):\n    model = GraphicalLasso()\n    ones = (y==1).astype(bool)\n    x2 = x[ones]\n    model.fit(x2)\n    p1 = model.precision_\n    m1 = model.location_\n    \n    onesb = (y==0).astype(bool)\n    x2b = x[onesb]\n    model.fit(x2b)\n    p2 = model.precision_\n    m2 = model.location_\n    \n    ms = np.stack([m1,m2])\n    ps = np.stack([p1,p2])\n    return ms,ps","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def trainer(train, test, cv, params, model_type='qda', _scaler=True, SVD=None, \n            variance=False, _model=None, _plot=True, threshold=1.5):\n    _train, _test = train.copy(), test.copy()\n#     train_org, _ = load_data(verbose=False)\n    _test['target'] = -1\n    cols = [col for col in _train.columns if not col in ['id', 'target', 'wheezy-copper-turtle-magic']]\n    \n    # Setting\n    preds = np.zeros(len(_test))\n    oof = np.zeros(len(_train))\n    oof_org = np.zeros(len(train_org))\n    \n    for i in tqdm(range(512)):\n        train_2 = _train[_train['wheezy-copper-turtle-magic'] == i]\n        test_2 = _test[_test['wheezy-copper-turtle-magic'] == i]\n        train_2_org = train_org[train_org['wheezy-copper-turtle-magic'] == i]\n        \n        len_train = len(train_2)\n        Y = train_2['target'].reset_index(drop=True).values\n        train_index = train_2.index\n        test_index = test_2.index\n        train_org_index = train_2_org.index\n        \n        # Concat Train/Test Data\n        merged_data = pd.concat([train_2, test_2], axis=0).reset_index(drop=True)[cols].values\n        \n        # Dimension Compression\n        if SVD is not None:\n            clf = TruncatedSVD(n_components=SVD)\n            _X = clf.fit_transform(merged_data)\n            X_org = clf.transform(train_2_org[cols].values)\n        elif variance:\n            clf = VarianceThreshold(threshold=threshold)\n            _X = clf.fit_transform(merged_data)\n            X_org = clf.transform(train_2_org[cols].values)\n        else:\n            _X = merged_data\n            X_org = clf.transform(train_2_org[cols].values)            \n        \n        # Scaler\n        if _scaler:\n            _scaler = StandardScaler()\n            _X = _scaler.fit_transform(_X)\n            X_org = _scaler.transform(X_org)\n        \n        # Divide Train, Test data\n        X = _X[:len_train]\n        test_2 = _X[len_train:]\n        del merged_data, _X\n        gc.collect()\n        \n        # Change dtype\n        X = X.astype(np.float32)\n        test_2 = test_2.astype(np.float32)\n        X_org = X_org.astype(np.float32)\n        \n        # Model\n        for i, (trn_index, val_index) in enumerate(cv.split(X, Y)):\n            \n            if model_type == 'gmm':\n                ms, ps = get_mean_cov(X, Y)\n            \n            X_trn, X_val = X[trn_index], X[val_index]\n            Y_trn, Y_val = Y[trn_index], Y[val_index]\n            \n            if model_type == 'nusvc':\n                model = NuSVC(**params)\n            elif model_type == 'qda':\n                model = QuadraticDiscriminantAnalysis(**params)\n            elif model_type == 'gmm':\n                model = GaussianMixture(**params, means_init=ms, precisions_init=ps)\n            elif model_type == None:\n                model = _model\n                \n            model.fit(X_trn, Y_trn)\n            oof[train_index[val_index]] += model.predict_proba(X_val)[:,1]\n            preds[test_index] += model.predict_proba(test_2)[:,1] / cv.n_splits\n            oof_org[train_org_index] += model.predict_proba(X_org)[:,1] / cv.n_splits\n    \n    # reverse result only GMM\n    if model_type == 'gmm':\n        oof = 1.0 - oof\n        preds = 1.0 - preds\n        oof_org = 1.0 - oof_org\n          \n    # AUC\n    auc = roc_auc_score(_train['target'], oof[:_train.shape[0]])\n    # AUC By Original Train Data\n    auc_org = roc_auc_score(train_org['target'], oof_org)\n    \n    # Plotting Histogram\n    if _plot:\n        fig = plt.figure(figsize=(6, 4))\n        sns.distplot(np.array(preds), kde=False, bins=25)\n        plt.title('Model [{}]  AUC: {:.5f}'.format(model_type, auc_org))\n        plt.show()\n    \n    return np.array(preds), oof[:train.shape[0]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train, test = load_data()\ntrain_org = train.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Psuedo Labeling"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def PsuedoLabeling(train, test, reg_param, _min=0.01, _max=0.99):\n    _train = train.copy()\n    _test = test.copy()\n    # First Model\n    qda_params = {\n        'reg_param': reg_param\n    }\n    cv = StratifiedKFold(n_splits=15)\n    preds, oof = trainer(_train, _test, cv, qda_params, model_type='qda', variance=True, _plot=False, _scaler=False)\n    \n    _test['target'] = preds\n    test_0 = _test[_test['target'] < _min]\n    test_0['target'] = 0\n    test_1 = _test[_test['target'] > _max]\n    test_1['target'] = 1\n    \n    train = pd.concat([train, test_0, test_1], axis=0).reset_index(drop=True)\n    \n    return train\n\n\ndef run_PsuedoLabeling(train, test, reg_param, _min=0.01, _max=0.99, _iter=20):\n    reg_param = random.uniform(0.1, 0.6)\n    for _ in range(_iter):\n        before_train_shape = train.shape[0]\n        train = PsuedoLabeling(train, test, reg_param, _min=_min, _max=_max)\n        train = train.drop_duplicates(keep='first').reset_index(drop=True)\n        print('Train Data Shape: {}'.format(train.shape))\n        if train.shape[0] ==before_train_shape:\n            break\n            \n    return train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def change_label(train, test, reg_param):\n    modified_train = train.copy()\n    _train = train.copy()\n    _test = test.copy()\n    \n    qda_params = {\n        'reg_param': reg_param\n    }\n    cv = StratifiedKFold(n_splits=15, random_state=46)\n    preds, oof = trainer(_train, _test, cv, qda_params, model_type='qda', variance=True, _plot=False, _scaler=False)\n    \n    df = pd.DataFrame({\n        'id': train['id'].values,\n        'target': train['target'].values,\n        'oof': oof\n    })\n    \n    # oof, targetと乖離がある範囲を抽出\n    df_0 = df[(df['target'] == 1) & (df['oof'] < 0.01)]\n    df_1 = df[(df['target'] == 0) & (df['oof'] > 0.99)]\n    \n    modified_train.loc[df_0.index, 'target'] = 0\n    modified_train.loc[df_1.index, 'target'] = 1\n    modified_rows = len(df_0) + len(df_1)\n    \n    print('Modified {} rows'.format(modified_rows))\n    \n    return modified_train, modified_rows\n\n\ndef run_change_label(train, test, reg_param, _iter=20):\n    reg_param = random.uniform(0.1, 0.6)\n    for i in range(_iter):\n        train, modified_rows = change_label(train, test, reg_param)\n        train.reset_index(drop=True, inplace=True)\n        if modified_rows < 10:\n            break\n    return train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"%%time\nprep_start = time.time()\n\n# PsuedoLabeling\ntrain = run_PsuedoLabeling(train, test, reg_param=0.5, _min=0.04, _max=0.96, _iter=10)\n\n# Change Effective Label\ntrain = run_change_label(train, test, reg_param=0.5, _iter=3)\n\ntrain.to_csv('Preprocessed_train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prep_end = time.time()\nelapsedtime = prep_end - prep_start\ns = datetime.timedelta(seconds=elapsedtime)\nprint('Preprocessing 1 epoch time: {}'.format(str(s)))\nprint('Final Train Data Shape: {}'.format(train.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# oof, pred Data Container\ntr = pd.DataFrame({\n    'target': train['target'].values\n})\n\nte = pd.DataFrame()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**QDA**"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nname = 'qda'\nqda_params = {\n    'reg_param':  0.01849,\n    'priors': 17.0978,\n    'tol': 6.943e-06\n}\n\ncv = StratifiedKFold(n_splits=15, random_state=46, shuffle=True)\n\npreds, oof = trainer(train, test, cv, qda_params, model_type=name.split('_')[0], \n                     variance=True, _scaler=True, threshold=2.99)\n\ntr[name] = oof\nte[name] = preds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**NuSVC**"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nname = 'nusvc'\nnusvc_params = {\n    'probability': True,\n    'kernel': 'poly',\n    'degree': 3,\n    'gamma': 'auto',\n    'coef0': 0.159,\n    'nu': 0.1486,\n    'tol': 1.8909e-4\n}\n\ncv = StratifiedKFold(n_splits=5, random_state=46)\n\npreds, oof = trainer(train, test, cv, nusvc_params, model_type=name.split('_')[0], SVD=20)\n\ntr[name] = oof\nte[name] = preds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**GMM**"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nname = 'gmm'\ngmm_params = {\n    'n_components': 2, \n    'init_params': 'kmeans', \n    'covariance_type': 'full', \n    'tol': 0.001,\n    'reg_covar': 0.000389422,\n    'max_iter': 536, \n    'n_init': 1\n}\n\ncv = StratifiedKFold(n_splits=5, random_state=46, shuffle=True)\n\npreds, oof = trainer(train, test, cv, gmm_params, model_type=name.split('_')[0], \n                     variance=True, _scaler=False, threshold=2.21)\n\ntr[name] = oof\nte[name] = preds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Stacking"},{"metadata":{"trusted":false},"cell_type":"code","source":"# create Rank\nfor col in te.columns:\n    tr_rank = tr[col].rank(ascending=True)\n    te_rank = te[col].rank(ascending=True)\n    \n    # ランクを正規化\n    tr['{}_rank'.format(col)] = tr_rank.apply(lambda x: (x - tr_rank.min()) / (tr_rank.max() - tr_rank.min()))\n    te['{}_rank'.format(col)] = te_rank.apply(lambda x: (x - te_rank.min()) / (te_rank.max() - te_rank.min()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Stacking_trainer(tr, te, cols, clf, NAME, _scaler=False):\n    _tr, _te = tr.copy(), te.copy()\n    \n    cv = StratifiedKFold(n_splits=5, random_state=46)\n    X = _tr[cols].values\n    Y = _tr['target'].values\n    oof = np.zeros(len(tr)) \n    preds = np.zeros(len(te))\n    _te = _te[cols]\n    \n    if _scaler:\n        scaler = StandardScaler()\n        X = scaler.fit_transform(X)\n        _te = scaler.transform(_te)\n\n    for trn_index, val_index in cv.split(X, Y):\n        X_trn, Y_trn = X[trn_index], Y[trn_index]\n        X_val, Y_val = X[val_index], Y[val_index]\n\n        clf.fit(X_trn, Y_trn)\n        oof[val_index] = clf.predict_proba(X_val)[:,1]\n        preds += clf.predict_proba(_te)[:,1] / cv.n_splits\n\n    # Contain oof, pred data\n    tr[NAME] = oof\n    te[NAME] = preds\n    \n    # Plotting Histogram\n    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\n    sns.distplot(tr[NAME], ax=axes[0])\n    sns.distplot(te[NAME], ax=axes[1])\n    axes[0].set_title('oof')\n    axes[1].set_title('pred')\n    plt.show()\n    \n    print(NAME)\n    print('AUC: {:.5f}'.format(roc_auc_score(tr['target'].values, oof)))\n    \n    return tr, te","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tr_stage0 = tr.copy()\ncol_not_rank = [col for col in tr_stage0.columns if col not in ['target'] and 'rank' not in col]\ncol_in_rank = [col for col in tr_stage0.columns if col not in ['target'] and 'rank' in col]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nstack_stage1_dict = {\n    'Stack_LR_1': LogisticRegression(penalty='l2', solver='liblinear', random_state=1),\n    'Stack_LR_2': LogisticRegression(penalty='l2', solver='liblinear', random_state=10),\n    'Stack_LR_3': LogisticRegression(penalty='l2', solver='liblinear', random_state=100),\n    'Stack_LR_4': LogisticRegression(penalty='l2', solver='liblinear', random_state=1000) \n}\n\nfor name, clf in stack_stage1_dict.items():\n    # Only use Rank\n    tr, te = Stacking_trainer(tr, te, col_in_rank, clf, name, _scaler=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tr['Stack_LR_Mean'] = (tr['Stack_LR_1'] + tr['Stack_LR_2'] + tr['Stack_LR_3'] + tr['Stack_LR_4']) / 4\nte['Stack_LR_Mean'] = (te['Stack_LR_1'] + te['Stack_LR_2'] + te['Stack_LR_3'] + te['Stack_LR_4']) / 4","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Create HeatMap of Correlation"},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(12, 8))\ncols = [col for col in tr.columns if col not in ['target'] and '_rank' not in col]\nsns.heatmap(te[cols].corr(), vmax=1.0, vmin=0.0, center=0.5, cmap='coolwarm', annot=True, fmt='.2f', square=True)\nplt.title('Correlation Table by Model Predictions')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Plotting"},{"metadata":{"trusted":false},"cell_type":"code","source":"def oof_distribution(tr, cols):\n    ncols = 3\n    nrows = int(np.ceil(len(cols) / 3))\n    figsize = (12, 4 * nrows)\n    fig, axes = plt.subplots(ncols=ncols, nrows=nrows, figsize=figsize)\n    \n    for col, ax in zip(cols, axes.ravel()):\n        \n        auc = roc_auc_score(tr['target'], tr[col])\n        \n        tr_0 = tr[tr['target'] == 0]\n        tr_1 = tr[tr['target'] == 1]\n        \n        tr_0 = tr_0[tr_0[col] > 0.0]\n        tr_0 = tr_0[tr_0[col] < 1.0]\n        tr_1 = tr_1[tr_1[col] > 0.0]\n        tr_1 = tr_1[tr_1[col] < 1.0]\n        \n        tr_0 = tr_0.sample(5000)\n        tr_1 = tr_1.sample(5000)\n        \n        sns.kdeplot(tr_0[col].values, ax=ax, shade=True, color=\"b\", label='0')\n        sns.kdeplot(tr_1[col].values, ax=ax, shade=True, color=\"r\", label='1')\n        ax.set_title('{}  AUC: {:.5f}'.format(col, auc))\n        \n    plt.tight_layout()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"cols = [col for col in tr.columns if col not in ['target'] and '_rank' not in col]\noof_distribution(tr, cols)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submit"},{"metadata":{"trusted":false},"cell_type":"code","source":"submit = pd.read_csv('../input/sample_submission.csv')\nsubmit['target'] = te['Stack_LR_Mean'].values\nsubmit.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"end = time.time()\nelapsedtime = end - start\ns = datetime.timedelta(seconds=elapsedtime)\n\nprint('This Kernel Runnnig Time: {}'.format(str(s)))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}