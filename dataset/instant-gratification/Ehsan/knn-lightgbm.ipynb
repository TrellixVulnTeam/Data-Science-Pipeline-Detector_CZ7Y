{"cells":[{"metadata":{},"cell_type":"markdown","source":"In this kernel, I'm presenting another approach to the problem, that I didn't see in the public kernels. (Maybe I have not noticed, sorry in that case).\n\nThe approach is to take the closest samples from 0 and 1 classes, for each querry sample, then extract some features from them, and train a classifier on top of those features. Here I have used LightGBM, and my features are normal statistical features calculated from the distance matrices."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np, pandas as pd, os\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.metrics import roc_auc_score as auc\nfrom functools import partial\n\ntrain_df = pd.read_csv('../input/train.csv')\ntest_df = pd.read_csv('../input/test.csv')\nMagicFeat = 'wheezy-copper-turtle-magic'\ncols = [c for c in train_df.columns if c not in ['id', 'target', MagicFeat]]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This part is mainly taken from Chris's kernel:\nhttps://www.kaggle.com/cdeotte/support-vector-machine-0-925\n\nThanks [@cdeotte](https://www.kaggle.com/cdeotte)"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"code","source":"def trainer(Model, train, test):\n    oof = np.zeros(len(train))\n    preds = np.zeros(len(test))\n    # BUILD 512 SEPARATE MODELS\n    for i in range(512):\n        # ONLY TRAIN WITH DATA WHERE WHEEZY EQUALS I\n        train2 = train[train[MagicFeat]==i]\n        test2 = test[test[MagicFeat]==i]\n        idx1 = train2.index; idx2 = test2.index\n        train2.reset_index(drop=True,inplace=True)\n\n        # FEATURE SELECTION (USE APPROX 40 OF 255 FEATURES)\n        sel = VarianceThreshold(threshold=1.5).fit(train2[cols])\n        train3 = sel.transform(train2[cols])\n        test3 = sel.transform(test2[cols])\n        \n        # STRATIFIED K-FOLD\n        skf = StratifiedKFold(n_splits=11, random_state=42, shuffle=True)\n        for train_index, test_index in skf.split(train3, train2['target']):\n            # Train MODEL AND PREDICT\n            clf = Model()\n            clf.fit(train3[train_index,:], train2.loc[train_index]['target'])\n            oof[idx1[test_index]] = clf.predict_proba(train3[test_index,:])[:,1]\n            preds[idx2] += clf.predict_proba(test3)[:,1] / skf.n_splits\n    return oof, preds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The wrapper class of the KNNs and the classifier:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import NearestNeighbors\nfrom lightgbm import LGBMClassifier\nfrom scipy.stats import skew, kurtosis, hmean, gmean\n\ndef getFeatures(dists):\n    '''\n    Generates features from the distance matrix of shape (n_samples by n_neighbors)\n    '''\n    Features = []\n    Funcs = [np.amin, np.amax, np.mean, np.std, np.median, hmean, gmean, kurtosis, skew]+\\\n                [partial(np.percentile, q=p) for p in [1,5,10,90,95,99]]\n    for func in Funcs:\n        Features.append(func(dists, axis=1).reshape(-1,1))\n    return np.concatenate(Features, axis=1)\n\n\nclass KNN_CLF():\n    '''\n    Binary KNN Classifier:\n    It takes the distances to the k nearest neighbors of a node from\n    samples from both 0, and 1 classes, extracts some features from\n    the distance matrices, and trains a classifier over it.\n    By default the classifier is LightGBM.\n    '''\n    __slots__ = ['k', 'knn0', 'knn1', 'clf']\n    def __init__(self, knn_params={}, k=5, CLF=LGBMClassifier, clf_params={}):\n        self.k = k\n        self.knn0 = NearestNeighbors(**knn_params)\n        self.knn1 = NearestNeighbors(**knn_params)\n        self.clf = CLF(**clf_params)\n        \n    def fit(self, X, y):\n        self.knn0.fit(X[y==0, :])\n        self.knn1.fit(X[y==1, :])\n        # during training the first neighbor is the sample itself, \n        # so we take k+1 neighbors and take the first out\n        F0 = getFeatures(\n            self.knn0.kneighbors(X,\n                                 n_neighbors=min(self.k+1, self.knn0._fit_X.shape[0]),\n                                 return_distance=True)[0][:,1:])\n        F1 = getFeatures(\n            self.knn1.kneighbors(X,\n                                 n_neighbors=min(self.k+1, self.knn1._fit_X.shape[0]),\n                                 return_distance=True)[0][:,1:])\n        XX = np.concatenate((F0, F1, F1-F0), axis=1)\n        self.clf.fit(XX, y, verbose=0)\n\n    def predict_proba(self, X):\n        F0 = getFeatures(\n            self.knn0.kneighbors(X,\n                                 n_neighbors=min(self.k, self.knn0._fit_X.shape[0]),\n                                 return_distance=True)[0])\n        F1 = getFeatures(\n            self.knn1.kneighbors(X,\n                                 n_neighbors=min(self.k, self.knn1._fit_X.shape[0]),\n                                 return_distance=True)[0])\n        XX = np.concatenate((F0, F1, F1-F0), axis=1)\n        return self.clf.predict_proba(XX)\n        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"training ..."},{"metadata":{"trusted":true},"cell_type":"code","source":"Model = partial(KNN_CLF, k=150)\ntrOut, tsOut = trainer(Model, train_df, test_df)\nprint(auc(train_df.target, trOut))\n\nimport matplotlib.pyplot as plt\nplt.hist(trOut,bins=100)\nplt.title('OOF predictions')\nplt.show()\n\nplt.figure()\nplt.hist(tsOut,bins=100)\nplt.title('Test.csv predictions')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv('../input/sample_submission.csv')\nsub['target'] = tsOut\nsub.to_csv('submission.csv',index=False)\n\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}