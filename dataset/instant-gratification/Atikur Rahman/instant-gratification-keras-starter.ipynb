{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Imports"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom keras import layers, Input, Model\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\n%matplotlib inline","execution_count":26,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Loading data"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"data_dir = '../input/'\n!ls {data_dir}","execution_count":27,"outputs":[{"output_type":"stream","text":"sample_submission.csv  test.csv  train.csv\r\n","name":"stdout"}]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train_raw = pd.read_csv(f'{data_dir}train.csv')\ntrain_raw.head()","execution_count":28,"outputs":[{"output_type":"execute_result","execution_count":28,"data":{"text/plain":"                                 id   ...    target\n0  707b395ecdcbb4dc2eabea00e4d1b179   ...         0\n1  5880c03c6582a7b42248668e56b4bdec   ...         0\n2  4ccbcb3d13e5072ff1d9c61afe2c4f77   ...         1\n3  e350f17a357f12a1941f0837afb7eb8d   ...         0\n4  a8f910ea6075b6376af079055965ff68   ...         0\n\n[5 rows x 258 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>muggy-smalt-axolotl-pembus</th>\n      <th>dorky-peach-sheepdog-ordinal</th>\n      <th>slimy-seashell-cassowary-goose</th>\n      <th>snazzy-harlequin-chicken-distraction</th>\n      <th>frumpy-smalt-mau-ordinal</th>\n      <th>stealthy-beige-pinscher-golden</th>\n      <th>chummy-cream-tarantula-entropy</th>\n      <th>hazy-emerald-cuttlefish-unsorted</th>\n      <th>nerdy-indigo-wolfhound-sorted</th>\n      <th>leaky-amaranth-lizard-sorted</th>\n      <th>ugly-tangerine-chihuahua-important</th>\n      <th>shaggy-silver-indri-fimbus</th>\n      <th>flaky-chocolate-beetle-grandmaster</th>\n      <th>squirrely-harlequin-sheep-sumble</th>\n      <th>freaky-tan-angelfish-noise</th>\n      <th>lousy-plum-penguin-sumble</th>\n      <th>bluesy-rose-wallaby-discard</th>\n      <th>baggy-copper-oriole-dummy</th>\n      <th>stealthy-scarlet-hound-fepid</th>\n      <th>greasy-cinnamon-bonobo-contributor</th>\n      <th>cranky-cardinal-dogfish-ordinal</th>\n      <th>snippy-auburn-vole-learn</th>\n      <th>greasy-sepia-coral-dataset</th>\n      <th>flabby-tangerine-fowl-entropy</th>\n      <th>lousy-smalt-pinscher-dummy</th>\n      <th>bluesy-brass-chihuahua-distraction</th>\n      <th>goopy-eggplant-indri-entropy</th>\n      <th>homey-sepia-bombay-sorted</th>\n      <th>homely-ruby-bulldog-entropy</th>\n      <th>hasty-blue-sheep-contributor</th>\n      <th>blurry-wisteria-oyster-master</th>\n      <th>snoopy-auburn-dogfish-expert</th>\n      <th>stinky-maroon-blue-kernel</th>\n      <th>bumpy-amaranth-armadillo-important</th>\n      <th>slaphappy-peach-oyster-master</th>\n      <th>dorky-tomato-ragdoll-dataset</th>\n      <th>messy-mauve-wolverine-ordinal</th>\n      <th>geeky-pumpkin-moorhen-important</th>\n      <th>crabby-teal-otter-unsorted</th>\n      <th>...</th>\n      <th>beady-mauve-frog-distraction</th>\n      <th>surly-brass-maltese-ordinal</th>\n      <th>beady-asparagus-opossum-expert</th>\n      <th>beady-rust-impala-dummy</th>\n      <th>droopy-amethyst-dachshund-hint</th>\n      <th>homey-crimson-budgerigar-grandmaster</th>\n      <th>droopy-cardinal-impala-important</th>\n      <th>woozy-apricot-moose-hint</th>\n      <th>paltry-sapphire-labradoodle-dummy</th>\n      <th>crappy-carmine-eagle-entropy</th>\n      <th>greasy-magnolia-spider-grandmaster</th>\n      <th>crabby-carmine-flounder-sorted</th>\n      <th>skimpy-copper-fowl-grandmaster</th>\n      <th>hasty-seashell-woodpecker-hint</th>\n      <th>snappy-purple-bobcat-important</th>\n      <th>thirsty-carmine-corgi-ordinal</th>\n      <th>homely-auburn-reindeer-unsorted</th>\n      <th>crappy-beige-tiger-fepid</th>\n      <th>cranky-auburn-swan-novice</th>\n      <th>chewy-bistre-buzzard-expert</th>\n      <th>skinny-cyan-macaque-pembus</th>\n      <th>slimy-periwinkle-otter-expert</th>\n      <th>snazzy-burgundy-clam-novice</th>\n      <th>cozy-ochre-gorilla-gaussian</th>\n      <th>homey-sangria-wolfhound-dummy</th>\n      <th>snazzy-asparagus-hippopotamus-contributor</th>\n      <th>paltry-red-hamster-sorted</th>\n      <th>zippy-dandelion-insect-golden</th>\n      <th>baggy-coral-bandicoot-unsorted</th>\n      <th>goopy-lavender-wolverine-fimbus</th>\n      <th>wheezy-myrtle-mandrill-entropy</th>\n      <th>wiggy-lilac-lemming-sorted</th>\n      <th>gloppy-cerise-snail-contributor</th>\n      <th>woozy-silver-havanese-gaussian</th>\n      <th>jumpy-thistle-discus-sorted</th>\n      <th>muggy-turquoise-donkey-important</th>\n      <th>blurry-buff-hyena-entropy</th>\n      <th>bluesy-chocolate-kudu-fepid</th>\n      <th>gamy-white-monster-expert</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>707b395ecdcbb4dc2eabea00e4d1b179</td>\n      <td>-2.070654</td>\n      <td>1.018160</td>\n      <td>0.228643</td>\n      <td>0.857221</td>\n      <td>0.052271</td>\n      <td>0.230303</td>\n      <td>-6.385090</td>\n      <td>0.439369</td>\n      <td>-0.721946</td>\n      <td>-0.227027</td>\n      <td>0.575964</td>\n      <td>1.541908</td>\n      <td>1.745286</td>\n      <td>-0.624271</td>\n      <td>3.600958</td>\n      <td>1.176489</td>\n      <td>-0.182776</td>\n      <td>-0.228391</td>\n      <td>1.682263</td>\n      <td>-0.833236</td>\n      <td>-4.377688</td>\n      <td>-5.372410</td>\n      <td>-0.477742</td>\n      <td>-0.179005</td>\n      <td>-0.516475</td>\n      <td>0.127391</td>\n      <td>-0.857591</td>\n      <td>-0.461500</td>\n      <td>2.160303</td>\n      <td>-2.118371</td>\n      <td>0.515493</td>\n      <td>-1.201493</td>\n      <td>-0.027377</td>\n      <td>-1.154024</td>\n      <td>0.753204</td>\n      <td>-0.179651</td>\n      <td>-0.807341</td>\n      <td>-1.663626</td>\n      <td>0.893806</td>\n      <td>...</td>\n      <td>-1.829848</td>\n      <td>2.347131</td>\n      <td>0.082462</td>\n      <td>-1.012654</td>\n      <td>0.593752</td>\n      <td>2.904654</td>\n      <td>-0.428974</td>\n      <td>-0.919979</td>\n      <td>2.849575</td>\n      <td>-0.906744</td>\n      <td>0.729459</td>\n      <td>0.386140</td>\n      <td>0.319814</td>\n      <td>-0.407682</td>\n      <td>-0.170667</td>\n      <td>-1.242919</td>\n      <td>-1.719046</td>\n      <td>-0.132395</td>\n      <td>-0.368991</td>\n      <td>-5.112553</td>\n      <td>-2.085988</td>\n      <td>-0.897257</td>\n      <td>1.080671</td>\n      <td>-0.273262</td>\n      <td>0.342824</td>\n      <td>0.640177</td>\n      <td>-0.415298</td>\n      <td>-0.483126</td>\n      <td>-0.080799</td>\n      <td>2.416224</td>\n      <td>0.351895</td>\n      <td>0.618824</td>\n      <td>-1.542423</td>\n      <td>0.598175</td>\n      <td>0.611757</td>\n      <td>0.678772</td>\n      <td>0.247059</td>\n      <td>-0.806677</td>\n      <td>-0.193649</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5880c03c6582a7b42248668e56b4bdec</td>\n      <td>-0.491702</td>\n      <td>0.082645</td>\n      <td>-0.011193</td>\n      <td>1.071266</td>\n      <td>-0.346347</td>\n      <td>-0.082209</td>\n      <td>0.110579</td>\n      <td>-0.382374</td>\n      <td>-0.229620</td>\n      <td>0.783980</td>\n      <td>-1.280579</td>\n      <td>-1.003480</td>\n      <td>-7.753201</td>\n      <td>-1.320547</td>\n      <td>0.919078</td>\n      <td>-1.036068</td>\n      <td>0.030213</td>\n      <td>0.910172</td>\n      <td>-0.905345</td>\n      <td>0.646641</td>\n      <td>-0.465291</td>\n      <td>-0.531735</td>\n      <td>-0.756781</td>\n      <td>0.193724</td>\n      <td>0.224277</td>\n      <td>-0.474412</td>\n      <td>1.885805</td>\n      <td>0.205439</td>\n      <td>-6.481422</td>\n      <td>1.035620</td>\n      <td>-0.453623</td>\n      <td>0.375936</td>\n      <td>-0.320670</td>\n      <td>-0.144646</td>\n      <td>-0.220129</td>\n      <td>0.577826</td>\n      <td>-0.360512</td>\n      <td>-0.600107</td>\n      <td>0.008111</td>\n      <td>...</td>\n      <td>0.982205</td>\n      <td>-1.161978</td>\n      <td>0.532269</td>\n      <td>1.133215</td>\n      <td>0.003503</td>\n      <td>-1.390962</td>\n      <td>0.158572</td>\n      <td>0.143794</td>\n      <td>-0.317185</td>\n      <td>1.017192</td>\n      <td>-0.395342</td>\n      <td>-0.642357</td>\n      <td>-0.627209</td>\n      <td>0.257271</td>\n      <td>-1.461564</td>\n      <td>0.325613</td>\n      <td>1.628369</td>\n      <td>0.640040</td>\n      <td>0.750735</td>\n      <td>1.164573</td>\n      <td>0.900373</td>\n      <td>0.063489</td>\n      <td>0.948158</td>\n      <td>0.273014</td>\n      <td>-1.269147</td>\n      <td>-0.251101</td>\n      <td>-2.271731</td>\n      <td>-0.044167</td>\n      <td>-0.443766</td>\n      <td>-1.144794</td>\n      <td>-0.645115</td>\n      <td>-1.246090</td>\n      <td>2.613357</td>\n      <td>-0.479664</td>\n      <td>1.581289</td>\n      <td>0.931258</td>\n      <td>0.151937</td>\n      <td>-0.766595</td>\n      <td>0.474351</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4ccbcb3d13e5072ff1d9c61afe2c4f77</td>\n      <td>-1.680473</td>\n      <td>0.860529</td>\n      <td>-1.076195</td>\n      <td>0.740124</td>\n      <td>3.678445</td>\n      <td>0.288558</td>\n      <td>0.515875</td>\n      <td>0.920590</td>\n      <td>-1.223277</td>\n      <td>-1.029780</td>\n      <td>-2.203397</td>\n      <td>-7.088717</td>\n      <td>0.438218</td>\n      <td>-0.848173</td>\n      <td>1.542666</td>\n      <td>-2.166858</td>\n      <td>-0.867670</td>\n      <td>-0.980947</td>\n      <td>0.567793</td>\n      <td>1.323430</td>\n      <td>-2.076700</td>\n      <td>-0.291598</td>\n      <td>-1.564816</td>\n      <td>-8.718695</td>\n      <td>0.340144</td>\n      <td>-0.566402</td>\n      <td>0.844324</td>\n      <td>0.816421</td>\n      <td>-1.019114</td>\n      <td>-0.881431</td>\n      <td>-2.285710</td>\n      <td>-0.090958</td>\n      <td>-0.898440</td>\n      <td>-0.584417</td>\n      <td>-0.143660</td>\n      <td>-0.182084</td>\n      <td>0.798516</td>\n      <td>0.010756</td>\n      <td>-0.347155</td>\n      <td>...</td>\n      <td>0.829467</td>\n      <td>0.588236</td>\n      <td>0.427946</td>\n      <td>-0.563037</td>\n      <td>-0.103990</td>\n      <td>-0.817698</td>\n      <td>1.251046</td>\n      <td>-0.977157</td>\n      <td>2.732600</td>\n      <td>1.997984</td>\n      <td>-0.214285</td>\n      <td>-0.389428</td>\n      <td>-1.007633</td>\n      <td>0.336435</td>\n      <td>-0.851292</td>\n      <td>-0.024184</td>\n      <td>0.455908</td>\n      <td>0.458753</td>\n      <td>-0.267230</td>\n      <td>-2.032402</td>\n      <td>0.203082</td>\n      <td>0.654107</td>\n      <td>-3.512338</td>\n      <td>-0.840937</td>\n      <td>0.519407</td>\n      <td>-0.028053</td>\n      <td>-1.621083</td>\n      <td>0.142132</td>\n      <td>1.514664</td>\n      <td>0.828815</td>\n      <td>0.516422</td>\n      <td>0.130521</td>\n      <td>-0.459210</td>\n      <td>2.028205</td>\n      <td>-0.093968</td>\n      <td>-0.218274</td>\n      <td>-0.163136</td>\n      <td>-0.870289</td>\n      <td>0.064038</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>e350f17a357f12a1941f0837afb7eb8d</td>\n      <td>0.183774</td>\n      <td>0.919134</td>\n      <td>-0.946958</td>\n      <td>0.918492</td>\n      <td>0.862278</td>\n      <td>1.155287</td>\n      <td>0.911106</td>\n      <td>0.562598</td>\n      <td>-1.349685</td>\n      <td>-1.182729</td>\n      <td>0.003159</td>\n      <td>-0.626847</td>\n      <td>0.368980</td>\n      <td>1.560784</td>\n      <td>0.502851</td>\n      <td>-0.108050</td>\n      <td>0.633208</td>\n      <td>-0.411502</td>\n      <td>-3.201592</td>\n      <td>-0.710612</td>\n      <td>0.786816</td>\n      <td>0.500979</td>\n      <td>-1.040048</td>\n      <td>-1.369170</td>\n      <td>0.987666</td>\n      <td>-0.681838</td>\n      <td>-0.331372</td>\n      <td>2.254289</td>\n      <td>-0.009330</td>\n      <td>2.007067</td>\n      <td>1.203750</td>\n      <td>-2.003928</td>\n      <td>-0.566088</td>\n      <td>0.223452</td>\n      <td>0.434202</td>\n      <td>-1.203766</td>\n      <td>-0.103490</td>\n      <td>0.441111</td>\n      <td>1.818458</td>\n      <td>...</td>\n      <td>-2.231836</td>\n      <td>0.833236</td>\n      <td>-0.454226</td>\n      <td>-1.614694</td>\n      <td>0.159948</td>\n      <td>-0.150059</td>\n      <td>-1.570599</td>\n      <td>0.960839</td>\n      <td>0.102214</td>\n      <td>0.077236</td>\n      <td>0.852834</td>\n      <td>-1.265608</td>\n      <td>-3.219190</td>\n      <td>0.251194</td>\n      <td>0.215861</td>\n      <td>-0.009520</td>\n      <td>1.611203</td>\n      <td>1.679806</td>\n      <td>-0.008419</td>\n      <td>0.658384</td>\n      <td>-0.132437</td>\n      <td>-1.466823</td>\n      <td>-1.577080</td>\n      <td>-0.800346</td>\n      <td>1.960795</td>\n      <td>-4.042900</td>\n      <td>1.722143</td>\n      <td>-0.261888</td>\n      <td>-1.145005</td>\n      <td>-1.864582</td>\n      <td>-1.168967</td>\n      <td>1.385089</td>\n      <td>-0.353028</td>\n      <td>3.316150</td>\n      <td>-0.524087</td>\n      <td>-0.794327</td>\n      <td>3.936365</td>\n      <td>0.682989</td>\n      <td>-2.521211</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>a8f910ea6075b6376af079055965ff68</td>\n      <td>-0.203933</td>\n      <td>-0.177252</td>\n      <td>0.368074</td>\n      <td>-0.701320</td>\n      <td>-1.104391</td>\n      <td>0.735760</td>\n      <td>0.894273</td>\n      <td>-1.375826</td>\n      <td>-5.144946</td>\n      <td>-2.048711</td>\n      <td>0.629773</td>\n      <td>-4.252669</td>\n      <td>-0.087420</td>\n      <td>-0.794367</td>\n      <td>-1.063963</td>\n      <td>0.115997</td>\n      <td>0.895180</td>\n      <td>3.184848</td>\n      <td>2.057840</td>\n      <td>-0.950821</td>\n      <td>0.961059</td>\n      <td>-1.837828</td>\n      <td>-0.437156</td>\n      <td>-0.828433</td>\n      <td>0.373747</td>\n      <td>-0.099787</td>\n      <td>-0.976280</td>\n      <td>-0.165921</td>\n      <td>3.297221</td>\n      <td>3.914132</td>\n      <td>-4.971376</td>\n      <td>-0.286520</td>\n      <td>-0.160133</td>\n      <td>-3.301453</td>\n      <td>-1.021032</td>\n      <td>-0.562744</td>\n      <td>0.574065</td>\n      <td>-0.368194</td>\n      <td>-0.507458</td>\n      <td>...</td>\n      <td>0.178099</td>\n      <td>-0.410396</td>\n      <td>-1.184236</td>\n      <td>1.681727</td>\n      <td>0.589606</td>\n      <td>0.064222</td>\n      <td>0.258885</td>\n      <td>0.560241</td>\n      <td>-1.545597</td>\n      <td>0.822283</td>\n      <td>1.518209</td>\n      <td>0.460143</td>\n      <td>0.822488</td>\n      <td>1.362718</td>\n      <td>0.218560</td>\n      <td>-1.038514</td>\n      <td>1.000763</td>\n      <td>-0.975878</td>\n      <td>-0.551268</td>\n      <td>-0.133044</td>\n      <td>-0.393092</td>\n      <td>1.236473</td>\n      <td>1.657100</td>\n      <td>0.833020</td>\n      <td>0.665379</td>\n      <td>-0.900025</td>\n      <td>0.291908</td>\n      <td>0.482727</td>\n      <td>0.552399</td>\n      <td>0.970496</td>\n      <td>-0.279168</td>\n      <td>1.544356</td>\n      <td>2.959727</td>\n      <td>1.641201</td>\n      <td>-0.130818</td>\n      <td>-0.264292</td>\n      <td>-0.748668</td>\n      <td>0.964218</td>\n      <td>0.087079</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_raw = pd.read_csv(f'{data_dir}test.csv')\ntest_raw.head()","execution_count":29,"outputs":[{"output_type":"execute_result","execution_count":29,"data":{"text/plain":"                                 id            ...              gamy-white-monster-expert\n0  1c13f2701648e0b0d46d8a2a5a131a53            ...                              -0.237948\n1  ba88c155ba898fc8b5099893036ef205            ...                              -1.666513\n2  7cbab5cea99169139e7e6d8ff74ebb77            ...                               1.099160\n3  ca820ad57809f62eb7b4d13f5d4371a0            ...                               0.102878\n4  7baaf361537fbd8a1aaa2c97a6d4ccc7            ...                               0.842452\n\n[5 rows x 257 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>muggy-smalt-axolotl-pembus</th>\n      <th>dorky-peach-sheepdog-ordinal</th>\n      <th>slimy-seashell-cassowary-goose</th>\n      <th>snazzy-harlequin-chicken-distraction</th>\n      <th>frumpy-smalt-mau-ordinal</th>\n      <th>stealthy-beige-pinscher-golden</th>\n      <th>chummy-cream-tarantula-entropy</th>\n      <th>hazy-emerald-cuttlefish-unsorted</th>\n      <th>nerdy-indigo-wolfhound-sorted</th>\n      <th>leaky-amaranth-lizard-sorted</th>\n      <th>ugly-tangerine-chihuahua-important</th>\n      <th>shaggy-silver-indri-fimbus</th>\n      <th>flaky-chocolate-beetle-grandmaster</th>\n      <th>squirrely-harlequin-sheep-sumble</th>\n      <th>freaky-tan-angelfish-noise</th>\n      <th>lousy-plum-penguin-sumble</th>\n      <th>bluesy-rose-wallaby-discard</th>\n      <th>baggy-copper-oriole-dummy</th>\n      <th>stealthy-scarlet-hound-fepid</th>\n      <th>greasy-cinnamon-bonobo-contributor</th>\n      <th>cranky-cardinal-dogfish-ordinal</th>\n      <th>snippy-auburn-vole-learn</th>\n      <th>greasy-sepia-coral-dataset</th>\n      <th>flabby-tangerine-fowl-entropy</th>\n      <th>lousy-smalt-pinscher-dummy</th>\n      <th>bluesy-brass-chihuahua-distraction</th>\n      <th>goopy-eggplant-indri-entropy</th>\n      <th>homey-sepia-bombay-sorted</th>\n      <th>homely-ruby-bulldog-entropy</th>\n      <th>hasty-blue-sheep-contributor</th>\n      <th>blurry-wisteria-oyster-master</th>\n      <th>snoopy-auburn-dogfish-expert</th>\n      <th>stinky-maroon-blue-kernel</th>\n      <th>bumpy-amaranth-armadillo-important</th>\n      <th>slaphappy-peach-oyster-master</th>\n      <th>dorky-tomato-ragdoll-dataset</th>\n      <th>messy-mauve-wolverine-ordinal</th>\n      <th>geeky-pumpkin-moorhen-important</th>\n      <th>crabby-teal-otter-unsorted</th>\n      <th>...</th>\n      <th>dorky-purple-kiwi-hint</th>\n      <th>beady-mauve-frog-distraction</th>\n      <th>surly-brass-maltese-ordinal</th>\n      <th>beady-asparagus-opossum-expert</th>\n      <th>beady-rust-impala-dummy</th>\n      <th>droopy-amethyst-dachshund-hint</th>\n      <th>homey-crimson-budgerigar-grandmaster</th>\n      <th>droopy-cardinal-impala-important</th>\n      <th>woozy-apricot-moose-hint</th>\n      <th>paltry-sapphire-labradoodle-dummy</th>\n      <th>crappy-carmine-eagle-entropy</th>\n      <th>greasy-magnolia-spider-grandmaster</th>\n      <th>crabby-carmine-flounder-sorted</th>\n      <th>skimpy-copper-fowl-grandmaster</th>\n      <th>hasty-seashell-woodpecker-hint</th>\n      <th>snappy-purple-bobcat-important</th>\n      <th>thirsty-carmine-corgi-ordinal</th>\n      <th>homely-auburn-reindeer-unsorted</th>\n      <th>crappy-beige-tiger-fepid</th>\n      <th>cranky-auburn-swan-novice</th>\n      <th>chewy-bistre-buzzard-expert</th>\n      <th>skinny-cyan-macaque-pembus</th>\n      <th>slimy-periwinkle-otter-expert</th>\n      <th>snazzy-burgundy-clam-novice</th>\n      <th>cozy-ochre-gorilla-gaussian</th>\n      <th>homey-sangria-wolfhound-dummy</th>\n      <th>snazzy-asparagus-hippopotamus-contributor</th>\n      <th>paltry-red-hamster-sorted</th>\n      <th>zippy-dandelion-insect-golden</th>\n      <th>baggy-coral-bandicoot-unsorted</th>\n      <th>goopy-lavender-wolverine-fimbus</th>\n      <th>wheezy-myrtle-mandrill-entropy</th>\n      <th>wiggy-lilac-lemming-sorted</th>\n      <th>gloppy-cerise-snail-contributor</th>\n      <th>woozy-silver-havanese-gaussian</th>\n      <th>jumpy-thistle-discus-sorted</th>\n      <th>muggy-turquoise-donkey-important</th>\n      <th>blurry-buff-hyena-entropy</th>\n      <th>bluesy-chocolate-kudu-fepid</th>\n      <th>gamy-white-monster-expert</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1c13f2701648e0b0d46d8a2a5a131a53</td>\n      <td>-5.490030</td>\n      <td>0.593976</td>\n      <td>1.532732</td>\n      <td>-0.361234</td>\n      <td>-0.155282</td>\n      <td>-1.340582</td>\n      <td>1.626819</td>\n      <td>4.280084</td>\n      <td>-0.340155</td>\n      <td>-0.653070</td>\n      <td>2.817394</td>\n      <td>1.211543</td>\n      <td>-0.819354</td>\n      <td>-0.019935</td>\n      <td>0.634466</td>\n      <td>-1.604789</td>\n      <td>0.756771</td>\n      <td>0.451960</td>\n      <td>0.309252</td>\n      <td>-4.628353</td>\n      <td>-0.270163</td>\n      <td>-0.183827</td>\n      <td>0.099529</td>\n      <td>0.719685</td>\n      <td>-1.077965</td>\n      <td>-1.704726</td>\n      <td>0.263344</td>\n      <td>1.183083</td>\n      <td>-1.967262</td>\n      <td>-0.364729</td>\n      <td>-1.553005</td>\n      <td>0.667643</td>\n      <td>-1.647120</td>\n      <td>-0.118086</td>\n      <td>1.095897</td>\n      <td>-3.725086</td>\n      <td>0.571565</td>\n      <td>5.407131</td>\n      <td>1.599479</td>\n      <td>...</td>\n      <td>-0.007170</td>\n      <td>-0.723353</td>\n      <td>-0.317083</td>\n      <td>3.892687</td>\n      <td>0.732599</td>\n      <td>-0.829353</td>\n      <td>-1.281645</td>\n      <td>5.256029</td>\n      <td>0.484761</td>\n      <td>0.315440</td>\n      <td>-1.256456</td>\n      <td>-0.065537</td>\n      <td>0.243787</td>\n      <td>-0.085292</td>\n      <td>0.293226</td>\n      <td>-0.727176</td>\n      <td>-0.026991</td>\n      <td>-0.452266</td>\n      <td>1.733475</td>\n      <td>-0.511201</td>\n      <td>-0.913642</td>\n      <td>-2.237558</td>\n      <td>-1.607618</td>\n      <td>-0.497308</td>\n      <td>0.004124</td>\n      <td>-1.854472</td>\n      <td>1.499102</td>\n      <td>-0.701304</td>\n      <td>-0.573827</td>\n      <td>0.121455</td>\n      <td>0.386831</td>\n      <td>0.817764</td>\n      <td>-1.215121</td>\n      <td>0.238812</td>\n      <td>-0.507346</td>\n      <td>1.360599</td>\n      <td>-1.743894</td>\n      <td>1.412043</td>\n      <td>-0.562730</td>\n      <td>-0.237948</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ba88c155ba898fc8b5099893036ef205</td>\n      <td>0.571698</td>\n      <td>0.495494</td>\n      <td>1.133975</td>\n      <td>-0.315875</td>\n      <td>-2.142458</td>\n      <td>-0.478650</td>\n      <td>-1.458941</td>\n      <td>0.977352</td>\n      <td>0.159189</td>\n      <td>0.425304</td>\n      <td>1.366214</td>\n      <td>-1.097810</td>\n      <td>-0.128929</td>\n      <td>-0.563375</td>\n      <td>-0.389307</td>\n      <td>-0.167991</td>\n      <td>0.314843</td>\n      <td>0.530220</td>\n      <td>-0.512497</td>\n      <td>0.526404</td>\n      <td>1.612939</td>\n      <td>-0.800211</td>\n      <td>0.692056</td>\n      <td>2.541566</td>\n      <td>0.290910</td>\n      <td>-0.726153</td>\n      <td>1.307040</td>\n      <td>0.964062</td>\n      <td>0.235339</td>\n      <td>3.553998</td>\n      <td>-0.769491</td>\n      <td>-1.706831</td>\n      <td>0.227793</td>\n      <td>-1.027914</td>\n      <td>1.359388</td>\n      <td>-1.296281</td>\n      <td>0.216550</td>\n      <td>0.166792</td>\n      <td>-1.361746</td>\n      <td>...</td>\n      <td>8.867017</td>\n      <td>0.145251</td>\n      <td>-2.429107</td>\n      <td>-1.994984</td>\n      <td>0.692641</td>\n      <td>-0.922336</td>\n      <td>-0.442788</td>\n      <td>-0.357402</td>\n      <td>0.606270</td>\n      <td>-0.144259</td>\n      <td>-0.019312</td>\n      <td>0.132073</td>\n      <td>-1.038829</td>\n      <td>-0.361597</td>\n      <td>-0.019181</td>\n      <td>0.656320</td>\n      <td>-0.029080</td>\n      <td>-0.263428</td>\n      <td>-1.674349</td>\n      <td>0.237658</td>\n      <td>-0.539413</td>\n      <td>3.683862</td>\n      <td>0.341788</td>\n      <td>0.321800</td>\n      <td>1.919056</td>\n      <td>-0.680074</td>\n      <td>-0.851474</td>\n      <td>-0.802017</td>\n      <td>1.984299</td>\n      <td>0.606214</td>\n      <td>-1.120400</td>\n      <td>-0.953432</td>\n      <td>0.266619</td>\n      <td>-2.088512</td>\n      <td>0.252637</td>\n      <td>-1.034676</td>\n      <td>-0.804765</td>\n      <td>0.639112</td>\n      <td>1.172667</td>\n      <td>-1.666513</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7cbab5cea99169139e7e6d8ff74ebb77</td>\n      <td>0.478202</td>\n      <td>-1.429985</td>\n      <td>0.410429</td>\n      <td>-1.329215</td>\n      <td>-0.888456</td>\n      <td>0.721798</td>\n      <td>-0.809630</td>\n      <td>-0.216637</td>\n      <td>4.038961</td>\n      <td>1.605018</td>\n      <td>0.904172</td>\n      <td>0.087588</td>\n      <td>-0.357494</td>\n      <td>-0.009733</td>\n      <td>-0.972136</td>\n      <td>0.554583</td>\n      <td>-1.034569</td>\n      <td>1.599066</td>\n      <td>-4.408794</td>\n      <td>-1.228320</td>\n      <td>-0.025644</td>\n      <td>0.955930</td>\n      <td>0.449956</td>\n      <td>0.095414</td>\n      <td>-0.678564</td>\n      <td>-1.813493</td>\n      <td>0.490975</td>\n      <td>0.926852</td>\n      <td>-0.267794</td>\n      <td>0.356474</td>\n      <td>2.663074</td>\n      <td>-0.563031</td>\n      <td>-0.137845</td>\n      <td>-0.115935</td>\n      <td>-0.886347</td>\n      <td>0.766096</td>\n      <td>-0.984472</td>\n      <td>-1.104296</td>\n      <td>-0.976197</td>\n      <td>...</td>\n      <td>2.895123</td>\n      <td>0.293281</td>\n      <td>1.273641</td>\n      <td>10.777882</td>\n      <td>-0.099083</td>\n      <td>0.011016</td>\n      <td>-0.962202</td>\n      <td>0.066103</td>\n      <td>-0.871683</td>\n      <td>-0.889083</td>\n      <td>2.844812</td>\n      <td>1.574031</td>\n      <td>4.696900</td>\n      <td>7.411444</td>\n      <td>-0.184039</td>\n      <td>-0.019929</td>\n      <td>0.271968</td>\n      <td>0.465649</td>\n      <td>-1.083295</td>\n      <td>1.727432</td>\n      <td>-0.120461</td>\n      <td>0.066681</td>\n      <td>0.361057</td>\n      <td>-1.097419</td>\n      <td>-0.324316</td>\n      <td>0.222555</td>\n      <td>-0.261927</td>\n      <td>1.259449</td>\n      <td>-3.427776</td>\n      <td>1.147655</td>\n      <td>1.330744</td>\n      <td>-0.757244</td>\n      <td>1.289617</td>\n      <td>1.824534</td>\n      <td>-0.306249</td>\n      <td>-0.875231</td>\n      <td>-0.436228</td>\n      <td>0.007315</td>\n      <td>-0.841215</td>\n      <td>1.099160</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ca820ad57809f62eb7b4d13f5d4371a0</td>\n      <td>0.124178</td>\n      <td>-0.347069</td>\n      <td>-0.457690</td>\n      <td>1.248395</td>\n      <td>0.368441</td>\n      <td>0.832838</td>\n      <td>-2.050084</td>\n      <td>3.168626</td>\n      <td>-0.399716</td>\n      <td>0.063732</td>\n      <td>-0.469298</td>\n      <td>-0.036313</td>\n      <td>-0.410907</td>\n      <td>0.628839</td>\n      <td>-1.735393</td>\n      <td>0.086511</td>\n      <td>0.706585</td>\n      <td>-1.208220</td>\n      <td>0.501175</td>\n      <td>0.045957</td>\n      <td>0.977490</td>\n      <td>1.420726</td>\n      <td>-0.364428</td>\n      <td>-0.249452</td>\n      <td>0.026976</td>\n      <td>-1.945365</td>\n      <td>0.831696</td>\n      <td>0.246105</td>\n      <td>1.355068</td>\n      <td>0.377014</td>\n      <td>-1.904320</td>\n      <td>-0.897405</td>\n      <td>0.372223</td>\n      <td>-2.139312</td>\n      <td>1.697729</td>\n      <td>-4.581853</td>\n      <td>0.236018</td>\n      <td>-2.752859</td>\n      <td>0.792100</td>\n      <td>...</td>\n      <td>1.855223</td>\n      <td>0.262627</td>\n      <td>-1.999935</td>\n      <td>-0.388276</td>\n      <td>4.624725</td>\n      <td>-2.172119</td>\n      <td>-6.802108</td>\n      <td>-0.994671</td>\n      <td>-0.604755</td>\n      <td>-0.166381</td>\n      <td>0.934626</td>\n      <td>-1.036947</td>\n      <td>6.623598</td>\n      <td>-0.576301</td>\n      <td>0.668864</td>\n      <td>0.695632</td>\n      <td>0.149364</td>\n      <td>-0.216632</td>\n      <td>-2.041128</td>\n      <td>0.040286</td>\n      <td>-1.166278</td>\n      <td>1.134429</td>\n      <td>-0.393061</td>\n      <td>-0.258318</td>\n      <td>-1.499241</td>\n      <td>-0.352147</td>\n      <td>1.924381</td>\n      <td>0.424894</td>\n      <td>-1.180044</td>\n      <td>7.875893</td>\n      <td>-0.839954</td>\n      <td>-2.029489</td>\n      <td>3.218734</td>\n      <td>1.091271</td>\n      <td>1.384581</td>\n      <td>-0.313402</td>\n      <td>-0.816372</td>\n      <td>1.079761</td>\n      <td>-0.168144</td>\n      <td>0.102878</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7baaf361537fbd8a1aaa2c97a6d4ccc7</td>\n      <td>-1.806586</td>\n      <td>0.660947</td>\n      <td>-0.004663</td>\n      <td>-3.383956</td>\n      <td>1.018235</td>\n      <td>1.670834</td>\n      <td>-0.208904</td>\n      <td>0.124925</td>\n      <td>0.224575</td>\n      <td>2.111725</td>\n      <td>0.168708</td>\n      <td>-0.919217</td>\n      <td>0.025722</td>\n      <td>-0.982574</td>\n      <td>-1.721172</td>\n      <td>-0.285439</td>\n      <td>-0.532431</td>\n      <td>-0.383761</td>\n      <td>-0.577360</td>\n      <td>1.147020</td>\n      <td>0.105081</td>\n      <td>-0.226773</td>\n      <td>-1.401038</td>\n      <td>-0.492829</td>\n      <td>-0.871760</td>\n      <td>1.439129</td>\n      <td>0.217948</td>\n      <td>1.548338</td>\n      <td>0.497559</td>\n      <td>0.318772</td>\n      <td>0.266790</td>\n      <td>-0.968017</td>\n      <td>0.280303</td>\n      <td>-1.248275</td>\n      <td>-1.636719</td>\n      <td>0.051156</td>\n      <td>-1.943901</td>\n      <td>-0.159367</td>\n      <td>-0.783593</td>\n      <td>...</td>\n      <td>-1.758356</td>\n      <td>0.057032</td>\n      <td>-0.518035</td>\n      <td>0.257793</td>\n      <td>8.445820</td>\n      <td>-1.004632</td>\n      <td>1.857807</td>\n      <td>0.673306</td>\n      <td>0.268968</td>\n      <td>0.990404</td>\n      <td>0.139993</td>\n      <td>2.826678</td>\n      <td>-0.315463</td>\n      <td>-0.546489</td>\n      <td>-1.422807</td>\n      <td>-0.006208</td>\n      <td>-0.746203</td>\n      <td>-1.951766</td>\n      <td>-0.360050</td>\n      <td>-1.189089</td>\n      <td>-0.659353</td>\n      <td>0.084771</td>\n      <td>-0.526160</td>\n      <td>1.876591</td>\n      <td>-1.308766</td>\n      <td>6.012720</td>\n      <td>-1.378603</td>\n      <td>-1.246954</td>\n      <td>0.202484</td>\n      <td>-0.321746</td>\n      <td>0.024204</td>\n      <td>0.680223</td>\n      <td>0.801870</td>\n      <td>-0.681506</td>\n      <td>1.340432</td>\n      <td>-0.645714</td>\n      <td>0.002157</td>\n      <td>0.733017</td>\n      <td>2.039239</td>\n      <td>0.842452</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_raw.shape, test_raw.shape","execution_count":30,"outputs":[{"output_type":"execute_result","execution_count":30,"data":{"text/plain":"((262144, 258), (131073, 257))"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_raw.isnull().sum().sum(), test_raw.isnull().sum().sum()","execution_count":31,"outputs":[{"output_type":"execute_result","execution_count":31,"data":{"text/plain":"(0, 0)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"So there are no missing values in either training or test set."},{"metadata":{},"cell_type":"markdown","source":"### Target distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(train_raw.target)\nplt.show()","execution_count":32,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_raw.target.value_counts()","execution_count":33,"outputs":[{"output_type":"execute_result","execution_count":33,"data":{"text/plain":"1    131131\n0    131013\nName: target, dtype: int64"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Looks like class labels are uniformly distributed in training data."},{"metadata":{},"cell_type":"markdown","source":"### Train-validation split\n\nLet's split our training set such that 15% of data are used for validation -"},{"metadata":{"trusted":true},"cell_type":"code","source":"trn_x, valid_x, trn_y, valid_y = train_test_split(train_raw.drop(['id', 'target'], axis=1), train_raw.target, random_state=33, test_size=0.15)\ntrn_x.shape, valid_x.shape, trn_y.shape, valid_y.shape","execution_count":34,"outputs":[{"output_type":"execute_result","execution_count":34,"data":{"text/plain":"((222822, 256), (39322, 256), (222822,), (39322,))"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Categorical Feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"trn_wheezy = pd.get_dummies(trn_x['wheezy-copper-turtle-magic'])\nvalid_wheezy = pd.get_dummies(valid_x['wheezy-copper-turtle-magic'])\ntest_wheezy = pd.get_dummies(test_raw['wheezy-copper-turtle-magic'])\n\ntrn_wheezy.shape, valid_wheezy.shape, test_wheezy.shape","execution_count":35,"outputs":[{"output_type":"execute_result","execution_count":35,"data":{"text/plain":"((222822, 512), (39322, 512), (131073, 512))"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"trn_x.drop('wheezy-copper-turtle-magic', axis=1, inplace=True)\nvalid_x.drop('wheezy-copper-turtle-magic', axis=1, inplace=True)\ntest_raw.drop('wheezy-copper-turtle-magic', axis=1, inplace=True)","execution_count":36,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Normalize features"},{"metadata":{"trusted":true},"cell_type":"code","source":"sc = StandardScaler()\ntrn_x = sc.fit_transform(trn_x)\n\nvalid_x = sc.transform(valid_x)\ntest_x = sc.transform(test_raw.drop('id', axis=1))","execution_count":37,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trn_x = np.concatenate([trn_x, trn_wheezy.values], axis=1)\nvalid_x = np.concatenate([valid_x, valid_wheezy.values], axis=1)\ntest_x = np.concatenate([test_x, test_wheezy.values], axis=1)","execution_count":38,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model():\n    inp = Input(shape=(trn_x.shape[1],), name='input')\n    x = layers.Dense(1000, activation='relu')(inp)\n    x = layers.Dropout(0.65)(x)\n    x = layers.Dense(750, activation='relu')(x)\n    x = layers.Dropout(0.65)(x)\n    x = layers.Dense(500, activation='relu')(x)\n    x = layers.Dropout(0.6)(x)\n    x = layers.Dense(1, activation='sigmoid')(x)\n    \n    model = Model(inp, x)\n    model.compile(optimizer='adam',\n                 loss='binary_crossentropy', metrics=['acc'])\n    \n    return model\n\nmodel = build_model()\nmodel.summary()","execution_count":39,"outputs":[{"output_type":"stream","text":"_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput (InputLayer)           (None, 767)               0         \n_________________________________________________________________\ndense_5 (Dense)              (None, 1000)              768000    \n_________________________________________________________________\ndropout_4 (Dropout)          (None, 1000)              0         \n_________________________________________________________________\ndense_6 (Dense)              (None, 750)               750750    \n_________________________________________________________________\ndropout_5 (Dropout)          (None, 750)               0         \n_________________________________________________________________\ndense_7 (Dense)              (None, 500)               375500    \n_________________________________________________________________\ndropout_6 (Dropout)          (None, 500)               0         \n_________________________________________________________________\ndense_8 (Dense)              (None, 1)                 501       \n=================================================================\nTotal params: 1,894,751\nTrainable params: 1,894,751\nNon-trainable params: 0\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"weights_path = f'weights.best.hdf5'\nval_loss_checkpoint = ModelCheckpoint(weights_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\nreduceLR = ReduceLROnPlateau(monitor='val_loss', patience=5, verbose=1, mode='min', min_lr=1e-6)","execution_count":40,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(trn_x, trn_y, epochs=80, validation_data=(valid_x, valid_y),\n         callbacks=[val_loss_checkpoint, reduceLR], batch_size=512, verbose=1)","execution_count":41,"outputs":[{"output_type":"stream","text":"Train on 222822 samples, validate on 39322 samples\nEpoch 1/80\n222822/222822 [==============================] - 3s 15us/step - loss: 0.7038 - acc: 0.5015 - val_loss: 0.6931 - val_acc: 0.5076\n\nEpoch 00001: val_loss improved from inf to 0.69312, saving model to weights.best.hdf5\nEpoch 2/80\n222822/222822 [==============================] - 3s 13us/step - loss: 0.6939 - acc: 0.5034 - val_loss: 0.6931 - val_acc: 0.5129\n\nEpoch 00002: val_loss improved from 0.69312 to 0.69308, saving model to weights.best.hdf5\nEpoch 3/80\n222822/222822 [==============================] - 3s 12us/step - loss: 0.6933 - acc: 0.5074 - val_loss: 0.6929 - val_acc: 0.5144\n\nEpoch 00003: val_loss improved from 0.69308 to 0.69285, saving model to weights.best.hdf5\nEpoch 4/80\n222822/222822 [==============================] - 3s 13us/step - loss: 0.6924 - acc: 0.5172 - val_loss: 0.6924 - val_acc: 0.5266\n\nEpoch 00004: val_loss improved from 0.69285 to 0.69240, saving model to weights.best.hdf5\nEpoch 5/80\n222822/222822 [==============================] - 3s 13us/step - loss: 0.6913 - acc: 0.5257 - val_loss: 0.6916 - val_acc: 0.5292\n\nEpoch 00005: val_loss improved from 0.69240 to 0.69159, saving model to weights.best.hdf5\nEpoch 6/80\n222822/222822 [==============================] - 3s 13us/step - loss: 0.6903 - acc: 0.5320 - val_loss: 0.6906 - val_acc: 0.5353\n\nEpoch 00006: val_loss improved from 0.69159 to 0.69059, saving model to weights.best.hdf5\nEpoch 7/80\n222822/222822 [==============================] - 3s 13us/step - loss: 0.6878 - acc: 0.5426 - val_loss: 0.6894 - val_acc: 0.5470\n\nEpoch 00007: val_loss improved from 0.69059 to 0.68941, saving model to weights.best.hdf5\nEpoch 8/80\n222822/222822 [==============================] - 3s 13us/step - loss: 0.6843 - acc: 0.5546 - val_loss: 0.6872 - val_acc: 0.5596\n\nEpoch 00008: val_loss improved from 0.68941 to 0.68721, saving model to weights.best.hdf5\nEpoch 9/80\n222822/222822 [==============================] - 3s 12us/step - loss: 0.6783 - acc: 0.5699 - val_loss: 0.6842 - val_acc: 0.5716\n\nEpoch 00009: val_loss improved from 0.68721 to 0.68418, saving model to weights.best.hdf5\nEpoch 10/80\n222822/222822 [==============================] - 3s 13us/step - loss: 0.6718 - acc: 0.5846 - val_loss: 0.6801 - val_acc: 0.5782\n\nEpoch 00010: val_loss improved from 0.68418 to 0.68008, saving model to weights.best.hdf5\nEpoch 11/80\n222822/222822 [==============================] - 3s 13us/step - loss: 0.6649 - acc: 0.5972 - val_loss: 0.6740 - val_acc: 0.5907\n\nEpoch 00011: val_loss improved from 0.68008 to 0.67404, saving model to weights.best.hdf5\nEpoch 12/80\n222822/222822 [==============================] - 3s 13us/step - loss: 0.6570 - acc: 0.6114 - val_loss: 0.6731 - val_acc: 0.6009\n\nEpoch 00012: val_loss improved from 0.67404 to 0.67314, saving model to weights.best.hdf5\nEpoch 13/80\n222822/222822 [==============================] - 3s 13us/step - loss: 0.6495 - acc: 0.6221 - val_loss: 0.6669 - val_acc: 0.6084\n\nEpoch 00013: val_loss improved from 0.67314 to 0.66688, saving model to weights.best.hdf5\nEpoch 14/80\n222822/222822 [==============================] - 3s 13us/step - loss: 0.6419 - acc: 0.6330 - val_loss: 0.6638 - val_acc: 0.6139\n\nEpoch 00014: val_loss improved from 0.66688 to 0.66380, saving model to weights.best.hdf5\nEpoch 15/80\n222822/222822 [==============================] - 3s 13us/step - loss: 0.6337 - acc: 0.6432 - val_loss: 0.6602 - val_acc: 0.6167\n\nEpoch 00015: val_loss improved from 0.66380 to 0.66023, saving model to weights.best.hdf5\nEpoch 16/80\n222822/222822 [==============================] - 3s 13us/step - loss: 0.6268 - acc: 0.6519 - val_loss: 0.6576 - val_acc: 0.6239\n\nEpoch 00016: val_loss improved from 0.66023 to 0.65763, saving model to weights.best.hdf5\nEpoch 17/80\n222822/222822 [==============================] - 3s 12us/step - loss: 0.6203 - acc: 0.6588 - val_loss: 0.6552 - val_acc: 0.6284\n\nEpoch 00017: val_loss improved from 0.65763 to 0.65520, saving model to weights.best.hdf5\nEpoch 18/80\n222822/222822 [==============================] - 3s 13us/step - loss: 0.6122 - acc: 0.6669 - val_loss: 0.6484 - val_acc: 0.6318\n\nEpoch 00018: val_loss improved from 0.65520 to 0.64841, saving model to weights.best.hdf5\nEpoch 19/80\n222822/222822 [==============================] - 3s 13us/step - loss: 0.6067 - acc: 0.6737 - val_loss: 0.6450 - val_acc: 0.6387\n\nEpoch 00019: val_loss improved from 0.64841 to 0.64502, saving model to weights.best.hdf5\nEpoch 20/80\n222822/222822 [==============================] - 3s 12us/step - loss: 0.5981 - acc: 0.6796 - val_loss: 0.6427 - val_acc: 0.6428\n\nEpoch 00020: val_loss improved from 0.64502 to 0.64270, saving model to weights.best.hdf5\nEpoch 21/80\n222822/222822 [==============================] - 3s 12us/step - loss: 0.5936 - acc: 0.6854 - val_loss: 0.6401 - val_acc: 0.6450\n\nEpoch 00021: val_loss improved from 0.64270 to 0.64009, saving model to weights.best.hdf5\nEpoch 22/80\n222822/222822 [==============================] - 3s 12us/step - loss: 0.5847 - acc: 0.6938 - val_loss: 0.6397 - val_acc: 0.6495\n\nEpoch 00022: val_loss improved from 0.64009 to 0.63974, saving model to weights.best.hdf5\nEpoch 23/80\n222822/222822 [==============================] - 3s 12us/step - loss: 0.5796 - acc: 0.6983 - val_loss: 0.6317 - val_acc: 0.6547\n\nEpoch 00023: val_loss improved from 0.63974 to 0.63172, saving model to weights.best.hdf5\nEpoch 24/80\n222822/222822 [==============================] - 3s 13us/step - loss: 0.5713 - acc: 0.7055 - val_loss: 0.6248 - val_acc: 0.6570\n\nEpoch 00024: val_loss improved from 0.63172 to 0.62481, saving model to weights.best.hdf5\nEpoch 25/80\n222822/222822 [==============================] - 3s 13us/step - loss: 0.5650 - acc: 0.7101 - val_loss: 0.6296 - val_acc: 0.6613\n\nEpoch 00025: val_loss did not improve from 0.62481\nEpoch 26/80\n222822/222822 [==============================] - 3s 13us/step - loss: 0.5594 - acc: 0.7156 - val_loss: 0.6205 - val_acc: 0.6653\n\nEpoch 00026: val_loss improved from 0.62481 to 0.62054, saving model to weights.best.hdf5\nEpoch 27/80\n222822/222822 [==============================] - 3s 12us/step - loss: 0.5517 - acc: 0.7218 - val_loss: 0.6175 - val_acc: 0.6657\n\nEpoch 00027: val_loss improved from 0.62054 to 0.61746, saving model to weights.best.hdf5\nEpoch 28/80\n222822/222822 [==============================] - 3s 12us/step - loss: 0.5449 - acc: 0.7257 - val_loss: 0.6166 - val_acc: 0.6722\n\nEpoch 00028: val_loss improved from 0.61746 to 0.61660, saving model to weights.best.hdf5\nEpoch 29/80\n222822/222822 [==============================] - 3s 13us/step - loss: 0.5384 - acc: 0.7317 - val_loss: 0.6090 - val_acc: 0.6736\n\nEpoch 00029: val_loss improved from 0.61660 to 0.60903, saving model to weights.best.hdf5\nEpoch 30/80\n222822/222822 [==============================] - 3s 13us/step - loss: 0.5323 - acc: 0.7368 - val_loss: 0.6080 - val_acc: 0.6750\n\nEpoch 00030: val_loss improved from 0.60903 to 0.60798, saving model to weights.best.hdf5\nEpoch 31/80\n222822/222822 [==============================] - 3s 12us/step - loss: 0.5260 - acc: 0.7414 - val_loss: 0.6097 - val_acc: 0.6769\n\nEpoch 00031: val_loss did not improve from 0.60798\nEpoch 32/80\n222822/222822 [==============================] - 3s 13us/step - loss: 0.5193 - acc: 0.7467 - val_loss: 0.6010 - val_acc: 0.6804\n\nEpoch 00032: val_loss improved from 0.60798 to 0.60099, saving model to weights.best.hdf5\nEpoch 33/80\n222822/222822 [==============================] - 3s 13us/step - loss: 0.5135 - acc: 0.7499 - val_loss: 0.6052 - val_acc: 0.6842\n\nEpoch 00033: val_loss did not improve from 0.60099\nEpoch 34/80\n222822/222822 [==============================] - 3s 13us/step - loss: 0.5063 - acc: 0.7557 - val_loss: 0.5995 - val_acc: 0.6849\n\nEpoch 00034: val_loss improved from 0.60099 to 0.59951, saving model to weights.best.hdf5\nEpoch 35/80\n222822/222822 [==============================] - 3s 13us/step - loss: 0.5021 - acc: 0.7596 - val_loss: 0.5921 - val_acc: 0.6906\n\nEpoch 00035: val_loss improved from 0.59951 to 0.59206, saving model to weights.best.hdf5\nEpoch 36/80\n222822/222822 [==============================] - 3s 13us/step - loss: 0.4959 - acc: 0.7627 - val_loss: 0.5924 - val_acc: 0.6910\n\nEpoch 00036: val_loss did not improve from 0.59206\nEpoch 37/80\n","name":"stdout"},{"output_type":"stream","text":"222822/222822 [==============================] - 3s 13us/step - loss: 0.4901 - acc: 0.7657 - val_loss: 0.5903 - val_acc: 0.6936\n\nEpoch 00037: val_loss improved from 0.59206 to 0.59032, saving model to weights.best.hdf5\nEpoch 38/80\n222822/222822 [==============================] - 3s 14us/step - loss: 0.4838 - acc: 0.7709 - val_loss: 0.5845 - val_acc: 0.6954\n\nEpoch 00038: val_loss improved from 0.59032 to 0.58446, saving model to weights.best.hdf5\nEpoch 39/80\n222822/222822 [==============================] - 3s 13us/step - loss: 0.4769 - acc: 0.7744 - val_loss: 0.5852 - val_acc: 0.6965\n\nEpoch 00039: val_loss did not improve from 0.58446\nEpoch 40/80\n222822/222822 [==============================] - 3s 13us/step - loss: 0.4742 - acc: 0.7768 - val_loss: 0.5866 - val_acc: 0.6989\n\nEpoch 00040: val_loss did not improve from 0.58446\nEpoch 41/80\n222822/222822 [==============================] - 3s 12us/step - loss: 0.4686 - acc: 0.7797 - val_loss: 0.5852 - val_acc: 0.6993\n\nEpoch 00041: val_loss did not improve from 0.58446\nEpoch 42/80\n222822/222822 [==============================] - 3s 12us/step - loss: 0.4644 - acc: 0.7826 - val_loss: 0.5797 - val_acc: 0.7028\n\nEpoch 00042: val_loss improved from 0.58446 to 0.57970, saving model to weights.best.hdf5\nEpoch 43/80\n222822/222822 [==============================] - 3s 13us/step - loss: 0.4582 - acc: 0.7866 - val_loss: 0.5773 - val_acc: 0.7031\n\nEpoch 00043: val_loss improved from 0.57970 to 0.57728, saving model to weights.best.hdf5\nEpoch 44/80\n222822/222822 [==============================] - 3s 13us/step - loss: 0.4538 - acc: 0.7896 - val_loss: 0.5760 - val_acc: 0.7048\n\nEpoch 00044: val_loss improved from 0.57728 to 0.57597, saving model to weights.best.hdf5\nEpoch 45/80\n222822/222822 [==============================] - 3s 12us/step - loss: 0.4501 - acc: 0.7922 - val_loss: 0.5781 - val_acc: 0.7039\n\nEpoch 00045: val_loss did not improve from 0.57597\nEpoch 46/80\n222822/222822 [==============================] - 3s 13us/step - loss: 0.4453 - acc: 0.7947 - val_loss: 0.5697 - val_acc: 0.7083\n\nEpoch 00046: val_loss improved from 0.57597 to 0.56975, saving model to weights.best.hdf5\nEpoch 47/80\n222822/222822 [==============================] - 3s 13us/step - loss: 0.4422 - acc: 0.7964 - val_loss: 0.5714 - val_acc: 0.7083\n\nEpoch 00047: val_loss did not improve from 0.56975\nEpoch 48/80\n222822/222822 [==============================] - 3s 12us/step - loss: 0.4352 - acc: 0.7999 - val_loss: 0.5715 - val_acc: 0.7076\n\nEpoch 00048: val_loss did not improve from 0.56975\nEpoch 49/80\n222822/222822 [==============================] - 3s 13us/step - loss: 0.4314 - acc: 0.8030 - val_loss: 0.5684 - val_acc: 0.7104\n\nEpoch 00049: val_loss improved from 0.56975 to 0.56844, saving model to weights.best.hdf5\nEpoch 50/80\n222822/222822 [==============================] - 3s 13us/step - loss: 0.4282 - acc: 0.8040 - val_loss: 0.5738 - val_acc: 0.7087\n\nEpoch 00050: val_loss did not improve from 0.56844\nEpoch 51/80\n222822/222822 [==============================] - 3s 13us/step - loss: 0.4259 - acc: 0.8073 - val_loss: 0.5671 - val_acc: 0.7083\n\nEpoch 00051: val_loss improved from 0.56844 to 0.56714, saving model to weights.best.hdf5\nEpoch 52/80\n222822/222822 [==============================] - 3s 13us/step - loss: 0.4205 - acc: 0.8085 - val_loss: 0.5637 - val_acc: 0.7114\n\nEpoch 00052: val_loss improved from 0.56714 to 0.56374, saving model to weights.best.hdf5\nEpoch 53/80\n222822/222822 [==============================] - 3s 13us/step - loss: 0.4169 - acc: 0.8110 - val_loss: 0.5632 - val_acc: 0.7117\n\nEpoch 00053: val_loss improved from 0.56374 to 0.56323, saving model to weights.best.hdf5\nEpoch 54/80\n222822/222822 [==============================] - 3s 13us/step - loss: 0.4134 - acc: 0.8132 - val_loss: 0.5608 - val_acc: 0.7126\n\nEpoch 00054: val_loss improved from 0.56323 to 0.56080, saving model to weights.best.hdf5\nEpoch 55/80\n222822/222822 [==============================] - 3s 13us/step - loss: 0.4105 - acc: 0.8162 - val_loss: 0.5599 - val_acc: 0.7136\n\nEpoch 00055: val_loss improved from 0.56080 to 0.55992, saving model to weights.best.hdf5\nEpoch 56/80\n222822/222822 [==============================] - 3s 13us/step - loss: 0.4071 - acc: 0.8173 - val_loss: 0.5603 - val_acc: 0.7149\n\nEpoch 00056: val_loss did not improve from 0.55992\nEpoch 57/80\n222822/222822 [==============================] - 3s 13us/step - loss: 0.4013 - acc: 0.8208 - val_loss: 0.5595 - val_acc: 0.7151\n\nEpoch 00057: val_loss improved from 0.55992 to 0.55950, saving model to weights.best.hdf5\nEpoch 58/80\n222822/222822 [==============================] - 3s 13us/step - loss: 0.4021 - acc: 0.8208 - val_loss: 0.5593 - val_acc: 0.7136\n\nEpoch 00058: val_loss improved from 0.55950 to 0.55931, saving model to weights.best.hdf5\nEpoch 59/80\n222822/222822 [==============================] - 3s 12us/step - loss: 0.3968 - acc: 0.8216 - val_loss: 0.5603 - val_acc: 0.7144\n\nEpoch 00059: val_loss did not improve from 0.55931\nEpoch 60/80\n222822/222822 [==============================] - 3s 13us/step - loss: 0.3966 - acc: 0.8232 - val_loss: 0.5612 - val_acc: 0.7146\n\nEpoch 00060: val_loss did not improve from 0.55931\nEpoch 61/80\n222822/222822 [==============================] - 3s 13us/step - loss: 0.3917 - acc: 0.8254 - val_loss: 0.5567 - val_acc: 0.7162\n\nEpoch 00061: val_loss improved from 0.55931 to 0.55671, saving model to weights.best.hdf5\nEpoch 62/80\n222822/222822 [==============================] - 3s 13us/step - loss: 0.3877 - acc: 0.8279 - val_loss: 0.5567 - val_acc: 0.7181\n\nEpoch 00062: val_loss improved from 0.55671 to 0.55667, saving model to weights.best.hdf5\nEpoch 63/80\n222822/222822 [==============================] - 3s 14us/step - loss: 0.3867 - acc: 0.8283 - val_loss: 0.5569 - val_acc: 0.7188\n\nEpoch 00063: val_loss did not improve from 0.55667\nEpoch 64/80\n222822/222822 [==============================] - 3s 13us/step - loss: 0.3832 - acc: 0.8301 - val_loss: 0.5556 - val_acc: 0.7185\n\nEpoch 00064: val_loss improved from 0.55667 to 0.55557, saving model to weights.best.hdf5\nEpoch 65/80\n222822/222822 [==============================] - 3s 12us/step - loss: 0.3787 - acc: 0.8334 - val_loss: 0.5550 - val_acc: 0.7212\n\nEpoch 00065: val_loss improved from 0.55557 to 0.55502, saving model to weights.best.hdf5\nEpoch 66/80\n222822/222822 [==============================] - 3s 13us/step - loss: 0.3775 - acc: 0.8336 - val_loss: 0.5543 - val_acc: 0.7193\n\nEpoch 00066: val_loss improved from 0.55502 to 0.55432, saving model to weights.best.hdf5\nEpoch 67/80\n222822/222822 [==============================] - 3s 13us/step - loss: 0.3715 - acc: 0.8374 - val_loss: 0.5539 - val_acc: 0.7205\n\nEpoch 00067: val_loss improved from 0.55432 to 0.55389, saving model to weights.best.hdf5\nEpoch 68/80\n222822/222822 [==============================] - 3s 13us/step - loss: 0.3693 - acc: 0.8386 - val_loss: 0.5549 - val_acc: 0.7191\n\nEpoch 00068: val_loss did not improve from 0.55389\nEpoch 69/80\n222822/222822 [==============================] - 3s 12us/step - loss: 0.3683 - acc: 0.8388 - val_loss: 0.5508 - val_acc: 0.7215\n\nEpoch 00069: val_loss improved from 0.55389 to 0.55078, saving model to weights.best.hdf5\nEpoch 70/80\n222822/222822 [==============================] - 3s 12us/step - loss: 0.3661 - acc: 0.8403 - val_loss: 0.5498 - val_acc: 0.7219\n\nEpoch 00070: val_loss improved from 0.55078 to 0.54977, saving model to weights.best.hdf5\nEpoch 71/80\n222822/222822 [==============================] - 3s 13us/step - loss: 0.3629 - acc: 0.8420 - val_loss: 0.5518 - val_acc: 0.7230\n\nEpoch 00071: val_loss did not improve from 0.54977\nEpoch 72/80\n222822/222822 [==============================] - 3s 13us/step - loss: 0.3598 - acc: 0.8428 - val_loss: 0.5546 - val_acc: 0.7239\n\nEpoch 00072: val_loss did not improve from 0.54977\nEpoch 73/80\n222822/222822 [==============================] - 3s 13us/step - loss: 0.3580 - acc: 0.8444 - val_loss: 0.5523 - val_acc: 0.7226\n\nEpoch 00073: val_loss did not improve from 0.54977\nEpoch 74/80\n222822/222822 [==============================] - 3s 12us/step - loss: 0.3555 - acc: 0.8449 - val_loss: 0.5530 - val_acc: 0.7219\n\nEpoch 00074: val_loss did not improve from 0.54977\nEpoch 75/80\n","name":"stdout"},{"output_type":"stream","text":"222822/222822 [==============================] - 3s 12us/step - loss: 0.3553 - acc: 0.8453 - val_loss: 0.5557 - val_acc: 0.7208\n\nEpoch 00075: val_loss did not improve from 0.54977\n\nEpoch 00075: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\nEpoch 76/80\n222822/222822 [==============================] - 3s 12us/step - loss: 0.3289 - acc: 0.8585 - val_loss: 0.5554 - val_acc: 0.7220\n\nEpoch 00076: val_loss did not improve from 0.54977\nEpoch 77/80\n222822/222822 [==============================] - 3s 12us/step - loss: 0.3226 - acc: 0.8618 - val_loss: 0.5559 - val_acc: 0.7234\n\nEpoch 00077: val_loss did not improve from 0.54977\nEpoch 78/80\n222822/222822 [==============================] - 3s 13us/step - loss: 0.3164 - acc: 0.8647 - val_loss: 0.5542 - val_acc: 0.7235\n\nEpoch 00078: val_loss did not improve from 0.54977\nEpoch 79/80\n222822/222822 [==============================] - 3s 12us/step - loss: 0.3127 - acc: 0.8667 - val_loss: 0.5544 - val_acc: 0.7244\n\nEpoch 00079: val_loss did not improve from 0.54977\nEpoch 80/80\n222822/222822 [==============================] - 3s 12us/step - loss: 0.3090 - acc: 0.8691 - val_loss: 0.5556 - val_acc: 0.7252\n\nEpoch 00080: val_loss did not improve from 0.54977\n\nEpoch 00080: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n","name":"stdout"},{"output_type":"execute_result","execution_count":41,"data":{"text/plain":"<keras.callbacks.History at 0x7fefe5faacc0>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.load_weights(weights_path)","execution_count":43,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### roc_auc_score on validation data"},{"metadata":{"trusted":true},"cell_type":"code","source":"val_preds = model.predict(valid_x, batch_size=2048, verbose=1)","execution_count":44,"outputs":[{"output_type":"stream","text":"39322/39322 [==============================] - 0s 7us/step\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_auc_score(valid_y.values, val_preds.reshape(-1))","execution_count":45,"outputs":[{"output_type":"execute_result","execution_count":45,"data":{"text/plain":"0.7952916141713527"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Prediction on test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds = model.predict(test_x, batch_size=2048, verbose=1)","execution_count":46,"outputs":[{"output_type":"stream","text":"131073/131073 [==============================] - 1s 4us/step\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df = pd.read_csv(f'{data_dir}sample_submission.csv')\nsub_df.target = test_preds.reshape(-1)\nsub_df.head()","execution_count":47,"outputs":[{"output_type":"execute_result","execution_count":47,"data":{"text/plain":"                                 id    target\n0  1c13f2701648e0b0d46d8a2a5a131a53  0.674349\n1  ba88c155ba898fc8b5099893036ef205  0.801480\n2  7cbab5cea99169139e7e6d8ff74ebb77  0.743347\n3  ca820ad57809f62eb7b4d13f5d4371a0  0.267272\n4  7baaf361537fbd8a1aaa2c97a6d4ccc7  0.200581","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1c13f2701648e0b0d46d8a2a5a131a53</td>\n      <td>0.674349</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ba88c155ba898fc8b5099893036ef205</td>\n      <td>0.801480</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7cbab5cea99169139e7e6d8ff74ebb77</td>\n      <td>0.743347</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ca820ad57809f62eb7b4d13f5d4371a0</td>\n      <td>0.267272</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7baaf361537fbd8a1aaa2c97a6d4ccc7</td>\n      <td>0.200581</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df.to_csv('solution.csv', index=False)","execution_count":48,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}