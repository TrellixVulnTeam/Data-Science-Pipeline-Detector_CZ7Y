{"cells":[{"metadata":{},"cell_type":"markdown","source":"This competition was a fun one and I learned a lot from it mostly because it had a relatively small dataset that let me design various experiments and run them in a short time.\n\nHere I am going to share how I achieved public LB score 0.97482; at this point I assume all of the competitiors who broke the 0.973 barrier found the same thing that I will describe here. If you still don't know how the dataset is generated first read my other kernel at https://www.kaggle.com/mhviraf/synthetic-data-for-next-instant-gratification\n\nMoreover, you can find my complete code at https://www.kaggle.com/mhviraf/mhviraf-s-best-submission-in-instant-gratification\n\nAfter the abovementioned kernel and Vlad's public kernel on QDA were published, everyone assumed that after removing useless columns in each `wheezy-copper-turtle-magic` the dataset must look like somehow to the following graph (of course in higher dimensions) because QDA gave us more accurate results than SVD, Logistic regression, LGBM, etc. etc. "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from sklearn.datasets import make_classification \nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')\nnp.random.seed(2019)\n\n# generate dataset \nX, y = make_classification(1024, 2, n_informative=2, n_clusters_per_class=1, n_redundant=0, flip_y=0.05, random_state=125)\n\n\nplt.scatter(X[y==0, 0], X[y==0, 1], label='target=0')\nplt.scatter(X[y==1, 0], X[y==1, 1], label='target=1')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is actually very close to what the competition dataset is. HOWEVER, there is one important and different thing. In `make_classification`, the default value of `n_clusters_per_class` is 2 not 1! and I think it is set to 3 in Instant Gratification competition dataset but for the sake of easier illustration i'm gonna stick with 2 here. So the above graph would turn into the following graph if `n_clusters_per_class=2`:"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from sklearn.datasets import make_classification \nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nnp.random.seed(2019)\n\n# generate dataset \nX, y = make_classification(1024, 2, n_informative=2, n_clusters_per_class=2, n_redundant=0, flip_y=0.05, random_state=5)\n\n\nplt.scatter(X[y==0, 0], X[y==0, 1], label='target=0')\nplt.scatter(X[y==1, 0], X[y==1, 1], label='target=1')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"QDA would still perform ok on this dataset, particularly if you use the exact same parameters but a different `random_state`, the dataset might be much easier for QDA to model. refer to the figure below. Still 2 clusters per class but you can imagine fiting 1 gaussian distribution to each class too."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from sklearn.datasets import make_classification \nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n# generate dataset \nX, y = make_classification(1024, 2, n_informative=2, n_clusters_per_class=2, n_redundant=0, flip_y=0.05, random_state=0)\n\n\nplt.scatter(X[y==0, 0], X[y==0, 1], label='target=0')\nplt.scatter(X[y==1, 0], X[y==1, 1], label='target=1')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now if we fit a guassian model to the former dataset with 2 clusters per class, whether it's a `QDA` or `mixture.GaussianMixture` we will get the model shown below. Still doing ok, accurate enough but not the best possible model."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from sklearn import mixture\nfrom matplotlib.colors import LogNorm\nfrom sklearn.covariance import OAS\n\ndef get_mean_cov(X):\n    model = OAS(assume_centered=False)\n    \n    ms = []\n    ps = []\n    for xi in X:\n        model.fit(xi)\n        ms.append(model.location_)\n        ps.append(model.precision_)\n    return np.array(ms), np.array(ps)\n\nknn_clf = mixture.GaussianMixture(n_components=2, init_params='random',\n                          covariance_type='full',\n                          n_init=1, \n                          random_state=0)\n\n\nX, y = make_classification(1024, 2, n_informative=2, n_clusters_per_class=2, n_redundant=0, flip_y=0.05, random_state=5)\n\nx_t =X.copy()\ny_t =y.copy()\ntrain3_pos = X[y==1]\ntrain3_neg = X[y==0]\n\nprint(train3_pos.shape, train3_neg.shape)\nms, ps = get_mean_cov([train3_pos, train3_neg])\n\nclf = mixture.GaussianMixture(n_components=2, covariance_type='full', means_init=ms, precisions_init=ps,)\nclf.fit(X)\n\nx = np.linspace(-5., 5.)\ny = np.linspace(-5., 5.)\nX, Y = np.meshgrid(x, y)\nXX = np.array([X.ravel(), Y.ravel()]).T\nZ = -clf.score_samples(XX)\nZ = Z.reshape(X.shape)\n\nplt.figure()\nplt.contour(X, Y, Z, norm=LogNorm(vmin=1.0, vmax=1000.0),  levels=np.logspace(0, 3, 10))\nplt.scatter(x_t[y_t==0, 0], x_t[y_t==0, 1], label='target=0', alpha=.3)\nplt.scatter(x_t[y_t==1, 0], x_t[y_t==1, 1], label='target=1', alpha=.3)\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The key!\nIf we rather cluster each class into 2 or 3 clusters before fitting the model and then set `n_components=4 or 6` in `GaussianMixture` we will achieve an even more accurate model such as the one shown below."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from sklearn import mixture\nfrom matplotlib.colors import LogNorm\nfrom sklearn.covariance import OAS\nX, y = make_classification(1024, 2, n_informative=2, n_clusters_per_class=2, n_redundant=0, flip_y=0.05, random_state=5)\n\nx_t =X.copy()\ny_t =y.copy()\ntrain3_pos = X[y==1]\ntrain3_neg = X[y==0]\n\ncluster_num_pos = knn_clf.fit_predict(train3_pos)\ntrain3_pos_1 = train3_pos[cluster_num_pos==0]\ntrain3_pos_2 = train3_pos[cluster_num_pos==1]\n#print(train3_pos.shape, train3_pos_1.shape, train3_pos_2.shape, train3_pos_3.shape)\n\n## FIND CLUSTERS IN CHUNKS WITH TARGET = 0\ncluster_num_neg = knn_clf.fit_predict(train3_neg)\ntrain3_neg_1 = train3_neg[cluster_num_neg==0]\ntrain3_neg_2 = train3_neg[cluster_num_neg==1]\n        \n    \nprint(train3_pos.shape, train3_neg.shape)\nms, ps = get_mean_cov([train3_pos_1, train3_pos_2, train3_neg_1, train3_neg_2])\n\nclf = mixture.GaussianMixture(n_components=4, covariance_type='full', means_init=ms, precisions_init=ps,)\nclf.fit(X)\n\nx = np.linspace(-5., 5.)\ny = np.linspace(-5., 5.)\nX, Y = np.meshgrid(x, y)\nXX = np.array([X.ravel(), Y.ravel()]).T\nZ = -clf.score_samples(XX)\nZ = Z.reshape(X.shape)\n\nplt.figure()\nplt.contour(X, Y, Z, norm=LogNorm(vmin=1.0, vmax=1000.0),  levels=np.logspace(0, 3, 10))\nplt.scatter(x_t[y_t==0, 0], x_t[y_t==0, 1], label='target=0', alpha=.3)\nplt.scatter(x_t[y_t==1, 0], x_t[y_t==1, 1], label='target=1', alpha=.3)\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" Hopefully other competitiors will share their solutions soon but I think everyone with LB score > 0.974 has been using the same technique that is described here."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}