{"cells":[{"metadata":{},"cell_type":"markdown","source":"\n## Description\n\nUPDATE: In this version of the kernel we will try to test the idea of selecting features using LOFO. For more details about LOFO please see Ahmet Erdem's kernel available [at this link](https://www.kaggle.com/divrikwicky/instantgratification-lofo-feature-importance). The feature selection step is going to slow down the training process, so this new version will run longer than 1 minute. If you want to see the original kernel that runs less than a minute please refer to Version 1 of this kernel. \n\nThe original kernel scores 0.99610 on the LB. Unfortunately, we won't be able to use this result as a baseline for comparison because we won't be able to submit our work to LB: in order for LOFO to work, an external package, `lofo-importance`, must be loaded but the usage of external packages is banned by the competion rules. However, it is possible to compute the cross-validation score for the QDA model without LOFO. As a matter of fact, I have already done it in a different kernel: [link](https://www.kaggle.com/graf10a/tuning-512-separate-qda-models) (see the \"Repeat Using the Standard Parameters\" section). The result was a CV score of 0.96629.  Let's see if selecting features with LOFO can improve this baseline. \n\nSPOILER: Basically, the resutl is very inconclusive -- the combined AUC went up from 0.96629 to 0.96727, the fold-average AUC went down from 0.96628 to 0.96213, and the standard deviation increased from 9e-05 to 0.0097. It would be nice to submit it to the LB to see how well it performs."},{"metadata":{},"cell_type":"markdown","source":"## Setting things up\n### Loading Libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np \nimport pandas as pd \nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.preprocessing import StandardScaler\n\n\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Loading Data"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"%%time\n\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n\ntrain['wheezy-copper-turtle-magic'] = train['wheezy-copper-turtle-magic'].astype('category')\ntest['wheezy-copper-turtle-magic'] = test['wheezy-copper-turtle-magic'].astype('category')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Computing LOFO Importance\n\nHere is the adapted code from [Ahmet's notebook](https://www.kaggle.com/divrikwicky/instantgratification-lofo-feature-importance):"},{"metadata":{"trusted":true},"cell_type":"code","source":"from lofo import LOFOImportance, FLOFOImportance, plot_importance\nfrom tqdm import tqdm_notebook\n\ndef get_model():\n    return Pipeline([('scaler', StandardScaler()),\n                    ('qda', QuadraticDiscriminantAnalysis(reg_param=0.111))\n                   ])\n\nfeatures = [c for c in train.columns if c not in ['id', 'target', 'wheezy-copper-turtle-magic']]\n\n\ndef get_lofo_importance(wctm_num):\n    sub_df = train[train['wheezy-copper-turtle-magic'] == wctm_num]\n    sub_features = [f for f in features if sub_df[f].std() > 1.5]\n    lofo_imp = LOFOImportance(sub_df, target=\"target\",\n                              features=sub_features, \n                              cv=StratifiedKFold(n_splits=4, random_state=42, shuffle=True), scoring=\"roc_auc\",\n                              model=get_model(), n_jobs=4)\n    return lofo_imp.get_importance()\n\nfeatures_to_remove = []\npotential_gain = []\n\nn_models=512\nfor i in tqdm_notebook(range(n_models)):\n    imp = get_lofo_importance(i)\n    features_to_remove.append(imp[\"feature\"].values[-1])\n    potential_gain.append(-imp[\"importance_mean\"].values[-1])\n    \nprint(\"Potential gain (AUC):\", np.round(np.mean(potential_gain), 5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Building the QDA Classifier with LOFO"},{"metadata":{},"cell_type":"markdown","source":"### Preparing Things for Cross-Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_name='QDA'\n\nNFOLDS=25\nRS=42\n\noof=np.zeros(len(train))\npreds=np.zeros(len(test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training the Classifiers on All Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nprint(f'Cross-validation for the {clf_name} classifier:')\n\ndefault_cols = [c for c in train.columns if c not in ['id', 'target', 'wheezy-copper-turtle-magic']]\n\n# BUILD 512 SEPARATE NON-LINEAR MODELS\nfor i in range(512):  \n    \n    # EXTRACT SUBSET OF DATASET WHERE WHEEZY-MAGIC EQUALS i     \n    X = train[train['wheezy-copper-turtle-magic']==i].copy()\n    Y = X.pop('target').values\n    X_test = test[test['wheezy-copper-turtle-magic']==i].copy()\n    idx_train = X.index \n    idx_test = X_test.index\n    X.reset_index(drop=True,inplace=True)\n\n    #cols = [c for c in X.columns if c not in ['id', 'wheezy-copper-turtle-magic']]\n    cols = [c for c in default_cols if c != features_to_remove[i]]\n    X = X[cols].values             # numpy.ndarray\n    X_test = X_test[cols].values   # numpy.ndarray\n\n    # FEATURE SELECTION (USE APPROX 40 OF 255 FEATURES)\n    vt = VarianceThreshold(threshold=1.5).fit(X)\n    X = vt.transform(X)            # numpy.ndarray\n    X_test = vt.transform(X_test)  # numpy.ndarray   \n\n    # STRATIFIED K FOLD\n    auc_all_folds=np.array([])\n    folds = StratifiedKFold(n_splits=NFOLDS, random_state=RS)\n\n    for fold_num, (train_index, val_index) in enumerate(folds.split(X, Y), 1):\n\n        X_train, Y_train = X[train_index, :], Y[train_index]\n        X_val, Y_val = X[val_index, :], Y[val_index]\n\n        pipe = Pipeline([('scaler', StandardScaler()),\n                         (clf_name, QuadraticDiscriminantAnalysis(reg_param=0.111)),\n                       ])  \n\n        pipe.fit(X_train, Y_train)\n\n        oof[idx_train[val_index]] = pipe.predict_proba(X_val)[:,1]\n        preds[idx_test] += pipe.predict_proba(X_test)[:,1]/NFOLDS\n\n        auc = roc_auc_score(Y_val, oof[idx_train[val_index]])\n        auc_all_folds = np.append(auc_all_folds, auc)\n            \n# PRINT CROSS-VALIDATION AUC FOR THE CLASSFIER\nauc_combo = roc_auc_score(train['target'].values, oof)\nauc_folds_average = np.mean(auc_all_folds)\nstd = np.std(auc_all_folds)/np.sqrt(NFOLDS)\n\nprint(f'The combined CV score is {round(auc_combo,5)}.')    \nprint(f'The folds average CV score is {round(auc_folds_average,5)}.')\nprint(f'The standard deviation is {round(std, 5)}.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Creating the Submission File\n\nAll done! At this point we are ready to make our submission file! (We won't be able to submit it but let's make it anyway.)"},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv('../input/sample_submission.csv')\nsub['target'] = preds\nsub.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}