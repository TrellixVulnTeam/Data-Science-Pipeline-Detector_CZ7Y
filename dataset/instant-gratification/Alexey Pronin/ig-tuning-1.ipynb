{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Loading Libraries"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":false},"cell_type":"code","source":"import time\nimport pickle\nfrom tqdm import tqdm\nfrom pathlib import Path\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np \nimport pandas as pd \nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures, QuantileTransformer\nfrom sklearn.mixture import GaussianMixture, BayesianGaussianMixture\n\nfrom scipy.stats import rankdata\nfrom sklearn.svm import SVC, NuSVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis, LinearDiscriminantAnalysis","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Parameters"},{"metadata":{"trusted":false},"cell_type":"code","source":"NFOLDS=5          # The number of folds\nRS=42             # The random seed\ndebug=0          # The debuging mode switch: 1-debugging on; 0-debugging off \n\nparams_2={'n_components' : 2,              # The parameters of the auxiliary \n          'init_params': 'random',         # GMM classifier computing clusters'\n          'covariance_type': 'full',       # stats (means and covariance)\n          'tol':0.001, \n          'reg_covar': 0.001, \n          'max_iter': 55, \n          'n_init': 10,\n          'random_state': RS,\n         }\n\nparams_2_qda={'n_components' : 2,              # The parameters of the auxiliary \n          'init_params': 'random',         # GMM classifier computing clusters'\n          'covariance_type': 'full',       # stats (means and covariance)\n          'tol':0.001, \n          'reg_covar': 0.001, \n          'max_iter': 50, \n          'n_init': 10,\n          'random_state': RS,\n         }\n\n#PARAMETERS FOR THE GMM CLASSIFIER (2 clusters per class; 4 clusters in total)\nparams_4={'n_components' : 4, \n          'init_params': 'random', \n          'covariance_type': 'full', \n          'tol':0.001, \n          'reg_covar': 0.001, \n          'max_iter': 55, \n          'n_init': 10, \n          'random_state': RS,\n         }\n\n#PARAMETERS FOR THE QDA CLASSIFIER (2 clusters per class; 4 clusters in total)\nparams_qda={'reg_param' : 0.111,\n         }\n\n#PARAMETERS TO BE USED FOR PSEUDOLABELING GMM\nlow=0.0067\nhigh=1-low\n\n#PARAMETERS TO BE USED FOR PSEUDOLABELING QDA\n# low_vals = [None, 0.01, 0.01, 0.01, 0.001, 0.0001]\n# high_vals = [None, 0.99, 0.99, 0.99, 0.999, 0.9999]\n# low_vals = [0.01, 0.01, 0.01, 0.001, 0.0001]\n# high_vals = [0.99, 0.99, 0.99, 0.999, 0.9999]\n\nrp_values = [0.8, 0.7, 0.6, 0.6]\n#rp_values = [0.8]#[0.76, 0.77, 0.78, 0.79, 0.80, 0.81, 0.82, 0.83, 0.84, 0.85]\n\nlow_vals = [0.01, 0.01, 0.01, 0.01]\nhigh_vals = [1-low for low in low_vals]\n\n# low_vals.insert(0, None)\n# high_vals.insert(0, None)\n\nprint(low_vals, high_vals)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Loading Data"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"code","source":"%%time\n\npath = Path('../input')\n\ntrain = pd.read_csv(path/'train.csv')\ntest = pd.read_csv(path/'test.csv')\nsub = pd.read_csv(path/'sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Handle Debuging\n\nChecking and handling the debuging mode (low values of magic_max and NFOLDS save a lot of time; the latter breaks cross-validation):"},{"metadata":{"trusted":false},"cell_type":"code","source":"if debug:\n    magic_max=2\n    magic_min=0\n    NFOLDS=2\nelse:\n    magic_max=train['wheezy-copper-turtle-magic'].max()\n    magic_min=train['wheezy-copper-turtle-magic'].min()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Some Useful Functions and Preprocessing\n\nIn this part, we will collect and preprocess data from all 512 model (one model per one value of the `'wheezy-copper-turtle-magic'` categorical variable as was explained by Chris Deotte [here](https://www.kaggle.com/cdeotte/support-vector-machine-0-925). Later we will be able to load all these data from a single dictionary."},{"metadata":{"trusted":false},"cell_type":"code","source":"def preprocess(train=train, test=test):\n       \n    prepr = {} \n    \n    #PREPROCESS 512 SEPARATE MODELS\n    for i in range(magic_min, magic_max+1):\n\n        # EXTRACT SUBSET OF DATASET WHERE WHEEZY-MAGIC EQUALS i     \n        X = train[train['wheezy-copper-turtle-magic']==i].copy()\n        Y = X.pop('target').values\n        X_test = test[test['wheezy-copper-turtle-magic']==i].copy()\n        idx_train = X.index \n        idx_test = X_test.index\n        X.reset_index(drop=True,inplace=True)\n\n        cols = np.array([c for c in X.columns if c not in ['id', 'wheezy-copper-turtle-magic']])\n\n        l=len(X)\n        X_all = pd.concat([X[cols], X_test[cols]], ignore_index=True)\n        \n        sel = VarianceThreshold(threshold=2)\n        X_vt = sel.fit_transform(X_all)               # np.ndarray\n        \n        prepr['vt_' + str(i)] = X_vt\n        prepr['n_vt' + str(i)] = X_vt.shape[1]\n        prepr['feats_vt' + str(i)] = cols[sel.get_support(indices=True)]        \n        prepr['train_size_' + str(i)] = l\n        prepr['idx_train_' + str(i)] = idx_train\n        prepr['idx_test_' + str(i)] = idx_test\n        prepr['target_' + str(i)] = Y\n        \n    return prepr","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\n\ndata = preprocess()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And here is a handy function to get data for any value of `i`."},{"metadata":{"trusted":false},"cell_type":"code","source":"def get_data(i, data):\n    \n    l = data['train_size_' + str(i)]\n    \n    X_all = data['vt_' + str(i)]                \n\n    X = X_all[:l, :]\n    X_test = X_all[l:, :]\n\n    Y = data['target_' + str(i)]\n\n    idx_train = data['idx_train_' + str(i)]\n    idx_test = data['idx_test_' + str(i)]\n    \n    return X, X_test, Y, idx_train, idx_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here is another very useful function initializing storage arrays for our cross-validation results: AUC, out-of-fold predictions, and test set prediction."},{"metadata":{"trusted":false},"cell_type":"code","source":"def initialize_cv():\n    \n    auc = np.array([])\n    oof = np.zeros(len(train))\n    preds = np.zeros(len(test)) \n    return preds, oof, auc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And another useful function to report the result of the cross-validation procedure."},{"metadata":{"trusted":false},"cell_type":"code","source":"def report_results(oof, auc_all, clf_name='GMM'):\n    \n    # PRINT VALIDATION CV AUC FOR THE CLASSFIER\n    print(f'The result summary for the {clf_name} classifier:')\n    auc_combo = roc_auc_score(train['target'].values, oof)\n    auc_av = np.mean(auc_all)\n    std = np.std(auc_all)/(np.sqrt(NFOLDS)*np.sqrt(magic_max+1))\n\n    print(f'The combined CV score is {round(auc_combo, 5)}.')    \n    print(f'The folds average CV score is {round(auc_av, 5)}.')\n    print(f'The standard deviation is {round(std, 5)}.\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Identifying Clusters and Predicting Classes with GMM\n\nIn what follows, I will assume that there are 2 clusters per class in the data set. To identify these clusters we will run `GMM` on positive and negative instances separately. In each case, our goal is to label instances that belong to two different clusters. Here is a handy function that computes the means and covariances for all 4 clusters (2 clusters per each class):"},{"metadata":{"trusted":false},"cell_type":"code","source":"def clusters_stats(X_train, Y_train, params=params_2):\n    \n    X_train_0 = X_train[Y_train==0]\n    Y_train_0 = Y_train[Y_train==0].reshape(-1, 1)\n\n    X_train_1 = X_train[Y_train==1]\n    Y_train_1 = Y_train[Y_train==1].reshape(-1, 1)\n\n    clf_0 = GaussianMixture(**params)\n\n    clf_0.fit(X_train_0)\n    means_0 = clf_0.means_\n    covs_0 = clf_0.covariances_\n    ps_0 = [np.linalg.inv(m) for m in covs_0]\n\n    clf_1 = GaussianMixture(**params)\n\n    clf_1.fit(X_train_1)\n    means_1 = clf_1.means_\n    covs_1 = clf_1.covariances_\n    ps_1 = [np.linalg.inv(m) for m in covs_1]\n\n    #SAVING CLUSTERS' MEANS AND COVARIANCES       \n    ms = np.stack((means_0[0], means_0[1], means_1[0], means_1[1]))\n    ps = np.stack((ps_0[0], ps_0[1], ps_1[0], ps_1[1]))\n    \n    return ms, ps","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here is another function that we will be using for pseudolabeling. "},{"metadata":{"trusted":false},"cell_type":"code","source":"def pseudolabeling(X_train, X_test, Y_train, Y_pseudo, \n                   idx_test, test=test, low=low, high=high):\n    \n    assert len(test) == len(Y_pseudo), \"The length of test does not match that of Y_pseudo!\"\n    \n    #SELECT ONLY THE PSEUDOLABLES CORRESPONDING TO THE CURRENT VALUES OF 'wheezy-copper-turtle-magic'\n    Y_aug = np.copy(Y_pseudo[idx_test])\n    \n    assert len(Y_aug) == len(X_test), \"The length of Y_aug does not match that of X_test!\"\n\n    Y_aug[Y_aug > high] = 1\n    Y_aug[Y_aug < low] = 0\n    \n    mask = (Y_aug == 1) | (Y_aug == 0)\n    \n    Y_useful = Y_aug[mask]\n    X_test_useful = X_test[mask]\n    \n    X_train_aug = np.vstack((X_train, X_test_useful))\n    Y_train_aug = np.vstack((Y_train.reshape(-1, 1), Y_useful.reshape(-1, 1)))\n    \n    return X_train_aug, Y_train_aug","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And, finally, here is our main function for training our classifiers."},{"metadata":{"trusted":false},"cell_type":"code","source":"def train_classifier(Y_pseudo, params=params_4):\n    \n    preds, oof, auc_all = initialize_cv()\n\n    print(f\"Computing centroids and covariances for the four clusters (two per class).\")\n\n    # BUILD 512 SEPARATE NON-LINEAR MODELS\n    for i in tqdm(range(magic_min, magic_max+1)):   \n\n        X, X_test, Y, idx_train, idx_test = get_data(i=i, data=data) \n\n        # STRATIFIED K FOLD\n        auc_folds=np.array([])\n\n        folds = StratifiedKFold(n_splits=NFOLDS, random_state=RS)\n\n        for train_index, val_index in folds.split(X, Y):\n\n            X_train, Y_train = X[train_index, :], Y[train_index]\n            X_val, Y_val = X[val_index, :], Y[val_index]\n            \n            if Y_pseudo is None:\n                params['means_init'], params['precisions_init'] = clusters_stats(X_train, Y_train)\n            else:\n                X_aug, Y_aug = pseudolabeling(X_train, X_test, Y_train, Y_pseudo, idx_test)\n                params['means_init'], params['precisions_init'] = clusters_stats(X_aug, Y_aug.ravel())  \n            \n            #INSTANTIATING THE MAIN CLASSIFIER\n            clf = GaussianMixture(**params) \n            \n            clf.fit(np.concatenate([X_train, X_test], axis = 0))\n\n            oof[idx_train[val_index]] = np.sum(clf.predict_proba(X_val)[:, 2:], axis=1)\n            preds[idx_test] += np.sum(clf.predict_proba(X_test)[:,2: ], axis=1)/NFOLDS\n\n            auc = roc_auc_score(Y_val, oof[idx_train[val_index]])\n            auc_folds = np.append(auc_folds, auc)\n\n        auc_all = np.append(auc_all, np.mean(auc_folds))\n\n    report_results(oof, auc_all)\n    \n    return preds, oof, auc_all","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let's actually train our first classifier:"},{"metadata":{"trusted":false},"cell_type":"code","source":"Y_pseudo, oof, auc_all = train_classifier(Y_pseudo=None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And repeat with pseudolabeling:"},{"metadata":{"trusted":false},"cell_type":"code","source":"preds_gmm, oof_gmm, auc_gmm = train_classifier(Y_pseudo=Y_pseudo)\n\nsub['target'] = preds_gmm\nsub.to_csv('submission_gmm.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Computations with QDA\n\n### Identifying Clusters and Predicting Classes\n\nIn what follows, I will assume that there are 2 clusters per class in the data set. To identify these clusters we will run GMM on positive and negative instances separately. In each case, our goal is to label instances that belong to two different clusters. Here is a handy function that computes the means and covariances for all 4 clusters (2 clusters per each class):"},{"metadata":{"trusted":false},"cell_type":"code","source":"def get_labels(X_train, Y_train, params=params_2_qda):\n    \n    X_train_0 = X_train[Y_train==0]\n    X_train_1 = X_train[Y_train==1]\n\n    clf_0 = GaussianMixture(**params)\n    labels_0 = clf_0.fit_predict(X_train_0).reshape(-1, 1)\n\n    clf_1 = GaussianMixture(**params)\n    labels_1 = clf_1.fit_predict(X_train_1).reshape(-1, 1)\n    \n    labels_1[labels_1==0] = 2\n    labels_1[labels_1==1] = 3\n\n    #CREATE LABELED DATA \n    \n    X_l = np.vstack((X_train_0, X_train_1))\n    Y_l = np.vstack((labels_0, labels_1))\n    \n    perm = np.random.permutation(len(X_l))\n    \n    X_l = X_l[perm]\n    Y_l = Y_l[perm]\n    \n    return X_l, Y_l","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"QDA pseudolabeling function:"},{"metadata":{"trusted":false},"cell_type":"code","source":"def pseudolabeling_qda(X_train, X_test, Y_train, Y_pseudo, \n                       idx_test, low, high, test=test):\n    \n    assert len(test) == len(Y_pseudo), \"The length of test does not match that of Y_pseudo!\"\n    \n    #SELECT ONLY THE PSEUDOLABLES CORRESPONDING TO THE CURRENT VALUES OF 'wheezy-copper-turtle-magic'\n    Y_aug = np.copy(Y_pseudo[idx_test])\n    \n    assert len(Y_aug) == len(X_test), \"The length of Y_aug does not match that of X_test!\"\n\n    Y_aug[Y_aug > high] = 1\n    Y_aug[Y_aug < low] = 0\n    \n    mask = (Y_aug == 1) | (Y_aug == 0)\n    \n    Y_useful = Y_aug[mask]\n    X_test_useful = X_test[mask]\n    \n    X_train_aug = np.vstack((X_train, X_test_useful))\n    Y_train_aug = np.vstack((Y_train.reshape(-1, 1), Y_useful.reshape(-1, 1)))\n    \n    return X_train_aug, Y_train_aug","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here is the function for training the QDA classifier."},{"metadata":{"trusted":false},"cell_type":"code","source":"def train_qda(Y_pseudo, low, high, params=params_qda):\n    \n    preds, oof, auc_all = initialize_cv()\n\n    print(f\"Computing centroids and covariances for the four clusters (two per class).\")\n\n    # BUILD 512 SEPARATE NON-LINEAR MODELS\n    for i in tqdm(range(magic_min, magic_max+1)):   \n\n        X, X_test, Y, idx_train, idx_test = get_data(i=i, data=data) \n\n        # STRATIFIED K FOLD\n        auc_folds=np.array([])\n\n        folds = StratifiedKFold(n_splits=NFOLDS, random_state=RS)\n\n        for train_index, val_index in folds.split(X, Y):\n\n            X_train, Y_train = X[train_index, :], Y[train_index]\n            X_val, Y_val = X[val_index, :], Y[val_index]\n            \n            #INSTANTIATING THE MAIN CLASSIFIER\n            clf = QuadraticDiscriminantAnalysis(**params)  \n            \n            if Y_pseudo is None:\n                X_l, Y_l = get_labels(X_train, Y_train)\n            else:\n                X_aug, Y_aug = pseudolabeling_qda(X_train, X_test, Y_train, Y_pseudo, idx_test, low, high)\n                X_l, Y_l = get_labels(X_aug, Y_aug.ravel()) \n                \n            clf.fit(X_l, Y_l.ravel())\n                \n            oof[idx_train[val_index]] = np.sum(clf.predict_proba(X_val)[:, 2:], axis=1)\n            preds[idx_test] += np.sum(clf.predict_proba(X_test)[:,2: ], axis=1)/NFOLDS\n\n            auc = roc_auc_score(Y_val, oof[idx_train[val_index]])\n            auc_folds = np.append(auc_folds, auc)\n\n        auc_all = np.append(auc_all, np.mean(auc_folds))\n\n    report_results(oof, auc_all, clf_name='QDA')\n    \n    return preds, oof, auc_all","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let's actually train the QDA classifier:"},{"metadata":{"trusted":false},"cell_type":"code","source":"Y_pseudo=preds_gmm#None\n\nfor rp, low, high in zip(rp_values, low_vals, high_vals):\n    parmas_qda = {'reg_param': rp}\n    Y_pseudo, oof_qda, auc_qda = train_qda(Y_pseudo=Y_pseudo, low=low, high=high, params=params_qda)\n\n#THE LAST PREDICTIONS \npreds_qda = Y_pseudo\n\nsub['target'] = preds_qda\nsub.to_csv('submission_qda.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Selecting the Strongest GMM/QDA Results\n\nIdea: For each value of the 'magic' variable, let's keep the predictions of the model (GMM or QDA) that has the largest AUC for this value of the variable. "},{"metadata":{"trusted":false},"cell_type":"code","source":"preds_highest = preds_gmm\noof_highest = oof_gmm\n\nmask = (auc_qda > auc_gmm)\n\nprint(f\"The number of models where QDA's predictions are better is {sum(mask)}.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"for i in tqdm(range(magic_min, magic_max+1)):\n    \n    if mask[i]:\n        _, _, _, idx_train, idx_test = get_data(i=i, data=data)\n        oof_highest[idx_train] = oof_qda[idx_train]\n        preds_highest[idx_test] = preds_qda[idx_test]\n        \nauc = roc_auc_score(train['target'].values, oof_highest)\nprint(f\"The 'highest' ROC AUC score is {auc}.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sub['target'] = preds_highest\nsub.to_csv('submission_highest.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Stacking GMM and QDA with Logistic Regression"},{"metadata":{"trusted":false},"cell_type":"code","source":"oof_all = pd.DataFrame()\npreds_all = pd.DataFrame()\n\noof_all['gmm'] = rankdata(oof_gmm)/len(oof_gmm)\noof_all['qda'] = rankdata(oof_qda)/len(oof_qda)\npreds_all['gmm'] = rankdata(preds_gmm)/len(preds_gmm)\npreds_all['qda'] = rankdata(preds_qda)/len(preds_qda)\n\nlr = LogisticRegression()\n\nlr.fit(oof_all.values, train['target'].values)\npreds_lr = lr.predict_proba(preds_all.values)[:,1]\n\npreds_train = lr.predict_proba(oof_all)[:,1]\n\nauc = roc_auc_score(train['target'].values, preds_train)\nprint(f\"The final ROC AUC score is {auc}.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating the submission file"},{"metadata":{"trusted":false},"cell_type":"code","source":"sub['target'] = preds_lr\nsub.to_csv('submission_lr.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Picking the Final Predictions\n\nIn our final submission, we will keep the GMM prediction results that were made with high degree of certainty. For the least certain predictions we will use the stacking results."},{"metadata":{"trusted":false},"cell_type":"code","source":"w = 0.02\nmask = (preds_gmm < (0.5 + w))&(preds_gmm > (0.5 - w))\n\npreds = rankdata(preds_gmm)/len(preds_gmm)\n\npreds[mask] = preds_lr[mask]\n\nsub['target'] = preds\nsub.to_csv('submission_picking.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}