{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"'''\n\n1. GMM, QDA, nuSVC, LogReg, NB, LabelProp, LGBM, RF\n2. Boosting (unfair CV)\n\nLOCAL CV = 0.9747141236427626\n\n'''","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import numpy as np, pandas as pd\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn import svm, neighbors, linear_model, neural_network\nfrom sklearn.svm import NuSVC\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.decomposition import PCA\nfrom tqdm import tqdm_notebook\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn import mixture\nfrom scipy.stats.mstats import gmean\nfrom bayes_opt import BayesianOptimization\nfrom sklearn.naive_bayes import GaussianNB\nimport pickle\nimport lightgbm as lgb\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.semi_supervised import LabelPropagation\n\nfrom sklearn.covariance import GraphicalLasso\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RANDOM_SEED = 4123\n\ncols = [\n    c for c in train.columns \n    if c not in ['id', 'target', 'wheezy-copper-turtle-magic']\n]\n\ndef get_mean_cov(x,y):\n    model = GraphicalLasso()\n    ones = (y==1).astype(bool)\n    x2 = x[ones]\n    model.fit(x2)\n    p1 = model.precision_\n    m1 = model.location_\n    \n    onesb = (y==0).astype(bool)\n    x2b = x[onesb]\n    model.fit(x2b)\n    p2 = model.precision_\n    m2 = model.location_\n    \n    ms = np.stack([m1,m2])\n    ps = np.stack([p1,p2])\n    return ms,ps","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nSKIP_COMMIT = True\n\nif SKIP_COMMIT:\n    \n    # to not waste time on commit\n    \n    sub = pd.read_csv('../input/sample_submission.csv')\n\n    if sub.shape[0] < 200000:\n        sub = pd.read_csv('../input/sample_submission.csv')\n        #sub.to_csv('submission.csv', index=False)\n        sub.to_csv('submission.csv', index=False)\n\n        raise ValueError('Stop!!!')\n\n\noof_nusvc = np.zeros(len(train)) \npreds_nusvc = np.zeros(len(test))\n\noof_nb= np.zeros(len(train)) \npreds_nb = np.zeros(len(test))\n\noof_lr = np.zeros(len(train)) \npreds_lr = np.zeros(len(test))\n\noof_qda = np.zeros(len(train)) \npreds_qda = np.zeros(len(test))\n\noof_lp = np.zeros(len(train))\npreds_lp = np.zeros(len(test))\n\noof_lgbm = np.zeros(len(train)) \npreds_lgbm = np.zeros(len(test))\n\noof_gm = np.zeros(len(train)) \npreds_gm = np.zeros(len(test))\n\noof_rf = np.zeros(len(train)) \npreds_rf = np.zeros(len(test))\n\n\nparams_lgbm_1 = {\n    'boosting_type': 'gbdt',\n    'objective': 'xentropy',\n    'metric': ['auc'],\n    'num_leaves': 31,\n    'learning_rate': 0.5,\n    'feature_fraction': 1.0,\n    'bagging_fraction': 1.0,\n    'bagging_freq': 18,\n    'num_threads': 8,\n    'lambda_l2': 5.0,\n    'max_bin': 3\n}\n\n\nfor i in range(512):\n    \n    print(i, end=' ')\n    \n    train2 = train[train['wheezy-copper-turtle-magic']==i] \n    idx1 = train2.index \n    test2 = test[test['wheezy-copper-turtle-magic']==i]\n    idx2 = test2.index\n    \n    data = pd.concat(\n        [\n            train2,\n            test2\n        ],\n        axis=0\n    )\n    train2.reset_index(drop=True, inplace=True)\n    \n    train_size = train2.shape[0]\n    \n    \n    # remove unnecessary fields\n    sel = VarianceThreshold(threshold=1.5)\n    tmp = sel.fit_transform(\n        data[cols]\n    )\n    train3 = tmp[:train_size, :]\n    test3 = tmp[train_size:, :]\n    \n    # scale data for non-QDA methods\n    ss = StandardScaler()\n    tmp_scaled = ss.fit_transform(tmp)\n    \n    train3_scaled = tmp_scaled[:train_size, :]\n    test3_scaled = tmp_scaled[train_size:, :]\n    \n    # Polynomial features for LogReg\n    poly = PolynomialFeatures(degree=2)\n    tmp_poly = poly.fit_transform(tmp_scaled)\n    \n    train3_poly = tmp_poly[:train_size, :]\n    test3_poly = tmp_poly[train_size:, :]\n    \n    # GM features 4\n    gm_clf_4 = mixture.GaussianMixture(\n        n_components=4, \n        random_state=RANDOM_SEED\n    )\n    gm_tmp_4 = gm_clf_4.fit_predict(tmp).reshape(-1, 1)\n    \n    le_4 = OneHotEncoder()\n    gm_tmp_4 = le_4.fit_transform(gm_tmp_4).todense()\n    \n    gm_train3_4 = gm_tmp_4[:train_size, :]\n    gm_test3_4 = gm_tmp_4[train_size:, :]\n    \n    # GM features 6\n    gm_clf_6 = mixture.GaussianMixture(\n        n_components=6, \n        random_state=RANDOM_SEED\n    )\n    gm_tmp_6 = gm_clf_6.fit_predict(tmp).reshape(-1, 1)\n    \n    le_6 = OneHotEncoder()\n    gm_tmp_6 = le_6.fit_transform(gm_tmp_6).todense()\n    \n    gm_train3_6 = gm_tmp_6[:train_size, :]\n    gm_test3_6 = gm_tmp_6[train_size:, :]\n    \n    \n    skf = StratifiedKFold(\n        n_splits=11, \n        random_state=RANDOM_SEED, \n        shuffle=True\n    )\n    \n    for train_index, test_index in skf.split(train3, train2['target']):\n        \n        train_train_index, train_val_index = train_test_split(\n            train_index, \n            test_size=0.3, \n            random_state=RANDOM_SEED\n        )\n        \n        # LGBM\n        \n        train_dataset = lgb.Dataset(\n            np.hstack(\n                (\n                    train3_scaled[train_train_index,:], \n                    gm_tmp_4[:train_size, :][train_train_index, :].tolist(),\n                    gm_tmp_6[:train_size, :][train_train_index, :].tolist(),\n                    train3_poly[train_train_index,:]\n                )\n            ),\n            train2.loc[train_train_index]['target'],\n            free_raw_data=False\n        )\n        \n        valid_dataset = lgb.Dataset(\n            np.hstack(\n                (\n                    train3_scaled[train_val_index,:],\n                    gm_tmp_4[:train_size, :][train_val_index, :].tolist(),\n                    gm_tmp_6[:train_size, :][train_val_index, :].tolist(),\n                    train3_poly[train_val_index,:]\n                )\n            ),\n            train2.loc[train_val_index]['target'],\n            free_raw_data=False\n        )\n        \n        gm = lgb.train(\n            params_lgbm_1,\n            train_dataset,\n            num_boost_round=1000,\n            early_stopping_rounds=20,\n            valid_sets=(train_dataset, valid_dataset),\n            valid_names=('train', 'valid'),\n            feature_name=[str(l) for l in range(\n                np.hstack(\n                    (\n                        train3_scaled[test_index,:],\n                        gm_tmp_4[:train_size, :][test_index, :].tolist(),\n                        gm_tmp_6[:train_size, :][test_index, :].tolist(),\n                        train3_poly[test_index,:]\n                    )\n                ).shape[1]\n            )],\n            categorical_feature=[str(l) for l in \n                range(\n                    train3_scaled.shape[1], \n                    train3_scaled.shape[1] + gm_tmp_4.shape[1] + gm_tmp_6.shape[1]\n                )\n            ],\n            verbose_eval=0\n        )\n        \n        oof_lgbm[idx1[test_index]] = gm.predict(\n            np.hstack(\n                (\n                    train3_scaled[test_index,:],\n                    gm_tmp_4[:train_size, :][test_index, :].tolist(),\n                    gm_tmp_6[:train_size, :][test_index, :].tolist(),\n                    train3_poly[test_index,:]\n                )\n            )\n        )\n        preds_lgbm[idx2] += gm.predict(\n            np.hstack(\n                (\n                    test3_scaled,\n                    gm_test3_4.tolist(),\n                    gm_test3_6.tolist(),\n                    test3_poly\n                )\n            )\n        ) / skf.n_splits\n        \n        # GMM\n        \n        ms, ps = get_mean_cov(\n            train3[train_index, :],\n            train2.loc[train_index]['target'].values\n        )\n        \n        gm = mixture.GaussianMixture(\n            n_components=2, \n            init_params='random', \n            covariance_type='full', \n            tol=0.001,\n            reg_covar=0.001,\n            max_iter=100,\n            n_init=1,\n            means_init=ms,\n            precisions_init=ps,\n            random_state=RANDOM_SEED\n        )\n        gm.fit(tmp)\n        oof_gm[idx1[test_index]] = gm.predict_proba(\n            train3[test_index,:]\n        )[:, 0]\n        preds_gm[idx2] += gm.predict_proba(\n            test3\n        )[:, 0] / skf.n_splits\n        \n        # LabelProp\n\n        lp = LabelPropagation(\n            kernel='rbf', \n            gamma=0.15301581563198507, \n            n_jobs=-1\n        )\n        lp.fit(\n            train3_scaled[train_index,:],\n            train2.loc[train_index]['target']\n        )\n        oof_lp[idx1[test_index]] = lp.predict_proba(\n            train3_scaled[test_index, :]\n        )[:,1]\n        preds_lp[idx2] += lp.predict_proba(\n            test3_scaled\n        )[:,1] / skf.n_splits\n        \n        # nuSVC\n        \n        clf = NuSVC(\n            probability=True, \n            kernel='poly', \n            degree=2,\n            gamma='auto', \n            random_state=RANDOM_SEED, \n            nu=0.27312143533915767, \n            coef0=0.4690615598786931\n        )\n        \n        clf.fit(\n            np.hstack(\n                (\n                    train3_scaled[train_index,:], \n                    gm_train3_4[train_index, :],\n                    gm_train3_6[train_index, :]\n                )\n            ),\n            train2.loc[train_index]['target']\n        )\n        oof_nusvc[idx1[test_index]] = clf.predict_proba(\n            np.hstack(\n                (\n                    train3_scaled[test_index,:],\n                    gm_train3_4[test_index, :],\n                    gm_train3_6[test_index, :]\n                )\n            )\n        )[:,1]\n        \n        preds_nusvc[idx2] += clf.predict_proba(\n            np.hstack(\n                (\n                    test3_scaled, \n                    gm_test3_4,\n                    gm_test3_6\n                )\n            )\n        )[:,1] / skf.n_splits\n        \n        # RF\n        \n        clf = RandomForestClassifier(\n            max_depth=4, \n            n_jobs=-1, \n            n_estimators=20\n        )\n        \n        clf.fit(\n            np.hstack(\n                (\n                    train3_scaled[train_index,:], \n                    gm_train3_4[train_index, :],\n                    gm_train3_6[train_index, :]\n                )\n            ),\n            train2.loc[train_index]['target']\n        )\n        oof_rf[idx1[test_index]] = clf.predict_proba(\n            np.hstack(\n                (\n                    train3_scaled[test_index,:],\n                    gm_train3_4[test_index, :],\n                    gm_train3_6[test_index, :]\n                )\n            )\n        )[:,1]\n        \n        preds_rf[idx2] += clf.predict_proba(\n            np.hstack(\n                (\n                    test3_scaled, \n                    gm_test3_4,\n                    gm_test3_6\n                )\n            )\n        )[:,1] / skf.n_splits\n\n        # QDA\n        clf = QuadraticDiscriminantAnalysis(\n            reg_param=0.5674164995882528\n        )\n        clf.fit(\n            train3[train_index,:],\n            train2.loc[train_index]['target']\n        )\n        oof_qda[idx1[test_index]] += clf.predict_proba(\n            train3[test_index, :]\n        )[:,1]\n        preds_qda[idx2] += clf.predict_proba(\n            test3\n        )[:,1] / skf.n_splits\n        \n        # LogReg Poly\n        \n        clf = linear_model.LogisticRegression(\n            solver='saga',\n            penalty='l2',\n            C=0.01,\n            tol=0.001,\n            random_state=RANDOM_SEED\n        )\n        clf.fit(\n            train3_poly[train_index,:],\n            train2.loc[train_index]['target']\n        )\n        oof_lr[idx1[test_index]] = clf.predict_proba(\n            train3_poly[test_index,:]\n        )[:,1]\n        preds_lr[idx2] += clf.predict_proba(\n            test3_poly\n        )[:,1] / skf.n_splits\n        \n        # GaussianNB with GM 6\n        \n        clf = GaussianNB()\n        clf.fit(\n            np.hstack(\n                (\n                    train3_scaled[train_index,:], \n                    gm_train3_6[train_index, :],\n                    gm_train3_4[train_index, :]\n                )\n            ),\n            train2.loc[train_index]['target']\n        )\n        oof_nb[idx1[test_index]] = clf.predict_proba(\n            np.hstack(\n                (\n                    train3_scaled[test_index,:],\n                    gm_train3_6[test_index, :],\n                    gm_train3_4[test_index, :]\n                )\n            )\n        )[:,1]\n        \n        preds_nb[idx2] += clf.predict_proba(\n            np.hstack(\n                (\n                    test3_scaled,\n                    gm_test3_6,\n                    gm_test3_4\n                )\n            )\n        )[:,1] / skf.n_splits\n\n        \nprint('\\nsvcnu', roc_auc_score(train['target'], oof_nusvc))\nprint('gm', roc_auc_score(train['target'], oof_gm))\nprint('qda', roc_auc_score(train['target'], oof_qda))\nprint('log reg poly', roc_auc_score(train['target'], oof_lr))\nprint('gnb', roc_auc_score(train['target'], oof_nb))\nprint('lp', roc_auc_score(train['target'], oof_lp))\nprint('lgbm', roc_auc_score(train['target'], oof_lgbm))\nprint('rf', roc_auc_score(train['target'], oof_rf))\n\noof_qda = oof_qda.reshape(-1, 1)\npreds_qda = preds_qda.reshape(-1, 1)\n\noof_lr = oof_lr.reshape(-1, 1)\npreds_lr = preds_lr.reshape(-1, 1)\n\noof_nusvc = oof_nusvc.reshape(-1, 1)\npreds_nusvc = preds_nusvc.reshape(-1, 1)\n\noof_nb = oof_nb.reshape(-1, 1)\npreds_nb = preds_nb.reshape(-1, 1)\n\noof_lp = oof_lp.reshape(-1, 1)\npreds_lp = preds_lp.reshape(-1, 1)\n\noof_gm = oof_gm.reshape(-1, 1)\npreds_gm = preds_gm.reshape(-1, 1)\n\noof_lgbm = oof_lgbm.reshape(-1, 1)\npreds_lgbm = preds_lgbm.reshape(-1, 1)\n\noof_rf = oof_rf.reshape(-1, 1)\npreds_rf = preds_rf.reshape(-1, 1)\n\ntr_2 = np.concatenate(\n    (\n        oof_qda,\n        oof_nusvc,\n        oof_lr,\n        oof_nb,\n        oof_lp,\n        oof_gm,\n        oof_lgbm,\n        oof_rf\n    ), \n    axis=1\n)\nte_2 = np.concatenate(\n    (\n        preds_qda, \n        preds_nusvc, \n        preds_lr, \n        preds_nb,\n        preds_lp,\n        preds_gm,\n        preds_lgbm,\n        preds_rf\n    ), \n    axis=1\n)\n\n\nprint(np.corrcoef(tr_2, rowvar=False))\n\n\nparams = {\n    'boosting_type': 'gbdt',\n    'objective': 'xentropy',\n    'metric': ['auc'],\n    'num_leaves': 3,\n    'learning_rate': 0.1,\n    'feature_fraction': 0.4,\n    'bagging_fraction': 0.4,\n    'bagging_freq': 5,\n    'num_threads': 8\n}\n\nparams.update(\n    {\n        'bagging_fraction': 0.9687497922020039, \n        'bagging_freq': 100, \n        'feature_fraction': 0.7578027095458152, \n        'lambda_l2': 4.871836452096843, \n        'learning_rate': 0.41230192513715164, \n        'max_bin': 20, \n        'num_leaves': 8\n    }\n)\n\noof_boosting_2_bad_cv = np.zeros(train.shape[0])\npred_te_boosting_2_bad_cv = np.zeros(test.shape[0])\n\n\ntrain2 = train.copy()\ntrain2.reset_index(drop=True,inplace=True)\n\nskf = StratifiedKFold(\n    n_splits=11, \n    random_state=RANDOM_SEED, \n    shuffle=True\n)\n\nfor train_index, test_index in skf.split(tr_2, train2['target']):\n    \n    train_dataset = lgb.Dataset(\n        np.hstack(\n            (\n                tr_2[train_index, :],\n                tr_2[train_index, :]  ** 2.587508645172711\n            )\n        ),\n        train2['target'][train_index],\n        free_raw_data=False\n    )\n    valid_dataset = lgb.Dataset(\n        np.hstack(\n            (\n                tr_2[test_index, :] ,\n                tr_2[test_index, :]  ** 2.587508645172711\n            )\n        ),\n        train2['target'][test_index],\n        free_raw_data=False\n    )\n\n    gbm = lgb.train(\n        params,\n        train_dataset,\n        num_boost_round=1000,\n        early_stopping_rounds=100,\n        valid_sets=(train_dataset, valid_dataset),\n        valid_names=('train', 'valid'),\n        verbose_eval=100\n    )\n\n    oof_boosting_2_bad_cv[test_index] = gbm.predict(\n        np.hstack(\n            (\n                tr_2[test_index, :],\n                tr_2[test_index, :] ** 2.587508645172711\n            )\n        )\n    )\n    \n    pred_te_boosting_2_bad_cv += gbm.predict(\n        np.hstack(\n            (\n                te_2,\n                te_2 ** 2.587508645172711\n            )\n        )\n    ) / skf.n_splits\n\n    \nprint('gnb', roc_auc_score(train['target'], oof_boosting_2_bad_cv))\n\n##############\n# SAVE RESULTS\n##############\nsub = pd.read_csv('../input/sample_submission.csv')\nsub['target'] = pred_te_boosting_2_bad_cv\nsub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}