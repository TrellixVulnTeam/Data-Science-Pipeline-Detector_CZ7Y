{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import numpy as np, pandas as pd, os\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.feature_selection import VarianceThreshold\nfrom tqdm import tqdm\n\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.decomposition import PCA, FastICA, FactorAnalysis\nfrom matplotlib import pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.cluster import KMeans\nimport sklearn.mixture as mixture\n# https://scikit-learn.org/stable/modules/classes.html#module-sklearn.covariance\nfrom sklearn.covariance import EmpiricalCovariance\nfrom sklearn.covariance import GraphicalLasso, GraphicalLassoCV, MinCovDet, LedoitWolf, OAS, ShrunkCovariance\nfrom sklearn.cross_decomposition import CCA\nfrom sklearn.cross_decomposition import PLSCanonical\nfrom scipy.stats import rankdata\n\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Return clusters-list\ndef get_kmeans_clusters(x, y, k_pos=1, k_neg=1):\n    \n    x_zeros = x[y==0]\n    x_ones  = x[y==1]\n\n    model_0 = KMeans(n_clusters=k_neg)\n    model_1 = KMeans(n_clusters=k_pos)\n\n    model_0.fit(x_zeros)\n    model_1.fit(x_ones)\n\n    model_0_clus = [x_zeros[model_0.labels_==k] for k in range(model_0.n_clusters)]\n    model_1_clus = [x_ones[model_1.labels_==k] for k in range(model_1.n_clusters)]\n\n    return model_1_clus + model_0_clus\n\n# Return gmm model\ndef fit_multicluster_gmm(x, y, xt, k_pos, k_neg, max_iter=100):\n    \"\"\"\n    x = train predictors\n    y = binary target\n    xt = test predictors\n    k_pos = number clusters when y==1\n    k_neg = number clusters when y==0\n    \"\"\"\n\n    clusters = get_kmeans_clusters(x, y, k_pos=k_pos, k_neg=k_neg)\n\n    for i in range(len(clusters)):\n\n        x_cluster = clusters[i]\n\n        #if(x_cluster.shape[0]<x_cluster.shape[1]):\n            #model = GraphicalLasso(mode='lars', max_iter=max_iter)\n        #else:\n        model = ShrunkCovariance()\n\n        model.fit(x_cluster)\n\n        if (i==0):\n            ps = np.expand_dims(model.precision_, axis=0)\n            ms = np.expand_dims(model.location_,  axis=0)\n        else:\n            ps = np.concatenate([ps, np.expand_dims(model.precision_, axis=0)], axis=0)\n            ms = np.concatenate([ms, np.expand_dims(model.location_, axis=0)], axis=0)\n\n    gm = mixture.GaussianMixture(n_components=k_pos+k_neg, \n                                 init_params='random', \n                                 covariance_type='full',\n                                 tol=0.001,\n                                 reg_covar=0.001, \n                                 max_iter=100,\n                                 n_init=5,\n                                 means_init=ms,\n                                 precisions_init=ps)\n\n    gm.fit(np.vstack((x.astype(np.float), xt.astype(np.float))))\n    preds = gm.predict_proba(x.astype(np.float))[:,0]\n    score = roc_auc_score(y, preds)\n    return score, gm, k_pos, k_neg\n\ndef get_mean_cov(x,y, model=GraphicalLasso(), max_iter=100):\n    \n    try:\n        model.set_params(**{'max_iter':200})\n    except:\n        pass\n    \n    ones = (y==1).astype(bool)\n    x2 = x[ones]\n    model.fit(x2)\n    p1 = model.precision_\n    m1 = model.location_\n    \n    onesb = (y==0).astype(bool)\n    x2b = x[onesb]\n    model.fit(x2b)\n    p2 = model.precision_\n    m2 = model.location_\n    \n    ms = np.stack([m1,m2])\n    ps = np.stack([p1,p2])\n    return ms,ps","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_wheezy_copper_turtle_magic(train, i):\n    train2 = train[train['wheezy-copper-turtle-magic']==i].copy()\n        \n    test2 = test[test['wheezy-copper-turtle-magic']==i]\n    idx1 = train2.index; idx2 = test2.index\n    target = train2['target'].astype(np.int).values\n    \n    train2.reset_index(drop=True, inplace=True)\n    \n    return train2.drop(['id', 'target'], axis=1), test2.drop(['id'], axis=1), idx1, idx2, target","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def data_augmentation(X, y, gm_means, gm_dist):\n    for clus in range(len(gm_means)):\n        A = 2*gm_means[clus,].reshape(1,-1) - X[gm_dist==clus]\n        B = X[gm_dist==clus]\n        t = y[gm_dist==clus]\n        if clus==0:\n            Xn = np.vstack((A, B))\n            yn = np.concatenate((t,t))\n        else:\n            Xn = np.vstack((Xn, A, B))\n            yn = np.concatenate((yn, t,t))\n    return Xn, yn\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here we initialize variables\ncols = [c for c in train.columns if c not in ['id', 'target']]\ncols.remove('wheezy-copper-turtle-magic')\noof = np.zeros(len(train))\ntest_preds = np.zeros(len(test))\noof_gmm = np.zeros(len(train))\ntest_preds_gmm = np.zeros(len(test))\noof_gmm_2 = np.zeros(len(train))\ntest_preds_gmm_2 = np.zeros(len(test))\ntrials = 3\ncat_dict = dict()\ncluster_report = list()\n\n# BUILD 512 SEPARATE MODELS\nfor i in tqdm(range(512)):\n    # ONLY TRAIN WITH DATA WHERE WHEEZY EQUALS I\n    train2, test2, idx1, idx2, target = extract_wheezy_copper_turtle_magic(train, i)\n    \n    # FEATURE SELECTION (USE APPROX 40 OF 255 FEATURES)\n    \n    data = pd.concat([pd.DataFrame(train2[cols]), pd.DataFrame(test2[cols])])\n    #pipe = Pipeline([('vt', VarianceThreshold(threshold=1.5)), ('scaler', StandardScaler())])\n    pipe = Pipeline([('vt', VarianceThreshold(threshold=1.5))])\n   \n    train3 = pipe.fit_transform(train2[cols])\n    test3 = pipe.fit_transform(test2[cols])\n    \n    # We just record in cat_dict how many variables have a threshold above 1.5\n    cat_dict[i] = train3.shape[1]\n    \n    # remove correlations between features\n    svd = PCA(n_components=cat_dict[i])\n    svd.fit(np.vstack((train3, test3)))\n    train3 = svd.transform(train3)\n    test3 = svd.transform(test3)\n        \n\n    ################# First GMM with 3 positive clusters and 3 negative clusters\n    \n    try:\n        score, gm, k_pos, k_neg = fit_multicluster_gmm(x=train3, y=target, xt=test3, k_pos=3, k_neg=3)\n    except:\n        try:\n            print(\"Falling back to k_pos/k_neg=2\")\n            score, gm, k_pos, k_neg = fit_multicluster_gmm(x=train3, y=target, xt=test3, k_pos=2, k_neg=2)\n        except:\n            print(\"Falling back to k_pos/k_neg=1\")\n            score, gm, k_pos, k_neg = fit_multicluster_gmm(x=train3, y=target, xt=test3, k_pos=1, k_neg=1)\n    \n    clusters = gm.predict_proba(train3).shape[1]\n    oof_gmm[idx1] = np.sum(gm.predict_proba(train3)[:,:clusters//2], axis=1)\n    test_preds_gmm[idx2] += np.sum(gm.predict_proba(test3)[:,:clusters//2], axis=1)\n    \n    ################# Try data augmentation and second GMM with 3 positive clusters and 3 negative clusters\n    \n    #train3, target = data_augmentation(train3, target, gm_means=gm.means_, gm_dist=gm.predict(train3))\n    \n    try:\n        score, gm, k_pos, k_neg = fit_multicluster_gmm(x=train3, y=target, xt=test3, k_pos=1, k_neg=3)\n    except:\n        try:\n            print(\"Falling back to k_pos/k_neg=2\")\n            score, gm, k_pos, k_neg = fit_multicluster_gmm(x=train3, y=target, xt=test3, k_pos=1, k_neg=2)\n        except:\n            print(\"Falling back to k_pos/k_neg=1\")\n            score, gm, k_pos, k_neg = fit_multicluster_gmm(x=train3, y=target, xt=test3, k_pos=1, k_neg=1)\n    \n    clusters = gm.predict_proba(train3).shape[1]\n    oof_gmm_2[idx1] = gm.predict_proba(train3)[:,0]\n    test_preds_gmm_2[idx2] += gm.predict_proba(test3)[:,0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# PRINT CV AUC\noof_auc_gmm = roc_auc_score(train['target'], oof_gmm)\nprint('OOF AUC: =',round(oof_auc_gmm, 5))\n\noof_auc_gmm_2 = roc_auc_score(train['target'], oof_gmm_2)\nprint('OOF AUC: =',round(oof_auc_gmm_2, 5))\n\noof_auc_blend = roc_auc_score(train['target'], (oof_gmm+oof_gmm_2)/2)\nprint('OOF AUC: =',round(oof_auc_blend, 5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv('../input/sample_submission.csv')\n\nsub['target'] = test_preds_gmm\nsub.to_csv('submission_gmm.csv',index=False)\n\nsub['target'] = test_preds_gmm_2\nsub.to_csv('submission_gmm_2.csv',index=False)\n\nsub['target'] = (test_preds_gmm + test_preds_gmm_2)/2\nsub.to_csv('submission_blend.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.hist(test_preds_gmm, bins=100)\nplt.title('Final Test.csv predictions')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(test_preds_gmm_2, bins=100)\nplt.title('Final Test.csv predictions')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist((test_preds_gmm + test_preds)/2, bins=100)\nplt.title('Final Test.csv predictions')\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}