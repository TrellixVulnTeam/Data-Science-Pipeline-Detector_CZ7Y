{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Dependencies"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"import warnings\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom keras import optimizers\nfrom keras.models import Sequential\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom keras.layers import Dense, Dropout, BatchNormalization, Activation\n\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, roc_auc_score\n\n# Set seeds to make the experiment more reproducible.\nfrom tensorflow import set_random_seed\nfrom numpy.random import seed\nset_random_seed(0)\nseed(0)\n\n%matplotlib inline\nsns.set(style=\"whitegrid\")\nwarnings.filterwarnings(\"ignore\")","execution_count":120,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load data"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')","execution_count":121,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print('Train set shape:', train.shape)\nprint('Test set shape:', test.shape)\nprint('Train set overview:')\ndisplay(train.head())","execution_count":122,"outputs":[{"output_type":"stream","text":"Train set shape: (262144, 258)\nTest set shape: (131073, 257)\nTrain set overview:\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"                                 id   ...    target\n0  707b395ecdcbb4dc2eabea00e4d1b179   ...         0\n1  5880c03c6582a7b42248668e56b4bdec   ...         0\n2  4ccbcb3d13e5072ff1d9c61afe2c4f77   ...         1\n3  e350f17a357f12a1941f0837afb7eb8d   ...         0\n4  a8f910ea6075b6376af079055965ff68   ...         0\n\n[5 rows x 258 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>muggy-smalt-axolotl-pembus</th>\n      <th>dorky-peach-sheepdog-ordinal</th>\n      <th>slimy-seashell-cassowary-goose</th>\n      <th>snazzy-harlequin-chicken-distraction</th>\n      <th>frumpy-smalt-mau-ordinal</th>\n      <th>stealthy-beige-pinscher-golden</th>\n      <th>chummy-cream-tarantula-entropy</th>\n      <th>hazy-emerald-cuttlefish-unsorted</th>\n      <th>nerdy-indigo-wolfhound-sorted</th>\n      <th>leaky-amaranth-lizard-sorted</th>\n      <th>ugly-tangerine-chihuahua-important</th>\n      <th>shaggy-silver-indri-fimbus</th>\n      <th>flaky-chocolate-beetle-grandmaster</th>\n      <th>squirrely-harlequin-sheep-sumble</th>\n      <th>freaky-tan-angelfish-noise</th>\n      <th>lousy-plum-penguin-sumble</th>\n      <th>bluesy-rose-wallaby-discard</th>\n      <th>baggy-copper-oriole-dummy</th>\n      <th>stealthy-scarlet-hound-fepid</th>\n      <th>greasy-cinnamon-bonobo-contributor</th>\n      <th>cranky-cardinal-dogfish-ordinal</th>\n      <th>snippy-auburn-vole-learn</th>\n      <th>greasy-sepia-coral-dataset</th>\n      <th>flabby-tangerine-fowl-entropy</th>\n      <th>lousy-smalt-pinscher-dummy</th>\n      <th>bluesy-brass-chihuahua-distraction</th>\n      <th>goopy-eggplant-indri-entropy</th>\n      <th>homey-sepia-bombay-sorted</th>\n      <th>homely-ruby-bulldog-entropy</th>\n      <th>hasty-blue-sheep-contributor</th>\n      <th>blurry-wisteria-oyster-master</th>\n      <th>snoopy-auburn-dogfish-expert</th>\n      <th>stinky-maroon-blue-kernel</th>\n      <th>bumpy-amaranth-armadillo-important</th>\n      <th>slaphappy-peach-oyster-master</th>\n      <th>dorky-tomato-ragdoll-dataset</th>\n      <th>messy-mauve-wolverine-ordinal</th>\n      <th>geeky-pumpkin-moorhen-important</th>\n      <th>crabby-teal-otter-unsorted</th>\n      <th>...</th>\n      <th>beady-mauve-frog-distraction</th>\n      <th>surly-brass-maltese-ordinal</th>\n      <th>beady-asparagus-opossum-expert</th>\n      <th>beady-rust-impala-dummy</th>\n      <th>droopy-amethyst-dachshund-hint</th>\n      <th>homey-crimson-budgerigar-grandmaster</th>\n      <th>droopy-cardinal-impala-important</th>\n      <th>woozy-apricot-moose-hint</th>\n      <th>paltry-sapphire-labradoodle-dummy</th>\n      <th>crappy-carmine-eagle-entropy</th>\n      <th>greasy-magnolia-spider-grandmaster</th>\n      <th>crabby-carmine-flounder-sorted</th>\n      <th>skimpy-copper-fowl-grandmaster</th>\n      <th>hasty-seashell-woodpecker-hint</th>\n      <th>snappy-purple-bobcat-important</th>\n      <th>thirsty-carmine-corgi-ordinal</th>\n      <th>homely-auburn-reindeer-unsorted</th>\n      <th>crappy-beige-tiger-fepid</th>\n      <th>cranky-auburn-swan-novice</th>\n      <th>chewy-bistre-buzzard-expert</th>\n      <th>skinny-cyan-macaque-pembus</th>\n      <th>slimy-periwinkle-otter-expert</th>\n      <th>snazzy-burgundy-clam-novice</th>\n      <th>cozy-ochre-gorilla-gaussian</th>\n      <th>homey-sangria-wolfhound-dummy</th>\n      <th>snazzy-asparagus-hippopotamus-contributor</th>\n      <th>paltry-red-hamster-sorted</th>\n      <th>zippy-dandelion-insect-golden</th>\n      <th>baggy-coral-bandicoot-unsorted</th>\n      <th>goopy-lavender-wolverine-fimbus</th>\n      <th>wheezy-myrtle-mandrill-entropy</th>\n      <th>wiggy-lilac-lemming-sorted</th>\n      <th>gloppy-cerise-snail-contributor</th>\n      <th>woozy-silver-havanese-gaussian</th>\n      <th>jumpy-thistle-discus-sorted</th>\n      <th>muggy-turquoise-donkey-important</th>\n      <th>blurry-buff-hyena-entropy</th>\n      <th>bluesy-chocolate-kudu-fepid</th>\n      <th>gamy-white-monster-expert</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>707b395ecdcbb4dc2eabea00e4d1b179</td>\n      <td>-2.070654</td>\n      <td>1.018160</td>\n      <td>0.228643</td>\n      <td>0.857221</td>\n      <td>0.052271</td>\n      <td>0.230303</td>\n      <td>-6.385090</td>\n      <td>0.439369</td>\n      <td>-0.721946</td>\n      <td>-0.227027</td>\n      <td>0.575964</td>\n      <td>1.541908</td>\n      <td>1.745286</td>\n      <td>-0.624271</td>\n      <td>3.600958</td>\n      <td>1.176489</td>\n      <td>-0.182776</td>\n      <td>-0.228391</td>\n      <td>1.682263</td>\n      <td>-0.833236</td>\n      <td>-4.377688</td>\n      <td>-5.372410</td>\n      <td>-0.477742</td>\n      <td>-0.179005</td>\n      <td>-0.516475</td>\n      <td>0.127391</td>\n      <td>-0.857591</td>\n      <td>-0.461500</td>\n      <td>2.160303</td>\n      <td>-2.118371</td>\n      <td>0.515493</td>\n      <td>-1.201493</td>\n      <td>-0.027377</td>\n      <td>-1.154024</td>\n      <td>0.753204</td>\n      <td>-0.179651</td>\n      <td>-0.807341</td>\n      <td>-1.663626</td>\n      <td>0.893806</td>\n      <td>...</td>\n      <td>-1.829848</td>\n      <td>2.347131</td>\n      <td>0.082462</td>\n      <td>-1.012654</td>\n      <td>0.593752</td>\n      <td>2.904654</td>\n      <td>-0.428974</td>\n      <td>-0.919979</td>\n      <td>2.849575</td>\n      <td>-0.906744</td>\n      <td>0.729459</td>\n      <td>0.386140</td>\n      <td>0.319814</td>\n      <td>-0.407682</td>\n      <td>-0.170667</td>\n      <td>-1.242919</td>\n      <td>-1.719046</td>\n      <td>-0.132395</td>\n      <td>-0.368991</td>\n      <td>-5.112553</td>\n      <td>-2.085988</td>\n      <td>-0.897257</td>\n      <td>1.080671</td>\n      <td>-0.273262</td>\n      <td>0.342824</td>\n      <td>0.640177</td>\n      <td>-0.415298</td>\n      <td>-0.483126</td>\n      <td>-0.080799</td>\n      <td>2.416224</td>\n      <td>0.351895</td>\n      <td>0.618824</td>\n      <td>-1.542423</td>\n      <td>0.598175</td>\n      <td>0.611757</td>\n      <td>0.678772</td>\n      <td>0.247059</td>\n      <td>-0.806677</td>\n      <td>-0.193649</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5880c03c6582a7b42248668e56b4bdec</td>\n      <td>-0.491702</td>\n      <td>0.082645</td>\n      <td>-0.011193</td>\n      <td>1.071266</td>\n      <td>-0.346347</td>\n      <td>-0.082209</td>\n      <td>0.110579</td>\n      <td>-0.382374</td>\n      <td>-0.229620</td>\n      <td>0.783980</td>\n      <td>-1.280579</td>\n      <td>-1.003480</td>\n      <td>-7.753201</td>\n      <td>-1.320547</td>\n      <td>0.919078</td>\n      <td>-1.036068</td>\n      <td>0.030213</td>\n      <td>0.910172</td>\n      <td>-0.905345</td>\n      <td>0.646641</td>\n      <td>-0.465291</td>\n      <td>-0.531735</td>\n      <td>-0.756781</td>\n      <td>0.193724</td>\n      <td>0.224277</td>\n      <td>-0.474412</td>\n      <td>1.885805</td>\n      <td>0.205439</td>\n      <td>-6.481422</td>\n      <td>1.035620</td>\n      <td>-0.453623</td>\n      <td>0.375936</td>\n      <td>-0.320670</td>\n      <td>-0.144646</td>\n      <td>-0.220129</td>\n      <td>0.577826</td>\n      <td>-0.360512</td>\n      <td>-0.600107</td>\n      <td>0.008111</td>\n      <td>...</td>\n      <td>0.982205</td>\n      <td>-1.161978</td>\n      <td>0.532269</td>\n      <td>1.133215</td>\n      <td>0.003503</td>\n      <td>-1.390962</td>\n      <td>0.158572</td>\n      <td>0.143794</td>\n      <td>-0.317185</td>\n      <td>1.017192</td>\n      <td>-0.395342</td>\n      <td>-0.642357</td>\n      <td>-0.627209</td>\n      <td>0.257271</td>\n      <td>-1.461564</td>\n      <td>0.325613</td>\n      <td>1.628369</td>\n      <td>0.640040</td>\n      <td>0.750735</td>\n      <td>1.164573</td>\n      <td>0.900373</td>\n      <td>0.063489</td>\n      <td>0.948158</td>\n      <td>0.273014</td>\n      <td>-1.269147</td>\n      <td>-0.251101</td>\n      <td>-2.271731</td>\n      <td>-0.044167</td>\n      <td>-0.443766</td>\n      <td>-1.144794</td>\n      <td>-0.645115</td>\n      <td>-1.246090</td>\n      <td>2.613357</td>\n      <td>-0.479664</td>\n      <td>1.581289</td>\n      <td>0.931258</td>\n      <td>0.151937</td>\n      <td>-0.766595</td>\n      <td>0.474351</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4ccbcb3d13e5072ff1d9c61afe2c4f77</td>\n      <td>-1.680473</td>\n      <td>0.860529</td>\n      <td>-1.076195</td>\n      <td>0.740124</td>\n      <td>3.678445</td>\n      <td>0.288558</td>\n      <td>0.515875</td>\n      <td>0.920590</td>\n      <td>-1.223277</td>\n      <td>-1.029780</td>\n      <td>-2.203397</td>\n      <td>-7.088717</td>\n      <td>0.438218</td>\n      <td>-0.848173</td>\n      <td>1.542666</td>\n      <td>-2.166858</td>\n      <td>-0.867670</td>\n      <td>-0.980947</td>\n      <td>0.567793</td>\n      <td>1.323430</td>\n      <td>-2.076700</td>\n      <td>-0.291598</td>\n      <td>-1.564816</td>\n      <td>-8.718695</td>\n      <td>0.340144</td>\n      <td>-0.566402</td>\n      <td>0.844324</td>\n      <td>0.816421</td>\n      <td>-1.019114</td>\n      <td>-0.881431</td>\n      <td>-2.285710</td>\n      <td>-0.090958</td>\n      <td>-0.898440</td>\n      <td>-0.584417</td>\n      <td>-0.143660</td>\n      <td>-0.182084</td>\n      <td>0.798516</td>\n      <td>0.010756</td>\n      <td>-0.347155</td>\n      <td>...</td>\n      <td>0.829467</td>\n      <td>0.588236</td>\n      <td>0.427946</td>\n      <td>-0.563037</td>\n      <td>-0.103990</td>\n      <td>-0.817698</td>\n      <td>1.251046</td>\n      <td>-0.977157</td>\n      <td>2.732600</td>\n      <td>1.997984</td>\n      <td>-0.214285</td>\n      <td>-0.389428</td>\n      <td>-1.007633</td>\n      <td>0.336435</td>\n      <td>-0.851292</td>\n      <td>-0.024184</td>\n      <td>0.455908</td>\n      <td>0.458753</td>\n      <td>-0.267230</td>\n      <td>-2.032402</td>\n      <td>0.203082</td>\n      <td>0.654107</td>\n      <td>-3.512338</td>\n      <td>-0.840937</td>\n      <td>0.519407</td>\n      <td>-0.028053</td>\n      <td>-1.621083</td>\n      <td>0.142132</td>\n      <td>1.514664</td>\n      <td>0.828815</td>\n      <td>0.516422</td>\n      <td>0.130521</td>\n      <td>-0.459210</td>\n      <td>2.028205</td>\n      <td>-0.093968</td>\n      <td>-0.218274</td>\n      <td>-0.163136</td>\n      <td>-0.870289</td>\n      <td>0.064038</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>e350f17a357f12a1941f0837afb7eb8d</td>\n      <td>0.183774</td>\n      <td>0.919134</td>\n      <td>-0.946958</td>\n      <td>0.918492</td>\n      <td>0.862278</td>\n      <td>1.155287</td>\n      <td>0.911106</td>\n      <td>0.562598</td>\n      <td>-1.349685</td>\n      <td>-1.182729</td>\n      <td>0.003159</td>\n      <td>-0.626847</td>\n      <td>0.368980</td>\n      <td>1.560784</td>\n      <td>0.502851</td>\n      <td>-0.108050</td>\n      <td>0.633208</td>\n      <td>-0.411502</td>\n      <td>-3.201592</td>\n      <td>-0.710612</td>\n      <td>0.786816</td>\n      <td>0.500979</td>\n      <td>-1.040048</td>\n      <td>-1.369170</td>\n      <td>0.987666</td>\n      <td>-0.681838</td>\n      <td>-0.331372</td>\n      <td>2.254289</td>\n      <td>-0.009330</td>\n      <td>2.007067</td>\n      <td>1.203750</td>\n      <td>-2.003928</td>\n      <td>-0.566088</td>\n      <td>0.223452</td>\n      <td>0.434202</td>\n      <td>-1.203766</td>\n      <td>-0.103490</td>\n      <td>0.441111</td>\n      <td>1.818458</td>\n      <td>...</td>\n      <td>-2.231836</td>\n      <td>0.833236</td>\n      <td>-0.454226</td>\n      <td>-1.614694</td>\n      <td>0.159948</td>\n      <td>-0.150059</td>\n      <td>-1.570599</td>\n      <td>0.960839</td>\n      <td>0.102214</td>\n      <td>0.077236</td>\n      <td>0.852834</td>\n      <td>-1.265608</td>\n      <td>-3.219190</td>\n      <td>0.251194</td>\n      <td>0.215861</td>\n      <td>-0.009520</td>\n      <td>1.611203</td>\n      <td>1.679806</td>\n      <td>-0.008419</td>\n      <td>0.658384</td>\n      <td>-0.132437</td>\n      <td>-1.466823</td>\n      <td>-1.577080</td>\n      <td>-0.800346</td>\n      <td>1.960795</td>\n      <td>-4.042900</td>\n      <td>1.722143</td>\n      <td>-0.261888</td>\n      <td>-1.145005</td>\n      <td>-1.864582</td>\n      <td>-1.168967</td>\n      <td>1.385089</td>\n      <td>-0.353028</td>\n      <td>3.316150</td>\n      <td>-0.524087</td>\n      <td>-0.794327</td>\n      <td>3.936365</td>\n      <td>0.682989</td>\n      <td>-2.521211</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>a8f910ea6075b6376af079055965ff68</td>\n      <td>-0.203933</td>\n      <td>-0.177252</td>\n      <td>0.368074</td>\n      <td>-0.701320</td>\n      <td>-1.104391</td>\n      <td>0.735760</td>\n      <td>0.894273</td>\n      <td>-1.375826</td>\n      <td>-5.144946</td>\n      <td>-2.048711</td>\n      <td>0.629773</td>\n      <td>-4.252669</td>\n      <td>-0.087420</td>\n      <td>-0.794367</td>\n      <td>-1.063963</td>\n      <td>0.115997</td>\n      <td>0.895180</td>\n      <td>3.184848</td>\n      <td>2.057840</td>\n      <td>-0.950821</td>\n      <td>0.961059</td>\n      <td>-1.837828</td>\n      <td>-0.437156</td>\n      <td>-0.828433</td>\n      <td>0.373747</td>\n      <td>-0.099787</td>\n      <td>-0.976280</td>\n      <td>-0.165921</td>\n      <td>3.297221</td>\n      <td>3.914132</td>\n      <td>-4.971376</td>\n      <td>-0.286520</td>\n      <td>-0.160133</td>\n      <td>-3.301453</td>\n      <td>-1.021032</td>\n      <td>-0.562744</td>\n      <td>0.574065</td>\n      <td>-0.368194</td>\n      <td>-0.507458</td>\n      <td>...</td>\n      <td>0.178099</td>\n      <td>-0.410396</td>\n      <td>-1.184236</td>\n      <td>1.681727</td>\n      <td>0.589606</td>\n      <td>0.064222</td>\n      <td>0.258885</td>\n      <td>0.560241</td>\n      <td>-1.545597</td>\n      <td>0.822283</td>\n      <td>1.518209</td>\n      <td>0.460143</td>\n      <td>0.822488</td>\n      <td>1.362718</td>\n      <td>0.218560</td>\n      <td>-1.038514</td>\n      <td>1.000763</td>\n      <td>-0.975878</td>\n      <td>-0.551268</td>\n      <td>-0.133044</td>\n      <td>-0.393092</td>\n      <td>1.236473</td>\n      <td>1.657100</td>\n      <td>0.833020</td>\n      <td>0.665379</td>\n      <td>-0.900025</td>\n      <td>0.291908</td>\n      <td>0.482727</td>\n      <td>0.552399</td>\n      <td>0.970496</td>\n      <td>-0.279168</td>\n      <td>1.544356</td>\n      <td>2.959727</td>\n      <td>1.641201</td>\n      <td>-0.130818</td>\n      <td>-0.264292</td>\n      <td>-0.748668</td>\n      <td>0.964218</td>\n      <td>0.087079</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"On both our train and test sets all columns are numerical, and we also don't have any missing data."},{"metadata":{},"cell_type":"markdown","source":"# EDA"},{"metadata":{},"cell_type":"markdown","source":"## Target distribution"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(6, 6))\nax = sns.countplot(x=\"target\", data=train, label=\"Label count\")\nsns.despine(bottom=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Also we have a balanced target distribution.\n\n## Feature distribution"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def plot_distribution():\n    f, axes = plt.subplots(1, 3, figsize=(20, 8), sharex=True)\n    for feature in train.columns[1:31]:\n        sns.distplot(train[feature], ax=axes[0], axlabel='First 30 features').set_title(\"Complete set\")\n        sns.distplot(train[train['target']==1][feature], ax=axes[1], axlabel='First 30 features').set_title(\"target = 1\")\n        sns.distplot(train[train['target']==0][feature], ax=axes[2], axlabel='First 30 features').set_title(\"target = 0\")\n    sns.despine(left=True)\n    plt.tight_layout()\n\nplot_distribution()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The features seems to be normalized.\n\n## Process data for model\n\n### Turn \"wheezy-copper-turtle-magic\" into a categorical feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.concat([train, pd.get_dummies(train['wheezy-copper-turtle-magic'], prefix='magic', drop_first=True)], axis=1).drop(['wheezy-copper-turtle-magic'], axis=1)\ntest = pd.concat([test, pd.get_dummies(test['wheezy-copper-turtle-magic'], prefix='magic', drop_first=True)], axis=1).drop(['wheezy-copper-turtle-magic'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train/validation random split (80% train / 20% validation)"},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = train['target']\ntrain.drop('target', axis=1, inplace=True)\ntrain.drop('id', axis=1, inplace=True)\ntest.drop('id', axis=1, inplace=True)\n\nX_train, X_val, Y_train, Y_val = train_test_split(train, labels, test_size=0.2, random_state=1)","execution_count":123,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Normalize data using MinMaxScaler"},{"metadata":{"trusted":true},"cell_type":"code","source":"non_cat_features = list(train.filter(regex='^(?!magic_)'))\nscaler = MinMaxScaler()\nX_train[non_cat_features] = scaler.fit_transform(X_train[non_cat_features])\nX_val[non_cat_features] = scaler.transform(X_val[non_cat_features])\ntest[non_cat_features] = scaler.transform(test[non_cat_features])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model\n\n## Model parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 128\nEPOCHS = 30\nLEARNING_RATE = 0.01\nES_PATIENCE = 5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Dense(1024, input_dim=X_train.shape[1]))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(256))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, activation=\"sigmoid\"))\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=ES_PATIENCE)\ncallback_list = [es]\n\noptimizer = optimizers.Adam(lr=0.0001)\nmodel.compile(optimizer=optimizer, loss=\"binary_crossentropy\",  metrics=['binary_accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"history = model.fit(X_train, Y_train, validation_data=(X_val, Y_val), \n                    callbacks=callback_list, \n                    epochs=EPOCHS, \n                    batch_size=BATCH_SIZE, \n                    verbose=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model graph loss"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"sns.set_style(\"whitegrid\")\nfig, (ax1, ax2) = plt.subplots(1, 2, sharex='col', figsize=(20, 7))\n\nax1.plot(history.history['loss'], label='Train loss')\nax1.plot(history.history['val_loss'], label='Validation loss')\nax1.legend(loc='best')\nax1.set_title('Loss')\n\nax2.plot(history.history['binary_accuracy'], label='Train Accuracy')\nax2.plot(history.history['val_binary_accuracy'], label='Validation accuracy')\nax2.legend(loc='best')\nax2.set_title('Accuracy')\n\nplt.xlabel('Epochs')\nsns.despine()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model evaluation\n\n## Confusion matrix"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train_pred = model.predict_classes(X_train)\nval_pred = model.predict_classes(X_val)\n\nf, axes = plt.subplots(1, 2, figsize=(16, 5), sharex=True)\ntrain_cnf_matrix = confusion_matrix(Y_train, train_pred)\nval_cnf_matrix = confusion_matrix(Y_val, val_pred)\n\ntrain_cnf_matrix_norm = train_cnf_matrix / train_cnf_matrix.sum(axis=1)[:, np.newaxis]\nval_cnf_matrix_norm = val_cnf_matrix / val_cnf_matrix.sum(axis=1)[:, np.newaxis]\n\ntrain_df_cm = pd.DataFrame(train_cnf_matrix_norm, index=[0, 1], columns=[0, 1])\nval_df_cm = pd.DataFrame(val_cnf_matrix_norm, index=[0, 1], columns=[0, 1])\n\nsns.heatmap(train_df_cm, annot=True, fmt='.2f', cmap=\"Blues\", ax=axes[0]).set_title(\"Train\")\nsns.heatmap(val_df_cm, annot=True, fmt='.2f', cmap=\"Blues\", ax=axes[1]).set_title(\"Validation\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Metrics ROC AUC"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print('Train AUC %.2f' % roc_auc_score(Y_train.values, train_pred))\nprint('Validation AUC %.2f' % roc_auc_score(Y_val.values, val_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Test predictions"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"predictions = model.predict(test)\ndf = pd.read_csv('../input/sample_submission.csv')\ndf['target'] = predictions\ndf.to_csv('submission.csv', index=False)\ndf.head(10)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}