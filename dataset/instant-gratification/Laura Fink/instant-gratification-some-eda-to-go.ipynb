{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Table of contents\n\n1. Prepare to start\n2. Sneak a peek\n    * Take a look\n    * Size of the data\n    * Dtypes\n    * A magic feature?\n    * Missing values\n3. Basic Exploratory Analysis\n    * Feature correlations\n    * Distances between class medians\n    * What logistic regression wants to tell us\n    * The magic turtle again \n    * What about the column names?"},{"metadata":{},"cell_type":"markdown","source":"## Prepare to start"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nsns.set()\n\nimport os\nprint(os.listdir(\"../input\"))\n\nfrom sklearn.model_selection import RepeatedStratifiedKFold, train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import StandardScaler\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go","execution_count":2,"outputs":[{"output_type":"stream","text":"['train.csv', 'sample_submission.csv', 'test.csv']\n","name":"stdout"},{"output_type":"display_data","data":{"text/html":"        <script type=\"text/javascript\">\n        window.PlotlyConfig = {MathJaxConfig: 'local'};\n        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n        if (typeof require !== 'undefined') {\n        require.undef(\"plotly\");\n        requirejs.config({\n            paths: {\n                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n            }\n        });\n        require(['plotly'], function(Plotly) {\n            window._Plotly = Plotly;\n        });\n        }\n        </script>\n        "},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Sneak a peek"},{"metadata":{},"cell_type":"markdown","source":"### Take a look"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/train.csv\")\ntrain.head()","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"                                 id   ...    target\n0  707b395ecdcbb4dc2eabea00e4d1b179   ...         0\n1  5880c03c6582a7b42248668e56b4bdec   ...         0\n2  4ccbcb3d13e5072ff1d9c61afe2c4f77   ...         1\n3  e350f17a357f12a1941f0837afb7eb8d   ...         0\n4  a8f910ea6075b6376af079055965ff68   ...         0\n\n[5 rows x 258 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>muggy-smalt-axolotl-pembus</th>\n      <th>dorky-peach-sheepdog-ordinal</th>\n      <th>slimy-seashell-cassowary-goose</th>\n      <th>snazzy-harlequin-chicken-distraction</th>\n      <th>frumpy-smalt-mau-ordinal</th>\n      <th>stealthy-beige-pinscher-golden</th>\n      <th>chummy-cream-tarantula-entropy</th>\n      <th>hazy-emerald-cuttlefish-unsorted</th>\n      <th>nerdy-indigo-wolfhound-sorted</th>\n      <th>leaky-amaranth-lizard-sorted</th>\n      <th>ugly-tangerine-chihuahua-important</th>\n      <th>shaggy-silver-indri-fimbus</th>\n      <th>flaky-chocolate-beetle-grandmaster</th>\n      <th>squirrely-harlequin-sheep-sumble</th>\n      <th>freaky-tan-angelfish-noise</th>\n      <th>lousy-plum-penguin-sumble</th>\n      <th>bluesy-rose-wallaby-discard</th>\n      <th>baggy-copper-oriole-dummy</th>\n      <th>stealthy-scarlet-hound-fepid</th>\n      <th>greasy-cinnamon-bonobo-contributor</th>\n      <th>cranky-cardinal-dogfish-ordinal</th>\n      <th>snippy-auburn-vole-learn</th>\n      <th>greasy-sepia-coral-dataset</th>\n      <th>flabby-tangerine-fowl-entropy</th>\n      <th>lousy-smalt-pinscher-dummy</th>\n      <th>bluesy-brass-chihuahua-distraction</th>\n      <th>goopy-eggplant-indri-entropy</th>\n      <th>homey-sepia-bombay-sorted</th>\n      <th>homely-ruby-bulldog-entropy</th>\n      <th>hasty-blue-sheep-contributor</th>\n      <th>blurry-wisteria-oyster-master</th>\n      <th>snoopy-auburn-dogfish-expert</th>\n      <th>stinky-maroon-blue-kernel</th>\n      <th>bumpy-amaranth-armadillo-important</th>\n      <th>slaphappy-peach-oyster-master</th>\n      <th>dorky-tomato-ragdoll-dataset</th>\n      <th>messy-mauve-wolverine-ordinal</th>\n      <th>geeky-pumpkin-moorhen-important</th>\n      <th>crabby-teal-otter-unsorted</th>\n      <th>...</th>\n      <th>beady-mauve-frog-distraction</th>\n      <th>surly-brass-maltese-ordinal</th>\n      <th>beady-asparagus-opossum-expert</th>\n      <th>beady-rust-impala-dummy</th>\n      <th>droopy-amethyst-dachshund-hint</th>\n      <th>homey-crimson-budgerigar-grandmaster</th>\n      <th>droopy-cardinal-impala-important</th>\n      <th>woozy-apricot-moose-hint</th>\n      <th>paltry-sapphire-labradoodle-dummy</th>\n      <th>crappy-carmine-eagle-entropy</th>\n      <th>greasy-magnolia-spider-grandmaster</th>\n      <th>crabby-carmine-flounder-sorted</th>\n      <th>skimpy-copper-fowl-grandmaster</th>\n      <th>hasty-seashell-woodpecker-hint</th>\n      <th>snappy-purple-bobcat-important</th>\n      <th>thirsty-carmine-corgi-ordinal</th>\n      <th>homely-auburn-reindeer-unsorted</th>\n      <th>crappy-beige-tiger-fepid</th>\n      <th>cranky-auburn-swan-novice</th>\n      <th>chewy-bistre-buzzard-expert</th>\n      <th>skinny-cyan-macaque-pembus</th>\n      <th>slimy-periwinkle-otter-expert</th>\n      <th>snazzy-burgundy-clam-novice</th>\n      <th>cozy-ochre-gorilla-gaussian</th>\n      <th>homey-sangria-wolfhound-dummy</th>\n      <th>snazzy-asparagus-hippopotamus-contributor</th>\n      <th>paltry-red-hamster-sorted</th>\n      <th>zippy-dandelion-insect-golden</th>\n      <th>baggy-coral-bandicoot-unsorted</th>\n      <th>goopy-lavender-wolverine-fimbus</th>\n      <th>wheezy-myrtle-mandrill-entropy</th>\n      <th>wiggy-lilac-lemming-sorted</th>\n      <th>gloppy-cerise-snail-contributor</th>\n      <th>woozy-silver-havanese-gaussian</th>\n      <th>jumpy-thistle-discus-sorted</th>\n      <th>muggy-turquoise-donkey-important</th>\n      <th>blurry-buff-hyena-entropy</th>\n      <th>bluesy-chocolate-kudu-fepid</th>\n      <th>gamy-white-monster-expert</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>707b395ecdcbb4dc2eabea00e4d1b179</td>\n      <td>-2.070654</td>\n      <td>1.018160</td>\n      <td>0.228643</td>\n      <td>0.857221</td>\n      <td>0.052271</td>\n      <td>0.230303</td>\n      <td>-6.385090</td>\n      <td>0.439369</td>\n      <td>-0.721946</td>\n      <td>-0.227027</td>\n      <td>0.575964</td>\n      <td>1.541908</td>\n      <td>1.745286</td>\n      <td>-0.624271</td>\n      <td>3.600958</td>\n      <td>1.176489</td>\n      <td>-0.182776</td>\n      <td>-0.228391</td>\n      <td>1.682263</td>\n      <td>-0.833236</td>\n      <td>-4.377688</td>\n      <td>-5.372410</td>\n      <td>-0.477742</td>\n      <td>-0.179005</td>\n      <td>-0.516475</td>\n      <td>0.127391</td>\n      <td>-0.857591</td>\n      <td>-0.461500</td>\n      <td>2.160303</td>\n      <td>-2.118371</td>\n      <td>0.515493</td>\n      <td>-1.201493</td>\n      <td>-0.027377</td>\n      <td>-1.154024</td>\n      <td>0.753204</td>\n      <td>-0.179651</td>\n      <td>-0.807341</td>\n      <td>-1.663626</td>\n      <td>0.893806</td>\n      <td>...</td>\n      <td>-1.829848</td>\n      <td>2.347131</td>\n      <td>0.082462</td>\n      <td>-1.012654</td>\n      <td>0.593752</td>\n      <td>2.904654</td>\n      <td>-0.428974</td>\n      <td>-0.919979</td>\n      <td>2.849575</td>\n      <td>-0.906744</td>\n      <td>0.729459</td>\n      <td>0.386140</td>\n      <td>0.319814</td>\n      <td>-0.407682</td>\n      <td>-0.170667</td>\n      <td>-1.242919</td>\n      <td>-1.719046</td>\n      <td>-0.132395</td>\n      <td>-0.368991</td>\n      <td>-5.112553</td>\n      <td>-2.085988</td>\n      <td>-0.897257</td>\n      <td>1.080671</td>\n      <td>-0.273262</td>\n      <td>0.342824</td>\n      <td>0.640177</td>\n      <td>-0.415298</td>\n      <td>-0.483126</td>\n      <td>-0.080799</td>\n      <td>2.416224</td>\n      <td>0.351895</td>\n      <td>0.618824</td>\n      <td>-1.542423</td>\n      <td>0.598175</td>\n      <td>0.611757</td>\n      <td>0.678772</td>\n      <td>0.247059</td>\n      <td>-0.806677</td>\n      <td>-0.193649</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5880c03c6582a7b42248668e56b4bdec</td>\n      <td>-0.491702</td>\n      <td>0.082645</td>\n      <td>-0.011193</td>\n      <td>1.071266</td>\n      <td>-0.346347</td>\n      <td>-0.082209</td>\n      <td>0.110579</td>\n      <td>-0.382374</td>\n      <td>-0.229620</td>\n      <td>0.783980</td>\n      <td>-1.280579</td>\n      <td>-1.003480</td>\n      <td>-7.753201</td>\n      <td>-1.320547</td>\n      <td>0.919078</td>\n      <td>-1.036068</td>\n      <td>0.030213</td>\n      <td>0.910172</td>\n      <td>-0.905345</td>\n      <td>0.646641</td>\n      <td>-0.465291</td>\n      <td>-0.531735</td>\n      <td>-0.756781</td>\n      <td>0.193724</td>\n      <td>0.224277</td>\n      <td>-0.474412</td>\n      <td>1.885805</td>\n      <td>0.205439</td>\n      <td>-6.481422</td>\n      <td>1.035620</td>\n      <td>-0.453623</td>\n      <td>0.375936</td>\n      <td>-0.320670</td>\n      <td>-0.144646</td>\n      <td>-0.220129</td>\n      <td>0.577826</td>\n      <td>-0.360512</td>\n      <td>-0.600107</td>\n      <td>0.008111</td>\n      <td>...</td>\n      <td>0.982205</td>\n      <td>-1.161978</td>\n      <td>0.532269</td>\n      <td>1.133215</td>\n      <td>0.003503</td>\n      <td>-1.390962</td>\n      <td>0.158572</td>\n      <td>0.143794</td>\n      <td>-0.317185</td>\n      <td>1.017192</td>\n      <td>-0.395342</td>\n      <td>-0.642357</td>\n      <td>-0.627209</td>\n      <td>0.257271</td>\n      <td>-1.461564</td>\n      <td>0.325613</td>\n      <td>1.628369</td>\n      <td>0.640040</td>\n      <td>0.750735</td>\n      <td>1.164573</td>\n      <td>0.900373</td>\n      <td>0.063489</td>\n      <td>0.948158</td>\n      <td>0.273014</td>\n      <td>-1.269147</td>\n      <td>-0.251101</td>\n      <td>-2.271731</td>\n      <td>-0.044167</td>\n      <td>-0.443766</td>\n      <td>-1.144794</td>\n      <td>-0.645115</td>\n      <td>-1.246090</td>\n      <td>2.613357</td>\n      <td>-0.479664</td>\n      <td>1.581289</td>\n      <td>0.931258</td>\n      <td>0.151937</td>\n      <td>-0.766595</td>\n      <td>0.474351</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4ccbcb3d13e5072ff1d9c61afe2c4f77</td>\n      <td>-1.680473</td>\n      <td>0.860529</td>\n      <td>-1.076195</td>\n      <td>0.740124</td>\n      <td>3.678445</td>\n      <td>0.288558</td>\n      <td>0.515875</td>\n      <td>0.920590</td>\n      <td>-1.223277</td>\n      <td>-1.029780</td>\n      <td>-2.203397</td>\n      <td>-7.088717</td>\n      <td>0.438218</td>\n      <td>-0.848173</td>\n      <td>1.542666</td>\n      <td>-2.166858</td>\n      <td>-0.867670</td>\n      <td>-0.980947</td>\n      <td>0.567793</td>\n      <td>1.323430</td>\n      <td>-2.076700</td>\n      <td>-0.291598</td>\n      <td>-1.564816</td>\n      <td>-8.718695</td>\n      <td>0.340144</td>\n      <td>-0.566402</td>\n      <td>0.844324</td>\n      <td>0.816421</td>\n      <td>-1.019114</td>\n      <td>-0.881431</td>\n      <td>-2.285710</td>\n      <td>-0.090958</td>\n      <td>-0.898440</td>\n      <td>-0.584417</td>\n      <td>-0.143660</td>\n      <td>-0.182084</td>\n      <td>0.798516</td>\n      <td>0.010756</td>\n      <td>-0.347155</td>\n      <td>...</td>\n      <td>0.829467</td>\n      <td>0.588236</td>\n      <td>0.427946</td>\n      <td>-0.563037</td>\n      <td>-0.103990</td>\n      <td>-0.817698</td>\n      <td>1.251046</td>\n      <td>-0.977157</td>\n      <td>2.732600</td>\n      <td>1.997984</td>\n      <td>-0.214285</td>\n      <td>-0.389428</td>\n      <td>-1.007633</td>\n      <td>0.336435</td>\n      <td>-0.851292</td>\n      <td>-0.024184</td>\n      <td>0.455908</td>\n      <td>0.458753</td>\n      <td>-0.267230</td>\n      <td>-2.032402</td>\n      <td>0.203082</td>\n      <td>0.654107</td>\n      <td>-3.512338</td>\n      <td>-0.840937</td>\n      <td>0.519407</td>\n      <td>-0.028053</td>\n      <td>-1.621083</td>\n      <td>0.142132</td>\n      <td>1.514664</td>\n      <td>0.828815</td>\n      <td>0.516422</td>\n      <td>0.130521</td>\n      <td>-0.459210</td>\n      <td>2.028205</td>\n      <td>-0.093968</td>\n      <td>-0.218274</td>\n      <td>-0.163136</td>\n      <td>-0.870289</td>\n      <td>0.064038</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>e350f17a357f12a1941f0837afb7eb8d</td>\n      <td>0.183774</td>\n      <td>0.919134</td>\n      <td>-0.946958</td>\n      <td>0.918492</td>\n      <td>0.862278</td>\n      <td>1.155287</td>\n      <td>0.911106</td>\n      <td>0.562598</td>\n      <td>-1.349685</td>\n      <td>-1.182729</td>\n      <td>0.003159</td>\n      <td>-0.626847</td>\n      <td>0.368980</td>\n      <td>1.560784</td>\n      <td>0.502851</td>\n      <td>-0.108050</td>\n      <td>0.633208</td>\n      <td>-0.411502</td>\n      <td>-3.201592</td>\n      <td>-0.710612</td>\n      <td>0.786816</td>\n      <td>0.500979</td>\n      <td>-1.040048</td>\n      <td>-1.369170</td>\n      <td>0.987666</td>\n      <td>-0.681838</td>\n      <td>-0.331372</td>\n      <td>2.254289</td>\n      <td>-0.009330</td>\n      <td>2.007067</td>\n      <td>1.203750</td>\n      <td>-2.003928</td>\n      <td>-0.566088</td>\n      <td>0.223452</td>\n      <td>0.434202</td>\n      <td>-1.203766</td>\n      <td>-0.103490</td>\n      <td>0.441111</td>\n      <td>1.818458</td>\n      <td>...</td>\n      <td>-2.231836</td>\n      <td>0.833236</td>\n      <td>-0.454226</td>\n      <td>-1.614694</td>\n      <td>0.159948</td>\n      <td>-0.150059</td>\n      <td>-1.570599</td>\n      <td>0.960839</td>\n      <td>0.102214</td>\n      <td>0.077236</td>\n      <td>0.852834</td>\n      <td>-1.265608</td>\n      <td>-3.219190</td>\n      <td>0.251194</td>\n      <td>0.215861</td>\n      <td>-0.009520</td>\n      <td>1.611203</td>\n      <td>1.679806</td>\n      <td>-0.008419</td>\n      <td>0.658384</td>\n      <td>-0.132437</td>\n      <td>-1.466823</td>\n      <td>-1.577080</td>\n      <td>-0.800346</td>\n      <td>1.960795</td>\n      <td>-4.042900</td>\n      <td>1.722143</td>\n      <td>-0.261888</td>\n      <td>-1.145005</td>\n      <td>-1.864582</td>\n      <td>-1.168967</td>\n      <td>1.385089</td>\n      <td>-0.353028</td>\n      <td>3.316150</td>\n      <td>-0.524087</td>\n      <td>-0.794327</td>\n      <td>3.936365</td>\n      <td>0.682989</td>\n      <td>-2.521211</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>a8f910ea6075b6376af079055965ff68</td>\n      <td>-0.203933</td>\n      <td>-0.177252</td>\n      <td>0.368074</td>\n      <td>-0.701320</td>\n      <td>-1.104391</td>\n      <td>0.735760</td>\n      <td>0.894273</td>\n      <td>-1.375826</td>\n      <td>-5.144946</td>\n      <td>-2.048711</td>\n      <td>0.629773</td>\n      <td>-4.252669</td>\n      <td>-0.087420</td>\n      <td>-0.794367</td>\n      <td>-1.063963</td>\n      <td>0.115997</td>\n      <td>0.895180</td>\n      <td>3.184848</td>\n      <td>2.057840</td>\n      <td>-0.950821</td>\n      <td>0.961059</td>\n      <td>-1.837828</td>\n      <td>-0.437156</td>\n      <td>-0.828433</td>\n      <td>0.373747</td>\n      <td>-0.099787</td>\n      <td>-0.976280</td>\n      <td>-0.165921</td>\n      <td>3.297221</td>\n      <td>3.914132</td>\n      <td>-4.971376</td>\n      <td>-0.286520</td>\n      <td>-0.160133</td>\n      <td>-3.301453</td>\n      <td>-1.021032</td>\n      <td>-0.562744</td>\n      <td>0.574065</td>\n      <td>-0.368194</td>\n      <td>-0.507458</td>\n      <td>...</td>\n      <td>0.178099</td>\n      <td>-0.410396</td>\n      <td>-1.184236</td>\n      <td>1.681727</td>\n      <td>0.589606</td>\n      <td>0.064222</td>\n      <td>0.258885</td>\n      <td>0.560241</td>\n      <td>-1.545597</td>\n      <td>0.822283</td>\n      <td>1.518209</td>\n      <td>0.460143</td>\n      <td>0.822488</td>\n      <td>1.362718</td>\n      <td>0.218560</td>\n      <td>-1.038514</td>\n      <td>1.000763</td>\n      <td>-0.975878</td>\n      <td>-0.551268</td>\n      <td>-0.133044</td>\n      <td>-0.393092</td>\n      <td>1.236473</td>\n      <td>1.657100</td>\n      <td>0.833020</td>\n      <td>0.665379</td>\n      <td>-0.900025</td>\n      <td>0.291908</td>\n      <td>0.482727</td>\n      <td>0.552399</td>\n      <td>0.970496</td>\n      <td>-0.279168</td>\n      <td>1.544356</td>\n      <td>2.959727</td>\n      <td>1.641201</td>\n      <td>-0.130818</td>\n      <td>-0.264292</td>\n      <td>-0.748668</td>\n      <td>0.964218</td>\n      <td>0.087079</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv(\"../input/test.csv\")","execution_count":4,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Insights\n\n* Hey, we are given crazy column names! Can we find a meaning behind them? Hmm... ;-)\n* Very interesting id-column, isn't it?"},{"metadata":{},"cell_type":"markdown","source":"### Size of the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape[0] / test.shape[0]","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"1.999984741327352"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Roughly we have twice as much train data than test data."},{"metadata":{},"cell_type":"markdown","source":"### Dtypes"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":6,"outputs":[{"output_type":"stream","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 262144 entries, 0 to 262143\nColumns: 258 entries, id to target\ndtypes: float64(255), int64(2), object(1)\nmemory usage: 516.0+ MB\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Hmm... two int columns?"},{"metadata":{"trusted":true},"cell_type":"code","source":"test.info()","execution_count":7,"outputs":[{"output_type":"stream","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 131073 entries, 0 to 131072\nColumns: 257 entries, id to gamy-white-monster-expert\ndtypes: float64(255), int64(1), object(1)\nmemory usage: 257.0+ MB\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### A magic feature?"},{"metadata":{"trusted":true},"cell_type":"code","source":"column_types = train.dtypes\ncolumn_types[column_types==np.int64]","execution_count":8,"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"wheezy-copper-turtle-magic    int64\ntarget                        int64\ndtype: object"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Ohhh! :-D Cool! The wheezy-copper-turtle-magic oooohhh magic! What does that mean for the comp?"},{"metadata":{},"cell_type":"markdown","source":"### Missing values\n\nLet's start with obvious once:"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().sum().sum()","execution_count":9,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"0"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.isnull().sum().sum()","execution_count":10,"outputs":[{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"0"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"No obvious missing values in train and test. Perhaps there are some non-obvious once but let's move this topic to exploratory data analysis."},{"metadata":{},"cell_type":"markdown","source":"## Basic exploratory analysis\n\nOk, now the colorful part starts. We have already found an interesting feature by peeking at the data. Perhaps we can find some more during basic EDA. Let's stay curious and critical! Do we know if test and train behave the same? No! For this reason, I don't like to combine train and test right now... :-)"},{"metadata":{},"cell_type":"markdown","source":"### Class balance"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(train.target, palette=\"Set2\");","execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Very balanced in train!"},{"metadata":{},"cell_type":"markdown","source":"### Feature correlation"},{"metadata":{"trusted":true},"cell_type":"code","source":"magic = \"wheezy-copper-turtle-magic\"\ntrain_corr = train.drop([\"target\", magic], axis=1).corr()\ntest_corr = test.drop(magic, axis=1).corr()","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_corr_flat = train_corr.values.flatten()\ntrain_corr_flat = train_corr_flat[train_corr_flat != 1]\n\ntest_corr_flat = test_corr.values.flatten()\ntest_corr_flat = test_corr_flat[test_corr_flat != 1]\n\nfig, ax = plt.subplots(1,2,figsize=(20,5))\nsns.distplot(train_corr_flat, ax=ax[0], color=\"tomato\")\nsns.distplot(test_corr_flat, ax=ax[1], color=\"limegreen\");\nax[0].set_title(\"Off-diagonal train corr \\n distribution\")\nax[1].set_title(\"Off-diagonal test corr \\n distribution\");\nax[0].set_xlabel(\"feature correlation value\")\nax[1].set_xlabel(\"feature correlation value\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Insights\n\n* Looks almost decorrelated. But don't be too fast. Maybe this slight values are still something fruitful. \n* Do you see what is missing in train? We have +- 0.015 in test on both sides but only -0.01 and +0.015 in train. There must be some differences in feature distributions between train and test."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(25,25))\nsns.heatmap(train_corr, vmin=-0.016, vmax=0.016, cmap=\"RdYlBu_r\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(25,25))\nsns.heatmap(test_corr, vmin=-0.016, vmax=0.016, cmap=\"RdYlBu_r\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Insights\n\n* If you compare correlations carefully, you can observe that some of the nearby features have some slight correlations in train and test. This is especially interesting for some subsequent features. Can we find some sense in the way of column order?\n* This is not always true. Take a look at the lower right corner. The correlation between the last two features in train looks strong enough to \"be\" something, but in test this correlation has gone! As we have already seen by the correlation distributions we can expect some differences in the feature distributions of train and test. "},{"metadata":{"trusted":true},"cell_type":"code","source":"target_medians = train.groupby(\"target\").median()\nsorted_target_distance = np.abs(target_medians.iloc[0]-target_medians.iloc[1]).sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sorted_target_distance.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sorted_target_distance.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":":-o What's that? Wheezy-copper-turtle-magic... again!"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(2,2,figsize=(20,10))\nsns.distplot(train.loc[train.target==0, \"wheezy-myrtle-mandrill-entropy\"], color=\"Blue\", ax=ax[0,0])\nsns.distplot(train.loc[train.target==1, \"wheezy-myrtle-mandrill-entropy\"], color=\"Red\", ax=ax[0,0])\nsns.distplot(train.loc[train.target==0, \"wheezy-copper-turtle-magic\"], color=\"Blue\", ax=ax[0,1])\nsns.distplot(train.loc[train.target==1, \"wheezy-copper-turtle-magic\"], color=\"Red\", ax=ax[0,1])\nax[1,0].scatter(train[\"wheezy-myrtle-mandrill-entropy\"].values,\n                train[\"skanky-carmine-rabbit-contributor\"].values, c=train.target.values,\n                cmap=\"coolwarm\", s=1, alpha=0.5)\nax[1,0].set_xlabel(\"wheezy-myrtle-mandrill-entropy\")\nax[1,0].set_ylabel(\"skanky-carmine-rabbit-contributor\")\nax[1,1].scatter(train[\"wheezy-myrtle-mandrill-entropy\"].values,\n                train[\"wheezy-copper-turtle-magic\"].values, c=train.target.values,\n                cmap=\"coolwarm\", s=1, alpha=0.5)\nax[1,1].set_xlabel(\"wheezy-myrtle-mandrill-entropy\")\nax[1,1].set_ylabel(\"wheezy-copper-turtle-magic\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Insights\n\nCrazy turtle! :-D\n\n* By taking the feature mean per target and computing distances between the classes I wanted to find features that show clear separating forces. But... even the feature with highest difference in classes medians looks like \"no big deal\" at all. \n* Our crazy magic turtle feature has no difference at all! And it's distribution is almost uniform even though the small bin peaks look interesting (just an artifact of plotting?). \n* By plotting two features with highest distances in class medians I wanted to see if we can see something like in don't overfit (overlapping gaussians)... but this time it doesn't look like that. Let's see what logistic regression tells us with its weights in the next step.\n* The scatter plot of wheezy-copper-turtle-magic and our highest class median distance feature, \"wheezy_myrtle-mandrill-entropy\", looks indeed crazy! It's not that the turtle magic spreads smoothly with lower density over the whole space of the mandrills tails. No! It has some subsequent \"peak\" tails depending on the value of magic turtle. This again leads to the question: What role does the magic turtle play in this game?"},{"metadata":{},"cell_type":"markdown","source":"### What logistic regression wants to tell us..."},{"metadata":{"trusted":true},"cell_type":"code","source":"n_splits=3\nn_repeats=3\n\nX=train.drop([\"target\", \"id\"], axis=1).values\ny=train.target.values\nXTest = test.drop(\"id\", axis=1).values\n\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\nXTest = scaler.transform(XTest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X, x_val, y, y_val = train_test_split(X,y,test_size=0.2, stratify=y, random_state=2019)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"skf = RepeatedStratifiedKFold(n_repeats=n_repeats,\n                              n_splits=n_splits,\n                              random_state=2019)\n\np_val = np.zeros(y_val.shape)\npTest = np.zeros(XTest.shape[0])\nfor train_idx, test_idx in skf.split(X,y):\n    \n    x_train, x_test = X[train_idx], X[test_idx]\n    y_train, y_test = y[train_idx], y[test_idx]\n    \n    lr=LogisticRegression(penalty=\"l1\", C=1, solver=\"saga\")\n    lr.fit(x_train, y_train)\n    p_test = lr.predict_proba(x_test)[:,1]\n    p_val += lr.predict_proba(x_val)[:,1]\n    pTest += lr.predict_proba(XTest)[:,1]\n    print(roc_auc_score(y_test, p_test))\n\np_val /= (n_splits*n_repeats)\npTest /= (n_splits*n_repeats)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(roc_auc_score(y_val, p_val))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As Chris already pointed out, we won't get far by ignoring the pattern found with our magic turtle. "},{"metadata":{},"cell_type":"markdown","source":"### The magic turtle again\n\nCan't get enough! :-) Let's look at a 3D-Scatterplot with magic turtle:"},{"metadata":{"trusted":true},"cell_type":"code","source":"feat1 = \"wheezy-myrtle-mandrill-entropy\"\nfeat2 = \"skanky-carmine-rabbit-contributor\"\nfeat3 = \"wheezy-copper-turtle-magic\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"N = 10000\n\ntrace1 = go.Scatter3d(\n    x=train[feat1].values[0:N], \n    y=train[feat2].values[0:N],\n    z=train[feat3].values[0:N],\n    mode='markers',\n    marker=dict(\n        color=train.target.values[0:N],\n        colorscale = \"Jet\",\n        opacity=0.3,\n        size=2\n    )\n)\n\nfigure_data = [trace1]\nlayout = go.Layout(\n    title = 'The turtle place',\n    scene = dict(\n        xaxis = dict(title=feat1),\n        yaxis = dict(title=feat2),\n        zaxis = dict(title=feat3),\n    ),\n    margin=dict(\n        l=0,\n        r=0,\n        b=0,\n        t=0\n    ),\n    showlegend=True\n)\n\nfig = go.Figure(data=figure_data, layout=layout)\npy.iplot(fig, filename='simple-3d-scatter')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(5,5,figsize=(20,25))\nfor turtle1 in range(5):\n    for turtle2 in range(5):\n        my_turtle=turtle2+turtle1*5\n        ax[turtle1, turtle2].scatter(train.loc[train[\"wheezy-copper-turtle-magic\"]==my_turtle, feat1].values,\n                                     train.loc[train[\"wheezy-copper-turtle-magic\"]==my_turtle, feat2].values,\n                                     c=train.loc[train[\"wheezy-copper-turtle-magic\"]==my_turtle, \"target\"].values, cmap=\"coolwarm\", s=5, alpha=0.5)\n        ax[turtle1, turtle2].set_xlim([-15,15])\n        ax[turtle1, turtle2].set_ylim([-15,15])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Insights\n\n* Very cool and beautiful again! :-)\n* Choose some other features for feat1 and feat2. By doing so you will definitely find some spread that separates target classes! "},{"metadata":{},"cell_type":"markdown","source":"### What about the column names?\n\nHave you asked yourself as well who had this nice phantasy to build up the column names? Is there a logic behind it? Can we find connections given the column names? Well I don't know it and we all have to work on it but perhaps a sketch helps..."},{"metadata":{"trusted":true},"cell_type":"code","source":"names = list(train.drop([\"id\", \"target\"], axis=1).columns.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"first_names = []\nsecond_names = []\nthird_names = []\nfourth_names = []\n\nfor name in names:\n    words = name.split(\"-\")\n    first_names.append(words[0])\n    second_names.append(words[1])\n    third_names.append(words[2])\n    fourth_names.append(words[3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(first_names), len(np.unique(first_names)))\nprint(len(second_names), len(np.unique(second_names)))\nprint(len(third_names), len(np.unique(third_names)))\nprint(len(fourth_names), len(np.unique(fourth_names)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's follow the idea that these names indeed have some meaning... in this case: what are features to discard? Is there one more magic feature and what makes it magically?"},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_names = pd.DataFrame(index=train.drop([\"target\", \"id\"], axis=1).columns.values, data=first_names, columns=[\"kind\"])\nfeature_names[\"color\"] = second_names\nfeature_names[\"animal\"] = third_names\nfeature_names[\"goal\"] = fourth_names\nfeature_names.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,5))\nsns.countplot(x=\"kind\", data=feature_names, order=feature_names.kind.value_counts().index, palette=\"Greens_r\")\nplt.xticks(rotation=90);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,5))\nsns.countplot(x=\"animal\", data=feature_names, order=feature_names.animal.value_counts().index, palette=\"Oranges_r\")\nplt.xticks(rotation=90);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,5))\nsns.countplot(x=\"color\", data=feature_names, order=feature_names.color.value_counts().index, palette=\"Purples_r\")\nplt.xticks(rotation=90);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,5))\nsns.countplot(x=\"goal\", data=feature_names, order=feature_names.goal.value_counts().index, palette=\"Reds_r\")\nplt.xticks(rotation=90);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_names[feature_names.goal==\"learn\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ideas and insights\n\nPuhhh.... breath in and out. That looks like a further puzzle to solve... perhaps... probably... hopefully :-) \n\n1. The one turtle magic, is one magic.\n2. There a lot of further goal descriptions that could be interpretable or at least leave a guess on what to try out with them.\n3. Color has a natural order. Do we need this kind of order somewhere?\n4. A lot of animals only have one or two occurences. But perhaps it's not about a single animal name and we can introduce a higher level of abstraction like \"bird\" or \"dog\" instead of a specific species of dogs, birds etc..\n5. The kind is a mysterium for me... perhaps the length of these adjectives is important or the first character or we can find a grouping given the other feature names?!\n\nWould be great to create a connection map... who is connected with who? Can we visualize a network? Could be a good timepoint to learn how to realize that. "},{"metadata":{},"cell_type":"markdown","source":"### Duplicates?"},{"metadata":{"trusted":true},"cell_type":"code","source":"combined = train.drop([\"id\", \"target\"], axis=1).append(test.drop(\"id\", axis=1))\ncombined[combined.duplicated()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Diving into single datasets\n\nHow many single rows do we have given a single magic turtle value? Do all subsets in the data have the same amount of samples?"},{"metadata":{"trusted":true},"cell_type":"code","source":"n_subsamples_test = test.groupby(magic).size() \nn_subsamples_train = train.groupby(magic).size() \n\nplt.figure(figsize=(20,5))\nplt.plot(n_subsamples_test.values, '.-', label=\"test\")\nplt.plot(n_subsamples_train.values, '.-', label=\"train\")\nplt.plot(n_subsamples_test.values + n_subsamples_train.values, '.-', label=\"total\")\nplt.legend();\nplt.xlabel(magic)\nplt.ylabel(\"sample count\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Obviously not! The zick-zack nature above seemed to be a plotting artifact. "},{"metadata":{},"cell_type":"markdown","source":"### Choosing a subset"},{"metadata":{"trusted":true},"cell_type":"code","source":"my_magic=0\n\ntrain_subset = train.loc[train[magic]==my_magic].copy()\ntest_subset = test.loc[test[magic]==my_magic].copy()","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_splits=20\nn_repeats=5\n\nX=train_subset.drop([\"target\", \"id\"], axis=1).values\ny=train_subset.target.values\nXTest = test_subset.drop(\"id\", axis=1).values\n\n#scaler = StandardScaler()\n#X = scaler.fit_transform(X)\n#XTest = scaler.transform(XTest)\n\nX, x_val, y, y_val = train_test_split(X,y,test_size=0.2, stratify=y, random_state=2019)","execution_count":34,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"skf = RepeatedStratifiedKFold(n_repeats=n_repeats,\n                              n_splits=n_splits,\n                              random_state=2019)\n\nimportances = np.zeros(shape=(n_splits*n_repeats, XTest.shape[1]))\np_val = np.zeros(y_val.shape)\npTest = np.zeros(XTest.shape[0])\n\nm=0\nfor train_idx, test_idx in skf.split(X,y):\n    \n    x_train, x_test = X[train_idx], X[test_idx]\n    y_train, y_test = y[train_idx], y[test_idx]\n    \n    lr=LogisticRegression(penalty=\"l1\", C=0.1, solver=\"liblinear\", max_iter=300)\n    lr.fit(x_train, y_train)\n    p_test = lr.predict_proba(x_test)[:,1]\n    p_val += lr.predict_proba(x_val)[:,1]\n    pTest += lr.predict_proba(XTest)[:,1]\n    importances[m,:] += lr.coef_[0]\n    print(roc_auc_score(y_test, p_test))\n    m+=1\n\np_val /= (n_splits*n_repeats)\npTest /= (n_splits*n_repeats)","execution_count":58,"outputs":[{"output_type":"stream","text":"0.8016528925619836\n0.8264462809917356\n0.8099173553719008\n0.7355371900826446\n0.859504132231405\n0.8016528925619835\n0.8677685950413223\n0.6115702479338844\n0.7768595041322314\n0.6694214876033058\n0.8999999999999999\n0.9181818181818182\n0.6909090909090909\n0.8636363636363636\n0.8\n0.9363636363636363\n0.8272727272727273\n0.77\n0.72\n0.92\n0.7520661157024794\n0.8347107438016529\n0.8347107438016529\n0.8016528925619835\n0.7107438016528925\n0.7851239669421487\n0.8181818181818182\n0.8181818181818181\n0.8099173553719008\n0.8842975206611571\n0.8545454545454545\n0.6454545454545454\n0.6181818181818182\n0.7636363636363636\n0.7636363636363637\n1.0\n0.8545454545454545\n0.68\n0.8200000000000001\n0.8900000000000001\n0.8925619834710744\n0.8677685950413223\n0.7933884297520661\n0.7024793388429752\n0.7107438016528926\n0.7933884297520661\n0.9256198347107438\n0.8925619834710743\n0.7272727272727273\n0.5785123966942148\n0.6\n0.7454545454545455\n0.8363636363636364\n0.9636363636363636\n0.7727272727272727\n0.9545454545454545\n0.8636363636363636\n0.71\n0.9800000000000001\n0.89\n0.9090909090909091\n0.8099173553719008\n0.9504132231404958\n0.8181818181818181\n0.7107438016528925\n0.8181818181818182\n0.8429752066115702\n0.5867768595041323\n0.7603305785123967\n0.884297520661157\n0.8\n1.0\n0.5909090909090908\n0.8\n0.7454545454545454\n0.7363636363636363\n0.790909090909091\n0.8400000000000001\n0.85\n0.77\n0.8429752066115702\n0.9256198347107438\n0.7768595041322314\n0.9338842975206612\n0.9256198347107437\n0.7520661157024793\n0.9173553719008265\n0.8429752066115702\n0.8925619834710743\n0.7107438016528925\n0.8818181818181817\n0.6454545454545455\n0.7454545454545455\n0.7000000000000001\n0.8636363636363636\n0.9181818181818182\n0.7181818181818183\n0.62\n0.74\n0.74\n","name":"stdout"}]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}