{"cells":[{"metadata":{},"cell_type":"markdown","source":"# General information\nIn this kernel I work with Instant Gratification challenge. This is a binary classification problem with immediate \"Stage 2\".\n\nI'll do some EDA and basic modelling and then use feature engineering to improve the model."},{"metadata":{},"cell_type":"markdown","source":"Loading libraries and preparing functions"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nfrom numba import jit\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom tqdm import tqdm_notebook\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import NuSVR, SVR\nfrom sklearn.metrics import mean_absolute_error\npd.options.display.precision = 15\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score, StratifiedKFold, GridSearchCV\nfrom sklearn import svm\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostRegressor, CatBoostClassifier\nfrom sklearn import metrics\nimport time\nimport datetime\nfrom catboost import CatBoostRegressor\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn import neighbors\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn import linear_model\nimport gc\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.ensemble import ExtraTreesRegressor, AdaBoostRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.signal import hilbert\nfrom scipy.signal import hann\nfrom scipy.signal import convolve\nfrom scipy import stats\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.neighbors import NearestNeighbors\nimport librosa, librosa.display\nimport builtins\nfrom sklearn.ensemble import RandomForestRegressor\nimport eli5\nimport shap\nfrom sklearn.feature_selection import GenericUnivariateSelect, SelectPercentile, SelectKBest, f_classif, mutual_info_classif, RFE\nfrom sklearn import metrics\nfrom IPython.display import HTML\nimport json\nimport altair as alt\nfrom collections import Counter","execution_count":1,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data overview"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have more than 200 columns which seem to be anonymized. Let's have a quick look at them."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(train[train.columns[10]]);\nplt.title(f'Distribution of {train.columns[10]}');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The column seem to have a normal distribution. This reminds me of the recent Santander competition..."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(train.mean(1));\nplt.title('Distribution of mean values of train columns');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(train.std(1));\nplt.title('Distribution of standard deviations of train columns');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems that the data was normalized."},{"metadata":{"trusted":true},"cell_type":"code","source":"train['target'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have a balanced dataset!"},{"metadata":{},"cell_type":"markdown","source":"## Basic model"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train.drop(['id', 'target'], axis=1)\nX_test = test.drop(['id'], axis=1)\ny = train['target']\nn_fold = 5\nfolds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=11)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Training function:"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"@jit\ndef fast_auc(y_true, y_prob):\n    \"\"\"\n    fast roc_auc computation: https://www.kaggle.com/c/microsoft-malware-prediction/discussion/76013\n    \"\"\"\n    y_true = np.asarray(y_true)\n    y_true = y_true[np.argsort(y_prob)]\n    nfalse = 0\n    auc = 0\n    n = len(y_true)\n    for i in range(n):\n        y_i = y_true[i]\n        nfalse += (1 - y_i)\n        auc += y_i * nfalse\n    auc /= (nfalse * (n - nfalse))\n    return auc\n\n\ndef eval_auc(y_true, y_pred):\n    \"\"\"\n    Fast auc eval function for lgb.\n    \"\"\"\n    return 'auc', fast_auc(y_true, y_pred), True\n\n\ndef train_model_classification(X, X_test, y, params, folds, model_type='lgb', eval_metric='auc', columns=None, plot_feature_importance=False, model=None,\n                               verbose=10000, early_stopping_rounds=200, n_estimators=50000):\n    \"\"\"\n    A function to train a variety of regression models.\n    Returns dictionary with oof predictions, test predictions, scores and, if necessary, feature importances.\n    \n    :params: X - training data, can be pd.DataFrame or np.ndarray (after normalizing)\n    :params: X_test - test data, can be pd.DataFrame or np.ndarray (after normalizing)\n    :params: y - target\n    :params: folds - folds to split data\n    :params: model_type - type of model to use\n    :params: eval_metric - metric to use\n    :params: columns - columns to use. If None - use all columns\n    :params: plot_feature_importance - whether to plot feature importance of LGB\n    :params: model - sklearn model, works only for \"sklearn\" model type\n    \n    \"\"\"\n    columns = X.columns if columns == None else columns\n    X_test = X_test[columns]\n    \n    # to set up scoring parameters\n    metrics_dict = {'auc': {'lgb_metric_name': eval_auc,\n                        'catboost_metric_name': 'AUC',\n                        'sklearn_scoring_function': metrics.roc_auc_score},\n                    }\n    \n    result_dict = {}\n    \n    # out-of-fold predictions on train data\n    oof = np.zeros((len(X), len(set(y.values))))\n    \n    # averaged predictions on train data\n    prediction = np.zeros((len(X_test), oof.shape[1]))\n    \n    # list of scores on folds\n    scores = []\n    feature_importance = pd.DataFrame()\n    \n    # split and train on folds\n    for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y)):\n        print(f'Fold {fold_n + 1} started at {time.ctime()}')\n        if type(X) == np.ndarray:\n            X_train, X_valid = X[columns][train_index], X[columns][valid_index]\n            y_train, y_valid = y[train_index], y[valid_index]\n        else:\n            X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n            \n        if model_type == 'lgb':\n            model = lgb.LGBMClassifier(**params, n_estimators=n_estimators, n_jobs = -1)\n            model.fit(X_train, y_train, \n                    eval_set=[(X_train, y_train), (X_valid, y_valid)],\n                    verbose=verbose, early_stopping_rounds=early_stopping_rounds)\n            \n            y_pred_valid = model.predict_proba(X_valid)\n            y_pred = model.predict_proba(X_test, num_iteration=model.best_iteration_)\n            \n        if model_type == 'xgb':\n            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X.columns)\n            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X.columns)\n\n            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n            model = xgb.train(dtrain=train_data, num_boost_round=n_estimators, evals=watchlist, early_stopping_rounds=early_stopping_rounds, verbose_eval=verbose, params=params)\n            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n        \n        if model_type == 'sklearn':\n            model = model\n            model.fit(X_train, y_train)\n            \n            y_pred_valid = model.predict(X_valid).reshape(-1,)\n            score = metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid)\n            print(f'Fold {fold_n}. {eval_metric}: {score:.4f}.')\n            print('')\n            \n            y_pred = model.predict_proba(X_test)\n        \n        if model_type == 'cat':\n            model = CatBoostClassifier(iterations=n_estimators, eval_metric=metrics_dict[eval_metric]['catboost_metric_name'], **params,\n                                      loss_function=metrics_dict[eval_metric]['catboost_metric_name'])\n            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n\n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test)\n        \n        oof[valid_index] = y_pred_valid\n        scores.append(metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid[:, 1]))\n\n        prediction += y_pred    \n        \n        if model_type == 'lgb' and plot_feature_importance:\n            # feature importance\n            fold_importance = pd.DataFrame()\n            fold_importance[\"feature\"] = columns\n            fold_importance[\"importance\"] = model.feature_importances_\n            fold_importance[\"fold\"] = fold_n + 1\n            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n\n    prediction /= folds.n_splits\n    \n    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n    \n    result_dict['oof'] = oof\n    result_dict['prediction'] = prediction\n    result_dict['scores'] = scores\n    \n    if model_type == 'lgb':\n        if plot_feature_importance:\n            feature_importance[\"importance\"] /= folds.n_splits\n            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n                by=\"importance\", ascending=False)[:50].index\n\n            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\n            plt.figure(figsize=(16, 12));\n            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n            plt.title('LGB Features (avg over folds)');\n            \n            result_dict['feature_importance'] = feature_importance\n        \n    return result_dict\n    ","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'num_leaves': 128,\n          'min_child_samples': 79,\n          'objective': 'binary',\n          'max_depth': -1,\n          'learning_rate': 0.1,\n          \"boosting_type\": \"gbdt\",\n          \"subsample_freq\": 5,\n          \"subsample\": 0.9,\n          \"bagging_seed\": 11,\n          \"metric\": 'auc',\n          \"verbosity\": -1,\n          'reg_alpha': 0.1302650970728192,\n          'reg_lambda': 0.3603427518866501,\n          'colsample_bytree': 0.8\n         }\nresult_dict_lgb = train_model_classification(X=X, X_test=X_test, y=y, params=params, folds=folds, model_type='lgb',\n                                                                                  eval_metric='auc', plot_feature_importance=True, verbose=50, n_estimators=200)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv(\"../input/sample_submission.csv\")\nsub['target'] = result_dict_lgb['prediction'][:, 1]\nsub.to_csv(\"submission.csv\", index=False)\nsub.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature engineering\n\nWe can see that there is one feature which has much higher importance that other features: `wheezy-copper-turtle-magic`."},{"metadata":{"trusted":true},"cell_type":"code","source":"train['wheezy-copper-turtle-magic'].nunique(), test['wheezy-copper-turtle-magic'].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sorted(train['wheezy-copper-turtle-magic'].unique()) == sorted(test['wheezy-copper-turtle-magic'].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(train.loc[train['target'] == 0, 'wheezy-copper-turtle-magic'], color='r', bins=512, label='0');\nplt.hist(train.loc[train['target'] == 1, 'wheezy-copper-turtle-magic'], color='g', bins=512, label='1');\nplt.hist(test['wheezy-copper-turtle-magic'], color='b', bins=512, label='1');\nplt.title('Distribution of wheezy-copper-turtle-magic');\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems that this is a categorical feature! And values in train and test are the same! Let's create some features based on it!\n\nAnother idea is scaling features, it seems that it works quite well.\n\nAlso let's try looking at column names. They are quite interesting - they contain several words separated by \"-\". Let's have a look."},{"metadata":{"trusted":true},"cell_type":"code","source":"col_part_names = [col.split('-') for col in train.columns]\ncol_part_names = [i for j in col_part_names for i in j]\nCounter(col_part_names).most_common(10)","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"[('important', 19),\n ('entropy', 15),\n ('unsorted', 15),\n ('sorted', 15),\n ('grandmaster', 15),\n ('expert', 12),\n ('hint', 12),\n ('novice', 11),\n ('ordinal', 10),\n ('distraction', 10)]","text/html":"[('important', 19), ('entropy', 15), ('unsorted', 15), ('sorted', 15), ('grandmaster', 15), ('expert', 12), ('hint', 12), ('novice', 11), ('ordinal', 10), ('distraction', 10)]"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"I'm going to take some of these columns and calculate statistics based on them."},{"metadata":{"trusted":true},"cell_type":"code","source":"some_cols = [i[0] for i in Counter(col_part_names).most_common() if i[1] > 4]","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['target'] = -1\nlen_train = train.shape[0]\nscaler = StandardScaler()\nall_data = pd.concat([train, test], axis=0, sort=False, ignore_index=True).reset_index(drop=True)\nfor c in some_cols:\n    more_such_cols = [col for col in all_data.columns if c in col]\n    all_data[f'{c}_mean'] = all_data[more_such_cols].mean(1)\n    all_data[f'{c}_min'] = all_data[more_such_cols].min(1)\n    all_data[f'{c}_max'] = all_data[more_such_cols].max(1)\n    all_data[f'{c}_std'] = all_data[more_such_cols].std(1)\ncols = [c for c in train.columns if c not in ['id', 'target', 'wheezy-copper-turtle-magic']]\nall_data[cols] = scaler.fit_transform(all_data[cols])\n#frequencies\nall_data['wheezy-copper-turtle-magic_count'] = all_data.groupby(['wheezy-copper-turtle-magic'])['id'].transform('count')","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = all_data[:len_train].reset_index(drop=True)\ntest = all_data[len_train:].reset_index(drop=True)","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.concat([train, pd.get_dummies(train['wheezy-copper-turtle-magic'], prefix='wctm')], axis=1)\ntest = pd.concat([test, pd.get_dummies(test['wheezy-copper-turtle-magic'], prefix='wctm')], axis=1)","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train.drop(['id', 'target'], axis=1)\nX_test = test.drop(['id', 'target'], axis=1)\ny = train['target']\nn_fold = 5\nfolds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42)","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'num_leaves': 1024,\n          'min_child_samples': 10,\n          'objective': 'binary',\n          'max_depth': -1,\n          'learning_rate': 0.1,\n          \"boosting_type\": \"gbdt\",\n          \"subsample_freq\": 5,\n          \"subsample\": 1.0,\n          \"bagging_seed\": 11,\n          \"metric\": 'auc',\n          \"verbosity\": -1,\n          'reg_alpha': 0.1302650970728192,\n          'reg_lambda': 0.3603427518866501,\n          'colsample_bytree': 1.0,\n          'min_sum_hessian_in_leaf': 10,\n          'num_threads': -1\n         }\n\n\nresult_dict_lgb = train_model_classification(X=X, X_test=X_test, y=y, params=params, folds=folds, model_type='lgb',\n                                                                                  eval_metric='auc', plot_feature_importance=True, verbose=500, n_estimators=10000)","execution_count":null,"outputs":[{"output_type":"stream","text":"Fold 1 started at Mon May 20 05:46:05 2019\nTraining until validation scores don't improve for 200 rounds.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv(\"../input/sample_submission.csv\")\nsub['target'] = result_dict_lgb['prediction'][:, 1]\nsub.to_csv(\"submission_1.csv\", index=False)\nsub.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}