{"cells":[{"metadata":{},"cell_type":"markdown","source":"## General information\n\nIn this kernel I'll train a simple Pytorch model."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import torch\nimport numpy as np\nimport pandas as pd\nfrom torch import nn\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nimport os\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn import metrics\nimport time","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Code for modelling"},{"metadata":{},"cell_type":"markdown","source":"Converting to tensor"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"class ToTensor:\n    def __init__(self, excluded_keys=(\"id\")):\n        if not isinstance(excluded_keys, set):\n            excluded_keys = set(excluded_keys)\n        self.excluded = excluded_keys\n\n    def __call__(self, x):\n        result = {k: torch.from_numpy(v) for k, v in x.items() if k not in self.excluded}\n        for k in self.excluded:\n            if k in x.keys():\n                result[k] = x[k]\n        return result","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Custom dataset class"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"class IGDataset(Dataset):\n    def __init__(self, df, transform=None, test=False):\n        self.transform = transform\n        data = df\n        self.test = test\n        if not test:\n            self.y = data[\"target\"].astype(int)\n        else:\n            self.y = pd.Series(np.array([-1] * len(data)))\n            \n        data = data.drop(['target'], axis=1)\n        \n        if 'id' in data.columns:\n            data.drop('id', axis=1, inplace=True)\n        # moving wheezy-copper-turtle-magic to the last position for convenience\n        cols = [col for col in data.columns if col != 'wheezy-copper-turtle-magic']\n        data = data[cols + ['wheezy-copper-turtle-magic']]\n        self.x = data.values\n\n    def __len__(self):\n        return self.x.shape[0]\n\n    def __getitem__(self, idx):\n        x = self.x[idx]\n        item = {'inputs': x}\n        y = np.array([self.y.iloc[idx]]) \n        item['targets'] = y\n\n        if self.transform:\n            item = self.transform(item)\n        \n        return x, y        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Neural net class"},{"metadata":{"trusted":true},"cell_type":"code","source":"class IGClassifier(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.inference = False\n        self.activation = nn.ELU(inplace=True)\n        # self.sigmoid = nn.Sigmoid()\n        # self.relu = nn.ReLU()\n        self.emb = nn.Embedding(513, 512)\n\n        self.fc1 = nn.Linear(767, 128)\n        self.bn1 = nn.BatchNorm1d(128)\n        self.fc2 = nn.Linear(128, 16)\n        self.bn2 = nn.BatchNorm1d(16)\n        self.fc3 = nn.Linear(16, 1)\n        self.drop = nn.Dropout(0.2)\n\n    def forward(self, x):\n        # notice cast to long tensor here\n        # print(x)\n        emb = self.emb(x[:, -1].long())\n        num = x[:, :-1].float()\n        data = torch.cat([emb, num], 1)\n        out = self.drop(self.bn1(self.activation(self.fc1(data))))\n        out = self.drop(self.bn2(self.activation(self.fc2(out))))\n        out = self.fc3(out)\n\n        return out","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"CyclicLR"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"class CyclicLR(object):\n    def __init__(self, optimizer, base_lr=1e-3, max_lr=6e-3,\n                 step_size=2000, factor=0.6, min_lr=1e-4, mode='triangular', gamma=1.,\n                 scale_fn=None, scale_mode='cycle', last_batch_iteration=-1):\n\n        if not isinstance(optimizer, torch.optim.Optimizer):\n            raise TypeError('{} is not an Optimizer'.format(\n                type(optimizer).__name__))\n        self.optimizer = optimizer\n\n        if isinstance(base_lr, list) or isinstance(base_lr, tuple):\n            if len(base_lr) != len(optimizer.param_groups):\n                raise ValueError(\"expected {} base_lr, got {}\".format(\n                    len(optimizer.param_groups), len(base_lr)))\n            self.base_lrs = list(base_lr)\n        else:\n            self.base_lrs = [base_lr] * len(optimizer.param_groups)\n\n        if isinstance(max_lr, list) or isinstance(max_lr, tuple):\n            if len(max_lr) != len(optimizer.param_groups):\n                raise ValueError(\"expected {} max_lr, got {}\".format(\n                    len(optimizer.param_groups), len(max_lr)))\n            self.max_lrs = list(max_lr)\n        else:\n            self.max_lrs = [max_lr] * len(optimizer.param_groups)\n\n        self.step_size = step_size\n\n        if mode not in ['triangular', 'triangular2', 'exp_range'] \\\n                and scale_fn is None:\n            raise ValueError('mode is invalid and scale_fn is None')\n\n        self.mode = mode\n        self.gamma = gamma\n\n        if scale_fn is None:\n            if self.mode == 'triangular':\n                self.scale_fn = self._triangular_scale_fn\n                self.scale_mode = 'cycle'\n            elif self.mode == 'triangular2':\n                self.scale_fn = self._triangular2_scale_fn\n                self.scale_mode = 'cycle'\n            elif self.mode == 'exp_range':\n                self.scale_fn = self._exp_range_scale_fn\n                self.scale_mode = 'iterations'\n        else:\n            self.scale_fn = scale_fn\n            self.scale_mode = scale_mode\n\n        self.batch_step(last_batch_iteration + 1)\n        self.last_batch_iteration = last_batch_iteration\n\n        self.last_loss = np.inf\n        self.min_lr = min_lr\n        self.factor = factor\n\n    def batch_step(self, batch_iteration=None):\n        if batch_iteration is None:\n            batch_iteration = self.last_batch_iteration + 1\n        self.last_batch_iteration = batch_iteration\n        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n            param_group['lr'] = lr\n\n    def step(self, loss):\n        if loss > self.last_loss:\n            self.base_lrs = [max(lr * self.factor, self.min_lr) for lr in self.base_lrs]\n            self.max_lrs = [max(lr * self.factor, self.min_lr) for lr in self.max_lrs]\n\n    def _triangular_scale_fn(self, x):\n        return 1.\n\n    def _triangular2_scale_fn(self, x):\n        return 1 / (2. ** (x - 1))\n\n    def _exp_range_scale_fn(self, x):\n        return self.gamma ** (x)\n\n    def get_lr(self):\n        step_size = float(self.step_size)\n        cycle = np.floor(1 + self.last_batch_iteration / (2 * step_size))\n        x = np.abs(self.last_batch_iteration / step_size - 2 * cycle + 1)\n\n        lrs = []\n        param_lrs = zip(self.optimizer.param_groups, self.base_lrs, self.max_lrs)\n        for param_group, base_lr, max_lr in param_lrs:\n            base_height = (max_lr - base_lr) * np.maximum(0, (1 - x))\n            if self.scale_mode == 'cycle':\n                lr = base_lr + base_height * self.scale_fn(cycle)\n            else:\n                lr = base_lr + base_height * self.scale_fn(self.last_batch_iteration)\n            lrs.append(lr)\n        return lrs","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Model training"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef train_model(model, train_loader,valid_loader, test_loader, loss_fn, lr=0.001,\n                batch_size=512, n_epochs=4, validate=False):\n    param_lrs = [{'params': param, 'lr': lr} for param in model.parameters()]\n    optimizer = torch.optim.Adam(param_lrs, lr=lr)\n\n    scheduler = CyclicLR(optimizer, base_lr=0.001, max_lr=0.003, min_lr=0.000001,\n                         step_size=300, mode='exp_range', gamma=0.99994)\n\n    valid_loss_min = np.Inf\n    patience = 3\n    stop = False\n    for epoch in range(n_epochs):\n        start_time = time.time()\n\n        model.train()\n        avg_loss = 0.\n\n        for step, (seq_batch, y_batch) in enumerate(train_loader):\n            y_pred = model(seq_batch.cuda()).float()\n            scheduler.batch_step()\n            loss = loss_fn(y_pred.cpu(), y_batch.float().cpu())\n\n            optimizer.zero_grad()\n            loss.backward()\n\n            optimizer.step()\n            avg_loss += loss.item() / len(train_loader)\n\n        model.eval()\n        test_preds = np.zeros((len(test_loader.dataset)))\n\n        val_loss = 0\n        if validate:\n\n            valid_preds = np.zeros((len(valid_loader.dataset)))\n            y_true = []\n            y_prediction = []\n            for i, (seq_batch, y_batch) in enumerate(valid_loader):\n                y_pred = model(seq_batch.cuda()).float()\n                val_loss += loss_fn(y_pred.cpu(), y_batch.float()).item() / len(valid_loader)\n                valid_preds[i * batch_size:(i+1) * batch_size] = y_pred.detach().cpu().numpy()[:, 0]\n                # print(y_batch.detach().cpu().numpy())\n                # print(valid_preds[i * batch_size:(i+1) * batch_size])\n                y_true.extend(list(y_batch.detach().cpu().numpy()))\n                y_prediction.extend(list(y_pred.detach().cpu().numpy()))\n                # print('local score', print(metrics.roc_auc_score(y_batch.detach().cpu().numpy(), valid_preds[i * batch_size:(i+1) * batch_size])))\n\n        # all_test_preds.append(test_preds)\n        elapsed_time = time.time() - start_time\n        if epoch > 0 and epoch % 10 == 0:\n            print(f'Epoch {epoch + 1}/{n_epochs} \\t loss={avg_loss:.4f} val_loss={val_loss:.4f} \\t val_auc={metrics.roc_auc_score(y_true, y_prediction):.4f} time={elapsed_time:.2f}s')\n        \n        valid_loss = avg_loss\n        # print('epoch', epoch, 'valid_loss_min', np.round(valid_loss_min, 4), 'valid_loss', np.round(valid_loss, 4))\n        \n        if valid_loss <= valid_loss_min:\n#             print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n#             valid_loss_min,\n#             valid_loss))\n#             torch.save(model_conv.state_dict(), 'model.pt')\n            valid_loss_min = valid_loss\n            p = 0\n\n        # check if validation loss didn't improve\n        if valid_loss > valid_loss_min:\n            p += 1\n            # print(f'{p} epochs of increasing val loss')\n            if p > patience:\n                print('Stopping training')\n                stop = True\n                break        \n\n        if stop:\n            break\n        \n\n    for i, (seq_batch, _) in enumerate(test_loader):\n        y_pred = model(seq_batch.long().cuda()).float().detach()\n\n        test_preds[i * batch_size:(i+1) * batch_size] = y_pred.cpu().numpy()[:, 0]\n        \n        \n    results_dict = {}\n    results_dict['test_preds'] = test_preds\n    if validate:\n        results_dict['oof'] = y_prediction\n\n    return results_dict","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Training on folds"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def train_on_folds(train, test, splits, n_epochs=50, validate=False):\n    if validate:\n        scores = []\n    \n    transf = ToTensor()\n\n    test_preds = np.zeros((len(test), len(splits)))\n    train_oof = np.zeros((len(train), 1))\n    \n    test_dataset = IGDataset(test, transform=transf, test=True)\n    test_dataloader = DataLoader(test_dataset, batch_size=batchsize, shuffle=False, num_workers=0, pin_memory=False, drop_last=False)\n\n    for i, (train_idx, valid_idx) in enumerate(splits):\n\n        train_dataset = IGDataset(train.iloc[train_idx, :], transform=transf)\n        valid_dataset = IGDataset(train.iloc[valid_idx, :], transform=transf)\n        train_dataloader = DataLoader(train_dataset, batch_size=batchsize, shuffle=False, num_workers=0, pin_memory=False, drop_last=False)\n        valid_dataloader = DataLoader(valid_dataset, batch_size=batchsize, shuffle=False, num_workers=0, pin_memory=False, drop_last=False)\n        print(f'Fold {i + 1}')\n\n        set_seed(42 + i)\n        model = IGClassifier()\n        loss_fn = nn.BCEWithLogitsLoss()\n        model.cuda()\n\n        results_dict = train_model(model, train_dataloader, valid_dataloader, test_dataloader, loss_fn=loss_fn, n_epochs=n_epochs, validate=validate, batch_size=batchsize)\n\n        if validate:\n            train_oof[valid_idx] = results_dict['oof']#.reshape(-1, 1)\n            score = metrics.roc_auc_score(train['target'].iloc[valid_idx], train_oof[valid_idx])\n            print('score', score)\n            scores.append(score)\n            \n        test_preds[:, i] = results_dict['test_preds']\n    print(f'CV mean score: {np.mean(scores)}. Std: {np.std(scores)}')\n    \n    return test_preds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fixing random state"},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\ndef set_seed(seed: int = 0):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nset_seed(42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preparing data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/train.csv\") \ntest = pd.read_csv(\"../input/test.csv\")\nsub = pd.read_csv('../input/sample_submission.csv')\nlen_train = train.shape[0]\nall_data = pd.concat([train, test], axis=0, sort=False)\n\nscaler = StandardScaler()\ncols = [c for c in train.columns if c not in ['id', 'target', 'wheezy-copper-turtle-magic']]\nall_data[cols] = scaler.fit_transform(all_data[cols])\ntrain = all_data[:len_train].reset_index(drop=True)\ntest = all_data[len_train:].reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batchsize = 1024\nn_fold = 5\nsplits = list(StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42).split(train, train['target']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model training"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds = train_on_folds(train, test, splits, n_epochs=200, validate=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub['target'] = sigmoid(test_preds.mean(1))\nsub.to_csv(\"submission.csv\", index=False)\nsub.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}