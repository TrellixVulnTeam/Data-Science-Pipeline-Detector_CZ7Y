{"cells":[{"metadata":{},"cell_type":"markdown","source":"**A Kullback-Leibler Approach for Instant Gratification **"},{"metadata":{},"cell_type":"markdown","source":"The main idea was to have different distance estimations for each class centroid. As the Kullback-Leibler divergence is not a metric it will return different estimations according to the way that is calculated. "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom scipy.spatial import distance as ds\nfrom sklearn.covariance import LedoitWolf\n\nfrom sklearn.svm import NuSVC\nfrom sklearn.linear_model import LogisticRegression\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#General PlotStyle\ndef PlotStyle(Axes,Title,x_label,y_label):\n    \n    Axes.spines['top'].set_visible(False)\n    Axes.spines['right'].set_visible(False)\n    Axes.spines['bottom'].set_visible(True)\n    Axes.spines['left'].set_visible(True)\n    Axes.xaxis.set_tick_params(labelsize=12)\n    Axes.yaxis.set_tick_params(labelsize=12)\n    Axes.set_ylabel(y_label,fontsize=14)\n    Axes.set_xlabel(x_label,fontsize=14)\n    Axes.set_title(Title)\n\n#MeshGeneration    \ndef MeshData(TargetData,Model,MeshSteps):\n  \n  nVals=MeshSteps\n  xMin,xMax=1.2*min(TargetData[:,0]),1.2*max(TargetData[:,0])\n  yMin,yMax=1.2*min(TargetData[:,1]),1.2*max(TargetData[:,1])\n    \n  xx, yy = np.meshgrid(np.linspace(xMin, xMax, nVals), np.linspace(yMin, yMax, nVals))\n  Z = Model.decision_function(np.c_[xx.ravel(), yy.ravel()])\n\n  return xx,yy,Z\n \n#Decision function plot\ndef PlotDecisionFunction(TargetData,Targetlabels,Model,MeshSteps,Axis):\n  \n  localMesh=MeshData(TargetData,Model,MeshSteps)\n  \n  Class0=[k for k in range(len(labls)) if labls[k]==0]\n  Class1=[k for k in range(len(labls)) if labls[k]==1]\n  \n  Axis.contourf(localMesh[0],localMesh[1],localMesh[2].reshape(localMesh[0].shape),cmap=plt.cm.bwr)\n  Axis.plot(TargetData[Class0,0],TargetData[Class0,1],'bo',alpha=0.25)\n  Axis.plot(TargetData[Class1,0],TargetData[Class1,1],'ro',alpha=0.25)\n  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Calculate the covariance matrix for each label\ndef GetModelParams(DataFrame,ColumnIndex):\n  \n  cDataSet=DataFrame\n  \n  cData0=cDataSet[cDataSet['target']==0]\n  cData1=cDataSet[cDataSet['target']==1]\n  \n  bData0=np.array(cData0[ColumnIndex])\n  bData1=np.array(cData1[ColumnIndex])\n  \n  Cov0=LedoitWolf(assume_centered=False).fit(bData0)\n  Cov1=LedoitWolf(assume_centered=False).fit(bData1)\n  \n  Mean0=np.mean(bData0,axis=0)\n  Mean1=np.mean(bData1,axis=0)\n  \n  RegCov0=Cov0.covariance_\n  RegCov1=Cov1.covariance_\n  \n  return RegCov0,RegCov1,Mean0,Mean1\n\n#Calculation of the coefficients \ndef KullbackLeiberCoefficients(CovarianceA,CovarianceB):\n  \n  invA=np.linalg.inv(CovarianceA)\n  coef1a=np.dot(CovarianceB,invA)\n  _,coefa=np.linalg.slogdet(coef1a)\n  coef1b=np.dot(invA,CovarianceB)\n  coefb=np.trace(coef1b)\n  \n  return invA,coefa,coefb\n\n#Kullback Leiber divergence for each sample\ndef KullbackLeiberDivergence(CoefficientA,CoefficientB,CoefficientC,Mean,Sample):\n  \n  distance=(ds.mahalanobis(Mean,Sample,CoefficientA))**2\n  divergence=CoefficientC+distance-CoefficientB-len(Mean)\n  \n  return divergence/2\n\n#Wrapper function for the Kullback Leiber Coefficients \ndef MakeModelParams(Data,ColumnIndex):\n  \n  cData=Data\n  Cov0,Cov1,Mean0,Mean1=GetModelParams(cData,ColumnIndex)\n  Inv0,CoefA0,CoefB0=KullbackLeiberCoefficients(Cov0,Cov1)\n  Inv1,CoefA1,CoefB1=KullbackLeiberCoefficients(Cov1,Cov0)\n  \n  return Mean0,Mean1,Inv0,CoefA0,CoefB0,Inv1,CoefA1,CoefB1\n\n#Calculation of the divergence ratios\ndef SampleFeatures(Sample,Params):\n  \n  cSample=Sample\n  Mean0,Mean1=Params[0],Params[1]\n  Inv0,CoefA0,CoefB0=Params[2],Params[3],Params[4]\n  Inv1,CoefA1,CoefB1=Params[5],Params[6],Params[7]\n  \n  div00=KullbackLeiberDivergence(Inv0,CoefA0,CoefB0,np.array(Mean0),cSample)\n  div01=KullbackLeiberDivergence(Inv0,CoefA0,CoefB0,np.array(Mean1),cSample)\n  div10=KullbackLeiberDivergence(Inv1,CoefA1,CoefB1,np.array(Mean0),cSample)\n  div11=KullbackLeiberDivergence(Inv1,CoefA1,CoefB1,np.array(Mean1),cSample)\n  \n  return [(div00-div10)/(div10+div00),(div01-div11)/(div11+div01)]\n\n#Model Features \ndef ModelFeatures(Data,Params,ColumnIndex):\n  \n  cData=Data\n  trainData=np.array(cData[ColumnIndex])  \n  container=[]\n  \n  for k in range(len(trainData)):\n    \n    cSample=trainData[k]\n    container.append(SampleFeatures(cSample,Params))\n    \n  return np.array(container)\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's select a random data set from the train data set. "},{"metadata":{"trusted":true},"cell_type":"code","source":"Xtrain=pd.read_csv('../input/train.csv')\nDataColumns=[c for c in Xtrain.columns if c not in ['id','target','wheezy-copper-turtle-magic']]\n\nModelNumber=np.random.randint(0,512)\ntrain2 = Xtrain[Xtrain['wheezy-copper-turtle-magic']==ModelNumber]\nVars=train2.std(axis=0)\nVarColumns=[Vars.index[k] for k in range(len(Vars)) if Vars.iloc[k]>1.5]\nParams=MakeModelParams(train2,VarColumns)\nmfeat=ModelFeatures(train2,Params,VarColumns)\nlabls=np.array(train2['target'])\n\nbls=[k for k in range(len(labls)) if labls[k]==0]\nrds=[k for k in range(len(labls)) if labls[k]==1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It can be noticed that both classes have a separation point around 0."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(mfeat[bls,0],mfeat[bls,1],'bo',alpha=0.15)\nplt.plot(mfeat[rds,0],mfeat[rds,1],'ro',alpha=0.15)\n\nax=plt.gca()\nPlotStyle(ax,'','MAPE-divergence 01','MAPE-divergence 10')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reg=LogisticRegression(solver='lbfgs',tol=1e-4,random_state =256)\nreg.fit(mfeat,labls)\n\nsvcLinear=NuSVC(kernel='linear',probability=True,random_state=256)\nsvcLinear.fit(mfeat,labls)\n\nsvcPoly=NuSVC(gamma='scale',kernel='poly',degree=3,probability=True,random_state=256)\nsvcPoly.fit(mfeat,labls)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By taking the MAPE from the divergences values, a linear classifier can be used  over the training data."},{"metadata":{"trusted":true},"cell_type":"code","source":"f, (ax0, ax1,ax2) = plt.subplots(1, 3,figsize=(15,5))\n\nPlotDecisionFunction(mfeat,labls,reg,200,ax0)\nPlotStyle(ax0,'Logistic Regression','MAPE-divergence 01','MAPE-divergence 10')\nPlotDecisionFunction(mfeat,labls,svcLinear,200,ax1)\nPlotStyle(ax1,'SVC Linear','MAPE-divergence 01','MAPE-divergence 10')\nPlotDecisionFunction(mfeat,labls,svcPoly,200,ax2)\nPlotStyle(ax2,'SVC Poly','MAPE-divergence 01','MAPE-divergence 10')\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By using only this pair of features a CV of 0.95 can be achieved. I did not use it for the final submission, however, a similar feature has the same behavior, just changes the separation point. The remaining features added to the CV around 0.14. As it increases the accuracy for a linear classifier, I thought that it is worth showing it."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}