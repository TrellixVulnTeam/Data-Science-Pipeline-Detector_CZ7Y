{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n\nfrom keras import layers, Input, Model, models\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\nfrom sklearn.metrics import roc_auc_score\nfrom keras import regularizers\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"データのロード"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"データの確認"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"欠損値を確認する"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().sum(axis=0).sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.isnull().sum(axis=0).sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"欠損値なし"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"わかったこと\n- 分類すべきTargetはちょうど半分程度入っている\n- データは±20程度に収まっていそう、\n\n次に分布を確認する。\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"feature = train.drop(\"target\", axis=1).drop(\"id\", axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plt.violinplot(train.drop(\"target\", axis=1))\n#plt.violinplot(train[\"muggy-smalt-axolotl-pembus\"],train[\"jumpy-thistle-discus-sorted\"])\n\nf, ax = plt.subplots(5, 1, figsize=(20, 8), sharex=True)\n#sns.violinplot(data=train.drop(\"target\", axis=1),ax=ax)\n\n\nfor cname in feature.columns[0:50]:\n    sns.distplot(feature[cname].sample(1000), ax=ax[0])\nfor cname in feature.columns[50:100]:\n    sns.distplot(feature[cname].sample(1000), ax=ax[1])\nfor cname in feature.columns[100:150]:\n    sns.distplot(feature[cname].sample(100), ax=ax[2])\nfor cname in feature.columns[150:200]:\n    sns.distplot(feature[cname].sample(1000), ax=ax[3])\nfor cname in feature.columns[200:258]:\n    sns.distplot(feature[cname].sample(1000), ax=ax[4])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ほとんどのデータが同じように分布しているように見えるが・・・3つ目のグラフだけ飛びぬけて大きな値を持っている。\n\n列100番～150番にイレギュラーな値がありそうなので、詳しく見てみる\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"feature.describe().sort_values(by='max',ascending=False,axis=1)     ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"wheezy-copper-turtle-magic\tが怪しい。"},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(1, 1, figsize=(20, 8), sharex=True)\nsns.distplot(feature[\"wheezy-copper-turtle-magic\"].sample(1000), ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature[\"wheezy-copper-turtle-magic\"].sample(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"wheezy-copper-turtle-magicの値だけが飛びぬけて大きく、また、0以上の整数値が一様分布のように入っている。\nもしかしたら、カテゴリ変数かもしれない。\n\nこれを除外してもう一度分布を確認してみる"},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(5, 1, figsize=(20, 8), sharex=True)\n#sns.violinplot(data=train.drop(\"target\", axis=1),ax=ax)\n\nfeature_nolarge = feature.drop(\"wheezy-copper-turtle-magic\", axis=1)\n\nfor cname in feature_nolarge.columns[0:50]:\n    sns.distplot(feature_nolarge[cname].sample(1000), ax=ax[0])\nfor cname in feature_nolarge.columns[50:100]:\n    sns.distplot(feature_nolarge[cname].sample(1000), ax=ax[1])\nfor cname in feature_nolarge.columns[100:150]:\n    sns.distplot(feature_nolarge[cname].sample(1000), ax=ax[2])\nfor cname in feature_nolarge.columns[150:200]:\n    sns.distplot(feature_nolarge[cname].sample(1000), ax=ax[3])\nfor cname in feature_nolarge.columns[200:257]:\n    sns.distplot(feature_nolarge[cname].sample(1000), ax=ax[4])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"残りのデータはほとんど同じ分布をしているので、カテゴリ変数はないこと、正規化は不要であることが判断。"},{"metadata":{},"cell_type":"markdown","source":"モデル作成の準備として、訓練用データを分割する"},{"metadata":{"trusted":true},"cell_type":"code","source":"trn_x, valid_x, trn_y, valid_y = train_test_split(train.drop(['id', 'target'], axis=1), train.target, random_state=33, test_size=0.15)\ntrn_x.shape, valid_x.shape, trn_y.shape, valid_y.shape\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"適当にモデルを構築する。確率で提出するため、最後の段の活性化関数はシグモイドにする。"},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model():\n    inp = Input(shape=(trn_x.shape[1],), name='input')\n    x = layers.Dense(1000, activation='relu')(inp)\n    x = layers.Dense(750, activation='relu')(x)\n    x = layers.Dense(500, activation='relu')(x)\n    x = layers.Dense(1, activation='sigmoid')(x)\n    \n    model = Model(inp, x)\n    model.compile(optimizer='adam',\n                 loss='binary_crossentropy', metrics=['acc'])\n    \n    return model\n\nmodel = build_model()\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"学習"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nmodel = build_model()\n\nweights_path = f'weights.best.hdf5'\nval_loss_checkpoint = ModelCheckpoint(weights_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\nreduceLR = ReduceLROnPlateau(monitor='val_loss', patience=5, verbose=1, mode='min', min_lr=1e-6)\n\nmodel.fit(trn_x, trn_y, epochs=80, validation_data=(valid_x, valid_y),\n         callbacks=[val_loss_checkpoint, reduceLR], batch_size=512, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"評価"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.load_weights(weights_path)\nval_preds = model.predict(valid_x, batch_size=2048, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_auc_score(valid_y.values, val_preds.reshape(-1))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"50％の正解率なら適当に答えているのと同じなので、性能があまりよくない。\n\nカテゴリ変数と思しき特徴量をダミー変数に変換してみる。"},{"metadata":{"trusted":true},"cell_type":"code","source":"trn_wheezy = pd.get_dummies(trn_x['wheezy-copper-turtle-magic'])\nvalid_wheezy = pd.get_dummies(valid_x['wheezy-copper-turtle-magic'])\ntest_wheezy = pd.get_dummies(test['wheezy-copper-turtle-magic'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trn_x.drop('wheezy-copper-turtle-magic', axis=1, inplace=True)\nvalid_x.drop('wheezy-copper-turtle-magic', axis=1, inplace=True)\ntest.drop('wheezy-copper-turtle-magic', axis=1, inplace=True)\ntest.drop('id', axis=1, inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trn_x = np.concatenate([trn_x, trn_wheezy.values], axis=1)\nvalid_x = np.concatenate([valid_x, valid_wheezy.values], axis=1)\ntest = np.concatenate([test, test_wheezy.values], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trn_x.shape, valid_x.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"学習をリトライ"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = build_model()\nmodel.fit(trn_x, trn_y, epochs=80, validation_data=(valid_x, valid_y),\n         callbacks=[val_loss_checkpoint, reduceLR], batch_size=512, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.load_weights(weights_path)\nval_preds = model.predict(valid_x, batch_size=2048, verbose=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_auc_score(valid_y.values, val_preds.reshape(-1))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"性能がわずかに向上した。\n\naccとval_accの差が大きく、過学習しているかもしれない。\n過学習していないか確認するため重みを調べてみる"},{"metadata":{"trusted":true},"cell_type":"code","source":"for item in model.get_weights():\n    print(np.sort(item))\n    print(\"------------------------\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"オーダーの差が激しいわけではないので、ここだけでは過学習か判断できない。\n\nとはいえ、ドロップアウト層をいれてみる。"},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model2():\n    inp = Input(shape=(trn_x.shape[1],), name='input')\n    x = layers.Dense(1000, activation='relu')(inp)\n    x = layers.Dropout(0.6)(x)\n    x = layers.Dense(750, activation='relu')(x)\n    x = layers.Dropout(0.6)(x)\n    x = layers.Dense(500, activation='relu')(x)\n    x = layers.Dropout(0.6)(x)\n    x = layers.Dense(1, activation='sigmoid')(x)\n    \n    model = Model(inp, x)\n    model.compile(optimizer='adam',\n                 loss='binary_crossentropy', metrics=['acc'])\n    \n    return model\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nmodel = build_model2()\n\nmodel.fit(trn_x, trn_y, epochs=80, validation_data=(valid_x, valid_y),\n         callbacks=[val_loss_checkpoint, reduceLR], batch_size=512, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.load_weights(weights_path)\nval_preds = model.predict(valid_x, batch_size=2048, verbose=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_auc_score(valid_y.values, val_preds.reshape(-1))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ぐっと性能向上した。\n重みを見てもわからなかったが、過学習が改善されたのだろう。"},{"metadata":{},"cell_type":"markdown","source":"提出用データの作成"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds = model.predict(test, batch_size=2048, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df = pd.read_csv(f'../input/sample_submission.csv')\nsub_df.target = test_preds.reshape(-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df.to_csv('solution.csv', index=False)\nsub_df.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}