{"cells":[{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"---\n# Theory"},{"metadata":{},"cell_type":"markdown","source":"In Gaussian Mixture model, we maximize likelihood function $P(X_{train}|\\pi,\\mu,\\Sigma)$ about $\\pi, \\mu, \\Sigma$ by using EM algorithm (here, $\\pi$ means distribution parameter of label Y, $\\mu$ and $\\Sigma$ are set of mean vector and covariance matrix for each categories).\n\nIn this competiton, we have $X_{train}, Y_{tain}, X_{test}$. Gaussian Mixture model can treat $X_{train}, X_{test}$, and QDA (and other normal supervised model) can treat $X_{train}, Y_{train}$. Neither of them can treat $X_{train}, Y_{tain}, X_{test}$ at the same time.\n\nThen I tried to modify EM algorithm for Gaussian Mixture as to treat these at the same time. Equations are slightly complicated, but result is not so difficult. Please see chapter 9 of PRML book for detail (I used notation Z for representation of label-variable instead of Y, because I deduct formulas based on PRML)."},{"metadata":{},"cell_type":"markdown","source":"$$\n\\begin{align}\nP(X_{train},Y_{train},X_{test}|\\pi,\\mu,\\Sigma)\n&= P(X_{train},Y_{train}|\\pi,\\mu,\\Sigma)P(X_{test}|\\pi,\\mu,\\Sigma) \\\\\n&= \\Pi_{n=1}^{N_{train}}(\\Pi_{k=1}^K \\pi_k^{z_{nk}} N(x_n|\\mu_k,\\Sigma_k)^{z_{nk}})\n\\Pi_{n=1}^{N_{test}}\\Sigma_{k=1}^K \\pi_k N(x_n|\\mu_k,\\Sigma_k)\n\\end{align}\n$$\nand take log of this becomes below (log-likelihood).\n$$\n\\log P = \\Sigma_{n=1}^{N_{train}}\\Sigma_{k=1}^K z_{nk}(\\log \\pi_k + \\log N(x_n|\\mu_k,\\Sigma_k))\n+\\Sigma_{n=1}^{N_{test}}\\log\\Sigma_{k=1}^K \\pi_k N(x_n|\\mu_k,\\Sigma_k)\n$$"},{"metadata":{},"cell_type":"markdown","source":"take derivatives of this is\n$$\n\\frac{\\partial\\log P}{\\partial\\mu_s}\n=\\Sigma_{n=1}^{N_{train}}z_{ns}\\Sigma^{-1}_s(x_n-\\mu_s)\n+\\Sigma_{n=1}^{N_{test}}\\frac{\\pi_s N(x_n|\\mu_s,\\Sigma_s)}{\\Sigma_{k=1}^K\\pi_kN(x_n|\\mu_k,\\Sigma_k)}\n\\Sigma^{-1}_s(x_n-\\mu_s)=0\n$$\ntherefore, if we assume $\\Sigma_s$ is non-singular and define as below,\n$$\n\\gamma(z_{ns}):=\\frac{\\pi_s N(x_n|\\mu_s,\\Sigma_s)}{\\Sigma_{k=1}^K\\pi_k N(x_n|\\mu_k,\\Sigma_k)}\n$$\nwe get\n$$\n\\mu_s = \\frac{\\Sigma_{n=1}^{N_{train}}z_{ns}x_n+\\Sigma_{n=1}^{N_{test}}\\gamma(z_{ns})x_n}\n{\\Sigma_{n=1}^{N_{train}}z_{ns}+\\Sigma_{n=1}^{N_{test}}\\gamma(z_{ns})}.\n$$"},{"metadata":{},"cell_type":"markdown","source":"Let's compare this to normal Gaussian Mixture model.\n$$\nNormal Gaussian Mixture \\\\\n\\mu_s = \\frac{\\Sigma_{n=1}^{N_{test}}\\gamma(z_{ns})x_n}\n{\\Sigma_{n=1}^{N_{test}}\\gamma(z_{ns})}.\n$$\nThe meaning is very simple. We just use actual $z_{nk}$ instead of $\\gamma(z_{ns})$ for taking weighted average step."},{"metadata":{},"cell_type":"markdown","source":"Then I finally get\n$$\n\\begin{align}\n\\mu_s &=\\frac{\\Sigma_{n=1}^{N_{train}}z_{ns}x_n+\\Sigma_{n=1}^{N_{test}}\\gamma(z_{ns})x_n}\n{\\Sigma_{n=1}^{N_{train}}z_{ns}+\\Sigma_{n=1}^{N_{test}}\\gamma(z_{ns})} \\\\\n\\Sigma_s &=\\frac{\\Sigma_{n=1}^{N_{train}}z_{ns}(x_n-\\mu_s)(x_n-\\mu_s)^T+\\Sigma_{n=1}^{N_{test}}\\gamma(z_{ns})(x_n-\\mu_s)(x_n-\\mu_s)^T}\n{\\Sigma_{n=1}^{N_{train}}z_{ns}+\\Sigma_{n=1}^{N_{test}}\\gamma(z_{ns})} \\\\\n\\pi_s &=\\frac{\\Sigma_{n=1}^{N_{train}}z_{ns}+\\Sigma_{n=1}^{N_{test}}\\gamma(z_{ns})}\n{\\Sigma_t(\\Sigma_{n=1}^{N_{train}}z_{nt}+\\Sigma_{n=1}^{N_{test}}\\gamma(z_{nt}))}\n\\end{align}\n$$\n(Sorry, I deduct other than $\\mu_s$ intuitively, then they may be wrong)."},{"metadata":{},"cell_type":"markdown","source":"Above is the contents of previous Kernel.\nBut this procedure cannot treat sub-clustered problem.  \nThen I modified $Z_{train}$ part as not use actual data directly but masking and normalize it as below."},{"metadata":{},"cell_type":"markdown","source":"$initial\\ value\\ of\\ Z_{train}[i] = [1/3, 1/3, 1/3, 0, 0, 0]\\ for\\ Y_{train}[i]=0$ (assumed number of clusters in category is 3).  \nand $Z_{train}$ is updated using $\\gamma(z_{ns})$ like $Z_{test}$ but masked by actual data and normalize in E step\n(it is easy to understand in line in Modeling section)."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for submission\ndf_train = pd.read_csv('../input/train.csv')\ndf_test = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wcrms = df_train['wheezy-copper-turtle-magic'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n# Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom scipy import stats\n\nclass SSMCGaussianMixture(object):\n    def __init__(self, n_features, n_categories, n_clusters=3):\n        self.n_features = n_features\n        self.n_categories = n_categories\n        self.n_clusters = n_clusters\n        \n        self.mus = np.array([np.random.randn(n_features)]*n_categories*n_clusters)\n        self.sigmas = np.array([np.eye(n_features)]*n_categories*n_clusters)\n        self.pis = np.array([1/(n_categories*n_clusters)]*(n_categories*n_clusters))\n        \n        \n    def fit(self, X_train, y_train, X_test, threshold=0.00001, max_iter=100):\n        Z_train_mask = np.zeros(shape=(len(X_train), self.n_categories*self.n_clusters))\n        for i in range(len(y_train)):\n            Z_train_mask[i, y_train[i]*self.n_clusters:(y_train[i]+1)*self.n_clusters]=1\n            \n        Z_train = Z_train_mask/self.n_clusters\n        \n        for i in range(max_iter):\n        # EM algorithm\n            # M step\n            # Here is the mainly updated section.\n            Z_train = np.array([self.gamma(X_train, k) for k in range(self.n_categories*self.n_clusters)]).T\n            Z_train *= Z_train_mask\n            Z_train /= Z_train.sum(axis=1, keepdims=True)\n            Z_test = np.array([self.gamma(X_test, k) for k in range(self.n_categories*self.n_clusters)]).T\n            Z_test /= Z_test.sum(axis=1, keepdims=True)\n        \n            # E step\n            datas = [X_train, Z_train, X_test, Z_test]\n            mus = np.array([self._est_mu(k, *datas) for k in range(self.n_categories*self.n_clusters)])\n            sigmas = np.array([self._est_sigma(k, *datas) for k in range(self.n_categories*self.n_clusters)])\n            pis = np.array([self._est_pi(k, *datas) for k in range(self.n_categories*self.n_clusters)])\n            \n            diff = max(np.max(np.abs(mus-self.mus)), \n                       np.max(np.abs(sigmas-self.sigmas)), \n                       np.max(np.abs(pis-self.pis)))\n            #print(diff)\n            # below is for avoiding sigma becomes singluar\n            sigmas += np.array([np.eye(self.n_features)]*self.n_categories*self.n_clusters)*0.00000001\n            self.mus = mus\n            self.sigmas = sigmas\n            self.pis = pis\n            if diff<threshold:\n                break\n                \n                \n    def predict_proba(self, X):\n        Z_pred = np.array([self.gamma(X, k) for k in range(self.n_categories*self.n_clusters)]).T\n        Z_pred /= Z_pred.sum(axis=1, keepdims=True)\n        Y_pred = np.zeros(shape=(len(X), self.n_categories))\n        for i in range(self.n_categories):\n            Y_pred[:,i] = Z_pred[:,i*self.n_clusters:(i+1)*self.n_clusters].sum(axis=1)\n        return Y_pred\n\n\n    def gamma(self, X, k):\n        # X is input vectors, k is feature index\n        return stats.multivariate_normal.pdf(X, mean=self.mus[k], cov=self.sigmas[k])\n        \n    def _est_mu(self, k, X_train, Z_train, X_test, Z_test):\n        mu = (Z_train[:,k]@X_train + Z_test[:,k]@X_test).T / \\\n                 (Z_train[:,k].sum() + Z_test[:,k].sum())\n        return mu\n    \n    def _est_sigma(self, k, X_train, Z_train, X_test, Z_test):\n        cmp1 = (X_train-self.mus[k]).T@np.diag(Z_train[:,k])@(X_train-self.mus[k])\n        cmp2 = (X_test-self.mus[k]).T@np.diag(Z_test[:,k])@(X_test-self.mus[k])\n        sigma = (cmp1+cmp2) / (Z_train[:,k].sum() + Z_test[:k].sum())\n        return sigma\n        \n    def _est_pi(self, k, X_train, Z_train, X_test, Z_test):\n        pi = (Z_train[:,k].sum() + Z_test[:,k].sum()) / \\\n                 (Z_train.sum() + Z_test.sum())\n        return pi\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Below is just a lapper object.\n\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import KFold\n\nclass BaseClassifier(object):\n    def __init__(self):\n        self.preprocess = Pipeline([('vt', VarianceThreshold(threshold=2)), ('scaler', StandardScaler())])\n \n\n    def fit(self, X_train, y_train, X_test, cv_qda=2, cv_meta=2):\n        X_train_org = X_train\n        self.preprocess_tune(np.vstack([X_train, X_test]))\n        X_train = self.preprocess.transform(X_train)\n        X_test = self.preprocess.transform(X_test)\n        \n        self.cgm = SSMCGaussianMixture(n_features=X_train.shape[1], n_categories=2)\n        # to avoid error, I cut validation process.\n        # self.validation(X_train_org, y_train)\n        self.cgm.fit(X_train, y_train, X_test)\n\n    \n    def predict(self, X):\n        X = self.preprocess.transform(X)\n        return self.cgm.predict_proba(X)[:,1]\n    \n    \n    def preprocess_tune(self, X):\n        self.preprocess.fit(X)\n                \n        \n    def validation(self, X, y):\n        X = self.preprocess.transform(X)\n        kf = KFold(n_splits=3, shuffle=True)\n        scores = []\n        for train_index, test_index in kf.split(X):\n            X_train, X_test = X[train_index], X[test_index]\n            y_train, y_test = y[train_index], y[test_index]\n            self.cgm.fit(X_train, y_train, X_test)\n            y_pred = self.cgm.predict_proba(X_test)[:,1]\n            scores.append(roc_auc_score(y_test, y_pred))\n        self.score = np.array(scores).mean()\n        print('validation score = ', self.score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_sample = df_train[df_train['wheezy-copper-turtle-magic']==wcrms[3]]\nX_train_sample = df_train_sample.drop(['id', 'target', 'wheezy-copper-turtle-magic'], axis=1).values\ny_train_sample = df_train_sample['target'].values\n\ndf_test_sample = df_test[df_test['wheezy-copper-turtle-magic']==wcrms[3]]\nX_test_sample = df_test_sample.drop(['id', 'wheezy-copper-turtle-magic'], axis=1).values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bc = BaseClassifier()\nbc.fit(X_train_sample, y_train_sample, X_test_sample)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ConsolEstimator(object):\n    def __init__(self, ids):\n        self.clfs = {}\n        self.id_column = 'wheezy-copper-turtle-magic'\n        self.ids = ids\n        \n        \n    def predict(self, df_X):\n        y_pred = np.zeros(shape=(len(df_X)))\n        for id in df_X[self.id_column].unique():\n            id_rows = (df_X[self.id_column]==id)\n            X = df_X.drop(['id', self.id_column], axis=1).values[id_rows]\n            y_pred[id_rows] = self.clfs[id].predict(X)\n        return y_pred\n            \n        \n    def fit(self, df_train, df_test):\n        for i, id in enumerate(self.ids):\n            print(i, 'th training...')\n            df_train_id = df_train[df_train[self.id_column]==id]\n            df_test_id = df_test[df_test[self.id_column]==id]\n            if len(df_train_id)==0 or len(df_test_id)==0:\n                continue\n            \n            X_train = df_train_id.drop(['id', 'target', self.id_column], axis=1).values\n            y_train = df_train_id['target'].values\n            X_test = df_test_id.drop(['id', self.id_column], axis=1).values\n            \n            self.clfs[id] = BaseClassifier()\n            self.clfs[id].fit(X_train, y_train, X_test)\n            \n        # ˜print('mean score = ', np.array([clf.score for clf in self.clfs.values()]).mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ce = ConsolEstimator(ids=wcrms)\nce.fit(df_train, df_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n## Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = ce.predict(df_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_submission = pd.concat([df_test['id'], pd.Series(y_pred, name='target')], axis=1)\ndf_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}