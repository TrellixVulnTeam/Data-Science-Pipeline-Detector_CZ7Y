{"cells":[{"metadata":{"_cell_guid":"d304b92d-60db-48c3-8237-e66d86afb9b4","_uuid":"69231124cb006aae356154f7a34411645374a0eb"},"cell_type":"markdown","source":"# Overview\nThe script pre-reads through all the images and assesses their categories. This should make it easier to focus training on specific labels or groups of labels since not all occur in all images"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","collapsed":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom glob import glob\nimport matplotlib.pyplot as plt\nfrom skimage.io import imread\nfrom skimage.segmentation import mark_boundaries\nDATA_DIR = os.path.join('..', 'input')","execution_count":2,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"class_str = \"\"\"car, 33\nmotorbicycle, 34\nbicycle, 35\nperson, 36\nrider, 37\ntruck, 38\nbus, 39\ntricycle, 40\nothers, 0\nrover, 1\nsky, 17\ncar_groups, 161\nmotorbicycle_group, 162\nbicycle_group, 163\nperson_group, 164\nrider_group, 165\ntruck_group, 166\nbus_group, 167\ntricycle_group, 168\nroad, 49\nsiderwalk, 50\ntraffic_cone, 65\nroad_pile, 66\nfence, 67\ntraffic_light, 81\npole, 82\ntraffic_sign, 83\nwall, 84\ndustbin, 85\nbillboard, 86\nbuilding, 97\nbridge, 98\ntunnel, 99\noverpass, 100\nvegatation, 113\nunlabeled, 255\"\"\"\nclass_dict = {v.split(', ')[0]: int(v.split(', ')[-1]) for v in class_str.split('\\n')}\ndef get_label_info(in_path):\n    idx_image = imread(in_path)//1000\n    out_dict = {'dim': idx_image.shape}\n    count_dict = {k: np.sum(idx_image==k) for k in np.unique(idx_image)}\n    for k,v in class_dict.items():\n        out_dict[k] = count_dict.get(v, 0)*1.0/np.prod(idx_image.shape[0:2])\n    return out_dict","execution_count":3,"outputs":[]},{"metadata":{"_cell_guid":"6098a265-8f65-4b09-aa14-c0906ba06776","_uuid":"25cc2323ae74135f3150e1a044c4a3ce967a274b","trusted":true},"cell_type":"code","source":"all_paths = pd.DataFrame(dict(path = glob(os.path.join(DATA_DIR, '*', '*.*p*g'))))\nclassdict = {0:'others', 1:'rover', 17:'sky', 33:'car', 34:'motorbicycle', 35:'bicycle', 36:'person', 37:'rider', 38:'truck', 39:'bus', 40:'tricycle', 49:'road', 50:'siderwalk', 65:'traffic_cone'}\nall_paths['split'] = all_paths['path'].map(lambda x: x.split('/')[-2].split('_')[0])\nall_paths['group'] = all_paths['path'].map(lambda x: x.split('/')[-2].split('_')[-1])\nall_paths['group'] = all_paths['group'].map(lambda x: 'color' if x == 'test' else x)\nall_paths['id'] = all_paths['path'].map(lambda x: '_'.join(os.path.splitext(os.path.basename(x))[0].split('_')[:4]))\nall_paths.sample(5)","execution_count":4,"outputs":[]},{"metadata":{"_cell_guid":"d18ebca1-7a33-4358-9557-9b324b549f60","_uuid":"0bc5a92bfa970cfd251c6f6619f723161e42d87f","trusted":true},"cell_type":"code","source":"group_df = all_paths.pivot_table(values = 'path', columns = 'group', aggfunc = 'first', index = ['id', 'split']).reset_index()\ngroup_df.sample(5)","execution_count":5,"outputs":[]},{"metadata":{"_cell_guid":"3ee6260e-1115-49a5-9ad7-ed39bb78bbe4","_uuid":"cb96ad7d86632143b5c212f94369afa3ee1247e1","trusted":true},"cell_type":"code","source":"train_df = group_df.query('split==\"train\"')\nprint(train_df.shape[0], 'rows')\nsample_rows = 6\nfig, m_axs = plt.subplots(sample_rows, 3, figsize = (20, 6*sample_rows))\n[c_ax.axis('off') for c_ax in m_axs.flatten()]\nout_rows = []\nfor (ax1, ax2, ax3), (_, c_row) in zip(m_axs, train_df.sample(sample_rows).iterrows()):\n    c_img = imread(c_row['color'])\n    l_img = imread(c_row['label'])//1000\n    ax1.imshow(c_img)\n    ax1.set_title('Color')\n    ax2.imshow(l_img, cmap = 'nipy_spectral')\n    ax2.set_title('{car}'.format(**get_label_info(c_row['label'])))\n    xd, yd = np.where(l_img>0)\n    bound_img = mark_boundaries(image = c_img, label_img = l_img, color = (1,0,0), background_label = 255, mode = 'thick')\n    ax3.imshow(bound_img[xd.min():xd.max(), yd.min():yd.max(),:])\n    ax3.set_title('Cropped Overlay')\n    out_rows += [get_label_info(c_row['label'])]","execution_count":6,"outputs":[]},{"metadata":{"_cell_guid":"7a390ec3-ba65-4933-b868-aeabc36f2487","_uuid":"1c4f043ef2b22909957e2b767960f13b656304fb","trusted":true},"cell_type":"code","source":"pd.DataFrame(out_rows)","execution_count":7,"outputs":[]},{"metadata":{"_uuid":"e0cde8d49895dd83b49979d1856682c03a948c3d"},"cell_type":"markdown","source":"# Create overview for all images\nWe want to create this overview for all images, but to do it serially takes too long"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b12ea9ec752316f17f456228488aa1f1b2b425e2"},"cell_type":"code","source":"def read_row(in_row):\n    return dict(**in_row, **get_label_info(in_row['label']))","execution_count":24,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7e5bcffabc67225549f4c94d76a5d3170c40013f"},"cell_type":"code","source":"%%time\nall_rows = []\nfor _, c_row in list(train_df.sample(40).iterrows()):\n    all_rows += [read_row(c_row.to_dict())]","execution_count":25,"outputs":[]},{"metadata":{"_uuid":"bf70b9217e214ae395ff1ea4dad6a5dddd66da52"},"cell_type":"markdown","source":"Dask let's us speed up the processes substantially by utilizing multiple cores"},{"metadata":{"trusted":true,"_uuid":"f69913c29a9b3059ab372f4c67b24bce43a1073d"},"cell_type":"code","source":"%%time\nfrom dask import bag\nsome_rows = bag.from_sequence([x.to_dict() for _, x in train_df.sample(40).iterrows()]).map(read_row)\n_ = some_rows.compute()","execution_count":26,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e3a984bdc5975cd0037ac3e697703cb14ceea047"},"cell_type":"code","source":"all_rows = bag.from_sequence([x.to_dict() for _, x in train_df.iterrows()], npartitions = 10000).map(read_row)","execution_count":27,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c2926847f3e4b8d8cdc17ac4ba5b3aac966f801b"},"cell_type":"code","source":"%%time\nall_rows_df = pd.DataFrame(all_rows.compute())","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0ec73e1d-1613-4af9-bbe3-ca00c084d9d8","_uuid":"25a2c2062e68c7800c16f0cdddca24ab835eb73f","trusted":true},"cell_type":"code","source":"ordered_cols = list(train_df.columns)+['dim']\nfor c_col in all_rows_df.columns:\n    if c_col not in ordered_cols:\n        ordered_cols += [c_col]\nall_rows_df = all_rows_df[ordered_cols]\nall_rows_df.to_csv('label_breakdown.csv')\nall_rows_df.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9be0aae2-7004-4855-871a-c3b17e9f8c9f","_uuid":"cdd3e5291fc745321486a7aeb8eca759c8159934"},"cell_type":"markdown","source":"# Show some lazy visualizations and stats\nThis helps us understand the frequency and co-occurence of certain tags"},{"metadata":{"_cell_guid":"93ccf784-0ead-425a-90a1-aa631d454bad","_uuid":"a97fdcdcd3104cfdcecf28ed5f7f658acd279f83","trusted":true,"collapsed":true},"cell_type":"code","source":"import seaborn as sns\nall_keys = list(class_dict.keys())\nfor i in range(0, len(all_keys), 4):\n    sns.pairplot(all_rows_df[all_keys[i:(i+4)]])","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"8ed8dc26-3c09-44cd-a06d-ce3356e26a95","_uuid":"25a61ae32ef7ba187eaf92c4ae1d11dd1bfe471c","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}