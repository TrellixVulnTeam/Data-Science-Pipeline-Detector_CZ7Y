{"cells":[{"metadata":{"_cell_guid":"ca465211-a12e-498f-adc4-1cf2e87ef948","_uuid":"3dd5bd75262f0fddd9a0e18778542f7b9530aba6","trusted":false,"collapsed":true},"cell_type":"code","source":"#__author__ = 'kjeanclaude: https://kaggle.com/kjeanclaude'\n# Inspired from this kernel https://www.kaggle.com/keegil/keras-u-net-starter-lb-0-233\nimport os\nimport sys\nimport random\nimport warnings\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nfrom tqdm import tqdm\nfrom itertools import chain\nfrom skimage.io import imread, imshow, imread_collection, concatenate_images\nfrom skimage.transform import resize\nfrom scipy.misc import imresize \nfrom skimage.morphology import label\nfrom PIL import Image\n\nfrom keras.models import Model, load_model\nfrom keras.layers import Input\nfrom keras.layers.core import Lambda\nfrom keras.layers.convolutional import Conv2D, Conv2DTranspose\nfrom keras.layers.pooling import MaxPooling2D\nfrom keras.layers.merge import concatenate\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras import backend as K\n\nimport tensorflow as tf\n\n%matplotlib inline\n\n# Set some parameters\nIMG_WIDTH = 128\nIMG_HEIGHT = 128\nIMG_CHANNELS = 3\n\n# First, we work on the train-test-samples from my (https://www.kaggle.com/kjeanclaude/download-train-test-samples-for-local-prototype) kernel. \n# We could generalize later on the whole competition dataset.\nTRAIN_PATH = '../input/download-train-test-samples-for-local-prototype/train/train_color/'\nLABEL_PATH = '../input/download-train-test-samples-for-local-prototype/train/train_label/'\nTEST_PATH = '../input/download-train-test-samples-for-local-prototype/test/'\n\n#print(os.listdir(\"../input/\"))\nwarnings.filterwarnings('ignore', category=UserWarning, module='skimage')\nseed = 77\nrandom.seed = seed\nnp.random.seed = seed\nprint('Done!')\n\nprint(\"tf.__version__ : \", tf.__version__)\nprint(\"python --version : \", sys.version)\nprint(\"numpy --version : \", np.__version__)\nPyVersion = sys.version","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3359520a-d26c-4c43-bc99-480a24b97701","_uuid":"48f808e14b252b24031d48c96b0d51a0ab895c63","trusted":false,"collapsed":true},"cell_type":"code","source":"!os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"fc87a624-166c-409d-a206-7c064705b316","_uuid":"c87d0a7c2c2748bb75e0ca6134ddd0115b70f92e"},"cell_type":"markdown","source":"*      *** Train and Test IDs ***"},{"metadata":{"collapsed":true,"_cell_guid":"e997150a-59ec-4dca-afb7-0fbe5c047c1e","_uuid":"26b92c0c8818099c618c5e5079fb36c29abff304","trusted":false},"cell_type":"code","source":"# Get train and test IDs \ntrain_ids = next(os.walk(TRAIN_PATH))[2] \nlabel_ids = next(os.walk(LABEL_PATH))[2] \ntest_ids = next(os.walk(TEST_PATH))[2] ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"97666d76-34e0-4ebb-b283-b7d2c8ac457d","_uuid":"d85ae1be9769ea7fff38d21aa1ac641add3995f2","trusted":false,"collapsed":true},"cell_type":"code","source":"train_ids[:2]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a9a4ba37-9172-4e39-a6e6-1c7c3252594e","_uuid":"ec0ffc32e740d534264e300bd9e3e6abcddeefef"},"cell_type":"markdown","source":"*      *** Dataset content visualization ***"},{"metadata":{"_cell_guid":"43a6b45d-5de7-42cd-99c4-23d3f7be812b","_uuid":"b6f6f5a71ae14669df4e37fa04af3598dfb2f901","trusted":false,"collapsed":true},"cell_type":"code","source":"im = imread('../input/download-train-test-samples-for-local-prototype/train/train_color/170927_064455855_Camera_5.jpg', as_grey=False)\nim.shape","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c7fc0fbd-c8d4-45b3-8dfa-7b1617b78a23","_uuid":"8a9bbbec20902e96f572599d3004ca4f0df22a6a","trusted":false,"collapsed":true},"cell_type":"code","source":"imshow(im)\nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5474789a-0ece-469a-be2c-0b76e02e39f4","_uuid":"4ba064ea36f8f3c230e142b814a3b1195866f57a","trusted":false,"collapsed":true},"cell_type":"code","source":"im2 = imread('../input/download-train-test-samples-for-local-prototype/train/train_label/170908_072650121_Camera_5_instanceIds.png', as_grey=True)\nim2.shape","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ab02913b-b951-4e2b-8be5-29e02f92e4fb","_uuid":"f975220766a689a3c78c7759479e6667fb2f8114","trusted":false,"collapsed":true},"cell_type":"code","source":"from scipy.misc import imresize \nim2 = np.reshape(np.array(im2),(im2.shape[0],im2.shape[1])) \nimg = imresize(im2, (IMG_HEIGHT, IMG_WIDTH), mode='L', interp='nearest') \nimg = np.reshape(img,(img.shape[0], img.shape[1])) \nimg.shape ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"40aeb819-1be0-4e98-9885-ffc73ad618a2","_uuid":"6bd37adbfb04c7cb7498a96c85a43c378bfbd85c","trusted":false,"collapsed":true},"cell_type":"code","source":"imshow(img)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"867521e8-90f6-4c56-bef7-3ac64259f730","_uuid":"addc878023b4dab904eed32f9208c6ae19c9d2eb","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f661eccf-4a44-45a2-9dd7-868f06b615a6","_uuid":"cfc118628764cb78ccf270ca9057645591f7c7b2"},"cell_type":"markdown","source":"## 1- Data Preprocessing"},{"metadata":{"_cell_guid":"9aa16ce0-80f4-47dc-a596-1701afc1726d","_uuid":"37f8e4924e3dbf924b71f6d3fab1017f027d320c","trusted":false,"collapsed":true},"cell_type":"code","source":"# Get and resize train images and masks\nX_train = np.zeros((len(train_ids), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\nY_train = np.zeros((len(train_ids), IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool)\n\n# dtype=np.float32\nfeatures_im_path = []\n#features_im_path.append(path) \nprint('Getting and resizing train images and masks ... ')\nsys.stdout.flush()\nfor n, id_ in tqdm(enumerate(train_ids), total=len(train_ids)):\n    path = TRAIN_PATH + id_\n    img = imread(path)[:,:,:IMG_CHANNELS]\n    img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n    X_train[n] = img\n\nfor n, id_ in tqdm(enumerate(label_ids), total=len(label_ids)):\n    path = LABEL_PATH + id_ \n    img = imread(path, as_grey=True) \n    img = np.reshape(np.array(img), (img.shape[0], img.shape[1])) # To transform as an numpy array first\n    #img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True) \n    img = imresize(img, (IMG_HEIGHT, IMG_WIDTH), mode='L', interp='nearest') \n    img = np.reshape(img,(img.shape[0], img.shape[1], 1)) \n    Y_train[n] = img \n\n\n# Get and resize test images\nX_test = np.zeros((len(test_ids), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\nsizes_test = []\nprint('Getting and resizing test images ... ')\nsys.stdout.flush()\nfor n, id_ in tqdm(enumerate(test_ids), total=len(test_ids)):\n    path = TEST_PATH + id_\n    img = imread(path)[:,:,:IMG_CHANNELS]\n    sizes_test.append([img.shape[0], img.shape[1]])\n    img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n    X_test[n] = img\n\nprint('Done!') ","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"18e25fe4-4535-43f5-9bf5-9382337eec19","_uuid":"f1e58a06da27b088c7a42d2d44e20e55b371afd0","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"1cefe39b-68a7-4793-82ff-1c8643795db7","_uuid":"7adc7e5a6e76d7974d2cf3cbd104fa845de41f8c","trusted":false},"cell_type":"code","source":"def display_samples(nb, train_ids):\n    #imgs_ids = [c for c in category_to_product.keys() if category_id in str(c)]\n    #ix = random.randint(0, len(train_ids))\n    ix = random.randint(nb, len(train_ids)-1)\n    print('ix value : ', ix)\n    imgs_ids = [] \n    imgs_ids.append(X_train[ix]) \n    #imgs_ids.append(X_train[ix]) \n    \n    ### For printing purposes, we should reshape it without the last channel\n    Y_2 = np.reshape(Y_train[ix],(Y_train[ix].shape[0], Y_train[ix].shape[1])) \n    imgs_ids.append(Y_2) \n    \n    print('len imgs_ids : ', len(imgs_ids))\n    fig, axs = plt.subplots(1, len(imgs_ids), figsize=(10, 4))\n    \n    print('axs : ', axs)\n    for i, ax_i in enumerate(axs):\n        ax_i.imshow(imgs_ids[i])\n        ax_i.set_title(i)\n        ax_i.grid('off')\n        ax_i.axis('off')\n        \n    #return imgs_ids","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5aedc868-c192-4666-a525-edc3199d8e14","_uuid":"04dd45e5cd1c86aff527cc3d4787468f1c7229a9","trusted":false,"collapsed":true},"cell_type":"code","source":"display_samples(0, train_ids) ","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"45618b2b-f28a-4aec-8628-912cd412e3e0","_uuid":"71053de5be3c5c030534cf979e882867ef518045","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"30433378-510c-4282-8381-06d7dc767d19","_uuid":"5a5fc467b2d90f45a4ead42a76b7bca2139d52f3"},"cell_type":"markdown","source":"## 2- Model building"},{"metadata":{"_cell_guid":"57b72c1d-1c14-4917-a731-ac767abeb538","_uuid":"f3fc5d80efc97718be9facff724d11980baca09b"},"cell_type":"markdown","source":"#### 2.1- The mean_iou (mean intersection-over-union) metric"},{"metadata":{"collapsed":true,"_cell_guid":"c2ac6ed4-ef42-44e2-91da-e563602ae0f2","_uuid":"60e107a95f4f042f90b5d2bf26573fbd7e46de05","trusted":false},"cell_type":"code","source":"# Define IoU metric\ndef mean_iou(y_true, y_pred):\n    prec = []\n    for t in np.arange(0.5, 1.0, 0.05):\n        y_pred_ = tf.to_int32(y_pred > t)\n        score, up_opt = tf.metrics.mean_iou(y_true, y_pred_, 7)\n        K.get_session().run(tf.local_variables_initializer())\n        with tf.control_dependencies([up_opt]):\n            score = tf.identity(score)\n        prec.append(score)\n    return K.mean(K.stack(prec))","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"ff0f12c9-9528-46c2-bd70-8bb0ba08258f","_uuid":"de87249ec831e899f9139d7c0950082a4660240b","trusted":false},"cell_type":"code","source":"def mean_iou_(y_pred,y_true):\n    y_pred_ = tf.to_int64(y_pred > 0.5)\n    y_true_ = tf.to_int64(y_true > 0.5)\n    score, up_opt = tf.metrics.mean_iou(y_true_, y_pred_, 7)\n    with tf.control_dependencies([up_opt]):\n        score = tf.identity(score)\n    return score","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c674d67e-2f90-4782-a4f5-6113e5fc6e3c","_uuid":"60eef1dc2cddad2a5f2baf4b08e19b274d631128"},"cell_type":"markdown","source":"#### 2.2- The model"},{"metadata":{"_cell_guid":"f236c53e-f5e4-47a2-8362-0e762254844f","_uuid":"54cd74701a78f01fbd31e779f540eb2929d96cdc","trusted":false,"collapsed":true},"cell_type":"code","source":"# Build U-Net model \ninputs = Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\ns = Lambda(lambda x: x / 255) (inputs)\n#s = Lambda(lambda x: x) (inputs)\n\nc1 = Conv2D(8, (3, 3), activation='relu', padding='same') (s)\nc1 = Conv2D(8, (3, 3), activation='relu', padding='same') (c1)\np1 = MaxPooling2D((2, 2)) (c1)\n\nc2 = Conv2D(16, (3, 3), activation='relu', padding='same') (p1)\nc2 = Conv2D(16, (3, 3), activation='relu', padding='same') (c2)\np2 = MaxPooling2D((2, 2)) (c2)\n\nc3 = Conv2D(32, (3, 3), activation='relu', padding='same') (p2)\nc3 = Conv2D(32, (3, 3), activation='relu', padding='same') (c3)\np3 = MaxPooling2D((2, 2)) (c3)\n\nc4 = Conv2D(64, (3, 3), activation='relu', padding='same') (p3)\nc4 = Conv2D(64, (3, 3), activation='relu', padding='same') (c4)\np4 = MaxPooling2D(pool_size=(2, 2)) (c4)\n\nc5 = Conv2D(128, (3, 3), activation='relu', padding='same') (p4)\nc5 = Conv2D(128, (3, 3), activation='relu', padding='same') (c5)\n\nu6 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same') (c5)\nu6 = concatenate([u6, c4])\nc6 = Conv2D(64, (3, 3), activation='relu', padding='same') (u6)\nc6 = Conv2D(64, (3, 3), activation='relu', padding='same') (c6)\n\nu7 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same') (c6)\nu7 = concatenate([u7, c3])\nc7 = Conv2D(32, (3, 3), activation='relu', padding='same') (u7)\nc7 = Conv2D(32, (3, 3), activation='relu', padding='same') (c7)\n\nu8 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same') (c7)\nu8 = concatenate([u8, c2])\nc8 = Conv2D(16, (3, 3), activation='relu', padding='same') (u8)\nc8 = Conv2D(16, (3, 3), activation='relu', padding='same') (c8)\n\nu9 = Conv2DTranspose(8, (2, 2), strides=(2, 2), padding='same') (c8)\nu9 = concatenate([u9, c1], axis=3)\nc9 = Conv2D(8, (3, 3), activation='relu', padding='same') (u9)\nc9 = Conv2D(8, (3, 3), activation='relu', padding='same') (c9)\n\noutputs = Conv2D(1, (1, 1), activation='sigmoid') (c9)\n\nmodel = Model(inputs=[inputs], outputs=[outputs]) \nmodel.compile(optimizer='nadam', loss='binary_crossentropy', metrics=[mean_iou]) \n#model.compile(optimizer='nadam', loss='binary_crossentropy', metrics=['accuracy']) \nmodel.summary() ","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"190a9c0e-b422-46a5-9e1a-81043581ddb4","_uuid":"b65cdf18dc92dad8f2a4886967f61580601c0e1f","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"70a2820f-56b0-4ffe-96f5-3ba20b59369f","_uuid":"2380bc5561ffb4ecc9fcc74cb9d52c54ccf468ca"},"cell_type":"markdown","source":"#### 2.3- Training"},{"metadata":{"_cell_guid":"36a7119c-a14f-420c-a689-a6ca66c87e6b","_uuid":"8f57fedc63609c0c21fedc00b0ea91f1432551d1","trusted":false,"collapsed":true},"cell_type":"code","source":"results = model.fit(X_train, Y_train, validation_split=0.1, batch_size=8, epochs=10)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"0029aa17-5083-4f8e-b4d1-6d63d0a0ec13","_uuid":"aeacc4e8bef0eee6bdbaa80337a021388ce9ab82","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ece78ba6-7cd5-4eee-b090-7e850839d2f2","_uuid":"b0ea964c4e71bfc2946fdb37df9ad1d9c4764802"},"cell_type":"markdown","source":"#### 2.4- Predictions"},{"metadata":{"_cell_guid":"e59479dc-e810-4009-a3b5-7de23474991d","_uuid":"0eefb73c0399f8885dc7df9f92050d246adcf740","trusted":false,"collapsed":true},"cell_type":"code","source":"# Predict on train, val and test\n#model = load_model('../models/wad-video-seg.h5')\nprint('Predictions ... ')\npreds_train = model.predict(X_train[:int(X_train.shape[0]*0.9)], verbose=1)\npreds_val = model.predict(X_train[int(X_train.shape[0]*0.9):], verbose=1)\npreds_test = model.predict(X_test, verbose=1)\n\n# Threshold predictions\nprint('\\nPredictions thresholding ... \\n')\npreds_train_t = (preds_train > 0.5).astype(np.uint8)\npreds_val_t = (preds_val > 0.5).astype(np.uint8)\npreds_test_t = (preds_test > 0.5).astype(np.uint8)\n\n# Create list of upsampled test masks\nprint('Test images upsampling ... ')\npreds_test_upsampled = []\nfor i in range(len(preds_test)):\n    preds_test_upsampled.append(resize(preds_test[i], \n                                       (sizes_test[i][0], sizes_test[i][1]), \n                                       mode='constant', preserve_range=True))","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"2b6241d2-f814-4c69-ad75-f868e7901b54","_uuid":"26d301831db7c6cab11a0405b0cfdf9cfaafdd7b","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5f80c396-a1c3-4ffe-9cdc-50402db2acef","_uuid":"17fcf6eac15a83cf769df462b78cdefa3d9fbb09"},"cell_type":"markdown","source":"***     - Display prediction on the training set***"},{"metadata":{"collapsed":true,"_cell_guid":"8b05457f-0cf9-4986-a39b-ab47b0cd4e5a","_uuid":"491d5207cdb8fbdb42c701b9dcefdb7c08f85d46","trusted":false},"cell_type":"code","source":"def display_predictions(nb, preds_train_t):\n    #imgs_ids = [c for c in category_to_product.keys() if category_id in str(c)]\n    #ix = random.randint(0, len(train_ids))\n    ix = random.randint(nb, len(preds_train_t)-1)\n    print('ix value : ', ix)\n    imgs_ids = [] \n    imgs_labels = []\n    imgs_ids.append(X_train[ix]) \n    imgs_labels.append(\"Image\")\n    \n    ### For printing purposes, we should reshape it without the last channel\n    Y_2 = np.reshape(Y_train[ix],(Y_train[ix].shape[0], Y_train[ix].shape[1])) \n    imgs_ids.append(Y_2) \n    imgs_labels.append(\"Label\")\n    \n    ## train preds\n    tpreds = np.reshape(preds_train_t[ix],(preds_train_t[ix].shape[0], preds_train_t[ix].shape[1])) \n    imgs_ids.append(tpreds) \n    imgs_labels.append(\"Prediction\")\n    \n    print('len imgs_ids : ', len(imgs_ids))\n    fig, axs = plt.subplots(1, len(imgs_ids), figsize=(10, 4))\n    \n    print('axs : ', axs)\n    for i, ax_i in enumerate(axs):\n        ax_i.imshow(imgs_ids[i])\n        ax_i.set_title(imgs_labels[i])\n        ax_i.grid('off')\n        ax_i.axis('off')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3025ec02-6bfc-4c6d-afc8-c8dfb15ebae8","_uuid":"492fb9b57bb777b71f69d762d2efcddbc5a7221b","trusted":false,"collapsed":true},"cell_type":"code","source":"display_predictions(0, preds_train_t) ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ee6b23af-cc18-480a-9fd9-982eead29990","_uuid":"c68bc0f6b7afabab0ac97f8c736aa9687295a427"},"cell_type":"markdown","source":"***    - Display prediction on the validation set***"},{"metadata":{"_cell_guid":"8b85ea51-5091-40d9-94e6-305c709b7654","_uuid":"62ab0e3cc2581d9843683288550c300f4ba4881f","trusted":false,"collapsed":true},"cell_type":"code","source":"display_predictions(0, preds_val_t) ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"22201572-3c49-4a9c-99b4-df60d1c91680","_uuid":"f7d82975fc4754568376f9c2343be57a125d0c13"},"cell_type":"markdown","source":"#### <font color='red'>As we could see on prediction label, the model definitely needs more training and tuning.</font>"},{"metadata":{"collapsed":true,"_cell_guid":"37082dfc-09ab-4233-aa78-080d8bec3a78","_uuid":"628e8cf02bf8379ecf1d9c4188500fb86112a074","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2e765925-ae50-4c3c-b614-72daa30526e7","_uuid":"c7431230d4ef646b5bd47252652c3118f6acfbd8"},"cell_type":"markdown","source":"## 3- Result Submission"},{"metadata":{"_cell_guid":"06b514cf-07e7-465c-928e-16721cdcd54b","_uuid":"3a4f437cfb551d9659de1d6ccea66cfc5119d5f0"},"cell_type":"markdown","source":"**Important:**<br/>\n1- Each row in submission must have all the required fields with the exact order : ImageId,LabelId,Confidence,PixelCount,EncodedPixels.<br/>\n2- Each image should be encoded in the row-major order, which is different from OpenCV.<br/>\n3- In this competition, we **evaluate seven different instance-level annotations**, which are ***car, motorcycle, bicycle, pedestrian, truck, bus, and tricycle***."},{"metadata":{"_cell_guid":"66d0e748-de0a-44a6-b431-332388ca79f3","_uuid":"13420e233876d0550cac59ab70ca18fa00144ae0"},"cell_type":"markdown","source":"<font color='red'>**Roadmap :** <br/>\n 1- Solve the RLE encoding problem.<br/>\n 2- Write a functions to handle the submission variables : ImageId, LabelId, Confidence, PixelCount, EncodedPixels.<br/>\n 3- Improve the model and fine-tune it.<br/>\n<br/>\n**classdict = {0:'others', 33:'car', 34:'motorbicycle', 35:'bicycle', 36:'person', 38:'truck', 39:'bus', 40:'tricycle'}**</font>"},{"metadata":{"_cell_guid":"5b629b80-1c13-4a3c-858c-37fa24fd2c74","_uuid":"d464f24378d865cbb01c08b1a0f89effcc4774dd"},"cell_type":"markdown","source":"### 3.1- Encoding"},{"metadata":{"collapsed":true,"_cell_guid":"ff1e003e-aaed-40e4-a10a-26458a0f6f82","_uuid":"f4adfd521b8b26ecd6882e93dd7428c56b25c646","trusted":false},"cell_type":"code","source":"def rle_encoding(x):\n    dots = np.where(x.T.flatten() == 1)[0]\n    run_lengths = []\n    prev = -2\n    for b in dots:\n        if (b>prev+1): run_lengths.extend((b + 1, 0))\n        run_lengths[-1] += 1\n        prev = b\n    return run_lengths\n\ndef prob_to_rles(x, cutoff=0.5): \n    lab_img = label(x > cutoff) \n    for i in range(1, lab_img.max() + 1): \n        yield rle_encoding(lab_img == i) ","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"c7873694-0c32-43b3-be87-9537970a005a","_uuid":"b14d4d7326ac37ec9321423c3d3c80d4988f909f","trusted":false},"cell_type":"code","source":"# Let's iterate over the test IDs and generate run-length encodings \n# for each seperate mask identified by skimage in the label image ...\nnew_test_ids = [] \nrles = [] \nfor n, id_ in enumerate(test_ids):\n    rle = list(prob_to_rles(preds_test_upsampled[n])) \n    #print('rle : ', rle[0])\n    #break\n    rles.extend(rle) \n    new_test_ids.extend([id_] * len(rle)) ","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"ea2225a5-ddf1-44f4-804e-bef661f01da3","_uuid":"d578cc6a5dcff0a4e241ee27661cc74e7fec6d0c","trusted":false},"cell_type":"code","source":"#rles[0]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e364b898-dbe3-47cb-ba3c-2d4f7977b8c9","_uuid":"b73c673875c3eaa4c49fe4760afef64f5abc1053"},"cell_type":"markdown","source":"### 3.2- Submission"},{"metadata":{"_cell_guid":"0619cabc-0695-4125-bc57-8fd466a05c01","_uuid":"63271e83012da4ffa5e559156ce748bd0478341c","trusted":false,"collapsed":true},"cell_type":"code","source":"sub_sample = pd.read_csv('../input/cvpr-2018-autonomous-driving/sample_submission.csv')\nsub_sample.head(2)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f1df7e7f-1d15-4b46-a2b0-70b73519b44d","_uuid":"e42ac4ad73700466ed42d06f3796e8ec216afb49","trusted":false,"collapsed":true},"cell_type":"code","source":"# Create submission DataFrame\n# The variables order here is relative to the competition #evaluation requirements \n#(different from the 'sample_submission.csv' file)\nsubmission = pd.DataFrame()\nsubmission['ImageId'] = new_test_ids\nsubmission['LabelId'] = 33\nsubmission['Confidence'] = 1\nsubmission['PixelCount'] = 300\nsubmission['EncodedPixels'] = pd.Series(rles).apply(lambda x: ' '.join(str(y) for y in x))\nsubmission.to_csv('Sub-wad-video-seg.csv', index=False)\nsubmission.head(5)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"6c14ed92-133a-45e0-9a61-f582d8fbc076","_uuid":"c5321f09fa7b0dcc5ff66af92c911b5ffe7b90fd","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b4c78996-88c2-4264-872a-6a6437f01e3d","_uuid":"5d025f1fedc5e0c6a26db69c525abf25a77727ad","trusted":false,"collapsed":true},"cell_type":"code","source":"from IPython.display import FileLink\n#%cd $LESSON_HOME_DIR\nFileLink('Sub-wad-video-seg.csv')","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"c339ec79-0c44-4d5c-9d2d-3757a9e2ccbd","_uuid":"33c8b2dd10ac499c5e83d404956691cb3d6f9351","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}