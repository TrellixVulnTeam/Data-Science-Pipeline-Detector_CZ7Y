{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport sys\nimport time\nimport logging\nimport importlib\n\nimport numpy as np\nimport pandas as pd\nnp.set_printoptions(precision=2, linewidth=125)\n\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n%matplotlib inline\n\nfrom tensorflow.keras        import optimizers \nfrom tensorflow.keras.layers import LSTM, GRU\n\nmodule_path = os.path.abspath(os.path.join(r'..'))\nif module_path not in sys.path:\n    sys.path.append(module_path)\n        \n# Custom libraries\nimport ann_utils as ann\nimport gen_utils as gen\n\nimportlib.reload(ann)\nimportlib.reload(gen)\n\nsave_folder, model_folder = gen.create_model_and_figure_folders()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load the data","metadata":{}},{"cell_type":"code","source":"path = '../input/ventilator-pressure-prediction'\ntrain_data_df = pd.read_csv(f\"{path}/train.csv\")\n\n# Convert the dataframe to numpy array\ntrain_data = train_data_df.to_numpy()\n\n# Check the total number of samples\nsample_length  = 80\ntotal_number_of_samples = int(len(train_data) / sample_length)\nprint(\"Total number of samples: \", total_number_of_samples)\n\ntrain_data_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Divide the data into training, validation and testing sets","metadata":{}},{"cell_type":"code","source":"train_fraction = 0.8\nvalid_fraction = 0.1\nsample_length  = 80\n\n# Find the train/test fraction idx\ntrain_end_idx   = int(len(train_data) * train_fraction)\nvalid_start_idx = int(len(train_data) * valid_fraction) + train_end_idx\n\n# Divide the data into train and test\ntrain_x_y = train_data[:train_end_idx, :]\nvalid_x_y = train_data[train_end_idx:valid_start_idx, :]\ntest_x_y  = train_data[valid_start_idx:, :]\n\nnumber_of_train_samples = int(len(train_x_y) / sample_length)\nnumber_of_valid_samples = int(len(valid_x_y) / sample_length)\nnumber_of_test_samples  = int(len(test_x_y)  / sample_length)\n\nprint(\"Number of training samples: \",   number_of_train_samples)\nprint(\"Number of validation samples: \", number_of_valid_samples)\nprint(\"Number of testing samples:  \",   number_of_test_samples)\nprint(\"Total number of samples: \",      number_of_train_samples + number_of_valid_samples + number_of_test_samples)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create a Validation Timeseries Generator","metadata":{}},{"cell_type":"code","source":"importlib.reload(ann)\n\nnumber_of_timesteps = 80\ntrain_batch_size    = 80\n\n\ngenerator_parameters = {'Sample Length'           : sample_length,\n                        'Number of Timesteps'     : number_of_timesteps,\n                        'Data Batch Size'         : train_batch_size,\n                        'Padding'                 : True\n                       }                      \n\nvalidation_data = ann.Data_Generator(valid_x_y, 2 , generator_parameters)\n\nnumber_of_batches = len(validation_data)\nprint(\"Number of batches: \", number_of_batches)\n\n# for idx, (valid_x, valid_y) in enumerate(validation_data):\n#     print(valid_x.shape)\n#     print(idx)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create a GRU model","metadata":{}},{"cell_type":"code","source":"importlib.reload(ann)\n\n# Optimizers \nsgd     = optimizers.SGD(learning_rate=0.05, decay=1e-4, momentum=0.7, nesterov=True)\nrmsprop = optimizers.RMSprop(learning_rate=0.00001, rho=0.9, epsilon=None, decay=0.00001)\nadam    = optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\nnadam   = optimizers.Nadam(learning_rate=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n\n\nmy_model_parameters = {'train_batch_size'           : train_batch_size,\n                       'number_of_timesteps'        : number_of_timesteps,\n                       'number_of_features'         : 5,\n                       'number_of_outputs'          : 1,\n                       'cell_type'                  : LSTM,\n                       'number_of_recurrent_layers' : 4,\n                       'number_of_recurrent_units'  : 256,\n                       'number_of_FC_layers'        : 3,\n                       'number_of_FC_hidden_units'  : 128,\n                       'stateful_state'             : True,\n                       'hidden_dropout_rate'        : 0.25,\n                       'output_dropout_rate'        : 0.15,\n                       'optimizer'                  : sgd\n                      }\n\n\n\nmy_model = ann.create_GRU_model(my_model_parameters)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train the model","metadata":{}},{"cell_type":"code","source":"importlib.reload(ann)\nimportlib.reload(gen)\n\ntrain_x_pad = np.zeros((number_of_timesteps, number_of_timesteps + sample_length))\ntrain_y_pad = np.zeros(number_of_timesteps)\n\nnumber_of_epochs  = 1\n\nmy_train_parameters = {'number_of_epochs'        : number_of_epochs,\n                       'number_of_train_samples' : number_of_train_samples,\n                       'number_of_batches'       : number_of_batches,\n                       'batch_size'              : train_batch_size\n                      }\n\n# Create a logger object for saving the loss data and calculate ETA\nlogger = gen.training_logger(my_train_parameters)\n\n# Iterate epochs\nfor e_idx in range(number_of_epochs):\n       \n    # Iterate all the samples\n    for s_idx in range(number_of_train_samples):\n               \n        # Get the current sample using s_idx and create a generator\n        train_generator = ann.Data_Generator(train_x_y, s_idx , generator_parameters)\n        \n        # Reset the sample set timer\n        logger.reset_mean_loss_arrays()\n        logger.start_set_time()\n        \n        # Iterate all the batches and save the losses\n        for g_idx, (batch_x, batch_y) in enumerate(train_generator):\n            \n            # Train on batch\n            tr_loss, tr_acc = my_model.train_on_batch(batch_x, batch_y)\n            \n            # Update the loss arrays and print the progress\n            logger.update_mean_loss_arrays(tr_loss, tr_acc, g_idx)            \n            logger.print_training_progress(e_idx, s_idx, g_idx, set_flag=True)\n        \n        logger.update_set_time()\n        logger.print_training_progress(e_idx, s_idx, g_idx, set_flag=True)\n                   \n        # After finishing a sample, reset the model states\n        my_model.reset_states()\n        \n        # Do a validation every epoch\n        if (s_idx+1) % number_of_train_samples == 0:\n            model_save_dir  = model_folder\n            model_save_addr = os.path.join(model_save_dir, \"GRU_model_2\")\n\n            my_model.save(model_save_addr)\n            \n#             for valid_x, valid_y in validation_data:\n#                 validation_loss, _ = my_model.test_on_batch(valid_x, valid_y)\n#                 logger.update_validation_loss_array(validation_loss)\n\n    \n# Plot the losses\nplt.plot(logger.loss_array)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_save_dir  = model_folder\nmodel_save_addr = os.path.join(model_save_dir, \"GRU_model_2\")\nmy_model.save(model_save_addr)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load the test dataset","metadata":{}},{"cell_type":"code","source":"test_path = '../input/ventilator-pressure-prediction'\ntest_data_df = pd.read_csv(f\"{test_path}/test.csv\")\n\n# Convert the dataframe to numpy array\ntest_data = test_data_df.to_numpy()\n\n# Check the total number of samples\nsample_length  = 80\ntotal_number_of_samples = int(len(test_data) / sample_length)\nprint(\"Total number of samples: \", total_number_of_samples)\n\ntest_data_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Do the prediction for the test dataset","metadata":{}},{"cell_type":"code","source":"array_len  = int(len(test_data) / 503)\npred_array = np.zeros(array_len)\n\n\n# Give the generator parameters\nnumber_of_timesteps = 80\ntest_batch_size     = train_batch_size\nnumber_of_batches   = 1\n\ngenerator_parameters = {'Sample Length'           : sample_length,\n                        'Number of Timesteps'     : number_of_timesteps,\n                        'Data Batch Size'         : test_batch_size,\n                        'Padding'                 : True\n                       }\n\narray_len","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"importlib.reload(ann)\n\n         \nsave_idx   = 0 \nname_idx   = 0\ntotal_time = 0\n\ntest_start_idx = int(((name_idx*array_len)/80))\n\nfor test_idx in range(test_start_idx, total_number_of_samples+1):\n        \n   \n    sample_start = time.time()\n    \n    my_model.reset_states()\n    test_sample        = ann.Data_Generator(test_data, test_idx, generator_parameters)\n    sample_predictions = my_model.predict(test_sample,  batch_size = test_batch_size,\n                                                        steps      = 1,\n                                                        verbose    = 0)    \n    pred_array[(save_idx*sample_length):(save_idx+1)*sample_length]    = sample_predictions[:, 0]\n    save_idx += 1\n    \n    sample_time = time.time() - sample_start\n    total_time  = total_time + sample_time\n       \n    mean_time = total_time / (test_idx+1)\n    eta       = int(mean_time*(total_number_of_samples - test_idx))\n    \n    sample_str = \"Sample: [\"  + str(test_idx+1) + \"/\" + str(total_number_of_samples) + \"]. \"\n    time_str   = \"Time passed : {0:.1f}. \".format(total_time)\n    eta_str    = \"ETA: {0:.0f}. \".format(eta)\n    \n    progress_str = sample_str + time_str + eta_str\n    \n    print(progress_str, end='\\r')\n    \n    if (save_idx+1) % 100 == 0:\n        \n        # Create the ID array\n        id_idx_str = (name_idx*array_len) + 1\n        id_idx_end = ((name_idx+1)*array_len) + 1\n        id_array   = np.arange(id_idx_str, id_idx_end)\n        \n        # Combine with the predictions\n        output_array = np.vstack((id_array, pred_array)).T\n        \n        # Create the save name string and save the array\n        save_name  = \"model_2_results_\" + str(name_idx) + \".csv\"\n        save_dir   = os.path.join(save_folder, save_name)\n        np.savetxt(save_dir, output_array, delimiter=\",\")\n        \n        del output_array\n        \n        name_idx +=  1\n        save_idx  =  0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_dir       = os.path.join(save_folder, \"../\")\nsave_folder_list = next(os.walk(result_dir))[1]\n\ndesired_save_folder = os.path.join(result_dir, save_folder_list[-1])\nprint(desired_save_folder)\n\n\nmodel_list = []\nresult_list =[]\n\nprint(next(os.walk(desired_save_folder))[1])\n\nfor model_name in next(os.walk(desired_save_folder))[2]:\n    print(model_name)\n    model_addr = os.path.join(desired_save_folder, model_name)\n    model_list.append(model_addr)\n    \n    result_dummy = np.loadtxt(model_addr, delimiter=\",\")\n    result_list.append(result_dummy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_array = np.zeros((len(test_data), 2))\n\nfor idx, result in enumerate(result_list):\n    \n    result_idx_str = (idx*array_len)\n    result_idx_end = ((idx+1)*array_len)\n    \n    result_array[result_idx_str:result_idx_end, :] = result\n    \nlen(result_array)\n\nresult_array","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_name   = \"GRU_model_2_results_all.csv\"\nmain_folder = \"/kaggle/working/\"\nsave_dir   = os.path.join(main_folder + save_name)\n\nprint(save_dir)\n\n# np.savetxt(save_dir, result_array, delimiter=\",\")\n\n# df = pd.DataFrame(result_array, columns=['id', 'pressure'])\ndf = df.astype(int)\n\n# # save the dataframe as a csv file\ndf.to_csv(save_dir, index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n# for dirname, _, filenames in os.walk('/kaggle/working'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n\na = next(os.walk('/kaggle/working'))\na\n# result_folder = os.path.join('/kaggle/working', a)\n\n# exp_folder = os.path.join('/kaggle/working', next(os.walk(result_folder))[1][0])\n\n# next(os.walk(exp_folder))","metadata":{"execution":{"iopub.status.busy":"2021-10-24T20:36:51.622761Z","iopub.execute_input":"2021-10-24T20:36:51.623042Z","iopub.status.idle":"2021-10-24T20:36:51.632315Z","shell.execute_reply.started":"2021-10-24T20:36:51.623015Z","shell.execute_reply":"2021-10-24T20:36:51.631532Z"},"trusted":true},"execution_count":null,"outputs":[]}]}