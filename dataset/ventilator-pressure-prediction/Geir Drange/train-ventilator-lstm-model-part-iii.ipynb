{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"![logo](https://cdn.freelogovectors.net/wp-content/uploads/2018/07/tensorflow-logo.png)","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport gc\nimport matplotlib.pyplot as plt\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras import *\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.callbacks import LearningRateScheduler, ReduceLROnPlateau\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\nfrom sklearn.preprocessing import RobustScaler, normalize\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.metrics import mean_absolute_error\nfrom pickle import load\nimport matplotlib.pyplot as plt\nimport json\n!cp ../input/ventilator-feature-engineering/VFE.py .","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-10-05T12:37:41.579073Z","iopub.execute_input":"2021-10-05T12:37:41.580003Z","iopub.status.idle":"2021-10-05T12:37:48.094018Z","shell.execute_reply.started":"2021-10-05T12:37:41.57987Z","shell.execute_reply":"2021-10-05T12:37:48.093104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training - LSTM based model\nThis notebook is part of a series:  \n  * [Ventilator: Feature engineering](https://www.kaggle.com/mistag/ventilator-feature-engineering)\n  * [Keras model tuning with Optuna](https://www.kaggle.com/mistag/keras-model-tuning-with-optuna)\n  * [[train] Ventilator LSTM Model - part 1](https://www.kaggle.com/mistag/train-ventilator-lstm-model-part-i)\n  * [[train] Ventilator LSTM Model - part 2](https://www.kaggle.com/mistag/train-ventilator-lstm-model-part-ii)\n  * [[pred] Ventilator LSTM Model](https://www.kaggle.com/mistag/pred-ventilator-lstm-model)\n  \n## References\nThe code is based on these references:  \n  * [Improvement base on Tensor Bidirect LSTM](https://www.kaggle.com/kensit/improvement-base-on-tensor-bidirect-lstm-0-173/notebook) by [Ken Sit](https://www.kaggle.com/kensit)\n  * [Ensemble Folds with MEDIAN - [0.153]](https://www.kaggle.com/cdeotte/ensemble-folds-with-median-0-153) by [Chris Deotte](https://www.kaggle.com/cdeotte)","metadata":{}},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"train = np.load('../input/ventilator-feature-engineering/x_train.npy')\ntargets = np.load('../input/ventilator-feature-engineering/y_train.npy')","metadata":{"execution":{"iopub.status.busy":"2021-10-05T12:37:54.883896Z","iopub.execute_input":"2021-10-05T12:37:54.884779Z","iopub.status.idle":"2021-10-05T12:38:18.446484Z","shell.execute_reply.started":"2021-10-05T12:37:54.88474Z","shell.execute_reply":"2021-10-05T12:38:18.44559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"# model creation\ndef create_lstm_model():\n\n    x0 = tf.keras.layers.Input(shape=(train.shape[-2], train.shape[-1]))  \n\n    lstm_layers = 4 # number of LSTM layers\n    lstm_units = [940, 540, 462, 316]\n    lstm = Bidirectional(keras.layers.LSTM(lstm_units[0], return_sequences=True))(x0)\n    for i in range(lstm_layers-1):\n        lstm = Bidirectional(keras.layers.LSTM(lstm_units[i+1], return_sequences=True))(lstm)    \n    lstm = Dropout(0.002)(lstm)\n    lstm = Dense(lstm_units[-1], activation='swish')(lstm)\n    lstm = Dense(1)(lstm)\n\n    model = keras.Model(inputs=x0, outputs=lstm)\n    model.compile(optimizer=\"adam\", loss=\"mae\")\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2021-10-05T12:38:18.448574Z","iopub.execute_input":"2021-10-05T12:38:18.448911Z","iopub.status.idle":"2021-10-05T12:38:18.459019Z","shell.execute_reply.started":"2021-10-05T12:38:18.448871Z","shell.execute_reply":"2021-10-05T12:38:18.45842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training\nFirst define a few parameters that will also be used in other notebooks:","metadata":{}},{"cell_type":"code","source":"with open('../input/train-ventilator-lstm-model-part-i/train_params.json', 'r') as fp:\n    config = json.load(fp)\n\nBATCH_SIZE = config['BATCH_SIZE']\nNFOLDS = config['NFODLS']\nSEED = config['SEED']\nEPOCHS = config['EPOCHS']","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-10-05T12:38:18.460052Z","iopub.execute_input":"2021-10-05T12:38:18.460664Z","iopub.status.idle":"2021-10-05T12:38:18.486203Z","shell.execute_reply.started":"2021-10-05T12:38:18.460627Z","shell.execute_reply":"2021-10-05T12:38:18.485579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"HW strategy:","metadata":{}},{"cell_type":"code","source":"# Function to get hardware strategy\ndef get_hardware_strategy():\n    try:\n        # TPU detection. No parameters necessary if TPU_NAME environment variable is\n        # set: this is always the case on Kaggle.\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        print('Running on TPU ', tpu.master())\n    except ValueError:\n        tpu = None\n\n    if tpu:\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n        tf.config.optimizer.set_jit(True)\n    else:\n        # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n        strategy = tf.distribute.get_strategy()\n\n    return tpu, strategy\n\ntpu, strategy = get_hardware_strategy()","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-10-05T12:38:18.487807Z","iopub.execute_input":"2021-10-05T12:38:18.488502Z","iopub.status.idle":"2021-10-05T12:38:23.992164Z","shell.execute_reply.started":"2021-10-05T12:38:18.488464Z","shell.execute_reply":"2021-10-05T12:38:23.991493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hist = []\nfolds = [3] # folds to train\n\nwith strategy.scope():\n    kf = KFold(n_splits=NFOLDS, shuffle=True, random_state=SEED)\n    \n    for fold, (train_idx, test_idx) in enumerate(kf.split(train, targets)):\n        if fold in folds:\n            print('-'*15, '>', f'Fold {fold+1}', '<', '-'*15)\n            folds.append(fold)\n            X_train, X_valid = train[train_idx], train[test_idx]\n            y_train, y_valid = targets[train_idx], targets[test_idx]\n            \n            model = create_lstm_model()\n            model.compile(optimizer=\"adam\", loss=\"mae\")\n            \n            #checkpoint_filepath = f\"lstm_fold_{fold}.hdf5\"\n            checkpoint_filepath = '/kaggle/working/lstm_fold{}.hdf5'.format(fold)\n\n            scheduler = ExponentialDecay(1e-3, 400*((len(train)*0.8)/BATCH_SIZE), 1e-5)\n            #lr = LearningRateScheduler(scheduler, verbose=0)\n            lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=10, verbose=1)\n            es = EarlyStopping(monitor=\"val_loss\", patience=60, verbose=1, mode=\"min\", restore_best_weights=True)\n            sv = keras.callbacks.ModelCheckpoint(\n                checkpoint_filepath, monitor='val_loss', verbose=1, save_best_only=True,\n                save_weights_only=False, mode='auto', save_freq='epoch',\n                options=None\n            )\n            hist.append(model.fit(X_train, y_train, \n                                  validation_data=(X_valid, y_valid), \n                                  epochs=EPOCHS, batch_size=BATCH_SIZE, \n                                  callbacks=[lr, es, sv]))\n        \n            del X_train, X_valid, y_train, y_valid, model\n            gc.collect()","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-10-05T12:38:23.993296Z","iopub.execute_input":"2021-10-05T12:38:23.993564Z","iopub.status.idle":"2021-10-05T12:40:21.24155Z","shell.execute_reply.started":"2021-10-05T12:38:23.993537Z","shell.execute_reply":"2021-10-05T12:40:21.238947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's take a look at the learning curves.","metadata":{}},{"cell_type":"code","source":"colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple']\nplt.figure(figsize=(16,16))\nfor i in range(len(hist)):\n    plt.plot(hist[i].history['loss'], linestyle='-', color=colors[i], label='Train, fold #{}'.format(str(folds[i])))\nfor i in range(len(hist)):\n    plt.plot(hist[i].history['val_loss'], linestyle='--', color=colors[i], label='Validation, fold #{}'.format(str(folds[i])))\nplt.ylim(top=1)\nplt.title('Model Loss')\nplt.ylabel('MAE')\nplt.xlabel('Epoch')\nplt.legend()\nplt.grid(which='major', axis='both')\nplt.show();","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-10-05T06:01:32.094327Z","iopub.execute_input":"2021-10-05T06:01:32.094575Z","iopub.status.idle":"2021-10-05T06:01:32.472982Z","shell.execute_reply.started":"2021-10-05T06:01:32.094546Z","shell.execute_reply":"2021-10-05T06:01:32.47194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}}]}