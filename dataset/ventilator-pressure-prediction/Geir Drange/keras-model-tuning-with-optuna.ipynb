{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"![logo](https://optuna.org/assets/img/bg.jpg)","metadata":{}},{"cell_type":"markdown","source":"There are many hyperparameter optimization frameworks available, and in this notebook we will give [Optuna](https://optuna.org/) a spin.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport optuna\nfrom optuna.integration import TFKerasPruningCallback\nfrom optuna.trial import TrialState\nfrom optuna.visualization import plot_intermediate_values\nfrom optuna.visualization import plot_optimization_history\nfrom optuna.visualization import plot_param_importances\nfrom optuna.visualization import plot_contour\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras import *\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import LearningRateScheduler\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\nfrom sklearn.preprocessing import RobustScaler, normalize\nfrom sklearn.model_selection import train_test_split\nfrom pickle import load\n!cp ../input/ventilator-feature-engineering/VFE.py .","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-10-04T15:38:45.05172Z","iopub.execute_input":"2021-10-04T15:38:45.052489Z","iopub.status.idle":"2021-10-04T15:38:59.734039Z","shell.execute_reply.started":"2021-10-04T15:38:45.05239Z","shell.execute_reply":"2021-10-04T15:38:59.73136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset creation\nTraining dataset is loaded from the [feature engineering notebook](https://www.kaggle.com/mistag/ventilator-feature-engineering).\nFeature engineering is based on [Ensemble Folds with MEDIAN](https://www.kaggle.com/cdeotte/ensemble-folds-with-median-0-153) by [Chris Deotte](https://www.kaggle.com/cdeotte). The optimization is run on a smaller subset of the dataset.","metadata":{}},{"cell_type":"code","source":"from VFE import add_features\n\ntrain = np.load('../input/ventilator-feature-engineering/x_train.npy')\ntargets = np.load('../input/ventilator-feature-engineering/y_train.npy')\n\nBATCH_SIZE = 1024\n\n# test set\ntest_ori = pd.read_csv('../input/ventilator-pressure-prediction/test.csv')\ntest = add_features(test_ori)\ntest.drop(['id', 'breath_id'], axis=1, inplace=True)\n\nRS = load(open('../input/ventilator-feature-engineering/RS.pkl', 'rb'))\ntest = RS.transform(test)\ntest = test.reshape(-1, 80, test.shape[-1])","metadata":{"execution":{"iopub.status.busy":"2021-10-04T16:01:56.732271Z","iopub.execute_input":"2021-10-04T16:01:56.732653Z","iopub.status.idle":"2021-10-04T16:02:26.0222Z","shell.execute_reply.started":"2021-10-04T16:01:56.732617Z","shell.execute_reply":"2021-10-04T16:02:26.021192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally we split the data into train and test sets. We keep a large holdout set for model evaluation.","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(train, targets, test_size=0.59284, random_state=21)\nX_test, X_valid, y_test, y_valid = train_test_split(X_test, y_test, test_size=0.79395, random_state=21)\nX_train.shape, X_test.shape, X_valid.shape","metadata":{"execution":{"iopub.status.busy":"2021-10-04T16:06:57.284929Z","iopub.execute_input":"2021-10-04T16:06:57.28525Z","iopub.status.idle":"2021-10-04T16:06:58.586756Z","shell.execute_reply.started":"2021-10-04T16:06:57.285219Z","shell.execute_reply":"2021-10-04T16:06:58.585303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model building\nThe model below is from  [Ensemble Folds with MEDIAN](https://www.kaggle.com/cdeotte/ensemble-folds-with-median-0-153) by [Chris Deotte](https://www.kaggle.com/cdeotte).. Hopefully Optuna will be able to figure out the optimal parameters in the model. All parameters that we want to explore are created with a trial.suggest_() function. ","metadata":{}},{"cell_type":"code","source":"# model creation\ndef create_lstm_model(trial):\n\n    x0 = tf.keras.layers.Input(shape=(train.shape[-2], train.shape[-1]))  \n\n    lstm_layers = 4\n    lstm_units = np.zeros(lstm_layers, dtype=np.int)\n    lstm_units[0] = trial.suggest_int(\"lstm_units_L1\", 768, 1536)\n    lstm = Bidirectional(keras.layers.LSTM(lstm_units[0], return_sequences=True))(x0)\n    for i in range(lstm_layers-1):\n        lstm_units[i+1] = trial.suggest_int(\"lstm_units_L{}\".format(i+2), lstm_units[i]//2, lstm_units[i])\n        lstm = Bidirectional(keras.layers.LSTM(lstm_units[i+1], return_sequences=True))(lstm)    \n    dropout_rate = trial.suggest_float(\"lstm_dropout\", 0.0, 0.3)\n    lstm = Dropout(dropout_rate)(lstm)\n    dense_units = lstm_units[-1]\n    # try different activations\n    activation = trial.suggest_categorical(\"activation\", [\"relu\", \"selu\", \"elu\", \"swish\"])\n    lstm = Dense(dense_units, activation=activation)(lstm)\n    lstm = Dense(1)(lstm)\n\n    model = keras.Model(inputs=x0, outputs=lstm)\n    metrics = [\"mae\"]\n    model.compile(optimizer=\"adam\", loss=\"mae\", metrics=metrics)\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2021-10-04T16:07:05.753983Z","iopub.execute_input":"2021-10-04T16:07:05.75433Z","iopub.status.idle":"2021-10-04T16:07:05.766164Z","shell.execute_reply.started":"2021-10-04T16:07:05.754301Z","shell.execute_reply":"2021-10-04T16:07:05.765108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Objective function\nHere we define the Optuna objective function. The number of epochs per trial is a balance between execution time per trial and confidence in the result of each trial.","metadata":{}},{"cell_type":"code","source":"# Function to get hardware strategy\ndef get_hardware_strategy():\n    try:\n        # TPU detection. No parameters necessary if TPU_NAME environment variable is\n        # set: this is always the case on Kaggle.\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        print('Running on TPU ', tpu.master())\n    except ValueError:\n        tpu = None\n\n    if tpu:\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n        tf.config.optimizer.set_jit(True)\n    else:\n        # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n        strategy = tf.distribute.get_strategy()\n\n    return tpu, strategy\n\ntpu, strategy = get_hardware_strategy()","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-10-04T16:07:06.365127Z","iopub.execute_input":"2021-10-04T16:07:06.365469Z","iopub.status.idle":"2021-10-04T16:07:11.755197Z","shell.execute_reply.started":"2021-10-04T16:07:06.365436Z","shell.execute_reply":"2021-10-04T16:07:11.754309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EPOCHS = 30 # number of epocs per trial\n\ndef objective(trial):\n    \n    # Clear clutter from previous session graphs.\n    keras.backend.clear_session()\n    \n    with strategy.scope():\n        # Generate our trial model.\n        model = create_lstm_model(trial)\n\n        # learning rate scheduler\n        scheduler = ExponentialDecay(1e-3, 400*((len(train)*0.8)/BATCH_SIZE), 1e-5)\n        lr = LearningRateScheduler(scheduler, verbose=0)\n    \n        # Fit the model on the training data.\n        # The TFKerasPruningCallback checks for pruning condition every epoch.\n        model.fit(\n            X_train,\n            y_train,\n            batch_size=BATCH_SIZE,\n            callbacks=[TFKerasPruningCallback(trial, \"val_loss\")],\n            epochs=EPOCHS,\n            validation_data=(X_test, y_test),\n            verbose=1,\n        )\n\n        # Evaluate the model accuracy on the validation set.\n        score = model.evaluate(X_valid, y_valid, verbose=0)\n        return score[1]","metadata":{"execution":{"iopub.status.busy":"2021-10-04T16:07:11.757076Z","iopub.execute_input":"2021-10-04T16:07:11.757405Z","iopub.status.idle":"2021-10-04T16:07:11.766215Z","shell.execute_reply.started":"2021-10-04T16:07:11.757359Z","shell.execute_reply":"2021-10-04T16:07:11.765145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Run optimization\nThere are different samplers and pruners to choose from, here we go for TPESampler and HyperbandPruner.","metadata":{}},{"cell_type":"code","source":"study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.HyperbandPruner())\nstudy.optimize(objective, n_trials=100)\npruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\ncomplete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-10-04T16:07:11.767835Z","iopub.execute_input":"2021-10-04T16:07:11.768267Z","iopub.status.idle":"2021-10-04T16:09:32.735994Z","shell.execute_reply.started":"2021-10-04T16:07:11.768227Z","shell.execute_reply":"2021-10-04T16:09:32.734165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Result\nNow we can create a few interesting plots with the Optuna builtin visualization functions, starting with optimization history:","metadata":{}},{"cell_type":"code","source":"plot_optimization_history(study)","metadata":{"execution":{"iopub.status.busy":"2021-10-04T15:49:46.716431Z","iopub.status.idle":"2021-10-04T15:49:46.716794Z","shell.execute_reply.started":"2021-10-04T15:49:46.716611Z","shell.execute_reply":"2021-10-04T15:49:46.716634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Visualize the loss curves of the trials:","metadata":{}},{"cell_type":"code","source":"plot_intermediate_values(study)","metadata":{"execution":{"iopub.status.busy":"2021-10-04T15:49:46.718256Z","iopub.status.idle":"2021-10-04T15:49:46.718612Z","shell.execute_reply.started":"2021-10-04T15:49:46.718435Z","shell.execute_reply":"2021-10-04T15:49:46.718458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Parameter contour plots - useful or confusing?","metadata":{}},{"cell_type":"code","source":"plot_contour(study)","metadata":{"execution":{"iopub.status.busy":"2021-10-04T15:49:46.719973Z","iopub.status.idle":"2021-10-04T15:49:46.720319Z","shell.execute_reply.started":"2021-10-04T15:49:46.720136Z","shell.execute_reply":"2021-10-04T15:49:46.720158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The parameter importance plot is really interesting:","metadata":{}},{"cell_type":"code","source":"plot_param_importances(study)","metadata":{"execution":{"iopub.status.busy":"2021-10-04T15:49:46.721518Z","iopub.status.idle":"2021-10-04T15:49:46.721867Z","shell.execute_reply.started":"2021-10-04T15:49:46.721674Z","shell.execute_reply":"2021-10-04T15:49:46.721696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally list the optimized model parameters:","metadata":{}},{"cell_type":"code","source":"print(\"Study statistics: \")\nprint(\"  Number of finished trials: \", len(study.trials))\nprint(\"  Number of pruned trials: \", len(pruned_trials))\nprint(\"  Number of complete trials: \", len(complete_trials))\n\nprint(\"Best trial:\")\ntrial = study.best_trial\n\nprint(\"  Value: \", trial.value)\n\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(\"    {}: {}\".format(key, value))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-10-04T15:49:46.723088Z","iopub.status.idle":"2021-10-04T15:49:46.724851Z","shell.execute_reply.started":"2021-10-04T15:49:46.724628Z","shell.execute_reply":"2021-10-04T15:49:46.724654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Summary\nUsing Optuna we found a set of optimal model parameters. Next step is to [test the optimal model](https://www.kaggle.com/mistag/optuna-optimized-base-keras-model).","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}