{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"![logo](https://cdn.freelogovectors.net/wp-content/uploads/2018/07/tensorflow-logo.png)","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport gc\nimport matplotlib.pyplot as plt\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras import *\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.callbacks import LearningRateScheduler, ReduceLROnPlateau\nfrom sklearn.preprocessing import RobustScaler, normalize\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.metrics import mean_absolute_error\nfrom pickle import load\nimport json\n!cp ../input/ventilator-feature-engineering/VFE.py .","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-10-05T13:09:11.839517Z","iopub.execute_input":"2021-10-05T13:09:11.84027Z","iopub.status.idle":"2021-10-05T13:09:17.474399Z","shell.execute_reply.started":"2021-10-05T13:09:11.840156Z","shell.execute_reply":"2021-10-05T13:09:17.473386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training - LSTM based model\nThis notebook is part of a series:  \n  * [Ventilator: Feature engineering](https://www.kaggle.com/mistag/ventilator-feature-engineering)\n  * [Keras model tuning with Optuna](https://www.kaggle.com/mistag/keras-model-tuning-with-optuna)\n  * [[train] Ventilator LSTM Model - part I](https://www.kaggle.com/mistag/train-ventilator-lstm-model-part-i)\n  * [[train] Ventilator LSTM Model - part II](https://www.kaggle.com/mistag/train-ventilator-lstm-model-part-ii)\n  * [[train] Ventilator LSTM Model - part III](https://www.kaggle.com/mistag/train-ventilator-lstm-model-part-iii)\n  * [[train] Ventilator LSTM Model - part IV](https://www.kaggle.com/mistag/train-ventilator-lstm-model-part-iv)\n  * [[pred] Ventilator LSTM Model](https://www.kaggle.com/mistag/pred-ventilator-lstm-model)\n  \n## References\nThe code is based on these references:  \n  * [Improvement base on Tensor Bidirect LSTM](https://www.kaggle.com/kensit/improvement-base-on-tensor-bidirect-lstm-0-173/notebook) by [Ken Sit](https://www.kaggle.com/kensit)\n  * [Ensemble Folds with MEDIAN - [0.153]](https://www.kaggle.com/cdeotte/ensemble-folds-with-median-0-153) by [Chris Deotte](https://www.kaggle.com/cdeotte)\n","metadata":{}},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"train = np.load('../input/ventilator-feature-engineering/x_train.npy')\ntargets = np.load('../input/ventilator-feature-engineering/y_train.npy')","metadata":{"execution":{"iopub.status.busy":"2021-10-05T13:09:17.476491Z","iopub.execute_input":"2021-10-05T13:09:17.476793Z","iopub.status.idle":"2021-10-05T13:09:41.11616Z","shell.execute_reply.started":"2021-10-05T13:09:17.476757Z","shell.execute_reply":"2021-10-05T13:09:41.115389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"# model creation\ndef create_lstm_model():\n\n    x0 = tf.keras.layers.Input(shape=(train.shape[-2], train.shape[-1]))  \n\n    lstm_layers = 4 # number of LSTM layers\n    lstm_units = [940, 540, 462, 316]\n    lstm = Bidirectional(keras.layers.LSTM(lstm_units[0], return_sequences=True))(x0)\n    for i in range(lstm_layers-1):\n        lstm = Bidirectional(keras.layers.LSTM(lstm_units[i+1], return_sequences=True))(lstm)    \n    lstm = Dropout(0.002)(lstm)\n    lstm = Dense(lstm_units[-1], activation='swish')(lstm)\n    lstm = Dense(1)(lstm)\n\n    model = keras.Model(inputs=x0, outputs=lstm)\n    model.compile(optimizer=\"adam\", loss=\"mae\")\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2021-10-05T13:09:41.117472Z","iopub.execute_input":"2021-10-05T13:09:41.117756Z","iopub.status.idle":"2021-10-05T13:09:41.126128Z","shell.execute_reply.started":"2021-10-05T13:09:41.117723Z","shell.execute_reply":"2021-10-05T13:09:41.125443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training\nFirst, fetch some training parameters from the first training notebook:","metadata":{}},{"cell_type":"code","source":"with open('../input/train-ventilator-lstm-model-part-i/train_params.json', 'r') as fp:\n    config = json.load(fp)\n\nBATCH_SIZE = config['BATCH_SIZE']\nNFOLDS = config['NFODLS']\nSEED = config['SEED']\nEPOCHS = config['EPOCHS']","metadata":{"execution":{"iopub.status.busy":"2021-10-05T13:09:59.499754Z","iopub.execute_input":"2021-10-05T13:09:59.500171Z","iopub.status.idle":"2021-10-05T13:09:59.514882Z","shell.execute_reply.started":"2021-10-05T13:09:59.500138Z","shell.execute_reply":"2021-10-05T13:09:59.514237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"HW strategy:","metadata":{}},{"cell_type":"code","source":"# Function to get hardware strategy\ndef get_hardware_strategy():\n    try:\n        # TPU detection. No parameters necessary if TPU_NAME environment variable is\n        # set: this is always the case on Kaggle.\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        print('Running on TPU ', tpu.master())\n    except ValueError:\n        tpu = None\n\n    if tpu:\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n        tf.config.optimizer.set_jit(True)\n    else:\n        # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n        strategy = tf.distribute.get_strategy()\n\n    return tpu, strategy\n\ntpu, strategy = get_hardware_strategy()","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-10-05T13:09:41.137838Z","iopub.execute_input":"2021-10-05T13:09:41.138204Z","iopub.status.idle":"2021-10-05T13:09:41.152996Z","shell.execute_reply.started":"2021-10-05T13:09:41.13817Z","shell.execute_reply":"2021-10-05T13:09:41.152183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hist = []\nfolds = [4] # folds to train\n\nwith strategy.scope():\n    kf = KFold(n_splits=NFOLDS, shuffle=True, random_state=SEED)\n    \n    for fold, (train_idx, test_idx) in enumerate(kf.split(train, targets)):\n        if fold in folds:\n            print('-'*15, '>', f'Fold {fold+1}', '<', '-'*15)\n            folds.append(fold)\n            X_train, X_valid = train[train_idx], train[test_idx]\n            y_train, y_valid = targets[train_idx], targets[test_idx]\n            \n            model = create_lstm_model()\n            model.compile(optimizer=\"adam\", loss=\"mae\")\n            \n            checkpoint_filepath = '/kaggle/working/lstm_fold{}.hdf5'.format(fold)\n\n            lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=10, verbose=1)\n            es = EarlyStopping(monitor=\"val_loss\", patience=60, verbose=1, mode=\"min\", restore_best_weights=True)\n            sv = keras.callbacks.ModelCheckpoint(\n                checkpoint_filepath, monitor='val_loss', verbose=1, save_best_only=True,\n                save_weights_only=False, mode='auto', save_freq='epoch',\n                options=None\n            )\n            hist.append(model.fit(X_train, y_train, \n                                  validation_data=(X_valid, y_valid), \n                                  epochs=EPOCHS, batch_size=BATCH_SIZE, \n                                  callbacks=[lr, es, sv]))\n        \n            del X_train, X_valid, y_train, y_valid, model\n            gc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-10-05T13:10:02.070773Z","iopub.execute_input":"2021-10-05T13:10:02.071024Z","iopub.status.idle":"2021-10-05T13:10:39.501164Z","shell.execute_reply.started":"2021-10-05T13:10:02.070998Z","shell.execute_reply":"2021-10-05T13:10:39.499246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's take a look at the learning curves.","metadata":{}},{"cell_type":"code","source":"colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple']\nplt.figure(figsize=(16,16))\nfor i in range(len(hist)):\n    plt.plot(hist[i].history['loss'], linestyle='-', color=colors[i], label='Train, fold #{}'.format(str(folds[i])))\nfor i in range(len(hist)):\n    plt.plot(hist[i].history['val_loss'], linestyle='--', color=colors[i], label='Validation, fold #{}'.format(str(folds[i])))\nplt.ylim(top=1)\nplt.title('Model Loss')\nplt.ylabel('MAE')\nplt.xlabel('Epoch')\nplt.legend()\nplt.grid(which='major', axis='both')\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-10-05T13:09:56.445715Z","iopub.status.idle":"2021-10-05T13:09:56.4463Z","shell.execute_reply.started":"2021-10-05T13:09:56.44605Z","shell.execute_reply":"2021-10-05T13:09:56.446085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}