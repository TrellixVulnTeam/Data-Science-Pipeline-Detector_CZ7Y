{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# TensorFlow Transformer - CV 0.135 LB 0.112\nThis transformer in this notebook has CV 0.143 on `u_out=0` and uses three targets `pressure`, `pressure.diff()`, and `pressure.cumsum()`. If you change loss to just `pressure` and `pressure.diff()`, this notebook achieves CV 0.135! Both achieve LB 0.112 if we use 32 folds. And LB 0.115 if we use 11 folds.\n\nI'm very proud of this transformer. In Kaggle's Vent Comp, there are many pubic notebooks demonstrating how to build RNNs but there are not public notebooks about transformers. I needed to design this network and tune this network by myself. I am very happy with the results.\n\nMy teammate @drhabib , @mmotoki , @lihuajing , @yamsam gave me many helpful suggestions to improve CV from 0.173 to 0.135. Thanks teammates!\n\nThis notebook only submits `submission.csv` inferred from 1 fold (and scores LB 0.146). If you train 11 folds (and use 100% train data per fold), the public LB will be 0.115 and private 0.117. If you train 32 folds (and 100% data), the public LB will be 0.112 and private 0.113!","metadata":{}},{"cell_type":"code","source":"import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n\nVER = 81\nFIRST_FOLD_ONLY = True\nTRAIN_MODEL = True","metadata":{"execution":{"iopub.status.busy":"2021-11-05T13:06:00.121821Z","iopub.execute_input":"2021-11-05T13:06:00.122666Z","iopub.status.idle":"2021-11-05T13:06:00.149219Z","shell.execute_reply.started":"2021-11-05T13:06:00.122537Z","shell.execute_reply":"2021-11-05T13:06:00.148503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.callbacks import LearningRateScheduler, ReduceLROnPlateau\n\nimport pandas as pd, numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.metrics import mean_absolute_error","metadata":{"execution":{"iopub.status.busy":"2021-11-05T13:06:00.15048Z","iopub.execute_input":"2021-11-05T13:06:00.151308Z","iopub.status.idle":"2021-11-05T13:06:06.403948Z","shell.execute_reply.started":"2021-11-05T13:06:00.151256Z","shell.execute_reply":"2021-11-05T13:06:06.402896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Data and Feature Engineer\nWe will use the features from [DLastStark's][2] great notebook [here][1]. Most likely if we remove and/or add features we can improve this transformers CV LB.\n\n[1]: https://www.kaggle.com/dlaststark/gb-vpp-pulp-fiction\n[2]: https://www.kaggle.com/dlaststark","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/ventilator-pressure-prediction/train.csv')\ntest = pd.read_csv('../input/ventilator-pressure-prediction/test.csv')\nsubmission = pd.read_csv('../input/ventilator-pressure-prediction/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-11-05T13:06:06.405178Z","iopub.execute_input":"2021-11-05T13:06:06.405419Z","iopub.status.idle":"2021-11-05T13:06:21.95156Z","shell.execute_reply.started":"2021-11-05T13:06:06.405391Z","shell.execute_reply":"2021-11-05T13:06:21.950875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def add_features(df):\n    df['cross']= df['u_in'] * df['u_out']\n    df['cross2']= df['time_step'] * df['u_out']\n    df['area'] = df['time_step'] * df['u_in']\n    df['area'] = df.groupby('breath_id')['area'].cumsum()\n    df['time_step_cumsum'] = df.groupby(['breath_id'])['time_step'].cumsum()\n    df['u_in_cumsum'] = (df['u_in']).groupby(df['breath_id']).cumsum()\n    print(\"Step-1...Completed\")\n    \n    df['u_in_lag1'] = df.groupby('breath_id')['u_in'].shift(1)\n    df['u_out_lag1'] = df.groupby('breath_id')['u_out'].shift(1)\n    df['u_in_lag_back1'] = df.groupby('breath_id')['u_in'].shift(-1)\n    df['u_out_lag_back1'] = df.groupby('breath_id')['u_out'].shift(-1)\n    df['u_in_lag2'] = df.groupby('breath_id')['u_in'].shift(2)\n    df['u_out_lag2'] = df.groupby('breath_id')['u_out'].shift(2)\n    df['u_in_lag_back2'] = df.groupby('breath_id')['u_in'].shift(-2)\n    df['u_out_lag_back2'] = df.groupby('breath_id')['u_out'].shift(-2)\n    df['u_in_lag3'] = df.groupby('breath_id')['u_in'].shift(3)\n    df['u_out_lag3'] = df.groupby('breath_id')['u_out'].shift(3)\n    df['u_in_lag_back3'] = df.groupby('breath_id')['u_in'].shift(-3)\n    df['u_out_lag_back3'] = df.groupby('breath_id')['u_out'].shift(-3)\n    df['u_in_lag4'] = df.groupby('breath_id')['u_in'].shift(4)\n    df['u_out_lag4'] = df.groupby('breath_id')['u_out'].shift(4)\n    df['u_in_lag_back4'] = df.groupby('breath_id')['u_in'].shift(-4)\n    df['u_out_lag_back4'] = df.groupby('breath_id')['u_out'].shift(-4)\n    df = df.fillna(0)\n    print(\"Step-2...Completed\")\n    \n    df['breath_id__u_in__max'] = df.groupby(['breath_id'])['u_in'].transform('max')\n    df['breath_id__u_in__mean'] = df.groupby(['breath_id'])['u_in'].transform('mean')\n    df['breath_id__u_in__diffmax'] = df.groupby(['breath_id'])['u_in'].transform('max') - df['u_in']\n    df['breath_id__u_in__diffmean'] = df.groupby(['breath_id'])['u_in'].transform('mean') - df['u_in']\n\n    print(\"Step-3...Completed\")\n    \n    df['u_in_diff1'] = df['u_in'] - df['u_in_lag1']\n    df['u_out_diff1'] = df['u_out'] - df['u_out_lag1']\n    df['u_in_diff2'] = df['u_in'] - df['u_in_lag2']\n    df['u_out_diff2'] = df['u_out'] - df['u_out_lag2']\n    df['u_in_diff3'] = df['u_in'] - df['u_in_lag3']\n    df['u_out_diff3'] = df['u_out'] - df['u_out_lag3']\n    df['u_in_diff4'] = df['u_in'] - df['u_in_lag4']\n    df['u_out_diff4'] = df['u_out'] - df['u_out_lag4']\n    print(\"Step-4...Completed\")\n    \n    df['one'] = 1\n    df['count'] = (df['one']).groupby(df['breath_id']).cumsum()\n    df['u_in_cummean'] =df['u_in_cumsum'] /df['count']\n    \n    df['breath_id_lag']=df['breath_id'].shift(1).fillna(0)\n    df['breath_id_lag2']=df['breath_id'].shift(2).fillna(0)\n    df['breath_id_lagsame']=np.select([df['breath_id_lag']==df['breath_id']],[1],0)\n    df['breath_id_lag2same']=np.select([df['breath_id_lag2']==df['breath_id']],[1],0)\n    df['breath_id__u_in_lag'] = df['u_in'].shift(1).fillna(0)\n    df['breath_id__u_in_lag'] = df['breath_id__u_in_lag'] * df['breath_id_lagsame']\n    df['breath_id__u_in_lag2'] = df['u_in'].shift(2).fillna(0)\n    df['breath_id__u_in_lag2'] = df['breath_id__u_in_lag2'] * df['breath_id_lag2same']\n    print(\"Step-5...Completed\")\n    \n    df['time_step_diff'] = df.groupby('breath_id')['time_step'].diff().fillna(0)\n    df['ewm_u_in_mean'] = (df\\\n                           .groupby('breath_id')['u_in']\\\n                           .ewm(halflife=9)\\\n                           .mean()\\\n                           .reset_index(level=0,drop=True))\n    df[[\"15_in_sum\",\"15_in_min\",\"15_in_max\",\"15_in_mean\"]] = (df\\\n                                                              .groupby('breath_id')['u_in']\\\n                                                              .rolling(window=15,min_periods=1)\\\n                                                              .agg({\"15_in_sum\":\"sum\",\n                                                                    \"15_in_min\":\"min\",\n                                                                    \"15_in_max\":\"max\",\n                                                                    \"15_in_mean\":\"mean\"\n                                                                    #\"15_in_std\":\"std\"\n                                                               })\\\n                                                               .reset_index(level=0,drop=True))\n    print(\"Step-6...Completed\")\n        \n    df['u_in_lagback_diff1'] = df['u_in'] - df['u_in_lag_back1']\n    df['u_out_lagback_diff1'] = df['u_out'] - df['u_out_lag_back1']\n    df['u_in_lagback_diff2'] = df['u_in'] - df['u_in_lag_back2']\n    df['u_out_lagback_diff2'] = df['u_out'] - df['u_out_lag_back2']\n    print(\"Step-7...Completed\")\n    \n    df['R'] = df['R'].astype(str)\n    df['C'] = df['C'].astype(str)\n    df['R__C'] = df[\"R\"].astype(str) + '__' + df[\"C\"].astype(str)\n    df = pd.get_dummies(df)\n    print(\"Step-8...Completed\")\n    \n    return df\n\n# train = add_features(train)\n# test = add_features(test)","metadata":{"execution":{"iopub.status.busy":"2021-11-05T13:06:21.953699Z","iopub.execute_input":"2021-11-05T13:06:21.954026Z","iopub.status.idle":"2021-11-05T13:06:21.98957Z","shell.execute_reply.started":"2021-11-05T13:06:21.953987Z","shell.execute_reply":"2021-11-05T13:06:21.988517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Train shape is now:', train.shape )\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-05T13:06:21.991353Z","iopub.execute_input":"2021-11-05T13:06:21.991604Z","iopub.status.idle":"2021-11-05T13:06:22.027619Z","shell.execute_reply.started":"2021-11-05T13:06:21.991575Z","shell.execute_reply":"2021-11-05T13:06:22.026794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Auxilliary Target\nIn addition to using `pressure` as target, we will use `pressure.diff()` and `pressure.cumsum()` as an auxilliary target to help the model predict `pressure` and it's derivative and it's integral correctly. This boosts CV LB by huge +0.015! Thank you to my teammate @lihuajing for discovering this!","metadata":{}},{"cell_type":"code","source":"train['pressure_diff'] = train.groupby('breath_id').pressure.diff().fillna(0)\ntrain['pressure_integral'] = train.groupby('breath_id').pressure.cumsum()/200\ntargets = train[['pressure','pressure_diff','pressure_integral']].to_numpy().reshape(-1, 80, 3)\n\n# train.drop(['pressure','pressure_diff','pressure_integral','id', 'breath_id','one','count',\n#             'breath_id_lag','breath_id_lag2','breath_id_lagsame',\n#             'breath_id_lag2same'], axis=1, inplace=True)\n\n# test.drop(['id', 'breath_id','one','count','breath_id_lag',\n#             'breath_id_lag2','breath_id_lagsame',\n#             'breath_id_lag2same'], axis=1, inplace=True)\ntrain = np.load('../input/ventilatorpressurepredictionfeatures/train.npy')\ntest = np.load('../input/ventilatorpressurepredictionfeatures/test.npy')","metadata":{"execution":{"iopub.status.busy":"2021-11-05T13:06:22.028802Z","iopub.execute_input":"2021-11-05T13:06:22.029132Z","iopub.status.idle":"2021-11-05T13:07:26.256695Z","shell.execute_reply.started":"2021-11-05T13:06:22.029098Z","shell.execute_reply":"2021-11-05T13:07:26.255902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Targets shape is',targets.shape)","metadata":{"execution":{"iopub.status.busy":"2021-11-05T13:07:26.258291Z","iopub.execute_input":"2021-11-05T13:07:26.258628Z","iopub.status.idle":"2021-11-05T13:07:26.263352Z","shell.execute_reply.started":"2021-11-05T13:07:26.258591Z","shell.execute_reply":"2021-11-05T13:07:26.262706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Rearrange Column Order\nRearranging columns is not needed. However by doing this now we can reference the basic features with `inputs[:,:,:18]` in our model if we want to add a CNN to process those inputs so the model can learn its own features. Like `Transformer( Concatenate(inputs, CNN(inputs[:,:,18]) ) )`","metadata":{}},{"cell_type":"markdown","source":"COL_ORDER = list(train.columns[:3]) + list(train.columns[-15:]) + list(train.columns[3:-15])\ntrain = train[COL_ORDER]\ntest = test[COL_ORDER]\n\nprint('Train columns:')\nnp.array( COL_ORDER )","metadata":{"execution":{"iopub.status.busy":"2021-11-05T03:06:58.927227Z","iopub.execute_input":"2021-11-05T03:06:58.92744Z","iopub.status.idle":"2021-11-05T03:07:00.441795Z","shell.execute_reply.started":"2021-11-05T03:06:58.927416Z","shell.execute_reply":"2021-11-05T03:07:00.440888Z"}}},{"cell_type":"markdown","source":"# Normalize Features\nSince competition metric is MAE, we will normalize with RobustScaler which subtracts median and divides by interquartile range","metadata":{}},{"cell_type":"markdown","source":"RS = RobustScaler()\ntrain = RS.fit_transform(train.astype('float32'))\ntest = RS.transform(test.astype('float32'))\n\ntrain = train.reshape(-1, 80, train.shape[-1])\ntest = test.reshape(-1, 80, train.shape[-1])","metadata":{"execution":{"iopub.status.busy":"2021-11-05T03:07:00.443278Z","iopub.execute_input":"2021-11-05T03:07:00.443581Z","iopub.status.idle":"2021-11-05T03:07:12.393642Z","shell.execute_reply.started":"2021-11-05T03:07:00.443542Z","shell.execute_reply":"2021-11-05T03:07:12.392993Z"}}},{"cell_type":"markdown","source":"# Mask Loss\nWe will only apply loss to pressures with `u_out=0` during training. We will use Keras `sample_weight` and compile our model with `sample_weight_mode=\"temporal\"`. This improves this model for the competition metric.","metadata":{}},{"cell_type":"code","source":"#U_OUT_IDX = 2\nU_OUT_IDX = 4\ny_weight = np.ones_like( targets )\nu_out_values = train[:,:,U_OUT_IDX]\ny_weight[ u_out_values==0 ] = 0 # because robust scaler changes 1 to 0","metadata":{"execution":{"iopub.status.busy":"2021-11-05T13:07:26.264442Z","iopub.execute_input":"2021-11-05T13:07:26.264879Z","iopub.status.idle":"2021-11-05T13:07:26.493628Z","shell.execute_reply.started":"2021-11-05T13:07:26.264845Z","shell.execute_reply":"2021-11-05T13:07:26.492895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape, targets.shape, y_weight.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-05T13:07:26.494618Z","iopub.execute_input":"2021-11-05T13:07:26.495343Z","iopub.status.idle":"2021-11-05T13:07:26.501488Z","shell.execute_reply.started":"2021-11-05T13:07:26.495307Z","shell.execute_reply":"2021-11-05T13:07:26.500619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Setup GPU\nBelow we declare whether to use 1 GPU or multiple GPU. (Change `CUDA_VISIBLE_DEVICES` in code cell 1 to use more GPUs). Also we use mixed precision which doesn't make a big difference on Kaggle's P100 GPU but does speed up more recent GPUs.","metadata":{}},{"cell_type":"code","source":"IS_TPU = False\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    BATCH_SIZE = tpu_strategy.num_replicas_in_sync * 64\n    print(\"Running on TPU:\", tpu.master())\n    print(f\"Batch Size: {BATCH_SIZE}\")\n    IS_TPU = True    \nexcept ValueError:\n    tpu_strategy = tf.distribute.get_strategy()\n    BATCH_SIZE = 512\n    print(f\"Running on {tpu_strategy.num_replicas_in_sync} replicas\")\n    print(f\"Batch Size: {BATCH_SIZE}\")","metadata":{"execution":{"iopub.status.busy":"2021-11-05T13:07:26.504448Z","iopub.execute_input":"2021-11-05T13:07:26.504683Z","iopub.status.idle":"2021-11-05T13:07:32.213237Z","shell.execute_reply.started":"2021-11-05T13:07:26.504657Z","shell.execute_reply":"2021-11-05T13:07:32.212219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# USE MIXED PRECISION\n# UNFORTUNATELY FOR THIS MODEL, MIXED PRECISION HURTS MODEL PERFORMANCE\ntf.config.optimizer.set_experimental_options({\"auto_mixed_precision\": True})\nprint('Mixed precision enabled')","metadata":{"execution":{"iopub.status.busy":"2021-11-05T13:07:32.214581Z","iopub.execute_input":"2021-11-05T13:07:32.214907Z","iopub.status.idle":"2021-11-05T13:07:32.22109Z","shell.execute_reply.started":"2021-11-05T13:07:32.214863Z","shell.execute_reply":"2021-11-05T13:07:32.220002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Build Model\nWe will use 12 layers of Transformer encoder. We tried adding decoder but it doesn't help. \n\nAdditionally we will add a 30% skip connection bypassing each layer to help the model train. We will use `gelu` activation. Our data is numerical and not categorical (like word tokens) so we do not need a fancy embedding layer. One of our features is `time_step` so we do not need positional encoding. We do not use dropout. Hyperparameters were tuned using trial and error to maximize CV score.\n\nKeras provides tutorials on transformers [here][1] and [here][2]\n\n[1]: https://keras.io/examples/nlp/text_classification_with_transformer/\n[2]: https://www.tensorflow.org/text/tutorials/transformer","metadata":{}},{"cell_type":"code","source":"class TransformerBlock(layers.Layer):\n    def __init__(self, embed_dim, feat_dim, num_heads, ff_dim, rate=0.1):\n        super(TransformerBlock, self).__init__()\n        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.ffn = keras.Sequential(\n            [layers.Dense(ff_dim, activation=\"gelu\"), layers.Dense(feat_dim),]\n        )\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = layers.Dropout(rate)\n        self.dropout2 = layers.Dropout(rate)\n\n    def call(self, inputs, training):\n        attn_output = self.att(inputs, inputs)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(inputs + attn_output)\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        return self.layernorm2(out1 + ffn_output)","metadata":{"execution":{"iopub.status.busy":"2021-11-05T13:07:32.222485Z","iopub.execute_input":"2021-11-05T13:07:32.223058Z","iopub.status.idle":"2021-11-05T13:07:32.243873Z","shell.execute_reply.started":"2021-11-05T13:07:32.223028Z","shell.execute_reply":"2021-11-05T13:07:32.242859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feat_dim = train.shape[-1] + 32\nembed_dim = 64  # Embedding size for attention\nnum_heads = 8  # Number of attention heads\nff_dim = 128  # Hidden layer size in feed forward network inside transformer\ndropout_rate = 0.0\nnum_blocks = 12\n\ndef build_model():\n    inputs = layers.Input(shape=train.shape[-2:])\n        \n    # \"EMBEDDING LAYER\"\n    x = layers.Dense(feat_dim)(inputs)\n    x = layers.LayerNormalization(epsilon=1e-6)(x)\n    \n    old_weight = layers.Dense(1, activation='sigmoid')(x)\n    \n    # TRANSFORMER BLOCKS\n    for k in range(num_blocks):\n        x_old = x\n        transformer_block = TransformerBlock(embed_dim, feat_dim, num_heads, ff_dim, dropout_rate)\n        x = transformer_block(x)\n#         x = 0.7*x + 0.3*x_old # SKIP CONNECTION\n        x = x_old * old_weight + x * (1 - old_weight)# SKIP CONNECTION\n    \n    # REGRESSION HEAD\n    x = layers.Dense(128, activation=\"selu\")(x)\n    x = layers.Dropout(dropout_rate)(x)\n    outputs = layers.Dense(3, activation=\"linear\")(x)\n    model = keras.Model(inputs=inputs, outputs=outputs)\n        \n    return model","metadata":{"execution":{"iopub.status.busy":"2021-11-05T13:08:31.275641Z","iopub.execute_input":"2021-11-05T13:08:31.27659Z","iopub.status.idle":"2021-11-05T13:08:31.28733Z","shell.execute_reply.started":"2021-11-05T13:08:31.276541Z","shell.execute_reply":"2021-11-05T13:08:31.28617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Learning Schedule - Cosine with restarts\nUsing cosine with restarts instead of single cosine gives huge boost of CV LB by +0.020! Thank you to my teammate @lihuajing for discovering this!","metadata":{}},{"cell_type":"code","source":"import math \nimport matplotlib.pyplot as plt\n\nLR_START = 1e-6\nLR_MAX = 6e-4\nLR_MIN = 1e-6\nLR_RAMPUP_EPOCHS = 0\nLR_SUSTAIN_EPOCHS = 0\nEPOCHS = 420\nSTEPS = [60,120,240]\n\n\ndef lrfn(epoch):\n    if epoch<STEPS[0]:\n        epoch2 = epoch\n        EPOCHS2 = STEPS[0]\n    elif epoch<STEPS[0]+STEPS[1]:\n        epoch2 = epoch-STEPS[0]\n        EPOCHS2 = STEPS[1]\n    elif epoch<STEPS[0]+STEPS[1]+STEPS[2]:\n        epoch2 = epoch-STEPS[0]-STEPS[1]\n        EPOCHS2 = STEPS[2]\n    \n    if epoch2 < LR_RAMPUP_EPOCHS:\n        lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch2 + LR_START\n    elif epoch2 < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n        lr = LR_MAX\n    else:\n        decay_total_epochs = EPOCHS2 - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS - 1\n        decay_epoch_index = epoch2 - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS\n        phase = math.pi * decay_epoch_index / decay_total_epochs\n        cosine_decay = 0.5 * (1 + math.cos(phase))\n        lr = (LR_MAX - LR_MIN) * cosine_decay + LR_MIN\n    return lr\n\nrng = [i for i in range(EPOCHS)]\nlr_y = [lrfn(x) for x in rng]\nplt.figure(figsize=(10, 4))\nplt.plot(rng, lr_y, '-o')\nprint(\"Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\". \\\n          format(lr_y[0], max(lr_y), lr_y[-1]))\nlr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)\nplt.xlabel('Epoch',size=14)\nplt.ylabel('Learning Rate',size=14)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-05T13:07:32.261442Z","iopub.execute_input":"2021-11-05T13:07:32.262236Z","iopub.status.idle":"2021-11-05T13:07:32.51367Z","shell.execute_reply.started":"2021-11-05T13:07:32.262199Z","shell.execute_reply":"2021-11-05T13:07:32.512841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train Model\nUsing one Nvidia V100 GPU with mixed precision, each epoch takes 60 seconds. Therefore 1 fold of 420 epochs takes 7 hours. Training 11 folds (LB 115) takes 77 GPU hours and training 32 folds (LB 112) takes 224 GPU hours. This model was trained offline using multiple GPUs. Each GPU trained one fold and we could train multiple folds in parallel in only 7 hours. This allowed us to train a submission in one evening.","metadata":{}},{"cell_type":"code","source":"EPOCH = EPOCHS\nBATCH_SIZE = 64\n#BATCH_SIZE = 512\nNUM_FOLDS = 11\nSEED = 42\nVERBOSE = 2\n\nwith tpu_strategy.scope():\n    kf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=SEED)\n    \n    test_preds = []\n    oof_preds = []\n    oof_true = []\n    all_mask = []\n    test_folds = []\n    \n    for fold, (train_idx, test_idx) in enumerate(kf.split(train, targets)):\n        print('-'*15, '>', f'Fold {fold+1}', '<', '-'*15)\n        X_train, X_valid = train[train_idx], train[test_idx]\n        y_train, y_valid = targets[train_idx], targets[test_idx]\n        test_folds.append(test_idx)\n                \n        checkpoint_filepath = f\"folds{fold}_{VER}.hdf5\"\n\n        model = build_model()\n        opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n        model.compile(optimizer=opt, loss=\"mae\", sample_weight_mode=\"temporal\")\n\n        sv = keras.callbacks.ModelCheckpoint(\n                checkpoint_filepath, monitor='val_loss', verbose=1, save_best_only=True,\n                save_weights_only=True, mode='auto', save_freq='epoch',\n                options=None\n        )\n        if TRAIN_MODEL:\n            history = model.fit(X_train, y_train, verbose=VERBOSE,\n                                validation_data=(X_valid, y_valid, y_weight[test_idx,:,:1]\n                                                ), \n                                epochs=EPOCH, batch_size=BATCH_SIZE, callbacks=[lr_callback, sv]\n                                , sample_weight=y_weight[train_idx,:,:1]\n                               )\n        else:\n            model.load_weights(f'../input/vent-tranformer/folds{fold}_{VER}.hdf5')\n       \n        # PREDICT TEST\n        print('Predicting Test...')\n        test_preds.append(model.predict(test, batch_size=BATCH_SIZE, verbose=VERBOSE)[:,:,0]\\\n                          .squeeze().reshape(-1, 1).squeeze())\n        \n        # PREDICT OOF\n        print('Predicting OOF...')\n        oof_preds.append( model.predict(X_valid, verbose=VERBOSE)[:,:,0].squeeze().reshape(-1, 1) )\n        oof_true.append( y_valid[:,:,0].squeeze().reshape(-1, 1) )\n        score = mean_absolute_error(oof_true[-1], oof_preds[-1])\n        print(f\"Fold-{fold+1} | OOF all u_out Score: {score}\")\n        \n        mask = np.where( X_valid[:,:,U_OUT_IDX].reshape((-1,1))==-1 )[0]\n        mask_score = mean_absolute_error(oof_true[-1][mask], oof_preds[-1][mask])\n        print(f\"Fold-{fold+1} | OOF u_out=0 Score: {mask_score}\")\n        all_mask.append(mask)\n        \n        np.save(f'oof_v{VER}_trans',oof_preds)\n        #np.save(f'oof_true_v{VER}_trans',oof_true)\n        \n        if FIRST_FOLD_ONLY: break","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-11-05T13:08:43.757521Z","iopub.execute_input":"2021-11-05T13:08:43.757834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Plot Train Loss Valid Loss\nNote that the loss display below (but this notebook version doesn't have plot) isn't just `pressure` MAE because it includes masked time steps and includes auxilliary targets `presssure.diff()` and `pressure.cumsum()`.","metadata":{}},{"cell_type":"code","source":"# PLOT TRAIN AND VAL HISTORY\nplt.figure(figsize=(20,5))\nval = history.history['val_loss']\ntrn = history.history['loss']\nplt.plot(np.arange(len(val)),trn,label='train loss')\nplt.plot(np.arange(len(val)),val,label='valid loss')\nplt.xlabel('Epoch',size=14)\nplt.ylabel('Loss',size=14)\nplt.title('Train and Valid Loss',size=16)\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-05T13:07:35.83762Z","iopub.status.idle":"2021-11-05T13:07:35.838303Z","shell.execute_reply.started":"2021-11-05T13:07:35.838082Z","shell.execute_reply":"2021-11-05T13:07:35.838105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Display CV Score","metadata":{}},{"cell_type":"code","source":"if FIRST_FOLD_ONLY: \n    NUM_FOLDS=1\n\nt = 0\nfor k in range(NUM_FOLDS):\n    mask = all_mask[k]\n    mae = np.mean(np.abs( oof_preds[k].flatten()[mask] - oof_true[k].reshape(-1, 1).squeeze()[mask] ))\n    t += mae\n    print('Fold',k,'has u_out MAE =',mae)\nprint('Overall CV MAE =',t/NUM_FOLDS)","metadata":{"execution":{"iopub.status.busy":"2021-11-05T13:07:35.839292Z","iopub.status.idle":"2021-11-05T13:07:35.840042Z","shell.execute_reply.started":"2021-11-05T13:07:35.839855Z","shell.execute_reply":"2021-11-05T13:07:35.839874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Display CV Score with Post Process Rounding","metadata":{}},{"cell_type":"code","source":"t = 0\nfor k in range(NUM_FOLDS):\n    oof = oof_preds[k].copy()\n    oof2 = np.round( (oof+1.895744294564641)/0.07030214545121005 ) * 0.07030214545121005 -1.895744294564641\n    mae = np.mean(np.abs( oof2.flatten()[mask] - oof_true[k].reshape(-1, 1).squeeze()[mask] ))\n    t += mae\n    print('Fold',k,'has u_out MAE with PP =',mae)\nprint('Overall CV MAE with PP =',t/NUM_FOLDS)","metadata":{"execution":{"iopub.status.busy":"2021-11-05T13:07:35.841016Z","iopub.status.idle":"2021-11-05T13:07:35.841326Z","shell.execute_reply.started":"2021-11-05T13:07:35.84116Z","shell.execute_reply":"2021-11-05T13:07:35.841175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Save OOF DataFrame","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/ventilator-pressure-prediction/train.csv')\n\n# CREATE FOLDS\nfolds = test_folds.copy()\nfor k in range(len(folds)):\n    folds[k] = np.ones_like(folds[k])*k\nfolds = np.hstack(folds)\nfolds = np.repeat(folds,80)\n\n# CREATE LIST OF IDS USED DURING VALIDATION\ntest_folds = np.hstack(test_folds)\ntest_folds = 80 * np.repeat(test_folds,80)\nshifter = np.tile( np.arange(80), len(test_folds)//80 )\ntest_folds += shifter\n\n# SAVE OOF TO TRAIN DATAFRAME\ntrain = train.loc[test_folds]\noof_preds = np.vstack(oof_preds)\noof_len = min(oof.shape)\ntrain['oof'] = oof.squeeze()\ntrain['fold'] = folds \n\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-05T13:07:35.842245Z","iopub.status.idle":"2021-11-05T13:07:35.842545Z","shell.execute_reply.started":"2021-11-05T13:07:35.842386Z","shell.execute_reply":"2021-11-05T13:07:35.842401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# SAVE OOF TO DISK\ntrain.id = train.id.astype('int32')\ntrain.oof = train.oof.astype('float32')\ntrain.fold = train.fold.astype('int8')\ntrain[['id','oof','fold']].to_csv(f'oof_v{VER}.csv')\ntrain[['id','oof','fold']].head()","metadata":{"execution":{"iopub.status.busy":"2021-11-05T13:07:35.844239Z","iopub.status.idle":"2021-11-05T13:07:35.844594Z","shell.execute_reply.started":"2021-11-05T13:07:35.844394Z","shell.execute_reply":"2021-11-05T13:07:35.844419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create Submission File","metadata":{}},{"cell_type":"code","source":"submission = pd.read_csv('../input/ventilator-pressure-prediction/sample_submission.csv')\nsubmission[\"pressure\"] = sum(test_preds)/NUM_FOLDS\nsubmission.to_csv(f'submission_mean_{VER}.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-05T13:07:35.845567Z","iopub.status.idle":"2021-11-05T13:07:35.845914Z","shell.execute_reply.started":"2021-11-05T13:07:35.845708Z","shell.execute_reply":"2021-11-05T13:07:35.845728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission[\"pressure\"] = np.median(np.vstack(test_preds),axis=0)\nsubmission.to_csv(f'submission_median_{VER}.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-05T13:07:35.846943Z","iopub.status.idle":"2021-11-05T13:07:35.847247Z","shell.execute_reply.started":"2021-11-05T13:07:35.847081Z","shell.execute_reply":"2021-11-05T13:07:35.847096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-05T13:07:35.848514Z","iopub.status.idle":"2021-11-05T13:07:35.849434Z","shell.execute_reply.started":"2021-11-05T13:07:35.849192Z","shell.execute_reply":"2021-11-05T13:07:35.849218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.pressure =\\\n    np.round( (submission.pressure+1.895744294564641)/0.07030214545121005 ) * 0.07030214545121005 -1.895744294564641\nsubmission.to_csv(f'submission_median_snap_{VER}.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-05T13:07:35.850673Z","iopub.status.idle":"2021-11-05T13:07:35.85108Z","shell.execute_reply.started":"2021-11-05T13:07:35.850874Z","shell.execute_reply":"2021-11-05T13:07:35.850891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-05T13:07:35.852306Z","iopub.status.idle":"2021-11-05T13:07:35.852781Z","shell.execute_reply.started":"2021-11-05T13:07:35.852571Z","shell.execute_reply":"2021-11-05T13:07:35.852598Z"},"trusted":true},"execution_count":null,"outputs":[]}]}