{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# This is a modeling notebook for [EDA about: LSTM Feature Importance](https://www.kaggle.com/marutama/eda-about-lstm-feature-importance).","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport gc\n\nimport optuna\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.callbacks import LearningRateScheduler, ReduceLROnPlateau\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\n\nfrom sklearn.metrics import mean_absolute_error as mae\nfrom sklearn.preprocessing import RobustScaler, normalize\nfrom sklearn.model_selection import train_test_split, GroupKFold, KFold\n\nfrom IPython.display import display","metadata":{"_uuid":"e331dbcc-0346-4019-9ff6-b890154a878b","_cell_guid":"3e5a0bd1-3e22-4b2c-a565-68985e55f95e","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-10-12T06:11:36.071345Z","iopub.execute_input":"2021-10-12T06:11:36.071716Z","iopub.status.idle":"2021-10-12T06:11:43.352022Z","shell.execute_reply.started":"2021-10-12T06:11:36.071619Z","shell.execute_reply":"2021-10-12T06:11:43.351364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DEBUG = False\n\ntrain = pd.read_csv('../input/ventilator-pressure-prediction/train.csv')\ntest = pd.read_csv('../input/ventilator-pressure-prediction/test.csv')\nsubmission = pd.read_csv('../input/ventilator-pressure-prediction/sample_submission.csv')\n\nif DEBUG:\n    train = train[:80*1000]","metadata":{"_uuid":"ca71d87d-6594-4f31-906d-0b53cd4c1374","_cell_guid":"89eb257e-0ed2-4f8b-94d6-462a5e995eb1","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-10-12T06:11:43.353645Z","iopub.execute_input":"2021-10-12T06:11:43.35397Z","iopub.status.idle":"2021-10-12T06:12:00.541035Z","shell.execute_reply.started":"2021-10-12T06:11:43.353945Z","shell.execute_reply":"2021-10-12T06:12:00.540337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def add_features(df):\n#     # rewritten calculation of lag features from this notebook: https://www.kaggle.com/patrick0302/add-lag-u-in-as-new-feat\n#     # some of ideas from this notebook: https://www.kaggle.com/mst8823/google-brain-lightgbm-baseline\n#     df['last_value_u_in'] = df.groupby('breath_id')['u_in'].transform('last')\n#     df['u_in_lag1'] = df.groupby('breath_id')['u_in'].shift(1)\n#     df['u_out_lag1'] = df.groupby('breath_id')['u_out'].shift(1)\n#     df['u_in_lag_back1'] = df.groupby('breath_id')['u_in'].shift(-1)\n#     df['u_out_lag_back1'] = df.groupby('breath_id')['u_out'].shift(-1)\n#     df['u_in_lag2'] = df.groupby('breath_id')['u_in'].shift(2)\n#     df['u_out_lag2'] = df.groupby('breath_id')['u_out'].shift(2)\n#     df['u_in_lag_back2'] = df.groupby('breath_id')['u_in'].shift(-2)\n#     df['u_out_lag_back2'] = df.groupby('breath_id')['u_out'].shift(-2)\n#     df['u_in_lag3'] = df.groupby('breath_id')['u_in'].shift(3)\n#     df['u_out_lag3'] = df.groupby('breath_id')['u_out'].shift(3)\n#     df['u_in_lag_back3'] = df.groupby('breath_id')['u_in'].shift(-3)\n#     df['u_out_lag_back3'] = df.groupby('breath_id')['u_out'].shift(-3)\n#     df = df.fillna(0)\n\n\n#     df['R__C'] = df[\"R\"].astype(str) + '__' + df[\"C\"].astype(str)\n\n#     # max value of u_in and u_out for each breath\n#     df['breath_id__u_in__max'] = df.groupby(['breath_id'])['u_in'].transform('max')\n#     df['breath_id__u_out__max'] = df.groupby(['breath_id'])['u_out'].transform('max')\n\n#     # difference between consequitive values\n#     df['u_in_diff1'] = df['u_in'] - df['u_in_lag1']\n#     df['u_out_diff1'] = df['u_out'] - df['u_out_lag1']\n#     df['u_in_diff2'] = df['u_in'] - df['u_in_lag2']\n#     df['u_out_diff2'] = df['u_out'] - df['u_out_lag2']\n#     # from here: https://www.kaggle.com/yasufuminakama/ventilator-pressure-lstm-starter\n#     df.loc[df['time_step'] == 0, 'u_in_diff'] = 0\n#     df.loc[df['time_step'] == 0, 'u_out_diff'] = 0\n\n#     # difference between the current value of u_in and the max value within the breath\n#     df['breath_id__u_in__diffmax'] = df.groupby(['breath_id'])['u_in'].transform('max') - df['u_in']\n#     df['breath_id__u_in__diffmean'] = df.groupby(['breath_id'])['u_in'].transform('mean') - df['u_in']\n\n#     # OHE\n#     df = df.merge(pd.get_dummies(df['R'], prefix='R'), left_index=True, right_index=True).drop(['R'], axis=1)\n#     df = df.merge(pd.get_dummies(df['C'], prefix='C'), left_index=True, right_index=True).drop(['C'], axis=1)\n#     df = df.merge(pd.get_dummies(df['R__C'], prefix='R__C'), left_index=True, right_index=True).drop(['R__C'], axis=1)\n\n#     # https://www.kaggle.com/c/ventilator-pressure-prediction/discussion/273974\n#     df['u_in_cumsum'] = df.groupby(['breath_id'])['u_in'].cumsum()\n#     df['time_step_cumsum'] = df.groupby(['breath_id'])['time_step'].cumsum()\n#     return df","metadata":{"execution":{"iopub.status.busy":"2021-10-12T06:12:00.54213Z","iopub.execute_input":"2021-10-12T06:12:00.542377Z","iopub.status.idle":"2021-10-12T06:12:00.548317Z","shell.execute_reply.started":"2021-10-12T06:12:00.542343Z","shell.execute_reply":"2021-10-12T06:12:00.5475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def add_features(df):\n    df['area'] = df['time_step'] * df['u_in']\n    df['area'] = df.groupby('breath_id')['area'].cumsum()\n\n    #######################################\n    # fast area calculation\n    df['time_delta'] = df['time_step'].diff()\n    df['time_delta'].fillna(0, inplace=True)\n    df['time_delta'].mask(df['time_delta'] < 0, 0, inplace=True)\n    df['tmp'] = df['time_delta'] * df['u_in']\n    df['area_true'] = df.groupby('breath_id')['tmp'].cumsum()\n    \n    #u_in_max_dict = df.groupby('breath_id')['u_in'].max().to_dict()\n    #df['u_in_max'] = df['breath_id'].map(u_in_max_dict)\n    #u_in_min_dict = df.groupby('breath_id')['u_in'].min().to_dict()\n    #df['u_in_min'] = df['breath_id'].map(u_in_min_dict)\n    u_in_mean_dict = df.groupby('breath_id')['u_in'].mean().to_dict()\n    df['u_in_mean'] = df['breath_id'].map(u_in_mean_dict)\n    del u_in_mean_dict\n    u_in_std_dict = df.groupby('breath_id')['u_in'].std().to_dict()\n    df['u_in_std'] = df['breath_id'].map(u_in_std_dict)\n    del u_in_std_dict\n    \n    # u_in_half is time:0 - time point of u_out:1 rise (almost 1.0s)\n    df['tmp'] = df['u_out']*(-1)+1 # inversion of u_out\n    df['u_in_half'] = df['tmp'] * df['u_in']\n    \n    # u_in_half: max, min, mean, std\n    u_in_half_max_dict = df.groupby('breath_id')['u_in_half'].max().to_dict()\n    df['u_in_half_max'] = df['breath_id'].map(u_in_half_max_dict)\n    del u_in_half_max_dict\n    u_in_half_min_dict = df.groupby('breath_id')['u_in_half'].min().to_dict()\n    df['u_in_half_min'] = df['breath_id'].map(u_in_half_min_dict)\n    del u_in_half_min_dict\n    u_in_half_mean_dict = df.groupby('breath_id')['u_in_half'].mean().to_dict()\n    df['u_in_half_mean'] = df['breath_id'].map(u_in_half_mean_dict)\n    del u_in_half_mean_dict\n    u_in_half_std_dict = df.groupby('breath_id')['u_in_half'].std().to_dict()\n    df['u_in_half_std'] = df['breath_id'].map(u_in_half_std_dict)\n    del u_in_half_std_dict\n    \n    gc.collect()\n    \n    # All entries are first point of each breath_id\n    first_df = df.loc[0::80,:]\n    # All entries are first point of each breath_id\n    last_df = df.loc[79::80,:]\n    \n    # The Main mode DataFrame and flag\n    main_df= last_df[(last_df['u_in']>4.8)&(last_df['u_in']<5.1)]\n    main_mode_dict = dict(zip(main_df['breath_id'], [1]*len(main_df)))\n    df['main_mode'] = df['breath_id'].map(main_mode_dict)\n    df['main_mode'].fillna(0, inplace=True)\n    del main_df\n    del main_mode_dict\n\n    # u_in: first point, last point\n    u_in_first_dict = dict(zip(first_df['breath_id'], first_df['u_in']))\n    df['u_in_first'] = df['breath_id'].map(u_in_first_dict)\n    del u_in_first_dict\n    u_in_last_dict = dict(zip(first_df['breath_id'], last_df['u_in']))\n    df['u_in_last'] = df['breath_id'].map(u_in_last_dict)\n    del u_in_last_dict\n    # time(sec) of end point\n    time_end_dict = dict(zip(last_df['breath_id'], last_df['time_step']))     \n    df['time_end'] = df['breath_id'].map(time_end_dict)\n    del time_end_dict\n    del last_df\n    \n    # u_out1_timing flag and DataFrame: speed up\n    # 高速版 uout1_df 作成\n    df['u_out_diff'] = df['u_out'].diff()\n    df['u_out_diff'].fillna(0, inplace=True)\n    df['u_out_diff'].replace(-1, 0, inplace=True)\n    uout1_df = df[df['u_out_diff']==1]\n    \n    gc.collect()\n    \n    #main_uout1 = uout1_df[uout1_df['main_mode']==1]\n    #nomain_uout1 = uout1_df[uout1_df['main_mode']==1]\n    \n    # Register Area when u_out becomes 1\n    uout1_area_dict = dict(zip(first_df['breath_id'], first_df['u_in']))\n    df['area_uout1'] = df['breath_id'].map(uout1_area_dict)\n    del uout1_area_dict\n    \n    # time(sec) when u_out becomes 1\n    uout1_dict = dict(zip(uout1_df['breath_id'], uout1_df['time_step']))\n    df['time_uout1'] = df['breath_id'].map(uout1_dict)\n    del uout1_dict\n    \n    # u_in when u_out becomes1\n    u_in_uout1_dict = dict(zip(uout1_df['breath_id'], uout1_df['u_in']))\n    df['u_in_uout1'] = df['breath_id'].map(u_in_uout1_dict)\n    del u_in_uout1_dict\n    \n    # Dict that puts 0 at the beginning of the 80row cycle\n    first_0_dict = dict(zip(first_df['id'], [0]*len(uout1_df)))\n\n    del first_df\n    del uout1_df   \n    \n    gc.collect()\n    \n    # Faster version u_in_diff creation, faster than groupby\n    df['u_in_diff'] = df['u_in'].diff()\n    df['tmp'] = df['id'].map(first_0_dict) # put 0, the 80row cycle\n    df.iloc[0::80, df.columns.get_loc('u_in_diff')] = df.iloc[0::80, df.columns.get_loc('tmp')]\n\n    # Create u_in vibration\n    df['diff_sign'] = np.sign(df['u_in_diff'])\n    df['sign_diff'] = df['diff_sign'].diff()\n    df['tmp'] = df['id'].map(first_0_dict) # put 0, the 80row cycle\n    df.iloc[0::80, df.columns.get_loc('sign_diff')] = df.iloc[0::80, df.columns.get_loc('tmp')]\n    del first_0_dict\n    \n    # Count the number of inversions, so take the absolute value and sum\n    df['sign_diff'] = abs(df['sign_diff']) \n    sign_diff_dict = df.groupby('breath_id')['sign_diff'].sum().to_dict()\n    df['diff_vib'] = df['breath_id'].map(sign_diff_dict)\n    \n    if 'diff_sign' in df.columns:\n        df.drop(['diff_sign', 'sign_diff'], axis=1, inplace=True)\n    if 'tmp' in df.columns:\n        df.drop(['tmp'], axis=1, inplace=True)\n    \n    gc.collect()\n    #######################################\n    '''\n    '''\n    \n    df['u_in_cumsum'] = (df['u_in']).groupby(df['breath_id']).cumsum()\n    \n    df['u_in_lag1'] = df.groupby('breath_id')['u_in'].shift(1)\n    #df['u_out_lag1'] = df.groupby('breath_id')['u_out'].shift(1)\n    df['u_in_lag_back1'] = df.groupby('breath_id')['u_in'].shift(-1)\n    #df['u_out_lag_back1'] = df.groupby('breath_id')['u_out'].shift(-1)\n    df['u_in_lag2'] = df.groupby('breath_id')['u_in'].shift(2)\n    #df['u_out_lag2'] = df.groupby('breath_id')['u_out'].shift(2)\n    df['u_in_lag_back2'] = df.groupby('breath_id')['u_in'].shift(-2)\n    #df['u_out_lag_back2'] = df.groupby('breath_id')['u_out'].shift(-2)\n    df['u_in_lag3'] = df.groupby('breath_id')['u_in'].shift(3)\n    #df['u_out_lag3'] = df.groupby('breath_id')['u_out'].shift(3)\n    df['u_in_lag_back3'] = df.groupby('breath_id')['u_in'].shift(-3)\n    #df['u_out_lag_back3'] = df.groupby('breath_id')['u_out'].shift(-3)\n    df['u_in_lag4'] = df.groupby('breath_id')['u_in'].shift(4)\n    #df['u_out_lag4'] = df.groupby('breath_id')['u_out'].shift(4)\n    df['u_in_lag_back4'] = df.groupby('breath_id')['u_in'].shift(-4)\n    #df['u_out_lag_back4'] = df.groupby('breath_id')['u_out'].shift(-4)\n    df = df.fillna(0)\n    \n    df['breath_id__u_in__max'] = df.groupby(['breath_id'])['u_in'].transform('max')\n    #df['breath_id__u_out__max'] = df.groupby(['breath_id'])['u_out'].transform('max')\n    \n    df['u_in_diff1'] = df['u_in'] - df['u_in_lag1']\n    #df['u_out_diff1'] = df['u_out'] - df['u_out_lag1']\n    df['u_in_diff2'] = df['u_in'] - df['u_in_lag2']\n    #df['u_out_diff2'] = df['u_out'] - df['u_out_lag2']\n    \n    #df['breath_id__u_in__diffmax'] = df.groupby(['breath_id'])['u_in'].transform('max') - df['u_in']\n    df['breath_id__u_in__diffmean'] = df.groupby(['breath_id'])['u_in'].transform('mean') - df['u_in']\n    \n    df['u_in_diff3'] = df['u_in'] - df['u_in_lag3']\n    #df['u_out_diff3'] = df['u_out'] - df['u_out_lag3']\n    df['u_in_diff4'] = df['u_in'] - df['u_in_lag4']\n    #df['u_out_diff4'] = df['u_out'] - df['u_out_lag4']\n    #df['cross']= df['u_in']*df['u_out']\n    #df['cross2']= df['time_step']*df['u_out']\n    \n    df['R'] = df['R'].astype(str)\n    df['C'] = df['C'].astype(str)\n    df['R__C'] = df[\"R\"].astype(str) + '__' + df[\"C\"].astype(str)\n      \n    df = pd.get_dummies(df)\n    \n    # Drop an unimportant features\n    df.drop(['R__C_20__20', 'R__C_20__10', 'R__C_5__10', 'R__C_5__10', 'R_20', 'R_50', 'C_20'], axis=1, inplace=True)\n    \n    return df","metadata":{"_uuid":"dc41dbf2-f199-4b9d-bbd9-bf6084162b47","_cell_guid":"13a36b46-7067-4b29-aad3-7e3b15e8415b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-10-12T06:12:00.549772Z","iopub.execute_input":"2021-10-12T06:12:00.550134Z","iopub.status.idle":"2021-10-12T06:12:00.591869Z","shell.execute_reply.started":"2021-10-12T06:12:00.550099Z","shell.execute_reply":"2021-10-12T06:12:00.591186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrain = add_features(train)","metadata":{"execution":{"iopub.status.busy":"2021-10-12T06:12:00.594412Z","iopub.execute_input":"2021-10-12T06:12:00.594851Z","iopub.status.idle":"2021-10-12T06:12:48.811434Z","shell.execute_reply.started":"2021-10-12T06:12:00.594813Z","shell.execute_reply":"2021-10-12T06:12:48.810506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntest = add_features(test)","metadata":{"execution":{"iopub.status.busy":"2021-10-12T06:12:48.812999Z","iopub.execute_input":"2021-10-12T06:12:48.813486Z","iopub.status.idle":"2021-10-12T06:13:14.50265Z","shell.execute_reply.started":"2021-10-12T06:12:48.813445Z","shell.execute_reply":"2021-10-12T06:13:14.501392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-12T06:13:14.504768Z","iopub.execute_input":"2021-10-12T06:13:14.505114Z","iopub.status.idle":"2021-10-12T06:13:14.541278Z","shell.execute_reply.started":"2021-10-12T06:13:14.505071Z","shell.execute_reply":"2021-10-12T06:13:14.540451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"targets = train[['pressure']].to_numpy().reshape(-1, 80)\ntrain.drop(['pressure', 'id', 'breath_id'], axis=1, inplace=True)\ntest = test.drop(['id', 'breath_id'], axis=1)","metadata":{"_uuid":"346bf2c0-96d2-4da5-8837-c0f820294a85","_cell_guid":"01328860-fa2a-421c-9e5f-ea0048246f98","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-10-12T06:13:14.542445Z","iopub.execute_input":"2021-10-12T06:13:14.542766Z","iopub.status.idle":"2021-10-12T06:13:15.901594Z","shell.execute_reply.started":"2021-10-12T06:13:14.542732Z","shell.execute_reply":"2021-10-12T06:13:15.900815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"RS = RobustScaler()\ntrain = RS.fit_transform(train)\ntest = RS.transform(test)","metadata":{"execution":{"iopub.status.busy":"2021-10-12T06:13:15.902873Z","iopub.execute_input":"2021-10-12T06:13:15.903067Z","iopub.status.idle":"2021-10-12T06:13:26.671368Z","shell.execute_reply.started":"2021-10-12T06:13:15.903045Z","shell.execute_reply":"2021-10-12T06:13:26.670453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train.reshape(-1, 80, train.shape[-1])\ntest = test.reshape(-1, 80, train.shape[-1])","metadata":{"execution":{"iopub.status.busy":"2021-10-12T06:13:26.672542Z","iopub.execute_input":"2021-10-12T06:13:26.672765Z","iopub.status.idle":"2021-10-12T06:13:26.679103Z","shell.execute_reply.started":"2021-10-12T06:13:26.672737Z","shell.execute_reply":"2021-10-12T06:13:26.678346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import Callback\nimport tensorflow.keras.backend as K\nclass WarmupExponentialDecay(Callback):\n    def __init__(self,lr_base=0.0002,lr_min=0.0,decay=0,warmup_epochs=0):\n        self.num_passed_batchs = 0   #一个计数器\n        self.warmup_epochs=warmup_epochs  \n        self.lr=lr_base #learning_rate_base\n        self.lr_min=lr_min #最小的起始学习率,此代码尚未实现\n        self.decay=decay  #指数衰减率\n        self.steps_per_epoch=0 #也是一个计数器\n        \n    def on_batch_begin(self, batch, logs=None):\n        # params是模型自动传递给Callback的一些参数\n        if self.steps_per_epoch==0:\n            #防止跑验证集的时候呗更改了\n            if self.params['steps'] == None:\n                self.steps_per_epoch = np.ceil(1. * self.params['samples'] / self.params['batch_size'])\n            else:\n                self.steps_per_epoch = self.params['steps']\n        if self.num_passed_batchs < self.steps_per_epoch * self.warmup_epochs:\n            K.set_value(self.model.optimizer.lr,\n                        self.lr*(self.num_passed_batchs + 1) / self.steps_per_epoch / self.warmup_epochs)\n        else:\n            K.set_value(self.model.optimizer.lr,\n                        self.lr*((1-self.decay)**(self.num_passed_batchs-self.steps_per_epoch*self.warmup_epochs)))\n        self.num_passed_batchs += 1\n        \n    def on_epoch_begin(self,epoch,logs=None):\n        #用来输出学习率的,可以删除\n        print(\"learning_rate:\",K.get_value(self.model.optimizer.lr))","metadata":{"execution":{"iopub.status.busy":"2021-10-12T06:13:26.680176Z","iopub.execute_input":"2021-10-12T06:13:26.680443Z","iopub.status.idle":"2021-10-12T06:13:26.695682Z","shell.execute_reply.started":"2021-10-12T06:13:26.680413Z","shell.execute_reply":"2021-10-12T06:13:26.69493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EPOCH = 300\nBATCH_SIZE = 1024\nNUM_FOLDS = 10\n\nTPU = False\n\nif TPU:\n    # detect and init the TPU\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n\n    ## instantiate a distribution strategy\n    xpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # GET GPU STRATEGY\n    xpu_strategy = tf.distribute.get_strategy()\n\nwith xpu_strategy.scope():\n    kf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=2021)\n    test_preds = []\n    for fold, (train_idx, test_idx) in enumerate(kf.split(train, targets)):\n        print('-'*15, '>', f'Fold {fold+1}', '<', '-'*15)\n        X_train, X_valid = train[train_idx], train[test_idx]\n        y_train, y_valid = targets[train_idx], targets[test_idx]\n        model = keras.models.Sequential([\n            keras.layers.Input(shape=train.shape[-2:]),\n            keras.layers.Bidirectional(keras.layers.LSTM(1024, return_sequences=True)),\n            keras.layers.Bidirectional(keras.layers.LSTM(512, return_sequences=True)),\n            keras.layers.Bidirectional(keras.layers.LSTM(256, return_sequences=True)),\n            keras.layers.Bidirectional(keras.layers.LSTM(128, return_sequences=True)),\n#             keras.layers.Bidirectional(keras.layers.LSTM(128, return_sequences=True)),\n            keras.layers.Dense(128, activation='selu'),\n#             keras.layers.Dropout(0.1),\n            keras.layers.Dense(1),\n        ])\n        model.compile(optimizer=\"adam\", loss=\"mae\")\n\n#         scheduler = ExponentialDecay(1e-3, 40*((len(train)*0.8)/BATCH_SIZE), 1e-5)\n#         lr = LearningRateScheduler(scheduler, verbose=1)\n        lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=10, verbose=1)\n#         lr = WarmupExponentialDecay(lr_base=1e-3, decay=1e-5, warmup_epochs=30)\n        es = EarlyStopping(monitor=\"val_loss\", patience=60, verbose=1, mode=\"min\", restore_best_weights=True)\n    \n        checkpoint_filepath = f\"folds{fold}.hdf5\"\n        sv = keras.callbacks.ModelCheckpoint(\n            checkpoint_filepath, monitor='val_loss', verbose=1, save_best_only=True,\n            save_weights_only=False, mode='auto', save_freq='epoch',\n            options=None\n        )\n\n        model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=EPOCH, batch_size=BATCH_SIZE, callbacks=[lr, es, sv])\n        #model.save(f'Fold{fold+1} RNN Weights')\n        test_preds.append(model.predict(test).squeeze().reshape(-1, 1).squeeze())","metadata":{"execution":{"iopub.status.busy":"2021-10-12T06:13:26.696786Z","iopub.execute_input":"2021-10-12T06:13:26.697073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls ./","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission[\"pressure\"] = sum(test_preds)/NUM_FOLDS\nsubmission.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}