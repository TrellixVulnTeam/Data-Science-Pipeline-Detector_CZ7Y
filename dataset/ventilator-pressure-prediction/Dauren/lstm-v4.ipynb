{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom tqdm.notebook import tqdm\nfrom collections import Counter\n\n\nimport random\n\nimport time\n\nimport gc\n\nfrom sklearn.metrics import mean_absolute_error as mae\nfrom sklearn.preprocessing import RobustScaler, normalize\nfrom sklearn.model_selection import train_test_split, GroupKFold, KFold\n\nfrom IPython.display import display\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-02T10:05:56.523952Z","iopub.execute_input":"2021-11-02T10:05:56.524345Z","iopub.status.idle":"2021-11-02T10:05:56.533919Z","shell.execute_reply.started":"2021-11-02T10:05:56.524274Z","shell.execute_reply":"2021-11-02T10:05:56.532776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install einops","metadata":{"execution":{"iopub.status.busy":"2021-11-02T10:05:56.536145Z","iopub.execute_input":"2021-11-02T10:05:56.536621Z","iopub.status.idle":"2021-11-02T10:06:04.395663Z","shell.execute_reply.started":"2021-11-02T10:05:56.536587Z","shell.execute_reply":"2021-11-02T10:06:04.394766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from einops.layers.torch import Rearrange","metadata":{"execution":{"iopub.status.busy":"2021-11-02T10:06:04.397528Z","iopub.execute_input":"2021-11-02T10:06:04.398181Z","iopub.status.idle":"2021-11-02T10:06:04.40434Z","shell.execute_reply.started":"2021-11-02T10:06:04.398133Z","shell.execute_reply":"2021-11-02T10:06:04.402014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed = 0\n\nrandom.seed(seed)\nos.environ[\"PYTHONHASHSEED\"] = str(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False","metadata":{"execution":{"iopub.status.busy":"2021-11-02T10:06:04.406958Z","iopub.execute_input":"2021-11-02T10:06:04.407473Z","iopub.status.idle":"2021-11-02T10:06:04.414388Z","shell.execute_reply.started":"2021-11-02T10:06:04.407433Z","shell.execute_reply":"2021-11-02T10:06:04.413544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATA_PATH = \"/kaggle/input/ventilator-pressure-prediction/\"\ntrain = pd.read_csv(DATA_PATH + \"train.csv\")\ntest = pd.read_csv(DATA_PATH + \"test.csv\")\n# submission = pd.read_csv(DATA_PATH + \"sample_submission.csv\")\n\ntest_ids= test['id'].to_numpy()","metadata":{"execution":{"iopub.status.busy":"2021-11-02T10:06:04.416351Z","iopub.execute_input":"2021-11-02T10:06:04.416868Z","iopub.status.idle":"2021-11-02T10:06:13.662763Z","shell.execute_reply.started":"2021-11-02T10:06:04.416829Z","shell.execute_reply":"2021-11-02T10:06:13.661984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def add_features(df):\n    df['area'] = df['time_step'] * df['u_in']\n    df['area'] = df.groupby('breath_id')['area'].cumsum()\n    \n    df['u_in_cumsum'] = (df['u_in']).groupby(df['breath_id']).cumsum()\n    \n    df['u_in_lag1'] = df.groupby('breath_id')['u_in'].shift(1)\n    df['u_in_lag_back1'] = df.groupby('breath_id')['u_in'].shift(-1)\n    df['u_in_lag2'] = df.groupby('breath_id')['u_in'].shift(2)\n    df['u_in_lag_back2'] = df.groupby('breath_id')['u_in'].shift(-2)\n    df['u_in_lag3'] = df.groupby('breath_id')['u_in'].shift(3)\n    df['u_in_lag_back3'] = df.groupby('breath_id')['u_in'].shift(-3)\n    df['u_in_lag4'] = df.groupby('breath_id')['u_in'].shift(4)\n    df['u_in_lag_back4'] = df.groupby('breath_id')['u_in'].shift(-4)\n    df = df.fillna(0)\n    \n    df['breath_id__u_in__max'] = df.groupby(['breath_id'])['u_in'].transform('max')\n    \n    df['u_in_diff1'] = df['u_in'] - df['u_in_lag1']\n    df['u_in_diff2'] = df['u_in'] - df['u_in_lag2']\n    \n    df['breath_id__u_in__diffmax'] = df.groupby(['breath_id'])['u_in'].transform('max') - df['u_in']\n    df['breath_id__u_in__diffmean'] = df.groupby(['breath_id'])['u_in'].transform('mean') - df['u_in']\n    \n    df['u_in_diff3'] = df['u_in'] - df['u_in_lag3']\n    df['u_in_diff4'] = df['u_in'] - df['u_in_lag4']\n    \n    \n#     df['area'] = df['time_step'] * df['u_in']\n#     df['area'] = df.groupby('breath_id')['area'].cumsum()\n    df['time_step_cumsum'] = df.groupby(['breath_id'])['time_step'].cumsum()\n#     df['u_in_cumsum'] = (df['u_in']).groupby(df['breath_id']).cumsum()\n    df['exponent']=(- df['time_step'])/(df['R']*df['C'])\n    df['factor']=np.exp(df['exponent'])\n    df['vf']=(df['u_in_cumsum']*df['R'])/df['factor']\n    df['vt']=0\n    df.loc[df['time_step'] != 0, 'vt']=df['area']/(df['C']*(1 - df['factor']))\n    df['v']=df['vf']+df['vt']\n    \n#     df['R'] = df['R'].astype(str)\n#     df['C'] = df['C'].astype(str)\n    df['R__C'] = df[\"R\"].astype(str) + '__' + df[\"C\"].astype(str)\n    df = pd.get_dummies(df)\n    return df\n\ntrain = add_features(train)\ntest = add_features(test)\n\ntargets = train[['pressure']].to_numpy().reshape(-1, 80)\ntrain['w'] = 1 - train['u_out']\ntrain.drop(['pressure', 'id', 'breath_id', 'u_out'], axis=1, inplace=True)\ntest['w'] = 1 - test['u_out']\ntest = test.drop(['id', 'breath_id', 'u_out'], axis=1)\n\nr = train['R'].values.astype(np.float32).reshape(-1, 80, 1)\nr = (r== 5)*1 + (r==20)*2 + (r==50)*3\nc = train['C'].values.astype(np.float32).reshape(-1, 80, 1)\nc = (c==10)*1 + (c==20)*2 + (c==50)*3\n\nr_test = test['R'].values.astype(np.float32).reshape(-1, 80, 1)\nr_test = (r_test== 5)*1 + (r_test==20)*2 + (r_test==50)*3\nc_test = test['C'].values.astype(np.float32).reshape(-1, 80, 1)\nc_test = (c_test==10)*1 + (c_test==20)*2 + (c_test==50)*3\n\nRS = RobustScaler()\ntrain = RS.fit_transform(train.iloc[:,2:])\ntest = RS.transform(test.iloc[:,2:])\n\ntrain = train.reshape(-1, 80, train.shape[-1])\ntest = test.reshape(-1, 80, train.shape[-1])\n\ntrain = np.concatenate((r, c, train), axis=2)\ntest = np.concatenate((r_test, c_test, test), axis=2)\n\ny_test= np.zeros((test.shape[0],1))","metadata":{"execution":{"iopub.status.busy":"2021-11-02T10:06:13.664161Z","iopub.execute_input":"2021-11-02T10:06:13.664418Z","iopub.status.idle":"2021-11-02T10:07:06.758414Z","shell.execute_reply.started":"2021-11-02T10:06:13.664384Z","shell.execute_reply":"2021-11-02T10:07:06.751602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# General\nverbose = 1\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nNUM_WORKERS = 0\nsave_weights = True\n\n# Model\nHIDDEN_SIZE = 256\nNUM_LAYERS = 1\nNUM_CLASSES = 1\n\n# Main\nINPUT_SIZE = train.shape[-1]-1\nEPOCHS = 150\nBATCH_SIZE = 512\n\n# LR = 1e-3","metadata":{"execution":{"iopub.status.busy":"2021-11-02T11:42:18.844183Z","iopub.execute_input":"2021-11-02T11:42:18.844717Z","iopub.status.idle":"2021-11-02T11:42:18.849701Z","shell.execute_reply.started":"2021-11-02T11:42:18.844681Z","shell.execute_reply":"2021-11-02T11:42:18.848935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class VentilatorDataset(Dataset):\n    def __init__(self, data, target):\n        self.data = data\n        self.target = target\n        self.w = data[:,:,-1]\n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        df_sample = torch.tensor(self.data[idx, :, :-1],dtype= torch.float)\n        df_target = torch.tensor(self.target[idx, :],dtype= torch.float)\n        df_w = torch.tensor(self.data[idx, :, -1],dtype= torch.float)\n        \n        return df_sample, df_target, df_w","metadata":{"execution":{"iopub.status.busy":"2021-11-02T10:07:06.790454Z","iopub.execute_input":"2021-11-02T10:07:06.790862Z","iopub.status.idle":"2021-11-02T10:07:06.809205Z","shell.execute_reply.started":"2021-11-02T10:07:06.790816Z","shell.execute_reply":"2021-11-02T10:07:06.80841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RNNModel(nn.Module):\n    def __init__(self, input_size=INPUT_SIZE):\n        super(RNNModel, self).__init__()        \n        self.r_embed = nn.Embedding(4, 2)\n        self.c_embed = nn.Embedding(4, 2)\n        self.seq_embed = nn.Sequential(\n            Rearrange('b l d -> b d l'),\n            nn.Conv1d(2 + input_size, 32, kernel_size=5, padding=2, stride=1),\n            Rearrange('b d l -> b l d'),\n            nn.LayerNorm(32),\n            nn.SiLU(),\n            nn.Dropout(0.),\n        )\n        \n        self.lstm1 = nn.LSTM(32, 400, batch_first=True, bidirectional=True)\n        self.lstm2 = nn.LSTM(2*400, 300, batch_first=True, bidirectional=True)\n        self.lstm3 = nn.LSTM(2*300, 200, batch_first=True, bidirectional=True)\n        self.lstm4 = nn.LSTM(2*200, 100, batch_first=True, bidirectional=True)\n        self.selu = nn.SELU()\n#         self.dropout = nn.Dropout(p=0.5)\n        \n        self.head = nn.Sequential(\n            nn.Linear(2*100, 50),\n            nn.SiLU(),\n        )\n        self.pressure = nn.Linear(50, 1)\n#         self.pressure_out = nn.Linear(50, 1)\n        \n        for name, p in self.named_parameters():\n            if 'lstm' in name:\n                if 'weight_ih' in name:\n                    nn.init.xavier_uniform_(p.data)\n                elif 'weight_hh' in name:\n                    nn.init.orthogonal_(p.data)\n                elif 'bias_ih' in name:\n                    p.data.fill_(0)\n                    # Set forget-gate bias to 1\n                    n = p.size(0)\n                    p.data[(n // 4):(n // 2)].fill_(1)\n                elif 'bias_hh' in name:\n                    p.data.fill_(0)\n\n        for name, m in self.named_modules():\n            if isinstance(m, nn.Linear):\n                #print(name,m)\n                nn.init.xavier_uniform_(m.weight.data)\n                m.bias.data.fill_(0)\n\n        \n        \n        \n    def forward(self, x):\n        batch_size = len(x)\n        r = x[:,:,0].long()\n        c = x[:,:,1].long()\n        r = self.r_embed(r)\n        c = self.c_embed(c)\n        seq = torch.cat((r, c, x[:,:,2:]), 2)\n        x = self.seq_embed(seq)\n        \n        x, _ = self.lstm1(x)\n        x = self.selu(x)\n        x, _ = self.lstm2(x)\n        x = self.selu(x)\n        x, _ = self.lstm3(x)\n        x = self.selu(x)\n        x, _ = self.lstm4(x)\n        x = self.selu(x)\n#         x = self.dropout(x)\n        x = self.head(x)\n        \n        pressure = self.pressure(x).reshape(batch_size, 80)\n#         pressure_out = self.pressure_out(x).reshape(batch_size, 80)\n        return pressure #, pressure_out\n        \n#         self.rnn1 = nn.LSTM(input_size, 256, num_layers, batch_first=True, bidirectional=True)\n# #         self.rnn2 = nn.LSTM(2*512, 256, num_layers, batch_first=True, bidirectional=True)\n#         self.selu = nn.SELU()\n#         self.fc1 = nn.Linear(2*256, num_classes)\n    \n    \n    \n#         nn.init.xavier_uniform_(self.rnn1.weight_ih_l0)\n#         nn.init.orthogonal_(self.rnn1.weight_hh_l0)\n#         nn.init.xavier_uniform_(self.rnn1.weight_ih_l0_reverse)\n#         nn.init.orthogonal_(self.rnn1.weight_hh_l0_reverse)\n        \n# #         nn.init.xavier_uniform_(self.rnn2.weight_ih_l0)\n# #         nn.init.orthogonal_(self.rnn2.weight_hh_l0)\n# #         nn.init.xavier_uniform_(self.rnn2.weight_ih_l0_reverse)\n# #         nn.init.orthogonal_(self.rnn2.weight_hh_l0_reverse)\n        \n#         nn.init.xavier_uniform_(self.fc1.weight)\n                \n#     def forward(self, x):\n#         out, _ = self.rnn1(x)\n# #         out = self.selu(out)\n# #         out, _ = self.rnn2(out)\n#         out = self.selu(out)\n#         out = self.fc1(out)\n                      \n#         return out","metadata":{"execution":{"iopub.status.busy":"2021-11-02T10:37:36.92818Z","iopub.execute_input":"2021-11-02T10:37:36.928955Z","iopub.status.idle":"2021-11-02T10:37:36.956761Z","shell.execute_reply.started":"2021-11-02T10:37:36.928909Z","shell.execute_reply":"2021-11-02T10:37:36.955833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NUM_FOLDS = 10\n\nkf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=0)\nsplits = kf.split(train, targets)","metadata":{"execution":{"iopub.status.busy":"2021-11-02T12:24:43.077246Z","iopub.execute_input":"2021-11-02T12:24:43.077516Z","iopub.status.idle":"2021-11-02T12:24:43.083386Z","shell.execute_reply.started":"2021-11-02T12:24:43.077487Z","shell.execute_reply":"2021-11-02T12:24:43.080809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class VentilatorLoss(nn.Module):\n    def __call__(self, preds, y, w):\n        mae = w * (y - preds).abs()\n        mae = mae.sum(-1) / w.sum(-1)\n\n        return mae","metadata":{"execution":{"iopub.status.busy":"2021-11-02T10:07:06.849109Z","iopub.execute_input":"2021-11-02T10:07:06.849359Z","iopub.status.idle":"2021-11-02T10:07:06.856269Z","shell.execute_reply.started":"2021-11-02T10:07:06.849327Z","shell.execute_reply":"2021-11-02T10:07:06.855592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions= np.zeros(test_ids.shape[0])\n\nfor i, (train_idx, val_idx) in enumerate(splits):\n#     if i < 1:\n    print(f\"\\n-------------   Fold {i + 1} / {5}  -------------\\n\")\n\n    model = RNNModel().to(device)\n    model.load_state_dict(torch.load('../input/weights-0171/ventilator0_val0171.pth', map_location=device))\n#         model.load_state_dict(torch.load('../input/val-0221/ventilator0_val_0221.pth', map_location=device))\n#         model.load_state_dict(torch.load('ventilator%d.pth' % i, map_location=device))\n#         optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-2)\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n#     optimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum=0.9)\n    scheduler= torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=30, T_mult=1, eta_min=0, last_epoch=-1, verbose=True)\n#     scheduler= torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor= 0.5, patience= 5, verbose= True)\n    criterion = nn.L1Loss()\n\n    X_train, X_val= train[train_idx], train[val_idx]\n    y_train, y_val= targets[train_idx], targets[val_idx]\n\n    train_dataset = VentilatorDataset(data=X_train, target=y_train)\n    val_dataset = VentilatorDataset(data= X_val, target= y_val)\n\n\n    # Data loaders\n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=NUM_WORKERS)\n    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n\n    best_valid_loss = float('inf')\n\n    loss_fct = VentilatorLoss()\n\n    for epoch in range(EPOCHS):\n        print(f\"Epoch: {epoch+1}\")\n        model.train()\n        start_time = time.time()\n\n        sum_train_loss = 0\n        for batch_idx, (X, y, w) in enumerate(train_loader):\n            pred = model(X.to(device)).squeeze(-1) #(batch, seq, feature)\n\n#                 train_loss =  w.to(device) * criterion(pred, y.to(device)) #for u_out == 1\n            train_loss = loss_fct(pred, y.to(device), w.to(device)).mean()\n            optimizer.zero_grad()\n            train_loss.backward()\n            sum_train_loss += train_loss.item()\n            optimizer.step()\n        epoch_train_loss = sum_train_loss / len(train_loader)\n        print(f\"epoch_train_loss={epoch_train_loss:.3f}\")\n\n\n        model.eval()\n        sum_val_loss = 0\n\n        with torch.no_grad():\n            for batch_idx, (X, y, w) in enumerate(val_loader):\n                pred = model(X.to(device)).squeeze(-1)\n\n                val_loss = loss_fct(pred, y.to(device), w.to(device)).mean()\n                sum_val_loss += val_loss.item()\n\n\n\n        epoch_val_loss = sum_val_loss / len(val_loader)\n\n        #save best model\n        if (epoch_val_loss < best_valid_loss):\n            best_valid_loss = epoch_val_loss\n            ofilename = 'ventilator%d.pth' % i\n            torch.save(model.state_dict(),  ofilename)\n\n        scheduler.step(epoch_val_loss)\n        \n        print(f\"LR={optimizer.param_groups[0]['lr']:.9f}\")\n        print(f\"epoch_val_loss={epoch_val_loss:.3f}\", f\"   min_val_loss={best_valid_loss:.3f}\")\n        end_time = time.time()\n        epoch_time = end_time - start_time\n        print(f\"epoch_time={epoch_time:.0f}\")\n\n    model.load_state_dict(torch.load('ventilator%d.pth' % i, map_location=device))\n    model.eval()\n\n    test_dataset = VentilatorDataset(data=test, target=y_test)\n    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n\n    preds = []\n    with torch.no_grad():\n        for batch_idx, (X, y, _) in enumerate(test_loader):\n            pred = model(X.to(device)).squeeze(-1)\n            preds.append(pred.detach().cpu().numpy())\n\n    preds = np.concatenate(preds, 0)\n    pred_test = preds.flatten()\n    predictions += pred_test/1","metadata":{"execution":{"iopub.status.busy":"2021-11-02T12:24:46.048018Z","iopub.execute_input":"2021-11-02T12:24:46.048465Z","iopub.status.idle":"2021-11-02T12:37:08.284141Z","shell.execute_reply.started":"2021-11-02T12:24:46.048429Z","shell.execute_reply":"2021-11-02T12:37:08.281906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions= np.zeros(test_ids.shape[0])\nmodel = RNNModel().to(device)\nmodel.load_state_dict(torch.load('../input/weights-val0174/ventilator0_val0174.pth', map_location=device))\nmodel.eval()\n\ntest_dataset = VentilatorDataset(data=test, target=y_test)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n\npreds = []\nwith torch.no_grad():\n    for batch_idx, (X, y, _) in enumerate(test_loader):\n        pred = model(X.to(device)).squeeze(-1)\n        preds.append(pred.detach().cpu().numpy())\n\npreds = np.concatenate(preds, 0)\npred_test = preds.flatten()\npredictions += pred_test/1","metadata":{"execution":{"iopub.status.busy":"2021-11-02T10:38:20.617786Z","iopub.execute_input":"2021-11-02T10:38:20.618096Z","iopub.status.idle":"2021-11-02T10:38:31.211001Z","shell.execute_reply.started":"2021-11-02T10:38:20.618058Z","shell.execute_reply":"2021-11-02T10:38:31.210232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame({'id': test_ids, 'pressure': predictions})\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-02T10:38:31.212509Z","iopub.execute_input":"2021-11-02T10:38:31.212761Z","iopub.status.idle":"2021-11-02T10:38:44.167753Z","shell.execute_reply.started":"2021-11-02T10:38:31.212726Z","shell.execute_reply":"2021-11-02T10:38:44.166804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"s = pd.read_csv('./submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-11-02T10:38:44.169341Z","iopub.execute_input":"2021-11-02T10:38:44.169619Z","iopub.status.idle":"2021-11-02T10:38:45.182239Z","shell.execute_reply.started":"2021-11-02T10:38:44.169581Z","shell.execute_reply":"2021-11-02T10:38:45.181491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"s","metadata":{"execution":{"iopub.status.busy":"2021-11-02T10:38:45.184161Z","iopub.execute_input":"2021-11-02T10:38:45.184433Z","iopub.status.idle":"2021-11-02T10:38:45.19541Z","shell.execute_reply.started":"2021-11-02T10:38:45.184397Z","shell.execute_reply":"2021-11-02T10:38:45.194629Z"},"trusted":true},"execution_count":null,"outputs":[]}]}