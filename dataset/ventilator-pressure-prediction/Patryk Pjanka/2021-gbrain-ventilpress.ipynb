{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nlast_only = True # only process the last model\nmodels_to_train = {} # a dictionary with models to train in the current session, if last_only == False\ndisplay_mode = True # train the models to be displayed in the final version of the notebook\ntrain_all = True # deprecated, keep true\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport pickle as pkl\n\nfrom tensorflow import keras\n\nimport matplotlib.pyplot as plt\n\nfrom tqdm import tqdm\n\n# initialize random\nrandom_state = 472837\nnp.random.seed(random_state)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-30T13:41:05.424414Z","iopub.execute_input":"2021-10-30T13:41:05.425294Z","iopub.status.idle":"2021-10-30T13:41:11.755588Z","shell.execute_reply.started":"2021-10-30T13:41:05.42519Z","shell.execute_reply":"2021-10-30T13:41:11.754721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's have a look at the data first\n\ndata = pd.read_csv('/kaggle/input/ventilator-pressure-prediction/train.csv')","metadata":{"execution":{"iopub.status.busy":"2021-10-30T13:41:11.756805Z","iopub.execute_input":"2021-10-30T13:41:11.757164Z","iopub.status.idle":"2021-10-30T13:41:20.522157Z","shell.execute_reply.started":"2021-10-30T13:41:11.757134Z","shell.execute_reply":"2021-10-30T13:41:20.521355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-30T13:41:20.523189Z","iopub.execute_input":"2021-10-30T13:41:20.523407Z","iopub.status.idle":"2021-10-30T13:41:20.546714Z","shell.execute_reply.started":"2021-10-30T13:41:20.523382Z","shell.execute_reply":"2021-10-30T13:41:20.5459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.describe()","metadata":{"execution":{"iopub.status.busy":"2021-10-30T13:41:20.54846Z","iopub.execute_input":"2021-10-30T13:41:20.548744Z","iopub.status.idle":"2021-10-30T13:41:21.905938Z","shell.execute_reply.started":"2021-10-30T13:41:20.548716Z","shell.execute_reply":"2021-10-30T13:41:21.905154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ok, we have quite a lot of data here (6M time stamps!). From the header, we can also see that the pressure clearly rises while the output valve is closed -- it will be nice to plot it later!\n\nLet's have a look at a single breath..","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(15,4))\n\nax = plt.subplot(131)\nbreath = data[data['breath_id'] == 1]\nbreath.plot(x='time_step', y=['R','C', 'u_in', 'u_out', 'pressure'], ax=ax)\n\nax = plt.subplot(132)\nbreath = data[data['breath_id'] == 100]\nbreath.plot(x='time_step', y=['R','C', 'u_in', 'u_out', 'pressure'], ax=ax)\n\nax = plt.subplot(133)\nbreath = data[data['breath_id'] == 5]\nbreath.plot(x='time_step', y=['R','C', 'u_in', 'u_out', 'pressure'], ax=ax)\n\nplt.show()\nplt.close()","metadata":{"execution":{"iopub.status.busy":"2021-10-30T13:41:21.907103Z","iopub.execute_input":"2021-10-30T13:41:21.907354Z","iopub.status.idle":"2021-10-30T13:41:22.537535Z","shell.execute_reply.started":"2021-10-30T13:41:21.907328Z","shell.execute_reply":"2021-10-30T13:41:22.536662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" - R and C remain constant throughout a breath\n - pressure rises when u_out=0, then drops and fluctuates when u_out=1\n - u_in can have a complex pattern while u_out=0, then is set to 0 when u_out first opens, and then rises as $\\propto$ (1-exp(1/x)) or so\n - clearly, pressure has a significant memory effect (duh), and the history of pressure is very important in predicting the future value\n - however, **some of the rapid fluctuations in pressure may not be significant** (they may be temporary obstrucitons in the airways, e.g.) -- removing them through pre-smoothing the pressure time series may be beneficial","metadata":{}},{"cell_type":"code","source":"# How many unique breath time series do we have?\nn_breaths = len(data.breath_id.unique())\nprint(\"No of breaths:\", n_breaths)\n\nbreath_length = data.groupby('breath_id').C.count().unique()\nif len(breath_length) == 1:\n    breath_length = breath_length[0]\nprint(\"No of data points per breath:\", breath_length)\n\nbreath_length_tmin = data.groupby('breath_id').time_step.min().unique()\nif len(breath_length_tmin) == 1:\n    breath_length_tmin = breath_length_tmin[0]\nprint(\"Breath tmin:\", breath_length_tmin)\n\nbreath_length_tmax = data.groupby('breath_id').time_step.max().unique()\nif len(breath_length_tmax) == 1:\n    breath_length_tmax = breath_length_tmax[0]\nprint(\"Breath tmax:\", breath_length_tmax)\n\ndata.groupby('breath_id').time_step.max().hist()","metadata":{"execution":{"iopub.status.busy":"2021-10-30T13:41:22.539117Z","iopub.execute_input":"2021-10-30T13:41:22.539458Z","iopub.status.idle":"2021-10-30T13:41:23.347072Z","shell.execute_reply.started":"2021-10-30T13:41:22.539417Z","shell.execute_reply":"2021-10-30T13:41:23.346242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" - All breaths are exactly 80 time stamps long.\n - They all start at t=0\n - Their length ranges between 2.5 and 2.8, with two clear peaks -- **it may be beneficial to split the dataset into these two subpopulations later**","metadata":{}},{"cell_type":"code","source":"# What is the initial pressure? -- we will probably need to guess it..\ndata.groupby('breath_id').pressure.first().hist()","metadata":{"execution":{"iopub.status.busy":"2021-10-30T13:41:23.348214Z","iopub.execute_input":"2021-10-30T13:41:23.348847Z","iopub.status.idle":"2021-10-30T13:41:23.677016Z","shell.execute_reply.started":"2021-10-30T13:41:23.348814Z","shell.execute_reply":"2021-10-30T13:41:23.676018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"What are the statistics of u_out ON / OFF times?","metadata":{}},{"cell_type":"code","source":"print(\"Initial u_out state:\", data.groupby('breath_id').u_out.first().unique())\nprint(\"Final u_out state:\", data.groupby('breath_id').u_out.last().unique())\nprint(\"Number of u_out switches:\", data.groupby('breath_id').u_out.aggregate(lambda x : np.sum(np.abs(np.array(x)[1:]-np.array(x)[:-1]))).unique())","metadata":{"execution":{"iopub.status.busy":"2021-10-30T13:41:23.678397Z","iopub.execute_input":"2021-10-30T13:41:23.67862Z","iopub.status.idle":"2021-10-30T13:41:25.702582Z","shell.execute_reply.started":"2021-10-30T13:41:23.678595Z","shell.execute_reply":"2021-10-30T13:41:25.701493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"u_out switch time step:\", data.groupby('breath_id').u_out.aggregate(lambda x : np.where(np.abs(np.array(x)[1:]-np.array(x)[:-1]))[0]).unique())\nprint(\"u_out switch time:\", data[data.u_out > 0].groupby('breath_id').time_step.min().hist())","metadata":{"execution":{"iopub.status.busy":"2021-10-30T13:41:25.704312Z","iopub.execute_input":"2021-10-30T13:41:25.704649Z","iopub.status.idle":"2021-10-30T13:41:27.797295Z","shell.execute_reply.started":"2021-10-30T13:41:25.704608Z","shell.execute_reply":"2021-10-30T13:41:27.796383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ok, so each breath starts with the outflow valve closed, and then it is opened once (and only once) at time step 25-30, or about 1s.","metadata":{}},{"cell_type":"code","source":"data.columns","metadata":{"execution":{"iopub.status.busy":"2021-10-30T13:41:27.799777Z","iopub.execute_input":"2021-10-30T13:41:27.800005Z","iopub.status.idle":"2021-10-30T13:41:27.807097Z","shell.execute_reply.started":"2021-10-30T13:41:27.799979Z","shell.execute_reply":"2021-10-30T13:41:27.805862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let us create a function that will transform the pandas DataFrame into 3D array more useful for our time-series analysis\n# , with an axis dedicated to breath_id\ndef parse_df (data):\n    return np.array(list(data.groupby('breath_id').apply(np.array)))","metadata":{"execution":{"iopub.status.busy":"2021-10-30T13:41:27.808622Z","iopub.execute_input":"2021-10-30T13:41:27.8092Z","iopub.status.idle":"2021-10-30T13:41:27.819548Z","shell.execute_reply.started":"2021-10-30T13:41:27.809131Z","shell.execute_reply":"2021-10-30T13:41:27.818439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"npdata = parse_df(data)","metadata":{"execution":{"iopub.status.busy":"2021-10-30T13:41:27.820903Z","iopub.execute_input":"2021-10-30T13:41:27.821421Z","iopub.status.idle":"2021-10-30T13:41:33.511441Z","shell.execute_reply.started":"2021-10-30T13:41:27.821288Z","shell.execute_reply":"2021-10-30T13:41:33.510775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"npdata.shape","metadata":{"execution":{"iopub.status.busy":"2021-10-30T13:41:33.512794Z","iopub.execute_input":"2021-10-30T13:41:33.513662Z","iopub.status.idle":"2021-10-30T13:41:33.519726Z","shell.execute_reply.started":"2021-10-30T13:41:33.513581Z","shell.execute_reply":"2021-10-30T13:41:33.518827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure()\nplt.plot(npdata[233,:,4], npdata[4,:,7])\nplt.show()\nplt.close()","metadata":{"execution":{"iopub.status.busy":"2021-10-30T13:41:33.521385Z","iopub.execute_input":"2021-10-30T13:41:33.522347Z","iopub.status.idle":"2021-10-30T13:41:33.668132Z","shell.execute_reply.started":"2021-10-30T13:41:33.52231Z","shell.execute_reply":"2021-10-30T13:41:33.667168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The general properties of the model should then be:\n - since we are not given any initial value, the initial pressure must be guessed based on the full feature set of other parameters,\n - features:\n   - single values of R and C,\n   - full time series of u_in, u_out, optionally as a function of time_step,\n   - pressure values before the current time step.\n - we need to predict:\n   - the time series of pressur **only** while u_out=0.\n - optional adjustments:\n   - pre-smooth the pressure curve before training.\n   \nIt is a bit odd that we know the full time series of u_in, u_out before calculting the pressures (if one thinks of it from the point of view of an actual application to a physical ventilator), but perhaps these are taken from a previous breath?","metadata":{}},{"cell_type":"code","source":"def preprocess (data, val_size=0.15, idx=None, submission=False):\n    \n    if submission:\n        datasets = ['subm',]\n        idx = {}\n        idx['subm'] = range(len(data.breath_id.unique()))\n    else:\n        datasets = ['train', 'val']\n        # generate indices for a train-validation split, if needed\n        if not isinstance(idx, dict) and not submission:\n            idx = {}\n            idx['val'] = np.random.choice(n_breaths, size=int(val_size*n_breaths), replace=False)\n            idx['train'] = [x for x in np.arange(n_breaths, dtype=int) if x not in idx['val']]\n    \n    # preprocess features\n    X = {}\n    for t in datasets:\n        X[t] = {}\n        # single-value features\n        for feature in ['R', 'C']:\n            X[t][feature] = np.array(data.groupby('breath_id')[feature].first())[idx[t]]\n        # time series\n        for feature in ['time_step', 'u_in', 'u_out']:\n            X[t][feature] = np.array(list(data.groupby('breath_id')[feature].apply(np.array)))[idx[t]]\n        \n    # preprocess labels\n    y = {}\n    if 'pressure' in data.columns:\n        for t in datasets:\n            y[t] = {}\n            y[t] = np.array(list(data.groupby('breath_id').pressure.apply(np.array)))[idx[t]]\n            # limit only to the inspiratory phase\n            y[t] *= (1 - np.array(list(data.groupby('breath_id').u_out.apply(np.array))))[idx[t]]\n        \n    return X, y, idx","metadata":{"execution":{"iopub.status.busy":"2021-10-30T13:41:33.669635Z","iopub.execute_input":"2021-10-30T13:41:33.670015Z","iopub.status.idle":"2021-10-30T13:41:33.682241Z","shell.execute_reply.started":"2021-10-30T13:41:33.669975Z","shell.execute_reply":"2021-10-30T13:41:33.681341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X, y, idx = preprocess(data)","metadata":{"execution":{"iopub.status.busy":"2021-10-30T13:41:33.683334Z","iopub.execute_input":"2021-10-30T13:41:33.683578Z","iopub.status.idle":"2021-10-30T13:41:49.257766Z","shell.execute_reply.started":"2021-10-30T13:41:33.683551Z","shell.execute_reply":"2021-10-30T13:41:49.256892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def score (y_pred, y_true, X=None, print_val=True, return_val=False, plot_examples=False):\n    mask = np.where(y_true > 0)[0]\n    result = np.mean(np.abs(y_pred-y_true)[mask])\n    if print_val:\n        print(\"Score: %.2f\" % result)\n    if plot_examples and isinstance(X, dict):\n        # draw 3 random indices to plot\n        fig = plt.figure(figsize=(15,4))\n        for i in range(3):\n            plt.subplot(1,3,i+1)\n            idx = np.random.randint(low=0,high=len(X['time_step']))\n            plt.title('Idx = %i' % idx)\n            for feature in ['R', 'C']:\n                plt.plot(X['time_step'][idx], X[feature][idx] * np.ones(X['time_step'][idx].shape), label=feature)\n            for feature in ['u_in', 'u_out']:\n                plt.plot(X['time_step'][idx], X[feature][idx], label=feature)\n            plt.plot(X['time_step'][idx], y_true.reshape(X['time_step'].shape)[idx], label='True')\n            plt.plot(X['time_step'][idx], y_pred.reshape(X['time_step'].shape)[idx], label='Pred')\n            plt.legend()\n        plt.show()\n        plt.close()\n    if return_val:\n        return result","metadata":{"execution":{"iopub.status.busy":"2021-10-30T13:41:49.258913Z","iopub.execute_input":"2021-10-30T13:41:49.259215Z","iopub.status.idle":"2021-10-30T13:41:49.268551Z","shell.execute_reply.started":"2021-10-30T13:41:49.259185Z","shell.execute_reply":"2021-10-30T13:41:49.267938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# As a baseline model, let's try a simple linear regression\nmodel_name = 'baseline'\nif model_name not in models_to_train.keys():\n    models_to_train[model_name] = True\n\nif display_mode or (not last_only and models_to_train[model_name]):\n\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.linear_model import LinearRegression\n    from sklearn.pipeline import make_pipeline\n\n    def preprocess_sgdr (X):\n        return np.concatenate([\n            X['R'].reshape((-1,1)),\n            X['C'].reshape((-1,1)),\n            X['u_in'], \n            X['u_out'], \n        ], axis=1)\n\n    reg = make_pipeline(StandardScaler(), LinearRegression())\n\n    reg.fit(preprocess_sgdr(X['train']), y['train'])\n    score(reg.predict(preprocess_sgdr(X['val'])).flatten(), y['val'].flatten(), X=X['val'], plot_examples=True)\n    \n# Score: 3.68 (1.48 taking into account only u_out==0)","metadata":{"execution":{"iopub.status.busy":"2021-10-30T13:41:49.269445Z","iopub.execute_input":"2021-10-30T13:41:49.27006Z","iopub.status.idle":"2021-10-30T13:41:49.288967Z","shell.execute_reply.started":"2021-10-30T13:41:49.27001Z","shell.execute_reply":"2021-10-30T13:41:49.288124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# submit to make sure that we calculate the score properly\n\nif False:\n\n    data_submit = pd.read_csv('/kaggle/input/ventilator-pressure-prediction/test.csv')\n    data_submit.head()\n\n    X_submit, _, _ = preprocess(data_submit, submission=True)\n\n    data_submit['pressure'] = reg.predict(preprocess_sgdr(X_submit['subm'])).flatten()\n\n    data_submit[['id','pressure']].to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-10-30T13:41:49.290218Z","iopub.execute_input":"2021-10-30T13:41:49.290525Z","iopub.status.idle":"2021-10-30T13:41:49.30392Z","shell.execute_reply.started":"2021-10-30T13:41:49.290488Z","shell.execute_reply":"2021-10-30T13:41:49.303274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The submission gives a score of 3.71, so now we know that our scoring implementation is correct. Let's work to improve the model!","metadata":{}},{"cell_type":"markdown","source":"The space of the input is not terribly large, we have 2+2*80 = 162 features, not counting the past pressure predictions.. Thus, we may be able to get something useful with just a simple deep neural network.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, models, callbacks","metadata":{"execution":{"iopub.status.busy":"2021-10-30T13:41:49.305175Z","iopub.execute_input":"2021-10-30T13:41:49.305426Z","iopub.status.idle":"2021-10-30T13:41:49.31979Z","shell.execute_reply.started":"2021-10-30T13:41:49.305398Z","shell.execute_reply":"2021-10-30T13:41:49.318746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = 'simple_dense_block'\nif model_name not in models_to_train.keys():\n    models_to_train[model_name] = True\n\nif not last_only and models_to_train[model_name]:\n\n    def preprocess_nn (X):\n        return np.dstack([\n            np.concatenate([X['R'].reshape((-1,1)),X['u_in']], axis=1),\n            np.concatenate([X['C'].reshape((-1,1)),X['u_out']], axis=1), \n        ])\n\n    nn = models.Sequential()\n    nn.add(layers.Input(shape=(81,2)))\n    nn.add(layers.Flatten())\n    nn.add(layers.Dense(128, activation='relu'))\n    nn.add(layers.Dense(128, activation='relu'))\n    nn.add(layers.Dense(128, activation='relu'))\n    nn.add(layers.Dense(80))\n\n    nn.compile(optimizer='adam',\n              loss=tf.keras.losses.MeanSquaredError(),\n              metrics=['RootMeanSquaredError'])\n\n    history = nn.fit(preprocess_nn(X['train']), y['train'],\n                    epochs=10,\n                    validation_data=(preprocess_nn(X['val']),y['val']))\n\n    plt.plot(history.history['root_mean_squared_error'], label='root_mean_squared_error')\n    plt.plot(history.history['val_root_mean_squared_error'], label = 'val_root_mean_squared_error')\n    plt.xlabel('Epoch')\n    plt.ylabel('RMSE')\n    plt.legend(loc='best')\n\n    score(nn.predict(preprocess_nn(X['val'])).flatten(), y['val'].flatten(), X=X['val'], plot_examples=True)\n    \n# Score: 0.95","metadata":{"execution":{"iopub.status.busy":"2021-10-30T13:41:49.320975Z","iopub.execute_input":"2021-10-30T13:41:49.321804Z","iopub.status.idle":"2021-10-30T13:41:49.50014Z","shell.execute_reply.started":"2021-10-30T13:41:49.321767Z","shell.execute_reply":"2021-10-30T13:41:49.499273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Oh, wow.. That is **much** better than SGDR!\n\nLet us fine tune the hyperparameters of such a single-block dense neural network, to see how much we can squeeze out of it.","metadata":{}},{"cell_type":"code","source":"import keras_tuner as kt","metadata":{"execution":{"iopub.status.busy":"2021-10-30T13:41:49.501134Z","iopub.execute_input":"2021-10-30T13:41:49.501866Z","iopub.status.idle":"2021-10-30T13:41:50.213439Z","shell.execute_reply.started":"2021-10-30T13:41:49.501825Z","shell.execute_reply":"2021-10-30T13:41:50.21222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_nn_singleBlock (hp):\n    nn = models.Sequential()\n    nn.add(layers.Input(shape=(81,2)))\n    nn.add(layers.Flatten())\n    #num_layers = hp.Int('num_layers',2,4)\n    num_layers = hp.Int('num_layers',4,6)\n    for i in range(num_layers):\n        with hp.conditional_scope('num_layers', list(range(i+1,8+1))): # num > i\n            if i > 0:\n                nn.add(layers.Dropout(\n                    hp.Float('dropout',0.,0.8,step=0.2)\n                ))\n            nn.add(layers.Dense(\n                #2**hp.Int('logSize_layer', 3,9, step=2), # this returns 9 so might want to try larger\n                2**hp.Int('logSize_layer', 8,12, step=1),\n                activation='relu'\n            ))\n    nn.add(layers.Dense(80))\n    \n    nn.compile(\n        optimizer=tf.keras.optimizers.Adam(),\n        loss=tf.keras.losses.MeanSquaredError(),\n        metrics=['RootMeanSquaredError',]\n    )\n    \n    return nn","metadata":{"execution":{"iopub.status.busy":"2021-10-30T13:41:50.21477Z","iopub.execute_input":"2021-10-30T13:41:50.215017Z","iopub.status.idle":"2021-10-30T13:41:50.224345Z","shell.execute_reply.started":"2021-10-30T13:41:50.214989Z","shell.execute_reply":"2021-10-30T13:41:50.223228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fit_from_hp_dict (build_model, hp_dict, \n                      X_train, y_train,\n                      X_val=None, y_val=None, validation=True,\n                      early_stopping=True, epochs=1024,\n                      train_model=True):\n    '''Using saved HyperParameter.values dict,\n    build the tuned model, train it, and plot diagnostics.'''\n    best_hyperparameters = kt.HyperParameters()\n    best_hyperparameters.values = hp_dict\n    best_model = build_model(best_hyperparameters)\n    if early_stopping:\n        stop_early = tf.keras.callbacks.EarlyStopping(\n            monitor='val_loss',\n            patience=16,\n            restore_best_weights=True\n        )\n        callbacks = [stop_early,]\n    else:\n        callbacks = []\n    if validation:\n        validation_data=(X_val, y_val)\n    else:\n        validation_data=None\n    \n    if train_model:\n        history = best_model.fit(\n            X_train, y_train,\n            epochs=epochs,\n            validation_data=validation_data,\n            callbacks=callbacks\n        )\n        plt.clf()\n        plt.plot(history.history['loss'], label='loss')\n        plt.plot(history.history['val_loss'], label = 'val_loss')\n        plt.xlabel('Epoch')\n        plt.ylabel('RMSE')\n        plt.legend(loc='lower right')\n        plt.show()\n        plt.close()\n        \n    return best_model","metadata":{"execution":{"iopub.status.busy":"2021-10-30T13:41:50.225915Z","iopub.execute_input":"2021-10-30T13:41:50.22656Z","iopub.status.idle":"2021-10-30T13:41:50.239598Z","shell.execute_reply.started":"2021-10-30T13:41:50.226517Z","shell.execute_reply":"2021-10-30T13:41:50.238865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# As a baseline model, let's try a simple linear regression\nmodel_name = 'tuned_dense_block'\nif model_name not in models_to_train.keys():\n    models_to_train[model_name] = True\n\nif not last_only and models_to_train[model_name]:\n\n    if False:# force_retune: # or not os.path.exists('....model'):\n        tuner = kt.Hyperband(\n            build_nn_singleBlock,\n            objective=kt.Objective('val_root_mean_squared_error',\n                                        direction='min'),\n            max_epochs=512,\n            hyperband_iterations=2,\n            overwrite=True\n        )\n        tuner.search(preprocess_nn(X['train']), y['train'],\n               epochs=10,\n               validation_data=(preprocess_nn(X['val']),y['val']))\n        best_model = tuner.get_best_models(1)[0]\n        best_hyperparameters = tuner.get_best_hyperparameters(1)[0]\n        # save for later reuse\n        with open('nn_singleBlock.pkl', 'wb') as f:\n            pkl.dump(best_hyperparameters, f)\n        best_model.save('nn_singleBlock.model')\n        print(best_hyperparameters.values)\n    else:\n        if False: # for some reason, this doesn't work with kaggle notebooks :/\n            print('Loading best_model from previous tuning..', flush=True, end='')\n            with open('nn_singleBlock.pkl', 'rb') as f:\n                best_hyperparameters = pkl.load(f)\n            best_model = models.load_model('nn_singleBlock.model')\n        else:\n            print('Setting hyperparameters to best from previous tuning..', flush=True)\n            #best_hyperparameters_dict = {\n            #    'num_layers': 3, 'logSize_layer': 9, \n            #    'dropout': 0.0, \n            #    'tuner/epochs': 3, 'tuner/initial_epoch': 0, \n            #    'tuner/bracket': 5, 'tuner/round': 0\n            #} # RMSE 0.9161117672920227\n            best_hyperparameters_dict = {\n                'num_layers': 4, 'logSize_layer': 10, \n                'dropout': 0.0, \n                'tuner/epochs': 3, 'tuner/initial_epoch': 0, \n                'tuner/bracket': 5, 'tuner/round': 0\n            } # RMSE 0.9090515971183777\n            best_model = fit_from_hp_dict(\n                build_nn_singleBlock, best_hyperparameters_dict, \n                preprocess_nn(X['train']), y['train'],\n                preprocess_nn(X['val']),y['val'],\n                train_model=True\n            )\n        print('done.')\n\n    print(best_model.summary())\n    score(best_model.predict(preprocess_nn(X['val'])).flatten(), y['val'].flatten(), X=X['val'], plot_examples=True)\n\n# Score: 0.67","metadata":{"execution":{"iopub.status.busy":"2021-10-30T13:41:50.240682Z","iopub.execute_input":"2021-10-30T13:41:50.24101Z","iopub.status.idle":"2021-10-30T13:41:50.257331Z","shell.execute_reply.started":"2021-10-30T13:41:50.240984Z","shell.execute_reply":"2021-10-30T13:41:50.256508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# submit\n\nif False:\n\n    data_submit = pd.read_csv('/kaggle/input/ventilator-pressure-prediction/test.csv')\n\n    X_submit, _, _ = preprocess(data_submit, submission=True)\n\n    data_submit['pressure'] = best_model.predict(preprocess_nn(X_submit['subm'])).flatten()\n\n    print(data_submit.head())\n    data_submit[['id','pressure']].to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-10-30T13:41:50.258479Z","iopub.execute_input":"2021-10-30T13:41:50.258817Z","iopub.status.idle":"2021-10-30T13:41:50.275168Z","shell.execute_reply.started":"2021-10-30T13:41:50.25879Z","shell.execute_reply":"2021-10-30T13:41:50.274172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The greatest missing point in our models so far is probably the omission of the fact that any pressure value is very dependent on the previous pressure values in the series. Let us try and take this fact into account.\n\nTo start simple, we can add a polynomial of time stamp values to our linear model to filter out any trends and seasonality.","metadata":{}},{"cell_type":"code","source":"X['train'].keys()","metadata":{"execution":{"iopub.status.busy":"2021-10-30T13:41:50.277312Z","iopub.execute_input":"2021-10-30T13:41:50.277692Z","iopub.status.idle":"2021-10-30T13:41:50.295819Z","shell.execute_reply.started":"2021-10-30T13:41:50.277648Z","shell.execute_reply":"2021-10-30T13:41:50.294646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# As a baseline model, let's try a simple linear regression\nmodel_name = 'LinReg_with_time'\nif model_name not in models_to_train.keys():\n    models_to_train[model_name] = True\n\nif not last_only and models_to_train[model_name]:\n    \n    from sklearn.preprocessing import StandardScaler\n    from sklearn.linear_model import LinearRegression\n    from sklearn.pipeline import make_pipeline\n\n    def preprocess_sgdr (X, time_order=0):\n        if time_order == 0:\n            times = [len(X['R']) * [[],],]\n        else:\n            times = []\n            for n in range(1,time_order+1):\n                times.append(X['time_step']**n)\n        return np.concatenate(\n            times + [\n            X['R'].reshape((-1,1)),\n            X['C'].reshape((-1,1)),\n            X['u_in'], \n            X['u_out'], \n        ], axis=1)\n\n    reg = make_pipeline(StandardScaler(), LinearRegression())\n\n    for time_order in range(5):\n        print(\"time_order = %i\" % time_order)\n        reg.fit(preprocess_sgdr(X['train'], time_order=time_order), y['train'])\n        score(reg.predict(preprocess_sgdr(X['val'], time_order=time_order)).flatten(), y['val'].flatten(), X=X['val'], plot_examples=True)\n\n# Scores: 3.7 -- 3.9","metadata":{"execution":{"iopub.status.busy":"2021-10-30T13:41:50.301515Z","iopub.execute_input":"2021-10-30T13:41:50.301893Z","iopub.status.idle":"2021-10-30T13:41:50.312147Z","shell.execute_reply.started":"2021-10-30T13:41:50.301856Z","shell.execute_reply":"2021-10-30T13:41:50.311022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Well, this doesn't help at all. Let us continue ignoring the time_step feature then, it doesn't seem to be relevant.\n\nInstead, let us focus on taking into account previously predicted pressure values in predicting the new pressure value. Since our NN performs much better than the linear model anyway, it is perhaps best to include this information there.. In principle, it may just do to replace the last of Dense layers in the network with an LSTM layer. Let's test this on our test NN, to quickly see the difference.","metadata":{}},{"cell_type":"code","source":"model_name = 'LSTM_corrective'\nif model_name not in models_to_train.keys():\n    models_to_train[model_name] = True\n\nif not last_only and models_to_train[model_name]:\n\n    print(\"LSTM as a correction step:\")\n\n    # use the pre-trained deep model from before\n    nn = models.clone_model(best_model)\n    for layer in nn.layers:\n        layer.trainable=False\n\n    # use lstm for correction\n    nn.add(layers.Reshape((80,1)))\n    nn.add(layers.LSTM(128))\n    nn.add(layers.Dense(80))\n\n    nn.compile(optimizer='adam',\n              loss=tf.keras.losses.MeanSquaredError(),\n              metrics=['RootMeanSquaredError'])\n\n    nn.summary()\n\n    stop_early = tf.keras.callbacks.EarlyStopping(\n        monitor='val_loss',\n        patience=16,\n        restore_best_weights=True\n    )\n    callbacks = [stop_early,] \n\n    if train_all:\n\n        history = nn.fit(preprocess_nn(X['train']), y['train'],\n                        epochs=1024,\n                        validation_data=(preprocess_nn(X['val']),y['val']),\n                        callbacks=callbacks)\n\n        plt.plot(history.history['root_mean_squared_error'], label='root_mean_squared_error')\n        plt.plot(history.history['val_root_mean_squared_error'], label = 'val_root_mean_squared_error')\n        plt.xlabel('Epoch')\n        plt.ylabel('RMSE')\n        plt.legend(loc='best')\n\n        score(nn.predict(preprocess_nn(X['val'])).flatten(), y['val'].flatten(), X=X['val'], plot_examples=True)\n\n# Score: 1.06","metadata":{"execution":{"iopub.status.busy":"2021-10-30T13:41:50.313993Z","iopub.execute_input":"2021-10-30T13:41:50.314499Z","iopub.status.idle":"2021-10-30T13:41:50.326057Z","shell.execute_reply.started":"2021-10-30T13:41:50.31445Z","shell.execute_reply":"2021-10-30T13:41:50.325117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = 'LSTM_simple_noFuture'\nif model_name not in models_to_train.keys():\n    models_to_train[model_name] = True\n\nif not last_only and models_to_train[model_name]:\n\n    print(\"LSTM only (no future data):\")\n\n    def preprocess_lstm (X):\n        return np.dstack([\n            np.tile(X['R'], (80,1)).transpose(), \n            np.tile(X['C'], (80,1)).transpose(), \n            X['u_in'], X['u_out']])\n\n    # use lstm for correction\n    nn = models.Sequential()\n    nn.add(layers.Input(shape=(80,4)))\n    nn.add(layers.LSTM(512))\n    nn.add(layers.Dense(512, activation='relu'))\n    nn.add(layers.Dense(512, activation='relu'))\n    nn.add(layers.Dense(80))\n\n    nn.compile(optimizer='adam',\n              loss=tf.keras.losses.MeanSquaredError(),\n              metrics=['RootMeanSquaredError'])\n\n    nn.summary()\n\n    stop_early = tf.keras.callbacks.EarlyStopping(\n        monitor='val_loss',\n        patience=16,\n        restore_best_weights=True\n    )\n    callbacks = [stop_early,]\n\n    history = nn.fit(preprocess_lstm(X['train']), y['train'],\n                    epochs=1024,\n                    validation_data=(preprocess_lstm(X['val']),y['val']),\n                    callbacks=callbacks)\n\n    plt.plot(history.history['root_mean_squared_error'], label='root_mean_squared_error')\n    plt.plot(history.history['val_root_mean_squared_error'], label = 'val_root_mean_squared_error')\n    plt.xlabel('Epoch')\n    plt.ylabel('RMSE')\n    plt.legend(loc='best')\n\n    score(nn.predict(preprocess_lstm(X['val'])).flatten(), y['val'].flatten(), X=X['val'], plot_examples=True)\n\n# Score: 0.54","metadata":{"execution":{"iopub.status.busy":"2021-10-30T13:41:50.328126Z","iopub.execute_input":"2021-10-30T13:41:50.328722Z","iopub.status.idle":"2021-10-30T13:41:50.34368Z","shell.execute_reply.started":"2021-10-30T13:41:50.328677Z","shell.execute_reply":"2021-10-30T13:41:50.342512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = 'LSTM_simple_hybrid'\nif model_name not in models_to_train.keys():\n    models_to_train[model_name] = True\n\nif display_mode or (not last_only and models_to_train[model_name]):\n\n    print(\"LSTM hybrid (future data mixed-in):\")\n\n    def preprocess_lstm (X):\n        return np.dstack([\n            np.tile(X['R'], (80,1)).transpose(), \n            np.tile(X['C'], (80,1)).transpose(), \n            X['u_in'], X['u_out']])\n\n    nn = models.Sequential()\n    nn.add(layers.Input(shape=(80,4)))\n    nn.add(layers.Dense(512, activation='relu'))\n    nn.add(layers.LSTM(512))\n    nn.add(layers.Dense(512, activation='relu'))\n    nn.add(layers.Dense(80))\n\n    nn.compile(optimizer='adam',\n              loss=tf.keras.losses.MeanSquaredError(),\n              metrics=['RootMeanSquaredError'])\n\n    nn.summary()\n\n    stop_early = tf.keras.callbacks.EarlyStopping(\n        monitor='val_loss',\n        patience=16,\n        restore_best_weights=True\n    )\n    callbacks = [stop_early,] \n\n    history = nn.fit(preprocess_lstm(X['train']), y['train'],\n                    epochs=1024,\n                    validation_data=(preprocess_lstm(X['val']),y['val']),\n                    callbacks=callbacks)\n\n    plt.plot(history.history['root_mean_squared_error'], label='root_mean_squared_error')\n    plt.plot(history.history['val_root_mean_squared_error'], label = 'val_root_mean_squared_error')\n    plt.xlabel('Epoch')\n    plt.ylabel('RMSE')\n    plt.legend(loc='best')\n\n    score(nn.predict(preprocess_lstm(X['val'])).flatten(), y['val'].flatten(), X=X['val'], plot_examples=True)\n\n# Score: 0.54","metadata":{"execution":{"iopub.status.busy":"2021-10-30T13:41:50.344989Z","iopub.execute_input":"2021-10-30T13:41:50.345237Z","iopub.status.idle":"2021-10-30T13:41:50.361434Z","shell.execute_reply.started":"2021-10-30T13:41:50.345211Z","shell.execute_reply":"2021-10-30T13:41:50.360449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ok, so using the LSTM as a correction step to our Dense neural network doesn't really help, but an LSTM network trained from scratch does do a bit better than just Dense layers, regardless of where the LSTM layer is. Let's use it to calculate a new submission then!","metadata":{}},{"cell_type":"code","source":"# submit\nif False:\n    data_submit = pd.read_csv('/kaggle/input/ventilator-pressure-prediction/test.csv')\n\n    X_submit, _, _ = preprocess(data_submit, submission=True)\n    data_submit['pressure'] = nn.predict(preprocess_lstm(X_submit['subm'])).flatten()\n\n    print(data_submit.head())\n    data_submit[['id','pressure']].to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-10-30T13:41:50.362545Z","iopub.execute_input":"2021-10-30T13:41:50.362826Z","iopub.status.idle":"2021-10-30T13:41:50.377572Z","shell.execute_reply.started":"2021-10-30T13:41:50.362794Z","shell.execute_reply":"2021-10-30T13:41:50.376668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Nice. Note that the LSTM network takes a long time to train. Before we proceed and tune it, it would be useful to adjust the loss function of our training so that it matches the competition's scoring function (at the moment, our loss takes into account the full pressure curve, while only the part with pressure > 0 is scored).","metadata":{}},{"cell_type":"code","source":"def competition_loss (y_true, y_pred):\n    #return tf.experimental.numpy.nanmean(tf.where(y_true > 0, tf.abs(y_true-y_pred), np.nan))\n    return tf.experimental.numpy.nanmean(tf.where(y_true > 0, (y_true-y_pred)**2, np.nan))","metadata":{"execution":{"iopub.status.busy":"2021-10-30T13:41:50.378695Z","iopub.execute_input":"2021-10-30T13:41:50.378908Z","iopub.status.idle":"2021-10-30T13:41:50.390984Z","shell.execute_reply.started":"2021-10-30T13:41:50.378884Z","shell.execute_reply":"2021-10-30T13:41:50.390359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = 'simple_dense_compLoss'\nif model_name not in models_to_train.keys():\n    models_to_train[model_name] = True\n\nif not last_only and models_to_train[model_name]:\n\n    # quick test to see if it works\n    def preprocess_nn (X):\n        return np.dstack([\n            np.concatenate([X['R'].reshape((-1,1)),X['u_in']], axis=1),\n            np.concatenate([X['C'].reshape((-1,1)),X['u_out']], axis=1), \n        ])\n\n    nn = models.Sequential()\n    nn.add(layers.Input(shape=(81,2)))\n    nn.add(layers.Flatten())\n    nn.add(layers.Dense(128, activation='relu'))\n    nn.add(layers.Dense(128, activation='relu'))\n    nn.add(layers.Dense(128, activation='relu'))\n    nn.add(layers.Dense(80))\n\n    nn.compile(optimizer='adam',\n              loss=competition_loss)\n\n    history = nn.fit(preprocess_nn(X['train']), y['train'],\n        epochs=10,\n        validation_data=(preprocess_nn(X['val']),y['val']))\n\n    plt.plot(history.history['loss'], label='loss')\n    plt.plot(history.history['val_loss'], label = 'val_loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend(loc='best')\n\n    score(nn.predict(preprocess_nn(X['val'])).flatten(), y['val'].flatten(), X=X['val'], plot_examples=True)\n\n# Score: 0.88","metadata":{"execution":{"iopub.status.busy":"2021-10-30T13:41:50.391963Z","iopub.execute_input":"2021-10-30T13:41:50.392357Z","iopub.status.idle":"2021-10-30T13:41:50.4037Z","shell.execute_reply.started":"2021-10-30T13:41:50.392324Z","shell.execute_reply":"2021-10-30T13:41:50.40284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Great! And the score indeed gets a bit better (it was ~1 originally).\n\nNow, let's use this loss function to re-tune out single-block dense NN, and the LSTM network..","metadata":{}},{"cell_type":"code","source":"def build_nn_singleBlock_comploss (hp):\n    nn = models.Sequential()\n    nn.add(layers.Input(shape=(81,2)))\n    nn.add(layers.Flatten())\n    num_layers = hp.Int('num_layers',3,5)\n    for i in range(num_layers):\n        with hp.conditional_scope('num_layers', list(range(i+1,8+1))): # num > i\n            if i > 0:\n                nn.add(layers.Dropout(\n                    hp.Float('dropout',0.,0.8,step=0.2)\n                ))\n            nn.add(layers.Dense(\n                2**hp.Int('logSize_layer', 8,12, step=1),\n                activation='relu'\n            ))\n    nn.add(layers.Dense(80))\n    \n    nn.compile(\n        optimizer=tf.keras.optimizers.Adam(),\n        loss=competition_loss\n    )\n    \n    return nn","metadata":{"execution":{"iopub.status.busy":"2021-10-30T13:41:50.404821Z","iopub.execute_input":"2021-10-30T13:41:50.405068Z","iopub.status.idle":"2021-10-30T13:41:50.415147Z","shell.execute_reply.started":"2021-10-30T13:41:50.405021Z","shell.execute_reply":"2021-10-30T13:41:50.414366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = 'tuned_dense_compLoss'\nif model_name not in models_to_train.keys():\n    models_to_train[model_name] = True\n\nif not last_only and models_to_train[model_name]:\n\n    if False:# force_retune: # or not os.path.exists('....model'):\n        tuner = kt.Hyperband(\n            build_nn_singleBlock_comploss,\n            objective=kt.Objective('val_loss',\n                                        direction='min'),\n            max_epochs=512,\n            hyperband_iterations=2,\n            overwrite=True\n        )\n        tuner.search(preprocess_nn(X['train']), y['train'],\n               epochs=10,\n               validation_data=(preprocess_nn(X['val']),y['val']))\n        best_model = tuner.get_best_models(1)[0]\n        best_hyperparameters = tuner.get_best_hyperparameters(1)[0]\n        # save for later reuse\n        with open('nn_singleBlock.pkl', 'wb') as f:\n            pkl.dump(best_hyperparameters, f)\n        best_model.save('nn_singleBlock.model')\n        print(best_hyperparameters.values)\n    else:\n        if False: # for some reason, this doesn't work with kaggle notebooks :/\n            print('Loading best_model from previous tuning..', flush=True, end='')\n            with open('nn_singleBlock.pkl', 'rb') as f:\n                best_hyperparameters = pkl.load(f)\n            best_model = models.load_model('nn_singleBlock.model')\n        else:\n            print('Setting hyperparameters to best from previous tuning..', flush=True)\n            best_hyperparameters_dict = {\n                'num_layers': 4, 'logSize_layer': 10, \n                'dropout': 0.0, \n                'tuner/epochs': 3, 'tuner/initial_epoch': 0, \n                'tuner/bracket': 5, 'tuner/round': 0\n            }\n            best_model = fit_from_hp_dict(\n                build_nn_singleBlock_comploss, best_hyperparameters_dict, \n                preprocess_nn(X['train']), y['train'],\n                preprocess_nn(X['val']),y['val'],\n                train_model=True\n            )\n        print('done.')\n\n    print(best_model.summary())\n    if train_all:\n        score(best_model.predict(preprocess_nn(X['val'])).flatten(), y['val'].flatten(), X=X['val'], plot_examples=True)\n\n# Score: 0.63","metadata":{"execution":{"iopub.status.busy":"2021-10-30T13:41:50.416543Z","iopub.execute_input":"2021-10-30T13:41:50.417001Z","iopub.status.idle":"2021-10-30T13:41:50.430795Z","shell.execute_reply.started":"2021-10-30T13:41:50.41696Z","shell.execute_reply":"2021-10-30T13:41:50.430175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# submit\nif False:\n    data_submit = pd.read_csv('/kaggle/input/ventilator-pressure-prediction/test.csv')\n\n    X_submit, _, _ = preprocess(data_submit, submission=True)\n    data_submit['pressure'] = best_model.predict(preprocess_nn(X_submit['subm'])).flatten()\n\n    print(data_submit.head())\n    data_submit[['id','pressure']].to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-10-30T13:41:50.431673Z","iopub.execute_input":"2021-10-30T13:41:50.432224Z","iopub.status.idle":"2021-10-30T13:41:50.44527Z","shell.execute_reply.started":"2021-10-30T13:41:50.432195Z","shell.execute_reply":"2021-10-30T13:41:50.444509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = 'LSTM_hybrid_compLoss'\nif model_name not in models_to_train.keys():\n    models_to_train[model_name] = True\n\nif not last_only and models_to_train[model_name]:\n\n    print(\"LSTM hybrid (future data mixed-in):\")\n\n    def preprocess_lstm (X):\n        return np.dstack([\n            np.tile(X['R'], (80,1)).transpose(), \n            np.tile(X['C'], (80,1)).transpose(), \n            X['u_in'], X['u_out']])\n\n    nn = models.Sequential()\n    nn.add(layers.Input(shape=(80,4)))\n    nn.add(layers.Dense(512, activation='relu'))\n    nn.add(layers.LSTM(512))\n    nn.add(layers.Dense(512, activation='relu'))\n    nn.add(layers.Dense(80))\n\n    nn.compile(optimizer='adam',\n              loss=competition_loss)\n\n    nn.summary()\n\n    stop_early = tf.keras.callbacks.EarlyStopping(\n        monitor='val_loss',\n        patience=16,\n        restore_best_weights=True\n    )\n    callbacks = [stop_early,]\n\n    history = nn.fit(preprocess_lstm(X['train']), y['train'],\n                    epochs=1024,\n                    validation_data=(preprocess_lstm(X['val']),y['val']),\n                    callbacks=callbacks)\n\n    plt.plot(history.history['loss'], label='loss')\n    plt.plot(history.history['val_loss'], label = 'val_loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend(loc='best')\n\n    score(nn.predict(preprocess_lstm(X['val'])).flatten(), y['val'].flatten(), X=X['val'], plot_examples=True)\n\n# Score: 0.66","metadata":{"execution":{"iopub.status.busy":"2021-10-30T13:41:50.446452Z","iopub.execute_input":"2021-10-30T13:41:50.446688Z","iopub.status.idle":"2021-10-30T13:41:50.457727Z","shell.execute_reply.started":"2021-10-30T13:41:50.44666Z","shell.execute_reply":"2021-10-30T13:41:50.456933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# submit\nif False:\n    data_submit = pd.read_csv('/kaggle/input/ventilator-pressure-prediction/test.csv')\n\n    X_submit, _, _ = preprocess(data_submit, submission=True)\n    data_submit['pressure'] = nn.predict(preprocess_lstm(X_submit['subm'])).flatten()\n\n    print(data_submit.head())\n    data_submit[['id','pressure']].to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-10-30T13:41:50.459261Z","iopub.execute_input":"2021-10-30T13:41:50.45971Z","iopub.status.idle":"2021-10-30T13:41:50.471861Z","shell.execute_reply.started":"2021-10-30T13:41:50.459669Z","shell.execute_reply":"2021-10-30T13:41:50.471197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Interesting! So discarding the exhale phase improves the deep network predictions (slightly), but worsens the LSTM ones. In hindsight, it does make sense -- LSTM learns to predict the time evolution of the system, so it benefits from using the full time range given.\n\nLet us then tune the LSTM network with the RMSE loss, as initially used.","metadata":{}},{"cell_type":"code","source":"print(\"LSTM hybrid (future data mixed-in):\")\n\ndef preprocess_lstm (X):\n    return np.dstack([\n        np.tile(X['R'], (80,1)).transpose(), \n        np.tile(X['C'], (80,1)).transpose(), \n        X['u_in'], X['u_out']])\n\ndef build_LSTMhybrid (hp):\n    nn = models.Sequential()\n    nn.add(layers.Input(shape=(80,4)))\n    # pre-conditioning with Dense layers\n    #num_layers_DensePre = hp.Int('num_layers_DensePre',0,2)\n    #for i in range(num_layers_DensePre):\n    #    with hp.conditional_scope('num_layers_DensePre', list(range(i+1,8+1))): # num > i\n    #        #if i > 0:\n    #        #    nn.add(layers.Dropout(\n    #        #        hp.Float('dropout',0.,0.4,step=0.2)\n    #        #    ))\n    #        nn.add(layers.Dense(\n    #            2**hp.Int('logSize_DensePre', 8,10, step=1),\n    #            activation='relu'\n    #        ))\n    # LSTM layer\n    nn.add(layers.LSTM(\n        2**hp.Int('logSize_LSTM', 10,12, step=1)\n    ))\n    # post-processing with Dense layers\n    num_layers_DensePost = hp.Int('num_layers_DensePost',3,5)\n    for i in range(num_layers_DensePost):\n        with hp.conditional_scope('num_layers_DensePost', list(range(i+1,8+1))): # num > i\n            #if i > 0:\n            #    nn.add(layers.Dropout(\n            #        hp.Float('dropout',0.,0.4,step=0.2)\n            #    ))\n            nn.add(layers.Dense(\n                2**hp.Int('logSize_DensePost', 10,12, step=1),\n                activation='relu'\n            ))\n    nn.add(layers.Dense(80))\n    \n    nn.compile(\n        optimizer=tf.keras.optimizers.Adam(),\n        loss=tf.keras.losses.MeanSquaredError(),\n        metrics=['RootMeanSquaredError',]\n    )\n    \n    return nn","metadata":{"execution":{"iopub.status.busy":"2021-10-30T13:41:50.472911Z","iopub.execute_input":"2021-10-30T13:41:50.473167Z","iopub.status.idle":"2021-10-30T13:41:50.485604Z","shell.execute_reply.started":"2021-10-30T13:41:50.473138Z","shell.execute_reply":"2021-10-30T13:41:50.485003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = 'tuned_LSTM_hybrid'\nif model_name not in models_to_train.keys():\n    models_to_train[model_name] = True\n\nif not last_only and models_to_train[model_name]:\n    \n    if False:# force_retune: # or not os.path.exists('....model'):\n        tuner = kt.Hyperband(\n            build_LSTMhybrid,\n            objective=kt.Objective('val_root_mean_squared_error',\n                                        direction='min'),\n            max_epochs=512,\n            hyperband_iterations=2,\n            overwrite=True\n        )\n        tuner.search(preprocess_lstm(X['train']), y['train'],\n               epochs=10,\n               validation_data=(preprocess_lstm(X['val']),y['val']))\n        best_model = tuner.get_best_models(1)[0]\n        best_hyperparameters = tuner.get_best_hyperparameters(1)[0]\n        # save for later reuse\n        with open('nn_singleBlock.pkl', 'wb') as f:\n            pkl.dump(best_hyperparameters, f)\n        best_model.save('nn_singleBlock.model')\n        print(best_hyperparameters.values)\n    else:\n        if False: # for some reason, this doesn't work with kaggle notebooks :/\n            print('Loading best_model from previous tuning..', flush=True, end='')\n            with open('nn_singleBlock.pkl', 'rb') as f:\n                best_hyperparameters = pkl.load(f)\n            best_model = models.load_model('nn_singleBlock.model')\n        else:\n            print('Setting hyperparameters to best from previous tuning..', flush=True)\n            #best_hyperparameters_dict = {\n            #    'num_layers_DensePre': 0, 'logSize_LSTM': 10, \n            #    'num_layers_DensePost': 3, 'logSize_DensePost': 10, \n            #    'tuner/epochs': 3, 'tuner/initial_epoch': 0, \n            #    'tuner/bracket': 5, 'tuner/round': 0\n            #} # Score: 0.60\n            best_hyperparameters_dict = {\n                'logSize_LSTM': 10, # tuner found 12, but it takes forever to train...\n                'num_layers_DensePost': 3, \n                'logSize_DensePost': 12, 'tuner/epochs': 3, \n                'tuner/initial_epoch': 0, 'tuner/bracket': 5, \n                'tuner/round': 0\n            } # Score: 0.57\n            best_model = fit_from_hp_dict(\n                build_LSTMhybrid, best_hyperparameters_dict, \n                preprocess_lstm(X['train']), y['train'],\n                preprocess_lstm(X['val']),y['val'],\n                train_model=True\n            )\n        print('done.')\n\n    print(best_model.summary())\n    score(best_model.predict(preprocess_lstm(X['val'])).flatten(), y['val'].flatten(), X=X['val'], plot_examples=True)\n\n# Score: 0.57","metadata":{"execution":{"iopub.status.busy":"2021-10-30T13:41:50.486824Z","iopub.execute_input":"2021-10-30T13:41:50.487053Z","iopub.status.idle":"2021-10-30T13:41:50.50073Z","shell.execute_reply.started":"2021-10-30T13:41:50.487016Z","shell.execute_reply":"2021-10-30T13:41:50.499688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# submit\nif False:\n    data_submit = pd.read_csv('/kaggle/input/ventilator-pressure-prediction/test.csv')\n\n    X_submit, _, _ = preprocess(data_submit, submission=True)\n    data_submit['pressure'] = best_model.predict(preprocess_lstm(X_submit['subm'])).flatten()\n\n    print(data_submit.head())\n    data_submit[['id','pressure']].to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-10-30T13:41:50.502009Z","iopub.execute_input":"2021-10-30T13:41:50.502479Z","iopub.status.idle":"2021-10-30T13:41:50.519381Z","shell.execute_reply.started":"2021-10-30T13:41:50.502447Z","shell.execute_reply":"2021-10-30T13:41:50.518505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There does not seem to be any real improvement over our generic LSTM network (which still holds the best result...). Let us look at the problem differently then. Let's come back to the generic LSTM network (which is fairly small and reasonably easy to train) and inspect the cases where its prediction on our validation set were the worst.","metadata":{}},{"cell_type":"code","source":"model_name = 'split_LSTM_hybrid'\nif model_name not in models_to_train.keys():\n    models_to_train[model_name] = True\n\nif display_mode or (not last_only and models_to_train[model_name]):\n\n    print(\"LSTM hybrid (future data mixed-in):\")\n\n    def preprocess_lstm (X):\n        return np.dstack([\n            np.tile(X['R'], (80,1)).transpose(), \n            np.tile(X['C'], (80,1)).transpose(), \n            X['u_in'], X['u_out']])\n\n    nn = models.Sequential()\n    nn.add(layers.Input(shape=(80,4)))\n    nn.add(layers.Dense(512, activation='relu'))\n    nn.add(layers.LSTM(512))\n    nn.add(layers.Dense(512, activation='relu'))\n    nn.add(layers.Dense(512, activation='relu'))\n    nn.add(layers.Dense(512, activation='relu'))\n    nn.add(layers.Dense(80))\n\n    nn.compile(optimizer='adam',\n              loss=tf.keras.losses.MeanSquaredError(),\n              metrics=['RootMeanSquaredError'])\n\n    nn.summary()\n\n    stop_early = tf.keras.callbacks.EarlyStopping(\n        monitor='val_loss',\n        patience=16,\n        restore_best_weights=True\n    )\n    callbacks = [stop_early,] \n\n    if True:# train_all:\n\n        history = nn.fit(preprocess_lstm(X['train']), y['train'],\n                        epochs=1024,\n                        validation_data=(preprocess_lstm(X['val']),y['val']),\n                        callbacks=callbacks)\n\n        plt.plot(history.history['root_mean_squared_error'], label='root_mean_squared_error')\n        plt.plot(history.history['val_root_mean_squared_error'], label = 'val_root_mean_squared_error')\n        plt.xlabel('Epoch')\n        plt.ylabel('RMSE')\n        plt.legend(loc='best')\n\n        score(nn.predict(preprocess_lstm(X['val'])).flatten(), y['val'].flatten(), X=X['val'], plot_examples=True)\n\n        nn.save('genericLSTM.model')\n\n    else:\n\n        nn = keras.models.load_model('genericLSTM.model')\n        score(nn.predict(preprocess_lstm(X['val'])).flatten(), y['val'].flatten(), X=X['val'], plot_examples=True)\n\n# Score: 0.54","metadata":{"execution":{"iopub.status.busy":"2021-10-30T13:41:50.520537Z","iopub.execute_input":"2021-10-30T13:41:50.520775Z","iopub.status.idle":"2021-10-30T13:41:50.535785Z","shell.execute_reply.started":"2021-10-30T13:41:50.520749Z","shell.execute_reply":"2021-10-30T13:41:50.534769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if display_mode or (not last_only and models_to_train[model_name]):\n\n    predictions = nn.predict(preprocess_lstm(X['val']))\n    truth = y['val']\n    difference = np.sqrt(np.sum((predictions - truth)**2, axis=1))\n    \n    plt.hist(difference, 64)\n    plt.show()\n    plt.close()\n    \n    best_indices = np.argpartition(difference, 16)[:16]\n    print(best_indices)\n\n    worst_indices = np.argpartition(difference, -16)[-16:]\n    print(worst_indices)","metadata":{"execution":{"iopub.status.busy":"2021-10-30T13:41:50.537026Z","iopub.execute_input":"2021-10-30T13:41:50.537412Z","iopub.status.idle":"2021-10-30T13:41:50.551142Z","shell.execute_reply.started":"2021-10-30T13:41:50.537377Z","shell.execute_reply":"2021-10-30T13:41:50.550439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Hmm there are no clear sub-populations visible in our predictions. So splitting the sample to use two different models can be more difficult than expected..","metadata":{}},{"cell_type":"code","source":"if display_mode or (not last_only and models_to_train[model_name]):\n\n    fig = plt.figure(figsize=(12,12))\n    for j in range(4):\n        plt.suptitle(\"Worst | Best \")\n        for i in range(2):\n            idx = worst_indices[2*j+i]\n            plt.subplot(4,4,4*j+i+1)\n            plt.title('Idx = %i' % idx)\n            for feature in ['R', 'C']:\n                plt.plot(X['val']['time_step'][idx], X['val'][feature][idx] * np.ones(X['val']['time_step'][idx].shape), label=feature)\n            for feature in ['u_in', 'u_out']:\n                plt.plot(X['val']['time_step'][idx], X['val'][feature][idx], label=feature)\n            plt.plot(X['val']['time_step'][idx], truth[idx], label='True')\n            plt.plot(X['val']['time_step'][idx], predictions[idx], label='Pred')\n            plt.legend()\n        for i in range(2):\n            idx = best_indices[2*j+i]\n            plt.subplot(4,4,4*j+i+3)\n            plt.title('Idx = %i' % idx)\n            for feature in ['R', 'C']:\n                plt.plot(X['val']['time_step'][idx], X['val'][feature][idx] * np.ones(X['val']['time_step'][idx].shape), label=feature)\n            for feature in ['u_in', 'u_out']:\n                plt.plot(X['val']['time_step'][idx], X['val'][feature][idx], label=feature)\n            plt.plot(X['val']['time_step'][idx], truth[idx], label='True')\n            plt.plot(X['val']['time_step'][idx], predictions[idx], label='Pred')\n            plt.legend()\n    plt.show()\n    plt.close()","metadata":{"execution":{"iopub.status.busy":"2021-10-30T13:41:50.552138Z","iopub.execute_input":"2021-10-30T13:41:50.552803Z","iopub.status.idle":"2021-10-30T13:41:50.568508Z","shell.execute_reply.started":"2021-10-30T13:41:50.552771Z","shell.execute_reply":"2021-10-30T13:41:50.567568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"By eye, it's not immediately clear to me what is the underlying difference between the cases that were predicted correctly, and the incorrect predictions. Let us try to automate this process. Lets take the best 25% and the worst 25% of predictions, and try to make a classifier predicting whether a given case corresponds to the latter class (given only its feature values).","metadata":{}},{"cell_type":"code","source":"if display_mode or (not last_only and models_to_train[model_name]):\n\n    no_samples = int(0.25*len(y['val']))\n\n    best_indices = np.argpartition(difference, no_samples)[:no_samples]\n\n    worst_indices = np.argpartition(difference, -no_samples)[-no_samples:]\n\n    all_indices = np.concatenate([best_indices,worst_indices])","metadata":{"execution":{"iopub.status.busy":"2021-10-30T13:41:50.569852Z","iopub.execute_input":"2021-10-30T13:41:50.570327Z","iopub.status.idle":"2021-10-30T13:41:50.580646Z","shell.execute_reply.started":"2021-10-30T13:41:50.570288Z","shell.execute_reply":"2021-10-30T13:41:50.579739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if display_mode or (not last_only and models_to_train[model_name]):\n\n    def preprocess_lstm (X):\n        return np.dstack([\n            np.tile(X['R'], (80,1)).transpose(), \n            np.tile(X['C'], (80,1)).transpose(), \n            X['u_in'], X['u_out']])\n    def preprocess_class (X, selection=[]):\n        X_class = {}\n        if len(selection) > 0:\n            for key in X.keys():\n                X_class[key] = X[key][all_indices]\n        else:\n            for key in X.keys():\n                X_class[key] = 1 * X[key]\n        X_class = preprocess_lstm(X_class)\n        X_class = X_class.reshape(X_class.shape[0],-1)\n        return X_class\n    X_class = preprocess_class(X['val'], selection=all_indices)\n\n    y_class = np.concatenate([np.zeros(no_samples), np.ones(no_samples)])\n\n    print(X_class.shape, y_class.shape)","metadata":{"execution":{"iopub.status.busy":"2021-10-30T13:41:50.581992Z","iopub.execute_input":"2021-10-30T13:41:50.582468Z","iopub.status.idle":"2021-10-30T13:41:50.595936Z","shell.execute_reply.started":"2021-10-30T13:41:50.58243Z","shell.execute_reply":"2021-10-30T13:41:50.595262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if display_mode or (not last_only and models_to_train[model_name]):\n\n    from sklearn.linear_model import SGDClassifier\n    clf = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=5)\n\n    clf.fit(X_class, y_class)\n\n    from sklearn.metrics import classification_report\n    print(classification_report(y_class, clf.predict(X_class)))","metadata":{"execution":{"iopub.status.busy":"2021-10-30T13:41:50.597409Z","iopub.execute_input":"2021-10-30T13:41:50.598515Z","iopub.status.idle":"2021-10-30T13:41:50.608175Z","shell.execute_reply.started":"2021-10-30T13:41:50.598468Z","shell.execute_reply":"2021-10-30T13:41:50.607253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if display_mode or (not last_only and models_to_train[model_name]):\n    from sklearn.pipeline import make_pipeline\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.svm import SVC\n    clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n    clf.fit(X_class, y_class)\n    print(classification_report(y_class, clf.predict(X_class)))","metadata":{"execution":{"iopub.status.busy":"2021-10-30T13:41:50.609724Z","iopub.execute_input":"2021-10-30T13:41:50.610204Z","iopub.status.idle":"2021-10-30T13:41:50.620745Z","shell.execute_reply.started":"2021-10-30T13:41:50.610164Z","shell.execute_reply":"2021-10-30T13:41:50.619894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This may be good enough for now, we can always make something more sophisticated later.\n\nLet's use this classifier to build two different versions of our LSTM network -- one for the \"easy\" cases (with class 0) and one for the \"hard\" ones (with class 1).","metadata":{}},{"cell_type":"code","source":"if display_mode or (not last_only and models_to_train[model_name]):\n    train_classes = clf.predict(preprocess_class(X['train']))\n    val_classes = clf.predict(preprocess_class(X['val']))","metadata":{"execution":{"iopub.status.busy":"2021-10-30T13:41:50.621861Z","iopub.execute_input":"2021-10-30T13:41:50.622528Z","iopub.status.idle":"2021-10-30T13:41:50.632026Z","shell.execute_reply.started":"2021-10-30T13:41:50.622494Z","shell.execute_reply":"2021-10-30T13:41:50.631147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if display_mode or (not last_only and models_to_train[model_name]):\n\n    def preprocess_lstm (X):\n        return np.dstack([\n            np.tile(X['R'], (80,1)).transpose(), \n            np.tile(X['C'], (80,1)).transpose(), \n            X['u_in'], X['u_out']])\n\n    X_all = preprocess_lstm(X['train'])\n\n    X_train_easy = X_all[train_classes == 0]\n    y_train_easy = y['train'][train_classes == 0]\n\n    X_train_hard = X_all[train_classes == 1]\n    y_train_hard = y['train'][train_classes == 1]\n\n    print(X_train_easy.shape, y_train_easy.shape)\n    print(X_train_hard.shape, y_train_hard.shape)\n\n    X_all = preprocess_lstm(X['val'])\n\n    X_val_easy = X_all[val_classes == 0]\n    y_val_easy = y['val'][val_classes == 0]\n\n    X_val_hard = X_all[val_classes == 1]\n    y_val_hard = y['val'][val_classes == 1]\n\n    print(X_val_easy.shape, y_val_easy.shape)\n    print(X_val_hard.shape, y_val_hard.shape)","metadata":{"execution":{"iopub.status.busy":"2021-10-30T13:41:50.633259Z","iopub.execute_input":"2021-10-30T13:41:50.633656Z","iopub.status.idle":"2021-10-30T13:41:50.643673Z","shell.execute_reply.started":"2021-10-30T13:41:50.633628Z","shell.execute_reply":"2021-10-30T13:41:50.643008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Split LSTM hybrid (future data mixed-in):\")\n\ndef genericLSTM_build_fit (Xtrain, ytrain, Xval, yval, base=None):\n\n    if base == None: # train from scratch\n    \n        nn = models.Sequential()\n        nn.add(layers.Input(shape=(80,4)))\n        nn.add(layers.Dense(512, activation='relu'))\n        nn.add(layers.LSTM(512))\n        nn.add(layers.Dense(512, activation='relu'))\n        nn.add(layers.Dense(512, activation='relu'))\n        nn.add(layers.Dense(80))\n        \n    elif isinstance(base, str):\n        \n        nn = keras.models.load_model(base)\n        \n    else:\n        \n        nn = base\n\n    nn.summary()\n\n    nn.compile(optimizer='adam',\n              loss=tf.keras.losses.MeanSquaredError(),\n              metrics=['RootMeanSquaredError'])\n\n    stop_early = tf.keras.callbacks.EarlyStopping(\n        monitor='val_loss',\n        patience=16,\n        restore_best_weights=True\n    )\n    callbacks = [stop_early,] \n\n    if train_all:\n\n        history = nn.fit(Xtrain, ytrain,\n                        epochs=1024,\n                        validation_data=(Xval, yval),\n                        callbacks=callbacks)\n\n        plt.plot(history.history['root_mean_squared_error'], label='root_mean_squared_error')\n        plt.plot(history.history['val_root_mean_squared_error'], label = 'val_root_mean_squared_error')\n        plt.xlabel('Epoch')\n        plt.ylabel('RMSE')\n        plt.legend(loc='best')\n\n        score(nn.predict(Xval).flatten(), yval.flatten(), X=Xval, plot_examples=True)\n        \n    return nn","metadata":{"execution":{"iopub.status.busy":"2021-10-30T13:41:50.644948Z","iopub.execute_input":"2021-10-30T13:41:50.645387Z","iopub.status.idle":"2021-10-30T13:41:50.661361Z","shell.execute_reply.started":"2021-10-30T13:41:50.645357Z","shell.execute_reply":"2021-10-30T13:41:50.660477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the easy branch\nif display_mode or (not last_only and models_to_train[model_name]):\n    nn_easy = genericLSTM_build_fit(X_train_easy, y_train_easy, X_val_easy, y_val_easy, base=keras.models.clone_model(nn))\n    nn_easy.save('SplitLSTM_easy.model')","metadata":{"execution":{"iopub.status.busy":"2021-10-30T13:41:50.66297Z","iopub.execute_input":"2021-10-30T13:41:50.663503Z","iopub.status.idle":"2021-10-30T13:41:50.67682Z","shell.execute_reply.started":"2021-10-30T13:41:50.663464Z","shell.execute_reply":"2021-10-30T13:41:50.675919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the hard branch\nif display_mode or (not last_only and models_to_train[model_name]):\n    nn_hard = genericLSTM_build_fit(X_train_hard, y_train_hard, X_val_hard, y_val_hard, base=keras.models.clone_model(nn))\n    nn_hard.save('SplitLSTM_hard.model')","metadata":{"execution":{"iopub.status.busy":"2021-10-30T13:41:50.680085Z","iopub.execute_input":"2021-10-30T13:41:50.680324Z","iopub.status.idle":"2021-10-30T13:41:50.688911Z","shell.execute_reply.started":"2021-10-30T13:41:50.680297Z","shell.execute_reply":"2021-10-30T13:41:50.688129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Join as a single model and score\nclass SplitLSTM:\n    \n    def __init__ (self):\n        pass\n    \n    def predict (self,X):\n        Xnn = preprocess_lstm(X)\n        result = np.zeros((Xnn.shape[0],80))\n        classes = clf.predict(preprocess_class(X))\n        result[classes == 0] = nn_easy.predict(Xnn[classes == 0])\n        result[classes == 1] = nn_hard.predict(Xnn[classes == 1])\n        return result","metadata":{"execution":{"iopub.status.busy":"2021-10-30T13:41:50.690121Z","iopub.execute_input":"2021-10-30T13:41:50.690508Z","iopub.status.idle":"2021-10-30T13:41:50.702339Z","shell.execute_reply.started":"2021-10-30T13:41:50.690478Z","shell.execute_reply":"2021-10-30T13:41:50.701437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if display_mode or (not last_only and models_to_train[model_name]):\n    splitLSTM = SplitLSTM()\n    score(splitLSTM.predict(X['val']).flatten(), y['val'].flatten(), X=X['val'], plot_examples=True)","metadata":{"execution":{"iopub.status.busy":"2021-10-30T13:41:50.703689Z","iopub.execute_input":"2021-10-30T13:41:50.703933Z","iopub.status.idle":"2021-10-30T13:41:50.716559Z","shell.execute_reply.started":"2021-10-30T13:41:50.703889Z","shell.execute_reply":"2021-10-30T13:41:50.715892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# submit\nif False:\n    data_submit = pd.read_csv('/kaggle/input/ventilator-pressure-prediction/test.csv')\n\n    X_submit, _, _ = preprocess(data_submit, submission=True)\n    data_submit['pressure'] = splitLSTM.predict(X_submit['subm']).flatten()\n\n    print(data_submit.head())\n    data_submit[['id','pressure']].to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-10-30T13:41:50.718102Z","iopub.execute_input":"2021-10-30T13:41:50.718677Z","iopub.status.idle":"2021-10-30T13:41:50.728249Z","shell.execute_reply.started":"2021-10-30T13:41:50.718637Z","shell.execute_reply":"2021-10-30T13:41:50.727274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Well, this is diappointing. Let us get back to our best model -- the simple generic LSTM network. During training, the network was clearly improving very slowly, with a tight correlation between training and validation data. Thus, it would benefit from more parameters. Let us try to tune it to make it wider / deeper.\n\nFirst, however, let's decide on the best type of LSTM network for our work.","metadata":{}},{"cell_type":"code","source":"from datetime import datetime\nimport time","metadata":{"execution":{"iopub.status.busy":"2021-10-30T13:42:12.414464Z","iopub.execute_input":"2021-10-30T13:42:12.41512Z","iopub.status.idle":"2021-10-30T13:42:12.418338Z","shell.execute_reply.started":"2021-10-30T13:42:12.415089Z","shell.execute_reply":"2021-10-30T13:42:12.41742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Current Time =\", datetime.now().strftime(\"%H:%M:%S\"))","metadata":{"execution":{"iopub.status.busy":"2021-10-30T13:42:25.065334Z","iopub.execute_input":"2021-10-30T13:42:25.065612Z","iopub.status.idle":"2021-10-30T13:42:25.070736Z","shell.execute_reply.started":"2021-10-30T13:42:25.065583Z","shell.execute_reply":"2021-10-30T13:42:25.069901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = 'LSTM_hybrid'\nif model_name not in models_to_train.keys():\n    models_to_train[model_name] = True\n\nif not last_only and models_to_train[model_name]:\n\n    print(\"LSTM hybrid (future data mixed-in):\")\n    print(\"Current Time =\", datetime.now().strftime(\"%H:%M:%S\"))\n    start = time.time()\n\n    def preprocess_lstm (X):\n        return np.dstack([\n            np.tile(X['R'], (80,1)).transpose(), \n            np.tile(X['C'], (80,1)).transpose(), \n            X['u_in'], X['u_out']])\n\n    nn = models.Sequential()\n    nn.add(layers.Input(shape=(80,4)))\n    nn.add(layers.Dense(512, activation='relu'))\n    nn.add(layers.LSTM(512))\n    nn.add(layers.Dense(512, activation='relu'))\n    nn.add(layers.Dense(512, activation='relu'))\n    nn.add(layers.Dense(512, activation='relu'))\n    nn.add(layers.Dense(80))\n\n    nn.compile(optimizer='adam',\n              loss=tf.keras.losses.MeanSquaredError(),\n              metrics=['RootMeanSquaredError'])\n\n    nn.summary()\n\n    stop_early = tf.keras.callbacks.EarlyStopping(\n        monitor='val_loss',\n        patience=16,\n        restore_best_weights=True\n    )\n    callbacks = [stop_early,] \n\n    if True:# train_all:\n\n        history = nn.fit(preprocess_lstm(X['train']), y['train'],\n                        epochs=1024,\n                        validation_data=(preprocess_lstm(X['val']),y['val']),\n                        callbacks=callbacks)\n\n        plt.plot(history.history['root_mean_squared_error'], label='root_mean_squared_error')\n        plt.plot(history.history['val_root_mean_squared_error'], label = 'val_root_mean_squared_error')\n        plt.xlabel('Epoch')\n        plt.ylabel('RMSE')\n        plt.legend(loc='best')\n\n        score(nn.predict(preprocess_lstm(X['val'])).flatten(), y['val'].flatten(), X=X['val'], plot_examples=True)\n\n        nn.save('genericLSTM.model')\n\n    else:\n\n        nn = keras.models.load_model('genericLSTM.model')\n        score(nn.predict(preprocess_lstm(X['val'])).flatten(), y['val'].flatten(), X=X['val'], plot_examples=True)\n\n    print(\"Current Time =\", datetime.now().strftime(\"%H:%M:%S\"))\n    print(\"Elapsed [sec]: \", time.time()-start)\n    \n# Score: 0.57\n# Training time: 4301.8sec","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = 'SimpleRNN_hybrid'\nif model_name not in models_to_train.keys():\n    models_to_train[model_name] = True\n\nif not last_only and models_to_train[model_name]:\n\n    print(\"SimpleRNN hybrid (future data mixed-in):\")\n    print(\"Current Time =\", datetime.now().strftime(\"%H:%M:%S\"))\n    start = time.time()\n\n    def preprocess_lstm (X):\n        return np.dstack([\n            np.tile(X['R'], (80,1)).transpose(), \n            np.tile(X['C'], (80,1)).transpose(), \n            X['u_in'], X['u_out']])\n\n    nn = models.Sequential()\n    nn.add(layers.Input(shape=(80,4)))\n    nn.add(layers.Dense(512, activation='relu'))\n    nn.add(layers.SimpleRNN(512))\n    nn.add(layers.Dense(512, activation='relu'))\n    nn.add(layers.Dense(512, activation='relu'))\n    nn.add(layers.Dense(512, activation='relu'))\n    nn.add(layers.Dense(80))\n\n    nn.compile(optimizer='adam',\n              loss=tf.keras.losses.MeanSquaredError(),\n              metrics=['RootMeanSquaredError'])\n\n    nn.summary()\n\n    stop_early = tf.keras.callbacks.EarlyStopping(\n        monitor='val_loss',\n        patience=16,\n        restore_best_weights=True\n    )\n    callbacks = [stop_early,] \n\n    if True:# train_all:\n\n        history = nn.fit(preprocess_lstm(X['train']), y['train'],\n                        epochs=1024,\n                        validation_data=(preprocess_lstm(X['val']),y['val']),\n                        callbacks=callbacks)\n\n        plt.plot(history.history['root_mean_squared_error'], label='root_mean_squared_error')\n        plt.plot(history.history['val_root_mean_squared_error'], label = 'val_root_mean_squared_error')\n        plt.xlabel('Epoch')\n        plt.ylabel('RMSE')\n        plt.legend(loc='best')\n\n        score(nn.predict(preprocess_lstm(X['val'])).flatten(), y['val'].flatten(), X=X['val'], plot_examples=True)\n\n        nn.save('genericLSTM.model')\n\n    else:\n\n        nn = keras.models.load_model('genericLSTM.model')\n        score(nn.predict(preprocess_lstm(X['val'])).flatten(), y['val'].flatten(), X=X['val'], plot_examples=True)\n\n    print(\"Current Time =\", datetime.now().strftime(\"%H:%M:%S\"))\n    print(\"Elapsed [sec]: \", time.time()-start)\n    \n# Score: 6.27\n# Training time: 6084.5sec","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = 'GRU_hybrid'\nif model_name not in models_to_train.keys():\n    models_to_train[model_name] = True\n\nif not last_only and models_to_train[model_name]:\n\n    print(\"GRU hybrid (future data mixed-in):\")\n    print(\"Current Time =\", datetime.now().strftime(\"%H:%M:%S\"))\n    start = time.time()\n\n    def preprocess_lstm (X):\n        return np.dstack([\n            np.tile(X['R'], (80,1)).transpose(), \n            np.tile(X['C'], (80,1)).transpose(), \n            X['u_in'], X['u_out']])\n\n    nn = models.Sequential()\n    nn.add(layers.Input(shape=(80,4)))\n    nn.add(layers.Dense(512, activation='relu'))\n    nn.add(layers.GRU(512))\n    nn.add(layers.Dense(512, activation='relu'))\n    nn.add(layers.Dense(512, activation='relu'))\n    nn.add(layers.Dense(512, activation='relu'))\n    nn.add(layers.Dense(80))\n\n    nn.compile(optimizer='adam',\n              loss=tf.keras.losses.MeanSquaredError(),\n              metrics=['RootMeanSquaredError'])\n\n    nn.summary()\n\n    stop_early = tf.keras.callbacks.EarlyStopping(\n        monitor='val_loss',\n        patience=16,\n        restore_best_weights=True\n    )\n    callbacks = [stop_early,] \n\n    if True:# train_all:\n\n        history = nn.fit(preprocess_lstm(X['train']), y['train'],\n                        epochs=1024,\n                        validation_data=(preprocess_lstm(X['val']),y['val']),\n                        callbacks=callbacks)\n\n        plt.plot(history.history['root_mean_squared_error'], label='root_mean_squared_error')\n        plt.plot(history.history['val_root_mean_squared_error'], label = 'val_root_mean_squared_error')\n        plt.xlabel('Epoch')\n        plt.ylabel('RMSE')\n        plt.legend(loc='best')\n\n        score(nn.predict(preprocess_lstm(X['val'])).flatten(), y['val'].flatten(), X=X['val'], plot_examples=True)\n\n        nn.save('genericLSTM.model')\n\n    else:\n\n        nn = keras.models.load_model('genericLSTM.model')\n        score(nn.predict(preprocess_lstm(X['val'])).flatten(), y['val'].flatten(), X=X['val'], plot_examples=True)\n\n    print(\"Current Time =\", datetime.now().strftime(\"%H:%M:%S\"))\n    print(\"Elapsed [sec]: \", time.time()-start)\n    \n# Score: 0.80\n# Training time: 3805.7sec","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Well, the LSTM model seems to work best in this configuration, with significantly better scores that the other two versions.\n\nSince the competition deadline has now passed, I will stop here. This was fun ;)","metadata":{}},{"cell_type":"code","source":"last_only = False # anything after this marker is considered the \"last\" model and will be trained","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(models_to_train) # list all models created in this notebook","metadata":{},"execution_count":null,"outputs":[]}]}