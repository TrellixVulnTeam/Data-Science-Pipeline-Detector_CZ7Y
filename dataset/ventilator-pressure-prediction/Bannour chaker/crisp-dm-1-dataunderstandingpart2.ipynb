{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n<a id=\"top\"></a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\"><center>CRISP-DM Methodology</center></h3>\n\n* [Buissness Understanding](#1)\n* [Data Understanding](#2)\n* [Data Preparation](#3)\n* [Data Modeling](#4)   \n* [Data Evaluation](#5)\n\nIn this section we overview our selected method for engineering our solution. CRISP-DM stands for Cross-Industry Standard Process for Data Mining. It is an open standard guide that describes common approaches that are used by data mining experts. CRISP-DM includes descriptions of the typical phases of a project, including tasks details and provides an overview of the data mining lifecycle. The lifecycle model consists of six phases with arrows indicating the most important and frequent dependencies between phases. The sequence of the phases is not strict. In fact, most projects move back and forth between phases as necessary. It starts with business understanding, and then moves to data understanding, data preparation, modelling, evaluation, and deployment. The CRISP-DM model is flexible and can be customized easily.\n## Buissness Understanding\n\n    Tasks:\n\n    1.Determine business objectives\n\n    2.Assess situation\n\n    3.Determine data mining goals\n\n    4.Produce project plan\n\n## Data Understanding\n     Tasks:\n\n    1.Collect data\n\n    2.Describe data\n\n    3.Explore data    \n\n## Data Preparation\n    Tasks\n    1.Data selection\n\n    2.Data preprocessing\n\n    3.Feature engineering\n\n    4.Dimensionality reduction\n\n            Steps:\n\n            Data cleaning\n\n            Data integration\n\n            Data sampling\n\n            Data dimensionality reduction\n\n            Data formatting\n\n            Data transformation\n\n            Scaling\n\n            Aggregation\n\n            Decomposition\n\n## Data Modeling :\n\nModeling is the part of the Cross-Industry Standard Process for Data Mining (CRISP-DM) process model that i like best. Our data is already in good shape, and now we can search for useful patterns in our data.\n\n    Tasks\n    1. Select modeling technique Select technique\n\n    2. Generate test design\n\n    3. Build model\n\n    4. Assess model\n\n## Data Evaluation :\n    Tasks\n\n    1.Evaluate Result\n\n    2.Review Process\n\n    3.Determine next steps\n\n<a id=\"top\"></a>\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\">\n<center>Buissness Understanding</center></h3>\n\n    \nWhat do doctors do when a patient has trouble breathing? They use a ventilator to pump oxygen into a sedated patient's lungs via a tube in the windpipe. But mechanical ventilation is a clinician-intensive procedure, a limitation that was prominently on display during the early days of the COVID-19 pandemic. At the same time, developing new methods for controlling mechanical ventilators is prohibitively expensive, even before reaching clinical trials. High-quality simulators could reduce this barrier.\n\nCurrent simulators are trained as an ensemble, where each model simulates a single lung setting. However, lungs and their attributes form a continuous space, so a parametric approach must be explored that would consider the differences in patient lungs.\n\nPartnering with Princeton University, the team at Google Brain aims to grow the community around machine learning for mechanical ventilation control. They believe that neural networks and deep learning can better generalize across lungs with varying characteristics than the current industry standard of PID controllers.\n\nIn this competition, you’ll simulate a ventilator connected to a sedated patient's lung. The best submissions will take lung attributes compliance and resistance into account.\n\nIf successful, you'll help overcome the cost barrier of developing new methods for controlling mechanical ventilators. This will pave the way for algorithms that adapt to patients and reduce the burden on clinicians during these novel times and beyond. As a result, ventilator treatments may become more widely available to help patients breathe.\n\n**Eval Metric**: The competition will be scored as the mean absolute error between the predicted and actual pressures during the inspiratory phase of each breath. The expiratory phase is not scored. The score is given by:\n\n|X−Y|\n\nwhere X is the vector of predicted pressure and Y is the vector of actual pressures across all breaths in the test set.\n    \n  **Reminder**\n\n* id - globally-unique time step identifier across an entire file\n\n* breath_id - globally-unique time step for breaths\n\n* R - lung attribute indicating how restricted the airway is (in cmH2O/L/S). Physically, this is the change in pressure per change in flow (air volume per time). Intuitively, one can imagine blowing up a balloon through a straw. We can change R by changing the diameter of the straw, with higher R being harder to blow.\n\n* C - lung attribute indicating how compliant the lung is (in mL/cmH2O). Physically, this is the change in volume per change in pressure. Intuitively, one can imagine the same balloon example. We can change C by changing the thickness of the balloon’s latex, with higher C having thinner latex and easier to blow.\n\n* time_step - the actual time stamp.\n\n* u_in - the control input for the inspiratory solenoid valve. Ranges from 0 to 100.\n\n* u_out - the control input for the exploratory solenoid valve. Either 0 or 1.\n\n* pressure - the airway pressure measured in the respiratory circuit, measured in cmH2O.\n    \n<a id=\"top\"></a>\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\">\n<center>Data Understanding</center></h3>\n    \n    \n    \nThis is the part 1:\n    \nhttps://www.kaggle.com/bannourchaker/crispdm-1-dataunderstanding-part1\n    \n    \n## Step 1: Import helpful libraries","metadata":{}},{"cell_type":"code","source":"#Load the librarys\nimport pandas as pd #To work with dataset\nimport numpy as np #Math library\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns #Graph library that use matplot in background\nimport matplotlib.pyplot as plt #to plot some parameters in seaborn\nimport warnings\nfrom sklearn.preprocessing import LabelEncoder, OrdinalEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import PowerTransformer, StandardScaler,Normalizer,RobustScaler,MaxAbsScaler,MinMaxScaler,QuantileTransformer\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.neighbors import KNeighborsClassifier\n# Import StandardScaler from scikit-learn\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.impute import KNNImputer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.pipeline import make_pipeline, Pipeline\n\nfrom sklearn.manifold import TSNE\n# Import train_test_split()\n# Metrics\nfrom sklearn.metrics import roc_auc_score, average_precision_score\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import roc_curve\nfrom datetime import datetime, date\nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.linear_model import LinearRegression, RidgeCV\n\nimport lightgbm as lgbm\nfrom catboost import CatBoostRegressor\nimport tensorflow as tf \nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import LearningRateScheduler\n#import smogn\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.ensemble import GradientBoostingRegressor,RandomForestRegressor\n# For training random forest model\nimport lightgbm as lgb\nfrom scipy import sparse\nfrom sklearn.neighbors import KNeighborsRegressor \nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans \n# Model selection\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_regression,f_classif\nfrom sklearn.feature_selection import mutual_info_regression\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\n\nfrom itertools import combinations\n#import smong \n# Plotlty : \nimport pprint\nfrom plotly.offline import iplot, init_notebook_mode\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly import tools\nimport plotly.io as pio\npp = pprint.PrettyPrinter(indent=4)\npio.templates.default = \"plotly_white\"\n\n\nimport category_encoders as ce\nimport warnings\nimport optuna \nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:20:21.011787Z","iopub.execute_input":"2021-10-18T10:20:21.012441Z","iopub.status.idle":"2021-10-18T10:20:32.538784Z","shell.execute_reply.started":"2021-10-18T10:20:21.012329Z","shell.execute_reply":"2021-10-18T10:20:32.537838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Time series EDA ","metadata":{}},{"cell_type":"markdown","source":"\n## Step 2: Load the data\n\nNext, we'll load the training and test data.\n\nWe set index_col=0 in the code cell below to use the id column to index the DataFrame. (If you're not sure how this works, try temporarily removing index_col=0 and see how it changes the result.)\n","metadata":{}},{"cell_type":"code","source":"%%time\ntrain = pd.read_csv('../input/ventilator-pressure-prediction/train.csv')\npressure_values = np.sort( train.pressure.unique() )\ntest = pd.read_csv('../input/ventilator-pressure-prediction/test.csv')\nsubmission = pd.read_csv('../input/ventilator-pressure-prediction/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:20:32.540402Z","iopub.execute_input":"2021-10-18T10:20:32.540661Z","iopub.status.idle":"2021-10-18T10:20:49.241116Z","shell.execute_reply.started":"2021-10-18T10:20:32.540631Z","shell.execute_reply":"2021-10-18T10:20:49.236749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visual Exploratory \nThe first, and perhaps most popular, visualization for time series is the line plot.","metadata":{}},{"cell_type":"code","source":"import itertools\ndef plot_sample(dataframe, seed = 42):\n    \"\"\" Plot time series for each combinations of R and C \"\"\"\n    \n    np.random.seed(seed)\n    \n    cols = ['u_in', 'u_out', 'pressure']\n\n    for (r, c) in list(itertools.product(dataframe.R.unique(), dataframe.C.unique())):\n        \n        subfig = make_subplots(specs=[[{\"secondary_y\": True}]])\n        \n        plot_data = dataframe[(dataframe.R.isin([r]) & dataframe.C.isin([c]))]\n        sample_id = plot_data.breath_id.sample(n=1)\n        plot_data = plot_data[plot_data.breath_id.isin(sample_id)]\n\n        x_breath_changing_state = plot_data.loc[max(plot_data.loc[plot_data.u_out < 1].index), 'time_step']\n\n        fig1 = px.line()\n        fig1.add_scatter(x=plot_data.time_step, y=plot_data.pressure, name='pressure')\n        fig1.add_scatter(x=plot_data.time_step, y=plot_data.u_in, name='u_in')\n        fig1.add_vline(x_breath_changing_state)\n        \n        fig2 = px.line()\n        fig2.add_scatter(x=plot_data.time_step, y=plot_data.u_out, name='u_out')\n        fig2.update_traces(yaxis=\"y2\")\n\n        subfig.add_traces(fig1.data + fig2.data)\n        subfig.for_each_trace(lambda t: t.update(line=dict(color=t.marker.color)))\n                \n        subfig.layout.title = f'Sample {sample_id.values[0]} - R={r}, C={c}'\n        subfig.layout.yaxis1.title=\"u_in/pressure Y\"\n        subfig.layout.yaxis2.title=\"u_out Y\"\n        \n        subfig.show()\n        #title=f'Sample {sample_id.values[0]} - R={r}, C={c}'\n        \nplot_sample(train)","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:20:49.244037Z","iopub.execute_input":"2021-10-18T10:20:49.24429Z","iopub.status.idle":"2021-10-18T10:20:51.355159Z","shell.execute_reply.started":"2021-10-18T10:20:49.244263Z","shell.execute_reply":"2021-10-18T10:20:51.354339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Target Plot  as complet time series ","metadata":{}},{"cell_type":"code","source":"df = train.iloc[0:8000].copy()","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:20:51.357018Z","iopub.execute_input":"2021-10-18T10:20:51.35728Z","iopub.status.idle":"2021-10-18T10:20:51.362699Z","shell.execute_reply.started":"2021-10-18T10:20:51.35725Z","shell.execute_reply":"2021-10-18T10:20:51.36147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.line(df, x=\"id\", y=df.columns,\n              title='Data labels')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:20:51.363924Z","iopub.execute_input":"2021-10-18T10:20:51.364221Z","iopub.status.idle":"2021-10-18T10:20:51.854232Z","shell.execute_reply.started":"2021-10-18T10:20:51.364186Z","shell.execute_reply":"2021-10-18T10:20:51.853129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfig = px.line(df, x='id', y=\"pressure\")\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:20:51.855393Z","iopub.execute_input":"2021-10-18T10:20:51.855637Z","iopub.status.idle":"2021-10-18T10:20:51.985564Z","shell.execute_reply.started":"2021-10-18T10:20:51.855611Z","shell.execute_reply":"2021-10-18T10:20:51.984741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The line plot is quite dense.\n\nSometimes it can help to change the style of the line plot; for example, to use a dashed line or dots.\n\nBelow is an example of changing the style of the line to be black dots instead of a connected line (the style=’k.’ argument).","metadata":{}},{"cell_type":"code","source":"fig = px.scatter(df, x='id', y=\"pressure\")\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:20:51.987172Z","iopub.execute_input":"2021-10-18T10:20:51.98784Z","iopub.status.idle":"2021-10-18T10:20:52.084521Z","shell.execute_reply.started":"2021-10-18T10:20:51.987803Z","shell.execute_reply":"2021-10-18T10:20:52.083893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Grouped TimeSeries :\n","metadata":{}},{"cell_type":"code","source":"groups = df.groupby(df['breath_id']).agg(['sum', 'mean', 'max','count'])\ngroups","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:20:52.085727Z","iopub.execute_input":"2021-10-18T10:20:52.086115Z","iopub.status.idle":"2021-10-18T10:20:52.170464Z","shell.execute_reply.started":"2021-10-18T10:20:52.086073Z","shell.execute_reply":"2021-10-18T10:20:52.169048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndf.groupby(df['breath_id']).pressure.plot(figsize=(10, 6))","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:20:52.172174Z","iopub.execute_input":"2021-10-18T10:20:52.172479Z","iopub.status.idle":"2021-10-18T10:20:53.717651Z","shell.execute_reply.started":"2021-10-18T10:20:52.172448Z","shell.execute_reply":"2021-10-18T10:20:53.716584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##  Time Series Histogram and Density Plots\nAnother important visualization is of the distribution of observations themselves.\n\nThis means a plot of the values without the temporal ordering.\n\nSome linear time series forecasting methods assume a well-behaved distribution of observations (i.e. a bell curve or normal distribution). This can be explicitly checked using tools like statistical hypothesis tests. But plots can provide a useful first check of the distribution of observations both on raw observations and after any type of data transform has been performed.\n\nThe example below creates a histogram plot of the observations  dataset. A histogram groups values into bins, and the frequency or count of observations in each bin can provide insight into the underlying distribution of the observations.","metadata":{}},{"cell_type":"code","source":"fig = px.histogram(df, x=\"pressure\")\nfig.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:20:53.721778Z","iopub.execute_input":"2021-10-18T10:20:53.722152Z","iopub.status.idle":"2021-10-18T10:20:53.854271Z","shell.execute_reply.started":"2021-10-18T10:20:53.722109Z","shell.execute_reply":"2021-10-18T10:20:53.853352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nWe can get a better idea of the shape of the distribution of observations by using a density plot.\n\nThis is like the histogram, except a function is used to fit the distribution of observations and a nice, smooth line is used to summarize this distribution.\n\nBelow is an example of a density plot ","metadata":{}},{"cell_type":"code","source":"import plotly.figure_factory as ff\nhist_data = [df.pressure]\ngroup_labels = ['distplot of pressure'] # name of the dataset\nfig = ff.create_distplot(hist_data, group_labels)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:20:53.855903Z","iopub.execute_input":"2021-10-18T10:20:53.856197Z","iopub.status.idle":"2021-10-18T10:20:55.701891Z","shell.execute_reply.started":"2021-10-18T10:20:53.856157Z","shell.execute_reply":"2021-10-18T10:20:55.700595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Histograms and density plots provide insight into the distribution of all observations, but we may be interested in the distribution of values by time interval.\n\nAnother type of plot that is useful to summarize the distribution of observations is the **box and whisker** plot. This plot draws a box around the 25th and 75th percentiles of the data that captures the middle 50% of observations. A line is drawn at the 50th percentile (the median) and whiskers are drawn above and below the box to summarize the general extents of the observations. Dots are drawn for outliers outside the whiskers or extents of the data.\n\nBox and whisker plots can be created and compared for each interval in a time series, such as years, months, or days.","metadata":{}},{"cell_type":"code","source":"sns.set_style(\"whitegrid\") \nsns.boxplot(x = 'breath_id', y = 'pressure', data = df.iloc[0:800])","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:20:55.705119Z","iopub.execute_input":"2021-10-18T10:20:55.70551Z","iopub.status.idle":"2021-10-18T10:20:56.189545Z","shell.execute_reply.started":"2021-10-18T10:20:55.705462Z","shell.execute_reply":"2021-10-18T10:20:56.188055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.boxplot(x = 'breath_id', y = 'u_in', data = df.iloc[0:800])","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:20:56.192415Z","iopub.execute_input":"2021-10-18T10:20:56.192761Z","iopub.status.idle":"2021-10-18T10:20:56.657052Z","shell.execute_reply.started":"2021-10-18T10:20:56.192702Z","shell.execute_reply":"2021-10-18T10:20:56.656435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.boxplot(x = 'breath_id', y = 'time_step', data = df.iloc[0:800])","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:20:56.658438Z","iopub.execute_input":"2021-10-18T10:20:56.658844Z","iopub.status.idle":"2021-10-18T10:20:57.096041Z","shell.execute_reply.started":"2021-10-18T10:20:56.658813Z","shell.execute_reply":"2021-10-18T10:20:57.094918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,5))\nseries2= df.iloc[0:800].pivot(\"time_step\", \"breath_id\", \"pressure\")\n\nax = sns.heatmap(series2, annot=True,linewidths=.5)","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:20:57.097308Z","iopub.execute_input":"2021-10-18T10:20:57.097531Z","iopub.status.idle":"2021-10-18T10:21:01.056354Z","shell.execute_reply.started":"2021-10-18T10:20:57.097505Z","shell.execute_reply":"2021-10-18T10:21:01.055365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Time Series Lag Scatter Plots\nTime series modeling assumes a relationship between an observation and the previous observation.\n\nPrevious observations in a time series are called lags, with the observation at the previous time step called lag1, the observation at two time steps ago lag2, and so on.\n\nA useful type of plot to explore the relationship between each observation and a lag of that observation is called the scatter plot.\n\nPandas has a built-in function for exactly this called the lag plot. It plots the observation at time t on the x-axis and the lag1 observation (t-1) on the y-axis.\n\nIf the points cluster along a diagonal line from the bottom-left to the top-right of the plot, it suggests a positive correlation relationship.\nIf the points cluster along a diagonal line from the top-left to the bottom-right, it suggests a negative correlation relationship.\nEither relationship is good as they can be modeled.\n\nMore points tighter in to the diagonal line suggests a stronger relationship and more spread from the line suggests a weaker relationship.\n\nA ball in the middle or a spread across the plot suggests a weak or no relationship.\n\nBelow is an example of a lag plot for pressure  dataset.","metadata":{}},{"cell_type":"code","source":"from matplotlib import pyplot\nfrom pandas.plotting import lag_plot\nseries3=df.loc[:79,'pressure']\nlag_plot(series3)\npyplot.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:21:01.057793Z","iopub.execute_input":"2021-10-18T10:21:01.058041Z","iopub.status.idle":"2021-10-18T10:21:01.347438Z","shell.execute_reply.started":"2021-10-18T10:21:01.058011Z","shell.execute_reply":"2021-10-18T10:21:01.346472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The plot created from running the example shows a relatively strong positive correlation between observations and their lag1 values.\n\nWe can repeat this process for an observation and any lag values. Perhaps with the observation at the same time last week, last month, or last year, or any other domain-specific knowledge we may wish to explore.\n\nFor example, we can create a scatter plot for the observation with each value in the previous seven days. Below is an example of this for the Minimum Daily Temperatures dataset.\n\nFirst, a new DataFrame is created with the lag values as new columns. The columns are named appropriately. Then a new subplot is created that plots each observation with a different lag value","metadata":{}},{"cell_type":"code","source":"from pandas import DataFrame\nfrom pandas import concat\nfrom matplotlib import pyplot\nfrom pandas.plotting import scatter_matrix\nplt.figure(figsize=(20,10))\nvalues = DataFrame(series3.values)\nlags = 7\ncolumns = [values]\nfor i in range(1,(lags + 1)):\n\tcolumns.append(values.shift(i))\ndataframe = concat(columns, axis=1)\ncolumns = ['t+1']\nfor i in range(1,(lags + 1)):\n\tcolumns.append('t-' + str(i))\ndataframe.columns = columns\npyplot.figure(1)\nfor i in range(1,(lags + 1)):\n\tax = pyplot.subplot(240 + i)\n\tax.set_title('t+1 vs t-' + str(i))\n\tpyplot.scatter(x=dataframe['t+1'].values, y=dataframe['t-'+str(i)].values)\npyplot.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:21:01.349007Z","iopub.execute_input":"2021-10-18T10:21:01.349315Z","iopub.status.idle":"2021-10-18T10:21:02.796606Z","shell.execute_reply.started":"2021-10-18T10:21:01.349273Z","shell.execute_reply":"2021-10-18T10:21:02.795676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can quantify the strength and type of relationship between observations and their lags.\n\nIn statistics, this is called correlation, and when calculated against lag values in time series, it is called autocorrelation (self-correlation).\n\nA correlation value calculated between two groups of numbers, such as observations and their lag1 values, results in a number between -1 and 1. The sign of this number indicates a negative or positive correlation respectively. A value close to zero suggests a weak correlation, whereas a value closer to -1 or 1 indicates a strong correlation.\n\nCorrelation values, called correlation coefficients, can be calculated for each observation and different lag values. Once calculated, a plot can be created to help better understand how this relationship changes over the lag.\n\nThis type of plot is called an autocorrelation plot and Pandas provides this capability built in, called the autocorrelation_plot() function.\n\nThe example below creates an autocorrelation plot for pressure dataset","metadata":{}},{"cell_type":"markdown","source":"We can quantify the strength and type of relationship between observations and their lags.\n\nIn statistics, this is called correlation, and when calculated against lag values in time series, it is called autocorrelation (self-correlation).\n\nA correlation value calculated between two groups of numbers, such as observations and their lag1 values, results in a number between -1 and 1. The sign of this number indicates a negative or positive correlation respectively. A value close to zero suggests a weak correlation, whereas a value closer to -1 or 1 indicates a strong correlation.\n\nCorrelation values, called correlation coefficients, can be calculated for each observation and different lag values. Once calculated, a plot can be created to help better understand how this relationship changes over the lag.\n\nThis type of plot is called an autocorrelation plot and Pandas provides this capability built in, called the autocorrelation_plot() function.\n\nThe example below creates an autocorrelation plot for the Minimum Daily Temperatures dataset","metadata":{}},{"cell_type":"code","source":"from pandas.plotting import autocorrelation_plot\n\nautocorrelation_plot(series3)\npyplot.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:21:02.79836Z","iopub.execute_input":"2021-10-18T10:21:02.798706Z","iopub.status.idle":"2021-10-18T10:21:03.082366Z","shell.execute_reply.started":"2021-10-18T10:21:02.798664Z","shell.execute_reply":"2021-10-18T10:21:03.081535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nThe statsmodels library also provides a version of the plot in the plot_acf() function as a line plot.","metadata":{}},{"cell_type":"code","source":"from statsmodels.graphics.tsaplots import plot_acf\nplot_acf(series3, lags=31)\npyplot.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:21:03.083623Z","iopub.execute_input":"2021-10-18T10:21:03.083865Z","iopub.status.idle":"2021-10-18T10:21:03.429635Z","shell.execute_reply.started":"2021-10-18T10:21:03.083829Z","shell.execute_reply":"2021-10-18T10:21:03.428582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"breath_ids = train.breath_id.sample(n = 5000//80, replace = False)\ntrain_EDA = train.loc[train.breath_id.isin(breath_ids), :].reset_index(drop = True)\nfig = px.histogram(\n    train_EDA, \n    x=\"pressure\",\n    marginal=\"box\",\n    color=\"u_out\",\n    hover_data=train_EDA.columns,\n    nbins = 50\n)\n\nfig.update_layout(\n    title=\"Pressure distribution\"\n)\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:21:03.430972Z","iopub.execute_input":"2021-10-18T10:21:03.431214Z","iopub.status.idle":"2021-10-18T10:21:04.451373Z","shell.execute_reply.started":"2021-10-18T10:21:03.431184Z","shell.execute_reply":"2021-10-18T10:21:04.450474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.histogram(\n    train_EDA, \n    x=\"u_in\",\n    marginal=\"box\",\n    color=\"u_out\",\n    hover_data=train_EDA.columns,\n    nbins = 50\n)\n\nfig.update_layout(\n    title=\"u_in distribution\"\n)\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:21:04.452743Z","iopub.execute_input":"2021-10-18T10:21:04.453044Z","iopub.status.idle":"2021-10-18T10:21:04.587913Z","shell.execute_reply.started":"2021-10-18T10:21:04.453011Z","shell.execute_reply":"2021-10-18T10:21:04.587115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dict_data = dict(train_EDA.u_out.value_counts())\n\nfig = go.Figure(\n    data=[\n        go.Bar(\n            x = list(dict_data.keys()),\n            y = list(dict_data.values())\n        )\n    ],\n    layout_title_text=\"u_out distribution\",\n)\n\nfig.update_layout(\n    xaxis = dict(\n        tickmode = 'linear',\n        tick0 = 0,\n        dtick = 1\n    )\n)\n\nfig.show()\n\ndel dict_data","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:21:04.589108Z","iopub.execute_input":"2021-10-18T10:21:04.58946Z","iopub.status.idle":"2021-10-18T10:21:04.612785Z","shell.execute_reply.started":"2021-10-18T10:21:04.589395Z","shell.execute_reply":"2021-10-18T10:21:04.612199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def display_ts_examples(dataframe, graph_indexes = np.arange(9)):\n    import random \n    \n    # plot first few images\n    plt.figure(figsize=(12,12))\n    \n    for graph_index in graph_indexes:\n        \n        breath_id = random.choice(dataframe.breath_id.unique())\n        \n        # define subplot\n        plt.subplot(330 + 1 + graph_index)\n        plt.title('Breath id: %s \\n'%breath_id,\n                 fontsize=18)\n        # plot raw pixel data\n        ts_to_plot = dataframe.loc[dataframe.breath_id == breath_id, ['time_step', 'pressure']]\n        pd.Series(ts_to_plot.pressure.values, index=ts_to_plot.time_step.values).plot()\n        \n    plt.subplots_adjust(bottom = 0.001)  # the bottom of the subplots of the figure\n    plt.subplots_adjust(top = 1.25)\n    # show the figure\n    plt.show()\n    \ndisplay_ts_examples(train_EDA)","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:21:04.614013Z","iopub.execute_input":"2021-10-18T10:21:04.614437Z","iopub.status.idle":"2021-10-18T10:21:06.25652Z","shell.execute_reply.started":"2021-10-18T10:21:04.614388Z","shell.execute_reply":"2021-10-18T10:21:06.25586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install tslearn\nfrom tqdm import tqdm\nfrom tslearn.clustering import TimeSeriesKMeans","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:21:06.257847Z","iopub.execute_input":"2021-10-18T10:21:06.258279Z","iopub.status.idle":"2021-10-18T10:21:19.385605Z","shell.execute_reply.started":"2021-10-18T10:21:06.258248Z","shell.execute_reply":"2021-10-18T10:21:19.384558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create clusters with pressure data\n\nhttps://towardsdatascience.com/dynamic-time-warping-3933f25fcdd\n\nhttps://rtavenar.github.io/blog/dtw.html\n\nhttps://medium.com/walmartglobaltech/time-series-similarity-using-dynamic-time-warping-explained-9d09119e48ec","metadata":{}},{"cell_type":"code","source":"def generate_matrix_cluster(dataframe, n = 300, seed = 42):\n    \"\"\" Clustering of time series based on dynamic time warp \"\"\"\n    \n    np.random.seed(seed)\n    matrix = []\n\n    for breath_id in tqdm(breath_ids):\n        df_ = dataframe.loc[dataframe.breath_id == breath_id, ['time_step', 'pressure']]\n        matrix.append(np.array(pd.Series(df_.pressure.values, index=df_.time_step.values)))\n        \n    matrix = np.matrix(matrix)[:,:,np.newaxis]\n    \n    return matrix\n\ndef run_clustering(matrix):\n    \"\"\" Perform KMeans on matrix of time series \"\"\"\n    \n    model = TimeSeriesKMeans(n_clusters=3, metric=\"dtw\", max_iter=10)\n    model.fit(matrix)\n    \n    return model\n\nmatrix = generate_matrix_cluster(train_EDA)\ncluster_p_model = run_clustering(matrix)","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:21:19.388527Z","iopub.execute_input":"2021-10-18T10:21:19.388837Z","iopub.status.idle":"2021-10-18T10:21:22.728129Z","shell.execute_reply.started":"2021-10-18T10:21:19.388797Z","shell.execute_reply":"2021-10-18T10:21:22.727227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from yellowbrick.cluster.elbow import kelbow_visualizer\n\nkelbow_visualizer(TimeSeriesKMeans(metric=\"dtw\", max_iter=10),\n                  df,\n                  k=(2, 10),\n                  timings=False)","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:21:22.729588Z","iopub.execute_input":"2021-10-18T10:21:22.729802Z","iopub.status.idle":"2021-10-18T10:33:49.928121Z","shell.execute_reply.started":"2021-10-18T10:21:22.729776Z","shell.execute_reply":"2021-10-18T10:33:49.92724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def display_ts_clusters(model, n_clusters=3):\n    \n    # plot first few images\n    plt.figure(figsize=(12,12))\n    \n    for graph_index in range(n_clusters):\n                \n        # define subplot\n        plt.subplot(330 + 1 + graph_index)\n        plt.title('Cluster No: %s \\n'%graph_index,\n                 fontsize=18)\n        \n        # plot raw pixel data\n        array_cluster = model.cluster_centers_[graph_index]\n        pd.Series(array_cluster.ravel()).plot()\n        \n    plt.subplots_adjust(bottom = 0.001)\n    plt.subplots_adjust(top = 1.25)\n    plt.show()\n    \ndisplay_ts_clusters(cluster_p_model)","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:33:49.929468Z","iopub.execute_input":"2021-10-18T10:33:49.930118Z","iopub.status.idle":"2021-10-18T10:33:50.530027Z","shell.execute_reply.started":"2021-10-18T10:33:49.930075Z","shell.execute_reply":"2021-10-18T10:33:50.529106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Convert Dtypes ","metadata":{}},{"cell_type":"code","source":"train[train.select_dtypes(['float64']).columns] = train[train.select_dtypes(['float64']).columns].apply(pd.to_numeric)\ntrain[train.select_dtypes(['object','int64']).columns] = train.select_dtypes(['object','int64']).apply(lambda x: x.astype('category'))\ntest[test.select_dtypes(['float64']).columns] = test[test.select_dtypes(['float64']).columns].apply(pd.to_numeric)\ntest[test.select_dtypes(['object','int64']).columns] = test.select_dtypes(['object','int64']).apply(lambda x: x.astype('category'))","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:33:50.533808Z","iopub.execute_input":"2021-10-18T10:33:50.534079Z","iopub.status.idle":"2021-10-18T10:33:55.84222Z","shell.execute_reply.started":"2021-10-18T10:33:50.534049Z","shell.execute_reply":"2021-10-18T10:33:55.841444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Num/Cat Features ","metadata":{}},{"cell_type":"code","source":"cat_columns = train.drop(['id','pressure','breath_id'], axis=1).select_dtypes(exclude=['float64']).columns\nnum_columns = train.drop(['id','pressure','breath_id'], axis=1).select_dtypes(include=['int64','float64','category']).columns","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:42:02.923695Z","iopub.execute_input":"2021-10-18T10:42:02.924033Z","iopub.status.idle":"2021-10-18T10:42:03.117577Z","shell.execute_reply.started":"2021-10-18T10:42:02.924Z","shell.execute_reply":"2021-10-18T10:42:03.116584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"top\"></a>\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\"><center>Data Preparation</center></h3>\n\n## Data preprocessing\n\nData preprocessing comes after you've cleaned up your data and after you've done some exploratory analysis to understand your dataset. Once you understand your dataset, you'll probably have some idea about how you want to model your data. Machine learning models in Python require numerical input, so if your dataset has categorical variables, you'll need to transform them. Think of data preprocessing as a prerequisite for modeling:\n\nOutlier Handling\n\nScaling\n\nFeature Engineering\n\nFeature Selection \n\n\n","metadata":{}},{"cell_type":"markdown","source":"<a id=\"top\"></a>\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\"><center>Data Modeling</center></h3>\n\nModeling is the part of the Cross-Industry Standard Process for Data Mining (CRISP-DM) process model that i like best. Our data is already in good shape, and now we can search for useful patterns in our data.\n\n<a id=\"top\"></a>\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\"><center>Data Evaluation  </center></h3>\n\n\n\n\n**MAE**\n\nRegression predictive modeling is the task of approximating a mapping function (f) from input variables (X) to a continuous output variable (y).\n\nRegression is different from classification, which involves predicting a category or class label.\n\nEvaluating Regression Models\n\nA common question by beginners to regression predictive modeling projects is:\n\n    How do I calculate accuracy for my regression model?\n\nAccuracy (e.g. classification accuracy) is a measure for classification, not regression.\n\nWe cannot calculate accuracy for a regression model.\n\nThe skill or performance of a regression model must be reported as an error in those predictions.\n\nThis makes sense if you think about it. If you are predicting a numeric value like a height or a dollar amount, you don’t want to know if the model predicted the value exactly (this might be intractably difficult in practice); instead, we want to know how close the predictions were to the expected values.\n\nError addresses exactly this and summarizes on average how close predictions were to their expected values.\n\nThere are three error metrics that are commonly used for evaluating and reporting the performance of a regression model; they are:\n\n    Mean Squared Error (MSE).\n    Root Mean Squared Error (RMSE).\n    Mean Absolute Error (MAE)\n\n**Mean Absolute Error**, or MAE, is a popular metric because, like RMSE, the units of the error score match the units of the target value that is being predicted.\n\nUnlike the RMSE, the changes in MAE are linear and therefore intuitive.\n\nThat is, MSE and RMSE punish larger errors more than smaller errors, inflating or magnifying the mean error score. This is due to the square of the error value. The MAE does not give more or less weight to different types of errors and instead the scores increase linearly with increases in error.\n\nAs its name suggests, the MAE score is calculated as the average of the absolute error values. Absolute or abs() is a mathematical function that simply makes a number positive. Therefore, the difference between an expected and predicted value may be positive or negative and is forced to be positive when calculating the MAE.\n\nThe MAE can be calculated as follows:\n\n    MAE = 1 / N * sum for i to N abs(y_i – yhat_i)\n\nWhere y_i is the i’th expected value in the dataset, yhat_i is the i’th predicted value and abs() is the absolute function.\n\nwe have done all EDA needed to chose the best preprocessing steps and begin modeling .\nWork is in progress .. \n\nUpvote if you find it useful .","metadata":{}}]}