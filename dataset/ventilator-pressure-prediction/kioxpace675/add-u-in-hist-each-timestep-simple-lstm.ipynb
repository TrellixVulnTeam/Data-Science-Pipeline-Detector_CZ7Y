{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# はじめに\n\nscoreを求めるよりは、スコアを上げる為の情報をどう得るかを書き溜めます。<br>\n学習済みmodelのsave、train結果とtargetとの差の可視化、どのfeatureが重要かの可視化を目指します。<br>\n後で見返せるように、自分用にまとめています。<br>\n\n# NOTATION\n\n### <font color=\"red\">I try to find hints of feature engineering by comparing model result and train data.<br>You can find visualization result of model output and train data. </font>\n\n#### <font color=\"red\">This notebook use the trained model of following link due to run visualize with model result quckly and easy to change model of yours.</font>\n[copied model to this notebook](https://www.kaggle.com/tfukuda675/update-simple-lstm-simple-data-model)\n\n#### <font color=\"red\">When you use trained model in this notebook, please set \"use_trained_model = True\" in cell of </font>\n[Settings of this notebook](https://www.kaggle.com/tfukuda675/update-simple-lstm-simple-data#Settings) <br>\nnote : above model is copied of this notebook result.\n\n<font color=\"red\">If you want to look over data visualize result, please refer save version 37</font><br>\n<font color=\"red\"><s>I create u_out = 1 data for my understanding until version 37.</s></font><br>\n<font color=\"red\">From version 38, <s>I treat u_out = 0\" data!!</s></font><br>\n<font color=\"red\">From version 38, at first, create model with simple data with above stragety.</font><br>\n<font color=\"red\">Next, compare model result of train data and pressure of train data due to consider feature engineering of data.</font><br>\n\n## New Notebook\n\nThis notebook takes long runtime to compair model result vs train data, because this notebook run all steps, create data, create model, compair and visualize difference of model result and train data.<br>\nSo, I am editing new notebooks as following, due to run all steps quickly and easy to swap of your data or models.<br>\n\n* https://www.kaggle.com/tfukuda675/simple-lstm-create-train-data\n* https://www.kaggle.com/tfukuda675/simple-lstm-run-train\n* https://www.kaggle.com/tfukuda675/simple-lstm-compair-model-result\n\n## train data strategy (戦略)\ntimestepに直線性が無いデータがあります。<br>\npressure(ターゲットですが)に負のものがあります。<br>\n評価に関係しないu_out = 0のデータもあります。<br>\nそういったキレイではないデータを削除して学習します。<br>\ntestデータにはキレイでないデータも入っていますが、無視する戦略です。<br>\n\nStrategy of this notebook is \"simplify data as much as possible\"\n* Ignore non liner time_step data\n* Ignore negative pressure data\n* <s>use u_out = 0 data only for train/test</s><br>\nPlease let me now, when you have any recommendations.<br>\n\nAt first, create model with above stragety.<br>\nNext, compare model result with train data and train pressure data.<br>\nThen consider additional feature of data with above result.<br>\n\n## refer to for visualize\n過去に作ったものです。plotly/seaborn/matplotlibが期待するdfの形状に言及しています。<br>\nhttps://www.kaggle.com/tfukuda675/data-visualization-plotly-seaborn-matplot?scriptVersionId=76838377&cellId=1\n\n","metadata":{}},{"cell_type":"markdown","source":"# Visualize original data u_in histgram for each time step id","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import mean_absolute_error as mae\nfrom sklearn.preprocessing import RobustScaler, normalize, StandardScaler\nfrom sklearn.model_selection import train_test_split, GroupKFold, KFold\n\n\npath = '../input/ventilator-pressure-prediction'\ntrain = pd.read_csv(f\"{path}/train.csv\")\ntrain[\"time_step_id\"] = list(range(1,81,1)) * int(len(train)/80)\nrange_bins = [0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65,70,75,80,85,90,95,100]\nbins_name = ['~5', '~10', '~15','~20','~25', '~30', '~35', '~40','~45', '~50', '~55', '~60','~65','~70','~75','~80','~85','~90','~95','~100']\ntrain[\"u_in_range\"] = pd.cut(train[\"u_in\"],bins=range_bins,labels=bins_name)\ntmp_df = pd.DataFrame()\nfor k,grp in train.groupby(\"time_step_id\"):\n    tmp_df = tmp_df.append(grp[\"u_in_range\"].value_counts())\ntmp_df = tmp_df.reset_index(drop=True)\ntmp_df.columns = tmp_df.columns.astype(str)\ntmp_df = tmp_df.reindex(columns=bins_name)\nax = sns.heatmap(tmp_df)\nax.set_xlabel(\"u_in_range\", fontsize = 20)\nax.set_ylabel(\"time_step_id\", fontsize = 20)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-23T08:43:18.25206Z","iopub.execute_input":"2021-10-23T08:43:18.252396Z","iopub.status.idle":"2021-10-23T08:43:28.706902Z","shell.execute_reply.started":"2021-10-23T08:43:18.252357Z","shell.execute_reply":"2021-10-23T08:43:28.705916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualize u_in hist with StandardScaler() for each time step id","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(f\"{path}/train.csv\")\ncolumns = train.columns\n#scaler = RobustScaler()\nscaler = StandardScaler()\ntrain = scaler.fit_transform(train)\ntrain = pd.DataFrame(train,columns= columns)\ntrain[\"time_step_id\"] = list(range(1,81,1)) * int(len(train)/80)\nrange_bins = [-1.8, -1.6, -1.4, -1.2, -1.0, -0.8, -0.6, -0.4,-0.2,0,0.2,0.4,0.6,0.8,1.0,1.2,1.4,1.6,1.8]\nbins_name = ['~-1.6', '~-1.4', '~-1.2','~-1.0','~-0.8','~-0.6','~-0.4','~-0.2','~0','~0.2','~0.4','~0.6','~0.8','~1.0','~1.2','~1.4','~1.6','~1.8']\ntrain[\"pressure_range\"] = pd.cut(train[\"pressure\"],bins=range_bins,labels=bins_name)\ntmp_df = pd.DataFrame()\nfor k,grp in train.groupby(\"time_step_id\"):\n    tmp_df = tmp_df.append(grp[\"pressure_range\"].value_counts())\ntmp_df = tmp_df.reset_index(drop=True)\ntmp_df.columns = tmp_df.columns.astype(str)\ntmp_df = tmp_df.reindex(columns=bins_name)\nax = sns.heatmap(tmp_df)\nax.set_xlabel(\"pressure_range\", fontsize = 20)\nax.set_ylabel(\"time_step_id\", fontsize = 20)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-23T08:43:28.70855Z","iopub.execute_input":"2021-10-23T08:43:28.708841Z","iopub.status.idle":"2021-10-23T08:43:40.312748Z","shell.execute_reply.started":"2021-10-23T08:43:28.70881Z","shell.execute_reply":"2021-10-23T08:43:40.311943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualize u_in hist with RobustScaler() for each time step id","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(f\"{path}/train.csv\")\ncolumns = train.columns\nscaler = RobustScaler()\n#scaler = StandardScaler()\ntrain = scaler.fit_transform(train)\ntrain = pd.DataFrame(train,columns= columns)\ntrain[\"time_step_id\"] = list(range(1,81,1)) * int(len(train)/80)\n#range_bins = [-0.9, -0.8, -0.7, -0.6, -0.5, -0.4, -0.3, -0.2,-0.1,0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\nrange_bins = [-9, -8, -7, -6, -5, -4, -3, -2,-1,0,1,2,3,4,5,6,7,8,9,10]\nbins_name = ['~-8', '~-7','~-6','~-5','~-4','~-3','~-2','~-1','~0','~1','~2','~3','~4','~5','~6','~7','~8','~9','10']\n#train[\"pressure_range\"] = pd.cut(train[\"pressure\"],bins=range_bins,labels=bins_name)\ntrain[\"u_in_range\"] = pd.cut(train[\"u_in\"],bins=range_bins,labels=bins_name)\ntmp_df = pd.DataFrame()\nfor k,grp in train.groupby(\"time_step_id\"):\n    #tmp_df = tmp_df.append(grp[\"pressure_range\"].value_counts())\n    tmp_df = tmp_df.append(grp[\"u_in_range\"].value_counts())\ntmp_df = tmp_df.reset_index(drop=True)\ntmp_df.columns = tmp_df.columns.astype(str)\ntmp_df = tmp_df.reindex(columns=bins_name)\nax = sns.heatmap(tmp_df)\nax.set_xlabel(\"u_in_range\", fontsize = 20)\nax.set_ylabel(\"time_step_id\", fontsize = 20)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-23T08:43:40.31423Z","iopub.execute_input":"2021-10-23T08:43:40.314461Z","iopub.status.idle":"2021-10-23T08:43:52.142004Z","shell.execute_reply.started":"2021-10-23T08:43:40.314434Z","shell.execute_reply":"2021-10-23T08:43:52.140901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Codes that surprised me recently\n\n## use for in groupby()\n\nIt can modify data after groupby.\ngroupbyしたあとのdfを使いたい事が多々あったのですが、まさかforで実現できるとは思いませんでした。\n\n```\nfor key, grp in train.groupby('breath_id'):\n    fig.add_trace(go.Violin(x=grp[\"u_in\"]))\n```\n<br><br>\n\n## use tqdm in groupby()\n\nCan use tqdm in groupby process.\n\n```\nfrom tqdm import tqdm\ntqdm.pandas()\nfor key, grp in tqdm(train.groupby('breath_id')):\n    fig.add_trace(go.Violin(x=grp[\"u_in\"]))\n```\n\n\n## 任意の範囲でheatmap\n\n出現頻度を任意の範囲で確認したいことがあったのですが、binsを作って、それをcut()で利用することで実現可能でした。\n\n```\nrange_bins = pd.interval_range(start=-10, freq=10, end=100)\npd.cut(grp,bins=range_bins).value_counts()\n```\n<br><br>","metadata":{"execution":{"iopub.status.busy":"2021-10-12T12:34:35.859805Z","iopub.execute_input":"2021-10-12T12:34:35.860091Z","iopub.status.idle":"2021-10-12T12:34:35.867115Z","shell.execute_reply.started":"2021-10-12T12:34:35.860063Z","shell.execute_reply":"2021-10-12T12:34:35.865741Z"}}},{"cell_type":"markdown","source":"# 用語集\n\n### twinx @ matplotlib\n\nmatplotlibで左右のx軸を使う宣言。\n\nhttps://qiita.com/supersaiakujin/items/e2ee4019adefce08e381\n<br><br>\n\n### RobustScaler @scikit-learn\n\nscikit-learn を使った特徴量のスケーリング方法の一つ。<br>\n第1分位点 Q_1Q と第3分位点 Q_3Q の範囲が1になるように変換。<br>\n外れ値に引っ張られるのを防ぐ。<br>\n\nhttps://pystyle.info/ml-feature-scaling/\n\nこのノートブックでは使わない。<br>\n\n<br><br>\n\n### fitとtransformについて\nfitで最大・最小を計算<br>\nmmscaler.fit(x)           # xの最大・最小を計算<br>\ntransformで値をfitの間に入るように変換<br>\ny = mmscaler.transform(x) # xを変換<br>\n\ntrainでfitで最大・最小を計算<br>\ntransformをtrainとtestへ実施。<br>\n\n【注意】<br>\nRobustScalerのfitで値を変換するその内容を決定し、transformで値の変更を実施する。<br>\nそのため、fitしたときのcolumnの幅と、transformするときのcolumnの幅が揃わないとだめ。<br>\n\n<br><br>\n### add_prefix / add_suffix @ dataframe\n\n列名にプレフィックス（接頭辞）、サフィックス（接尾辞）を追加するためのメソッド。\n```\nprint(df.add_prefix('X_'))\n#        X_A  X_B  X_C\n```\nhttps://note.nkmk.me/python-pandas-dataframe-rename/\n<br><br>\n\n### 行列の@ @ numpy / tf\n\ndotの意味。numpyとtfで使える。\n\n```\nA = np.random.rand(3,3)\nB = tf.random.uniform((3,3))\n\nAAA = A @ A @ A # AAA = A.dot(A).dot(A)\nBBB = B @ B @ B # BBB = tf.matmul(ft.matmul(B,B),B)\n```\n<br><br>\n\n### .agg(list) @ groupby()\ngroupbyした項目以外のデータをまるまるlistにする。\n\n```\ndf2 = df.groupby('breath_id').agg(list).reset_index()\n```\n\n|breath_id|id|R|C|time_step|u_in|u_out|pressure|\n|---|---|---|---|---|---|---|---|\n|0|1|[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...|[20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 2...|[50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 5...|[0.0, 0.0336523056030273, 0.067514419555664, 0...|[0.0833340056346443, 18.38304147263472, 22.509...|[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...|[5.837491705069121, 5.907793850520346, 7.87625...|\n\n\n<br><br>\n### tf.test.is_gpu_available()\n\nGPUが使えるかどうか。使えればTrueが帰ってくる。\n\n<br><br>\n\n### model.save @ tf\nmode.save()でディレクトリを指定。.hd5をつければhd5で保存。<br>\n\n利用は次の通り。.hd5であれば拡張子も忘れずに。<br>\n```\nnew_model = tf.keras.models.load_model('saved_model/my_model')\nloss, acc = new_model.evaluate(test_images,  test_labels, verbose=2)\n```\nhttps://www.tensorflow.org/guide/keras/save_and_serialize?hl=ja#savedmodel_%E5%BD%A2%E5%BC%8F\n\n<br><br>\n### model.save_weights @ tf\n\n学習中や終了中にsaveが可能。weightだけ保存するので、同じmodel構成を作れば他のtfインスタンスにコピーが可能。<br>\nちょっとめんどくさそうだけど、こっちのほうが良さそう。\n\nhttps://www.tensorflow.org/tutorials/keras/save_and_load?hl=ja#%E8%A8%93%E7%B7%B4%E4%B8%AD%E3%81%AB%E3%83%81%E3%82%A7%E3%83%83%E3%82%AF%E3%83%9D%E3%82%A4%E3%83%B3%E3%83%88%E3%82%92%E4%BF%9D%E5%AD%98%E3%81%99%E3%82%8B\n\n```\n# ファイル名に(`str.format`を使って)エポック数を埋め込む\ncheckpoint_path = \"training_2/cp-{epoch:04d}.ckpt\"\ncheckpoint_dir = os.path.dirname(checkpoint_path)\n\n# 5エポックごとにモデルの重みを保存するコールバックを作成\ncp_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_path, \n    verbose=1, \n    save_weights_only=True,\n    period=5)\n\n# 新しいモデルのインスタンスを作成\nmodel = create_model()\n\n# `checkpoint_path` フォーマットで重みを保存\nmodel.save_weights(checkpoint_path.format(epoch=0))\n\n# 新しいコールバックを使い、モデルを訓練\nmodel.fit(train_images, \n          train_labels,\n          epochs=50, \n          callbacks=[cp_callback],\n          validation_data=(test_images,test_labels),\n          verbose=0)\n```\n\n以下でモデルをチェック\nls {checkpoint_dir}\n\n```\ncheckpoint            cp-0025.ckpt.index\ncp-0000.ckpt.data-00000-of-00001  cp-0030.ckpt.data-00000-of-00001\ncp-0000.ckpt.index        cp-0030.ckpt.index\ncp-0005.ckpt.data-00000-of-00001  cp-0035.ckpt.data-00000-of-00001\ncp-0005.ckpt.index        cp-0035.ckpt.index\n```\n最新版の利用は\n```\nlatest = tf.train.latest_checkpoint(checkpoint_dir)\n\n# 新しいモデルのインスタンスを作成\nmodel = create_model()\n\n# 先ほど保存した重みを読み込み\nmodel.load_weights(latest)\n\n# モデルを再評価\nloss, acc = model.evaluate(test_images,  test_labels, verbose=2)\nprint(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))\n```\n<br><br>\n\n### Dense layer　全結合層\n\n次のコードの結果が下の絵。\n```\nmodel = models.Sequential()\nmodel.add(Dense(10, activation='relu', input_shape=(4,)))   \nmodel.add(Dense(3, activation='softmax'))\n```\n\n<img src=\"https://sinyblog.com/wp/wp-content/uploads/2019/04/keras-003.jpg\" width=\"500\">\n\n<br><br>\n\n### selu @ 活性化関数 tf dense layer\n\n勾配喪失に強い活性化関数。Reluの時代は終わった、SeLuらしい。\n\nhttps://data-analysis-stats.jp/%E6%B7%B1%E5%B1%9E%E5%AD%A6%E7%BF%92/%E6%B4%BB%E6%80%A7%E5%8C%96%E9%96%A2%E6%95%B0%E3%81%AE%E3%81%BE%E3%81%A8%E3%82%81/\n\n<img src=\"https://data-analysis-stats.jp/wp-content/uploads/2021/03/SELU-activation-01.png\" width=\"500\">\n\n<br><br>\n\n### keras.layers.Bidirectional\n\n従来のRNNのニューロンをフォワード(未来)なものとバックワード(過去)なものとの2つに分ける。<br>\n今回は必要かな？肺モデルからのフィードバックが入力側にあるなら、Bidirectionalが必要そうだけど。<br>\n\nhttps://www.i2tutorials.com/deep-dive-into-bidirectional-lstm/\n\n<img src=\"https://d1zx6djv3kb1v7.cloudfront.net/wp-content/media/2019/05/Deep-Dive-into-Bidirectional-LSTM-i2tutorials.jpg\" width=\"500\">","metadata":{}},{"cell_type":"markdown","source":"\n\n# ライブラリ読み込み","metadata":{}},{"cell_type":"code","source":"#!pip install joypy\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\n#from joypy import joyplot for matplotlib\nimport tensorflow as tf\nfrom tqdm import tqdm\nimport optuna\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import LearningRateScheduler\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\n\nfrom sklearn.metrics import mean_absolute_error as mae\nfrom sklearn.preprocessing import RobustScaler, normalize, StandardScaler\nfrom sklearn.model_selection import train_test_split, GroupKFold, KFold\n\ntqdm.pandas()\n\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2021-10-21T12:48:04.687059Z","iopub.execute_input":"2021-10-21T12:48:04.68731Z","iopub.status.idle":"2021-10-21T12:48:08.935853Z","shell.execute_reply.started":"2021-10-21T12:48:04.687285Z","shell.execute_reply":"2021-10-21T12:48:08.934979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if tf.device('/CPU:0'):\n    print(tf.device('/CPU:0'))","metadata":{"execution":{"iopub.status.busy":"2021-10-21T12:48:08.937689Z","iopub.execute_input":"2021-10-21T12:48:08.937952Z","iopub.status.idle":"2021-10-21T12:48:08.94757Z","shell.execute_reply.started":"2021-10-21T12:48:08.937924Z","shell.execute_reply":"2021-10-21T12:48:08.946374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Settings","metadata":{}},{"cell_type":"code","source":"# When Debug mode is true, this notebook use small train data.\ndebug_mode = True\ndebug_data_count = int(1000)\n\ntqdm.pandas()\n\n# folloiwng settig takes time due to plotly is so slow.\nvisualize = False  # [True | False]\nvisualize2 = False\n\n# Use trained model. no run training. Just use trained model only.\n\nuse_trained_model = True","metadata":{"execution":{"iopub.status.busy":"2021-10-21T12:48:08.949373Z","iopub.execute_input":"2021-10-21T12:48:08.950121Z","iopub.status.idle":"2021-10-21T12:48:08.962695Z","shell.execute_reply.started":"2021-10-21T12:48:08.950079Z","shell.execute_reply":"2021-10-21T12:48:08.961853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Read data","metadata":{}},{"cell_type":"code","source":"path = '../input/ventilator-pressure-prediction'\ntrain = pd.read_csv(f\"{path}/train.csv\")\ntest = pd.read_csv(f\"{path}/test.csv\")\nsubmission = pd.read_csv(f'{path}/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-10-21T12:48:08.965707Z","iopub.execute_input":"2021-10-21T12:48:08.965963Z","iopub.status.idle":"2021-10-21T12:48:18.60668Z","shell.execute_reply.started":"2021-10-21T12:48:08.965937Z","shell.execute_reply":"2021-10-21T12:48:18.605839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# confirm linearity of time_step","metadata":{}},{"cell_type":"code","source":"if visualize:\n    time_step_diff_limit = 0.04\n    non_liner_timestep_breath_ids = list()\n    for k, grp in tqdm(train.groupby(\"breath_id\")):\n        diff_se = grp[\"time_step\"].diff()\n        diff_chk = diff_se[diff_se > time_step_diff_limit]\n        if len(diff_chk) != 0:\n            non_liner_timestep_breath_ids.append(k)\n#\n#print(non_liner_timestep_breath_ids)\n## results are following:\n## [803, 2327, 3178, 4199, 5830, 10277, 11502, 13238, 15803, 16315, 16634, 18117, 18600, 24127, 25397, 28189, 28942, 30181, 32296, 36128, 36175, 37711, 38237, 38415, 39045, 39722, 42317, 42988, 43344, 44245, 45197, 46324, 49849, 53877, 54129, 55244, 55851, 61454, 64662, 67422, 67748, 72104, 74766, 76037, 78768, 79105, 80375, 87127, 87776, 89084, 91883, 93186, 98677, 102063, 104001, 106034, 107067, 109693, 111439, 112027, 115588, 119689, 120878, 121135, 125136]","metadata":{"execution":{"iopub.status.busy":"2021-10-21T12:48:18.609244Z","iopub.execute_input":"2021-10-21T12:48:18.609572Z","iopub.status.idle":"2021-10-21T12:48:18.615273Z","shell.execute_reply.started":"2021-10-21T12:48:18.609534Z","shell.execute_reply":"2021-10-21T12:48:18.614533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# visualize non linearity time_step (直線でないtime_stepの可視化)","metadata":{}},{"cell_type":"code","source":"if visualize:\n    non_liner_timestep_df = train[train[\"breath_id\"].isin(non_liner_timestep_breath_ids)]\n    fig = go.Figure()\n    for k,grp in non_liner_timestep_df.groupby(\"breath_id\"):\n        grp = grp.reset_index(drop=True)\n        fig.add_trace(go.Scatter(x=grp.index, y=grp[\"time_step\"], mode='lines', name=k))\n    fig.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-21T12:48:18.616349Z","iopub.execute_input":"2021-10-21T12:48:18.616567Z","iopub.status.idle":"2021-10-21T12:48:18.634449Z","shell.execute_reply.started":"2021-10-21T12:48:18.616543Z","shell.execute_reply":"2021-10-21T12:48:18.633419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# visualize linearity time_step (直線なtime_stepの可視化)\n\nおっと、傾きが色々とある！\n計測機器側の計測インターバルがずれるとは思えないので、一回のサイクルを80分割したのだと思う。","metadata":{}},{"cell_type":"code","source":"if visualize:\n    liner_timestep_df = train[~train[\"breath_id\"].isin(non_liner_timestep_breath_ids)]\n    fig = go.Figure()\n    for k,grp in tqdm(liner_timestep_df[:80*10000].groupby(\"breath_id\")):\n        grp = grp.reset_index(drop=True)\n        fig.add_trace(go.Scatter(x=grp.index, y=grp[\"time_step\"], mode='lines', name=k))\n    fig.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-21T12:48:18.636055Z","iopub.execute_input":"2021-10-21T12:48:18.636307Z","iopub.status.idle":"2021-10-21T12:48:18.645713Z","shell.execute_reply.started":"2021-10-21T12:48:18.636278Z","shell.execute_reply":"2021-10-21T12:48:18.644915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# find minus pressure data (ターゲット値ですが)にマイナス値を持つデータを探す","metadata":{}},{"cell_type":"code","source":"if visualize:\n    minus_pressure_breath_ids = list()\n    for k, grp in tqdm(train.groupby(\"breath_id\")):\n        m = grp[\"pressure\"].min()\n        if m < 0:\n            minus_pressure_breath_ids.append(k)\n# print(minus_pressure_breath_ids)\n# [542, 851, 3928, 7949, 11216, 13594, 16599, 19236, 20075, 22164, 23710, 27195, 27731, 30127, 36474, 40431, 40753, 43103, 43630, 44309, 45099, 45681, 45877, 46018, 46020, 46486, 47325, 49376, 49941, 50459, 52137, 53057, 54206, 56152, 56760, 57119, 58835, 59101, 60949, 65596, 67080, 67788, 70753, 71461, 72011, 74977, 77803, 83713, 85391, 86508, 90584, 91132, 91464, 92955, 94037, 97520, 98041, 98080, 101951, 106703, 108406, 109424, 109761, 110499, 111419, 112036, 113323, 113639, 118131, 119582, 120445, 121570, 124575]","metadata":{"execution":{"iopub.status.busy":"2021-10-21T12:48:18.647128Z","iopub.execute_input":"2021-10-21T12:48:18.647338Z","iopub.status.idle":"2021-10-21T12:48:18.658682Z","shell.execute_reply.started":"2021-10-21T12:48:18.647314Z","shell.execute_reply":"2021-10-21T12:48:18.657872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# visualize minus pressure data (負のpressureを持つデータを可視化)","metadata":{}},{"cell_type":"code","source":"if visualize:\n    minus_pressure_df = train[train[\"breath_id\"].isin(minus_pressure_breath_ids)]\n    minus_pressure_df_plotly = pd.melt(minus_pressure_df,id_vars=[\"time_step\",\"breath_id\"], value_vars=[\"pressure\"])\n    fig = px.line(minus_pressure_df_plotly, x=\"time_step\" , y=\"value\",color = \"variable\",line_group =\"breath_id\")\n    for line in fig.data:\n        line['line']['color']='rgba(0, 0, 255, 0.1)'\n    fig.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-21T12:48:18.659837Z","iopub.execute_input":"2021-10-21T12:48:18.660071Z","iopub.status.idle":"2021-10-21T12:48:18.673703Z","shell.execute_reply.started":"2021-10-21T12:48:18.660045Z","shell.execute_reply":"2021-10-21T12:48:18.672936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# count steps of u_out = 1   (排気段階)のstep数をtrain/testデータで確認","metadata":{}},{"cell_type":"code","source":"if visualize:\n    u_out_open_step_counts = list()\n    for k, grp in tqdm(train.groupby(\"breath_id\")):\n        count = grp.groupby(\"u_out\")[\"id\"].count()[1]\n        u_out_open_step_counts.append(count)\n    \n    u_out_open_step_counts_test = list()\n    for k, grp in tqdm(test.groupby(\"breath_id\")):\n        count = grp.groupby(\"u_out\")[\"id\"].count()[1]\n        u_out_open_step_counts_test.append(count)","metadata":{"execution":{"iopub.status.busy":"2021-10-21T12:48:18.67483Z","iopub.execute_input":"2021-10-21T12:48:18.675179Z","iopub.status.idle":"2021-10-21T12:48:18.683488Z","shell.execute_reply.started":"2021-10-21T12:48:18.675134Z","shell.execute_reply":"2021-10-21T12:48:18.682946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# visualize by histgram counts of u_out = 1 (排気段階)のstep数の分布を可視化\n48 ~ 51にまとまっている。裾のは、55まで。","metadata":{}},{"cell_type":"code","source":"if visualize: \n    fig = px.histogram(x=u_out_open_step_counts,nbins=8)\n    fig.update_layout(title=\"u_out = 1 count histgram in train\")\n    fig.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-21T12:48:18.684566Z","iopub.execute_input":"2021-10-21T12:48:18.684946Z","iopub.status.idle":"2021-10-21T12:48:18.694515Z","shell.execute_reply.started":"2021-10-21T12:48:18.684912Z","shell.execute_reply":"2021-10-21T12:48:18.693909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if visualize:\n    fig = px.histogram(x=u_out_open_step_counts_test,nbins=8)\n    fig.update_layout(title=\"u_out = 1 count histgram in test\")\n    fig.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-21T12:48:18.695759Z","iopub.execute_input":"2021-10-21T12:48:18.696248Z","iopub.status.idle":"2021-10-21T12:48:18.705939Z","shell.execute_reply.started":"2021-10-21T12:48:18.696211Z","shell.execute_reply":"2021-10-21T12:48:18.705108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# data count of counts over 52 of u_out = 1 (排気段階)のstep数が52以上のデータ数","metadata":{}},{"cell_type":"code","source":"if visualize: \n    u_out_open_step_counts_over52 = list()\n    for k, grp in tqdm(train.groupby(\"breath_id\")):\n        count = grp.groupby(\"u_out\")[\"id\"].count()[1]\n        if count > 51:\n            u_out_open_step_counts_over52.append(count)\n    len(u_out_open_step_counts_over52)","metadata":{"execution":{"iopub.status.busy":"2021-10-21T12:48:18.707041Z","iopub.execute_input":"2021-10-21T12:48:18.707357Z","iopub.status.idle":"2021-10-21T12:48:18.716595Z","shell.execute_reply.started":"2021-10-21T12:48:18.707332Z","shell.execute_reply":"2021-10-21T12:48:18.716045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# visualize pressuer of u_out = 1","metadata":{}},{"cell_type":"code","source":"if visualize:\n    train = train[:80*1000]\n    train = train[train[\"u_out\"] == 1]\n    pressure_df_uout = pd.melt(train,id_vars=[\"time_step\",\"breath_id\"], value_vars=[\"pressure\"])\n    fig = px.line(pressure_df_uout, x=\"time_step\" , y=\"value\",color = \"variable\",line_group =\"breath_id\")\n    for line in fig.data:\n        line['line']['color']='rgba(0, 0, 255, 0.05)'\n    fig.show()\n    ","metadata":{"execution":{"iopub.status.busy":"2021-10-21T12:48:18.719224Z","iopub.execute_input":"2021-10-21T12:48:18.71954Z","iopub.status.idle":"2021-10-21T12:48:18.733036Z","shell.execute_reply.started":"2021-10-21T12:48:18.719507Z","shell.execute_reply":"2021-10-21T12:48:18.732229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# count steps of u_out = 0 step","metadata":{}},{"cell_type":"code","source":"if visualize2:\n    u_out_close_step_counts = list()\n    for k, grp in tqdm(train.groupby(\"breath_id\")):\n        count = grp.groupby(\"u_out\")[\"id\"].count()[0]\n        u_out_close_step_counts.append(count)\n    \n    u_out_close_step_counts_test = list()\n    for k, grp in tqdm(test.groupby(\"breath_id\")):\n        count = grp.groupby(\"u_out\")[\"id\"].count()[0]\n        u_out_close_step_counts_test.append(count)","metadata":{"execution":{"iopub.status.busy":"2021-10-21T12:48:18.734091Z","iopub.execute_input":"2021-10-21T12:48:18.734391Z","iopub.status.idle":"2021-10-21T12:48:18.745383Z","shell.execute_reply.started":"2021-10-21T12:48:18.734366Z","shell.execute_reply":"2021-10-21T12:48:18.744487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if visualize2: \n    fig = px.histogram(x=u_out_close_step_counts,nbins=8)\n    fig.update_layout(title=\"u_out = 0 count histgram in train\")\n    fig.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-21T12:48:18.746674Z","iopub.execute_input":"2021-10-21T12:48:18.747438Z","iopub.status.idle":"2021-10-21T12:48:18.755632Z","shell.execute_reply.started":"2021-10-21T12:48:18.747393Z","shell.execute_reply":"2021-10-21T12:48:18.754951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if visualize2:\n    fig = px.histogram(x=u_out_close_step_counts_test,nbins=8)\n    fig.update_layout(title=\"u_out = 0 count histgram in test\")\n    fig.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-21T12:48:18.756494Z","iopub.execute_input":"2021-10-21T12:48:18.757087Z","iopub.status.idle":"2021-10-21T12:48:18.766264Z","shell.execute_reply.started":"2021-10-21T12:48:18.757055Z","shell.execute_reply":"2021-10-21T12:48:18.76539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Utilitys\n\ndata_cleanの中で、直線性の無いデータ、負のpressureを持つデータ、u_out = 1のstep数52以上あるデータを削除。<br>","metadata":{}},{"cell_type":"code","source":"def data_clean(df):\n    ## pickup ignore breath id\n    ignore_breath_ids = set()\n    \n    \n    time_step_diff_limit = 0.04\n    for k, grp in tqdm(df.groupby(\"breath_id\")):\n        \n        ## ignore non liner time_step data\n        diff_se = grp[\"time_step\"].diff()\n        diff_chk = diff_se[diff_se > time_step_diff_limit]\n        if len(diff_chk) != 0:\n            ignore_breath_ids.add(k)\n            \n        ## ignor negative pressure data\n        m = grp[\"pressure\"].min()\n        if m < 0:\n            ignore_breath_ids.add(k)\n            \n        ## ignore (u_out = 0 step) < 29\n        #count0 = grp.groupby(\"u_out\")[\"id\"].count()[0]\n        #if count0 < 29:\n        #    ignore_breath_ids.add(k)\n    \n    df = df[~df[\"breath_id\"].isin(np.array(list(ignore_breath_ids)))]\n    return df\n\ndef change_type(df):\n    df['R'] = df['R'].astype(str)\n    df['C'] = df['C'].astype(str)\n    return df\n\ndef add_features(df):\n    df['u_in_cumsum'] = df.groupby('breath_id')['u_in'].cumsum()\n    df['u_in_lag1'] = df.groupby('breath_id')['u_in'].shift(1)\n    df['u_in_lag2'] = df.groupby('breath_id')['u_in'].shift(2)\n    df = df.fillna(0)\n    return df\n\n\ndef tf_tpu_or_gpu_or_cpu():\n    tpu = None\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        print('Running on TPU ', tpu.master())\n    except ValueError:\n        tpu = None\n\n    if tpu:\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n        return \"tpu\"\n\n    elif tf.test.is_gpu_available():\n        strategy = tf.distribute.get_strategy()\n        print('Running on GPU')\n        return \"gpu\"\n\n    else:\n        strategy = tf.distribute.get_strategy()\n        print('Running on CPU')\n        return \"cpu\"\n","metadata":{"execution":{"iopub.status.busy":"2021-10-21T12:48:18.76732Z","iopub.execute_input":"2021-10-21T12:48:18.767986Z","iopub.status.idle":"2021-10-21T12:48:18.781277Z","shell.execute_reply.started":"2021-10-21T12:48:18.767954Z","shell.execute_reply":"2021-10-21T12:48:18.78052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 排気時のみのデータ作成\n\nu_outのstep数最大(今回は51)でcut。短い場合は、一番最後のデータをコピーして51に合わせる。","metadata":{}},{"cell_type":"code","source":"def exhaust_mode_df(df):\n    grp_len = int(51)\n    new_df = pd.DataFrame()\n    for k, grp in df.groupby(\"breath_id\"):\n        tmp_df = grp[grp[\"u_out\"] == 1]\n        rowno = tmp_df.shape[0]\n        for l in range(grp_len - rowno):\n            tmp_df = tmp_df.append(grp.tail(1),ignore_index=True)\n        new_df = new_df.append(tmp_df,ignore_index=True)\n    return new_df","metadata":{"execution":{"iopub.status.busy":"2021-10-21T12:48:18.782625Z","iopub.execute_input":"2021-10-21T12:48:18.783084Z","iopub.status.idle":"2021-10-21T12:48:18.795609Z","shell.execute_reply.started":"2021-10-21T12:48:18.783045Z","shell.execute_reply":"2021-10-21T12:48:18.794873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def uout0_mode_df(df):\n    grp_len = int(32)\n    new_df = pd.DataFrame()\n    for k, grp in df.groupby(\"breath_id\"):\n        tmp_df = grp[grp[\"u_out\"] == 0]\n        rowno = tmp_df.shape[0]\n        if \"pressure\" in tmp_df.columns:\n            for l in range(grp_len - rowno):\n                tmp_df = tmp_df.append({\"\"},ignore_index=True)\n        else:\n            for l in range(grp_len - rowno):\n                tmp_df = tmp_df.append(grp.tail(1),ignore_index=True)\n        new_df = new_df.append(tmp_df,ignore_index=True)\n    return new_df","metadata":{"execution":{"iopub.status.busy":"2021-10-21T12:48:18.796918Z","iopub.execute_input":"2021-10-21T12:48:18.797219Z","iopub.status.idle":"2021-10-21T12:48:18.81045Z","shell.execute_reply.started":"2021-10-21T12:48:18.797183Z","shell.execute_reply":"2021-10-21T12:48:18.809615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# データ読み込みとconfig設定","metadata":{}},{"cell_type":"code","source":"path = '../input/ventilator-pressure-prediction'\ntrain = pd.read_csv(f\"{path}/train.csv\")\ntest = pd.read_csv(f\"{path}/test.csv\")\nsubmission = pd.read_csv(f'{path}/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-10-21T12:48:18.811647Z","iopub.execute_input":"2021-10-21T12:48:18.812543Z","iopub.status.idle":"2021-10-21T12:48:31.811275Z","shell.execute_reply.started":"2021-10-21T12:48:18.81251Z","shell.execute_reply":"2021-10-21T12:48:31.810448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Debug モード時の処理\nデータ数を削減<br>\nIn use_trained_model case, use new data.","metadata":{}},{"cell_type":"code","source":"if debug_mode:\n    if use_trained_model:\n        train = train[80*debug_data_count:80*debug_data_count*2]\n        test = test[80*debug_data_count:80*debug_data_count*2]\n    else:\n        train = train[:80*debug_data_count]\n        test = test[:80*debug_data_count]","metadata":{"execution":{"iopub.status.busy":"2021-10-21T12:48:31.81244Z","iopub.execute_input":"2021-10-21T12:48:31.812797Z","iopub.status.idle":"2021-10-21T12:48:31.818077Z","shell.execute_reply.started":"2021-10-21T12:48:31.812769Z","shell.execute_reply":"2021-10-21T12:48:31.81747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# apply utilitys for data\nデータの追加と削除がメイン","metadata":{}},{"cell_type":"code","source":"# exhaust_mode_df() do not work collectry\nu_out1_only = False\n\n\n## trainのみ、data_cleanを実施\ntrain = data_clean(train)  ## time_stepがリニア出ないものは削除\nif u_out1_only:\n    train = exhaust_mode_df(train)\ntrain = add_features(train)\ntrain = change_type(train)\n\nif u_out1_only:\n    test = exhaust_mode_df(test)\ntest = add_features(test)\ntest = change_type(test)","metadata":{"execution":{"iopub.status.busy":"2021-10-21T12:48:31.819145Z","iopub.execute_input":"2021-10-21T12:48:31.819489Z","iopub.status.idle":"2021-10-21T12:48:32.84871Z","shell.execute_reply.started":"2021-10-21T12:48:31.819459Z","shell.execute_reply":"2021-10-21T12:48:32.847879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# choose useful data for train and test and create target data\ntargetデータの作成と、不要なcolumnの削除。","metadata":{}},{"cell_type":"code","source":"if u_out1_only:\n    targets = train[['pressure']].to_numpy().reshape(-1, 51)\n    train = train.drop(['pressure', 'id', 'breath_id','u_out'], axis=1)\n    test = test.drop(['id', 'breath_id','u_out'], axis=1)\nelse:\n    targets = train[['pressure']].to_numpy().reshape(-1, 80)\n    train = train.drop(['pressure', 'id', 'breath_id'], axis=1)\n    test = test.drop(['id', 'breath_id'], axis=1)\n","metadata":{"execution":{"iopub.status.busy":"2021-10-21T12:48:32.850256Z","iopub.execute_input":"2021-10-21T12:48:32.850545Z","iopub.status.idle":"2021-10-21T12:48:32.891898Z","shell.execute_reply.started":"2021-10-21T12:48:32.850508Z","shell.execute_reply":"2021-10-21T12:48:32.891009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 入力データのデータそのものの変形\n\n### RobustScalerについて\n入力データの分布に応じで変形。外れ値に強くしたり、正規分布に近くしたりできる。<br>\nhttps://helve-blog.com/posts/python/scikit-learn-feature-scaling/\n<br><br>\n### numpy bload cast error について\n原因は、trainとtestのデータの幅が違ったから。<br>\nRobustScalerのfitで値を変換するその内容を決定し、transformで値の変更を実施する。<br>\nそのため、fitしたときのcolumnの幅と、transformするときのcolumnの幅が揃わないとだめ。<br>\nhttps://www.headboost.jp/numpy-array-broadcasting/\n<br><br>","metadata":{}},{"cell_type":"code","source":"#scaler = RobustScaler()\nscaler = StandardScaler()\ntrain = scaler.fit_transform(train)\ntest = scaler.transform(test)","metadata":{"execution":{"iopub.status.busy":"2021-10-21T12:48:32.894437Z","iopub.execute_input":"2021-10-21T12:48:32.894706Z","iopub.status.idle":"2021-10-21T12:48:33.196768Z","shell.execute_reply.started":"2021-10-21T12:48:32.894674Z","shell.execute_reply":"2021-10-21T12:48:33.196087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# データ形状をTF向けに変更\nbreath_idごとのデータになるように整形。<br>","metadata":{}},{"cell_type":"code","source":"if u_out1_only:\n    train = train.reshape(-1, 51, train.shape[-1])\n    test = test.reshape(-1, 51, test.shape[-1])\nelse:\n    train = train.reshape(-1, 80, train.shape[-1])\n    test = test.reshape(-1, 80, test.shape[-1])","metadata":{"execution":{"iopub.status.busy":"2021-10-21T12:48:33.197776Z","iopub.execute_input":"2021-10-21T12:48:33.198434Z","iopub.status.idle":"2021-10-21T12:48:33.203974Z","shell.execute_reply.started":"2021-10-21T12:48:33.198401Z","shell.execute_reply":"2021-10-21T12:48:33.203189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# run tf function","metadata":{}},{"cell_type":"code","source":"def run_tf_blstm(epoch=int(50),batch_size=int(1024),train=None,test=None,targets=None):\n    #kf = KFold(n_splits=5, shuffle=True, random_state=2000)\n    kf = KFold(n_splits=5, shuffle=True)\n    test_preds = []\n    test_history = []\n    for fold, (train_idx, test_idx) in enumerate(kf.split(train, targets)):\n        print('-'*15, '>', f'Fold {fold+1}', '<', '-'*15)\n        X_train, X_valid = train[train_idx], train[test_idx]\n        y_train, y_valid = targets[train_idx], targets[test_idx]\n        model = keras.models.Sequential([\n            #keras.layers.Embedding(input_dim=train.shape[-2:], output_dim=300, mask_zero=True),\n            keras.layers.Input(shape=train.shape[-2:]),\n            keras.layers.Bidirectional(keras.layers.LSTM(300, return_sequences=True)),          \n            keras.layers.Bidirectional(keras.layers.LSTM(250, return_sequences=True)),\n            keras.layers.Bidirectional(keras.layers.LSTM(150, return_sequences=True)),\n            keras.layers.Bidirectional(keras.layers.LSTM(100, return_sequences=True)),\n            keras.layers.Dense(50, activation='selu'),\n            keras.layers.Dense(1),\n        ])\n        model.compile(optimizer=\"adam\", loss=\"mae\")    \n        \n        scheduler = ExponentialDecay(1e-3, 400*((len(train)*0.8)/batch_size), 1e-5)\n        lr = LearningRateScheduler(scheduler, verbose=1)\n\n        #es = EarlyStopping(monitor=\"val_loss\", patience=15, verbose=1, mode=\"min\", restore_best_weights=True)\n\n        history = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=epoch, batch_size=batch_size, callbacks=[lr])\n        test_history.append(history.history)\n        model.save(f'model_save_fold{fold+1}')\n        #test_preds.append(model.predict(test).squeeze().reshape(-1, 1).squeeze())\n        test_preds.append(model.predict(test).squeeze())\n    \n    return test_preds, test_history\n\ndef run_tf_lstm(epoch=int(50),batch_size=int(1024),train=None,test=None,targets=None):\n    #kf = KFold(n_splits=5, shuffle=True, random_state=2000)\n    kf = KFold(n_splits=5, shuffle=True)\n    test_preds = []\n    test_history = []\n    for fold, (train_idx, test_idx) in enumerate(kf.split(train, targets)):\n        print('-'*15, '>', f'Fold {fold+1}', '<', '-'*15)\n        X_train, X_valid = train[train_idx], train[test_idx]\n        y_train, y_valid = targets[train_idx], targets[test_idx]\n        model = keras.models.Sequential([\n            #keras.layers.Embedding(input_dim=train.shape[-2:], output_dim=300, mask_zero=True),\n            keras.layers.Input(shape=train.shape[-2:]),\n            keras.layers.LSTM(300, return_sequences=True),\n            keras.layers.LSTM(250, return_sequences=True),\n            keras.layers.LSTM(150, return_sequences=True),\n            keras.layers.LSTM(100, return_sequences=True),\n            keras.layers.Dense(50, activation='selu'),\n            keras.layers.Dense(1),\n        ])\n        model.compile(optimizer=\"adam\", loss=\"mae\")    \n        \n        scheduler = ExponentialDecay(1e-3, 400*((len(train)*0.8)/batch_size), 1e-5)\n        lr = LearningRateScheduler(scheduler, verbose=1)\n\n        #es = EarlyStopping(monitor=\"val_loss\", patience=15, verbose=1, mode=\"min\", restore_best_weights=True)\n\n        history = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=epoch, batch_size=batch_size, callbacks=[lr])\n        test_history.append(history.history)\n        model.save(f'model_save_fold{fold+1}')\n        result = model.predict(test)\n        #print(result.shape) #(50300, 80, 1)\n        #print(result.squeeze().shape)  #(50300, 80)\n        #print(result.squeeze().reshape(-1,1).shape) #(4024000, 1)\n        #test_preds.append(result.squeeze().reshape(-1, 1).squeeze())\n        test_preds.append(result.squeeze())\n    \n    return test_preds, test_history","metadata":{"execution":{"iopub.status.busy":"2021-10-21T12:48:33.20547Z","iopub.execute_input":"2021-10-21T12:48:33.205963Z","iopub.status.idle":"2021-10-21T12:48:33.228382Z","shell.execute_reply.started":"2021-10-21T12:48:33.205932Z","shell.execute_reply":"2021-10-21T12:48:33.227511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train と Test\n\n## 戦略\n* full randamでロバスト性を狙う。外れ値は外したデータなので、randamで振られても安定するはず。\n* modelをsave。後で答えとどれだけずれているか見る。\n* <s>use only u_out = 1 data.</s>\n\nA.I<br>\ncheck ","metadata":{}},{"cell_type":"code","source":"EPOCH = 200\nBATCH_SIZE = 1024\ntest_preds = list()\n\ndevice = tf_tpu_or_gpu_or_cpu()\n\nif use_trained_model:\n    pass\nelse:\n    if device == \"cpu\" :\n        test_preds,history = run_tf_blstm(epoch=EPOCH,batch_size=BATCH_SIZE,train=train,test=test,targets=targets)\n\n    elif device == \"gpu\":\n        test_preds,history = run_tf_blstm(epoch=EPOCH,batch_size=BATCH_SIZE,train=train,test=test,targets=targets)\n    elif device == \"tpu\":\n        try:\n            with tpu_strategy.scope():\n                test_preds,history = run_tf_blstm(epoch=EPOCH,batch_size=BATCH_SIZE,train=train,test=test,targets=targets)\n        except :\n            print('Error')","metadata":{"execution":{"iopub.status.busy":"2021-10-21T12:48:33.229519Z","iopub.execute_input":"2021-10-21T12:48:33.229749Z","iopub.status.idle":"2021-10-21T12:48:33.247615Z","shell.execute_reply.started":"2021-10-21T12:48:33.229722Z","shell.execute_reply":"2021-10-21T12:48:33.246697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# re shape prediction results","metadata":{}},{"cell_type":"code","source":"if use_trained_model:\n    pass\nelse:\n    preds = np.sum(test_preds, axis=0).tolist()","metadata":{"execution":{"iopub.status.busy":"2021-10-21T12:48:33.249103Z","iopub.execute_input":"2021-10-21T12:48:33.249469Z","iopub.status.idle":"2021-10-21T12:48:33.263764Z","shell.execute_reply.started":"2021-10-21T12:48:33.249427Z","shell.execute_reply":"2021-10-21T12:48:33.26312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# re read test.csv and add pred results to \"pressure\"\n\nIn \"u_out1_only = False\" case, do not need to run following function.<br>\nJust only for \"u_out1_only = True\" case.","metadata":{}},{"cell_type":"code","source":"def add_pressure_data(df,preds):\n    index = 0\n    new_df = pd.DataFrame()\n    for k,grp in df.groupby(\"breath_id\"):\n        u_out_len = len(grp[grp[\"u_out\"] == 1])\n        if u_out_len < 52:\n            grp.loc[grp[\"u_out\"] == 1,\"pressure\"] = preds[index][:u_out_len]\n        else:\n            out_preds = preds[index]\n            for l in range(u_out_len - 51):\n                out_preds = out_preds.append(out_preds[-1])\n            grp.loc[grp[\"u_out\"] == 1,\"pressure\"] = out_preds\n        new_df = new_df.append(grp,ignore_index=True)\n        index += 1\n    return new_df","metadata":{"execution":{"iopub.status.busy":"2021-10-21T12:48:33.265157Z","iopub.execute_input":"2021-10-21T12:48:33.265997Z","iopub.status.idle":"2021-10-21T12:48:33.276277Z","shell.execute_reply.started":"2021-10-21T12:48:33.265963Z","shell.execute_reply":"2021-10-21T12:48:33.275363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# In non debug mode, create submission data","metadata":{}},{"cell_type":"code","source":"if debug_mode:\n    pass\nelse:\n    submission[\"pressure\"] = preds\n    submission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-10-21T12:48:33.277771Z","iopub.execute_input":"2021-10-21T12:48:33.278282Z","iopub.status.idle":"2021-10-21T12:48:33.28749Z","shell.execute_reply.started":"2021-10-21T12:48:33.278242Z","shell.execute_reply":"2021-10-21T12:48:33.286834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Pre Train model","metadata":{}},{"cell_type":"code","source":"if use_trained_model:\n    model = tf.keras.models.load_model('../input/update-simple-lstm-simple-data-model/update-simple-lstm-simple-data/model_save_fold5')\n    result = model.predict(train)\n    result = result.squeeze()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Compare model result and train data, And visualize","metadata":{}},{"cell_type":"code","source":"## res and tgt is numpy array\nfor res, tgt in zip(result,targets):\n    abs_diff =  np.sum(np.abs(res - tgt))\n    if abs_diff > 500:\n        df = pd.DataFrame(res)\n        df = pd.concat([df, pd.DataFrame(tgt)], axis=1)\n        df.columns = [\"res\",\"tgt\"]\n        df[\"id\"] = df.index\n        df = pd.melt(df,id_vars=[\"id\"])\n        plt.figure()\n        fig = sns.lineplot(data=df, x='id', y=\"value\", hue=\"variable\")\n        fig.set_title(f\"abs_diff : {abs_diff}\")\n    elif abs_diff < 30:\n        df = pd.DataFrame(res)\n        df = pd.concat([df, pd.DataFrame(tgt)], axis=1)\n        df.columns = [\"res\",\"tgt\"]\n        df[\"id\"] = df.index\n        df = pd.melt(df,id_vars=[\"id\"])\n        plt.figure()\n        fig = sns.lineplot(data=df, x='id', y=\"value\", hue=\"variable\")\n        fig.set_title(f\"abs_diff : {abs_diff}\")","metadata":{"execution":{"iopub.status.busy":"2021-10-21T13:08:58.264058Z","iopub.execute_input":"2021-10-21T13:08:58.264471Z","iopub.status.idle":"2021-10-21T13:09:07.24378Z","shell.execute_reply.started":"2021-10-21T13:08:58.264443Z","shell.execute_reply":"2021-10-21T13:09:07.24292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ","metadata":{}}]}