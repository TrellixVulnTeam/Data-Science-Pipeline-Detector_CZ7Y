{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os \nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\nimport gc\nimport random\nimport numpy as np\nimport pandas as pd\nfrom scipy.fft import fft\nfrom scipy.signal import hilbert, blackman\nimport matplotlib.pyplot as plt\nfrom IPython.display import display\n\nimport tensorflow as tf\nfrom tensorflow import keras\nimport tensorflow.keras.layers as L\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.callbacks import LearningRateScheduler, ReduceLROnPlateau\n\n\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.preprocessing import RobustScaler, normalize\nfrom sklearn.model_selection import train_test_split, GroupKFold, KFold\n\n%matplotlib inline","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-01T20:30:20.978734Z","iopub.execute_input":"2021-11-01T20:30:20.97964Z","iopub.status.idle":"2021-11-01T20:30:27.602768Z","shell.execute_reply.started":"2021-11-01T20:30:20.97953Z","shell.execute_reply":"2021-11-01T20:30:27.601832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class config:\n    paths = {\n        # data\n        'train': '../input/ventilator-pressure-prediction/train.csv',\n        'test' : '../input/ventilator-pressure-prediction/test.csv',\n        'ss'   : '../input/ventilator-pressure-prediction/sample_submission.csv',    \n    }\n    \n    model_params = {\n        'debug'   : False,\n        \n        'EPOCH'   : 300,\n        'BATCH_SIZE' : 512,\n        'NUM_FOLDS' : 10,\n    }\n    \n    post_processing = {\n        'max_pressure': 64.82099173863948,\n        'min_pressure': -1.8957442945646408,\n        'diff_pressure': 0.07030215,\n    }\n    \n    random_state = 1351","metadata":{"execution":{"iopub.status.busy":"2021-11-01T20:30:27.604764Z","iopub.execute_input":"2021-11-01T20:30:27.605013Z","iopub.status.idle":"2021-11-01T20:30:27.611727Z","shell.execute_reply.started":"2021-11-01T20:30:27.604985Z","shell.execute_reply":"2021-11-01T20:30:27.610819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_all(seed=config.random_state):\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \nseed_all()","metadata":{"execution":{"iopub.status.busy":"2021-11-01T20:30:27.613138Z","iopub.execute_input":"2021-11-01T20:30:27.613535Z","iopub.status.idle":"2021-11-01T20:30:27.626767Z","shell.execute_reply.started":"2021-11-01T20:30:27.613491Z","shell.execute_reply":"2021-11-01T20:30:27.625892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(config.paths[\"train\"])\ntest = pd.read_csv(config.paths[\"test\"])\nsubmission = pd.read_csv(config.paths[\"ss\"])\n\nif config.model_params[\"debug\"]:\n    print(\"[INFO] Debug Mode...\")\n    train = train[:80*1000]\n    config.model_params[\"EPOCH\"] = 2\n    config.model_params[\"BATCH_SIZE\"] = 128\n    config.model_params[\"NUM_FOLDS\"] = 5","metadata":{"execution":{"iopub.status.busy":"2021-11-01T20:30:27.62894Z","iopub.execute_input":"2021-11-01T20:30:27.62933Z","iopub.status.idle":"2021-11-01T20:30:42.57307Z","shell.execute_reply.started":"2021-11-01T20:30:27.629283Z","shell.execute_reply":"2021-11-01T20:30:42.572018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w = blackman(81)\n\nffta = lambda x: np.abs(fft(np.append(x.values,x.values[0]))[:80])\nffta.__name__ = 'ffta'\n\nfftw = lambda x: np.abs(fft(np.append(x.values,x.values[0])*w)[:80])\nfftw.__name__ = 'fftw'\n\ndef add_features(df):\n    # new features\n    df[\"cross_u_in\"] = df[\"u_in\"] * (1 - df[\"u_out\"])\n    df[\"cross_time_step\"] = df[\"time_step\"] * (1 - df[\"u_out\"])\n    df[\"area_frac\"] = df[\"u_in\"] * df[\"time_step\"]\n    df[\"cross_area_frac\"] = df[\"area_frac\"] * (1 - df[\"u_out\"])\n\n    # group by breath_id cache\n    bid_cache = df.groupby([\"breath_id\"])\n    \n    # cumsum/diff features per breath id\n    df[\"area_cumsum\"] = bid_cache[\"area_frac\"].cumsum()\n    df[\"u_in_cumsum\"] = bid_cache[\"u_in\"].cumsum()\n    df[\"timestep_cumsum\"] = bid_cache[\"time_step\"].cumsum()\n    \n    df['u_in_lag1'] = bid_cache['u_in'].shift(1)\n    df['u_out_lag1'] = bid_cache['u_out'].shift(1)\n    df['u_in_lag_back1'] = bid_cache['u_in'].shift(-1)\n    df['u_out_lag_back1'] = bid_cache['u_out'].shift(-1)\n    df['u_in_lag2'] = bid_cache['u_in'].shift(2)\n    df['u_out_lag2'] = bid_cache['u_out'].shift(2)\n    df['u_in_lag_back2'] = bid_cache['u_in'].shift(-2)\n    df['u_out_lag_back2'] = bid_cache['u_out'].shift(-2)\n    df['u_in_lag3'] = bid_cache['u_in'].shift(3)\n    df['u_out_lag3'] = bid_cache['u_out'].shift(3)\n    df['u_in_lag_back3'] = bid_cache['u_in'].shift(-3)\n    df['u_out_lag_back3'] = bid_cache['u_out'].shift(-3)\n    df['u_in_lag4'] = bid_cache['u_in'].shift(4)\n    df['u_out_lag4'] = bid_cache['u_out'].shift(4)\n    df['u_in_lag_back4'] = bid_cache['u_in'].shift(-4)\n    df['u_out_lag_back4'] = bid_cache['u_out'].shift(-4)\n    df = df.bfill().ffill()\n    \n    # statistical transform features\n    df[\"u_in_diff_max\"] = bid_cache[\"u_in\"].transform(\"max\") - df[\"u_in\"]\n    df[\"u_in_diff_mean\"] = bid_cache[\"u_in\"].transform(\"mean\") - df[\"u_in\"]\n    df[\"u_in_deviation\"] = ((bid_cache[\"u_in\"].transform(\"max\") - bid_cache[\"u_in\"].transform(\"min\")) - df[\"u_in\"])/bid_cache[\"u_in\"].transform(\"std\")\n    \n    df['u_in_diff1'] = df['u_in'] - df['u_in_lag1']\n    df['u_out_diff1'] = df['u_out'] - df['u_out_lag1']\n    df['u_in_diff2'] = df['u_in'] - df['u_in_lag2']\n    df['u_out_diff2'] = df['u_out'] - df['u_out_lag2']\n    df['u_in_diff3'] = df['u_in'] - df['u_in_lag3']\n    df['u_out_diff3'] = df['u_out'] - df['u_out_lag3']\n    df['u_in_diff4'] = df['u_in'] - df['u_in_lag4']\n    df['u_out_diff4'] = df['u_out'] - df['u_out_lag4']\n    \n    df['one'] = 1\n    df['count'] = (df['one']).groupby(df['breath_id']).cumsum()\n    df['u_in_cummean'] = df['u_in_cumsum'] /df['count']\n    \n    df['time_step_diff'] = bid_cache['time_step'].diff().fillna(0)\n    df['ewm_u_in_mean'] = (bid_cache['u_in']\\\n                           .ewm(halflife=9)\\\n                           .mean()\\\n                           .reset_index(level=0,drop=True))\n    \n    df['u_in_lagback_diff1'] = df['u_in'] - df['u_in_lag_back1']\n    df['u_out_lagback_diff1'] = df['u_out'] - df['u_out_lag_back1']\n    df['u_in_lagback_diff2'] = df['u_in'] - df['u_in_lag_back2']\n    df['u_out_lagback_diff2'] = df['u_out'] - df['u_out_lag_back2']\n    \n    df['time_gap'] = df['time_step'] - df.shift(1).fillna(0)['time_step']\n    df['u_in_gap'] = df['u_in'] - df.shift(1).fillna(0)['u_in']\n    df['u_in_rate'] = df['u_in_gap'] / df['time_gap']\n\n    df.loc[list(range(0, len(df), 80)), 'time_gap'] = 0\n    df.loc[list(range(0, len(df), 80)), 'u_in_gap'] = 0\n    df.loc[list(range(0, len(df), 80)), 'u_in_rate'] = 0\n    \n    df['area_gap'] = df['u_in'] * df['time_gap']\n    df['area_cumsum_gap'] = (df['area_gap']).groupby(df['breath_id']).cumsum()\n    \n    # spectral features\n    df['fft_u_in'] = bid_cache['u_in'].transform(ffta)\n    df['fft_u_in_w'] = bid_cache['u_in'].transform(fftw)\n    df['analytical'] = bid_cache['u_in'].transform(hilbert)\n    df['envelope'] = np.abs(df['analytical'])\n    df['phase'] = np.angle(df['analytical'])\n    df['unwrapped_phase'] = df.groupby([\"breath_id\"])['phase'].transform(np.unwrap)\n    df['phase_shift'] = df.groupby([\"breath_id\"])['unwrapped_phase'].shift(1).astype(np.float32)\n    df['IF'] = df['unwrapped_phase'] - df['phase_shift'].astype(np.float32)\n    df = df.fillna(0)\n    \n    df['exponent'] = (-1 * df['time_step'])/(df['R']*df['C'])\n    df['factor'] = df[\"exponent\"].apply(lambda x: np.exp(x))\n    df[\"del_Prc\"] = (df['u_in']*df['R'])/df['factor']\n    \n    df['R__C'] = list(map(lambda x: str(x[0])+str(x[1]), zip(df[\"R\"].values, df[\"C\"].values)))\n    df = pd.get_dummies(df)\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-11-01T20:30:42.575807Z","iopub.execute_input":"2021-11-01T20:30:42.576135Z","iopub.status.idle":"2021-11-01T20:30:42.612485Z","shell.execute_reply.started":"2021-11-01T20:30:42.576093Z","shell.execute_reply":"2021-11-01T20:30:42.611704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = add_features(train)\ntest = add_features(test)\n\ntargets = train[['pressure']].to_numpy().reshape(-1, 80)\n\ntrain.drop(['pressure','id', 'breath_id','one','count', 'exponent', 'analytical'], axis=1, inplace=True)\ntest.drop(['id', 'breath_id','one','count', 'exponent', 'analytical'], axis=1, inplace=True)\n\nRS = RobustScaler()\ntrain = RS.fit_transform(train)\ntest = RS.transform(test)\n\ntrain = train.reshape(-1, 80, train.shape[-1])\ntest = test.reshape(-1, 80, train.shape[-1])","metadata":{"execution":{"iopub.status.busy":"2021-11-01T20:30:42.613883Z","iopub.execute_input":"2021-11-01T20:30:42.614499Z","iopub.status.idle":"2021-11-01T20:34:17.157093Z","shell.execute_reply.started":"2021-11-01T20:30:42.614462Z","shell.execute_reply":"2021-11-01T20:34:17.156049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_model(strategy, data):   \n    with strategy.scope():\n        inp = L.Input(shape=(data.shape[-2:]),name=\"Input\")\n\n        x1 = L.Bidirectional(L.LSTM(units=1024, return_sequences=True),name=\"BiLSTM1\")(inp)\n        x2 = L.Bidirectional(L.LSTM(units=512, return_sequences=True),name=\"BiLSTM2\")(x1)\n        x3 = L.Bidirectional(L.LSTM(units=256, return_sequences=True),name=\"BiLSTM3\")(x2)\n        x4 = L.Bidirectional(L.LSTM(units=128, return_sequences=True),name=\"BiLSTM4\")(x3)\n\n        z2 = L.Bidirectional(L.GRU(units=256, return_sequences=True),name=\"BiGRU1\")(x2)\n        z3 = L.Bidirectional(L.GRU(units=128, return_sequences=True),name=\"BiGRU2\")(L.Add()([x3, z2]))\n        z4 = L.Bidirectional(L.GRU(units=128, return_sequences=True),name=\"BiGRU3\")(L.Add()([x4, z3]))\n\n        x = L.Concatenate(axis=2, name=\"Concat\")([x4, z2, z3, z4])\n\n        out = L.Dense(128, activation='selu', kernel_initializer=tf.keras.initializers.lecun_normal(seed=config.random_state))(x)\n        out = L.Dense(1)(out)\n\n        model = tf.keras.Model(inputs=inp, outputs=out)\n        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0015), loss='mae')\n    \n    return model\n\ndef plot_hist(hist):\n    plt.plot(hist.history[\"loss\"])\n    plt.plot(hist.history[\"val_loss\"])\n    plt.title(\"Model Performance\")\n    plt.ylabel(\"MAE\")\n    plt.xlabel(\"epoch\")\n    plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-01T20:34:17.158805Z","iopub.execute_input":"2021-11-01T20:34:17.159166Z","iopub.status.idle":"2021-11-01T20:34:17.173524Z","shell.execute_reply.started":"2021-11-01T20:34:17.159125Z","shell.execute_reply":"2021-11-01T20:34:17.172886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Detect hardware, return appropriate distribution strategy\nprint(tf.version.VERSION)\ntry:\n    tpu = None\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept ValueError:\n    strategy = tf.distribute.MirroredStrategy() # works on GPU and multi-GPU\n    policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n    tf.config.optimizer.set_jit(True) # XLA compilation\n    tf.keras.mixed_precision.experimental.set_policy(policy)\n    print('Mixed precision enabled')\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","metadata":{"execution":{"iopub.status.busy":"2021-11-01T20:34:17.174652Z","iopub.execute_input":"2021-11-01T20:34:17.175019Z","iopub.status.idle":"2021-11-01T20:34:22.784009Z","shell.execute_reply.started":"2021-11-01T20:34:17.174992Z","shell.execute_reply":"2021-11-01T20:34:22.783083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_ = gc.collect()\nkf = KFold(n_splits=config.model_params[\"NUM_FOLDS\"], shuffle=True, random_state=config.random_state)\noof_preds = np.zeros((train.shape[0], 80))\ntest_preds = []\n\nfor fold, (train_idx, test_idx) in enumerate(kf.split(train, targets)):\n    if fold >=5:\n        print(\"[INFO] Exiting training loop at fold: \", fold)\n        break\n    K.clear_session()\n    print(f\"\\nFOLD: {fold}\")\n    X_train, X_valid = train[train_idx], train[test_idx]\n    y_train, y_valid = targets[train_idx], targets[test_idx]\n    \n    checkpoint_filepath = f\"./folds_{fold}.hdf5\"\n    lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.75, patience=5, verbose=0)\n    es = EarlyStopping(monitor=\"val_loss\", patience=25, verbose=0, mode=\"min\", restore_best_weights=True)\n    sv = keras.callbacks.ModelCheckpoint(\n        checkpoint_filepath,\n        monitor='val_loss',\n        verbose=0, \n        save_best_only=True,\n        save_weights_only=False,\n        mode='auto',\n        save_freq='epoch'\n    )\n    \n    model = create_model(strategy, X_train)\n    history = model.fit(X_train, y_train,\n                        validation_data=(X_valid, y_valid),\n                        epochs=config.model_params[\"EPOCH\"],\n                        batch_size=config.model_params[\"BATCH_SIZE\"], \n                        callbacks = [lr, es, sv])\n    model.load_weights(checkpoint_filepath) \n    \n    y_true = y_valid.squeeze().reshape(-1, 1)\n    y_pred = model.predict(X_valid, verbose=1, batch_size=config.model_params[\"BATCH_SIZE\"]).squeeze().reshape(-1, 1)\n    score = mean_absolute_error(y_true, y_pred)\n    print(f\"OOF MAE Fold {fold}: {score}\")\n    oof_preds[test_idx] = y_pred.reshape(-1, 80)\n    \n    del X_train, X_valid, y_train, y_valid, y_true, y_pred, train_idx, test_idx\n    _ = gc.collect()\n    \n    test_preds.append(model.predict(test, batch_size=config.model_params[\"BATCH_SIZE\"], verbose=1).squeeze().reshape(-1, 1).squeeze())\n    plot_hist(history)\n\nprint(f\"SEED MAE: \", mean_absolute_error(targets.squeeze().reshape(-1, 1), oof_preds.squeeze().reshape(-1, 1)))\npickle.dump(oof_preds, open(f\"./oof_preds_res_{config.random_state}.pkl\", \"wb\"))\npickle.dump(test_preds, open(f\"./test_preds_res_{config.random_state}.pkl\", \"wb\"))","metadata":{"execution":{"iopub.status.busy":"2021-11-01T20:34:22.785633Z","iopub.execute_input":"2021-11-01T20:34:22.785913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission[\"pressure\"] = np.median(np.vstack(test_preds), axis=0)\nsubmission[\"pressure\"] = np.round((submission.pressure - config.post_processing[\"min_pressure\"])/config.post_processing[\"diff_pressure\"]) * config.post_processing[\"diff_pressure\"] + config.post_processing[\"min_pressure\"]\nsubmission.pressure = np.clip(submission.pressure, config.post_processing[\"min_pressure\"], config.post_processing[\"max_pressure\"])\ndisplay(submission.head())\nsubmission.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"EOF!","metadata":{}}]}