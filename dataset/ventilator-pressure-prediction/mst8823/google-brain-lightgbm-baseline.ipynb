{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Config","metadata":{}},{"cell_type":"code","source":"class Config:\n    name_v1 = \"lgb-baseline\"\n    model_params = dict(objective=\"mae\",\n                        n_estimators=7000,\n                        num_leaves=31,\n                        random_state=2021,\n                        importance_type=\"gain\",\n                        colsample_bytree=0.3,\n                        learning_rate=0.5\n                       )\n    fit_params = dict(early_stopping_rounds=100, verbose=100)\n    feature_select_num = 30\n    n_fold = 5\n    trn_fold = [0]  # [0, 1, 2, 3, 4]\n    seeds = [2021]\n    target_col = \"pressure\"\n    debug = False","metadata":{"execution":{"iopub.status.busy":"2021-09-27T02:45:14.135911Z","iopub.execute_input":"2021-09-27T02:45:14.137603Z","iopub.status.idle":"2021-09-27T02:45:14.169239Z","shell.execute_reply.started":"2021-09-27T02:45:14.13741Z","shell.execute_reply":"2021-09-27T02:45:14.168295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Library","metadata":{}},{"cell_type":"code","source":"import os\nimport joblib\nimport logging\nimport warnings\nimport datetime\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n\nfrom sklearn import model_selection\nfrom sklearn.metrics import mean_absolute_error\n\nfrom lightgbm import LGBMModel\nfrom matplotlib_venn import venn2\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2021-09-27T02:45:14.17081Z","iopub.execute_input":"2021-09-27T02:45:14.171049Z","iopub.status.idle":"2021-09-27T02:45:17.35544Z","shell.execute_reply.started":"2021-09-27T02:45:14.171023Z","shell.execute_reply":"2021-09-27T02:45:17.354426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Logger:\n    \"\"\"save log\"\"\"\n    def __init__(self, path):\n        self.general_logger = logging.getLogger(path)\n        stream_handler = logging.StreamHandler()\n        file_general_handler = logging.FileHandler(os.path.join(path, 'Experiment.log'))\n        if len(self.general_logger.handlers) == 0:\n            self.general_logger.addHandler(stream_handler)\n            self.general_logger.addHandler(file_general_handler)\n            self.general_logger.setLevel(logging.INFO)\n\n    def info(self, message):\n        # display time\n        self.general_logger.info('[{}] - {}'.format(self.now_string(), message))\n\n    @staticmethod\n    def now_string():\n        return str(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n    \n    \nclass Util:\n    \"\"\"save & load\"\"\"\n    @classmethod\n    def dump(cls, value, path):\n        joblib.dump(value, path, compress=True)\n\n    @classmethod\n    def load(cls, path):\n        return joblib.load(path)\n    \n    \nclass HorizontalDisplay:\n    \"\"\"display dataframe\"\"\"\n    def __init__(self, *args):\n        self.args = args\n\n    def _repr_html_(self):\n        template = '<div style=\"float: left; padding: 10px;\">{0}</div>'\n        return \"\\n\".join(template.format(arg._repr_html_())\n                         for arg in self.args)\n    \n    \ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2 \n    dfs = []\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    dfs.append(df[col].astype(np.int8))\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    dfs.append(df[col].astype(np.int16))\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    dfs.append(df[col].astype(np.int32))\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    dfs.append(df[col].astype(np.int64) ) \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    dfs.append(df[col].astype(np.float16))\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    dfs.append(df[col].astype(np.float32))\n                else:\n                    dfs.append(df[col].astype(np.float64))\n        else:\n            dfs.append(df[col])\n    \n    df_out = pd.concat(dfs, axis=1)\n    if verbose:\n        end_mem = df_out.memory_usage().sum() / 1024**2\n        num_reduction = str(100 * (start_mem - end_mem) / start_mem)\n        print(f'Mem. usage decreased to {str(end_mem)[:3]}Mb:  {num_reduction[:2]}% reduction')\n    return df_out","metadata":{"execution":{"iopub.status.busy":"2021-09-27T02:45:17.357588Z","iopub.execute_input":"2021-09-27T02:45:17.357848Z","iopub.status.idle":"2021-09-27T02:45:17.383853Z","shell.execute_reply.started":"2021-09-27T02:45:17.357818Z","shell.execute_reply":"2021-09-27T02:45:17.382956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## SetUp","metadata":{}},{"cell_type":"code","source":"INPUT = \"../input/ventilator-pressure-prediction\"\nEXP = \"./\"\nEXP_MODEL = os.path.join(EXP, \"model\")\nEXP_FIG = os.path.join(EXP, \"fig\")\nEXP_PREDS = os.path.join(EXP, \"preds\")\n\n# make dirs\nfor d in [EXP_MODEL, EXP_FIG, EXP_PREDS]:\n    os.makedirs(d, exist_ok=True)\n    \n# utils\nlogger = Logger(EXP)\nwarnings.filterwarnings(\"ignore\")\nsns.set(style='whitegrid')","metadata":{"execution":{"iopub.status.busy":"2021-09-27T02:45:17.385347Z","iopub.execute_input":"2021-09-27T02:45:17.38589Z","iopub.status.idle":"2021-09-27T02:45:17.402668Z","shell.execute_reply.started":"2021-09-27T02:45:17.385848Z","shell.execute_reply":"2021-09-27T02:45:17.401777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load Data","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(os.path.join(INPUT, \"train.csv\"))\ntest = pd.read_csv(os.path.join(INPUT, \"test.csv\"))\nsample_submission = pd.read_csv(os.path.join(INPUT, \"sample_submission.csv\"))\n\nif Config.debug:\n    train = train[train[\"breath_id\"].isin(np.random.choice(train[\"breath_id\"].unique(), 100))].reset_index(drop=True)\n    test = test[test[\"breath_id\"].isin(np.random.choice(test[\"breath_id\"].unique(), 100))].reset_index(drop=True)\n    sample_submission = sample_submission[sample_submission[\"id\"].isin(test[\"id\"].tolist())].reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T02:45:17.40562Z","iopub.execute_input":"2021-09-27T02:45:17.405996Z","iopub.status.idle":"2021-09-27T02:45:32.770809Z","shell.execute_reply.started":"2021-09-27T02:45:17.405952Z","shell.execute_reply":"2021-09-27T02:45:32.769973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Simple EDA","metadata":{}},{"cell_type":"code","source":"HorizontalDisplay(train, test, sample_submission)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T02:45:32.772033Z","iopub.execute_input":"2021-09-27T02:45:32.772555Z","iopub.status.idle":"2021-09-27T02:45:32.798037Z","shell.execute_reply.started":"2021-09-27T02:45:32.772505Z","shell.execute_reply":"2021-09-27T02:45:32.797029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.concat([train, test]).reset_index(drop=True)\nHorizontalDisplay(\n    data[\"breath_id\"].value_counts().to_frame(), \n    data[\"R\"].value_counts().to_frame(),\n    data[\"C\"].value_counts().to_frame()\n)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T02:45:32.799417Z","iopub.execute_input":"2021-09-27T02:45:32.799719Z","iopub.status.idle":"2021-09-27T02:45:32.822668Z","shell.execute_reply.started":"2021-09-27T02:45:32.79968Z","shell.execute_reply":"2021-09-27T02:45:32.821844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_intersection(left, right, column, set_labels, ax=None):\n    left_set = set(left[column])\n    right_set = set(right[column])\n    venn2(subsets=(left_set, right_set), set_labels=set_labels, ax=ax)\n    return ax\n\n\ndef plot_right_left_intersection(train_df, test_df, columns='__all__'):\n    \"\"\"visualize venn(by nyk510)\"\"\"\n    if columns == '__all__':\n        columns = set(train_df.columns) & set(test_df.columns)\n\n    columns = list(columns)\n    nfigs = len(columns)\n    ncols = 5\n    nrows = - (- nfigs // ncols)\n    fig, axes = plt.subplots(figsize=(3 * ncols, 3 * nrows), ncols=ncols, nrows=nrows)\n    axes = np.ravel(axes)\n    for c, ax in zip(columns, axes):\n        plot_intersection(train_df, test_df, column=c, set_labels=('Train', 'Test'), ax=ax)\n        ax.set_title(c)\n    return fig\n\n\ntrain[\"R+C+u_out\"] = train[\"R\"].astype(str) +\"+\"+ train[\"C\"].astype(str) + \"+\" + train[\"u_out\"].astype(str)\ntest[\"R+C+u_out\"] = test[\"R\"].astype(str) +\"+\"+ test[\"C\"].astype(str) + \"+\" + test[\"u_out\"].astype(str)\nfig = plot_right_left_intersection(train, test, columns=[\"breath_id\", \"R\", \"C\", \"u_out\", \"R+C+u_out\"])\n\ndel train[\"R+C+u_out\"], test[\"R+C+u_out\"]","metadata":{"execution":{"iopub.status.busy":"2021-09-27T02:45:32.823906Z","iopub.execute_input":"2021-09-27T02:45:32.824799Z","iopub.status.idle":"2021-09-27T02:45:33.28809Z","shell.execute_reply.started":"2021-09-27T02:45:32.824757Z","shell.execute_reply":"2021-09-27T02:45:33.287242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pressure distribution (by u out)\nfig, ax = plt.subplots(figsize=(14, 7))\nax = sns.distplot(train.loc[train[\"u_out\"] == 0, \"pressure\"], ax=ax, label=\"u_out=0\", bins=200)\nax = sns.distplot(train.loc[train[\"u_out\"] == 1, \"pressure\"], ax=ax, label=\"u_out=1\", bins=200)\nax.legend(loc='upper right')","metadata":{"execution":{"iopub.status.busy":"2021-09-27T02:45:33.289367Z","iopub.execute_input":"2021-09-27T02:45:33.290243Z","iopub.status.idle":"2021-09-27T02:45:34.474902Z","shell.execute_reply.started":"2021-09-27T02:45:33.290174Z","shell.execute_reply":"2021-09-27T02:45:34.474037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pressur distribution (by R)\nfig, ax = plt.subplots(figsize=(14, 7))\nax = sns.distplot(train.loc[train[\"R\"] == 5, \"pressure\"], ax=ax, label=\"R=5\", bins=200)\nax = sns.distplot(train.loc[train[\"R\"] == 20, \"pressure\"], ax=ax, label=\"R=20\", bins=200)\nax = sns.distplot(train.loc[train[\"R\"] == 50, \"pressure\"], ax=ax, label=\"R=50\", bins=200)\nax.legend(loc='upper right')","metadata":{"execution":{"iopub.status.busy":"2021-09-27T02:45:34.476392Z","iopub.execute_input":"2021-09-27T02:45:34.476653Z","iopub.status.idle":"2021-09-27T02:45:36.234337Z","shell.execute_reply.started":"2021-09-27T02:45:34.476623Z","shell.execute_reply":"2021-09-27T02:45:36.233352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pressure distribution (by C)\nfig, ax = plt.subplots(figsize=(14, 7))\nax = sns.distplot(train.loc[train[\"C\"] == 10, \"pressure\"], ax=ax, label=\"C=10\", bins=200)\nax = sns.distplot(train.loc[train[\"C\"] == 20, \"pressure\"], ax=ax, label=\"C=20\", bins=200)\nax = sns.distplot(train.loc[train[\"C\"] == 50, \"pressure\"], ax=ax, label=\"C=50\", bins=200)\nax.legend(loc='upper right')","metadata":{"execution":{"iopub.status.busy":"2021-09-27T02:45:36.237619Z","iopub.execute_input":"2021-09-27T02:45:36.237893Z","iopub.status.idle":"2021-09-27T02:45:37.941966Z","shell.execute_reply.started":"2021-09-27T02:45:36.237861Z","shell.execute_reply":"2021-09-27T02:45:37.941224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_pressure_line(train, breath_ids=None, oof_col=None):\n    if not breath_ids:\n        breath_ids = train[\"breath_id\"].unique()[:12].tolist()\n        \n    fig, axes = plt.subplots(figsize=(25, 18), ncols=4, nrows=3)\n    axes = np.ravel(axes)\n    plot_cols = [\"pressure\", \"u_in\", \"u_out\"]\n    \n    if oof_col:\n        plot_cols += [oof_col]\n        \n    for b, ax in zip(breath_ids, axes):\n        _df = train[train[\"breath_id\"]==b].copy()\n        (_df\n         .set_index(\"time_step\")[plot_cols]\n         .plot(colormap='Paired',\n               ax=ax,\n               title=f\"breath_id={b}, R={_df['R'].unique()}, C={_df['C'].unique()}\", \n               linewidth=2)\n        )\n\n    fig.subplots_adjust(hspace=0.3)\n    return fig","metadata":{"execution":{"iopub.status.busy":"2021-09-27T02:45:37.943067Z","iopub.execute_input":"2021-09-27T02:45:37.943921Z","iopub.status.idle":"2021-09-27T02:45:37.951415Z","shell.execute_reply.started":"2021-09-27T02:45:37.943886Z","shell.execute_reply":"2021-09-27T02:45:37.950813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"breath_ids = list(train[\"breath_id\"].sample(12))\nfig = plot_pressure_line(train, breath_ids)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T02:45:37.952359Z","iopub.execute_input":"2021-09-27T02:45:37.95306Z","iopub.status.idle":"2021-09-27T02:45:41.332328Z","shell.execute_reply.started":"2021-09-27T02:45:37.953023Z","shell.execute_reply":"2021-09-27T02:45:41.331276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Enginnering","metadata":{}},{"cell_type":"code","source":"def aggregation(input_df, group_key, group_values, agg_methods):\n    \"\"\"ref:https://github.com/pfnet-research/xfeat/blob/master/xfeat/helper.py\"\"\"\n    new_df = []\n    for agg_method in agg_methods:\n        for col in group_values:\n            if callable(agg_method):\n                agg_method_name = agg_method.__name__\n            else:\n                agg_method_name = agg_method\n            new_col = f\"agg_{agg_method_name}_{col}_grpby_{group_key}\"\n            df_agg = (input_df[[col] + [group_key]].groupby(group_key)[[col]].agg(agg_method))\n            df_agg.columns = [new_col]\n            new_df.append(df_agg)\n            \n    _df = pd.concat(new_df, axis=1).reset_index()\n    output_df = pd.merge(input_df[[group_key]], _df, on=group_key, how=\"left\")\n    return output_df.drop(group_key, axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T02:45:41.335005Z","iopub.execute_input":"2021-09-27T02:45:41.335288Z","iopub.status.idle":"2021-09-27T02:45:41.34409Z","shell.execute_reply.started":"2021-09-27T02:45:41.335258Z","shell.execute_reply":"2021-09-27T02:45:41.343039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_raw_features(input_df):\n    cols = [\n        \"R\",\n        \"C\",\n        \"time_step\",\n        \"u_in\",\n        \"u_out\"\n    ]\n    output_df = input_df[cols].copy()\n    return output_df\n\n\ndef get_cross_features(input_df):\n    output_df = pd.DataFrame()\n    output_df[\"R+C\"] = (input_df[\"R\"].astype(str) + input_df[\"C\"].astype(str)).astype(int)\n    return output_df\n\n\ndef get_shift_grpby_breath_id_features(input_df):\n    # future feats\n    shift_times = [-1, -2, -3, -4, -5, 1, 2, 3, 4, 5]\n    group_key = \"breath_id\"\n    \n    group_values = [\"u_in\"]\n    \n    output_df = pd.DataFrame()\n    for t in shift_times:\n        _df = input_df.groupby(group_key)[group_values].shift(t)\n        _df.columns = [f'shift={t}_{col}_grpby_{group_key}' for col in group_values]\n        output_df = pd.concat([output_df, _df], axis=1)\n    return output_df\n\n\ndef get_diff_grpby_breath_id_features(input_df):\n    # future feats\n    diff_times = [-1, -2, -3, -4, -5, 1, 2, 3, 4, 5]\n    group_key = \"breath_id\"\n    group_values = [\"u_in\"]\n    \n    output_df = pd.DataFrame()\n    for t in diff_times:\n        _df = input_df.groupby(group_key)[group_values].shift(t)\n        _df.columns = [f'diff={t}_{col}_grpby_{group_key}' for col in group_values]\n        output_df = pd.concat([output_df, _df], axis=1)\n    return output_df\n\n\ndef get_cumsum_grpby_breath_id_features(input_df):\n    group_key = \"breath_id\"\n    group_values = [\"time_step\", \"u_in\", \"u_out\"]\n    \n    output_df = pd.DataFrame()\n    for group_val in group_values:\n        col_name = f\"agg_cumsum_{group_val}_grpby_{group_key}\"\n        output_df[col_name] = input_df.groupby(group_key)[group_val].cumsum()\n        \n    return output_df\n\n\ndef get_time_step_cat_features(input_df):\n    output_df = pd.DataFrame()    \n    output_df[\"time_step_cat\"] = input_df[\"time_step\"].copy()\n    output_df.loc[input_df[\"time_step\"] < 1, \"time_step_cat\"] = 0\n    output_df.loc[(1 < input_df[\"time_step\"]) & (input_df[\"time_step\"]< 1.5), \"time_step_cat\"] = 1\n    output_df.loc[1.5 < input_df[\"time_step\"], \"time_step_cat\"] = 2\n    return output_df\n\n\ndef get_breath_id_pivot_features(input_df):\n    _df = input_df[[\"breath_id\", \"time_step\", \"u_in\"]].copy()\n    _df[\"time_step_id\"] =  _df.groupby([\"breath_id\"])[\"time_step\"].rank(ascending=True)\n    _df = pd.pivot_table(_df, columns=\"time_step_id\", index=\"breath_id\", values=\"u_in\")\n    _df.columns = [f\"time_step_id={int(i):02}_u_in\" for i in _df.columns]\n    output_df = pd.merge(input_df[[\"breath_id\"]], _df, left_on=\"breath_id\", right_index=True, how=\"left\")\n    \n    use_cols = _df.columns.tolist()[:5] + _df.columns.tolist()[-5:]\n    return output_df.drop(\"breath_id\", axis=1)[use_cols]\n\n\ndef get_agg_breath_id_whole_features(whole_df):\n    # do not have to use whole_df\n    group_key = \"breath_id\"\n    group_values = [\"u_in\"]\n    agg_methods = [\"mean\", \"std\", \"median\", \"max\", \"sum\"]\n    \n    output_df = aggregation(whole_df, group_key, group_values, agg_methods)\n    \n    # z-score\n    z_col_name = _get_agg_col_name(group_key, group_values, [\"z-score\"])\n    m_col_name = _get_agg_col_name(group_key, group_values, [\"mean\"])\n    s_col_name = _get_agg_col_name(group_key, group_values, [\"std\"])\n    \n    output_df[z_col_name] = ((whole_df[group_values].values - output_df[m_col_name].values) \n                             / (output_df[m_col_name].values + 1e-8))    \n    return output_df\n\n\ndef _get_agg_col_name(group_key, group_values, agg_methods):\n    out_cols = []\n    for group_val in group_values:\n        for agg_method in agg_methods:\n            out_cols.append(f\"agg_{agg_method}_{group_val}_grpby_{group_key}\")\n    return out_cols","metadata":{"execution":{"iopub.status.busy":"2021-09-27T02:45:41.346309Z","iopub.execute_input":"2021-09-27T02:45:41.346664Z","iopub.status.idle":"2021-09-27T02:45:41.375059Z","shell.execute_reply.started":"2021-09-27T02:45:41.346621Z","shell.execute_reply":"2021-09-27T02:45:41.374211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_features(input_df):\n    output_df = pd.DataFrame()\n    funcs = [\n        get_raw_features,\n        get_cross_features,\n        get_shift_grpby_breath_id_features,\n        get_diff_grpby_breath_id_features,\n        get_time_step_cat_features,\n        get_breath_id_pivot_features,\n        get_cumsum_grpby_breath_id_features\n    ]\n    for func in funcs:\n        print(func.__name__)\n        _df = func(input_df)\n        _df = reduce_mem_usage(_df)\n        output_df = pd.concat([output_df, _df], axis=1)\n        \n    return output_df\n\n\ndef get_whole_features(train, test):\n    whole_df = pd.concat([train, test]).reset_index(drop=True)\n    output_df = pd.DataFrame()\n    funcs = [\n        get_agg_breath_id_whole_features,\n    ]\n    \n    if not funcs:\n        return pd.DataFrame(), pd.DataFrame()\n    \n    for func in funcs:\n        print(func.__name__)\n        _df = func(whole_df)\n        _df = reduce_mem_usage(_df)\n        output_df = pd.concat([output_df, _df], axis=1)\n        \n    train_x = output_df.iloc[:len(train)]\n    test_x = output_df.iloc[len(train):].reset_index(drop=True)\n    \n    return train_x, test_x\n    \n    \ndef preprocess(train, test):\n    # whole feature\n    train_x, test_x = get_whole_features(train, test)\n    train_x = pd.concat([train_x, get_features(train)], axis=1)\n    test_x = pd.concat([test_x, get_features(test)], axis=1)\n    train_y = train[Config.target_col]\n    return train_x, train_y, test_x","metadata":{"execution":{"iopub.status.busy":"2021-09-27T02:45:41.376546Z","iopub.execute_input":"2021-09-27T02:45:41.376871Z","iopub.status.idle":"2021-09-27T02:45:41.394692Z","shell.execute_reply.started":"2021-09-27T02:45:41.376833Z","shell.execute_reply":"2021-09-27T02:45:41.394059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"def lgb_metrics(y_true, y_pred, weight):\n    weight = (weight).astype(bool)\n    y_true, y_pred = y_true[weight], y_pred[weight]\n        \n    score = mean_absolute_error(y_true, y_pred)\n    return \"custom_mae\", score, False\n\n\ndef transform_lgb_custom_metric(fit_params, sample_weight=None, eval_sample_weight=None):\n    fit_params[\"eval_metric\"] = lgb_metrics\n    if sample_weight is not None:\n        fit_params[\"sample_weight\"] = sample_weight\n        \n    if eval_sample_weight is not None:\n        fit_params[\"eval_sample_weight\"] = eval_sample_weight\n        \n    return fit_params\n\n\nclass LGBM:\n    \"\"\"MyLGBMModel\"\"\"\n    def __init__(self, model_params={}, fit_params={}):\n        self.model = None\n        self.model_params = model_params\n        self.fit_params = fit_params\n\n    def build(self):\n        self.model = LGBMModel(**self.model_params)\n    \n    def fit(self, tr_x, tr_y, va_x, va_y):\n        self.fit_params = transform_lgb_custom_metric(\n            self.fit_params, \n            sample_weight=((1 - tr_x[\"u_out\"].values) + 1),\n            eval_sample_weight=[(1 - va_x[\"u_out\"].values)]\n        )\n        \n        self.model.fit(tr_x, tr_y, eval_set=[(va_x, va_y)], **self.fit_params)\n    \n    def predict(self, x):\n        preds = self.model.predict(x)\n        return preds\n\n    def save(self, filepath):\n        Util.dump(self.model, filepath + \".pkl\")\n    \n    def load(self, filepath):\n        self.model = Util.load(filepath + \".pkl\")","metadata":{"execution":{"iopub.status.busy":"2021-09-27T02:45:41.395546Z","iopub.execute_input":"2021-09-27T02:45:41.395777Z","iopub.status.idle":"2021-09-27T02:45:41.413774Z","shell.execute_reply.started":"2021-09-27T02:45:41.395752Z","shell.execute_reply":"2021-09-27T02:45:41.412787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Funcs","metadata":{}},{"cell_type":"code","source":"def metrics(y_true, y_pred, weight=None):\n    \n    if weight is not None:\n        y_true, y_pred = y_true[weight], y_pred[weight]\n        \n    score = mean_absolute_error(y_true, y_pred)\n    return score\n\n\nclass GroupKFold:\n    \"\"\"\n    GroupKFold with random shuffle with a sklearn-like structure (by katsu1110)\n    \"\"\"\n    def __init__(self, n_splits=4, shuffle=True, random_state=42):\n        self.n_splits = n_splits\n        self.shuffle = shuffle\n        self.random_state = random_state\n\n    def get_n_splits(self, X=None, y=None, group=None):\n        return self.n_splits\n\n    def split(self, X=None, y=None, group=None):\n        kf = model_selection.KFold(n_splits=self.n_splits, shuffle=self.shuffle, random_state=self.random_state)\n        unique_ids = group.unique()\n        for tr_group_idx, va_group_idx in kf.split(unique_ids):\n            # split group\n            tr_group, va_group = unique_ids[tr_group_idx], unique_ids[va_group_idx]\n            train_idx = np.where(group.isin(tr_group))[0]\n            val_idx = np.where(group.isin(va_group))[0]\n            yield train_idx, val_idx\n\n\ndef train_cv_v1(X, y, model, cv, metrics, name, directory):\n    oof = np.zeros(len(y))\n    for i_fold, (tr_idx, va_idx) in enumerate(cv):\n        if i_fold in Config.trn_fold:\n            filepath = os.path.join(directory, f\"{name}_fold{i_fold+1}\")\n            tr_x, va_x = X.iloc[tr_idx].reset_index(drop=True), X.iloc[va_idx].reset_index(drop=True)\n            tr_y, va_y = y.values[tr_idx], y.values[va_idx]        \n\n            model.build()\n            model.fit(tr_x, tr_y, va_x, va_y)\n            preds = model.predict(va_x)\n            model.save(filepath)\n            oof[va_idx] = preds\n\n            score = metrics(np.array(va_y), np.array(preds), weight=va_x[\"u_out\"] == 0)\n            logger.info(f\"{name}_fold{i_fold+1} >>> val socre:{score:.4f}\")\n        \n    score = metrics(np.array(y), oof, weight=X[\"u_out\"] == 0)\n    logger.info(f\"{name} >>> val score:{score:.4f}\")\n    return oof\n\n\ndef predict_cv_v1(X, model, name, directory):\n    preds_fold = []\n    for i_fold in range(Config.n_fold):\n        if i_fold in Config.trn_fold:\n            filepath = os.path.join(directory, f\"{name}_fold{i_fold+1}\")\n            logger.info(f\"{name}_fold{i_fold+1} inference\")\n            model.build()\n            model.load(filepath)\n            preds = model.predict(X)\n            preds_fold.append(preds)\n\n    preds = np.mean(preds_fold, axis=0)\n    return preds\n\n\ndef tree_importance(X, y, model, cv):\n    \"\"\"get importance\"\"\"\n    feature_importance_df = pd.DataFrame()\n    for i, (tr_idx, va_idx) in enumerate(cv):\n        if i in Config.trn_fold:\n            tr_x, va_x = X.iloc[tr_idx], X.iloc[va_idx]\n            tr_y, va_y = y.iloc[tr_idx], y.iloc[va_idx]\n\n            model.build()\n            model.fit(tr_x, tr_y, va_x, va_y) \n            _df = pd.DataFrame()\n            _df['feature_importance'] = model.model.feature_importances_\n            _df['column'] = X.columns\n            _df['fold'] = i + 1\n            feature_importance_df = pd.concat([feature_importance_df, _df], axis=0, ignore_index=True)\n\n    order = feature_importance_df.groupby('column') \\\n                .sum()[['feature_importance']] \\\n                .sort_values('feature_importance', ascending=False).index[:50]\n    fig, ax = plt.subplots(figsize=(12, max(4, len(order) * .2)))\n    sns.boxenplot(data=feature_importance_df, y='column', x='feature_importance', order=order, ax=ax,\n                  palette='viridis')\n    fig.tight_layout()\n    ax.grid()\n    ax.set_title('feature importance')\n    fig.tight_layout()\n    \n    feature_importance_df = (feature_importance_df.groupby('column')\n                             .sum()[['feature_importance']]\n                             .sort_values('feature_importance', ascending=False))\n    \n    return fig, feature_importance_df\n\ndef plot_regression_result(y, oof, directory):\n    fig, ax = plt.subplots(figsize=(8, 8))\n    sns.distplot(y, label='y', color='cyan', ax=ax)\n    sns.distplot(oof, label='oof', color=\"magenta\", ax=ax)\n    # sns.distplot(preds, label='preds', color=\"yellow\", ax=ax, kde=True)\n    \n    ax.legend()\n    ax.grid()\n    ax.set_title(\"regression_result\")\n    fig.tight_layout()\n    return fig\n\n\ndef gkf(X, group, n_splits=2, random_state=2021):\n    gkf = GroupKFold(n_splits=n_splits, random_state=random_state, shuffle=True)\n    return list(gkf.split(X, group=group))\n\n\ndef one_fold_gss(X, group, n_splits=0.5, random_state=2021):\n    # test_size = n_splits\n    gss = model_selection.GroupShuffleSplit(test_size=n_splits, n_splits=1, random_state=random_state)\n    return list(gss.split(X, groups=group))\n\n\ndef get_cv_strategy():\n    if isinstance(Config.n_fold, float):\n        return one_fold_gss\n    else:\n        return gkf","metadata":{"execution":{"iopub.status.busy":"2021-09-27T02:45:41.416457Z","iopub.execute_input":"2021-09-27T02:45:41.417085Z","iopub.status.idle":"2021-09-27T02:45:41.452501Z","shell.execute_reply.started":"2021-09-27T02:45:41.417034Z","shell.execute_reply":"2021-09-27T02:45:41.451519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Main","metadata":{}},{"cell_type":"code","source":"# preprocess\nprint(\"# ============= # Preprocess # ============= #\")\ntrain_x, train_y, test_x = preprocess(train, test)\nprint(train_x.shape)\n\n# feature importace \nprint(\"# ============= # Importance # ============= #\")\nfig, importance_df = tree_importance(\n    X=train_x,\n    y=train_y, \n     model=LGBM(model_params=Config.model_params,\n                fit_params=Config.fit_params),\n    cv=gkf(\n        X=train,\n        group=train[\"breath_id\"],\n        random_state=Config.seeds[0])\n)\nfig.savefig(os.path.join(EXP_FIG, \"importance.png\"), dpi=300)\n\n# feature selection\nif Config.feature_select_num:\n    cols = importance_df.index.tolist()[:Config.feature_select_num]\n    cols = cols + [\"u_out\"] if \"u_out\" not in cols else cols\n    train_x, test_x = train_x[cols], test_x[cols]\n    print(f\"after feature selection: {train_x.shape, test_x.shape}\")\n\n# training\nprint(\"# ============= # Training # ============= #\")\noof_df = pd.DataFrame()\nfor seed in Config.seeds:\n    name = f\"{Config.name_v1}-{seed}\"\n    Config.model_params[\"random_state\"] = seed\n    oof = train_cv_v1(\n        X=train_x,\n        y=train_y, \n        model=LGBM(model_params=Config.model_params, \n                   fit_params=Config.fit_params),\n        cv=gkf(\n            X=train, \n            group=train[\"breath_id\"],\n            n_splits=Config.n_fold, \n            random_state=seed),\n        \n        metrics=metrics, \n        name=name, \n        directory=EXP_MODEL)\n\n    oof_df[name] = oof\noof_df.to_csv(os.path.join(EXP_PREDS, \"oof.csv\"), index=False)\n\n# get oof score \ny_true = train[Config.target_col]\ny_pred = oof_df.mean(axis=1)\nmask = train[\"u_out\"] == 0\n\noof_score = metrics(y_true, y_pred, weight=mask)\nlogger.info(f\"{Config.name_v1} score:{oof_score:.4f}\")\n\nfig = plot_regression_result(y_true[mask], y_pred[mask], directory=EXP_FIG)\nfig.savefig(os.path.join(EXP_FIG, \"regression_result.png\"), dpi=300)\n\ntrain[\"oof\"] = y_pred\nfig = plot_pressure_line(train, breath_ids=None, oof_col=\"oof\")\nfig.savefig(os.path.join(EXP_FIG, \"oof_pressure.png\"), dpi=300)\n\n# inference\nprint(\"# ============= # Inference # ============= #\")\npreds_df = pd.DataFrame()\nfor seed in Config.seeds:\n    name = f\"{Config.name_v1}-{seed}\"\n    preds = predict_cv_v1(\n        test_x,\n         model=LGBM(),\n        name=name, \n        directory=EXP_MODEL\n    )\n    preds_df[name] = preds\n\npreds_df.to_csv(os.path.join(EXP_PREDS, \"preds.csv\"), index=False)\ntest_pred = preds_df.mean(axis=1)\n\nsample_submission['pressure'] = test_pred\nsample_submission.to_csv('submission.csv', index=False) ","metadata":{"execution":{"iopub.status.busy":"2021-09-27T02:45:41.45456Z","iopub.execute_input":"2021-09-27T02:45:41.454967Z","iopub.status.idle":"2021-09-27T02:45:52.901838Z","shell.execute_reply.started":"2021-09-27T02:45:41.454938Z","shell.execute_reply":"2021-09-27T02:45:52.901158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}