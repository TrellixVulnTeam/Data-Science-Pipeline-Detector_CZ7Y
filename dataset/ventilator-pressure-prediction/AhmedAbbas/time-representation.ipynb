{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import math\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport seaborn as sns\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset, SequentialSampler\nimport pickle\nimport pandas as pd\nimport numpy as np\nimport math\nimport pickle","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-16T07:16:59.305255Z","iopub.execute_input":"2021-10-16T07:16:59.305636Z","iopub.status.idle":"2021-10-16T07:16:59.313307Z","shell.execute_reply.started":"2021-10-16T07:16:59.305584Z","shell.execute_reply":"2021-10-16T07:16:59.31232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# please check my notebook for data preprocessing\nhttps://www.kaggle.com/ahmedmoabbas/pytorch-models-getting-data","metadata":{}},{"cell_type":"code","source":"data_path = '../input/pytorch-models-getting-data'","metadata":{"execution":{"iopub.status.busy":"2021-10-16T06:23:52.853412Z","iopub.execute_input":"2021-10-16T06:23:52.853747Z","iopub.status.idle":"2021-10-16T06:23:52.859225Z","shell.execute_reply.started":"2021-10-16T06:23:52.853708Z","shell.execute_reply":"2021-10-16T06:23:52.858179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pickle.load(open(data_path+'/preprocessed_data','rb'))\ntrain = data['train']\ntest=data['test']","metadata":{"execution":{"iopub.status.busy":"2021-10-16T06:23:54.476107Z","iopub.execute_input":"2021-10-16T06:23:54.477462Z","iopub.status.idle":"2021-10-16T06:25:10.632952Z","shell.execute_reply.started":"2021-10-16T06:23:54.477368Z","shell.execute_reply":"2021-10-16T06:25:10.632018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Data(Dataset):\n    def __init__(self, df, flip =0):\n        super().__init__()\n        self.df = df.values.tolist()\n        self.flip = flip\n    def __getitem__(self, idx):\n        row = self.df[idx]\n        tensors = torch.as_tensor(row[:-2], dtype = torch.float)\n        segment = torch.as_tensor(row[-3], dtype=torch.long)\n        target = torch.as_tensor(row[-2], dtype=torch.float)\n        if np.random.rand() < self.flip:\n            tensors = tensors.flip(-1)\n        return {\n            'tensors':tensors,\n            'segment':segment,\n            'target':target,\n        }\n    def __len__(self):\n        return len(self.df)","metadata":{"execution":{"iopub.status.busy":"2021-10-16T07:00:43.704095Z","iopub.execute_input":"2021-10-16T07:00:43.704404Z","iopub.status.idle":"2021-10-16T07:00:43.714367Z","shell.execute_reply.started":"2021-10-16T07:00:43.704371Z","shell.execute_reply":"2021-10-16T07:00:43.713683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# one of the methods by with we can feed our models by benefitable demintion of time is to transform the timesteps to learnable vector that can learn the meaning of the time \n# please check this paper  \nhttps://openreview.net/pdf?id=rklklCVYvB","metadata":{}},{"cell_type":"code","source":"# we can use either the Sine or the Cosine funtion\nclass TimeToVector(nn.Module):\n    def __init__(self, n_inputs, seq_len, n_outputs, act=torch.cos):\n        super(TimeToVector, self).__init__()\n        self.n_inputs = n_inputs\n        self.n_outputs = n_outputs\n        self.seq_len = seq_len\n        self.w_weiht = nn.parameter.Parameter(torch.randn(self.n_inputs, n_outputs-1))\n        self.b_weight = nn.parameter.Parameter(torch.randn(self.seq_len, n_outputs-1))\n        \n        self.w_bias = nn.parameter.Parameter(torch.randn(self.n_inputs, 1))\n        self.b_bias = nn.parameter.Parameter(torch.randn(self.seq_len, 1))\n        self.act  = act\n    def forward(self, inputs):\n        bias = torch.matmul(inputs, self.w_bias) + self.b_bias\n        weights = self.act(torch.matmul(inputs, self.w_weiht) + self.b_weight)\n        return torch.cat([weights, bias], -1)","metadata":{"execution":{"iopub.status.busy":"2021-10-16T06:59:17.504105Z","iopub.execute_input":"2021-10-16T06:59:17.505102Z","iopub.status.idle":"2021-10-16T06:59:17.51678Z","shell.execute_reply.started":"2021-10-16T06:59:17.50502Z","shell.execute_reply":"2021-10-16T06:59:17.515397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### we can concatenate this time vector with our features befor feeding them to the model or in any stage of the model","metadata":{}},{"cell_type":"markdown","source":"### Here is an exapmle of encoding our features concatenated with the time vector","metadata":{}},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, n_inputs, d_emb, d_time):\n        super(Encoder, self).__init__()\n        self.act = nn.ReLU()\n        self.rnn = nn.LSTM(n_inputs + d_time , d_emb//2, 3,batch_first=True, bidirectional=True)\n        self.time = TimeToVector(35,80,d_time)\n    def forward(self, input):\n        time = self.time(input)\n        input = torch.cat([input, time], -1)\n        out, hidden = self.rnn(input)\n        return out","metadata":{"execution":{"iopub.status.busy":"2021-10-16T07:13:46.378054Z","iopub.execute_input":"2021-10-16T07:13:46.378648Z","iopub.status.idle":"2021-10-16T07:13:46.386844Z","shell.execute_reply.started":"2021-10-16T07:13:46.378594Z","shell.execute_reply":"2021-10-16T07:13:46.385706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#here we will add 10 extra features as time representation\nencoder = Encoder(n_inputs=35, d_emb=512, d_time=10)","metadata":{"execution":{"iopub.status.busy":"2021-10-16T07:13:47.433074Z","iopub.execute_input":"2021-10-16T07:13:47.433388Z","iopub.status.idle":"2021-10-16T07:13:47.473503Z","shell.execute_reply.started":"2021-10-16T07:13:47.433357Z","shell.execute_reply":"2021-10-16T07:13:47.472268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loader = DataLoader(Data(train), 64)\nfeatures = next(iter(loader))['tensors'].permute(0,2,1)\nfeatures.shape","metadata":{"execution":{"iopub.status.busy":"2021-10-16T07:14:02.509202Z","iopub.execute_input":"2021-10-16T07:14:02.509512Z","iopub.status.idle":"2021-10-16T07:14:03.065499Z","shell.execute_reply.started":"2021-10-16T07:14:02.50948Z","shell.execute_reply":"2021-10-16T07:14:03.064667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#here is our outputs that we can feed to the next stage of the model\nencoder(features).shape","metadata":{"execution":{"iopub.status.busy":"2021-10-16T07:15:55.883894Z","iopub.execute_input":"2021-10-16T07:15:55.885061Z","iopub.status.idle":"2021-10-16T07:15:56.491644Z","shell.execute_reply.started":"2021-10-16T07:15:55.884997Z","shell.execute_reply":"2021-10-16T07:15:56.490657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Another way for step representation is using the position embedding like what transformers models use to add to the words embedding to give each word extra meaning depends on its position in the sentence \n","metadata":{"execution":{"iopub.status.busy":"2021-10-16T07:07:31.096037Z","iopub.execute_input":"2021-10-16T07:07:31.096406Z","iopub.status.idle":"2021-10-16T07:07:31.102349Z","shell.execute_reply.started":"2021-10-16T07:07:31.096371Z","shell.execute_reply":"2021-10-16T07:07:31.100909Z"}}},{"cell_type":"code","source":"class PositionalEncoder(nn.Module):\n    def __init__(self, d_model: int, max_len: int = 80):\n        super().__init__()\n        position = torch.arange(max_len).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe = torch.zeros(1, max_len, d_model)\n        pe[0, :, 0::2] = torch.sin(position * div_term)\n        pe[0, :, 1::2] = torch.cos(position * div_term)\n        self.register_buffer('pe', pe)\n    def forward(self, x):\n        x = x + self.pe[:x.size(0)]\n        return x","metadata":{"execution":{"iopub.status.busy":"2021-10-16T07:14:18.690909Z","iopub.execute_input":"2021-10-16T07:14:18.691957Z","iopub.status.idle":"2021-10-16T07:14:18.701179Z","shell.execute_reply.started":"2021-10-16T07:14:18.691909Z","shell.execute_reply":"2021-10-16T07:14:18.699846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Encoder2(nn.Module):\n    def __init__(self, n_inputs, d_emb):\n        super(Encoder2, self).__init__()\n        self.d_emb = d_emb\n        self.act = nn.ReLU(True)\n        self.position = PositionalEncoder(d_emb)\n        self.rnn = nn.LSTM(n_inputs, d_emb//2, 3,batch_first=True, bidirectional=True)\n        self.norm = nn.LayerNorm(d_emb)\n        \n    def forward(self, input):\n        out, hidden = self.rnn(input)\n        out = out * math.sqrt(self.d_emb) # here we giving the main features -or the main embedding- \n        #extra importance befor adding the position embedding\n        out = self.position(out)\n        out =self.norm(out)\n        return out","metadata":{"execution":{"iopub.status.busy":"2021-10-16T07:20:00.659811Z","iopub.execute_input":"2021-10-16T07:20:00.660125Z","iopub.status.idle":"2021-10-16T07:20:00.669858Z","shell.execute_reply.started":"2021-10-16T07:20:00.66009Z","shell.execute_reply":"2021-10-16T07:20:00.668698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#here is the output before sending to the next stage\nencoder2 = Encoder2(35, 512)\nencoder2(features).shape","metadata":{"execution":{"iopub.status.busy":"2021-10-16T07:20:41.684473Z","iopub.execute_input":"2021-10-16T07:20:41.685762Z","iopub.status.idle":"2021-10-16T07:20:42.274827Z","shell.execute_reply.started":"2021-10-16T07:20:41.685679Z","shell.execute_reply":"2021-10-16T07:20:42.273951Z"},"trusted":true},"execution_count":null,"outputs":[]}]}