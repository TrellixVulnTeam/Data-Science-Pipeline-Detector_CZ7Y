{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebook trained a bidirectional LSTM-based regression model within feature important featuring, which keeps the most important features from [here][1]. The training procedure was fastly proceeded on TPU devices, and used Huber Loss instead of MAE. The LB result is 0.304.  \n\n[1]: https://www.kaggle.com/cdeotte/lstm-feature-importance","metadata":{}},{"cell_type":"markdown","source":"# Load Libraries and Data ","metadata":{}},{"cell_type":"code","source":"import numpy as np, os\nimport pandas as pd\nimport random\n\nimport optuna\n\n# https://www.kaggle.com/c/ventilator-pressure-prediction/discussion/274717 \nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\n\nimport tensorflow as tf\nfrom tensorflow import keras\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.callbacks import LearningRateScheduler, ReduceLROnPlateau\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\n\nfrom sklearn.metrics import mean_absolute_error as mae\nfrom sklearn.preprocessing import RobustScaler, normalize\nfrom sklearn.model_selection import train_test_split, GroupKFold, KFold\n\nfrom IPython.display import display","metadata":{"_uuid":"e331dbcc-0346-4019-9ff6-b890154a878b","_cell_guid":"3e5a0bd1-3e22-4b2c-a565-68985e55f95e","collapsed":false,"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-10-28T03:46:39.372827Z","iopub.execute_input":"2021-10-28T03:46:39.373274Z","iopub.status.idle":"2021-10-28T03:46:47.684127Z","shell.execute_reply.started":"2021-10-28T03:46:39.373164Z","shell.execute_reply":"2021-10-28T03:46:47.683046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    tf.random.set_seed(seed)","metadata":{"execution":{"iopub.status.busy":"2021-10-28T03:46:47.68577Z","iopub.execute_input":"2021-10-28T03:46:47.686085Z","iopub.status.idle":"2021-10-28T03:46:47.691893Z","shell.execute_reply.started":"2021-10-28T03:46:47.686046Z","shell.execute_reply":"2021-10-28T03:46:47.690939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DEBUG = False\nTRAIN_MODEL = True\nINFER_TEST = True\nONE_FOLD_ONLY = False\nCOMPUTE_LSTM_IMPORTANCE = False\n\ntrain = pd.read_csv('../input/ventilator-pressure-prediction/train.csv')\npressure_values = np.sort( train.pressure.unique() )\ntest = pd.read_csv('../input/ventilator-pressure-prediction/test.csv')\nsubmission = pd.read_csv('../input/ventilator-pressure-prediction/sample_submission.csv')\n\nif DEBUG:\n    train = train[:80*100]\n    test = test[:80*100]\n    submission = submission[:8000]","metadata":{"execution":{"iopub.status.busy":"2021-10-28T03:46:47.693711Z","iopub.execute_input":"2021-10-28T03:46:47.694251Z","iopub.status.idle":"2021-10-28T03:47:06.169192Z","shell.execute_reply.started":"2021-10-28T03:46:47.694199Z","shell.execute_reply":"2021-10-28T03:47:06.16846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to get hardware strategy\ndef get_hardware_strategy():\n    try:\n        # TPU detection. No parameters necessary if TPU_NAME environment variable is\n        # set: this is always the case on Kaggle.\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        print('Running on TPU ', tpu.master())\n    except ValueError:\n        tpu = None\n\n    if tpu:\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n        tf.config.optimizer.set_jit(True)\n    else:\n        # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n        strategy = tf.distribute.get_strategy()\n\n    print(\"REPLICAS: \", strategy.num_replicas_in_sync)\n    return tpu, strategy\n\ntpu, strategy = get_hardware_strategy()","metadata":{"execution":{"iopub.status.busy":"2021-10-28T03:47:06.17117Z","iopub.execute_input":"2021-10-28T03:47:06.17169Z","iopub.status.idle":"2021-10-28T03:47:12.361185Z","shell.execute_reply.started":"2021-10-28T03:47:06.171656Z","shell.execute_reply":"2021-10-28T03:47:12.359966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Engineer Features","metadata":{}},{"cell_type":"code","source":"def add_features(df):\n    df['area'] = df['time_step'] * df['u_in']\n    df['area'] = df.groupby('breath_id')['area'].cumsum()\n    \n    df['u_in_cumsum'] = (df['u_in']).groupby(df['breath_id']).cumsum()\n    \n    df['u_in_lag1'] = df.groupby('breath_id')['u_in'].shift(1)\n    df['u_in_lag_back1'] = df.groupby('breath_id')['u_in'].shift(-1)\n    df['u_in_lag2'] = df.groupby('breath_id')['u_in'].shift(2)\n    df['u_in_lag_back2'] = df.groupby('breath_id')['u_in'].shift(-2)\n    df['u_in_lag3'] = df.groupby('breath_id')['u_in'].shift(3)\n    df['u_in_lag_back3'] = df.groupby('breath_id')['u_in'].shift(-3)\n    df['u_in_lag4'] = df.groupby('breath_id')['u_in'].shift(4)\n    df['u_in_lag_back4'] = df.groupby('breath_id')['u_in'].shift(-4)\n    df['u_out_lag3'] = df.groupby('breath_id')['u_out'].shift(3)\n    df['u_out_lag2'] = df.groupby('breath_id')['u_out'].shift(2)\n    df = df.fillna(0)\n    \n    df['breath_id__u_in__max'] = df.groupby(['breath_id'])['u_in'].transform('max')\n    \n    df['u_in_diff1'] = df['u_in'] - df['u_in_lag1']\n    df['u_in_diff2'] = df['u_in'] - df['u_in_lag2']\n    df['u_out_diff2'] = df['u_out'] - df['u_out_lag2']\n    \n    df['breath_id__u_in__diffmax'] = df.groupby(['breath_id'])['u_in'].transform('max') - df['u_in']\n    df['breath_id__u_in__diffmean'] = df.groupby(['breath_id'])['u_in'].transform('mean') - df['u_in']\n    \n    df['breath_id__u_in__diffmax'] = df.groupby(['breath_id'])['u_in'].transform('max') - df['u_in']\n    df['breath_id__u_in__diffmean'] = df.groupby(['breath_id'])['u_in'].transform('mean') - df['u_in']\n    \n    df['u_in_diff3'] = df['u_in'] - df['u_in_lag3']\n    df['u_out_diff3'] = df['u_out'] - df['u_out_lag3']\n    df['u_in_diff4'] = df['u_in'] - df['u_in_lag4']\n    df['cross']= df['u_in']*df['u_out']\n    \n    df['R'] = df['R'].astype(str)\n    df['C'] = df['C'].astype(str)\n    df['R__C'] = df[\"R\"].astype(str) + '__' + df[\"C\"].astype(str)\n    df = pd.get_dummies(df)\n    return df\n\ntrain = add_features(train)\ntest = add_features(test)\n\nprint('Train dataframe shape',train.shape)\ntrain.head()","metadata":{"_uuid":"dc41dbf2-f199-4b9d-bbd9-bf6084162b47","_cell_guid":"13a36b46-7067-4b29-aad3-7e3b15e8415b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-10-28T03:47:12.362809Z","iopub.execute_input":"2021-10-28T03:47:12.36314Z","iopub.status.idle":"2021-10-28T03:48:05.92886Z","shell.execute_reply.started":"2021-10-28T03:47:12.363087Z","shell.execute_reply":"2021-10-28T03:48:05.927777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"targets = train[['pressure']].to_numpy().reshape(-1, 80)\ntrain.drop(['pressure', 'id', 'breath_id', 'time_step' , 'u_out', 'u_out_lag2', 'u_out_lag3'], axis=1, inplace=True)\ntest = test.drop(['id', 'breath_id', 'time_step', 'u_out', 'u_out_lag2', 'u_out_lag3'], axis=1)\n\nCOLS = list(train.columns)\nprint('Number of feature columns =', len(COLS) )\n\nRS = RobustScaler()\ntrain = RS.fit_transform(train)\ntest = RS.transform(test)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-10-28T03:48:05.930151Z","iopub.execute_input":"2021-10-28T03:48:05.930392Z","iopub.status.idle":"2021-10-28T03:48:16.774466Z","shell.execute_reply.started":"2021-10-28T03:48:05.930363Z","shell.execute_reply":"2021-10-28T03:48:16.773615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(targets.shape)\nprint(train.shape)","metadata":{"execution":{"iopub.status.busy":"2021-10-28T03:48:16.777863Z","iopub.execute_input":"2021-10-28T03:48:16.778663Z","iopub.status.idle":"2021-10-28T03:48:16.783327Z","shell.execute_reply.started":"2021-10-28T03:48:16.778627Z","shell.execute_reply":"2021-10-28T03:48:16.782379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train.reshape(-1, 80, train.shape[-1])\ntest = test.reshape(-1, 80, train.shape[-1])","metadata":{"execution":{"iopub.status.busy":"2021-10-28T03:48:16.784771Z","iopub.execute_input":"2021-10-28T03:48:16.785045Z","iopub.status.idle":"2021-10-28T03:48:16.795525Z","shell.execute_reply.started":"2021-10-28T03:48:16.785017Z","shell.execute_reply":"2021-10-28T03:48:16.794584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train.shape)\nprint(test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-10-28T03:48:16.796713Z","iopub.execute_input":"2021-10-28T03:48:16.797079Z","iopub.status.idle":"2021-10-28T03:48:16.809187Z","shell.execute_reply.started":"2021-10-28T03:48:16.797038Z","shell.execute_reply":"2021-10-28T03:48:16.807989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Compute LSTM Feature Importance\nAfter we train (or load) each fold model, we will compute LSTM feature importance for all of our features. We do this with a for-loop of size `N` where `N` is the number of features we have. For each feature we wish to evaluate, we infer our OOF with that feature column randomly shuffled. If this feature column is important to our LSTM model, then the OOF MAE will become worse for that for-loop step. After our for-loop, we display bars equal to the size of how much MAE worsened without each feature, which is the importance of each feature.\n\nNote that computing LSTM feature importance after each fold will add about 1 minute for every 5 features.","metadata":{}},{"cell_type":"code","source":"def get_model():\n    with strategy.scope():\n        model = keras.models.Sequential([\n            keras.layers.Input(shape=train.shape[-2:]),\n            keras.layers.Dense(128, activation='selu'),\n            keras.layers.Bidirectional(keras.layers.LSTM(1024, return_sequences=True)),\n            keras.layers.Bidirectional(keras.layers.LSTM(512, return_sequences=True)),\n            keras.layers.Bidirectional(keras.layers.LSTM(256, return_sequences=True)),\n            keras.layers.Bidirectional(keras.layers.LSTM(128, return_sequences=True)),\n            keras.layers.Dense(128, activation='selu'),\n            keras.layers.Dense(1)\n        ])\n        model.compile(optimizer=\"adam\", loss=\"huber_loss\")\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-10-28T03:48:37.717871Z","iopub.execute_input":"2021-10-28T03:48:37.71818Z","iopub.status.idle":"2021-10-28T03:48:37.727238Z","shell.execute_reply.started":"2021-10-28T03:48:37.718151Z","shell.execute_reply":"2021-10-28T03:48:37.726242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EPOCH = 150\nBATCH_SIZE = 1204*strategy.num_replicas_in_sync\nNUM_FOLDS = 10\n\nkf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=2021)\ntest_preds = []\n\nfor fold, (train_idx, test_idx) in enumerate(kf.split(train, targets)):\n\n    print('-'*15, '>', f'Fold {fold+1}', '<', '-'*15)\n    X_train, X_valid = train[train_idx], train[test_idx]\n    y_train, y_valid = targets[train_idx], targets[test_idx]\n\n    checkpoint_filepath = f\"folds{fold}.hdf5\"\n    if tpu:\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        \n    K.clear_session()\n    seed_everything(20211710)\n    model = get_model()\n    lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=10, verbose=1)\n    es = EarlyStopping(monitor=\"val_loss\", patience=60, verbose=1, mode=\"min\", restore_best_weights=True)\n    sv = keras.callbacks.ModelCheckpoint(\n        checkpoint_filepath, monitor='val_loss', verbose=1, save_best_only=True,\n        save_weights_only=False, mode='auto', save_freq='epoch',\n        options=None\n    )\n    model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=EPOCH, batch_size=BATCH_SIZE, callbacks=[lr, es, sv])\n\n    print(' Predicting test data...')\n    test_preds.append(model.predict(test,verbose=0).squeeze().reshape(-1, 1).squeeze())","metadata":{"execution":{"iopub.status.busy":"2021-10-28T03:48:38.614019Z","iopub.execute_input":"2021-10-28T03:48:38.614337Z"},"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Write Submission CSV\nIf we set `INFER_TEST` boolean to `True` in first code block, then we will write `submission.csv` below. Note that `submission.csv` will be either all folds ensemble or just the first fold depending on the variable `ONE_FOLD_ONLY`.","metadata":{}},{"cell_type":"code","source":"if INFER_TEST:\n    PRESSURE_MIN = pressure_values[0]\n    PRESSURE_MAX = pressure_values[-1]\n    PRESSURE_STEP = pressure_values[1] - pressure_values[0]\n\n    # NAME POSTFIX\n    postfix = ''\n    if ONE_FOLD_ONLY: \n        NUM_FOLDS = 1\n        postfix = '_fold_1'\n        \n    # ENSEMBLE FOLDS WITH MEAN\n    submission[\"pressure\"] = sum(test_preds)/NUM_FOLDS\n    submission.to_csv(f'submission_mean{postfix}.csv', index=False)\n\n    # ENSEMBLE FOLDS WITH MEDIAN\n    submission[\"pressure\"] = np.median(np.vstack(test_preds),axis=0)\n    submission.to_csv(f'submission_median{postfix}.csv', index=False)\n\n    # ENSEMBLE FOLDS WITH MEDIAN AND ROUND PREDICTIONS\n    submission[\"pressure\"] =\\\n        np.round( (submission.pressure - PRESSURE_MIN)/PRESSURE_STEP ) * PRESSURE_STEP + PRESSURE_MIN\n    submission.pressure = np.clip(submission.pressure, PRESSURE_MIN, PRESSURE_MAX)\n    submission.to_csv(f'submission_median_round{postfix}.csv', index=False)\n    \n    # DISPLAY SUBMISSION.CSV\n    print(f'khoa_submission{postfix}.csv head')\n    display( submission.head() )","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]}]}