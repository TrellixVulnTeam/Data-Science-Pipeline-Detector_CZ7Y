{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebook is inspired by recent notebooks from [Zhangxin](https://www.kaggle.com/tenffe/finetune-of-tensorflow-bidirectional-lstm) and [Chris Deotte](https://www.kaggle.com/cdeotte/ensemble-folds-with-median-0-153). Since it is important to dicretize the output, I propose a custom TensorFlow layer that will automatically do that for you. The optimization will therefore happen under contrains that the output should be bounded and discrete as the inputed.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.callbacks import LearningRateScheduler, ReduceLROnPlateau\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\nfrom tensorflow.keras.callbacks import Callback\nimport tensorflow.keras.backend as K\n\nfrom sklearn.metrics import mean_absolute_error as mae\nfrom sklearn.preprocessing import RobustScaler, normalize\nfrom sklearn.model_selection import train_test_split, GroupKFold, KFold","metadata":{"execution":{"iopub.status.busy":"2021-10-04T12:12:57.087908Z","iopub.execute_input":"2021-10-04T12:12:57.08925Z","iopub.status.idle":"2021-10-04T12:13:03.592437Z","shell.execute_reply.started":"2021-10-04T12:12:57.08914Z","shell.execute_reply":"2021-10-04T12:13:03.591821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DEBUG = False\n\ntrain = pd.read_csv('../input/ventilator-pressure-prediction/train.csv')\ntest = pd.read_csv('../input/ventilator-pressure-prediction/test.csv')\nsubmission = pd.read_csv('../input/ventilator-pressure-prediction/sample_submission.csv')\n\nif DEBUG:\n    train = train[:80*1000]","metadata":{"execution":{"iopub.status.busy":"2021-10-04T12:13:03.593602Z","iopub.execute_input":"2021-10-04T12:13:03.594135Z","iopub.status.idle":"2021-10-04T12:13:19.983552Z","shell.execute_reply.started":"2021-10-04T12:13:03.5941Z","shell.execute_reply":"2021-10-04T12:13:19.982818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train","metadata":{"execution":{"iopub.status.busy":"2021-10-04T12:13:19.984686Z","iopub.execute_input":"2021-10-04T12:13:19.985511Z","iopub.status.idle":"2021-10-04T12:13:20.016227Z","shell.execute_reply.started":"2021-10-04T12:13:19.985477Z","shell.execute_reply":"2021-10-04T12:13:20.015303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_pressure = sorted(train.pressure.unique())\nPRESSURE_MIN = np.min(all_pressure)\nPRESSURE_MAX = np.max(all_pressure)\nPRESSURE_STEP = all_pressure[1] - all_pressure[0]","metadata":{"execution":{"iopub.status.busy":"2021-10-04T12:13:20.018682Z","iopub.execute_input":"2021-10-04T12:13:20.019043Z","iopub.status.idle":"2021-10-04T12:13:20.099561Z","shell.execute_reply.started":"2021-10-04T12:13:20.019002Z","shell.execute_reply":"2021-10-04T12:13:20.098666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def add_features(df):\n    df['area'] = df['time_step'] * df['u_in']\n    df['area'] = df.groupby('breath_id')['area'].cumsum()\n    \n    df['u_in_cumsum'] = (df['u_in']).groupby(df['breath_id']).cumsum()\n    \n    df['u_in_lag1'] = df.groupby('breath_id')['u_in'].shift(1)\n    df['u_out_lag1'] = df.groupby('breath_id')['u_out'].shift(1)\n    df['u_in_lag_back1'] = df.groupby('breath_id')['u_in'].shift(-1)\n    df['u_out_lag_back1'] = df.groupby('breath_id')['u_out'].shift(-1)\n    \n    df['u_in_lag2'] = df.groupby('breath_id')['u_in'].shift(2)\n    df['u_out_lag2'] = df.groupby('breath_id')['u_out'].shift(2)\n    df['u_in_lag_back2'] = df.groupby('breath_id')['u_in'].shift(-2)\n    df['u_out_lag_back2'] = df.groupby('breath_id')['u_out'].shift(-2)\n    \n    df['u_in_lag3'] = df.groupby('breath_id')['u_in'].shift(3)\n    df['u_out_lag3'] = df.groupby('breath_id')['u_out'].shift(3)\n    df['u_in_lag_back3'] = df.groupby('breath_id')['u_in'].shift(-3)\n    df['u_out_lag_back3'] = df.groupby('breath_id')['u_out'].shift(-3)\n    \n    df['u_in_lag4'] = df.groupby('breath_id')['u_in'].shift(4)\n    df['u_out_lag4'] = df.groupby('breath_id')['u_out'].shift(4)\n    df['u_in_lag_back4'] = df.groupby('breath_id')['u_in'].shift(-4)\n    df['u_out_lag_back4'] = df.groupby('breath_id')['u_out'].shift(-4)\n    df = df.fillna(0)\n    \n    df['breath_id__u_in__max'] = df.groupby(['breath_id'])['u_in'].transform('max')\n    df['breath_id__u_out__max'] = df.groupby(['breath_id'])['u_out'].transform('max')\n    \n    df['u_in_diff1'] = df['u_in'] - df['u_in_lag1']\n    df['u_out_diff1'] = df['u_out'] - df['u_out_lag1']\n    df['u_in_diff2'] = df['u_in'] - df['u_in_lag2']\n    df['u_out_diff2'] = df['u_out'] - df['u_out_lag2']\n    \n    df['breath_id__u_in__diffmax'] = df.groupby(['breath_id'])['u_in'].transform('max') - df['u_in']\n    df['breath_id__u_in__diffmean'] = df.groupby(['breath_id'])['u_in'].transform('mean') - df['u_in']\n    \n    df['breath_id__u_in__diffmax'] = df.groupby(['breath_id'])['u_in'].transform('max') - df['u_in']\n    df['breath_id__u_in__diffmean'] = df.groupby(['breath_id'])['u_in'].transform('mean') - df['u_in']\n    \n    df['u_in_diff3'] = df['u_in'] - df['u_in_lag3']\n    df['u_out_diff3'] = df['u_out'] - df['u_out_lag3']\n    df['u_in_diff4'] = df['u_in'] - df['u_in_lag4']\n    df['u_out_diff4'] = df['u_out'] - df['u_out_lag4']\n    df['cross']= df['u_in']*df['u_out']\n    df['cross2']= df['time_step']*df['u_out']\n    \n    df['R'] = df['R'].astype(str)\n    df['C'] = df['C'].astype(str)\n    df['R__C'] = df[\"R\"].astype(str) + '__' + df[\"C\"].astype(str)\n    df = pd.get_dummies(df)\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-10-04T12:13:20.10087Z","iopub.execute_input":"2021-10-04T12:13:20.101249Z","iopub.status.idle":"2021-10-04T12:13:20.122957Z","shell.execute_reply.started":"2021-10-04T12:13:20.101216Z","shell.execute_reply":"2021-10-04T12:13:20.122367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = add_features(train)\ntest = add_features(test)","metadata":{"execution":{"iopub.status.busy":"2021-10-04T12:13:20.124158Z","iopub.execute_input":"2021-10-04T12:13:20.124605Z","iopub.status.idle":"2021-10-04T12:14:11.973916Z","shell.execute_reply.started":"2021-10-04T12:13:20.124571Z","shell.execute_reply":"2021-10-04T12:14:11.972958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"targets = train[['pressure']].to_numpy().reshape(-1, 80)\ntrain.drop(['pressure', 'id', 'breath_id'], axis=1, inplace=True)\ntest = test.drop(['id', 'breath_id'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-10-04T12:14:11.975393Z","iopub.execute_input":"2021-10-04T12:14:11.976227Z","iopub.status.idle":"2021-10-04T12:14:15.413838Z","shell.execute_reply.started":"2021-10-04T12:14:11.976181Z","shell.execute_reply":"2021-10-04T12:14:15.412876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"RS = RobustScaler()\ntrain = RS.fit_transform(train)\ntest = RS.transform(test)","metadata":{"execution":{"iopub.status.busy":"2021-10-04T12:14:15.415994Z","iopub.execute_input":"2021-10-04T12:14:15.416382Z","iopub.status.idle":"2021-10-04T12:14:26.414608Z","shell.execute_reply.started":"2021-10-04T12:14:15.416341Z","shell.execute_reply":"2021-10-04T12:14:26.413676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train.reshape(-1, 80, train.shape[-1]).astype(np.float32)\ntest = test.reshape(-1, 80, train.shape[-1]).astype(np.float32)\ntargets = targets.astype(np.float32)","metadata":{"execution":{"iopub.status.busy":"2021-10-04T12:14:26.416174Z","iopub.execute_input":"2021-10-04T12:14:26.416506Z","iopub.status.idle":"2021-10-04T12:14:27.256145Z","shell.execute_reply.started":"2021-10-04T12:14:26.416456Z","shell.execute_reply":"2021-10-04T12:14:27.255305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The following custom layer will rescale the output to fit the discrete steps in values to be found in the target. In such a way, you will force your network to learn how to provide outputs that do not need further post processing.","metadata":{}},{"cell_type":"markdown","source":"Please notice the custom rounding **round_with_gradients** function since tf.round has no gradients and it won't be differentiable.","metadata":{}},{"cell_type":"code","source":"@tf.custom_gradient\ndef round_with_gradients(x):\n    def grad(dy):\n        return dy\n    return tf.round(x), grad\n\nclass ScaleLayer(tf.keras.layers.Layer):\n    def __init__(self):\n        super(ScaleLayer, self).__init__()\n        self.min = tf.constant(PRESSURE_MIN, dtype=np.float32)\n        self.max = tf.constant(PRESSURE_MAX, dtype=np.float32)\n        self.step = tf.constant(PRESSURE_STEP, dtype=np.float32)\n\n    def call(self, inputs):\n        steps = tf.math.divide(tf.math.add(inputs, -self.min), self.step)\n        int_steps = round_with_gradients(steps)\n        rescaled_steps = tf.math.add(tf.math.multiply(int_steps, self.step), self.min)\n        clipped = tf.clip_by_value(rescaled_steps, self.min, self.max)\n        return clipped","metadata":{"execution":{"iopub.status.busy":"2021-10-04T12:14:27.258865Z","iopub.execute_input":"2021-10-04T12:14:27.259517Z","iopub.status.idle":"2021-10-04T12:14:27.268616Z","shell.execute_reply.started":"2021-10-04T12:14:27.259471Z","shell.execute_reply":"2021-10-04T12:14:27.267678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EPOCH = 300\nBATCH_SIZE = 1024\nNUM_FOLDS = 10","metadata":{"execution":{"iopub.status.busy":"2021-10-04T12:14:27.269549Z","iopub.execute_input":"2021-10-04T12:14:27.269755Z","iopub.status.idle":"2021-10-04T12:14:27.28036Z","shell.execute_reply.started":"2021-10-04T12:14:27.269732Z","shell.execute_reply":"2021-10-04T12:14:27.279681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# detect and init the TPU\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n\n# instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\nwith tpu_strategy.scope():\n    \n    K = keras.backend\n\n    def create_model():\n        inputs = keras.layers.Input(shape=train.shape[-2:])\n        x = inputs\n        for units in [1024, 512, 256, 128]:\n            x = keras.layers.Bidirectional(keras.layers.LSTM(units, return_sequences=True))(x)\n        x = keras.layers.Dense(128, activation='selu')(x)\n        outputs = keras.layers.Dense(1)(x)\n        outputs = ScaleLayer()(outputs)\n        \n        model  = keras.Model(inputs=inputs, outputs=outputs)\n        model.compile(optimizer=\"adam\", loss='mae') \n        return model\n    \n    kf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=1970)\n    test_preds = []\n    for fold, (train_idx, test_idx) in enumerate(kf.split(train, targets)):\n        print('-'*15, '>', f'Fold {fold+1}', '<', '-'*15)\n        X_train, X_valid = train[train_idx], train[test_idx]\n        y_train, y_valid = targets[train_idx], targets[test_idx]\n        \n        model = create_model()\n\n        lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=10, verbose=1)\n        es = EarlyStopping(monitor=\"val_loss\", patience=60, verbose=1, \n                           mode=\"min\", restore_best_weights=True)\n    \n        checkpoint_filepath = f\"folds{fold}.hdf5\"\n        sv = keras.callbacks.ModelCheckpoint(\n            checkpoint_filepath, monitor='val_loss', verbose=1, save_best_only=True,\n            save_weights_only=False, mode='auto', save_freq='epoch',\n            options=None\n        )\n\n        model.fit(X_train, y_train, validation_data=(X_valid, y_valid), \n                  epochs=EPOCH, batch_size=BATCH_SIZE, callbacks=[lr, es, sv])\n        \n        test_preds.append(model.predict(test, batch_size=BATCH_SIZE, verbose=2)\n                          .squeeze().reshape(-1, 1).squeeze())","metadata":{"execution":{"iopub.status.busy":"2021-10-04T12:14:27.281537Z","iopub.execute_input":"2021-10-04T12:14:27.281816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission[\"pressure\"] = np.median(np.vstack(test_preds), axis=0)\nsubmission.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}