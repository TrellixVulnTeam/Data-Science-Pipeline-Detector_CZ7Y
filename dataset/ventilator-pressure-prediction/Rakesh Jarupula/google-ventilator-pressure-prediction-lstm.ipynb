{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Google - VPP ","metadata":{}},{"cell_type":"code","source":"import gc\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.metrics import mean_absolute_error\n\nimport tensorflow as tf\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.layers import Dense, Dropout, Input\nfrom tensorflow.keras.layers import Concatenate, LSTM, GRU\nfrom tensorflow.keras.layers import Bidirectional, Multiply\n\nfrom scipy.signal import hilbert, chirp\nfrom scipy.signal import blackman\nfrom scipy.fft import fft, fftfreq\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nnp.random.seed(42)\ntf.random.set_seed(42)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-30T04:01:41.599847Z","iopub.execute_input":"2021-10-30T04:01:41.600352Z","iopub.status.idle":"2021-10-30T04:01:48.265025Z","shell.execute_reply.started":"2021-10-30T04:01:41.600258Z","shell.execute_reply":"2021-10-30T04:01:48.264352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/ventilator-pressure-prediction/train.csv')\ntest = pd.read_csv('../input/ventilator-pressure-prediction/test.csv')\nsample_sub = pd.read_csv('../input/ventilator-pressure-prediction/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-10-30T04:01:48.266237Z","iopub.execute_input":"2021-10-30T04:01:48.266817Z","iopub.status.idle":"2021-10-30T04:02:03.50425Z","shell.execute_reply.started":"2021-10-30T04:01:48.266782Z","shell.execute_reply":"2021-10-30T04:02:03.503444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_pressure = sorted(train.pressure.unique())\n\nPRESSURE_MIN = np.min(all_pressure)\nPRESSURE_MAX = np.max(all_pressure)\nPRESSURE_STEP = all_pressure[1] - all_pressure[0]","metadata":{"execution":{"iopub.status.busy":"2021-10-30T04:02:03.505299Z","iopub.execute_input":"2021-10-30T04:02:03.505522Z","iopub.status.idle":"2021-10-30T04:02:03.587873Z","shell.execute_reply.started":"2021-10-30T04:02:03.505495Z","shell.execute_reply":"2021-10-30T04:02:03.587194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# FE","metadata":{}},{"cell_type":"code","source":"N = 80\nw = blackman(N+1)\n\nffta = lambda x: np.abs(fft(np.append(x.values,x.values[0]))[:80])\nffta.__name__ = 'ffta'\n\nfftw = lambda x: np.abs(fft(np.append(x.values,x.values[0])*w)[:80])\nfftw.__name__ = 'fftw'\n\ndef add_features(df):\n    print('Basic FE')\n    df['cross']= df['u_in'] * df['u_out']\n    df['cross2']= df['time_step'] * df['u_out']\n    df['area'] = df['time_step'] * df['u_in']\n    df['area'] = df.groupby('breath_id')['area'].cumsum()\n    df['time_step_cumsum'] = df.groupby(['breath_id'])['time_step'].cumsum()\n    df['u_in_cumsum'] = (df['u_in']).groupby(df['breath_id']).cumsum()\n    \n    df['u_in_lag2'] = df.groupby('breath_id')['u_in'].shift(2)\n    df['u_out_lag2'] = df.groupby('breath_id')['u_out'].shift(2)\n    df['u_in_lag4'] = df.groupby('breath_id')['u_in'].shift(4)\n    df['u_out_lag4'] = df.groupby('breath_id')['u_out'].shift(4)\n    df = df.fillna(0)\n    \n    df['breath_id__u_in__max'] = df.groupby(['breath_id'])['u_in'].transform('max')\n    df['breath_id__u_out__max'] = df.groupby(['breath_id'])['u_out'].transform('max')\n    df['breath_id__u_in__diffmax'] = df.groupby(['breath_id'])['u_in'].transform('max') - df['u_in']\n    df['breath_id__u_in__diffmean'] = df.groupby(['breath_id'])['u_in'].transform('mean') - df['u_in']\n    df['breath_id__u_in__diffmax'] = df.groupby(['breath_id'])['u_in'].transform('max') - df['u_in']\n    df['breath_id__u_in__diffmean'] = df.groupby(['breath_id'])['u_in'].transform('mean') - df['u_in']\n    \n    df['u_in_diff2'] = df['u_in'] - df['u_in_lag2']\n    df['u_out_diff2'] = df['u_out'] - df['u_out_lag2']\n    df['u_in_diff4'] = df['u_in'] - df['u_in_lag4']\n    df['u_out_diff4'] = df['u_out'] - df['u_out_lag4']\n    \n    df['one'] = 1\n    df['count'] = (df['one']).groupby(df['breath_id']).cumsum()\n    df['u_in_cummean'] =df['u_in_cumsum'] /df['count']\n\n    print('EWM FE')\n    df['ewm_u_in_mean'] = (df\\\n                           .groupby('breath_id')['u_in']\\\n                           .ewm(halflife=9)\\\n                           .mean()\\\n                           .reset_index(level=0,drop=True))\n    df['ewm_u_in_std'] = df.groupby('breath_id')['u_in'].ewm(halflife=10).std().reset_index(level=0,drop=True)\n    df['ewm_u_in_corr'] = df.groupby('breath_id')['u_in'].ewm(halflife=10).corr().reset_index(level=0,drop=True)\n    \n    df['u_in_1st_derivative'] = (df['u_in'].diff().fillna(0) / df['time_step'].diff().fillna(0)).fillna(0)\n    df['expand_mean_1sr_der'] = df.groupby('breath_id')['u_in_1st_derivative'].expanding(2).mean().reset_index(level=0,drop=True)\n    df['u_in_1st_der_mean10'] = df.groupby('breath_id')['u_in_1st_derivative'].rolling(window=10, min_periods=1).mean().reset_index(level=0,drop=True)\n    \n    df[[\"15_in_sum\",\"15_in_min\",\"15_in_max\",\"15_in_mean\"]] = (df\\\n                                                              .groupby('breath_id')['u_in']\\\n                                                              .rolling(window=15,min_periods=1)\\\n                                                              .agg({\"15_in_sum\":\"sum\",\n                                                                    \"15_in_min\":\"min\",\n                                                                    \"15_in_max\":\"max\",\n                                                                    \"15_in_mean\":\"mean\"\n                                                                    #\"15_in_std\":\"std\"\n                                                               })\\\n                                                               .reset_index(level=0,drop=True))\n    print('Rolling')\n    df['rolling_10_mean'] = df.groupby('breath_id')['u_in'].rolling(window=10, min_periods=1).mean().reset_index(level=0,drop=True)\n    df['rolling_10_max'] = df.groupby('breath_id')['u_in'].rolling(window=10, min_periods=1).max().reset_index(level=0,drop=True)\n    df['rolling_10_std'] = df.groupby('breath_id')['u_in'].rolling(window=10, min_periods=1).std().reset_index(level=0,drop=True)\n             \n    df['expand_mean'] = df.groupby('breath_id')['u_in'].expanding(2).mean().reset_index(level=0,drop=True)\n    df['expand_max'] = df.groupby('breath_id')['u_in'].expanding(2).max().reset_index(level=0,drop=True)\n    df['expand_std'] = df.groupby('breath_id')['u_in'].expanding(2).std().reset_index(level=0,drop=True)\n    \n    df['delta_u_in'] = abs(df.groupby(df['breath_id'])['u_in'].diff().fillna(0)).reset_index(level=0,drop=True)\n    df['delta_u_in_exp'] = df.groupby(df['breath_id'])['delta_u_in'].rolling(window=10, min_periods=1).mean().reset_index(level=0,drop=True)\n    df['delta_rolling_10_mean'] = df.groupby('breath_id')['delta_u_in'].rolling(window=10, min_periods=1).mean().reset_index(level=0,drop=True)\n    df['delta_rolling_10_max'] = df.groupby('breath_id')['delta_u_in'].rolling(window=10, min_periods=1).max().reset_index(level=0,drop=True)\n    \n    print('Work')\n    df['work']=((df['u_in'] + df['u_in'].shift(1).fillna(0))/2 * df['time_step'].diff().fillna(0)).clip(0,)\n    df['work_roll_10']=df.groupby(df['breath_id'])['work'].rolling(window=10, min_periods=1).sum().reset_index(level=0,drop=True)\n    df['work_roll_15']=df.groupby(df['breath_id'])['work'].rolling(window=15, min_periods=1).sum().reset_index(level=0,drop=True)\n      \n    df['u_in_rol_q0.1'] = df.groupby(df['breath_id'])['u_in'].rolling(window=10, min_periods=1).quantile(0.1).reset_index(level=0,drop=True)\n    df['u_in_rol_q0.25'] = df.groupby(df['breath_id'])['u_in'].rolling(window=10, min_periods=1).quantile(0.25).reset_index(level=0,drop=True)\n    df['u_in_rol_q0.5'] = df.groupby(df['breath_id'])['u_in'].rolling(window=10, min_periods=1).quantile(0.5).reset_index(level=0,drop=True)\n    df['u_in_rol_q0.75'] = df.groupby(df['breath_id'])['u_in'].rolling(window=10, min_periods=1).quantile(0.75).reset_index(level=0,drop=True)\n    df['u_in_rol_q0.9'] = df.groupby(df['breath_id'])['u_in'].rolling(window=10, min_periods=1).quantile(0.9).reset_index(level=0,drop=True)\n    \n    print('Fourier')\n    df['fft_u_in'] = df.groupby('breath_id')['u_in'].transform(ffta)\n    df['fft_u_in_w'] = df.groupby('breath_id')['u_in'].transform(fftw)\n    df['analytical'] = df.groupby('breath_id')['u_in'].transform(hilbert)\n    df['envelope'] = np.abs(df['analytical'])\n    df['phase'] = np.angle(df['analytical'])\n    df['unwrapped_phase'] = df.groupby('breath_id')['phase'].transform(np.unwrap)\n    df['phase_shift1'] = df.groupby('breath_id')['unwrapped_phase'].shift(1).astype(np.float32)\n    df['IF'] = df['unwrapped_phase'] - df['phase_shift1'].astype(np.float32)\n    \n    df['R'] = df['R'].astype(str)\n    df['C'] = df['C'].astype(str)\n    df['R__C'] = df[\"R\"].astype(str) + '__' + df[\"C\"].astype(str)\n    df = pd.get_dummies(df)\n    df = df.fillna(0)\n    df = df.drop('analytical',axis=1)\n    print('Done processing...\\n')\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-10-30T04:02:03.58986Z","iopub.execute_input":"2021-10-30T04:02:03.590524Z","iopub.status.idle":"2021-10-30T04:02:03.638099Z","shell.execute_reply.started":"2021-10-30T04:02:03.590486Z","shell.execute_reply":"2021-10-30T04:02:03.637154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Processing training set...\\n')\ntrain = add_features(train)\nprint('Processing test set...\\n')\ntest = add_features(test)","metadata":{"execution":{"iopub.status.busy":"2021-10-30T04:02:03.639354Z","iopub.execute_input":"2021-10-30T04:02:03.639622Z","iopub.status.idle":"2021-10-30T04:09:24.935276Z","shell.execute_reply.started":"2021-10-30T04:02:03.639589Z","shell.execute_reply":"2021-10-30T04:09:24.934335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2021-10-30T04:09:24.937115Z","iopub.execute_input":"2021-10-30T04:09:24.937437Z","iopub.status.idle":"2021-10-30T04:09:24.955492Z","shell.execute_reply.started":"2021-10-30T04:09:24.937395Z","shell.execute_reply":"2021-10-30T04:09:24.954089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)","metadata":{"execution":{"iopub.status.busy":"2021-10-30T04:09:24.956906Z","iopub.execute_input":"2021-10-30T04:09:24.95781Z","iopub.status.idle":"2021-10-30T04:10:12.022532Z","shell.execute_reply.started":"2021-10-30T04:09:24.957763Z","shell.execute_reply":"2021-10-30T04:10:12.021499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"@tf.custom_gradient\ndef round_with_gradients(x):\n    def grad(dy):\n        return dy\n    return tf.round(x), grad\n\nclass ScaleLayer(tf.keras.layers.Layer):\n    def __init__(self):\n        super(ScaleLayer, self).__init__()\n        self.min = tf.constant(PRESSURE_MIN, dtype=np.float32)\n        self.max = tf.constant(PRESSURE_MAX, dtype=np.float32)\n        self.step = tf.constant(PRESSURE_STEP, dtype=np.float32)\n\n    def call(self, inputs):\n        steps = tf.math.divide(tf.math.add(inputs, -self.min), self.step)\n        int_steps = round_with_gradients(steps)\n        rescaled_steps = tf.math.add(tf.math.multiply(int_steps, self.step), self.min)\n        clipped = tf.clip_by_value(rescaled_steps, self.min, self.max)\n        return clipped","metadata":{"execution":{"iopub.status.busy":"2021-10-30T04:10:12.02392Z","iopub.execute_input":"2021-10-30T04:10:12.024267Z","iopub.status.idle":"2021-10-30T04:10:12.036885Z","shell.execute_reply.started":"2021-10-30T04:10:12.024223Z","shell.execute_reply":"2021-10-30T04:10:12.03538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"targets = train[['pressure']].to_numpy().reshape(-1, 80)\n\ntrain.drop(['pressure','id', 'breath_id','one','count'], axis=1, inplace=True)\n\ntest = test.drop(['id', 'breath_id','one','count'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-10-30T04:10:12.038468Z","iopub.execute_input":"2021-10-30T04:10:12.038742Z","iopub.status.idle":"2021-10-30T04:10:15.736023Z","shell.execute_reply.started":"2021-10-30T04:10:12.0387Z","shell.execute_reply":"2021-10-30T04:10:15.735395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"RS = RobustScaler()\ntrain = RS.fit_transform(train)\ntest = RS.transform(test)","metadata":{"execution":{"iopub.status.busy":"2021-10-30T04:10:15.738197Z","iopub.execute_input":"2021-10-30T04:10:15.738601Z","iopub.status.idle":"2021-10-30T04:10:58.254048Z","shell.execute_reply.started":"2021-10-30T04:10:15.738554Z","shell.execute_reply":"2021-10-30T04:10:58.253376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train.reshape(-1, 80, train.shape[-1])\ntest = test.reshape(-1, 80, train.shape[-1])","metadata":{"execution":{"iopub.status.busy":"2021-10-30T04:10:58.255167Z","iopub.execute_input":"2021-10-30T04:10:58.25583Z","iopub.status.idle":"2021-10-30T04:10:58.26026Z","shell.execute_reply.started":"2021-10-30T04:10:58.255797Z","shell.execute_reply":"2021-10-30T04:10:58.259327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    BATCH_SIZE = strategy.num_replicas_in_sync * 64\n    print(\"Running on TPU:\", tpu.master())\n    print(f\"Batch Size: {BATCH_SIZE}\")\n    \nexcept ValueError:\n    strategy = tf.distribute.get_strategy()\n    BATCH_SIZE = 512\n    print(f\"Running on {strategy.num_replicas_in_sync} replicas\")\n    print(f\"Batch Size: {BATCH_SIZE}\")","metadata":{"execution":{"iopub.status.busy":"2021-10-30T04:10:58.261841Z","iopub.execute_input":"2021-10-30T04:10:58.262165Z","iopub.status.idle":"2021-10-30T04:11:04.346362Z","shell.execute_reply.started":"2021-10-30T04:10:58.262126Z","shell.execute_reply":"2021-10-30T04:11:04.345423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def dnn_model():\n    \n    x_input = Input(shape=(train.shape[-2:]))\n    \n    x1 = Bidirectional(LSTM(units=768, return_sequences=True))(x_input)\n    x2 = Bidirectional(LSTM(units=512, return_sequences=True))(x1)\n    x3 = Bidirectional(LSTM(units=384, return_sequences=True))(x2)\n    x4 = Bidirectional(LSTM(units=256, return_sequences=True))(x3)\n    x5 = Bidirectional(LSTM(units=128, return_sequences=True))(x4)\n    \n    z2 = Bidirectional(GRU(units=384, return_sequences=True))(x2)\n    \n    z31 = Multiply()([x3, z2])\n    z31 = BatchNormalization()(z31)\n    z3 = Bidirectional(GRU(units=256, return_sequences=True))(z31)\n    \n    z41 = Multiply()([x4, z3])\n    z41 = BatchNormalization()(z41)\n    z4 = Bidirectional(GRU(units=128, return_sequences=True))(z41)\n    \n    z51 = Multiply()([x5, z4])\n    z51 = BatchNormalization()(z51)\n    z5 = Bidirectional(GRU(units=64, return_sequences=True))(z51)\n    \n    x = Concatenate(axis=2)([x5, z2, z3, z4, z5])\n    \n    x = Dense(units=128, activation='selu')(x)\n    \n    x_output = Dense(units=1)(x)\n\n    model = Model(inputs=x_input, outputs= x_output, \n                  name='DNN_Model')\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-10-30T04:11:04.347803Z","iopub.execute_input":"2021-10-30T04:11:04.34843Z","iopub.status.idle":"2021-10-30T04:11:04.365042Z","shell.execute_reply.started":"2021-10-30T04:11:04.348387Z","shell.execute_reply":"2021-10-30T04:11:04.363921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = dnn_model()\n\nplot_model(\n    model, \n    to_file='Google_Brain_Keras_Model.png', \n    show_shapes=True,\n    show_layer_names=True\n)","metadata":{"execution":{"iopub.status.busy":"2021-10-30T04:11:04.366352Z","iopub.execute_input":"2021-10-30T04:11:04.366648Z","iopub.status.idle":"2021-10-30T04:11:11.605109Z","shell.execute_reply.started":"2021-10-30T04:11:04.366614Z","shell.execute_reply":"2021-10-30T04:11:11.604143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    \n    VERBOSE = 0\n    test_preds = []\n    \n    kf = KFold(n_splits=7, shuffle=True, random_state=42)\n    \n    for fold, (train_idx, test_idx) in enumerate(kf.split(train, targets)):\n        X_train, X_valid = train[train_idx], train[test_idx]\n        y_train, y_valid = targets[train_idx], targets[test_idx]\n        \n        model = dnn_model()\n        model.compile(optimizer=\"adam\", loss=\"mae\")\n\n        lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.75, \n                               patience=10, verbose=VERBOSE)\n        \n        save_locally = tf.saved_model.SaveOptions(experimental_io_device='/job:localhost')\n        chk_point = ModelCheckpoint(f'./Bidirect_LSTM_model_{fold+1}C.h5', options=save_locally, \n                                    monitor='val_loss', verbose=VERBOSE, \n                                    save_best_only=True, mode='min')\n\n        es = EarlyStopping(monitor=\"val_loss\", patience=50, \n                           verbose=VERBOSE, mode=\"min\", \n                           restore_best_weights=True)\n        \n        model.fit(X_train, y_train, \n                  validation_data=(X_valid, y_valid), \n                  epochs=300,\n                  verbose=VERBOSE,\n                  batch_size=BATCH_SIZE, \n                  callbacks=[lr, chk_point, es])\n        \n        load_locally = tf.saved_model.LoadOptions(experimental_io_device='/job:localhost')\n        model = load_model(f'./Bidirect_LSTM_model_{fold+1}C.h5', options=load_locally)\n        \n        y_true = y_valid.squeeze().reshape(-1, 1)\n        y_pred = model.predict(X_valid, batch_size=BATCH_SIZE).squeeze().reshape(-1, 1)\n        score = mean_absolute_error(y_true, y_pred)\n        print(f\"Fold-{fold+1} | OOF Score: {score}\")\n        \n        test_preds.append(model.predict(test, batch_size=BATCH_SIZE).squeeze().reshape(-1, 1).squeeze())","metadata":{"execution":{"iopub.status.busy":"2021-10-30T04:11:11.606944Z","iopub.execute_input":"2021-10-30T04:11:11.607229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submitting","metadata":{}},{"cell_type":"code","source":"sample_sub[\"pressure\"] = np.median(np.vstack(test_preds), axis=0)\nsample_sub.to_csv('submission.csv', index=False)\nsample_sub.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}