{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nimport time, logging, gc\nfrom sklearn.preprocessing import RobustScaler\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import KFold\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras import *\nfrom tensorflow.keras.callbacks import *","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/ventilator-pressure-prediction/train.csv')\ntest = pd.read_csv('../input/ventilator-pressure-prediction/test.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# copy from https://www.kaggle.com/hijest/gaps-features-tf-lstm-resnet-like-ff/notebook?scriptVersionId=77500256&cellId=3\ndef add_features(df):\n    df['cross']= df['u_in'] * df['u_out']\n    df['cross2']= df['time_step'] * df['u_out']\n    #df['area'] = df['time_step'] * df['u_in']\n    #df['area'] = df.groupby('breath_id')['area'].cumsum()\n    df['time_delta'] = df.groupby('breath_id')['time_step'].diff()\n    df.fillna(0, inplace=True)\n    df['delta'] = df['time_delta'] * df['u_in']\n    df['area'] = df.groupby('breath_id')['delta'].cumsum()\n    #\n    df['time_step_cumsum'] = df.groupby(['breath_id'])['time_step'].cumsum()\n    df['u_in_cumsum'] = (df['u_in']).groupby(df['breath_id']).cumsum()\n    print(\"Step-1...Completed\")\n    \n    df['u_in_lag1'] = df.groupby('breath_id')['u_in'].shift(1)\n    df['u_out_lag1'] = df.groupby('breath_id')['u_out'].shift(1)\n    df['u_in_lag_back1'] = df.groupby('breath_id')['u_in'].shift(-1)\n    df['u_out_lag_back1'] = df.groupby('breath_id')['u_out'].shift(-1)\n    df['u_in_lag2'] = df.groupby('breath_id')['u_in'].shift(2)\n    df['u_out_lag2'] = df.groupby('breath_id')['u_out'].shift(2)\n    df['u_in_lag_back2'] = df.groupby('breath_id')['u_in'].shift(-2)\n    df['u_out_lag_back2'] = df.groupby('breath_id')['u_out'].shift(-2)\n    df['u_in_lag3'] = df.groupby('breath_id')['u_in'].shift(3)\n    df['u_out_lag3'] = df.groupby('breath_id')['u_out'].shift(3)\n    df['u_in_lag_back3'] = df.groupby('breath_id')['u_in'].shift(-3)\n    df['u_out_lag_back3'] = df.groupby('breath_id')['u_out'].shift(-3)\n    df['u_in_lag4'] = df.groupby('breath_id')['u_in'].shift(4)\n    df['u_out_lag4'] = df.groupby('breath_id')['u_out'].shift(4)\n    df['u_in_lag_back4'] = df.groupby('breath_id')['u_in'].shift(-4)\n    df['u_out_lag_back4'] = df.groupby('breath_id')['u_out'].shift(-4)\n    df = df.fillna(0)\n    print(\"Step-2...Completed\")\n    \n    df['breath_id__u_in__max'] = df.groupby(['breath_id'])['u_in'].transform('max')\n    df['breath_id__u_in__mean'] = df.groupby(['breath_id'])['u_in'].transform('mean')\n    df['breath_id__u_in__diffmax'] = df.groupby(['breath_id'])['u_in'].transform('max') - df['u_in']\n    df['breath_id__u_in__diffmean'] = df.groupby(['breath_id'])['u_in'].transform('mean') - df['u_in']\n    print(\"Step-3...Completed\")\n    \n    df['u_in_diff1'] = df['u_in'] - df['u_in_lag1']\n    df['u_out_diff1'] = df['u_out'] - df['u_out_lag1']\n    df['u_in_diff2'] = df['u_in'] - df['u_in_lag2']\n    df['u_out_diff2'] = df['u_out'] - df['u_out_lag2']\n    df['u_in_diff3'] = df['u_in'] - df['u_in_lag3']\n    df['u_out_diff3'] = df['u_out'] - df['u_out_lag3']\n    df['u_in_diff4'] = df['u_in'] - df['u_in_lag4']\n    df['u_out_diff4'] = df['u_out'] - df['u_out_lag4']\n    print(\"Step-4...Completed\")\n    \n    df['one'] = 1\n    df['count'] = (df['one']).groupby(df['breath_id']).cumsum()\n    df['u_in_cummean'] =df['u_in_cumsum'] /df['count']\n    \n    df['breath_id_lag']=df['breath_id'].shift(1).fillna(0)\n    df['breath_id_lag2']=df['breath_id'].shift(2).fillna(0)\n    df['breath_id_lagsame']=np.select([df['breath_id_lag']==df['breath_id']],[1],0)\n    df['breath_id_lag2same']=np.select([df['breath_id_lag2']==df['breath_id']],[1],0)\n    df['breath_id__u_in_lag'] = df['u_in'].shift(1).fillna(0)\n    df['breath_id__u_in_lag'] = df['breath_id__u_in_lag'] * df['breath_id_lagsame']\n    df['breath_id__u_in_lag2'] = df['u_in'].shift(2).fillna(0)\n    \n    df['breath_id__u_in_lag2'] = df['breath_id__u_in_lag2'] * df['breath_id_lag2same']\n    print(\"Step-5...Completed\")\n    \n    df['time_step_diff'] = df.groupby('breath_id')['time_step'].diff().fillna(0)\n    df['ewm_u_in_mean'] = (df\\\n                           .groupby('breath_id')['u_in']\\\n                           .ewm(halflife=9)\\\n                           .mean()\\\n                           .reset_index(level=0,drop=True))\n    df[[\"15_in_sum\",\"15_in_min\",\"15_in_max\",\"15_in_mean\"]] = (df\\\n                                                              .groupby('breath_id')['u_in']\\\n                                                              .rolling(window=15,min_periods=1)\\\n                                                              .agg({\"15_in_sum\":\"sum\",\n                                                                    \"15_in_min\":\"min\",\n                                                                    \"15_in_max\":\"max\",\n                                                                    \"15_in_mean\":\"mean\"})\\\n                                                               .reset_index(level=0,drop=True))\n    print(\"Step-6...Completed\")\n    \n    df['u_in_lagback_diff1'] = df['u_in'] - df['u_in_lag_back1']\n    df['u_out_lagback_diff1'] = df['u_out'] - df['u_out_lag_back1']\n    df['u_in_lagback_diff2'] = df['u_in'] - df['u_in_lag_back2']\n    df['u_out_lagback_diff2'] = df['u_out'] - df['u_out_lag_back2']\n    \n    df['time_gap'] = df['time_step'] - df.shift(1).fillna(0)['time_step']\n    df['u_in_gap'] = df['u_in'] - df.shift(1).fillna(0)['u_in']\n    df['u_in_rate'] = df['u_in_gap'] / df['time_gap']\n    \n    df.loc[list(range(0, len(df), 80)), 'time_gap'] = 0\n    df.loc[list(range(0, len(df), 80)), 'u_in_gap'] = 0\n    df.loc[list(range(0, len(df), 80)), 'u_in_rate'] = 0\n    \n    df['area'] = df['u_in'] * df['time_gap']\n    df['area_cumsum'] = (df['area']).groupby(df['breath_id']).cumsum()\n    \n    df['R'] = df['R'].astype(str)\n    df['C'] = df['C'].astype(str)\n    df['R__C'] = df[\"R\"].astype(str) + '__' + df[\"C\"].astype(str)\n    df = pd.get_dummies(df)\n    return df\n#\ntrain = add_features(train)\ntest = add_features(test)\n#-------------cell------------------\ntargets = train[['pressure']].to_numpy().reshape(-1, 80)\ntrain.drop(['time_delta','pressure','id', 'breath_id','one','count',\n            'breath_id_lag','breath_id_lag2','breath_id_lagsame',\n            'breath_id_lag2same'], axis=1, inplace=True)\ntest = test.drop(['time_delta','id', 'breath_id','one','count',\n            'breath_id_lag','breath_id_lag2','breath_id_lagsame',\n            'breath_id_lag2same'], axis=1)\n#-------------cell------------------\nCOLS = train.columns.to_numpy()\n#-------------cell------------------\ntrain = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)\n#-------------cell------------------\nRS = RobustScaler()\ntrain = RS.fit_transform(train)\ntest = RS.transform(test)\n#-------------cell------------------\ntrain = train.reshape(-1, 80, train.shape[-1])\ntest = test.reshape(-1, 80, train.shape[-1])\n#-------------cell------------------","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape,test.shape,targets.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def add_features(df):\n#     #df['area'] = df['time_step'] * df['u_in']\n#     #df['area'] = df.groupby('breath_id')['area'].cumsum()\n#     df['time_delta'] = df.groupby('breath_id')['time_step'].diff()\n#     df.fillna(0, inplace=True)\n#     df['delta'] = df['time_delta'] * df['u_in']\n#     df['area'] = df.groupby('breath_id')['delta'].cumsum()\n#     #\n#     df['u_in_cumsum'] = (df['u_in']).groupby(df['breath_id']).cumsum()\n#     df['u_in_lag1'] = df.groupby('breath_id')['u_in'].shift(1)\n#     df['u_out_lag1'] = df.groupby('breath_id')['u_out'].shift(1)\n#     df['u_in_lag_back1'] = df.groupby('breath_id')['u_in'].shift(-1)\n#     df['u_out_lag_back1'] = df.groupby('breath_id')['u_out'].shift(-1)\n#     df['u_in_lag2'] = df.groupby('breath_id')['u_in'].shift(2)\n#     df['u_out_lag2'] = df.groupby('breath_id')['u_out'].shift(2)\n#     df['u_in_lag_back2'] = df.groupby('breath_id')['u_in'].shift(-2)\n#     df['u_out_lag_back2'] = df.groupby('breath_id')['u_out'].shift(-2)\n#     df['u_in_lag3'] = df.groupby('breath_id')['u_in'].shift(3)\n#     df['u_out_lag3'] = df.groupby('breath_id')['u_out'].shift(3)\n#     df['u_in_lag_back3'] = df.groupby('breath_id')['u_in'].shift(-3)\n#     df['u_out_lag_back3'] = df.groupby('breath_id')['u_out'].shift(-3)\n#     df['u_in_lag4'] = df.groupby('breath_id')['u_in'].shift(4)\n#     df['u_out_lag4'] = df.groupby('breath_id')['u_out'].shift(4)\n#     df['u_in_lag_back4'] = df.groupby('breath_id')['u_in'].shift(-4)\n#     df['u_out_lag_back4'] = df.groupby('breath_id')['u_out'].shift(-4)\n#     df = df.fillna(0)\n    \n#     df['breath_id__u_in__max'] = df.groupby(['breath_id'])['u_in'].transform('max')\n#     df['breath_id__u_out__max'] = df.groupby(['breath_id'])['u_out'].transform('max')\n    \n#     df['u_in_diff1'] = df['u_in'] - df['u_in_lag1']\n#     df['u_out_diff1'] = df['u_out'] - df['u_out_lag1']\n#     df['u_in_diff2'] = df['u_in'] - df['u_in_lag2']\n#     df['u_out_diff2'] = df['u_out'] - df['u_out_lag2']\n    \n#     df['breath_id__u_in__diffmax'] = df.groupby(['breath_id'])['u_in'].transform('max') - df['u_in']\n#     df['breath_id__u_in__diffmean'] = df.groupby(['breath_id'])['u_in'].transform('mean') - df['u_in']\n    \n#     df['breath_id__u_in__diffmax'] = df.groupby(['breath_id'])['u_in'].transform('max') - df['u_in']\n#     df['breath_id__u_in__diffmean'] = df.groupby(['breath_id'])['u_in'].transform('mean') - df['u_in']\n    \n#     df['u_in_diff3'] = df['u_in'] - df['u_in_lag3']\n#     df['u_out_diff3'] = df['u_out'] - df['u_out_lag3']\n#     df['u_in_diff4'] = df['u_in'] - df['u_in_lag4']\n#     df['u_out_diff4'] = df['u_out'] - df['u_out_lag4']\n#     df['cross']= df['u_in']*df['u_out']\n#     df['cross2']= df['time_step']*df['u_out']\n    \n#     df['R'] = df['R'].astype(str)\n#     df['C'] = df['C'].astype(str)\n#     df['R__C'] = df[\"R\"].astype(str) + '__' + df[\"C\"].astype(str)\n#     df = pd.get_dummies(df)\n#     return df\n\n# train = add_features(train)\n# test = add_features(test)\n\n# targets = train[['pressure']].to_numpy().reshape(-1, 80)\n# train.drop(['time_delta','pressure', 'id', 'breath_id'], axis=1, inplace=True)\n# test = test.drop(['time_delta','id', 'breath_id'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# rb = RobustScaler()\n# rb.fit(train)\n# train = rb.transform(train)\n# test = rb.transform(test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train.shape,test.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train = train.reshape(-1, 80, train.shape[-1])\n# test = test.reshape(-1, 80, train.shape[-1])\n# gc.collect","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Detect hardware, return appropriate distribution strategy\nimport tensorflow as tf\nprint(tf.version.VERSION)\n#tf.get_logger().setLevel(logging.ERROR)\ntry: # detect TPU\n    tpu = None\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept ValueError: # detect GPU(s) and enable mixed precision\n    strategy = tf.distribute.MirroredStrategy() # works on GPU and multi-GPU\n    policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n    tf.config.optimizer.set_jit(True) # XLA compilation\n    tf.keras.mixed_precision.experimental.set_policy(policy)\n    print('Mixed precision enabled')\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_hist(hist):\n    plt.plot(hist.history[\"loss\"])\n    plt.plot(hist.history[\"val_loss\"])\n    plt.title(\"model performance\")\n    plt.ylabel(\"mean_absolute_error\")\n    plt.xlabel(\"epoch\")\n    plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow_addons.layers import MultiHeadAttention\nfrom tensorflow.keras import backend as K\nfrom tensorflow import keras \nclass Time2Vec(tf.keras.layers.Layer):\n    def __init__(self, kernel_size=1):\n        super(Time2Vec, self).__init__(trainable=True, name='Time2VecLayer')\n        self.k = kernel_size\n    \n    def build(self, input_shape):\n        # trend\n        self.wb = self.add_weight(name='wb',shape=(input_shape[1],),initializer='uniform',trainable=True)\n        self.bb = self.add_weight(name='bb',shape=(input_shape[1],),initializer='uniform',trainable=True)\n        # periodic\n        self.wa = self.add_weight(name='wa',shape=(1, input_shape[1], self.k),initializer='uniform',trainable=True)\n        self.ba = self.add_weight(name='ba',shape=(1, input_shape[1], self.k),initializer='uniform',trainable=True)\n        super(Time2Vec, self).build(input_shape)\n    \n    def call(self, inputs, **kwargs):\n        bias = self.wb * inputs + self.bb\n        dp = K.dot(inputs, self.wa) + self.ba\n        wgts = K.sin(dp) # or K.cos(.)\n\n        ret = K.concatenate([K.expand_dims(bias, -1), wgts], -1)\n        ret = K.reshape(ret, (-1, inputs.shape[1]*(self.k+1)))\n        return ret\n    \n    def compute_output_shape(self, input_shape):\n        return (input_shape[0], input_shape[1]*(self.k + 1))\n#\nclass AttentionBlock(tf.keras.Model):\n    def __init__(self, name='AttentionBlock', num_heads=2, head_size=128, ff_dim=None, dropout=0, **kwargs):\n        super().__init__(name=name, **kwargs)\n\n        if ff_dim is None:\n            ff_dim = head_size\n\n        self.attention = MultiHeadAttention(num_heads=num_heads, head_size=head_size, dropout=dropout)\n        self.attention_dropout = keras.layers.Dropout(dropout)\n        self.attention_norm = keras.layers.LayerNormalization(epsilon=1e-6)\n\n        self.ff_conv1 = keras.layers.Conv1D(filters=ff_dim, kernel_size=1, activation='relu')\n        # self.ff_conv2 at build()\n        self.ff_dropout = keras.layers.Dropout(dropout)\n        self.ff_norm = keras.layers.LayerNormalization(epsilon=1e-6)\n\n    def build(self, input_shape):\n        self.ff_conv2 = keras.layers.Conv1D(filters=input_shape[-1], kernel_size=1) \n\n    def call(self, inputs):\n        x = self.attention([inputs, inputs])\n        x = self.attention_dropout(x)\n        x = self.attention_norm(inputs + x)\n\n        x = self.ff_conv1(x)\n        x = self.ff_conv2(x)\n        x = self.ff_dropout(x)\n\n        x = self.ff_norm(inputs + x)\n        return x\n#\nclass ModelTrunk(tf.keras.Model):\n    def __init__(self, name='ModelTrunk', time2vec_dim=1, num_heads=2, head_size=128, ff_dim=None, num_layers=1, dropout=0, **kwargs):\n        super().__init__(name=name, **kwargs)\n        self.time2vec = Time2Vec(kernel_size=time2vec_dim)\n        if ff_dim is None:\n            ff_dim = head_size\n        self.dropout = dropout\n        self.attention_layers = [AttentionBlock(num_heads=num_heads, head_size=head_size, ff_dim=ff_dim, dropout=dropout) for _ in range(num_layers)]\n        #self.pooling=tf.keras.layers.AveragePooling1D(pool_size=4,data_format='channels_first')\n        self.final_layer = Sequential([\n                Bidirectional(LSTM(256, return_sequences=True)),\n                Bidirectional(LSTM(256, return_sequences=True)),\n                Dense(128, activation='selu'),\n                Dense(1)])\n    def call(self, inputs):\n        time_embedding = tf.keras.layers.TimeDistributed(self.time2vec)(inputs)\n        #time_embedding=inputs\n        #print(time_embedding.shape)\n        x = K.concatenate([inputs, time_embedding], -1)\n        for attention_layer in self.attention_layers:\n            x = attention_layer(x)\n        x=self.final_layer(x)\n        return K.reshape(x, (-1, x.shape[1] * x.shape[2])) # flat vector of features out\nt2v=ModelTrunk(ff_dim=256,num_heads=4,num_layers=2,)\nx=tf.random.uniform((64,80,68))\nx=t2v(x)\nprint(x.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_lr():\n    initial_learning_rate=0.003\n    decay_rate=0.83#1e-5\n    batch_size=256\n    train_len=60360\n    epoch_iter=int(train_len/batch_size)\n    decay_steps=5#200*epoch_iter\n    def decayed_learning_rate(step):\n        return initial_learning_rate * pow(decay_rate , (step / decay_steps))\n    lr_lst=[]\n    for i in range(200):\n        lr=decayed_learning_rate(i)\n        lr_lst.append(lr)\n    print(min(lr_lst))\n    plt.plot(lr_lst)\n    plt.show()\n    plt.close()\nplot_lr()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kf = KFold(n_splits=15, shuffle=True, random_state=42)\n\ntest_preds = []\nbatch_size=128\nfor fold, (train_idx, test_idx) in enumerate(kf.split(train, targets)):\n    print(f\"****** fold: {fold+1} *******\")\n    X_train, X_valid = train[train_idx], train[test_idx]\n    y_train, y_valid = targets[train_idx], targets[test_idx]\n    #scheduler = tf.keras.optimizers.schedules.ExponentialDecay(1e-3, 200*((len(train)*0.8)/batch_size), 1e-5)\n    scheduler = tf.keras.optimizers.schedules.ExponentialDecay(1e-3, 5, 0.88)\n    es = EarlyStopping(monitor='val_loss',mode='min', patience=20, verbose=1,restore_best_weights=True)\n    checkpoint_filepath = f\"folds{fold}.tf\"#f\"folds{fold}.hdf5\"\n#     sv = keras.callbacks.ModelCheckpoint(\n#             checkpoint_filepath, monitor='val_loss', verbose=1, save_best_only=True,\n#             save_weights_only=False, mode='auto', save_freq='epoch',\n#             options=None\n#         )\n    with strategy.scope():\n        model = ModelTrunk(ff_dim=256,num_heads=8,num_layers=2,)\n        model.compile(optimizer=\"adam\",loss = \"mae\")\n    history = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=250, batch_size = batch_size,\n                        callbacks = [es,tf.keras.callbacks.LearningRateScheduler(scheduler)])\n    test_preds.append(model.predict(test).squeeze().reshape(-1, 1).squeeze())\n    #plot_hist(history)\n    del X_train, X_valid, y_train, y_valid, model\n    gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def create_model():   \n#     #with strategy.scope():\n#     model = Sequential([\n#         Input(shape=(80, 25)),\n#         Bidirectional(LSTM(700, return_sequences=True)),\n#         Bidirectional(LSTM(512, return_sequences=True)),\n#         #Bidirectional(LSTM(300, return_sequences=True)),\n#         Bidirectional(LSTM(256, return_sequences=True)),\n#         Bidirectional(LSTM(128, return_sequences=True)),\n#         Dense(100, activation='selu'),\n#         #Dropout(0.1),\n#         Dense(1)\n#     ])\n#     model.compile(optimizer=\"adam\",loss = \"mae\")\n#     return(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# kf = KFold(n_splits=5, shuffle=True, random_state=42)\n\n# test_preds = []\n\n# for fold, (train_idx, test_idx) in enumerate(kf.split(train, y)):\n#     print(f\"****** fold: {fold+1} *******\")\n#     X_train, X_valid = train[train_idx], train[test_idx]\n#     y_train, y_valid = y[train_idx], y[test_idx]\n    \n#     scheduler = tf.keras.optimizers.schedules.ExponentialDecay(1e-3, 200*((len(train)*0.8)/512), 1e-5)\n#     es = EarlyStopping(monitor='val_loss',mode='min', patience=35, verbose=1,restore_best_weights=True)\n    \n#     model = create_model()\n        \n#     history = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=340, batch_size = 512, callbacks = [es,tf.keras.callbacks.LearningRateScheduler(scheduler)])\n#     test_preds.append(model.predict(test).squeeze().reshape(-1, 1).squeeze())\n#     plot_hist(history)\n#     del X_train, X_valid, y_train, y_valid, model\n#     gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.read_csv('../input/ventilator-pressure-prediction/sample_submission.csv')\nsubmission[\"pressure\"] =sum(test_preds)/len(test_preds)# test_preds[0]\nsubmission.to_csv('submission.csv', index=False)\nsubmission","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_gf = pd.read_csv('../input/ventilator-pressure-prediction/train.csv')\nall_pressure = sorted(train_gf.pressure.unique())\nPRESSURE_MIN = all_pressure[0]\nPRESSURE_MAX = all_pressure[-1]\nPRESSURE_STEP = (all_pressure[1] - all_pressure[0])\nsubmission[\"pressure\"] = np.median(np.vstack(test_preds),axis=0)\nsubmission[\"pressure\"] =np.round( (submission.pressure - PRESSURE_MIN)/PRESSURE_STEP ) * PRESSURE_STEP + PRESSURE_MIN\nsubmission.pressure = np.clip(submission.pressure, PRESSURE_MIN, PRESSURE_MAX)\npressure_unique = np.array(sorted(train_gf['pressure'].unique()))\nsubmission['pressure'] = submission['pressure'].map(lambda x: pressure_unique[np.abs(pressure_unique-x).argmin()])\nsubmission.to_csv('submission.csv', index = 0)\nsubmission","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import cupy, cudf, matplotlib.pyplot as plt\n# train_gf = cudf.read_csv('../input/ventilator-pressure-prediction/train.csv')\n# # plt.title('Histogram of Train Pressures',size=14)\n# # plt.hist(train_gf.sample(100_000).pressure.to_array(),bins=100)\n# # plt.show()\n# print('Max pressure =',train_gf.pressure.max(), 'Min pressure =',train_gf.pressure.min())\n# all_pressure = cupy.sort( train_gf.pressure.unique().values )\n# print('The first 25 unique pressures...')\n# PRESSURE_MIN = all_pressure[0].item()\n# PRESSURE_MAX = all_pressure[-1].item()\n# #\n# PRESSURE_STEP = ( all_pressure[1] - all_pressure[0] ).item()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# submission = pd.read_csv('../input/ventilator-pressure-prediction/sample_submission.csv')\n# sub_1=pd.read_csv('../input/submission-list/submission172.csv')\n# sub_2=pd.read_csv('../input/submission-list/submission_mean_LB157.csv')\n# submission['pressure'] = (sub_1['pressure'].values*0.2)+(sub_2['pressure'].values*0.8)\n# submission[\"pressure\"]=np.round((submission.pressure-PRESSURE_MIN)/PRESSURE_STEP)*PRESSURE_STEP+PRESSURE_MIN\n# submission.pressure=np.clip(submission.pressure, PRESSURE_MIN, PRESSURE_MAX)\n# submission.to_csv('submission.csv', index=False)\n# submission.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}