{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-16T01:21:34.602675Z","iopub.execute_input":"2021-11-16T01:21:34.603408Z","iopub.status.idle":"2021-11-16T01:21:34.617492Z","shell.execute_reply.started":"2021-11-16T01:21:34.603367Z","shell.execute_reply":"2021-11-16T01:21:34.61598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**XGBoost**\n\nO resultado utilizando XGBoost ficou longe do aceitável, acabei usando primeiramente só para ter ideia do resultado. Acabei rodando algumas vezes e desistindo de utilizar pelo resultado, partindo para a LSTM.","metadata":{}},{"cell_type":"code","source":"#from sklearn.model_selection import train_test_split\n\n#df_train=pd.read_csv(r\"/kaggle/input/ventilator-pressure-prediction/train.csv\",encoding='latin-1')\n#df_train.head()\n\n#X = df_train.drop(['pressure','id'], axis='columns')\n#y = df_train['pressure']\n\n#Xtr, Xval, ytr, yval = train_test_split(X, y, test_size = 0.2, random_state=0)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-16T01:21:34.620398Z","iopub.execute_input":"2021-11-16T01:21:34.620869Z","iopub.status.idle":"2021-11-16T01:21:34.628057Z","shell.execute_reply.started":"2021-11-16T01:21:34.620823Z","shell.execute_reply":"2021-11-16T01:21:34.627063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-16T01:21:34.632382Z","iopub.execute_input":"2021-11-16T01:21:34.632779Z","iopub.status.idle":"2021-11-16T01:21:34.640412Z","shell.execute_reply.started":"2021-11-16T01:21:34.632728Z","shell.execute_reply":"2021-11-16T01:21:34.639248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df_test=pd.read_csv(r\"/kaggle/input/ventilator-pressure-prediction/test.csv\",encoding='latin-1')\n\n#test = df_test.drop('id',axis='columns')","metadata":{"execution":{"iopub.status.busy":"2021-11-16T01:21:34.643833Z","iopub.execute_input":"2021-11-16T01:21:34.644971Z","iopub.status.idle":"2021-11-16T01:21:34.654812Z","shell.execute_reply.started":"2021-11-16T01:21:34.644892Z","shell.execute_reply":"2021-11-16T01:21:34.653222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#types = ['id','breath_id', 'R', 'C', 'time_step', 'u_in', 'u_out', 'pressure']\n\n#df_train.dtypes","metadata":{"execution":{"iopub.status.busy":"2021-11-16T01:21:34.657447Z","iopub.execute_input":"2021-11-16T01:21:34.657834Z","iopub.status.idle":"2021-11-16T01:21:34.666931Z","shell.execute_reply.started":"2021-11-16T01:21:34.65779Z","shell.execute_reply":"2021-11-16T01:21:34.665186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#for tp in types:\n    #print(len(corpus_treino.loc[corpus_treino[tp] == ' ']))\n    #print(len(corpus_treino.loc[corpus_treino[tp] == 0]))\n    #corpus_treino[tp] = pd.to_numeric(corpus_treino[tp])","metadata":{"execution":{"iopub.status.busy":"2021-11-16T01:21:34.669626Z","iopub.execute_input":"2021-11-16T01:21:34.670295Z","iopub.status.idle":"2021-11-16T01:21:34.681626Z","shell.execute_reply.started":"2021-11-16T01:21:34.670237Z","shell.execute_reply":"2021-11-16T01:21:34.680342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from sklearn.metrics import mean_squared_error\n#import xgboost as xgb\n\n#model = xgb.XGBRegressor(learning_rate=0.01, n_estimators=500, random_state=0)\n\n#model.fit(Xtr, ytr)","metadata":{"execution":{"iopub.status.busy":"2021-11-16T01:21:34.684234Z","iopub.execute_input":"2021-11-16T01:21:34.685631Z","iopub.status.idle":"2021-11-16T01:21:34.694155Z","shell.execute_reply.started":"2021-11-16T01:21:34.685577Z","shell.execute_reply":"2021-11-16T01:21:34.693214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#predictions = model.predict(Xval)","metadata":{"execution":{"iopub.status.busy":"2021-11-16T01:21:34.695855Z","iopub.execute_input":"2021-11-16T01:21:34.696324Z","iopub.status.idle":"2021-11-16T01:21:34.709112Z","shell.execute_reply.started":"2021-11-16T01:21:34.696292Z","shell.execute_reply":"2021-11-16T01:21:34.707061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from sklearn.metrics import explained_variance_score\n#print(explained_variance_score((yval), (predictions)))","metadata":{"execution":{"iopub.status.busy":"2021-11-16T01:21:34.710671Z","iopub.execute_input":"2021-11-16T01:21:34.711109Z","iopub.status.idle":"2021-11-16T01:21:34.722311Z","shell.execute_reply.started":"2021-11-16T01:21:34.711057Z","shell.execute_reply":"2021-11-16T01:21:34.721142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#np.sqrt(mean_squared_error(yval,predictions))","metadata":{"execution":{"iopub.status.busy":"2021-11-16T01:21:34.725966Z","iopub.execute_input":"2021-11-16T01:21:34.726291Z","iopub.status.idle":"2021-11-16T01:21:34.738537Z","shell.execute_reply.started":"2021-11-16T01:21:34.726232Z","shell.execute_reply":"2021-11-16T01:21:34.737334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**LSTM**\n\nLSTM foi projetado para evitar o problema de dependência de longo prazo. O seu comportamento é lembrar de informações por longos períodos de tempo, como o nosso problema é de séries temporais esse modelo acaba sendo muito efetivo.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport optuna\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.callbacks import LearningRateScheduler, ReduceLROnPlateau\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\n\nfrom sklearn.metrics import mean_absolute_error as mae\nfrom sklearn.preprocessing import RobustScaler, normalize\nfrom sklearn.model_selection import train_test_split, GroupKFold, KFold\n\nfrom IPython.display import display","metadata":{"execution":{"iopub.status.busy":"2021-11-16T01:21:34.740549Z","iopub.execute_input":"2021-11-16T01:21:34.740941Z","iopub.status.idle":"2021-11-16T01:21:34.751944Z","shell.execute_reply.started":"2021-11-16T01:21:34.740894Z","shell.execute_reply":"2021-11-16T01:21:34.75119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Primeiramente é realizada a leitura dos arquivos teste e treino disponibilizados pela competição. Nota-se que o tamanho do arquivo de treino foi diminuido, isso foi feito porque o modelo demora muito para ser treinado utilizando o arquivo inteiro. Com o arquivo completo, o modelo demorava muito para rodar, com essa diminuição do tamanho do arquivo foi possível rodar o modelo em 3 a 4 horas. Com isso, foi possível realizar testes de uma quantidade maior de variações em um tempo menor. Não foram realizados tantos testes a ponto de obter o melhor resultado, foram realizados alguns testes melhorando o resultado através de algumas features. O arquivo original tem 6036000 linhas, com a diminuição ficaram 80000 linhas. Entende-se que isso pode afetar muito no aprendizado do modelo, mas nesse caso foi escolhido correr esse risco pela demora que é rodar com o arquivo completo. No começo foi utilizado KFolds para gerar modelos diferentes, mas acabei deixando de lado pelo tempo que estava levando para treinar, não afetava muito o desempenho final.","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/ventilator-pressure-prediction/train.csv')\ntest = pd.read_csv('../input/ventilator-pressure-prediction/test.csv')","metadata":{"execution":{"iopub.status.busy":"2021-11-16T01:21:34.754029Z","iopub.execute_input":"2021-11-16T01:21:34.754758Z","iopub.status.idle":"2021-11-16T01:21:44.812334Z","shell.execute_reply.started":"2021-11-16T01:21:34.75471Z","shell.execute_reply":"2021-11-16T01:21:44.811459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"O arquivo de teste tinha 6036000 linhas.","metadata":{}},{"cell_type":"code","source":"train.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-16T01:21:44.813594Z","iopub.execute_input":"2021-11-16T01:21:44.813832Z","iopub.status.idle":"2021-11-16T01:21:44.821027Z","shell.execute_reply.started":"2021-11-16T01:21:44.813804Z","shell.execute_reply":"2021-11-16T01:21:44.819909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"E passou a ter 80000 linhas.","metadata":{}},{"cell_type":"code","source":"train = train[:80000]","metadata":{"execution":{"iopub.status.busy":"2021-11-16T01:21:44.822545Z","iopub.execute_input":"2021-11-16T01:21:44.82287Z","iopub.status.idle":"2021-11-16T01:21:44.831109Z","shell.execute_reply.started":"2021-11-16T01:21:44.822829Z","shell.execute_reply":"2021-11-16T01:21:44.830341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-16T01:21:44.832142Z","iopub.execute_input":"2021-11-16T01:21:44.832594Z","iopub.status.idle":"2021-11-16T01:21:44.844841Z","shell.execute_reply.started":"2021-11-16T01:21:44.832561Z","shell.execute_reply":"2021-11-16T01:21:44.843923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-16T01:21:44.846121Z","iopub.execute_input":"2021-11-16T01:21:44.846525Z","iopub.status.idle":"2021-11-16T01:21:44.865028Z","shell.execute_reply.started":"2021-11-16T01:21:44.846466Z","shell.execute_reply":"2021-11-16T01:21:44.863985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nsns.lineplot(x='breath_id',y='pressure', data=train)","metadata":{"execution":{"iopub.status.busy":"2021-11-16T01:21:44.866621Z","iopub.execute_input":"2021-11-16T01:21:44.867429Z","iopub.status.idle":"2021-11-16T01:22:13.5411Z","shell.execute_reply.started":"2021-11-16T01:21:44.867393Z","shell.execute_reply":"2021-11-16T01:22:13.540084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Os dados foram disponibilizados de maneira que não precisam de tratamento. O que precisava ser feito era adicionar algumas features nos dados para ver se o modelo obtinha um resultado melhor.\n\nPara realizar as features foram utilizadas algumas funções:\n1. A função groupby() que agrupa os objetos de mesmo valor, aplica uma função e combina os resultados;\n2. A função cumsum() que a soma os elementos ao longo de um determinado eixo;\n3. A função shift() que é imprescindível, porque é ela que desloca os dados da coluna, é deslocado para usar dados anteriores para prever o futuro, sem deslocar estariamos prevendo de maneira errada. Diferentes valores de deslocamento foram testados para avaliar seus impactos.","metadata":{}},{"cell_type":"code","source":"def add_features(df):\n    \n    #Gerando um time geral com o time_step, u_in e breath_id, depois o breath_id é retirado\n    df['area'] = df['time_step'] * df['u_in']\n    df['area'] = df.groupby('breath_id')['area'].cumsum()\n    df['u_in_cumsum'] = (df['u_in']).groupby(df['breath_id']).cumsum()\n    \n    #Agrupando valores com o mesmo breath_id, adicionando diferentes valores de deslocamento\n    df['u_in_lag1'] = df.groupby('breath_id')['u_in'].shift(1)\n    df['u_out_lag1'] = df.groupby('breath_id')['u_out'].shift(1)\n    df['u_in_lag_back1'] = df.groupby('breath_id')['u_in'].shift(-1)\n    df['u_out_lag_back1'] = df.groupby('breath_id')['u_out'].shift(-1)\n    df['u_in_lag2'] = df.groupby('breath_id')['u_in'].shift(2)\n    df['u_out_lag2'] = df.groupby('breath_id')['u_out'].shift(2)\n    df['u_in_lag_back2'] = df.groupby('breath_id')['u_in'].shift(-2)\n    df['u_out_lag_back2'] = df.groupby('breath_id')['u_out'].shift(-2)\n    df['u_in_lag3'] = df.groupby('breath_id')['u_in'].shift(3)\n    df['u_out_lag3'] = df.groupby('breath_id')['u_out'].shift(3)\n    df['u_in_lag_back3'] = df.groupby('breath_id')['u_in'].shift(-3)\n    df['u_out_lag_back3'] = df.groupby('breath_id')['u_out'].shift(-3)\n    df['u_in_lag4'] = df.groupby('breath_id')['u_in'].shift(4) #5\n    df['u_out_lag4'] = df.groupby('breath_id')['u_out'].shift(4)\n    df['u_in_lag_back4'] = df.groupby('breath_id')['u_in'].shift(-4)\n    df['u_out_lag_back4'] = df.groupby('breath_id')['u_out'].shift(-4)\n    \n    #Preencher vazios\n    df = df.fillna(0)\n\n    #Subtraindo valores com deslocamentos diferentes\n    df['u_in_diff1'] = df['u_in'] - df['u_in_lag1']\n    df['u_out_diff1'] = df['u_out'] - df['u_out_lag1']\n    df['u_in_diff2'] = df['u_in'] - df['u_in_lag2']\n    df['u_out_diff2'] = df['u_out'] - df['u_out_lag2']\n    df['u_in_diff3'] = df['u_in'] - df['u_in_lag3']\n    df['u_out_diff3'] = df['u_out'] - df['u_out_lag3']\n    df['u_in_diff4'] = df['u_in'] - df['u_in_lag4']\n    df['u_out_diff4'] = df['u_out'] - df['u_out_lag4']\n    \n    df['R'] = df['R'].astype(str)\n    df['C'] = df['C'].astype(str)\n    df['R__C'] = df[\"R\"].astype(str) + '__' + df[\"C\"].astype(str)\n    df = pd.get_dummies(df)\n    return df\n\ntrain = add_features(train)\ntest = add_features(test)","metadata":{"execution":{"iopub.status.busy":"2021-11-16T01:22:13.542845Z","iopub.execute_input":"2021-11-16T01:22:13.543737Z","iopub.status.idle":"2021-11-16T01:22:33.034317Z","shell.execute_reply.started":"2021-11-16T01:22:13.543694Z","shell.execute_reply":"2021-11-16T01:22:33.033326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-16T01:22:33.036087Z","iopub.execute_input":"2021-11-16T01:22:33.036462Z","iopub.status.idle":"2021-11-16T01:22:33.06016Z","shell.execute_reply.started":"2021-11-16T01:22:33.036429Z","shell.execute_reply":"2021-11-16T01:22:33.059223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Retirando do arquivo de treino os valores de predição do modelo e o breath_id que foi utilizado \n# para gerar outra variável representando o tempo geral\ntargets = train[['pressure']].to_numpy().reshape(-1, 80)\ntrain.drop(['pressure', 'id', 'breath_id'], axis=1, inplace=True)\ntest = test.drop(['id', 'breath_id'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-11-16T01:22:33.061888Z","iopub.execute_input":"2021-11-16T01:22:33.062412Z","iopub.status.idle":"2021-11-16T01:22:34.192824Z","shell.execute_reply.started":"2021-11-16T01:22:33.062357Z","shell.execute_reply":"2021-11-16T01:22:34.192162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-16T01:22:34.194169Z","iopub.execute_input":"2021-11-16T01:22:34.19441Z","iopub.status.idle":"2021-11-16T01:22:34.200215Z","shell.execute_reply.started":"2021-11-16T01:22:34.194381Z","shell.execute_reply":"2021-11-16T01:22:34.19931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fazendo uma padronização dos conjunto de dados, redimensionando os datasets para evitar também\n# os outliers.\nRS = RobustScaler()\ntrain = RS.fit_transform(train)\ntest = RS.transform(test)","metadata":{"execution":{"iopub.status.busy":"2021-11-16T01:22:34.201194Z","iopub.execute_input":"2021-11-16T01:22:34.201419Z","iopub.status.idle":"2021-11-16T01:22:35.543212Z","shell.execute_reply.started":"2021-11-16T01:22:34.201394Z","shell.execute_reply":"2021-11-16T01:22:35.542248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Reshape nos datasets para ficarem com as mesmas dimensões\ntrain = train.reshape(-1, 80, train.shape[-1])\ntest = test.reshape(-1, 80, train.shape[-1])","metadata":{"execution":{"iopub.status.busy":"2021-11-16T01:22:35.544715Z","iopub.execute_input":"2021-11-16T01:22:35.545055Z","iopub.status.idle":"2021-11-16T01:22:35.551192Z","shell.execute_reply.started":"2021-11-16T01:22:35.545013Z","shell.execute_reply":"2021-11-16T01:22:35.550166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"targets.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-16T01:22:35.552652Z","iopub.execute_input":"2021-11-16T01:22:35.552979Z","iopub.status.idle":"2021-11-16T01:22:35.565421Z","shell.execute_reply.started":"2021-11-16T01:22:35.552928Z","shell.execute_reply":"2021-11-16T01:22:35.564517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"O modelo escolhido LSTM é uma rede neural reccorente LSTM (Long Short Term Memory), muito utilizada para fazer previsões com base de dados de série temporal.\nComo hyperparâmetros foram selecionados: epoch e batch_size. O batch_size é um hiperparâmetro que define o número de amostras a serem processadas antes da atualização do modelo. O número de epoch é um hiperparâmetro que define o número de vezes que o algoritmo de aprendizado funcionará em todo o conjunto de dados de treinamento.","metadata":{}},{"cell_type":"code","source":"EPOCH = 300\nBATCH_SIZE = 1024","metadata":{"execution":{"iopub.status.busy":"2021-11-16T01:22:35.566927Z","iopub.execute_input":"2021-11-16T01:22:35.56762Z","iopub.status.idle":"2021-11-16T01:22:35.577133Z","shell.execute_reply.started":"2021-11-16T01:22:35.567583Z","shell.execute_reply":"2021-11-16T01:22:35.576284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tamanho_treino = int(len(train)*0.9) # Pegando 90% dos dados para treino\ntamanho_teste = len(train)-tamanho_treino # Pegando o resto para teste\n\n#Os valores de treino serão utilizados para treinar o modelo, e os valores de teste para testar \n#o modelo conforme ele for sendo treinado, é possível treinar e analisar ao mesmo tempo","metadata":{"execution":{"iopub.status.busy":"2021-11-16T01:22:35.578227Z","iopub.execute_input":"2021-11-16T01:22:35.578475Z","iopub.status.idle":"2021-11-16T01:22:35.593481Z","shell.execute_reply.started":"2021-11-16T01:22:35.578441Z","shell.execute_reply":"2021-11-16T01:22:35.592607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"No treinamento do modelo (.fit()) é passado o parâmetro validation_data(X_valid, y_valid), esses dados de validação não são usados para o ajuste da rede, eles são utilizados para verificar a evolução da função de custo, tanto para os dados de treino quanto para os dados de teste conforme a rede for sendo treinada, isso é printado conforme a rede for sendo treinada, val_loss é o valor da função de custo para seus dados de validação cruzada e o loss é o valor da função de custo para seus dados de treinamento. \n\nComo comentado, o arquivo de treino tinha sido dividido em NUM_FOLDS vezes, gerando diferentes modelos treinados e testados por diferentes partes do arquivo de treino, o que se faz é pegar o modelo com maior capacidade de generalização entre os modelos gerados. Como o tempo estava muito longo, acabei deixando de lado essa parte.\n\nO modelo criado tem 1 camada de entrada, 1 camada de saída e entre esses 2 camadas há 5 camadas invisíveis de 1024, 512, 256, 128 e 128 unidades.\n\nO código foi executado utilizando uma TPU para treinar o modelo mais rápido. Foi utilizado o acelerador TPU v3-8 disponibilizado pelo kaggle.","metadata":{}},{"cell_type":"code","source":"#Valores para treinar o modelo\nX_train = train[0:tamanho_treino]\nX_valid = train[tamanho_treino:len(train)]\n\n#Valores para verificar a evolução\ny_train = targets[0:tamanho_treino]\ny_valid = targets[tamanho_treino:len(targets)]\n\n#Iniciar a TPU para o uso, deixando o tempo de treinamento muito menor\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n\n#Inicializando a estratégia de distribuição\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\nwith tpu_strategy.scope():\n        \n    model = keras.models.Sequential([\n        keras.layers.Input(shape=train.shape[-2:]),\n        keras.layers.Bidirectional(keras.layers.LSTM(1024, return_sequences=True)),\n        keras.layers.Bidirectional(keras.layers.LSTM(512, return_sequences=True)),\n        keras.layers.Bidirectional(keras.layers.LSTM(256, return_sequences=True)),\n        keras.layers.Bidirectional(keras.layers.LSTM(128, return_sequences=True)),\n        keras.layers.Dense(128, activation='selu'),\n        keras.layers.Dense(1),\n    ])\n    model.compile(optimizer=\"adam\", loss=\"mae\")\n\n    model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=EPOCH, batch_size=BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2021-11-16T01:22:35.595062Z","iopub.execute_input":"2021-11-16T01:22:35.595323Z","iopub.status.idle":"2021-11-16T01:26:16.713802Z","shell.execute_reply.started":"2021-11-16T01:22:35.595286Z","shell.execute_reply":"2021-11-16T01:26:16.712904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A função LOSS escolhida foi a **Mean Absolute Error Loss (MAE)**, ela é calculada como a média da diferença absoluta entre os valores reais e previstos. Os valores finais do modelo nessa última execução obtiveram o **loss: 0.2994** (para os dados de treino) e o **val_loss: 0.7055** (para os dados de teste). Acredito que ainda é possível melhorar adicionando mais features que impactem no resultado final e mexer mais nos hyperparâmetros.","metadata":{}}]}