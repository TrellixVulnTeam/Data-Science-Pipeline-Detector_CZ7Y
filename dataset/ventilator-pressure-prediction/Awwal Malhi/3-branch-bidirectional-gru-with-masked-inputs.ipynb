{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Enhanced Performance using Masked Input for a 3 Branched Bidirectional GRU neural net architecture\n\nThe following work is an attempt to create an achitecture better than the standard deep bidirectional LSTM/GRU architecture. The architecture proposed has 3 parallel deep Bidirectional GRU neural nets, all 3 of them work on their own set of inputs and the output of these neural nets are concatenated and passed to a dense layer. Through some experimentation it was found out that the architecture proposed performed better than the standard deep bidirectional GRU neural net. The 3 different inputs passed are as follows:\n* **Input 1** : 80 time step time sequence with [ : 36] time step being masked.\n* **Input 2** : 80 time step time sequence with [36 : ] time step being masked.\n* **Input 1** : 80 time step time sequence with no masking i.e the standard input.\n\nfor speeding up the training we will only be using a smaller dataset of about 3000 samples instead of 75450 samples.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/ventilator-pressure-prediction/train.csv')\ntest = pd.read_csv('../input/ventilator-pressure-prediction/test.csv')\n\ntrain.drop('id', axis=1, inplace=True)\ntest.drop('id', axis=1, inplace=True)\n\nX = train.drop(['pressure', 'breath_id'], axis=1)\ny = train.pressure.values\ntest = test.drop('breath_id', axis=1)\n\n\nscaler = StandardScaler()\n\nX = scaler.fit_transform(X)\nX_test = scaler.transform(test)\n\nX = X.reshape(75450, 80, 5)\ny = y.reshape(75450, 80)\n\nX_test = X_test.reshape(50300, 80, 5)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Bidirectional GRUs (model 1)\n\n\nThis is the standard Bidirectional GRU neural net model, it is being trained for bench marking.","metadata":{}},{"cell_type":"code","source":"def model():\n   \n    input_ = keras.layers.Input(shape=(80, 5))\n    x = keras.layers.Bidirectional(keras.layers.GRU(1024, return_sequences=True))(input_)\n    x = keras.layers.Bidirectional(keras.layers.GRU(512, return_sequences=True))(x)\n    x = keras.layers.Bidirectional(keras.layers.GRU(256, return_sequences=True))(x)\n    x = keras.layers.Bidirectional(keras.layers.GRU(256, return_sequences=True))(x)\n    x = keras.layers.Bidirectional(keras.layers.GRU(128, return_sequences=True))(x)\n\n    x = keras.layers.Dense(1000, activation='relu')(x)\n    output_ = keras.layers.Dense(1)(x)\n\n    model = keras.models.Model(inputs=input_, outputs=output_)\n    \n    return model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dnn_model = model()\n\ndnn_model.compile(optimizer=keras.optimizers.Nadam(), loss='mae')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"es = keras.callbacks.EarlyStopping(patience=30)\nls = keras.callbacks.ReduceLROnPlateau(patience=15, factor=0.7)\n\nhistory_1 = dnn_model.fit(X[:2000], y[:2000], validation_split=0.3, batch_size=64, epochs=100, callbacks=[es, ls])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2 Branch Bidirectional GRU neural net architecture(model 2)\n\nThis neural net architecture just contains the 2 branches and the input passed is of type 1 and 2 as discussed in the beginning.","metadata":{}},{"cell_type":"code","source":"def get_split_data(X):\n    data_A = np.zeros(X.shape)\n    data_B = np.zeros(X.shape)\n    \n    data_A[:, :36, :] = X[:, :36, :]\n    data_B[:, 36:, :] = X[:, 36:, :]\n    \n    return data_A, data_B","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_A = keras.layers.Input(shape=(80, 5))\ninput_B = keras.layers.Input(shape=(80, 5))\ninput_C = keras.layers.Input(shape=(80, 5))\n\ninput_A_masked = keras.layers.Masking(mask_value=0., input_shape=(80, 5))(input_A)\nx_A = keras.layers.Bidirectional(keras.layers.GRU(512, return_sequences=True))(input_A_masked)\nx_A = keras.layers.Bidirectional(keras.layers.GRU(256, return_sequences=True))(x_A)\nx_A = keras.layers.Bidirectional(keras.layers.GRU(256, return_sequences=True))(x_A)\nx_A = keras.layers.Bidirectional(keras.layers.GRU(128, return_sequences=True))(x_A)\nx_A = keras.layers.Bidirectional(keras.layers.GRU(128, return_sequences=True))(x_A)\nx_A = keras.layers.Bidirectional(keras.layers.GRU(128, return_sequences=True))(x_A)\n\ninput_B_masked = keras.layers.Masking(mask_value=0., input_shape=(80, 5))(input_B)\nx_B = keras.layers.Bidirectional(keras.layers.GRU(512, return_sequences=True))(input_B_masked)\nx_B = keras.layers.Bidirectional(keras.layers.GRU(256, return_sequences=True))(x_B)\nx_B = keras.layers.Bidirectional(keras.layers.GRU(256, return_sequences=True))(x_B)\nx_B = keras.layers.Bidirectional(keras.layers.GRU(128, return_sequences=True))(x_B)\nx_B = keras.layers.Bidirectional(keras.layers.GRU(128, return_sequences=True))(x_B)\nx_B = keras.layers.Bidirectional(keras.layers.GRU(128, return_sequences=True))(x_B)\n\nx = keras.layers.Dense(1000, activation='relu')(tf.concat([x_A, x_B], axis=-1))\n\ny_hat = keras.layers.Dense(1)(x)\n\nmodel = keras.models.Model(inputs=[input_A, input_B], outputs=y_hat)\n\nmodel.compile(optimizer=keras.optimizers.Nadam(), loss='mae')\n\ndata_A, data_B = get_split_data(X[:2000])\nes = keras.callbacks.EarlyStopping(patience=30)\nls = keras.callbacks.ReduceLROnPlateau(patience=15, factor=0.7)\n\nhistory_2 = model.fit(x=[data_A, data_B], y=y[:2000], validation_split=0.3, batch_size=64, epochs=100, callbacks=[es, ls])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3 Branch Bidirectional GRU neural net architecture(model 3)\n\nThis is the final and the main architecture that we will compare with the other 2 architectures. **The ideology** behind creating this model is the sequence in which we have the pressure, for the first few timesteps the pressure is high compared to final 50 or so time step. By giving neural net to have the flexibility to tune two different branches of bidirectional neural nets specifically on both of these two phases we allow it to learn those phases better independent from the other phase's internal state. The third branch is to emphasize the internal state passed through the RNN as a whole.","metadata":{}},{"cell_type":"code","source":"input_A = keras.layers.Input(shape=(80, 5))\ninput_B = keras.layers.Input(shape=(80, 5))\ninput_C = keras.layers.Input(shape=(80, 5))\n\ninput_A_masked = keras.layers.Masking(mask_value=0., input_shape=(80, 5))(input_A)\nx_A = keras.layers.Bidirectional(keras.layers.GRU(512, return_sequences=True))(input_A_masked)\nx_A = keras.layers.Bidirectional(keras.layers.GRU(256, return_sequences=True))(x_A)\nx_A = keras.layers.Bidirectional(keras.layers.GRU(256, return_sequences=True))(x_A)\nx_A = keras.layers.Bidirectional(keras.layers.GRU(128, return_sequences=True))(x_A)\nx_A = keras.layers.Bidirectional(keras.layers.GRU(128, return_sequences=True))(x_A)\nx_A = keras.layers.Bidirectional(keras.layers.GRU(128, return_sequences=True))(x_A)\n\ninput_B_masked = keras.layers.Masking(mask_value=0., input_shape=(80, 5))(input_B)\nx_B = keras.layers.Bidirectional(keras.layers.GRU(512, return_sequences=True))(input_B_masked)\nx_B = keras.layers.Bidirectional(keras.layers.GRU(256, return_sequences=True))(x_B)\nx_B = keras.layers.Bidirectional(keras.layers.GRU(256, return_sequences=True))(x_B)\nx_B = keras.layers.Bidirectional(keras.layers.GRU(128, return_sequences=True))(x_B)\nx_B = keras.layers.Bidirectional(keras.layers.GRU(128, return_sequences=True))(x_B)\nx_B = keras.layers.Bidirectional(keras.layers.GRU(128, return_sequences=True))(x_B)\n\nx_C = keras.layers.Bidirectional(keras.layers.GRU(1024, return_sequences=True))(input_C)\nx_C = keras.layers.Bidirectional(keras.layers.GRU(512, return_sequences=True))(x_C)\nx_C = keras.layers.Bidirectional(keras.layers.GRU(256, return_sequences=True))(x_C)\nx_C = keras.layers.Bidirectional(keras.layers.GRU(256, return_sequences=True))(x_C)\nx_C = keras.layers.Bidirectional(keras.layers.GRU(128, return_sequences=True))(x_C)\n\nx = keras.layers.Dense(1000, activation='relu')(tf.concat([x_A, x_B, x_C], axis=-1))\n\ny_hat = keras.layers.Dense(1)(x)\n\nmodel = keras.models.Model(inputs=[input_A, input_B, input_C], outputs=y_hat)\n\nmodel.compile(optimizer=keras.optimizers.Nadam(), loss='mae')\n\ndata_A, data_B = get_split_data(X[:2000])\nes = keras.callbacks.EarlyStopping(patience=30)\nls = keras.callbacks.ReduceLROnPlateau(patience=15, factor=0.7)\n\n\nhistory_3 = model.fit(x=[data_A, data_B, X[:2000]], y=y[:2000], validation_split=0.3, batch_size=64, epochs=100, callbacks=[es, ls])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Comparison","metadata":{}},{"cell_type":"markdown","source":"### Validation loss","metadata":{}},{"cell_type":"code","source":"dictionary_1 = history_1.history\ndictionary_2 = history_2.history\ndictionary_3 = history_3.history\nmodel_names = ['standard model', '2 Branch', '3 Branch']\n\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(16, 8))\nax = ax.reshape(-1,)\nfor i, loss in enumerate([dictionary_1, dictionary_2, dictionary_3]):\n    ax[0].plot(loss['val_loss'], label=f'Val Loss {model_names[i]}')\n    ax[0].set_title('Validation Loss', fontsize=20)\n    ax[0].set_xlabel('Epochs', fontsize=18)\n    ax[0].set_ylabel('Mean Absolute Error', fontsize=18)\n    ax[0].legend()\n    ax[0].grid(alpha=0.4)\n    \nfor i, loss in enumerate([dictionary_1, dictionary_2, dictionary_3]):\n    ax[1].plot(loss['loss'], label=f'Training Loss {model_names[i]}')\n    ax[1].set_title('Training Loss', fontsize=20)\n    ax[1].set_xlabel('Epochs', fontsize=18)\n    ax[1].set_ylabel('Mean Absolute Error', fontsize=18)\n    ax[1].legend()\n    ax[1].grid(alpha=0.4)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can clearly see that our model has way better training loss compared to other models. Validation loss is also slightly better but not a difference as good as we can see in the training loss case, This can be attributed to the fact that we did not have many samples to generalize the model on, and having more samples would have made this result better, which is also the case.\n\nFurther improvements can be made to this architecture by properly tuning the parameters.","metadata":{}},{"cell_type":"markdown","source":"If you have any suggestions or constructive criticism, please leave them in the comments below, it will be highly appreciated.\n\n**If you like the work please upvote!!** :)","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}