{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport copy\nimport glob\nimport random\nfrom random import random as rd\nimport gc","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-07T00:28:19.851317Z","iopub.execute_input":"2021-10-07T00:28:19.851646Z","iopub.status.idle":"2021-10-07T00:28:19.872401Z","shell.execute_reply.started":"2021-10-07T00:28:19.851565Z","shell.execute_reply":"2021-10-07T00:28:19.871653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reference: \n* https://www.kaggle.com/cdeotte/ensemble-folds-with-median-0-153 by Chris Deotte\n* https://www.kaggle.com/snnclsr/a-dummy-approach-to-improve-your-score-postprocess by Sinan Calisir\n* Public notebooks with score less than 0.158","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv(\"../input/ventilator-pressure-prediction/train.csv\")\nunique_pressures = df_train[\"pressure\"].unique()\nsorted_pressures = np.sort(unique_pressures)\ntotal_pressures_len = len(sorted_pressures)\n\ndef find_nearest(prediction):\n    insert_idx = np.searchsorted(sorted_pressures, prediction)\n    if insert_idx == total_pressures_len:\n        return sorted_pressures[-1]\n    elif insert_idx == 0:\n        return sorted_pressures[0]\n    lower_val = sorted_pressures[insert_idx - 1]\n    upper_val = sorted_pressures[insert_idx]\n    return lower_val if abs(lower_val - prediction) < abs(upper_val - prediction) else upper_val\n\ndef set_seed(seed = 2021):\n    np.random.seed(seed)\n    random_state = np.random.RandomState(seed)\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    return random_state\n\ndef wc(input_list):\n    l = []\n    for i in range(len(input_list)):\n        public_lb_score = int(input_list[i].split(\"/\")[-1].split(\".\")[1].split(\" \")[0]) \n        l.append(public_lb_score)\n        input_list[i] = (pd.read_csv(input_list[i]).pressure).ravel()\n    output = 0\n    l_sum = sum(l)\n    if len(input_list) == 1:\n        output = input_list[0]\n    else:\n        weight1 = (l[1] / l_sum) + 0.1\n        weight2= 1 - weight1\n        output += input_list[0] * weight1 + input_list[1] * weight2\n    return output\n\ndef g(dp):\n# input: the dataset path of the prediction result files\n# file name format: public lb score or pulbic lb score + name, e.g., 0.335 LSTM baseline\n\n    # get all the files to blend\n    l = []\n    for i in glob.iglob(f'{dp}/*'):\n        l.append(i)\n    file_count = len(l)\n    loop_time = 500 // file_count\n    # calculate the number of files in the input dataset\n    # and split them 2 by 2\n    splits = file_count // 2\n    # sort the file based on their public lb score\n    l.sort()\n    flist = []\n    # create a file list\n    # append the 2 by 2 files as one element\n    # in the last loop, append all the files which are not necessarily 2 files\n    for i in range(splits):\n        if i == splits - 1:\n            flist.append(l[i * round(len(l) / splits): ])\n        else:\n            flist.append(l[i * round(len(l) / splits): (i + 1) * round(len(l) / splits)])\n    # transfrom each element in the file list into one blended prediction\n    for i in range(len(flist)):   \n        flist[i] = wc(flist[i])\n    pred_list = []\n    # loop a large number of times\n    # to converge the result into a stable expected value\n    for i in range(loop_time):      \n        weight = []        \n        set_seed(i)\n        # create a weight list with the same length as the file list\n        for i in range(len(flist)):\n            weight.append(rd())  \n        weight_sum = sum(weight)\n        # normalize the weights\n        for i in range(len(weight)):\n            weight[i] /= weight_sum\n        weight.sort(reverse = True)\n        temp = 0\n        # assign each weights to each blended prediction\n        for i in range(len(flist)):\n            temp += flist[i] * weight[i]\n        pred_list.append(temp)\n        del temp\n        gc.collect()\n    output = pd.read_csv(\"../input/ventilator-pressure-prediction/sample_submission.csv\")\n    output.pressure = np.median(np.vstack(pred_list), axis = 0)\n    output[\"pressure\"] = output[\"pressure\"].apply(find_nearest)\n    output.to_csv(f'rwb {loop_time} loops.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2021-10-07T00:28:19.873907Z","iopub.execute_input":"2021-10-07T00:28:19.874218Z","iopub.status.idle":"2021-10-07T00:28:27.572199Z","shell.execute_reply.started":"2021-10-07T00:28:19.874189Z","shell.execute_reply":"2021-10-07T00:28:27.571619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g('../input/gb-rwbt-files')","metadata":{"execution":{"iopub.status.busy":"2021-10-07T00:28:27.573485Z","iopub.execute_input":"2021-10-07T00:28:27.574273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}