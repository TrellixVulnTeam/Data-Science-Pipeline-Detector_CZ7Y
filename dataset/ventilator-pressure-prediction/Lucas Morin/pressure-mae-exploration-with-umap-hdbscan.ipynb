{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Pressure / MAE exploration with umap / hdbscan","metadata":{}},{"cell_type":"markdown","source":"The general idea of this notebook is to explore where we get high MAE in terms of clusters of target. We use umap and hdbscan to proceed to dimensionnality reduction and clustering. This allow for 2D plot and aggregation of MAE by cluster to priorize wich clusters we have to deal with. Another version of this notebook, exploring u_in and MAE, including some specific changes (masking u_in ?) is available here (https://www.kaggle.com/lucasmorin/u-in-mae-exploration-with-umap-hdbscan).","metadata":{}},{"cell_type":"markdown","source":"# Base construction","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport optuna\n\nimport os \nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n# https://www.kaggle.com/c/ventilator-pressure-prediction/discussion/274717\n\nimport matplotlib.pyplot as plt\nimport tensorflow as tf, gc\nfrom tensorflow import keras\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.callbacks import LearningRateScheduler, ReduceLROnPlateau\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\n\nfrom sklearn.metrics import mean_absolute_error as mae\nfrom sklearn.preprocessing import RobustScaler, normalize\nfrom sklearn.model_selection import train_test_split, GroupKFold, KFold\n\nfrom IPython.display import display\n\nDEBUG = False\nTRAIN_MODEL = False\n\ndf_train = pd.read_csv('../input/ventilator-pressure-prediction/train.csv')\ndf_test = pd.read_csv('../input/ventilator-pressure-prediction/test.csv')\nsubmission = pd.read_csv('../input/ventilator-pressure-prediction/sample_submission.csv')\n\nif DEBUG:\n    train = train[:80*1000]\n\ndef add_features(df):\n    df['area'] = df['time_step'] * df['u_in']\n    df['area'] = df.groupby('breath_id')['area'].cumsum()\n    \n    df['u_in_cumsum'] = (df['u_in']).groupby(df['breath_id']).cumsum()\n    \n    df['u_in_lag1'] = df.groupby('breath_id')['u_in'].shift(1)\n    df['u_out_lag1'] = df.groupby('breath_id')['u_out'].shift(1)\n    df['u_in_lag_back1'] = df.groupby('breath_id')['u_in'].shift(-1)\n    df['u_out_lag_back1'] = df.groupby('breath_id')['u_out'].shift(-1)\n    df['u_in_lag2'] = df.groupby('breath_id')['u_in'].shift(2)\n    df['u_out_lag2'] = df.groupby('breath_id')['u_out'].shift(2)\n    df['u_in_lag_back2'] = df.groupby('breath_id')['u_in'].shift(-2)\n    df['u_out_lag_back2'] = df.groupby('breath_id')['u_out'].shift(-2)\n    df['u_in_lag3'] = df.groupby('breath_id')['u_in'].shift(3)\n    df['u_out_lag3'] = df.groupby('breath_id')['u_out'].shift(3)\n    df['u_in_lag_back3'] = df.groupby('breath_id')['u_in'].shift(-3)\n    df['u_out_lag_back3'] = df.groupby('breath_id')['u_out'].shift(-3)\n    df['u_in_lag4'] = df.groupby('breath_id')['u_in'].shift(4)\n    df['u_out_lag4'] = df.groupby('breath_id')['u_out'].shift(4)\n    df['u_in_lag_back4'] = df.groupby('breath_id')['u_in'].shift(-4)\n    df['u_out_lag_back4'] = df.groupby('breath_id')['u_out'].shift(-4)\n    df = df.fillna(0)\n    \n    df['breath_id__u_in__max'] = df.groupby(['breath_id'])['u_in'].transform('max')\n    df['breath_id__u_out__max'] = df.groupby(['breath_id'])['u_out'].transform('max')\n    \n    df['u_in_diff1'] = df['u_in'] - df['u_in_lag1']\n    df['u_out_diff1'] = df['u_out'] - df['u_out_lag1']\n    df['u_in_diff2'] = df['u_in'] - df['u_in_lag2']\n    df['u_out_diff2'] = df['u_out'] - df['u_out_lag2']\n    \n    df['breath_id__u_in__diffmax'] = df.groupby(['breath_id'])['u_in'].transform('max') - df['u_in']\n    df['breath_id__u_in__diffmean'] = df.groupby(['breath_id'])['u_in'].transform('mean') - df['u_in']\n    \n    df['breath_id__u_in__diffmax'] = df.groupby(['breath_id'])['u_in'].transform('max') - df['u_in']\n    df['breath_id__u_in__diffmean'] = df.groupby(['breath_id'])['u_in'].transform('mean') - df['u_in']\n    \n    df['u_in_diff3'] = df['u_in'] - df['u_in_lag3']\n    df['u_out_diff3'] = df['u_out'] - df['u_out_lag3']\n    df['u_in_diff4'] = df['u_in'] - df['u_in_lag4']\n    df['u_out_diff4'] = df['u_out'] - df['u_out_lag4']\n    df['cross']= df['u_in']*df['u_out']\n    df['cross2']= df['time_step']*df['u_out']\n    \n    df['R'] = df['R'].astype(str)\n    df['C'] = df['C'].astype(str)\n    df['R__C'] = df[\"R\"].astype(str) + '__' + df[\"C\"].astype(str)\n    df = pd.get_dummies(df)\n    return df\n\ntrain = add_features(df_train)\ntest = add_features(df_test)\n\ntargets = train[['pressure']].to_numpy().reshape(-1, 80)\ntrain.drop(['pressure', 'id', 'breath_id'], axis=1, inplace=True)\ntest = test.drop(['id', 'breath_id'], axis=1)\n\nRS = RobustScaler()\ntrain = RS.fit_transform(train)\ntest = RS.transform(test)\n\ntrain = train.reshape(-1, 80, train.shape[-1])\ntest = test.reshape(-1, 80, train.shape[-1])","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-10-08T08:36:52.200907Z","iopub.execute_input":"2021-10-08T08:36:52.201296Z","iopub.status.idle":"2021-10-08T08:38:20.740629Z","shell.execute_reply.started":"2021-10-08T08:36:52.201265Z","shell.execute_reply":"2021-10-08T08:38:20.739535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Infer train data to compare","metadata":{}},{"cell_type":"code","source":"EPOCH = 300\nBATCH_SIZE = 1024\nNUM_FOLDS = 10\n\ngpu_strategy = tf.distribute.get_strategy()\n\nwith gpu_strategy.scope():\n    kf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=2021)\n    train_preds = []\n    for fold, (train_idx, test_idx) in enumerate(kf.split(train, targets)):\n        if fold>0:\n            break\n        K.clear_session()\n        print('-'*15, '>', f'Fold {fold+1}', '<', '-'*15)\n        checkpoint_filepath = f\"folds{fold}.hdf5\"\n        model = keras.models.load_model('../input/finetune-of-tensorflow-bidirectional-lstm/'+checkpoint_filepath)\n        train_preds.append(model.predict(train, batch_size=BATCH_SIZE, verbose=2).squeeze().reshape(-1, 1).squeeze())\n        del model\n        gc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-10-08T07:49:36.716885Z","iopub.execute_input":"2021-10-08T07:49:36.717168Z","iopub.status.idle":"2021-10-08T07:50:42.182013Z","shell.execute_reply.started":"2021-10-08T07:49:36.71714Z","shell.execute_reply":"2021-10-08T07:50:42.18114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame({'target': targets.flatten(), 'preds': np.mean(train_preds,axis=0), 'id': [i+1 for i in range(75450) for e in range(80)]})\npreds_by_id = np.mean(train_preds,axis=0).reshape(-1, 80)\ndf['u_out_mask'] = (train[:,:,2] == -1).flatten()\ndf['error'] = np.abs(df['target']-df['preds'])\ndf_masked = df.mask(~df['u_out_mask'])\nMAE_id = df_masked.groupby('id').agg(np.nanmean)['error']","metadata":{"execution":{"iopub.status.busy":"2021-10-08T07:50:42.184541Z","iopub.execute_input":"2021-10-08T07:50:42.185076Z","iopub.status.idle":"2021-10-08T07:50:45.724028Z","shell.execute_reply.started":"2021-10-08T07:50:42.185036Z","shell.execute_reply":"2021-10-08T07:50:45.723106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Basic Exploration","metadata":{}},{"cell_type":"code","source":"MAE_id.describe()","metadata":{"execution":{"iopub.status.busy":"2021-10-08T07:50:45.725616Z","iopub.execute_input":"2021-10-08T07:50:45.725941Z","iopub.status.idle":"2021-10-08T07:50:45.743232Z","shell.execute_reply.started":"2021-10-08T07:50:45.725903Z","shell.execute_reply":"2021-10-08T07:50:45.742224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.log(MAE_id).hist(bins=100)","metadata":{"execution":{"iopub.status.busy":"2021-10-08T07:50:45.74462Z","iopub.execute_input":"2021-10-08T07:50:45.74497Z","iopub.status.idle":"2021-10-08T07:50:46.10047Z","shell.execute_reply.started":"2021-10-08T07:50:45.744935Z","shell.execute_reply":"2021-10-08T07:50:46.099485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAE_id_sorted = MAE_id.sort_values(ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-10-08T07:50:46.101803Z","iopub.execute_input":"2021-10-08T07:50:46.102197Z","iopub.status.idle":"2021-10-08T07:50:46.11686Z","shell.execute_reply.started":"2021-10-08T07:50:46.102157Z","shell.execute_reply":"2021-10-08T07:50:46.116137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# worst predictions","metadata":{}},{"cell_type":"code","source":"for i in MAE_id_sorted.index[:10]:\n    ind = np.int(i)\n    ind_labels = (MAE_id.index.values == i)\n    \n    print('cluster:'+str(i) + ' MAE: '+str(np.round(MAE_id_sorted[i],3)))\n\n    plt.figure(figsize=(24,8))\n    \n    plt.subplot(1, 3, 1)\n    plt.plot(train[ind_labels,:,1].transpose());\n    plt.title('u_in')\n    \n    plt.subplot(1, 3, 2)\n    plt.plot(targets[ind_labels].transpose(), label='target');\n    plt.plot(preds_by_id[ind_labels].transpose(), label='preds');\n    plt.legend()\n    plt.title('pressure')\n    \n    plt.subplot(1, 3, 3)\n    plt.plot((targets[ind_labels] - preds_by_id[ind_labels]).transpose());\n    plt.title('error')\n    \n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-08T07:50:46.118179Z","iopub.execute_input":"2021-10-08T07:50:46.118571Z","iopub.status.idle":"2021-10-08T07:50:50.469568Z","shell.execute_reply.started":"2021-10-08T07:50:46.118533Z","shell.execute_reply":"2021-10-08T07:50:50.46878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"LTSM seems to be able to predict weird noise, but has some constant error on some ids. However such errors seems rather rare. The instance with 15 MAE only account for 0.0002 global MAE. We must find if some clusters of output have more important global error (less individual MAE but high number of instances).","metadata":{}},{"cell_type":"markdown","source":"# UMAP / Hdbscan","metadata":{}},{"cell_type":"code","source":"!mkdir -p /tmp/pip/cache/\n!cp ../input/hdbscan0827-whl/hdbscan-0.8.27-cp37-cp37m-linux_x86_64.whl /tmp/pip/cache/\n!pip install --no-index --find-links /tmp/pip/cache/ hdbscan","metadata":{"execution":{"iopub.status.busy":"2021-10-08T07:50:50.470835Z","iopub.execute_input":"2021-10-08T07:50:50.471196Z","iopub.status.idle":"2021-10-08T07:50:59.31563Z","shell.execute_reply.started":"2021-10-08T07:50:50.47116Z","shell.execute_reply":"2021-10-08T07:50:59.31466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# exploration on pressure - UMAP Embedding - hdbscan clustering","metadata":{}},{"cell_type":"markdown","source":"As the idea is to observe the target, we embed pressure with umap then we fit hdbscan on the embedding. Maybe better to fit on the original data. \n","metadata":{}},{"cell_type":"code","source":"%%time\n\nimport hdbscan\nimport umap\n\nn=75450\n\n# Switch to do the exploration on u_in\n#X = train[:,:,1]\nX = targets\n\nreducer = umap.UMAP(random_state=42, n_components=2)\nembedding = reducer.fit_transform(X)\nclusterer = hdbscan.HDBSCAN(prediction_data=True, min_cluster_size = 50).fit(embedding)\nu, counts = np.unique(clusterer.labels_, return_counts=True)\n\nprint(u)\nprint(counts)","metadata":{"execution":{"iopub.status.busy":"2021-10-08T07:50:59.31882Z","iopub.execute_input":"2021-10-08T07:50:59.319185Z","iopub.status.idle":"2021-10-08T07:53:49.314822Z","shell.execute_reply.started":"2021-10-08T07:50:59.319149Z","shell.execute_reply":"2021-10-08T07:53:49.313773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10, 8))\nplt.scatter(embedding[:, 0], embedding[:, 1], s=5, c=clusterer.labels_, edgecolors='none', cmap='jet');","metadata":{"execution":{"iopub.status.busy":"2021-10-08T07:53:49.316792Z","iopub.execute_input":"2021-10-08T07:53:49.317293Z","iopub.status.idle":"2021-10-08T07:53:49.993234Z","shell.execute_reply.started":"2021-10-08T07:53:49.317253Z","shell.execute_reply":"2021-10-08T07:53:49.992259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Doesn't seems to good of a clustering... there seems to be lot of 'outliers'.","metadata":{}},{"cell_type":"markdown","source":"# Plot error - log_scale","metadata":{}},{"cell_type":"code","source":"import matplotlib.colors as colors\nplt.figure(figsize=(10, 8))\nplt.scatter(embedding[:, 0], embedding[:, 1], s=5, c=MAE_id[:n], edgecolors='none', cmap='jet', norm=colors.LogNorm(vmin=MAE_id[:n].quantile(0.05), vmax=MAE_id[:n].quantile(0.95)));\nplt.colorbar();","metadata":{"execution":{"iopub.status.busy":"2021-10-08T07:53:49.99459Z","iopub.execute_input":"2021-10-08T07:53:49.994945Z","iopub.status.idle":"2021-10-08T07:53:50.980898Z","shell.execute_reply.started":"2021-10-08T07:53:49.994907Z","shell.execute_reply":"2021-10-08T07:53:50.979908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# MAE by cluster","metadata":{}},{"cell_type":"markdown","source":"not exact as mean of MAE is not exactly the evaluation metric (due to u_out not starting at the same time), but should be ok for exploration.","metadata":{}},{"cell_type":"code","source":"MAE_by_cluster = pd.DataFrame({'cluster':clusterer.labels_,'MAE':MAE_id})\n\nMAE_cluster = MAE_by_cluster.groupby('cluster').agg(np.sum)['MAE']\n\n#remove some base MAE, then divide by gloabl number\nMAE_cluster_global = (MAE_cluster - counts * 0.15)/counts.sum()\nMAE_cluster_global = MAE_cluster_global.sort_values(ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-10-08T07:53:50.982382Z","iopub.execute_input":"2021-10-08T07:53:50.982718Z","iopub.status.idle":"2021-10-08T07:53:50.997662Z","shell.execute_reply.started":"2021-10-08T07:53:50.98268Z","shell.execute_reply":"2021-10-08T07:53:50.996957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAE_cluster_global","metadata":{"execution":{"iopub.status.busy":"2021-10-08T07:53:50.999697Z","iopub.execute_input":"2021-10-08T07:53:51.000298Z","iopub.status.idle":"2021-10-08T07:53:51.008196Z","shell.execute_reply.started":"2021-10-08T07:53:51.000258Z","shell.execute_reply":"2021-10-08T07:53:51.007097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in MAE_cluster_global.index[:10]:\n    ind_labels = (clusterer.labels_ == i)\n    print('cluster:'+str(i) + ' Global MAE: '+str(np.round(MAE_cluster_global[i],4)) + ' count: '+str(np.sum(ind_labels)))\n\n    plt.figure(figsize=(24,6))\n    \n    plt.subplot(1, 4, 1)\n    plt.scatter(embedding[:, 0], embedding[:, 1], s=5+100*ind_labels, c=ind_labels, edgecolors='none', cmap='viridis');\n    plt.title('position')\n    \n    plt.subplot(1, 4, 2)\n    plt.plot(train[ind_labels,:,1].transpose());\n    plt.title('u_in')\n    \n    plt.subplot(1, 4, 3)\n    plt.plot(targets[ind_labels].transpose());\n    plt.title('pressure')\n    \n    plt.subplot(1, 4, 4)\n    plt.plot((targets[ind_labels] - preds_by_id[ind_labels]).transpose());\n    plt.title('error: target-preds')\n    \n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-08T07:53:51.009809Z","iopub.execute_input":"2021-10-08T07:53:51.01025Z","iopub.status.idle":"2021-10-08T07:54:08.786544Z","shell.execute_reply.started":"2021-10-08T07:53:51.010197Z","shell.execute_reply":"2021-10-08T07:54:08.785609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have found some clusters with high MAE. The cluster 4 with 0.52 MAE and 360 instances account for 0.002 global MAE, while a single instance with 15 MAE account for 0,0002 of global MAE. It appears to be 10 times more important to deal with this cluster than to deal with the higher individual MAE. \n\nRegarding specific clusters and how to deal with them, some spike of errors seems concentrated. We might want to try some adapted features. ","metadata":{}},{"cell_type":"markdown","source":"# Study by R and C","metadata":{}},{"cell_type":"code","source":"df_rc = df_train[['breath_id','R','C']].groupby('breath_id').agg(np.min)\n\nfor r in [5,20,50]:\n    for c in [10,20,50]:\n        \n        print('r: '+str(r)+', c: '+str(c))\n        ind_rc = (df_rc.R==r) & (df_rc.C==c)\n\n        X = targets[ind_rc]\n        #u_in\n        X_train = train[ind_rc,:,1]\n        X_targets = targets[ind_rc]\n        X_preds = preds_by_id[ind_rc]\n\n        reducer = umap.UMAP(random_state=42, n_components=2)\n        \n        # switch to embed on u_in (not tested)\n        #embedding = reducer.fit_transform(X_train)\n        embedding = reducer.fit_transform(X)\n\n        clusterer = hdbscan.HDBSCAN(prediction_data=True, min_cluster_size = 20).fit(embedding)\n        u, counts = np.unique(clusterer.labels_, return_counts=True)\n\n        plt.figure(figsize=(20, 8))\n        plt.subplot(1, 2, 1)\n        plt.scatter(embedding[:, 0], embedding[:, 1], s=5, c=clusterer.labels_, edgecolors='none', cmap='jet');\n        plt.title('clusters')\n        plt.subplot(1, 2, 2)\n        plt.scatter(embedding[:, 0], embedding[:, 1], s=5, c=MAE_id[ind_rc.values], edgecolors='none', cmap='jet', norm=colors.LogNorm(vmin=1e-2, vmax=1e0));\n        plt.title('MAE')\n        plt.colorbar();\n        plt.show();\n        \n        \n        MAE_by_cluster = pd.DataFrame({'cluster':clusterer.labels_,'MAE':MAE_id[ind_rc.values]})\n        MAE_cluster = MAE_by_cluster.groupby('cluster').agg(np.sum)['MAE']\n        #remove some base MAE, then divide by gloabl number\n        MAE_cluster_global = (MAE_cluster - counts * 0.15)/counts.sum()\n        MAE_cluster_global = MAE_cluster_global.sort_values(ascending=False)\n        \n        \n        for i in MAE_cluster_global.index[:3]:\n            ind_labels = (clusterer.labels_ == i)\n            print('cluster:'+str(i) + ' Global MAE: '+str(np.round(MAE_cluster_global[i],4)) + ' count: '+str(np.sum(ind_labels)))\n\n            plt.figure(figsize=(24,6))\n            plt.subplot(1, 4, 1)\n            plt.scatter(embedding[:, 0], embedding[:, 1], s=5+100*ind_labels, c=ind_labels, edgecolors='none', cmap='viridis');\n            plt.title('position')\n            plt.subplot(1, 4, 2)\n            plt.plot(X_train[ind_labels,:].transpose());\n            plt.title('u_in')\n            plt.subplot(1, 4, 3)\n            plt.plot(X_targets[ind_labels].transpose());\n            plt.title('pressure')\n            plt.subplot(1, 4, 4)\n            plt.plot((X_targets[ind_labels] - X_preds[ind_labels]).transpose());\n            plt.title('error: target-preds')\n            plt.show()\n        \n        \n        \n        ","metadata":{"execution":{"iopub.status.busy":"2021-10-08T08:09:18.305783Z","iopub.execute_input":"2021-10-08T08:09:18.306152Z","iopub.status.idle":"2021-10-08T08:12:45.054212Z","shell.execute_reply.started":"2021-10-08T08:09:18.306118Z","shell.execute_reply":"2021-10-08T08:12:45.053284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Some observations :\n\n    - High Errors appear quite clustered for some R&C values. \n    \n    - This does seems exploitable as we identify some cluster that seems to be responsible for more than 0.01 MAE gloably.\n    \n    - There might be some work to get better parameters for hdbscan (the three clusters for r: 5, c: 50 should be one). \n    \n    - From the u_in graph and further inverstigation (coloring by first and last u_in - see below) the weirdest cluster appears linked to first and last values of u_in. ","metadata":{}},{"cell_type":"markdown","source":"# Role of u_in_first, u_in_last","metadata":{}},{"cell_type":"code","source":"df_rc = df_train[['breath_id','R','C']].groupby('breath_id').agg(np.min)\n\nfor r in [20]:\n    for c in [50]:\n        \n        print('r: '+str(r)+', c: '+str(c))\n        ind_rc = (df_rc.R==r) & (df_rc.C==c)\n\n        X = targets[ind_rc]\n        #u_in\n        X_train = train[ind_rc,:,1]\n        X_targets = targets[ind_rc]\n        X_preds = preds_by_id[ind_rc]\n\n        reducer = umap.UMAP(random_state=42, n_components=2)\n        embedding = reducer.fit_transform(X)\n\n        clusterer = hdbscan.HDBSCAN(prediction_data=True, min_cluster_size = 20).fit(embedding)\n        u, counts = np.unique(clusterer.labels_, return_counts=True)\n\n        plt.figure(figsize=(20, 8))\n        plt.subplot(1, 2, 1)\n        plt.scatter(embedding[:, 0], embedding[:, 1], s=5, c=clusterer.labels_, edgecolors='none', cmap='jet');\n        plt.title('clusters')\n        plt.subplot(1, 2, 2)\n        plt.scatter(embedding[:, 0], embedding[:, 1], s=5, c=MAE_id[ind_rc.values], edgecolors='none', cmap='jet', norm=colors.LogNorm(vmin=1e-2, vmax=1e0));\n        plt.title('MAE')\n        plt.colorbar();\n        plt.show();\n        \n        plt.figure(figsize=(20, 8))\n        plt.subplot(1, 2, 1)\n        plt.scatter(embedding[:, 0], embedding[:, 1], s=5, c=X_train[:,1], edgecolors='none', cmap='jet');\n        plt.title('u_in_first')\n        plt.colorbar();\n        plt.subplot(1, 2, 2)\n        plt.scatter(embedding[:, 0], embedding[:, 1], s=5, c=X_train[:,-1], edgecolors='none', cmap='jet');\n        plt.title('u_in_last')\n        plt.colorbar();\n        plt.show();","metadata":{"execution":{"iopub.status.busy":"2021-10-08T08:26:15.239674Z","iopub.execute_input":"2021-10-08T08:26:15.240042Z","iopub.status.idle":"2021-10-08T08:26:37.482208Z","shell.execute_reply.started":"2021-10-08T08:26:15.240009Z","shell.execute_reply":"2021-10-08T08:26:37.481376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" My idea was to  get the first and last values separately. I implemented some dedicated functions (in OPTIVER comptetition). But it doesn't exactly seems optimal to build constant time series for LSTM. Maybe we can build a second head that would take individual time series features. From my time series feature engineering notebook (https://www.kaggle.com/lucasmorin/time-series-agregation-functions) we have the following proof of concept for aggregating individual features:","metadata":{}},{"cell_type":"code","source":"get_first = lambda x: x.iloc[0]\nget_first.__name__ = 'get_first'\n\nget_last = lambda x: x.iloc[-1]\nget_last.__name__ = 'get_last'\n\nget_first_fn = [get_first,get_last]\n\ncreate_feature_dict = {\n    'u_in': get_first_fn,\n}\n\ntrain_features = df_train.groupby('breath_id').agg(create_feature_dict)\ntrain_features.columns = ['_'.join(col) for col in train_features.columns]\n\ntrain_features.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-08T08:39:25.438557Z","iopub.execute_input":"2021-10-08T08:39:25.43894Z","iopub.status.idle":"2021-10-08T08:39:29.741991Z","shell.execute_reply.started":"2021-10-08T08:39:25.438876Z","shell.execute_reply":"2021-10-08T08:39:29.740984Z"},"trusted":true},"execution_count":null,"outputs":[]}]}