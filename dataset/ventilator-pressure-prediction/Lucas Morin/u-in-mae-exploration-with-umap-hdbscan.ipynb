{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Taget / Error exploration with umap / hdbscan\n","metadata":{}},{"cell_type":"markdown","source":"The general idea of this notebook is to explore where we get high MAE in terms of clusters of the main feature u_in. We use umap and hdbscan to proceed to dimensionnality reduction and clustering. This allow for 2D plot and aggregation of MAE by cluster to priorize wich clusters we have to deal with. Another version of this notebook, including some changes, exploring pressure and MAE is available here (https://www.kaggle.com/lucasmorin/pressure-mae-exploration-with-umap-hdbscan/edit/run/76608318).","metadata":{}},{"cell_type":"markdown","source":"# Base construction","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport optuna\n\nimport os \nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n# https://www.kaggle.com/c/ventilator-pressure-prediction/discussion/274717\n\nimport matplotlib.pyplot as plt\nimport tensorflow as tf, gc\nfrom tensorflow import keras\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.callbacks import LearningRateScheduler, ReduceLROnPlateau\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\n\nfrom sklearn.metrics import mean_absolute_error as mae\nfrom sklearn.preprocessing import RobustScaler, normalize\nfrom sklearn.model_selection import train_test_split, GroupKFold, KFold\n\nfrom IPython.display import display\n\nimport pickle\n\nDEBUG = False\nTRAIN_MODEL = False\n\ndf_train = pd.read_csv('../input/ventilator-pressure-prediction/train.csv')\ndf_test = pd.read_csv('../input/ventilator-pressure-prediction/test.csv')\nsubmission = pd.read_csv('../input/ventilator-pressure-prediction/sample_submission.csv')\n\nif DEBUG:\n    n = 1000\n    df_train = df_train[:80*n]\n\ndef add_features(df):\n    df['area'] = df['time_step'] * df['u_in']\n    df['area'] = df.groupby('breath_id')['area'].cumsum()\n    \n    df['u_in_cumsum'] = (df['u_in']).groupby(df['breath_id']).cumsum()\n    \n    df['u_in_lag1'] = df.groupby('breath_id')['u_in'].shift(1)\n    df['u_out_lag1'] = df.groupby('breath_id')['u_out'].shift(1)\n    df['u_in_lag_back1'] = df.groupby('breath_id')['u_in'].shift(-1)\n    df['u_out_lag_back1'] = df.groupby('breath_id')['u_out'].shift(-1)\n    df['u_in_lag2'] = df.groupby('breath_id')['u_in'].shift(2)\n    df['u_out_lag2'] = df.groupby('breath_id')['u_out'].shift(2)\n    df['u_in_lag_back2'] = df.groupby('breath_id')['u_in'].shift(-2)\n    df['u_out_lag_back2'] = df.groupby('breath_id')['u_out'].shift(-2)\n    df['u_in_lag3'] = df.groupby('breath_id')['u_in'].shift(3)\n    df['u_out_lag3'] = df.groupby('breath_id')['u_out'].shift(3)\n    df['u_in_lag_back3'] = df.groupby('breath_id')['u_in'].shift(-3)\n    df['u_out_lag_back3'] = df.groupby('breath_id')['u_out'].shift(-3)\n    df['u_in_lag4'] = df.groupby('breath_id')['u_in'].shift(4)\n    df['u_out_lag4'] = df.groupby('breath_id')['u_out'].shift(4)\n    df['u_in_lag_back4'] = df.groupby('breath_id')['u_in'].shift(-4)\n    df['u_out_lag_back4'] = df.groupby('breath_id')['u_out'].shift(-4)\n    df = df.fillna(0)\n    \n    df['breath_id__u_in__max'] = df.groupby(['breath_id'])['u_in'].transform('max')\n    df['breath_id__u_out__max'] = df.groupby(['breath_id'])['u_out'].transform('max')\n    \n    df['u_in_diff1'] = df['u_in'] - df['u_in_lag1']\n    df['u_out_diff1'] = df['u_out'] - df['u_out_lag1']\n    df['u_in_diff2'] = df['u_in'] - df['u_in_lag2']\n    df['u_out_diff2'] = df['u_out'] - df['u_out_lag2']\n    \n    df['breath_id__u_in__diffmax'] = df.groupby(['breath_id'])['u_in'].transform('max') - df['u_in']\n    df['breath_id__u_in__diffmean'] = df.groupby(['breath_id'])['u_in'].transform('mean') - df['u_in']\n    \n    df['breath_id__u_in__diffmax'] = df.groupby(['breath_id'])['u_in'].transform('max') - df['u_in']\n    df['breath_id__u_in__diffmean'] = df.groupby(['breath_id'])['u_in'].transform('mean') - df['u_in']\n    \n    df['u_in_diff3'] = df['u_in'] - df['u_in_lag3']\n    df['u_out_diff3'] = df['u_out'] - df['u_out_lag3']\n    df['u_in_diff4'] = df['u_in'] - df['u_in_lag4']\n    df['u_out_diff4'] = df['u_out'] - df['u_out_lag4']\n    df['cross']= df['u_in']*df['u_out']\n    df['cross2']= df['time_step']*df['u_out']\n    \n    df['R'] = df['R'].astype(str)\n    df['C'] = df['C'].astype(str)\n    df['R__C'] = df[\"R\"].astype(str) + '__' + df[\"C\"].astype(str)\n    df = pd.get_dummies(df)\n    return df\n\ntrain = add_features(df_train)\ntest = add_features(df_test)\n\ntargets = train[['pressure']].to_numpy().reshape(-1, 80)\ntrain.drop(['pressure', 'id', 'breath_id'], axis=1, inplace=True)\ntest = test.drop(['id', 'breath_id'], axis=1)\n\nRS = RobustScaler()\ntrain = RS.fit_transform(train)\ntest = RS.transform(test)\n\ntrain = train.reshape(-1, 80, train.shape[-1])\ntest = test.reshape(-1, 80, train.shape[-1])","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-10-16T15:01:12.119948Z","iopub.execute_input":"2021-10-16T15:01:12.120271Z","iopub.status.idle":"2021-10-16T15:01:42.465479Z","shell.execute_reply.started":"2021-10-16T15:01:12.120241Z","shell.execute_reply":"2021-10-16T15:01:42.464551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Infer train data to compare","metadata":{}},{"cell_type":"code","source":"EPOCH = 300\nBATCH_SIZE = 1024\nNUM_FOLDS = 10\n\ngpu_strategy = tf.distribute.get_strategy()\n\nwith gpu_strategy.scope():\n    kf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=2021)\n    train_preds = []\n    for fold, (train_idx, test_idx) in enumerate(kf.split(train, targets)):\n        if fold>0:\n            break\n        K.clear_session()\n        print('-'*15, '>', f'Fold {fold+1}', '<', '-'*15)\n        checkpoint_filepath = f\"folds{fold}.hdf5\"\n        model = keras.models.load_model('../input/finetune-of-tensorflow-bidirectional-lstm/'+checkpoint_filepath)\n        train_preds.append(model.predict(train, batch_size=BATCH_SIZE, verbose=2).squeeze().reshape(-1, 1).squeeze())\n        del model\n        gc.collect()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-10-16T14:59:54.341688Z","iopub.execute_input":"2021-10-16T14:59:54.342034Z","iopub.status.idle":"2021-10-16T15:00:14.067667Z","shell.execute_reply.started":"2021-10-16T14:59:54.341997Z","shell.execute_reply":"2021-10-16T15:00:14.066803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if DEBUG: n=1000\nelse: n=75450\n    \ndf = pd.DataFrame({'target': targets.flatten(), 'preds': np.mean(train_preds,axis=0), 'id': [i+1 for i in range(n) for e in range(80)]})\npreds_by_id = np.mean(train_preds,axis=0).reshape(-1, 80)\ndf['u_out_mask'] = (train[:,:,2] == -1).flatten()\ndf['error'] = np.abs(df['target']-df['preds'])\ndf_masked = df.mask(~df['u_out_mask'])\nMAE_id = df_masked.groupby('id').agg(np.nanmean)['error']","metadata":{"execution":{"iopub.status.busy":"2021-10-16T15:04:23.976891Z","iopub.execute_input":"2021-10-16T15:04:23.977213Z","iopub.status.idle":"2021-10-16T15:04:24.039553Z","shell.execute_reply.started":"2021-10-16T15:04:23.977183Z","shell.execute_reply":"2021-10-16T15:04:24.038741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAE_file = \"MAE_id.pkl\"\n\nwith open(MAE_file,'wb') as f:\n    pickle.dump(MAE_id, f)\n    \nMAE_id.to_csv(\"MAE_id.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-10-16T15:05:29.596513Z","iopub.execute_input":"2021-10-16T15:05:29.596858Z","iopub.status.idle":"2021-10-16T15:05:29.604336Z","shell.execute_reply.started":"2021-10-16T15:05:29.596829Z","shell.execute_reply":"2021-10-16T15:05:29.603558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Basic Exploration","metadata":{}},{"cell_type":"code","source":"MAE_id.describe()","metadata":{"execution":{"iopub.status.busy":"2021-10-16T14:59:07.793746Z","iopub.status.idle":"2021-10-16T14:59:07.794245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.log(MAE_id).hist(bins=100)","metadata":{"execution":{"iopub.status.busy":"2021-10-08T09:11:54.785566Z","iopub.execute_input":"2021-10-08T09:11:54.786271Z","iopub.status.idle":"2021-10-08T09:11:55.379919Z","shell.execute_reply.started":"2021-10-08T09:11:54.78623Z","shell.execute_reply":"2021-10-08T09:11:55.378949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAE_id_sorted = MAE_id.sort_values(ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-10-08T09:11:55.387185Z","iopub.execute_input":"2021-10-08T09:11:55.387881Z","iopub.status.idle":"2021-10-08T09:11:55.418157Z","shell.execute_reply.started":"2021-10-08T09:11:55.387841Z","shell.execute_reply":"2021-10-08T09:11:55.417356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# worst predictions","metadata":{}},{"cell_type":"code","source":"for i in MAE_id_sorted.index[:10]:\n    ind = np.int(i)\n    ind_labels = (MAE_id.index.values == i)\n    \n    print('cluster:'+str(i) + ' MAE: '+str(np.round(MAE_id_sorted[i],3)))\n\n    plt.figure(figsize=(24,8))\n    \n    plt.subplot(1, 3, 1)\n    plt.plot(train[ind_labels,:,1].transpose());\n    plt.title('u_in')\n    \n    plt.subplot(1, 3, 2)\n    plt.plot(targets[ind_labels].transpose(), label='target');\n    plt.plot(preds_by_id[ind_labels].transpose(), label='preds');\n    plt.legend()\n    plt.title('pressure')\n    \n    plt.subplot(1, 3, 3)\n    plt.plot((targets[ind_labels] - preds_by_id[ind_labels]).transpose());\n    plt.title('error')\n    \n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-08T09:11:55.419716Z","iopub.execute_input":"2021-10-08T09:11:55.420446Z","iopub.status.idle":"2021-10-08T09:11:59.519969Z","shell.execute_reply.started":"2021-10-08T09:11:55.420392Z","shell.execute_reply":"2021-10-08T09:11:59.518962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"LTSM seems to be able to predict weird noise, but has some constant error on some ids. However such errors seems rather rare. The instance with 15 MAE only account for 0.0002 global MAE. We must find if some clusters of output have more important global error (less individual MAE but high number of instances).","metadata":{}},{"cell_type":"markdown","source":"# Explaining worst predictions\n\nWorst errors seems to come from a shift in values. \nFrom the discussion here: https://www.kaggle.com/c/ventilator-pressure-prediction/discussion/278935 we see that most of the time a change in R and C only translate in some shift. So my main hypothese for explaining the worst predictions was some noise in R & C that translate into shiting the predictions. See the discussion here:https://www.kaggle.com/c/ventilator-pressure-prediction/discussion/280356\n\nOne way to test this hypothese was to try and switch R & C values and see if we get better predictions.\nWith at least 3-4 values of the top 10 MAE explained by a shift in C, this appears something to be worth investigating more. Maybe we can try to predict better R and C from u_in values.","metadata":{}},{"cell_type":"code","source":"df_trainw = pd.read_csv('../input/ventilator-pressure-prediction/train.csv')\ndf_trainw['breath_id'] = [i+1 for i in range(75450) for e in range(80)]\n\nfor i in MAE_id_sorted.index[:10]:\n    \n    ind = np.int(i)\n    ind_labels = (MAE_id.index.values == i)\n    \n    print(ind)\n    \n    df = df_trainw[df_trainw.breath_id == np.int(i)].reset_index(drop=True)\n\n    print('original R: '+str(df.R.unique()[0]))\n    print('original C: '+str(df.C.unique()[0]))\n    \n    \n    basew_all = pd.DataFrame()\n\n    for R in [20, 50,  5]:\n        for C in [50, 20, 10]:\n            basew_temp = df.copy()\n            basew_temp['R']=R\n            basew_temp['C']=C\n            basew_temp['breath_id'] = basew_temp['breath_id']*100000 + C * 1000 + R\n            basew_all = basew_all.append(basew_temp)\n    \n    trainw = add_features(basew_all)\n    trainw.drop(['pressure', 'id', 'breath_id'], axis=1, inplace=True)\n    trainw = RS.transform(trainw)\n    trainw = trainw.reshape(-1, 80, trainw.shape[-1])\n    \n    gpu_strategy = tf.distribute.get_strategy()\n\n    with gpu_strategy.scope():\n        kf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=2021)\n        trainw_preds = []\n        for fold, (train_idx, test_idx) in enumerate(kf.split(train, targets)):\n            if fold>0:\n                break\n            K.clear_session()\n            print('-'*15, '>', f'Fold {fold+1}', '<', '-'*15)\n            checkpoint_filepath = f\"folds{fold}.hdf5\"\n            model = keras.models.load_model('../input/finetune-of-tensorflow-bidirectional-lstm/'+checkpoint_filepath)\n            trainw_preds.append(model.predict(trainw, batch_size=BATCH_SIZE, verbose=2).squeeze().reshape(-1, 1).squeeze())\n            del model\n            gc.collect()\n    \n    preds = trainw_preds[0]\n    \n    plt.figure(figsize=(16,8))\n    \n    plot_max = np.max([np.max(preds), np.max(df.pressure), np.max(preds_by_id[ind_labels])])\n    plot_min = np.min([np.max(preds), np.min(df.pressure), np.min(preds_by_id[ind_labels])])\n    \n    \n    plt.subplot(1, 2, 1)\n    plt.plot(df.pressure, label='target');\n    plt.plot(preds_by_id[ind_labels].transpose(), label='preds');\n    plt.ylim((np.min(plot_min), np.max(plot_max))) \n    plt.legend()\n    plt.title('pressure')\n    \n    plt.subplot(1, 2, 2)\n    plt.plot(preds.reshape(-1,80).transpose(), label = ('R '+basew_all.R.astype('str') +' - C '+ basew_all.C.astype('str')).unique());\n    plt.ylim((np.min(plot_min), np.max(plot_max))) \n    plt.title('predicted_pressure')\n    plt.legend()\n    plt.show()\n    \ndel trainw","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# UMAP / Hdbscan","metadata":{}},{"cell_type":"code","source":"!mkdir -p /tmp/pip/cache/\n!cp ../input/hdbscan0827-whl/hdbscan-0.8.27-cp37-cp37m-linux_x86_64.whl /tmp/pip/cache/\n!pip install --no-index --find-links /tmp/pip/cache/ hdbscan","metadata":{"execution":{"iopub.status.busy":"2021-10-08T09:11:59.521964Z","iopub.execute_input":"2021-10-08T09:11:59.522292Z","iopub.status.idle":"2021-10-08T09:12:09.008019Z","shell.execute_reply.started":"2021-10-08T09:11:59.522257Z","shell.execute_reply":"2021-10-08T09:12:09.007043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# exploration on u_in - UMAP Embedding - hdbscan clustering","metadata":{}},{"cell_type":"markdown","source":"As the idea is to observe the main feature, we embed u_in with umap then we fit hdbscan on the embedding. Maybe better to fit hdbscan on the original data...","metadata":{}},{"cell_type":"code","source":"%%time\n\nimport hdbscan\nimport umap\n\nn=75450\n\n# Switch to do the exploration on pressure\nX = train[:,:,1]\n#X = targets\n\nreducer = umap.UMAP(random_state=42, n_components=2)\nembedding = reducer.fit_transform(X)\nclusterer = hdbscan.HDBSCAN(prediction_data=True, min_cluster_size = 50).fit(embedding)\nu, counts = np.unique(clusterer.labels_, return_counts=True)\n\nprint(u)\nprint(counts)","metadata":{"execution":{"iopub.status.busy":"2021-10-08T09:19:21.253235Z","iopub.execute_input":"2021-10-08T09:19:21.253626Z","iopub.status.idle":"2021-10-08T09:21:24.529295Z","shell.execute_reply.started":"2021-10-08T09:19:21.253591Z","shell.execute_reply":"2021-10-08T09:21:24.524081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10, 8))\nplt.scatter(embedding[:, 0], embedding[:, 1], s=5, c=clusterer.labels_, edgecolors='none', cmap='jet');","metadata":{"execution":{"iopub.status.busy":"2021-10-08T09:21:28.378277Z","iopub.execute_input":"2021-10-08T09:21:28.378653Z","iopub.status.idle":"2021-10-08T09:21:29.047755Z","shell.execute_reply.started":"2021-10-08T09:21:28.378619Z","shell.execute_reply":"2021-10-08T09:21:29.046689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Doesn't seems to good of a clustering... there seems to be lot of 'outliers'.","metadata":{}},{"cell_type":"markdown","source":"# Plot error - log_scale","metadata":{}},{"cell_type":"code","source":"import matplotlib.colors as colors\nplt.figure(figsize=(10, 8))\nplt.scatter(embedding[:, 0], embedding[:, 1], s=5, c=MAE_id[:n], edgecolors='none', cmap='jet', norm=colors.LogNorm(vmin=MAE_id[:n].quantile(0.05), vmax=MAE_id[:n].quantile(0.95)));\nplt.colorbar();","metadata":{"execution":{"iopub.status.busy":"2021-10-08T09:21:48.463634Z","iopub.execute_input":"2021-10-08T09:21:48.463965Z","iopub.status.idle":"2021-10-08T09:21:49.73919Z","shell.execute_reply.started":"2021-10-08T09:21:48.463935Z","shell.execute_reply":"2021-10-08T09:21:49.738413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# MAE by cluster","metadata":{}},{"cell_type":"markdown","source":"not exact as mean of MAE is not exactly the evaluation metric (due to u_out not starting at the same time), but should be ok for exploration.","metadata":{}},{"cell_type":"code","source":"MAE_by_cluster = pd.DataFrame({'cluster':clusterer.labels_,'MAE':MAE_id})\n\nMAE_cluster = MAE_by_cluster.groupby('cluster').agg(np.sum)['MAE']\n\n#remove some base MAE, then divide by gloabl number\nMAE_cluster_global = (MAE_cluster - counts * 0.1)/counts.sum()\nMAE_cluster_global = MAE_cluster_global.sort_values(ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-10-08T09:21:55.312323Z","iopub.execute_input":"2021-10-08T09:21:55.312667Z","iopub.status.idle":"2021-10-08T09:21:55.326426Z","shell.execute_reply.started":"2021-10-08T09:21:55.312635Z","shell.execute_reply":"2021-10-08T09:21:55.325039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAE_cluster_global","metadata":{"execution":{"iopub.status.busy":"2021-10-08T09:21:56.114989Z","iopub.execute_input":"2021-10-08T09:21:56.11532Z","iopub.status.idle":"2021-10-08T09:21:56.12462Z","shell.execute_reply.started":"2021-10-08T09:21:56.115289Z","shell.execute_reply":"2021-10-08T09:21:56.123626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in MAE_cluster_global.index[:10]:\n    ind_labels = (clusterer.labels_ == i)\n    print('cluster:'+str(i) + ' Global MAE: '+str(np.round(MAE_cluster_global[i],4)) + ' count: '+str(np.sum(ind_labels)))\n\n    plt.figure(figsize=(24,6))\n    \n    plt.subplot(1, 4, 1)\n    plt.scatter(embedding[:, 0], embedding[:, 1], s=5+100*ind_labels, c=ind_labels, edgecolors='none', cmap='viridis');\n    plt.title('position')\n    \n    plt.subplot(1, 4, 2)\n    plt.plot(train[ind_labels,:,1].transpose());\n    plt.title('u_in')\n    \n    plt.subplot(1, 4, 3)\n    plt.plot(targets[ind_labels].transpose());\n    plt.title('pressure')\n    \n    plt.subplot(1, 4, 4)\n    plt.plot((targets[ind_labels] - preds_by_id[ind_labels]).transpose());\n    plt.title('error: target-preds')\n    \n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-08T09:21:57.215891Z","iopub.execute_input":"2021-10-08T09:21:57.21621Z","iopub.status.idle":"2021-10-08T09:22:39.37952Z","shell.execute_reply.started":"2021-10-08T09:21:57.21618Z","shell.execute_reply":"2021-10-08T09:22:39.378692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have found some clusters with high MAE. The cluster 4 with 0.52 MAE and 360 instances account for 0.002 global MAE, while a single instance with 15 MAE account for 0,0002 of global MAE. It appears to be 10 times more important to deal with this cluster than to deal with the higher individual MAE. \n\nRegarding specific clusters and how to deal with them, some spike of errors seems concentrated. We might want to try some adapted features. ","metadata":{}},{"cell_type":"markdown","source":"# Study with u_out masking","metadata":{}},{"cell_type":"markdown","source":"We mask u_in with ~u_out to see if we get a better clustering.","metadata":{}},{"cell_type":"code","source":"%%time\n\nimport hdbscan\nimport umap\n\nn=75450\n\n# Switch to do the exploration on pressure\nX = train[:,:,1]\nX_mask = X*((train[:,:,2]==-1)+0)\n#X = targets\n\nreducer_mask = umap.UMAP(random_state=42, n_components=2)\nembedding_mask = reducer_mask.fit_transform(X_mask)\nclusterer_mask = hdbscan.HDBSCAN(prediction_data=True, min_cluster_size = 50).fit(embedding_mask)\nu_mask, counts_mask = np.unique(clusterer_mask.labels_, return_counts=True)\n\nprint(u_mask)\nprint(counts_mask)","metadata":{"execution":{"iopub.status.busy":"2021-10-08T09:28:58.361808Z","iopub.execute_input":"2021-10-08T09:28:58.362148Z","iopub.status.idle":"2021-10-08T09:30:55.201648Z","shell.execute_reply.started":"2021-10-08T09:28:58.362115Z","shell.execute_reply":"2021-10-08T09:30:55.200538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10, 8))\nplt.scatter(embedding_mask[:, 0], embedding_mask[:, 1], s=5, c=clusterer_mask.labels_, edgecolors='none', cmap='jet');","metadata":{"execution":{"iopub.status.busy":"2021-10-08T09:31:14.880776Z","iopub.execute_input":"2021-10-08T09:31:14.881143Z","iopub.status.idle":"2021-10-08T09:31:15.604268Z","shell.execute_reply.started":"2021-10-08T09:31:14.881106Z","shell.execute_reply":"2021-10-08T09:31:15.603276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.colors as colors\nplt.figure(figsize=(10, 8))\nplt.scatter(embedding_mask[:, 0], embedding_mask[:, 1], s=5, c=MAE_id[:n], edgecolors='none', cmap='jet', norm=colors.LogNorm(vmin=MAE_id[:n].quantile(0.05), vmax=MAE_id[:n].quantile(0.95)));\nplt.colorbar();","metadata":{"execution":{"iopub.status.busy":"2021-10-08T09:31:16.2787Z","iopub.execute_input":"2021-10-08T09:31:16.279053Z","iopub.status.idle":"2021-10-08T09:31:17.445419Z","shell.execute_reply.started":"2021-10-08T09:31:16.279004Z","shell.execute_reply":"2021-10-08T09:31:17.444284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Study by R and C","metadata":{}},{"cell_type":"code","source":"df_rc = df_train[['breath_id','R','C']].groupby('breath_id').agg(np.min)\n\nfor r in [5,20,50]:\n    for c in [10,20,50]:\n        \n        print('r: '+str(r)+', c: '+str(c))\n        ind_rc = (df_rc.R==r) & (df_rc.C==c)\n\n        X = targets[ind_rc]\n        #u_in\n        X_train = train[ind_rc,:,1]\n        X_targets = targets[ind_rc]\n        X_preds = preds_by_id[ind_rc]\n\n        reducer = umap.UMAP(random_state=42, n_components=2)\n        \n        # switch to embed on u_in (not tested)\n        embedding = reducer.fit_transform(X_train)\n        #embedding = reducer.fit_transform(X)\n\n        clusterer = hdbscan.HDBSCAN(prediction_data=True, min_cluster_size = 20).fit(embedding)\n        u, counts = np.unique(clusterer.labels_, return_counts=True)\n\n        plt.figure(figsize=(20, 8))\n        plt.subplot(1, 2, 1)\n        plt.scatter(embedding[:, 0], embedding[:, 1], s=5, c=clusterer.labels_, edgecolors='none', cmap='jet');\n        plt.title('clusters')\n        plt.subplot(1, 2, 2)\n        plt.scatter(embedding[:, 0], embedding[:, 1], s=5, c=MAE_id[ind_rc.values], edgecolors='none', cmap='jet', norm=colors.LogNorm(vmin=1e-2, vmax=1e0));\n        plt.title('MAE')\n        plt.colorbar();\n        plt.show();\n        \n        \n        MAE_by_cluster = pd.DataFrame({'cluster':clusterer.labels_,'MAE':MAE_id[ind_rc.values]})\n        MAE_cluster = MAE_by_cluster.groupby('cluster').agg(np.sum)['MAE']\n        #remove some base MAE, then divide by gloabl number\n        MAE_cluster_global = (MAE_cluster - counts * 0.15)/counts.sum()\n        MAE_cluster_global = MAE_cluster_global.sort_values(ascending=False)\n        \n        \n        for i in MAE_cluster_global.index[:3]:\n            ind_labels = (clusterer.labels_ == i)\n            print('cluster:'+str(i) + ' Global MAE: '+str(np.round(MAE_cluster_global[i],4)) + ' count: '+str(np.sum(ind_labels)))\n\n            plt.figure(figsize=(24,6))\n            plt.subplot(1, 4, 1)\n            plt.scatter(embedding[:, 0], embedding[:, 1], s=5+100*ind_labels, c=ind_labels, edgecolors='none', cmap='viridis');\n            plt.title('position')\n            plt.subplot(1, 4, 2)\n            plt.plot(X_train[ind_labels,:].transpose());\n            plt.title('u_in')\n            plt.subplot(1, 4, 3)\n            plt.plot(X_targets[ind_labels].transpose());\n            plt.title('pressure')\n            plt.subplot(1, 4, 4)\n            plt.plot((X_targets[ind_labels] - X_preds[ind_labels]).transpose());\n            plt.title('error: target-preds')\n            plt.show()\n        \n        \n        \n        ","metadata":{"execution":{"iopub.status.busy":"2021-10-08T09:38:44.93516Z","iopub.execute_input":"2021-10-08T09:38:44.935555Z","iopub.status.idle":"2021-10-08T09:42:14.779254Z","shell.execute_reply.started":"2021-10-08T09:38:44.935518Z","shell.execute_reply":"2021-10-08T09:42:14.778282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Some observations :\n\n    - High Errors appear quite clustered for some R&C values. \n    \n    - This does seems exploitable as we identify some cluster that seems to be responsible for more than 0.01 MAE gloably.\n    \n    - There might be some work to get better parameters for hdbscan (the three clusters for r: 5, c: 50 should be one). \n    \n    - From the u_in graph and further inverstigation (coloring by first and last u_in - see below) the weirdest cluster appears linked to first and last values of u_in. ","metadata":{}},{"cell_type":"markdown","source":"# Role of u_in_first, u_in_last","metadata":{}},{"cell_type":"code","source":"df_rc = df_train[['breath_id','R','C']].groupby('breath_id').agg(np.min)\n\nfor r in [20]:\n    for c in [50]:\n        \n        print('r: '+str(r)+', c: '+str(c))\n        ind_rc = (df_rc.R==r) & (df_rc.C==c)\n\n        X = targets[ind_rc]\n        #u_in\n        X_train = train[ind_rc,:,1]\n        X_targets = targets[ind_rc]\n        X_preds = preds_by_id[ind_rc]\n\n        reducer = umap.UMAP(random_state=42, n_components=2)\n        embedding = reducer.fit_transform(X)\n\n        clusterer = hdbscan.HDBSCAN(prediction_data=True, min_cluster_size = 20).fit(embedding)\n        u, counts = np.unique(clusterer.labels_, return_counts=True)\n\n        plt.figure(figsize=(20, 8))\n        plt.subplot(1, 2, 1)\n        plt.scatter(embedding[:, 0], embedding[:, 1], s=5, c=clusterer.labels_, edgecolors='none', cmap='jet');\n        plt.title('clusters')\n        plt.subplot(1, 2, 2)\n        plt.scatter(embedding[:, 0], embedding[:, 1], s=5, c=MAE_id[ind_rc.values], edgecolors='none', cmap='jet', norm=colors.LogNorm(vmin=1e-2, vmax=1e0));\n        plt.title('MAE')\n        plt.colorbar();\n        plt.show();\n        \n        plt.figure(figsize=(20, 8))\n        plt.subplot(1, 2, 1)\n        plt.scatter(embedding[:, 0], embedding[:, 1], s=5, c=X_train[:,1], edgecolors='none', cmap='jet');\n        plt.title('u_in_first')\n        plt.colorbar();\n        plt.subplot(1, 2, 2)\n        plt.scatter(embedding[:, 0], embedding[:, 1], s=5, c=X_train[:,-1], edgecolors='none', cmap='jet');\n        plt.title('u_in_last')\n        plt.colorbar();\n        plt.show();","metadata":{"execution":{"iopub.status.busy":"2021-10-08T08:26:15.239674Z","iopub.execute_input":"2021-10-08T08:26:15.240042Z","iopub.status.idle":"2021-10-08T08:26:37.482208Z","shell.execute_reply.started":"2021-10-08T08:26:15.240009Z","shell.execute_reply":"2021-10-08T08:26:37.481376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" My idea was to  get the first and last values separately. I implemented some dedicated functions. But it doesn't exactly seems optimal to build constant time series for LSTM. Maybe we can build a second head that would take individual time series features. From my time series feature engineering notebook here :https://www.kaggle.com/lucasmorin/time-series-agregation-functions:","metadata":{}},{"cell_type":"code","source":"get_first = lambda x: x.iloc[0]\nget_first.__name__ = 'get_first'\n\nget_last = lambda x: x.iloc[-1]\nget_last.__name__ = 'get_last'\n\nget_first_fn = [get_first,get_last]\n\ncreate_feature_dict = {\n    'u_in': get_first_fn,\n}\n\ntrain_features = df_train.groupby('breath_id').agg(create_feature_dict)\ntrain_features.columns = ['_'.join(col) for col in train_features.columns]\n\ntrain_features.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-08T08:39:25.438557Z","iopub.execute_input":"2021-10-08T08:39:25.43894Z","iopub.status.idle":"2021-10-08T08:39:29.741991Z","shell.execute_reply.started":"2021-10-08T08:39:25.438876Z","shell.execute_reply":"2021-10-08T08:39:29.740984Z"},"trusted":true},"execution_count":null,"outputs":[]}]}