{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Google Brain - Ventilator Pressure Prediction \n### 1D-Unet with Deep Supervision Approach\n\nVladimir Sim√µes da Luz Junior\n\n[LinkedIn](https://www.linkedin.com/in/vladimir-simoes-da-luz-junior/)\n\n[GitHub](https://www.linkedin.com/in/vladimir-simoes-da-luz-junior/)\n","metadata":{}},{"cell_type":"markdown","source":"This solution makes reference to [PPG2ABP](https://arxiv.org/abs/2005.01669), that have used a 1 dimensional U-Net with deep supervision to predict the arterial blood pressure waveform from the photopletysmography signal.\n\nWe have selected each individual breath from the Google Brain - VPP dataset as one single 1D input array containing the *u_in* time series. The model architecture encodes the feature map from the *u_in* breath signal and decode the feature into the *pressure* signal from the ventilator. ","metadata":{}},{"cell_type":"markdown","source":"## Libraries","metadata":{}},{"cell_type":"code","source":"#!pip install h5py==2.9.0 numpy==1.17.0 tqdm==4.19.5 matplotlib==2.2.3 seaborn==0.9.0 scipy==1.4.1 scikit-learn==0.19.2 tensorflow-gpu==1.15.4 Keras==2.2.4 Keras-Applications==1.0.8 Keras-Preprocessing==1.1.0 --force\n","metadata":{"execution":{"iopub.status.busy":"2021-10-29T18:59:30.314686Z","iopub.execute_input":"2021-10-29T18:59:30.315018Z","iopub.status.idle":"2021-10-29T18:59:30.33537Z","shell.execute_reply.started":"2021-10-29T18:59:30.314933Z","shell.execute_reply":"2021-10-29T18:59:30.334654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n#import tensorflow as tf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time","metadata":{"execution":{"iopub.status.busy":"2021-10-29T18:59:30.337374Z","iopub.execute_input":"2021-10-29T18:59:30.338239Z","iopub.status.idle":"2021-10-29T18:59:31.208549Z","shell.execute_reply.started":"2021-10-29T18:59:30.338193Z","shell.execute_reply":"2021-10-29T18:59:31.207753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Models \n### Approximation Network - UNetDS64","metadata":{}},{"cell_type":"code","source":"\"\"\"\n    Models used in experiments\n\"\"\"\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, UpSampling1D, concatenate, BatchNormalization, Activation, add\nfrom tensorflow.keras.models import Model, model_from_json\n#from keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\n\n\n\ndef UNetDS64(length, n_channel=1):\n    \"\"\"\n        Deeply supervised U-Net with kernels multiples of 64\n    \n    Arguments:\n        length {int} -- length of the input signal\n    \n    Keyword Arguments:\n        n_channel {int} -- number of channels in the output (default: {1})\n    \n    Returns:\n        keras.model -- created model\n    \"\"\"\n    \n    x = 64\n\n    inputs = Input((length, n_channel))\n    conv1 = Conv1D(x,3, activation='relu', padding='same')(inputs)\n    conv1 = BatchNormalization()(conv1)\n    conv1 = Conv1D(x,3, activation='relu', padding='same')(conv1)\n    conv1 = BatchNormalization()(conv1)\n    pool1 = MaxPooling1D(pool_size=2)(conv1)\n\n    conv2 = Conv1D(x*2,3, activation='relu', padding='same')(pool1)\n    conv2 = BatchNormalization()(conv2)\n    conv2 = Conv1D(x*2,3, activation='relu', padding='same')(conv2)\n    conv2 = BatchNormalization()(conv2)\n    pool2 = MaxPooling1D(pool_size=2)(conv2)\n\n    conv3 = Conv1D(x*4,3, activation='relu', padding='same')(pool2)\n    conv3 = BatchNormalization()(conv3)\n    conv3 = Conv1D(x*4,3, activation='relu', padding='same')(conv3)\n    conv3 = BatchNormalization()(conv3)\n    pool3 = MaxPooling1D(pool_size=2)(conv3)\n\n    conv4 = Conv1D(x*8,3, activation='relu', padding='same')(pool3)\n    conv4 = BatchNormalization()(conv4)\n    conv4 = Conv1D(x*8,3, activation='relu', padding='same')(conv4)\n    conv4 = BatchNormalization()(conv4)\n    pool4 = MaxPooling1D(pool_size=2)(conv4)\n\n    conv5 = Conv1D(x*16, 3, activation='relu', padding='same')(pool4)\n    conv5 = BatchNormalization()(conv5)\n    conv5 = Conv1D(x*16, 3, activation='relu', padding='same')(conv5)\n    conv5 = BatchNormalization()(conv5)\n    \n    level4 = Conv1D(1, 1, name=\"level4\")(conv5)\n\n    up6 = concatenate([UpSampling1D(size=2)(conv5), conv4], axis=2)\n    conv6 = Conv1D(x*8, 3, activation='relu', padding='same')(up6)\n    conv6 = BatchNormalization()(conv6)\n    conv6 = Conv1D(x*8, 3, activation='relu', padding='same')(conv6)\n    conv6 = BatchNormalization()(conv6)\n    \n    level3 = Conv1D(1, 1, name=\"level3\")(conv6)\n\n    up7 = concatenate([UpSampling1D(size=2)(conv6), conv3], axis=2)\n    conv7 = Conv1D(x*4, 3, activation='relu', padding='same')(up7)\n    conv7 = BatchNormalization()(conv7)\n    conv7 = Conv1D(x*4, 3, activation='relu', padding='same')(conv7)\n    conv7 = BatchNormalization()(conv7)\n    \n    level2 = Conv1D(1, 1, name=\"level2\")(conv7)\n\n    up8 = concatenate([UpSampling1D(size=2)(conv7), conv2], axis=2)\n    conv8 = Conv1D(x*2, 3, activation='relu', padding='same')(up8)\n    conv8 = BatchNormalization()(conv8)\n    conv8 = Conv1D(x*2, 3, activation='relu', padding='same')(conv8)\n    conv8 = BatchNormalization()(conv8)\n    \n    level1 = Conv1D(1, 1, name=\"level1\")(conv8)\n\n    up9 = concatenate([UpSampling1D(size=2)(conv8), conv1], axis=2)\n    conv9 = Conv1D(x, 3, activation='relu', padding='same')(up9)\n    conv9 = BatchNormalization()(conv9)\n    conv9 = Conv1D(x, 3, activation='relu', padding='same')(conv9)\n    conv9 = BatchNormalization()(conv9)\n\n    out = Conv1D(1, 1, name=\"out\")(conv9)\n\n    model = Model(inputs=[inputs], outputs=[out, level1, level2, level3, level4])\n    \n    \n\n    return model\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-10-29T18:59:31.210111Z","iopub.execute_input":"2021-10-29T18:59:31.210816Z","iopub.status.idle":"2021-10-29T18:59:37.05752Z","shell.execute_reply.started":"2021-10-29T18:59:31.210772Z","shell.execute_reply":"2021-10-29T18:59:37.05688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load training data and first exploratory data analysis","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv('../input/ventilator-pressure-prediction/train.csv')","metadata":{"execution":{"iopub.status.busy":"2021-10-29T18:59:37.254921Z","iopub.execute_input":"2021-10-29T18:59:37.255668Z","iopub.status.idle":"2021-10-29T18:59:47.266309Z","shell.execute_reply.started":"2021-10-29T18:59:37.255588Z","shell.execute_reply":"2021-10-29T18:59:47.265637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.shape","metadata":{"execution":{"iopub.status.busy":"2021-10-29T18:59:47.267446Z","iopub.execute_input":"2021-10-29T18:59:47.267726Z","iopub.status.idle":"2021-10-29T18:59:47.276059Z","shell.execute_reply.started":"2021-10-29T18:59:47.267697Z","shell.execute_reply":"2021-10-29T18:59:47.27512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-29T18:59:47.277351Z","iopub.execute_input":"2021-10-29T18:59:47.27774Z","iopub.status.idle":"2021-10-29T18:59:47.300833Z","shell.execute_reply.started":"2021-10-29T18:59:47.277692Z","shell.execute_reply":"2021-10-29T18:59:47.299972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.time_step.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-10-29T18:59:47.302215Z","iopub.execute_input":"2021-10-29T18:59:47.302465Z","iopub.status.idle":"2021-10-29T18:59:48.767044Z","shell.execute_reply.started":"2021-10-29T18:59:47.302435Z","shell.execute_reply":"2021-10-29T18:59:48.766124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unique_breaths = df_train['breath_id'].unique()\nnum_breaths = len(unique_breaths)\nprint(num_breaths)","metadata":{"execution":{"iopub.status.busy":"2021-10-29T18:59:48.772426Z","iopub.execute_input":"2021-10-29T18:59:48.774604Z","iopub.status.idle":"2021-10-29T18:59:48.851666Z","shell.execute_reply.started":"2021-10-29T18:59:48.774153Z","shell.execute_reply":"2021-10-29T18:59:48.850997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['breath_id'][:500].plot();","metadata":{"execution":{"iopub.status.busy":"2021-10-29T18:59:48.852765Z","iopub.execute_input":"2021-10-29T18:59:48.852968Z","iopub.status.idle":"2021-10-29T18:59:49.157591Z","shell.execute_reply.started":"2021-10-29T18:59:48.852943Z","shell.execute_reply":"2021-10-29T18:59:49.156703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"breath_lengths = df_train[['id','breath_id']].groupby('breath_id').count()['id']\nbreath_lengths.unique()","metadata":{"execution":{"iopub.status.busy":"2021-10-29T18:59:49.158466Z","iopub.execute_input":"2021-10-29T18:59:49.158685Z","iopub.status.idle":"2021-10-29T18:59:49.456783Z","shell.execute_reply.started":"2021-10-29T18:59:49.158659Z","shell.execute_reply":"2021-10-29T18:59:49.455882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BREATH_LENGTH = breath_lengths.unique()[0]","metadata":{"execution":{"iopub.status.busy":"2021-10-29T18:59:49.458877Z","iopub.execute_input":"2021-10-29T18:59:49.459136Z","iopub.status.idle":"2021-10-29T18:59:49.46539Z","shell.execute_reply.started":"2021-10-29T18:59:49.459109Z","shell.execute_reply":"2021-10-29T18:59:49.464176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## R and C\nR and C values are constant within each breath (having zero standard deviation)\n\n","metadata":{}},{"cell_type":"code","source":"r_c_std_in_breaths = df_train[['breath_id','R','C']].groupby('breath_id').std()\nprint(r_c_std_in_breaths['R'].unique())\nprint(r_c_std_in_breaths['C'].unique())","metadata":{"execution":{"iopub.status.busy":"2021-10-29T18:59:49.468863Z","iopub.execute_input":"2021-10-29T18:59:49.469131Z","iopub.status.idle":"2021-10-29T18:59:49.89772Z","shell.execute_reply.started":"2021-10-29T18:59:49.46907Z","shell.execute_reply":"2021-10-29T18:59:49.896848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"R has only three distinct values:","metadata":{}},{"cell_type":"code","source":"r_values = df_train[['breath_id', 'R']].groupby('breath_id').mean()['R']\nprint(r_values)\nprint()\nprint('Unique values:')\nprint(r_values.value_counts())\n\nr_unique = np.sort(r_values.unique()).astype(int)","metadata":{"execution":{"iopub.status.busy":"2021-10-29T18:59:49.901645Z","iopub.execute_input":"2021-10-29T18:59:49.902566Z","iopub.status.idle":"2021-10-29T18:59:50.164624Z","shell.execute_reply.started":"2021-10-29T18:59:49.902413Z","shell.execute_reply":"2021-10-29T18:59:50.163653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So does C:\n","metadata":{}},{"cell_type":"code","source":"c_values = df_train[['breath_id', 'C']].groupby('breath_id').mean()['C']\nprint(c_values)\nprint()\nprint('Unique values:')\nprint(c_values.value_counts())\n\nc_unique = np.sort(c_values.unique()).astype(int)","metadata":{"execution":{"iopub.status.busy":"2021-10-29T18:59:50.165867Z","iopub.execute_input":"2021-10-29T18:59:50.166085Z","iopub.status.idle":"2021-10-29T18:59:50.429005Z","shell.execute_reply.started":"2021-10-29T18:59:50.166059Z","shell.execute_reply":"2021-10-29T18:59:50.428031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is about a factor two scatter in the various R/C combinations.\n\nFor R = 20 we see C = 50 most often, for R = 5, 50 we see C = 10 most often.\n\n","metadata":{}},{"cell_type":"markdown","source":"## Time steps in individual breaths\nTake a look at time sampling for the first two breaths. Looks like pretty uniform sampling in time.\n\n","metadata":{}},{"cell_type":"code","source":"rc_values = np.array([\n    [r, c, len(df_train[(df_train['R'] == r) & (df_train['C'] == c)])//BREATH_LENGTH] \n    for r in r_unique \n    for c in c_unique\n])\n\nx = range(len(rc_values))\nplt.bar(x, rc_values[:,2])\nplt.xticks(x, [str(r) + '_' + str(c) for r, c in rc_values[:,:2] ])\nplt.xlabel('R_C')\nplt.ylabel('Number counts')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-29T18:59:50.430002Z","iopub.execute_input":"2021-10-29T18:59:50.430223Z","iopub.status.idle":"2021-10-29T18:59:51.158165Z","shell.execute_reply.started":"2021-10-29T18:59:50.430195Z","shell.execute_reply":"2021-10-29T18:59:51.157196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"first_breath  = df_train[df_train['breath_id'] == 1]\nsecond_breath = df_train[df_train['breath_id'] == 2]\n\nx = range(BREATH_LENGTH)\nt1 = first_breath['time_step']\nt2 = second_breath['time_step']\nplt.plot(x, t1)\nplt.plot(x, t2, ls = '--')","metadata":{"execution":{"iopub.status.busy":"2021-10-29T18:59:51.159399Z","iopub.execute_input":"2021-10-29T18:59:51.159729Z","iopub.status.idle":"2021-10-29T18:59:51.404588Z","shell.execute_reply.started":"2021-10-29T18:59:51.159687Z","shell.execute_reply":"2021-10-29T18:59:51.40357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"One time step seems to correspond to about\n\n","metadata":{}},{"cell_type":"code","source":"(max(t1) - min(t1)) / BREATH_LENGTH","metadata":{"execution":{"iopub.status.busy":"2021-10-29T18:59:51.405996Z","iopub.execute_input":"2021-10-29T18:59:51.40628Z","iopub.status.idle":"2021-10-29T18:59:51.412723Z","shell.execute_reply.started":"2021-10-29T18:59:51.406248Z","shell.execute_reply":"2021-10-29T18:59:51.411884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The two time series for the first two breaths are not perfectly aligned","metadata":{}},{"cell_type":"code","source":"plt.plot(t1.values - t2.values);","metadata":{"execution":{"iopub.status.busy":"2021-10-29T18:59:51.414181Z","iopub.execute_input":"2021-10-29T18:59:51.414458Z","iopub.status.idle":"2021-10-29T18:59:51.647354Z","shell.execute_reply.started":"2021-10-29T18:59:51.414429Z","shell.execute_reply":"2021-10-29T18:59:51.646434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## What about the target vector \"pressure\"","metadata":{}},{"cell_type":"code","source":"# Pressure in first breath\nplt.plot(df_train.pressure[:800])","metadata":{"execution":{"iopub.status.busy":"2021-10-29T18:59:51.648618Z","iopub.execute_input":"2021-10-29T18:59:51.648866Z","iopub.status.idle":"2021-10-29T18:59:51.889857Z","shell.execute_reply.started":"2021-10-29T18:59:51.648828Z","shell.execute_reply":"2021-10-29T18:59:51.889236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(df_train.pressure[:800])","metadata":{"execution":{"iopub.status.busy":"2021-10-29T18:59:51.891042Z","iopub.execute_input":"2021-10-29T18:59:51.891909Z","iopub.status.idle":"2021-10-29T18:59:52.133194Z","shell.execute_reply.started":"2021-10-29T18:59:51.891842Z","shell.execute_reply":"2021-10-29T18:59:52.132291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"we note a strong correlation between te columns *pressure* and *u_in*","metadata":{}},{"cell_type":"code","source":"plt.plot(df_train.u_in[:80])","metadata":{"execution":{"iopub.status.busy":"2021-10-29T18:59:52.134319Z","iopub.execute_input":"2021-10-29T18:59:52.134539Z","iopub.status.idle":"2021-10-29T18:59:52.353732Z","shell.execute_reply.started":"2021-10-29T18:59:52.134512Z","shell.execute_reply":"2021-10-29T18:59:52.353201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(df_train.u_in[:1000])","metadata":{"execution":{"iopub.status.busy":"2021-10-29T18:59:52.354776Z","iopub.execute_input":"2021-10-29T18:59:52.354972Z","iopub.status.idle":"2021-10-29T18:59:52.572448Z","shell.execute_reply.started":"2021-10-29T18:59:52.354947Z","shell.execute_reply":"2021-10-29T18:59:52.571363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(df_train.u_out[:1000])","metadata":{"execution":{"iopub.status.busy":"2021-10-29T18:59:52.573681Z","iopub.execute_input":"2021-10-29T18:59:52.573883Z","iopub.status.idle":"2021-10-29T18:59:52.785911Z","shell.execute_reply.started":"2021-10-29T18:59:52.573858Z","shell.execute_reply":"2021-10-29T18:59:52.78399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Are there outliers in the dataset?","metadata":{}},{"cell_type":"code","source":"plt.boxplot(df_train.u_in);","metadata":{"execution":{"iopub.status.busy":"2021-10-29T18:59:52.786935Z","iopub.execute_input":"2021-10-29T18:59:52.787143Z","iopub.status.idle":"2021-10-29T18:59:54.665385Z","shell.execute_reply.started":"2021-10-29T18:59:52.787117Z","shell.execute_reply":"2021-10-29T18:59:54.664455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pressao = plt.boxplot(df_train.pressure);","metadata":{"execution":{"iopub.status.busy":"2021-10-29T18:59:54.666571Z","iopub.execute_input":"2021-10-29T18:59:54.666793Z","iopub.status.idle":"2021-10-29T18:59:56.151625Z","shell.execute_reply.started":"2021-10-29T18:59:54.666767Z","shell.execute_reply":"2021-10-29T18:59:56.150519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"percentiles = [item.get_ydata()[1] for item in pressao['whiskers']]","metadata":{"execution":{"iopub.status.busy":"2021-10-29T18:59:56.154001Z","iopub.execute_input":"2021-10-29T18:59:56.15429Z","iopub.status.idle":"2021-10-29T18:59:56.166214Z","shell.execute_reply.started":"2021-10-29T18:59:56.154245Z","shell.execute_reply":"2021-10-29T18:59:56.164781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"percentiles","metadata":{"execution":{"iopub.status.busy":"2021-10-29T18:59:56.16774Z","iopub.execute_input":"2021-10-29T18:59:56.168129Z","iopub.status.idle":"2021-10-29T18:59:56.178738Z","shell.execute_reply.started":"2021-10-29T18:59:56.16808Z","shell.execute_reply":"2021-10-29T18:59:56.17783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## From that we can train our 1D-Unet based on the *u_in* and *pressure* time series","metadata":{}},{"cell_type":"markdown","source":"### Fisrt we will prepare our X input *u_in* array","metadata":{}},{"cell_type":"code","source":"list1 = df_train.u_in.tolist()","metadata":{"execution":{"iopub.status.busy":"2021-10-29T18:59:56.19166Z","iopub.execute_input":"2021-10-29T18:59:56.191933Z","iopub.status.idle":"2021-10-29T18:59:56.840142Z","shell.execute_reply.started":"2021-10-29T18:59:56.191897Z","shell.execute_reply":"2021-10-29T18:59:56.838903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#list1","metadata":{"execution":{"iopub.status.busy":"2021-10-29T18:59:56.841932Z","iopub.execute_input":"2021-10-29T18:59:56.842289Z","iopub.status.idle":"2021-10-29T18:59:56.854507Z","shell.execute_reply.started":"2021-10-29T18:59:56.842231Z","shell.execute_reply":"2021-10-29T18:59:56.853695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Normalize input","metadata":{}},{"cell_type":"code","source":"#input_minima = np.min(list1)\n#input_maxima = np.max(list1)\n#print(\"Minimum value of X\",input_minima)\n#print(\"Maximum values of X\",input_maxima)","metadata":{"execution":{"iopub.status.busy":"2021-10-29T18:59:56.855804Z","iopub.execute_input":"2021-10-29T18:59:56.856579Z","iopub.status.idle":"2021-10-29T18:59:56.866433Z","shell.execute_reply.started":"2021-10-29T18:59:56.856536Z","shell.execute_reply":"2021-10-29T18:59:56.865543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#list1 = pd.Series(list1)\n#list1 -= input_minima                       # normalizing\n#list1 /= (input_maxima-input_minima)","metadata":{"execution":{"iopub.status.busy":"2021-10-29T18:59:56.868242Z","iopub.execute_input":"2021-10-29T18:59:56.868751Z","iopub.status.idle":"2021-10-29T18:59:56.877244Z","shell.execute_reply.started":"2021-10-29T18:59:56.86871Z","shell.execute_reply":"2021-10-29T18:59:56.876541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(list1[:160])","metadata":{"execution":{"iopub.status.busy":"2021-10-29T18:59:56.878673Z","iopub.execute_input":"2021-10-29T18:59:56.879456Z","iopub.status.idle":"2021-10-29T18:59:57.272018Z","shell.execute_reply.started":"2021-10-29T18:59:56.879414Z","shell.execute_reply":"2021-10-29T18:59:57.271114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Select each individual breath from input signal","metadata":{}},{"cell_type":"code","source":"n = 80\nX = [list1[i:i + n] for i in range(0, len(list1), n)]","metadata":{"execution":{"iopub.status.busy":"2021-10-29T18:59:57.273507Z","iopub.execute_input":"2021-10-29T18:59:57.273804Z","iopub.status.idle":"2021-10-29T18:59:57.961254Z","shell.execute_reply.started":"2021-10-29T18:59:57.273761Z","shell.execute_reply":"2021-10-29T18:59:57.960418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print(\"Minimum value of X Normalized\",np.min(X))\n#print(\"Maximum values of X Normalized\",np.max(X))\nplt.plot(X[0])","metadata":{"execution":{"iopub.status.busy":"2021-10-29T18:59:57.962632Z","iopub.execute_input":"2021-10-29T18:59:57.962889Z","iopub.status.idle":"2021-10-29T18:59:58.189195Z","shell.execute_reply.started":"2021-10-29T18:59:57.96286Z","shell.execute_reply":"2021-10-29T18:59:58.188474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#X","metadata":{"execution":{"iopub.status.busy":"2021-10-29T18:59:58.194038Z","iopub.execute_input":"2021-10-29T18:59:58.194542Z","iopub.status.idle":"2021-10-29T18:59:58.197864Z","shell.execute_reply.started":"2021-10-29T18:59:58.19451Z","shell.execute_reply":"2021-10-29T18:59:58.196889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Now preparing our Y *pressure* array","metadata":{}},{"cell_type":"code","source":"list2 = df_train.pressure.tolist()","metadata":{"execution":{"iopub.status.busy":"2021-10-29T18:59:58.211685Z","iopub.execute_input":"2021-10-29T18:59:58.212382Z","iopub.status.idle":"2021-10-29T18:59:58.846996Z","shell.execute_reply.started":"2021-10-29T18:59:58.212348Z","shell.execute_reply":"2021-10-29T18:59:58.846065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#list2","metadata":{"execution":{"iopub.status.busy":"2021-10-29T18:59:58.848271Z","iopub.execute_input":"2021-10-29T18:59:58.848525Z","iopub.status.idle":"2021-10-29T18:59:58.858958Z","shell.execute_reply.started":"2021-10-29T18:59:58.848488Z","shell.execute_reply":"2021-10-29T18:59:58.85819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Normalize output","metadata":{}},{"cell_type":"code","source":"#output_minima = np.min(list2)\n#output_maxima = np.max(list2)\n#print(\"Minimum value of Y\",output_minima)\n#print(\"Maximum values of Y\",output_maxima)","metadata":{"execution":{"iopub.status.busy":"2021-10-29T18:59:58.862152Z","iopub.execute_input":"2021-10-29T18:59:58.862389Z","iopub.status.idle":"2021-10-29T18:59:58.870905Z","shell.execute_reply.started":"2021-10-29T18:59:58.86236Z","shell.execute_reply":"2021-10-29T18:59:58.8699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#list2 = pd.Series(list2)\n#list2 -= output_minima                       # normalizing\n#list2 /= (output_maxima-output_minima)","metadata":{"execution":{"iopub.status.busy":"2021-10-29T18:59:58.872178Z","iopub.execute_input":"2021-10-29T18:59:58.872416Z","iopub.status.idle":"2021-10-29T18:59:58.882221Z","shell.execute_reply.started":"2021-10-29T18:59:58.872385Z","shell.execute_reply":"2021-10-29T18:59:58.881324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(list2[:160])","metadata":{"execution":{"iopub.status.busy":"2021-10-29T18:59:58.883708Z","iopub.execute_input":"2021-10-29T18:59:58.883951Z","iopub.status.idle":"2021-10-29T18:59:59.238362Z","shell.execute_reply.started":"2021-10-29T18:59:58.88392Z","shell.execute_reply":"2021-10-29T18:59:59.237533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Select each individual breath from output signal","metadata":{}},{"cell_type":"code","source":"i = 0\nY = [list2[i:i + n] for i in range(0, len(list2), n)]\n","metadata":{"execution":{"iopub.status.busy":"2021-10-29T18:59:59.239682Z","iopub.execute_input":"2021-10-29T18:59:59.239877Z","iopub.status.idle":"2021-10-29T18:59:59.427301Z","shell.execute_reply.started":"2021-10-29T18:59:59.239852Z","shell.execute_reply":"2021-10-29T18:59:59.426334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print(\"Minimum value of Y Normalized\",np.min(Y))\n#print(\"Maximum values of Y Normalized\",np.max(Y))\nplt.plot(Y[0])","metadata":{"execution":{"iopub.status.busy":"2021-10-29T18:59:59.428591Z","iopub.execute_input":"2021-10-29T18:59:59.428853Z","iopub.status.idle":"2021-10-29T18:59:59.675658Z","shell.execute_reply.started":"2021-10-29T18:59:59.428822Z","shell.execute_reply":"2021-10-29T18:59:59.674782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Y","metadata":{"execution":{"iopub.status.busy":"2021-10-29T18:59:59.676841Z","iopub.execute_input":"2021-10-29T18:59:59.677142Z","iopub.status.idle":"2021-10-29T18:59:59.680837Z","shell.execute_reply.started":"2021-10-29T18:59:59.677111Z","shell.execute_reply":"2021-10-29T18:59:59.680006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Train and Validation data split","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n","metadata":{"execution":{"iopub.status.busy":"2021-10-29T18:59:59.682197Z","iopub.execute_input":"2021-10-29T18:59:59.68252Z","iopub.status.idle":"2021-10-29T18:59:59.858617Z","shell.execute_reply.started":"2021-10-29T18:59:59.682466Z","shell.execute_reply":"2021-10-29T18:59:59.857819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(X,Y)","metadata":{"execution":{"iopub.status.busy":"2021-10-29T18:59:59.859891Z","iopub.execute_input":"2021-10-29T18:59:59.860115Z","iopub.status.idle":"2021-10-29T18:59:59.932389Z","shell.execute_reply.started":"2021-10-29T18:59:59.860088Z","shell.execute_reply":"2021-10-29T18:59:59.931598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = np.array(X_train)\nX_val = np.array(X_val)\ny_train = np.array(y_train)\ny_val = np.array(y_val)","metadata":{"execution":{"iopub.status.busy":"2021-10-29T18:59:59.933518Z","iopub.execute_input":"2021-10-29T18:59:59.933745Z","iopub.status.idle":"2021-10-29T19:00:01.929191Z","shell.execute_reply.started":"2021-10-29T18:59:59.933716Z","shell.execute_reply":"2021-10-29T19:00:01.928265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train.shape)\nprint(X_val.shape)\nprint(y_train.shape)\nprint(y_val.shape)","metadata":{"execution":{"iopub.status.busy":"2021-10-29T19:00:01.930237Z","iopub.execute_input":"2021-10-29T19:00:01.930449Z","iopub.status.idle":"2021-10-29T19:00:01.935816Z","shell.execute_reply.started":"2021-10-29T19:00:01.930423Z","shell.execute_reply":"2021-10-29T19:00:01.935009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train Approximation Network","metadata":{}},{"cell_type":"markdown","source":"### GPU accelerator config","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\nimport tensorflow as tf\nACCELERATOR_TYPE = 'GPU'\n\nif ACCELERATOR_TYPE == 'GPU':\n    strategy = tf.distribute.MirroredStrategy()\n    print(\"GPU\")","metadata":{"execution":{"iopub.status.busy":"2021-10-29T19:00:01.937098Z","iopub.execute_input":"2021-10-29T19:00:01.937336Z","iopub.status.idle":"2021-10-29T19:00:02.023489Z","shell.execute_reply.started":"2021-10-29T19:00:01.9373Z","shell.execute_reply":"2021-10-29T19:00:02.022832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Deep Supervision configs","metadata":{}},{"cell_type":"code","source":"def prepareLabel(Y):\n\n    \"\"\"\n    Prepare label for deep supervised pipeline\n    \n    Returns:\n        dictionary -- dictionary containing the 5 level ground truth outputs of the network\n    \"\"\"\n    \n    def approximate(inp,w_len):\n        \"\"\"\n        Downsamples using taking mean over window\n        \n        Arguments:\n            inp {array} -- signal\n            w_len {int} -- length of window\n\n        Returns:\n            array -- downsampled signal\n        \"\"\"\n        \n        op = []\n        \n        for i in range(0,len(inp),w_len):\n        \n            op.append(np.mean(inp[i:i+w_len]))\n            \n        return np.array(op)\n\n    out = {}\n    out['out'] = []\n    out['level1'] = []\n    out['level2'] = []\n    out['level3'] = []\n    out['level4'] = []\n    \n    \n    for y in tqdm(Y,desc='Preparing Label for DS'):\n    \n                                                                    # computing approximations\n        cA1 = approximate(np.array(y).reshape(length), 2)\n\n        cA2 = approximate(np.array(y).reshape(length), 4)\n\n        cA3 = approximate(np.array(y).reshape(length), 8)\n\n        cA4 = approximate(np.array(y).reshape(length), 16)\n        \n\n\n                                                                    # populating the labels for different labels\n        out['out'].append(np.array(y.reshape(length,1)))\n        out['level1'].append(np.array(cA1.reshape(length//2,1)))\n        out['level2'].append(np.array(cA2.reshape(length//4,1)))\n        out['level3'].append(np.array(cA3.reshape(length//8,1)))\n        out['level4'].append(np.array(cA4.reshape(length//16,1)))\n\n    out['out'] = np.array(out['out'])                                # converting to numpy array\n    out['level1'] = np.array(out['level1'])\n    out['level2'] = np.array(out['level2'])\n    out['level3'] = np.array(out['level3'])\n    out['level4'] = np.array(out['level4'])\n    \n\n    return out","metadata":{"execution":{"iopub.status.busy":"2021-10-29T19:00:02.025762Z","iopub.execute_input":"2021-10-29T19:00:02.026393Z","iopub.status.idle":"2021-10-29T19:00:02.042754Z","shell.execute_reply.started":"2021-10-29T19:00:02.026342Z","shell.execute_reply":"2021-10-29T19:00:02.041709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Fitting loop","metadata":{}},{"cell_type":"code","source":"import os\nimport pickle \n\nlength = 80\nmodel = UNetDS64(length, n_channel=1)\nmdlName1 = 'UNetDS64'\n\ntry:                                                        # create directory to save training model\n    os.makedirs('models')\nexcept:\n    pass\n\ntry:                                                        # create directory to save training history\n    os.makedirs('History')\nexcept:\n    pass\n\ndef train_approximation_network(model,X_train, X_val, y_train, y_val):\n    for foldname in range(10):\n\n            print('----------------')\n            print('Training Fold {}'.format(foldname+1))\n            print('----------------')\n\n          \n\n            Y_train = prepareLabel(y_train)                                         # prepare labels for training deep supervision\n\n            Y_val = prepareLabel(y_val)                                             # prepare labels for training deep supervision\n\n\n\n            mdl1 = model          # create approximation network\n\n                                                                                # loss = mse, with deep supervision weights\n            mdl1.compile(loss='mean_absolute_error',optimizer='adam',metrics=['mean_squared_error'], loss_weights=[1., 0.9, 0.8, 0.7, 0.6])                                                         \n\n            # Reduce Learning Rate\n            lr = ReduceLROnPlateau(monitor=\"val_out_loss\", factor=0.85, \n                               patience=7, verbose=1)\n            # Checkpoint callbakc\n            checkpoint1_ = ModelCheckpoint(os.path.join('models','{}_model1_fold{}.h5'.format(mdlName1,foldname)), verbose=1, monitor='val_out_loss',save_best_only=True, mode='auto')  \n                                                                            \n            # Early Stopping to avoid overfitting\n            es = EarlyStopping(monitor=\"val_out_loss\", patience=30, \n                           verbose=1, mode=\"min\", \n                           restore_best_weights=True)\n            # train approximation network for 100 epochs\n            history1 = mdl1.fit(X_train,{'out': Y_train['out'], 'level1': Y_train['level1'], 'level2':Y_train['level2'], 'level3':Y_train['level3'] , 'level4':Y_train['level4']},epochs=100,batch_size=512,\n                                validation_data=(X_val,{'out': Y_val['out'], 'level1': Y_val['level1'], 'level2':Y_val['level2'], 'level3':Y_val['level3'] , 'level4':Y_val['level4']}),callbacks=[lr, checkpoint1_, es],verbose=1)\n\n            pickle.dump(history1.history, open('History/{}_model1_fold{}.p'.format(mdlName1,foldname),'wb'))    # save training history\n\n\n            mdl1 = None                                             # garbage collection\n\n            #time.sleep(300)                                         # pause execution for a while to free the gpu'''\n    \n","metadata":{"execution":{"iopub.status.busy":"2021-10-29T19:00:02.046292Z","iopub.execute_input":"2021-10-29T19:00:02.04683Z","iopub.status.idle":"2021-10-29T19:00:03.247807Z","shell.execute_reply.started":"2021-10-29T19:00:02.046792Z","shell.execute_reply":"2021-10-29T19:00:03.24689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_approximation_network(model, X_train, X_val, y_train, y_val)","metadata":{"execution":{"iopub.status.busy":"2021-10-29T19:00:03.25201Z","iopub.execute_input":"2021-10-29T19:00:03.252278Z","iopub.status.idle":"2021-10-29T19:26:22.490501Z","shell.execute_reply.started":"2021-10-29T19:00:03.252247Z","shell.execute_reply":"2021-10-29T19:26:22.489031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Approximation Model 1 Training/Validation History","metadata":{}},{"cell_type":"code","source":"fold_val_loss = []\nimport pickle\n# for fold in training history\nfor fold in range(10):\n    # open and load pickle file as bytes\n    file = open('./History/UNetDS64_model1_fold{}.p'.format(fold), 'rb')\n    history = pickle.load(file)\n    \n    # print best score and epoch for each fold\n    print(\"Fold: \", fold)\n    print(\"Best model scored: {} \\nin epoch: {}\".format(np.min(history['val_loss']),np.argmin(history['val_loss'])),\"\\n\")\n    \n    # append validation loss to select best weights\n    fold_val_loss.append((np.min(history['val_loss']),np.argmin(history['val_loss'])))\n    \n    \n    # Plot trainig history loss and val_loss \n    plt.plot(history['loss'][:])\n    plt.plot(history['val_loss'][:])\n    plt.title('model loss')\n    plt.ylabel('MAE loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'val'], loc='upper left')\n    plt.show()\n    \n","metadata":{"execution":{"iopub.status.busy":"2021-10-29T19:26:22.491839Z","iopub.status.idle":"2021-10-29T19:26:22.492172Z","shell.execute_reply.started":"2021-10-29T19:26:22.491995Z","shell.execute_reply":"2021-10-29T19:26:22.492012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Which fold weight lead to the lowest error metric?","metadata":{}},{"cell_type":"code","source":"\nfold_loss = [i[0] for i in fold_val_loss]            # list of val_loss per fold\nmin_loss = np.min(fold_val_loss)                     # get min val_loss\nmin_loss_fold = np.argmin(fold_loss)                 # get min val_los fold\n_, min_loss_epoch = fold_val_loss[min_loss_fold-1]   # get min val_loss epoch\nprint(\"Best combination in fold {}, val loss: {}, epoch: {}\".format(min_loss_fold,min_loss,min_loss_epoch))    # Print best model information for prediciton","metadata":{"execution":{"iopub.status.busy":"2021-10-29T19:26:22.493302Z","iopub.status.idle":"2021-10-29T19:26:22.493829Z","shell.execute_reply.started":"2021-10-29T19:26:22.493632Z","shell.execute_reply":"2021-10-29T19:26:22.493659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Predict on test set","metadata":{}},{"cell_type":"markdown","source":"Load test set","metadata":{}},{"cell_type":"code","source":"df_test = pd.read_csv('../input/ventilator-pressure-prediction/test.csv')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Select each individual breath","metadata":{}},{"cell_type":"code","source":"list1 = df_test.u_in.tolist()\nn = 80\nX_test = [list1[i:i + n] for i in range(0, len(list1), n)]\nplt.plot(X_test[0])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Instantiate model architecture with best fold weights and predict over test dataset","metadata":{}},{"cell_type":"code","source":"import os\nmdl1 = UNetDS64(80)                                             # creating approximation network\npath = \"./models\"\nmdl1.load_weights(os.path.join(path,'UNetDS64_model1_fold{}.h5'.format(min_loss_fold)))   # loading weights\nY_test_pred_approximate = mdl1.predict(X_test,verbose=1)            # predicting approximate abp waveform","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Concatenate results into a single list","metadata":{}},{"cell_type":"code","source":"Y_test_pred_approximate = np.array(Y_test_pred_approximate[0])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"samples, _,_ = Y_test_pred_approximate.shape\npressure = []\nfor signal in range(samples):\n    breath_pressure = [j for i in Y_test_pred_approximate[signal] for j in i]\n    pressure.extend(breath_pressure)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create submission file","metadata":{}},{"cell_type":"markdown","source":"Load submission csv","metadata":{}},{"cell_type":"code","source":"sub = pd.read_csv('../input/ventilator-pressure-prediction/sample_submission.csv')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Assign predicted values to *pressure* column","metadata":{}},{"cell_type":"code","source":"sub.pressure = pressure\nsub.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Save submission csv","metadata":{}},{"cell_type":"code","source":"sub.to_csv('approximation_submission.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]}]}