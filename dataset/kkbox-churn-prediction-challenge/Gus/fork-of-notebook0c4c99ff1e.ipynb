{"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"}},"nbformat":4,"cells":[{"execution_count":null,"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.\n\nfrom multiprocessing import Pool, cpu_count\nimport gc; gc.enable()\nimport xgboost as xgb\nimport pandas as pd\nimport numpy as np\nfrom sklearn import *\nimport sklearn","cell_type":"code","metadata":{"_cell_guid":"425b469a-c9e8-4f28-a409-8b1ff0455c3f","_uuid":"740b2d4eb0c984b6b11235d9236135b142c45def"},"outputs":[]},{"execution_count":null,"source":"train = pd.read_csv('../input/train.csv')\ntrain = pd.concat((train, pd.read_csv('../input/train_v2.csv')), axis=0, ignore_index=True).reset_index(drop=True)\ntest = pd.read_csv('../input/sample_submission_v2.csv')","cell_type":"code","metadata":{"_cell_guid":"4442a32f-ff9c-443d-a5c1-17de492cffd4","collapsed":true,"_uuid":"8cbc62b353f718bc9ffbcf9e732d13742f4ce126"},"outputs":[]},{"execution_count":null,"source":"members = pd.read_csv('../input/members_v3.csv')","cell_type":"code","metadata":{"_cell_guid":"21a8844e-4b5b-457d-a71b-542925446e5b","collapsed":true,"_uuid":"dd0220f46f3a2fbb0e3d458c56c6d5c0e58bb08d"},"outputs":[]},{"execution_count":null,"source":"transactions = pd.read_csv('../input/transactions.csv')\ntransactions = pd.concat((transactions, pd.read_csv('../input/transactions_v2.csv')), axis=0, ignore_index=True).reset_index(drop=True)\n#transactions = pd.DataFrame(transactions['msno'].value_counts().reset_index())\ntransactions = transactions.sort_values(by=['transaction_date'], ascending=[False]).reset_index(drop=True)\ntransactions = transactions.drop_duplicates(subset=['msno'], keep='first')","cell_type":"code","metadata":{"_cell_guid":"bd634cdf-5ad9-4fc8-b956-d468d17b7744","collapsed":true,"_uuid":"e0b655cb52849852c28d8f3faafa8599b65aabf6"},"outputs":[]},{"execution_count":null,"source":"#discount\ntransactions['discount'] = transactions['plan_list_price'] - transactions['actual_amount_paid']\n#amt_per_day\ntransactions['amt_per_day'] = transactions['actual_amount_paid'] / transactions['payment_plan_days']\n#is_discount\ntransactions['is_discount'] = transactions.discount.apply(lambda x: 1 if x > 0 else 0)\n#membership_duration\ntransactions['membership_days'] = pd.to_datetime(transactions['membership_expire_date']).subtract(pd.to_datetime(transactions['transaction_date'])).dt.days.astype(int)\n","cell_type":"code","metadata":{},"outputs":[]},{"execution_count":null,"source":"train['is_train'] = 1\ntest['is_train'] = 0\ncombined = pd.concat([train, test], axis = 0)\n\ncombined = pd.merge(combined, members, how='left', on='msno')\nmembers = []; print('members merge...') \n\ngender = {'male':1, 'female':2}\ncombined['gender'] = combined['gender'].map(gender)\n\ncombined = pd.merge(combined, transactions, how='left', on='msno')\ntransactions=[]; print('transaction merge...')\n\ntrain = combined[combined['is_train'] == 1]\ntest = combined[combined['is_train'] == 0]\n\ntrain.drop(['is_train'], axis = 1, inplace = True)\ntest.drop(['is_train'], axis = 1, inplace = True)\n\ndel combined","cell_type":"code","metadata":{},"outputs":[]},{"execution_count":null,"source":"def transform_df(df):\n    df = pd.DataFrame(df)\n    df = df.sort_values(by=['date'], ascending=[False])\n    df = df.reset_index(drop=True)\n    df = df.drop_duplicates(subset=['msno'], keep='first')\n    return df\n\ndef transform_df2(df):\n    df = df.sort_values(by=['date'], ascending=[False])\n    df = df.reset_index(drop=True)\n    df = df.drop_duplicates(subset=['msno'], keep='first')\n    return df\nlast_user_logs = []\n\ndf_iter = pd.read_csv('../input/user_logs.csv', low_memory=False, iterator=True, chunksize=10000000)\n\n\ni = 0 #~400 Million Records - starting at the end but remove locally if needed\nfor df in df_iter:\n    if i>35:\n        if len(df)>0:\n            print(df.shape)\n            p = Pool(cpu_count())\n            df = p.map(transform_df, np.array_split(df, cpu_count()))   \n            df = pd.concat(df, axis=0, ignore_index=True).reset_index(drop=True)\n            df = transform_df2(df)\n            p.close(); p.join()\n            last_user_logs.append(df)\n            print('...', df.shape)\n            df = []\n    i+=1\n\n\nlast_user_logs.append(transform_df(pd.read_csv('../input/user_logs_v2.csv')))\nlast_user_logs = pd.concat(last_user_logs, axis=0, ignore_index=True).reset_index(drop=True)\nlast_user_logs = transform_df2(last_user_logs)\n\ntrain = pd.merge(train, last_user_logs, how='left', on='msno')\ntest = pd.merge(test, last_user_logs, how='left', on='msno')\nlast_user_logs=[]\n","cell_type":"code","metadata":{},"outputs":[]},{"execution_count":null,"source":"train['autorenew_&_not_cancel'] = ((train.is_auto_renew == 1) == (train.is_cancel == 0)).astype(np.int8)\ntest['autorenew_&_not_cancel'] = ((test.is_auto_renew == 1) == (test.is_cancel == 0)).astype(np.int8)\n\ntrain['notAutorenew_&_cancel'] = ((train.is_auto_renew == 0) == (train.is_cancel == 1)).astype(np.int8)\ntest['notAutorenew_&_cancel'] = ((test.is_auto_renew == 0) == (test.is_cancel == 1)).astype(np.int8)","cell_type":"code","metadata":{"collapsed":true},"outputs":[]},{"execution_count":null,"source":"train = train.fillna(0)\ntest = test.fillna(0)\n\ncols = [c for c in train.columns if c not in ['is_churn','msno']]","cell_type":"code","metadata":{"collapsed":true},"outputs":[]},{"execution_count":null,"source":"def xgb_score(preds, dtrain):\n    labels = dtrain.get_label()\n    return 'log_loss', metrics.log_loss(labels, preds)\n\nfold = 1\nfor i in range(fold):\n    params = {\n        'eta': 0.02,\n        'min_child_weight': 5,\n        'gamma': 0.3,\n        'max_depth': 7,\n        'objective': 'binary:logistic',\n        'eval_metric': 'logloss',\n        'seed': 17,\n        'silent': True\n    }\n    x1, x2, y1, y2 = model_selection.train_test_split(train[cols], train['is_churn'], test_size=0.3, random_state=i)\n    watchlist = [(xgb.DMatrix(x1, y1), 'train'), (xgb.DMatrix(x2, y2), 'valid')]\n    model = xgb.train(params, xgb.DMatrix(x1, y1), 300,  watchlist, feval=xgb_score, maximize=False, verbose_eval=50, early_stopping_rounds=50) #use 1500\n    if i != 0:\n        pred1 += model.predict(xgb.DMatrix(test[cols]), ntree_limit=model.best_ntree_limit)\n    else:\n        pred1 = model.predict(xgb.DMatrix(test[cols]), ntree_limit=model.best_ntree_limit)\npred1 /= fold","cell_type":"code","metadata":{},"outputs":[]},{"execution_count":null,"source":"test['is_churn'] = pred1.clip(0.+1e-15, 1-1e-15)\ntest[['msno','is_churn']].to_csv('xgbsub2.csv.gz', index=False, compression='gzip')","cell_type":"code","metadata":{"collapsed":true},"outputs":[]},{"execution_count":null,"source":"","cell_type":"code","metadata":{},"outputs":[]},{"execution_count":null,"source":"","cell_type":"code","metadata":{"collapsed":true},"outputs":[]}],"nbformat_minor":1}