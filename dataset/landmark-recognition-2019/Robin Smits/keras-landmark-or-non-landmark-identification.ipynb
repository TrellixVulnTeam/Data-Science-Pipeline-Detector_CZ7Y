{"cells":[{"metadata":{},"cell_type":"markdown","source":"In this competition it is not only important to predict the landmark for an image but also to make sure that if an image is not a landmark that we don't make a prediction.\nCorrectly identifying non-landmark images in the test set will increase the score of a submission. \nRemoving as much of the non-landmark images from the training set will decrease the total amount of images that we need to train on and it will improve the 'correctness' of the model if it is trained on the landmark images and not on all the selfies, hotel-rooms and beds etcetera that were made near the landmark.\n\nA good way to start the identification is to use the Places365 dataset and models as described on this [webpage](http://places2.csail.mit.edu/). I've created a small dataset of 50 images selected from the 4+ million in the trainset to show how it could be done. In the Places365 dataset there 365 classes and each class is also marked as either 'indoor' or 'outdoor'. We could interpret this also as 'non-landmark' or 'landmark'. It could be further optimized offcourse...an image of the inside of a castle will be marked as 'indoor' but will very likely be a legitimate 'landmark'.\n\nJust by using the default 'indoor/outdoor' marker and determining it for both train and test data I was able to improve my best model so far with about 0.015 - 0.02 score on the leaderboard. "},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":false},"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd \nfrom PIL import Image\nfrom cv2 import resize\nimport matplotlib.pyplot as plt\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I've found a very nice repository on Github where the trained models were already converted to Keras models. See this [webpage](https://github.com/GKalliatakis/Keras-VGG16-places365) for the original sources. Since I had issues using the 'git clone' command I've put the code into a Kaggle Dataset. Let's import the necessary modules from this dataset."},{"metadata":{"trusted":false},"cell_type":"code","source":"# VGG 16 Places 365 scripts in custom dataset\nos.chdir(\"/kaggle/input/keras-vgg16-places365/\")\nfrom vgg16_places_365 import VGG16_Places365\nos.chdir(\"/kaggle/working/\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets get the list of all image samples and resize them to the format that we will also use to predict with the model later on.\nNote that I just resize the images as-is. Very likely that creating multiple crops (or any other pre-processing step) for an image could lead to a better average prediction of landmark or non-landmark"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Get List of Images\nimage_samples = '../input/google-landmark-2019-samples/'\nall_images = os.listdir(image_samples)\n\n# Resize all images\nall_images_resized = []\nfor filename in all_images:    \n    im = np.array(Image.open(image_samples + filename).resize((224, 224), Image.LANCZOS))    \n    all_images_resized.append(im)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To show the sample images..."},{"metadata":{"trusted":false},"cell_type":"code","source":"# Plot image examples\nfig = plt.figure(figsize = (16, 32))\nfor index, im in zip(range(1, len(all_images_resized)+1), all_images_resized):\n    fig.add_subplot(10, 5, index)\n    plt.title(filename)\n    plt.imshow(im)   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First we will now create the model and make a Top-3 of class predictions for each of the images."},{"metadata":{"trusted":false},"cell_type":"code","source":"# Placeholders for predictions\np0, p1, p2 = [], [], []\n\n# Places365 Model\nmodel = VGG16_Places365(weights='places')\ntopn = 5\n\n# Loop through all images\nfor image in all_images_resized:\n    \n    # Predict Top N Image Classes\n    image = np.expand_dims(image, 0)\n    topn_preds = np.argsort(model.predict(image)[0])[::-1][0:topn]\n\n    p0.append(topn_preds[0])\n    p1.append(topn_preds[1])\n    p2.append(topn_preds[2])\n\n# Create dataframe for later usage\ntopn_df = pd.DataFrame()\ntopn_df['filename'] = np.array(all_images)\ntopn_df['p0'] = np.array(p0)\ntopn_df['p1'] = np.array(p1)\ntopn_df['p2'] = np.array(p2)\ntopn_df.to_csv('topn_class_numbers.csv', index = False)\n\n# Summary\ntopn_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we have our Top-3 class predictions for each image we can get the name of each class and the 'indoor' or 'outdoor' marker. I've modified the default files and created the 'categories_places365_extended.csv' file which includes the 'indoor(1)/outdoor(2)' label as specified in this [file](https://github.com/CSAILVision/places365/blob/master/IO_places365.txt).\n\nAs stated earlier we will use the default 'indoor' label to mark images as 'non-landmark' and 'outdoor' as 'landmark'."},{"metadata":{"trusted":false},"cell_type":"code","source":"# Read Class number, class name and class indoor/outdoor marker\nclass_information = pd.read_csv('../input/keras-vgg16-places365/categories_places365_extended.csv')\nclass_information.head()\n\n# Set Class Labels\nfor col in ['p0', 'p1', 'p2']:\n    topn_df[col + '_label'] = topn_df[col].map(class_information.set_index('class')['label'])\n    topn_df[col + '_landmark'] = topn_df[col].map(class_information.set_index('class')['io'].replace({1:'non-landmark', 2:'landmark'}))\ntopn_df.to_csv('topn_all_info.csv', index = False)\n\n# Summary\ntopn_df.head()   \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see which images are now predicted as 'landmark' based on only 'p0_landmark'."},{"metadata":{"trusted":false},"cell_type":"code","source":"# Get 'landmark' images\nn = 9\nlandmark_images =  topn_df[topn_df['p0_landmark'] == 'landmark']['filename']\nlandmark_indexes = landmark_images[:n].index.values\n\n# Plot image examples\nfig = plt.figure(figsize = (16, 16))\nfor index, im in zip(range(1, n+1), [ all_images_resized[i] for i in landmark_indexes]):\n    fig.add_subplot(3, 3, index)\n    plt.title(filename)\n    plt.imshow(im)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now the same for images that are predicted as 'non-landmark' based on only 'p0_landmark'."},{"metadata":{"trusted":false},"cell_type":"code","source":"# Get 'non-landmark' images\nn = 9\nlandmark_images =  topn_df[topn_df['p0_landmark'] == 'non-landmark']['filename']\nlandmark_indexes = landmark_images[:n].index.values\n\n# Plot image examples\nfig = plt.figure(figsize = (16, 16))\nfor index, im in zip(range(1, n+1), [ all_images_resized[i] for i in landmark_indexes]):\n    fig.add_subplot(3, 3, index)\n    plt.title(filename)\n    plt.imshow(im)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That for sure looks promising! The majority of predicted 'non-landmarks' for sure look like they are indeed not landmarks.\n\n**Summary**\n\nSo we looked at a very usable way to predict which images are landmarks or not.\nSome ideas/notes from my side when you start using the code.\n1. Think about the image-preprocessing.. a simple resize, multiple-crops, augmentation etc.\n2. How to use the predictions? Will you use the Top-N in a majority vote or with probability above a certain threshold?\n3. Take into account the time needed to predict over 4+ million images...\n\nI hope you enjoyed this notebook and please let me know if you have any questions or remarks.\nAnd enjoy the competition."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}