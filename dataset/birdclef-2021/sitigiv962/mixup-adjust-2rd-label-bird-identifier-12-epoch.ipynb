{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torchtoolbox\nfrom torchtoolbox.tools import mixup_data, mixup_criterion","metadata":{"_uuid":"880b656f-6d87-4668-ac00-74c8683bf95b","_cell_guid":"163d4eec-87c1-437d-ba41-253b228595e7","collapsed":false,"id":"C0vy_lvxVQf5","outputId":"0ad3c95b-49a6-4e37-c750-8b7f79e76f57","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Notes","metadata":{"_uuid":"e723fe76-6d97-4063-8bca-713b93f9f200","_cell_guid":"29c92c8a-fed8-4f5e-9f2b-7bac5b6af6e5","trusted":true}},{"cell_type":"markdown","source":"Days back, I've shared this [infernece kernel](https://www.kaggle.com/kneroma/clean-fast-simple-bird-identifier-inference). But its weights are static as you can't retrain the model. In this work, I'm gonna release the training notebook which is almost my internal training pipeline. I removed some experimentation ideas to make things clearer and straightforward. Don't mind adding new ideas at your side as well.","metadata":{"_uuid":"f2fdaa9e-3fe6-4177-a387-36875b2b0c04","_cell_guid":"aa0ee82e-333e-4494-bf8f-b307256d8ea4","id":"py6GyjyZVark","trusted":true}},{"cell_type":"markdown","source":"To make the training faster, we cached the training set into RAM. The whole training records are already [converted into handy  melspecs images](https://www.kaggle.com/kneroma/kkiller-birdclef-2021). These images are from 7 seconds extracts (training on 7 seconds seems to be more effective than 5 seconds). Longer records are truncated into random 7x10 seconds.\n\n**If one is interessted in to the whole records' melspecs** (no truncation):\n* https://www.kaggle.com/kneroma/kkiller-birdclef-mels-computer-d7-part1\n* https://www.kaggle.com/kneroma/kkiller-birdclef-mels-computer-d7-part2\n* https://www.kaggle.com/kneroma/kkiller-birdclef-mels-computer-d7-part3\n* https://www.kaggle.com/kneroma/kkiller-birdclef-mels-computer-d7-part4","metadata":{"_uuid":"e0d187f5-6d2e-4597-b02d-e7b68a5dcf80","_cell_guid":"b421f95a-e232-436a-b1f8-4de419021583","trusted":true}},{"cell_type":"markdown","source":"### Tips & suggestions\n* You can choose a wide set of models from the **get_model** interface : [\"resnest*\", \"resnet*\", \"resnext*\", \"efficientnet*\" ...]\n* You can change the learning rate scheduler: OneCycle ? ReduceOnPlateau ?\n* Adds secondary labels\n* Use train & test metadata (dates, positions (longitude, latitude), ...)\n* Add melspecs augmentation","metadata":{"_uuid":"5c2c9787-5b6f-4a9f-9c85-30e3ecbe5cd3","_cell_guid":"d800bf12-bb17-4a3b-86ea-7653b4f5ce43","trusted":true}},{"cell_type":"markdown","source":"**For Colab training, you just have to uncomment the first cells**","metadata":{"_uuid":"127e9895-b9ef-46fe-892d-07824d956dc3","_cell_guid":"25ecce03-2ac5-480c-b557-f67a49ae065a","trusted":true}},{"cell_type":"markdown","source":"# Versions","metadata":{"_uuid":"219e2755-02e6-4225-85c9-63c68bfe92ab","_cell_guid":"3105a0a5-0902-4f7d-8852-7cd8317c13eb","trusted":true}},{"cell_type":"markdown","source":"* **v1** : initial version\n* **v3** : enable training on whole (no truncation) record melspecs","metadata":{"_uuid":"51a9ecd8-fbe9-4589-988c-5f9e6126cc63","_cell_guid":"0f3c06c2-dc1d-4197-bc9c-742276188afc","trusted":true}},{"cell_type":"code","source":"# from google.colab import drive\n# drive.mount('/content/drive')","metadata":{"_uuid":"3231fb20-8096-4984-8bad-fc37ae8862fb","_cell_guid":"26c68bbc-2c34-4b48-aea2-0a3581a5f1b3","collapsed":false,"id":"oYPb42V-Vaza","outputId":"f9846f5e-bb8b-407c-d15e-bab4b8604301","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ! pip install --upgrade --force-reinstall --no-deps  kaggle > /dev/null\n# ! mkdir ~/.kaggle\n# ! cp \"/content/drive/My Drive/Kaggle/kaggle.json\" ~/.kaggle/\n# ! chmod 600 ~/.kaggle/kaggle.json","metadata":{"_uuid":"9587e68f-66e1-4f23-a744-2dc56a1e1f09","_cell_guid":"b1bbc4f7-4e03-41bd-ad76-8f1c9e013e81","collapsed":false,"id":"IRn4fOj5XDp6","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %%time\n\n# import os\n# if not os.path.exists(\"/content/datasets/audio_images\"):\n#   !mkdir datasets\n#   !kaggle datasets download -d kneroma/kkiller-birdclef-2021\n#   !unzip /content//kkiller-birdclef-2021.zip -d datasets","metadata":{"_uuid":"a15b7652-4e00-427c-97fd-5cc6db05e578","_cell_guid":"8226166a-028d-465b-8496-e50432f8490b","collapsed":false,"id":"KAnAPecOXDtf","outputId":"fb981400-b8c5-45bc-9e7d-5d4a77f2550e","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[](http://)","metadata":{"_uuid":"7e4e613d-4de9-4087-b7b4-df3c24e4f556","_cell_guid":"4d4f86fa-c0aa-42a3-84ff-b7b91870199c","trusted":true}},{"cell_type":"code","source":"!pip install -q pysndfx SoundFile audiomentations pretrainedmodels efficientnet_pytorch resnest","metadata":{"_uuid":"8831e316-7c92-4bc6-aadc-dffe411ad63e","_cell_guid":"597a7693-801a-482e-aa17-5328d511c3ad","collapsed":false,"id":"Yn1Ybf15VAqW","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport librosa as lb\nimport librosa.display as lbd\nimport soundfile as sf\nfrom  soundfile import SoundFile\nimport pandas as pd\nfrom  IPython.display import Audio\nfrom pathlib import Path\n\nimport torch\nfrom torch import nn, optim\nfrom  torch.utils.data import Dataset, DataLoader\n\nfrom resnest.torch import resnest50\n\nfrom matplotlib import pyplot as plt\n\nimport os, random, gc\nimport re, time, json\nfrom  ast import literal_eval\n\n\nfrom IPython.display import Audio\nfrom sklearn.metrics import label_ranking_average_precision_score\n\nfrom tqdm.notebook import tqdm\nimport joblib","metadata":{"_uuid":"8e29bdb0-ac67-40cf-ae89-8ae268428e42","_cell_guid":"536cd2e2-dd0a-4c09-a4bb-64162f8a2e9f","collapsed":false,"id":"2dt7oG43VAqc","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from efficientnet_pytorch import EfficientNet\nimport pretrainedmodels\nimport resnest.torch as resnest_torch","metadata":{"_uuid":"21f8754c-8de0-47e5-9dbe-f59fd9541c0f","_cell_guid":"258945e3-42ff-4f95-a3f0-b4a919aa89d8","collapsed":false,"id":"162Vl9uxe1Mj","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\nseed_everything()","metadata":{"_uuid":"a78cfee0-04c6-4710-955d-be38e4e3e485","_cell_guid":"add7614d-badd-4981-b962-9cfe5cd070dd","collapsed":false,"id":"Q39ZsGAhVAqe","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Hparams():\n    def __init__(self):\n        #resnet50 resnext50_32x4d mobilenet_v2 efficientnet-b3  densenet121 densenet169 \n        self.models_name = ['resnet50','efficientnet-b0','efficientnet-b0','efficientnet-b0','efficientnet-b0','resnet50']\n        #self.chk = ['resnet50_78_0.830_0.666.pt','enet0_101_0.771_0.692.pt','enet0_45_0.558.pt','enet0_133_0.707_0.691.pt',\n        #            '150enet0_116_0.707_0.703.pt','2.5resnet50_113_0.715_0.693.pt']\n        self.chk = ['']\n        self.count_bird = [265,265,265,265,150,265] #count birds|Количество птиц, 264 - all, 265 + nocall\n        self.len_chack = [448,448,448,448,448,224] # The duration of the training files 448 = 5 second|Длительность обучающих файлов\n        \n        self.mel_folder = './mel/'\n        self.n_fft = 892\n        self.sr = 21952 \n        self.hop_length=245\n        self.n_mels =  224\n        self.win_length = self.n_fft\n        self.batch_size = 10 # 3 - b7, 8 - b5,  12 - b3, 25 - b0, 18 - b1 70\n        self.lr = 0.001\n        self.border = 0.5\n        self.save_interval = 20 #Model saving interval\n        # Список из count_bird птиц по пополуярности\n#         self.bird_count = pd.read_csv('bird_count.csv').ebird_code.to_numpy()        \n#         self.BIRD_CODE = {b:i for i,b in enumerate(self.bird_count)}\n#         self.INV_BIRD_CODE = {v: k for k, v in self.BIRD_CODE.items()}\n#         self.bird_count = self.bird_count[:self.count_bird[0]]\n\n\nhp = Hparams()","metadata":{"_uuid":"e7d47a27-5dc4-4a8c-8ec2-792b8396c6c7","_cell_guid":"6db02e51-06c0-46de-88a1-6596fb3e8771","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NUM_CLASSES = 397\nSR = 32_000\nDURATION = 7\n\nMAX_READ_SAMPLES = 5 # Each record will have 10 melspecs at most, you can increase this on Colab with High Memory Enabled\n\n# # For colab\n# DATA_ROOT = Path(\"/content/datasets/\")\n# TRAIN_IMAGES_ROOT = Path(\"/content/datasets/audio_images\")\n# TRAIN_LABELS_FILE = Path(\"/content/datasets/rich_train_metadata.csv\")\n# MODEL_ROOT = Path(\"/content/drive/My Drive/Kaggle/BirdClef2021/models\")\n\nDATA_ROOT = Path(\"../input/birdclef-2021\")\n# TRAIN_IMAGES_ROOT = Path(\"../input/kkiller-birdclef-2021/audio_images\")\n# TRAIN_LABELS_FILE = Path(\"../input/kkiller-birdclef-2021/rich_train_metadata.csv\")\n\nMEL_PATHS = sorted(Path(\"../input\").glob(\"kkiller-birdclef-mels-computer-d7-part?/rich_train_metadata.csv\"))\nTRAIN_LABEL_PATHS = sorted(Path(\"../input\").glob(\"kkiller-birdclef-mels-computer-d7-part?/LABEL_IDS.json\"))\n\nMODEL_ROOT = Path(\".\")","metadata":{"_uuid":"1cc1a7c0-55f3-4c5c-ae31-442d9cd535c8","_cell_guid":"5dd76981-b8b8-4e1c-81cf-934dd019c969","collapsed":false,"id":"2NfkUn9SCWs6","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MEL_PATHS","metadata":{"_uuid":"9017a485-84f2-4276-b576-a0a09fc404d4","_cell_guid":"423952a1-2c19-4623-b638-940f3f5f2328","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TRAIN_BATCH_SIZE = 100\nTRAIN_NUM_WORKERS = 2\n\nVAL_BATCH_SIZE = 128\nVAL_NUM_WORKERS = 2\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nprint(\"Device:\", DEVICE)","metadata":{"_uuid":"083477ae-dfc8-47af-b043-6b591922a749","_cell_guid":"3c550a97-768b-4386-9935-ad4025c4501f","collapsed":false,"id":"Iu56f-7VVAqf","outputId":"0f3fa344-0ed4-47d8-f3c0-218cf3bf5a78","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"a607714f-6818-4d22-b718-626504e73fe3","_cell_guid":"c0eafe5e-d7a3-4133-8688-d70cf725105d","collapsed":false,"id":"GuyjwJnACWs6","outputId":"cdaca87a-567c-4299-9840-7b3cb06be13f","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_df(mel_paths=MEL_PATHS, train_label_paths=TRAIN_LABEL_PATHS):\n  df = None\n  LABEL_IDS = {}\n    \n  for file_path in mel_paths:\n    temp = pd.read_csv(str(file_path), index_col=0)\n    temp[\"impath\"] = temp.apply(lambda row: file_path.parent/\"audio_images/{}/{}.npy\".format(row.primary_label, row.filename), axis=1) \n    df = temp if df is None else df.append(temp)\n    \n  df[\"secondary_labels\"] = df[\"secondary_labels\"].apply(literal_eval)\n\n  for file_path in train_label_paths:\n    with open(str(file_path)) as f:\n      LABEL_IDS.update(json.load(f))\n\n  return LABEL_IDS, df","metadata":{"_uuid":"a05e1b59-20ca-43e1-9611-ef5aa56634c2","_cell_guid":"e6bd95f0-f914-4a81-bd32-4ecd96e3b7f7","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df = pd.read_csv(TRAIN_LABELS_FILE, nrows=None)\n# df[\"secondary_labels\"] = df[\"secondary_labels\"].apply(literal_eval)\n# LABEL_IDS = {label: label_id for label_id,label in enumerate(sorted(df[\"primary_label\"].unique()))}\n\n# print(df.shape)\n# df.head()","metadata":{"_uuid":"94105fd3-6f54-4415-9f45-d9e391788453","_cell_guid":"92f63057-db61-4dfc-bea8-3fcbae9eec9d","collapsed":false,"id":"Kmh6xx5_NCjJ","outputId":"3e92e880-2e37-46c7-ffc0-5551e2641e7b","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LABEL_IDS, df = get_df()\n\nprint(df.shape)","metadata":{"_uuid":"1fb932a3-31a5-4630-8be1-51e18e7d2308","_cell_guid":"91bc3865-926f-4d60-b3a7-c8d7bafb26c2","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def check_secondary(primary,secondary):\n#     for label in secondary:\n#         if label==primary:\n#             return 1\n#     return 0\n# df['dup'] = df.apply(lambda row: check_secondary(row['primary_label'], row['secondary_labels']), axis=1)\n# np.sum(df['dup'])","metadata":{"_uuid":"e81d5140-b353-4b44-aef4-ac82197965b2","_cell_guid":"9b6c8dbb-00f6-4b99-8288-c520e241df71","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def merge_secondary(primary,secondary):\n    out = []\n    for label in secondary:\n        if label==primary:\n            continue\n        if label in LABEL_IDS.keys():\n            out.append(LABEL_IDS[label])\n        else:\n            out.append(LABEL_IDS[label[:-1]])\n    return out\n#     return secondary+[primary]\n\ndf['mix_label_id'] = df.apply(lambda row: merge_secondary(row['primary_label'], row['secondary_labels']), axis=1)\n# df['ALL']#.apply(LABEL_IDS)","metadata":{"_uuid":"7da028c4-f4a5-41fc-ae36-c32213fdce32","_cell_guid":"c8eaee8b-1b21-43b8-957d-fc1d12893fb6","collapsed":false,"id":"W_Xf_natBGGL","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_model(name, num_classes=NUM_CLASSES):\n    \"\"\"\n    Loads a pretrained model. \n    Supports ResNest, ResNext-wsl, EfficientNet, ResNext and ResNet.\n\n    Arguments:\n        name {str} -- Name of the model to load\n\n    Keyword Arguments:\n        num_classes {int} -- Number of classes to use (default: {1})\n\n    Returns:\n        torch model -- Pretrained model\n    \"\"\"\n    if \"resnest\" in name:\n        model = getattr(resnest_torch, name)(pretrained=False)\n        model.load_state_dict(torch.load('../input/resnest-package/resnest50-528c19ca.pth'))\n    elif \"wsl\" in name:\n        model = torch.hub.load(\"facebookresearch/WSL-Images\", name)\n    elif name.startswith(\"resnext\") or  name.startswith(\"resnet\"):\n        model = torch.hub.load(\"pytorch/vision:v0.6.0\", name, pretrained=True)\n    elif name.startswith(\"tf_efficientnet_b\"):\n        model = getattr(timm.models.efficientnet, name)(pretrained=True)\n    elif \"efficientnet-b\" in name:\n        model = EfficientNet.from_pretrained(name)\n    else:\n        model = pretrainedmodels.__dict__[name](pretrained='imagenet')\n\n    if hasattr(model, \"fc\"):\n        nb_ft = model.fc.in_features\n        model.fc = nn.Linear(nb_ft, num_classes)\n    elif hasattr(model, \"_fc\"):\n        nb_ft = model._fc.in_features\n        model._fc = nn.Linear(nb_ft, num_classes)\n    elif hasattr(model, \"classifier\"):\n        nb_ft = model.classifier.in_features\n        model.classifier = nn.Linear(nb_ft, num_classes)\n    elif hasattr(model, \"last_linear\"):\n        nb_ft = model.last_linear.in_features\n        model.last_linear = nn.Linear(nb_ft, num_classes)\n\n    return model","metadata":{"_uuid":"669d90a5-7093-463d-aa11-16efe7cbdbdc","_cell_guid":"e586fbbe-6682-43a0-a6c1-f137dc64ad99","collapsed":false,"id":"OGPDuihmVAqi","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_data(df):\n    def load_row(row):\n        # impath = TRAIN_IMAGES_ROOT/f\"{row.primary_label}/{row.filename}.npy\"\n        return row.filename, np.load(str(row.impath))[:MAX_READ_SAMPLES]\n    pool = joblib.Parallel(4)\n    mapper = joblib.delayed(load_row)\n    tasks = [mapper(row) for row in df.itertuples(False)]\n    res = pool(tqdm(tasks))\n    res = dict(res)\n    return res","metadata":{"_uuid":"efca3115-413d-4615-bb5f-3bcf5ddb9567","_cell_guid":"72d6e286-a1c9-4cca-9515-f07958a7c96a","collapsed":false,"id":"7HYQwAyBCWs8","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We cache the train set to reduce training time\n\naudio_image_store = load_data(df)\nlen(audio_image_store)","metadata":{"_uuid":"33923b51-9bb4-417b-a14b-9e15aaebc8eb","_cell_guid":"68607fb9-3594-46cc-85c8-2d304915431c","collapsed":false,"id":"Vw19bB7mCWs9","outputId":"09a5e374-7e5c-4c92-91e4-b60313cbb1a9","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"shape:\", next(iter(audio_image_store.values())).shape)\nlbd.specshow(next(iter(audio_image_store.values()))[0])","metadata":{"_uuid":"650783e5-d0b3-463b-9681-a9aedea354fe","_cell_guid":"fb60adba-6876-4052-aef1-41b4243c9f38","collapsed":false,"id":"4TNYmT7XCWs9","outputId":"b5052897-ca12-4c25-a49c-3799a88b9f9d","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BirdClefDataset(Dataset):\n\n    def __init__(self, audio_image_store, meta, sr=SR, is_train=True, num_classes=NUM_CLASSES, duration=DURATION):\n        \n        self.audio_image_store = audio_image_store\n        self.meta = meta.copy().reset_index(drop=True)\n        self.sr = sr\n        self.is_train = is_train\n        self.num_classes = num_classes\n        self.duration = duration\n        self.audio_length = self.duration*self.sr\n        self.hp = hp\n        self.level_noise = 0.05 \n    @staticmethod\n    def normalize(image):\n        image = image.astype(\"float32\", copy=False) / 255.0\n        image = np.stack([image, image, image])\n        return image\n\n    def __len__(self):\n        return len(self.meta)\n    \n    def __getitem__(self, idx):\n        self.len_chack = random.randint(self.hp.len_chack[0]-48, self.hp.len_chack[0]+52)\n        row = self.meta.iloc[idx]\n        image = self.audio_image_store[row.filename]\n#         print('22',image.shape,image[np.random.choice(len(image))].shape,'sss')\n\n        image = image[np.random.choice(len(image))]\n        image = self.normalize(image)\n#         print(image.shape,'****')\n#         if random.random()<0.9:\n\n#             image = image + (np.random.sample((self.hp.n_mels,self.len_chack)).astype(np.float32)+9) * image.mean() * self.level_noise * (np.random.sample() + 0.3)\n        \n        # Add pink noise | Добавить розовый шум\n#         if random.random()<0.9:\n#             r = random.randint(1,self.hp.n_mels)\n#             pink_noise = np.array([np.concatenate((1 - np.arange(r)/r,np.zeros(self.hp.n_mels-r)))]).T\n#             image = image + (np.random.sample((self.hp.n_mels,self.len_chack)).astype(np.float32)+9) * 2  * image.mean() * self.level_noise * (np.random.sample() + 0.3)\n        \n        # Add bandpass noise | Добавить полосовой шум\n#         if random.random()<0.9:\n#             a = random.randint(0, self.hp.n_mels//2)\n#             b = random.randint(a+20, self.hp.n_mels)\n#             image[a:b,:] = image[a:b,:] + (np.random.sample((b-a,self.len_chack)).astype(np.float32)+9) * 0.05 * image.mean() * self.level_noise  * (np.random.sample() + 0.3)\n        t = np.zeros(self.num_classes, dtype=np.float32) + 0.0025 # Label smoothing\n        t[row.label_id] = 0.995\n\n        if self.is_train:\n            if random.random()>0.5:\n                image = image + np.random.normal(image.mean(), 0.001 ** 0.5, image.shape)\n#                 image = image.astype(np.float)\n            t[row.mix_label_id] = 0.75\n        return image, t","metadata":{"_uuid":"36ffd349-bafc-48cd-88cf-6bc5340bd253","_cell_guid":"4c845393-801e-4cf4-81ba-0a0305186e16","collapsed":false,"id":"OWSkCXyhCWs-","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ds = BirdClefDataset(audio_image_store, meta=df, sr=SR, duration=DURATION, is_train=True)\nlen(df)","metadata":{"_uuid":"8e498314-0642-4e98-97bd-f6807ec3ba47","_cell_guid":"b4f8e9ed-303c-4da6-8c73-b24f2c9027a6","collapsed":false,"id":"Np-56XrXVAqm","outputId":"84e45885-200f-4c98-a9a1-23ef459e5ade","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x, y = ds[np.random.choice(len(ds))]\n# x, y = ds[0]\nx.shape, y.shape, np.where(y >= 0.5),lbd.specshow(x[0])","metadata":{"_uuid":"69c9404e-fa23-40f3-994e-3d7ab0a6b17b","_cell_guid":"0eb25291-27ea-4548-b0d4-fb8659f35a9f","collapsed":false,"id":"UNVUxIMpVAqm","outputId":"96e0930f-51f7-4268-9939-2d81999c002f","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.sum(y==0.75)","metadata":{"_uuid":"a332ceb5-c77b-4c80-9929-b15978709d02","_cell_guid":"327db3ba-b5b6-4f97-bff1-b58eec0cbc5c","collapsed":false,"id":"GBr32Q9FCWs_","outputId":"141ba3fd-99b1-4878-8462-2a6eb3cb5e19","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"c46d9430-b73a-427b-ba34-55779a4b11aa","_cell_guid":"56de24e9-304f-4bbc-afe2-be4e4385a879","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training the model","metadata":{"_uuid":"4829575b-8610-4427-b0be-ae02266f8968","_cell_guid":"f2971f31-d7dc-4b43-a5cf-fa43c825614a","id":"F56zXq8CVAqn","trusted":true}},{"cell_type":"code","source":"","metadata":{"_uuid":"936ff1b7-829d-4f29-8e00-98d9f6be5117","_cell_guid":"d9b9ca5b-01a9-41cb-ac95-7c7e5f04df0e","collapsed":false,"id":"THm438BwMTeR","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def one_step( xb, yb, net, criterion, optimizer, scheduler=None):\n  alpha = random.random()\n  xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n  xb, labels_a, labels_b, lam = mixup_data(xb, yb, alpha)\n  optimizer.zero_grad()\n  o = net(xb.float())\n  loss = mixup_criterion(criterion, o, labels_a, labels_b, lam)\n#   loss = criterion(o, yb)\n  loss.backward()\n  optimizer.step()\n  \n  with torch.no_grad():\n      l = loss.item()\n\n      o = o.sigmoid()\n      yb = (yb > 0.5 )*1.0\n      lrap = label_ranking_average_precision_score(yb.cpu().numpy(), o.cpu().numpy())\n\n      o = (o > 0.5)*1.0\n\n      prec = (o*yb).sum()/(1e-6 + o.sum())\n      rec = (o*yb).sum()/(1e-6 + yb.sum())\n      f1 = 2*prec*rec/(1e-6+prec+rec)\n\n  if  scheduler is not None:\n    scheduler.step()\n\n  return l, lrap, f1.item(), rec.item(), prec.item()","metadata":{"_uuid":"110a215b-9c75-4f2a-a286-c5740e9a579e","_cell_guid":"cdd6a6d0-2f42-4aa1-a328-a41c5643304a","collapsed":false,"id":"9Kjy1uquIGZw","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@torch.no_grad()\ndef evaluate(net, criterion, val_laoder):\n    net.eval()\n\n    os, y = [], []\n    val_laoder = tqdm(val_laoder, leave = False, total=len(val_laoder))\n\n    for icount, (xb, yb) in  enumerate(val_laoder):\n\n        y.append(yb.to(DEVICE))\n\n        xb = xb.to(DEVICE)\n        o = net(xb.float())\n\n        os.append(o)\n\n    y = torch.cat(y)\n    o = torch.cat(os)\n\n    l = criterion(o, y).item()\n    \n    o = o.sigmoid()\n    y = (y > 0.5)*1.0\n\n    lrap = label_ranking_average_precision_score(y.cpu().numpy(), o.cpu().numpy())\n\n    o = (o > 0.5)*1.0\n\n    prec = ((o*y).sum()/(1e-6 + o.sum())).item()\n    rec = ((o*y).sum()/(1e-6 + y.sum())).item()\n    f1 = 2*prec*rec/(1e-6+prec+rec)\n\n    return l, lrap, f1, rec, prec,","metadata":{"_uuid":"67e02637-d6c0-46a8-8da2-1a1a2fe97cac","_cell_guid":"8fa54b23-7b43-45ae-aa71-88e3a109f30e","collapsed":false,"id":"q9v79J0pvXy1","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def one_epoch(net, criterion, optimizer, scheduler, train_laoder, val_laoder):\n  net.train()\n  l, lrap, prec, rec, f1, icount = 0.,0.,0.,0., 0., 0\n  train_laoder = tqdm(train_laoder, leave = False)\n  epoch_bar = train_laoder\n  \n  for (xb, yb) in  epoch_bar:\n      # epoch_bar.set_description(\"----|----|----|----|---->\")\n      _l, _lrap, _f1, _rec, _prec = one_step(xb, yb, net, criterion, optimizer)\n      l += _l\n      lrap += _lrap\n      f1 += _f1\n      rec += _rec\n      prec += _prec\n\n      icount += 1\n        \n      if hasattr(epoch_bar, \"set_postfix\") and not icount%10:\n          epoch_bar.set_postfix(\n            loss=\"{:.6f}\".format(l/icount),\n            lrap=\"{:.3f}\".format(lrap/icount),\n            prec=\"{:.3f}\".format(prec/icount),\n            rec=\"{:.3f}\".format(rec/icount),\n            f1=\"{:.3f}\".format(f1/icount),\n          )\n  \n  scheduler.step()\n\n  l /= icount\n  lrap /= icount\n  f1 /= icount\n  rec /= icount\n  prec /= icount\n  \n  l_val, lrap_val, f1_val, rec_val, prec_val = evaluate(net, criterion, val_laoder)\n  \n  return (l, l_val), (lrap, lrap_val), (f1, f1_val), (rec, rec_val), (prec, prec_val)","metadata":{"_uuid":"4d7cd938-eb86-4c89-868c-ae3e6063b0d3","_cell_guid":"76a4dbd5-2fd6-43e2-8977-5ad12d268255","collapsed":false,"id":"qeDgf4LdLWGN","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"deba91cb-6938-427f-bc1c-06f1571147d6","_cell_guid":"b560fe6d-bef7-46f9-8f1e-8d8eb4200b36","collapsed":false,"id":"IY4ET5V0RMJm","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AutoSave:\n  def __init__(self, top_k=44, metric=\"f1\", mode=\"min\", root=None, name=\"ckpt\"):\n    self.top_k = top_k\n    self.logs = []\n    self.metric = metric\n    self.mode = mode\n    self.root = Path(root or MODEL_ROOT)\n    assert self.root.exists()\n    self.name = name\n\n    self.top_models = []\n    self.top_metrics = []\n\n  def log(self, model, metrics):\n    metric = metrics[self.metric]\n    rank = self.rank(metric)\n\n    self.top_metrics.insert(rank+1, metric)\n    if len(self.top_metrics) > self.top_k:\n      self.top_metrics.pop(0)\n\n    self.logs.append(metrics)\n    self.save(model, metric, rank, metrics[\"epoch\"])\n\n\n  def save(self, model, metric, rank, epoch):\n    t = time.strftime(\"%Y%m%d%H%M%S\")\n    name = \"{}_epoch_{:02d}_{}_{:.04f}_{}\".format(self.name, epoch, self.metric, metric, t)\n    name = re.sub(r\"[^\\w_-]\", \"\", name) + \".pth\"\n    path = self.root.joinpath(name)\n\n    old_model = None\n    self.top_models.insert(rank+1, name)\n    if len(self.top_models) > self.top_k:\n      old_model = self.root.joinpath(self.top_models[0])\n      self.top_models.pop(0)      \n\n    torch.save(model.state_dict(), path.as_posix())\n\n    if old_model is not None:\n      old_model.unlink()\n\n    self.to_json()\n\n\n  def rank(self, val):\n    r = -1\n    for top_val in self.top_metrics:\n      if val <= top_val:\n        return r\n      r += 1\n\n    return r\n  \n  def to_json(self):\n    # t = time.strftime(\"%Y%m%d%H%M%S\")\n    name = \"{}_logs\".format(self.name)\n    name = re.sub(r\"[^\\w_-]\", \"\", name) + \".json\"\n    path = self.root.joinpath(name)\n\n    with path.open(\"w\") as f:\n      json.dump(self.logs, f, indent=2)","metadata":{"_uuid":"52832904-76a9-4957-9626-8ca0c889de2c","_cell_guid":"cd4888df-6121-4b27-904c-0251cb58a39d","collapsed":false,"id":"Cz7XPBvtPLO1","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def one_fold(model_name, fold, train_set, val_set, epochs=20, save=True, save_root=None):\n\n  save_root = Path(save_root) or MODEL_ROOT\n\n  saver = AutoSave(root=save_root, name=f\"birdclef_{model_name}_fold{fold}\", metric=\"f1_val\")\n\n  net = get_model(model_name).to(DEVICE)\n\n  criterion = nn.BCEWithLogitsLoss()\n\n  optimizer = optim.Adam(net.parameters(), lr=8e-4)\n  scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=1e-5, T_max=epochs)\n\n  train_data = BirdClefDataset(audio_image_store, meta=df.iloc[train_set].reset_index(drop=True),\n                           sr=SR, duration=DURATION, is_train=True)\n  train_laoder = DataLoader(train_data, batch_size=TRAIN_BATCH_SIZE, num_workers=TRAIN_NUM_WORKERS, shuffle=True, pin_memory=True)\n\n  val_data = BirdClefDataset(audio_image_store, meta=df.iloc[val_set].reset_index(drop=True),  sr=SR, duration=DURATION, is_train=False)\n  val_laoder = DataLoader(val_data, batch_size=VAL_BATCH_SIZE, num_workers=VAL_NUM_WORKERS, shuffle=False)\n\n  epochs_bar = tqdm(list(range(epochs)), leave=False)\n  for epoch  in epochs_bar:\n    epochs_bar.set_description(f\"--> [EPOCH {epoch:02d}]\")\n    net.train()\n\n    (l, l_val), (lrap, lrap_val), (f1, f1_val), (rec, rec_val), (prec, prec_val) = one_epoch(\n        net=net,\n        criterion=criterion,\n        optimizer=optimizer,\n        scheduler=scheduler,\n        train_laoder=train_laoder,\n        val_laoder=val_laoder,\n      )\n\n    epochs_bar.set_postfix(\n    loss=\"({:.6f}, {:.6f})\".format(l, l_val),\n    prec=\"({:.3f}, {:.3f})\".format(prec, prec_val),\n    rec=\"({:.3f}, {:.3f})\".format(rec, rec_val),\n    f1=\"({:.3f}, {:.3f})\".format(f1, f1_val),\n    lrap=\"({:.3f}, {:.3f})\".format(lrap, lrap_val),\n    )\n\n    print(\n        \"[{epoch:02d}] loss: {loss} lrap: {lrap} f1: {f1} rec: {rec} prec: {prec}\".format(\n            epoch=epoch,\n            loss=\"({:.6f}, {:.6f})\".format(l, l_val),\n            prec=\"({:.3f}, {:.3f})\".format(prec, prec_val),\n            rec=\"({:.3f}, {:.3f})\".format(rec, rec_val),\n            f1=\"({:.3f}, {:.3f})\".format(f1, f1_val),\n            lrap=\"({:.3f}, {:.3f})\".format(lrap, lrap_val),\n        )\n    )\n\n    if save:\n      metrics = {\n          \"loss\": l, \"lrap\": lrap, \"f1\": f1, \"rec\": rec, \"prec\": prec,\n          \"loss_val\": l_val, \"lrap_val\": lrap_val, \"f1_val\": f1_val, \"rec_val\": rec_val, \"prec_val\": prec_val,\n          \"epoch\": epoch,\n      }\n\n      saver.log(net, metrics)","metadata":{"_uuid":"923cb15a-e6e8-4491-a9fd-5475ad1612c0","_cell_guid":"598d5edc-d134-4dbd-99da-b9e3f5cb1e9b","collapsed":false,"id":"8X1dt_aWNi6F","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(model_name, epochs=20, save=True, n_splits=5, seed=177, save_root=None, suffix=\"\", folds=None):\n  gc.collect()\n  torch.cuda.empty_cache()\n\n  save_root = save_root or MODEL_ROOT/f\"{model_name}{suffix}\"\n  save_root.mkdir(exist_ok=True, parents=True)\n  \n  fold_bar = tqdm(df.reset_index().groupby(\"fold\").index.apply(list).items(), total=df.fold.max()+1)\n  \n  for fold, val_set in fold_bar:\n      if folds and not fold in folds:\n        continue\n      \n      print(f\"\\n############################### [FOLD {fold}]\")\n      fold_bar.set_description(f\"[FOLD {fold}]\")\n      train_set = np.setdiff1d(df.index, val_set)\n        \n      one_fold(model_name, fold=fold, train_set=train_set , val_set=val_set , epochs=epochs, save=save, save_root=save_root)\n    \n      gc.collect()\n      torch.cuda.empty_cache()","metadata":{"_uuid":"e6c712c4-3762-40dc-95c0-483cd8a4bc7c","_cell_guid":"af7647eb-0f51-42f3-bb76-316e8a7f3740","collapsed":false,"id":"ljqr4e2zQmzB","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MODEL_NAMES = [\n      \"resnest50\",\n#       \"resnet50\",\n\n]","metadata":{"_uuid":"ff89d543-f98e-4966-8605-33c84511c1e8","_cell_guid":"7c28846d-10bc-4c8f-8cd5-a74eb46d9cea","collapsed":false,"id":"aqN6xL7gVAqq","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for model_name in MODEL_NAMES:\n  print(\"\\n\\n###########################################\", model_name.upper())\n  try:\n    train(model_name, epochs=12, suffix=f\"_gauss_sr{SR}_d{DURATION}_v1_v1\", folds=[0])\n  except Exception as e:\n    # print(f\"Error {model_name} : \\n{e}\")\n    raise ValueError() from  e","metadata":{"_uuid":"a1b14082-afc4-43a9-a3c7-7e61535d606a","_cell_guid":"2c4f4e43-a21a-49c2-8e64-112824010446","collapsed":false,"id":"WyFnAQGWELb_","outputId":"d676c647-65e4-43d1-a8f3-69f3d949e727","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"8480c88f-dde6-4df6-a36d-0d5ff768d8a0","_cell_guid":"d4344f94-0987-4ac8-9094-ea2780524637","collapsed":false,"id":"Q46u71ImEL4E","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"c0ea994a-d57d-437d-86e3-d02e9fff4f2c","_cell_guid":"b25f03ba-2a87-40f1-a11d-d066c2f27a90","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}