{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sys\nsys.path.append('../input/timm-pytorch-image-models/pytorch-image-models-master')","metadata":{"execution":{"iopub.status.busy":"2021-12-01T09:20:20.726474Z","iopub.execute_input":"2021-12-01T09:20:20.727149Z","iopub.status.idle":"2021-12-01T09:20:20.738293Z","shell.execute_reply.started":"2021-12-01T09:20:20.727037Z","shell.execute_reply":"2021-12-01T09:20:20.737083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport torch\n\nimport cv2\nfrom os import path\nfrom PIL import Image\nimport torch.utils.data as data\nfrom torchvision import models, transforms\nimport torch.nn as nn\nfrom tqdm import tqdm\nimport torch.nn.functional as F\nimport h5py\nimport timm\n\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import median_absolute_error\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import explained_variance_score\nfrom scipy.stats import pearsonr","metadata":{"execution":{"iopub.status.busy":"2021-12-01T09:20:20.740907Z","iopub.execute_input":"2021-12-01T09:20:20.742808Z","iopub.status.idle":"2021-12-01T09:20:24.596746Z","shell.execute_reply.started":"2021-12-01T09:20:20.742762Z","shell.execute_reply":"2021-12-01T09:20:24.595599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class args:\n    seed = 22\n    batch_size = 32\n    epochs = 10\n    image_size = 384  # scale shorter end of image to this size and centre crop\n    central_fraction = 0.875  # only take this much of the centre when scaling and centre cropping\n    load_img = False # whether load img or not\n    workers = 2\n    feature_extract = True\n\ndef seed_torch(seed):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    print(\"seed: \", seed)\n\nseed_torch(args.seed)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T09:20:24.598998Z","iopub.execute_input":"2021-12-01T09:20:24.599441Z","iopub.status.idle":"2021-12-01T09:20:24.614536Z","shell.execute_reply.started":"2021-12-01T09:20:24.599392Z","shell.execute_reply":"2021-12-01T09:20:24.612731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dir_path = '/kaggle/input/petfinder-pawpularity-score'\n# dir_path = '../input/petfinder-pawpularity-score' # This is used in Kaggle Code\n\ncached_dir = path.join(dir_path, 'cached_data')\ntrain_img_dir = path.join(dir_path, 'train')\ntest_img_dir = path.join(dir_path, 'test')\n\ndf_train = pd.read_csv(path.join(dir_path, 'train.csv'))\ndf_test = pd.read_csv(path.join(dir_path, 'test.csv'))\n\ntrain_img_paths = [path.join(train_img_dir, f\"{img_id}.jpg\") for img_id in df_train[\"Id\"].values]\ntest_img_paths = [path.join(test_img_dir, f\"{img_id}.jpg\") for img_id in df_test[\"Id\"].values]","metadata":{"execution":{"iopub.status.busy":"2021-12-01T09:20:24.618588Z","iopub.execute_input":"2021-12-01T09:20:24.619584Z","iopub.status.idle":"2021-12-01T09:20:24.676474Z","shell.execute_reply.started":"2021-12-01T09:20:24.619535Z","shell.execute_reply":"2021-12-01T09:20:24.675556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"property_names = [col for col in df_train.columns if col not in ['Id','Pawpularity']]\nproperty_names # 12 properties\n\n# train metda data\ntrain_meta_X = df_train[property_names]\ntrain_Y = df_train['Pawpularity']\n\n# test metda data\ntest_id = df_test['Id']\ntest_meta_X = df_test.drop('Id',axis=1)\ntest_meta_X.shape","metadata":{"execution":{"iopub.status.busy":"2021-12-01T09:20:24.67794Z","iopub.execute_input":"2021-12-01T09:20:24.678537Z","iopub.status.idle":"2021-12-01T09:20:24.694705Z","shell.execute_reply.started":"2021-12-01T09:20:24.678492Z","shell.execute_reply":"2021-12-01T09:20:24.693263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Pytorch Data loader\n\n\nclass PawpularDataset(data.Dataset):\n    def __init__(self, image_data, meta_features, labels, img_paths, augmentations=None):\n        super(PawpularDataset, self).__init__()\n        self.load_img = False\n        self.image_data = image_data\n        self.meta_features = meta_features\n        self.labels = labels\n        self.augmentations = augmentations\n        self.image_paths = img_paths\n        if self.augmentations is not None:\n            self.load_img = True\n\n\n    def __getitem__(self, item):\n        if self.load_img:\n            image = cv2.imread(self.image_paths[item])\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n            \n            augmented = self.augmentations(image=image)[\"image\"]\n            image = np.transpose(augmented, (2, 0, 1)).astype(np.float32)\n        else:\n            image = self.image_data[item]\n\n        meta = self.meta_features[item]\n        label = self.labels[item]\n        return image.astype('float32'), meta.astype('float32'), label\n\n    def __len__(self):\n        return len(self.labels)\n\n\ndef get_loader(split, features, meta_data, labels, img_paths=[], batch_size=args.batch_size, augmentations=None):\n    \"\"\" Returns a data loader for the desired split \"\"\"\n    dataset = PawpularDataset(features, meta_data, labels, img_paths, augmentations)\n    loader = torch.utils.data.DataLoader(\n        dataset,\n        batch_size=batch_size,\n        shuffle=True if split != 'test' else False,  # only shuffle the data in training\n        pin_memory=True,\n        num_workers=args.workers,\n    )\n    return loader","metadata":{"execution":{"iopub.status.busy":"2021-12-01T09:20:24.69698Z","iopub.execute_input":"2021-12-01T09:20:24.697421Z","iopub.status.idle":"2021-12-01T09:20:24.71466Z","shell.execute_reply.started":"2021-12-01T09:20:24.69736Z","shell.execute_reply":"2021-12-01T09:20:24.713435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import albumentations\n\ndef get_augmentations(train=True):\n    if train:\n        return albumentations.Compose(\n            [\n                albumentations.Resize(args.image_size, args.image_size, p=1),\n                albumentations.HorizontalFlip(p=0.5),\n                albumentations.VerticalFlip(p=0.5),\n                albumentations.Rotate(limit=180, p=0.7),\n                albumentations.ShiftScaleRotate(\n                    shift_limit = 0.1, scale_limit=0.1, rotate_limit=45, p=0.5\n                ),\n                albumentations.HueSaturationValue(\n                    hue_shift_limit=0.2, sat_shift_limit=0.2,\n                    val_shift_limit=0.2, p=0.5\n                ),\n                albumentations.RandomBrightnessContrast(\n                    brightness_limit=(-0.1, 0.1),\n                    contrast_limit=(-0.1, 0.1), p=0.5\n                ),\n                albumentations.Normalize(\n                    mean=[0.485, 0.456, 0.406],\n                    std=[0.229, 0.224, 0.225],\n                    max_pixel_value=255.0,\n                    p=1.0,\n                ),\n            ]\n        )\n    else:\n        return albumentations.Compose(\n            [\n                albumentations.Resize(args.image_size, args.image_size, p=1),\n                albumentations.Normalize(\n                    mean=[0.485, 0.456, 0.406],\n                    std=[0.229, 0.224, 0.225],\n                    max_pixel_value=255.0,\n                    p=1.0,\n                ),\n            ]\n        )\n\naugmentations = get_augmentations(train=True)\ntrainval_loader = get_loader('trainval', None, train_meta_X.values, train_Y.values.astype('float32'), train_img_paths,augmentations=augmentations)\n\naugmentations = get_augmentations(train=False)\ntest_loader = get_loader('test', None, test_meta_X.values, torch.zeros(len(test_img_paths)), test_img_paths, augmentations=augmentations)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T09:20:24.718549Z","iopub.execute_input":"2021-12-01T09:20:24.719217Z","iopub.status.idle":"2021-12-01T09:20:25.179364Z","shell.execute_reply.started":"2021-12-01T09:20:24.719115Z","shell.execute_reply":"2021-12-01T09:20:25.178116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_parameter_requires_grad(model, feature_extracting):\n    if feature_extracting:\n        for param in model.parameters():\n            param.requires_grad = False\n\nclass PretrainedCNN(nn.Module):\n    def __init__(self, model_name='resnet', use_meta=False):\n        super(PretrainedCNN, self).__init__()\n        if model_name == 'resnet':\n            self.model = models.resnet152(pretrained=True)\n            set_parameter_requires_grad(self.model, feature_extracting=True)\n            num_ftrs = self.model.fc.in_features\n        self.model.fc = nn.Linear(num_ftrs, 128)\n        self.drop = nn.Dropout(0.5)\n        self.use_meta =use_meta\n        if self.use_meta:\n            self.fc = nn.Linear(128+12, 1)\n        else:\n            self.fc = nn.Linear(128, 1)\n        \n\n    def forward(self, img_data, meta=None, targets=None):\n        x = self.model(img_data)\n        if self.use_meta:\n            x = torch.cat([x, meta], dim=1)\n        x = self.fc(self.drop(x)) # [b, o]\n        return x\n\n\n\nclass BaseCNN(nn.Module):\n    def __init__(self, num_filters=[], use_meta=False):\n        super(BaseCNN, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(3, num_filters[0], kernel_size = 3, padding = 1),\n            nn.ReLU(),\n            nn.MaxPool2d(2,2),\n            nn.Conv2d(num_filters[0], num_filters[1], kernel_size = 4, stride = 1, padding = 1),\n            nn.ReLU(),\n            nn.MaxPool2d(2,2),\n        \n            nn.Conv2d(num_filters[1], num_filters[2], kernel_size = 5, stride = 1, padding = 1),\n            nn.ReLU(),\n            nn.MaxPool2d(2,2),\n            nn.Conv2d(num_filters[2] ,num_filters[3], kernel_size = 8, stride = 1, padding = 1),\n            nn.ReLU(),\n            nn.MaxPool2d(2,2),\n            \n            nn.Conv2d(num_filters[3], num_filters[4], kernel_size = 6, stride = 1, padding = 1),\n            nn.ReLU(),\n            nn.Conv2d(num_filters[4], num_filters[4], kernel_size = 6, stride = 1, padding = 1),\n            nn.ReLU(),\n            nn.MaxPool2d(2,2),\n\n            nn.Flatten(),\n            nn.Linear(12544, 1024),\n            nn.ReLU(),\n            nn.Linear(1024, 512),\n            nn.ReLU(),\n            nn.Linear(512,128)\n        )\n        # fc\n        self.use_meta = use_meta\n        self.drop = nn.Dropout(0.5)\n        if self.use_meta:\n            self.fc = nn.Linear(128+12, 1)\n        else:\n            self.fc = nn.Linear(128, 1)\n\n\n    def forward(self, img_data, meta=None, targets=None):\n        o = self.conv(self.drop(img_data)) # [b, 1, m]\n        if self.use_meta:\n            o = torch.cat([o, meta], dim=1)\n        x = self.fc(self.drop(o)) # [b, o]\n        return x\n\n\nclass BaseMLP(nn.Module):\n    def __init__(self, use_meta=False, input_size=args.image_size):\n        super(BaseMLP, self).__init__()\n        # fc\n        self.use_meta = use_meta\n        self.drop = nn.Dropout(0.5)\n        self.lin = nn.Sequential(\n            nn.Linear(input_size*input_size, 2048),\n            nn.ReLU(),\n            nn.BatchNorm1d(3),\n            nn.Dropout(0.5),\n            nn.Linear(2048, 512),\n            nn.ReLU(),\n            nn.BatchNorm1d(3),\n            nn.Dropout(0.5),\n            nn.Linear(512,128)\n        )\n        if self.use_meta:\n            self.fc = nn.Linear((128*3)+12, 1)\n        else:\n            self.fc = nn.Linear(128*3, 1)\n\n\n    def forward(self, img_data, meta=None, targets=None):\n        b, c, img_size, _ = img_data.shape\n        img_data = img_data.view(b, c, img_size*img_size)\n        o = self.lin(self.drop(img_data)) # [b, 3, 128]\n        o = o.view(b, -1)\n        if self.use_meta:\n            o = torch.cat([o, meta], dim=1)\n        x = self.fc(self.drop(o)) # [b, o]\n        return x\n\n\nclass PretrainedTransformer(nn.Module):\n    def __init__(self, model_name='', use_meta=False, pretrained=True):\n        super(PretrainedTransformer, self).__init__()\n        # fc\n        self.use_meta = use_meta\n        self.drop = nn.Dropout(0.5)\n\n        self.model = timm.create_model(model_name, pretrained=pretrained, num_classes=0, in_chans=3)\n        set_parameter_requires_grad(self.model, feature_extracting=True)\n        num_features = self.model.num_features\n\n        if self.use_meta:\n            self.fc = nn.Linear(num_features+12, 1)\n        else:\n            self.fc = nn.Linear(num_features, 1)\n\n\n    def forward(self, img_data, meta=None, targets=None):\n        x = self.model(img_data)\n        if self.use_meta:\n            x = torch.cat([x, meta], dim=1)\n        x = self.fc(self.drop(x)) # [b, o]\n        return x\n\n\n\n\ntorch.backends.cudnn.enabled = True\ntorch.backends.cudnn.benchmark = True","metadata":{"execution":{"iopub.status.busy":"2021-12-01T09:20:25.181282Z","iopub.execute_input":"2021-12-01T09:20:25.182138Z","iopub.status.idle":"2021-12-01T09:20:25.220093Z","shell.execute_reply.started":"2021-12-01T09:20:25.182053Z","shell.execute_reply":"2021-12-01T09:20:25.218828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train \ndef train(model, optimizer, train_loader, val_loader, train=True, val=False, epoch=0, grad_clip=False):\n    labels, preds = [], []\n    if train:\n        mse = nn.MSELoss()\n        train_loader = tqdm(train_loader, desc='{} E{:03d}'.format('train', epoch), ncols=0)\n        model.train()\n        for i, (img, meta_feature, label) in enumerate(train_loader):\n            img = img.cuda().float()\n            meta_feature = meta_feature.cuda()\n            label = label.cuda()\n            pred = model(img, meta_feature, label)\n            loss = mse(pred, label.view(-1, 1))\n            loss.backward()\n            if grad_clip:\n                nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n            optimizer.step()\n            optimizer.zero_grad()\n    \n    if val:\n        model.eval()\n        val_loader = tqdm(val_loader, desc='{} E{:03d}'.format('val', epoch), ncols=0)\n        for i, (img, meta_feature, label) in enumerate(val_loader):\n            img = img.cuda().float()\n            pred = model(img, meta_feature, None)\n\n            labels.append(label.detach().cpu())\n            preds.append(pred.detach().cpu())\n\n        labels = torch.cat(labels, dim=0).numpy() # [num_seg]\n        preds = torch.cat(preds, dim=0).numpy() # [num_seg]\n\n    return labels, preds ","metadata":{"execution":{"iopub.status.busy":"2021-12-01T09:20:25.224247Z","iopub.execute_input":"2021-12-01T09:20:25.224628Z","iopub.status.idle":"2021-12-01T09:20:25.239729Z","shell.execute_reply.started":"2021-12-01T09:20:25.22458Z","shell.execute_reply":"2021-12-01T09:20:25.23814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.optim as optim\n\nname = 'tf_efficientnet_b0_ns'\nmodel = PretrainedTransformer(name, use_meta=False, pretrained=False)\n\nparams_to_update = model.parameters()\nprint(\"Params to learn:\")\nif args.feature_extract:\n    params_to_update = []\n    for name,param in model.named_parameters():\n        if param.requires_grad == True:\n            params_to_update.append(param)\n            print(\"\\t\",name)\nelse:\n    for name,param in model.named_parameters():\n        if param.requires_grad == True:\n            print(\"\\t\",name)\n            \noptimizer = optim.Adam(params_to_update, lr=1e-2, amsgrad=False)\nmodel = nn.DataParallel(model).cuda()","metadata":{"execution":{"iopub.status.busy":"2021-12-01T09:20:25.244398Z","iopub.execute_input":"2021-12-01T09:20:25.245094Z","iopub.status.idle":"2021-12-01T09:20:27.801419Z","shell.execute_reply.started":"2021-12-01T09:20:25.245023Z","shell.execute_reply":"2021-12-01T09:20:27.800368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # # Train & val the model\n# best_score = 0\n# epochs = args.epochs\n# for epoch in range(epochs):\n#     labels, pred = train(model, optimizer, trainval_loader, None, train=True, val=False, epoch=epoch, grad_clip=True)\n#     torch.save({\"model\": model.state_dict()}, \"efficient_model.pth\")","metadata":{"execution":{"iopub.status.busy":"2021-12-01T09:20:27.803363Z","iopub.execute_input":"2021-12-01T09:20:27.803698Z","iopub.status.idle":"2021-12-01T09:20:27.808483Z","shell.execute_reply.started":"2021-12-01T09:20:27.803653Z","shell.execute_reply":"2021-12-01T09:20:27.807213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test the model\nlogs = torch.load(\"../input/efficient-model-paw/efficient_model.pth\")\nmodel.load_state_dict(logs['model'])\n_, pred = train(model, optimizer, None, test_loader, train=False, val=True, epoch=0, grad_clip=False)\nprint(\"score:{}\".format(pred.mean().item()))\n\nsubmission = pd.DataFrame({'Id':test_id,'Pawpularity':pred.squeeze(1)})","metadata":{"execution":{"iopub.status.busy":"2021-12-01T09:20:27.810381Z","iopub.execute_input":"2021-12-01T09:20:27.810982Z","iopub.status.idle":"2021-12-01T09:20:30.218014Z","shell.execute_reply.started":"2021-12-01T09:20:27.810902Z","shell.execute_reply":"2021-12-01T09:20:30.21684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T09:20:37.937032Z","iopub.execute_input":"2021-12-01T09:20:37.9374Z","iopub.status.idle":"2021-12-01T09:20:37.944506Z","shell.execute_reply.started":"2021-12-01T09:20:37.937367Z","shell.execute_reply":"2021-12-01T09:20:37.943366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}