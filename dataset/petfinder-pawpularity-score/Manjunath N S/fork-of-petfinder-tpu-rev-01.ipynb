{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### importing all required library","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n''' importing seaborn QQ plot module '''\ntry :\n    from seaborn_qqplot import pplot\nexcept:\n    !pip install seaborn_qqplot\n    from seaborn_qqplot import pplot","metadata":{"execution":{"iopub.status.busy":"2021-11-22T14:10:20.399242Z","iopub.execute_input":"2021-11-22T14:10:20.399645Z","iopub.status.idle":"2021-11-22T14:10:20.406183Z","shell.execute_reply.started":"2021-11-22T14:10:20.399594Z","shell.execute_reply":"2021-11-22T14:10:20.405142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from kaggle_datasets import KaggleDatasets","metadata":{"execution":{"iopub.status.busy":"2021-11-22T14:10:20.408339Z","iopub.execute_input":"2021-11-22T14:10:20.409228Z","iopub.status.idle":"2021-11-22T14:10:20.421644Z","shell.execute_reply.started":"2021-11-22T14:10:20.409179Z","shell.execute_reply":"2021-11-22T14:10:20.420908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Changing plot style","metadata":{}},{"cell_type":"code","source":"sns.set_style(\"whitegrid\")","metadata":{"execution":{"iopub.status.busy":"2021-11-22T14:10:20.422785Z","iopub.execute_input":"2021-11-22T14:10:20.423504Z","iopub.status.idle":"2021-11-22T14:10:20.437971Z","shell.execute_reply.started":"2021-11-22T14:10:20.423455Z","shell.execute_reply":"2021-11-22T14:10:20.43704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# image reading and transformation library fromscikit","metadata":{}},{"cell_type":"code","source":"from skimage import io\nfrom skimage.transform import resize","metadata":{"execution":{"iopub.status.busy":"2021-11-22T14:10:20.439771Z","iopub.execute_input":"2021-11-22T14:10:20.440045Z","iopub.status.idle":"2021-11-22T14:10:20.452273Z","shell.execute_reply.started":"2021-11-22T14:10:20.440015Z","shell.execute_reply":"2021-11-22T14:10:20.451206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reading train and test csv file ","metadata":{}},{"cell_type":"code","source":"train_file = r\"../input/petfinder-pawpularity-score/train.csv\"\ntest_file =r\"../input/petfinder-pawpularity-score/test.csv\"\ntrain_jpeg_path =\"../input/petfinder-pawpularity-score/train/\"\ntest_jpeg_path = \"../input/petfinder-pawpularity-score/test/\"\ntrain_df = pd.read_csv( train_file, sep =\",\")\ntest_df = pd.read_csv( test_file, sep =\",\" )\n\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-22T14:10:20.453996Z","iopub.execute_input":"2021-11-22T14:10:20.455112Z","iopub.status.idle":"2021-11-22T14:10:20.505493Z","shell.execute_reply.started":"2021-11-22T14:10:20.455069Z","shell.execute_reply":"2021-11-22T14:10:20.50475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# THis to store min max value for MIn max Scaling opration in TF record ","metadata":{}},{"cell_type":"code","source":"MIN_DATA = train_df[\"Pawpularity\"].min()\nDENOMINATOR = train_df[\"Pawpularity\"].max() - train_df[\"Pawpularity\"].min()","metadata":{"execution":{"iopub.status.busy":"2021-11-22T14:10:20.506826Z","iopub.execute_input":"2021-11-22T14:10:20.507189Z","iopub.status.idle":"2021-11-22T14:10:20.512194Z","shell.execute_reply.started":"2021-11-22T14:10:20.507159Z","shell.execute_reply":"2021-11-22T14:10:20.511544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Checking for any missing values in feature column","metadata":{}},{"cell_type":"code","source":"train_df.info()","metadata":{"execution":{"iopub.status.busy":"2021-11-22T14:10:20.514121Z","iopub.execute_input":"2021-11-22T14:10:20.514491Z","iopub.status.idle":"2021-11-22T14:10:20.537634Z","shell.execute_reply.started":"2021-11-22T14:10:20.514459Z","shell.execute_reply":"2021-11-22T14:10:20.536531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cheking distribution of train dataframe","metadata":{}},{"cell_type":"code","source":"train_df.describe().T","metadata":{"execution":{"iopub.status.busy":"2021-11-22T14:10:20.538954Z","iopub.execute_input":"2021-11-22T14:10:20.539212Z","iopub.status.idle":"2021-11-22T14:10:20.598975Z","shell.execute_reply.started":"2021-11-22T14:10:20.539181Z","shell.execute_reply":"2021-11-22T14:10:20.597915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data set size","metadata":{}},{"cell_type":"code","source":"print ( \"train data frame size ={}\".format( train_df.shape))\nprint ( \"test data frame size ={}\".format( test_df.shape))","metadata":{"execution":{"iopub.status.busy":"2021-11-22T14:10:20.601421Z","iopub.execute_input":"2021-11-22T14:10:20.601981Z","iopub.status.idle":"2021-11-22T14:10:20.607962Z","shell.execute_reply.started":"2021-11-22T14:10:20.601931Z","shell.execute_reply":"2021-11-22T14:10:20.60704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Lets look at metat data to understand how each features are influencing popularity of each image/animal adaption","metadata":{}},{"cell_type":"code","source":"def create_line_plot( x_value, y_value, color,title ):\n    \n    fig = plt.figure( figsize = (20,6), dpi = 90 )\n    fig.suptitle( title  )\n    sns.lineplot(  x = x_value, y =y_value, hue = color )\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-22T14:10:20.609307Z","iopub.execute_input":"2021-11-22T14:10:20.610136Z","iopub.status.idle":"2021-11-22T14:10:20.618898Z","shell.execute_reply.started":"2021-11-22T14:10:20.610091Z","shell.execute_reply":"2021-11-22T14:10:20.618222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for each_feature in train_df.columns:\n    if \"Id\" == each_feature or \"Pawpularity\" == each_feature: continue\n        \n    create_line_plot ( x_value = train_df.index, y_value = train_df[\"Pawpularity\"], color = train_df[each_feature] , title =\" Trend popularity if photo has \" + each_feature )","metadata":{"execution":{"iopub.status.busy":"2021-11-22T14:10:20.620423Z","iopub.execute_input":"2021-11-22T14:10:20.620783Z","iopub.status.idle":"2021-11-22T14:10:32.00309Z","shell.execute_reply.started":"2021-11-22T14:10:20.620742Z","shell.execute_reply":"2021-11-22T14:10:32.002011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Checking Distribution of ID for each category","metadata":{}},{"cell_type":"code","source":"fig = plt.figure( figsize = (20, 10 ), dpi = 90 )\ncounter = 0\nfor each_feature in train_df.columns:\n    if \"Id\" == each_feature or \"Pawpularity\" == each_feature: continue\n    counter =counter+1\n    plt.subplot( 3, 4,counter )\n    sns.countplot( data = train_df, x = each_feature )\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-22T14:10:32.004767Z","iopub.execute_input":"2021-11-22T14:10:32.005079Z","iopub.status.idle":"2021-11-22T14:10:33.599709Z","shell.execute_reply.started":"2021-11-22T14:10:32.005042Z","shell.execute_reply":"2021-11-22T14:10:33.598834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Check popularity score distribution for different categoryfig = plt.figure( figsize = (20, 10 ), dpi = 90 )","metadata":{}},{"cell_type":"code","source":"fig = plt.figure( figsize = (20, 10 ), dpi = 90 )\ncounter = 0\nfor each_feature in train_df.columns:\n    if \"Id\" == each_feature or \"Pawpularity\" == each_feature: continue\n    counter =counter+1\n    plt.subplot( 3, 4,counter )\n    plt.title(  \"Pawpularity for different \" + each_feature)\n    sns.histplot(x = train_df[\"Pawpularity\"], hue = train_df[each_feature], kde = True)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-22T14:10:33.603102Z","iopub.execute_input":"2021-11-22T14:10:33.603628Z","iopub.status.idle":"2021-11-22T14:10:41.372748Z","shell.execute_reply.started":"2021-11-22T14:10:33.603564Z","shell.execute_reply":"2021-11-22T14:10:41.371827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Lets visualize few images ","metadata":{}},{"cell_type":"code","source":"sns.set_style(\"white\")\nfig = plt.figure( figsize = (8,8),dpi = 300 )\nfor each_index in range( 0, 9 ):\n    plt.subplot( 3, 3, each_index + 1 )\n    data = train_df.iloc[each_index ]\n    img_data = io.imread( train_jpeg_path + train_df[\"Id\"][each_index] + \".jpg\" )\n    plt.imshow(  resize( img_data, (800,800 ), anti_aliasing=True ) )\n    \n    title_text = \"Photo with \\n\" +\\\n    \",\".join([ col_name for col_name, value in zip( train_df.columns, train_df.iloc[each_index].values ) if (value != 0 ) & (col_name !=\"Id\") & (col_name !=\"Pawpularity\") ] )\n    plt.xticks( fontsize = 1 )\n    plt.yticks( fontsize = 1 )\n    plt.title( title_text ,fontsize=5)\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-22T14:10:41.374123Z","iopub.execute_input":"2021-11-22T14:10:41.374482Z","iopub.status.idle":"2021-11-22T14:10:48.258708Z","shell.execute_reply.started":"2021-11-22T14:10:41.374443Z","shell.execute_reply":"2021-11-22T14:10:48.257792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Thing to note, image size are not uniform. Have to consider resizing all image while working with model","metadata":{}},{"cell_type":"markdown","source":"# Lets start bulidng model ","metadata":{}},{"cell_type":"code","source":"## import all required building library \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nimport tensorflow as tf","metadata":{"execution":{"iopub.status.busy":"2021-11-22T14:10:48.260011Z","iopub.execute_input":"2021-11-22T14:10:48.260984Z","iopub.status.idle":"2021-11-22T14:10:48.265364Z","shell.execute_reply.started":"2021-11-22T14:10:48.260941Z","shell.execute_reply":"2021-11-22T14:10:48.264663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CFG():\n    ''' Class to store all setting parameters'''\n    SEED = 100\n    TEST_SIZE= 0.2\n    SHUFFLE_DATA = True\n    IMG_HEIGHT = 300 # controler for image height\n    IMG_WIDTH  = 300 # controler for image width\n    BUFFER_SIZE = 1000 # Buffer control size for shuffle \n    BATCH_SIZE =200 # Number of images in each batch\n    ","metadata":{"execution":{"iopub.status.busy":"2021-11-22T14:10:48.266945Z","iopub.execute_input":"2021-11-22T14:10:48.267201Z","iopub.status.idle":"2021-11-22T14:10:48.285553Z","shell.execute_reply.started":"2021-11-22T14:10:48.267171Z","shell.execute_reply":"2021-11-22T14:10:48.2848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Adding each image complete path to new column usefull while working with Tensorflow datagenerator\ntrain_df[\"img_path\"] =  train_jpeg_path + train_df[\"Id\"]  +\".jpg\"\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-22T14:10:48.28687Z","iopub.execute_input":"2021-11-22T14:10:48.287575Z","iopub.status.idle":"2021-11-22T14:10:48.313935Z","shell.execute_reply.started":"2021-11-22T14:10:48.28754Z","shell.execute_reply":"2021-11-22T14:10:48.312869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef create_test_val_train_split():\n    \n    train_split_df,test_split_df = train_test_split( train_df , test_size = CFG.TEST_SIZE , shuffle = CFG.SHUFFLE_DATA, random_state = CFG.SEED )\n    train_split_df,val_split_df = train_test_split( train_split_df , test_size = CFG.TEST_SIZE , shuffle = CFG.SHUFFLE_DATA, random_state = CFG.SEED )\n    min_max_scalar_obj =MinMaxScaler()\n    train_split_df[\"popularity_transformed\"] = min_max_scalar_obj.fit_transform( train_split_df[\"Pawpularity\"].values.reshape(-1,1) )\n    test_split_df[\"popularity_transformed\"] = min_max_scalar_obj.transform( test_split_df[\"Pawpularity\"].values.reshape(-1,1) )\n    val_split_df[\"popularity_transformed\"] = min_max_scalar_obj.transform( val_split_df[\"Pawpularity\"].values.reshape(-1,1) )\n    \n    return train_split_df, val_split_df, test_split_df, min_max_scalar_obj\n\ntrain_split_df, val_split_df, test_split_df,_ = create_test_val_train_split()    \nprint ( \"Test data frame size after split = {}\".format( test_split_df.shape ))\nprint ( \"Train data frame size after split = {}\".format( train_split_df.shape ))\nprint ( \"Validation data frame size after split = {}\".format( val_split_df.shape ))","metadata":{"execution":{"iopub.status.busy":"2021-11-22T14:10:48.315664Z","iopub.execute_input":"2021-11-22T14:10:48.315957Z","iopub.status.idle":"2021-11-22T14:10:48.338927Z","shell.execute_reply.started":"2021-11-22T14:10:48.315923Z","shell.execute_reply":"2021-11-22T14:10:48.338065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# check updated Train dataframe","metadata":{}},{"cell_type":"code","source":"train_split_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-22T14:10:48.340248Z","iopub.execute_input":"2021-11-22T14:10:48.340492Z","iopub.status.idle":"2021-11-22T14:10:48.357441Z","shell.execute_reply.started":"2021-11-22T14:10:48.340464Z","shell.execute_reply":"2021-11-22T14:10:48.356761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Check Updated Test Data Frame","metadata":{}},{"cell_type":"code","source":"test_split_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-22T14:10:48.358817Z","iopub.execute_input":"2021-11-22T14:10:48.359208Z","iopub.status.idle":"2021-11-22T14:10:48.383853Z","shell.execute_reply.started":"2021-11-22T14:10:48.359171Z","shell.execute_reply":"2021-11-22T14:10:48.382779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Check Updated Validation Data Frame","metadata":{}},{"cell_type":"code","source":"val_split_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-22T14:10:48.385357Z","iopub.execute_input":"2021-11-22T14:10:48.38582Z","iopub.status.idle":"2021-11-22T14:10:48.407249Z","shell.execute_reply.started":"2021-11-22T14:10:48.38578Z","shell.execute_reply":"2021-11-22T14:10:48.406235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Check Distribution of Train and test popularity_transformed column ","metadata":{}},{"cell_type":"code","source":"sns.set_style(\"whitegrid\")\nfig,axis = plt.subplots( nrows = 1, ncols = 3 , figsize = (15,8), dpi = 90  )\n\naxis = axis.flatten()\n\n\nsns.histplot( x=train_split_df[\"popularity_transformed\"] , kde = True , ax = axis[0] )\naxis[0].set_title (\"Train split data frame transformed popularity score \")\n\nsns.histplot( x=test_split_df[\"popularity_transformed\"] , kde = True , ax = axis[1])\naxis[1].set_title (\"Test split data frame transformed popularity score \")\n\nsns.histplot( x=val_split_df[\"popularity_transformed\"] , kde = True , ax = axis[2])\naxis[2].set_title (\"Validation split data frame transformed popularity score \")\n\nplt.show()\n\n","metadata":{"execution":{"iopub.status.busy":"2021-11-22T14:10:48.408803Z","iopub.execute_input":"2021-11-22T14:10:48.409083Z","iopub.status.idle":"2021-11-22T14:10:49.365333Z","shell.execute_reply.started":"2021-11-22T14:10:48.409052Z","shell.execute_reply":"2021-11-22T14:10:49.364337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test QQ plot","metadata":{}},{"cell_type":"code","source":"\npplot( data =train_split_df, \n      x=\"popularity_transformed\" ,\n      y = \"popularity_transformed\", \n      kind = \"qq\", height = 4, aspect = 2, display_kws={\"identity\":False, \"fit\":True})\n","metadata":{"execution":{"iopub.status.busy":"2021-11-22T14:10:49.367151Z","iopub.execute_input":"2021-11-22T14:10:49.36746Z","iopub.status.idle":"2021-11-22T14:10:49.799587Z","shell.execute_reply.started":"2021-11-22T14:10:49.367419Z","shell.execute_reply":"2021-11-22T14:10:49.798776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test QQ plot","metadata":{}},{"cell_type":"code","source":"pplot( data =test_split_df, \n      x=\"popularity_transformed\" ,\n      y = \"popularity_transformed\", \n      kind = \"qq\", height = 4, aspect = 2, display_kws={\"identity\":False, \"fit\":True})\n\n","metadata":{"execution":{"iopub.status.busy":"2021-11-22T14:10:49.801052Z","iopub.execute_input":"2021-11-22T14:10:49.801287Z","iopub.status.idle":"2021-11-22T14:10:50.216776Z","shell.execute_reply.started":"2021-11-22T14:10:49.801259Z","shell.execute_reply":"2021-11-22T14:10:50.215836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Validation QQ Plot","metadata":{}},{"cell_type":"code","source":"pplot( data =val_split_df, \n      x=\"popularity_transformed\" ,\n      y = \"popularity_transformed\", \n      kind = \"qq\", height = 4, aspect = 2, display_kws={\"identity\":False, \"fit\":True})\n\n","metadata":{"execution":{"iopub.status.busy":"2021-11-22T14:10:50.220179Z","iopub.execute_input":"2021-11-22T14:10:50.220422Z","iopub.status.idle":"2021-11-22T14:10:50.64343Z","shell.execute_reply.started":"2021-11-22T14:10:50.220395Z","shell.execute_reply":"2021-11-22T14:10:50.642488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating TF record","metadata":{}},{"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Device:', tpu.master())\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept:\n    strategy = tf.distribute.get_strategy()\nprint('Number of replicas:', strategy.num_replicas_in_sync)","metadata":{"execution":{"iopub.status.busy":"2021-11-22T14:10:50.645217Z","iopub.execute_input":"2021-11-22T14:10:50.645555Z","iopub.status.idle":"2021-11-22T14:10:56.839937Z","shell.execute_reply.started":"2021-11-22T14:10:50.645511Z","shell.execute_reply":"2021-11-22T14:10:56.839217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef create_test_val_train_split():\n    \n    train_split_df,test_split_df = train_test_split( train_df , test_size = CFG.TEST_SIZE , shuffle = CFG.SHUFFLE_DATA, random_state = CFG.SEED )\n    train_split_df,val_split_df = train_test_split( train_split_df , test_size = CFG.TEST_SIZE , shuffle = CFG.SHUFFLE_DATA, random_state = CFG.SEED )\n    min_max_scalar_obj =MinMaxScaler()\n    train_split_df[\"popularity_transformed\"] = min_max_scalar_obj.fit_transform( train_split_df[\"Pawpularity\"].values.reshape(-1,1) )\n    test_split_df[\"popularity_transformed\"] = min_max_scalar_obj.transform( test_split_df[\"Pawpularity\"].values.reshape(-1,1) )\n    val_split_df[\"popularity_transformed\"] = min_max_scalar_obj.transform( val_split_df[\"Pawpularity\"].values.reshape(-1,1) )\n    \n    return train_split_df, val_split_df, test_split_df, min_max_scalar_obj\n\ntrain_split_df, val_split_df, test_split_df,_ = create_test_val_train_split()    \nprint ( \"Test data frame size after split = {}\".format( test_split_df.shape ))\nprint ( \"Train data frame size after split = {}\".format( train_split_df.shape ))\nprint ( \"Validation data frame size after split = {}\".format( val_split_df.shape ))","metadata":{"execution":{"iopub.status.busy":"2021-11-22T14:10:56.841078Z","iopub.execute_input":"2021-11-22T14:10:56.841323Z","iopub.status.idle":"2021-11-22T14:10:56.865864Z","shell.execute_reply.started":"2021-11-22T14:10:56.841296Z","shell.execute_reply":"2021-11-22T14:10:56.863719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CREATE_TF_RECORD = False\n\ndef _bytes_feature(value):\n    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n    if isinstance(value, type(tf.constant(0))):\n        value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\ndef _float_feature(value):\n    \"\"\"Returns a float_list from a float / double.\"\"\"\n    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n\ndef _uint8_feature(value):\n    \"\"\"Returns a float_list from a float / double.\"\"\"\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\n\ndef _int64_feature(value):\n    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n\n\ndef train_serialize_example(image, img_id, score ):\n    feature = {\n      'image'         : _uint8_feature(image),\n      'image_id'      : _bytes_feature(img_id),   \n      'score'        : _float_feature(score),\n      }\n    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n    return example_proto.SerializeToString()\n\n\n\n\nif CREATE_TF_RECORD :\n    \n    dest_file=\"./\"\n    each_tfrec_size = 1\n    \n    temp_train, temp_test, temp_val, min_max_obj = create_test_val_train_split()\n    \n    \n    for i in range(0, 3 ) :\n        if i ==0 :\n            img_id = temp_train[\"Id\"].values\n            file_name = temp_train[\"img_path\"].values\n            target = temp_train[\"popularity_transformed\"].values\n            tf_record_name = \"Pet_Finder_Score_300x300_tf_record_train_img_count-\" +str(len(file_name)) +\".tfrec\"\n        if i == 1:\n            img_id = temp_test[\"Id\"].values\n            file_name = temp_test[\"img_path\"].values\n            target = temp_test[\"popularity_transformed\"].values\n            tf_record_name = \"Pet_Finder_Score_300x300_tf_record_test_img_count-\" +str(len(file_name)) +\".tfrec\"\n            \n        \n        if i == 2:\n            img_id = temp_val[\"Id\"].values\n            file_name = temp_val[\"img_path\"].values\n            target = temp_val[\"popularity_transformed\"].values\n            tf_record_name = \"Pet_Finder_Score_300x300_tf_record_val_img_count-\" +str(len(file_name)) +\".tfrec\"\n        \n        \n        with tf.io.TFRecordWriter( dest_file +tf_record_name ) as writer:\n                    counter = 0\n                    for each_id, each_file, each_target  in zip( img_id,file_name, target ):\n                        ##read_img = tf.io.read_file(  each_file )\n                        ##read_img = tf.io.decode_jpeg( read_img, channels =3  )\n                        ##read_img = tf.image.resize( images = read_img, size = [ 300, 300 ] )\n                        ##read_img = read_img.numpy()\n                        ##read_img = read_img.astype(np.float32 )\n                        read_img = io.imread(each_file )\n                        read_img = np.resize( read_img, [ 300, 300,3 ] )\n                        #read_img = tf.cast( read_img, dtype = tf.float32 ).numpy()\n                        #writer.write(train_serialize_example( read_img.tobytes() , str.encode(each_id ), each_target ))\n                        writer.write(train_serialize_example( read_img.tobytes() , str.encode(each_id ), each_target ))\n                        counter += 1\n                        #if counter> 2: break \n                            \n                    writer.close()\n            \n        print ( \"completed \" + tf_record_name )\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2021-11-22T14:10:56.867456Z","iopub.execute_input":"2021-11-22T14:10:56.867705Z","iopub.status.idle":"2021-11-22T14:10:56.887383Z","shell.execute_reply.started":"2021-11-22T14:10:56.867677Z","shell.execute_reply":"2021-11-22T14:10:56.886579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Reading TF record File to train model ","metadata":{}},{"cell_type":"code","source":"\n\ndef prepare_target(target):    \n    target = tf.cast(target, tf.float32)   \n    #target = tf.divide( tf.subtract( target , MIN_DATA ), DENOMINATOR) \n    \n    #target = tf.reshape(target, [1])         \n    return target\n\ndef read_labeled_tfrecord(example):\n    LABELED_TFREC_FORMAT = {\n        \"image\" : tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n        \"image_id\":tf.io.FixedLenFeature([], tf.string),\n        \"score\": tf.io.FixedLenFeature([], tf.float32),  # shape [] means single element\n    }\n    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n    \n    image =  tf.io.decode_raw( example['image'] ,tf.uint8 )\n    #image = example['image'] \n    #image = tf.reshape(image, ( CFG.IMG_HEIGHT, CFG.IMG_WIDTH ,3) )\n    image = tf.divide(image,  255 )\n    \n    target = prepare_target(example['score'])\n    return tf.reshape(image, ( CFG.IMG_HEIGHT, CFG.IMG_WIDTH ,3) ), target # returns a dataset of (image, label) pairs\n\n\n\ndef load_dataset(fileids, labeled=True, ordered=False ):\n    # Read from TFRecords. For optimal performance, reading from multiple files at once and\n    # disregarding data order. Order does not matter since we will be shuffling the data anyway.\n\n    ignore_order = tf.data.Options()\n    if not ordered:\n        ignore_order.experimental_deterministic = False # disable order, increase speed\n\n    dataset = tf.data.TFRecordDataset(fileids, num_parallel_reads=tf.data.AUTOTUNE) # automatically interleaves reads from multiple files\n    dataset = dataset.with_options(ignore_order) # uses data as soon as it streams in, rather than in its original order\n    dataset = dataset.map(read_labeled_tfrecord,num_parallel_calls= tf.data.AUTOTUNE)\n    return dataset\n\n\n## Main function \ndef get_training_dataset_tf_rec(file_ist,repeat = True, order = False , drop_remainder= True   ):\n    dataset = load_dataset(file_ist, labeled=True, ordered = False )\n    dataset = dataset.repeat()  if repeat else  dataset # the training dataset must repeat for several epochs\n    dataset = dataset.shuffle(20, seed=CFG.SEED)\n    dataset = dataset.batch(CFG.BATCH_SIZE, drop_remainder=drop_remainder)\n    dataset = dataset.prefetch(tf.data.AUTOTUNE) # prefetch next batch while training (autotune prefetch buffer size)\n    dataset = dataset.cache()\n\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2021-11-22T14:10:56.889435Z","iopub.execute_input":"2021-11-22T14:10:56.889867Z","iopub.status.idle":"2021-11-22T14:10:56.905435Z","shell.execute_reply.started":"2021-11-22T14:10:56.889818Z","shell.execute_reply":"2021-11-22T14:10:56.90466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train_data_gen = get_training_dataset_tf_rec ( train_files ,repeat = True, order = False , drop_remainder= False   )","metadata":{"execution":{"iopub.status.busy":"2021-11-22T14:10:56.906675Z","iopub.execute_input":"2021-11-22T14:10:56.907048Z","iopub.status.idle":"2021-11-22T14:10:56.92198Z","shell.execute_reply.started":"2021-11-22T14:10:56.90702Z","shell.execute_reply":"2021-11-22T14:10:56.921104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_jpeg( image_path ):\n    read_img = tf.io.read_file(  image_path )\n    read_img = tf.io.decode_jpeg( read_img, channels =3  )\n    read_img = tf.image.resize( images = read_img, size = [ CFG.IMG_HEIGHT, CFG.IMG_WIDTH ] )\n    read_img = tf.divide(read_img,255 )\n    return read_img\n    \n    \n    \ndef transform_jepg( ):\n    \n    def read_transform_jpeg(image_path, score ):\n        \n        return read_jpeg( image_path ), score\n    \n    return read_transform_jpeg\n\ndef data_generator( image_files, score ):\n    read_transform_jpeg = transform_jepg()\n    AUTO_TUNE = tf.data.AUTOTUNE\n    \n    data_gen = tf.data.Dataset.from_tensor_slices( ( image_files, score) )\n    \n    data_gen = data_gen.map( map_func = read_transform_jpeg , num_parallel_calls= AUTO_TUNE )\n    #data_gen = data_gen.cache()\n    data_gen = data_gen.prefetch(buffer_size = CFG.BUFFER_SIZE )\n    data_gen = data_gen.shuffle( buffer_size = CFG.BUFFER_SIZE, seed = CFG.SEED, reshuffle_each_iteration = True )\n    data_gen = data_gen.batch( batch_size = CFG.BATCH_SIZE,drop_remainder=False)#, num_parallel_calls = AUTO_TUNE )\n    data_gen = data_gen.prefetch(buffer_size = CFG.BUFFER_SIZE )\n    data_gen = data_gen.repeat( True )\n    return data_gen\n\n#train_data_gen = data_generator( image_files = train_split_df[\"img_path\"].values[:10], score = train_split_df[\"popularity_transformed\"].values[:10] )","metadata":{"execution":{"iopub.status.busy":"2021-11-22T14:10:56.923671Z","iopub.execute_input":"2021-11-22T14:10:56.923927Z","iopub.status.idle":"2021-11-22T14:10:56.934602Z","shell.execute_reply.started":"2021-11-22T14:10:56.923899Z","shell.execute_reply":"2021-11-22T14:10:56.933864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def mobile_net_model( pre_trained_model_trainable = False ):\n\n    input_layer = tf.keras.layers.Input( shape = ( CFG.IMG_HEIGHT, CFG.IMG_WIDTH, 3 ) , name =\"input_layer\" )\n    mobile_net_model = tf.keras.applications.MobileNetV2( input_shape = [ CFG.IMG_HEIGHT, CFG.IMG_WIDTH, 3 ], \n                                                         input_tensor = input_layer ,\n                                                         include_top= False, \n                                                         weights = \"imagenet\",\n                                                         pooling = True )\n    mobile_net_model.trainable =pre_trained_model_trainable\n    \n    gaussian_noise = tf.keras.layers.GaussianNoise( stddev = 0.3 ) ( input_layer )\n    random_crop = tf.keras.layers.experimental.preprocessing.RandomCrop( height = 30, width = 30  ) (gaussian_noise)\n    random_flip =tf.keras.layers.experimental.preprocessing.RandomFlip( mode=\"horizontal_and_vertical\") ( random_crop )\n    zoom_layer = tf.keras.layers.experimental.preprocessing.RandomZoom(  height_factor =(-0.3, -0.2)  , width_factor=(-0.3, -0.2), fill_mode='reflect', interpolation='bilinear', fill_value=0.0 ) ( random_flip)\n    random_contrast = tf.keras.layers.experimental.preprocessing.RandomContrast( factor =[0.2, 0.8 ]  ) ( zoom_layer )\n    \n    mobile_net_model.layers[0]( random_contrast )\n\n    layer_00 = mobile_net_model.layers[-1].output\n    layer_01 = tf.keras.layers.GlobalAvgPool2D () ( layer_00 )\n    layer_02 = tf.keras.layers.Flatten(name =\"flatten_layer\")(layer_01)\n    layer_03 = tf.keras.layers.Dense(units = 100, activation= tf.keras.layers.LeakyReLU(alpha = 1 ),use_bias=True, kernel_initializer='glorot_uniform' )(layer_02 )\n    layer_04 = tf.keras.layers.Dense(units = 1, activation= tf.keras.layers.LeakyReLU(alpha = 1 ),use_bias=True, kernel_initializer='glorot_uniform' )(layer_03 )\n\n    model = tf.keras.Model( inputs = input_layer, outputs = layer_04 )\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-11-22T14:10:56.935729Z","iopub.execute_input":"2021-11-22T14:10:56.936413Z","iopub.status.idle":"2021-11-22T14:10:56.953146Z","shell.execute_reply.started":"2021-11-22T14:10:56.936366Z","shell.execute_reply":"2021-11-22T14:10:56.952312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nif strategy.num_replicas_in_sync !=1:\n    gcs_path= KaggleDatasets().get_gcs_path(\"pet-finder-tf-record-300x300-rev-01\")\n    \nelse:\n    \n    gcs_path = KaggleDatasets().get_gcs_path(\"petfinder-pawpularity-score\")\n\n    \nval_file_name = [gcs_path + \"/Pet_Finder_Score_300x300_tf_record_val_img_count-1983.tfrec\"]\ntrain_file_name = [ gcs_path + \"/Pet_Finder_Score_300x300_tf_record_train_img_count-6343.tfrec\"  ]\ntest_file_name= [ gcs_path + \"/Pet_Finder_Score_300x300_tf_record_test_img_count-1586.tfrec\"]\n\n\n\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Device:', tpu.master())\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept:\n    strategy = tf.distribute.get_strategy()\nprint('Number of replicas:', strategy.num_replicas_in_sync)\n\nclass CFG():\n    ''' Class to store all setting parameters'''\n    SEED = 100\n    TEST_SIZE= 0.3\n    SHUFFLE_DATA = True\n    IMG_HEIGHT = 300 # controler for image height\n    IMG_WIDTH  = 300 # controler for image width\n    BUFFER_SIZE = 1024 # Buffer control size for shuffle \n    BATCH_SIZE =  100 # Number of images in each batch\n\nif strategy.num_replicas_in_sync != 1:  CFG.BATCH_SIZE =  strategy.num_replicas_in_sync * 70\nCFG.BATCH_SIZE","metadata":{"execution":{"iopub.status.busy":"2021-11-22T14:10:56.95452Z","iopub.execute_input":"2021-11-22T14:10:56.955058Z","iopub.status.idle":"2021-11-22T14:11:05.84689Z","shell.execute_reply.started":"2021-11-22T14:10:56.955021Z","shell.execute_reply":"2021-11-22T14:11:05.845876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    model_1 = mobile_net_model( pre_trained_model_trainable= True )\n    optimizer1 = tf.keras.optimizers.Adam( learning_rate = 0.001 )\n    model_1.compile( optimizer = optimizer1, loss = tf.keras.losses.MeanSquaredError() , metrics = [ tf.keras.metrics.MeanSquaredError() ])\n\ntrain_split_df, val_split_df, test_split_df,min_max_scalar_obj = create_test_val_train_split()\n\nif False:\n    '''Data generator using JPEG files '''\n    #train_split_df [\"img_path\"] = gcs_path  +\"/train/\" + train_split_df[\"Id\"] +\".jpg\" #../input/petfinder-pawpularity-score/train\n    #val_split_df[\"img_path\"] = gcs_path  +\"/train/\" +  val_split_df[\"Id\"] +\".jpg\"\n    train_data_gen = data_generator( image_files = train_split_df[\"img_path\"].values, \n                                                    score = train_split_df[\"popularity_transformed\"].values )\n\n    val_data_gen = data_generator( image_files = val_split_df[\"img_path\"].values, \n                                                    score = val_split_df[\"popularity_transformed\"].values )\n    val_steps  = int ( val_split_df.shape[0] / CFG.BATCH_SIZE ) + ( 1 if (val_split_df.shape[0] % CFG.BATCH_SIZE ) != 0 else 0 )\n    train_steps  = int( train_split_df.shape[0] / CFG.BATCH_SIZE ) + ( 1 if (train_split_df.shape[0] % CFG.BATCH_SIZE) != 0 else 0 )\n\nelse:\n    \n    '''Data generator using TFRECORD files '''\n    train_data_gen = get_training_dataset_tf_rec ( train_file_name ,repeat = True, order = False , drop_remainder= False   )\n    val_data_gen   = get_training_dataset_tf_rec ( val_file_name ,repeat = True, order = False , drop_remainder= False   )\n\n    val_steps  = int ( 1983 / CFG.BATCH_SIZE ) + ( 1 if (1983 % CFG.BATCH_SIZE ) != 0 else 0 )\n    train_steps  = int( 6343 / CFG.BATCH_SIZE ) + ( 1 if (6343 % CFG.BATCH_SIZE) != 0 else 0 )\n\n\n\nlr_reducer = tf.keras.callbacks.ReduceLROnPlateau(  patience=38,\n                                                    min_lr= 0.000001,\n                                                    monitor='val_loss', \n                                                    factor=0.45, \n                                                    verbose=1,\n                                                    min_delta = 0.02,\n                                                    cooldown=3,\n                                                    mode='auto', \n                                                   )\n\ncheckpoint = tf.keras.callbacks.ModelCheckpoint( f'./model{1}.h5', save_best_only=True, monitor='val_loss', mode='min')\n\nmodel_history = model_1.fit( train_data_gen,\n                        \n                         steps_per_epoch= train_steps, \n                         epochs = 100, \n                         validation_data= val_data_gen,\n                         validation_steps = val_steps,\n                         callbacks=[ checkpoint,lr_reducer ]\n                       )\n","metadata":{"execution":{"iopub.status.busy":"2021-11-22T14:11:05.848262Z","iopub.execute_input":"2021-11-22T14:11:05.848501Z","iopub.status.idle":"2021-11-22T14:14:34.693689Z","shell.execute_reply.started":"2021-11-22T14:11:05.848476Z","shell.execute_reply":"2021-11-22T14:14:34.692262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#val_steps,train_steps","metadata":{"execution":{"iopub.status.busy":"2021-11-22T14:14:34.694982Z","iopub.status.idle":"2021-11-22T14:14:34.696041Z","shell.execute_reply.started":"2021-11-22T14:14:34.695741Z","shell.execute_reply":"2021-11-22T14:14:34.69577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if False:\n    model_1 = mobile_net_model( pre_trained_model_trainable= True )\n    model_1.load_weights(\"./model1.h5\")\n    #test_data_gen = data_generator( image_files = test_split_df[\"img_path\"].values,  score = test_split_df[\"popularity_transformed\"].values , test_data = True )\n    test_data_gen   = get_training_dataset_tf_rec ( [\"./test_300x300_tf_record.tfrec\"],repeat = True, order = True , drop_remainder= False   )\n    test_split_df[\"model_predict\"] = model_1.predict( test_data_gen )","metadata":{"execution":{"iopub.status.busy":"2021-11-22T14:14:34.697571Z","iopub.status.idle":"2021-11-22T14:14:34.698313Z","shell.execute_reply.started":"2021-11-22T14:14:34.69806Z","shell.execute_reply":"2021-11-22T14:14:34.698091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#sns.scatterplot( data = test_split_df, x = \"model_predict\", y =\"popularity_transformed\")","metadata":{"execution":{"iopub.status.busy":"2021-11-22T14:14:34.699311Z","iopub.status.idle":"2021-11-22T14:14:34.699681Z","shell.execute_reply.started":"2021-11-22T14:14:34.699493Z","shell.execute_reply":"2021-11-22T14:14:34.699509Z"},"trusted":true},"execution_count":null,"outputs":[]}]}