{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## This is just Idea to generate text data with pretrained Image Captioning Model\n\n- To improve your model with this method, you maybe have to prepare more vocab and more training\n\n- If you think this is interesting, please upvote this notebook ðŸ˜‰","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-30T15:42:10.751308Z","iopub.execute_input":"2021-09-30T15:42:10.752241Z","iopub.status.idle":"2021-09-30T15:42:10.780523Z","shell.execute_reply.started":"2021-09-30T15:42:10.75212Z","shell.execute_reply":"2021-09-30T15:42:10.779622Z"}}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/yunjey/pytorch-tutorial.git","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd pytorch-tutorial/tutorials/03-advanced/image_captioning/\n!pip install -r requirements.txt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!wget https://www.dropbox.com/s/ne0ixz5d58ccbbz/pretrained_model.zip .","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!unzip pretrained_model.zip","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!wget https://www.dropbox.com/s/26adb7y9m98uisa/vocap.zip .","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir models data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cp encoder-5-3000.pkl models/encoder-2-1000.ckpt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cp decoder-5-3000.pkl models/decoder-2-1000.ckpt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!unzip vocap.zip -d data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls ../../../../../input/petfinder-pawpularity-score/train/0007de18844b0dbbb5e1f607da0606e0.jpg","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pycocotools > /dev/null","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np \nimport argparse\nimport pickle \nimport os\nimport torch\nfrom torchvision import transforms \nfrom build_vocab import Vocabulary\nfrom model import EncoderCNN, DecoderRNN\nfrom PIL import Image\n \n \n# Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n \ndef load_image(image_path, transform=None):\n    image = Image.open(image_path)\n    image = image.resize([224, 224], Image.LANCZOS)\n    \n    if transform is not None:\n        image = transform(image).unsqueeze(0)\n    \n    return image\n \ndef show_text(image_path):\n    # Image preprocessing\n    transform = transforms.Compose([\n        transforms.ToTensor(), \n        transforms.Normalize((0.485, 0.456, 0.406), \n                             (0.229, 0.224, 0.225))])\n    \n    # Load vocabulary wrapper\n    with open(vocab_path, 'rb') as f:\n        vocab = pickle.load(f)\n \n    # Build models\n    encoder = EncoderCNN(embed_size).eval()  # eval mode (batchnorm uses moving mean/variance)\n    decoder = DecoderRNN(embed_size, hidden_size, len(vocab), num_layers)\n    encoder = encoder.to(device)\n    decoder = decoder.to(device)\n \n    # Load the trained model parameters\n    encoder.load_state_dict(torch.load(encoder_path))\n    decoder.load_state_dict(torch.load(decoder_path))\n \n    # Prepare an image\n    image = load_image(image_path, transform)\n    image_tensor = image.to(device)\n    \n    # Generate an caption from the image\n    feature = encoder(image_tensor)\n    sampled_ids = decoder.sample(feature)\n    sampled_ids = sampled_ids[0].cpu().numpy()          # (1, max_seq_length) -> (max_seq_length)\n    \n    # Convert word_ids to words\n    sampled_caption = []\n    for word_id in sampled_ids:\n        word = vocab.idx2word[word_id]\n        sampled_caption.append(word)\n        if word == '<end>':\n            break\n    sentence = ' '.join(sampled_caption)\n    \n    # Print out the image and the generated caption\n    image = Image.open(image_path)\n    plt.imshow(np.asarray(image))\n    plt.show()\n    print (sentence)\n\n\nencoder_path ='models/encoder-2-1000.ckpt'\ndecoder_path ='models/decoder-2-1000.ckpt'\nvocab_path ='data/vocab.pkl'\n    \n# Model parameters (should be same as paramters in train.py)\nembed_size=256\nhidden_size=512\nnum_layers=1\n    \nimport glob\nfiles = sorted(glob.glob('../../../../../input/petfinder-pawpularity-score/train/*.jpg'))\nfor i, image_path in enumerate (files):\n    sentence = show_text(image_path)\n    if i == 5:\n        break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Wow it's noisy ... haha.\n\n#### We need more vocab.  )^o^(","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('../../../../../input/petfinder-pawpularity-score/train.csv')\ntrain_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_text(image_path):\n    # Image preprocessing\n    transform = transforms.Compose([\n        transforms.ToTensor(), \n        transforms.Normalize((0.485, 0.456, 0.406), \n                             (0.229, 0.224, 0.225))])\n    \n    # Load vocabulary wrapper\n    with open(vocab_path, 'rb') as f:\n        vocab = pickle.load(f)\n \n    # Build models\n    encoder = EncoderCNN(embed_size).eval()  # eval mode (batchnorm uses moving mean/variance)\n    decoder = DecoderRNN(embed_size, hidden_size, len(vocab), num_layers)\n    encoder = encoder.to(device)\n    decoder = decoder.to(device)\n \n    # Load the trained model parameters\n    encoder.load_state_dict(torch.load(encoder_path))\n    decoder.load_state_dict(torch.load(decoder_path))\n \n    # Prepare an image\n    image = load_image(image_path, transform)\n    image_tensor = image.to(device)\n    \n    # Generate an caption from the image\n    feature = encoder(image_tensor)\n    sampled_ids = decoder.sample(feature)\n    sampled_ids = sampled_ids[0].cpu().numpy()          # (1, max_seq_length) -> (max_seq_length)\n    \n    # Convert word_ids to words\n    sampled_caption = []\n    for word_id in sampled_ids:\n        word = vocab.idx2word[word_id]\n        sampled_caption.append(word)\n        if word == '<end>':\n            break\n    sentence = ' '.join(sampled_caption)\n    return sentence","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file_ids = []\nsentences = []\nfiles = sorted(glob.glob('../../../../../input/petfinder-pawpularity-score/train/*.jpg'))\nfor i, image_path in enumerate (files):\n    file_id = image_path.split('/')[-1].split('.')[0]\n    sentence = generate_text(image_path)[8:-6]\n    \n    file_ids.append(file_id)\n    sentences.append(sentence)\n        \nres = pd.DataFrame()\nres['Id'] = file_ids\nres['Description'] = sentences\n\nprint(res.shape)\nres.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"res.to_csv('../../../../../working/PetFinder_ImageCaptionedDescriptionDataset.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# EOF","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}