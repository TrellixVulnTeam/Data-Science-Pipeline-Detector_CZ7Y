{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Read me\n\nHello, This is for begginers who want to know how you can handle this dataset which consists of some csv file and image datasaet.\n\nTraditionally, a large dataset needs to be transoformed with TF but, if you don't know how it works, this notebook is of use to you.\n\nThis notebook is just for handling data such as loading the dataset, stacking dataset, and analyzing with deep neural network (especially, CNN).\n\nIt will be helpful for preparing analysis to someone who wants to submit.\n\nIf you have any questions, please leave the comments.\n\n\n## **Knowledge can be improved by being shared.**\n\nPlease upvote!!\n\n\n## [You can learn more skills for handling dataset or neural network.]\n\n### [Advanced parallel combination DNN with CNN] - Pawpularity Contest\n - https://www.kaggle.com/pythonash/improved-dnn-cnn-models-for-beginners\n \n### [Image data handling without memory exploded] - Pawpularity Contest\n - https://www.kaggle.com/pythonash/how-to-handle-dataset-for-beginners\n\n### [Data handling & Deep learning] - Titanic competition (best score!!)\n - https://www.kaggle.com/pythonash/how-to-handle-raw-dataset-and-analyze-with-dl\n \n### [Deep learning model with SeLU activation function] - Titanic competition\n- https://www.kaggle.com/pythonash/selu-activation-function-in-dl\n\n### [Preparing a completed dataset with proper imputation method] - Titanic competition\n - https://www.kaggle.com/pythonash/making-completed-dataset\n\n**Let's start!**","metadata":{}},{"cell_type":"markdown","source":"# Import some libraries for handling dataset.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport tensorflow as tf\nimport cv2\nimport os\nimport matplotlib.pyplot as plt\nimport numpy as np\n# seaborn is of use for visualizing.\nimport seaborn as sns\n\n# load train, test, and submission sample dataset.\ntrain_csv = pd.read_csv('../input/petfinder-pawpularity-score/train.csv')\ntest_csv = pd.read_csv('../input/petfinder-pawpularity-score/test.csv')\nsubmission = pd.read_csv('../input/petfinder-pawpularity-score/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-12-31T09:32:59.486445Z","iopub.execute_input":"2021-12-31T09:32:59.486905Z","iopub.status.idle":"2021-12-31T09:33:04.664469Z","shell.execute_reply.started":"2021-12-31T09:32:59.486835Z","shell.execute_reply":"2021-12-31T09:33:04.663672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Glancing the train csv dataset.\n\nAll columns contain binary values except for 'Id' and 'Pawpularity'","metadata":{}},{"cell_type":"code","source":"train_csv","metadata":{"execution":{"iopub.status.busy":"2021-12-31T09:33:04.665999Z","iopub.execute_input":"2021-12-31T09:33:04.666238Z","iopub.status.idle":"2021-12-31T09:33:04.691984Z","shell.execute_reply.started":"2021-12-31T09:33:04.666205Z","shell.execute_reply":"2021-12-31T09:33:04.691248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Wow, there is no null-value.\n\nSo, we don't have to mind about imputation.","metadata":{}},{"cell_type":"code","source":"train_csv.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-12-31T09:33:04.693192Z","iopub.execute_input":"2021-12-31T09:33:04.693466Z","iopub.status.idle":"2021-12-31T09:33:04.704726Z","shell.execute_reply.started":"2021-12-31T09:33:04.693431Z","shell.execute_reply":"2021-12-31T09:33:04.703914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Furthermore, there is no duplicated data.","metadata":{}},{"cell_type":"code","source":"train_csv.drop_duplicates()","metadata":{"execution":{"iopub.status.busy":"2021-12-31T09:33:04.708262Z","iopub.execute_input":"2021-12-31T09:33:04.709541Z","iopub.status.idle":"2021-12-31T09:33:04.73896Z","shell.execute_reply.started":"2021-12-31T09:33:04.7095Z","shell.execute_reply":"2021-12-31T09:33:04.73813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Count the binary values.\n\nAll data is unbalanced, but you don't have to worry about that.\n\nI will expalin the reason why it is okay in next notebook (coming soon).","metadata":{}},{"cell_type":"code","source":"for i in train_csv.drop(['Id','Pawpularity'],axis=1):\n    sns.countplot(train_csv[i])\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-31T09:33:04.740282Z","iopub.execute_input":"2021-12-31T09:33:04.740654Z","iopub.status.idle":"2021-12-31T09:33:06.461678Z","shell.execute_reply.started":"2021-12-31T09:33:04.740559Z","shell.execute_reply":"2021-12-31T09:33:06.460882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The target variable of train dataset is distributed as below.\n\nIn this part, you have to determine whether you truncate some outliers or not.\n\nHowever, I recommend that you don't truncate because I think those are a part of dataset, too.\n\nWe don't know what the truncated data affects in using deep learning.\n\nIt's just on my experience so, you can select and it's all up to you!","metadata":{}},{"cell_type":"code","source":"sns.distplot(train_csv['Pawpularity'])","metadata":{"execution":{"iopub.status.busy":"2021-12-31T09:33:06.463164Z","iopub.execute_input":"2021-12-31T09:33:06.463418Z","iopub.status.idle":"2021-12-31T09:33:06.794381Z","shell.execute_reply.started":"2021-12-31T09:33:06.463368Z","shell.execute_reply":"2021-12-31T09:33:06.793723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Glancing the test dataset.","metadata":{}},{"cell_type":"code","source":"test_csv","metadata":{"execution":{"iopub.status.busy":"2021-12-31T09:33:06.797013Z","iopub.execute_input":"2021-12-31T09:33:06.797198Z","iopub.status.idle":"2021-12-31T09:33:06.812864Z","shell.execute_reply.started":"2021-12-31T09:33:06.797175Z","shell.execute_reply":"2021-12-31T09:33:06.812149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It has not null-values, either.","metadata":{}},{"cell_type":"code","source":"test_csv.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-12-31T09:33:06.814437Z","iopub.execute_input":"2021-12-31T09:33:06.814907Z","iopub.status.idle":"2021-12-31T09:33:06.824268Z","shell.execute_reply.started":"2021-12-31T09:33:06.814869Z","shell.execute_reply":"2021-12-31T09:33:06.823364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Finally, Let's identify the submission form.","metadata":{}},{"cell_type":"code","source":"submission","metadata":{"execution":{"iopub.status.busy":"2021-12-31T09:33:06.825913Z","iopub.execute_input":"2021-12-31T09:33:06.82625Z","iopub.status.idle":"2021-12-31T09:33:06.837425Z","shell.execute_reply.started":"2021-12-31T09:33:06.826217Z","shell.execute_reply":"2021-12-31T09:33:06.836456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Handling image dataset.","metadata":{}},{"cell_type":"code","source":"# Set the path for loading image dataset.\nos.chdir('../input/petfinder-pawpularity-score/train')\n\n# We can find the size of each image data from this procedure\nsize_data = pd.DataFrame()\nfor file in os.listdir():\n    imgg = cv2.imread(file)\n    w,h,c = imgg.shape\n    size_data=size_data.append([[w,h,c,imgg.size/3]])\nsize_data","metadata":{"execution":{"iopub.status.busy":"2021-12-31T09:33:06.842208Z","iopub.execute_input":"2021-12-31T09:33:06.84244Z","iopub.status.idle":"2021-12-31T09:35:38.564934Z","shell.execute_reply.started":"2021-12-31T09:33:06.842389Z","shell.execute_reply":"2021-12-31T09:35:38.564204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## What is the minimum size?","metadata":{}},{"cell_type":"code","source":"size_data[size_data[3] == size_data[3].min()]","metadata":{"execution":{"iopub.status.busy":"2021-12-31T09:35:38.566078Z","iopub.execute_input":"2021-12-31T09:35:38.566334Z","iopub.status.idle":"2021-12-31T09:35:38.575539Z","shell.execute_reply.started":"2021-12-31T09:35:38.566298Z","shell.execute_reply":"2021-12-31T09:35:38.574874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## What is the number of most things?","metadata":{}},{"cell_type":"code","source":"size_data[3].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-12-31T09:35:38.57694Z","iopub.execute_input":"2021-12-31T09:35:38.577429Z","iopub.status.idle":"2021-12-31T09:35:38.591915Z","shell.execute_reply.started":"2021-12-31T09:35:38.577376Z","shell.execute_reply":"2021-12-31T09:35:38.591131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Checking the pixel structure of that.\n\n- It is derived from 960 * 720.","metadata":{}},{"cell_type":"code","source":"size_data[size_data[3] == 691200]","metadata":{"execution":{"iopub.status.busy":"2021-12-31T09:35:38.593251Z","iopub.execute_input":"2021-12-31T09:35:38.593835Z","iopub.status.idle":"2021-12-31T09:35:38.60813Z","shell.execute_reply.started":"2021-12-31T09:35:38.593799Z","shell.execute_reply":"2021-12-31T09:35:38.60727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load train image dataset and rescaling.\n\nThis part is crucial for analysis and is devided into 3 parts.\n\n1. Loading the image dataset.\n\n2. Changing the shape of each image into 64 * 64.\n\n- Why 64 * 64 ?? the reason is that a large image dataset need many memories so, if you change your image pixel as small thing, memory allocation will be more free.\n\n3. Rescaling the pixels by deviding with 255.\n\nIn this procedure, the parameter, cv2.INTER_AREA, is useful for interpolation.\n\nThere are several interpolation methods, but I recommend this.","metadata":{}},{"cell_type":"code","source":"train_img = []\nfor i in os.listdir():\n    file = cv2.imread(i)\n    file=cv2.resize(file,(64,64), interpolation=cv2.INTER_AREA)\n    train_img.append(file/255)\ntrain_img[:5]","metadata":{"execution":{"iopub.status.busy":"2021-12-31T09:35:38.609038Z","iopub.execute_input":"2021-12-31T09:35:38.609221Z","iopub.status.idle":"2021-12-31T09:37:29.334905Z","shell.execute_reply.started":"2021-12-31T09:35:38.609199Z","shell.execute_reply":"2021-12-31T09:37:29.334219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## For corresponding the image with each id.\n\nEach image data has to correspond to its own id.","metadata":{}},{"cell_type":"code","source":"train_img_name = []\nfor i in os.listdir():\n    train_img_name.append(i)\ntrain_img_name[:5]","metadata":{"execution":{"iopub.status.busy":"2021-12-31T09:37:29.336086Z","iopub.execute_input":"2021-12-31T09:37:29.336819Z","iopub.status.idle":"2021-12-31T09:37:29.349615Z","shell.execute_reply.started":"2021-12-31T09:37:29.33678Z","shell.execute_reply":"2021-12-31T09:37:29.348809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Checking the file name.\n\nBefore we match the image data with its own id, we have to check the file name whether file name has identical rule or not.\n\nIf a file name has not '.jpg', it will be shown.","metadata":{}},{"cell_type":"code","source":"for name in train_img_name:\n    if name[-4:] != '.jpg':\n        print(name)","metadata":{"execution":{"iopub.status.busy":"2021-12-31T09:37:29.350844Z","iopub.execute_input":"2021-12-31T09:37:29.351144Z","iopub.status.idle":"2021-12-31T09:37:29.357287Z","shell.execute_reply.started":"2021-12-31T09:37:29.351109Z","shell.execute_reply":"2021-12-31T09:37:29.356488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Matching the image dataset order with csv file order.","metadata":{}},{"cell_type":"code","source":"train_csv_data = pd.DataFrame()\nfor img, name in zip(train_img, train_img_name):\n    name=name[:-4]\n    location = train_csv[train_csv['Id'] == name].index[0]\n    train_csv_data= train_csv_data.append([train_csv.loc[location]])\ntrain_csv_data","metadata":{"execution":{"iopub.status.busy":"2021-12-31T09:37:29.358723Z","iopub.execute_input":"2021-12-31T09:37:29.359432Z","iopub.status.idle":"2021-12-31T09:38:04.675032Z","shell.execute_reply.started":"2021-12-31T09:37:29.359383Z","shell.execute_reply":"2021-12-31T09:38:04.674265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Reindexing the train csv data.","metadata":{}},{"cell_type":"code","source":"train_csv_data=train_csv_data.reset_index().drop(['index'],axis=1)\ntrain_csv_data","metadata":{"execution":{"iopub.status.busy":"2021-12-31T09:38:04.676352Z","iopub.execute_input":"2021-12-31T09:38:04.676673Z","iopub.status.idle":"2021-12-31T09:38:04.697716Z","shell.execute_reply.started":"2021-12-31T09:38:04.676636Z","shell.execute_reply":"2021-12-31T09:38:04.69704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Checking the resized and rescaled image with original image data.","metadata":{}},{"cell_type":"markdown","source":"It's original data.","metadata":{}},{"cell_type":"code","source":"image_1 = cv2.imread('./'+train_csv_data['Id'][0]+'.jpg')\nplt.imshow(image_1)","metadata":{"execution":{"iopub.status.busy":"2021-12-31T09:38:04.699084Z","iopub.execute_input":"2021-12-31T09:38:04.699603Z","iopub.status.idle":"2021-12-31T09:38:04.995949Z","shell.execute_reply.started":"2021-12-31T09:38:04.699559Z","shell.execute_reply":"2021-12-31T09:38:04.995203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It's resized and rescaled data.","metadata":{}},{"cell_type":"code","source":"plt.imshow(train_img[0])","metadata":{"execution":{"iopub.status.busy":"2021-12-31T09:38:04.997159Z","iopub.execute_input":"2021-12-31T09:38:04.99786Z","iopub.status.idle":"2021-12-31T09:38:05.207759Z","shell.execute_reply.started":"2021-12-31T09:38:04.997814Z","shell.execute_reply":"2021-12-31T09:38:05.207102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It's original data.","metadata":{}},{"cell_type":"code","source":"image_2 = cv2.imread('./'+train_csv_data['Id'][1]+'.jpg')\nplt.imshow(image_2)","metadata":{"execution":{"iopub.status.busy":"2021-12-31T09:38:05.209122Z","iopub.execute_input":"2021-12-31T09:38:05.209381Z","iopub.status.idle":"2021-12-31T09:38:05.44793Z","shell.execute_reply.started":"2021-12-31T09:38:05.209348Z","shell.execute_reply":"2021-12-31T09:38:05.447301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It's resized and rescaled data.","metadata":{}},{"cell_type":"code","source":"plt.imshow(train_img[1])","metadata":{"execution":{"iopub.status.busy":"2021-12-31T09:38:05.449305Z","iopub.execute_input":"2021-12-31T09:38:05.449818Z","iopub.status.idle":"2021-12-31T09:38:05.643952Z","shell.execute_reply.started":"2021-12-31T09:38:05.449781Z","shell.execute_reply":"2021-12-31T09:38:05.643274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"## Load test image dataset and rescaling","metadata":{}},{"cell_type":"markdown","source":"What is the shape of test dataset??\n\nIt's (128, 128, 3), but we have to transforme this simze into (64, 64, 3) because of memory allocation.\n\nFor analyzing the test dataset, we will match the sizes of both train image and test image, identically.","metadata":{}},{"cell_type":"code","source":"os.chdir('../test')\n\nfor i in os.listdir():\n    file = cv2.imread(i)\n    print(file.shape)","metadata":{"execution":{"iopub.status.busy":"2021-12-31T09:38:05.645338Z","iopub.execute_input":"2021-12-31T09:38:05.645835Z","iopub.status.idle":"2021-12-31T09:38:05.725105Z","shell.execute_reply.started":"2021-12-31T09:38:05.645799Z","shell.execute_reply":"2021-12-31T09:38:05.724451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"test dataset resize","metadata":{}},{"cell_type":"code","source":"test_img = []\nfor i in os.listdir():\n    file = cv2.imread(i)\n    file=cv2.resize(file,(64,64), interpolation=cv2.INTER_AREA)\n    test_img.append(file/255)\ntest_img[:5]","metadata":{"execution":{"iopub.status.busy":"2021-12-31T09:38:05.726233Z","iopub.execute_input":"2021-12-31T09:38:05.726975Z","iopub.status.idle":"2021-12-31T09:38:05.757121Z","shell.execute_reply.started":"2021-12-31T09:38:05.726939Z","shell.execute_reply":"2021-12-31T09:38:05.756432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Below procedure is identical with train dataset.","metadata":{}},{"cell_type":"code","source":"test_img_name = []\nfor i in os.listdir():\n    test_img_name.append(i)\ntest_img_name[:5]","metadata":{"execution":{"iopub.status.busy":"2021-12-31T09:38:05.758105Z","iopub.execute_input":"2021-12-31T09:38:05.758885Z","iopub.status.idle":"2021-12-31T09:38:05.76529Z","shell.execute_reply.started":"2021-12-31T09:38:05.758849Z","shell.execute_reply":"2021-12-31T09:38:05.764542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_csv_data = pd.DataFrame()\nfor img, name in zip(test_img, test_img_name):\n    name=name[:-4]\n    location = test_csv[test_csv['Id'] == name].index[0]\n    test_csv_data= test_csv_data.append([test_csv.loc[location]])\ntest_csv_data=test_csv_data.reset_index().drop(['index'],axis=1)\ntest_csv_data","metadata":{"execution":{"iopub.status.busy":"2021-12-31T09:38:05.766772Z","iopub.execute_input":"2021-12-31T09:38:05.767337Z","iopub.status.idle":"2021-12-31T09:38:05.806704Z","shell.execute_reply.started":"2021-12-31T09:38:05.767298Z","shell.execute_reply":"2021-12-31T09:38:05.806008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_1 = cv2.imread('./'+test_csv_data['Id'][0]+'.jpg')\nplt.imshow(test_1)","metadata":{"execution":{"iopub.status.busy":"2021-12-31T09:38:05.807974Z","iopub.execute_input":"2021-12-31T09:38:05.808487Z","iopub.status.idle":"2021-12-31T09:38:06.026837Z","shell.execute_reply.started":"2021-12-31T09:38:05.808448Z","shell.execute_reply":"2021-12-31T09:38:06.02618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(test_img[0])","metadata":{"execution":{"iopub.status.busy":"2021-12-31T09:38:06.031161Z","iopub.execute_input":"2021-12-31T09:38:06.031627Z","iopub.status.idle":"2021-12-31T09:38:06.228003Z","shell.execute_reply.started":"2021-12-31T09:38:06.031581Z","shell.execute_reply":"2021-12-31T09:38:06.227378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prepare the train input, target and test input.","metadata":{}},{"cell_type":"code","source":"train_csv_x = train_csv_data.drop(['Id','Pawpularity'],axis=1)\ntrain_y = train_csv_data['Pawpularity']\n\ntest_csv_x = test_csv_data.drop(['Id'],axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-12-31T09:38:06.233921Z","iopub.execute_input":"2021-12-31T09:38:06.2343Z","iopub.status.idle":"2021-12-31T09:38:06.240932Z","shell.execute_reply.started":"2021-12-31T09:38:06.234263Z","shell.execute_reply":"2021-12-31T09:38:06.240048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# It has done! \n\nYou are ready to use this dataset as train and test for getting score.\n\nNext step is preparing your submission.","metadata":{}},{"cell_type":"markdown","source":"# Analyzing with neural network.\n\n\nWe will use a special deep learning structure.\n\nThese file is devided into csv file and image file.\n\nSo, It is proper approach to use two input layers.\n\nHow can it work??\n\nLet's start!","metadata":{}},{"cell_type":"markdown","source":"## Set your model\n\nIn this part, please read the codes carefully, we will use two input layers.","metadata":{}},{"cell_type":"code","source":"# ##################### CSV FILE INPUT & IMG FILE INPUT LAYER ###########################\n# csv_input = tf.keras.Input(shape = train_csv_x.shape[1:], name = 'CSV_Input')        ##\n# img_input = tf.keras.Input(shape = np.array(train_img).shape[1:], name = 'IMG_Input')##\n# #######################################################################################\n#                                         ##\n#                                         ##\n#                                         ##\n# ##################### CSV FILE HIDDEN LAYER STRUCTURE  ######################################\n# csv_hidden1 = tf.keras.layers.Dense(200, activation='relu', name='CSV_Hidden1')(csv_input)   ##\n# csv_hidden2 = tf.keras.layers.Dense(200, activation='relu', name='CSV_Hidden2')(csv_hidden1)##\n# csv_hidden3 = tf.keras.layers.Dense(200, activation='relu', name='CSV_Hidden3')(csv_hidden2)##\n# csv_dropout = tf.keras.layers.Dropout(0.5, name ='CSV_Dropout')(csv_hidden3)               ##\n# csv_hidden4 = tf.keras.layers.Dense(200, activation='relu', name='CSV_Hidden4')(csv_dropout)##\n# csv_hidden5 = tf.keras.layers.Dense(200, activation='relu', name='CSV_Hidden5')(csv_hidden4)##\n# #############################################################################################\n#                                          #\n#                                          #\n#                                          #\n# ##################### IMG FILE CONVOLUTIONAL LAYER STRUCTURE  ######################################\n# img_conv1 = tf.keras.layers.Conv2D(filters = 120, kernel_size = 5, strides= 1, padding = 'same',   ##\n#                                    activation = 'relu', name='IMG_Conv1')(img_input)              ##\n# img_pool1 = tf.keras.layers.MaxPool2D(3, name = 'IMG_Pool1')(img_conv1)                           ##\n# img_conv2 = tf.keras.layers.Conv2D(filters = 120, kernel_size = 4, strides= 1, padding = 'same',  ##\n#                                    activation = 'relu', name='IMG_Conv2')(img_pool1)              ##\n# img_conv3 = tf.keras.layers.Conv2D(filters = 120, kernel_size = 4, strides =1, padding = 'same',  ##\n#                                    activation = 'relu', name='IMG_Conv3')(img_conv2)              ##\n# img_pool2 = tf.keras.layers.MaxPool2D(3, name = 'IMG_Pool2')(img_conv3)                           ##\n# img_conv4 = tf.keras.layers.Conv2D(filters = 120, kernel_size = 3, strides =1, padding = 'same',  ##\n#                                    activation = 'relu', name='IMG_Conv4')(img_pool2)              ##\n# img_pool3 = tf.keras.layers.MaxPool2D(3, name = 'IMG_Pool3')(img_conv4)                           ##\n# img_conv5 = tf.keras.layers.Conv2D(filters = 120, kernel_size = 3, strides =1, padding = 'same',  ##\n#                                    activation = 'relu', name='IMG_Conv5')(img_pool3)              ##\n# img_conv6 = tf.keras.layers.Conv2D(filters = 120, kernel_size = 3, strides =1, padding = 'same',  ##\n#                                    activation = 'relu', name='IMG_Conv6')(img_conv5)              ##\n# img_pool4 = tf.keras.layers.MaxPool2D(2, name = 'IMG_Pool4')(img_conv6)                           ##\n# img_flatten = tf.keras.layers.Flatten(name = 'IMG_Flatten')(img_pool4)                            ##\n# img_dense1 = tf.keras.layers.Dense(300, activation = 'relu', name='IMG_Dense1')(img_flatten)     ##\n# img_dropout1 = tf.keras.layers.Dropout(0.5, name='IMG_Dropout1')(img_dense1)                      ##\n# img_dense2 = tf.keras.layers.Dense(300, activation = 'relu', name='IMG_Dense2')(img_dropout1)    ##\n# img_dropout2 = tf.keras.layers.Dropout(0.5, name='IMG_Dropout2')(img_dense2)                      ##\n# ####################################################################################################\n#                                         ##\n#                                         ##\n#                                         ##\n# ##################### CSV FILE INPUT & IMG FILE OUTPUT LAYER ############\n# csv_output = tf.keras.layers.Dense(1, name = 'CSV_Output')(csv_hidden5)##\n# img_output = tf.keras.layers.Dense(1,name = 'IMG_Output')(img_dropout2)##\n# #########################################################################\n#                                         ##\n#                                         ##\n#                                         ##\n# ############################################# MODEL SETTING  ####################################################\n# model = tf.keras.Model(inputs=[csv_input, img_input], outputs=[csv_output, img_output], name='Pythonash_model')##\n# #################################################################################################################","metadata":{"execution":{"iopub.status.busy":"2021-12-31T09:38:06.242164Z","iopub.execute_input":"2021-12-31T09:38:06.242578Z","iopub.status.idle":"2021-12-31T09:38:06.252878Z","shell.execute_reply.started":"2021-12-31T09:38:06.242537Z","shell.execute_reply":"2021-12-31T09:38:06.252123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"csv_input = tf.keras.Input(shape = train_csv_x.shape[1:], name = 'CSV_Input')\nimg_input = tf.keras.Input(shape = np.array(train_img).shape[1:], name = 'IMG_Input')\n\ncsv_hidden1 = tf.keras.layers.Dense(200, activation='elu', kernel_initializer = 'he_normal', name='CSV_Hidden1')(csv_input)\ncsv_hidden2 = tf.keras.layers.Dense(200, activation='elu', kernel_initializer = 'he_normal', name='CSV_Hidden2')(csv_hidden1)\ncsv_hidden3 = tf.keras.layers.Dense(200, activation='elu', kernel_initializer = 'he_normal', name='CSV_Hidden3')(csv_hidden2)\ncsv_hidden4 = tf.keras.layers.Dense(200, activation='elu', kernel_initializer = 'he_normal', name='CSV_Hidden4')(csv_hidden3)\ncsv_hidden5 = tf.keras.layers.Dense(200, activation='elu', kernel_initializer = 'he_normal', name='CSV_Hidden5')(csv_hidden4)\ncsv_hidden6 = tf.keras.layers.Dense(200, activation='elu', kernel_initializer = 'he_normal', name='CSV_Hidden6')(csv_hidden5)\ncsv_dropout = tf.keras.layers.Dropout(0.5, name ='CSV_Dropout')(csv_hidden6)\n\nimg_conv1 = tf.keras.layers.Conv2D(120, 4, padding = 'same', activation = 'elu', kernel_initializer = 'he_normal',name='IMG_Conv1')(img_input)\nimg_conv2 = tf.keras.layers.Conv2D(120, 4, padding = 'same', activation = 'elu', kernel_initializer = 'he_normal',name='IMG_Conv2')(img_conv1)\nimg_pooling1 = tf.keras.layers.MaxPooling2D(4, name= 'IMG_Max1')(img_conv2)\n\nimg_conv3 = tf.keras.layers.Conv2D(120, 4, padding = 'same', activation = 'elu', kernel_initializer = 'he_normal',name='IMG_Conv3')(img_pooling1)\nimg_conv4 = tf.keras.layers.Conv2D(120, 4, padding = 'same', activation = 'elu', kernel_initializer = 'he_normal',name='IMG_Conv4')(img_conv3)\nimg_pooling2 = tf.keras.layers.MaxPooling2D(4, name= 'IMG_Max2')(img_conv4)\n\nimg_conv5 = tf.keras.layers.Conv2D(120, 4, padding = 'same', activation = 'elu', kernel_initializer = 'he_normal',name='IMG_Conv5')(img_pooling2)\nimg_conv6 = tf.keras.layers.Conv2D(120, 4, padding = 'same', activation = 'elu', kernel_initializer = 'he_normal',name='IMG_Conv6')(img_conv5)\nimg_pooling3 = tf.keras.layers.MaxPooling2D(3, name= 'IMG_Max3')(img_conv6)\n\nimg_dropout = tf.keras.layers.Dropout(0.5, name = 'IMG_Dropout')(img_pooling3)\nimg_conv7 = tf.keras.layers.Conv2D(120, 4, padding = 'same', activation = 'elu', kernel_initializer = 'he_normal',name='IMG_Conv7')(img_dropout)\n\nimg_hidden1 = tf.keras.layers.Dense(300, activation='elu', kernel_initializer = 'he_normal',name='IMG_hidden1')(img_conv7)\nimg_dropout1 = tf.keras.layers.Dropout(0.5, name='IMG_Dropout1')(img_hidden1)\n\nimg_hidden2 = tf.keras.layers.Dense(600, activation='elu', kernel_initializer = 'he_normal',name='IMG_hidden2')(img_dropout1)\n\nimg_gpool = tf.keras.layers.GlobalAvgPool2D(name = 'IMG_Gpool')(img_hidden2)\n\nimg_dropout2 = tf.keras.layers.Dropout(0.5, name='IMG_Dropout2')(img_gpool)\n\n\n\ncsv_output = tf.keras.layers.Dense(1, name = 'CSV_Output')(csv_dropout)\nimg_output = tf.keras.layers.Dense(1,name = 'IMG_Output')(img_dropout2)\n\nmodel = tf.keras.Model(inputs=[csv_input, img_input], outputs=[csv_output, img_output], name='Pythonash_model')","metadata":{"execution":{"iopub.status.busy":"2021-12-31T09:38:06.25423Z","iopub.execute_input":"2021-12-31T09:38:06.254696Z","iopub.status.idle":"2021-12-31T09:38:10.235489Z","shell.execute_reply.started":"2021-12-31T09:38:06.254662Z","shell.execute_reply":"2021-12-31T09:38:10.234728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## model summary\n\nIt shows that your model structure, simply.","metadata":{}},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2021-12-31T09:38:10.236561Z","iopub.execute_input":"2021-12-31T09:38:10.236957Z","iopub.status.idle":"2021-12-31T09:38:10.257826Z","shell.execute_reply.started":"2021-12-31T09:38:10.236921Z","shell.execute_reply":"2021-12-31T09:38:10.257125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## plot your model.\n\nThis plot shows your model and you can figure out you model structure, intuitively.","metadata":{}},{"cell_type":"code","source":"# move you current directory to back.\nos.chdir('../')\nos.chdir('../')\nos.chdir('../')\ntf.keras.utils.plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True, rankdir='TB')","metadata":{"execution":{"iopub.status.busy":"2021-12-31T09:38:10.259267Z","iopub.execute_input":"2021-12-31T09:38:10.259518Z","iopub.status.idle":"2021-12-31T09:38:11.156456Z","shell.execute_reply.started":"2021-12-31T09:38:10.259486Z","shell.execute_reply":"2021-12-31T09:38:11.155717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Compile and fit your model.","metadata":{}},{"cell_type":"code","source":"learning_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\ninitial_learning_rate = 0.002,\ndecay_steps = 10000,\ndecay_rate = 0.99)\n\nopt = tf.keras.optimizers.Adam(learning_rate = learning_schedule)\nmodel.compile(loss=['mse','mse'], loss_weights=[0.5, 0.5], optimizer = opt, metrics = tf.keras.metrics.RootMeanSquaredError())\n\nepoch_number = 20\n\ncheck_1 = tf.keras.callbacks.ModelCheckpoint('pythonash_model.h5', save_best_only=True, verbose=2)\n# check_2 = tf.keras.callbacks.EarlyStopping(patience = epoch_number * 0.1, monitoring ='val_loss', verbose=2,\n#                                           restore_best_weight=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-31T09:38:11.158105Z","iopub.execute_input":"2021-12-31T09:38:11.158477Z","iopub.status.idle":"2021-12-31T09:38:11.184493Z","shell.execute_reply.started":"2021-12-31T09:38:11.158441Z","shell.execute_reply":"2021-12-31T09:38:11.183881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit( \n    x= [train_csv_x, np.array(train_img)], y = [train_y, train_y], epochs=epoch_number, \n    validation_split=0.2, verbose =2, workers=3, batch_size = 100, validation_batch_size = 100,\n    callbacks = [check_1])","metadata":{"execution":{"iopub.status.busy":"2021-12-31T09:38:11.185914Z","iopub.execute_input":"2021-12-31T09:38:11.186165Z","iopub.status.idle":"2021-12-31T09:39:35.577072Z","shell.execute_reply.started":"2021-12-31T09:38:11.18613Z","shell.execute_reply":"2021-12-31T09:39:35.576343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_model = tf.keras.models.load_model('pythonash_model.h5')\ncsv_result, img_result = best_model.predict([test_csv_x, np.array(test_img)])\nfinal_result = pd.DataFrame(0.5 * csv_result + 0.5 * img_result)\nfinal_result.columns =['Pawpularity']\nfinal_result","metadata":{"execution":{"iopub.status.busy":"2021-12-31T09:39:35.57855Z","iopub.execute_input":"2021-12-31T09:39:35.578969Z","iopub.status.idle":"2021-12-31T09:39:36.132792Z","shell.execute_reply.started":"2021-12-31T09:39:35.578932Z","shell.execute_reply":"2021-12-31T09:39:36.132029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for ids, paw in zip(test_csv_data['Id'], final_result['Pawpularity']):\n    location = submission[submission['Id'] == ids].index[0]\n    submission['Pawpularity'].loc[location] = paw\nsubmission\nsubmission.to_csv('./working/submission.csv',index=False)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-12-31T09:39:36.13414Z","iopub.execute_input":"2021-12-31T09:39:36.134508Z","iopub.status.idle":"2021-12-31T09:39:36.154348Z","shell.execute_reply.started":"2021-12-31T09:39:36.134471Z","shell.execute_reply":"2021-12-31T09:39:36.153543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# It's your turn!!\n\nYou have many opportunities that you can change this model parameters and get your submission score.\n\nI recommend that you change the hyper parameters such as learning_rate, batch_size, activation function, the number of neurons, layers, and so on...\n\nIf you get any helps from my notebook, please upvote!!\n\nFingers crossed!!","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}