{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Read me\n\nHello, This is for begginers who want to know how you can handle this dataset which consists of some csv file and image datasaet.\n\nThis notebook is just for handling data such as loading the dataset, stacking dataset, and etc...\n\nIt will be helpful for preparing analysis to someone who wants to submit.\n\nFurthermore, I will upload the deep learning model through this data handling procedure for getting score.\n\nIf you have any questions, please leave the comments.\n\n\nYou can learn about using deep learning through my previous notebook (it is for titanic analysis).\n\n## [Data handling & Deep learning]\n - https://www.kaggle.com/pythonash/how-to-handle-raw-dataset-and-analyze-with-dl\n \n## [Preparing a completed dataset with proper imputation method]\n - https://www.kaggle.com/pythonash/making-completed-dataset\n \n## [Deep learning model with SeLU activation function]\n- https://www.kaggle.com/pythonash/selu-activation-function-in-dl\n\n**Let's start!**","metadata":{}},{"cell_type":"markdown","source":"# Import some libraries for handling dataset.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport tensorflow as tf\nimport cv2\nimport os\nimport matplotlib.pyplot as plt\nimport numpy as np\n# seaborn is of use for visualizing.\nimport seaborn as sns\n\n# load train, test, and submission sample dataset.\ntrain_csv = pd.read_csv('../input/petfinder-pawpularity-score/train.csv')\ntest_csv = pd.read_csv('../input/petfinder-pawpularity-score/test.csv')\nsubmission = pd.read_csv('../input/petfinder-pawpularity-score/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-12-22T14:05:40.530886Z","iopub.execute_input":"2021-12-22T14:05:40.531217Z","iopub.status.idle":"2021-12-22T14:05:44.679593Z","shell.execute_reply.started":"2021-12-22T14:05:40.531128Z","shell.execute_reply":"2021-12-22T14:05:44.678829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Glancing the train csv dataset.\n\nAll columns contain binary values except for 'Id' and 'Pawpularity'","metadata":{}},{"cell_type":"code","source":"train_csv","metadata":{"execution":{"iopub.status.busy":"2021-12-22T14:05:44.681616Z","iopub.execute_input":"2021-12-22T14:05:44.682153Z","iopub.status.idle":"2021-12-22T14:05:44.703333Z","shell.execute_reply.started":"2021-12-22T14:05:44.682113Z","shell.execute_reply":"2021-12-22T14:05:44.702605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Wow, there is no null-value.\n\nSo, we don't have to mind about imputation.","metadata":{}},{"cell_type":"code","source":"train_csv.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-12-22T14:05:44.70457Z","iopub.execute_input":"2021-12-22T14:05:44.704984Z","iopub.status.idle":"2021-12-22T14:05:44.715736Z","shell.execute_reply.started":"2021-12-22T14:05:44.704945Z","shell.execute_reply":"2021-12-22T14:05:44.714906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Furthermore, there is no duplicated data.","metadata":{}},{"cell_type":"code","source":"train_csv.drop_duplicates()","metadata":{"execution":{"iopub.status.busy":"2021-12-22T14:05:44.718472Z","iopub.execute_input":"2021-12-22T14:05:44.718829Z","iopub.status.idle":"2021-12-22T14:05:44.743961Z","shell.execute_reply.started":"2021-12-22T14:05:44.718791Z","shell.execute_reply":"2021-12-22T14:05:44.743313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Count the binary values.\n\nAll data is unbalanced, but you don't have to worry about that.\n\nI will expalin the reason why it is okay in next notebook (coming soon).","metadata":{}},{"cell_type":"code","source":"for i in train_csv.drop(['Id','Pawpularity'],axis=1):\n    sns.countplot(train_csv[i])\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-22T14:05:44.745241Z","iopub.execute_input":"2021-12-22T14:05:44.745476Z","iopub.status.idle":"2021-12-22T14:05:46.366424Z","shell.execute_reply.started":"2021-12-22T14:05:44.745444Z","shell.execute_reply":"2021-12-22T14:05:46.365635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The target variable of train dataset is distributed as below.\n\nIn this part, you have to determine whether you truncate some outliers or not.\n\nHowever, I recommend that you don't truncate because I think those are a part of dataset, too.\n\nWe don't know what the truncated data affects in using deep learning.\n\nIt's just on my experience so, you can select and it's all up to you!","metadata":{}},{"cell_type":"code","source":"sns.distplot(train_csv['Pawpularity'])","metadata":{"execution":{"iopub.status.busy":"2021-12-22T14:05:46.367839Z","iopub.execute_input":"2021-12-22T14:05:46.368306Z","iopub.status.idle":"2021-12-22T14:05:46.690793Z","shell.execute_reply.started":"2021-12-22T14:05:46.368267Z","shell.execute_reply":"2021-12-22T14:05:46.690105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Glancing the test dataset.","metadata":{}},{"cell_type":"code","source":"test_csv","metadata":{"execution":{"iopub.status.busy":"2021-12-22T14:05:46.692098Z","iopub.execute_input":"2021-12-22T14:05:46.69251Z","iopub.status.idle":"2021-12-22T14:05:46.707576Z","shell.execute_reply.started":"2021-12-22T14:05:46.692472Z","shell.execute_reply":"2021-12-22T14:05:46.706816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It has not null-values, either.","metadata":{}},{"cell_type":"code","source":"test_csv.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-12-22T14:05:46.708828Z","iopub.execute_input":"2021-12-22T14:05:46.709107Z","iopub.status.idle":"2021-12-22T14:05:46.715968Z","shell.execute_reply.started":"2021-12-22T14:05:46.709069Z","shell.execute_reply":"2021-12-22T14:05:46.715243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Finally, Let's identify the submission form.","metadata":{}},{"cell_type":"code","source":"submission","metadata":{"execution":{"iopub.status.busy":"2021-12-22T14:05:46.717293Z","iopub.execute_input":"2021-12-22T14:05:46.717684Z","iopub.status.idle":"2021-12-22T14:05:46.731435Z","shell.execute_reply.started":"2021-12-22T14:05:46.717649Z","shell.execute_reply":"2021-12-22T14:05:46.730577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Handling image dataset.","metadata":{}},{"cell_type":"code","source":"# Set the path for loading image dataset.\nos.chdir('../input/petfinder-pawpularity-score/train')\n\n# We can find the size of each image data from this procedure\nsize_data = pd.DataFrame()\nfor file in os.listdir():\n    imgg = cv2.imread(file)\n    w,h,c = imgg.shape\n    size_data=size_data.append([[w,h,c,imgg.size/3]])\nsize_data","metadata":{"execution":{"iopub.status.busy":"2021-12-22T14:05:46.732694Z","iopub.execute_input":"2021-12-22T14:05:46.73296Z","iopub.status.idle":"2021-12-22T14:08:16.356631Z","shell.execute_reply.started":"2021-12-22T14:05:46.732906Z","shell.execute_reply":"2021-12-22T14:08:16.355796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## What is the minimum size?","metadata":{}},{"cell_type":"code","source":"size_data[size_data[3] == size_data[3].min()]","metadata":{"execution":{"iopub.status.busy":"2021-12-22T14:08:16.359129Z","iopub.execute_input":"2021-12-22T14:08:16.359729Z","iopub.status.idle":"2021-12-22T14:08:16.369392Z","shell.execute_reply.started":"2021-12-22T14:08:16.359677Z","shell.execute_reply":"2021-12-22T14:08:16.368579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## What is the number of most things?","metadata":{}},{"cell_type":"code","source":"size_data[3].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-12-22T14:08:16.370731Z","iopub.execute_input":"2021-12-22T14:08:16.37126Z","iopub.status.idle":"2021-12-22T14:08:16.385605Z","shell.execute_reply.started":"2021-12-22T14:08:16.371219Z","shell.execute_reply":"2021-12-22T14:08:16.384889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Checking the pixel structure of that.\n\n- It is derived from 960 * 720.","metadata":{}},{"cell_type":"code","source":"size_data[size_data[3] == 691200]","metadata":{"execution":{"iopub.status.busy":"2021-12-22T14:08:16.390442Z","iopub.execute_input":"2021-12-22T14:08:16.390724Z","iopub.status.idle":"2021-12-22T14:08:16.405791Z","shell.execute_reply.started":"2021-12-22T14:08:16.390694Z","shell.execute_reply":"2021-12-22T14:08:16.404708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load train image dataset and rescaling.\n\nThis part is crucial for analysis and is devided into 3 parts.\n\n1. Loading the image dataset.\n\n2. Changing the shape of each image into 64 * 64.\n\n- Why 64 * 64 ?? the reason is that you will stuck in memory allocation.\n\n- That is, there are so many image dataset which make your memory be exploded.\n\n- So, reszing the image into 64 * 64, you can take your memory with sufficient.\n\n3. Rescaling the pixels by deviding with 255.\n\nIn this procedure, the parameter, cv2.INTER_AREA, is useful for interpolation.\n\nThere are several interpolation methods, but I recommend this.","metadata":{}},{"cell_type":"code","source":"train_img = []\nfor i in os.listdir():\n    file = cv2.imread(i)\n    file=cv2.resize(file,(64,64), interpolation=cv2.INTER_AREA)\n    train_img.append(file/255)\ntrain_img[:5]","metadata":{"execution":{"iopub.status.busy":"2021-12-22T14:08:16.407715Z","iopub.execute_input":"2021-12-22T14:08:16.408209Z","iopub.status.idle":"2021-12-22T14:10:09.008617Z","shell.execute_reply.started":"2021-12-22T14:08:16.408146Z","shell.execute_reply":"2021-12-22T14:10:09.0078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## For corresponding the image with each id.\n\nEach image data has to correspond to its own id.","metadata":{}},{"cell_type":"code","source":"train_img_name = []\nfor i in os.listdir():\n    train_img_name.append(i)\ntrain_img_name[:5]","metadata":{"execution":{"iopub.status.busy":"2021-12-22T14:10:09.010223Z","iopub.execute_input":"2021-12-22T14:10:09.010654Z","iopub.status.idle":"2021-12-22T14:10:09.024586Z","shell.execute_reply.started":"2021-12-22T14:10:09.010607Z","shell.execute_reply":"2021-12-22T14:10:09.023562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Checking the file name.\n\nBefore we match the image data with its own id, we have to check the file name whether file name has identical rule or not.\n\nIf a file name has not '.jpg', it will be shown.","metadata":{}},{"cell_type":"code","source":"for name in train_img_name:\n    if name[-4:] != '.jpg':\n        print(name)","metadata":{"execution":{"iopub.status.busy":"2021-12-22T14:10:09.02654Z","iopub.execute_input":"2021-12-22T14:10:09.02701Z","iopub.status.idle":"2021-12-22T14:10:09.03295Z","shell.execute_reply.started":"2021-12-22T14:10:09.026972Z","shell.execute_reply":"2021-12-22T14:10:09.03203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Matching the image dataset order with csv file order.","metadata":{}},{"cell_type":"code","source":"train_csv_data = pd.DataFrame()\nfor img, name in zip(train_img, train_img_name):\n    name=name[:-4]\n    location = train_csv[train_csv['Id'] == name].index[0]\n    train_csv_data= train_csv_data.append([train_csv.loc[location]])\ntrain_csv_data","metadata":{"execution":{"iopub.status.busy":"2021-12-22T14:10:09.034314Z","iopub.execute_input":"2021-12-22T14:10:09.034741Z","iopub.status.idle":"2021-12-22T14:10:44.951645Z","shell.execute_reply.started":"2021-12-22T14:10:09.034699Z","shell.execute_reply":"2021-12-22T14:10:44.950868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Reindexing the train csv data.","metadata":{}},{"cell_type":"code","source":"train_csv_data=train_csv_data.reset_index().drop(['index'],axis=1)\ntrain_csv_data","metadata":{"execution":{"iopub.status.busy":"2021-12-22T14:10:44.952789Z","iopub.execute_input":"2021-12-22T14:10:44.953065Z","iopub.status.idle":"2021-12-22T14:10:44.973655Z","shell.execute_reply.started":"2021-12-22T14:10:44.953028Z","shell.execute_reply":"2021-12-22T14:10:44.972991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Checking the resized and rescaled image with original image data.","metadata":{}},{"cell_type":"markdown","source":"It's original data.","metadata":{}},{"cell_type":"code","source":"image_1 = cv2.imread('./'+train_csv_data['Id'][0]+'.jpg')\nplt.imshow(image_1)","metadata":{"execution":{"iopub.status.busy":"2021-12-22T14:10:44.974745Z","iopub.execute_input":"2021-12-22T14:10:44.975476Z","iopub.status.idle":"2021-12-22T14:10:45.264379Z","shell.execute_reply.started":"2021-12-22T14:10:44.975435Z","shell.execute_reply":"2021-12-22T14:10:45.263609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It's resized and rescaled data.","metadata":{}},{"cell_type":"code","source":"plt.imshow(train_img[0])","metadata":{"execution":{"iopub.status.busy":"2021-12-22T14:10:45.265375Z","iopub.execute_input":"2021-12-22T14:10:45.265646Z","iopub.status.idle":"2021-12-22T14:10:45.458646Z","shell.execute_reply.started":"2021-12-22T14:10:45.265607Z","shell.execute_reply":"2021-12-22T14:10:45.457962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It's original data.","metadata":{}},{"cell_type":"code","source":"image_2 = cv2.imread('./'+train_csv_data['Id'][1]+'.jpg')\nplt.imshow(image_2)","metadata":{"execution":{"iopub.status.busy":"2021-12-22T14:10:45.459866Z","iopub.execute_input":"2021-12-22T14:10:45.460471Z","iopub.status.idle":"2021-12-22T14:10:45.726759Z","shell.execute_reply.started":"2021-12-22T14:10:45.46043Z","shell.execute_reply":"2021-12-22T14:10:45.725979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It's resized and rescaled data.","metadata":{}},{"cell_type":"code","source":"plt.imshow(train_img[1])","metadata":{"execution":{"iopub.status.busy":"2021-12-22T14:10:45.728304Z","iopub.execute_input":"2021-12-22T14:10:45.728603Z","iopub.status.idle":"2021-12-22T14:10:45.945556Z","shell.execute_reply.started":"2021-12-22T14:10:45.728562Z","shell.execute_reply":"2021-12-22T14:10:45.944805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"## Load test image dataset and rescaling","metadata":{}},{"cell_type":"markdown","source":"What is the shape of test dataset??\n\nIt's (128, 128, 3), but we have to resize the test image into 64 * 64 because of memory allocation problem.\n\nFor analyzing the test dataset, we will match the sizes of both train image and test image, identically.","metadata":{}},{"cell_type":"code","source":"os.chdir('../test')\n\nfor i in os.listdir():\n    file = cv2.imread(i)\n    print(file.shape)","metadata":{"execution":{"iopub.status.busy":"2021-12-22T14:10:45.947117Z","iopub.execute_input":"2021-12-22T14:10:45.947396Z","iopub.status.idle":"2021-12-22T14:10:45.967677Z","shell.execute_reply.started":"2021-12-22T14:10:45.947358Z","shell.execute_reply":"2021-12-22T14:10:45.966979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_img = []\nfor i in os.listdir():\n    file = cv2.imread(i)\n    file=cv2.resize(file,(64,64), interpolation=cv2.INTER_AREA)\n    test_img.append(file/255)\ntest_img[:5]","metadata":{"execution":{"iopub.status.busy":"2021-12-22T14:10:45.968731Z","iopub.execute_input":"2021-12-22T14:10:45.968921Z","iopub.status.idle":"2021-12-22T14:10:45.996964Z","shell.execute_reply.started":"2021-12-22T14:10:45.968897Z","shell.execute_reply":"2021-12-22T14:10:45.996204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Below procedure is identical with train dataset.","metadata":{}},{"cell_type":"code","source":"test_img_name = []\nfor i in os.listdir():\n    test_img_name.append(i)\ntest_img_name[:5]","metadata":{"execution":{"iopub.status.busy":"2021-12-22T14:10:45.998474Z","iopub.execute_input":"2021-12-22T14:10:45.999431Z","iopub.status.idle":"2021-12-22T14:10:46.008559Z","shell.execute_reply.started":"2021-12-22T14:10:45.999389Z","shell.execute_reply":"2021-12-22T14:10:46.007735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_csv_data = pd.DataFrame()\nfor img, name in zip(test_img, test_img_name):\n    name=name[:-4]\n    location = test_csv[test_csv['Id'] == name].index[0]\n    test_csv_data= test_csv_data.append([test_csv.loc[location]])\ntest_csv_data=test_csv_data.reset_index().drop(['index'],axis=1)\ntest_csv_data","metadata":{"execution":{"iopub.status.busy":"2021-12-22T14:10:46.010093Z","iopub.execute_input":"2021-12-22T14:10:46.010906Z","iopub.status.idle":"2021-12-22T14:10:46.050223Z","shell.execute_reply.started":"2021-12-22T14:10:46.010815Z","shell.execute_reply":"2021-12-22T14:10:46.049463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_1 = cv2.imread('./'+test_csv_data['Id'][0]+'.jpg')\nplt.imshow(test_1)","metadata":{"execution":{"iopub.status.busy":"2021-12-22T14:10:46.051498Z","iopub.execute_input":"2021-12-22T14:10:46.052144Z","iopub.status.idle":"2021-12-22T14:10:46.287455Z","shell.execute_reply.started":"2021-12-22T14:10:46.052103Z","shell.execute_reply":"2021-12-22T14:10:46.286761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(test_img[0])","metadata":{"execution":{"iopub.status.busy":"2021-12-22T14:10:46.288582Z","iopub.execute_input":"2021-12-22T14:10:46.28895Z","iopub.status.idle":"2021-12-22T14:10:46.503223Z","shell.execute_reply.started":"2021-12-22T14:10:46.288902Z","shell.execute_reply":"2021-12-22T14:10:46.502427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prepare the train input, target and test input.","metadata":{}},{"cell_type":"code","source":"train_csv_x = train_csv_data.drop(['Id','Pawpularity'],axis=1)\ntrain_y = train_csv_data['Pawpularity']\n\ntest_csv_x = test_csv_data.drop(['Id'],axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-12-22T14:10:46.505029Z","iopub.execute_input":"2021-12-22T14:10:46.505334Z","iopub.status.idle":"2021-12-22T14:10:46.512682Z","shell.execute_reply.started":"2021-12-22T14:10:46.505294Z","shell.execute_reply":"2021-12-22T14:10:46.511612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# It has done! \n\nYou are ready to use this dataset as train and test for getting score.\n\nI think you may want to know why test image doesn't seem like animal.\n\nAnd, you will wonder whether this dataset can be a useful resource for analyzing or not.\n\nThe clue will be on next notebook.\n\nAnyway, fingers crossed!","metadata":{}}]}