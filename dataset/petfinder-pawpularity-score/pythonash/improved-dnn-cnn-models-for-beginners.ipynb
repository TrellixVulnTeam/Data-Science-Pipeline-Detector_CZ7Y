{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Read me\n\nHello, This notebook is more upgraded than my previous notebook (https://www.kaggle.com/pythonash/parallel-dnn-and-cnn-network-for-beginners).\n\nIn this part, you can study how you can concatenate DNN model for handling CSV file and CNN model for handling IMG file.\n\nThe data handling procedure will be skipped. If you want to know how it worked in detail, just clik on my previous notebook (https://www.kaggle.com/pythonash/how-to-handle-dataset-for-beginners).\n\nIf you have any questions, please leave the comments.\n\nI hope you to gain more imformation about data handling, DNN, CNN, and etc..\n\n## **Knowledge can be improved by being shared.**\n\nPlease upvote!!\n\n\n## [You can learn more skills for handling dataset or neural network.]\n\n### [Parallel combination DNN with CNN] - Pawpularity Contest\n - https://www.kaggle.com/pythonash/parallel-dnn-and-cnn-network-for-beginners\n \n### [Image data handling without memory exploded] - Pawpularity Contest\n - https://www.kaggle.com/pythonash/how-to-handle-dataset-for-beginners\n\n### [Data handling & Deep learning] - Titanic competition (best score!!)\n - https://www.kaggle.com/pythonash/how-to-handle-raw-dataset-and-analyze-with-dl\n \n### [Deep learning model with SeLU activation function] - Titanic competition\n- https://www.kaggle.com/pythonash/selu-activation-function-in-dl\n\n### [Preparing a completed dataset with proper imputation method] - Titanic competition\n - https://www.kaggle.com/pythonash/making-completed-dataset\n\n**Let's start!**","metadata":{}},{"cell_type":"markdown","source":"# Just run this code before you set your model.\n\n- This code is for preparing dataset to input at your model.\n\n- The details are described as in my notebook, \"Image data handling without memory exploded\".","metadata":{}},{"cell_type":"code","source":"##################################JUST RUN THIS CODE FOR PREPARING DATASET##################################\nimport pandas as pd\nimport tensorflow as tf\nimport cv2\nimport os\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\ntrain_csv = pd.read_csv('../input/petfinder-pawpularity-score/train.csv')\ntest_csv = pd.read_csv('../input/petfinder-pawpularity-score/test.csv')\nsubmission = pd.read_csv('../input/petfinder-pawpularity-score/sample_submission.csv')\n\nos.chdir('../input/petfinder-pawpularity-score/train')\n\nsize_data = pd.DataFrame()\nfor file in os.listdir():\n    imgg = cv2.imread(file)\n    w,h,c = imgg.shape\n    size_data=size_data.append([[w,h,c,imgg.size/3]])\n    \n\ntrain_img = []\nfor i in os.listdir():\n    file = cv2.imread(i)\n    file=cv2.resize(file,(64,64), interpolation=cv2.INTER_AREA)\n    train_img.append(file/255)\n\n\ntrain_img_name = []\nfor i in os.listdir():\n    train_img_name.append(i)\n\n\ntrain_csv_data = pd.DataFrame()\nfor img, name in zip(train_img, train_img_name):\n    name=name[:-4]\n    location = train_csv[train_csv['Id'] == name].index[0]\n    train_csv_data= train_csv_data.append([train_csv.loc[location]])\n\ntrain_csv_data=train_csv_data.reset_index().drop(['index'],axis=1)\n\nos.chdir('../test')\n\ntest_img = []\nfor i in os.listdir():\n    file = cv2.imread(i)\n    file=cv2.resize(file,(64,64), interpolation=cv2.INTER_AREA)\n    test_img.append(file/255)\n\ntest_img_name = []\nfor i in os.listdir():\n    test_img_name.append(i)\n\ntest_csv_data = pd.DataFrame()\nfor img, name in zip(test_img, test_img_name):\n    name=name[:-4]\n    location = test_csv[test_csv['Id'] == name].index[0]\n    test_csv_data= test_csv_data.append([test_csv.loc[location]])\ntest_csv_data=test_csv_data.reset_index().drop(['index'],axis=1)\n\ntrain_csv_x = train_csv_data.drop(['Id','Pawpularity'],axis=1)\ntrain_y = train_csv_data['Pawpularity']\ntest_csv_x = test_csv_data.drop(['Id'],axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-12-27T13:15:25.870163Z","iopub.execute_input":"2021-12-27T13:15:25.870712Z","iopub.status.idle":"2021-12-27T13:20:25.899372Z","shell.execute_reply.started":"2021-12-27T13:15:25.870621Z","shell.execute_reply":"2021-12-27T13:20:25.898641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The comnibation of DNN and CNN\n\nWe will use a parallel structure which consists of DNN for CSV and CNN for IMAGE.\n\nThe idea is very simple.\n\n1. Set the DNN model and CNN model, respectively.\n\n2. Combinate two models with two outputs.\n\n> In this procedure, you designate the weights for DNN model and CNN model, respectively.\n\n> This model will output two results which are CSV result and IMG result.\n\n> Thus, each result needs its own weight (you can set this weight).\n\n\nHow can it work??\n\nLet's start!","metadata":{}},{"cell_type":"markdown","source":"## Set your model\n\nIn this part, please read the codes carefully, we will use two input layers.","metadata":{}},{"cell_type":"code","source":"##################### CSV FILE INPUT & IMG FILE INPUT LAYER ###########################\ncsv_input = tf.keras.Input(shape = train_csv_x.shape[1:], name = 'CSV_Input')        ##\nimg_input = tf.keras.Input(shape = np.array(train_img).shape[1:], name = 'IMG_Input')##\n#######################################################################################\n                                        ##\n                                        ##\n                                        ##\n##################### CSV FILE HIDDEN LAYER STRUCTURE  ######################################\ncsv_hidden1 = tf.keras.layers.Dense(8, activation='relu', name='CSV_Hidden1')(csv_input)   ##\ncsv_hidden2 = tf.keras.layers.Dense(30, activation='relu', name='CSV_Hidden2')(csv_hidden1)##\ncsv_hidden3 = tf.keras.layers.Dense(50, activation='relu', name='CSV_Hidden3')(csv_hidden2)##\ncsv_dropout = tf.keras.layers.Dropout(0.5, name ='CSV_Dropout')(csv_hidden3)               ##\ncsv_hidden4 = tf.keras.layers.Dense(30, activation='relu', name='CSV_Hidden4')(csv_dropout)##\ncsv_hidden5 = tf.keras.layers.Dense(10, activation='relu', name='CSV_Hidden5')(csv_hidden4)##\n#############################################################################################\n                                         #\n                                         #\n                                         #\n##################### IMG FILE CONVOLUTIONAL LAYER STRUCTURE  ######################################\nimg_conv1 = tf.keras.layers.Conv2D(filters = 64, kernel_size = 5, strides= 1, padding = 'same',   ##\n                                   activation = 'relu', name='IMG_Conv1')(img_input)              ##\nimg_pool1 = tf.keras.layers.MaxPool2D(3, name = 'IMG_Pool1')(img_conv1)                           ##\nimg_conv2 = tf.keras.layers.Conv2D(filters = 128, kernel_size = 4, strides= 1, padding = 'same',  ##\n                                   activation = 'relu', name='IMG_Conv2')(img_pool1)              ##\nimg_conv3 = tf.keras.layers.Conv2D(filters = 128, kernel_size = 4, strides =1, padding = 'same',  ##\n                                   activation = 'relu', name='IMG_Conv3')(img_conv2)              ##\nimg_pool2 = tf.keras.layers.MaxPool2D(3, name = 'IMG_Pool2')(img_conv3)                           ##\nimg_conv4 = tf.keras.layers.Conv2D(filters = 256, kernel_size = 3, strides =1, padding = 'same',  ##\n                                   activation = 'relu', name='IMG_Conv4')(img_pool2)              ##\nimg_pool3 = tf.keras.layers.MaxPool2D(3, name = 'IMG_Pool3')(img_conv4)                           ##\nimg_conv5 = tf.keras.layers.Conv2D(filters = 512, kernel_size = 3, strides =1, padding = 'same',  ##\n                                   activation = 'relu', name='IMG_Conv5')(img_pool3)              ##\nimg_conv6 = tf.keras.layers.Conv2D(filters = 512, kernel_size = 3, strides =1, padding = 'same',  ##\n                                   activation = 'relu', name='IMG_Conv6')(img_conv5)              ##\nimg_pool4 = tf.keras.layers.MaxPool2D(2, name = 'IMG_Pool4')(img_conv6)                           ##\nimg_flatten = tf.keras.layers.Flatten(name = 'IMG_Flatten')(img_pool4)                            ##\nimg_dense1 = tf.keras.layers.Dense(3000, activation = 'relu', name='IMG_Dense1')(img_flatten)     ##\nimg_dropout1 = tf.keras.layers.Dropout(0.5, name='IMG_Dropout1')(img_dense1)                      ##\nimg_dense2 = tf.keras.layers.Dense(3000, activation = 'relu', name='IMG_Dense2')(img_dropout1)    ##\nimg_dropout2 = tf.keras.layers.Dropout(0.5, name='IMG_Dropout2')(img_dense2)                      ##\n####################################################################################################\n                                        ##\n                                        ##\n                                        ##\n##################### CSV FILE INPUT & IMG FILE OUTPUT LAYER ############\ncsv_output = tf.keras.layers.Dense(1, name = 'CSV_Output')(csv_hidden5)##\nimg_output = tf.keras.layers.Dense(1,name = 'IMG_Output')(img_dropout2)##\n#########################################################################\n                                        ##\n                                        ##\n                                        ##\n############################################# MODEL SETTING  ####################################################\nmodel = tf.keras.Model(inputs=[csv_input, img_input], outputs=[csv_output, img_output], name='Pythonash_model')##\n#################################################################################################################","metadata":{"execution":{"iopub.status.busy":"2021-12-27T13:23:35.219916Z","iopub.execute_input":"2021-12-27T13:23:35.220398Z","iopub.status.idle":"2021-12-27T13:23:35.758612Z","shell.execute_reply.started":"2021-12-27T13:23:35.220349Z","shell.execute_reply":"2021-12-27T13:23:35.753457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## model summary\n\nIt shows that your model structure, simply.","metadata":{}},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2021-12-27T13:23:52.779198Z","iopub.execute_input":"2021-12-27T13:23:52.779457Z","iopub.status.idle":"2021-12-27T13:23:52.796991Z","shell.execute_reply.started":"2021-12-27T13:23:52.779428Z","shell.execute_reply":"2021-12-27T13:23:52.796344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## plot your model\n\nThis plot shows your model and you can figure out you model structure, intuitively.\n\nYou can see that DNN model and CNN model are parallel and two output layer are in this structure.\n\nSo, you have to set each weight when you comile your model with the hyper parameter, \"loss_weights\".","metadata":{}},{"cell_type":"code","source":"# move you current directory to back.\nos.chdir('../')\nos.chdir('../')\nos.chdir('../')\ntf.keras.utils.plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True, rankdir='TB')","metadata":{"execution":{"iopub.status.busy":"2021-12-27T13:21:29.463951Z","iopub.execute_input":"2021-12-27T13:21:29.464213Z","iopub.status.idle":"2021-12-27T13:21:30.285705Z","shell.execute_reply.started":"2021-12-27T13:21:29.464186Z","shell.execute_reply":"2021-12-27T13:21:30.284924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Compile and fit your model.","metadata":{}},{"cell_type":"code","source":"opt = tf.keras.optimizers.Ftrl(learning_rate = 0.003, learning_rate_power = -0.5)\nmodel.compile(loss=['mse','mse'], loss_weights=[0.15, 0.85], optimizer = opt, metrics = ['mape'])\n\nepoch_number = 100\n\ncheck_1 = tf.keras.callbacks.ModelCheckpoint('pythonash_model.h5', save_best_only=True, verbose=2)","metadata":{"execution":{"iopub.status.busy":"2021-12-27T13:24:03.164377Z","iopub.execute_input":"2021-12-27T13:24:03.164936Z","iopub.status.idle":"2021-12-27T13:24:03.177245Z","shell.execute_reply.started":"2021-12-27T13:24:03.164897Z","shell.execute_reply":"2021-12-27T13:24:03.176526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit( \n    x= [train_csv_x, np.array(train_img)], y = [train_y, train_y], epochs=epoch_number, \n    validation_split=0.3, verbose =2, workers=3, batch_size = 15, validation_batch_size = 15,\n    callbacks = [check_1])","metadata":{"execution":{"iopub.status.busy":"2021-12-27T13:24:03.802953Z","iopub.execute_input":"2021-12-27T13:24:03.803406Z","iopub.status.idle":"2021-12-27T13:24:46.855136Z","shell.execute_reply.started":"2021-12-27T13:24:03.80337Z","shell.execute_reply":"2021-12-27T13:24:46.854388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Weighted result\n\nIn line 3, you can see the weighted result.\n\nIt is as follows in above loss weights.","metadata":{}},{"cell_type":"code","source":"best_model = tf.keras.models.load_model('pythonash_model.h5')\ncsv_result, img_result = best_model.predict([test_csv_x, np.array(test_img)])\nfinal_result = pd.DataFrame(0.15 * csv_result + 0.85 * img_result)\nfinal_result.columns =['Pawpularity']\nfinal_result","metadata":{"execution":{"iopub.status.busy":"2021-12-24T08:14:38.445694Z","iopub.execute_input":"2021-12-24T08:14:38.44589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submit your result.\n\nIt has done!\n\nI think it might be easy to you who already know about DNN and CNN.","metadata":{}},{"cell_type":"code","source":"for ids, paw in zip(test_csv_data['Id'], final_result['Pawpularity']):\n    location = submission[submission['Id'] == ids].index[0]\n    submission['Pawpularity'].loc[location] = paw\nsubmission\nsubmission.to_csv('./working/submission.csv',index=False)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# It's your turn!!\n\nYou have many opportunities that you can change this model parameters and get your submission score.\n\nI recommend that you change the hyper parameters such as learning_rate, batch_size, activation function, the number of neurons, layers, and so on...\n\nIf you get any helps from my notebook, please upvote!!\n\nFingers crossed!!","metadata":{}}]}