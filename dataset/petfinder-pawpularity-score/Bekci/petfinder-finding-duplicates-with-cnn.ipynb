{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **PetFinder-Finding Duplicates With CNN**","metadata":{}},{"cell_type":"markdown","source":"Inspired from [schulta's](https://www.kaggle.com/schulta) [work on identifying duplicates](https://www.kaggle.com/schulta/petfinder-identify-duplicates-and-share-findings/notebook), I tried to find the duplicate images with a pretrained CNN rather than image hashing. The reason for that is, CNNs are better at identifying same images when one of them is rotated, translated, etc.\n\n","metadata":{}},{"cell_type":"markdown","source":"# Method\n\nI used a pretrained EfficientnetB2 model with Imagenet weights as a feature extractor (I consider the features just after the Global Average Pooling layer). \n\nThen I calculated the cosine similarity between each images and visualize them.","metadata":{}},{"cell_type":"markdown","source":"# Findings\n\n* 37 images are exatcly same. This number is obtained when cosine similarity threshold is set to 0.9\n* Between 0.8-0.9 similarity threshold values, images from the same animal are captured. In this range images are very similar yet paw score of them differ much\n* Above showed that, scaling, rotating or applying similar augmentations may yield different paw scores. In my opinion, we should either discard one of these images or change our augmentation methods","metadata":{}},{"cell_type":"code","source":"!pip uninstall efficientnet\n!pip install -U git+https://github.com/qubvel/efficientnet","metadata":{"execution":{"iopub.status.busy":"2021-10-23T09:55:51.758051Z","iopub.execute_input":"2021-10-23T09:55:51.758336Z","iopub.status.idle":"2021-10-23T09:56:05.092018Z","shell.execute_reply.started":"2021-10-23T09:55:51.758262Z","shell.execute_reply":"2021-10-23T09:56:05.090934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfrom tensorflow.keras.models import Model\nfrom PIL import Image\nimport efficientnet.tfkeras as efn\nfrom efficientnet.tfkeras import preprocess_input\nimport math\nfrom tqdm import tqdm\nimport random\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2021-10-23T09:56:05.094318Z","iopub.execute_input":"2021-10-23T09:56:05.094624Z","iopub.status.idle":"2021-10-23T09:56:10.776465Z","shell.execute_reply.started":"2021-10-23T09:56:05.094584Z","shell.execute_reply":"2021-10-23T09:56:10.775714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_size(current_size, target_size):\n    \"\"\"\n    Calculates the size of the image when it is resized to target_size while keeping the aspect ratio.\n    Params:\n    current_size (tuple): Size of the current image\n    target_size (tuple):  Desired size of the image\n    Returns:\n    A calculated size which is most closest to target_size when aspect ratio is kept.\n    \"\"\"\n    w_ratio = target_size[0] / current_size[0]\n    h_ratio = target_size[1] / current_size[1]\n    scale = min(w_ratio, h_ratio)\n    return (int(scale * current_size[0]), int(scale * current_size[1]))\n\n\ndef pad_to_n(arr, new_dim):\n    \"\"\"\n    Apply a 2d padding on array to match its size to a new dimension.\n    Params:\n    arr (ndarray): Array which padding will be applied.\n    new_dim (tuple): Dimension after the padding.\n    Returns:\n    Padded input array\n    \"\"\"\n    if len(arr.shape)== 3:\n        h,w,c = arr.shape\n        x_new = np.zeros((new_dim[0], new_dim[1], c))\n        x_new[:h, :w, :c] = arr.copy()\n    else:\n        h,w = arr.shape\n        x_new = np.zeros((new_dim[0], new_dim[1]))\n        x_new[:h, :w] = arr.copy()\n    return x_new\n\ndef load_img(img_path, preprocess=None, target_size=None, same_aspect=False):\n    \"\"\"\n    Loads image file in given target size and applies preprocess to loaded file.\n    Params:\n    img_path: Path of the image file to be loaded.\n    preprocess: Preprocess function to apply the images. If passed as None then\n    no preprocess will be applied. \n    target_size: Size of the image to be loaded in (width, height). If passed \n    as None then the image will be loaded in its orginal size\n    same_aspect: Whether to keep the same aspect ratio while resizing.\n    If not none then target_size should provided.\n    Returns:\n    Preprocess applied loaded image in target size \n    \"\"\"\n    img = Image.open(img_path)\n    org_img_size = img.size\n  \n    if target_size is not None:\n        if same_aspect:\n            # When aspect ratio kept same, it may not be possbile to \n            # resize to a target size.\n            possible_size = calculate_size(org_img_size, target_size)\n            img = img.resize(possible_size)\n            img = np.array(img)\n            new_target_size = (target_size[1], target_size[0])\n            img = pad_to_n(img,  new_target_size).astype('uint8')\n        else:\n            img = img.resize(target_size)\n            img = np.array(img)\n    \n    if preprocess is not None:\n        img = preprocess(img)\n\n    return img","metadata":{"execution":{"iopub.status.busy":"2021-10-23T09:56:10.777909Z","iopub.execute_input":"2021-10-23T09:56:10.778159Z","iopub.status.idle":"2021-10-23T09:56:10.790781Z","shell.execute_reply.started":"2021-10-23T09:56:10.778124Z","shell.execute_reply":"2021-10-23T09:56:10.789326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Creation","metadata":{}},{"cell_type":"code","source":"model = efn.EfficientNetB2(weights='imagenet')\nfeature_layer = model.get_layer('avg_pool')\nfeature_extractor = Model(model.input, feature_layer.output)","metadata":{"execution":{"iopub.status.busy":"2021-10-23T09:56:10.793333Z","iopub.execute_input":"2021-10-23T09:56:10.793615Z","iopub.status.idle":"2021-10-23T09:56:16.202838Z","shell.execute_reply.started":"2021-10-23T09:56:10.793578Z","shell.execute_reply":"2021-10-23T09:56:16.201945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_imgs_dir = '/kaggle/input/petfinder-pawpularity-score/train/'\ntrain_df = pd.read_csv('/kaggle/input/petfinder-pawpularity-score/train.csv')\nN_EXAMPLES = len(train_df)","metadata":{"execution":{"iopub.status.busy":"2021-10-23T09:56:16.204136Z","iopub.execute_input":"2021-10-23T09:56:16.204393Z","iopub.status.idle":"2021-10-23T09:56:16.238203Z","shell.execute_reply.started":"2021-10-23T09:56:16.204357Z","shell.execute_reply":"2021-10-23T09:56:16.23755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = []\npaw_scores = []\nimg_paths = []\nimage_ids = []\nbatch_size = 24\niter_num = math.ceil(len(train_df) / batch_size)","metadata":{"execution":{"iopub.status.busy":"2021-10-23T09:56:16.239502Z","iopub.execute_input":"2021-10-23T09:56:16.239857Z","iopub.status.idle":"2021-10-23T09:56:16.244597Z","shell.execute_reply.started":"2021-10-23T09:56:16.239817Z","shell.execute_reply":"2021-10-23T09:56:16.243898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in tqdm(range(iter_num)) :\n    batch_df = train_df.iloc[i*batch_size:(i+1)*batch_size]\n    batch_paths = [os.path.join(train_imgs_dir, pid + '.jpg') for pid in batch_df['Id']]\n    batch_paws = batch_df['Pawpularity'].tolist()\n    batch_ids = batch_df['Id'].tolist()\n    batch_imgs = np.array([load_img(img_path, preprocess=preprocess_input, target_size=(260, 260), same_aspect=False) for img_path in batch_paths])\n    \n    batch_features = feature_extractor.predict(batch_imgs)\n    features.append(batch_features)\n    paw_scores.extend(batch_paws)\n    img_paths.extend(batch_paths)\n    image_ids.extend(batch_ids)","metadata":{"execution":{"iopub.status.busy":"2021-10-23T09:56:16.246215Z","iopub.execute_input":"2021-10-23T09:56:16.24673Z","iopub.status.idle":"2021-10-23T10:00:45.984317Z","shell.execute_reply.started":"2021-10-23T09:56:16.246694Z","shell.execute_reply":"2021-10-23T10:00:45.983586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = np.array(features)\n# Rearange batch dimension\nnp_features = features.reshape(features.shape[0] * features.shape[1], features.shape[2])\npaw_scores = np.array(paw_scores)\nimg_paths = np.array(img_paths)","metadata":{"execution":{"iopub.status.busy":"2021-10-23T10:00:45.985901Z","iopub.execute_input":"2021-10-23T10:00:45.986617Z","iopub.status.idle":"2021-10-23T10:00:46.010957Z","shell.execute_reply.started":"2021-10-23T10:00:45.986574Z","shell.execute_reply":"2021-10-23T10:00:46.010127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_similar_pairs(xs, ys):\n    \"\"\"\n    Plots similar pairs given their index lists\n    Params:\n        xs: Similar item index list\n        ys: Similar item index list. Each item must be correspond to its pair in parameter xs\n    \"\"\"    \n    for x,y in zip(xs,ys):\n        img_path_1 = img_paths[x]\n        img_path_2 = img_paths[y]\n        img1 = load_img(img_path_1, preprocess=None, target_size=None, same_aspect=False)\n        img2 = load_img(img_path_2, preprocess=None, target_size=None, same_aspect=False)\n        paw_score_1 = paw_scores[x]\n        paw_score_2 = paw_scores[y]\n\n        fig, ax = plt.subplots(1, 2)\n        ax[0].imshow(img1)\n        ax[0].set_title(\"Paw Score: {:.2f}\".format(paw_score_1))\n        ax[1].imshow(img2)\n        ax[1].set_title(\"Paw Score: {:.2f}\".format(paw_score_2))\n\n        for j in range(2):\n            ax[j].set_xticks([])\n            ax[j].set_yticks([])\n\n        plt.show()    ","metadata":{"execution":{"iopub.status.busy":"2021-10-23T10:00:46.012415Z","iopub.execute_input":"2021-10-23T10:00:46.012689Z","iopub.status.idle":"2021-10-23T10:00:46.020589Z","shell.execute_reply.started":"2021-10-23T10:00:46.012655Z","shell.execute_reply":"2021-10-23T10:00:46.019752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics.pairwise import cosine_similarity\n\ncos_similarity = cosine_similarity(np_features)","metadata":{"execution":{"iopub.status.busy":"2021-10-23T10:00:46.023441Z","iopub.execute_input":"2021-10-23T10:00:46.023894Z","iopub.status.idle":"2021-10-23T10:00:47.381813Z","shell.execute_reply.started":"2021-10-23T10:00:46.02386Z","shell.execute_reply":"2021-10-23T10:00:47.381039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Diagonal entries are similarity scores between same images\n# Make them 0 to not include them later\ncos_similarity[np.eye(N_EXAMPLES, dtype='bool')] = 0","metadata":{"execution":{"iopub.status.busy":"2021-10-23T10:00:47.383231Z","iopub.execute_input":"2021-10-23T10:00:47.383488Z","iopub.status.idle":"2021-10-23T10:00:47.424304Z","shell.execute_reply.started":"2021-10-23T10:00:47.383452Z","shell.execute_reply":"2021-10-23T10:00:47.42357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Samples With Similarity > 0.9","metadata":{}},{"cell_type":"code","source":"SIM_THRES = 0.9\nsim_mask = np.where(cos_similarity > SIM_THRES, True, False)\n# Consider upper half of the similarity mask to remove duplicate similar pairs\nhalf_sim_mask = np.triu(sim_mask)\nxs, ys = np.where(half_sim_mask)","metadata":{"execution":{"iopub.status.busy":"2021-10-15T09:06:48.322396Z","iopub.execute_input":"2021-10-15T09:06:48.322651Z","iopub.status.idle":"2021-10-15T09:06:48.793976Z","shell.execute_reply.started":"2021-10-15T09:06:48.322623Z","shell.execute_reply":"2021-10-15T09:06:48.793235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_similar_pairs(xs, ys)","metadata":{"execution":{"iopub.status.busy":"2021-10-15T09:06:48.79542Z","iopub.execute_input":"2021-10-15T09:06:48.795758Z","iopub.status.idle":"2021-10-15T09:06:57.656554Z","shell.execute_reply.started":"2021-10-15T09:06:48.795719Z","shell.execute_reply":"2021-10-15T09:06:57.655839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Samples With Similarity Between 0.85-0.9","metadata":{}},{"cell_type":"code","source":"UP_SIM_THRES = 0.9\nLOW_SIM_THRES = 0.85\nsim_mask_low = np.where(cos_similarity > LOW_SIM_THRES, True, False)\nsim_mask_up = np.where(cos_similarity < UP_SIM_THRES, True, False)\nsim_mask = np.logical_and(sim_mask_low, sim_mask_up)\n# Consider upper half of the similarity mask to remove duplicate similar pairs\nhalf_sim_mask = np.triu(sim_mask)\nxs, ys = np.where(half_sim_mask)","metadata":{"execution":{"iopub.status.busy":"2021-10-23T10:04:30.30299Z","iopub.execute_input":"2021-10-23T10:04:30.303682Z","iopub.status.idle":"2021-10-23T10:04:31.008191Z","shell.execute_reply.started":"2021-10-23T10:04:30.303642Z","shell.execute_reply":"2021-10-23T10:04:31.00743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_similar_pairs(xs, ys)","metadata":{"execution":{"iopub.status.busy":"2021-10-15T09:07:47.797825Z","iopub.execute_input":"2021-10-15T09:07:47.798169Z","iopub.status.idle":"2021-10-15T09:07:50.877517Z","shell.execute_reply.started":"2021-10-15T09:07:47.798116Z","shell.execute_reply":"2021-10-15T09:07:50.8767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"UP_SIM_THRES = 1.01\nLOW_SIM_THRES = 0.77\nsim_mask_low = np.where(cos_similarity > LOW_SIM_THRES, True, False)\nsim_mask_up = np.where(cos_similarity < UP_SIM_THRES, True, False)\nsim_mask = np.logical_and(sim_mask_low, sim_mask_up)\n# Consider upper half of the similarity mask to remove duplicate similar pairs\nhalf_sim_mask = np.triu(sim_mask)\nxs, ys = np.where(half_sim_mask)","metadata":{"execution":{"iopub.status.busy":"2021-10-23T10:05:55.711248Z","iopub.execute_input":"2021-10-23T10:05:55.711508Z","iopub.status.idle":"2021-10-23T10:05:56.405862Z","shell.execute_reply.started":"2021-10-23T10:05:55.711474Z","shell.execute_reply":"2021-10-23T10:05:56.405099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"to_delete_indx = np.unique(np.concatenate([xs, ys]))\nprint(\"There are {} images to be deleted\".format(len(to_delete_indx)))\nto_delete_img_ids = [image_ids[idx] for idx in to_delete_indx]","metadata":{"execution":{"iopub.status.busy":"2021-10-23T10:08:26.83408Z","iopub.execute_input":"2021-10-23T10:08:26.834726Z","iopub.status.idle":"2021-10-23T10:08:26.839458Z","shell.execute_reply.started":"2021-10-23T10:08:26.834683Z","shell.execute_reply":"2021-10-23T10:08:26.838727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cleaned_df = train_df[train_df['Id'].apply(lambda x: x not in to_delete_img_ids)]\ncleaned_df.to_csv('duplicate_removed_train.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-10-23T10:10:26.577803Z","iopub.execute_input":"2021-10-23T10:10:26.578217Z","iopub.status.idle":"2021-10-23T10:10:26.740696Z","shell.execute_reply.started":"2021-10-23T10:10:26.578176Z","shell.execute_reply":"2021-10-23T10:10:26.739958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}