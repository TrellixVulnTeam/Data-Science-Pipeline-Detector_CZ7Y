{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# PetFinder PoolFormer PyTorch Trainer + PoolFormer + fp16 + Mixup + KFolds + W&B Tracking ✨\n\n<!-- ![](https://d3tfjnq35srlo8.cloudfront.net/uploads/2020/11/0061269_PETFINDER-FOUNDATION-SUB-PAGE-02.jpg)\n -->\n![](https://user-images.githubusercontent.com/15921929/142746124-1ab7635d-2536-4a0e-ad43-b4fe2c5a525d.png)\n\nThis notebook features a modular PyTorch Trainer with Support for CutMix augmentation along with Apex. The model I'm training here is PoolFormer, which was announced recently in [this](https://arxiv.org/abs/2111.11418) paper.\n\nI've tried my best to make this PyTorch training script as modular and fault-tolerant as possible and it doesn't generally throw errors or breaks, should you forget to pass an argument or two.\n\nThen again, if there's some bug or improvements that you notice, please do tell me in the comments and I'll have them fixed in the next commit.","metadata":{}},{"cell_type":"markdown","source":"### Please leave an upvote if you found this kernel helpful!","metadata":{}},{"cell_type":"code","source":"%%sh\npip install -q wandb\ngit clone --quiet https://github.com/sail-sg/poolformer.git\npip install -q git+https://github.com/rwightman/pytorch-image-models.git@9d6aad44f8fd32e89e5cca503efe3ada5071cc2a","metadata":{"id":"-JO6jg8LrzbZ","papermill":{"duration":38.86128,"end_time":"2021-11-22T12:55:06.244959","exception":false,"start_time":"2021-11-22T12:54:27.383679","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-27T12:24:33.967517Z","iopub.execute_input":"2021-11-27T12:24:33.968227Z","iopub.status.idle":"2021-11-27T12:24:54.893688Z","shell.execute_reply.started":"2021-11-27T12:24:33.968136Z","shell.execute_reply":"2021-11-27T12:24:54.892744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nsys.path.append('./poolformer')","metadata":{"execution":{"iopub.status.busy":"2021-11-27T12:24:54.895946Z","iopub.execute_input":"2021-11-27T12:24:54.896469Z","iopub.status.idle":"2021-11-27T12:24:54.900945Z","shell.execute_reply.started":"2021-11-27T12:24:54.896426Z","shell.execute_reply":"2021-11-27T12:24:54.899613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport os\nimport cv2\nimport timm\nfrom timm.models import load_checkpoint\n\nimport models\nfrom PIL import Image\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torchvision.transforms as T\nfrom torch.cuda.amp import autocast, GradScaler\nfrom torch.utils.data import Dataset, DataLoader\n\nimport gc\nimport wandb\nimport warnings\nfrom tqdm.notebook import tqdm\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold\n\nwarnings.simplefilter('ignore')","metadata":{"id":"rK-YtcAWrr5U","papermill":{"duration":8.459847,"end_time":"2021-11-22T12:55:14.716131","exception":false,"start_time":"2021-11-22T12:55:06.256284","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-27T12:24:54.902288Z","iopub.execute_input":"2021-11-27T12:24:54.902759Z","iopub.status.idle":"2021-11-27T12:25:03.007815Z","shell.execute_reply.started":"2021-11-27T12:24:54.902692Z","shell.execute_reply":"2021-11-27T12:25:03.006915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Config = {\n    'CSV_PATH': \"../input/petfinder-pawpularity-score/train.csv\",\n    'IMG_PATH': \"../input/petfinder-pawpularity-score/train\",\n    'N_ACCUM': 2,\n    'N_SPLITS': 5,\n    'TRAIN_BS': 64,\n    'VALID_BS': 64,\n    'N_EPOCHS': 5,\n    'NUM_WORKERS': 4,\n    'LR': 1e-5,\n    'OPTIM': \"AdamW\",\n    'LOSS': \"BCELogits\",\n    'ARCH': \"../input/poolformerweights/poolformer_m36.pth.tar\",\n    'IMG_SIZE': 224,\n    'DEVICE': \"cuda\",\n    \"T_0\": 20,\n    \"η_min\": 1e-4,\n    'infra': \"Kaggle\",\n    'competition': 'petfinder',\n    '_wandb_kernel': 'tanaym',\n    \"wandb\": True,\n}","metadata":{"id":"LyoL1zR_stux","papermill":{"duration":0.018392,"end_time":"2021-11-22T12:55:14.802359","exception":false,"start_time":"2021-11-22T12:55:14.783967","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-27T12:25:03.010484Z","iopub.execute_input":"2021-11-27T12:25:03.010951Z","iopub.status.idle":"2021-11-27T12:25:03.017483Z","shell.execute_reply.started":"2021-11-27T12:25:03.010908Z","shell.execute_reply":"2021-11-27T12:25:03.016231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## About W&B:\n<center><img src=\"https://i.imgur.com/gb6B4ig.png\" width=\"400\" alt=\"Weights & Biases\"/></center><br>\n<p style=\"text-align:center\">WandB is a developer tool for companies turn deep learning research projects into deployed software by helping teams track their models, visualize model performance and easily automate training and improving models.\nWe will use their tools to log hyperparameters and output metrics from your runs, then visualize and compare results and quickly share findings with your colleagues.<br><br></p>","metadata":{}},{"cell_type":"markdown","source":"To login to W&B, you can use below snippet.\n\n```python\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nwb_key = user_secrets.get_secret(\"WANDB_API_KEY\")\n\nwandb.login(key=wb_key)\n```\nMake sure you have your W&B key stored as `WANDB_API_KEY` under Add-ons -> Secrets\n\nYou can view [this](https://www.kaggle.com/ayuraj/experiment-tracking-with-weights-and-biases) notebook to learn more about W&B tracking.\n\nIf you don't want to login to W&B, the kernel will still work and log everything to W&B in anonymous mode.","metadata":{}},{"cell_type":"code","source":"# Start W&B logging\nif Config['wandb']:\n    run = wandb.init(\n        project='pytorch',\n        config=Config,\n        group='vision',\n        job_type='train',\n        anonymous='must'\n    )","metadata":{"execution":{"iopub.status.busy":"2021-11-27T12:25:03.01882Z","iopub.execute_input":"2021-11-27T12:25:03.019703Z","iopub.status.idle":"2021-11-27T12:25:10.260273Z","shell.execute_reply.started":"2021-11-27T12:25:03.019664Z","shell.execute_reply":"2021-11-27T12:25:10.259503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Some utility functions\ndef wandb_log(**kwargs):\n    \"\"\"\n    Logs a key-value pair to W&B\n    \"\"\"\n    for k, v in kwargs.items():\n        wandb.log({k: v})\n\ndef rmse(output, target):\n    \"\"\"\n    Returns root mean squared error loss\n    \"\"\"\n    return mean_squared_error(output, target, squared=False)","metadata":{"id":"TcKg0-K8c_OK","papermill":{"duration":0.017684,"end_time":"2021-11-22T12:55:14.773075","exception":false,"start_time":"2021-11-22T12:55:14.755391","status":"completed"},"tags":[],"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-27T12:25:10.26159Z","iopub.execute_input":"2021-11-27T12:25:10.261857Z","iopub.status.idle":"2021-11-27T12:25:10.278891Z","shell.execute_reply.started":"2021-11-27T12:25:10.261817Z","shell.execute_reply":"2021-11-27T12:25:10.278258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 align='center' style='color: #8532a8; font-size: 1.5em; font-weight: 300; font-size: 32px'>1. Dataset Class</h1>","metadata":{}},{"cell_type":"code","source":"class PetfinderData(Dataset):\n    def __init__(self, df, config=Config, augments=None, is_test=False):\n        self.df = df\n        self.augments = augments\n        self.is_test = is_test\n        self.config = config\n        \n        self.img_paths = self._get_img_paths(self.df, self.config)\n        self.meta_feats = self._get_meta_feats(self.df, self.is_test)\n\n    def __getitem__(self, idx):\n        img = cv2.imread(self.img_paths[idx])\n        img = cv2.resize(img, (Config['IMG_SIZE'], Config['IMG_SIZE']))\n        meta_feats = torch.tensor(self.meta_feats.iloc[idx].values).float()\n\n        if self.augments:\n            img = Image.fromarray(img)\n            img = self.augments(img)\n        \n        if self.is_test:\n            return (img, meta_feats)\n        else:\n            target = torch.tensor(self.df['Pawpularity'].iloc[idx]).float()\n            return (img, meta_feats, target)\n    \n    def __len__(self):\n        return len(self.df)\n\n    def _get_img_paths(self, df, config):\n        \"\"\"\n        Returns the image paths in a list\n        \"\"\"\n        imgs = df['Id'].apply(lambda x: os.path.join(config['IMG_PATH'], x + \".jpg\")).tolist()\n        return imgs\n    \n    def _get_meta_feats(self, df, is_test):\n        \"\"\"\n        Returns the meta features in a df\n        \"\"\"\n        if self.is_test:\n            meta = self.df.drop(['Id'], axis=1)\n            return meta\n        else:\n            meta = self.df.drop(['Id', 'Pawpularity'], axis=1)\n            return meta","metadata":{"id":"V2kmmhnisUgC","papermill":{"duration":0.023961,"end_time":"2021-11-22T12:55:14.837033","exception":false,"start_time":"2021-11-22T12:55:14.813072","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-27T12:25:10.283315Z","iopub.execute_input":"2021-11-27T12:25:10.285249Z","iopub.status.idle":"2021-11-27T12:25:10.318653Z","shell.execute_reply.started":"2021-11-27T12:25:10.285213Z","shell.execute_reply":"2021-11-27T12:25:10.318002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 align='center' style='color: #8532a8; font-size: 1.5em; font-weight: 300; font-size: 32px'>2. Model Class</h1>","metadata":{}},{"cell_type":"markdown","source":"## About PoolFormer (MetaFormer)\n\nRecent works have shown that the Attention-based module in transformers can be replaced by spatial MLPs and the resulted models still perform quite well. Based on this observation, it is hypothesized that the general architecture of the transformers, instead of the specific token mixer module, is more essential to the model’s performance. \n\nTo verify this, the attention module in transformers is replaced with a simple spatial pooling operator to conduct only the most basic token mixing. The derived model, termed as **PoolFormer**, achieves competitive performance on multiple computer vision tasks.\n\nLink to the [original paper](https://arxiv.org/pdf/2111.11418.pdf) and their GitHub [repo](https://github.com/sail-sg/poolformer).","metadata":{}},{"cell_type":"code","source":"class RegressionHeadModel(nn.Module):\n    def __init__(self, backbone_arch, pretrained=True, in_chans=3):\n        super(RegressionHeadModel, self).__init__()\n        self.backbone = models.poolformer_m36(pretrained=pretrained)\n        load_checkpoint(model=self.backbone, checkpoint_path=backbone_arch)\n        self.backbone.head = nn.Linear(self.backbone.head.in_features, 128)\n        self.drop = nn.Dropout(0.3)\n        self.fc1 = nn.Linear(140, 64)\n        self.fc2 = nn.Linear(64, 32)\n        self.fc3 = nn.Linear(32, 1)\n    \n    def forward(self, img, meta):\n        emb = self.backbone(img)\n        x = self.drop(emb)\n        x = torch.cat([x, meta], dim=1)\n        x = self.fc1(x)\n        x = self.fc2(x)\n        x = self.fc3(x)\n\n        return x","metadata":{"id":"aIys8hdbv47i","papermill":{"duration":0.034265,"end_time":"2021-11-22T12:55:14.881725","exception":false,"start_time":"2021-11-22T12:55:14.84746","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-27T12:25:10.322961Z","iopub.execute_input":"2021-11-27T12:25:10.325294Z","iopub.status.idle":"2021-11-27T12:25:10.341611Z","shell.execute_reply.started":"2021-11-27T12:25:10.325243Z","shell.execute_reply":"2021-11-27T12:25:10.340272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 align='center' style='color: #8532a8; font-size: 1.5em; font-weight: 300; font-size: 32px'>3. Mixup and Augmentations</h1>","metadata":{}},{"cell_type":"code","source":"def mixup_augmentation(x:torch.Tensor, y:torch.Tensor, alpha:float = 1.0):\n    \"\"\"\n    Function which performs Mixup augmentation\n    \"\"\"\n    assert alpha > 0, \"Alpha must be greater than 0\"\n    assert x.shape[0] > 1, \"Need more than 1 sample to apply mixup\"\n\n    lam = np.random.beta(alpha, alpha)\n    rand_idx = torch.randperm(x.shape[0])\n    mixed_x = lam * x + (1 - lam) * x[rand_idx, :]\n\n    target_a, target_b = y, y[rand_idx]\n\n    return mixed_x, target_a, target_b, lam","metadata":{"id":"cTBi1vKPs3j8","papermill":{"duration":0.031065,"end_time":"2021-11-22T12:55:14.931026","exception":false,"start_time":"2021-11-22T12:55:14.899961","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-27T12:25:10.345523Z","iopub.execute_input":"2021-11-27T12:25:10.353531Z","iopub.status.idle":"2021-11-27T12:25:10.378055Z","shell.execute_reply.started":"2021-11-27T12:25:10.353492Z","shell.execute_reply":"2021-11-27T12:25:10.376048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Augments:\n    IMAGENET_MEAN = [0.485, 0.456, 0.406]  # RGB\n    IMAGENET_STD = [0.229, 0.224, 0.225]  # RGB\n    train_augments = T.Compose(\n            [\n                T.RandomHorizontalFlip(),\n                T.RandomVerticalFlip(),\n                T.RandomAffine(15, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n                T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n                T.ToTensor(),\n                T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n            ]\n        )\n    valid_augments = T.Compose(\n            [\n                T.ToTensor(),\n                T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n            ]\n        )","metadata":{"id":"yzzrD7XugseU","papermill":{"duration":0.032324,"end_time":"2021-11-22T12:55:15.08286","exception":false,"start_time":"2021-11-22T12:55:15.050536","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-27T12:25:10.38446Z","iopub.execute_input":"2021-11-27T12:25:10.385017Z","iopub.status.idle":"2021-11-27T12:25:10.410861Z","shell.execute_reply.started":"2021-11-27T12:25:10.384981Z","shell.execute_reply":"2021-11-27T12:25:10.408621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 align='center' style='color: #8532a8; font-size: 1.5em; font-weight: 300; font-size: 32px'>4. Trainer Class</h1>\n\n<center>I've decided to take my old signature trainer class and add more utilities and functions to it in order to make it much better, efficient and flexible.</center>","metadata":{}},{"cell_type":"code","source":"class Trainer:\n    def __init__(self, config, dataloaders, optimizer, model, loss_fns, scheduler, device=\"cuda:0\", apex=False):\n        self.train_loader, self.valid_loader = dataloaders\n        self.train_loss_fn, self.valid_loss_fn = loss_fns\n        self.scheduler = scheduler\n        self.optimizer = optimizer\n        self.model = model\n        self.device = torch.device(device)\n        self.apex = apex\n        self.config = config\n\n    def train_one_epoch(self):\n        \"\"\"\n        Trains the model for 1 epoch\n        \"\"\"\n        if self.apex:\n            scaler = GradScaler()\n\n        self.model.train()\n        train_pbar = tqdm(enumerate(self.train_loader), total=len(self.train_loader))\n        train_preds, train_labels = [], []\n        running_loss = 0\n\n        for bnum, data_cache in train_pbar:\n            img = self._convert_if_not_tensor(data_cache[0], dtype=torch.float32)\n            meta = self._convert_if_not_tensor(data_cache[1], dtype=torch.float32)\n            target = self._convert_if_not_tensor(data_cache[2], dtype=torch.float32)\n            target = target / 100.0\n\n            bs = img.shape[0]\n\n            # Support of Apex with Mixup 🛠️\n            if self.apex:\n                # Mixup - allowed\n                if torch.randn(1)[0] < 0.5:\n                    mix_img, tar_a, tar_b, lam = mixup_augmentation(img, target, alpha=0.5)\n                    \n                    with autocast(enabled=True):\n                        output = self.model(mix_img, meta).squeeze()\n                        \n                        # Mixup loss calculation\n                        loss_a = self.train_loss_fn(output, tar_a)\n                        loss_b = self.train_loss_fn(output, tar_b)\n                        loss = loss_a * lam + (1 - lam) * loss_b\n\n                        loss = loss / self.config['N_ACCUM']\n                    scaler.scale(loss).backward()\n\n                    if (bnum + 1) % self.config['N_ACCUM'] == 0:\n                        scaler.step(self.optimizer)\n                        scaler.update()\n                        optimizer.zero_grad()\n\n                        if self.scheduler:\n                            self.scheduler.step()\n                    running_loss += (loss.item() * bs)\n                \n                # Mixup - not allowed\n                else:\n                    with autocast(enabled=True):\n                        output = self.model(img, meta).squeeze()\n                        loss = self.train_loss_fn(output, target)\n                        loss = loss / self.config['N_ACCUM']\n                    scaler.scale(loss).backward()\n\n                    if (bnum + 1) % self.config['N_ACCUM'] == 0:\n                        scaler.step(self.optimizer)\n                        scaler.update()\n                        optimizer.zero_grad()\n\n                        if self.scheduler:\n                            self.scheduler.step()\n                    running_loss += (loss.item() * bs)\n            # No Apex\n            else:\n                # Mixup - allowed\n                if torch.randn(1)[0] < 0.5:\n                    mix_img, tar_a, tar_b, lam = mixup_augmentation(img, target, alpha=0.5)\n                    output = self.model(mix_img, meta).squeeze()\n                    \n                    # Mixup loss calculation\n                    loss_a = self.train_loss_fn(output, tar_a)\n                    loss_b = self.train_loss_fn(output, tar_b)\n                    loss = loss_a * lam + (1 - lam) * loss_b\n                    \n                    loss = loss / self.config['N_ACCUM']\n                    loss.backward()\n\n                    if (bnum + 1) % self.config['N_ACCUM'] == 0:\n                        self.optimizer.step()\n                        optimizer.zero_grad()\n\n                        if self.scheduler:\n                            self.scheduler.step()\n                    running_loss += (loss.item() * bs)\n                \n                # Mixup - not allowed\n                else:\n                    output = self.model(img, meta).squeeze()\n                    loss = self.train_loss_fn(output, target)\n                    loss = loss / self.config['N_ACCUM']\n                    loss.backward()\n\n                    if (bnum + 1) % self.config['N_ACCUM'] == 0:\n                        self.optimizer.step()\n                        optimizer.zero_grad()\n\n                        if self.scheduler:\n                            self.scheduler.step()\n                    running_loss += (loss.item() * bs)\n\n            train_pbar.set_description(desc=f\"loss: {loss.item():.4f}\")\n            running_loss /= len(self.train_loader)\n\n            # Rescale the targets and output before chugging in a matrix\n            output = output.sigmoid().detach() * 100.0\n            target = target.detach() * 100.0\n            train_preds += [output.cpu().numpy()]\n            train_labels += [target.cpu().numpy()]\n        \n        all_train_preds = np.concatenate(train_preds)\n        all_train_labels = np.concatenate(train_labels)\n        \n        # Tidy\n        del output, target, train_preds, train_labels, loss, img, meta, all_train_preds, all_train_labels\n        gc.collect()\n        torch.cuda.empty_cache()\n        \n        return running_loss\n\n    @torch.no_grad()\n    def valid_one_epoch(self):\n        \"\"\"\n        Validates the model for 1 epoch\n        \"\"\"\n        self.model.eval()\n        valid_pbar = tqdm(enumerate(self.valid_loader), total=len(self.valid_loader))\n        valid_preds, valid_targets = [], []\n\n        for idx, cache in valid_pbar:\n            img = self._convert_if_not_tensor(cache[0], dtype=torch.float32)\n            meta = self._convert_if_not_tensor(cache[1], dtype=torch.float32)\n            target = self._convert_if_not_tensor(cache[2], dtype=torch.float32)\n            target = target / 100.0\n\n            output = self.model(img, meta).squeeze()\n            valid_loss = torch.sqrt(self.valid_loss_fn(output, target))\n\n            valid_pbar.set_description(desc=f\"val_loss: {valid_loss.item():.4f}\")\n\n            output = output.sigmoid().detach() * 100.0\n            target = target.detach() * 100.0\n\n            valid_preds += [output.cpu().numpy()]\n            valid_targets += [target.cpu().numpy()]\n\n        all_valid_preds = np.concatenate(valid_preds)\n        all_valid_targets = np.concatenate(valid_targets)\n\n        total_valid_loss = rmse(all_valid_targets, all_valid_preds)\n        \n        # Tidy\n        del img, meta, target, valid_preds, valid_targets, all_valid_targets, output, valid_loss\n        gc.collect()\n        torch.cuda.empty_cache()\n        \n        return total_valid_loss, all_valid_preds\n\n    def fit(self, fold: str, epochs: int = 10, output_dir: str = \"/kaggle/working/\", custom_name: str = 'model.pth'):\n        \"\"\"\n        Low-effort alternative for doing the complete training and validation process\n        \"\"\"\n        best_loss = int(1e+7)\n        best_preds = None\n        for epx in range(epochs):\n            print(f\"{'='*20} Epoch: {epx+1} / {epochs} {'='*20}\")\n\n            train_running_loss = self.train_one_epoch()\n            print(f\"Training loss: {train_running_loss:.4f}\")\n\n            valid_loss, preds = self.valid_one_epoch()\n            print(f\"Validation loss: {valid_loss:.4f}\")\n\n            if valid_loss < best_loss:\n                best_loss = valid_loss\n                self.save_model(output_dir, custom_name)\n                print(f\"Saved model with val_loss: {best_loss:.4f}\")\n                best_preds = preds\n            \n            # Log\n            if Config['wandb']:\n                wandb_log(\n                    train_loss=train_running_loss,\n                    val_loss=valid_loss\n                )\n        return best_preds\n            \n    def save_model(self, path, name, verbose=False):\n        \"\"\"\n        Saves the model at the provided destination\n        \"\"\"\n        try:\n            if not os.path.exists(path):\n                os.makedirs(path)\n        except:\n            print(\"Errors encountered while making the output directory\")\n\n        torch.save(self.model.state_dict(), os.path.join(path, name))\n        if verbose:\n            print(f\"Model Saved at: {os.path.join(path, name)}\")\n\n    def _convert_if_not_tensor(self, x, dtype):\n        if self._tensor_check(x):\n            return x.to(self.device, dtype=dtype)\n        else:\n            return torch.tensor(x, dtype=dtype, device=self.device)\n\n    def _tensor_check(self, x):\n        return isinstance(x, torch.Tensor)","metadata":{"id":"iqGwtdDZANNf","papermill":{"duration":0.080994,"end_time":"2021-11-22T12:55:15.030222","exception":false,"start_time":"2021-11-22T12:55:14.949228","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-27T12:25:10.415667Z","iopub.execute_input":"2021-11-27T12:25:10.415982Z","iopub.status.idle":"2021-11-27T12:25:10.570758Z","shell.execute_reply.started":"2021-11-27T12:25:10.415948Z","shell.execute_reply":"2021-11-27T12:25:10.568418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 align='center' style='color: #8532a8; font-size: 1.5em; font-weight: 300; font-size: 32px'>5. Training Cell</h1>\n<center>\n    The below cell is where the main training happens.\n</center>","metadata":{}},{"cell_type":"code","source":"if __name__ == '__main__':\n    kf = StratifiedKFold(n_splits=Config['N_SPLITS'])\n    train_file = pd.read_csv(Config['CSV_PATH'])\n    \n    for fold_, (train_idx, valid_idx) in enumerate(kf.split(X=train_file, y=train_file['Pawpularity'])):\n        print(f\"{'='*40} Fold: {fold_+1} / {Config['N_SPLITS']} {'='*40}\")\n        \n        train_ = train_file.loc[train_idx]\n        valid_ = train_file.loc[valid_idx]\n        \n        train_set = PetfinderData(\n            df = train_,\n            config = Config,\n            augments = Augments.train_augments\n        )\n        valid_set = PetfinderData(\n            df = valid_,\n            config = Config,\n            augments = Augments.valid_augments\n        )\n        \n        train_loader = DataLoader(\n            train_set,\n            batch_size = Config['TRAIN_BS'],\n            shuffle = True,\n            num_workers = Config['NUM_WORKERS'],\n            pin_memory = True\n        )\n        \n        valid_loader = DataLoader(\n            valid_set,\n            batch_size = Config['VALID_BS'],\n            shuffle = False,\n            num_workers = Config['NUM_WORKERS'],\n        )\n        \n        model = RegressionHeadModel(backbone_arch=Config['ARCH'])\n        model = model.to(torch.device(Config['DEVICE']))\n        if Config['wandb']:\n            wandb.watch(model)\n            \n        optimizer = torch.optim.AdamW(model.parameters(), lr=Config['LR'])\n        scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n            optimizer, \n            T_0=Config['T_0'], \n            eta_min=Config['η_min']\n        )\n        train_lfn, valid_lfn = nn.BCEWithLogitsLoss(), nn.BCEWithLogitsLoss()\n        \n        trainer = Trainer(\n            config = Config,\n            dataloaders=(train_loader, valid_loader),\n            loss_fns=(train_lfn, valid_lfn),\n            optimizer=optimizer,\n            model = model,\n            scheduler=scheduler,\n            apex=True\n        )\n        \n        best_pred = trainer.fit(\n            fold = fold_,\n            epochs = Config['N_EPOCHS'],\n            custom_name = f\"poolformer_s36_fold_{fold_}_model.bin\"\n        )\n        \n        valid_['preds'] = best_pred\n        valid_.to_csv(f\"fold_{fold_}_oof_df.csv\", index=None)","metadata":{"papermill":{"duration":20562.426512,"end_time":"2021-11-22T18:37:57.526652","exception":false,"start_time":"2021-11-22T12:55:15.10014","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-27T12:25:10.57359Z","iopub.execute_input":"2021-11-27T12:25:10.578058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Code taken from https://www.kaggle.com/ayuraj/interactive-eda-using-w-b-tables\n\n# Finish the logging run\nif Config['wandb']:\n    run.finish()","metadata":{"papermill":{"duration":0.063991,"end_time":"2021-11-22T18:37:57.656342","exception":false,"start_time":"2021-11-22T18:37:57.592351","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<center>\n<img src=\"https://img.shields.io/badge/Upvote-If%20you%20like%20my%20work-07b3c8?style=for-the-badge&logo=kaggle\">\n</center>","metadata":{}}]}