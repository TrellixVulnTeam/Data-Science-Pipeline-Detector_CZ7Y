{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ENV = \"COLAB\"\nENV = \"KAGGLE\"\n\nVAL_SPLIT = 0.1\n\nIMG_SIZE = 128\n\nTRAIN_ONLY_CLASSIFIER_FOR_EPOCHS = 5\nN_EPOCHS = 5\n\nBATCH_SIZE = 64","metadata":{"id":"ndlQOCumMTHv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls /kaggle/input/vgg11-bn\nif ENV == 'COLAB':\n    # Will automatically show cell execution times.\n    !pip install ipython-autotime\n    %load_ext autotime","metadata":{"id":"ZPcV-eDgaN47","outputId":"77b3e05b-7852-45c0-c32c-e87440cb7de7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Replicate the kaggle directory structure in Google Colab\n","metadata":{"id":"QsChtykrOgFO"}},{"cell_type":"code","source":"import os\n\n# Only run this cell in Colab\nif ENV == \"COLAB\" and not os.path.exists('/kaggle'):\n    from google.colab import drive\n    drive.mount('/content/gdrive')\n    !cp -r '/content/gdrive/My Drive/kaggle/petfinder-pawpularity-score.zip' petfinder-pawpularity-score.zip\n    !mkdir /kaggle\n    !mkdir /kaggle/input\n    !unzip -qq petfinder-pawpularity-score.zip -d /kaggle/input/petfinder-pawpularity-score/\n    !ls","metadata":{"id":"J9nWP3_N4EJY","outputId":"5ac23f73-d20b-4d0a-c2bc-94da9a39b29b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Get a pretrained model from PyTorch","metadata":{"id":"bUo6Cao5O1-2"}},{"cell_type":"code","source":"import torch\nimport torchvision\n\n\nif ENV == \"COLAB\":\n    !pip install torch torchvision -U\n    model = torchvision.models.vgg11_bn(pretrained=True)\n    torch.save(model, \"vgg11_bn.model\")\n    !ls -lh\n    !cp vgg11_bn.model '/content/gdrive/My Drive/kaggle/vgg11_bn.model'\nelse:  # ENV = \"KAGGLE\"\n    model = torch.load('/kaggle/input/vgg11-bn/vgg11_bn.model')\n\nprint(model)","metadata":{"id":"yGGI2FSuOW-P","outputId":"6262ec46-576f-4249-ae4a-455b4149c510"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Add the last classification layer to our VGG.","metadata":{"id":"4x78Pw5uUUg5"}},{"cell_type":"code","source":"import torch\n\nmodel.classifier = torch.nn.Sequential(\n    torch.nn.Linear(in_features=25088, out_features=4096, bias=True),\n    torch.nn.ReLU(inplace=True),\n    torch.nn.Dropout(p=0.5, inplace=False),\n    torch.nn.Linear(in_features=4096, out_features=4096, bias=True),\n    torch.nn.ReLU(inplace=True),\n    torch.nn.Dropout(p=0.5, inplace=False),\n    torch.nn.Linear(in_features=4096, out_features=1, bias=True)\n)\nprint(model)","metadata":{"id":"R2SRlLU9Ue3W","outputId":"09651a84-d68c-4fb2-fab7-cb7f3acbac86"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define the Dataset","metadata":{"id":"F1J_y4dLRdSt"}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport torchvision\n\ndef show(imgs):\n    if not isinstance(imgs, list):\n        imgs = [imgs]\n    fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n    for i, img in enumerate(imgs):\n        img = img.detach()\n        img = torchvision.transforms.functional.to_pil_image(img)\n        axs[0, i].imshow(np.asarray(img))\n        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])","metadata":{"id":"oTsfTPqod_yk","outputId":"c3990539-de73-4d9c-e290-cf4b9951c8ab"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport random\nfrom torch.utils.data import Dataset\nfrom PIL import Image, ImageOps\nimport numpy as np\nimport pandas as pd\nimport os\nimport scipy\nfrom tqdm.notebook import trange, tqdm\n\n# https://stackoverflow.com/questions/43391205/add-padding-to-images-to-get-them-into-the-same-shape\ndef padding(img, expected_size):\n    desired_size = expected_size\n    delta_width = desired_size - img.size[0]\n    delta_height = desired_size - img.size[1]\n    pad_width = delta_width // 2\n    pad_height = delta_height // 2\n    padding = (pad_width, pad_height, delta_width - pad_width, delta_height - pad_height)\n    return ImageOps.expand(img, padding)\n\n# https://stackoverflow.com/questions/43391205/add-padding-to-images-to-get-them-into-the-same-shape\ndef resize_with_padding(img, expected_size):\n    img.thumbnail((expected_size[0], expected_size[1]))\n    # print(img.size)\n    delta_width = expected_size[0] - img.size[0]\n    delta_height = expected_size[1] - img.size[1]\n    pad_width = delta_width // 2\n    pad_height = delta_height // 2\n    padding = (pad_width, pad_height, delta_width - pad_width, delta_height - pad_height)\n    return ImageOps.expand(img, padding)\n\n\n\nclass ImagesDataset(Dataset):\n    def __init__(self, labels):\n      self.labels_ = labels.copy()\n      self.cached_items_ = {}\n\n    def cache_all(self):\n      print(\"[ImagesDataset] caching all samples\")\n      for i in trange(len(self.labels_)):\n        self.cached_items_[i] = self._compute_for(i)\n\n    def __len__(self):\n      return len(self.labels_)\n\n    def _compute_for(self, idx):\n      img_id = self.labels_.iat[idx, 0]\n      img_filename = os.path.join('/kaggle/input/petfinder-pawpularity-score/train/', img_id + '.jpg')\n      pawpularity = self.labels_.at[idx, 'Pawpularity']\n      image = torchvision.transforms.ToTensor()(np.asarray(\n          resize_with_padding(Image.open(img_filename), (IMG_SIZE, IMG_SIZE))\n          ))\n      image = torchvision.transforms.functional.convert_image_dtype(image, dtype=torch.float)\n      return image, torch.as_tensor([pawpularity], dtype=torch.float)\n\n    def __getitem__(self, idx):\n      if idx in self.cached_items_:\n        return self.cached_items_[idx]\n      self.cached_items_[idx] = self._compute_for(idx)\n      return self.cached_items_[idx]\n\n\nclass DatasetWithTransforms(Dataset):\n    def __init__(self, dataset, transforms):\n      self.dataset_ = dataset\n      self.transforms_ = transforms\n    \n    def __len__(self):\n      return (len(self.transforms_) + 1) * len(self.dataset_)\n    \n    def __getitem__(self, idx):\n      transform_id = idx % (len(self.transforms_) + 1)\n      sample_id = idx // (len(self.transforms_) + 1)\n      x, y = self.dataset_[sample_id]\n      if transform_id == len(self.transforms_):\n        return x, y\n      return self.transforms_[transform_id](x), y\n\n\ndef create_datasets(val_split=0.2, do_cache = True, transforms = []):\n    \"\"\"Returns a pair (train_dataset, val_dataset)\"\"\"\n    with open('/kaggle/input/petfinder-pawpularity-score/train.csv', 'r') as labels_file:\n        labels = pd.read_csv(labels_file)\n    all_idxs = list(range(len(labels)))\n    random.shuffle(all_idxs)\n    num_val_samples = int(val_split * len(all_idxs))\n    val_idxs = all_idxs[:num_val_samples]\n    train_idxs = all_idxs[num_val_samples:]\n    # Assert that the two subsets are distinct.\n    assert len(val_idxs) + len(train_idxs) == len(set(val_idxs + train_idxs))\n    train_dataset = DatasetWithTransforms(\n        ImagesDataset(labels.iloc[train_idxs].reset_index(drop=True)),\n        transforms\n    )\n    if do_cache:\n      train_dataset.dataset_.cache_all()\n    val_dataset = ImagesDataset(labels.iloc[val_idxs].reset_index(drop=True))\n    if do_cache:\n      val_dataset.cache_all()\n    return train_dataset, val_dataset\n\n\ntransforms = [\n  torchvision.transforms.ColorJitter(brightness=.5, hue=.3),\n  torchvision.transforms.RandomHorizontalFlip(p=1),\n  torchvision.transforms.RandomAffine((-90, 90)),\n  torchvision.transforms.functional.autocontrast\n]\ntrain_dataset, val_dataset = create_datasets(val_split=VAL_SPLIT,\n                                             do_cache=True,\n                                             transforms=transforms)\nprint(len(train_dataset), len(val_dataset))\nshow(train_dataset[0][0])\nshow(train_dataset[1][0])\nshow(train_dataset[2][0])\nshow(train_dataset[3][0])","metadata":{"id":"zYsD-71ARbRv","outputId":"c43a1637-4e0d-4a77-cd19-5d41cf5025aa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train","metadata":{"id":"X0TaUbusWVEa"}},{"cell_type":"code","source":"from tqdm.notebook import trange, tqdm\n\n\ntrainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE,\n                                          shuffle=True, num_workers=2)\n\nSHOW_DISTR_EVERY_N = 15\nMAX_ITERATIONS = 10000\n\n\ncriterion = torch.nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\ndevice = torch.device(\"cuda:0\")\nmodel.to(device)\n\nrunning_loss = []\nrunning_rmse = []\n\nfor epoch in range(N_EPOCHS):  # loop over the dataset multiple times\n\n    # Training\n    if epoch + 1 <= TRAIN_ONLY_CLASSIFIER_FOR_EPOCHS:\n        model.eval()  # Don't train layers of VGG, only the classifier.\n        model.classifier.train()\n    else:\n        model.train()\n\n    t = tqdm(enumerate(trainloader, 0), total=len(trainloader.dataset) / trainloader.batch_size)\n    for i, data in t:\n        # get the inputs; data is a list of [inputs, labels]\n        inputs, labels = data\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        # compute RMSE loss\n        mse_loss_fn = torch.nn.MSELoss()\n        rmse = torch.sqrt(mse_loss_fn(outputs, labels))\n  \n        # print statistics\n        running_loss.append(loss.item())\n        running_rmse.append(rmse.item())\n        t.set_description('[TRAIN] [%d, %5d] loss: %.3f avg loss: %.3f rmse: %.3f avg rmse: %.3f' %\n              (epoch + 1, i + 1, loss.item(), np.mean(running_loss[-40:]),\n               rmse.item(), np.mean(running_rmse[-40:])))\n        if i + 1 == MAX_ITERATIONS:\n            break\n    \n    plt.plot(running_rmse)\n    plt.plot(list(map(lambda x: np.mean(running_rmse[max(0, x-40):x]),\n                      range(len((running_rmse))))))\n    plt.title('Training - epoch %d/%d' % (epoch + 1, N_EPOCHS))\n    plt.show()\n\n    # Validation\n    if VAL_SPLIT == 0.:\n        continue\n    valloader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE,\n                                            shuffle=True, num_workers=2)\n    model.eval()\n\n    val_loss_all = []\n    val_rmse_all = []\n    t = tqdm(enumerate(valloader, 0), total=len(valloader.dataset) / valloader.batch_size)\n    for i, data in t:\n        # get the inputs; data is a list of [inputs, labels]\n        inputs, labels = data\n        inputs, labels = inputs.to(device), labels.to(device)\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        mse_loss_fn = torch.nn.MSELoss()\n        rmse = torch.sqrt(mse_loss_fn(outputs, labels))\n\n        val_loss_all.append(loss.item())\n        val_rmse_all.append(rmse.item())\n        t.set_description('[VAL] [%d, %5d] loss: %.3f avg loss: %.3f rmse: %.3f avg rmse: %.3f' %\n              (epoch + 1, i + 1, loss.item(), np.mean(val_loss_all),\n               rmse.item(), np.mean(val_rmse_all)))","metadata":{"id":"eycuNOm9WXU_","outputId":"dad2c638-7f69-42b8-e277-fd09b427889a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Generate submissions\n","metadata":{"id":"yjIFciVqUj3r"}},{"cell_type":"code","source":"with open('/kaggle/input/petfinder-pawpularity-score/test.csv', 'r') as labels_file:\n    test_labels = pd.read_csv(labels_file)\nprint(test_labels)\n\npredictions = []\nfor id in test_labels['Id']:\n    img_filename = os.path.join('/kaggle/input/petfinder-pawpularity-score/test/', id + '.jpg')\n    image = torchvision.transforms.ToTensor()(np.asarray(Image.open(img_filename).resize((IMG_SIZE, IMG_SIZE))))\n    image = torchvision.transforms.functional.convert_image_dtype(image, dtype=torch.float)\n    # show(image)\n    image = torch.reshape(image, (1, 3, IMG_SIZE, IMG_SIZE)).cuda()\n    pawpularity = model(image)\n    predictions.append(min(100, pawpularity.item()))\n    # print(pawpularity)\n\ntest_labels['Pawpularity'] = predictions\nprint(test_labels[['Id', 'Pawpularity']])\ntest_labels[['Id', 'Pawpularity']].to_csv('submission.csv', index=False)\n!cat submission.csv\n# \n# pawpularity = self.labels_.at[idx, 'Pawpularity']","metadata":{"id":"UNEE4w4uXL8Q"},"execution_count":null,"outputs":[]}]}