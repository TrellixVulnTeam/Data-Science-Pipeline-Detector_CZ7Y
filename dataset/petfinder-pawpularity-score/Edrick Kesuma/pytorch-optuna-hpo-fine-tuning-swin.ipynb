{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Summary","metadata":{}},{"cell_type":"markdown","source":"This notebook is written in vanilla pytorch - no lightning / tez to remove some of the abstractions that come with the libraries.\n\nIt uses Optuna to find the best combination of hyperparameters which are:\n* Number of layers\n* Number of neurons in each layer\n* Mixup coefficient\n* Dropout probability\n* Learning rate scheduler hyperparameters\n\nThe model is fine tuning the swin_224 transformer\n\nIt takes in only images\n\nIt uses BCEwLogitLoss in training and validates using RMSELoss","metadata":{}},{"cell_type":"markdown","source":"## Issues","metadata":{}},{"cell_type":"markdown","source":"The two main issues are:\n* Each trial takes around 30 minutes to complete\n* RMSE loss starts at around 20.5, meanwhile normal fine tuning notebooks start at ~18.5\n\nLooking for fixes, please give a comment if you have any leads. Thanks :3","metadata":{}},{"cell_type":"markdown","source":"## Libraries","metadata":{}},{"cell_type":"code","source":"import sys\nsys.path.append(\"../input/tez-lib\")\nsys.path.append(\"../input/timmmaster\")","metadata":{"execution":{"iopub.status.busy":"2021-12-07T05:14:29.569852Z","iopub.execute_input":"2021-12-07T05:14:29.570309Z","iopub.status.idle":"2021-12-07T05:14:29.588772Z","shell.execute_reply.started":"2021-12-07T05:14:29.570275Z","shell.execute_reply":"2021-12-07T05:14:29.587977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport cv2\nimport tez\nimport albumentations\nimport timm\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.optim as optim\nfrom sklearn import metrics\nimport torch\nfrom tez.callbacks import EarlyStopping\nfrom tqdm import tqdm\nimport os\nimport random\nimport optuna\nfrom optuna.trial import TrialState","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-07T05:14:29.590942Z","iopub.execute_input":"2021-12-07T05:14:29.592679Z","iopub.status.idle":"2021-12-07T05:14:39.297977Z","shell.execute_reply.started":"2021-12-07T05:14:29.592054Z","shell.execute_reply":"2021-12-07T05:14:39.29724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Configs","metadata":{}},{"cell_type":"code","source":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything()","metadata":{"execution":{"iopub.status.busy":"2021-12-07T05:14:39.302121Z","iopub.execute_input":"2021-12-07T05:14:39.302318Z","iopub.status.idle":"2021-12-07T05:14:39.310563Z","shell.execute_reply.started":"2021-12-07T05:14:39.302291Z","shell.execute_reply":"2021-12-07T05:14:39.309831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class args:\n    batch_size=32\n    image_size=224\n    epochs = 10\n    model_name = \"swin_tiny_patch4_window7_224\"\n    device = \"cuda\"","metadata":{"execution":{"iopub.status.busy":"2021-12-07T05:14:39.311874Z","iopub.execute_input":"2021-12-07T05:14:39.312153Z","iopub.status.idle":"2021-12-07T05:14:39.317464Z","shell.execute_reply.started":"2021-12-07T05:14:39.312116Z","shell.execute_reply":"2021-12-07T05:14:39.316662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Setup Datasets and DataLoaders","metadata":{}},{"cell_type":"code","source":"class PawpularDataset(Dataset):\n    def __init__(self, image_paths, targets, augmentations):\n        self.image_paths = image_paths\n        self.targets = targets\n        self.augmentations = augmentations\n        \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, index):\n        # read in as BGR\n        image = cv2.imread(self.image_paths[index])\n        # convert to RGB\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        if self.augmentations is not None:\n            # applies compose function from albumentations on image\n            augmented = self.augmentations(image=image)\n            # maybe cv2 returns a dict and to access info on image have to call 'image' key\n            image = augmented['image']\n            \n        # transform from HxWxC to CxHxW    \n        image = np.transpose(image, (2,0,1)).astype(np.float32)\n        \n        # normalize to [0-1]\n        targets = self.targets[index] / 100.\n        \n        return torch.tensor(image, dtype=torch.float), torch.tensor(targets, dtype=torch.float)","metadata":{"execution":{"iopub.status.busy":"2021-12-07T05:14:39.318865Z","iopub.execute_input":"2021-12-07T05:14:39.319132Z","iopub.status.idle":"2021-12-07T05:14:39.328417Z","shell.execute_reply.started":"2021-12-07T05:14:39.319098Z","shell.execute_reply":"2021-12-07T05:14:39.327557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fold = 0\n\ndf = pd.read_csv('../input/no-dupes-pawpularity/train_5folds.csv')\n\ndf_train = df[df.kfold != fold].reset_index(drop=True)\ndf_valid = df[df.kfold == fold].reset_index(drop=True)\n\ntrain_img_paths = [f\"../input/petfinder-pawpularity-score/train/{x}.jpg\" for x in df_train['Id'].values]\nvalid_img_paths = [f\"../input/petfinder-pawpularity-score/train/{x}.jpg\" for x in df_valid['Id'].values]","metadata":{"execution":{"iopub.status.busy":"2021-12-07T05:14:39.329656Z","iopub.execute_input":"2021-12-07T05:14:39.330269Z","iopub.status.idle":"2021-12-07T05:14:39.382767Z","shell.execute_reply.started":"2021-12-07T05:14:39.330234Z","shell.execute_reply":"2021-12-07T05:14:39.382114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_aug = albumentations.Compose([\n    #albumentations.Resize(args.image_size, args.image_size, p=1.0),\n    albumentations.RandomResizedCrop(\n        height=args.image_size, width=args.image_size,\n        scale=(0.08,1), ratio=(0.75, 1), p=1.0\n    ),\n    # color shift\n    albumentations.HueSaturationValue(\n        hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5\n    ),\n    albumentations.RandomBrightnessContrast(\n        brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5\n    ),\n    albumentations.HorizontalFlip(p=0.5),\n    #albumentations.VerticalFlip(p=0.5),\n    albumentations.Rotate(limit=180, p=0.7),\n    albumentations.ShiftScaleRotate(\n        shift_limit=0.1, scale_limit=0.1, rotate_limit=45, p=0.5\n    ),\n    albumentations.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225],\n        max_pixel_value=255.0,\n        p=1.0\n    ),\n], p=1.0)\n\nvalid_aug = albumentations.Compose([\n    albumentations.Resize(args.image_size, args.image_size, p=1.0),\n    albumentations.Normalize(\n        mean = [0.485, 0.456, 0.406],\n        std = [0.229, 0.224, 0.225],\n        max_pixel_value = 255.0,\n        p = 1.0,\n    ),\n], p=1.0)","metadata":{"execution":{"iopub.status.busy":"2021-12-07T05:14:39.383983Z","iopub.execute_input":"2021-12-07T05:14:39.384237Z","iopub.status.idle":"2021-12-07T05:14:39.393933Z","shell.execute_reply.started":"2021-12-07T05:14:39.384204Z","shell.execute_reply":"2021-12-07T05:14:39.392005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = PawpularDataset(\n        image_paths = train_img_paths,\n        targets = df_train.Pawpularity.values,\n        augmentations = train_aug,\n        )\n\nvalid_dataset = PawpularDataset(\n        image_paths = valid_img_paths,\n        targets = df_valid.Pawpularity.values,\n        augmentations = valid_aug,\n        )","metadata":{"execution":{"iopub.status.busy":"2021-12-07T05:14:39.395506Z","iopub.execute_input":"2021-12-07T05:14:39.395755Z","iopub.status.idle":"2021-12-07T05:14:39.405224Z","shell.execute_reply.started":"2021-12-07T05:14:39.395723Z","shell.execute_reply":"2021-12-07T05:14:39.404507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader = DataLoader(\n    train_dataset,\n    batch_size=args.batch_size,\n    shuffle=True,\n    pin_memory=True,\n    num_workers=2\n)\n\nvalid_loader = DataLoader(\n    valid_dataset,\n    batch_size=args.batch_size,\n    shuffle=True,\n    pin_memory=True,\n    num_workers=2\n)","metadata":{"execution":{"iopub.status.busy":"2021-12-07T05:14:39.408159Z","iopub.execute_input":"2021-12-07T05:14:39.408406Z","iopub.status.idle":"2021-12-07T05:14:39.414923Z","shell.execute_reply.started":"2021-12-07T05:14:39.408375Z","shell.execute_reply":"2021-12-07T05:14:39.414285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# x is images, y is targets\ndef mixup_data(x, y, mixup_alpha):\n    if mixup_alpha > 0:\n        lam = np.random.beta(mixup_alpha, mixup_alpha)\n    else:\n        lam = 1\n        \n    batch_size = x.size()[0]\n    # returns list of shuffled indices in batch size\n    index = torch.randperm(batch_size).cuda()\n    \n    # mix current x with lambda n rest with pics from the shuffled indices\n    mixed_x = lam * x + (1 - lam) * x[index, :]\n    \n    # returns targets for current x n ones used for mix\n    y_a, y_b = y, y[index]\n    \n    return mixed_x, y_a, y_b, lam\n\n# where pred is the output from the forward - predictions basically\ndef mixup_loss(loss_fn, pred, y_a, y_b, lam):\n    # get loss from current x n loss from watermarks n add\n    return lam * loss_fn(pred, y_a) + (1 - lam) * loss_fn(pred, y_b)","metadata":{"execution":{"iopub.status.busy":"2021-12-07T05:14:39.424578Z","iopub.execute_input":"2021-12-07T05:14:39.424836Z","iopub.status.idle":"2021-12-07T05:14:39.433066Z","shell.execute_reply.started":"2021-12-07T05:14:39.424802Z","shell.execute_reply":"2021-12-07T05:14:39.432335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Set model to optimize [Optuna]","metadata":{}},{"cell_type":"code","source":"# Source: https://github.com/optuna/optuna-examples/blob/main/pytorch/pytorch_simple.py\n\ndef define_model(trial):\n    pretrained_model = timm.create_model(args.model_name, pretrained=True, in_chans=3)\n    # Suggest how many linear layers to try (tries different # of layers each run)\n    n_layers = trial.suggest_int('n_layers', 1,2)\n    layers = []\n    \n    # get number of features from pretrained + 13 meta features\n    in_features = pretrained_model.head.in_features\n    for i in range(n_layers):\n        # try different number of out features for a specific layer\n        out_features = trial.suggest_int('n_units_l{}'.format(i), 64, 512)\n        if (i == 0):\n            # add pretrained model layer only on 1st layer\n            pretrained_model.head = nn.Linear(in_features, out_features)\n            layers.append(pretrained_model)\n        else:\n            # add layer to layers list\n            layers.append(nn.Linear(in_features, out_features))\n        # try different probabilities for dropout\n        p = trial.suggest_float('dropout_l{}'.format(i), 0.1, 0.5)\n        # add dropout layer with that probability\n        layers.append(nn.Dropout(p))\n        # Add the activation function\n        layers.append(nn.ReLU())\n        \n        # first layer's out features becomes next layer's in features\n        in_features = out_features\n    \n    # After run through optimization: insert these layers\n    layers.append(nn.Linear(in_features, 1))\n    \n    return nn.Sequential(*layers)","metadata":{"execution":{"iopub.status.busy":"2021-12-07T05:14:39.434232Z","iopub.execute_input":"2021-12-07T05:14:39.435171Z","iopub.status.idle":"2021-12-07T05:14:39.444711Z","shell.execute_reply.started":"2021-12-07T05:14:39.435133Z","shell.execute_reply":"2021-12-07T05:14:39.444101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Set objective function [Optuna]","metadata":{}},{"cell_type":"code","source":"def objective(trial):\n    model = define_model(trial).to(args.device)\n    \n    optimizer = optim.Adam(model.parameters())\n    T_0 = trial.suggest_int('T_0', 10,20)\n    eta_min = trial.suggest_loguniform('eta_min', 1e-6, 1e-4)\n    lr_scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n        optimizer = optimizer, \n        T_0 = T_0, \n        eta_min = eta_min\n    )\n    mixup_alpha = trial.suggest_float('mixup_alpha', 0.1, 0.4)\n    # other hyperparams to consider including: activation funcs (relu, gelu, leaky relu)\n    \n    for epoch in range(args.epochs):\n        print(f'Epoch {epoch} start!')\n        # Credit: https://www.kaggle.com/c/petfinder-pawpularity-score/discussion/289790 for calculating epoch rmse\n        epoch_mse = 0\n        val_size = 0\n        model.train()\n        \n        for batch_idx, (img, target) in enumerate(train_loader):\n            \n            # mixup the data and send tensors to gpu\n            mixed_img, target_a, target_b, lam = mixup_data(img.to(args.device), target.view(-1,1).to(args.device), mixup_alpha)\n            mixed_img, target_a, target_b = mixed_img.to(args.device), target_a.to(args.device), target_b.to(args.device)\n                        \n            # small hunch: i think gpu not being used when calc loss, backward n step \n            \n            # standard zero out gradients, calc loss, backprop and step pattern\n            optimizer.zero_grad()\n            output = model(mixed_img).to(args.device)\n            loss = mixup_loss(nn.BCEWithLogitsLoss(), output, target_a, target_b, lam).to(args.device)\n            loss.backward()\n            optimizer.step()\n\n        model.eval()\n        \n        with torch.no_grad():\n            for batch_idx, (img, target) in enumerate(valid_loader):\n                img, target = img.to(args.device), target.to(args.device)\n                # make a prediction with the model\n                output = model(img)\n                # output still in the form of logits, use sigmoid to convert to 0-1 and * 100 to be in range [0-100]\n                normalized_output = torch.sigmoid(output) * 100\n                batch_mse = nn.MSELoss()(normalized_output, target.view(-1,1) * 100)\n                # Note: apparently torch doesn't go thru entire dataset here\n                # size goes up to 320 here: 32 x 10 epochs\n                # so dataloader randomly picks 32 samples for an epoch (not whole dataset)\n                batch_size = target.shape[0]\n                epoch_mse += batch_mse * batch_size # original: batch_data.shape[0] \n                val_size += batch_size\n        \n        epoch_mse = epoch_mse / val_size \n        # basically sqrt\n        epoch_rmse = epoch_mse ** 0.5\n        print(f'Epoch RMSE: {epoch_rmse}')\n        \n        trial.report(epoch_rmse, epoch)\n        \n        if trial.should_prune():\n            raise optuna.exceptions.TrialPruned()\n        \n    return epoch_rmse","metadata":{"execution":{"iopub.status.busy":"2021-12-07T05:14:39.44595Z","iopub.execute_input":"2021-12-07T05:14:39.446414Z","iopub.status.idle":"2021-12-07T05:14:39.462936Z","shell.execute_reply.started":"2021-12-07T05:14:39.446378Z","shell.execute_reply":"2021-12-07T05:14:39.462269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Optimization happens here","metadata":{}},{"cell_type":"code","source":"study = optuna.create_study(direction='minimize')\n# Optimize for 2 hours\nstudy.optimize(objective, timeout=60*60*2)","metadata":{"execution":{"iopub.status.busy":"2021-12-07T05:14:39.463954Z","iopub.execute_input":"2021-12-07T05:14:39.464168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\ncomplete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Study Statistics: \")\nprint(\"    Number of finished trials: \", len(study.trials))\nprint(\"    Number of pruned trials: \", len(pruned_trials))\nprint(\"    Number of complete trials: \", len(complete_trials))\nprint(\"-------------------------------\")\nprint(\"Best Trial: \")\ntrial = study.best_trial\nprint(\"    BCELoss: \", trial.value)\nprint(\"    Params: \")\nfor key, value in trial.params.items():\n    print(f\"    {key}: {value}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualizations","metadata":{}},{"cell_type":"code","source":"fig = optuna.visualization.plot_param_importances(study)\nfig.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = optuna.visualization.plot_optimization_history(study)\nfig.show()","metadata":{},"execution_count":null,"outputs":[]}]}