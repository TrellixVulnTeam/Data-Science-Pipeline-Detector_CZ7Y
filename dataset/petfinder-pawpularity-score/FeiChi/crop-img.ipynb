{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install ../input/timm-0-4-12/timm-0.4.12-py3-none-any.whl ../input/python-box-5-4-1/python_box-5.4.1-py3-none-any.whl ../input/webcolors/webcolors-1.11.1-py3-none-any.whl ","metadata":{"execution":{"iopub.status.busy":"2021-12-01T12:15:58.491573Z","iopub.execute_input":"2021-12-01T12:15:58.491936Z","iopub.status.idle":"2021-12-01T12:16:27.202299Z","shell.execute_reply.started":"2021-12-01T12:15:58.491833Z","shell.execute_reply":"2021-12-01T12:16:27.201344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nsys.path.append('../input/yetanotherefficientdetpytorch/')\n\nimport os\nimport time\nimport torch\nfrom torch.backends import cudnn\nfrom matplotlib import colors\n\nfrom backbone import EfficientDetBackbone\nimport cv2\nimport numpy as np\nimport pandas as pd\n\nfrom efficientdet.utils import BBoxTransform, ClipBoxes\nfrom utils.utils import preprocess, invert_affine, postprocess, STANDARD_COLORS, standard_to_bgr, get_index_label, plot_one_box","metadata":{"execution":{"iopub.status.busy":"2021-12-01T12:16:27.205584Z","iopub.execute_input":"2021-12-01T12:16:27.205809Z","iopub.status.idle":"2021-12-01T12:16:29.091189Z","shell.execute_reply.started":"2021-12-01T12:16:27.205781Z","shell.execute_reply":"2021-12-01T12:16:29.090429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_path = '../input/petfinder-pawpularity-score/test/'\ntrain_path = '../input/petfinder-pawpularity-score/train/'\ntrain_csv_path = '../input/petfinder-pawpularity-score/train.csv'\ntest_csv_path = '../input/petfinder-pawpularity-score/test.csv'\n\ntest_metadata = pd.read_csv(test_csv_path)\ntrain_metadata = pd.read_csv(train_csv_path)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T12:16:29.094025Z","iopub.execute_input":"2021-12-01T12:16:29.094454Z","iopub.status.idle":"2021-12-01T12:16:29.132597Z","shell.execute_reply.started":"2021-12-01T12:16:29.094415Z","shell.execute_reply":"2021-12-01T12:16:29.131895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def display(preds, imgs, imshow=True, imwrite=False):\n    max_area = 0\n    x_1, y_1, x_2, y_2 = 0, 0, 0, 0\n    for i in range(len(imgs)):\n        \n        if len(preds[i]['rois']) == 0:\n            continue\n\n        imgs[i] = imgs[i].copy()\n\n        for j in range(len(preds[i]['rois'])):\n            x1, y1, x2, y2 = preds[i]['rois'][j].astype(np.int)\n            obj = obj_list[preds[i]['class_ids'][j]]\n            score = float(preds[i]['scores'][j])\n#             print(obj)\n#             plot_one_box(imgs[i], [x1, y1, x2, y2], label=obj,score=score,color=color_list[get_index_label(obj, obj_list)])\n            \n            if obj == 'cat' or obj == 'dog':\n                area = (x2 - x1) * (y2 - y1)\n                if area >= max_area:\n                    max_area = area\n                    x_1, y_1, x_2, y_2 = x1, y1, x2, y2\n                \n    \n#         if imshow:\n#             cv2.imshow('img', imgs[i])\n#             cv2.waitKey(0)\n\n#         if imwrite:\n#             cv2.imwrite(f'./img_inferred_d{compound_coef}_this_repo_{i}.jpg', imgs[i])\n        \n    return x_1, y_1, x_2, y_2","metadata":{"execution":{"iopub.status.busy":"2021-12-01T12:16:29.133959Z","iopub.execute_input":"2021-12-01T12:16:29.134226Z","iopub.status.idle":"2021-12-01T12:16:29.1432Z","shell.execute_reply.started":"2021-12-01T12:16:29.134191Z","shell.execute_reply":"2021-12-01T12:16:29.142331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def crop(img_Id):\n    compound_coef = 6\n    force_input_size = None  # set None to use default size\n\n    # replace this part with your project's anchor config\n    anchor_ratios = [(1.0, 1.0), (1.4, 0.7), (0.7, 1.4)]\n    anchor_scales = [2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)]\n\n    threshold = 0.2\n    iou_threshold = 0.2\n\n    use_cuda = True\n    use_float16 = False\n    cudnn.fastest = True\n    cudnn.benchmark = True\n\n    obj_list = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light',\n                'fire hydrant', '', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep',\n                'cow', 'elephant', 'bear', 'zebra', 'giraffe', '', 'backpack', 'umbrella', '', '', 'handbag', 'tie',\n                'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',\n                'skateboard', 'surfboard', 'tennis racket', 'bottle', '', 'wine glass', 'cup', 'fork', 'knife', 'spoon',\n                'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut',\n                'cake', 'chair', 'couch', 'potted plant', 'bed', '', 'dining table', '', '', 'toilet', '', 'tv',\n                'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink',\n                'refrigerator', '', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier',\n                'toothbrush']\n\n    color_list = standard_to_bgr(STANDARD_COLORS)\n    # tf bilinear interpolation is different from any other's, just make do\n    input_sizes = [512, 640, 768, 896, 1024, 1280, 1280, 1536, 1536]\n    input_size = input_sizes[compound_coef] if force_input_size is None else force_input_size\n\n    model_det = EfficientDetBackbone(compound_coef=compound_coef, num_classes=len(obj_list),\n                                 ratios=anchor_ratios, scales=anchor_scales)\n    model_det.load_state_dict(torch.load(f'../input/efficientdet-d6/efficientdet-d{compound_coef}.pth', map_location='cpu'))\n    model_det.requires_grad_(False)\n    model_det.eval()\n\n    if use_cuda:\n        model_det = model_det.cuda()\n    if use_float16:\n        model_det = model_det.half()\n    \n    img_path = test_path + img_Id + '.jpg' \n#     print(img_path)\n    ori_imgs, framed_imgs, framed_metas = preprocess(img_path, max_size=input_size)\n\n    if use_cuda:\n        x = torch.stack([torch.from_numpy(fi).cuda() for fi in framed_imgs], 0)\n    else:\n        x = torch.stack([torch.from_numpy(fi) for fi in framed_imgs], 0)\n\n    x = x.to(torch.float32 if not use_float16 else torch.float16).permute(0, 3, 1, 2)\n    \n    with torch.no_grad():\n        features, regression, classification, anchors = model_det(x)\n\n        regressBoxes = BBoxTransform()\n        clipBoxes = ClipBoxes()\n\n        out = postprocess(x,\n                          anchors, regression, classification,\n                          regressBoxes, clipBoxes,\n                          threshold, iou_threshold)\n    \n    out = invert_affine(framed_metas, out)\n    x_1, y_1, x_2, y_2 = display(out, ori_imgs, imshow=False, imwrite=True)\n    if x_1 == 0 and x_2 == 0 and y_1==0 and y_2==0:\n        img = ori_imgs[0]\n    else:\n        img = ori_imgs[0][y_1:y_2, x_1:x_2]\n    \n    return img","metadata":{"execution":{"iopub.status.busy":"2021-12-01T12:16:29.146018Z","iopub.execute_input":"2021-12-01T12:16:29.14629Z","iopub.status.idle":"2021-12-01T12:16:29.165771Z","shell.execute_reply.started":"2021-12-01T12:16:29.146256Z","shell.execute_reply":"2021-12-01T12:16:29.164707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport cv2\nimport timm\nimport pandas as pd\nimport numpy as np\nfrom box import Box\nfrom timm import create_model\nfrom sklearn.model_selection import KFold\n\nfrom matplotlib import pyplot as plt\n\nimport albumentations\nfrom albumentations.pytorch import ToTensorV2\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import transforms\nfrom torchvision.io import read_image\n\nfrom torch.utils.data import Dataset, DataLoader\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning.utilities.seed import seed_everything\nfrom pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint","metadata":{"execution":{"iopub.status.busy":"2021-12-01T12:16:29.167201Z","iopub.execute_input":"2021-12-01T12:16:29.167481Z","iopub.status.idle":"2021-12-01T12:16:36.046826Z","shell.execute_reply.started":"2021-12-01T12:16:29.167446Z","shell.execute_reply":"2021-12-01T12:16:36.046046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_path = '../input/petfinder-pawpularity-score/train/'\ntest_path = '../input/petfinder-pawpularity-score/test/'\ntrain_metadata_path = '../input/petfinder-pawpularity-score/train.csv'\ntest_metadata_path = '../input/petfinder-pawpularity-score/test.csv'\n\ntrain_num = len(os.listdir(train_path))\ntest_num = len(os.listdir(test_path))\ntrain_metadata = pd.read_csv(train_metadata_path)\ntest_metadata = pd.read_csv(test_metadata_path)\n\nprint(f'Train Data num is : {train_num} \\nTest Data num is : {test_num}')","metadata":{"execution":{"iopub.status.busy":"2021-12-01T12:16:36.048517Z","iopub.execute_input":"2021-12-01T12:16:36.04879Z","iopub.status.idle":"2021-12-01T12:16:36.275Z","shell.execute_reply.started":"2021-12-01T12:16:36.048744Z","shell.execute_reply":"2021-12-01T12:16:36.274276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CONFIG = {\n    'RANDOM_SEED' : 2021,\n    'N_SPLITS' : 5,\n    'img_size' : 384,\n    'epochs' : 20,\n    'train_dataloader' : {\n        'batch_size' : 32,\n        'shuffle' : True,\n        'num_workers' : 2,        \n    },\n    'val_dataloader' : {\n        'batch_size' : 32,\n        'shuffle' : False,\n        'num_workers' : 2,\n    },\n    'test_dataloader' : {\n        'batch_size' : 1,\n        'shuffle' : False,\n        'num_workers' : 2,\n    },\n    'model' : {\n        'name' : 'efficientnet_b2',\n        'output_dim' : 1,\n    },\n    'optimizer' : {\n        'name' : 'optim.Adam',\n        'params' : {\n            'lr' : 1e-5,\n        }\n    },\n    # 学习率衰减\n    'scheduler' : {\n        'name' : 'optim.lr_scheduler.CosineAnnealingWarmRestarts',\n        'params' : {\n            'T_0' : 20,\n            'eta_min' : 1e-6,\n        }\n    },\n    'bce_loss' : 'nn.BCEWithLogitsLoss',\n    'rmse_loss' : 'nn.MSELoss',\n    \n}\n# 使用Box为字典增加点调用属性\nconfig = Box(CONFIG)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T12:16:36.276517Z","iopub.execute_input":"2021-12-01T12:16:36.277029Z","iopub.status.idle":"2021-12-01T12:16:36.286554Z","shell.execute_reply.started":"2021-12-01T12:16:36.276988Z","shell.execute_reply":"2021-12-01T12:16:36.285789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_default_transforms(mode):\n    if mode == 'train':\n        aug = albumentations.Compose([\n            albumentations.Resize(config.img_size, config.img_size, p=1),\n            albumentations.HorizontalFlip(p=0.5),\n            albumentations.RandomRotate90(p=0.5),\n            albumentations.HueSaturationValue(\n                hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5\n            ),\n            albumentations.RandomBrightnessContrast(\n                brightness_limit=(-0.1, 0.1), contrast_limit=(-0.1, 0.1), p=0.5\n            ),\n            albumentations.Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225],\n                max_pixel_value=255.0,\n                p=1.0,\n            ),\n            ToTensorV2(),\n        ])\n    else:\n        aug = albumentations.Compose([\n            albumentations.Resize(config.img_size, config.img_size, p=1),\n            albumentations.Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225],\n                max_pixel_value=255.0,\n                p=1.0,\n            ),\n            ToTensorV2(),\n        ])\n        \n    return aug","metadata":{"execution":{"iopub.status.busy":"2021-12-01T12:16:36.28871Z","iopub.execute_input":"2021-12-01T12:16:36.29004Z","iopub.status.idle":"2021-12-01T12:16:36.302652Z","shell.execute_reply.started":"2021-12-01T12:16:36.290002Z","shell.execute_reply":"2021-12-01T12:16:36.30188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IMAGENET_MEAN = [0.485, 0.456, 0.406]  # RGB\nIMAGENET_STD = [0.229, 0.224, 0.225]  # RGB\n\nclass PetDataSet(Dataset):\n    def __init__(self, df, mode, data_path, transforms=None):\n        # df的某一列为series,series的values为numpy.array\n        self.file_name = df['Id'].values\n        self.mode = mode\n        if self.mode != 'test':\n            self.Pawpularity = df['Pawpularity'].values\n        self.transforms = transforms\n        self.data_path = data_path\n    \n    def __len__(self):\n        return len(self.file_name)\n    \n    def __getitem__(self, idx):\n        img_name = self.file_name[idx]\n        img = cv2.imread(os.path.join(self.data_path, img_name + '.jpg'))\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        if self.transforms != None:\n            img = self.transforms(image=img)['image']\n        \n        img_Id = img_name.split('/')[-1].split('.')[0]\n        if self.mode != 'test':\n            label = self.Pawpularity[idx] / 100.\n            \n            return img, label\n        else:\n            img_Id = img_name.split('/')[-1].split('.')[0]\n            return img, img_Id\n\n# 声明一个DataModule,为了减少重复声明DataSet和Dataloader的冗余\nclass PetDataModule(pl.LightningDataModule):\n    def __init__(self, train_df, val_df, test_df, cfg):\n        super().__init__()\n        self.train_df = train_df\n        self.val_df = val_df\n        self.test_df = test_df\n        self.cfg = cfg\n    \n    def train_dataloader(self):\n        train_transforms = get_default_transforms(mode='test')\n        dataset = PetDataSet(self.train_df, mode='train', data_path=train_path, transforms=train_transforms)\n        \n        return DataLoader(dataset, **self.cfg.train_dataloader)\n    \n    def val_dataloader(self):\n        val_transforms = get_default_transforms(mode='test')\n        dataset = PetDataSet(self.val_df, mode='val', data_path=train_path, transforms=val_transforms)\n        \n        return DataLoader(dataset, **self.cfg.val_dataloader)\n    \n    def test_dataloader(self):\n        test_transforms = get_default_transforms(mode='test')\n        dataset = PetDataSet(self.test_df, mode='test', data_path=test_path, transforms=test_transforms)\n        \n        return DataLoader(dataset, **self.cfg.test_dataloader)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T12:16:36.304312Z","iopub.execute_input":"2021-12-01T12:16:36.304778Z","iopub.status.idle":"2021-12-01T12:16:36.327752Z","shell.execute_reply.started":"2021-12-01T12:16:36.304743Z","shell.execute_reply":"2021-12-01T12:16:36.326949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 为numpy,random,pytorch设置随机种子\nseed_everything(config.RANDOM_SEED)\n\n# mixup混合图像\n# 一种图像增强的trick，利用beta分布获取一个值，然后利用该值将两个图像叠加，增强泛化性\ndef mix_up(x, y, alpha):\n    lam = np.random.beta(alpha, alpha)\n    rand_idx = torch.randperm(x.size()[0]) # 返回一个随机打乱的数组\n    mix_x = lam * x + (1 - lam) * x[rand_idx, :]\n    target_a, target_b = y, y[rand_idx]\n    \n    return mix_x, target_a, target_b, lam\n\n# 使用pytorch_lightning构建模型(下面是模板)\n# 声明模型必须包含的三个组件:1、__init__初始化 2、training_step(),每个batch的训练 3、configure_optimizers() 优化器的定义\nclass Model(pl.LightningModule):\n    def __init__(self, cfg):\n        super(Model, self).__init__()\n        self.cfg = cfg\n        self.build_model()\n        # python的eval会执行一个字符串表达式,在config中定义的损失是以字符串的形式\n        self.criterion_bce = eval(self.cfg.bce_loss)()\n        self.criterion_rmse = eval(self.cfg.rmse_loss)()\n        \n    def build_model(self):\n        # timm中的网络结构，在一层层的加深之后，最后是一个全局的avg_pool层，返回一个x维度的向量，然后pool层后面是classifier层\n        # num_classes=0就是说分类层中的输出类别为1，只返回pool的特征向量\n        self.backbone = create_model(self.cfg.model.name, pretrained=False, num_classes=0, in_chans=3)\n        self.num_features = self.backbone.num_features\n        self.fc = nn.Sequential(nn.Dropout(0.5),\n                                nn.Linear(self.num_features, self.cfg.model.output_dim))\n    \n    def forward(self, x):\n        x = self.backbone(x)\n        out = self.fc(x)\n        \n        return out, x\n    \n    # 必须返回一个loss值(往往会以字典的形式返回多个数据)，batch为(img, label)的集合\n    def training_step(self, batch, batch_idx):\n        imgs, labels = batch\n        \n        # 以一半的概率进行mixup\n        if torch.rand(1)[0] > 0.5:\n            mix_imgs, target_a, target_b, lam = mix_up(imgs, labels, alpha=0.5)\n            pawpularity_preds = self.forward(mix_imgs)\n            loss_bce = self.criterion_bce(pawpularity_preds.view(-1), target_a.float()) * lam + \\\n                        self.criterion_bce(pawpularity_preds.view(-1), target_b.float()) * (1 - lam)\n        else:  \n            pawpularity_preds = self.forward(imgs)\n            loss_bce = self.criterion_bce(pawpularity_preds.view(-1), labels.float())\n        \n        preds = pawpularity_preds.sigmoid().detach().cpu() * 100.\n        labels = labels.detach().cpu() * 100.\n        \n        return {'loss' : loss_bce, 'preds' : preds, 'labels' : labels}\n    \n    # 每个batch返回的loss会放在一个列表中\n    # training_epoch_end不返回值，或者说期望的返回值为None，所以应该使用log来记录损失\n    def training_epoch_end(self, training_step_outputs):\n        preds = []\n        labels = []\n        for out in training_step_outputs:\n            pred, label =  out['preds'], out['labels']\n            preds.append(pred)\n            labels.append(label)\n        \n        preds = torch.cat(preds)\n        labels = torch.cat(labels)\n        train_Loss = torch.sqrt(self.criterion_rmse(preds.view(-1), labels))\n        \n        print(f'Epoch [{self.current_epoch + 1}] train_Loss : {train_Loss}')\n        self.log('train_Loss', train_Loss)\n    \n    def validation_step(self, batch, batch_idx):\n        imgs, labels = batch\n        pawpularity_preds = self.forward(imgs)\n        preds = pawpularity_preds.sigmoid().detach().cpu() * 100.\n        labels = labels.detach().cpu() * 100.\n        loss_rmse = torch.sqrt(self.criterion_rmse(preds.view(-1), labels))\n        \n        return {'loss' : loss_rmse, 'preds' : preds, 'labels' : labels}\n    \n    def validation_epoch_end(self, validation_step_outputs):\n        preds = []\n        labels = []\n        for out in validation_step_outputs:\n            pred, label =  out['preds'], out['labels']\n            preds.append(pred)\n            labels.append(label)\n        \n        preds = torch.cat(preds)\n        labels = torch.cat(labels)\n        val_Loss = torch.sqrt(self.criterion_rmse(preds.view(-1), labels))\n        \n        print(f'Epoch [{self.current_epoch + 1}] val_Loss : {val_Loss}')\n        self.log('val_Loss', val_Loss)\n    \n    # 声明优化器，当training_step返回loss的时候，会自动进行反向传播，可以在声明Trainer的时候关闭自动反向传播\n    def configure_optimizers(self):\n        # *表示将序列解包成位置参数, **表示将字典解包成关键字参数\n        # 位置参数按照函数定义的位置来传递参数,关键字参数则是以键-值的方式传递参数\n        # 简单的来理解当：mode=‘train’,这个样子传入的时候，mode就变成了一个关键字参数\n        optimizer = eval(self.cfg.optimizer.name)(self.parameters(), **self.cfg.optimizer.params)\n        scheduler = eval(self.cfg.scheduler.name)(optimizer, **self.cfg.scheduler.params)\n        \n        return [optimizer], [scheduler]","metadata":{"execution":{"iopub.status.busy":"2021-12-01T12:16:36.331728Z","iopub.execute_input":"2021-12-01T12:16:36.33196Z","iopub.status.idle":"2021-12-01T12:16:36.372799Z","shell.execute_reply.started":"2021-12-01T12:16:36.331923Z","shell.execute_reply":"2021-12-01T12:16:36.372048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_transforms = get_default_transforms(mode='test')\npreds = np.zeros(test_num)\nfor j in range(5):\n    pred = []\n    ckpt_path = '../input/eff-al/' + 'best_epoch_' + str(j) + '.pth'\n\n    model = Model(config)\n    checkpoint = torch.load(ckpt_path)\n    r = model.load_state_dict(checkpoint)\n    print(r)\n    model.eval()\n    model.cuda()\n    with torch.no_grad():\n        for i in range(len(test_metadata)):\n            img_Id = test_metadata.loc[i, 'Id']\n            print(img_Id)\n            img = crop(img_Id)\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            img = test_transforms(image=img)['image'].unsqueeze(axis=0)\n\n            img = img.cuda()\n            pred_pawpularity, embedding = model(img.float())\n            pred_pawpularity = pred_pawpularity.view(-1)\n            pred.extend(pred_pawpularity.sigmoid().data.cpu().numpy() * 100.)\n\n        preds += np.array(pred) / 5","metadata":{"execution":{"iopub.status.busy":"2021-12-01T12:16:36.3771Z","iopub.execute_input":"2021-12-01T12:16:36.379066Z","iopub.status.idle":"2021-12-01T12:18:00.639971Z","shell.execute_reply.started":"2021-12-01T12:16:36.379004Z","shell.execute_reply":"2021-12-01T12:18:00.639156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_sub_path = '../input/petfinder-pawpularity-score/sample_submission.csv'\nsubmission = pd.read_csv(sample_sub_path)\nsubmission['Pawpularity'] = preds\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T12:18:00.644937Z","iopub.execute_input":"2021-12-01T12:18:00.645434Z","iopub.status.idle":"2021-12-01T12:18:00.661662Z","shell.execute_reply.started":"2021-12-01T12:18:00.645393Z","shell.execute_reply":"2021-12-01T12:18:00.660933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission","metadata":{"execution":{"iopub.status.busy":"2021-12-01T12:18:00.666845Z","iopub.execute_input":"2021-12-01T12:18:00.667182Z","iopub.status.idle":"2021-12-01T12:18:00.692169Z","shell.execute_reply.started":"2021-12-01T12:18:00.667143Z","shell.execute_reply":"2021-12-01T12:18:00.686897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}