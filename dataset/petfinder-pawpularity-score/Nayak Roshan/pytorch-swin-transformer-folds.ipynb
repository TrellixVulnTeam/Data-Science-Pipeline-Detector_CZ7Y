{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebook walks you through the training process of the model. Efficient net version 2 proved to be better than visual transformers. Hence I have used efficient net from timm python library.\n\nFor the inference part checkout [this](https://www.kaggle.com/nayakroshan/fork-of-inference-pawpularity) particular notebook.","metadata":{}},{"cell_type":"code","source":"!pip install -qq timm\n!pip install -qq albumentations==1.0.3","metadata":{"execution":{"iopub.status.busy":"2021-10-17T04:50:44.390122Z","iopub.execute_input":"2021-10-17T04:50:44.390982Z","iopub.status.idle":"2021-10-17T04:50:59.740513Z","shell.execute_reply.started":"2021-10-17T04:50:44.390895Z","shell.execute_reply":"2021-10-17T04:50:59.73963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2021-10-17T04:50:59.742538Z","iopub.execute_input":"2021-10-17T04:50:59.742807Z","iopub.status.idle":"2021-10-17T04:51:00.442845Z","shell.execute_reply.started":"2021-10-17T04:50:59.742772Z","shell.execute_reply":"2021-10-17T04:51:00.442001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport cv2 as cv\nfrom tqdm.auto import tqdm\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport timm\nimport glob\nfrom xgboost import XGBClassifier\n\nimport albumentations\nfrom albumentations.pytorch.transforms import ToTensorV2\n\nimport gc\ngc.enable()\n\nimport warnings\nimport sklearn.exceptions\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\nwarnings.filterwarnings('ignore', category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)","metadata":{"execution":{"iopub.status.busy":"2021-10-17T04:51:00.446078Z","iopub.execute_input":"2021-10-17T04:51:00.446314Z","iopub.status.idle":"2021-10-17T04:51:07.634609Z","shell.execute_reply.started":"2021-10-17T04:51:00.446287Z","shell.execute_reply":"2021-10-17T04:51:07.633703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Augmentation","metadata":{}},{"cell_type":"code","source":"def train_transform_object(DIM = 384):\n    return albumentations.Compose(\n        [\n            albumentations.HorizontalFlip(p=0.5),\n            albumentations.VerticalFlip(p=0.5),\n            albumentations.Rotate(limit=180, p=0.5),\n            albumentations.RandomBrightnessContrast(\n                brightness_limit=(-0.1, 0.1),\n                contrast_limit=(-0.1, 0.1), p=0.5\n            ),\n            albumentations.Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225],\n            ),\n            ToTensorV2(p=1.0),\n        ]\n    )\n\ndef valid_transform_object(DIM = 384):\n    return albumentations.Compose(\n        [\n            albumentations.Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225],\n            ),\n            ToTensorV2(p=1.0)\n        ]\n    )","metadata":{"execution":{"iopub.status.busy":"2021-10-17T04:51:07.636119Z","iopub.execute_input":"2021-10-17T04:51:07.636402Z","iopub.status.idle":"2021-10-17T04:51:07.646008Z","shell.execute_reply.started":"2021-10-17T04:51:07.636367Z","shell.execute_reply":"2021-10-17T04:51:07.644326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset","metadata":{}},{"cell_type":"code","source":"class PetDataset(Dataset):\n    \n    def __init__(self, image_paths, dense_features, targets, transform=None):\n        self.image_paths = image_paths\n        self.dense_feats = dense_features\n        self.targets = targets\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, index):\n        #read the image using the path.\n        img = cv.imread(self.image_paths[index], 1)\n        img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n        img = cv.resize(img, (224, 224), interpolation = cv.INTER_AREA)\n        \n        if self.transform is not None:\n            img = self.transform(image=img)['image']\n            \n        img = img.float()\n            \n        #get the dense features.\n        dense = self.dense_feats[index, :]\n        \n        #get the label and convert it to 0 to 1.\n        label = torch.tensor(self.targets[index]).float()\n        \n        return (img, dense, label)\n        ","metadata":{"execution":{"iopub.status.busy":"2021-10-17T04:51:07.649611Z","iopub.execute_input":"2021-10-17T04:51:07.649996Z","iopub.status.idle":"2021-10-17T04:51:07.659314Z","shell.execute_reply.started":"2021-10-17T04:51:07.649962Z","shell.execute_reply":"2021-10-17T04:51:07.658544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PetTestset(Dataset):\n    \n    def __init__(self, image_paths, dense_features, transform=None):\n        self.image_paths = image_paths\n        self.dense_feats = dense_features\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, index):\n        #read the image using the path.\n        img = cv.imread(self.image_paths[index], 1)\n        img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n        img = cv.resize(img, (224, 224), interpolation = cv.INTER_AREA)\n        \n        if self.transform is not None:\n            img = self.transform(image=img)['image']\n            \n        img = img.float()\n            \n        #get the dense features.\n        dense = self.dense_feats[index, :]\n        \n        return (img, dense)","metadata":{"execution":{"iopub.status.busy":"2021-10-17T13:06:38.263936Z","iopub.execute_input":"2021-10-17T13:06:38.264255Z","iopub.status.idle":"2021-10-17T13:06:38.272264Z","shell.execute_reply.started":"2021-10-17T13:06:38.264205Z","shell.execute_reply":"2021-10-17T13:06:38.271245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"class PetNet(nn.Module):\n    def __init__(self, model_name, out_features, inp_channels, pretrained, num_dense):\n        super().__init__()\n        self.model = timm.create_model(model_name, pretrained=pretrained, in_chans=inp_channels)\n#         n_features = self.model.classifier.in_features\n#         self.model.classifier = nn.Linear(n_features, 128)\n        n_features = self.model.head.in_features\n        self.model.head = nn.Linear(n_features, 128)\n        self.fc = nn.Sequential(\n            nn.Linear(128 + num_dense, 64),\n            nn.ReLU(),\n            nn.Linear(64, out_features)\n        )\n        self.dropout = nn.Dropout(0.2)\n    \n    def forward(self, image, dense):\n        embeddings = self.model(image)\n        x = self.dropout(embeddings)\n        x = torch.cat([x, dense], dim=1)\n        output = self.fc(x)\n        return output","metadata":{"execution":{"iopub.status.busy":"2021-10-17T04:51:07.671996Z","iopub.execute_input":"2021-10-17T04:51:07.672376Z","iopub.status.idle":"2021-10-17T04:51:07.682005Z","shell.execute_reply.started":"2021-10-17T04:51:07.672341Z","shell.execute_reply":"2021-10-17T04:51:07.68118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Metric","metadata":{}},{"cell_type":"code","source":"def usr_rmse_score(output, target):\n    y_pred = torch.sigmoid(output).cpu()\n    y_pred = y_pred.detach().numpy()*100\n    target = target.cpu()*100\n    \n    return mean_squared_error(target, y_pred, squared=False)","metadata":{"execution":{"iopub.status.busy":"2021-10-17T04:51:07.683313Z","iopub.execute_input":"2021-10-17T04:51:07.683871Z","iopub.status.idle":"2021-10-17T04:51:07.691769Z","shell.execute_reply.started":"2021-10-17T04:51:07.683823Z","shell.execute_reply":"2021-10-17T04:51:07.691001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if torch.cuda.is_available():\n    device = torch.device('cuda')\nelse:\n    device = torch.device('cpu')","metadata":{"execution":{"iopub.status.busy":"2021-10-17T04:51:07.693032Z","iopub.execute_input":"2021-10-17T04:51:07.693351Z","iopub.status.idle":"2021-10-17T04:51:07.740511Z","shell.execute_reply.started":"2021-10-17T04:51:07.693321Z","shell.execute_reply":"2021-10-17T04:51:07.739668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_fn(train_loader, model, loss_fn, optimizer, epoch, device, scheduler=None):\n    model.train()\n    stream = tqdm(train_loader)\n    \n    for i, (image, dense, target) in enumerate(stream, start=1):\n        image = image.to(device, non_blocking=True)\n        dense = dense.to(device, non_blocking=True)\n        target = target.to(device, non_blocking=True).float().view(-1, 1)\n        \n        output = model(image, dense)\n        loss = loss_fn(output, target)\n        rmse = usr_rmse_score(output, target)\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        stream.set_description(f\"Epoch {epoch:02}. Train. Loss {loss}. RMSE {rmse}\")","metadata":{"execution":{"iopub.status.busy":"2021-10-17T04:51:07.743345Z","iopub.execute_input":"2021-10-17T04:51:07.743528Z","iopub.status.idle":"2021-10-17T04:51:07.753064Z","shell.execute_reply.started":"2021-10-17T04:51:07.743506Z","shell.execute_reply":"2021-10-17T04:51:07.752396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def validation_fn(validation_loader, model, loss_fn, epoch, device):\n    model.eval()\n    stream = tqdm(validation_loader)\n    final_targets = []\n    final_outputs = []\n    \n    with torch.no_grad():\n        for i, (image, dense, target) in enumerate(stream, start=1):\n            image = image.to(device, non_blocking=True)\n            dense = dense.to(device, non_blocking=True)\n            target = target.to(device, non_blocking=True).float().view(-1, 1)\n            \n            output = model(image, dense)\n            loss = loss_fn(output, target)\n            rmse_score = usr_rmse_score(output, target)\n            stream.set_description(f\"Epoch: {epoch:02}. Valid. Loss {loss}. RMSE {rmse_score}\")\n            \n            targets = (target.detach().cpu().numpy()*100).tolist()\n            outputs = (torch.sigmoid(output).detach().cpu().numpy()*100).tolist()\n            \n            final_targets.extend(targets)\n            final_outputs.extend(outputs)\n        \n    return final_targets, final_outputs","metadata":{"execution":{"iopub.status.busy":"2021-10-17T04:51:07.75613Z","iopub.execute_input":"2021-10-17T04:51:07.756413Z","iopub.status.idle":"2021-10-17T04:51:07.767003Z","shell.execute_reply.started":"2021-10-17T04:51:07.756388Z","shell.execute_reply":"2021-10-17T04:51:07.766307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test_fn(test_loader, model, device):\n    model.eval()\n    stream = tqdm(test_loader)\n    final_outputs = []\n    \n    with torch.no_grad():\n        for i, (image, dense) in enumerate(stream, start=1):\n            image = image.to(device, non_blocking=True)\n            dense = dense.to(device, non_blocking=True)\n            output = model(image, dense)\n            outputs = (torch.sigmoid(output).detach().cpu().numpy()*100).tolist()\n            final_outputs.extend(outputs)\n        \n    return final_outputs","metadata":{"execution":{"iopub.status.busy":"2021-10-17T04:51:07.76996Z","iopub.execute_input":"2021-10-17T04:51:07.770157Z","iopub.status.idle":"2021-10-17T04:51:07.778656Z","shell.execute_reply.started":"2021-10-17T04:51:07.77013Z","shell.execute_reply":"2021-10-17T04:51:07.777925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"best_models_of_each_fold = []\nrmse_tracker = []\nFOLDS = 10\nEPOCHS = 10\n\ndef get_dataset(df, images, state='training'):\n    ids = list(df['Id'])\n    image_paths = [os.path.join(images, idx + '.jpg') for idx in ids]\n    df['Pawpularity'] = df['Pawpularity']/100\n    target = df['Pawpularity'].values\n    df.drop(['Id', 'Pawpularity', 'kfold'], inplace=True, axis=1)\n    dense_feats = df.values\n\n    if state == 'training':\n        transform = train_transform_object(224)\n    elif state == 'validation' or state == 'testing':\n        transform = valid_transform_object(224)\n    else:\n        transform = None\n\n    return PetDataset(image_paths, dense_feats, target, transform)\n\nfor fold in range(FOLDS):    \n    data = pd.read_csv('../input/kfolddatasets/train_{}folds.csv'.format(FOLDS))\n\n    train = data[data['kfold'] != fold]\n    val = data[data['kfold'] == fold]\n    images = '../input/petfinder-pawpularity-score/train'\n\n    train_dataset = get_dataset(train, images)\n    val_dataset = get_dataset(val, images, state='validation')\n\n    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n\n    model_params = {\n        'model_name' : 'swin_small_patch4_window7_224',\n        'out_features' : 1,\n        'inp_channels' : 3,\n        'pretrained' : True,\n        'num_dense' : 12,\n    }\n    model = PetNet(**model_params)\n    model = model.to(device)\n    loss_fn = nn.BCEWithLogitsLoss()\n    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=1e-6, amsgrad=False)\n\n    best_rmse = np.inf\n    best_epoch = np.inf\n    best_model_name = None\n    for epoch in range(EPOCHS):\n        train_fn(train_loader, model, loss_fn, optimizer, epoch, device)\n        valid_targets, predictions = validation_fn(val_loader, model, loss_fn, epoch, device)\n        rmse = round(mean_squared_error(valid_targets, predictions, squared=False), 3)\n\n        if rmse < best_rmse:\n            best_rmse = rmse\n            best_epoch = epoch\n            if best_model_name is not None:\n                os.remove(best_model_name)\n            torch.save(model.state_dict(),f\"{model_params['model_name']}_{fold}_fold_{epoch}_epoch_{rmse}_rmse.pth\")\n            best_model_name = f\"{model_params['model_name']}_{fold}_fold_{epoch}_epoch_{rmse}_rmse.pth\"\n\n            print(f'The Best saved model is: {best_model_name}')\n            \n    best_models_of_each_fold.append(best_model_name)\n    rmse_tracker.append(best_rmse)\n    print(''.join(['#']*50))\n    del model\n    gc.collect()\n    torch.cuda.empty_cache()\n        \n        ","metadata":{"execution":{"iopub.status.busy":"2021-10-17T04:51:07.782025Z","iopub.execute_input":"2021-10-17T04:51:07.782243Z","iopub.status.idle":"2021-10-17T13:01:07.440934Z","shell.execute_reply.started":"2021-10-17T04:51:07.782202Z","shell.execute_reply":"2021-10-17T13:01:07.440199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted_labels = None\nmodels_dir = './'\nmodel_params = {\n    'model_name' : 'swin_small_patch4_window7_224',\n    'out_features' : 1,\n    'inp_channels' : 3,\n    'num_dense' : 12,\n    'pretrained' : False\n}\n\ndef get_testset(df, images):\n    ids = list(df['Id'])\n    image_paths = [os.path.join(images, idx + '.jpg') for idx in ids]\n    df.drop(['Id'], inplace=True, axis=1)\n    dense_feats = df.values\n    test_transform = valid_transform_object()\n    return PetTestset(image_paths, dense_feats, test_transform)\n\noutputs = None\nfor model_name in glob.glob(models_dir + '/*.pth'):\n    model = PetNet(**model_params)\n    model.load_state_dict(torch.load(model_name))\n    model = model.to(device)\n    \n    test_images = '../input/petfinder-pawpularity-score/test'\n    test_df = pd.read_csv('../input/petfinder-pawpularity-score/test.csv')\n    testset = get_testset(test_df, test_images)\n    test_loader = DataLoader(testset, batch_size=16, shuffle=False)\n    \n    if outputs is None:\n        outputs = test_fn(test_loader, model, device)\n    else:\n        temp = test_fn(test_loader, model, device)\n        for i in range(len(temp)):\n            outputs[i].append(temp[i][0])\n            \nfor i in range(len(outputs)):\n    outputs[i] = [sum(outputs[i]) / (len(glob.glob(models_dir + '/*.pth')))]","metadata":{"execution":{"iopub.status.busy":"2021-10-17T13:06:43.004698Z","iopub.execute_input":"2021-10-17T13:06:43.004979Z","iopub.status.idle":"2021-10-17T13:06:54.189112Z","shell.execute_reply.started":"2021-10-17T13:06:43.004948Z","shell.execute_reply":"2021-10-17T13:06:54.188327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_csv = pd.read_csv('../input/petfinder-pawpularity-score/sample_submission.csv')\nfor i in range(len(outputs)):\n    sub_csv.loc[i, 'Pawpularity'] = outputs[i][0]","metadata":{"execution":{"iopub.status.busy":"2021-10-17T13:06:59.236698Z","iopub.execute_input":"2021-10-17T13:06:59.237313Z","iopub.status.idle":"2021-10-17T13:06:59.250985Z","shell.execute_reply.started":"2021-10-17T13:06:59.237272Z","shell.execute_reply":"2021-10-17T13:06:59.250254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_csv.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-10-17T13:07:00.810717Z","iopub.execute_input":"2021-10-17T13:07:00.811419Z","iopub.status.idle":"2021-10-17T13:07:00.818956Z","shell.execute_reply.started":"2021-10-17T13:07:00.811383Z","shell.execute_reply":"2021-10-17T13:07:00.818211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_csv.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-17T13:07:02.431324Z","iopub.execute_input":"2021-10-17T13:07:02.431877Z","iopub.status.idle":"2021-10-17T13:07:02.449704Z","shell.execute_reply.started":"2021-10-17T13:07:02.431845Z","shell.execute_reply":"2021-10-17T13:07:02.449047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}