{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport sys\nimport numpy as np\nimport pandas as pd\nimport tensorflow_addons as tfa\nimport tensorflow as tf\nimport tensorflow.keras as keras\nimport tensorflow.keras.layers as layers\n\nimport pylab as pl\n\nsys.path.append('../input/swintransformertf')\nfrom swintransformer import SwinTransformer","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-14T15:43:56.594123Z","iopub.execute_input":"2021-11-14T15:43:56.594533Z","iopub.status.idle":"2021-11-14T15:43:56.600324Z","shell.execute_reply.started":"2021-11-14T15:43:56.59448Z","shell.execute_reply":"2021-11-14T15:43:56.599446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.random.set_seed(0)\nnp.random.seed(0)","metadata":{"execution":{"iopub.status.busy":"2021-11-14T15:43:56.602069Z","iopub.execute_input":"2021-11-14T15:43:56.602482Z","iopub.status.idle":"2021-11-14T15:43:56.650688Z","shell.execute_reply.started":"2021-11-14T15:43:56.602447Z","shell.execute_reply":"2021-11-14T15:43:56.649988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"RES = [384, 384]\nMETA_SHAPE = []\nBATCH_SIZE = 16\nBINS = 30\nFOLDS = 5\nEPOCHS = 5\nAUGMENT = True\nSEED = 2022","metadata":{"execution":{"iopub.status.busy":"2021-11-14T15:43:56.654676Z","iopub.execute_input":"2021-11-14T15:43:56.654913Z","iopub.status.idle":"2021-11-14T15:43:56.66113Z","shell.execute_reply.started":"2021-11-14T15:43:56.654856Z","shell.execute_reply":"2021-11-14T15:43:56.660437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# See https://www.kaggle.com/cdeotte/cutmix-and-mixup-on-gpu-tpu\ndef mixup(inputs, label, PROBABILITY=0.5):\n    # input image - is a batch of images of size [n,dim,dim,3] not a single image of [dim,dim,3]\n    # output - a batch of images with mixup applied\n    DIM = RES[0]\n    CLASSES = 1\n    \n    image = inputs['image_inp']\n    \n    batch_size = BATCH_SIZE\n    \n    imgs = []; labs = []\n    for j in range(batch_size):\n        # DO MIXUP WITH PROBABILITY DEFINED ABOVE\n        P = tf.cast( tf.random.uniform([],0,1)<=PROBABILITY, tf.float32)\n        # CHOOSE RANDOM\n        k = tf.cast( tf.random.uniform([],0,batch_size),tf.int32)\n        a = tf.random.uniform([],0,1)*P # this is beta dist with alpha=1.0\n        # MAKE MIXUP IMAGE\n        img1 = image[j,]\n        img2 = image[k,]\n        imgs.append((1-a)*img1 + a*img2)\n        \n        lab1 = label[j,]\n        lab2 = label[k,]\n        labs.append((1-a)*lab1 + a*lab2)\n            \n    # RESHAPE HACK SO TPU COMPILER KNOWS SHAPE OF OUTPUT TENSOR (maybe use Python typing instead?)\n    image2 = tf.reshape(tf.stack(imgs),(BATCH_SIZE,DIM,DIM,3))\n    label2 = tf.reshape(tf.stack(labs),(BATCH_SIZE,CLASSES))\n    return ({ 'image_inp': image2, 'meta_inp': inputs['meta_inp'] }, label2)","metadata":{"execution":{"iopub.status.busy":"2021-11-14T15:43:56.663561Z","iopub.execute_input":"2021-11-14T15:43:56.66381Z","iopub.status.idle":"2021-11-14T15:43:56.675431Z","shell.execute_reply.started":"2021-11-14T15:43:56.663778Z","shell.execute_reply":"2021-11-14T15:43:56.674734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_base_path = \"../input/petfinder-pawpularity-score/train/\"\ntrain_csv = pd.read_csv(\"../input/petfinder-pawpularity-score/train.csv\")\ntrain_data = np.array(train_csv)\n\nbinned = [[] for x in range(BINS)]\n\nfor datum in train_data:\n    datum[0] = train_base_path+datum[0]+\".jpg\"\n    binned[min(BINS-1, int(datum[-1]//(100/BINS)))].append(datum)\n    \ntrain_images = train_data[:,0]\ntrain_meta = train_data[:,1:-1].astype(np.float32)\ntrain_labels = train_data[:,-1].astype(np.float32) / 100.\n\nFOLD_SIZE = train_images.shape[0] // FOLDS\nprint(f\"Fold size: {FOLD_SIZE}\")\ntrain_datasets = []\nval_datasets = []\n\ndef get_preprocess_fn(training):\n    if training:\n        return preprocess_train\n    \n    return preprocess_val\n\ndef preprocess_general(inputs):\n    filename = inputs['image_inp']\n    image = tf.io.read_file(filename)\n    image = tf.io.decode_jpeg(image)\n    #image = tf.image.convert_image_dtype(image, tf.float32)\n    #image = tf.image.per_image_standardization(image)\n    image = tf.image.resize(image, RES)\n    image = keras.applications.imagenet_utils.preprocess_input(image, mode='torch')\n    \n    return image\n\ndef preprocess_train(inputs, labels):\n    image = preprocess_general(inputs)\n\n    if AUGMENT:\n        image = tf.image.random_flip_left_right(image, SEED)\n        image = tf.image.random_flip_up_down(image, SEED)\n        image = tf.image.random_hue(image, 0.05, SEED)\n        image = tf.image.random_contrast(image, 0.95, 1.05, SEED)\n        image = tf.image.random_brightness(image, 0.05, SEED)\n\n    return ({ 'image_inp': image, 'meta_inp': inputs['meta_inp'] }, labels)\n\ndef preprocess_val(inputs, labels):\n    image = preprocess_general(inputs)\n\n    return ({ 'image_inp': image, 'meta_inp': inputs['meta_inp'] }, labels)\n\ntotal_trainset = tf.data.Dataset.from_tensor_slices(\n        (\n            { 'image_inp': train_images, 'meta_inp': train_meta },\n            { 'output': train_labels }\n        )\n    ).map(get_preprocess_fn(True)).shuffle(1024).batch(BATCH_SIZE)\n\n# Returns train, val\ndef extract_validation_set(data, fold):\n    if FOLDS == 1:\n        return data[:int(0.8*len(data))], data[int(0.8*len(data)):]\n    \n    return np.concatenate((data[:fold*FOLD_SIZE], data[(fold+1)*FOLD_SIZE:]), axis=0), data[fold*FOLD_SIZE:(fold+1)*FOLD_SIZE]\n\nfor f in range(FOLDS):\n    train_images_fold, val_images_fold = extract_validation_set(train_images, f)\n    train_meta_fold, val_meta_fold = extract_validation_set(train_meta, f)\n    train_labels_fold, val_labels_fold = extract_validation_set(train_labels, f)\n\n    META_SHAPE = train_meta[0].shape\n\n    train_datasets.append(\n        tf.data.Dataset.from_tensor_slices(\n            (\n                { 'image_inp': train_images_fold, 'meta_inp': train_meta_fold },\n                train_labels_fold\n            )\n        ).map(get_preprocess_fn(True)).shuffle(256).batch(BATCH_SIZE, drop_remainder=True)#.map(mixup)\n    )\n    \n    val_datasets.append(\n        tf.data.Dataset.from_tensor_slices(\n            (\n                { 'image_inp': val_images_fold, 'meta_inp': val_meta_fold },\n                val_labels_fold\n            )\n        ).map(get_preprocess_fn(False)).shuffle(256).batch(BATCH_SIZE)\n    )","metadata":{"execution":{"iopub.status.busy":"2021-11-14T15:43:56.677247Z","iopub.execute_input":"2021-11-14T15:43:56.677914Z","iopub.status.idle":"2021-11-14T15:43:57.17623Z","shell.execute_reply.started":"2021-11-14T15:43:56.677878Z","shell.execute_reply":"2021-11-14T15:43:57.175553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_base_path = \"../input/petfinder-pawpularity-score/test/\"\ntest_csv = pd.read_csv(\"../input/petfinder-pawpularity-score/test.csv\")\ntest_ids = list(test_csv['Id'])\ntest_meta = np.array(test_csv.drop('Id', axis=1)).astype(np.float32)\n\ndef test_preprocess(input_dict):\n    image = preprocess_general(input_dict)\n    \n    return { 'image_inp': image, 'meta_inp': input_dict['meta_inp'] }\n\ntest_files = [test_base_path+s+\".jpg\" for s in test_ids]\ntest_dataset = tf.data.Dataset.from_tensor_slices({ 'image_inp': test_files, 'meta_inp': test_meta }).map(test_preprocess).batch(BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2021-11-14T15:43:57.179126Z","iopub.execute_input":"2021-11-14T15:43:57.179317Z","iopub.status.idle":"2021-11-14T15:43:57.232395Z","shell.execute_reply.started":"2021-11-14T15:43:57.179293Z","shell.execute_reply":"2021-11-14T15:43:57.231762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"RES = [RES[0], RES[1], 3]\n\nmodel_name = \"swin_tiny_224\"\n#backbone = SwinTransformer(model_name, \n#                           num_classes=1, \n#                           include_top=False, \n#                           pretrained=False)\n\nbackbone = keras.applications.EfficientNetB2(include_top=False, weights=None, pooling='avg')\n\ndef get_model():\n    image_inp = keras.Input(RES, name=\"image_inp\")\n    meta_inp = keras.Input(META_SHAPE, name=\"meta_inp\")\n    \n    #backbone.load_weights(f\"../input/swin-weights/{model_name}/{model_name}.ckpt\")\n    backbone.load_weights(\"../input/keras-applications-models/EfficientNetB2.h5\")\n    for layer in backbone.layers:\n        if isinstance(layer, layers.BatchNormalization):\n            layer.trainable = False\n        else:\n            layer.trainable = True\n    \n    #backbone.trainable = False\n    \n    x = backbone(image_inp)\n    x = layers.Dropout(0.2)(x)\n    #x = tf.concat([x, meta_inp], axis=-1)\n    #x = layers.Dense(128, activation='gelu')(x)\n    x = layers.Dense(1, name=\"output\")(x)\n    \n    return keras.Model(inputs=[image_inp, meta_inp], outputs=x)\n\nget_model()\n\ndef metric(labels, logits):\n    las = labels * 100.\n    los = tf.nn.sigmoid(logits) * 100.\n    mse = tf.math.reduce_mean(tf.math.square((las - los)))\n    rmse = tf.math.sqrt(mse)\n    \n    return rmse\n\ndef get_lr_callback():\n    lr_start   = 0.000005\n    lr_max     = 0.00000125 * BATCH_SIZE\n    lr_min     = 0.000001\n    lr_ramp_ep = 4\n    lr_sus_ep  = 0\n    lr_decay   = 0.8\n   \n    def lrfn(epoch):\n        if epoch < lr_ramp_ep:\n            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n            \n        elif epoch < lr_ramp_ep + lr_sus_ep:\n            lr = lr_max\n            \n        else:\n            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n            \n        return lr\n\n    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=True)\n    return lr_callback","metadata":{"execution":{"iopub.status.busy":"2021-11-14T15:43:57.235388Z","iopub.execute_input":"2021-11-14T15:43:57.235589Z","iopub.status.idle":"2021-11-14T15:44:01.05659Z","shell.execute_reply.started":"2021-11-14T15:43:57.235565Z","shell.execute_reply":"2021-11-14T15:44:01.055854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_predictions = []\n\nlr_schedule = keras.optimizers.schedules.ExponentialDecay(\n    initial_learning_rate=5e-5,\n    decay_steps=100, decay_rate=0.9,\n    staircase=True)\n\nopt = keras.optimizers.Adam(learning_rate=5e-6)\n\nfor fold in range(1):#fold, (train_dataset, val_dataset) in enumerate(zip(train_datasets, val_datasets)):\n    keras.backend.clear_session()\n    model = get_model()\n    model.compile(optimizer=opt, loss=keras.losses.BinaryCrossentropy(from_logits=True), metrics=metric)\n    print(f\"TRAINING FOLD: {fold+1}\")\n    #model.fit(train_dataset, validation_data=val_dataset, epochs=EPOCHS)\n    model.fit(total_trainset, epochs=EPOCHS)\n    fold_predictions = model.predict(test_dataset)\n    all_predictions.append(fold_predictions)\n    \nfinal_predictions = np.mean(all_predictions, axis=0)","metadata":{"execution":{"iopub.status.busy":"2021-11-14T15:44:01.058061Z","iopub.execute_input":"2021-11-14T15:44:01.058311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_dict = { 'Id': test_ids, 'Pawpularity': final_predictions[:,0] }\nsubmission = pd.DataFrame.from_dict(sub_dict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index=False)\nsubmission","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}