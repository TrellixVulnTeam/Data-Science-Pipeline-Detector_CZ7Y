{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Hi to everyone who decides to read this! \n\nThis is my first experience in computer vision and fine tuning pre-trained models, I couldn't get past such a nice dataset with cute animals :)\n\nThere will be no cross-validation, and I don't know how to use meta-information here either, so this is a very simple beginner's notebook, and I'm very glad that it works","metadata":{}},{"cell_type":"markdown","source":"Importing the necessary libraries and ensuring that timm works without an internet connection:","metadata":{}},{"cell_type":"code","source":"import os\nimport sys\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport cv2\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.transforms import ToTensor\n\nfrom sklearn.model_selection import train_test_split\n\nsys.path.append(\"../input/timm-pytorch-image-models\")\nimport timm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Downloading train and test meta DataFrames and appending path for each image:","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv('../input/petfinder-pawpularity-score/train.csv')\ndf_train['Img_path'] = df_train['Id'].apply(lambda i: '../input/petfinder-pawpularity-score/train/' + i + \".jpg\")\ndf_test = pd.read_csv('../input/petfinder-pawpularity-score/test.csv')\ndf_test['Pawpularity'] = 0\ndf_test['Img_path'] = df_test['Id'].apply(lambda j: '../input/petfinder-pawpularity-score/test/' + j + \".jpg\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Estimation of the Pawpularity value distribution:","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15,6))\nplt.hist(df_train['Pawpularity'], \n         bins=50,\n         color='lightsteelblue',\n         edgecolor='black')\nplt.ylabel('Count')\nplt.xlabel('Pawpularity')\nplt.title('Pawpularity distribution')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The distribution looks like a normal one. Values of Pawpularity=100 in this case will not be considered outliers due to the specifics of the data.","metadata":{}},{"cell_type":"markdown","source":"Defining some configuration variables:","metadata":{}},{"cell_type":"code","source":"image_size = 224\nbatch_size = 64\nnum_epochs = 10\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Defining three groups of transformations.\nFor validation, only Resize, Normalization and ToTensor are applied, and for visualization of a batch of images, on the contrary, Normalization and ToTensor are not applied.\nThe mean, std and max_pixel_value are taken from the imagenet dataset.","metadata":{}},{"cell_type":"code","source":"image_transforms = {'train_transform': A.Compose([A.Resize(image_size, image_size), \n                                                  A.HorizontalFlip(p=0.5), \n                                                  A.VerticalFlip(p=0.5), \n                                                  A.ToSepia(p=0.1), \n                                                  A.Normalize(mean=(0.485, 0.456, 0.406), \n                                                              std=(0.229, 0.224, 0.225), \n                                                              max_pixel_value=255.0, \n                                                              p=1.0), \n                                                  ToTensorV2()]),\n                    \n                   'validation_transform': A.Compose([A.Resize(image_size, image_size), \n                                                      A.Normalize(mean=(0.485, 0.456, 0.406), \n                                                                  std=(0.229, 0.224, 0.225), \n                                                                  max_pixel_value=255.0, \n                                                                  p=1.0), \n                                                      ToTensorV2()]),\n                   'visualization_transform': A.Compose([A.Resize(image_size, image_size), \n                                                         A.HorizontalFlip(p=0.5), \n                                                         A.VerticalFlip(p=0.5),\n                                                         A.ToSepia(p=0.1)])}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Creating custom dataset class:","metadata":{}},{"cell_type":"code","source":"class ImageDataset(Dataset):\n    def __init__(self, image_labels, image_dir, transform=None, target_transform=None):\n        self.image_labels = image_labels\n        self.image_dir = image_dir\n        self.transform = transform\n        self.target_transform = target_transform\n        \n        \n    def __len__(self):\n        return len(self.image_labels)\n    \n    \n    def __getitem__(self, index):\n        image_path = self.image_dir.iloc[index]\n        image = cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        label = self.image_labels.iloc[index]\n        if self.transform:\n            image = self.transform(image=image)['image']\n        if self.target_transform:\n            label = self.target_transform(label=label)\n        return image, label","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Splitting the data and creating train, val, test and visual instances of dataset class:","metadata":{}},{"cell_type":"code","source":"train_target = df_train['Pawpularity']\ntrain_features = df_train.drop(['Pawpularity'], axis=1)\n\ntest_target = df_test['Pawpularity']\ntest_features = df_test.drop(['Pawpularity'], axis=1)\n\nX_train, X_val, y_train, y_val = train_test_split(train_features, train_target, test_size=0.2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = ImageDataset(y_train, X_train['Img_path'], transform=image_transforms['train_transform'])\nval_dataset = ImageDataset(y_val, X_val['Img_path'], transform=image_transforms['validation_transform'])\ntest_dataset = ImageDataset(test_target, test_features['Img_path'], transform=image_transforms['validation_transform'])\nvisual_train_dataset =  ImageDataset(y_train, X_train['Img_path'], transform=image_transforms['visualization_transform'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Creating instances of DataLoader class for all of the datasets:","metadata":{}},{"cell_type":"code","source":"test_batch_size = len(test_dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False)\nvisual_loader = DataLoader(visual_train_dataset, batch_size=batch_size, shuffle=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visual_loader","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Checking that the size of the data in the batch matches the requirements of the neural network:","metadata":{}},{"cell_type":"code","source":"visual_train_f, visual_train_t = next(iter(visual_loader))\nprint(f'Feature batch shape: {visual_train_f.size()}')\nprint(f'Target batch shape: {visual_train_t.size()}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Yes, it's ok there.","metadata":{}},{"cell_type":"markdown","source":"Now just look at these augmented cuties that are in the same batch, I would take them all home if I could :)","metadata":{}},{"cell_type":"code","source":"def plot_batch(features, target, batch_size=batch_size):\n    '''Shows one batch of augmented images'''\n    plt.figure(figsize=(20, 60))\n    for i in range(batch_size):\n        img = features[i]\n        label = target[i]\n        \n        plt.subplot(16, 4, i+1)\n        plt.title(f'Pawpularity: {label}')\n        plt.imshow(img)\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_batch(visual_train_f, visual_train_t)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Downloading the model EfficientNet_b0 and pretrained state:","metadata":{}},{"cell_type":"code","source":"pretrained_path = '../input/timmefficientnet/tf_efficientnet_b0_ns-c0e6a31c.pth'\n\nmodel = timm.create_model('tf_efficientnet_b0_ns', pretrained=False, in_chans=3)\nmodel.load_state_dict(torch.load(pretrained_path))\n\nfor param in model.parameters():\n    param.requires_grad = False\n\nprint(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Changing the last Classifier layer for regression task:","metadata":{}},{"cell_type":"code","source":"model.classifier = nn.Sequential(nn.Linear(1280, 1000, bias=True), \n                                 nn.SiLU(inplace=True),\n                                 nn.Linear(1000, 1, bias=True))\n\nprint(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Defining optimizer and loss function:","metadata":{}},{"cell_type":"code","source":"model.to(device)\ncriterion = torch.nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Training the model with training loop:","metadata":{}},{"cell_type":"code","source":"def training_loop(model, training_loader, validation_loader, criterion, optimizer, epochs=num_epochs):\n    '''Training loop for train and eval modes'''\n    for epoch in range(1, epochs+1):\n        train_loss = 0\n        for image, target in training_loader:\n            image = image.to(device)\n            target = target.to(device)\n            target = target.unsqueeze(1)\n            optimizer.zero_grad()\n            outputs = model(image)\n            loss = torch.sqrt(criterion(outputs.float(), target.float()))\n            \n            loss.backward()\n            optimizer.step()\n            \n            train_loss += loss.item()\n            \n        with torch.no_grad():\n            model.eval()\n            valid_loss = 0\n            for val_image, val_target in validation_loader:\n                val_image = val_image.to(device)\n                val_target = val_target.to(device)\n                val_target = val_target.unsqueeze(1)\n                val_outputs = model(val_image)\n                val_loss = torch.sqrt(criterion(val_outputs.float(), val_target.float()))\n                \n                valid_loss += val_loss.item()\n                \n        print(f'Epoch: {epoch} Training loss: {train_loss/len(training_loader)}  Val loss: {valid_loss/len(validation_loader)}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_loop(model, train_loader, val_loader, criterion, optimizer, epochs=num_epochs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And finally, predictions:","metadata":{}},{"cell_type":"code","source":"model.eval()\npreds = []\nfor image, target in test_loader:\n    image = image.to(device)\n    target = target.to(device)\n    test_pred = model(image)\n    preds.extend(list(test_pred.cpu().detach().numpy().reshape(len(test_pred))))\n    \nimgs = list(df_test.iloc[:, 0].values)\npreds = [round(x, 2) for x in preds]\n\npred_df = pd.DataFrame({'Id': imgs, 'Pawpularity': preds})\npred_df.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pred_df.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Thank You for watching!","metadata":{}}]}