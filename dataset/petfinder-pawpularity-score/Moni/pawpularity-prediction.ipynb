{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Trabalho Final de Aprendizado de Máquina\n## PetFinder.my Pawpularity Contest\n\n---------------------------------------------\nAlexandre T. Bender e Moniele K. Ssntos ","metadata":{}},{"cell_type":"markdown","source":"\n## Descrição do Problema\n","metadata":{}},{"cell_type":"markdown","source":"Millions of stray animals suffer on the streets or are euthanized in shelters every day around the world. You might expect pets with attractive photos to generate more interest and be adopted faster. But what makes a good picture? With the help of data science, you may be able to accurately determine a pet photo’s appeal and even suggest improvements to give these rescue animals a higher chance of loving homes.\n\nIn this competition, you’ll analyze raw images and metadata to predict the “Pawpularity” of pet photos. You'll train and test your model on PetFinder.my's thousands of pet profiles. Winning versions will offer accurate recommendations that will improve animal welfare.","metadata":{}},{"cell_type":"markdown","source":"### Instalando dependências","metadata":{}},{"cell_type":"code","source":"!pip install git+https://github.com/rwightman/pytorch-image-models\n!pip install --upgrade wandb\n!pip install torch==1.10.0+cu113 torchvision==0.11.1+cu113 torchaudio==0.10.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html","metadata":{"execution":{"iopub.status.busy":"2021-12-03T17:25:25.315304Z","iopub.execute_input":"2021-12-03T17:25:25.315571Z","iopub.status.idle":"2021-12-03T17:28:13.580421Z","shell.execute_reply.started":"2021-12-03T17:25:25.315541Z","shell.execute_reply":"2021-12-03T17:28:13.579583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Importando Libs","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport cv2\nimport copy\nimport time\nimport random\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom torchvision.utils import make_grid\nfrom glob import glob\n\n# For data manipulation\nimport numpy as np\nimport pandas as pd\n\n# Pytorch Imports\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda import amp\n\n# Utils\nimport joblib\nfrom tqdm import tqdm\nfrom collections import defaultdict\n\n# Sklearn Imports\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold, KFold\n\nimport timm\n\n# Albumentations for augmentations\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\n# For colored terminal text\nfrom colorama import Fore, Back, Style\nb_ = Fore.BLUE\nsr_ = Style.RESET_ALL\n\n# Suppress warning messages\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# For descriptive error messages\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"","metadata":{"execution":{"iopub.status.busy":"2021-12-03T18:00:13.195899Z","iopub.execute_input":"2021-12-03T18:00:13.196771Z","iopub.status.idle":"2021-12-03T18:00:13.204756Z","shell.execute_reply.started":"2021-12-03T18:00:13.196731Z","shell.execute_reply":"2021-12-03T18:00:13.203776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Pesos e Biases\n\nWeights & Biases (W&B) is a set of machine learning tools that helps you build better models faster. Kaggle competitions require fast-paced model development and evaluation. There are a lot of components: exploring the training data, training different models, combining trained models in different combinations (ensembling), and so on.\n\n\nW&B can be useful for Kaggle competition with it's lightweight and interoperable tools:\n\n- Quickly track experiments,\n- Version and iterate on datasets,\n- Evaluate model performance,\n- Reproduce models,\n- Visualize results and spot regressions,\n- Share findings with colleagues.","metadata":{}},{"cell_type":"code","source":"import wandb\n\ntry:\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    api_key = user_secrets.get_secret(\"wandb_api\")\n    wandb.login(key=api_key)\n    anony = None\nexcept:\n    anony = \"must\"\n    print('If you want to use your W&B account, go to Add-ons -> Secrets and provide your W&B access token. Use the Label name as wandb_api. \\nGet your W&B access token from here: https://wandb.ai/authorize')","metadata":{"execution":{"iopub.status.busy":"2021-12-03T17:28:17.45172Z","iopub.execute_input":"2021-12-03T17:28:17.451974Z","iopub.status.idle":"2021-12-03T17:28:17.594573Z","shell.execute_reply.started":"2021-12-03T17:28:17.451941Z","shell.execute_reply":"2021-12-03T17:28:17.593825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ROOT_DIR = \"../input/petfinder-pawpularity-score\"\nTRAIN_DIR = \"../input/petfinder-pawpularity-score/train\"\nTEST_DIR = \"../input/petfinder-pawpularity-score/test\"","metadata":{"execution":{"iopub.status.busy":"2021-12-03T17:28:17.596589Z","iopub.execute_input":"2021-12-03T17:28:17.596842Z","iopub.status.idle":"2021-12-03T17:28:17.600628Z","shell.execute_reply.started":"2021-12-03T17:28:17.596808Z","shell.execute_reply":"2021-12-03T17:28:17.599788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CONFIG = dict(\n    seed = 313,\n    model_name = 'tf_efficientnet_b4_ns',\n    train_batch_size = 16,\n    valid_batch_size = 32,\n    img_size = 512,\n    epochs = 5,\n    learning_rate = 1e-4,\n    scheduler = 'CosineAnnealingLR',\n    min_lr = 1e-6,\n    T_max = 100,\n    T_0 = 25,\n    warmup_epochs = 0,\n    weight_decay = 1e-6,\n    n_accumulate = 1,\n    n_fold = 5,\n    num_classes = 1,\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n    competition = 'PetFinder',\n    _wandb_kernel = 'deb'\n)","metadata":{"execution":{"iopub.status.busy":"2021-12-03T17:28:17.602272Z","iopub.execute_input":"2021-12-03T17:28:17.602545Z","iopub.status.idle":"2021-12-03T17:28:17.649262Z","shell.execute_reply.started":"2021-12-03T17:28:17.602511Z","shell.execute_reply":"2021-12-03T17:28:17.648365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Configurando Semente para o experimento","metadata":{}},{"cell_type":"code","source":"def set_seed(seed = 313):\n    '''Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.'''\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    # When running on the CuDNN backend, two further options must be set\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \nset_seed(313)","metadata":{"execution":{"iopub.status.busy":"2021-12-03T17:28:17.650564Z","iopub.execute_input":"2021-12-03T17:28:17.651027Z","iopub.status.idle":"2021-12-03T17:28:17.661199Z","shell.execute_reply.started":"2021-12-03T17:28:17.650992Z","shell.execute_reply":"2021-12-03T17:28:17.660372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_train_file_path(id):\n    return f\"{TRAIN_DIR}/{id}.jpg\"","metadata":{"execution":{"iopub.status.busy":"2021-12-03T17:28:17.662623Z","iopub.execute_input":"2021-12-03T17:28:17.662989Z","iopub.status.idle":"2021-12-03T17:28:17.669332Z","shell.execute_reply.started":"2021-12-03T17:28:17.662953Z","shell.execute_reply":"2021-12-03T17:28:17.668381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Lendo os dados","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(f\"{ROOT_DIR}/train.csv\")\ndf['file_path'] = df['Id'].apply(get_train_file_path)","metadata":{"execution":{"iopub.status.busy":"2021-12-03T17:28:17.67116Z","iopub.execute_input":"2021-12-03T17:28:17.671728Z","iopub.status.idle":"2021-12-03T17:28:17.729696Z","shell.execute_reply.started":"2021-12-03T17:28:17.671691Z","shell.execute_reply":"2021-12-03T17:28:17.729061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_cols = [col for col in df.columns if col not in ['Id', 'Pawpularity', 'file_path']]","metadata":{"execution":{"iopub.status.busy":"2021-12-03T17:28:17.730919Z","iopub.execute_input":"2021-12-03T17:28:17.731167Z","iopub.status.idle":"2021-12-03T17:28:17.737537Z","shell.execute_reply.started":"2021-12-03T17:28:17.731133Z","shell.execute_reply":"2021-12-03T17:28:17.736834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualizando Imagens","metadata":{}},{"cell_type":"code","source":"img_path = '../input/petfinder-pawpularity-score/train/'\next = '.jpg'\n\ncols = ['Subject Focus', 'Eyes', 'Face', 'Near', 'Action', 'Accessory',\n       'Group', 'Collage', 'Human', 'Occlusion', 'Info', 'Blur']\n\nnums = len(cols)\n\npath = '../input/petfinder-pawpularity-score/'\n\n#Get the metadata (the .csv data) and put it into DataFrames\ntrain_df = pd.read_csv(path + 'train.csv')\n\n#Get the image data (the .jpg data) and put it into lists of filenames\ntrain_jpg = glob(path + \"train/*.jpg\")\n\n# col = 'Eyes'\nfor count, col in enumerate(cols):\n    sample = train_df.loc[train_df[col] == 1,'Id'].head(100).values[np.random.randint(10)]\n    \n    pawpularity = train_df.loc[train_df['Id'] == sample, 'Pawpularity'].head(1).values[-1]\n    \n    image_loc = img_path + sample + ext\n\n    image_array = plt.imread(image_loc)\n    plt.imshow(image_array)\n    \n    plt.title(f'Image of pet with {col}\\nPawpularity Score: {pawpularity}') \n    plt.axis('off') #turns off the gridlines\n    plt.show()\n\n    del sample, image_loc, image_array\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-12-03T18:00:24.454545Z","iopub.execute_input":"2021-12-03T18:00:24.455135Z","iopub.status.idle":"2021-12-03T18:00:29.949327Z","shell.execute_reply.started":"2021-12-03T18:00:24.455097Z","shell.execute_reply":"2021-12-03T18:00:29.948557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"run = wandb.init(project='Pawpularity', \n                 config=CONFIG,\n                 job_type='Visualization',\n                 group='Public_baseline',\n                 anonymous='must')","metadata":{"execution":{"iopub.status.busy":"2021-12-03T17:28:17.741137Z","iopub.execute_input":"2021-12-03T17:28:17.741347Z","iopub.status.idle":"2021-12-03T17:28:25.407103Z","shell.execute_reply.started":"2021-12-03T17:28:17.741323Z","shell.execute_reply":"2021-12-03T17:28:25.406412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preview_table = wandb.Table(columns=['Id', 'Image', 'Subject Focus', 'Eyes', 'Face', \n                                     'Near', 'Action', 'Accessory', 'Group', 'Collage', \n                                     'Human', 'Occlusion', 'Info', 'Blur', 'Pawpularity'])\ntmp_df = df.sample(1000, random_state=CONFIG['seed']).reset_index(drop=True)\nfor i in tqdm(range(len(tmp_df))):\n    row = tmp_df.loc[i]\n    img = Image.open(row.file_path)\n    preview_table.add_data(row['Id'],\n                           wandb.Image(img),\n                           row['Subject Focus'],\n                           row['Eyes'],\n                           row['Face'],\n                           row['Near'],\n                           row['Action'],\n                           row['Accessory'],\n                           row['Group'],\n                           row['Collage'],\n                           row['Human'],\n                           row['Occlusion'],\n                           row['Info'],\n                           row['Blur'],\n                           row['Pawpularity'])\n\nwandb.log({'Visualization': preview_table})\nrun.finish()","metadata":{"execution":{"iopub.status.busy":"2021-12-03T17:28:25.40836Z","iopub.execute_input":"2021-12-03T17:28:25.408657Z","iopub.status.idle":"2021-12-03T17:34:01.000293Z","shell.execute_reply.started":"2021-12-03T17:28:25.408586Z","shell.execute_reply":"2021-12-03T17:34:00.999542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Code taken from https://www.kaggle.com/ayuraj/interactive-eda-using-w-b-tables\n\n# This is just to display the W&B run page in this interactive session.\nfrom IPython import display\n\n# we create an IFrame and set the width and height\niF = display.IFrame(run.url, width=1080, height=720)\niF","metadata":{"execution":{"iopub.status.busy":"2021-12-03T17:34:01.001436Z","iopub.execute_input":"2021-12-03T17:34:01.001719Z","iopub.status.idle":"2021-12-03T17:34:01.012714Z","shell.execute_reply.started":"2021-12-03T17:34:01.001679Z","shell.execute_reply":"2021-12-03T17:34:01.011899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Criando Folds","metadata":{}},{"cell_type":"code","source":"def create_folds(df, n_s=5, n_grp=None):\n    df['kfold'] = -1\n    \n    if n_grp is None:\n        skf = KFold(n_splits=n_s, random_state=CONFIG['seed'])\n        target = df['Pawpularity']\n    else:\n        skf = StratifiedKFold(n_splits=n_s, shuffle=True, random_state=CONFIG['seed'])\n        df['grp'] = pd.cut(df['Pawpularity'], n_grp, labels=False)\n        target = df.grp\n    \n    for fold_no, (t, v) in enumerate(skf.split(target, target)):\n        df.loc[v, 'kfold'] = fold_no\n\n    df = df.drop('grp', axis=1)\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2021-12-03T17:34:01.01416Z","iopub.execute_input":"2021-12-03T17:34:01.015127Z","iopub.status.idle":"2021-12-03T17:34:01.022959Z","shell.execute_reply.started":"2021-12-03T17:34:01.015089Z","shell.execute_reply":"2021-12-03T17:34:01.022287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = create_folds(df, n_s=CONFIG['n_fold'], n_grp=14)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-03T17:34:01.024378Z","iopub.execute_input":"2021-12-03T17:34:01.024977Z","iopub.status.idle":"2021-12-03T17:34:01.067159Z","shell.execute_reply.started":"2021-12-03T17:34:01.024937Z","shell.execute_reply":"2021-12-03T17:34:01.06652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PawpularityDataset(Dataset):\n    def __init__(self, root_dir, df, transforms=None):\n        self.root_dir = root_dir\n        self.df = df\n        self.file_names = df['file_path'].values\n        self.targets = df['Pawpularity'].values\n        self.meta = df[feature_cols].values\n        self.transforms = transforms\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        img_path = self.file_names[index]\n        img = cv2.imread(img_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        meta = self.meta[index, :]\n        target = self.targets[index]\n        \n        if self.transforms:\n            img = self.transforms(image=img)[\"image\"]\n            \n        return img, meta, target","metadata":{"execution":{"iopub.status.busy":"2021-12-03T17:34:01.068284Z","iopub.execute_input":"2021-12-03T17:34:01.068511Z","iopub.status.idle":"2021-12-03T17:34:01.077561Z","shell.execute_reply.started":"2021-12-03T17:34:01.068483Z","shell.execute_reply":"2021-12-03T17:34:01.07441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Augmentations","metadata":{}},{"cell_type":"code","source":"data_transforms = {\n    \"train\": A.Compose([\n        A.Resize(CONFIG['img_size'], CONFIG['img_size']),\n        A.HorizontalFlip(p=0.5),\n        A.Normalize(\n                mean=[0.485, 0.456, 0.406], \n                std=[0.229, 0.224, 0.225], \n                max_pixel_value=255.0, \n                p=1.0\n            ),\n        ToTensorV2()], p=1.),\n    \n    \"valid\": A.Compose([\n        A.Resize(CONFIG['img_size'], CONFIG['img_size']),\n        A.Normalize(\n                mean=[0.485, 0.456, 0.406], \n                std=[0.229, 0.224, 0.225], \n                max_pixel_value=255.0, \n                p=1.0\n            ),\n        ToTensorV2()], p=1.)\n}","metadata":{"execution":{"iopub.status.busy":"2021-12-03T17:34:01.079571Z","iopub.execute_input":"2021-12-03T17:34:01.079932Z","iopub.status.idle":"2021-12-03T17:34:01.089397Z","shell.execute_reply.started":"2021-12-03T17:34:01.079898Z","shell.execute_reply":"2021-12-03T17:34:01.088666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PawpularityModel(nn.Module):\n    def __init__(self, model_name, pretrained=True):\n        super(PawpularityModel, self).__init__()\n        self.model = timm.create_model(model_name, pretrained=pretrained, num_classes=0)\n        self.fc = nn.LazyLinear(CONFIG['num_classes'])\n        self.dropout = nn.Dropout(p=0.3)\n\n    def forward(self, images, meta):\n        features = self.model(images)                 # features = (bs, embedding_size)\n        features = self.dropout(features)\n        features = torch.cat([features, meta], dim=1) # features = (bs, embedding_size + 12)\n        output = self.fc(features)                    # outputs  = (bs, num_classes)\n        return output\n    \nmodel = PawpularityModel(CONFIG['model_name'])\nmodel.to(CONFIG['device']);","metadata":{"execution":{"iopub.status.busy":"2021-12-03T17:34:01.091871Z","iopub.execute_input":"2021-12-03T17:34:01.092324Z","iopub.status.idle":"2021-12-03T17:34:06.455346Z","shell.execute_reply.started":"2021-12-03T17:34:01.092292Z","shell.execute_reply":"2021-12-03T17:34:06.454323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dummy run to initialize the layers \nimg = torch.randn(1, 3, CONFIG['img_size'], CONFIG['img_size']).to(CONFIG['device'])\nmeta = torch.randn(1, len(feature_cols)).to(CONFIG['device'])\nmodel(img, meta)","metadata":{"execution":{"iopub.status.busy":"2021-12-03T17:34:06.456985Z","iopub.execute_input":"2021-12-03T17:34:06.457265Z","iopub.status.idle":"2021-12-03T17:34:06.827402Z","shell.execute_reply.started":"2021-12-03T17:34:06.457227Z","shell.execute_reply":"2021-12-03T17:34:06.826588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Função Loss","metadata":{}},{"cell_type":"code","source":"def criterion(outputs, targets):\n    return torch.sqrt(nn.MSELoss()(outputs.view(-1), targets.view(-1)))","metadata":{"execution":{"iopub.status.busy":"2021-12-03T17:34:06.828854Z","iopub.execute_input":"2021-12-03T17:34:06.82913Z","iopub.status.idle":"2021-12-03T17:34:06.834602Z","shell.execute_reply.started":"2021-12-03T17:34:06.829083Z","shell.execute_reply":"2021-12-03T17:34:06.83378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Definindo funções de Treinamento e Validação","metadata":{}},{"cell_type":"code","source":"def train_one_epoch(model, optimizer, scheduler, dataloader, device, epoch):\n    model.train()\n    scaler = amp.GradScaler()\n    \n    dataset_size = 0\n    running_loss = 0.0\n    \n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    for step, (images, meta, targets) in bar:         \n        images = images.to(device, dtype=torch.float)\n        meta = meta.to(device, dtype=torch.float)\n        targets = targets.to(device, dtype=torch.float)\n        \n        batch_size = images.size(0)\n        \n        with amp.autocast(enabled=True):\n            outputs = model(images, meta)\n            loss = criterion(outputs, targets)\n            loss = loss / CONFIG['n_accumulate']\n            \n        scaler.scale(loss).backward()\n    \n        if (step + 1) % CONFIG['n_accumulate'] == 0:\n            scaler.step(optimizer)\n            scaler.update()\n\n            # zero the parameter gradients\n            optimizer.zero_grad()\n\n            if scheduler is not None:\n                scheduler.step()\n                \n        running_loss += (loss.item() * batch_size)\n        dataset_size += batch_size\n        \n        epoch_loss = running_loss / dataset_size\n        \n        bar.set_postfix(Epoch=epoch, Train_Loss=epoch_loss,\n                        LR=optimizer.param_groups[0]['lr'])\n    gc.collect()\n    \n    return epoch_loss","metadata":{"execution":{"iopub.status.busy":"2021-12-03T17:34:06.836438Z","iopub.execute_input":"2021-12-03T17:34:06.837018Z","iopub.status.idle":"2021-12-03T17:34:06.84948Z","shell.execute_reply.started":"2021-12-03T17:34:06.836977Z","shell.execute_reply":"2021-12-03T17:34:06.848586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@torch.no_grad()\ndef valid_one_epoch(model, dataloader, device, epoch):\n    model.eval()\n    \n    dataset_size = 0\n    running_loss = 0.0\n    \n    TARGETS = []\n    PREDS = []\n    \n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    for step, (images, meta, targets) in bar:        \n        images = images.to(device, dtype=torch.float)\n        meta = meta.to(device, dtype=torch.float)\n        targets = targets.to(device, dtype=torch.float)\n        \n        batch_size = images.size(0)\n        \n        outputs = model(images, meta)\n        loss = criterion(outputs, targets)\n        \n        running_loss += (loss.item() * batch_size)\n        dataset_size += batch_size\n        \n        epoch_loss = running_loss / dataset_size\n        \n        PREDS.append(outputs.view(-1).cpu().detach().numpy())\n        TARGETS.append(targets.view(-1).cpu().detach().numpy())\n        \n        bar.set_postfix(Epoch=epoch, Valid_Loss=epoch_loss,\n                        LR=optimizer.param_groups[0]['lr'])   \n    \n    TARGETS = np.concatenate(TARGETS)\n    PREDS = np.concatenate(PREDS)\n    val_rmse = mean_squared_error(TARGETS, PREDS, squared=False)\n    gc.collect()\n    \n    return epoch_loss, val_rmse","metadata":{"execution":{"iopub.status.busy":"2021-12-03T17:34:06.850945Z","iopub.execute_input":"2021-12-03T17:34:06.85136Z","iopub.status.idle":"2021-12-03T17:34:06.863714Z","shell.execute_reply.started":"2021-12-03T17:34:06.851316Z","shell.execute_reply":"2021-12-03T17:34:06.862851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Treinamento","metadata":{}},{"cell_type":"code","source":"def run_training(model, optimizer, scheduler, device, num_epochs):\n    # To automatically log gradients\n    wandb.watch(model, log_freq=100)\n    \n    if torch.cuda.is_available():\n        print(\"[INFO] Using GPU: {}\\n\".format(torch.cuda.get_device_name()))\n    \n    start = time.time()\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_epoch_rmse = np.inf\n    history = defaultdict(list)\n    \n    for epoch in range(1, num_epochs + 1): \n        gc.collect()\n        train_epoch_loss = train_one_epoch(model, optimizer, scheduler, \n                                           dataloader=train_loader, \n                                           device=CONFIG['device'], epoch=epoch)\n        \n        val_epoch_loss, val_epoch_rmse = valid_one_epoch(model, valid_loader, \n                                                         device=CONFIG['device'], \n                                                         epoch=epoch)\n    \n        history['Train Loss'].append(train_epoch_loss)\n        history['Valid Loss'].append(val_epoch_loss)\n        history['Valid RMSE'].append(val_epoch_rmse)\n        \n        # Log the metrics\n        wandb.log({\"Train Loss\": train_epoch_loss})\n        wandb.log({\"Valid Loss\": val_epoch_loss})\n        wandb.log({\"Valid RMSE\": val_epoch_rmse})\n        \n        print(f'Valid RMSE: {val_epoch_rmse}')\n        \n        # deep copy the model\n        if val_epoch_rmse <= best_epoch_rmse:\n            print(f\"{b_}Validation Loss Improved ({best_epoch_rmse} ---> {val_epoch_rmse})\")\n            best_epoch_rmse = val_epoch_rmse\n            run.summary[\"Best RMSE\"] = best_epoch_rmse\n            best_model_wts = copy.deepcopy(model.state_dict())\n            PATH = \"RMSE{:.4f}_epoch{:.0f}.bin\".format(best_epoch_rmse, epoch)\n            torch.save(model.state_dict(), PATH)\n            # Save a model file from the current directory\n            wandb.save(PATH)\n            print(f\"Model Saved{sr_}\")\n            \n        print()\n    \n    end = time.time()\n    time_elapsed = end - start\n    print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n        time_elapsed // 3600, (time_elapsed % 3600) // 60, (time_elapsed % 3600) % 60))\n    print(\"Best RMSE: {:.4f}\".format(best_epoch_rmse))\n    \n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    \n    return model, history","metadata":{"execution":{"iopub.status.busy":"2021-12-03T17:34:06.86524Z","iopub.execute_input":"2021-12-03T17:34:06.865537Z","iopub.status.idle":"2021-12-03T17:34:06.879876Z","shell.execute_reply.started":"2021-12-03T17:34:06.865503Z","shell.execute_reply":"2021-12-03T17:34:06.87915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_loaders(fold):\n    df_train = df[df.kfold != fold].reset_index(drop=True)\n    df_valid = df[df.kfold == fold].reset_index(drop=True)\n    \n    train_dataset = PawpularityDataset(TRAIN_DIR, df_train, transforms=data_transforms['train'])\n    valid_dataset = PawpularityDataset(TRAIN_DIR, df_valid, transforms=data_transforms['valid'])\n\n    train_loader = DataLoader(train_dataset, batch_size=CONFIG['train_batch_size'], \n                              num_workers=4, shuffle=True, pin_memory=True, drop_last=True)\n    valid_loader = DataLoader(valid_dataset, batch_size=CONFIG['valid_batch_size'], \n                              num_workers=4, shuffle=False, pin_memory=True)\n    \n    return train_loader, valid_loader","metadata":{"execution":{"iopub.status.busy":"2021-12-03T17:34:06.881204Z","iopub.execute_input":"2021-12-03T17:34:06.881968Z","iopub.status.idle":"2021-12-03T17:34:06.891466Z","shell.execute_reply.started":"2021-12-03T17:34:06.881927Z","shell.execute_reply":"2021-12-03T17:34:06.890477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fetch_scheduler(optimizer):\n    if CONFIG['scheduler'] == 'CosineAnnealingLR':\n        scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=CONFIG['T_max'], \n                                                   eta_min=CONFIG['min_lr'])\n    elif CONFIG['scheduler'] == 'CosineAnnealingWarmRestarts':\n        scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=CONFIG['T_0'], \n                                                             eta_min=CONFIG['min_lr'])\n    elif CONFIG['scheduler'] == None:\n        return None\n        \n    return scheduler","metadata":{"execution":{"iopub.status.busy":"2021-12-03T17:34:06.893181Z","iopub.execute_input":"2021-12-03T17:34:06.89364Z","iopub.status.idle":"2021-12-03T17:34:06.902127Z","shell.execute_reply.started":"2021-12-03T17:34:06.893601Z","shell.execute_reply":"2021-12-03T17:34:06.901334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Separando os dados","metadata":{}},{"cell_type":"code","source":"train_loader, valid_loader = prepare_loaders(fold=0)","metadata":{"execution":{"iopub.status.busy":"2021-12-03T17:34:06.90516Z","iopub.execute_input":"2021-12-03T17:34:06.90558Z","iopub.status.idle":"2021-12-03T17:34:06.916831Z","shell.execute_reply.started":"2021-12-03T17:34:06.905555Z","shell.execute_reply":"2021-12-03T17:34:06.91614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Otimizador","metadata":{}},{"cell_type":"code","source":"optimizer = optim.RAdam(model.parameters(), lr=CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'])\nscheduler = fetch_scheduler(optimizer)","metadata":{"execution":{"iopub.status.busy":"2021-12-03T17:43:13.747909Z","iopub.execute_input":"2021-12-03T17:43:13.748688Z","iopub.status.idle":"2021-12-03T17:43:13.756089Z","shell.execute_reply.started":"2021-12-03T17:43:13.748633Z","shell.execute_reply":"2021-12-03T17:43:13.755251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"run = wandb.init(project='Pawpularity', \n                 config=CONFIG,\n                 job_type='Train',\n                 group='Public_baseline',\n                 anonymous='must')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Iniciando treino...","metadata":{}},{"cell_type":"code","source":"model, history = run_training(model, optimizer, scheduler,\n                              device=CONFIG['device'],\n                              num_epochs=CONFIG['epochs'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"run.finish()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\n#plot\n\nplt.figure()\nplt.plot(history.history[\"rmse\"], label=\"train_rmse\")\nplt.plot(history.history[\"val_rmse\"], label=\"val_rmse\")\nplt.title(\"RMSE train/validation by Epoch\")\nplt.xlabel(\"Epoch #\")\nplt.ylabel(\"RMSE\")\nplt.legend(loc=\"upper right\");\n\n'''\n","metadata":{"execution":{"iopub.status.busy":"2021-12-03T18:08:42.816966Z","iopub.execute_input":"2021-12-03T18:08:42.81755Z","iopub.status.idle":"2021-12-03T18:08:42.822521Z","shell.execute_reply.started":"2021-12-03T18:08:42.817511Z","shell.execute_reply":"2021-12-03T18:08:42.821872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualizações","metadata":{}},{"cell_type":"code","source":"# This is just to display the W&B run page in this interactive session.\nfrom IPython import display\n\n# we create an IFrame and set the width and height\niF = display.IFrame(run.url, width=1080, height=720)\niF","metadata":{},"execution_count":null,"outputs":[]}]}