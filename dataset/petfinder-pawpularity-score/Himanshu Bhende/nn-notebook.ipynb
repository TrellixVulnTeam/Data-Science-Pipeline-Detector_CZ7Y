{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-15T04:48:22.90018Z","iopub.execute_input":"2022-01-15T04:48:22.900682Z","iopub.status.idle":"2022-01-15T04:48:27.469546Z","shell.execute_reply.started":"2022-01-15T04:48:22.900647Z","shell.execute_reply":"2022-01-15T04:48:27.468456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nimport matplotlib.pylab as plt\nimport os\nimport PIL","metadata":{"execution":{"iopub.status.busy":"2022-01-15T04:48:39.043162Z","iopub.execute_input":"2022-01-15T04:48:39.043667Z","iopub.status.idle":"2022-01-15T04:48:39.053531Z","shell.execute_reply.started":"2022-01-15T04:48:39.043622Z","shell.execute_reply":"2022-01-15T04:48:39.051981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('TensorFlow version: {}'.format(tf.__version__))\ndevice_name = tf.test.gpu_device_name()\nif device_name != '/device:GPU:0':\n    print('GPU device not found - On for CPU time!')\nelse:\n    print('Found GPU at {}'.format(device_name))","metadata":{"execution":{"iopub.status.busy":"2022-01-15T04:48:42.824954Z","iopub.execute_input":"2022-01-15T04:48:42.825241Z","iopub.status.idle":"2022-01-15T04:48:45.33224Z","shell.execute_reply.started":"2022-01-15T04:48:42.8252Z","shell.execute_reply":"2022-01-15T04:48:45.331017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's define some helpers\n\nIMG_HEIGHT = 150 # Let's try to arbitrarily set 150px x 150px images\nIMG_WIDTH = 150\nIMG_CHANNELS = 3\n\ndef read_and_decode(filename, reshape_dims):\n    # Read an image file to a tensor as a sequence of bytes\n    img = tf.io.read_file(filename)\n    # Convert the tensor to a 3D uint8 tensor\n    img = tf.image.decode_jpeg(img, channels=IMG_CHANNELS)\n    # Convert 3D uint8 tensor \n    img = tf.image.convert_image_dtype(img, tf.float32)\n    # Resize the image to the desired size\n    return tf.image.resize(img, reshape_dims)\n\ndef show_image(filename):\n    img = read_and_decode(filename, [IMG_HEIGHT, IMG_WIDTH])\n    plt.imshow(img.numpy());\n    plt.axis('off');\n\ndef decode_csv(csv_row):\n    record_defaults = ['Id', 'Pawpularity']\n    filename, pawpularity = tf.io.decode_csv(csv_row, record_defaults)\n    pawpularity = tf.convert_to_tensor(np.float(pawpularity), dtype=tf.float32)\n    img = read_and_decode(filename, [IMG_HEIGHT, IMG_WIDTH])\n    return img, pawpularity","metadata":{"execution":{"iopub.status.busy":"2022-01-15T04:48:47.798168Z","iopub.execute_input":"2022-01-15T04:48:47.798957Z","iopub.status.idle":"2022-01-15T04:48:47.811555Z","shell.execute_reply.started":"2022-01-15T04:48:47.79892Z","shell.execute_reply":"2022-01-15T04:48:47.810422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_path = '../input/petfinder-pawpularity-score/'\ndata = pd.read_csv(data_path+'train.csv')\n\n# Use stratified sampling\nsssplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2)\nfor train_index, test_index in sssplit.split(data, data['Pawpularity']):\n    training_set = data.iloc[train_index]\n    eval_set = data.iloc[test_index]\n    \n# Visually check distribution of pawpularity score in training and test sets\ntraining_set['Pawpularity'].hist(label='Training set')\neval_set['Pawpularity'].hist(label='Eval set')\nplt.title('Pawpularity score distribution in training and test set')\nplt.xlabel('Pawpularity score')\nplt.ylabel('Count')\nplt.legend(loc='upper right')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-01-15T04:48:51.696897Z","iopub.execute_input":"2022-01-15T04:48:51.697172Z","iopub.status.idle":"2022-01-15T04:48:52.087962Z","shell.execute_reply.started":"2022-01-15T04:48:51.697142Z","shell.execute_reply":"2022-01-15T04:48:52.086898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Export training and test sets as .csv files\ntraining_set['Id'] = training_set['Id'].apply(lambda x: '../input/petfinder-pawpularity-score/train/'+x+'.jpg')\ntraining_set[['Id', 'Pawpularity']].to_csv('/kaggle/working/training_set.csv', header=False, index=False)\neval_set['Id'] = eval_set['Id'].apply(lambda x: '../input/petfinder-pawpularity-score/train/'+x+'.jpg')\neval_set[['Id', 'Pawpularity']].to_csv('/kaggle/working/eval_set.csv', header=False, index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-15T04:49:03.124708Z","iopub.execute_input":"2022-01-15T04:49:03.124992Z","iopub.status.idle":"2022-01-15T04:49:03.195513Z","shell.execute_reply.started":"2022-01-15T04:49:03.124964Z","shell.execute_reply":"2022-01-15T04:49:03.19316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 32\nIMG_HEIGHT = 164\nIMG_WIDTH = 164\nIMG_CHANNELS = 3\n\ntrain_dataset = tf.data.TextLineDataset('/kaggle/working/training_set.csv').map(decode_csv).batch(BATCH_SIZE)\n\neval_dataset = tf.data.TextLineDataset('/kaggle/working/eval_set.csv').map(decode_csv).batch(BATCH_SIZE)\n\n# Our neural network is built as a Sequential model with a single hidden layer\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Flatten(input_shape=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)),\n    tf.keras.layers.Dense(units=300, activation='relu'), \n    tf.keras.layers.Dense(units=30, activation='relu'),\n    tf.keras.layers.Dense(units=1, activation=None)\n])\n# Let's compile it with Adam, a well-suited optimiser for CV problems\nmodel.compile(optimizer='adam', loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.RootMeanSquaredError()])\n\n# And let's now train our model with the training and evaluation data\nhistory = model.fit(train_dataset, validation_data=eval_dataset, epochs=20)","metadata":{"execution":{"iopub.status.busy":"2022-01-15T04:49:06.126405Z","iopub.execute_input":"2022-01-15T04:49:06.12684Z","iopub.status.idle":"2022-01-15T05:16:40.875706Z","shell.execute_reply.started":"2022-01-15T04:49:06.126804Z","shell.execute_reply":"2022-01-15T05:16:40.874681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's plot our neural network to see how data is passed through\ntf.keras.utils.plot_model(model, show_shapes=True, show_layer_names=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-15T05:16:41.755366Z","iopub.execute_input":"2022-01-15T05:16:41.75595Z","iopub.status.idle":"2022-01-15T05:16:41.926545Z","shell.execute_reply.started":"2022-01-15T05:16:41.755898Z","shell.execute_reply":"2022-01-15T05:16:41.92534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nsubmission = pd.read_csv('../input/petfinder-pawpularity-score/sample_submission.csv')\nsubmission['Id'] = submission['Id'].apply(lambda x: '../input/petfinder-pawpularity-score/test/'+x+'.jpg')\nsubmission.to_csv('/kaggle/working/submission.csv', index=False, header=False)\nsubmission = tf.data.TextLineDataset(\n    './submission.csv'\n).map(decode_csv).batch(BATCH_SIZE)\n\n# Make predictions with our model\nsample_prediction = model.predict(submission)","metadata":{"execution":{"iopub.status.busy":"2022-01-15T05:16:46.275844Z","iopub.execute_input":"2022-01-15T05:16:46.276163Z","iopub.status.idle":"2022-01-15T05:16:46.502172Z","shell.execute_reply.started":"2022-01-15T05:16:46.276129Z","shell.execute_reply":"2022-01-15T05:16:46.501056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Format predictions to output for submission\nsubmission_output = pd.concat(\n    [pd.read_csv('../input/petfinder-pawpularity-score/sample_submission.csv').drop('Pawpularity', axis=1),\n    pd.DataFrame(sample_prediction)],\n    axis=1\n)\nprint(submission_output)\nsubmission_output.columns = [['Id', 'Pawpularity']]\n\n# Output submission file to csv\nsubmission_output.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-15T05:26:04.307005Z","iopub.execute_input":"2022-01-15T05:26:04.307462Z","iopub.status.idle":"2022-01-15T05:26:04.330835Z","shell.execute_reply.started":"2022-01-15T05:26:04.307426Z","shell.execute_reply":"2022-01-15T05:26:04.329657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}