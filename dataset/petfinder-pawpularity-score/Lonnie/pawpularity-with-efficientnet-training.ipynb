{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Pawpularity EfficientNet: [training]\n\n## Table of Contents\n- Summary\n- Set up\n- Import datasets\n- Data Preprocessing\n- Model Development\n- Model Evaluation\n- Submission\n\n\n## Summary\nIn this Notebook, I will:\n* Use EfficientNetB0 as Image Model to fit on Image Data.\n* Use DNN as Tabular Model to fit on Tabular Data.\n* Combine the total result of Image Model and Tabular Model.\n* Use RMSE as Loss Function.\n* Use Data Augmentation and Regularization method to prevent overfiting.\n* Use K-Fold training to improve final score.\n\n## Set up","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport sklearn\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import KFold\nfrom tensorflow import keras","metadata":{"execution":{"iopub.status.busy":"2021-11-07T17:43:31.725358Z","iopub.execute_input":"2021-11-07T17:43:31.725729Z","iopub.status.idle":"2021-11-07T17:43:33.589986Z","shell.execute_reply.started":"2021-11-07T17:43:31.725662Z","shell.execute_reply":"2021-11-07T17:43:33.589286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Import datasets","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"../input/petfinder-pawpularity-score/train.csv\")\ntest = pd.read_csv(\"../input/petfinder-pawpularity-score/test.csv\")\nsample_submission = pd.read_csv(\"../input/petfinder-pawpularity-score/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-11-07T17:43:33.595808Z","iopub.execute_input":"2021-11-07T17:43:33.596308Z","iopub.status.idle":"2021-11-07T17:43:33.627949Z","shell.execute_reply.started":"2021-11-07T17:43:33.596273Z","shell.execute_reply":"2021-11-07T17:43:33.627305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Preprocessing","metadata":{}},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-07T17:43:33.628993Z","iopub.execute_input":"2021-11-07T17:43:33.629229Z","iopub.status.idle":"2021-11-07T17:43:33.650273Z","shell.execute_reply.started":"2021-11-07T17:43:33.629197Z","shell.execute_reply":"2021-11-07T17:43:33.649493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[\"file_path\"] = train[\"Id\"].apply(lambda identifier: \"../input/petfinder-pawpularity-score/train/\" + identifier + \".jpg\")\ntest[\"file_path\"] = test[\"Id\"].apply(lambda identifier: \"../input/petfinder-pawpularity-score/test/\" + identifier + \".jpg\")","metadata":{"execution":{"iopub.status.busy":"2021-11-07T17:43:33.652679Z","iopub.execute_input":"2021-11-07T17:43:33.652936Z","iopub.status.idle":"2021-11-07T17:43:33.663214Z","shell.execute_reply.started":"2021-11-07T17:43:33.652905Z","shell.execute_reply":"2021-11-07T17:43:33.662406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-07T17:43:33.664797Z","iopub.execute_input":"2021-11-07T17:43:33.665093Z","iopub.status.idle":"2021-11-07T17:43:33.683032Z","shell.execute_reply.started":"2021-11-07T17:43:33.665058Z","shell.execute_reply":"2021-11-07T17:43:33.682306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Distribution of Pawpularities","metadata":{}},{"cell_type":"code","source":"train[\"Pawpularity\"].hist()","metadata":{"execution":{"iopub.status.busy":"2021-11-07T17:43:33.684524Z","iopub.execute_input":"2021-11-07T17:43:33.684744Z","iopub.status.idle":"2021-11-07T17:43:33.905969Z","shell.execute_reply.started":"2021-11-07T17:43:33.684719Z","shell.execute_reply":"2021-11-07T17:43:33.905136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Development","metadata":{}},{"cell_type":"code","source":"tabular_columns = ['Subject Focus', 'Eyes', 'Face', 'Near', 'Action', 'Accessory', 'Group', 'Collage', 'Human', 'Occlusion', 'Info', 'Blur']\nimage_size = 224\nbatch_size = 128\nepochs = 100\ntrain_on_fold = None # Which Fold to train, train on all folds if None\nAUTO = tf.data.experimental.AUTOTUNE","metadata":{"execution":{"iopub.status.busy":"2021-11-07T17:43:33.908644Z","iopub.execute_input":"2021-11-07T17:43:33.908836Z","iopub.status.idle":"2021-11-07T17:43:33.915774Z","shell.execute_reply.started":"2021-11-07T17:43:33.908812Z","shell.execute_reply":"2021-11-07T17:43:33.915018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Random Blockout Augmentation","metadata":{}},{"cell_type":"code","source":"def random_erasing(img, sl=0.1, sh=0.2, rl=0.4, p=0.3):\n    h = tf.shape(img)[0]\n    w = tf.shape(img)[1]\n    c = tf.shape(img)[2]\n    origin_area = tf.cast(h*w, tf.float32)\n\n    e_size_l = tf.cast(tf.round(tf.sqrt(origin_area * sl * rl)), tf.int32)\n    e_size_h = tf.cast(tf.round(tf.sqrt(origin_area * sh / rl)), tf.int32)\n\n    e_height_h = tf.minimum(e_size_h, h)\n    e_width_h = tf.minimum(e_size_h, w)\n\n    erase_height = tf.random.uniform(shape=[], minval=e_size_l, maxval=e_height_h, dtype=tf.int32)\n    erase_width = tf.random.uniform(shape=[], minval=e_size_l, maxval=e_width_h, dtype=tf.int32)\n\n    erase_area = tf.zeros(shape=[erase_height, erase_width, c])\n    erase_area = tf.cast(erase_area, tf.uint8)\n\n    pad_h = h - erase_height\n    pad_top = tf.random.uniform(shape=[], minval=0, maxval=pad_h, dtype=tf.int32)\n    pad_bottom = pad_h - pad_top\n\n    pad_w = w - erase_width\n    pad_left = tf.random.uniform(shape=[], minval=0, maxval=pad_w, dtype=tf.int32)\n    pad_right = pad_w - pad_left\n\n    erase_mask = tf.pad([erase_area], [[0,0],[pad_top, pad_bottom], [pad_left, pad_right], [0,0]], constant_values=1)\n    erase_mask = tf.squeeze(erase_mask, axis=0)\n    erased_img = tf.multiply(tf.cast(img,tf.float32), tf.cast(erase_mask, tf.float32))\n\n    return tf.cond(tf.random.uniform([], 0, 1) > p, lambda: tf.cast(img, img.dtype), lambda:  tf.cast(erased_img, img.dtype))","metadata":{"execution":{"iopub.status.busy":"2021-11-07T17:43:34.126135Z","iopub.execute_input":"2021-11-07T17:43:34.126445Z","iopub.status.idle":"2021-11-07T17:43:34.138999Z","shell.execute_reply.started":"2021-11-07T17:43:34.126406Z","shell.execute_reply":"2021-11-07T17:43:34.13823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def data_augment(image):\n    image = tf.image.random_flip_left_right(image)\n    image = random_erasing(image)\n    return image","metadata":{"execution":{"iopub.status.busy":"2021-11-07T17:43:34.14014Z","iopub.execute_input":"2021-11-07T17:43:34.140933Z","iopub.status.idle":"2021-11-07T17:43:34.151257Z","shell.execute_reply.started":"2021-11-07T17:43:34.140892Z","shell.execute_reply":"2021-11-07T17:43:34.150441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Preprocess funciton\nI am doing data augmentation on training preprocess function, so that when make evaluation on validation dataset and prediction on test dataset, data augmentation won't be triggered.","metadata":{}},{"cell_type":"code","source":"def preprocess_training(image_url, tabular):\n    image_string = tf.io.read_file(image_url)\n    image = tf.image.decode_jpeg(image_string, channels=3)\n    image = data_augment(image)\n    image = tf.image.central_crop(image, 1.0)\n    image = tf.image.resize(image, (image_size, image_size))\n    return (image, tabular[1:]), tf.cast(tabular[0], tf.float32)\n\ndef preprocess_validation(image_url, tabular):\n    image_string = tf.io.read_file(image_url)\n    image = tf.image.decode_jpeg(image_string, channels=3)\n    image = tf.image.central_crop(image, 1.0)\n    image = tf.image.resize(image, (image_size, image_size))\n    return (image, tabular[1:]), tf.cast(tabular[0], tf.float32)","metadata":{"execution":{"iopub.status.busy":"2021-11-07T17:43:34.1523Z","iopub.execute_input":"2021-11-07T17:43:34.152517Z","iopub.status.idle":"2021-11-07T17:43:34.163101Z","shell.execute_reply.started":"2021-11-07T17:43:34.152482Z","shell.execute_reply":"2021-11-07T17:43:34.16215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### RMSE","metadata":{}},{"cell_type":"markdown","source":"I am using RMSE as Loss Function. Overfiting it can get about 3-4, If a better way is found to solve this problem, it can possibly get a better score.","metadata":{}},{"cell_type":"code","source":"def rmse(y_true, y_pred):\n    return tf.sqrt(tf.reduce_mean((y_true -  y_pred) ** 2))","metadata":{"execution":{"iopub.status.busy":"2021-11-07T17:43:34.16465Z","iopub.execute_input":"2021-11-07T17:43:34.165089Z","iopub.status.idle":"2021-11-07T17:43:34.175299Z","shell.execute_reply.started":"2021-11-07T17:43:34.165054Z","shell.execute_reply":"2021-11-07T17:43:34.174492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## The Base Model (Efficient Net)","metadata":{}},{"cell_type":"code","source":"def base_model():\n    efficient_net = tf.keras.applications.EfficientNetB0(\n        weights = \"../input/efficientnet-b0-for-keras-no-top/efficientnetb0_notop.h5\", \n        include_top = False, \n        input_shape = (image_size, image_size, 3)\n    )    \n    efficient_net.trainable = False\n    return efficient_net","metadata":{"execution":{"iopub.status.busy":"2021-11-07T17:43:34.178784Z","iopub.execute_input":"2021-11-07T17:43:34.179004Z","iopub.status.idle":"2021-11-07T17:43:34.184963Z","shell.execute_reply.started":"2021-11-07T17:43:34.178975Z","shell.execute_reply":"2021-11-07T17:43:34.184314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"efficient_net = base_model()","metadata":{"execution":{"iopub.status.busy":"2021-11-07T17:43:34.187116Z","iopub.execute_input":"2021-11-07T17:43:34.187645Z","iopub.status.idle":"2021-11-07T17:43:37.33313Z","shell.execute_reply.started":"2021-11-07T17:43:34.187612Z","shell.execute_reply":"2021-11-07T17:43:37.332324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### The Tabular Prediction Model","metadata":{}},{"cell_type":"markdown","source":"I wrote a notebook to find optimal Tabualr Prediciton Model [here](https://www.kaggle.com/lonnieqin/pawpularity-model-with-dnn-on-meta-data?scriptVersionId=79014727).","metadata":{}},{"cell_type":"code","source":"def get_tabular_model(inputs):\n    width = 32\n    depth = 3\n    activation = \"relu\"\n    kernel_regularizer = keras.regularizers.l2()\n    x = keras.layers.Dense(\n            width, \n            activation=activation,\n            kernel_regularizer=kernel_regularizer\n        )(inputs)\n    for i in range(depth):\n        if i == 0:\n            x = inputs\n        x = keras.layers.Dense(\n            width, \n            activation=activation,\n            kernel_regularizer=kernel_regularizer\n        )(x)\n        if (i + 1) % 3 == 0:\n            x = keras.layers.Concatenate()([x, inputs])\n    return x","metadata":{"execution":{"iopub.status.busy":"2021-11-07T17:43:37.334531Z","iopub.execute_input":"2021-11-07T17:43:37.33481Z","iopub.status.idle":"2021-11-07T17:43:37.342873Z","shell.execute_reply.started":"2021-11-07T17:43:37.334776Z","shell.execute_reply":"2021-11-07T17:43:37.341994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_model():\n    image_inputs = tf.keras.Input((image_size, image_size , 3))\n    tabular_inputs = tf.keras.Input(len(tabular_columns))\n    image_x = efficient_net(image_inputs)\n    image_x = tf.keras.layers.GlobalAveragePooling2D()(image_x)\n    #for _ in range(1):\n    #    image_x = tf.keras.layers.Dense(256, kernel_regularizer=tf.keras.regularizers.l2())(image_x)\n    image_x = tf.keras.layers.Dropout(0.5)(image_x)\n    tabular_x = get_tabular_model(tabular_inputs)\n    x = tf.keras.layers.Concatenate(axis=1)([image_x, tabular_x])\n    output = tf.keras.layers.Dense(1)(x)\n    model = tf.keras.Model(inputs=[image_inputs, tabular_inputs], outputs=[output])\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-11-07T17:43:37.344816Z","iopub.execute_input":"2021-11-07T17:43:37.345165Z","iopub.status.idle":"2021-11-07T17:43:37.353875Z","shell.execute_reply.started":"2021-11-07T17:43:37.34506Z","shell.execute_reply":"2021-11-07T17:43:37.352692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's have a big picture of how this Model looks like. This Model accepts images with shape (image_size, image_size, 3) and tabular information with shape (12) as input. Since it's a Regression problem, it generate output with shape (1). ","metadata":{}},{"cell_type":"code","source":"show_architecture = False\nif show_architecture:\n    model = get_model()\n    tf.keras.utils.plot_model(model, show_shapes=True)\n    print(model.summary())\n    image = np.random.normal(size=(1, image_size, image_size, 3))\n    tabular = np.random.normal(size=(1, len(tabular_columns)))\n    print(image.shape, tabular.shape)\n    print(model((image, tabular)).shape)","metadata":{"execution":{"iopub.status.busy":"2021-11-07T17:43:37.355394Z","iopub.execute_input":"2021-11-07T17:43:37.355876Z","iopub.status.idle":"2021-11-07T17:43:37.365347Z","shell.execute_reply.started":"2021-11-07T17:43:37.355824Z","shell.execute_reply":"2021-11-07T17:43:37.364588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Training\nI will use tensorflow Dataset here to preprocess and cache tensors, first epoch is very slow because it's preprocessing data; after that, it would be must faster.","metadata":{}},{"cell_type":"code","source":"tf.keras.backend.clear_session()\nmodels = []\nhistorys = []\nkfold = KFold(n_splits=5, shuffle=True, random_state=42)\nfor index, (train_indices, val_indices) in enumerate(kfold.split(train)):\n    if train_on_fold != None and train_on_fold != index:\n        continue\n    x_train = train.loc[train_indices, \"file_path\"]\n    tabular_train = train.loc[train_indices, [\"Pawpularity\"] + tabular_columns]\n    x_val= train.loc[val_indices, \"file_path\"]\n    tabular_val = train.loc[val_indices, [\"Pawpularity\"] + tabular_columns]\n    checkpoint_path = \"model_%d.h5\"%(index)\n    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n        checkpoint_path, \n        save_best_only=True\n    )\n    early_stop = tf.keras.callbacks.EarlyStopping(\n        min_delta=1e-4, \n        patience=10\n    )\n    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n        factor=0.3,\n        patience=2, \n        min_lr=1e-7\n    )\n    callbacks = [early_stop, checkpoint, reduce_lr]\n    \n    optimizer = tf.keras.optimizers.Adam(1e-3)\n    \n    train_ds = tf.data.Dataset.from_tensor_slices((x_train, tabular_train)).map(preprocess_training).shuffle(512).batch(batch_size).cache().prefetch(AUTO)\n    val_ds = tf.data.Dataset.from_tensor_slices((x_val, tabular_val)).map(preprocess_validation).batch(batch_size).cache().prefetch(AUTO)\n    model = get_model()\n    model.compile(loss=rmse, optimizer=optimizer, metrics=[\"mae\", \"mape\"])\n    history = model.fit(train_ds, epochs=epochs, validation_data=val_ds, callbacks=callbacks)\n    for metrics in [(\"loss\", \"val_loss\"), (\"mae\", \"val_mae\"), (\"mape\", \"val_mape\"), [\"lr\"]]:\n        pd.DataFrame(history.history, columns=metrics).plot()\n        plt.show()\n    model.load_weights(checkpoint_path)\n    historys.append(history)\n    models.append(model)","metadata":{"execution":{"iopub.status.busy":"2021-11-07T17:43:37.366566Z","iopub.execute_input":"2021-11-07T17:43:37.366922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.utils.plot_model(model, show_shapes=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submission","metadata":{}},{"cell_type":"code","source":"def preprocess_test_data(image_url, tabular):\n    print(image_url, tabular)\n    image_string = tf.io.read_file(image_url)\n    image = tf.image.decode_jpeg(image_string, channels=3)\n    image = tf.image.central_crop(image, 1.0)\n    image = tf.image.resize(image, (image_size, image_size))\n    # 0 won't be used in prediction, but it's needed in this senario or the tabular variable is treated as label.\n    return (image, tabular), 0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_ds = tf.data.Dataset.from_tensor_slices((test[\"file_path\"], test[tabular_columns])).map(preprocess_test_data).batch(batch_size).cache().prefetch(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_results = []\nfor model in models:\n    total_results.append(model.predict(test_ds).reshape(-1))\nresults = np.mean(total_results, axis=0).reshape(-1)\nsample_submission[\"Pawpularity\"] = results\nsample_submission.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}