{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Pawpularity Prediction: Vision Transformer from Scratch\n\n## Table of Contents\n- Summary\n- Setup\n- Configuration\n- Helpers\n- Import datasets\n- Data Preprocessing\n- Model Development\n- Submission\n- Reference\n\n## Summary\n* Create Vision Transformer from scratch, modify the Vision Transformer so that it can also accept Tabular inputs. Tabular inputs is optional, with the Functional API, it's easy to control whether or not to receive Tabular inputs.\n* Change Regression Problem to Classification Problem. Normalize the Pawpularity score from 0 to 1 and use BinaryCrossEntropy as loss function.\n* Apply Data Augmentation to dataset during Training.\n\n\n## Setup","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport tensorflow as tf\nimport sklearn\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import KFold\nimport tensorflow_addons as tfa","metadata":{"execution":{"iopub.status.busy":"2021-10-07T15:08:00.718022Z","iopub.execute_input":"2021-10-07T15:08:00.718335Z","iopub.status.idle":"2021-10-07T15:08:00.724635Z","shell.execute_reply.started":"2021-10-07T15:08:00.718298Z","shell.execute_reply":"2021-10-07T15:08:00.723155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Configuration","metadata":{}},{"cell_type":"code","source":"class Config:\n    image_size = 128\n    input_shape = [image_size, image_size, 3]\n    learning_rate = 4e-4\n    weight_decay = 0.0001\n    batch_size = 128\n    num_classes = 1\n    num_epochs = 30\n    patch_size = 16\n    num_patches = (image_size // patch_size) ** 2\n    projection_dim = 64\n    num_heads = 4\n    scale_factor = 100.0\n    transformer_units = [\n        projection_dim * 2,\n        projection_dim\n    ]\n    transformer_layers = 8\n    mlp_head_units = [2048, 1024]\n    tabular_columns = ['Subject Focus', 'Eyes', 'Face', 'Near', 'Action', 'Accessory', 'Group', 'Collage', 'Human', 'Occlusion', 'Info', 'Blur']","metadata":{"execution":{"iopub.status.busy":"2021-10-07T15:04:10.739487Z","iopub.execute_input":"2021-10-07T15:04:10.739923Z","iopub.status.idle":"2021-10-07T15:04:10.749387Z","shell.execute_reply.started":"2021-10-07T15:04:10.739894Z","shell.execute_reply":"2021-10-07T15:04:10.747771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Helpers","metadata":{}},{"cell_type":"markdown","source":"### Display images","metadata":{}},{"cell_type":"code","source":"def display_images(images, row_count, column_count):\n    fig, axs = plt.subplots(row_count, column_count, figsize=(10,10))\n    for i in range(row_count):\n        for j in range(column_count):\n            axs[i,j].imshow(images[i * column_count + j])\n            axs[i,j].axis('off')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-07T14:35:06.389366Z","iopub.execute_input":"2021-10-07T14:35:06.389643Z","iopub.status.idle":"2021-10-07T14:35:06.39792Z","shell.execute_reply.started":"2021-10-07T14:35:06.389616Z","shell.execute_reply":"2021-10-07T14:35:06.396835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Preprocess images","metadata":{}},{"cell_type":"code","source":"def preprocess_image(image_url):\n    image_string = tf.io.read_file(image_url)\n    image = tf.image.decode_jpeg(image_string, channels=3)\n    image = tf.cast(image, tf.float32) / 255.0\n    #image = tf.image.central_crop(image, 1.0)\n    image = tf.image.resize(image, (Config.image_size, Config.image_size))\n    return image","metadata":{"execution":{"iopub.status.busy":"2021-10-07T14:35:10.957175Z","iopub.execute_input":"2021-10-07T14:35:10.957882Z","iopub.status.idle":"2021-10-07T14:35:10.965375Z","shell.execute_reply.started":"2021-10-07T14:35:10.95781Z","shell.execute_reply":"2021-10-07T14:35:10.964017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Multi Layer Perceptron","metadata":{}},{"cell_type":"code","source":"def mlp(x, hidden_units, dropout_rate):\n    for units in hidden_units:\n        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n        x = layers.Dropout(dropout_rate)(x)\n    return x","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Custom RMSE function that can calculate Pawpularity Score correctly after Normalization","metadata":{}},{"cell_type":"code","source":"def rmse(y_true, y_pred):\n    return tf.sqrt(tf.reduce_mean(tf.square(y_true * Config.scale_factor - y_pred * Config.scale_factor)))","metadata":{"execution":{"iopub.status.busy":"2021-10-07T14:36:26.868931Z","iopub.execute_input":"2021-10-07T14:36:26.869255Z","iopub.status.idle":"2021-10-07T14:36:26.874555Z","shell.execute_reply.started":"2021-10-07T14:36:26.869226Z","shell.execute_reply":"2021-10-07T14:36:26.873523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Import datasets","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"../input/petfinder-pawpularity-score/train.csv\")\ntest = pd.read_csv(\"../input/petfinder-pawpularity-score/test.csv\")\nsample_submission = pd.read_csv(\"../input/petfinder-pawpularity-score/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-10-07T14:55:40.274851Z","iopub.execute_input":"2021-10-07T14:55:40.275274Z","iopub.status.idle":"2021-10-07T14:55:40.310227Z","shell.execute_reply.started":"2021-10-07T14:55:40.275245Z","shell.execute_reply":"2021-10-07T14:55:40.309227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Preprocessing","metadata":{}},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-07T14:55:42.901648Z","iopub.execute_input":"2021-10-07T14:55:42.902844Z","iopub.status.idle":"2021-10-07T14:55:42.923027Z","shell.execute_reply.started":"2021-10-07T14:55:42.90277Z","shell.execute_reply":"2021-10-07T14:55:42.921821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[\"file_path\"] = train[\"Id\"].apply(lambda identifier: \"../input/petfinder-pawpularity-score/train/\" + identifier + \".jpg\")\ntest[\"file_path\"] = test[\"Id\"].apply(lambda identifier: \"../input/petfinder-pawpularity-score/test/\" + identifier + \".jpg\")","metadata":{"execution":{"iopub.status.busy":"2021-10-07T14:55:45.397299Z","iopub.execute_input":"2021-10-07T14:55:45.397703Z","iopub.status.idle":"2021-10-07T14:55:45.414242Z","shell.execute_reply.started":"2021-10-07T14:55:45.39766Z","shell.execute_reply":"2021-10-07T14:55:45.413183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-07T14:55:47.530562Z","iopub.execute_input":"2021-10-07T14:55:47.530926Z","iopub.status.idle":"2021-10-07T14:55:47.553991Z","shell.execute_reply.started":"2021-10-07T14:55:47.530877Z","shell.execute_reply":"2021-10-07T14:55:47.552584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[\"Pawpularity\"].hist()","metadata":{"execution":{"iopub.status.busy":"2021-10-07T14:55:50.271399Z","iopub.execute_input":"2021-10-07T14:55:50.272107Z","iopub.status.idle":"2021-10-07T14:55:50.670375Z","shell.execute_reply.started":"2021-10-07T14:55:50.272064Z","shell.execute_reply":"2021-10-07T14:55:50.669405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Images that has High Score","metadata":{}},{"cell_type":"code","source":"item_width = 5\ndata = train[train.Pawpularity >= 90]\nimage_urls = data.iloc[np.random.choice(data.shape[0], item_width ** 2)][\"file_path\"]\nfor images in tf.data.Dataset.from_tensor_slices((image_urls)).map(preprocess_image).batch(item_width ** 2):\n    display_images(images.numpy(), item_width, item_width)","metadata":{"execution":{"iopub.status.busy":"2021-10-07T15:33:29.078075Z","iopub.execute_input":"2021-10-07T15:33:29.078946Z","iopub.status.idle":"2021-10-07T15:33:29.120389Z","shell.execute_reply.started":"2021-10-07T15:33:29.078885Z","shell.execute_reply":"2021-10-07T15:33:29.118762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Images with Low Scores","metadata":{}},{"cell_type":"code","source":"item_width = 5\ndata = train[train.Pawpularity <= 10]\nimage_urls = data.iloc[np.random.choice(data.shape[0], item_width ** 2)][\"file_path\"]\nfor images in tf.data.Dataset.from_tensor_slices((image_urls)).map(preprocess_image).batch(item_width ** 2):\n    display_images(images.numpy(), item_width, item_width)","metadata":{"execution":{"iopub.status.busy":"2021-10-07T15:33:02.865957Z","iopub.execute_input":"2021-10-07T15:33:02.866311Z","iopub.status.idle":"2021-10-07T15:33:05.487466Z","shell.execute_reply.started":"2021-10-07T15:33:02.866283Z","shell.execute_reply":"2021-10-07T15:33:05.4864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Images has median scores","metadata":{}},{"cell_type":"code","source":"item_width = 5\ndata = train[(train.Pawpularity >= 40) & (train.Pawpularity <= 60)]\nimage_urls = data.iloc[np.random.choice(data.shape[0], item_width ** 2)][\"file_path\"]\nfor images in tf.data.Dataset.from_tensor_slices((image_urls)).map(preprocess_image).batch(item_width ** 2):\n    display_images(images.numpy(), item_width, item_width)","metadata":{"execution":{"iopub.status.busy":"2021-10-07T15:02:45.396306Z","iopub.execute_input":"2021-10-07T15:02:45.396698Z","iopub.status.idle":"2021-10-07T15:02:47.58657Z","shell.execute_reply.started":"2021-10-07T15:02:45.396653Z","shell.execute_reply":"2021-10-07T15:02:47.585361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Normalize Pawpularity Score from 0 to 1.","metadata":{}},{"cell_type":"code","source":"train[\"Pawpularity\"] /= Config.scale_factor","metadata":{"execution":{"iopub.status.busy":"2021-10-07T15:03:16.673563Z","iopub.execute_input":"2021-10-07T15:03:16.673876Z","iopub.status.idle":"2021-10-07T15:03:16.681625Z","shell.execute_reply.started":"2021-10-07T15:03:16.673849Z","shell.execute_reply":"2021-10-07T15:03:16.680298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Development","metadata":{}},{"cell_type":"markdown","source":"### Preprocess function","metadata":{}},{"cell_type":"code","source":"def preprocess(image_url, tabular):\n    image =  preprocess_image(image_url)\n    return (image, tabular[1:]), tabular[0]","metadata":{"execution":{"iopub.status.busy":"2021-10-07T15:03:36.184838Z","iopub.execute_input":"2021-10-07T15:03:36.185454Z","iopub.status.idle":"2021-10-07T15:03:36.190969Z","shell.execute_reply.started":"2021-10-07T15:03:36.185332Z","shell.execute_reply":"2021-10-07T15:03:36.189544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Augmentation","metadata":{}},{"cell_type":"code","source":"augmentation_layer = keras.Sequential([\n    keras.layers.Input(Config.input_shape),\n    #keras.layers.experimental.preprocessing.RandomRotation(factor=0.02),\n    keras.layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n    keras.layers.experimental.preprocessing.RandomZoom(height_factor=0.2, width_factor=0.2),\n])","metadata":{"execution":{"iopub.status.busy":"2021-10-07T15:04:27.519071Z","iopub.execute_input":"2021-10-07T15:04:27.519953Z","iopub.status.idle":"2021-10-07T15:04:27.622725Z","shell.execute_reply.started":"2021-10-07T15:04:27.519903Z","shell.execute_reply":"2021-10-07T15:04:27.62178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Patch Creation Layer","metadata":{}},{"cell_type":"code","source":"class Patches(layers.Layer):\n    def __init__(self, patch_size):\n        super(Patches, self).__init__()\n        self.patch_size = patch_size\n        \n    def call(self, images):\n        batch_size = tf.shape(images)[0]\n        patches = tf.image.extract_patches(\n            images = images,\n            sizes=[1, self.patch_size, self.patch_size, 1],\n            strides=[1, self.patch_size, self.patch_size, 1],\n            rates=[1, 1, 1, 1],\n            padding=\"VALID\",\n        )\n        patch_dims = patches.shape[-1]\n        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n        return patches","metadata":{"execution":{"iopub.status.busy":"2021-10-07T15:04:30.817403Z","iopub.execute_input":"2021-10-07T15:04:30.817748Z","iopub.status.idle":"2021-10-07T15:04:30.826043Z","shell.execute_reply.started":"2021-10-07T15:04:30.817702Z","shell.execute_reply":"2021-10-07T15:04:30.824966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's understand what Patch Creation Layer do. It simply split the image to NxN grid.","metadata":{}},{"cell_type":"code","source":"index = np.random.choice(train.shape[0])\nplt.figure(figsize=(4, 4))\nimage = preprocess_image(tf.constant(train.iloc[index][\"file_path\"], dtype=tf.string))\nprint(image.shape)\nplt.imshow(np.squeeze(image))\nplt.axis(\"off\")\n\nresized_image = tf.image.resize(\n    tf.convert_to_tensor([image]), size=(Config.image_size, Config.image_size)\n)\nprint(resized_image.shape)\npatches = Patches(Config.patch_size)(resized_image)\nprint(f\"Image size: {Config.image_size} X {Config.image_size}\")\nprint(f\"Patch size: {Config.patch_size} X {Config.patch_size}\")\nprint(f\"Patches per image: {patches.shape[1]}\")\nprint(f\"Elements per patch: {patches.shape[-1]}\")\n\nn = int(np.sqrt(patches.shape[1]))\nplt.figure(figsize=(4, 4))\nfor i, patch in enumerate(patches[0]):\n    ax = plt.subplot(n, n, i + 1)\n    patch_img = tf.reshape(patch, (Config.patch_size, Config.patch_size, 3))\n    plt.imshow(patch_img.numpy())\n    plt.axis(\"off\")","metadata":{"execution":{"iopub.status.busy":"2021-10-07T15:05:37.961277Z","iopub.execute_input":"2021-10-07T15:05:37.961617Z","iopub.status.idle":"2021-10-07T15:05:40.941021Z","shell.execute_reply.started":"2021-10-07T15:05:37.961588Z","shell.execute_reply":"2021-10-07T15:05:40.940055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Patch Encoder Layer","metadata":{}},{"cell_type":"code","source":"class PatchEncoder(layers.Layer):\n    \n    def __init__(self, num_patches, projection_dim):\n        super(PatchEncoder, self).__init__()\n        self.num_patches = num_patches\n        self.projection = layers.Dense(projection_dim)\n        self.position_embedding = layers.Embedding(\n            input_dim=num_patches, output_dim=projection_dim\n        )\n    def call(self, patch):\n        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n        encoded = self.projection(patch) + self.position_embedding(positions)\n        return encoded","metadata":{"execution":{"iopub.status.busy":"2021-10-07T15:05:49.394745Z","iopub.execute_input":"2021-10-07T15:05:49.395131Z","iopub.status.idle":"2021-10-07T15:05:49.40335Z","shell.execute_reply.started":"2021-10-07T15:05:49.39509Z","shell.execute_reply":"2021-10-07T15:05:49.402101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Vision Transformer Model","metadata":{}},{"cell_type":"code","source":"def create_vision_transformer(use_tabular_inputs=False):\n    \n    tabular_inputs = tf.keras.Input(len(Config.tabular_columns))\n    # Inputs\n    inputs = layers.Input(shape=Config.input_shape)\n    # Data Augmentation\n    augmented = augmentation_layer(inputs)\n    # Patches\n    patches = Patches(Config.patch_size)(augmented)\n    encoder_patches = PatchEncoder(Config.num_patches, Config.projection_dim)(patches)\n    \n    for _ in range(Config.transformer_layers):\n        # Layer Normalization 1\n        x1 = layers.LayerNormalization(epsilon=1e-6)(encoder_patches)\n        # Multi-Head Attention Layer\n        attention_output = layers.MultiHeadAttention(\n            num_heads=Config.num_heads, \n            key_dim=Config.projection_dim,\n            dropout=0.1\n        )(x1, x1)\n        # Skip Connnection 1\n        x2 = layers.Add()([attention_output, encoder_patches])\n        \n        # Layer Normalization 2\n        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n        \n        # MLP\n        x3 = mlp(x3, hidden_units=Config.transformer_units, dropout_rate=0.1)\n        \n        # Skip Connnection 2\n        encoder_patches = layers.Add()([x3, x2])\n    \n    representation = layers.LayerNormalization(epsilon=1e-6)(encoder_patches)\n    representation = layers.Flatten()(representation)\n    representation = layers.Dropout(0.5)(representation)\n    \n    features = mlp(representation, hidden_units=Config.mlp_head_units, dropout_rate=0.5)\n    \n    if use_tabular_inputs:\n        image_x = layers.Dense(128, activation=tf.nn.gelu)(features)\n        tabular_x = mlp(tabular_inputs, hidden_units=[16] * 10, dropout_rate=0.5)\n        x = tf.keras.layers.Concatenate(axis=1)([image_x, tabular_x])\n    else:\n        x = features\n    \n    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n    \n    model = keras.Model(inputs=[inputs, tabular_inputs], outputs=outputs)\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-10-07T14:09:14.1902Z","iopub.execute_input":"2021-10-07T14:09:14.191269Z","iopub.status.idle":"2021-10-07T14:09:14.205195Z","shell.execute_reply.started":"2021-10-07T14:09:14.191222Z","shell.execute_reply":"2021-10-07T14:09:14.203928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's have a big picture of how this Model looks like.","metadata":{}},{"cell_type":"code","source":"model =  create_vision_transformer(True)\ntf.keras.utils.plot_model(model, show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2021-10-07T13:56:30.912752Z","iopub.execute_input":"2021-10-07T13:56:30.913082Z","iopub.status.idle":"2021-10-07T13:56:34.776293Z","shell.execute_reply.started":"2021-10-07T13:56:30.913052Z","shell.execute_reply":"2021-10-07T13:56:34.775025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2021-10-07T15:06:54.297754Z","iopub.execute_input":"2021-10-07T15:06:54.298071Z","iopub.status.idle":"2021-10-07T15:06:54.395244Z","shell.execute_reply.started":"2021-10-07T15:06:54.298042Z","shell.execute_reply":"2021-10-07T15:06:54.394283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This Model accepts images with shape (image_size, image_size, 3) and tabular information (if needed) with shape (12) as input. It generates output with shape (1). ","metadata":{}},{"cell_type":"code","source":"image = np.random.normal(size=(1, Config.image_size, Config.image_size, 3))\ntabular = np.random.normal(size=(1, len(Config.tabular_columns)))\nprint(image.shape, tabular.shape)\nprint(model((image, tabular)).shape)","metadata":{"execution":{"iopub.status.busy":"2021-10-07T15:07:01.775753Z","iopub.execute_input":"2021-10-07T15:07:01.776461Z","iopub.status.idle":"2021-10-07T15:07:01.877773Z","shell.execute_reply.started":"2021-10-07T15:07:01.776431Z","shell.execute_reply":"2021-10-07T15:07:01.876705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Training\nI will use tensorflow Dataset here to preprocess and cache tensors, first epoch is very slow because it's preprocessing data; after that, it would be must faster.","metadata":{}},{"cell_type":"code","source":"tf.keras.backend.clear_session()\nmodels = []\nhistorys = []\nkfold = KFold(n_splits=5, shuffle=True, random_state=997)\n# For the current random state, 5th fold can generate a better validation rmse and faster convergence.\ntrain_best_fold = False\nbest_fold = 4\nfor index, (train_indices, val_indices) in enumerate(kfold.split(train)):\n    if train_best_fold and index != best_fold:\n        continue\n    x_train = train.loc[train_indices, \"file_path\"]\n    tabular_train = train.loc[train_indices, [\"Pawpularity\"] + Config.tabular_columns]\n    x_val= train.loc[val_indices, \"file_path\"]\n    tabular_val = train.loc[val_indices, [\"Pawpularity\"] + Config.tabular_columns]\n    checkpoint_path = \"model_%d.h5\"%(index)\n    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n        checkpoint_path, \n        save_best_only=True,\n        save_weights_only=True\n    )\n    early_stop = tf.keras.callbacks.EarlyStopping(\n        min_delta=1e-4, \n        patience=20\n    )\n   \n    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n        factor=0.3,\n        patience=10, \n        min_lr=1e-7\n    )\n\n    callbacks = [checkpoint, reduce_lr, early_stop]\n    \n    loss = tf.keras.losses.BinaryCrossentropy()\n\n    optimizer = tfa.optimizers.AdamW(\n        learning_rate=Config.learning_rate,\n        weight_decay=Config.weight_decay\n    )\n    train_ds = tf.data.Dataset.from_tensor_slices((x_train, tabular_train)).map(preprocess).shuffle(512).batch(Config.batch_size).cache().prefetch(1)\n    val_ds = tf.data.Dataset.from_tensor_slices((x_val, tabular_val)).map(preprocess).batch(Config.batch_size).cache().prefetch(1)\n    # You can choose whether to use tabular inputs\n    model = create_vision_transformer(use_tabular_inputs=True)\n    model.compile(loss=loss, optimizer=optimizer, metrics=[rmse, \"mae\", \"mape\"])\n    history = model.fit(train_ds, epochs=Config.num_epochs, validation_data=val_ds, callbacks=callbacks)\n    for metrics in [(\"loss\", \"val_loss\"), (\"mae\", \"val_mae\", \"rmse\", \"val_rmse\"), (\"mape\", \"val_mape\")]:\n        pd.DataFrame(history.history, columns=metrics).plot()\n        plt.show()\n    model.load_weights(checkpoint_path)\n    historys.append(history)\n    models.append(model)","metadata":{"execution":{"iopub.status.busy":"2021-10-07T15:33:53.533177Z","iopub.execute_input":"2021-10-07T15:33:53.533525Z","iopub.status.idle":"2021-10-07T15:38:09.80749Z","shell.execute_reply.started":"2021-10-07T15:33:53.533495Z","shell.execute_reply":"2021-10-07T15:38:09.806545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submission","metadata":{}},{"cell_type":"code","source":"def preprocess_test_data(image_url, tabular):\n    print(image_url, tabular)\n    image = preprocess_image(image_url)\n    # 0 won't be used in prediction, but it's needed in this senario or the tabular variable is treated as label.\n    return (image, tabular), 0","metadata":{"execution":{"iopub.status.busy":"2021-10-07T14:18:49.313872Z","iopub.execute_input":"2021-10-07T14:18:49.314898Z","iopub.status.idle":"2021-10-07T14:18:49.323132Z","shell.execute_reply.started":"2021-10-07T14:18:49.31484Z","shell.execute_reply":"2021-10-07T14:18:49.321902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_ds = tf.data.Dataset.from_tensor_slices((test[\"file_path\"], test[Config.tabular_columns])).map(preprocess_test_data).batch(Config.batch_size).cache().prefetch(1)","metadata":{"execution":{"iopub.status.busy":"2021-10-07T14:19:01.51978Z","iopub.execute_input":"2021-10-07T14:19:01.520451Z","iopub.status.idle":"2021-10-07T14:19:01.598652Z","shell.execute_reply.started":"2021-10-07T14:19:01.52042Z","shell.execute_reply":"2021-10-07T14:19:01.597459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"When We Submit the result, don't forget to multiply the result by 100.","metadata":{}},{"cell_type":"code","source":"use_best_result = False\nif use_best_result:\n    if train_best_fold:\n        best_model = models[0]\n    else:\n        best_fold = 0\n        best_score = 10e8\n        for fold, history in enumerate(historys):\n            for val_rmse in history.history[\"val_rmse\"]:\n                if val_rmse < best_score:\n                    best_score = val_rmse\n                    best_fold = fold\n        print(\"Best Score:%.2f Best Fold: %d\"%(best_score, best_fold + 1))\n        best_model = models[best_fold]\n    sample_submission[\"Pawpularity\"] = Config.scale_factor * best_model.predict(test_ds).reshape(-1)\n    sample_submission.to_csv(\"submission.csv\", index=False)\nelse:\n    total_results = []\n    for model in models:\n        total_results.append(model.predict(test_ds).reshape(-1))\n    results = np.mean(total_results, axis=0).reshape(-1)\n    sample_submission[\"Pawpularity\"] = Config.scale_factor * results\n    sample_submission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-10-07T14:19:18.694577Z","iopub.execute_input":"2021-10-07T14:19:18.694853Z","iopub.status.idle":"2021-10-07T14:19:20.80804Z","shell.execute_reply.started":"2021-10-07T14:19:18.694826Z","shell.execute_reply":"2021-10-07T14:19:20.806934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Reference\n- [Image classification with Vision Transformer](https://keras.io/examples/vision/image_classification_with_vision_transformer/)\n- [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)","metadata":{}},{"cell_type":"markdown","source":"**If you find my notebook useful, give me an upvote.**","metadata":{}}]}