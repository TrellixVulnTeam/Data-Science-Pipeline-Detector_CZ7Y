{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Load Libraries","metadata":{}},{"cell_type":"code","source":"import sys\nsys.path.append(\"../input/d/ironluu/swintool/\")\nsys.path.append(\"../input/tez-lib/\")\nsys.path.append(\"../input/timmmaster/\")\nfrom pytorchtools import EarlyStopping\npatience = 7\nlr_patience, lr_factor = 0, 0.35\nclass CosineScheduler:\n    def __init__(self, max_update, base_lr=0.01, final_lr=0,\n               warmup_steps=0, warmup_begin_lr=0):\n        self.base_lr_orig = base_lr\n        self.max_update = max_update\n        self.final_lr = final_lr\n        self.warmup_steps = warmup_steps\n        self.warmup_begin_lr = warmup_begin_lr\n        self.max_steps = self.max_update - self.warmup_steps\n\n    def get_warmup_lr(self, epoch):\n        increase = (self.base_lr_orig - self.warmup_begin_lr) \\\n                       * float(epoch) / float(self.warmup_steps)\n        return self.warmup_begin_lr + increase\n\n    def __call__(self, epoch):\n#         if epoch == 0:\n#             return self.final_lr\n        if epoch < self.warmup_steps:\n            return self.get_warmup_lr(epoch)\n        if epoch <= self.max_update:\n            return self.final_lr + (\n                self.base_lr_orig - self.final_lr) * (1 + math.cos(\n                math.pi * (epoch - self.warmup_steps) / self.max_steps)) / 2\n        else:\n            return self.final_lr + (\n                self.base_lr_orig - self.final_lr) * (1 + math.cos(\n                math.pi * (self.max_update - self.warmup_steps) / self.max_steps)) / 2\n        return self.base_lr","metadata":{"execution":{"iopub.status.busy":"2021-12-18T00:54:20.975029Z","iopub.execute_input":"2021-12-18T00:54:20.975667Z","iopub.status.idle":"2021-12-18T00:54:25.513456Z","shell.execute_reply.started":"2021-12-18T00:54:20.975575Z","shell.execute_reply":"2021-12-18T00:54:25.512649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import argparse\nimport math\nimport tez\nimport timm\nimport albumentations\nimport os\nimport time\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchvision import transforms\nfrom utils_paw import read_split_data, train_one_epoch, evaluate\nfrom model import swin_large_patch4_window12_384_in22k as create_model\n\n\n# class args:\n#     batch_size = 16\n#     image_size = 384\n\n\ndef sigmoid(x):\n    return 1 / (1 + math.exp(-x))\n\ntest_aug = None\ndef lr_schedule_cosine(lr_min, lr_max, per_epochs):\n    def compute(epoch):\n        return lr_min + 0.5 * (lr_max - lr_min) * (1 + np.cos(epoch / per_epochs * np.pi))\n    return compute","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-18T00:54:25.515299Z","iopub.execute_input":"2021-12-18T00:54:25.515565Z","iopub.status.idle":"2021-12-18T00:54:29.729477Z","shell.execute_reply.started":"2021-12-18T00:54:25.515532Z","shell.execute_reply":"2021-12-18T00:54:29.72867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define Swim Model and Swim Dataset","metadata":{}},{"cell_type":"code","source":"class PawpularDataset(Dataset):\n    def __init__(self, images_paths, dense_features, images_class, transform, augmentations):\n        self.images_paths = images_paths\n        self.dense_features = dense_features\n        self.images_class = images_class  # torch.tensor(images_class, dtype=torch.float)\n        self.transform = transform\n        self.augmentations = augmentations\n\n    def __len__(self):\n        return len(self.images_paths)\n\n    def __getitem__(self, item):\n        image = Image.open(self.images_paths[item])\n\n        if self.transform is not None:\n            image = self.transform(image)\n        \n        if self.augmentations is not None:\n            augmented = self.augmentations(image=image)\n            image = augmented[\"image\"]\n\n        # image = np.transpose(image, (2, 0, 1)).astype(np.float32)\n\n        features = self.dense_features[item, :]\n        label = self.images_class[item]\n\n        return image, label, features\n        # return {\n        #     \"image\": torch.tensor(image, dtype=torch.float),\n        #     \"features\": torch.tensor(features, dtype=torch.float),\n        #     \"label\": torch.tensor(label, dtype=torch.float),\n        # }\n\n    @staticmethod\n    def collate_fn(batch):\n        # 官方实现的default_collate可以参考\n        # https://github.com/pytorch/pytorch/blob/67b7e751e6b5931a9f45274653f4f653a4e6cdf6/torch/utils/data/_utils/collate.py\n        images, labels, _ = tuple(zip(*batch))\n        # features=torch.tensor(features, dtype=torch.float)\n\n        images = torch.stack(images, dim=0)\n        labels = torch.as_tensor(labels)\n        return images, labels, _\n\n\nclass PawpularModel(tez.Model):\n    def __init__(self, args):\n        super().__init__()\n#         self.model = create_model(num_classes=128).to(args.device)\n        #.to(\n            #torch.device(args.device if torch.cuda.is_available() else \"cpu\"))  # args.num_classes\n        self.model = timm.create_model(\"swin_large_patch4_window12_384\", pretrained=False, in_chans=3).to(args.device)\n        self.model.head = nn.Linear(self.model.head.in_features, 128) .to(args.device)# 重置为128\n        self.dropout = nn.Dropout(0.1).to(args.device)\n        self.dense1 = nn.Linear(140, 64).to(args.device)\n        self.relu = nn.ReLU().to(args.device)\n        self.dense2 = nn.Linear(64, 1).to(args.device)\n\n    def forward(self, image, features, images_class=None):\n        x1 = self.model(image)\n        x = self.dropout(x1)\n        x = torch.cat([x, features], dim=1)\n        x = self.dense1(x)\n        x = self.relu(x)\n        x = self.dense2(x)\n\n        #x = torch.cat([x, x1, features], dim=1)\n        return x#, 0, {}\n\n","metadata":{"execution":{"iopub.status.busy":"2021-12-18T00:54:29.731011Z","iopub.execute_input":"2021-12-18T00:54:29.731319Z","iopub.status.idle":"2021-12-18T00:54:29.749272Z","shell.execute_reply.started":"2021-12-18T00:54:29.731282Z","shell.execute_reply":"2021-12-18T00:54:29.748614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import RAPIDS","metadata":{}},{"cell_type":"code","source":"def main(args):\n    device = torch.device(args.device if torch.cuda.is_available() else \"cpu\")\n\n    if os.path.exists(\"./weights\") is False:\n        os.makedirs(\"./weights\")\n\n    tb_writer = SummaryWriter()\n\n    df = pd.read_csv('../input/same-old-creating-folds/train_10folds.csv')\n    print('Train shape:', df.shape)\n\n    for fold_ in range(10):\n        if fold_ != 3:\n            continue\n        print('#' * 25)\n        print('### FOLD', fold_ + 1)\n        print('#' * 25)\n\n        # pawmodel = PawpularModel(args)\n        \n        df_test = pd.read_csv(\"../input/petfinder-pawpularity-score/test.csv\")#.head()\n        # map(lambda x: x - 1, df_test['Pawpularity'])\n        test_img_paths = [f\"../input/petfinder-pawpularity-score/test/{x}.jpg\" for x in df_test[\"Id\"].values]\n\n        df_valid = df[df.kfold == fold_].reset_index(drop=True)#.head(10)\n\n        df_valid['norm_score'] = df_valid['Pawpularity']/100\n\n        valid_img_paths = [f\"../input/petfinder-pawpularity-score/train/{x}.jpg\" for x in df_valid[\"Id\"].values]\n\n        dense_features = [\n            'Subject Focus', 'Eyes', 'Face', 'Near', 'Action', 'Accessory',\n            'Group', 'Collage', 'Human', 'Occlusion', 'Info', 'Blur'\n        ]\n\n        df_train = df[df.kfold != fold_].reset_index(drop=True)#.head(10)\n        df_train['norm_score'] = df_train['Pawpularity']/100\n        train_img_paths = [f\"../input/petfinder-pawpularity-score/train/{x}.jpg\" for x in df_train[\"Id\"].values]\n\n        # train_images_path, train_images_label, val_images_path, val_images_label = read_split_data(args.data_path)\n\n        img_size = 384\n        data_transform = {\n            \"train\": transforms.Compose([transforms.Resize(int(img_size )),# transforms.RandomResizedCrop(img_size),\n                                         transforms.CenterCrop(img_size),transforms.RandomHorizontalFlip(),\n                                         transforms.ToTensor(),\n                                         transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]),\n            \"val\": transforms.Compose([transforms.Resize(int(img_size )), #* 1.143\n                                       transforms.CenterCrop(img_size),\n                                       transforms.ToTensor(),\n                                       transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])}\n\n        # 实例化训练数据集\n        train_dataset = PawpularDataset(\n            images_paths=train_img_paths,\n            dense_features=df_train[dense_features].values,\n            images_class=df_train['norm_score'],#.values,  # list(map(gettarget, df_train['Pawpularity'].values)),\n            transform=data_transform[\"train\"],\n            augmentations=test_aug\n        )\n\n        # 实例化验证数据集\n        val_dataset = PawpularDataset(\n            images_paths=valid_img_paths,\n            dense_features=df_valid[dense_features].values,\n            images_class=df_valid['norm_score'],#.values,  # list(map(gettarget, df_valid['Pawpularity'].values)),\n            transform=data_transform[\"val\"],\n            augmentations=test_aug\n        )\n\n        nw = batch_size = args.batch_size\n#         nw = min([os.cpu_count(), batch_size if batch_size > 1 else 0, 8])  # number of workers\n        print('Using {} dataloader workers every process'.format(nw))\n        train_loader = torch.utils.data.DataLoader(train_dataset,\n                                                   batch_size=batch_size,\n                                                   shuffle=True,\n                                                   pin_memory=True,\n                                                   num_workers=nw,\n                                                   collate_fn=train_dataset.collate_fn)\n\n        val_loader = torch.utils.data.DataLoader(val_dataset,\n                                                 batch_size=batch_size,\n                                                 shuffle=False,\n                                                 pin_memory=True,\n                                                 num_workers=nw,\n                                                 collate_fn=val_dataset.collate_fn)\n\n        pawmodel = PawpularModel(args)\n        model = pawmodel#.model\n#         pawmodel.load(f\"../input/paw-models/model_f{fold_}.bin\", device=\"cuda\", weights_only=True)\n        \n        \n#         args.weights = f\"../input/paw-models/model_f{fold_}.bin\"\n#         args.weights = f\"../input/cut5m3m0wd51h03f1/weights/modelfold{fold_}-E.pth\"\n        if args.weights != \"\":\n            assert os.path.exists(args.weights), \"weights file: '{}' not exist.\".format(args.weights)\n            weights_dict = torch.load(args.weights, map_location=device)[\"model\"]\n            # 删除有关分类类别的权重\n            for k in list(weights_dict.keys()):\n                if \"head\" in k or \"dense\" in k: #  \n                    del weights_dict[k]\n                    continue\n#                 if \"model.\" in k:\n#                     weights_dict[k[6:]]=weights_dict[k]\n#                     del weights_dict[k]\n            \n            print(model.model.load_state_dict(weights_dict, strict=False))\n\n        feature_params = []\n        output_params = []\n        if args.freeze_layers:\n            for name, para in model.named_parameters():\n                # 除head外，其他权重全部冻结\n                if \"model\" in name:\n                    para.requires_grad_(False)\n                else:\n                    print(\"training {}\".format(name))\n        else:\n            for name, para in model.named_parameters():\n                # 除head外\n                if \"model\" in name and \"head\" not in name:\n                    feature_params.append(para)\n                else:\n                    output_params.append(para)\n\n        # head Linear\n\n        pg = [p for p in model.parameters() if p.requires_grad]\n        \n        scheduler = CosineScheduler(3+8, warmup_steps=3, base_lr=1*args.lr, final_lr=0.01*args.lr)\n        early_stopping = EarlyStopping(patience, verbose=False, path=\"./weights/modelfold{}-E.pth\".format(fold_))\n        for epoch in range(args.epochs):\n            if epoch == 0:\n                continue\n            args.lr = scheduler(epoch)\n            if epoch >3:\n                optimizer = optim.SGD(pg, lr=args.lr, momentum=0.999)\n            else:\n                optimizer = optim.SGD([{'params': feature_params, 'momentum':0.999},\n                                 {'params': output_params, 'lr': args.lr*1}], #  * 10, 'weight_decay': 5e-2\n                                lr=args.lr, momentum=0.9)\n            # train\n            train_loss, train_acc, _ = train_one_epoch(model=model,\n                                                    optimizer=optimizer,\n                                                    data_loader=train_loader,\n                                                    device=device,\n                                                    epoch=epoch, cutmix_prob = 0.5, mixup_prob=0.6, alpha = 0.8, lossf=True, bce=True)\n\n            # validate\n            val_loss, val_acc, _ = evaluate(model=model,\n                                         data_loader=val_loader,\n                                         device=device,\n                                         epoch=epoch, lossf=True, bce=True)\n\n            tags = [\"train_loss\", \"train_mse\", \"val_loss\", \"val_mse\", \"learning_rate\"]\n            tb_writer.add_scalar(tags[0], train_loss, epoch)\n            tb_writer.add_scalar(tags[1], train_acc, epoch)\n            tb_writer.add_scalar(tags[2], val_loss, epoch)\n            tb_writer.add_scalar(tags[3], val_acc, epoch)\n            tb_writer.add_scalar(tags[4], optimizer.param_groups[0][\"lr\"], epoch)\n            \n            if epoch > args.epochs-2:\n                torch.save(model.state_dict(), \"./weights/modelfold{}-{}.pth\".format(fold_, epoch))\n\n            early_stopping(val_loss, model)\n            # 若满足 early stopping 要求\n            if early_stopping.early_stop:\n                print(\"Early stopping\")\n                # 结束模型训练\n                break","metadata":{"execution":{"iopub.status.busy":"2021-12-18T00:54:29.751586Z","iopub.execute_input":"2021-12-18T00:54:29.752079Z","iopub.status.idle":"2021-12-18T00:54:29.783237Z","shell.execute_reply.started":"2021-12-18T00:54:29.752026Z","shell.execute_reply":"2021-12-18T00:54:29.782479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Infer Test and OOF\nIn version 1 of this notebook, we extract train embeddings and train RAPIDS SVR heads. (Click version 1 to see this). In later versions and during Kaggle submit, we load these saved RAPIDS SVR fold models and just infer data (without training anything).","metadata":{}},{"cell_type":"code","source":"if __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--num_classes', type=int, default=100)\n    parser.add_argument('--epochs', type=int, default=14)\n\n    # important\n    parser.add_argument('--batch-size', type=int, default=8)\n    parser.add_argument('--lr', type=float, default=5e-5) # 0.0001\n\n    # 数据集所在根目录\n    # http://download.tensorflow.org/example_images/flower_photos.tgz\n    # parser.add_argument('--data-path', type=str,\n    #                     default=\"../data/flower_photos\")\n\n    # 预训练权重路径，如果不想载入就设置为空字符\n    parser.add_argument('--weights', type=str, default='../input/swinweight/swin_large_patch4_window12_384_22k.pth',\n                        help='initial weights path')\n    # 是否冻结权重\n    parser.add_argument('--freeze-layers', type=bool, default=False)\n    parser.add_argument('--device', default='cuda:0', help='device id (i.e. 0 or 0,1 or cpu)')\n\n    args = parser.parse_args(args=[])\n\n    main(args)\n","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-12-18T00:54:29.784312Z","iopub.execute_input":"2021-12-18T00:54:29.784518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Compute CV Score\nBelow we compute the overall CV RSME scores of just the NN head, just the SVR head, and an ensemble of 50% NN and 50% SVR heads. Then we plot all ensemble weights to find the optimal weights for NN head and SVR heads.","metadata":{}},{"cell_type":"markdown","source":"# Make Submission CSV\nWe make a submission csv using an ensemble of both heads. We use the optimal ensemble weights that we discovered above.","metadata":{}},{"cell_type":"code","source":"# super_final_predictions = np.mean(np.column_stack(super_final_predictions), axis=1)\n# # super_final_predictions2 = np.mean(np.column_stack(super_final_predictions2), axis=1)\n# super_final_predictions3 = np.mean(np.column_stack(super_final_predictions3), axis=1)\n# df_test[\"Pawpularity\"] = (1-best_w)*super_final_predictions + best_w*super_final_predictions3\n# df_test = df_test[[\"Id\", \"Pawpularity\"]]\n# df_test.to_csv(\"submission.csv\", index=False)\n# df_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}