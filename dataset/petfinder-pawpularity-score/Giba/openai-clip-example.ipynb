{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\n\nimport cv2\nimport skimage\nimport IPython.display\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport numpy as np\nfrom glob import glob\n\nfrom collections import OrderedDict\nimport torch\nimport gc\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-16T21:50:38.636074Z","iopub.execute_input":"2022-01-16T21:50:38.636411Z","iopub.status.idle":"2022-01-16T21:50:38.90637Z","shell.execute_reply.started":"2022-01-16T21:50:38.636378Z","shell.execute_reply":"2022-01-16T21:50:38.905431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Install CLIP library","metadata":{}},{"cell_type":"code","source":"!pip install ../input/openaiclipweights/python-ftfy-master/python-ftfy-master\n!pip install ../input/openaiclipweights/clip/CLIP\n!cp ../input/openaiclipweights/CLIP-main/CLIP-main/clip/bpe_simple_vocab_16e6.txt /opt/conda/lib/python3.7/site-packages/clip/.\n!gzip -k /opt/conda/lib/python3.7/site-packages/clip/bpe_simple_vocab_16e6.txt\n!ls /opt/conda/lib/python3.7/site-packages/clip/.","metadata":{"execution":{"iopub.status.busy":"2022-01-16T21:43:00.799317Z","iopub.execute_input":"2022-01-16T21:43:00.799902Z","iopub.status.idle":"2022-01-16T21:43:34.76Z","shell.execute_reply.started":"2022-01-16T21:43:00.799867Z","shell.execute_reply":"2022-01-16T21:43:34.758841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport clip\nfrom tqdm.notebook import tqdm\nfrom torch.utils.data import DataLoader, Dataset\n\nprint(\"Torch version:\", torch.__version__)","metadata":{"execution":{"iopub.status.busy":"2022-01-16T21:45:06.237727Z","iopub.execute_input":"2022-01-16T21:45:06.238059Z","iopub.status.idle":"2022-01-16T21:45:06.245486Z","shell.execute_reply.started":"2022-01-16T21:45:06.238003Z","shell.execute_reply":"2022-01-16T21:45:06.244337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Download some images from open images collection.","metadata":{}},{"cell_type":"code","source":"!wget https://farm8.staticflickr.com/6036/6426668771_b5b915e46c_o.jpg\n!wget https://c6.staticflickr.com/8/7457/10806045045_02d3dbdcee_o.jpg\n!wget https://c1.staticflickr.com/4/3267/2888764405_0a0a608604_o.jpg\n!wget https://farm8.staticflickr.com/4028/4294212194_a49663b2b9_o.jpg\n!wget https://c5.staticflickr.com/9/8173/8019508216_6540c8686a_o.jpg\n!wget https://farm3.staticflickr.com/1146/1357102390_943c5cb999_o.jpg","metadata":{"execution":{"iopub.status.busy":"2022-01-16T22:37:49.9697Z","iopub.execute_input":"2022-01-16T22:37:49.970079Z","iopub.status.idle":"2022-01-16T22:37:57.101988Z","shell.execute_reply.started":"2022-01-16T22:37:49.970017Z","shell.execute_reply":"2022-01-16T22:37:57.10056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"files = glob('*.jpg')\nprint(files)","metadata":{"execution":{"iopub.status.busy":"2022-01-16T22:39:18.942801Z","iopub.execute_input":"2022-01-16T22:39:18.943145Z","iopub.status.idle":"2022-01-16T22:39:18.950589Z","shell.execute_reply.started":"2022-01-16T22:39:18.943106Z","shell.execute_reply":"2022-01-16T22:39:18.949394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# List pretrained CLIP models available","metadata":{}},{"cell_type":"code","source":"clip.available_models()","metadata":{"execution":{"iopub.status.busy":"2022-01-16T21:52:36.319181Z","iopub.execute_input":"2022-01-16T21:52:36.319477Z","iopub.status.idle":"2022-01-16T21:52:36.326508Z","shell.execute_reply.started":"2022-01-16T21:52:36.319446Z","shell.execute_reply":"2022-01-16T21:52:36.325453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# List pretrained weights available","metadata":{}},{"cell_type":"code","source":"!ls ../input/openaiclipweights/clip/CLIP/models/","metadata":{"execution":{"iopub.status.busy":"2022-01-16T21:53:32.893472Z","iopub.execute_input":"2022-01-16T21:53:32.894153Z","iopub.status.idle":"2022-01-16T21:53:33.728817Z","shell.execute_reply.started":"2022-01-16T21:53:32.894107Z","shell.execute_reply":"2022-01-16T21:53:33.727228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load CLIP Vision Transformer based model","metadata":{}},{"cell_type":"code","source":"model, preprocess = clip.load(\"../input/openaiclipweights/clip/CLIP/models/ViT-B-32.pt\")\nmodel.cuda().eval()","metadata":{"execution":{"iopub.status.busy":"2022-01-16T21:53:36.127499Z","iopub.execute_input":"2022-01-16T21:53:36.127813Z","iopub.status.idle":"2022-01-16T21:53:42.36108Z","shell.execute_reply.started":"2022-01-16T21:53:36.127776Z","shell.execute_reply":"2022-01-16T21:53:42.360039Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# For each image we will query for the following senteces and see what CLIP predicts. \n# You can add custom sentences here.","metadata":{}},{"cell_type":"code","source":"QUERIES = [\n    \"a dog\",\n    \"a cat\",\n    \"a elephant\",\n    \"a zebra\",\n    \"a sleeping dog\",\n    \"a sleeping cat\",\n    \"a giraffe\",\n    \"a poodle\",\n    \"animal inside a car\",\n    \"animal outside a car\",\n    \"a sofa\",\n    \"some animals\",\n    \"santa claus\",\n    \"ipod\",\n    \"two mugs\",\n    \"three mugs\",\n    \"blue sky\",\n] ","metadata":{"execution":{"iopub.status.busy":"2022-01-16T22:40:48.949668Z","iopub.execute_input":"2022-01-16T22:40:48.949979Z","iopub.status.idle":"2022-01-16T22:40:48.956157Z","shell.execute_reply.started":"2022-01-16T22:40:48.949935Z","shell.execute_reply":"2022-01-16T22:40:48.954748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Score images vs queries using clip model","metadata":{}},{"cell_type":"code","source":"with torch.no_grad():\n    for file in files:\n        print(file)\n        # Load image from file\n        img = Image.open(file).convert(\"RGB\")\n\n        # Just show image in the notebook\n        plt.imshow(cv2.resize(np.array(img), (256, 256)))\n        plt.show()\n        \n        # Preprocess image using clip\n        img = preprocess(img).unsqueeze(0).cuda()\n        \n        # Get Image embeddings\n        image_embeddings = model.encode_image(img)\n        image_embeddings /= image_embeddings.norm(dim=-1, keepdim=True)\n        \n        \n        score = []\n        for query in QUERIES:\n            texts = clip.tokenize(query).cuda()\n            \n            # Get Text Embeddings\n            text_embeddings = model.encode_text(texts)\n            text_embeddings /= text_embeddings.norm(dim=-1, keepdim=True)\n            \n            # Calc dot product between image and text embeddings\n            sc = float((image_embeddings @ text_embeddings.T).cpu().numpy())\n            score.append(sc)\n        \n        print( pd.DataFrame({'query': QUERIES, 'score': score}).sort_values('score', ascending=False) )\n        print('')\n        print('-------------------------')\n        print('')\n","metadata":{"execution":{"iopub.status.busy":"2022-01-16T22:40:49.95565Z","iopub.execute_input":"2022-01-16T22:40:49.95596Z","iopub.status.idle":"2022-01-16T22:40:56.249492Z","shell.execute_reply.started":"2022-01-16T22:40:49.955924Z","shell.execute_reply":"2022-01-16T22:40:56.248362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}