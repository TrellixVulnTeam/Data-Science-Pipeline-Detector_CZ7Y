{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sys\nimport os\nimport time\nfrom tqdm.notebook import tqdm\nimport pandas as pd\nimport numpy as np\nimport glob\nimport cv2\nimport matplotlib.pyplot as plt\nimport joblib\nimport gc\nfrom glob import glob\nfrom PIL import Image\nimport PIL\n\nsys.path.append('../input/timm-pytorch-image-models/pytorch-image-models-master')\nimport timm\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nimport cuml\n\nprint(np.__version__)\nprint(pd.__version__)\nprint(torch.__version__)\nprint(timm.__version__)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-15T20:46:51.914255Z","iopub.execute_input":"2022-02-15T20:46:51.914585Z","iopub.status.idle":"2022-02-15T20:47:02.845352Z","shell.execute_reply.started":"2022-02-15T20:46:51.9145Z","shell.execute_reply":"2022-02-15T20:47:02.843667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Install OpenAI CLIP","metadata":{}},{"cell_type":"code","source":"!pip install ftfy regex\n!pip install git+https://github.com/openai/CLIP.git","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import clip","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:47:02.846989Z","iopub.execute_input":"2022-02-15T20:47:02.847298Z","iopub.status.idle":"2022-02-15T20:47:03.061349Z","shell.execute_reply.started":"2022-02-15T20:47:02.847252Z","shell.execute_reply":"2022-02-15T20:47:03.060425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Images PATH","metadata":{}},{"cell_type":"code","source":"images_path = glob('../input/petfinder-pawpularity-score/train/*.jpg')\n\nlen(images_path), images_path[:4]","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:47:03.063587Z","iopub.execute_input":"2022-02-15T20:47:03.064035Z","iopub.status.idle":"2022-02-15T20:47:03.662324Z","shell.execute_reply.started":"2022-02-15T20:47:03.063991Z","shell.execute_reply":"2022-02-15T20:47:03.661576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"modelpath = { m.split('/')[-1].split('.')[0] :m for m in glob('../input/pytorch-pretrained-0/*.pt')+glob('../input/pytorch-pretrained-1/*.pt')+glob('../input/pytorch-pretrained-2/*.pt')+glob('../input/pytorch-pretrained-3/*.pt')}\nmodelpath","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:47:03.663837Z","iopub.execute_input":"2022-02-15T20:47:03.664318Z","iopub.status.idle":"2022-02-15T20:47:03.702059Z","shell.execute_reply.started":"2022-02-15T20:47:03.664275Z","shell.execute_reply":"2022-02-15T20:47:03.701239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The pretrained models found by the forward model selection algorithm used in this solution are listed above.","metadata":{}},{"cell_type":"code","source":"%%time\nstarttime = time.time()\n\nMODELS = {}\nPREPROC = {}\nfor arch in [\n    'tf_efficientnet_l2_ns',\n    'deit_base_distilled_patch16_384',\n    'ig_resnext101_32x48d',\n    ]:\n    print('Loading:', arch)\n    MODELS[arch + '_hflip_384'] = timm.create_model(arch, pretrained=False).to('cuda')\n    MODELS[arch + '_hflip_384'].load_state_dict(torch.load(modelpath[arch]))\n    MODELS[arch + '_hflip_384'] = MODELS[arch + '_hflip_384'].half()\n    MODELS[arch + '_hflip_384'].eval()\n    PREPROC[arch + '_hflip_384'] = create_transform(**resolve_data_config({}, model=MODELS[arch + '_hflip_384']))\n    \n    print( time.time() - starttime, 's' )\n    print( )\n    _ = gc.collect()\n    \nprint('Done')","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:47:03.704266Z","iopub.execute_input":"2022-02-15T20:47:03.704765Z","iopub.status.idle":"2022-02-15T20:48:33.399237Z","shell.execute_reply.started":"2022-02-15T20:47:03.704726Z","shell.execute_reply":"2022-02-15T20:48:33.398345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nstarttime = time.time()\n\nfor arch in [\n    'tf_efficientnet_l2_ns',\n    'ig_resnext101_32x48d',\n    'vit_large_r50_s32_384',\n    ]:\n    print('Loading:', arch)\n    MODELS[arch] = timm.create_model(arch, pretrained=False).to('cuda')\n    MODELS[arch].load_state_dict(torch.load(modelpath[arch]))\n    MODELS[arch] = MODELS[arch].half()\n    MODELS[arch].eval()    \n    PREPROC[arch] = create_transform(**resolve_data_config({}, model=MODELS[arch ]))\n    \n    print( time.time() - starttime, 's' )\n    print( )\n    _ = gc.collect()\n\nprint('Done')","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:48:33.400899Z","iopub.execute_input":"2022-02-15T20:48:33.40121Z","iopub.status.idle":"2022-02-15T20:50:00.99515Z","shell.execute_reply.started":"2022-02-15T20:48:33.401158Z","shell.execute_reply":"2022-02-15T20:50:00.994339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for arch in [\n    'RN50x4',\n    'ViT-B-16',\n    'RN50x16',\n    'ViT-B-32',\n    ]:\n    print('Loading:', arch)\n    MODELS['clip_' + arch], PREPROC['clip_' + arch] = clip.load(\"../input/openaiclipweights/clip/CLIP/models/\"+arch+\".pt\")\n    MODELS['clip_' + arch] = MODELS['clip_' + arch].float()\n    MODELS['clip_' + arch].cuda().eval()\n    \n    print( time.time() - starttime, 's' )\n    print( )\n    _ = gc.collect()   ","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:50:00.997386Z","iopub.execute_input":"2022-02-15T20:50:00.997891Z","iopub.status.idle":"2022-02-15T20:50:30.688293Z","shell.execute_reply.started":"2022-02-15T20:50:00.997847Z","shell.execute_reply":"2022-02-15T20:50:30.687535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_image_and_transform(path='', transform = None, flip = False, resize = 0):\n    img = Image.open(path).convert('RGB')\n\n    if resize>0:\n        img = img.resize((resize, resize))\n    \n    if flip==True:\n        img = img.transpose(PIL.Image.FLIP_LEFT_RIGHT)\n        width, height = img.size\n        img = img.crop((0.0*width, 0.02*height, 0.98*width, 0.98 * height))  \n    \n    img = transform(img).unsqueeze(0)\n    return img.half()\n\nSVRMODEL = joblib.load('../input/petfinder-svr-weight/svr-model-full.joblib')\nSVRSCALE = joblib.load('../input/petfinder-svr-weight/svr-scaler.joblib')\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:50:30.69196Z","iopub.execute_input":"2022-02-15T20:50:30.692206Z","iopub.status.idle":"2022-02-15T20:50:34.163324Z","shell.execute_reply.started":"2022-02-15T20:50:30.692167Z","shell.execute_reply":"2022-02-15T20:50:34.162623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"names = [\n    'tf_efficientnet_l2_ns_hflip_384',\n    'deit_base_distilled_patch16_384_hflip_384',\n    'ig_resnext101_32x48d_hflip_384',\n    \n    'tf_efficientnet_l2_ns',\n    'ig_resnext101_32x48d',\n    'vit_large_r50_s32_384',\n    \n    'clip_RN50x4',\n    'clip_ViT-B-16',\n    'clip_RN50x16',\n    'clip_ViT-B-32',\n]","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:50:34.164554Z","iopub.execute_input":"2022-02-15T20:50:34.164829Z","iopub.status.idle":"2022-02-15T20:50:34.169273Z","shell.execute_reply.started":"2022-02-15T20:50:34.164791Z","shell.execute_reply":"2022-02-15T20:50:34.168427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\ndef predict_single_image(file):\n    RES = []\n    with torch.no_grad():\n        for m in names:\n            #print('Extracting from:', m)\n            if arch == 'tf_efficientnet_l2_ns':\n                resize = 512\n            else: \n                resize = 0\n\n            if m.find('hflip')>=0:\n                image = load_image_and_transform(file, PREPROC[m], True, resize)\n            else:\n                image = load_image_and_transform(file, PREPROC[m], False, resize)\n\n            if m.find('clip')>=0:\n                res = MODELS[m].encode_image(image.to('cuda')).cpu().numpy()\n            else:\n                res = MODELS[m](image.to('cuda')).cpu().numpy()\n\n            RES.append(res)\n\n    RES = np.concatenate(RES, 1)\n    RES = SVRSCALE.transform(RES)\n    pawpularity = SVRMODEL.predict(RES)\n    \n    return pawpularity\n\npredict_single_image(images_path[1])","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:50:34.170658Z","iopub.execute_input":"2022-02-15T20:50:34.171143Z","iopub.status.idle":"2022-02-15T20:50:41.138513Z","shell.execute_reply.started":"2022-02-15T20:50:34.171104Z","shell.execute_reply":"2022-02-15T20:50:41.13769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PetNet(nn.Module):\n    def __init__(\n        self,\n        model_name = 'none',\n        out_features = 1,\n        inp_channels = 3,\n        pretrained = False,\n    ):\n        super().__init__()\n        self.model = timm.create_model(model_name, pretrained=False, in_chans=3, num_classes = 1)\n    \n    def forward(self, image):\n        output = self.model(image)\n        return output    \n\n\nEXP66 = []\nmodelfiles = glob('../input/petfinder-exp66/*.pth')\nfor mi, model_path in enumerate(modelfiles[:5]):\n    model = PetNet(\n        model_name = 'swin_large_patch4_window12_384_in22k',\n        out_features = 1,\n        inp_channels = 3,\n        pretrained = False\n    )\n    model.load_state_dict(torch.load(model_path))\n    model = model.to('cuda')\n    model = model.half()\n    model.eval()\n    EXP66.append(model)\n    \nprint(len(EXP66))","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:50:41.139842Z","iopub.execute_input":"2022-02-15T20:50:41.140326Z","iopub.status.idle":"2022-02-15T20:51:12.072955Z","shell.execute_reply.started":"2022-02-15T20:50:41.140283Z","shell.execute_reply":"2022-02-15T20:51:12.072159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PetNet2(nn.Module):\n    def __init__(\n        self,\n        model_name = 'beit_large_patch16_224',\n        out_features = 1,\n        inp_channels = 3,\n        pretrained = False\n    ):\n        super().__init__()\n        NC = 1000\n        self.model = timm.create_model(model_name, pretrained=False)\n        self.dropout = nn.Dropout(0.05)\n        self.head = nn.Linear(NC, 1)\n    \n    def forward(self, image):\n        output = self.model(image)\n        output = self.dropout(output)\n        output = self.head(output)\n        return output  \n    \n    \nEXP77 = []\nmodelfiles = glob('../input/petfinder-exp77/*.pth')\nfor mi, model_path in enumerate(modelfiles[:5]):\n    model = PetNet2(\n        model_name = 'beit_large_patch16_224',\n        out_features = 1,\n        inp_channels = 3,\n        pretrained = False\n    )\n    model.load_state_dict(torch.load(model_path))\n    model = model.to('cuda')\n    model = model.half()\n    model.eval()\n    EXP77.append(model)\n    \nprint(len(EXP77))    ","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:51:12.074555Z","iopub.execute_input":"2022-02-15T20:51:12.074829Z","iopub.status.idle":"2022-02-15T20:52:14.31424Z","shell.execute_reply.started":"2022-02-15T20:51:12.074791Z","shell.execute_reply":"2022-02-15T20:52:14.313427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import albumentations as A\n\ndef get_inference_fixed_transforms(dim = 224):\n    return A.Compose([\n            A.SmallestMaxSize(max_size=dim, p=1.0),\n            A.CenterCrop(height=dim, width=dim, p=1.0),\n        ], p=1.0)\n\n\ndef load_image(path='', transform = None):\n    image = Image.open(path).convert('RGB')\n    image = np.array(image)\n\n    if transform is not None:\n        image = transform(image = image)[\"image\"]\n\n    image = image / 255\n    image = np.transpose(image, (2, 0, 1)).astype(np.float32)\n    image = torch.tensor(image, dtype = torch.float)\n    image = image.unsqueeze(0) / 255.\n    \n    return image.half()","metadata":{"execution":{"iopub.status.busy":"2022-02-15T21:05:14.341722Z","iopub.execute_input":"2022-02-15T21:05:14.342036Z","iopub.status.idle":"2022-02-15T21:05:15.192989Z","shell.execute_reply.started":"2022-02-15T21:05:14.342001Z","shell.execute_reply":"2022-02-15T21:05:15.192183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def dl_inference(path, models, size=224):\n    img = load_image( path, get_inference_fixed_transforms(size) ).to('cuda')\n    res = []\n    with torch.no_grad():\n        for model in models:\n            res.append(model(img).sigmoid().cpu().numpy() * 100)\n    res = np.mean(res)\n    return res\n\n","metadata":{"execution":{"iopub.status.busy":"2022-02-15T21:05:17.370114Z","iopub.execute_input":"2022-02-15T21:05:17.371093Z","iopub.status.idle":"2022-02-15T21:05:17.877173Z","shell.execute_reply.started":"2022-02-15T21:05:17.371047Z","shell.execute_reply":"2022-02-15T21:05:17.876068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in tqdm(range(20)):\n    imagepath = images_path[i]\n    img = cv2.cvtColor(cv2.imread(imagepath), cv2.COLOR_RGB2BGR)\n    plt.imshow(img)\n\n    p0 = predict_single_image(imagepath) # SVR\n    p1 = dl_inference(imagepath, EXP66, 384) # Exp66\n    p2 = dl_inference(imagepath, EXP77, 224) # Exp77\n\n    pawpularity = 0.63994705 * p0 + 0.15912166 * p1 + 0.22570434 * p2\n    plt.title(str(pawpularity))\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-15T21:11:00.984459Z","iopub.execute_input":"2022-02-15T21:11:00.984766Z","iopub.status.idle":"2022-02-15T21:12:04.900379Z","shell.execute_reply.started":"2022-02-15T21:11:00.984734Z","shell.execute_reply":"2022-02-15T21:12:04.899207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}