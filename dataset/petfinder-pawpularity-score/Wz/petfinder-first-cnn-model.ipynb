{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **First CNN Model(PetFinder)**\n\n**Description**\nMillions of stray animals suffer on the streets or are euthanized in shelters every day around the world. A good picture of a stray animal might increase its chances of getting adopted. But what makes a good picture?\nOur mission is to build an ML model which is able to accurately determine a pet photo's appeal and even suggest improvements to give these rescue animals a higher chance of loving homes.\n\n**Data**\n9912 images of pet animals labeled with \"Pawpularity\".\nPhoto Metadata = (Focus, Eyes, Face, Near, Action, Accessory, Group, Collage, Human, Occlusion, Info, Blur)\n\nTo construct this ML, I have learned and refered the notebooks listed below.\n\nData transformation : https://www.kaggle.com/manabendrarout/transformers-classifier-method-starter-train\n\nPytorch tutorial : https://pytorch.org/tutorials/beginner/introyt/modelsyt_tutorial.html\n\n\n\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport path\nimport random\nimport glob\nimport cv2\nimport albumentations\nfrom albumentations.pytorch.transforms import ToTensorV2\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms","metadata":{"execution":{"iopub.status.busy":"2021-12-19T09:36:43.092103Z","iopub.execute_input":"2021-12-19T09:36:43.09243Z","iopub.status.idle":"2021-12-19T09:36:47.350944Z","shell.execute_reply.started":"2021-12-19T09:36:43.092338Z","shell.execute_reply":"2021-12-19T09:36:47.350244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Data loading from CSV files","metadata":{}},{"cell_type":"code","source":"df_data = pd.read_csv(\"../input/petfinder-pawpularity-score/train.csv\")\ndf_test = pd.read_csv(\"../input/petfinder-pawpularity-score/test.csv\")\n","metadata":{"execution":{"iopub.status.busy":"2021-12-19T09:36:47.352705Z","iopub.execute_input":"2021-12-19T09:36:47.352944Z","iopub.status.idle":"2021-12-19T09:36:47.392147Z","shell.execute_reply.started":"2021-12-19T09:36:47.35291Z","shell.execute_reply":"2021-12-19T09:36:47.391533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**1.1 Parameters settings**\n\nMany notebooks use this libarary to store all necessary parameters or config for easy editing","metadata":{}},{"cell_type":"code","source":"params = {\n    'folder_dir': '../input/petfinder-pawpularity-score/',\n    'image_dir': '../input/petfinder-pawpularity-score/train/',\n    'img_size' : 384,\n    'test_img_dir': '../input/petfinder-pawpularity-score/test',\n    'device' : 'gpu'\n}","metadata":{"execution":{"iopub.status.busy":"2021-12-19T09:36:47.393373Z","iopub.execute_input":"2021-12-19T09:36:47.393749Z","iopub.status.idle":"2021-12-19T09:36:47.398264Z","shell.execute_reply.started":"2021-12-19T09:36:47.393707Z","shell.execute_reply":"2021-12-19T09:36:47.397403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1.2 **Augmentation**\n\nAugmentation is used to transform image data into the desired type or shape, normalize it and turn it into a tensor form. At the same time, augmentation is also used to create multiple images out of the same image input by flipping, rotating or mixing up a few different images. This helps us to increase the volume of our data that we can use to train our model, which in the end will help the model predict general data correctly.","metadata":{}},{"cell_type":"code","source":"def Transform_data(DIM = params['img_size']):\n    return albumentations.Compose(\n        [\n            albumentations.Resize(DIM, DIM),\n            albumentations.Normalize(\n                mean = [0.485, 0.456, 0.406],\n                std = [0.229, 0.224, 0.225]\n            ),\n            albumentations.HorizontalFlip(p=0.5),\n            albumentations.VerticalFlip(p=0.5),\n            albumentations.Rotate(limit = 180, p=0.7),\n            albumentations.HueSaturationValue(\n                hue_shift_limit=0.2, sat_shift_limit=0.2,\n                val_shift_limit=0.2, p=0.5\n            ),\n            albumentations.RandomBrightnessContrast(\n                brightness_limit = (-0.1, 0.1),\n                contrast_limit = (-0.1, 0.1), p=0.5\n            ),\n            ToTensorV2(p=1.0)\n        ]\n    )\n","metadata":{"execution":{"iopub.status.busy":"2021-12-19T09:36:47.39966Z","iopub.execute_input":"2021-12-19T09:36:47.399998Z","iopub.status.idle":"2021-12-19T09:36:47.408917Z","shell.execute_reply.started":"2021-12-19T09:36:47.399961Z","shell.execute_reply":"2021-12-19T09:36:47.408131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Transform_val(DIM = params['img_size']):\n    return albumentations.Compose(\n        [\n            albumentations.Resize(DIM, DIM),\n            albumentations.Normalize(\n                mean = [0.485, 0.456, 0.406],\n                std = [0.229, 0.224, 0.225]\n            ),\n            ToTensorV2(p=1.0)\n        ]\n    )","metadata":{"execution":{"iopub.status.busy":"2021-12-19T09:36:47.411489Z","iopub.execute_input":"2021-12-19T09:36:47.411729Z","iopub.status.idle":"2021-12-19T09:36:47.418361Z","shell.execute_reply.started":"2021-12-19T09:36:47.411695Z","shell.execute_reply":"2021-12-19T09:36:47.417714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**1.3 Data preparation**\n\nClass created to simplify data preparation. Images are loaded and tranformed inside this class, and the resulting images and labels are returned for further process.","metadata":{}},{"cell_type":"code","source":"class PawDataSet():\n    def __init__(self,dataset, params, transform = None):\n        self.dataset = dataset\n        self.image_path = dataset['Id'].apply(lambda x: os.path.join(params['image_dir'],f'{x}.jpg'))\n        self.target_label = dataset['Pawpularity']\n        self.transform = transform\n        self.params = params\n\n    def __len__(self):\n        return len(self.image_path)\n\n    def __getitem__(self, idx):\n        image_filepath = self.image_path[idx]\n        image = cv2.imread(image_filepath)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        if self.transform is not None:\n            image = self.transform(image=image)['image']\n\n        label = torch.tensor(self.target_label[idx]).float()\n        return image, label\n","metadata":{"execution":{"iopub.status.busy":"2021-12-19T09:36:47.420552Z","iopub.execute_input":"2021-12-19T09:36:47.420984Z","iopub.status.idle":"2021-12-19T09:36:47.429246Z","shell.execute_reply.started":"2021-12-19T09:36:47.420947Z","shell.execute_reply":"2021-12-19T09:36:47.428549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TestDataSet():\n    def __init__(self,dataset, params, transform = None):\n        self.dataset = dataset\n        self.image_path = dataset['Id'].apply(lambda x: os.path.join(params['test_img_dir'],f'{x}.jpg'))\n        self.transform = transform\n        self.params = params\n\n    def __len__(self):\n        return len(self.image_path)\n\n    def __getitem__(self, idx):\n        image_filepath = self.image_path[idx]\n        image = cv2.imread(image_filepath)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        if self.transform is not None:\n            image = self.transform(image=image)['image']\n        return image\n\n","metadata":{"execution":{"iopub.status.busy":"2021-12-19T09:36:47.43131Z","iopub.execute_input":"2021-12-19T09:36:47.431551Z","iopub.status.idle":"2021-12-19T09:36:47.441929Z","shell.execute_reply.started":"2021-12-19T09:36:47.431519Z","shell.execute_reply":"2021-12-19T09:36:47.441242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load our training data through the class created above.\ntrain_df = PawDataSet(dataset = df_data, params=params, transform = Transform_data())\ntest_df = TestDataSet(dataset = df_test, params=params, transform = Transform_val())","metadata":{"execution":{"iopub.status.busy":"2021-12-19T09:36:47.442786Z","iopub.execute_input":"2021-12-19T09:36:47.444426Z","iopub.status.idle":"2021-12-19T09:36:47.474412Z","shell.execute_reply.started":"2021-12-19T09:36:47.444387Z","shell.execute_reply":"2021-12-19T09:36:47.473698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**1.4 Image visualization**","metadata":{}},{"cell_type":"code","source":"def show_image(train_dataset = train_df, inline=4):\n    plt.figure(figsize=(20, 10))\n    for i in range(inline):\n        rand = random.randint(0, len(train_dataset))\n        image, label = train_dataset[rand]\n        plt.subplot(1, inline, i%inline+1)\n        plt.axis('off')\n        plt.imshow(image.permute(2, 1, 0))\n        plt.title(f'Pawpularity: {label}')","metadata":{"execution":{"iopub.status.busy":"2021-12-19T09:36:47.477252Z","iopub.execute_input":"2021-12-19T09:36:47.477889Z","iopub.status.idle":"2021-12-19T09:36:47.485031Z","shell.execute_reply.started":"2021-12-19T09:36:47.477778Z","shell.execute_reply":"2021-12-19T09:36:47.484408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(3):\n    show_image(inline=4)","metadata":{"execution":{"iopub.status.busy":"2021-12-19T09:36:47.487184Z","iopub.execute_input":"2021-12-19T09:36:47.487827Z","iopub.status.idle":"2021-12-19T09:36:49.313789Z","shell.execute_reply.started":"2021-12-19T09:36:47.487788Z","shell.execute_reply":"2021-12-19T09:36:49.310361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**1.5 Data Loading**\n\nData(image, label) created above is loaded into dataset by using torch.utils.data.DataLoader. At the same time splitted into many small batches.","metadata":{}},{"cell_type":"code","source":"train_loader = torch.utils.data.DataLoader(train_df, batch_size=128, shuffle=True)\ntest_loader = torch.utils.data.DataLoader(test_df, batch_size=1, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-19T09:36:49.315311Z","iopub.execute_input":"2021-12-19T09:36:49.315797Z","iopub.status.idle":"2021-12-19T09:36:49.321749Z","shell.execute_reply.started":"2021-12-19T09:36:49.315759Z","shell.execute_reply":"2021-12-19T09:36:49.320793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Model building\n\n**2.1 Model**\n\nFor my first CNN model, I mostly followed the tutorial in PyTorch documentation. The model includes two convolution layers and three fully-connected layers.\n\nIn convolution layers, filters are multiplied through the pixels of every image to get a feature map. Then in fully-connected layers, data from the feature maps are compiled into smaller sizes that were specified.\n\nIn between the covolution layers, I have chosen ReLU(Rectified Linear Unit) as my linear function. This fuction drops negative numbered data while keeping the positive data as it is. Lastly, there is a Pooling layer which amplifies the data value while shrinking the size of image.","metadata":{}},{"cell_type":"code","source":"class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(3, 16, 3)\n        self.pool = nn.MaxPool2d(2,2)\n        self.conv2 = nn.Conv2d(16, 32, 3)\n        self.conv3 = nn.Conv2d(32, 64, 3)\n        self.fc1 = nn.Linear(64*46*46, 128)\n        self.fc2 = nn.Linear(128, 64)\n        self.fc3 = nn.Linear(64, 1)\n        self.drop = nn.Dropout(p=0.2)\n        self.count = 1\n        \n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = self.pool(F.relu(self.conv3(x)))\n        x = self.drop(x)\n        x = x.view(-1, self.num_flat_features(x))\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        x = x.squeeze(1)\n        return x   \n    \n    def num_flat_features(self, x):\n        size = x.size()[1:]\n        num_features =1 \n        for s in size:\n            num_features *= s\n        return num_features","metadata":{"execution":{"iopub.status.busy":"2021-12-19T09:36:49.32317Z","iopub.execute_input":"2021-12-19T09:36:49.323939Z","iopub.status.idle":"2021-12-19T09:36:49.338989Z","shell.execute_reply.started":"2021-12-19T09:36:49.323877Z","shell.execute_reply":"2021-12-19T09:36:49.338269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Loading model into GPU(cuda)**\nAt the code below, the model that I created are loaded into the GPU to accelerate the calculation.","metadata":{}},{"cell_type":"code","source":"net=Net()\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nnet.to(device)","metadata":{"execution":{"iopub.status.busy":"2021-12-19T09:36:49.342023Z","iopub.execute_input":"2021-12-19T09:36:49.342279Z","iopub.status.idle":"2021-12-19T09:36:52.923736Z","shell.execute_reply.started":"2021-12-19T09:36:49.342252Z","shell.execute_reply":"2021-12-19T09:36:52.922937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Loss and optimizer**\n\nThe loss and optimizer that I am going to use to optimize my model are coded below. \nI used criterion to calculate the loss (loss = model output - target \"Pawpularity) and pass the loss through the optimizer to change the weight of the filter that will be used in the convolution layer.","metadata":{}},{"cell_type":"code","source":"criterion = nn.MSELoss()\noptimizer = optim.AdamW(net.parameters(), lr=0.001)","metadata":{"execution":{"iopub.status.busy":"2021-12-19T09:36:52.92701Z","iopub.execute_input":"2021-12-19T09:36:52.927307Z","iopub.status.idle":"2021-12-19T09:36:52.931614Z","shell.execute_reply.started":"2021-12-19T09:36:52.927262Z","shell.execute_reply":"2021-12-19T09:36:52.930835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Train model\n\nTraining dataset are used to train the model here.","metadata":{}},{"cell_type":"code","source":"for epoch in range(2):\n    \n    running_loss = 0.0\n    for i, data in enumerate(train_loader):\n        inputs, label = data\n        inputs, label = data[0].to(device), data[1].to(device)\n        \n        optimizer.zero_grad()\n        preds = net(inputs)\n        loss = criterion(preds, label)\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n        print('[%d, %5d] loss: %.3f' %(epoch+1, i+1, running_loss/64))\n        running_loss=0.0\n        \nprint('Finished training()'.format(device))","metadata":{"execution":{"iopub.status.busy":"2021-12-19T09:36:52.933304Z","iopub.execute_input":"2021-12-19T09:36:52.933879Z","iopub.status.idle":"2021-12-19T09:44:27.342822Z","shell.execute_reply.started":"2021-12-19T09:36:52.933829Z","shell.execute_reply":"2021-12-19T09:44:27.341998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_result = []\nfor i, data in enumerate(test_loader):\n    image = data[0]\n    image = image.unsqueeze(0)\n    image = image.to(device)\n    print(image.size())\n    preds = net(image).detach().cpu()\n    preds = preds.squeeze()\n    preds = preds.tolist()\n    final_result.append(preds)\n    \nprint(final_result)\n\nfor i in final_result:\n    print(i)\n\ndf_test[\"Pawpularity\"] = final_result\ndf_test = df_test[['Id','Pawpularity']]\ndf_test.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-19T09:47:56.120703Z","iopub.execute_input":"2021-12-19T09:47:56.120963Z","iopub.status.idle":"2021-12-19T09:47:56.190865Z","shell.execute_reply.started":"2021-12-19T09:47:56.120932Z","shell.execute_reply":"2021-12-19T09:47:56.190169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test","metadata":{"execution":{"iopub.status.busy":"2021-12-19T09:44:27.517508Z","iopub.status.idle":"2021-12-19T09:44:27.518144Z","shell.execute_reply.started":"2021-12-19T09:44:27.517904Z","shell.execute_reply":"2021-12-19T09:44:27.517929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Conclusion**\n\nAccording to the printed loss, we have trained our model to have a mean squared error of about 5-10. ","metadata":{}},{"cell_type":"markdown","source":"By constructing this notebook, I have learned a lot on how a CNN Model is built and run.\nFirst, I have learned about the style of coding (Object Oriented Programming) from the notebook that I refered from. Then, through pytorch's tutorials, I have learned how pytorch helps to keep track of the gradient descent and update the parameters of filter. \n\nFor the next notebook.\nI would like to explore on how to build a model using pretrained model and construct my model so that I can test it on test data. Besides that, I would like to explore Kfold data and cross validations.","metadata":{}}]}