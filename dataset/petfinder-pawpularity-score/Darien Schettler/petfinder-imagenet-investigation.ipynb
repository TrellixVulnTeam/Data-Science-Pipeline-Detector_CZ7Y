{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"print(\"\\n... IMPORTS STARTING ...\\n\")\nprint(\"\\n\\tVERSION INFORMATION\")\n# Machine Learning and Data Science Imports\nimport tensorflow as tf; print(f\"\\t\\t– TENSORFLOW VERSION: {tf.__version__}\");\nimport tensorflow_addons as tfa; print(f\"\\t\\t– TENSORFLOW ADDONS VERSION: {tfa.__version__}\");\nimport pandas as pd; pd.options.mode.chained_assignment = None;\nimport numpy as np; print(f\"\\t\\t– NUMPY VERSION: {np.__version__}\");\nimport sklearn; print(f\"\\t\\t– SKLEARN VERSION: {sklearn.__version__}\");\nfrom sklearn.preprocessing import RobustScaler, PolynomialFeatures\nfrom sklearn.model_selection import GroupKFold, KFold;\n\n!pip install -q keras-cv-attention-models\nimport keras_cv_attention_models\n\n# Built In Imports\nfrom kaggle_datasets import KaggleDatasets\nfrom multiprocessing import cpu_count\nfrom collections import Counter\nfrom datetime import datetime\nfrom glob import glob\nimport warnings\nimport requests\nimport imageio\nimport IPython\nimport sklearn\nimport urllib\nimport zipfile\nimport pickle\nimport random\nimport shutil\nimport string\nimport math\nimport time\nimport gzip\nimport ast\nimport sys\nimport io\nimport os\nimport gc\nimport re\n\n# Visualization Imports\nfrom matplotlib.colors import ListedColormap\nimport matplotlib.patches as patches\nimport plotly.graph_objects as go\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm; tqdm.pandas();\nimport plotly.express as px\nimport seaborn as sns\nfrom PIL import Image\nimport matplotlib; print(f\"\\t\\t– MATPLOTLIB VERSION: {matplotlib.__version__}\");\nimport plotly\nimport PIL\nimport cv2\n\n\ndef seed_it_all(seed=7):\n    \"\"\" Attempt to be Reproducible \"\"\"\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\n    \nprint(\"\\n\\n... IMPORTS COMPLETE ...\\n\")\n    \nprint(\"\\n... SEEDING FOR DETERMINISTIC BEHAVIOUR ...\\n\")\nseed_it_all()","metadata":{"execution":{"iopub.status.busy":"2021-10-11T23:40:41.764811Z","iopub.execute_input":"2021-10-11T23:40:41.76521Z","iopub.status.idle":"2021-10-11T23:40:57.675494Z","shell.execute_reply.started":"2021-10-11T23:40:41.765141Z","shell.execute_reply":"2021-10-11T23:40:57.673981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"\\n... ACCELERATOR SETUP STARTING ...\\n\")\n\n# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n    TPU = tf.distribute.cluster_resolver.TPUClusterResolver()  \nexcept ValueError:\n    TPU = None\n\nif TPU:\n    print(f\"\\n... RUNNING ON TPU - {TPU.master()}...\")\n    tf.config.experimental_connect_to_cluster(TPU)\n    tf.tpu.experimental.initialize_tpu_system(TPU)\n    strategy = tf.distribute.experimental.TPUStrategy(TPU)\nelse:\n    print(f\"\\n... RUNNING ON CPU/GPU ...\")\n    # Yield the default distribution strategy in Tensorflow\n    #   --> Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy() \n\n# What Is a Replica?\n#    --> A single Cloud TPU device consists of FOUR chips, each of which has TWO TPU cores. \n#    --> Therefore, for efficient utilization of Cloud TPU, a program should make use of each of the EIGHT (4x2) cores. \n#    --> Each replica is essentially a copy of the training graph that is run on each core and \n#        trains a mini-batch containing 1/8th of the overall batch size\nN_REPLICAS = strategy.num_replicas_in_sync\n    \nprint(f\"... # OF REPLICAS: {N_REPLICAS} ...\\n\")\n\nprint(f\"\\n... ACCELERATOR SETUP COMPLTED ...\\n\")","metadata":{"execution":{"iopub.status.busy":"2021-10-11T23:40:57.682494Z","iopub.execute_input":"2021-10-11T23:40:57.682772Z","iopub.status.idle":"2021-10-11T23:40:57.697194Z","shell.execute_reply.started":"2021-10-11T23:40:57.682736Z","shell.execute_reply":"2021-10-11T23:40:57.6964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"\\n... DATA ACCESS SETUP STARTED ...\\n\")\n\nif TPU:\n    # Google Cloud Dataset path to training and validation images\n    DATA_DIR = KaggleDatasets().get_gcs_path('petfinder-pawpularity-score')\n    CAT_DIR = KaggleDatasets().get_gcs_path('cat-breeds-dataset')\n    DOG_DIR = KaggleDatasets().get_gcs_path('stanford-dogs-dataset')\n    save_locally = tf.saved_model.SaveOptions(experimental_io_device='/job:localhost')\nelse:\n    # Local path to training and validation images\n    DATA_DIR = \"/kaggle/input/petfinder-pawpularity-score\"\n    CAT_DIR = \"/kaggle/input/cat-breeds-dataset\"\n    DOG_DIR = \"/kaggle/input/stanford-dogs-dataset\"\n    save_locally = None\n    \nprint(f\"\\n... DATA DIRECTORY PATH IS:\\n\\t--> {DATA_DIR}\")\nprint(f\"\\n... CAT DIRECTORY PATH IS:\\n\\t--> {CAT_DIR}\")\nprint(f\"\\n... DOG DIRECTORY PATH IS:\\n\\t--> {DOG_DIR}\")\n\nprint(f\"\\n... IMMEDIATE CONTENTS OF DATA DIRECTORY IS:\")\nfor file in tf.io.gfile.glob(os.path.join(DATA_DIR, \"*\")): print(f\"\\t--> {file}\")\n\nprint(f\"\\n... IMMEDIATE CONTENTS OF CAT DIRECTORY IS:\")\nfor file in tf.io.gfile.glob(os.path.join(CAT_DIR, \"*\")): print(f\"\\t--> {file}\")\n\nprint(f\"\\n... IMMEDIATE CONTENTS OF DOG DIRECTORY IS:\")\nfor file in tf.io.gfile.glob(os.path.join(DOG_DIR, \"*\")): print(f\"\\t--> {file}\")\n\nprint(\"\\n\\n... DATA ACCESS SETUP COMPLETED ...\\n\")","metadata":{"execution":{"iopub.status.busy":"2021-10-11T23:40:57.698535Z","iopub.execute_input":"2021-10-11T23:40:57.699487Z","iopub.status.idle":"2021-10-11T23:40:57.726972Z","shell.execute_reply.started":"2021-10-11T23:40:57.699446Z","shell.execute_reply":"2021-10-11T23:40:57.726195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"\\n... XLA OPTIMIZATIONS STARTING ...\\n\")\n\nprint(f\"\\n... CONFIGURE JIT (JUST IN TIME) COMPILATION ...\\n\")\n# enable XLA optmizations (10% speedup when using @tf.function calls)\ntf.config.optimizer.set_jit(True)\n\nprint(f\"\\n... XLA OPTIMIZATIONS COMPLETED ...\\n\")","metadata":{"execution":{"iopub.status.busy":"2021-10-11T23:40:57.728324Z","iopub.execute_input":"2021-10-11T23:40:57.72866Z","iopub.status.idle":"2021-10-11T23:40:57.734824Z","shell.execute_reply.started":"2021-10-11T23:40:57.728612Z","shell.execute_reply":"2021-10-11T23:40:57.733986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"\\n... BASIC DATA SETUP STARTING ...\\n\\n\")\n\nprint(\"\\n... TRAIN DATAFRAME ..\\n\")\nTRAIN_CSV = os.path.join(DATA_DIR, \"train.csv\")\ntrain_df = pd.read_csv(TRAIN_CSV)\ntrain_df[\"img_path\"] = train_df.Id.apply(lambda x: os.path.join(DATA_DIR, \"train\", x+\".jpg\"))\ndisplay(train_df)\n\nprint(\"\\n... TEST DATAFRAME ..\\n\")\nTEST_CSV = os.path.join(DATA_DIR, \"test.csv\")\ntest_df = pd.read_csv(TEST_CSV)\ntest_df[\"img_path\"] = test_df.Id.apply(lambda x: os.path.join(DATA_DIR, \"test\", x+\".jpg\"))\n\ndisplay(test_df)\n\nprint(\"\\n... SAMPLE SUBMISSION DATAFRAME ..\\n\")\nSS_CSV = os.path.join(DATA_DIR, \"sample_submission.csv\")\nss_df = pd.read_csv(SS_CSV)\ndisplay(ss_df)\n\n# Set Other Variables\nprint(\"\\n... SETTING OTHER VARIABLES ..\\n\")\n\nINPUT_SHAPE = (224, 224, 3)\nN_CLASSES = train_df.Pawpularity.nunique()\nREPLICA_BATCH_SIZE = 32\nOVERALL_BATCH_SIZE = REPLICA_BATCH_SIZE * N_REPLICAS\nAUTO = tf.data.experimental.AUTOTUNE\n\nprint(\"\\n\\n... BASIC DATA SETUP FINISHING ...\\n\")","metadata":{"execution":{"iopub.status.busy":"2021-10-11T23:40:57.735946Z","iopub.execute_input":"2021-10-11T23:40:57.736345Z","iopub.status.idle":"2021-10-11T23:40:57.867689Z","shell.execute_reply.started":"2021-10-11T23:40:57.736307Z","shell.execute_reply":"2021-10-11T23:40:57.86648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"effnetb0_imagenet = tf.keras.applications.EfficientNetB0()\neffnetb0_imagenet.summary()","metadata":{"execution":{"iopub.status.busy":"2021-10-11T23:40:57.869422Z","iopub.execute_input":"2021-10-11T23:40:57.869745Z","iopub.status.idle":"2021-10-11T23:41:02.0325Z","shell.execute_reply.started":"2021-10-11T23:40:57.869706Z","shell.execute_reply":"2021-10-11T23:41:02.031737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def flatten_l_o_l(nested_list):\n    \"\"\" Flatten a list of lists \"\"\"\n    return [item for sublist in nested_list for item in sublist]\n\ndef tf_load_image(image, resize_to=INPUT_SHAPE):\n    image = tf.image.decode_jpeg(tf.io.read_file(image), channels=INPUT_SHAPE[-1])\n    image = tf.image.resize(image, size=resize_to[:-1])\n    return image\n\ndef rotate_and_crop(images):\n    \"\"\"Rotate the given image with the given rotation degree and crop for the black edges if necessary\n    Args:\n        image: A `Tensor` representing an image(s) of arbitrary size.\n    \n    Returns:\n        A rotated image.\n    \"\"\"\n    \n    \n    def _largest_rotated_rect(w, h, angle):\n        \"\"\"\n        \n        Given a rectangle of size wxh that has been rotated by 'angle' (in\n        radians), computes the width and height of the largest possible\n        axis-aligned rectangle within the rotated rectangle.\n        Original JS code by 'Andri' and Magnus Hoff from Stack Overflow\n        Converted to Python by Aaron Snoswell\n        \n        Source: http://stackoverflow.com/questions/16702966/rotate-image-and-crop-out-black-borders\n        \n        \"\"\"\n        \n        quadrant = tf.cast(tf.math.floor(angle / (math.pi/2)), dtype=tf.uint8)\n        quadrant = tf.bitwise.bitwise_and(quadrant, tf.constant(3, dtype=quadrant.dtype))\n        sign_alpha = tf.cond(tf.bitwise.bitwise_and(quadrant, tf.constant(1, dtype=quadrant.dtype))==tf.constant(0, dtype=quadrant.dtype), lambda: angle, lambda: math.pi-angle)\n        alpha = (sign_alpha % math.pi + math.pi) % math.pi\n\n        bb_w = w * tf.math.cos(alpha) + h * tf.math.sin(alpha)\n        bb_h = w * tf.math.sin(alpha) + h * tf.math.cos(alpha)\n\n        gamma = tf.cond(w<h, lambda: tf.math.atan2(bb_w, bb_w), lambda: tf.math.atan2(bb_w, bb_w))\n\n        delta = math.pi - alpha - gamma\n\n        length = tf.cond(w<h, lambda: h, lambda: w)\n\n        d = length * tf.math.cos(alpha)\n        a = d * tf.math.sin(alpha) / tf.math.sin(delta)\n\n        y = a * tf.math.cos(gamma)\n        x = y * tf.math.tan(gamma)\n\n        return (bb_w - 2 * x, bb_h - 2 * y)\n  \n    # Get desired output dimensions\n    output_height, output_width = tf.constant(INPUT_SHAPE[0], dtype=tf.float32), tf.constant(INPUT_SHAPE[1], dtype=tf.float32)\n\n    rotation_degree = (math.pi/180)*tf.random.normal(shape=(), stddev=10)\n    images = tfa.image.rotate(images, rotation_degree, interpolation='BILINEAR')\n\n    # Center crop to ommit black noise on the edges\n    lrr_width, lrr_height = _largest_rotated_rect(output_height, output_width, rotation_degree)\n    lrr_offset_w, lrr_offset_h = tf.cast(tf.math.round((output_width-lrr_width)/2), dtype=tf.int32), tf.cast(tf.math.round((output_height-lrr_height)/2), dtype=tf.int32)\n    lrr_width, lrr_height = tf.cast(tf.math.round(lrr_width), dtype=tf.int32), tf.cast(tf.math.round(lrr_height), dtype=tf.int32)\n\n    images = tf.image.crop_to_bounding_box(images, lrr_offset_h, lrr_offset_w, target_height=lrr_height, target_width=lrr_width)\n    images = tf.image.resize(images, (output_height, output_width))\n    \n    return images\n\n\ndef simple_augmentation(images, labels):\n    # Random Horizontal Flip\n    images = tf.image.random_flip_left_right(images)\n    \n    # images = rotate_and_crop(images)\n    \n    images = tfa.image.random_cutout(images, mask_size=(2*(INPUT_SHAPE[0]//30),2*(INPUT_SHAPE[0]//30)))\n    \n    # Random Saturation\n    images = tf.image.random_saturation(images, 0.975, 1.025)\n\n    # Random Hue\n    images = tf.image.random_hue(images, 0.0125)\n    \n    # Random Brightness\n    images = tf.image.random_brightness(images, 0.125)\n    \n    return images, labels","metadata":{"execution":{"iopub.status.busy":"2021-10-11T23:41:02.035342Z","iopub.execute_input":"2021-10-11T23:41:02.035574Z","iopub.status.idle":"2021-10-11T23:41:02.058673Z","shell.execute_reply.started":"2021-10-11T23:41:02.035549Z","shell.execute_reply":"2021-10-11T23:41:02.057946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_img_ds = tf.data.Dataset.from_tensor_slices(train_df.img_path.values)\ntrain_ds = train_img_ds.map(lambda x: (tf_load_image(x),x), num_parallel_calls=tf.data.AUTOTUNE).cache().batch(OVERALL_BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n\ntest_img_ds = tf.data.Dataset.from_tensor_slices(test_df.img_path.values)\ntest_ds = test_img_ds.map(lambda x: (tf_load_image(x),x), num_parallel_calls=tf.data.AUTOTUNE).batch(OVERALL_BATCH_SIZE).prefetch(tf.data.AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2021-10-11T23:41:02.061682Z","iopub.execute_input":"2021-10-11T23:41:02.061889Z","iopub.status.idle":"2021-10-11T23:41:02.19276Z","shell.execute_reply.started":"2021-10-11T23:41:02.061866Z","shell.execute_reply":"2021-10-11T23:41:02.192055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# all_train_preds = effnetb0_imagenet.predict(train_ds)\n# all_train_preds = tf.argmax(all_train_preds, axis=-1)\n","metadata":{"execution":{"iopub.status.busy":"2021-10-11T23:45:15.149952Z","iopub.execute_input":"2021-10-11T23:45:15.150814Z","iopub.status.idle":"2021-10-11T23:45:15.622114Z","shell.execute_reply.started":"2021-10-11T23:45:15.150763Z","shell.execute_reply":"2021-10-11T23:45:15.621251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_train_preds = []\nall_train_paths = []\nfor img, path, in tqdm(train_ds):\n    all_train_preds.append(tf.cast(tf.argmax(effnetb0_imagenet.predict(img), axis=-1), tf.int16))\n    all_train_paths.append(path)\n    \nall_test_preds = []\nall_test_paths = []\nfor img, path, in tqdm(train_ds):\n    all_test_preds.append(tf.cast(tf.argmax(effnetb0_imagenet.predict(img), axis=-1), tf.int16))\n    all_test_paths.append(path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.corr()[\"Pawpularity\"]","metadata":{"execution":{"iopub.status.busy":"2021-10-11T23:48:56.081221Z","iopub.execute_input":"2021-10-11T23:48:56.08151Z","iopub.status.idle":"2021-10-11T23:48:56.099446Z","shell.execute_reply.started":"2021-10-11T23:48:56.08148Z","shell.execute_reply":"2021-10-11T23:48:56.098729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **As we can see... imagenet classes are more correlated than any other given feature**","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}