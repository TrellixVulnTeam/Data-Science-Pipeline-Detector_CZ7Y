{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-05T17:41:29.962858Z","iopub.execute_input":"2021-12-05T17:41:29.963202Z","iopub.status.idle":"2021-12-05T17:41:29.992314Z","shell.execute_reply.started":"2021-12-05T17:41:29.963103Z","shell.execute_reply":"2021-12-05T17:41:29.991594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pycaret --ignore-installed llvmlite","metadata":{"execution":{"iopub.status.busy":"2021-12-05T17:41:29.994028Z","iopub.execute_input":"2021-12-05T17:41:29.994546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pycaret.regression import *\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os, cv2, re, random\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.preprocessing.image import img_to_array, load_img\nfrom tpot import TPOTClassifier\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd \nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def broad_analysis(data):\n    print('shape of the dataset')\n    print(data.shape)\n    print('============================================================')\n    print('============================================================')\n    print('columns in the dataset')\n    print(data.columns)\n    print('============================================================')\n    print('============================================================')\n    print('infos on the dataset')\n    i = 0\n    features_list = df.columns\n    while i < len(df.columns):\n        print(features_list[i])\n        print(df[str(features_list[i])].unique())\n        i += 1\n    print('============================================================')\n    print('============================================================')\n    print('infos on the type repartition')\n    print(df.dtypes.value_counts())\n    print('============================================================')\n    print('============================================================')\n    print(data.info())\n    print('============================================================')\n    print('============================================================')\n    print('head')\n    print(data.head())\n    print('============================================================')\n    print('============================================================')\n    print('tail')\n    print(data.tail())\n    print('============================================================')\n    print('============================================================')    \n    print('null data')\n    print(data.isnull().any())\n    print('============================================================')\n    print('============================================================')\n    print('description')\n    print(np.round(data.describe()))\n    print('============================================================')\n    print('============================================================')\n    print('counting_values')    \n    \ndef remove_unique_feature(df):\n    i = 0\n    features_list = df.columns\n    while i < len(features_list):\n        if len(df[features_list[i]].unique()) == 1:\n            print('dropping: ',features_list[i])\n            df.drop(features_list[i], 1, inplace=True)\n        i += 1\n    return df\n\nfrom PIL import Image\n  \ndef most_common_used_color(img):\n    # Get width and height of Image\n    width, height = img.size\n    # Initialize Variable\n    r_total = 0\n    g_total = 0\n    b_total = 0\n    count = 0\n    # Iterate throught each pixel\n    for x in range(0, width):\n        for y in range(0, height):\n            # r,g,b value of pixel\n            r, g, b = img.getpixel((x, y))\n            r_total += r\n            g_total += g\n            b_total += b\n            count += 1\n    return (r_total/count, g_total/count, b_total/count)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow import keras\nmodel = keras.models.load_model('/kaggle/input/cats-vs-dogs/model_keras.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/petfinder-pawpularity-score/train.csv\")\nbroad_analysis(df)\ndf = df.drop_duplicates()\ndf = remove_unique_feature(df)\nprint(df.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = pd.read_csv(\"/kaggle/input/petfinder-pawpularity-score/test.csv\")\nbroad_analysis(df_test)\nprint(df_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"files = []\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        if os.path.join(dirname, filename) not in [\"/kaggle/input/petfinder-pawpularity-score/sample_submission.csv\",\"/kaggle/input/petfinder-pawpularity-score/train.csv\",\"/kaggle/input/petfinder-pawpularity-score/test.csv\",\"/kaggle/input/cats-vs-dogs/model_keras.h5\"] and dirname != \"/kaggle/input/petfinder-pawpularity-score/test\":\n            files.append(os.path.join(dirname, filename))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nfrom PIL import Image\nbatch_size = 16\nimg_width = 150 \nimg_height = 150 \n\nx = []\nred = []\ngreen = []\nblue = []\nfor imgs in files:\n    img = mpimg.imread(imgs)\n    img = cv2.resize(img, (img_width,img_height), interpolation=cv2.INTER_CUBIC)\n    x.append(img)\n    img = Image.open(imgs)\n    # Convert Image into RGB\n    img = img.convert('RGB')\n    # call function\n    common_color = most_common_used_color(img)\n    red.append(common_color[0])\n    green.append(common_color[1])\n    blue.append(common_color[2])\n\nval_datagen = ImageDataGenerator(rescale=1. / 255)\ntest_generator = val_datagen.flow(np.array(x), batch_size=batch_size)\nprediction_probabilities = model.predict_generator(test_generator, verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"animal = []\nfor p in prediction_probabilities:\n    if p[0] < 0.5:\n        animal.append(0)\n    else:\n        animal.append(1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['animal'] = animal\ndf['red'] = red\ndf['green'] = green\ndf['blue'] = blue","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"files = df_test.Id.values.tolist()\nprint(files)\ni = 0\nwhile i < len(files):\n    files[i] = '/kaggle/input/petfinder-pawpularity-score/test/'+files[i]+'.jpg'\n    i += 1\nprint(files)\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nfrom PIL import Image\nbatch_size = 16\nimg_width = 150 \nimg_height = 150 \n\nx = []\nred = []\ngreen = []\nblue = []\nfor imgs in files:\n    img = mpimg.imread(imgs)\n    img = cv2.resize(img, (img_width,img_height), interpolation=cv2.INTER_CUBIC)\n    x.append(img)\n    img = Image.open(imgs)\n    # Convert Image into RGB\n    img = img.convert('RGB')\n    # call function\n    common_color = most_common_used_color(img)\n    red.append(common_color[0])\n    green.append(common_color[1])\n    blue.append(common_color[2])\n\nval_datagen = ImageDataGenerator(rescale=1. / 255)\ntest_generator = val_datagen.flow(np.array(x), batch_size=batch_size)\nprediction_probabilities = model.predict_generator(test_generator, verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"animal = []\nfor p in prediction_probabilities:\n    if p[0] < 0.5:\n        animal.append(0)\n    else:\n        animal.append(1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['animal'] = animal\ndf_test['red'] = red\ndf_test['green'] = green\ndf_test['blue'] = blue","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport math\nfrom plotnine import *\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nfrom sklearn import preprocessing\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import preprocessing\nfrom sklearn import decomposition\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nimport re\nfrom sklearn.preprocessing import StandardScaler\nimport math\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef visualise_correlation(data):\n    correlation = data.corr()\n    plt.figure(figsize=(14, 12))\n    heatmap = sns.heatmap(correlation, annot=True, linewidths=0, vmin=-1, cmap=\"RdBu_r\")\n    \ndef PCA_generator(df):\n    i = 0\n    j = 0\n    size = len(df.columns)\n    X = df.values\n    X_std = StandardScaler().fit_transform(X)\n    mean_vec = np.mean(X_std, axis=0)\n    cov_mat = (X_std - mean_vec).T.dot((X_std - mean_vec)) / (X_std.shape[0]-1)\n    print('Covariance matrix \\n%s' %cov_mat)\n    cov_mat = np.cov(X_std.T)\n    eig_vals, eig_vecs = np.linalg.eig(cov_mat)\n    print('Eigenvectors \\n%s' %eig_vecs)\n    print('\\nEigenvalues \\n%s' %eig_vals)\n    eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n    eig_pairs.sort(key=lambda x: x[0], reverse=True)\n    print('Eigenvalues in descending order:')\n    for i in eig_pairs:\n        print(i[0])\n    tot = sum(eig_vals)\n    var_exp_sorted = [(i / tot)*100 for i in sorted(eig_vals, reverse=True)]\n    cum_var_exp_sorted = np.cumsum(var_exp_sorted)\n    with plt.style.context('seaborn-whitegrid'):\n        plt.figure(figsize=(size, 4))\n        plt.bar(range(size), var_exp_sorted, alpha=0.5, align='center', label='individual explained variance')\n        plt.step(range(size), cum_var_exp_sorted, where='mid', label='cumulative explained variance')\n        plt.ylabel('Explained variance ratio')\n        plt.xlabel('Principal components')\n        plt.legend(loc='best')\n        plt.tight_layout()\n        plt.show()\n    pca = PCA(n_components=size)\n    pca.fit(X_std)\n    print('variance ratio ',pca.explained_variance_ratio_) \n\ndef pca_components(df, nb_components):\n    X = df.values\n    std_scale = preprocessing.StandardScaler().fit(X)\n    X_scaled = std_scale.transform(X)\n    pca = decomposition.PCA(n_components=nb_components)\n    pca.fit(X_scaled) \n    print (pca.explained_variance_ratio_)\n    print (pca.explained_variance_ratio_.sum())\n    X_projected = pca.transform(X_scaled)  \n    pcs = pca.components_\n\n \ndef PCA_representation(df):\n    X = df.values\n    std_scale = preprocessing.StandardScaler().fit(X)\n    X_scaled = std_scale.transform(X)\n    pca = decomposition.PCA(n_components=2)\n    pca.fit(X_scaled)\n    print(pca.explained_variance_ratio_)\n    print(pca.explained_variance_ratio_.sum())\n    X_projected = pca.transform(X_scaled)\n    pcs = pca.components_\n    for i, (x, y) in enumerate(zip(pcs[0, :], pcs[1, :])):\n        plt.plot([0, x], [0, y], color='k')\n        plt.text(x, y, df.columns[i], fontsize='14')\n    plt.plot([-0.7, 0.7], [0, 0], color='grey', ls='--')\n    plt.plot([0, 0], [-0.7, 0.7], color='grey', ls='--')\n    plt.xlim([-0.7, 0.7])\n    plt.ylim([-0.7, 0.7])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df[[ 'Subject Focus', 'Eyes', 'Face', 'Near', 'Action', 'Accessory',\n       'Group', 'Collage', 'Human', 'Occlusion', 'Info', 'Blur', 'Pawpularity',\n       'animal', 'red', 'green', 'blue']]\nlabel = df.Pawpularity.values.tolist()\nprint(list(set(df.Action.values.tolist())))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = df_test[[ 'Subject Focus', 'Eyes', 'Face', 'Near', 'Action', 'Accessory',\n       'Group', 'Collage', 'Human', 'Occlusion', 'Info', 'Blur',\n       'animal', 'red', 'green', 'blue']]\nprint(list(set(df_test.Action.values.tolist())))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df2 = df[['Subject Focus', 'Face', 'Near', 'Accessory','Action',\n       'Group', 'Collage', 'Occlusion', 'Info', 'Blur',\n       'animal', 'red', 'green', 'blue']]\n\ndf2_test = df_test[['Subject Focus', 'Face', 'Near', 'Accessory','Action',\n       'Group', 'Collage', 'Occlusion', 'Info', 'Blur',\n       'animal', 'red', 'green', 'blue']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reg = setup(data = df, \n             target = 'Pawpularity',\n             normalize = True,\n             silent = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"compare_models()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model  = create_model('omp')      ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tuned_model = tune_model(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"interpret_model(tuned_model)","metadata":{"execution":{"iopub.status.busy":"2021-12-03T10:56:07.680579Z","iopub.status.idle":"2021-12-03T10:56:07.680879Z","shell.execute_reply.started":"2021-12-03T10:56:07.680718Z","shell.execute_reply":"2021-12-03T10:56:07.680734Z"}}},{"cell_type":"code","source":"predictions = predict_model(tuned_model, data = df_test)\ndf_test['Pawpularity'] = predictions['Label']\ndf_test.to_csv('submission.csv',index=False)\ndf_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}