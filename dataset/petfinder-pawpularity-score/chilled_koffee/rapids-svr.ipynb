{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-15T22:33:30.955466Z","iopub.execute_input":"2022-01-15T22:33:30.955929Z","iopub.status.idle":"2022-01-15T22:33:34.947389Z","shell.execute_reply.started":"2022-01-15T22:33:30.955886Z","shell.execute_reply":"2022-01-15T22:33:34.946543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nsys.path.append(\"../input/tez-lib/\")\nsys.path.append(\"../input/timmmaster/\")\n\nimport tez\nimport albumentations\nimport pandas as pd\nimport cv2\nimport numpy as np\nimport timm\nimport torch.nn as nn\nfrom sklearn import metrics\nimport torch\nfrom tez.callbacks import EarlyStopping\nfrom tqdm import tqdm\nimport math\n\nclass args:\n    batch_size=32\n    image_size=384\n    \ndef sigmoid(x):\n    return 1/(1+math.exp(-x))","metadata":{"execution":{"iopub.status.busy":"2022-01-15T22:33:34.949539Z","iopub.execute_input":"2022-01-15T22:33:34.94985Z","iopub.status.idle":"2022-01-15T22:33:42.678059Z","shell.execute_reply.started":"2022-01-15T22:33:34.949809Z","shell.execute_reply":"2022-01-15T22:33:42.676986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PawpularDataset:\n    def __init__(self, image_paths, dense_features, targets, augmentations):\n        self.image_paths = image_paths\n        self.dense_features = dense_features\n        self.targets = targets\n        self.augmentations = augmentations\n        \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, item):\n        image = cv2.imread(self.image_paths[item])\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        if self.augmentations is not None:\n            augmented = self.augmentations(image=image)\n            image = augmented[\"image\"]\n            \n        image = np.transpose(image, (2, 0, 1)).astype(np.float32)\n        \n        features = self.dense_features[item, :]\n        targets = self.targets[item]\n        \n        return {\n            \"image\": torch.tensor(image, dtype=torch.float),\n            \"features\": torch.tensor(features, dtype=torch.float),\n            \"targets\": torch.tensor(targets, dtype=torch.float),\n        }\n    \nclass PawpularModel(tez.Model):\n    def __init__(self, model_name):\n        super().__init__()\n        self.model = timm.create_model(model_name, pretrained=False, in_chans=3)\n        self.model.head = nn.Linear(self.model.head.in_features, 128)\n        self.dropout = nn.Dropout(0.2)\n        self.dense1=nn.Linear(128+12,64)\n        self.dense2=nn.Linear(64,1)\n\n    def forward(self, image, features, targets=None):\n        x1 = self.model(image)\n        x = self.dropout(x1)\n        x = torch.cat([x, features], dim=1)\n        x = self.dense1(x)\n        x = self.dense2(x)\n        \n        x = torch.cat([x, x1, features], dim=1)\n        return x, 0, {}\n\ntest_aug=albumentations.Compose([\n    albumentations.RandomResizedCrop(args.image_size,args.image_size,p=1),\n    albumentations.Normalize(\n        mean=[0.485,0.456,0.406],\n        std=[0.229,0.224,0.225],\n        max_pixel_value=255.0,\n        p=1\n    )\n],p=1\n)","metadata":{"execution":{"iopub.status.busy":"2022-01-15T22:33:42.679726Z","iopub.execute_input":"2022-01-15T22:33:42.680036Z","iopub.status.idle":"2022-01-15T22:33:42.699757Z","shell.execute_reply.started":"2022-01-15T22:33:42.680001Z","shell.execute_reply":"2022-01-15T22:33:42.698524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cuml,pickle\nfrom cuml.svm import SVR\nprint(\"RAPIDS version\",cuml.__version__,'\\n')\n\nLOAD_SVR_FROM_PATH=None\ndf = pd.read_csv('../input/creating-folds/train_10folds.csv')\nprint(\"Train shape:\",df.shape)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-15T22:35:15.882469Z","iopub.execute_input":"2022-01-15T22:35:15.882858Z","iopub.status.idle":"2022-01-15T22:35:15.919608Z","shell.execute_reply.started":"2022-01-15T22:35:15.882817Z","shell.execute_reply":"2022-01-15T22:35:15.918535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"super_final_predictions = []\nsuper_final_predictions2 = []\nsuper_final_oof_predictions = []\nsuper_final_oof_predictions2 = []\nsuper_final_oof_true = []\n\nfor fold_ in range(10):\n    print('#'*25)\n    print('### FOLD',fold_+1)\n    print('#'*25)\n    \n    model = PawpularModel(model_name=\"swin_large_patch4_window12_384\")\n    model.load(f\"../input/large-swin-models/model_f{fold_}.bin\", device=\"cuda\", weights_only=True)\n\n    df_test = pd.read_csv(\"../input/petfinder-pawpularity-score/test.csv\")\n    test_img_paths = [f\"../input/petfinder-pawpularity-score/test/{x}.jpg\" for x in df_test[\"Id\"].values]\n        \n    df_valid = df[df.kfold == fold_].reset_index(drop=True)#.iloc[:160]\n    valid_img_paths = [f\"../input/petfinder-pawpularity-score/train/{x}.jpg\" for x in df_valid[\"Id\"].values]\n\n    dense_features = [\n        'Subject Focus', 'Eyes', 'Face', 'Near', 'Action', 'Accessory',\n        'Group', 'Collage', 'Human', 'Occlusion', 'Info', 'Blur'\n    ]\n    \n    name = f\"SVR_fold_{fold_}.pkl\" \n    if LOAD_SVR_FROM_PATH is None:\n        ##################\n        # EXTRACT TRAIN EMBEDDINGS\n        \n        df_train = df[df.kfold != fold_].reset_index(drop=True)#.iloc[:320]\n        train_img_paths = [f\"../input/petfinder-pawpularity-score/train/{x}.jpg\" for x in df_train[\"Id\"].values]\n        \n        train_dataset = PawpularDataset(\n            image_paths=train_img_paths,\n            dense_features=df_train[dense_features].values,\n            targets=df_train['Pawpularity'].values/100.0,\n            augmentations=test_aug,\n        )\n        print('Extracting train embedding...')\n        train_predictions = model.predict(train_dataset, batch_size=2*args.batch_size, n_jobs=-1)\n    \n        embed = np.array([]).reshape((0,128+12))\n        for preds in train_predictions:\n            embed = np.concatenate([embed,preds[:,1:]],axis=0)\n        \n        ##################\n        # FIT RAPIDS SVR\n        print('Fitting SVR...')\n        clf = SVR(C=20.0)\n        clf.fit(embed.astype('float32'), df_train.Pawpularity.values.astype('int32'))\n    \n        ##################\n        # SAVE RAPIDS SVR \n        pickle.dump(clf, open(name, \"wb\"))\n        \n    else:\n        ##################\n        # LOAD RAPIDS SVR \n        print('Loading SVR...',LOAD_SVR_FROM_PATH+name)\n        clf = pickle.load(open(LOAD_SVR_FROM_PATH+name, \"rb\"))\n\n    ##################\n    # TEST PREDICTIONS\n    test_dataset = PawpularDataset(\n        image_paths=test_img_paths,\n        dense_features=df_test[dense_features].values,\n        targets=np.ones(len(test_img_paths)),\n        augmentations=test_aug,\n    )\n    print('Predicting test...')\n    test_predictions = model.predict(test_dataset, batch_size=2*args.batch_size, n_jobs=-1)\n\n    final_test_predictions = []\n    embed = np.array([]).reshape((0,128+12))\n    for preds in test_predictions: #tqdm\n        final_test_predictions.extend(preds[:,:1].ravel().tolist())\n        embed = np.concatenate([embed,preds[:,1:]],axis=0)\n\n    final_test_predictions = [sigmoid(x) * 100 for x in final_test_predictions]\n    final_test_predictions2 = clf.predict(embed)\n    super_final_predictions.append(final_test_predictions)\n    super_final_predictions2.append(final_test_predictions2)\n    ##################\n    \n    ##################\n    # OOF PREDICTIONS\n    valid_dataset = PawpularDataset(\n        image_paths=valid_img_paths,\n        dense_features=df_valid[dense_features].values,\n        targets=df_valid['Pawpularity'].values/100.0,\n        augmentations=test_aug,\n    )\n    print('Predicting oof...')\n    valid_predictions = model.predict(valid_dataset, batch_size=2*args.batch_size, n_jobs=-1)\n\n    final_oof_predictions = []\n    embed = np.array([]).reshape((0,128+12))\n    for preds in valid_predictions:\n        final_oof_predictions.extend(preds[:,:1].ravel().tolist())\n        embed = np.concatenate([embed,preds[:,1:]],axis=0)\n\n    final_oof_predictions = [sigmoid(x) * 100 for x in final_oof_predictions]\n    final_oof_predictions2 = clf.predict(embed)    \n    super_final_oof_predictions.append(final_oof_predictions)\n    super_final_oof_predictions2.append(final_oof_predictions2)\n    \n    final_oof_true = df_valid['Pawpularity'].values\n    super_final_oof_true.append(final_oof_true)\n    ##################\n    \n    ##################\n    # COMPUTE RSME\n    \n    rsme = np.sqrt( np.mean( (super_final_oof_true[-1] - np.array(super_final_oof_predictions[-1]))**2.0 ) )\n    print('NN RSME =',rsme,'\\n')\n    rsme = np.sqrt( np.mean( (super_final_oof_true[-1] - np.array(super_final_oof_predictions2[-1]))**2.0 ) )\n    print('SVR RSME =',rsme,'\\n')\n    \n    w = 0.5\n    oof2 = (1-w)*np.array(super_final_oof_predictions[-1]) + w*np.array(super_final_oof_predictions2[-1])\n    rsme = np.sqrt( np.mean( (super_final_oof_true[-1] - oof2)**2.0 ) )\n    print('Ensemble RSME =',rsme,'\\n')","metadata":{"execution":{"iopub.status.busy":"2022-01-15T22:33:42.979008Z","iopub.status.idle":"2022-01-15T22:33:42.980283Z","shell.execute_reply.started":"2022-01-15T22:33:42.97994Z","shell.execute_reply":"2022-01-15T22:33:42.979978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"true = np.hstack(super_final_oof_true)\n\noof = np.hstack(super_final_oof_predictions)\nrsme = np.sqrt( np.mean( (oof - true)**2.0 ))\nprint('Overall CV NN head RSME =',rsme)\n\noof2 = np.hstack(super_final_oof_predictions2)\nrsme = np.sqrt( np.mean( (oof2 - true)**2.0 ))\nprint('Overall CV SVR head RSME =',rsme)\n\noof3 = (1-w)*oof + w*oof2\nrsme = np.sqrt( np.mean( (oof3 - true)**2.0 ))\nprint('Overall CV Ensemble heads RSME with 50% NN and 50% SVR =',rsme)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-15T22:33:42.98171Z","iopub.status.idle":"2022-01-15T22:33:42.982307Z","shell.execute_reply.started":"2022-01-15T22:33:42.982059Z","shell.execute_reply":"2022-01-15T22:33:42.982089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nscore = []\nfor ww in np.arange(0,1.05,0.05):\n    oof3 = (1-ww)*oof + ww*oof2\n    rsme = np.sqrt( np.mean( (oof3 - true)**2.0 ))\n    #print(f'{ww:0.2} CV Ensemble RSME =',rsme)\n    score.append(rsme)\nbest_w = np.argmin(score)*0.05\n\nplt.figure(figsize=(20,5))\nplt.plot(np.arange(21)/20.0,score,'-o')\nplt.plot([best_w],np.min(score),'o',color='black',markersize=15)\nplt.title(f'Best Overall CV RSME={np.min(score):.4} with SVR Ensemble Weight={best_w:.2}',size=16)\nplt.ylabel('Overall Ensemble RSME',size=14)\nplt.xlabel('SVR Weight',size=14)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-15T22:33:42.983377Z","iopub.status.idle":"2022-01-15T22:33:42.984381Z","shell.execute_reply.started":"2022-01-15T22:33:42.984163Z","shell.execute_reply":"2022-01-15T22:33:42.984196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"best_w = 0.2","metadata":{"execution":{"iopub.status.busy":"2022-01-15T22:33:42.985946Z","iopub.status.idle":"2022-01-15T22:33:42.98664Z","shell.execute_reply.started":"2022-01-15T22:33:42.986353Z","shell.execute_reply":"2022-01-15T22:33:42.986382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"super_final_predictions = np.mean(np.column_stack(super_final_predictions), axis=1)\nsuper_final_predictions2 = np.mean(np.column_stack(super_final_predictions2), axis=1)\ndf_test[\"Pawpularity\"] = (1-best_w)*super_final_predictions + best_w*super_final_predictions2\ndf_test = df_test[[\"Id\", \"Pawpularity\"]]\ndf_test.to_csv(\"submission.csv\", index=False)\ndf_test.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-15T22:33:42.988355Z","iopub.status.idle":"2022-01-15T22:33:42.989366Z","shell.execute_reply.started":"2022-01-15T22:33:42.98913Z","shell.execute_reply":"2022-01-15T22:33:42.989162Z"},"trusted":true},"execution_count":null,"outputs":[]}]}