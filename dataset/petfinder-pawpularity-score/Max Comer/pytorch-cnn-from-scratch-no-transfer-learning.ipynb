{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Pawpularity CNN from Scratch in Pytorch\n\nThis notebook implements a CNN with 9 convolutional layers and 2 fully connected layers from base pytorch, with no pretrained models or weights used.  Since pawpularity is bounded between 1 and 100 the final layer's activation is the sigmoid function * 100, and MSE is used as the loss function for optimization.  \n\n\nThe final competition evaluation metric is the square root of MSE or \n$ \\textrm{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2} $","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"### Load Dependencies","metadata":{"gradient":{"editing":false,"id":"57209741","kernelId":"e054bc54-048d-46c0-933a-55d23cc13c60"}}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport math\nimport time\nimport os\nfrom skimage import io, transform\nimport PIL","metadata":{"gradient":{"editing":false,"execution_count":3,"id":"e9ddbf1a","kernelId":"e054bc54-048d-46c0-933a-55d23cc13c60"},"execution":{"iopub.status.busy":"2022-01-05T16:03:35.150729Z","iopub.execute_input":"2022-01-05T16:03:35.151096Z","iopub.status.idle":"2022-01-05T16:03:36.378772Z","shell.execute_reply.started":"2022-01-05T16:03:35.150996Z","shell.execute_reply":"2022-01-05T16:03:36.378043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Config\ndata_dir = '../input/petfinder-pawpularity-score/'\nglobal_batch_size = 32\nworkers = 2\nnp.random.seed(10)\nprint(os.listdir(data_dir))\nprint(os.listdir(f'{data_dir}train')[0:4])","metadata":{"gradient":{"editing":false,"execution_count":2,"id":"e1d46881","kernelId":"e054bc54-048d-46c0-933a-55d23cc13c60"},"execution":{"iopub.status.busy":"2022-01-05T16:03:36.380441Z","iopub.execute_input":"2022-01-05T16:03:36.380674Z","iopub.status.idle":"2022-01-05T16:03:36.564491Z","shell.execute_reply.started":"2022-01-05T16:03:36.380637Z","shell.execute_reply":"2022-01-05T16:03:36.563729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\nimport matplotlib.pyplot as plt\n\nimport torch\nfrom torchvision import datasets, transforms\n\nfrom torch import nn, optim\nimport torch.nn.functional as F\nfrom torch.optim.lr_scheduler import ExponentialLR\nfrom torch.utils.data import Dataset\nfrom torch.utils.data.sampler import SubsetRandomSampler","metadata":{"execution":{"iopub.status.busy":"2022-01-05T16:03:36.567521Z","iopub.execute_input":"2022-01-05T16:03:36.568147Z","iopub.status.idle":"2022-01-05T16:03:38.255483Z","shell.execute_reply.started":"2022-01-05T16:03:36.568105Z","shell.execute_reply":"2022-01-05T16:03:38.254767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load and Explore data","metadata":{"gradient":{"editing":false,"id":"16e2ce88","kernelId":"e054bc54-048d-46c0-933a-55d23cc13c60"}}},{"cell_type":"markdown","source":"**Look at the annotations**","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(f'{data_dir}train.csv')","metadata":{"gradient":{"editing":false,"execution_count":4,"id":"f1f9f4df","kernelId":"e054bc54-048d-46c0-933a-55d23cc13c60"},"execution":{"iopub.status.busy":"2022-01-05T16:03:38.257491Z","iopub.execute_input":"2022-01-05T16:03:38.257685Z","iopub.status.idle":"2022-01-05T16:03:38.297145Z","shell.execute_reply.started":"2022-01-05T16:03:38.257661Z","shell.execute_reply":"2022-01-05T16:03:38.296487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"gradient":{"editing":false,"execution_count":4,"id":"342a38d4","kernelId":"e054bc54-048d-46c0-933a-55d23cc13c60"},"execution":{"iopub.status.busy":"2022-01-05T16:03:38.299645Z","iopub.execute_input":"2022-01-05T16:03:38.301203Z","iopub.status.idle":"2022-01-05T16:03:38.320536Z","shell.execute_reply.started":"2022-01-05T16:03:38.301165Z","shell.execute_reply":"2022-01-05T16:03:38.319922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.info()","metadata":{"execution":{"iopub.status.busy":"2022-01-05T16:03:38.3218Z","iopub.execute_input":"2022-01-05T16:03:38.322036Z","iopub.status.idle":"2022-01-05T16:03:38.342714Z","shell.execute_reply.started":"2022-01-05T16:03:38.322004Z","shell.execute_reply":"2022-01-05T16:03:38.342006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Annotations\nnp.array(train_df.iloc[2, 1:13])","metadata":{"gradient":{"editing":false,"execution_count":61,"id":"da269897","kernelId":"e054bc54-048d-46c0-933a-55d23cc13c60"},"execution":{"iopub.status.busy":"2022-01-05T16:03:38.343835Z","iopub.execute_input":"2022-01-05T16:03:38.344649Z","iopub.status.idle":"2022-01-05T16:03:38.351937Z","shell.execute_reply.started":"2022-01-05T16:03:38.344609Z","shell.execute_reply":"2022-01-05T16:03:38.351282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scores\ntrain_df.iloc[2, 13]","metadata":{"gradient":{"editing":false,"execution_count":27,"id":"791a08a2","kernelId":"e054bc54-048d-46c0-933a-55d23cc13c60"},"execution":{"iopub.status.busy":"2022-01-05T16:03:38.353075Z","iopub.execute_input":"2022-01-05T16:03:38.353434Z","iopub.status.idle":"2022-01-05T16:03:38.362452Z","shell.execute_reply.started":"2022-01-05T16:03:38.353399Z","shell.execute_reply":"2022-01-05T16:03:38.361596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n, bins, patches = plt.hist(train_df.iloc[:, 13], 50, density=True, facecolor='g', alpha=0.75)\n\nplt.xlabel('Pawpularity')\nplt.ylabel('Frequency')\nplt.title('Pawpularity Histogram')\nplt.xlim(0, 100)\n# plt.ylim(0, 0.03)\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-05T16:03:38.363561Z","iopub.execute_input":"2022-01-05T16:03:38.363943Z","iopub.status.idle":"2022-01-05T16:03:38.723506Z","shell.execute_reply.started":"2022-01-05T16:03:38.363907Z","shell.execute_reply":"2022-01-05T16:03:38.722822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Custom dataset class to attach annotations and scores to the images**","metadata":{}},{"cell_type":"code","source":"class PawpularityDataset(Dataset):\n    \"\"\"Dataset connecting animal images to the score and annotations\"\"\"\n\n    def __init__(self, csv_file, img_dir, transform=transforms.ToTensor()):\n        \"\"\"\n        Args:\n            csv_file (string): Path to the csv file with annotations.\n            img_dir (string): Directory with all the images.\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        \"\"\"\n\n        self.annotations_csv = pd.read_csv(csv_file)\n        self.img_dir = img_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.annotations_csv)\n\n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n\n        img_name = os.path.join(self.img_dir,\n                                self.annotations_csv.iloc[idx, 0])\n\n        # load each image in PIL format for compatibility with transforms\n        image = PIL.Image.open(img_name + '.jpg')\n        \n        # Columns 1 to 12 contain the annotations\n        annotations = np.array(self.annotations_csv.iloc[idx, 1:13])\n        annotations = annotations.astype('float')\n        # Column 13 has the scores\n        score = np.array(self.annotations_csv.iloc[idx, 13])\n        score = torch.tensor(score.astype('float')).view(1)\n\n        # Apply the transforms\n        image = self.transform(image)\n\n        sample = [image, annotations, score]\n        return sample","metadata":{"execution":{"iopub.status.busy":"2022-01-05T16:03:50.949035Z","iopub.execute_input":"2022-01-05T16:03:50.949323Z","iopub.status.idle":"2022-01-05T16:03:50.95853Z","shell.execute_reply.started":"2022-01-05T16:03:50.949293Z","shell.execute_reply":"2022-01-05T16:03:50.957863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Define global image transforms**","metadata":{}},{"cell_type":"code","source":"## Define transforms with image augmentation on the training set\nimg_transforms = transforms.Compose([transforms.Resize(255),\n                                     transforms.CenterCrop(224),\n                                     transforms.RandomHorizontalFlip(),\n                                     transforms.RandomRotation(20),\n                                     transforms.ToTensor(),\n                                     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                                          std=[0.229, 0.224, 0.225])])\n\nimg_transforms_valid = transforms.Compose([transforms.Resize(255),\n                                           transforms.CenterCrop(224),\n                                           transforms.ToTensor(),\n                                           transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                                                std=[0.229, 0.224, 0.225])])","metadata":{"execution":{"iopub.status.busy":"2022-01-05T16:04:15.08489Z","iopub.execute_input":"2022-01-05T16:04:15.085156Z","iopub.status.idle":"2022-01-05T16:04:15.092798Z","shell.execute_reply.started":"2022-01-05T16:04:15.085127Z","shell.execute_reply":"2022-01-05T16:04:15.09048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Load and check out the dataset**","metadata":{}},{"cell_type":"code","source":"# Load the dataset\ntrain_dataset = PawpularityDataset(f'{data_dir}train.csv', f'{data_dir}train', transform=img_transforms)\ntrain_dataset.img_dir","metadata":{"gradient":{"execution_count":444,"id":"8ef8eb2e","kernelId":"e054bc54-048d-46c0-933a-55d23cc13c60"},"execution":{"iopub.status.busy":"2022-01-05T16:04:16.442916Z","iopub.execute_input":"2022-01-05T16:04:16.443445Z","iopub.status.idle":"2022-01-05T16:04:16.469649Z","shell.execute_reply.started":"2022-01-05T16:04:16.443406Z","shell.execute_reply":"2022-01-05T16:04:16.468972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True)","metadata":{"gradient":{"execution_count":445,"id":"c8080ad0","kernelId":"e054bc54-048d-46c0-933a-55d23cc13c60"},"execution":{"iopub.status.busy":"2022-01-05T16:04:16.801394Z","iopub.execute_input":"2022-01-05T16:04:16.802142Z","iopub.status.idle":"2022-01-05T16:04:16.807106Z","shell.execute_reply.started":"2022-01-05T16:04:16.802101Z","shell.execute_reply":"2022-01-05T16:04:16.806331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Batch size of 8\nimages, annotations, scores = next(iter(dataloader))\nprint(images.shape)\nprint(scores.shape)\nprint(annotations.shape)","metadata":{"gradient":{"execution_count":446,"id":"2ffc7490","kernelId":"e054bc54-048d-46c0-933a-55d23cc13c60"},"execution":{"iopub.status.busy":"2022-01-05T16:04:21.274349Z","iopub.execute_input":"2022-01-05T16:04:21.274887Z","iopub.status.idle":"2022-01-05T16:04:21.538475Z","shell.execute_reply.started":"2022-01-05T16:04:21.27485Z","shell.execute_reply":"2022-01-05T16:04:21.53772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Look at some images**","metadata":{}},{"cell_type":"code","source":"# Helper function to de-normalize and plot images\ndef im_convert(tensor):\n    \"\"\" Display a tensor as an image. \"\"\"\n    \n    image = tensor.to(\"cpu\").clone().detach()\n    image = image.numpy().squeeze()\n    image = image * np.array((0.229, 0.224, 0.225)).reshape(3, 1, 1) + np.array((0.485, 0.456, 0.406)).reshape(3, 1, 1)\n    img = (image * 255).astype(np.uint8) # unnormalize\n    \n\n    return plt.imshow(np.transpose(img, (1, 2, 0)))","metadata":{"execution":{"iopub.status.busy":"2022-01-05T16:04:23.254805Z","iopub.execute_input":"2022-01-05T16:04:23.255431Z","iopub.status.idle":"2022-01-05T16:04:23.26181Z","shell.execute_reply.started":"2022-01-05T16:04:23.255392Z","shell.execute_reply":"2022-01-05T16:04:23.261144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"im_numpy = images.numpy() # convert images to numpy for display\n\n# plot the images in the batch, along with the corresponding labels\nfig = plt.figure(figsize=(20, 10))\n# display 20 images\nfor idx in np.arange(8):\n    ax = fig.add_subplot(2, 4, idx+1, xticks=[], yticks=[])\n    im_convert(images[idx])\n    ax.set_title(scores[idx].item())","metadata":{"gradient":{"execution_count":229,"id":"07100596","kernelId":"e054bc54-048d-46c0-933a-55d23cc13c60"},"execution":{"iopub.status.busy":"2022-01-05T16:04:25.417423Z","iopub.execute_input":"2022-01-05T16:04:25.417679Z","iopub.status.idle":"2022-01-05T16:04:27.196802Z","shell.execute_reply.started":"2022-01-05T16:04:25.417649Z","shell.execute_reply":"2022-01-05T16:04:27.196008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Set up the cnn structure","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as func\nimport torch.optim as optim","metadata":{"gradient":{"execution_count":336,"id":"70f480f6","kernelId":"e054bc54-048d-46c0-933a-55d23cc13c60"},"execution":{"iopub.status.busy":"2022-01-05T16:04:54.271393Z","iopub.execute_input":"2022-01-05T16:04:54.271903Z","iopub.status.idle":"2022-01-05T16:04:54.275958Z","shell.execute_reply.started":"2022-01-05T16:04:54.271865Z","shell.execute_reply":"2022-01-05T16:04:54.274884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the dense layer input size\n# Padding of 1 and of 3 means no change in the image dimensions apart from pooling\n\nsdim = 224/2/2/2/2/2 #maxpoolin layers reduce xy dimensions by 2\nprint(sdim)\nprint(sdim*sdim*256+12) # add in the annotations","metadata":{"gradient":{"execution_count":237,"id":"f09c2992","kernelId":"e054bc54-048d-46c0-933a-55d23cc13c60"},"execution":{"iopub.status.busy":"2022-01-05T16:04:54.717289Z","iopub.execute_input":"2022-01-05T16:04:54.718097Z","iopub.status.idle":"2022-01-05T16:04:54.722729Z","shell.execute_reply.started":"2022-01-05T16:04:54.718036Z","shell.execute_reply":"2022-01-05T16:04:54.721845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Regression(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        # covolutional layers\n        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n        \n        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n        self.conv3 = nn.Conv2d(32, 32, 3, padding=1)\n        \n        self.conv4 = nn.Conv2d(32, 64, 3, padding=1)\n        self.conv5 = nn.Conv2d(64, 64, 3, padding=1)\n        \n        self.conv6 = nn.Conv2d(64, 128, 3, padding=1)\n        self.conv7 = nn.Conv2d(128, 128, 3, padding=1)\n        \n        self.conv8 = nn.Conv2d(128, 256, 3, padding=1)\n        self.conv9 = nn.Conv2d(256, 256, 3, padding=1)\n        \n        # maxpooling layer\n        self.maxpool = nn.MaxPool2d(2, 2)\n        \n        # Dense layers\n        self.fc1 = nn.Linear(12556, 512)\n        \n        # regression output\n        self.fc4 = nn.Linear(512, 1)\n        \n        # dropout\n        self.dropout = nn.Dropout(0.3)\n    \n    def limit_range(self, x):\n        x = torch.where(x > 100.0, 100.0, x)\n        x = torch.where(x < 0.0 , 0.0, x)\n        return x\n        \n    def forward(self, data):\n        \n        img = data['images']\n        ann = data['annotations']\n        # Conv layers\n        x = self.maxpool(func.relu(self.conv1(img)))\n        \n        x = func.relu(self.conv2(x))\n        x = self.maxpool(func.relu(self.conv3(x)))\n        \n        x = func.relu(self.conv4(x))\n        x = self.maxpool(func.relu(self.conv5(x)))\n        \n        x = func.relu(self.conv6(x))\n        x = self.maxpool(func.relu(self.conv7(x)))\n        \n        x = func.relu(self.conv8(x))\n        x = self.maxpool(func.relu(self.conv9(x)))\n        \n        \n        # flatten and combine with annotations\n        x = x.view(x.shape[0], -1)\n        x = torch.cat((x, ann.float()), 1)\n        x = self.dropout(x)\n        \n        # Dense layers\n        x = self.dropout(func.relu(self.fc1(x)))\n        x = self.fc4(x).double()\n        \n        # Limit output to the 0 to 100 range\n        x = torch.sigmoid(x)*100\n        # x = self.limit_range(x)\n        \n        return x","metadata":{"gradient":{"execution_count":449,"id":"c4a436fe","kernelId":"e054bc54-048d-46c0-933a-55d23cc13c60"},"execution":{"iopub.status.busy":"2022-01-05T16:05:01.322019Z","iopub.execute_input":"2022-01-05T16:05:01.32229Z","iopub.status.idle":"2022-01-05T16:05:01.337875Z","shell.execute_reply.started":"2022-01-05T16:05:01.322262Z","shell.execute_reply":"2022-01-05T16:05:01.336807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define a custom weight initialization\n\ndef init_weights(m):\n    classname = m.__class__.__name__\n    \n    if classname.find('Linear') != -1:\n        n = m.in_features\n        y = 1./np.sqrt(n)\n        m.weight.data.normal_(0.0, y)\n        m.bias.data.fill_(0)","metadata":{"execution":{"iopub.status.busy":"2022-01-05T16:05:02.950179Z","iopub.execute_input":"2022-01-05T16:05:02.950622Z","iopub.status.idle":"2022-01-05T16:05:02.95556Z","shell.execute_reply.started":"2022-01-05T16:05:02.950587Z","shell.execute_reply":"2022-01-05T16:05:02.954584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the model and initalize loss function and optimizer\n\nmodel = None\ntorch.manual_seed(13)\nmodel = Regression()\n\n#Custom initialization\nmodel.apply(init_weights)\n\ncriterion = nn.MSELoss(reduction='sum')\n\n#Adam with L2 regularization\noptimizer = optim.AdamW(model.parameters(), lr=0.00007, weight_decay=0.2)\n\nscheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones = [1, 3, 6, 10], gamma=0.4)","metadata":{"gradient":{"execution_count":450,"id":"91b0e833","kernelId":"e054bc54-048d-46c0-933a-55d23cc13c60"},"execution":{"iopub.status.busy":"2022-01-05T16:05:04.379384Z","iopub.execute_input":"2022-01-05T16:05:04.379975Z","iopub.status.idle":"2022-01-05T16:05:04.502426Z","shell.execute_reply.started":"2022-01-05T16:05:04.379936Z","shell.execute_reply":"2022-01-05T16:05:04.501718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load a small batch to test out the forward pass\n\ntrain_dataset = PawpularityDataset(f'{data_dir}train.csv', f'{data_dir}train', transform=img_transforms)\ndataloader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\nimages, annotations, scores = next(iter(dataloader))","metadata":{"execution":{"iopub.status.busy":"2022-01-05T16:05:05.660033Z","iopub.execute_input":"2022-01-05T16:05:05.660553Z","iopub.status.idle":"2022-01-05T16:05:07.01643Z","shell.execute_reply.started":"2022-01-05T16:05:05.660515Z","shell.execute_reply":"2022-01-05T16:05:07.015679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test out the forward pass on a single batch\n# RMSE before any training (with random parameters): \nwith torch.no_grad():\n    train_loss = 0.0\n    output = model({'images': images, 'annotations': annotations})\n    loss = criterion(output, scores)\n    math.sqrt(loss.item()/64)","metadata":{"execution":{"iopub.status.busy":"2022-01-05T16:05:07.019122Z","iopub.execute_input":"2022-01-05T16:05:07.019631Z","iopub.status.idle":"2022-01-05T16:05:10.22377Z","shell.execute_reply.started":"2022-01-05T16:05:07.019592Z","shell.execute_reply":"2022-01-05T16:05:10.22311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(torch.mean(output))\nprint(torch.std(output))","metadata":{"execution":{"iopub.status.busy":"2022-01-05T16:05:10.22516Z","iopub.execute_input":"2022-01-05T16:05:10.225403Z","iopub.status.idle":"2022-01-05T16:05:10.240905Z","shell.execute_reply.started":"2022-01-05T16:05:10.225369Z","shell.execute_reply":"2022-01-05T16:05:10.24018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(model)","metadata":{"gradient":{"execution_count":451,"id":"9e7b6bf4","kernelId":"e054bc54-048d-46c0-933a-55d23cc13c60"},"execution":{"iopub.status.busy":"2022-01-05T16:05:10.24216Z","iopub.execute_input":"2022-01-05T16:05:10.242876Z","iopub.status.idle":"2022-01-05T16:05:10.247611Z","shell.execute_reply.started":"2022-01-05T16:05:10.24284Z","shell.execute_reply":"2022-01-05T16:05:10.246865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train the model","metadata":{}},{"cell_type":"code","source":"# check if CUDA is available\ntrain_on_gpu = torch.cuda.is_available()\ndevice = torch.cuda.get_device_name()\n\nif not train_on_gpu:\n    print('CUDA is not available.  Training on CPU ...')\nelse:\n    print(f'CUDA is available!  Training on GPU {device}...')","metadata":{"gradient":{"execution_count":453,"id":"edf14100","kernelId":"e054bc54-048d-46c0-933a-55d23cc13c60"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Dataset setup**","metadata":{}},{"cell_type":"code","source":"## Load and set up the final training and validation dataset (use different transforms)\n\ntrain_data = PawpularityDataset(f'{data_dir}train.csv', f'{data_dir}train', transform=img_transforms)\nvalid_data = PawpularityDataset(f'{data_dir}train.csv', f'{data_dir}train', transform=img_transforms_valid)\n\nnp.random.seed(13)\n\n# obtain random indices that will be used for traingin/validation split\nvalid_size = 0.1\nnum_train = len(train_data)\nindices = list(range(num_train))\nnp.random.shuffle(indices)\nsplit = int(np.floor(valid_size * num_train))\ntrain_idx, valid_idx = indices[split:], indices[:split]\n\n# define samplers for obtaining training and validation batches\ntrain_sampler = SubsetRandomSampler(train_idx)\nvalid_sampler = SubsetRandomSampler(valid_idx)\n\ntrain_loader = torch.utils.data.DataLoader(train_data, batch_size=global_batch_size,\n                                           sampler=train_sampler, num_workers=workers,\n                                           pin_memory=True) \nvalid_loader = torch.utils.data.DataLoader(valid_data, batch_size=global_batch_size,\n                                           sampler=valid_sampler, num_workers=workers,\n                                           pin_memory=True) \n\nprint(len(train_loader)*global_batch_size)\nprint(len(valid_loader)*global_batch_size)","metadata":{"gradient":{"execution_count":474,"id":"d8c207dd","kernelId":"e054bc54-048d-46c0-933a-55d23cc13c60"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Model training loop**","metadata":{}},{"cell_type":"code","source":"# number of epochs to train the model\n# Use 40 epochs\n\nif train_on_gpu:\n    model.cuda()\n\nn_epochs = 40\n\nvalid_loss_min = np.Inf # track change in validation loss\n\ntrain_losses, valid_losses = [], []\n\nfor epoch in range(1, n_epochs+1):\n    \n    start = time.time()\n    current_lr = scheduler.get_last_lr()[0]\n    \n    # keep track of training and validation loss\n    train_loss = 0.0\n    valid_loss = 0.0\n    \n    ###################\n    # train the model #\n    ###################\n    # put in training mode (enable dropout)\n    model.train()\n    for images, annotations, scores in train_loader:\n        # move tensors to GPU if CUDA is available\n        if train_on_gpu:\n            images, annotations, scores = images.cuda(), annotations.cuda(), scores.cuda()\n        # clear the gradients of all optimized variables\n        optimizer.zero_grad()\n        # forward pass: compute predicted outputs by passing inputs to the model\n        # the annotations get added in the dense layers\n        output = model({'images': images, 'annotations': annotations})\n        # print(output.dtype)\n        # print(scores.dtype)\n        # calculate the batch loss\n        loss = criterion(output, scores)\n        # backward pass: compute gradient of the loss with respect to model parameters\n        loss.backward()\n        # perform a single optimization step (parameter update)\n        optimizer.step()\n        # update training loss\n        train_loss += loss.item()\n        \n    ######################    \n    # validate the model #\n    ######################\n    # eval mode (no dropout)\n    model.eval()\n    with torch.no_grad():\n        for images, annotations, scores in valid_loader:\n            # move tensors to GPU if CUDA is available\n            if train_on_gpu:\n                images, annotations, scores = images.cuda(), annotations.cuda(), scores.cuda()\n            # forward pass: compute predicted outputs by passing inputs to the model\n            output = model({'images': images, 'annotations': annotations})\n            # calculate the batch loss\n            loss = criterion(output, scores)\n            # update average validation loss \n            valid_loss += loss.item()\n    \n    # calculate RMSE\n    train_loss = math.sqrt(train_loss/len(train_loader.sampler))\n    valid_loss = math.sqrt(valid_loss/len(valid_loader.sampler))\n    \n    train_losses.append(train_loss)\n    valid_losses.append(valid_loss)\n        \n    # increment learning rate decay\n    scheduler.step()\n    \n    # print training/validation statistics \n    # print(f'Epoch: {e}, {float(time.time() - start):.3f} seconds, lr={optimizer.lr}')\n    print('Epoch: {}, time: {:.3f}s, lr: {:.6f} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n        epoch, float(time.time() - start), current_lr, train_loss, valid_loss))\n    \n    # save model if validation loss has decreased\n    if valid_loss <= valid_loss_min:\n        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n        valid_loss_min,\n        valid_loss))\n        torch.save(model.state_dict(), 'pawpularity_best_model.pt')\n        valid_loss_min = valid_loss","metadata":{"gradient":{"execution_count":478,"id":"597dbdfb","kernelId":"e054bc54-048d-46c0-933a-55d23cc13c60"},"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Diagnostics and performance","metadata":{}},{"cell_type":"code","source":"# Load the best performing model on the validation set\nmodel.load_state_dict(torch.load('pawpularity_best_model.pt'))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get the distribution of predictions\n\npredictions = []\nscore_list = []\n\nmodel.eval()\nwith torch.no_grad():\n    for images, annotations, scores in valid_loader:\n        # move tensors to GPU if CUDA is available\n        if train_on_gpu:\n            images, annotations, scores = images.cuda(), annotations.cuda(), scores.cuda()\n        # forward pass: compute predicted outputs by passing inputs to the model\n        output = model({'images': images, 'annotations': annotations})\n        predictions.extend(list(output.cpu().detach().numpy().reshape(len(output),)))\n        score_list.extend(list(scores.cpu().detach().numpy().reshape(len(scores),)))\n        \n\npreds_df = pd.DataFrame({'preds': predictions})\npreds_df.describe()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Manually Check RMSE\ndiffs = np.array(score_list) - np.array(predictions)\nprint(math.sqrt((diffs @ diffs)/len(valid_loader.sampler)))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check that manually increasing the variance doesn't help\n# Currently slightly increasing the std dev manually actually does reduce validation error\n\nmean = np.mean(np.array(predictions))\nstddev = np.std((np.array(predictions)))\nprint(mean, stddev)\nupdated_normalized = 1.5*(predictions-mean)/stddev\nnew_predictions = updated_normalized+predictions\n\ndiffs = np.array(score_list) - np.array(new_predictions)\nprint(math.sqrt((diffs @ diffs)/len(valid_loader.sampler)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Histogram of validation predictions \n\nn, bins, patches = plt.hist(predictions, 50, density=True, facecolor='g', alpha=0.75)\n\nplt.xlabel('Pawpularity')\nplt.ylabel('Frequency')\nplt.title('Predicted Pawpularity Histogram')\nplt.xlim(0, 100)\nplt.ylim(0, .2)\nplt.grid(True)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The range could still be a lot greater, and the model is failing completely at predicting the highest ranked images that get a score of 100.  ","metadata":{}},{"cell_type":"code","source":"# Histogram of validation set actual scores\n\nn, bins, patches = plt.hist(train_df.iloc[valid_idx, 13], 50, density=True, facecolor='g', alpha=0.75)\n\nplt.xlabel('Pawpularity')\nplt.ylabel('Frequency')\nplt.title('Actual Pawpularity Histogram')\nplt.xlim(0, 100)\nplt.ylim(0, .2)\nplt.grid(True)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the losses\nfig = plt.figure()\nax = plt.axes()\nax.plot(list(range(1, len(train_losses))), train_losses[1:])\nax.plot(list(range(1, len(valid_losses))), valid_losses[1:]);\nprint(f'best score: {valid_loss_min}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Show examples of images and predicted vs. actual scores","metadata":{}},{"cell_type":"code","source":"images, annotations, scores = next(iter(valid_loader))\nimages, annotations, scores = images.cuda(), annotations.cuda(), scores.cuda()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_plot = model({'images': images, 'annotations': annotations}).cpu()\nimages, annotations, scores = images.cpu(), annotations.cpu(), scores.cpu()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot the images in the batch, along with the corresponding labels and predictions\n\nfig = plt.figure(figsize=(20, 10))\n# display 20 images\nfor idx in np.arange(12):\n    ax = fig.add_subplot(3, 4, idx+1, xticks=[], yticks=[])\n    im_convert(images[idx])\n    ax.set_title(f'Act: {round(scores[idx].item())} Pred: {round(output_plot[idx].item())}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Use the model to predict the test dataset","metadata":{}},{"cell_type":"code","source":"test_df = pd.read_csv(f'{data_dir}test.csv')\ntest_df.head(10)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the best performing model on the validation set\nmodel.load_state_dict(torch.load('pawpularity_best_model.pt'))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PawpularityTestDataset(Dataset):\n    \"\"\"Dataset connecting dog images to the score and annotations\"\"\"\n\n    def __init__(self, csv_file, img_dir, transform=transforms.ToTensor()):\n        \"\"\"\n        Args:\n            csv_file (string): Path to the csv file with annotations.\n            img_dir (string): Directory with all the images.\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        \"\"\"\n\n        self.annotations_csv = pd.read_csv(csv_file)\n        self.img_dir = img_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.annotations_csv)\n\n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n\n        img_name = os.path.join(self.img_dir,\n                                self.annotations_csv.iloc[idx, 0])\n\n        # load each image in PIL format for compatibility with transforms\n        image = PIL.Image.open(img_name + '.jpg')\n\n        annotations = np.array(self.annotations_csv.iloc[idx, 1:13])\n        annotations = annotations.astype('float')\n\n        # Apply the transforms\n        image = self.transform(image)\n\n        sample = [image, annotations]\n        return sample","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Load the test dataset\ntest_data = PawpularityTestDataset(f'{data_dir}test.csv', f'{data_dir}test', transform=img_transforms_valid)\n\nbatch_size = min(len(test_data), 32)\n\ntest_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=workers) ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step through with a reasonable batch size and build up the output dataset\n\nmodel.eval()\noutputs = []\nfor images, annotations in test_loader:\n    # move tensors to GPU if CUDA is available\n    if train_on_gpu:\n        images, annotations = images.cuda(), annotations.cuda()\n    test_output = model({'images': images, 'annotations': annotations})\n    outputs.extend(list(test_output.cpu().detach().numpy().reshape(len(test_output),)))\n    \nimg_names = list( test_df.iloc[:, 0].values)\noutputs = [round(x, 2) for x in outputs]\n\noutput_df = pd.DataFrame({'Id': img_names, 'Pawpularity': outputs})\noutput_df.head(10)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Write the output in the required format\noutput_df.to_csv('submission.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}