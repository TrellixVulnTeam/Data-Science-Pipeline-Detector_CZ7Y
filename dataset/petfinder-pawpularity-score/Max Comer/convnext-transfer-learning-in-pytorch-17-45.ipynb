{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Pawpularity ConvNext Transfer Learning Approach in Pytorch\n\nThis notebook implements the new convnext architecture ([A ConvNet for the 2020s](https://arxiv.org/abs/2201.03545)) with pre-trained weights, replacing the output layer and re-training the last stage of convlutional layers to predict a pawpularity score bounded between 0 and 100.  Everything is implemented in pytorch.  Training is also reasonably fast with only ~7 epochs per fold necessary to get to maximum performance on the validation set.  \n\n- A custom pytorch dataset class is implemented to attach scores to each image file, as well as the annotations.  Currently only the images are being used to train the model.  \n- The model is a convnext architecture where the final fully connected layer is replaced with two fully connected layers and output 1 value.\n- The model starts with pretrained weights for all of the convolutional layers, and the final set of layers in the model (stage 3) is unfrozen to allow it to learn a feature representation more specifi to this task. \n- The final activation is sigmoid to bound the output between 1 and 0, and output is multiplied by 100 in the training loop to give it a bounded output between 0 and 100 which matches the range of pawpularity scores.  \n- The model is optimising for mean squared error(MSE), using Adam with weight decay to reduce overfitting.\n- This uses 10 folds, and trains 10 models, keeping the weights from epoch that performed the best on the validation set for each.  The test dataset is predicted by running it through each of these 10 models and taking the average of the score predictions.\n- The final competition evaluation metric is the square root of MSE or \n$ \\textrm{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2} $\n","metadata":{}},{"cell_type":"markdown","source":"### Load Dependencies","metadata":{"gradient":{"editing":false,"id":"57209741","kernelId":"e054bc54-048d-46c0-933a-55d23cc13c60"}}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport math\nimport time\nimport os\nfrom skimage import io, transform\nimport PIL","metadata":{"gradient":{"editing":false,"execution_count":3,"id":"e9ddbf1a","kernelId":"e054bc54-048d-46c0-933a-55d23cc13c60"},"execution":{"iopub.status.busy":"2022-01-14T01:42:46.200911Z","iopub.execute_input":"2022-01-14T01:42:46.201628Z","iopub.status.idle":"2022-01-14T01:42:47.541419Z","shell.execute_reply.started":"2022-01-14T01:42:46.201534Z","shell.execute_reply":"2022-01-14T01:42:47.540713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\nimport matplotlib.pyplot as plt\n\nimport torch\nfrom torchvision import datasets, transforms, models\n\nfrom torch import nn, optim\nimport torch.nn.functional as F\nfrom torch.optim.lr_scheduler import ExponentialLR\nfrom torch.utils.data import Dataset\nfrom torch.utils.data.sampler import SubsetRandomSampler","metadata":{"execution":{"iopub.status.busy":"2022-01-14T01:42:47.544096Z","iopub.execute_input":"2022-01-14T01:42:47.544499Z","iopub.status.idle":"2022-01-14T01:42:49.324428Z","shell.execute_reply.started":"2022-01-14T01:42:47.544461Z","shell.execute_reply":"2022-01-14T01:42:49.323651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Config\ndata_dir = '../input/petfinder-pawpularity-score/'\nmodel_dir = '../input/convnext-pretrained-model-v2/'\nweights_dir = '../input/convnext-pretrained-weights-2-stage/'\nworking_dir = './'\nglobal_batch_size = 64\nworkers = 2\nnp.random.seed(10)\nprint(os.listdir(data_dir))\nprint(os.listdir(f'{data_dir}train')[0:4])","metadata":{"gradient":{"editing":false,"execution_count":2,"id":"e1d46881","kernelId":"e054bc54-048d-46c0-933a-55d23cc13c60"},"execution":{"iopub.status.busy":"2022-01-14T01:42:49.325782Z","iopub.execute_input":"2022-01-14T01:42:49.326047Z","iopub.status.idle":"2022-01-14T01:42:49.510985Z","shell.execute_reply.started":"2022-01-14T01:42:49.326012Z","shell.execute_reply":"2022-01-14T01:42:49.50954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load and Explore data","metadata":{"gradient":{"editing":false,"id":"16e2ce88","kernelId":"e054bc54-048d-46c0-933a-55d23cc13c60"}}},{"cell_type":"markdown","source":"**Look at the annotations**","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(f'{data_dir}train.csv')","metadata":{"gradient":{"editing":false,"execution_count":4,"id":"f1f9f4df","kernelId":"e054bc54-048d-46c0-933a-55d23cc13c60"},"execution":{"iopub.status.busy":"2022-01-14T01:42:49.513222Z","iopub.execute_input":"2022-01-14T01:42:49.513478Z","iopub.status.idle":"2022-01-14T01:42:49.546116Z","shell.execute_reply.started":"2022-01-14T01:42:49.513434Z","shell.execute_reply":"2022-01-14T01:42:49.545349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"gradient":{"editing":false,"execution_count":4,"id":"342a38d4","kernelId":"e054bc54-048d-46c0-933a-55d23cc13c60"},"execution":{"iopub.status.busy":"2022-01-14T01:42:49.548494Z","iopub.execute_input":"2022-01-14T01:42:49.548948Z","iopub.status.idle":"2022-01-14T01:42:49.570337Z","shell.execute_reply.started":"2022-01-14T01:42:49.548909Z","shell.execute_reply":"2022-01-14T01:42:49.569619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.info()","metadata":{"execution":{"iopub.status.busy":"2022-01-14T01:42:49.571441Z","iopub.execute_input":"2022-01-14T01:42:49.57177Z","iopub.status.idle":"2022-01-14T01:42:49.601385Z","shell.execute_reply.started":"2022-01-14T01:42:49.571717Z","shell.execute_reply":"2022-01-14T01:42:49.600495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Annotations\nnp.array(train_df.iloc[2, 1:13])","metadata":{"gradient":{"editing":false,"execution_count":61,"id":"da269897","kernelId":"e054bc54-048d-46c0-933a-55d23cc13c60"},"execution":{"iopub.status.busy":"2022-01-14T01:42:49.603067Z","iopub.execute_input":"2022-01-14T01:42:49.603476Z","iopub.status.idle":"2022-01-14T01:42:49.610779Z","shell.execute_reply.started":"2022-01-14T01:42:49.603439Z","shell.execute_reply":"2022-01-14T01:42:49.609088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scores\ntrain_df.iloc[2, 13]","metadata":{"gradient":{"editing":false,"execution_count":27,"id":"791a08a2","kernelId":"e054bc54-048d-46c0-933a-55d23cc13c60"},"execution":{"iopub.status.busy":"2022-01-14T01:42:49.613655Z","iopub.execute_input":"2022-01-14T01:42:49.614159Z","iopub.status.idle":"2022-01-14T01:42:49.629464Z","shell.execute_reply.started":"2022-01-14T01:42:49.614095Z","shell.execute_reply":"2022-01-14T01:42:49.628629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n, bins, patches = plt.hist(train_df.iloc[:, 13], 50, density=True, facecolor='g', alpha=0.75)\n\nplt.xlabel('Pawpularity')\nplt.ylabel('Frequency')\nplt.title('Pawpularity Histogram')\nplt.xlim(0, 100)\n# plt.ylim(0, 0.03)\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-14T01:42:49.632093Z","iopub.execute_input":"2022-01-14T01:42:49.63239Z","iopub.status.idle":"2022-01-14T01:42:49.991023Z","shell.execute_reply.started":"2022-01-14T01:42:49.632349Z","shell.execute_reply":"2022-01-14T01:42:49.990337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Custom dataset class to attach annotations and scores to the images**\n\nThis is a critical step to attach the classes and annotations to the image files and allow this to be put into a pytorch dataloader.  ","metadata":{}},{"cell_type":"code","source":"class PawpularityDataset(Dataset):\n    \"\"\"Dataset connecting animal images to the score and annotations\"\"\"\n\n    def __init__(self, csv_file, img_dir, transform=transforms.ToTensor()):\n        \"\"\"\n        Args:\n            csv_file (string): Path to the csv file with annotations.\n            img_dir (string): Directory with all the images.\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        \"\"\"\n\n        self.annotations_csv = pd.read_csv(csv_file)\n        self.img_dir = img_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.annotations_csv)\n\n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n\n        img_name = os.path.join(self.img_dir,\n                                self.annotations_csv.iloc[idx, 0])\n\n        # load each image in PIL format for compatibility with transforms\n        image = PIL.Image.open(img_name + '.jpg')\n        \n        # Columns 1 to 12 contain the annotations\n        annotations = np.array(self.annotations_csv.iloc[idx, 1:13])\n        annotations = annotations.astype('float')\n        # Column 13 has the scores\n        score = np.array(self.annotations_csv.iloc[idx, 13])\n        score = torch.tensor(score.astype('float')).view(1).to(torch.float32)\n\n        # Apply the transforms\n        image = self.transform(image)\n\n        sample = [image, annotations, score]\n        return sample","metadata":{"execution":{"iopub.status.busy":"2022-01-14T01:42:49.994609Z","iopub.execute_input":"2022-01-14T01:42:49.995052Z","iopub.status.idle":"2022-01-14T01:42:50.005061Z","shell.execute_reply.started":"2022-01-14T01:42:49.995021Z","shell.execute_reply":"2022-01-14T01:42:50.004382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Define global image transforms**","metadata":{}},{"cell_type":"code","source":"# Test out the transforms on an image (images need to be made the same size for the dataset to work)\n# Apply some image augmentation on the training set (rotation, flip)\n# Normalize using imagenet RGB mean and std\n\nimg_transforms = transforms.Compose([transforms.Resize(255),\n                                     transforms.CenterCrop(224),\n                                     transforms.RandomHorizontalFlip(),\n                                     transforms.RandomRotation(20),\n                                     transforms.ToTensor(),\n                                     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                                          std=[0.229, 0.224, 0.225])])\n\nimg_transforms_valid = transforms.Compose([transforms.Resize(255),\n                                           transforms.CenterCrop(224),\n                                           transforms.ToTensor(),\n                                           transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                                                std=[0.229, 0.224, 0.225])])","metadata":{"execution":{"iopub.status.busy":"2022-01-14T01:42:50.006312Z","iopub.execute_input":"2022-01-14T01:42:50.007823Z","iopub.status.idle":"2022-01-14T01:42:50.018073Z","shell.execute_reply.started":"2022-01-14T01:42:50.00778Z","shell.execute_reply":"2022-01-14T01:42:50.017391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Load and check out the datasets and create kfold dataloaders**","metadata":{}},{"cell_type":"code","source":"# Load and set up the final training and validation dataset (use different transforms)\n# Return a list of train/valid dataloaders with different train/test splits for cross validation\nfrom sklearn.model_selection import KFold\n\n# Create two versions of the dataset with and without image augmentation\naugmented_data = PawpularityDataset(f'{data_dir}train.csv', f'{data_dir}train', transform=img_transforms)\nbase_transform_data = PawpularityDataset(f'{data_dir}train.csv', f'{data_dir}train', transform=img_transforms_valid)\n\ndef get_cv_dataloaders(augmented_data, base_transform_data, folds=5, cv_shuffle=True, rands=10):\n    \n    num_images = len(augmented_data)\n    indices = list(range(num_images))\n    \n    dataloaders = []\n    \n    # use sklearn kfold to split into random training/validation indices\n    cv = KFold(n_splits=folds, random_state=rands, shuffle=cv_shuffle)\n    for train_idx, valid_idx in cv.split(indices):\n        # define samplers for obtaining training and validation batches\n        train_sampler = SubsetRandomSampler(train_idx)\n        valid_sampler = SubsetRandomSampler(valid_idx)\n\n        # create dataloaders using the cv indexes\n        train_loader = torch.utils.data.DataLoader(augmented_data, batch_size=global_batch_size,\n                                                   sampler=train_sampler, num_workers=workers,\n                                                   pin_memory=True) \n        # sample the validation dataset from a separate dataset the doesn't include the image aug transformations.\n        valid_loader = torch.utils.data.DataLoader(base_transform_data, batch_size=global_batch_size,\n                                                   sampler=valid_sampler, num_workers=workers,\n                                                   pin_memory=True) \n\n        # print('Train length: ', len(train_loader)*global_batch_size)\n        # print('Valid length: ', len(valid_loader)*global_batch_size)\n        \n        dataloaders.append((train_loader, valid_loader))\n        \n    return dataloaders\n\n        \n","metadata":{"execution":{"iopub.status.busy":"2022-01-14T01:42:50.020336Z","iopub.execute_input":"2022-01-14T01:42:50.021405Z","iopub.status.idle":"2022-01-14T01:42:50.196013Z","shell.execute_reply.started":"2022-01-14T01:42:50.021367Z","shell.execute_reply":"2022-01-14T01:42:50.195233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv_dataloaders = get_cv_dataloaders(augmented_data=augmented_data, \n                                base_transform_data=base_transform_data,\n                                folds=3, \n                                cv_shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-14T01:42:50.198046Z","iopub.execute_input":"2022-01-14T01:42:50.198243Z","iopub.status.idle":"2022-01-14T01:42:50.207286Z","shell.execute_reply.started":"2022-01-14T01:42:50.198218Z","shell.execute_reply":"2022-01-14T01:42:50.206611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Batch size of 64\ntl = cv_dataloaders[0][1]\nimages, annotations, scores = next(iter(tl))\nprint(images.shape)\nprint(scores.shape)\nprint(annotations.shape)","metadata":{"gradient":{"execution_count":446,"id":"2ffc7490","kernelId":"e054bc54-048d-46c0-933a-55d23cc13c60"},"scrolled":true,"execution":{"iopub.status.busy":"2022-01-14T01:42:50.208595Z","iopub.execute_input":"2022-01-14T01:42:50.209022Z","iopub.status.idle":"2022-01-14T01:42:57.746434Z","shell.execute_reply.started":"2022-01-14T01:42:50.208987Z","shell.execute_reply":"2022-01-14T01:42:57.745611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Look at some images**","metadata":{}},{"cell_type":"code","source":"# Helper function to unnormalize and plot images\ndef im_convert(tensor):\n    \"\"\" Display a tensor as an image. \"\"\"\n    \n    image = tensor.to(\"cpu\").clone().detach()\n    image = image.numpy().squeeze()\n    image = image * np.array((0.229, 0.224, 0.225)).reshape(3, 1, 1) + np.array((0.485, 0.456, 0.406)).reshape(3, 1, 1)\n    img = (image * 255).astype(np.uint8) # unnormalize\n    \n\n    return plt.imshow(np.transpose(img, (1, 2, 0)))","metadata":{"execution":{"iopub.status.busy":"2022-01-14T01:42:57.749492Z","iopub.execute_input":"2022-01-14T01:42:57.750113Z","iopub.status.idle":"2022-01-14T01:42:57.756298Z","shell.execute_reply.started":"2022-01-14T01:42:57.75008Z","shell.execute_reply":"2022-01-14T01:42:57.755603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"im_numpy = images.numpy() # convert images to numpy for display\n\n# plot the images in the batch, along with the corresponding labels\nfig = plt.figure(figsize=(20, 10))\n# display 20 images\nfor idx in np.arange(8):\n    ax = fig.add_subplot(2, 4, idx+1, xticks=[], yticks=[])\n    im_convert(images[idx])\n    ax.set_title(scores[idx].item())","metadata":{"gradient":{"execution_count":229,"id":"07100596","kernelId":"e054bc54-048d-46c0-933a-55d23cc13c60"},"execution":{"iopub.status.busy":"2022-01-14T01:42:57.757699Z","iopub.execute_input":"2022-01-14T01:42:57.75841Z","iopub.status.idle":"2022-01-14T01:42:59.597499Z","shell.execute_reply.started":"2022-01-14T01:42:57.758373Z","shell.execute_reply":"2022-01-14T01:42:59.596635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Set up the model structure","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as func\nimport torch.optim as optim","metadata":{"gradient":{"execution_count":336,"id":"70f480f6","kernelId":"e054bc54-048d-46c0-933a-55d23cc13c60"},"execution":{"iopub.status.busy":"2022-01-14T01:42:59.598603Z","iopub.execute_input":"2022-01-14T01:42:59.59889Z","iopub.status.idle":"2022-01-14T01:42:59.603579Z","shell.execute_reply.started":"2022-01-14T01:42:59.59885Z","shell.execute_reply":"2022-01-14T01:42:59.602988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Downloading the pretrained model**\n\nTo access the pretrained model in a kaggle notebook, download it via pytorch on a local notebook, save the model using torch.save.  Then upload it to your kaggle notebook as a dataset which you can then load via torch.load without having to connect to the internet.\n\nFor the convnext model, I downloaded the [model definition](https://github.com/facebookresearch/ConvNeXt/blob/dc7823d8a2ecc554fcd57ff6cdb7748011bcdedd/models/convnext.py) (which includes url paths for pretrained weights) to a file, and uploaded this as a dataset to use in my kaggle notebook.  \n\nI'm also using the a [timm dataset](https://www.kaggle.com/kozodoi/timm-pytorch-image-models) to load the timm module which is required for ConvNext.  ","metadata":{}},{"cell_type":"code","source":"# Pytorch implementation of convnext\n# Source: https://github.com/facebookresearch/ConvNeXt/blob/dc7823d8a2ecc554fcd57ff6cdb7748011bcdedd/models/convnext.py\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport sys\nsys.path.append('../input/timm-pytorch-image-models/pytorch-image-models-master')\nfrom timm.models.layers import trunc_normal_, DropPath\nfrom timm.models.registry import register_model\n\nclass Block(nn.Module):\n    r\"\"\" ConvNeXt Block. There are two equivalent implementations:\n    (1) DwConv -> LayerNorm (channels_first) -> 1x1 Conv -> GELU -> 1x1 Conv; all in (N, C, H, W)\n    (2) DwConv -> Permute to (N, H, W, C); LayerNorm (channels_last) -> Linear -> GELU -> Linear; Permute back\n    We use (2) as we find it slightly faster in PyTorch\n    \n    Args:\n        dim (int): Number of input channels.\n        drop_path (float): Stochastic depth rate. Default: 0.0\n        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.\n    \"\"\"\n    def __init__(self, dim, drop_path=0., layer_scale_init_value=1e-6):\n        super().__init__()\n        self.dwconv = nn.Conv2d(dim, dim, kernel_size=7, padding=3, groups=dim) # depthwise conv\n        self.norm = LayerNorm(dim, eps=1e-6)\n        self.pwconv1 = nn.Linear(dim, 4 * dim) # pointwise/1x1 convs, implemented with linear layers\n        self.act = nn.GELU()\n        self.pwconv2 = nn.Linear(4 * dim, dim)\n        self.gamma = nn.Parameter(layer_scale_init_value * torch.ones((dim)), \n                                    requires_grad=True) if layer_scale_init_value > 0 else None\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n\n    def forward(self, x):\n        input = x\n        x = self.dwconv(x)\n        x = x.permute(0, 2, 3, 1) # (N, C, H, W) -> (N, H, W, C)\n        x = self.norm(x)\n        x = self.pwconv1(x)\n        x = self.act(x)\n        x = self.pwconv2(x)\n        if self.gamma is not None:\n            x = self.gamma * x\n        x = x.permute(0, 3, 1, 2) # (N, H, W, C) -> (N, C, H, W)\n\n        x = input + self.drop_path(x)\n        return x\n\nclass ConvNeXt(nn.Module):\n    r\"\"\" ConvNeXt\n        A PyTorch impl of : `A ConvNet for the 2020s`  -\n          https://arxiv.org/pdf/2201.03545.pdf\n    Args:\n        in_chans (int): Number of input image channels. Default: 3\n        num_classes (int): Number of classes for classification head. Default: 1000\n        depths (tuple(int)): Number of blocks at each stage. Default: [3, 3, 9, 3]\n        dims (int): Feature dimension at each stage. Default: [96, 192, 384, 768]\n        drop_path_rate (float): Stochastic depth rate. Default: 0.\n        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.\n        head_init_scale (float): Init scaling value for classifier weights and biases. Default: 1.\n    \"\"\"\n    def __init__(self, in_chans=3, num_classes=1000, \n                 depths=[3, 3, 9, 3], dims=[96, 192, 384, 768], drop_path_rate=0., \n                 layer_scale_init_value=1e-6, head_init_scale=1.,\n                 ):\n        super().__init__()\n\n        self.downsample_layers = nn.ModuleList() # stem and 3 intermediate downsampling conv layers\n        stem = nn.Sequential(\n            nn.Conv2d(in_chans, dims[0], kernel_size=4, stride=4),\n            LayerNorm(dims[0], eps=1e-6, data_format=\"channels_first\")\n        )\n        self.downsample_layers.append(stem)\n        for i in range(3):\n            downsample_layer = nn.Sequential(\n                    LayerNorm(dims[i], eps=1e-6, data_format=\"channels_first\"),\n                    nn.Conv2d(dims[i], dims[i+1], kernel_size=2, stride=2),\n            )\n            self.downsample_layers.append(downsample_layer)\n\n        self.stages = nn.ModuleList() # 4 feature resolution stages, each consisting of multiple residual blocks\n        dp_rates=[x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))] \n        cur = 0\n        for i in range(4):\n            stage = nn.Sequential(\n                *[Block(dim=dims[i], drop_path=dp_rates[cur + j], \n                layer_scale_init_value=layer_scale_init_value) for j in range(depths[i])]\n            )\n            self.stages.append(stage)\n            cur += depths[i]\n\n        self.norm = nn.LayerNorm(dims[-1], eps=1e-6) # final norm layer\n        self.head = nn.Linear(dims[-1], num_classes)\n\n        self.apply(self._init_weights)\n        self.head.weight.data.mul_(head_init_scale)\n        self.head.bias.data.mul_(head_init_scale)\n\n    def _init_weights(self, m):\n        if isinstance(m, (nn.Conv2d, nn.Linear)):\n            trunc_normal_(m.weight, std=.02)\n            nn.init.constant_(m.bias, 0)\n\n    def forward_features(self, x):\n        for i in range(4):\n            x = self.downsample_layers[i](x)\n            x = self.stages[i](x)\n        return self.norm(x.mean([-2, -1])) # global average pooling, (N, C, H, W) -> (N, C)\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.head(x)\n        return x\n\nclass LayerNorm(nn.Module):\n    r\"\"\" LayerNorm that supports two data formats: channels_last (default) or channels_first. \n    The ordering of the dimensions in the inputs. channels_last corresponds to inputs with \n    shape (batch_size, height, width, channels) while channels_first corresponds to inputs \n    with shape (batch_size, channels, height, width).\n    \"\"\"\n    def __init__(self, normalized_shape, eps=1e-6, data_format=\"channels_last\"):\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(normalized_shape))\n        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n        self.eps = eps\n        self.data_format = data_format\n        if self.data_format not in [\"channels_last\", \"channels_first\"]:\n            raise NotImplementedError \n        self.normalized_shape = (normalized_shape, )\n    \n    def forward(self, x):\n        if self.data_format == \"channels_last\":\n            return F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n        elif self.data_format == \"channels_first\":\n            u = x.mean(1, keepdim=True)\n            s = (x - u).pow(2).mean(1, keepdim=True)\n            x = (x - u) / torch.sqrt(s + self.eps)\n            x = self.weight[:, None, None] * x + self.bias[:, None, None]\n            return x\n\n\nmodel_urls = {\n    \"convnext_tiny_1k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_tiny_1k_224_ema.pth\",\n    \"convnext_small_1k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_small_1k_224_ema.pth\",\n    \"convnext_base_1k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_base_1k_224_ema.pth\",\n    \"convnext_large_1k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_large_1k_224_ema.pth\",\n    \"convnext_base_22k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_base_22k_224.pth\",\n    \"convnext_large_22k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_large_22k_224.pth\",\n    \"convnext_xlarge_22k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_xlarge_22k_224.pth\",\n}\n\n@register_model\ndef convnext_tiny(pretrained=False, **kwargs):\n    model = ConvNeXt(depths=[3, 3, 9, 3], dims=[96, 192, 384, 768], **kwargs)\n    if pretrained:\n        url = model_urls['convnext_tiny_1k']\n        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location=\"cpu\", check_hash=True)\n        model.load_state_dict(checkpoint[\"model\"])\n    return model\n\n@register_model\ndef convnext_small(pretrained=False, **kwargs):\n    model = ConvNeXt(depths=[3, 3, 27, 3], dims=[96, 192, 384, 768], **kwargs)\n    if pretrained:\n        url = model_urls['convnext_small_1k']\n        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location=\"cpu\", check_hash=True)\n        model.load_state_dict(checkpoint[\"model\"])\n    return model\n\n@register_model\ndef convnext_base(pretrained=False, in_22k=False, **kwargs):\n    model = ConvNeXt(depths=[3, 3, 27, 3], dims=[128, 256, 512, 1024], **kwargs)\n    if pretrained:\n        url = model_urls['convnext_base_22k'] if in_22k else model_urls['convnext_base_1k']\n        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location=\"cpu\", check_hash=True)\n        model.load_state_dict(checkpoint[\"model\"])\n    return model\n\n@register_model\ndef convnext_large(pretrained=False, in_22k=False, **kwargs):\n    model = ConvNeXt(depths=[3, 3, 27, 3], dims=[192, 384, 768, 1536], **kwargs)\n    if pretrained:\n        url = model_urls['convnext_large_22k'] if in_22k else model_urls['convnext_large_1k']\n        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location=\"cpu\", check_hash=True)\n        model.load_state_dict(checkpoint[\"model\"])\n    return model\n\n@register_model\ndef convnext_xlarge(pretrained=False, in_22k=False, **kwargs):\n    model = ConvNeXt(depths=[3, 3, 27, 3], dims=[256, 512, 1024, 2048], **kwargs)\n    if pretrained:\n        url = model_urls['convnext_xlarge_22k'] if in_22k else model_urls['convnext_xlarge_1k']\n        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location=\"cpu\", check_hash=True)\n        model.load_state_dict(checkpoint[\"model\"])\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-01-14T01:42:59.605097Z","iopub.execute_input":"2022-01-14T01:42:59.605517Z","iopub.status.idle":"2022-01-14T01:43:01.282448Z","shell.execute_reply.started":"2022-01-14T01:42:59.605484Z","shell.execute_reply":"2022-01-14T01:43:01.281595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Load the model, replace the output layer, and choose which layers to freeze/train**\n\nI'm replacing the final fully connected layer of the convnext model with my own feed forward network, and also unfreezing the last stage of convnext to allow weights there to be trained.  Currently I'm not using the annotations in the model at all. ","metadata":{}},{"cell_type":"code","source":"# Load the pretrained resnet50 from a file\n\ndef load_model(path):\n\n    model = torch.load(path)\n\n    # Disable gradients on all model parameters to freeze the weights\n    for param in model.parameters():\n        param.requires_grad = False\n\n    # Replace the final fully connected resnet layer with a 2 fc layer network and sigmoid output\n    # Also use the annotations\n    model.head = nn.Sequential(nn.Linear(1024, 256),\n                             nn.ReLU(),\n                             nn.Linear(256, 1),\n                             nn.Sigmoid())\n\n\n\n    for param in model.head.parameters():\n        param.requires_grad = True\n\n    # Unfreeze the last stage\n    for param in model.stages[3].parameters():\n        param.requires_grad = True\n    \n    return model\n\nmodel = load_model(f'{model_dir}convnext_base_pretrained_v2.pt')","metadata":{"gradient":{"execution_count":449,"id":"c4a436fe","kernelId":"e054bc54-048d-46c0-933a-55d23cc13c60"},"execution":{"iopub.status.busy":"2022-01-14T01:43:01.284895Z","iopub.execute_input":"2022-01-14T01:43:01.285652Z","iopub.status.idle":"2022-01-14T01:43:04.296021Z","shell.execute_reply.started":"2022-01-14T01:43:01.285588Z","shell.execute_reply":"2022-01-14T01:43:04.295256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(model)","metadata":{"execution":{"iopub.status.busy":"2022-01-14T01:43:04.299265Z","iopub.execute_input":"2022-01-14T01:43:04.29947Z","iopub.status.idle":"2022-01-14T01:43:04.309019Z","shell.execute_reply.started":"2022-01-14T01:43:04.299445Z","shell.execute_reply":"2022-01-14T01:43:04.308238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def initialize_optimizer(starting_lr, lambd):\n    \n    # Loss function using MSE (the goal)\n    criterion = nn.MSELoss(reduction='sum')\n\n    #Adam with L2 regularization\n    optimizer = optim.AdamW(model.parameters(), lr=starting_lr, weight_decay=lambd)\n\n    # Learning rate decay\n    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones = [1, 2, 6], gamma=0.5)\n    \n    return criterion, optimizer, scheduler\n\ncriterion, optimizer, scheduler = initialize_optimizer(0.000025, 100)","metadata":{"gradient":{"execution_count":450,"id":"91b0e833","kernelId":"e054bc54-048d-46c0-933a-55d23cc13c60"},"execution":{"iopub.status.busy":"2022-01-14T01:43:04.310146Z","iopub.execute_input":"2022-01-14T01:43:04.310751Z","iopub.status.idle":"2022-01-14T01:43:04.320563Z","shell.execute_reply.started":"2022-01-14T01:43:04.3107Z","shell.execute_reply":"2022-01-14T01:43:04.319837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test out the forward pass on a single batch\n\nimages, annotations, scores = next(iter(cv_dataloaders[0][1]))\nwith torch.no_grad():\n    train_loss = 0.0\n    output = model(images)*100 # convert sigmoid output to pawpularity scale\n    loss = criterion(output, scores)\n    RMSE = math.sqrt(loss.item()/global_batch_size)\n\nprint(scores.dtype)\nprint(output.dtype)\nprint('Starting Prediction: ', torch.mean(output))\nprint('Starting RMSE: ', RMSE)\nprint('Prediction Standard Deviation: ', torch.std(output))","metadata":{"execution":{"iopub.status.busy":"2022-01-14T01:43:04.321779Z","iopub.execute_input":"2022-01-14T01:43:04.322202Z","iopub.status.idle":"2022-01-14T01:43:37.412215Z","shell.execute_reply.started":"2022-01-14T01:43:04.322165Z","shell.execute_reply":"2022-01-14T01:43:37.41147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train the model","metadata":{}},{"cell_type":"markdown","source":"**Model training loop**\n\nRun the training and validation steps for a fixed number of epochs, and save the model anytime the validation loss decreases.  ","metadata":{}},{"cell_type":"code","source":"# Training and validation loop\n\ndef train_validation_loop(fold, model, train_loader, valid_loader, train_on_gpu, n_epochs=6):\n\n    if train_on_gpu:\n        model.cuda()\n\n    valid_loss_min = np.Inf # track change in validation loss\n\n    train_losses, valid_losses = [], []\n\n    for epoch in range(1, n_epochs+10):\n\n        start = time.time()\n        current_lr = scheduler.get_last_lr()[0]\n\n        # keep track of training and validation loss\n        train_loss = 0.0\n        valid_loss = 0.0\n\n        # Stop training the convolutional layers after a certain point\n        #if epoch > 4:\n        #    for param in model.layer3.parameters():\n        #        param.requires_grad = False\n        \n        #if epoch > 5:\n        #    for param in model.layer4.parameters():\n        #        param.requires_grad = False\n\n        ###################\n        # train the model #\n        ###################\n        # put in training mode (enable dropout)\n        model.train()\n        for images, annotations, scores in train_loader:\n            # move tensors to GPU if CUDA is available\n            if train_on_gpu:\n                images, annotations, scores = images.cuda(), annotations.cuda(), scores.cuda()\n            # clear the gradients of all optimized variables\n            optimizer.zero_grad()\n            # forward pass: compute predicted outputs by passing inputs to the model\n            output = model(images)*100 # multiply by 100 the sigmoid output to 0-100 pawpularity scale\n            # print(output.dtype)\n            # print(scores.dtype)\n            # calculate the batch loss\n            loss = criterion(output, scores)\n            # backward pass: compute gradient of the loss with respect to model parameters\n            loss.backward()\n            # perform a single optimization step (parameter update)\n            optimizer.step()\n            # update training loss\n            train_loss += loss.item()\n\n        ######################    \n        # validate the model #\n        ######################\n        # eval mode (no dropout)\n        model.eval()\n        with torch.no_grad():\n            for images, annotations, scores in valid_loader:\n                # move tensors to GPU if CUDA is available\n                if train_on_gpu:\n                    images, annotations, scores = images.cuda(), annotations.cuda(), scores.cuda()\n                # forward pass: compute predicted outputs by passing inputs to the model\n                output = model(images)*100 # multiply by 100 the sigmoid output to 0-100 pawpularity scale\n                # calculate the batch loss\n                loss = criterion(output, scores)\n                # update average validation loss \n                valid_loss += loss.item()\n\n        # calculate RMSE\n        train_loss = math.sqrt(train_loss/len(train_loader.sampler))\n        valid_loss = math.sqrt(valid_loss/len(valid_loader.sampler))\n\n        train_losses.append(train_loss)\n        valid_losses.append(valid_loss)\n\n        # increment learning rate decay\n        scheduler.step()\n\n        # print training/validation statistics \n        # print(f'Epoch: {e}, {float(time.time() - start):.3f} seconds, lr={optimizer.lr}')\n        print('Epoch: {}, time: {:.1f}s, lr: {:.7f} \\tTraining Loss: {:.3f} \\tValidation Loss: {:.3f}'.format(\n            epoch, float(time.time() - start), current_lr, train_loss, valid_loss))\n\n        # save model if validation loss has decreased\n        if valid_loss <= valid_loss_min:\n            print('Validation loss decreased ({:.3f} --> {:.3f}).  Saving model ...'.format(\n            valid_loss_min,\n            valid_loss))\n            model_name = f'{working_dir}pawpularity_best_model_fold{fold}.pt'\n            torch.save(model.state_dict(), model_name)\n            valid_loss_min = valid_loss\n        \n        # Stop early if the min epochs is satisfied and score isn't improving\n        if valid_loss > valid_loss_min+0.04 and epoch >= n_epochs:\n            break\n    \n    # Plot the losses\n    fig = plt.figure()\n    ax = plt.axes()\n    ax.plot(list(range(0, len(train_losses))), train_losses[0:])\n    ax.plot(list(range(0, len(valid_losses))), valid_losses[0:]);\n    print(f'best score: {valid_loss_min}')\n        \n    return (model_name, valid_loss_min)","metadata":{"gradient":{"execution_count":478,"id":"597dbdfb","kernelId":"e054bc54-048d-46c0-933a-55d23cc13c60"},"scrolled":true,"execution":{"iopub.status.busy":"2022-01-14T01:43:37.413756Z","iopub.execute_input":"2022-01-14T01:43:37.414027Z","iopub.status.idle":"2022-01-14T01:43:37.430465Z","shell.execute_reply.started":"2022-01-14T01:43:37.413989Z","shell.execute_reply":"2022-01-14T01:43:37.429712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check if CUDA is available and set the training device\n\ntrain_on_gpu = torch.cuda.is_available()\ndevice = torch.cuda.get_device_name()\n\nif not train_on_gpu:\n    print('CUDA is not available.  Training on CPU ...')\nelse:\n    print(f'CUDA is available!  Training on GPU {device}...')","metadata":{"gradient":{"execution_count":453,"id":"edf14100","kernelId":"e054bc54-048d-46c0-933a-55d23cc13c60"},"execution":{"iopub.status.busy":"2022-01-14T01:43:37.431849Z","iopub.execute_input":"2022-01-14T01:43:37.432245Z","iopub.status.idle":"2022-01-14T01:43:37.449339Z","shell.execute_reply.started":"2022-01-14T01:43:37.432202Z","shell.execute_reply":"2022-01-14T01:43:37.448486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Skip the training step in this notebook and use model files already uploaded**\n\nThis makes submission much faster.  ","metadata":{}},{"cell_type":"code","source":"# Train for all of the cv folds\n\ncv_folds = 10\nepochs = 7\nstarting_lr = 0.00005\nlambd = 7 # Regularization \n\ncv_dataloaders = get_cv_dataloaders(augmented_data=augmented_data, \n                                    base_transform_data=base_transform_data,\n                                    folds=cv_folds, \n                                    cv_shuffle=True)\n\n# Skip training in the submission notebook\n'''\nsaved_models = []\n\nfor i, (train_loader, valid_loader) in enumerate(cv_dataloaders):\n    print('Starting Fold', i)\n    \n    # Reset the model and schedulers for each new dataset\n    model = load_model(f'{model_dir}convnext_base_pretrained.pt')\n    criterion, optimizer, scheduler = initialize_optimizer(starting_lr=starting_lr, lambd=lambd)\n    \n    # Run the training loop and add the best model's filepath\n    saved_models.append(train_validation_loop(i, model, train_loader, valid_loader, train_on_gpu, n_epochs=epochs))\n    \n    #if i >= 2:\n    #   break\n    print('\\n')\n'''","metadata":{"execution":{"iopub.status.busy":"2022-01-14T01:43:37.450513Z","iopub.execute_input":"2022-01-14T01:43:37.452776Z","iopub.status.idle":"2022-01-14T01:43:37.467683Z","shell.execute_reply.started":"2022-01-14T01:43:37.452734Z","shell.execute_reply":"2022-01-14T01:43:37.467013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Diagnostics and performance","metadata":{}},{"cell_type":"code","source":"# Load the best performing model from the first fold\nmodel = load_model(f'{model_dir}convnext_base_pretrained_v2.pt')\nmodel.load_state_dict(torch.load(f'{weights_dir}pawpularity_best_model_fold4.pt'))\n\nif train_on_gpu:\n    model.cuda()","metadata":{"execution":{"iopub.status.busy":"2022-01-14T01:43:37.469178Z","iopub.execute_input":"2022-01-14T01:43:37.469417Z","iopub.status.idle":"2022-01-14T01:43:40.600421Z","shell.execute_reply.started":"2022-01-14T01:43:37.469383Z","shell.execute_reply":"2022-01-14T01:43:40.599693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get the distribution of predictions\n\npredictions = []\nscore_list = []\n\nmodel.eval()\nwith torch.no_grad():\n    for images, annotations, scores in cv_dataloaders[4][1]: # fold 4, validation dataset\n        # move tensors to GPU if CUDA is available\n        if train_on_gpu:\n            images, annotations, scores = images.cuda(), annotations.cuda(), scores.cuda()\n        # forward pass: compute predicted outputs by passing inputs to the model\n        output = model(images)*100\n        predictions.extend(list(output.cpu().detach().numpy().reshape(len(output),)))\n        score_list.extend(list(scores.cpu().detach().numpy().reshape(len(scores),)))\n        \n\npreds_df = pd.DataFrame({'preds': predictions})\npreds_df.describe()","metadata":{"execution":{"iopub.status.busy":"2022-01-14T01:43:40.601803Z","iopub.execute_input":"2022-01-14T01:43:40.602054Z","iopub.status.idle":"2022-01-14T01:44:01.779376Z","shell.execute_reply.started":"2022-01-14T01:43:40.602021Z","shell.execute_reply":"2022-01-14T01:44:01.778584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Manually Check RMSE\n\ndiffs = np.array(score_list) - np.array(predictions)\nprint(math.sqrt((diffs @ diffs)/len(cv_dataloaders[4][1].sampler)))","metadata":{"execution":{"iopub.status.busy":"2022-01-14T01:44:01.785548Z","iopub.execute_input":"2022-01-14T01:44:01.785775Z","iopub.status.idle":"2022-01-14T01:44:01.791173Z","shell.execute_reply.started":"2022-01-14T01:44:01.785743Z","shell.execute_reply":"2022-01-14T01:44:01.790318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Histogram of validation predictions - if this is too narrow that's an issue\n\nn, bins, patches = plt.hist(predictions, 50, density=True, facecolor='g', alpha=0.75)\n\nplt.xlabel('Pawpularity')\nplt.ylabel('Frequency')\nplt.title('Predicted Pawpularity Histogram')\nplt.xlim(0, 100)\nplt.ylim(0, .2)\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-14T01:44:01.792606Z","iopub.execute_input":"2022-01-14T01:44:01.7931Z","iopub.status.idle":"2022-01-14T01:44:02.119429Z","shell.execute_reply.started":"2022-01-14T01:44:01.793065Z","shell.execute_reply":"2022-01-14T01:44:02.118786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Show examples of images and predicted vs. actual scores","metadata":{}},{"cell_type":"code","source":"output_plot = model(images).cpu()*100\nimages, annotations, scores = images.cpu(), annotations.cpu(), scores.cpu()","metadata":{"execution":{"iopub.status.busy":"2022-01-14T01:44:02.120821Z","iopub.execute_input":"2022-01-14T01:44:02.121233Z","iopub.status.idle":"2022-01-14T01:44:02.358099Z","shell.execute_reply.started":"2022-01-14T01:44:02.121196Z","shell.execute_reply":"2022-01-14T01:44:02.357352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot the images in the batch, along with the corresponding labels and predictions\n\nfig = plt.figure(figsize=(20, 10))\n# display 20 images\nfor idx in np.arange(12):\n    ax = fig.add_subplot(3, 4, idx+1, xticks=[], yticks=[])\n    im_convert(images[idx])\n    ax.set_title(f'Act: {round(scores[idx].item())} Pred: {round(output_plot[idx].item())}')","metadata":{"execution":{"iopub.status.busy":"2022-01-14T01:44:02.359499Z","iopub.execute_input":"2022-01-14T01:44:02.359786Z","iopub.status.idle":"2022-01-14T01:44:03.895113Z","shell.execute_reply.started":"2022-01-14T01:44:02.359748Z","shell.execute_reply":"2022-01-14T01:44:03.894292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Use the model to predict the test dataset\n\nDo inference on the best model from each fold, and then take the prediction for each value in the test set.  ","metadata":{}},{"cell_type":"code","source":"test_df = pd.read_csv(f'{data_dir}test.csv')\ntest_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-01-14T01:44:03.896509Z","iopub.execute_input":"2022-01-14T01:44:03.896773Z","iopub.status.idle":"2022-01-14T01:44:03.923529Z","shell.execute_reply.started":"2022-01-14T01:44:03.896739Z","shell.execute_reply":"2022-01-14T01:44:03.922931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PawpularityTestDataset(Dataset):\n    \"\"\"Dataset connecting dog images to the score and annotations\"\"\"\n\n    def __init__(self, csv_file, img_dir, transform=transforms.ToTensor()):\n        \"\"\"\n        Args:\n            csv_file (string): Path to the csv file with annotations.\n            img_dir (string): Directory with all the images.\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        \"\"\"\n\n        self.annotations_csv = pd.read_csv(csv_file)\n        self.img_dir = img_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.annotations_csv)\n\n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n\n        img_name = os.path.join(self.img_dir,\n                                self.annotations_csv.iloc[idx, 0])\n\n        # load each image in PIL format for compatibility with transforms\n        image = PIL.Image.open(img_name + '.jpg')\n\n        annotations = np.array(self.annotations_csv.iloc[idx, 1:13])\n        annotations = annotations.astype('float')\n\n        # Apply the transforms\n        image = self.transform(image)\n\n        sample = [image, annotations]\n        return sample","metadata":{"execution":{"iopub.status.busy":"2022-01-14T01:44:03.924717Z","iopub.execute_input":"2022-01-14T01:44:03.925403Z","iopub.status.idle":"2022-01-14T01:44:03.936134Z","shell.execute_reply.started":"2022-01-14T01:44:03.925366Z","shell.execute_reply":"2022-01-14T01:44:03.935187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Load the test dataset (careful to use validation transforms without img augmentation)\n\ntest_data = PawpularityTestDataset(f'{data_dir}test.csv', f'{data_dir}test', transform=img_transforms_valid)\n\nbatch_size = min(len(test_data), global_batch_size)\n\ntest_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=workers) ","metadata":{"execution":{"iopub.status.busy":"2022-01-14T01:44:03.938502Z","iopub.execute_input":"2022-01-14T01:44:03.938765Z","iopub.status.idle":"2022-01-14T01:44:03.953701Z","shell.execute_reply.started":"2022-01-14T01:44:03.938719Z","shell.execute_reply":"2022-01-14T01:44:03.952905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step through with a reasonable batch size and build up the output dataset\n\n\ndef test_predictions(model, test_loader, model_path, test_df):\n    \n    # Load a set of trained weights into the model\n    model.load_state_dict(torch.load(model_path))\n    \n    # Move to gpu if necessary\n    if train_on_gpu:\n        model.cuda()\n    \n    # Put in evaluation mode\n    model.eval()\n    \n    # Run predictions for each batch in the test dataset\n    preds = []\n    for images, annotations in test_loader:\n        # move tensors to GPU if CUDA is available\n        if train_on_gpu:\n            images, annotations = images.cuda(), annotations.cuda()\n        # get predictions\n        test_pred = model(images)*100\n        # add predictions from the current batch\n        preds.extend(list(test_pred.cpu().detach().numpy().reshape(len(test_pred),)))\n\n    # Get the list of image filenames from the test annotations file\n    img_names = np.array(test_df.iloc[:, 0].values)\n    \n    # Round the outputs\n    preds = [round(x, 2) for x in preds]\n    \n    # return a tuple with the image names and model predictions\n    return (img_names, np.array(preds))","metadata":{"execution":{"iopub.status.busy":"2022-01-14T01:44:03.957887Z","iopub.execute_input":"2022-01-14T01:44:03.958141Z","iopub.status.idle":"2022-01-14T01:44:03.967247Z","shell.execute_reply.started":"2022-01-14T01:44:03.958106Z","shell.execute_reply":"2022-01-14T01:44:03.966427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Using pre uploaded saved models')\nmodel_list = [(f'{weights_dir}pawpularity_best_model_fold0.pt', 17.75550977922519),\n              (f'{weights_dir}pawpularity_best_model_fold1.pt', 17.055042532540423),\n              (f'{weights_dir}pawpularity_best_model_fold2.pt', 17.723917203277402),\n              (f'{weights_dir}pawpularity_best_model_fold3.pt', 17.506054498891437),\n              (f'{weights_dir}pawpularity_best_model_fold4.pt', 16.693078011850073),\n              (f'{weights_dir}pawpularity_best_model_fold5.pt', 18.116734056509916),\n              (f'{weights_dir}pawpularity_best_model_fold6.pt', 17.94325075614874),\n              (f'{weights_dir}pawpularity_best_model_fold7.pt', 17.84976622654215),\n              (f'{weights_dir}pawpularity_best_model_fold8.pt', 17.56859209356934),\n              (f'{weights_dir}pawpularity_best_model_fold9.pt', 18.29225659309586)]\nmodel_list","metadata":{"execution":{"iopub.status.busy":"2022-01-14T01:45:00.570307Z","iopub.execute_input":"2022-01-14T01:45:00.570571Z","iopub.status.idle":"2022-01-14T01:45:00.582291Z","shell.execute_reply.started":"2022-01-14T01:45:00.570541Z","shell.execute_reply":"2022-01-14T01:45:00.581492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set up the model structure\nmodel = load_model(f'{model_dir}convnext_base_pretrained_v2.pt')\n\n# Iterate through saved models and calculate predictions for each one on the test data\noutputs = []\nfor best_model, rmse in model_list:\n    img_names, preds = test_predictions(model, test_loader, best_model, test_df)\n    outputs.append(preds)\n    \n# Get image names\nimg_list = img_names\n\n# Take the average of the outputs from each model's preds\nmean_outputs = np.mean(np.stack(outputs, axis=1), axis=1)\n\n# Join into a dataset\noutput_df = pd.DataFrame({'Id': img_list,'Pawpularity' : mean_outputs})\noutput_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-01-14T01:45:02.043424Z","iopub.execute_input":"2022-01-14T01:45:02.044164Z","iopub.status.idle":"2022-01-14T01:45:29.966335Z","shell.execute_reply.started":"2022-01-14T01:45:02.044127Z","shell.execute_reply":"2022-01-14T01:45:29.965573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Write the output in the required format\noutput_df.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-14T01:45:29.968292Z","iopub.execute_input":"2022-01-14T01:45:29.968757Z","iopub.status.idle":"2022-01-14T01:45:29.976619Z","shell.execute_reply.started":"2022-01-14T01:45:29.968701Z","shell.execute_reply":"2022-01-14T01:45:29.975734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}