{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\ndata = pd.read_csv(\"/kaggle/input/petfinder-pawpularity-score/train.csv\", sep=',')\ndata\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-16T12:43:21.276692Z","iopub.execute_input":"2021-12-16T12:43:21.277009Z","iopub.status.idle":"2021-12-16T12:43:21.326619Z","shell.execute_reply.started":"2021-12-16T12:43:21.276972Z","shell.execute_reply":"2021-12-16T12:43:21.325327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['Id'] = data['Id'] + '.jpg'\ndata = data.rename(columns={'Id': 'filename'})\ndata","metadata":{"execution":{"iopub.status.busy":"2021-12-16T12:43:24.172864Z","iopub.execute_input":"2021-12-16T12:43:24.173111Z","iopub.status.idle":"2021-12-16T12:43:24.193519Z","shell.execute_reply.started":"2021-12-16T12:43:24.173083Z","shell.execute_reply":"2021-12-16T12:43:24.192739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Je découpe en train / val / test","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain, val = train_test_split(data, test_size=0.2, random_state=1)  # val 20%\ntrain, test = train_test_split(train, test_size=0.01, random_state=1)  # test 1%\nprint(\"shape train: \", train.shape)\nprint(\"shape val: \", val.shape)\nprint(\"shape test: \", test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-12-16T12:43:27.679698Z","iopub.execute_input":"2021-12-16T12:43:27.680434Z","iopub.status.idle":"2021-12-16T12:43:28.674562Z","shell.execute_reply.started":"2021-12-16T12:43:27.680383Z","shell.execute_reply":"2021-12-16T12:43:28.672211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_dir = '/kaggle/input/petfinder-pawpularity-score/train'\nwidth, height = 512, 512\nbatch_size = 32","metadata":{"execution":{"iopub.status.busy":"2021-12-16T12:43:30.034574Z","iopub.execute_input":"2021-12-16T12:43:30.035289Z","iopub.status.idle":"2021-12-16T12:43:30.039861Z","shell.execute_reply.started":"2021-12-16T12:43:30.035242Z","shell.execute_reply":"2021-12-16T12:43:30.0392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Par curiosité, je regarde les dimensions des images du dataset","metadata":{}},{"cell_type":"code","source":"import os\nfrom PIL import Image\nimport numpy as np\nwidths, heights = [], []\nfor image_name in os.listdir(dataset_dir):\n    image_path = os.path.join(dataset_dir, image_name)\n    img = Image.open(image_path)\n    img_width, img_height = img.size\n    widths.append(img_width)\n    heights.append(img_height)\nprint(\"max width:\", max(widths), \"\\nmax height:\", max(heights),\n      \"\\nmin width:\", min(widths), \"\\nmin height:\", min(heights),\n      \"\\nmean width:\", np.array(widths).mean(),\n      \"\\nmean height:\", np.array(heights).mean())","metadata":{"execution":{"iopub.status.busy":"2021-12-16T12:48:46.64508Z","iopub.execute_input":"2021-12-16T12:48:46.645622Z","iopub.status.idle":"2021-12-16T12:48:51.12761Z","shell.execute_reply.started":"2021-12-16T12:48:46.645586Z","shell.execute_reply":"2021-12-16T12:48:51.126849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\ntrain_generator = ImageDataGenerator(\n    rescale=1.0 / 255,\n    rotation_range=5,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    brightness_range=(0.75, 1),\n    shear_range=0.1,\n    zoom_range=[0.75, 1],\n    horizontal_flip=True,\n    validation_split=0.2\n)\nvalidation_generator = ImageDataGenerator(\n    rescale=1.0 / 255\n)\ntest_generator = ImageDataGenerator(\n    rescale=1.0 / 255\n)","metadata":{"execution":{"iopub.status.busy":"2021-12-16T12:50:11.766554Z","iopub.execute_input":"2021-12-16T12:50:11.766812Z","iopub.status.idle":"2021-12-16T12:50:16.270335Z","shell.execute_reply.started":"2021-12-16T12:50:11.766783Z","shell.execute_reply":"2021-12-16T12:50:16.269574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nseries = data.iloc[2]\ndata_augmentation_viz = pd.concat([series, series], axis=1).transpose()\niterator_visualizations = train_generator.flow_from_dataframe(\n    dataframe=data_augmentation_viz,\n    directory=dataset_dir,\n    x_col=\"filename\",\n    y_col=\"Pawpularity\",\n    class_mode=\"raw\",\n    target_size=(width, height),\n    batch_size=1,  # 1 seule image pour vérifier\n)\nplt.figure(figsize=(10, 12))\nfor i in range(9):\n    ax = plt.subplot(3, 3, i + 1)  # 3x3 grid\n    batch = next(iterator_visualizations)  # toujours la même image\n    img = batch[0]\n    img = img[0, :, :, :]\n    plt.imshow(img)\nplt.show()\nplt.close()","metadata":{"execution":{"iopub.status.busy":"2021-12-16T12:50:16.271868Z","iopub.execute_input":"2021-12-16T12:50:16.272088Z","iopub.status.idle":"2021-12-16T12:50:17.535506Z","shell.execute_reply.started":"2021-12-16T12:50:16.272057Z","shell.execute_reply":"2021-12-16T12:50:17.534568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_generator = train_generator.flow_from_dataframe(\n    dataframe=train,\n    directory=dataset_dir,\n    x_col=\"filename\",\n    y_col=\"Pawpularity\",\n    class_mode=\"raw\",  # \"raw\" pour les regressions\n    target_size=(width, height),\n    batch_size=batch_size\n)\nvalidation_generator = validation_generator.flow_from_dataframe(\n    dataframe=val,\n    directory=dataset_dir,\n    x_col=\"filename\",\n    y_col=\"Pawpularity\",\n    class_mode=\"raw\",\n    target_size=(width, height),\n    batch_size=batch_size\n)\ntest_generator = test_generator.flow_from_dataframe(\n    dataframe=test,\n    directory=dataset_dir,\n    x_col=\"filename\",\n    y_col=\"Pawpularity\",\n    class_mode=\"raw\",\n    target_size=(width, height),\n    batch_size=batch_size\n)","metadata":{"execution":{"iopub.status.busy":"2021-12-16T12:50:18.961562Z","iopub.execute_input":"2021-12-16T12:50:18.961815Z","iopub.status.idle":"2021-12-16T12:50:22.006561Z","shell.execute_reply.started":"2021-12-16T12:50:18.961787Z","shell.execute_reply":"2021-12-16T12:50:22.005828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nearly_stopping_callback = EarlyStopping(\n    monitor=\"val_mean_absolute_percentage_error\",\n    min_delta=1,  # sous les 1% de mieux, on patiente\n    patience=10,  # on patiente max 10 epochs\n    verbose=2,\n    mode=\"min\",\n    restore_best_weights=True\n)\nmodel_checkpoint_callback = ModelCheckpoint(\n    'efficientNet_reg.h5',\n    monitor=\"val_mean_absolute_percentage_error\",\n    verbose=0,\n    save_best_only=True,\n    mode=\"min\",\n    save_freq=\"epoch\"\n)\ncallbacks = [early_stopping_callback, model_checkpoint_callback]","metadata":{"execution":{"iopub.status.busy":"2021-12-16T12:50:23.544894Z","iopub.execute_input":"2021-12-16T12:50:23.545466Z","iopub.status.idle":"2021-12-16T12:50:23.551861Z","shell.execute_reply.started":"2021-12-16T12:50:23.545427Z","shell.execute_reply":"2021-12-16T12:50:23.551177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Le notebook doit fonctionner sans accès internet","metadata":{}},{"cell_type":"code","source":"import sys\nimport os\nsys.path.insert(0, \"/kaggle/input/efnetv2src/efficientnet-v2-keras-main\")\nsys.path.append('../input/tfkeras-efficientnetsv2/')\nfrom efficientnet_v2 import EfficientNetV2XL","metadata":{"execution":{"iopub.status.busy":"2021-12-16T12:52:26.210649Z","iopub.execute_input":"2021-12-16T12:52:26.210897Z","iopub.status.idle":"2021-12-16T12:52:26.272727Z","shell.execute_reply.started":"2021-12-16T12:52:26.210868Z","shell.execute_reply":"2021-12-16T12:52:26.272062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import Input, Model\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, BatchNormalization, Dropout\nfrom tensorflow.keras.metrics import MeanAbsoluteError, MeanAbsolutePercentageError\nimport tensorflow_addons as tfa\n\nefficientnet = EfficientNetV2XL(\n    include_top=False,\n    weights='../input/tfkeras-efficientnetsv2/21_ft1k_notop/efficientnetv2-xl-21k-ft1k_notop.h5', \n    input_shape=(height, width, 3)\n)\n\nfor layer in efficientnet.layers:\n        layer.trainable = False\n\nx = GlobalAveragePooling2D(name=\"avg_pool\")(efficientnet.output)\nx = BatchNormalization()(x)\nx = Dropout(0.2, name=\"top_dropout\")(x)\noutputs = Dense(1, name=\"pred\")(x)\n\nefficientnet = Model(inputs=efficientnet.inputs, outputs=outputs)\n\nradam = tfa.optimizers.RectifiedAdam(learning_rate=0.001)\noptimizer = tfa.optimizers.Lookahead(radam, sync_period=6, slow_step_size=0.5)\n\nefficientnet.compile(\n    optimizer=optimizer,\n    loss=\"mean_absolute_error\",\n    metrics=[MeanAbsoluteError(), MeanAbsolutePercentageError()]\n)","metadata":{"execution":{"iopub.status.busy":"2021-12-16T12:52:35.191655Z","iopub.execute_input":"2021-12-16T12:52:35.191909Z","iopub.status.idle":"2021-12-16T12:52:55.838589Z","shell.execute_reply.started":"2021-12-16T12:52:35.191881Z","shell.execute_reply":"2021-12-16T12:52:55.837895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 100\nbatch_size = 8\nhistory_efficientnet = efficientnet.fit(\n    train_generator,\n    epochs=epochs,\n    validation_data=validation_generator,\n    callbacks=callbacks,\n    workers=6  # guess\n)","metadata":{"execution":{"iopub.status.busy":"2021-12-16T12:53:53.954199Z","iopub.execute_input":"2021-12-16T12:53:53.954846Z","iopub.status.idle":"2021-12-16T13:36:29.694451Z","shell.execute_reply.started":"2021-12-16T12:53:53.954811Z","shell.execute_reply":"2021-12-16T13:36:29.693628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[\"Pawpularity\"].mean()","metadata":{"execution":{"iopub.status.busy":"2021-12-16T13:36:29.696058Z","iopub.execute_input":"2021-12-16T13:36:29.696653Z","iopub.status.idle":"2021-12-16T13:36:29.704724Z","shell.execute_reply.started":"2021-12-16T13:36:29.696612Z","shell.execute_reply":"2021-12-16T13:36:29.704056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean_baseline = MeanAbsolutePercentageError()\nmean_baseline = mean_baseline(\n    val[\"Pawpularity\"], train[\"Pawpularity\"].mean()\n).numpy()","metadata":{"execution":{"iopub.status.busy":"2021-12-16T13:36:29.705917Z","iopub.execute_input":"2021-12-16T13:36:29.706354Z","iopub.status.idle":"2021-12-16T13:36:29.729253Z","shell.execute_reply.started":"2021-12-16T13:36:29.706319Z","shell.execute_reply":"2021-12-16T13:36:29.728491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dict1 = {\n    \"MAPE\": history_efficientnet.history[\"mean_absolute_percentage_error\"],\n    \"type\": \"training\"\n}\ndict2 = {\n    \"MAPE\": history_efficientnet.history[\"val_mean_absolute_percentage_error\"],\n    \"type\": \"validation\"\n}\ns1 = pd.DataFrame(dict1)\ns2 = pd.DataFrame(dict2)\ndf = pd.concat([s1, s2], axis=0).reset_index()\nimport seaborn as sns\ngrid = sns.relplot(\n    data=df,\n    x=df[\"index\"],\n    y=\"MAPE\",\n    col=\"type\",\n    kind=\"line\",\n    legend=False\n)\ngrid.set(ylim=(20, 100))\nfor ax in grid.axes.flat:\n    ax.axhline(\n        y=mean_baseline, color=\"lightcoral\", linestyle=\"dashed\"\n    )\n    ax.set(xlabel=\"Epoch\")\nplt.legend(labels=[\"efficientNet_reg\", \"mean_baseline\"])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-16T13:36:43.43211Z","iopub.execute_input":"2021-12-16T13:36:43.432602Z","iopub.status.idle":"2021-12-16T13:36:44.063039Z","shell.execute_reply.started":"2021-12-16T13:36:43.432566Z","shell.execute_reply":"2021-12-16T13:36:44.062338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\ndef preprocess(image):  \n    return (tf.cast(image, dtype=tf.float32) - 128.00) / 128.00","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nplt.figure(figsize=(10, 12))\nbatch = next(test_generator)  # renvoie 32 images\nfor i in range(12):\n    ax = plt.subplot(4, 3, i + 1)  # 4x3 grid\n    image = batch[0][i, :, :, :]  # i-ème image\n    img = preprocess(image.reshape(1, width, height, 3))\n    pawpularity = batch[1][i]  # i-ème pawpularity\n    preds = efficientnet.predict(img)\n    prediction = preds.flatten()[0]\n    print('pawpularity=', pawpularity, '\\tprediction=', round(prediction, 2))\n    diff = prediction - pawpularity\n    percentDiff = (diff / pawpularity) * 100\n    absPercentDiff = np.abs(percentDiff)\n    plt.title(\"Pawpularity: \" + str(pawpularity) +\\\n              \"\\npred: \" + str(round(prediction, 2)) +\\\n              \" (err=\" + str(int(absPercentDiff)) + \"%)\")\n    plt.imshow(image)\n    plt.axis(\"off\")\nplt.show()\nplt.close()","metadata":{"execution":{"iopub.status.busy":"2021-12-16T13:47:46.640949Z","iopub.execute_input":"2021-12-16T13:47:46.641211Z","iopub.status.idle":"2021-12-16T13:47:49.316627Z","shell.execute_reply.started":"2021-12-16T13:47:46.641184Z","shell.execute_reply":"2021-12-16T13:47:49.315996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom PIL import Image\ntest_dir = '/kaggle/input/petfinder-pawpularity-score/test'\nids = []\npawpularities = []\nfor test_image in os.listdir(test_dir):\n    image_path = os.path.join(test_dir, test_image)\n    id_image = test_image.split('.')[0]\n    ids.append(id_image)\n    img = Image.open(image_path) \n    img = img.resize((width, height))\n    img = preprocess(np.array(img).reshape(1, width, height, 3))\n    preds = efficientnet.predict(img)\n    prediction = preds.flatten()[0]\n    pawpularities.append(prediction)\n\nsubmission_dict = {\n    'Id': ids,\n    'Pawpularity': pawpularities\n}\nsubmission_df = pd.DataFrame(submission_dict)\nprint(submission_df)\nsubmission_df.to_csv('submission.csv', index=False, sep=',')","metadata":{"execution":{"iopub.status.busy":"2021-12-16T13:45:21.805582Z","iopub.execute_input":"2021-12-16T13:45:21.806354Z","iopub.status.idle":"2021-12-16T13:45:22.978621Z","shell.execute_reply.started":"2021-12-16T13:45:21.80631Z","shell.execute_reply":"2021-12-16T13:45:22.977864Z"},"trusted":true},"execution_count":null,"outputs":[]}]}