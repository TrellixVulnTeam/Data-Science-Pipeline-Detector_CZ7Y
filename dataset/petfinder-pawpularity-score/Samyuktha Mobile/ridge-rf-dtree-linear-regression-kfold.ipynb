{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"PetFinder\n\n![Paw](https://storage.googleapis.com/kaggle-media/competitions/Petfinder/PetFinder%20-%20Logo.png)\n\n![Cat](https://www.petfinder.my/images/cuteness_meter.jpg) \n\nA picture is worth a thousand words. But did you know a picture can save a thousand lives? Millions of stray animals suffer on the streets or are euthanized in shelters every day around the world. You might expect pets with attractive photos to generate more interest and be adopted faster. But what makes a good picture? With the help of data science, you may be able to accurately determine a pet photo‚Äôs appeal and even suggest improvements to give these rescue animals a higher chance of loving homes.\n\nPetFinder.my is Malaysia‚Äôs leading animal welfare platform, featuring over 180,000 animals with 54,000 happily adopted. PetFinder collaborates closely with animal lovers, media, corporations, and global organizations to improve animal welfare.\n\nCurrently, PetFinder.my uses a basic Cuteness Meter to rank pet photos. It analyzes picture composition and other factors compared to the performance of thousands of pet profiles. While this basic tool is helpful, it's still in an experimental stage and the algorithm could be improved.\n\nIn this competition, you‚Äôll analyze raw images and metadata to predict the ‚ÄúPawpularity‚Äù of pet photos. You'll train and test your model on PetFinder.my's thousands of pet profiles. Winning versions will offer accurate recommendations that will improve animal welfare.\n\nIf successful, your solution will be adapted into AI tools that will guide shelters and rescuers around the world to improve the appeal of their pet profiles, automatically enhancing photo quality and recommending composition improvements. As a result, stray dogs and cats can find their \"furever\" homes much faster. With a little assistance from the Kaggle community, many precious lives could be saved and more happy families created.\n\nTop participants may be invited to collaborate on implementing their solutions and creatively improve global animal welfare with their AI skills.\n\n![pets](https://www.petfinder.my/images/cuteness_meter-showcase.jpg)","metadata":{}},{"cell_type":"markdown","source":"# <span id = \"1\"></span>Overview\n<hr/>\nWelcome to my Kernel! In this kernel I aim to apply machine learning algorithms. By doing this, I believe that we will understand the mechansim and theory behind the science better. \n\n<font color = 'green'><b>UPVOTE</b></font>:**It will be very much appreciated and will motivate me to offer more content to the** <font color='red'><b>kaggle</b></font>**community**üòé \n<br/>\n<img src=\"https://i.imgur.com/QPWu3Rd.png\" title=\"source: Gradient Descent\" height=\"400\" width=\"800\" />","metadata":{}},{"cell_type":"markdown","source":"# Read Data","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# /kaggle/input/petfinder-pawpularity-score/train.csv\n# /kaggle/input/petfinder-pawpularity-score/test.csv\n# /kaggle/input/petfinder-pawpularity-score/test/\n\ntrain_df = pd.read_csv('/kaggle/input/petfinder-pawpularity-score/train.csv')\ntrain_df","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-14T00:21:19.889675Z","iopub.execute_input":"2021-12-14T00:21:19.890177Z","iopub.status.idle":"2021-12-14T00:21:19.93332Z","shell.execute_reply.started":"2021-12-14T00:21:19.89014Z","shell.execute_reply":"2021-12-14T00:21:19.932658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv('/kaggle/input/petfinder-pawpularity-score/test.csv')\ntest_df","metadata":{"execution":{"iopub.status.busy":"2021-12-14T00:21:19.93486Z","iopub.execute_input":"2021-12-14T00:21:19.935232Z","iopub.status.idle":"2021-12-14T00:21:19.957781Z","shell.execute_reply.started":"2021-12-14T00:21:19.935201Z","shell.execute_reply":"2021-12-14T00:21:19.956982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from glob import glob\nimport random as r\nimport os\n\n# Path variables\nbase_path = '/kaggle/input/petfinder-pawpularity-score/'\ntrain_path = base_path + 'train.csv'\ntest_path = base_path + 'test.csv'\n\n#In Python, the glob module is used to retrieve files/pathnames matching a specified pattern.\ntrain_images = glob(base_path+'train/*.jpg')\ntest_images = glob(base_path+'test/*.jpg')\n\ntarget = 'Pawpularity'\nseed = 5682\nr.seed(seed)\nnp.random.seed(seed)\n#If PYTHONHASHSEED is set to an integer value, it is used as a fixed seed for generating the hash() of the types covered by the hash randomization.\nos.environ['PYTHONHASHSEED'] = str(seed)\n\nfeatures = [c for c in train_df.columns if c not in ['Id','Target']]\ndf = train_df.copy()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-14T00:21:19.958916Z","iopub.execute_input":"2021-12-14T00:21:19.959604Z","iopub.status.idle":"2021-12-14T00:21:20.010116Z","shell.execute_reply.started":"2021-12-14T00:21:19.959562Z","shell.execute_reply":"2021-12-14T00:21:20.009105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Most popular Pet ","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nmp = df[df[target] == df[target].max()].iloc[0]\npath = f\"{base_path}train/{mp['Id']}.jpg\"\nim = plt.imread(path)\nplt.figure(figsize=(15,6))\nplt.imshow(im)\nplt.title('Most popular pet')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-14T00:21:20.011521Z","iopub.execute_input":"2021-12-14T00:21:20.011794Z","iopub.status.idle":"2021-12-14T00:21:20.398171Z","shell.execute_reply.started":"2021-12-14T00:21:20.01176Z","shell.execute_reply":"2021-12-14T00:21:20.397196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[df['Id'] == path.split('/')[-1].split('.')[0]]","metadata":{"execution":{"iopub.status.busy":"2021-12-14T00:21:20.400983Z","iopub.execute_input":"2021-12-14T00:21:20.401419Z","iopub.status.idle":"2021-12-14T00:21:20.418811Z","shell.execute_reply.started":"2021-12-14T00:21:20.401376Z","shell.execute_reply":"2021-12-14T00:21:20.417518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Least popular pet","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nmp = df[df[target] == df[target].min()].iloc[0]\npath = f\"{base_path}train/{mp['Id']}.jpg\"\nim = plt.imread(path)\nplt.figure(figsize=(15,6))\nplt.imshow(im)\nplt.title('Least popular pet')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-14T00:21:20.420254Z","iopub.execute_input":"2021-12-14T00:21:20.420781Z","iopub.status.idle":"2021-12-14T00:21:20.850476Z","shell.execute_reply.started":"2021-12-14T00:21:20.42074Z","shell.execute_reply":"2021-12-14T00:21:20.849632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Split the dataset","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nx = df[features]\ny = df[target]\n\nX_train, X_test, y_train, y_test = train_test_split(x, y,test_size=0.3,random_state=seed)\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-14T00:21:20.85168Z","iopub.execute_input":"2021-12-14T00:21:20.851906Z","iopub.status.idle":"2021-12-14T00:21:20.86316Z","shell.execute_reply.started":"2021-12-14T00:21:20.851878Z","shell.execute_reply":"2021-12-14T00:21:20.862356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import Ridge\n\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Ref: https://www.kdnuggets.com/2020/11/simple-python-package-comparing-plotting-evaluating-regression-models.html\n# Ref: https://machinelearningmastery.com/overfitting-machine-learning-models/","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-14T00:21:20.864519Z","iopub.execute_input":"2021-12-14T00:21:20.864849Z","iopub.status.idle":"2021-12-14T00:21:20.878506Z","shell.execute_reply.started":"2021-12-14T00:21:20.864805Z","shell.execute_reply":"2021-12-14T00:21:20.877399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluating the models\n","metadata":{}},{"cell_type":"markdown","source":"### K-Fold Cross Validation method : A quick recap\n\n\nK-Fold Cross Validation is a more sophisticated approach that generally results in a less biased model compared to other methods. This method consists in the following steps:\n\nDivides the n observations of the dataset into k mutually exclusive and equal or close-to-equal sized subsets known as ‚Äúfolds‚Äù. \nFit the model using k-1 folds as the training set and one fold (kth) as the test set. After each iteration has been finished, store the error of the model.\nRepeat this process k times using one different fold every time as a test set and the remaining folds (k-1) as the training set. \nOnce all the iterations have finished, take the mean of the k models. This would be the Mean Squared Error of the model.\nThe error model in using the K-Fold cross validation has the following formula:\n\n![Cv](https://financetrain.sgp1.cdn.digitaloceanspaces.com/error-model.png)","metadata":{}},{"cell_type":"markdown","source":"The following figure illustrates k-fold cross-validation with k=5. There are some other schemes to divide the training set, we'll look at them briefly later.","metadata":{}},{"cell_type":"markdown","source":"![K Fold Cross validation](https://financetrain.sgp1.cdn.digitaloceanspaces.com/image16.png)","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nmodels = []\nmodels.append(('LR',LinearRegression()))\nmodels.append(('DR',DecisionTreeRegressor(random_state = seed,max_depth=3)))\nmodels.append(('SVR',SVR(kernel='rbf', gamma='auto')))\nmodels.append(('RFR',RandomForestRegressor(n_estimators = 300, random_state = 0)))\nmodels.append(('Ridge',Ridge()))\nkfold = KFold(n_splits=10,random_state=seed)\nresults = []\nnames = []\nscoring = 'r2'\nfor name, model in models:\n    kfold = KFold(n_splits=10,random_state=seed)\n    cv_res = cross_val_score(model,X_train, y_train, cv=kfold,scoring = scoring)\n    results.append(cv_res)\n    names.append(name)\n    msg = '%s : %f (%f)' %(name,cv_res.mean(),cv_res.std())\n    print(msg)\n    plt.plot(cv_res , label = name)\n    plt.title('CV Results comparison')\n    plt.xlabel('Model'+name)\n    plt.ylabel('CV Result')\n\n    plt.legend()\n    plt.show()\n    \n# Ref: https://www.projectpro.io/recipes/compare-sklearn-classification-algorithms-in-python","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-14T00:21:20.879916Z","iopub.execute_input":"2021-12-14T00:21:20.880679Z","iopub.status.idle":"2021-12-14T00:21:50.852058Z","shell.execute_reply.started":"2021-12-14T00:21:20.880642Z","shell.execute_reply":"2021-12-14T00:21:50.851193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Observation\n\n* Linear and Ridge Regression Models have constant stable CV Results\n* Support vector, Random forest , Decision trees show fluctuating performance output. Kfold reduces overfitting.","metadata":{}},{"cell_type":"markdown","source":"> K-fold cross validation is a standard technique to detect overfitting. It cannot \"cause\" overfitting in the sense of causality. However, there is no guarantee that k-fold cross-validation removes overfitting.\n> The data set is divided into k subsets, and the holdout method is repeated k times. Each time, one of the k subsets is used as the test set and the other k-1 subsets are put together to form a training set. Then the average error across all k trials is computed. The advantage of this method is that it matters less how the data gets divided. Every data point gets to be in a test set exactly once, and gets to be in a training set k-1 times. The variance of the resulting estimate is reduced as k is increased. The disadvantage of this method is that the training algorithm has to be rerun from scratch k times, which means it takes k times as much computation to make an evaluation.","metadata":{}},{"cell_type":"code","source":"#We have also ploted Box Plot to clearly visualize the result.\nfig = plt.figure(figsize = (10,10))\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-14T00:21:50.853389Z","iopub.execute_input":"2021-12-14T00:21:50.854224Z","iopub.status.idle":"2021-12-14T00:21:51.088397Z","shell.execute_reply.started":"2021-12-14T00:21:50.854176Z","shell.execute_reply":"2021-12-14T00:21:51.087734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nv = pd.DataFrame([results],columns=names)\nax = sns.swarmplot(data = results)\nax.set_xticklabels(names)\nplt.xlabel('Model')\nplt.ylabel('Results')\nplt.title('Visualize model performance')\nplt.show()\n\n# https://seaborn.pydata.org/generated/seaborn.swarmplot.html","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-14T00:21:51.089624Z","iopub.execute_input":"2021-12-14T00:21:51.090653Z","iopub.status.idle":"2021-12-14T00:21:51.31676Z","shell.execute_reply.started":"2021-12-14T00:21:51.090609Z","shell.execute_reply":"2021-12-14T00:21:51.315962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n# Ridge regression model \n-- building and evaluation","metadata":{}},{"cell_type":"markdown","source":"Now below, we will perform cross-validation on the data set and get our scores. This might be more appropriate if we had a test/train set and were predicting. However, having the R square gives us some context and helps going into the next cell where we are looking at how alpha changes our R square.","metadata":{}},{"cell_type":"code","source":"# Import the necessary module\nfrom sklearn.model_selection import cross_val_score\n\n# Create a linear regression object: reg\nreg = LinearRegression()\n\n# Compute 5-fold cross-validation scores: cv_scores\ncv_scores = cross_val_score(reg, x, y, cv=5)\n\n# Print the 5-fold cross-validation scores\nprint(cv_scores)\n\n# find the mean of our cv scores here\nprint(\"Average 5-Fold CV Score: {}\".format(np.mean(cv_scores)))","metadata":{"execution":{"iopub.status.busy":"2021-12-14T00:21:51.318222Z","iopub.execute_input":"2021-12-14T00:21:51.318439Z","iopub.status.idle":"2021-12-14T00:21:51.383528Z","shell.execute_reply.started":"2021-12-14T00:21:51.318412Z","shell.execute_reply":"2021-12-14T00:21:51.382621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Below we'll run a ridge regression and see how score varies with different alphas. This will show how picking a different alpha score changes the R2.","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import Ridge\n\n# Create an array of alphas and lists to store scores\nalpha_space = np.logspace(-4, 0, 50)\nridge_scores = []\nridge_scores_std = []\n\n# Create a ridge regressor: ridge\nridge = Ridge(normalize=True)\n\n# Compute scores over range of alphas\nfor alpha in alpha_space:\n\n    # Specify the alpha value to use: ridge.alpha\n    ridge.alpha = alpha\n    \n    # Perform 10-fold CV: ridge_cv_scores\n    ridge_cv_scores = cross_val_score(ridge, x, y, cv=10)\n    \n    # Append the mean of ridge_cv_scores to ridge_scores\n    ridge_scores.append(np.mean(ridge_cv_scores))\n    \n    # Append the std of ridge_cv_scores to ridge_scores_std\n    ridge_scores_std.append(np.std(ridge_cv_scores))\n\n# Use this function to create a plot    \ndef display_plot(cv_scores, cv_scores_std):\n    fig = plt.figure()\n    ax = fig.add_subplot(1,1,1)\n    ax.plot(alpha_space, cv_scores)\n\n    std_error = cv_scores_std / np.sqrt(10)\n\n    ax.fill_between(alpha_space, cv_scores + std_error, cv_scores - std_error, alpha=0.2)\n    ax.set_ylabel('CV Score +/- Std Error')\n    ax.set_xlabel('Alpha')\n    ax.axhline(np.max(cv_scores), linestyle='--', color='.5')\n    ax.set_xlim([alpha_space[0], alpha_space[-1]])\n    ax.set_xscale('log')\n    plt.show()\n\n# Display the plot\ndisplay_plot(ridge_scores, ridge_scores_std)","metadata":{"execution":{"iopub.status.busy":"2021-12-14T00:21:51.385297Z","iopub.execute_input":"2021-12-14T00:21:51.38584Z","iopub.status.idle":"2021-12-14T00:21:57.713503Z","shell.execute_reply.started":"2021-12-14T00:21:51.385793Z","shell.execute_reply":"2021-12-14T00:21:57.712596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"markdown","source":"You can change the model and predict the popularity score","metadata":{}},{"cell_type":"code","source":"\nx = train_df.drop(columns='Id').iloc[:,:12]\ny = train_df['Pawpularity']\n\n# split data\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=210)\n\n# clf = Ridge(alpha = 1)\nreg = LinearRegression()\nreg.fit(x_train, y_train)\ny_pred = reg.predict(x_test)\n\n# from sklearn.metrics import mean_squared_error\n# mse=mean_squared_error(y_test,y_pred)\n# rmse=np.sqrt(mse)\n# rmse","metadata":{"execution":{"iopub.status.busy":"2021-12-14T00:47:50.814964Z","iopub.execute_input":"2021-12-14T00:47:50.815751Z","iopub.status.idle":"2021-12-14T00:47:50.836546Z","shell.execute_reply.started":"2021-12-14T00:47:50.815705Z","shell.execute_reply":"2021-12-14T00:47:50.835213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ids = test_df['Id'].to_list()\nsub_dict = {\"Id\": ids, \"Pawpularity\": list(y_pred[0:8])}\nfinal_df = pd.DataFrame(sub_dict)\nfinal_df.to_csv('./submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-14T00:47:53.373357Z","iopub.execute_input":"2021-12-14T00:47:53.373681Z","iopub.status.idle":"2021-12-14T00:47:53.381963Z","shell.execute_reply.started":"2021-12-14T00:47:53.37365Z","shell.execute_reply":"2021-12-14T00:47:53.381015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"![Upvote](https://i.pinimg.com/originals/c6/ef/35/c6ef35fef7a2c4baf86a5a6732b2652d.gif)","metadata":{}},{"cell_type":"markdown","source":"If this helped you, your **UPVOTES** would be very much appreciated ‚Äì as they are the source of motivation!\n\nHappy learning","metadata":{}},{"cell_type":"markdown","source":"# References:\n\n> * [Comparing Regression models](https://www.kaggle.com/ankitjha/comparing-regression-models)\n> * [Regression algorithms using 'scikit-learn'](https://www.kaggle.com/amar09/regression-algorithms-using-scikit-learn)\n","metadata":{}}]}