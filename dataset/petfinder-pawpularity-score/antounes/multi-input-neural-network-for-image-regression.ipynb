{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Multi-input Neural Network","metadata":{}},{"cell_type":"markdown","source":"The previous experiments we've conducted lead us to believe that a multimodal input approach would produce better results. Photo metadata and images consist in our two sources of data. They're of different nature. For example a densely connected network could tackle the metadata information, whilst a 2D convolutional neural network would deal with image data.\n\nHere we'll try to jointly learn information from both the data sources by using a model that can see all available input modalities simulataneously. Our model will have two input branches.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pylab as plt\nfrom sklearn.model_selection import StratifiedShuffleSplit\nimport csv\n\nconfig = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=2, \n                                  inter_op_parallelism_threads=2, \n                                  allow_soft_placement=True) # TensorFlow config\npd.options.mode.chained_assignment = None # Pandas config","metadata":{"execution":{"iopub.status.busy":"2021-12-26T19:48:44.130107Z","iopub.execute_input":"2021-12-26T19:48:44.130808Z","iopub.status.idle":"2021-12-26T19:48:46.067454Z","shell.execute_reply.started":"2021-12-26T19:48:44.130664Z","shell.execute_reply":"2021-12-26T19:48:46.066663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data handlers","metadata":{}},{"cell_type":"markdown","source":"## Images attributes definition","metadata":{}},{"cell_type":"markdown","source":"Input data are manipulated as tensors, the basic data structures of TensorFlow. Images generally have 3 dimensions: height, width, and number of colour channels. An image dataset is most of the time represented as a rank-4 tensor (or 4D tensor) of shape (samples, height, width, channels). For example, a batch of 32 colour images of size 150 x 150 pixels can be stored in the rank-4 tensor (32, 150, 150, 3).\n\nOur network will have to receive images of a fixed size. The images in the dataset being of different size, we'll have to resize them, here to the size of the smallest image. Let's explore some image attributes.","metadata":{}},{"cell_type":"code","source":"import os\nimport PIL\n\npath = '../input/petfinder-pawpularity-score/train/'\ntraining_img = os.listdir(path) # list all training images names\nprint('There are {} images in the training directory'.format(len(training_img)))\n\nimg_sz = {'width': list(),\n          'height': list()} # store image attributes for further analysis\nwidth, height = 9999, 9999 # start with fixed very high image size, and keep reducing it when iterating over images in the dataset\n\nfor im in training_img:\n    img = PIL.Image.open(path+im)\n    w, h = img.size\n    if w < width:\n        width = w\n    if h < height:\n        height = h\n\nIMG_WIDTH = width\nIMG_HEIGHT = height\nIMG_CHANNELS = 3\n\nprint('Min training image width: {} px'.format(IMG_WIDTH))\nprint('Min training image height: {} px'.format(IMG_HEIGHT))","metadata":{"execution":{"iopub.status.busy":"2021-12-26T19:48:46.069283Z","iopub.execute_input":"2021-12-26T19:48:46.069584Z","iopub.status.idle":"2021-12-26T19:48:50.831583Z","shell.execute_reply.started":"2021-12-26T19:48:46.069544Z","shell.execute_reply":"2021-12-26T19:48:50.830785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_and_decode(filename, reshape_dims=(IMG_HEIGHT, IMG_WIDTH)):\n    # Read an image file to a tensor as a sequence of bytes\n    image = tf.io.read_file(filename)\n    # Convert the tensor to a 3D uint8 tensor\n    image = tf.image.decode_jpeg(image, channels=IMG_CHANNELS)\n    # Convert 3D uint8 tensor with values in [0, 1]\n    image = tf.image.convert_image_dtype(image, tf.float32)\n    # Resize the image to the desired size\n    return tf.image.resize(image, reshape_dims).numpy()\n\ndef show_image(filename):\n    image = read_and_decode(filename, [IMG_HEIGHT, IMG_WIDTH])\n    plt.imshow(image);\n    plt.axis('off');\n\ndef training_plot(metrics, history):\n    f, ax = plt.subplots(1, len(metrics), figsize=(5*len(metrics), 5))\n    for idx, metric in enumerate(metrics):\n        ax[idx].plot(history.history[metric], ls='dashed')\n        ax[idx].set_xlabel('Epochs')\n        ax[idx].set_ylabel(metric)\n        ax[idx].plot(history.history['val_'+metric]);\n        ax[idx].legend(['train_'+metric, 'val_'+metric])","metadata":{"execution":{"iopub.status.busy":"2021-12-26T19:48:50.832825Z","iopub.execute_input":"2021-12-26T19:48:50.833321Z","iopub.status.idle":"2021-12-26T19:48:50.844334Z","shell.execute_reply.started":"2021-12-26T19:48:50.833279Z","shell.execute_reply":"2021-12-26T19:48:50.843362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display a random image from the dataset\n\nrand_idx = np.random.randint(0, len(training_img)-1)\nrand_img = training_img[rand_idx]\n\nshow_image(path+rand_img)","metadata":{"execution":{"iopub.status.busy":"2021-12-26T19:48:50.848414Z","iopub.execute_input":"2021-12-26T19:48:50.848741Z","iopub.status.idle":"2021-12-26T19:48:52.230335Z","shell.execute_reply.started":"2021-12-26T19:48:50.848706Z","shell.execute_reply":"2021-12-26T19:48:52.229556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Get the data","metadata":{}},{"cell_type":"markdown","source":"Let's create training, validation and test sets of images and their corresponding metadata. `Pawpularity` is the target to predict, we'll then create sets ensuring its distribution remains the same in each set. This is carried out using stratified sampling instead of random sampling.","metadata":{}},{"cell_type":"code","source":"path = \"../input/petfinder-pawpularity-score/\"\n\ndata = pd.read_csv(path+\"/train.csv\") # Dataset for images\ndata['Id'] = data['Id'].apply(lambda x: path+'train/'+x+'.jpg')\nx, y = data.drop([\"Id\", \"Pawpularity\"], axis=1), data[\"Pawpularity\"] # Subsets of the dataset for tabular data\n\n# Create training, validation and test sets for tabular and image data\n# First: test set is created by keeping apart 20% of the dataset\n# Second: validation set is created by keeping apart 20% of the remaining dataset\n# Third: Training set consists of the remaining samples after test and validation set creation\n\nsssplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2) # Use stratified sampling\nfor train_index, test_index in sssplit.split(x, y):\n    # Tabular tmp training and test sets\n    x_train_tmp, y_train_tmp = x.iloc[train_index], y.iloc[train_index]\n    x_test, y_test = x.iloc[test_index], y.iloc[test_index]\n    # Image tmp training and test sets\n    train_img_tmp = data.iloc[train_index]\n    test_img = data.iloc[test_index][['Id', 'Pawpularity']]\n    \nsssplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2)\nfor train_index, val_index in sssplit.split(x_train_tmp, y_train_tmp):\n    # Tabular training and validation set\n    x_train, y_train = x_train_tmp.iloc[train_index], y_train_tmp.iloc[train_index]\n    x_val, y_val = x_train_tmp.iloc[val_index], y_train_tmp.iloc[val_index]\n    # Image training and validation set\n    train_img = train_img_tmp.iloc[train_index][['Id', 'Pawpularity']]\n    val_img = train_img_tmp.iloc[val_index][['Id', 'Pawpularity']]\n    \n# Export image sets for futher loading and processing\ntrain_img.to_csv('/kaggle/working/training_img.csv', header=False, index=False)\nval_img.to_csv('/kaggle/working/val_img.csv', header=False, index=False)\ntest_img.to_csv('/kaggle/working/test_img.csv', header=False, index=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-26T19:48:52.231412Z","iopub.execute_input":"2021-12-26T19:48:52.231673Z","iopub.status.idle":"2021-12-26T19:48:52.353426Z","shell.execute_reply.started":"2021-12-26T19:48:52.231642Z","shell.execute_reply":"2021-12-26T19:48:52.35265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Pre-process tabular data for further input to the neural network\n\nx_train, y_train = np.asarray(x_train), np.asarray(y_train).astype('float32')\nx_val, y_val = np.asarray(x_val), np.asarray(y_val).astype('float32')\nx_test, y_test = np.asarray(x_test), np.asarray(y_test).astype('float32')","metadata":{"execution":{"iopub.status.busy":"2021-12-26T19:48:52.354891Z","iopub.execute_input":"2021-12-26T19:48:52.355184Z","iopub.status.idle":"2021-12-26T19:48:52.360761Z","shell.execute_reply.started":"2021-12-26T19:48:52.355146Z","shell.execute_reply":"2021-12-26T19:48:52.359934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Store images into arrays for further processing by the network\n\ntrain_dataset = list()\nprint('Loading training images...')\nwith open('/kaggle/working/training_img.csv', 'r') as file:\n    reader = csv.reader(file) \n    for i, row in enumerate(reader):\n        train_dataset.append(read_and_decode(row[0]))\nprint('...done!\\n')\ntrain_dataset = np.array(train_dataset)\nprint('Training dataset shape:', train_dataset.shape)\nprint()\n        \nval_dataset = list()\nprint('Loading validation images...')\nwith open('/kaggle/working/val_img.csv', 'r') as file:\n    reader = csv.reader(file)\n    for i, row in enumerate(reader):\n        val_dataset.append(read_and_decode(row[0]))\nprint('...done!')\nval_dataset = np.array(val_dataset)\nprint('Validation dataset shape:', val_dataset.shape)\nprint()\n        \ntest_dataset = list()\nwith open('/kaggle/working/test_img.csv', 'r') as file:\n    reader = csv.reader(file)\n    for i, row in enumerate(reader):\n        test_dataset.append(read_and_decode(row[0]))\ntest_dataset = np.array(test_dataset)\nprint('Test dataset shape:', test_dataset.shape)","metadata":{"execution":{"iopub.status.busy":"2021-12-26T19:48:52.362193Z","iopub.execute_input":"2021-12-26T19:48:52.362676Z","iopub.status.idle":"2021-12-26T19:49:46.859635Z","shell.execute_reply.started":"2021-12-26T19:48:52.362636Z","shell.execute_reply":"2021-12-26T19:49:46.858345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Building the model","metadata":{}},{"cell_type":"markdown","source":"The general architecture we'd like to use should accept two inputs: tabular and image data, and from there produce one input: a prediction for `Pawpularity`. \n\nTabular data is passed to a dense NN and image data is passed to a CNN. Outputs from both the newtorks are concatenated, and the resulting vector passes through a series of consecutive output units.","metadata":{}},{"cell_type":"code","source":"# Build models for each data type using Keras Functional API\n\nBATCH_SIZE = 128\nIMG_WIDTH = width\nIMG_HEIGHT = height\nIMG_CHANNELS = 3\n\n# Image data\ninput_img = tf.keras.layers.Input(shape=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\nx = tf.keras.layers.Conv2D(32, (3, 3), activation='relu')(input_img)\nx = tf.keras.layers.MaxPooling2D((2, 2))(x)\nx = tf.keras.layers.Conv2D(64, (3, 3), activation='relu')(x)\nx = tf.keras.layers.GlobalAveragePooling2D()(x)\n\n# Tabular data\ninput_tab = tf.keras.layers.Input(shape=(12,))\ny = tf.keras.layers.Dense(16, activation='relu')(input_tab)\ny = tf.keras.layers.Dense(32, activation='relu')(y)\n\n# Concatenate models outputs\nconcatenated = tf.keras.layers.concatenate([x, y], axis=-1)\n\n# Pass concatenated vector through a Dense layer with no activation to output a `Pawpularity` score prediction\noutput_score = tf.keras.layers.Dense(1, activation=None)(concatenated)\n\n# Build general model with Keras Functional API\nmodel = tf.keras.models.Model([input_img, input_tab], output_score)\nmodel.compile(optimizer='adam',\n              loss=tf.keras.losses.MeanSquaredError(),\n              metrics=[tf.keras.metrics.RootMeanSquaredError()])","metadata":{"execution":{"iopub.status.busy":"2021-12-26T19:54:28.030445Z","iopub.execute_input":"2021-12-26T19:54:28.031198Z","iopub.status.idle":"2021-12-26T19:54:28.09637Z","shell.execute_reply.started":"2021-12-26T19:54:28.031158Z","shell.execute_reply":"2021-12-26T19:54:28.095609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(model.summary())\ntf.keras.utils.plot_model(model, show_shapes=True, show_layer_names=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-26T19:54:29.579649Z","iopub.execute_input":"2021-12-26T19:54:29.58026Z","iopub.status.idle":"2021-12-26T19:54:29.853405Z","shell.execute_reply.started":"2021-12-26T19:54:29.580224Z","shell.execute_reply":"2021-12-26T19:54:29.852539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training the model","metadata":{}},{"cell_type":"code","source":"history = model.fit([train_dataset, tf.convert_to_tensor(x_train)], tf.convert_to_tensor(y_train), epochs=20, batch_size=BATCH_SIZE,\n                    validation_data=([val_dataset, tf.convert_to_tensor(x_val)], tf.convert_to_tensor(y_val)))","metadata":{"execution":{"iopub.status.busy":"2021-12-26T19:54:34.879907Z","iopub.execute_input":"2021-12-26T19:54:34.880791Z","iopub.status.idle":"2021-12-26T19:55:17.691335Z","shell.execute_reply.started":"2021-12-26T19:54:34.880748Z","shell.execute_reply":"2021-12-26T19:55:17.690475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_plot(['loss', 'root_mean_squared_error'], history)","metadata":{"execution":{"iopub.status.busy":"2021-12-26T19:55:17.693387Z","iopub.execute_input":"2021-12-26T19:55:17.693696Z","iopub.status.idle":"2021-12-26T19:55:18.074527Z","shell.execute_reply.started":"2021-12-26T19:55:17.693658Z","shell.execute_reply":"2021-12-26T19:55:18.073743Z"},"trusted":true},"execution_count":null,"outputs":[]}]}