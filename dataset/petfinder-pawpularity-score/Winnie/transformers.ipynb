{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#load in packages\nimport os\nimport pandas as pd\nfrom glob import glob\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nimport gc\nfrom tqdm import tqdm\n\n#images\nimport cv2\n\n#modeling\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.python.client import device_lib\n\n#visualizations\nimport seaborn as sns\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-07T06:41:03.144192Z","iopub.execute_input":"2021-12-07T06:41:03.144594Z","iopub.status.idle":"2021-12-07T06:41:10.077428Z","shell.execute_reply.started":"2021-12-07T06:41:03.144514Z","shell.execute_reply":"2021-12-07T06:41:10.076204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#source path (where the Pawpularity contest data resides)\npath = '../input/petfinder-pawpularity-score/'\n\n#Get the metadata (the .csv data) and put it into DataFrames\ntrain_df = pd.read_csv(path + 'train.csv')\ntest_df = pd.read_csv(path + 'test.csv')\n\n#Get the image data (the .jpg data) and put it into lists of filenames\ntrain_jpg = glob(path + \"train/*.jpg\")\ntest_jpg = glob(path + \"test/*.jpg\")","metadata":{"execution":{"iopub.status.busy":"2021-12-07T06:41:10.079906Z","iopub.execute_input":"2021-12-07T06:41:10.080269Z","iopub.status.idle":"2021-12-07T06:41:10.401192Z","shell.execute_reply.started":"2021-12-07T06:41:10.080222Z","shell.execute_reply":"2021-12-07T06:41:10.400221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.shape, len(train_jpg)","metadata":{"execution":{"iopub.status.busy":"2021-12-07T06:41:10.402709Z","iopub.execute_input":"2021-12-07T06:41:10.403046Z","iopub.status.idle":"2021-12-07T06:41:10.413465Z","shell.execute_reply.started":"2021-12-07T06:41:10.402999Z","shell.execute_reply":"2021-12-07T06:41:10.412251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-12-07T06:41:10.416547Z","iopub.execute_input":"2021-12-07T06:41:10.417571Z","iopub.status.idle":"2021-12-07T06:41:10.444195Z","shell.execute_reply.started":"2021-12-07T06:41:10.417518Z","shell.execute_reply":"2021-12-07T06:41:10.443177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.histplot(train_df['Pawpularity']);\n\nplt.axvline(train_df['Pawpularity'].mean(),color='red')\nplt.axvline(train_df['Pawpularity'].median(),color='green')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-07T06:41:10.446539Z","iopub.execute_input":"2021-12-07T06:41:10.446929Z","iopub.status.idle":"2021-12-07T06:41:10.91459Z","shell.execute_reply.started":"2021-12-07T06:41:10.446851Z","shell.execute_reply":"2021-12-07T06:41:10.913592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df[['Pawpularity']].describe().T","metadata":{"execution":{"iopub.status.busy":"2021-12-07T06:41:10.916381Z","iopub.execute_input":"2021-12-07T06:41:10.916876Z","iopub.status.idle":"2021-12-07T06:41:10.945598Z","shell.execute_reply.started":"2021-12-07T06:41:10.916824Z","shell.execute_reply":"2021-12-07T06:41:10.944433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.columns","metadata":{"execution":{"iopub.status.busy":"2021-12-07T06:41:10.947423Z","iopub.execute_input":"2021-12-07T06:41:10.948013Z","iopub.status.idle":"2021-12-07T06:41:10.955636Z","shell.execute_reply.started":"2021-12-07T06:41:10.947967Z","shell.execute_reply":"2021-12-07T06:41:10.954419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols = ['Subject Focus', 'Eyes', 'Face', 'Near', 'Action', 'Accessory',\n       'Group', 'Collage', 'Human', 'Occlusion', 'Info', 'Blur']\n\nfor col in cols:\n    fig, ax = plt.subplots(1,2)\n    sns.violinplot(data = train_df,y ='Pawpularity', x=col, ax=ax[0])\n    sns.histplot(data = train_df,x ='Pawpularity',hue=col,kde=True,fill=True, ax=ax[1])\n    \n    plt.suptitle(str(col), fontsize=18, fontweight='bold')\n    fig.show()","metadata":{"_kg_hide-output":false,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-07T06:41:10.957549Z","iopub.execute_input":"2021-12-07T06:41:10.958331Z","iopub.status.idle":"2021-12-07T06:41:22.293025Z","shell.execute_reply.started":"2021-12-07T06:41:10.958285Z","shell.execute_reply":"2021-12-07T06:41:22.292042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_path = '../input/petfinder-pawpularity-score/train/'\next = '.jpg'\n\nnums = len(cols)\n\n# col = 'Eyes'\nfor count, col in enumerate(cols):\n    sample = train_df.loc[train_df[col] == 1,'Id'].head(100).values[np.random.randint(10)]\n    \n    pawpularity = train_df.loc[train_df['Id'] == sample, 'Pawpularity'].head(1).values[-1]\n    \n    image_loc = img_path + sample + ext\n\n    image_array = plt.imread(image_loc)\n    plt.imshow(image_array)\n    \n    plt.title(f'Image of pet with {col}\\nPawpularity Score: {pawpularity}') \n    plt.axis('off') #turns off the gridlines\n    plt.show()\n\n    del sample, image_loc, image_array\n    gc.collect()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-07T06:41:22.294871Z","iopub.execute_input":"2021-12-07T06:41:22.295439Z","iopub.status.idle":"2021-12-07T06:41:27.730423Z","shell.execute_reply.started":"2021-12-07T06:41:22.295379Z","shell.execute_reply":"2021-12-07T06:41:27.729443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Modify the Id such that each Id is the full image path. In the form\ndef train_id_to_path(x):\n    return '../input/petfinder-pawpularity-score/train/' + x + \".jpg\"\ndef test_id_to_path(x):\n    return '../input/petfinder-pawpularity-score/test/' + x + \".jpg\"\n","metadata":{"execution":{"iopub.status.busy":"2021-12-07T06:41:27.734987Z","iopub.execute_input":"2021-12-07T06:41:27.735558Z","iopub.status.idle":"2021-12-07T06:41:27.741555Z","shell.execute_reply.started":"2021-12-07T06:41:27.73551Z","shell.execute_reply":"2021-12-07T06:41:27.740111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Check to see\ntf.config.get_visible_devices()","metadata":{"execution":{"iopub.status.busy":"2021-12-07T06:41:27.74364Z","iopub.execute_input":"2021-12-07T06:41:27.743975Z","iopub.status.idle":"2021-12-07T06:41:27.950562Z","shell.execute_reply.started":"2021-12-07T06:41:27.743932Z","shell.execute_reply":"2021-12-07T06:41:27.94918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Read in the data and drop unnecessary columns\ntrain = pd.read_csv('../input/petfinder-pawpularity-score/train.csv')\ntrain = train.drop(['Subject Focus', 'Eyes', 'Face', 'Near', 'Action', 'Accessory', 'Group', 'Collage', 'Human', 'Occlusion', 'Info', 'Blur'],axis=1)\n\ntest = pd.read_csv('../input/petfinder-pawpularity-score/test.csv')\ntest = test.drop(['Subject Focus', 'Eyes', 'Face', 'Near', 'Action', 'Accessory', 'Group', 'Collage', 'Human', 'Occlusion', 'Info', 'Blur'],axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-12-07T06:41:27.952949Z","iopub.execute_input":"2021-12-07T06:41:27.95376Z","iopub.status.idle":"2021-12-07T06:41:27.983184Z","shell.execute_reply.started":"2021-12-07T06:41:27.953711Z","shell.execute_reply":"2021-12-07T06:41:27.982267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Add the .jpg extensions to the image file name ids\ntrain[\"img_path\"] = train[\"Id\"].apply(train_id_to_path)\ntest[\"img_path\"] = test[\"Id\"].apply(test_id_to_path)","metadata":{"execution":{"iopub.status.busy":"2021-12-07T06:41:27.98534Z","iopub.execute_input":"2021-12-07T06:41:27.985559Z","iopub.status.idle":"2021-12-07T06:41:28.00176Z","shell.execute_reply.started":"2021-12-07T06:41:27.985517Z","shell.execute_reply":"2021-12-07T06:41:28.000546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#binning columns to test models\ntrain['two_bin_pawp'] = pd.qcut(train['Pawpularity'], q=2, labels=False)\ntrain = train.astype({\"two_bin_pawp\": str})\n\ntrain['four_bin_pawp'] = pd.qcut(train['Pawpularity'], q=4, labels=False)\ntrain = train.astype({\"four_bin_pawp\": str})\n\ntrain['ten_bin_pawp'] = pd.qcut(train['Pawpularity'], q=10, labels=False)\ntrain = train.astype({\"ten_bin_pawp\": str})","metadata":{"execution":{"iopub.status.busy":"2021-12-07T06:41:28.00363Z","iopub.execute_input":"2021-12-07T06:41:28.004047Z","iopub.status.idle":"2021-12-07T06:41:28.064057Z","shell.execute_reply.started":"2021-12-07T06:41:28.00399Z","shell.execute_reply":"2021-12-07T06:41:28.063144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Set the size image you want to use\nimage_height = 128\nimage_width = 128\n\n#define a function that accepts an image url and outputs an eager tensor\ndef path_to_eagertensor(image_path):\n    raw = tf.io.read_file(image_path)\n    image = tf.image.decode_jpeg(raw, channels=3)\n    image = tf.cast(image, tf.float32)\n    #image = tf.image.resize_with_pad(image, image_height, image_width) #optional with padding to retain original dimensions\n    image = tf.image.resize(image, (image_height, image_width))\n    return image/255.","metadata":{"execution":{"iopub.status.busy":"2021-12-07T06:41:28.066393Z","iopub.execute_input":"2021-12-07T06:41:28.067051Z","iopub.status.idle":"2021-12-07T06:41:28.075105Z","shell.execute_reply.started":"2021-12-07T06:41:28.066979Z","shell.execute_reply":"2021-12-07T06:41:28.073951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#get all the images in the training folder and put their tensors in a list\nX = []\nfor img in tqdm(train['img_path']):\n    new_img_tensor = path_to_eagertensor(img)\n    X.append(new_img_tensor)\n    \nprint(type(X),len(X))\nX = np.array(X)\nprint(type(X),X.shape)","metadata":{"execution":{"iopub.status.busy":"2021-12-07T06:41:28.077058Z","iopub.execute_input":"2021-12-07T06:41:28.077523Z","iopub.status.idle":"2021-12-07T06:43:15.486849Z","shell.execute_reply.started":"2021-12-07T06:41:28.077479Z","shell.execute_reply":"2021-12-07T06:43:15.484549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#get all the images in the test folder and put their tensors in a list\nX_submission = []\nfor img in tqdm(test['img_path']):\n    new_img_tensor = path_to_eagertensor(img)\n    X_submission.append(new_img_tensor)\n    \nprint(type(X_submission),len(X_submission))\nX_submission = np.array(X_submission)\nprint(type(X_submission),X_submission.shape)","metadata":{"execution":{"iopub.status.busy":"2021-12-07T06:43:15.488779Z","iopub.execute_input":"2021-12-07T06:43:15.489108Z","iopub.status.idle":"2021-12-07T06:43:15.565968Z","shell.execute_reply.started":"2021-12-07T06:43:15.489061Z","shell.execute_reply":"2021-12-07T06:43:15.56495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#grab the target variable. In our case, Pawpularity\ny = train['Pawpularity']\nprint(type(y))","metadata":{"execution":{"iopub.status.busy":"2021-12-07T06:43:15.567894Z","iopub.execute_input":"2021-12-07T06:43:15.568634Z","iopub.status.idle":"2021-12-07T06:43:15.575256Z","shell.execute_reply.started":"2021-12-07T06:43:15.568591Z","shell.execute_reply":"2021-12-07T06:43:15.574024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=2021)","metadata":{"execution":{"iopub.status.busy":"2021-12-07T06:43:15.577525Z","iopub.execute_input":"2021-12-07T06:43:15.578362Z","iopub.status.idle":"2021-12-07T06:43:16.320983Z","shell.execute_reply.started":"2021-12-07T06:43:15.578308Z","shell.execute_reply":"2021-12-07T06:43:16.319946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Show the shape of each of the new arrays\nprint(x_train.shape, x_test.shape, y_train.shape, y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-12-07T06:43:16.322799Z","iopub.execute_input":"2021-12-07T06:43:16.323204Z","iopub.status.idle":"2021-12-07T06:43:16.331211Z","shell.execute_reply.started":"2021-12-07T06:43:16.323137Z","shell.execute_reply":"2021-12-07T06:43:16.329825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!pip install -U tensorflow-addons","metadata":{"execution":{"iopub.status.busy":"2021-12-07T06:43:16.333389Z","iopub.execute_input":"2021-12-07T06:43:16.334104Z","iopub.status.idle":"2021-12-07T06:43:16.341313Z","shell.execute_reply.started":"2021-12-07T06:43:16.334059Z","shell.execute_reply":"2021-12-07T06:43:16.34004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nfrom tensorflow import keras\nfrom tensorflow.keras import layers","metadata":{"execution":{"iopub.status.busy":"2021-12-07T06:43:16.344468Z","iopub.execute_input":"2021-12-07T06:43:16.345389Z","iopub.status.idle":"2021-12-07T06:43:16.485715Z","shell.execute_reply.started":"2021-12-07T06:43:16.345262Z","shell.execute_reply":"2021-12-07T06:43:16.484704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_classes = 11\ninput_shape = (128, 128, 3)\n\n(x_train, y_train), (x_test, y_test) = (x_train,y_train),(x_test,y_test)\n\ny_train = keras.utils.to_categorical(y_train//10, num_classes)\ny_test = keras.utils.to_categorical(y_test//10, num_classes)\n\nprint(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\nprint(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")","metadata":{"execution":{"iopub.status.busy":"2021-12-07T06:43:16.487286Z","iopub.execute_input":"2021-12-07T06:43:16.487755Z","iopub.status.idle":"2021-12-07T06:43:16.499994Z","shell.execute_reply.started":"2021-12-07T06:43:16.487693Z","shell.execute_reply":"2021-12-07T06:43:16.498814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learning_rate = 0.001\nweight_decay = 0.0001\nbatch_size = 256\nnum_epochs = 50\nimage_size = 128  # We'll resize input images to this size\npatch_size = 12  # Size of the patches to be extract from the input images\nnum_patches = (image_size // patch_size) ** 2\nprojection_dim = 64\nnum_heads = 4\ntransformer_units = [\n    projection_dim * 2,\n    projection_dim,\n]  # Size of the transformer layers\ntransformer_layers = 6\nmlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier","metadata":{"execution":{"iopub.status.busy":"2021-12-07T06:43:16.503333Z","iopub.execute_input":"2021-12-07T06:43:16.504237Z","iopub.status.idle":"2021-12-07T06:43:16.512491Z","shell.execute_reply.started":"2021-12-07T06:43:16.504194Z","shell.execute_reply":"2021-12-07T06:43:16.511332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_augmentation = keras.Sequential(\n    [\n        layers.Normalization(),\n        layers.Resizing(image_size, image_size),\n        layers.RandomFlip(\"horizontal\"),\n        layers.RandomRotation(factor=0.2),\n        layers.RandomZoom(\n            height_factor=0.35, width_factor=0.2\n        ),\n    ],\n    name=\"data_augmentation\",\n)\n# Compute the mean and the variance of the training data for normalization.\ndata_augmentation.layers[0].adapt(x_train)","metadata":{"execution":{"iopub.status.busy":"2021-12-07T06:43:16.514189Z","iopub.execute_input":"2021-12-07T06:43:16.514576Z","iopub.status.idle":"2021-12-07T06:43:21.089899Z","shell.execute_reply.started":"2021-12-07T06:43:16.514483Z","shell.execute_reply":"2021-12-07T06:43:21.088866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def mlp(x, hidden_units, dropout_rate):\n    for units in hidden_units:\n        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n        x = layers.Dropout(dropout_rate)(x)\n    return x","metadata":{"execution":{"iopub.status.busy":"2021-12-07T06:43:21.091839Z","iopub.execute_input":"2021-12-07T06:43:21.092251Z","iopub.status.idle":"2021-12-07T06:43:21.098469Z","shell.execute_reply.started":"2021-12-07T06:43:21.092168Z","shell.execute_reply":"2021-12-07T06:43:21.097497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Patches(layers.Layer):\n    def __init__(self, patch_size):\n        super(Patches, self).__init__()\n        self.patch_size = patch_size\n\n    def call(self, images):\n        batch_size = tf.shape(images)[0]\n        patches = tf.image.extract_patches(\n            images=images,\n            sizes=[1, self.patch_size, self.patch_size, 1],\n            strides=[1, self.patch_size, self.patch_size, 1],\n            rates=[1, 1, 1, 1],\n            padding=\"VALID\",\n        )\n        patch_dims = patches.shape[-1]\n        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n        return patches","metadata":{"execution":{"iopub.status.busy":"2021-12-07T06:43:21.101059Z","iopub.execute_input":"2021-12-07T06:43:21.101879Z","iopub.status.idle":"2021-12-07T06:43:21.113625Z","shell.execute_reply.started":"2021-12-07T06:43:21.101833Z","shell.execute_reply":"2021-12-07T06:43:21.112479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(4, 4))\nimage = x_train[np.random.choice(range(x_train.shape[0]))]*225.\nplt.imshow(image.astype(\"uint8\"))\nplt.axis(\"off\")\n\nresized_image = tf.image.resize(\n    tf.convert_to_tensor([image]), size=(image_size, image_size)\n)\npatches = Patches(patch_size)(resized_image)\nprint(f\"Image size: {image_size} X {image_size}\")\nprint(f\"Patch size: {patch_size} X {patch_size}\")\nprint(f\"Patches per image: {patches.shape[1]}\")\nprint(f\"Elements per patch: {patches.shape[-1]}\")\n\nn = int(np.sqrt(patches.shape[1]))\nplt.figure(figsize=(4, 4))\nfor i, patch in enumerate(patches[0]):\n    ax = plt.subplot(n, n, i + 1)\n    patch_img = tf.reshape(patch, (patch_size, patch_size, 3))\n    plt.imshow(patch_img.numpy().astype(\"uint8\"))\n    plt.axis(\"off\")","metadata":{"execution":{"iopub.status.busy":"2021-12-07T06:43:21.120085Z","iopub.execute_input":"2021-12-07T06:43:21.120368Z","iopub.status.idle":"2021-12-07T06:43:26.225149Z","shell.execute_reply.started":"2021-12-07T06:43:21.120338Z","shell.execute_reply":"2021-12-07T06:43:26.224091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PatchEncoder(layers.Layer):\n    def __init__(self, num_patches, projection_dim):\n        super(PatchEncoder, self).__init__()\n        self.num_patches = num_patches\n        self.projection = layers.Dense(units=projection_dim)\n        self.position_embedding = layers.Embedding(\n            input_dim=num_patches, output_dim=projection_dim\n        )\n\n    def call(self, patch):\n        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n        encoded = self.projection(patch) + self.position_embedding(positions)\n        return encoded","metadata":{"execution":{"iopub.status.busy":"2021-12-07T06:43:26.227292Z","iopub.execute_input":"2021-12-07T06:43:26.227932Z","iopub.status.idle":"2021-12-07T06:43:26.237702Z","shell.execute_reply.started":"2021-12-07T06:43:26.227869Z","shell.execute_reply":"2021-12-07T06:43:26.236298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_vit_classifier():\n    inputs = layers.Input(shape=input_shape)\n    # Augment data.\n    augmented = data_augmentation(inputs)\n    # Create patches.\n    patches = Patches(patch_size)(augmented)\n    # Encode patches.\n    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n\n    # Create multiple layers of the Transformer block.\n    for _ in range(transformer_layers):\n        # Layer normalization 1.\n        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n        # Create a multi-head attention layer.\n        attention_output = layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n        )(x1, x1)\n        # Skip connection 1.\n        x2 = layers.Add()([attention_output, encoded_patches])\n        # Layer normalization 2.\n        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n        # MLP.\n        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n        # Skip connection 2.\n        encoded_patches = layers.Add()([x3, x2])\n\n    # Create a [batch_size, projection_dim] tensor.\n    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n    representation = layers.Flatten()(representation)\n    representation = layers.Dropout(0.5)(representation)\n    # Add MLP.\n    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n    # Classify outputs.\n    logits = layers.Dense(num_classes)(features)\n    # Create the Keras model.\n    model = keras.Model(inputs=inputs, outputs=logits)\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-12-07T06:43:26.239172Z","iopub.execute_input":"2021-12-07T06:43:26.239722Z","iopub.status.idle":"2021-12-07T06:43:26.256768Z","shell.execute_reply.started":"2021-12-07T06:43:26.239609Z","shell.execute_reply":"2021-12-07T06:43:26.255367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run_experiment(model):\n    optimizer = tfa.optimizers.AdamW(\n        learning_rate=learning_rate, weight_decay=weight_decay\n    )\n\n    model.compile(\n        optimizer=optimizer,\n        loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n        metrics=[\n            keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n            keras.metrics.TopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n        ],\n    )\n\n  #\n    history = model.fit(\n        x=x_train,\n        y=y_train,\n        batch_size=batch_size,\n        epochs=num_epochs,\n        validation_split=0.1)\n        #callbacks=[checkpoint_callback],\n    \n\n   #\n    return history\n\n\nvit_classifier = create_vit_classifier()\nhistory = run_experiment(vit_classifier)","metadata":{"execution":{"iopub.status.busy":"2021-12-07T06:47:37.984805Z","iopub.execute_input":"2021-12-07T06:47:37.98526Z","iopub.status.idle":"2021-12-07T06:48:33.2499Z","shell.execute_reply.started":"2021-12-07T06:47:37.985226Z","shell.execute_reply":"2021-12-07T06:48:33.248824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#predict on the submission data\ncnn_pred = tf.math.argmax(history.model.predict(X_submission),axis=1)*10\nprint(cnn_pred)\nprint(X_submission.shape, type(X_submission))\nprint(cnn_pred.shape, type(cnn_pred))","metadata":{"execution":{"iopub.status.busy":"2021-12-07T06:51:02.971708Z","iopub.execute_input":"2021-12-07T06:51:02.972139Z","iopub.status.idle":"2021-12-07T06:51:03.045266Z","shell.execute_reply.started":"2021-12-07T06:51:02.972093Z","shell.execute_reply":"2021-12-07T06:51:03.044273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#put the submission predictions alongside their associated Ids\ncnn = pd.DataFrame()\ncnn['Id'] = test['Id']\ncnn['Pawpularity'] = cnn_pred\ncnn.to_csv('submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-07T06:51:21.975127Z","iopub.execute_input":"2021-12-07T06:51:21.975466Z","iopub.status.idle":"2021-12-07T06:51:21.98649Z","shell.execute_reply.started":"2021-12-07T06:51:21.975425Z","shell.execute_reply":"2021-12-07T06:51:21.984655Z"},"trusted":true},"execution_count":null,"outputs":[]}]}