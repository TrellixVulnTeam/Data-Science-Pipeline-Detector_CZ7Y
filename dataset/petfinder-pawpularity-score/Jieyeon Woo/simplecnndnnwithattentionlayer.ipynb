{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import Useful Packages","metadata":{}},{"cell_type":"code","source":"# Basic packages\nimport pandas as pd\nimport numpy as np\nimport os\n# Analysis visualization packages\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Tensorflow packages for model building\nimport tensorflow as tf\nfrom tensorflow.keras.utils import Sequence\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, Conv2D, BatchNormalization, MaxPooling2D, Flatten, Concatenate\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, CSVLogger, TensorBoard\nfrom tensorflow.keras import backend as K\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2022-01-01T23:51:26.605414Z","iopub.execute_input":"2022-01-01T23:51:26.605946Z","iopub.status.idle":"2022-01-01T23:51:33.742139Z","shell.execute_reply.started":"2022-01-01T23:51:26.605844Z","shell.execute_reply":"2022-01-01T23:51:33.74134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Enable GPU Use","metadata":{}},{"cell_type":"code","source":"# Find GPU device\nphysical_device = tf.config.experimental.list_physical_devices('GPU')\nprint(f'Device found : {physical_device}')\n# If there is more than 1 visible GPU on the host\nif (len(physical_device) >= 1):\n    # Check if GPU is on used for training or not\n    if (tf.config.experimental.get_memory_growth(physical_device[0]) == 1):\n        # If the check returns False or nothing => set GPU for training\n        tf.config.experimental.set_memory_growth(physical_device[0],True)","metadata":{"execution":{"iopub.status.busy":"2022-01-01T23:51:33.743545Z","iopub.execute_input":"2022-01-01T23:51:33.743795Z","iopub.status.idle":"2022-01-01T23:51:33.755458Z","shell.execute_reply.started":"2022-01-01T23:51:33.743764Z","shell.execute_reply":"2022-01-01T23:51:33.754354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import Dataset from CSV files","metadata":{}},{"cell_type":"code","source":"dir_csv = '../input/petfinder-pawpularity-score/'\n\ntrain_df = pd.read_csv(dir_csv+'train.csv')\ntest_df = pd.read_csv(dir_csv+'test.csv')\n\n# Check if there are NaN values in the train & test dataset\nprint('Train dataset has NaN values: ', train_df.isnull().values.any())\nprint('Test dataset has NaN values: ', test_df.isnull().values.any())","metadata":{"execution":{"iopub.status.busy":"2022-01-01T23:51:33.757167Z","iopub.execute_input":"2022-01-01T23:51:33.758822Z","iopub.status.idle":"2022-01-01T23:51:33.833909Z","shell.execute_reply.started":"2022-01-01T23:51:33.758766Z","shell.execute_reply":"2022-01-01T23:51:33.832848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Analyze Train dataset","metadata":{}},{"cell_type":"markdown","source":"## Visualize Train dataset","metadata":{}},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-01T23:51:33.836486Z","iopub.execute_input":"2022-01-01T23:51:33.836842Z","iopub.status.idle":"2022-01-01T23:51:33.859242Z","shell.execute_reply.started":"2022-01-01T23:51:33.836798Z","shell.execute_reply":"2022-01-01T23:51:33.858388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualize Correlation between Train dataset labels","metadata":{}},{"cell_type":"code","source":"corr_train_df = train_df.corr()\nplt.figure(figsize=(14, 8))\nsns.set(font_scale=1)\nax = sns.heatmap(corr_train_df,\n        vmin=-1, vmax=1, annot=True, linewidths=.5,\n        xticklabels=corr_train_df.columns,\n        yticklabels=corr_train_df.columns)\nax.set_ylim(len(corr_train_df.keys()),0)","metadata":{"execution":{"iopub.status.busy":"2022-01-01T23:51:33.860644Z","iopub.execute_input":"2022-01-01T23:51:33.860897Z","iopub.status.idle":"2022-01-01T23:51:35.101545Z","shell.execute_reply.started":"2022-01-01T23:51:33.860866Z","shell.execute_reply":"2022-01-01T23:51:35.100564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualize Correlation for Pawpularity ","metadata":{}},{"cell_type":"code","source":"corr_train_df = train_df.corr()\nplt.figure(figsize=(10, 8))\nsns.set(font_scale=1)\nax = sns.heatmap(corr_train_df[['Pawpularity']],\n        vmin=-1, vmax=1, annot=True, linewidths=.5,\n        xticklabels=['Pawpularity'],\n        yticklabels=corr_train_df.columns\n        )\nax.set_ylim(len(corr_train_df.keys()),0)","metadata":{"execution":{"iopub.status.busy":"2022-01-01T23:51:35.103059Z","iopub.execute_input":"2022-01-01T23:51:35.103301Z","iopub.status.idle":"2022-01-01T23:51:35.476043Z","shell.execute_reply.started":"2022-01-01T23:51:35.103274Z","shell.execute_reply":"2022-01-01T23:51:35.475043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Remarks\nNot all features are positively correlated with our output feature(i.e. pawpularity).\n\n### Solution: Attention layer\nWe will later add an attention layer to our model so that our model can learn to select interesting features and learn from them.","metadata":{}},{"cell_type":"markdown","source":"### Pawpularity Min&Max Values","metadata":{}},{"cell_type":"code","source":"print('Min value of pawpularity: ', train_df['Pawpularity'].values.min())\nprint('Max value of pawpularity: ', train_df['Pawpularity'].values.max())","metadata":{"execution":{"iopub.status.busy":"2022-01-01T23:51:35.477742Z","iopub.execute_input":"2022-01-01T23:51:35.47828Z","iopub.status.idle":"2022-01-01T23:51:35.486781Z","shell.execute_reply.started":"2022-01-01T23:51:35.478233Z","shell.execute_reply":"2022-01-01T23:51:35.485837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualize Pawpularity(ylabel) distribution","metadata":{}},{"cell_type":"code","source":"%matplotlib inline\ntrain_df['Pawpularity'].plot(kind=\"hist\", bins=100)","metadata":{"execution":{"iopub.status.busy":"2022-01-01T23:51:35.487845Z","iopub.execute_input":"2022-01-01T23:51:35.488594Z","iopub.status.idle":"2022-01-01T23:51:35.935732Z","shell.execute_reply.started":"2022-01-01T23:51:35.488555Z","shell.execute_reply":"2022-01-01T23:51:35.935024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Remarks\nWe can see from the distribution is our data is imbalanced.\n\n### Solution: Oversampling\nTo address this problem of imbalanced data, we can resample the dataset by oversampling the minority classes.\nHere we can say that each pawpularity score(i.e. integers that ranges from 1 to 100) is an individual class. Thus, we will oversample the minority classes so that we have the same distribution.","metadata":{}},{"cell_type":"markdown","source":"## Count number of occurence for each pawpularity class","metadata":{}},{"cell_type":"code","source":"# Number of occurence sorted by pawpularity class\ntrain_df['Pawpularity'].value_counts().sort_index()","metadata":{"execution":{"iopub.status.busy":"2022-01-01T23:51:35.937037Z","iopub.execute_input":"2022-01-01T23:51:35.937238Z","iopub.status.idle":"2022-01-01T23:51:35.95034Z","shell.execute_reply.started":"2022-01-01T23:51:35.937213Z","shell.execute_reply":"2022-01-01T23:51:35.949408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Number of occurence sorted by the highest number of occurence\ntrain_df['Pawpularity'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-01-01T23:51:35.95321Z","iopub.execute_input":"2022-01-01T23:51:35.953477Z","iopub.status.idle":"2022-01-01T23:51:35.965138Z","shell.execute_reply.started":"2022-01-01T23:51:35.953436Z","shell.execute_reply":"2022-01-01T23:51:35.964294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Remarks\nWe can note that the dominating classes are pawpularity=28 and pawpularity=30 with the number of occurence of 318.\nThe remaining classes will be oversampled to the same number of occurence of 318.","metadata":{}},{"cell_type":"markdown","source":"## Split Train dataset into Training and Validation datasets\nBefore oversampling split the Train dataset into Training and Validation datasets for the training so the same samples are not seen in the validation dataset (for generalization)\n\nRatio of 0.9 for Train and 0.1 for Validation","metadata":{}},{"cell_type":"code","source":"tr_df, val_df = train_test_split(train_df, test_size=0.1, random_state=2)\nprint(tr_df.shape)\nprint(val_df.shape)","metadata":{"execution":{"iopub.status.busy":"2022-01-01T23:51:35.966732Z","iopub.execute_input":"2022-01-01T23:51:35.967043Z","iopub.status.idle":"2022-01-01T23:51:35.981076Z","shell.execute_reply.started":"2022-01-01T23:51:35.967003Z","shell.execute_reply":"2022-01-01T23:51:35.98011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualize Training dataset after data splitting","metadata":{}},{"cell_type":"code","source":"tr_df['Pawpularity'].plot(kind=\"hist\", bins=100)","metadata":{"execution":{"iopub.status.busy":"2022-01-01T23:51:35.983718Z","iopub.execute_input":"2022-01-01T23:51:35.984058Z","iopub.status.idle":"2022-01-01T23:51:36.413343Z","shell.execute_reply.started":"2022-01-01T23:51:35.984023Z","shell.execute_reply":"2022-01-01T23:51:36.412412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# New data fram for sampled train dataset\nsampled_tr_df = pd.DataFrame(columns=tr_df.keys())","metadata":{"execution":{"iopub.status.busy":"2022-01-01T23:51:36.414635Z","iopub.execute_input":"2022-01-01T23:51:36.414878Z","iopub.status.idle":"2022-01-01T23:51:36.422255Z","shell.execute_reply.started":"2022-01-01T23:51:36.414848Z","shell.execute_reply":"2022-01-01T23:51:36.421271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# number of max occurence\nmax_occ = tr_df['Pawpularity'].value_counts().max()\n\nfor class_i in range(1,101):\n    # If the class is not the dominating class (i.e. number of occurence <= max_occ), do oversampling\n    # and append to sampled_tr_df\n    if(tr_df[tr_df['Pawpularity'] == class_i]['Pawpularity'].value_counts().values[0] < max_occ):\n        ids_class_i = tr_df.index[tr_df['Pawpularity'] == class_i].tolist()\n        sampled_ids_class_i = np.random.choice(ids_class_i, max_occ)\n        sampled_tr_df = pd.concat([sampled_tr_df, tr_df.loc[sampled_ids_class_i]])\n    # If it is the dominating class, directly append to sampled_tr_df\n    else:\n        ids_class_i = tr_df.index[tr_df['Pawpularity'] == class_i].tolist()\n#         sampled_tr_df = sampled_tr_df.append(tr_df.loc[ids_class_i])\n        sampled_tr_df = pd.concat([sampled_tr_df, tr_df.loc[ids_class_i]])\n        \n# Reindex sampled_tr_df\nsampled_tr_df = sampled_tr_df.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-01T23:51:36.423505Z","iopub.execute_input":"2022-01-01T23:51:36.423742Z","iopub.status.idle":"2022-01-01T23:51:36.853831Z","shell.execute_reply.started":"2022-01-01T23:51:36.423713Z","shell.execute_reply":"2022-01-01T23:51:36.852741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualize the Sampled Pawpularity(ylabel) distribution","metadata":{}},{"cell_type":"code","source":"sampled_tr_df['Pawpularity'].plot(kind=\"hist\", bins=100)","metadata":{"execution":{"iopub.status.busy":"2022-01-01T23:51:36.85525Z","iopub.execute_input":"2022-01-01T23:51:36.855574Z","iopub.status.idle":"2022-01-01T23:51:37.506802Z","shell.execute_reply.started":"2022-01-01T23:51:36.855538Z","shell.execute_reply":"2022-01-01T23:51:37.505888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dataframe info\nprint('tr_df:', tr_df.info())\nprint('sampled_tr_df:', sampled_tr_df.info())","metadata":{"execution":{"iopub.status.busy":"2022-01-01T23:51:37.508304Z","iopub.execute_input":"2022-01-01T23:51:37.509198Z","iopub.status.idle":"2022-01-01T23:51:37.554143Z","shell.execute_reply.started":"2022-01-01T23:51:37.509145Z","shell.execute_reply":"2022-01-01T23:51:37.553193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Change data type (i.e. dtype) of all features except 'Id' to int64 \nfor key in sampled_tr_df.keys()[1:]:\n    sampled_tr_df[key] = sampled_tr_df[key].astype('int64')\nprint('sampled_tr_df:', sampled_tr_df.info())","metadata":{"execution":{"iopub.status.busy":"2022-01-01T23:51:37.556078Z","iopub.execute_input":"2022-01-01T23:51:37.556446Z","iopub.status.idle":"2022-01-01T23:51:37.613562Z","shell.execute_reply.started":"2022-01-01T23:51:37.556401Z","shell.execute_reply":"2022-01-01T23:51:37.61248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Generation\nAs the previously loaded datasets(i.e. train and test datasets from the CSV files) do not contain the image data(i.e. only image ids), a custom data generation class could be created. In the custom data generation class, the data will be split into batches, to enable batch training, and the image data of the corresponding image id will be generated along side the other feature data.","metadata":{}},{"cell_type":"markdown","source":"## Training Data Generator","metadata":{}},{"cell_type":"code","source":"class CustomTrainDataGen(Sequence):\n    \n    def __init__(self, df, X_col, y_col,\n                 batch_size,\n                 input_size=(250, 250, 3),\n                 shuffle=True): \n        self.df = df.copy()\n        self.X_col = X_col\n        self.y_col = y_col\n        self.batch_size = batch_size\n        self.input_size = input_size\n        self.list_IDs = np.arange(len(self.df.index))\n        self.indexes = np.arange(len(self.df.index))\n        self.shuffle = shuffle \n        self.n = len(self.df)\n    \n    def on_epoch_end(self):\n        if self.shuffle:\n            self.df = self.df.sample(frac=1).reset_index(drop=True)\n    \n    def __get_input(self, path, target_size):\n        # Check if train dataset: train_df\n        if (len(self.df.keys()) == len(train_df.keys())):\n            img_path = dir_csv+'train/'+str(path)+'.jpg'\n        else:\n            print(\"Generator Image Data Generation Error\")\n            return -1\n        image = tf.keras.preprocessing.image.load_img(img_path)\n        image_arr = tf.keras.preprocessing.image.img_to_array(image)\n        image_arr = tf.image.resize(image_arr,(target_size[0], target_size[1])).numpy()\n\n        return image_arr/255.\n    \n    def __get_data(self, batches):\n        # Generates data containing batch_size samples\n        subfocus_batch = self.df.iloc[batches, :]['Subject Focus'].values\n        eyes_batch = self.df.iloc[batches, :]['Eyes'].values\n        face_batch = self.df.iloc[batches, :]['Face'].values\n        near_batch = self.df.iloc[batches, :]['Near'].values\n        action_batch = self.df.iloc[batches, :]['Action'].values\n        acc_batch = self.df.iloc[batches, :]['Accessory'].values\n        group_batch = self.df.iloc[batches, :]['Group'].values\n        collage_batch = self.df.iloc[batches, :]['Collage'].values\n        human_batch = self.df.iloc[batches, :]['Human'].values\n        occlusion_batch = self.df.iloc[batches, :]['Occlusion'].values\n        info_batch = self.df.iloc[batches, :]['Info'].values\n        blur_batch = self.df.iloc[batches, :]['Blur'].values\n        # Resize to (self.batch_size, 1)\n        subfocus_batch = np.expand_dims(subfocus_batch, axis=1)\n        eyes_batch = np.expand_dims(eyes_batch, axis=1)\n        face_batch = np.expand_dims(face_batch, axis=1)\n        near_batch = np.expand_dims(near_batch, axis=1)\n        action_batch = np.expand_dims(action_batch, axis=1)\n        acc_batch = np.expand_dims(acc_batch, axis=1)\n        group_batch = np.expand_dims(group_batch, axis=1)\n        collage_batch = np.expand_dims(collage_batch, axis=1)\n        human_batch = np.expand_dims(human_batch, axis=1)\n        occlusion_batch = np.expand_dims(occlusion_batch, axis=1)\n        info_batch = np.expand_dims(info_batch, axis=1)\n        blur_batch = np.expand_dims(blur_batch, axis=1)\n\n        id_batch = self.df.iloc[batches, :]['Id'].values      \n        image_batch = np.asarray([self.__get_input(id, self.input_size) for id\\\n             in id_batch])\n        # Reshape to (self.batch_size, input_shape[0]*input_shape[1]*input_shape[2])\n        image_batch = np.reshape(image_batch,(self.batch_size,-1))\n        \n        pawpularity_batch = self.df.iloc[batches, :]['Pawpularity'].values\n#         # Convert pawpularity that ranges from 0 to 100 to a range of 0 to 1\n#         pawpularity_batch = pawpularity_batch / 100\n\n        X_batch = np.concatenate((subfocus_batch, eyes_batch, face_batch, near_batch,\\\n            action_batch, acc_batch, group_batch, collage_batch, human_batch,\\\n               occlusion_batch, info_batch, blur_batch, image_batch),axis=1)\n        y_batch = pawpularity_batch\n        return X_batch, y_batch\n    \n    def __getitem__(self, index):\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]    \n        X, y = self.__get_data(indexes)  \n        return X, y\n    \n    def __len__(self):\n        return self.n // self.batch_size","metadata":{"execution":{"iopub.status.busy":"2022-01-01T23:51:37.614895Z","iopub.execute_input":"2022-01-01T23:51:37.615138Z","iopub.status.idle":"2022-01-01T23:51:37.63466Z","shell.execute_reply.started":"2022-01-01T23:51:37.615107Z","shell.execute_reply":"2022-01-01T23:51:37.633621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Testing Data Generator\nLikewise a data generator for the testing is needed.","metadata":{}},{"cell_type":"code","source":"class CustomTestDataGen(Sequence):\n    \n    def __init__(self, df, X_col,\n                 batch_size,\n                 input_size=(250, 250, 3),\n                 shuffle=True): \n        self.df = df.copy()\n        self.X_col = X_col\n        self.batch_size = batch_size\n        self.input_size = input_size\n        self.list_IDs = np.arange(len(self.df.index))\n        self.indexes = np.arange(len(self.df.index))\n        self.shuffle = shuffle \n        self.n = len(self.df)\n    \n    def on_epoch_end(self):\n        if self.shuffle:\n            self.df = self.df.sample(frac=1).reset_index(drop=True)\n    \n    def __get_input(self, path, target_size):\n        # Check if test dataset: test_df\n        if (len(self.df.keys()) == len(test_df.keys())):\n            img_path = dir_csv+'test/'+str(path)+'.jpg'\n        else:\n            print(\"Generator Image Data Generation Error\")\n            return -1\n        image = tf.keras.preprocessing.image.load_img(img_path)\n        image_arr = tf.keras.preprocessing.image.img_to_array(image)\n        image_arr = tf.image.resize(image_arr,(target_size[0], target_size[1])).numpy()\n\n        return image_arr/255.\n    \n    def __get_data(self, batches):\n        # Generates data containing batch_size samples\n        subfocus_batch = self.df.iloc[batches, :]['Subject Focus'].values\n        eyes_batch = self.df.iloc[batches, :]['Eyes'].values\n        face_batch = self.df.iloc[batches, :]['Face'].values\n        near_batch = self.df.iloc[batches, :]['Near'].values\n        action_batch = self.df.iloc[batches, :]['Action'].values\n        acc_batch = self.df.iloc[batches, :]['Accessory'].values\n        group_batch = self.df.iloc[batches, :]['Group'].values\n        collage_batch = self.df.iloc[batches, :]['Collage'].values\n        human_batch = self.df.iloc[batches, :]['Human'].values\n        occlusion_batch = self.df.iloc[batches, :]['Occlusion'].values\n        info_batch = self.df.iloc[batches, :]['Info'].values\n        blur_batch = self.df.iloc[batches, :]['Blur'].values\n\n        # Resize to (self.batch_size, 1)\n        subfocus_batch = np.expand_dims(subfocus_batch, axis=1)\n        eyes_batch = np.expand_dims(eyes_batch, axis=1)\n        face_batch = np.expand_dims(face_batch, axis=1)\n        near_batch = np.expand_dims(near_batch, axis=1)\n        action_batch = np.expand_dims(action_batch, axis=1)\n        acc_batch = np.expand_dims(acc_batch, axis=1)\n        group_batch = np.expand_dims(group_batch, axis=1)\n        collage_batch = np.expand_dims(collage_batch, axis=1)\n        human_batch = np.expand_dims(human_batch, axis=1)\n        occlusion_batch = np.expand_dims(occlusion_batch, axis=1)\n        info_batch = np.expand_dims(info_batch, axis=1)\n        blur_batch = np.expand_dims(blur_batch, axis=1)\n        id_batch = self.df.iloc[batches, :]['Id'].values      \n        image_batch = np.asarray([self.__get_input(id, self.input_size) for id\\\n             in id_batch])\n        # Reshape to (self.batch_size, input_shape[0]*input_shape[1]*input_shape[2])\n        image_batch = np.reshape(image_batch,(self.batch_size,-1))\n\n        X_batch = np.concatenate((subfocus_batch, eyes_batch, face_batch, near_batch,\\\n            action_batch, acc_batch, group_batch, collage_batch, human_batch,\\\n               occlusion_batch, info_batch, blur_batch, image_batch),axis=1)\n        return X_batch\n    \n    def __getitem__(self, index):\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]    \n        X = self.__get_data(indexes)  \n        return X\n    \n    def __len__(self):\n        return self.n // self.batch_size","metadata":{"execution":{"iopub.status.busy":"2022-01-01T23:51:37.636423Z","iopub.execute_input":"2022-01-01T23:51:37.636749Z","iopub.status.idle":"2022-01-01T23:51:37.656523Z","shell.execute_reply.started":"2022-01-01T23:51:37.636713Z","shell.execute_reply":"2022-01-01T23:51:37.655517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"markdown","source":"## Build model\nWe will build a very simple model(i.e. CNN + DNN + Attention layer).\nThe model embeds the image data with:\n- 3 convolution layers(i.e. Conv2D => BN => MaxPool)\n- 1 dense layer\nThen the other feature data is concatenated with the image embedding.\nAn attention layer is applied and the output is created with a linear activation.","metadata":{}},{"cell_type":"code","source":"def build_model(nb_annotations, image_shape):\n\n    # Our input features of 12 annotations and corresponding image\n    input_shape = nb_annotations + image_shape[0]*image_shape[1]*image_shape[2]\n    inputs = Input(shape=input_shape)\n\n    annotations_input = inputs[:,:nb_annotations]\n    img_input = inputs[:,nb_annotations:]\n    # Reshape flattened image to original image_shape\n    img_input = tf.reshape(img_input,(tf.shape(inputs)[0],image_shape[0],image_shape[1],image_shape[2]))\n\n    # 3 convolution layers\n    x = Conv2D(16, 3, activation='relu')(img_input)\n    x = BatchNormalization(axis=-1)(x)\n    x = MaxPooling2D(2)(x)\n\n    x = Conv2D(32, 3, activation='relu')(x)\n    x = BatchNormalization(axis=-1)(x)\n    x = MaxPooling2D(2)(x)\n\n    x = Conv2D(64, 3, activation='relu')(x)\n    x = BatchNormalization(axis=-1)(x)\n    x = MaxPooling2D(2)(x)\n\n    # Flatten feature map to a 1-dim tensor so we can add fully connected layers\n    x = Flatten()(x)\n\n    # Create a fully connected layer with ReLU activation\n    x = Dense(16, activation='relu')(x)\n    x = BatchNormalization(axis=-1)(x)\n\n    # Concatenate 12 annotation inputs and image embedding\n    x = Concatenate()([annotations_input, x])\n\n    # Simple attention layer\n    attention = Dense(32, activation='relu')(x)\n    attention = Dense(28, activation='softmax')(attention)\n    x = attention*x\n\n    # Create output layer with a single node and linear activation\n    output = Dense(1, activation='linear')(x)\n\n    # Create model\n    model = Model(inputs=inputs, outputs=output)\n    \n    # Compile model\n    model.compile(loss='mse', optimizer='adam', metrics=[tf.keras.metrics.RootMeanSquaredError()])\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2022-01-01T23:51:37.658061Z","iopub.execute_input":"2022-01-01T23:51:37.65849Z","iopub.status.idle":"2022-01-01T23:51:37.672998Z","shell.execute_reply.started":"2022-01-01T23:51:37.658444Z","shell.execute_reply":"2022-01-01T23:51:37.672342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train Model","metadata":{}},{"cell_type":"markdown","source":"### Model training parameters","metadata":{}},{"cell_type":"code","source":"num_epochs = 100\nbatch_size = 32\ntarget_size = (250, 250, 3)\nannotations = ['Subject Focus', 'Eyes', 'Face', 'Near', 'Action',\\\n             'Accessory', 'Group', 'Collage', 'Human', 'Occlusion', 'Info',\\\n                  'Blur']","metadata":{"execution":{"iopub.status.busy":"2022-01-01T23:51:37.674867Z","iopub.execute_input":"2022-01-01T23:51:37.675925Z","iopub.status.idle":"2022-01-01T23:51:37.688375Z","shell.execute_reply.started":"2022-01-01T23:51:37.675881Z","shell.execute_reply":"2022-01-01T23:51:37.687504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Build model","metadata":{}},{"cell_type":"code","source":"# IF CPU or GPU\nmodel = build_model(len(annotations), target_size)","metadata":{"execution":{"iopub.status.busy":"2022-01-01T23:51:37.689522Z","iopub.execute_input":"2022-01-01T23:51:37.690272Z","iopub.status.idle":"2022-01-01T23:51:37.963545Z","shell.execute_reply.started":"2022-01-01T23:51:37.690236Z","shell.execute_reply":"2022-01-01T23:51:37.962629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualize model","metadata":{}},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-01-01T23:51:37.964973Z","iopub.execute_input":"2022-01-01T23:51:37.96522Z","iopub.status.idle":"2022-01-01T23:51:37.982253Z","shell.execute_reply.started":"2022-01-01T23:51:37.965189Z","shell.execute_reply":"2022-01-01T23:51:37.981566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Generate data for Train and Validation datasets","metadata":{}},{"cell_type":"code","source":"# Train data\ntraingen = CustomTrainDataGen(sampled_tr_df,\n                         X_col={'Id':'Id',\n                         'Subject Focus':'Subject Focus',\n                         'Eyes':'Eyes',\n                         'Face':'Face',\n                         'Near':'Near',\n                         'Action':'Action',\n                         'Accessory':'Accessory',\n                         'Group':'Group',\n                         'Collage':'Collage',\n                         'Human':'Human',\n                         'Occlusion':'Occlusion',\n                         'Info':'Info',\n                         'Blur':'Blur'},\n                         y_col={'Pawpularity': 'Pawpularity'},\n                         batch_size=batch_size,\n                         input_size=target_size)\n# Validation data\nvalgen = CustomTrainDataGen(val_df,\n                       X_col={'Id':'Id',\n                         'Subject Focus':'Subject Focus',\n                         'Eyes':'Eyes',\n                         'Face':'Face',\n                         'Near':'Near',\n                         'Action':'Action',\n                         'Accessory':'Accessory',\n                         'Group':'Group',\n                         'Collage':'Collage',\n                         'Human':'Human',\n                         'Occlusion':'Occlusion',\n                         'Info':'Info',\n                         'Blur':'Blur'},\n                       y_col={'Pawpularity': 'Pawpularity'},\n                       batch_size=batch_size,\n                       input_size=target_size)","metadata":{"execution":{"iopub.status.busy":"2022-01-01T23:51:37.983245Z","iopub.execute_input":"2022-01-01T23:51:37.983499Z","iopub.status.idle":"2022-01-01T23:51:38.001866Z","shell.execute_reply.started":"2022-01-01T23:51:37.983469Z","shell.execute_reply":"2022-01-01T23:51:38.001075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Callbacks\nCallbacks used for model training:\n- ReduceLROnPlateau: automatically reduce the learning rate during the training\n- EarlyStopping: automatically stops the training when it doesn't learn anymore\n- ModelCheckpoint: saves the weights as the model trains (here we only save the weights of the best model)\n- CSVLogger: saves the logs as a CSV file\n- Tensorboard: saves the logs for tensorboard visualization","metadata":{}},{"cell_type":"code","source":"reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.001)\n\nearly_stop = EarlyStopping(monitor='val_loss', patience=30)\n\ndir_path_batchtr = './Train_logs'\nos.makedirs(dir_path_batchtr, exist_ok=True)\n\n# Checkpoint\ndir_weight_path_batchtr = dir_path_batchtr + '/Weights'\nos.makedirs(dir_weight_path_batchtr, exist_ok=True)\ncheckpoint_name = dir_weight_path_batchtr + '/weights_best.hdf5'\ncheckpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True, mode='min')\n\n# Logger for history\ndir_hist_path_batchtr = dir_path_batchtr + '/Histories'\nos.makedirs(dir_hist_path_batchtr, exist_ok=True)\nlogger_name = dir_hist_path_batchtr + '/history_log.csv'\nlogger = CSVLogger(logger_name, append=True, separator=',')\n\n# Tensorboard log\ndir_tensorboard_log = \"./tensorboard_logs\"\nos.makedirs(dir_tensorboard_log, exist_ok=True)\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=dir_tensorboard_log)","metadata":{"execution":{"iopub.status.busy":"2022-01-01T23:51:38.003037Z","iopub.execute_input":"2022-01-01T23:51:38.003388Z","iopub.status.idle":"2022-01-01T23:51:38.015976Z","shell.execute_reply.started":"2022-01-01T23:51:38.003314Z","shell.execute_reply":"2022-01-01T23:51:38.015218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training","metadata":{}},{"cell_type":"code","source":"# num_epochs = 100 - 63\n\n# checkpoint_name = '../input/version2/weights_best.hdf5'\n# model.load_weights(checkpoint_name)","metadata":{"execution":{"iopub.status.busy":"2022-01-01T23:51:38.017448Z","iopub.execute_input":"2022-01-01T23:51:38.017705Z","iopub.status.idle":"2022-01-01T23:51:38.039266Z","shell.execute_reply.started":"2022-01-01T23:51:38.017664Z","shell.execute_reply":"2022-01-01T23:51:38.038569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # IF CPU or GPU\n# model.fit(traingen,\n#           validation_data=valgen,\n#           epochs=num_epochs,\n#           use_multiprocessing=False,\n#           callbacks=[reduce_lr,early_stop,checkpoint,logger,tensorboard_callback],\n#           verbose=2)","metadata":{"execution":{"iopub.status.busy":"2022-01-01T23:51:38.041306Z","iopub.execute_input":"2022-01-01T23:51:38.042155Z","iopub.status.idle":"2022-01-01T23:51:38.051129Z","shell.execute_reply.started":"2022-01-01T23:51:38.042116Z","shell.execute_reply":"2022-01-01T23:51:38.050117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load pretrained weights","metadata":{}},{"cell_type":"code","source":"checkpoint_name = '../input/version21/weights_best.hdf5'\nmodel.load_weights(checkpoint_name)","metadata":{"execution":{"iopub.status.busy":"2022-01-01T23:51:38.054382Z","iopub.execute_input":"2022-01-01T23:51:38.054685Z","iopub.status.idle":"2022-01-01T23:51:38.266708Z","shell.execute_reply.started":"2022-01-01T23:51:38.05465Z","shell.execute_reply":"2022-01-01T23:51:38.265728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Generate data for Test dataset","metadata":{}},{"cell_type":"code","source":"testgen = CustomTestDataGen(test_df,\n                         X_col={'Id':'Id',\n                         'Subject Focus':'Subject Focus',\n                         'Eyes':'Eyes',\n                         'Face':'Face',\n                         'Near':'Near',\n                         'Action':'Action',\n                         'Accessory':'Accessory',\n                         'Group':'Group',\n                         'Collage':'Collage',\n                         'Human':'Human',\n                         'Occlusion':'Occlusion',\n                         'Info':'Info',\n                         'Blur':'Blur'},\n                         batch_size=1,\n                         input_size=target_size,\n                         shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-01T23:51:38.27029Z","iopub.execute_input":"2022-01-01T23:51:38.270563Z","iopub.status.idle":"2022-01-01T23:51:38.276613Z","shell.execute_reply.started":"2022-01-01T23:51:38.270534Z","shell.execute_reply":"2022-01-01T23:51:38.275727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Predict","metadata":{}},{"cell_type":"code","source":"predictions = model.predict(testgen)","metadata":{"execution":{"iopub.status.busy":"2022-01-01T23:51:38.278121Z","iopub.execute_input":"2022-01-01T23:51:38.278453Z","iopub.status.idle":"2022-01-01T23:51:38.949078Z","shell.execute_reply.started":"2022-01-01T23:51:38.278409Z","shell.execute_reply":"2022-01-01T23:51:38.948403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Export prediction as a CSV file","metadata":{}},{"cell_type":"code","source":"# New dataframe for predictions with the id from test_df\npred_df = pd.DataFrame({'Id':test_df['Id']})\npred_df['Pawpularity'] = predictions\n\n# Save as a CSV file\npred_df.to_csv('./submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-01T23:51:38.950828Z","iopub.execute_input":"2022-01-01T23:51:38.951079Z","iopub.status.idle":"2022-01-01T23:51:38.960649Z","shell.execute_reply.started":"2022-01-01T23:51:38.951048Z","shell.execute_reply":"2022-01-01T23:51:38.959765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Recheck with validation data","metadata":{}},{"cell_type":"code","source":"valgen_test = CustomTrainDataGen(val_df,\n                       X_col={'Id':'Id',\n                         'Subject Focus':'Subject Focus',\n                         'Eyes':'Eyes',\n                         'Face':'Face',\n                         'Near':'Near',\n                         'Action':'Action',\n                         'Accessory':'Accessory',\n                         'Group':'Group',\n                         'Collage':'Collage',\n                         'Human':'Human',\n                         'Occlusion':'Occlusion',\n                         'Info':'Info',\n                         'Blur':'Blur'},\n                       y_col={'Pawpularity': 'Pawpularity'},\n                       batch_size=1,\n                       input_size=target_size,\n                       shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-01T23:51:38.962038Z","iopub.execute_input":"2022-01-01T23:51:38.962862Z","iopub.status.idle":"2022-01-01T23:51:38.970126Z","shell.execute_reply.started":"2022-01-01T23:51:38.962823Z","shell.execute_reply":"2022-01-01T23:51:38.969238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = model.predict(valgen_test)\nprint(predictions[:10])\nprint(val_df.iloc[:10]['Pawpularity'])","metadata":{"execution":{"iopub.status.busy":"2022-01-01T23:51:38.971421Z","iopub.execute_input":"2022-01-01T23:51:38.971907Z","iopub.status.idle":"2022-01-01T23:52:11.519929Z","shell.execute_reply.started":"2022-01-01T23:51:38.971868Z","shell.execute_reply":"2022-01-01T23:52:11.519308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}