{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**The learning code I used is the same as below.**\n\nhttps://www.kaggle.com/awsaf49/tf-petfinder-vit-cls-tpu-train\n\nHis notebook is very useful. Please keep that in mind.\n\nI modified only the build_model function.\n\nSeveral custom functions were used. -> Mish, Gradient Centralization\n\n**tf swin transformer**\n\nhttps://github.com/rishigami/Swin-Transformer-TF\n\nhttps://www.kaggle.com/rishigami/tpu-swin-transformer-tensorflow","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\n\nimport sys\nsys.path.append('../input/swintransformertf')\nfrom swintransformer import SwinTransformer","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-13T01:37:46.266665Z","iopub.execute_input":"2021-10-13T01:37:46.267002Z","iopub.status.idle":"2021-10-13T01:37:51.584561Z","shell.execute_reply.started":"2021-10-13T01:37:46.266971Z","shell.execute_reply":"2021-10-13T01:37:51.583525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    img_size = [384, 384]\n    model_name = 'swin_large_384'","metadata":{"execution":{"iopub.status.busy":"2021-10-13T01:37:51.586601Z","iopub.execute_input":"2021-10-13T01:37:51.586886Z","iopub.status.idle":"2021-10-13T01:37:51.591638Z","shell.execute_reply.started":"2021-10-13T01:37:51.586857Z","shell.execute_reply":"2021-10-13T01:37:51.590487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# define custom function\n**Gradient Centralization TensorFlow**\n\nhttps://github.com/Rishit-dagli/Gradient-Centralization-TensorFlow\n\nhttps://keras.io/examples/vision/gradient_centralization/\n\n**custom mish**\n\nhttps://www.kaggle.com/imokuri/mish-activation-function","metadata":{}},{"cell_type":"code","source":"#custom function\nclass Mish(tf.keras.layers.Activation):\n    def __init__(self, activation, **kwargs):\n        super(Mish, self).__init__(activation, **kwargs)\n        self.__name__ = 'Mish'\n\n\ndef mish(inputs):\n    return inputs * tf.math.tanh(tf.math.softplus(inputs))\n\ntf.keras.utils.get_custom_objects().update({'Mish': Mish(mish)})","metadata":{"execution":{"iopub.status.busy":"2021-10-13T01:39:16.622898Z","iopub.execute_input":"2021-10-13T01:39:16.623303Z","iopub.status.idle":"2021-10-13T01:39:16.629901Z","shell.execute_reply.started":"2021-10-13T01:39:16.623256Z","shell.execute_reply":"2021-10-13T01:39:16.629027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_centralized_gradients(optimizer, loss, params):\n    grads = []\n    for grad in K.gradients(loss, params):\n        grad_len = len(grad.shape)\n        if grad_len > 1:\n            axis = list(range(grad_len - 1))\n            grad -= tf.reduce_mean(grad,\n                                   axis=axis,\n                                   keep_dims=True)\n        grads.append(grad)\n\n    if None in grads:\n        raise ValueError('An operation has `None` for gradient. '\n                         'Please make sure that all of your ops have a '\n                         'gradient defined (i.e. are differentiable). '\n                         'Common ops without gradient: '\n                         'K.argmax, K.round, K.eval.')\n    if hasattr(optimizer, 'clipnorm') and optimizer.clipnorm > 0:\n        norm = K.sqrt(sum([K.sum(K.square(g)) for g in grads]))\n        grads = [\n            tf.keras.optimizers.clip_norm(\n                g,\n                optimizer.clipnorm,\n                norm) for g in grads]\n    if hasattr(optimizer, 'clipvalue') and optimizer.clipvalue > 0:\n        grads = [K.clip(g, -optimizer.clipvalue, optimizer.clipvalue)\n                 for g in grads]\n    return grads\n\n\ndef centralized_gradients_for_optimizer(optimizer):\n    def get_centralized_gradients_for_optimizer(loss, params):\n        return get_centralized_gradients(optimizer, loss, params)\n\n    return get_centralized_gradients_for_optimizer","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**If there is an error in the input layer, add the resize layer.**","metadata":{}},{"cell_type":"code","source":"def build_model(model_name=CFG.model_name, DIM=CFG.img_size[0], compile_model=True, include_top=False):\n    image_input = tf.keras.layers.Input(shape=(DIM,DIM,3))\n    #img_adjust_layer = tf.keras.layers.experimental.preprocessing.Resizing(DIM, DIM)\n    pretrained_model = SwinTransformer(CFG.model_name,include_top=False, pretrained=True, use_tpu=True)\n    base = tf.keras.Sequential([pretrained_model,\n                                tf.keras.layers.Dropout(0.2),\n                                tf.keras.layers.Dense(64, activation='Mish'),\n                                tf.keras.layers.Dropout(0.2),\n                                tf.keras.layers.Dense(1, activation='sigmoid')])\n    #x = img_adjust_layer(image_input)\n    x = base(image_input)\n    model = tf.keras.Model(inputs = image_input, outputs = x)\n    if compile_model:\n        #optimizer\n        optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n        optimizer.get_gradients=centralized_gradients_for_optimizer(optimizer) \n        #loss\n        loss = tf.keras.losses.BinaryCrossentropy(label_smoothing=0.01)\n        #metric\n        rmse = RMSE\n        model.compile(optimizer=optimizer,\n                      loss=loss,\n                      metrics=[rmse])\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-10-13T01:39:44.284179Z","iopub.execute_input":"2021-10-13T01:39:44.284451Z","iopub.status.idle":"2021-10-13T01:39:44.295403Z","shell.execute_reply.started":"2021-10-13T01:39:44.284425Z","shell.execute_reply":"2021-10-13T01:39:44.294752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tmp = build_model(CFG.model_name, DIM=CFG.img_size[0], compile_model=False)\ntmp.summary()","metadata":{"execution":{"iopub.status.busy":"2021-10-13T01:39:45.562468Z","iopub.execute_input":"2021-10-13T01:39:45.564868Z","iopub.status.idle":"2021-10-13T01:40:04.916371Z","shell.execute_reply.started":"2021-10-13T01:39:45.564818Z","shell.execute_reply":"2021-10-13T01:40:04.91545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tmp.layers[1].summary()","metadata":{"execution":{"iopub.status.busy":"2021-10-13T01:43:34.197996Z","iopub.execute_input":"2021-10-13T01:43:34.198375Z","iopub.status.idle":"2021-10-13T01:43:34.239634Z","shell.execute_reply.started":"2021-10-13T01:43:34.198342Z","shell.execute_reply":"2021-10-13T01:43:34.238377Z"},"trusted":true},"execution_count":null,"outputs":[]}]}