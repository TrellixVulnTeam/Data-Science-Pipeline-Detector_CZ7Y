{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Code for calculation\nMean average precision code from this link [https://github.com/tensorflow/models/blob/master/research/delf/delf/python/google_landmarks_dataset/metrics.py](http://)\n\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\ndef MeanAveragePrecision(predictions, retrieval_solution, max_predictions=100):\n    \"\"\"Computes mean average precision for retrieval prediction.\n    Args:\n        predictions: Dict mapping test image ID to a list of strings corresponding to index image IDs.\n        retrieval_solution: Dict mapping test image ID to list of ground-truth image IDs.\n        max_predictions: Maximum number of predictions per query to take into account. For the Google Landmark Retrieval challenge, this should be set to 100.\n    Returns:\n        mean_ap: Mean average precision score (float).\n    Raises:\n        ValueError: If a test image in `predictions` is not included in `retrieval_solutions`.\n    \"\"\"\n    # Compute number of test images.\n    num_test_images = len(retrieval_solution.keys())\n\n    # Loop over predictions for each query and compute mAP.\n    mean_ap = 0.0\n    for key, prediction in predictions.items():\n        if key not in retrieval_solution:\n            raise ValueError('Test image %s is not part of retrieval_solution' % key)\n\n        # Loop over predicted images, keeping track of those which were already\n        # used (duplicates are skipped).\n        ap = 0.0\n        already_predicted = set()\n        num_expected_retrieved = min(len(retrieval_solution[key]), max_predictions)\n        num_correct = 0\n        for i in range(min(len(prediction), max_predictions)):\n            if prediction[i] not in already_predicted:\n                if prediction[i] in retrieval_solution[key]:\n                    num_correct += 1\n                    ap += num_correct / (i + 1)\n                already_predicted.add(prediction[i])\n\n            ap /= num_expected_retrieved\n            mean_ap += ap\n\n        mean_ap /= num_test_images\n    return mean_ap\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Using the code","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\n\nPATH = '../input/landmark-retrieval-2020/'\n\n# Get labels (landmark ids) from train file as dict\nlabels = pd.read_csv(PATH+'train.csv', index_col='id')['landmark_id'].to_dict()\n\n# Check for this image id\nimage_id = '0000059611c7d079'\n\n# Dict mapping test image ID to a list of predicted strings corresponding to index image IDs.\npredictions = {image_id:['111e3a18bf0e529d', '3f5f7f38ea4dca61', '6cebd3221270bcc3', 'fb09f1e98c6d2f70']}\n\n# Dict mapping test image ID to list of ground-truth image IDs.\nretrieval_solution = {image_id: [k for k,v in labels.items() if v == labels[image_id]]}\n\nmap_score = MeanAveragePrecision(predictions, retrieval_solution)\nmap_score\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}