{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import argparse\nimport numpy as np\nimport pandas as pd\nimport os\nfrom tqdm import tqdm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import optim\nfrom torch.utils.data import DataLoader, Dataset\n#from apex import amp\nimport random\nimport re\nimport json\nfrom transformers import ( \n    BertTokenizer, \n    AdamW, \n    BertModel, \n    BertForPreTraining,\n    BertConfig,\n    get_linear_schedule_with_warmup,\n    BertTokenizerFast,\n    RobertaModel,\n    RobertaTokenizerFast,\n    RobertaConfig\n)\nimport transformers\n\n\ndef to_list(tensor):\n    return tensor.detach().cpu().tolist()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def process_with_offsets(args, tweet, sentiment, tokenizer):\n    \n    encoded = tokenizer.encode_plus(\n                    sentiment,\n                    tweet,\n                    max_length=args.max_seq_len,\n                    pad_to_max_length=True,\n                    return_token_type_ids=True,\n                    #return_offsets_mapping=True\n                )\n    \n    \n    encoded[\"tweet\"] = tweet\n    encoded[\"sentiment\"] = sentiment\n\n    return encoded","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TweetDataset:\n    def __init__(self, args, tokenizer, df, mode=\"test\"):\n        \n        self.mode = mode\n\n        \n        self.tweet = df.text.values\n        self.sentiment = df.sentiment.values\n\n        \n        self.tokenizer = tokenizer\n        self.args = args\n    \n    def __len__(self):\n        return len(self.tweet)\n\n    def __getitem__(self, item):\n\n        tweet = str(self.tweet[item])\n        sentiment = str(self.sentiment[item])\n        \n        features = process_with_offsets(\n                        args=self.args, \n                        tweet=tweet, \n                        sentiment=sentiment, \n                        tokenizer=self.tokenizer\n                    )\n        \n        return {\n            \"input_ids\":torch.tensor(features[\"input_ids\"], dtype=torch.long),\n            \"token_type_ids\":torch.tensor(features[\"token_type_ids\"], dtype=torch.long),\n            \"attention_mask\":torch.tensor(features[\"attention_mask\"], dtype=torch.long),\n            #\"offsets\":features[\"offset_mapping\"],\n\n            \"tweet\":features[\"tweet\"],\n            \"sentiment\":features[\"sentiment\"]\n\n        }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nclass TweetModel(transformers.BertPreTrainedModel):\n    def __init__(self, model_path, conf):\n        super(TweetModel, self).__init__(conf)\n        self.xlmroberta = transformers.XLMRobertaModel.from_pretrained(model_path, config=conf)\n        self.drop_out = nn.Dropout(0.1)\n        self.l0 = nn.Linear(768 * 2, 2) #768\n        torch.nn.init.normal_(self.l0.weight, std=0.02)\n\n\n    def forward(self,input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None):\n        _,_, out = self.xlmroberta(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids\n        )\n\n        out = torch.cat((out[-1], out[-2]), dim=-1)\n        #print(out.shape)\n        out = self.drop_out(out)\n        logits = self.l0(out)\n\n        start_logits, end_logits = logits.split(1, dim=-1)\n\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n\n        return start_logits, end_logits","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def test(args, test_loader, model):\n    \n    start_positions = []\n    end_positions = []\n\n    model.eval()\n\n    with torch.no_grad():\n        t = tqdm(test_loader)\n        for step, d in enumerate(t):\n            \n            input_ids = d[\"input_ids\"].to(args.device)\n            attention_mask = d[\"attention_mask\"].to(args.device)\n            token_type_ids = d[\"token_type_ids\"].to(args.device)\n\n            logits1, logits2 = model(\n                input_ids=input_ids, \n                attention_mask=attention_mask, \n                token_type_ids=token_type_ids, \n                position_ids=None, \n                head_mask=None\n            )\n\n            logits1 = F.softmax(logits1, dim=1).cpu().data.numpy()\n            logits2 = F.softmax(logits2, dim=1).cpu().data.numpy()\n            \n            start_positions.append(logits1)\n            end_positions.append(logits2)\n    \n    return start_positions, end_positions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class args:\n    max_seq_len = 128+64\n    batch_size = 16\n\nargs.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nargs.no_cuda = False if torch.cuda.is_available() else True\nargs.n_gpu = 0 if args.no_cuda else torch.cuda.device_count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_path = \"../input/xlm-roberta-base/\"\n\nconfig = transformers.XLMRobertaConfig.from_pretrained(model_path)\nconfig.output_hidden_states = True\ntokenizer = transformers.XLMRobertaTokenizer.from_pretrained(model_path, do_lower_case=True)\nmodel = TweetModel(model_path, config)\n\nmodel.to(args.device)\n\ntest_df = pd.read_csv(\"../input/tweet-sentiment-extraction/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dataset = TweetDataset(\n    args=args,\n    df=test_df,\n    mode=\"test\",\n    tokenizer=tokenizer\n)\ntest_loader = DataLoader(\n    test_dataset,\n    batch_size=args.batch_size,\n    shuffle=False,\n    num_workers=4,\n    pin_memory=True,\n    drop_last=False\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_weights_list = [\n    #\"../input/xml-roberta-base-weights/fold_0\",\n    #\"../input/xml-roberta-base-weights/fold_1\",\n    #\"../input/xml-roberta-base-weights/fold_2\",\n    #\"../input/xml-roberta-base-weights/fold_3\",\n    #\"../input/xml-roberta-base-weights/fold_4\",\n    \n    \"../input/tweet-xlm-roberta-3fold-tpu/xlm-roberta-base/xlm-roberta-base/fold_0\",\n    \"../input/tweet-xlm-roberta-3fold-tpu/xlm-roberta-base/xlm-roberta-base/fold_1\",\n    \"../input/tweet-xlm-roberta-3fold-tpu/xlm-roberta-base/xlm-roberta-base/fold_2\"\n]\n\nstart_list = []\nend_list = []\n\nfor path in model_weights_list:\n    model.load_state_dict(torch.load(path, map_location=args.device))\n    \n    start_positions, end_positions = test(args, test_loader, model)\n    \n    start_positions = np.concatenate(start_positions)\n    end_positions = np.concatenate(end_positions)\n    \n    start_list.append(start_positions)\n    end_list.append(end_positions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start_list = np.array(start_list)\nend_list = np.array(end_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start_positions = np.argmax(np.mean(start_list, axis=0), axis=1)\nend_positions = np.argmax(np.mean(end_list, axis=0), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df = pd.read_csv(\"../input/tweet-sentiment-extraction/sample_submission.csv\")\n\nfor i, (text, sentiment) in enumerate(zip(test_df.text.values, test_df.sentiment.values)):\n    \n    idx_start = start_positions[i]\n    idx_end = end_positions[i]\n    \n    encoded = tokenizer.encode_plus(\n                    sentiment,\n                    text,\n                    max_length=args.max_seq_len,\n                    pad_to_max_length=True,\n                    return_token_type_is=True,\n                    #return_offsets_mapping=True\n                )\n    input_id = encoded[\"input_ids\"]\n    #offsets = encoded[\"offset_mapping\"]\n    \n    if idx_end < idx_start:\n        idx_end = idx_start\n    \n    filtered_output = tokenizer.decode(input_id[idx_start:idx_end+1], skip_special_tokens=True)\n    \n    \n    #filtered_output = \"\"\n    #for ix in range(idx_start, idx_end):\n    #    filtered_output += text[offsets[ix][0]: offsets[ix][1]]\n    #    if (ix + 1) < len(offsets) and offsets[ix][1] < offsets[ix+1][0]:\n    #        filtered_output += \" \"\n    \n    \n    if sentiment == \"neutral\" or len(text.split()) < 2:\n        filtered_output = text\n    \n    filtered_output = filtered_output.strip()\n    \n    sub_df.loc[i, \"selected_text\"] = filtered_output\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}