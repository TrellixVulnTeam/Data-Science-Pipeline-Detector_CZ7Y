{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"test_set=pd.read_csv(\"/kaggle/input/tweet-sentiment-extraction/test.csv\")\ntest_set = test_set.fillna('-')\ntrain_set=pd.read_csv(\"/kaggle/input/tweet-sentiment-extraction/train.csv\")\ntrain_set = train_set.fillna('-')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS,CountVectorizer\nfrom sklearn.metrics import f1_score\nvect1 = CountVectorizer(token_pattern=r'\\w{3,10}',ngram_range=(1, 1),max_features=23000, stop_words=ENGLISH_STOP_WORDS).fit(train_set.text)\n#vect2 = TfidfVectorizer(ngram_range=(1, 1),max_features=1000, stop_words=ENGLISH_STOP_WORDS).fit(train_set.text)\nX1_txt = vect1.transform(test_set.text)\n#X2_txt = vect2.transform(train_set.text)\nX1=pd.DataFrame(X1_txt.toarray(), columns=vect1.get_feature_names())\n#X2=pd.DataFrame(X2_txt.toarray(), columns=vect2.get_feature_names())\ny=test_set.sentiment\n#X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y, test_size=0, random_state=42)\n#X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y, test_size=0, random_state=42)\nlog_reg1 = LogisticRegression(C=1500).fit(X1, y)\n#log_reg2 = LogisticRegression().fit(X2, y)\n#print('Accuracy on train set with BOW: ', log_reg1.score(X1_train, y1_train))\n#print('Accuracy on test set with BOW: ', log_reg1.score(X1_test, y1_test))\n#print('Accuracy on train set with TFIDF: ', log_reg2.score(X2_train, y2_train))\n#print('Accuracy on test set with TFIDF: ', log_reg2.score(X2_test, y2_test))\ny1_predicted = log_reg1.predict(X1)\n#y2_predicted = log_reg2.predict(X2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nprint('Accuracy score test set BOW: ', accuracy_score(test_set.sentiment, y1_predicted))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from sklearn.metrics import accuracy_score\n#print('Accuracy score test set BOW: ', accuracy_score(test_set.sentiment, y1_predicted))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prob = log_reg1.predict_proba(X1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ii=[i for i in np.where(X1_txt.toarray()[1,:]>=1)[0]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"indexes=[np.where(X1_txt.toarray()[index,:]>=1)[0] for index in range(len(X1_txt.toarray()))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features=[]\nfor j in range(len(indexes)):\n    tokens=[vect1.get_feature_names()[i] for i in indexes[j]]\n    features.append(tokens)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"coeff1=[]\ncoeff2=[]\ncoeff3=[]\nfor j in range(len(indexes)):\n    tokens1=[log_reg1.coef_[0][i] for i in indexes[j]]\n    coeff1.append(tokens1)\n    tokens2=[log_reg1.coef_[1][i] for i in indexes[j]]\n    coeff2.append(tokens2)\n    tokens3=[log_reg1.coef_[2][i] for i in indexes[j]]\n    coeff3.append(tokens3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#coeff1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list1=[]\nlist2=[]\nlist3=[]\nfor j in range(len(coeff1)):\n    index1=[i for i in range(len(coeff1[j])) if coeff1[j][i]>0]\n    list1.append(index1)\n    index2=[i for i in range(len(coeff2[j])) if coeff2[j][i]>0]\n    list2.append(index2)\n    index3=[i for i in range(len(coeff3[j])) if coeff3[j][i]>0]\n    list3.append(index3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#list1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combine=zip(list1,list2,list3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined_list=[*combine]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"index=[np.where(max(prob[i])==prob[i])[0][0] for i in range(len(prob))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_selected_tokens=[]\nfor j in range(len(features)):\n    if index[j]!=1:\n        selected_tokens=[features[j][i] for i in combined_list[j][index[j]]]\n    else:\n        selected_tokens=[test_set.text[j]]\n    total_selected_tokens.append(selected_tokens)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"selected_text_predicted=[' '.join(str) for str in total_selected_tokens ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#selected_text_predicted","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission=pd.read_csv(\"/kaggle/input/tweet-sentiment-extraction/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission[\"selected_text\"]=selected_text_predicted","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}