{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS,CountVectorizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set=pd.read_csv(\"/kaggle/input/tweet-sentiment-extraction/train.csv\")\ntrain_set = train_set.fillna('-')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vect1 = CountVectorizer(token_pattern=r'[a-z|A-Z|**]{3,10}',ngram_range=(1, 1),max_features=23000, stop_words=ENGLISH_STOP_WORDS).fit(train_set.text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X1_txt = vect1.transform(train_set.text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X1=pd.DataFrame(X1_txt.toarray(), columns=vect1.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y=train_set.sentiment","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"log_reg1 = LogisticRegression(C=1500).fit(X1, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y1_predicted = log_reg1.predict(X1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_set=pd.read_csv(\"/kaggle/input/tweet-sentiment-extraction/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X2_txt = vect1.transform(test_set.text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X2=pd.DataFrame(X2_txt.toarray(), columns=vect1.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y2_predicted = log_reg1.predict(X2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Accuracy score test set BOW: ', accuracy_score(train_set.sentiment, y1_predicted))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import spacy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import en_core_web_sm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nlp = en_core_web_sm.load()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"docs=[nlp(train_set.text[i]) for i in range(len(train_set.text))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokens = [[token.text.lower() for token in doc] for doc in docs]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokens","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a=train_set.sentiment.values == 'negative' ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"b=train_set.sentiment.values=='positive'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"A=[]\nfor x,y in zip(a,b):\n    A.append(x or y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokens_pos_neg=[]\n\nfor i,x in enumerate(A):\n    if x:\n        tokens_pos_neg.append(tokens[i])\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokens_pos_neg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"selected_text=list(train_set.selected_text.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"selected_text=list(train_set.selected_text.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"docs=[nlp(selected_text[i]) for i in range(len(selected_text))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokens = [[token.text.lower() for token in doc] for doc in docs]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"selected_text_pos_neg=[]\nfor i,x in enumerate(A):\n    if x:\n        selected_text_pos_neg.append(tokens[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c=zip(tokens_pos_neg,selected_text_pos_neg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"k=[*c]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dd={}\nfor i,P in enumerate(k):\n    d={}\n    if len(P[0])>0 and len(P[1])>0:\n        for rank,value in enumerate(P[0]):\n            if rank<=len(P[0])-len(P[1]):\n                L=[]\n                for j in range(len(P[1])):\n                    L.append(P[0][rank+j])\n                d[rank]=L\n    \n    dd[i]=d","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"detects=[]\nfor rank1,value1 in dd.items():\n    detect=[]\n    for rank,value in value1.items():\n        if value==selected_text_pos_neg[rank1]:\n            detect.append(rank)\n            detect.append(value)\n    detects.append(detect)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for rank1,value1 in enumerate(detects):\n    if len(value1)>0  and value1[1]!=selected_text_pos_neg[rank1]:\n        value1[1]=[]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tagget_extracted_tokens=[]\nfor rank1,value1 in enumerate(detects):\n    if len(value1)>0 and len(tokens_pos_neg)>0:\n        val=np.zeros(len(tokens_pos_neg[rank1]))\n        #print(value1)\n        #print(value1[0])\n        #print(value1[1])\n        if value1[0]!=0:\n            val[value1[0]-1:(value1[0]-1)+len(value1[1])+1]=list(range(len(value1[1])+1))\n        else:\n            val[0:(value1[0]-1)+len(value1[1])+1]=range(len(value1[1]))+np.ones(len(value1[1]),dtype=int)\n        tagget_extracted_tokens.append(list(val))\n    else:\n        tagget_extracted_tokens.append([])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tagget_extracted_tokens","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for k,w in enumerate(tagget_extracted_tokens):\n    w.insert(0,0)\n    w.append(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for k,w in enumerate(tokens_pos_neg):\n    w.insert(0,'#')\n    w.append('#')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word2idx = {}\nword_idx = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for k,w in enumerate(tokens_pos_neg):\n    for k1,w1 in enumerate(w):\n        if w1 not in word2idx:\n            word2idx[w1]=word_idx\n            word_idx += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word2idx","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len_word2idx=len(word2idx)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len_word2idx","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pos2idx = {}\npos_idx = 0\nfor k,w in enumerate(tagget_extracted_tokens):\n    for k1,w1 in enumerate(w):\n        if w1 not in pos2idx:\n            pos2idx[w1]=pos_idx\n            pos_idx += 1\npos2idx","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Xtrain=[]\nfor k,w in enumerate(tokens_pos_neg):\n    l=[]\n    for k1,w1 in enumerate(w):\n        #print(w1)\n        #print(w1)\n        l.append(word2idx[w1])\n    Xtrain.append(l)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Xtrain","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Ptrain=[]\nfor k,w in enumerate(tagget_extracted_tokens):\n    l=[]\n    for k1,w1 in enumerate(w):\n        #print(w1)\n        #print(w1)\n        l.append(pos2idx[w1])\n    Ptrain.append(l)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Ptrain","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nX=[]\nP=[]\n\n\nselected_text_pos_neg_modified=[]\nfor x,z,s in zip(Xtrain,Ptrain,selected_text_pos_neg):\n    if len(x)==len(z):\n        \n        X.append(x)\n        P.append(z)\n        selected_text_pos_neg_modified.append(s)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lenght_list=[]\nfor l in X:\n    lenght_list.append(len(l))\nmax_len=np.max(lenght_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_len","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_data=np.zeros((2000,max_len+1,len(word2idx)),dtype='float32')\ntarget_data=np.zeros((2000,max_len+1,len(pos2idx)),dtype='float32')\nfor k,w in enumerate(X[0:2000]):\n    for k1,w1 in enumerate(w):\n        input_data[k,k1,X[k][k1]]=1\nfor k,w in enumerate(X[0:2000]):\n    for k1,w1 in enumerate(w):\n        target_data[k,k1,P[k][k1]]=1\nfrom keras.layers import SimpleRNN,Dense,Activation,TimeDistributed\nfrom keras.models import Sequential\nmodel=Sequential()\nmodel.add(SimpleRNN(50,input_shape=(max_len+1,len(word2idx)),return_sequences=True))\nmodel.add(TimeDistributed(Dense(len(pos2idx),activation='softmax')))\nmodel.compile(loss=\"categorical_crossentropy\",optimizer=\"adam\")\nmodel.fit(input_data,target_data,batch_size=100,epochs=15)\nfor k2 in range(1,7):\n    input_data=np.zeros((2000,max_len+1,len(word2idx)),dtype='float32')\n    target_data=np.zeros((2000,max_len+1,len(pos2idx)),dtype='float32')\n    for k,w in enumerate(X[k2*2000:k2*2000+2000]):\n        for k1,w1 in enumerate(w):\n            input_data[k,k1,X[k+k2*2000][k1]]=1\n    for k,w in enumerate(X[k2*2000:k2*2000+2000]):\n        for k1,w1 in enumerate(w):\n            target_data[k,k1,P[k+k2*2000][k1]]=1\n    model.fit(input_data,target_data,batch_size=100,epochs=15)\ninput_data=np.zeros((586,max_len+1,len(word2idx)),dtype='float32')\ntarget_data=np.zeros((586,max_len+1,len(pos2idx)),dtype='float32')\nk2=7\nfor k,w in enumerate(X[k2*2000:k2*2000+586]):\n        for k1,w1 in enumerate(w):\n            input_data[k,k1,X[k+k2*2000][k1]]=1\nfor k,w in enumerate(X[k2*2000:k2*2000+586]):\n        for k1,w1 in enumerate(w):\n            target_data[k,k1,P[k+k2*2000][k1]]=1\nmodel.fit(input_data,target_data,batch_size=100,epochs=15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(tokens_pos_neg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokens_pos_neg_3=[]\ntokens_pos_neg_2=[]\ntokens_pos_neg_1=[]\ntokens_pos_neg_33=[]\ntokens_pos_neg_22=[]\ntokens_pos_neg_11=[]\nfor k,w in enumerate(tokens_pos_neg):\n    for k11,w11 in enumerate(w):\n        w1=w11[:3]\n        tokens_pos_neg_3.append(w1)\n        w2=w11[:2]\n        tokens_pos_neg_2.append(w2)\n        w3=w11[:1]\n        tokens_pos_neg_1.append(w3)\n    tokens_pos_neg_33.append(tokens_pos_neg_3)\n    tokens_pos_neg_22.append(tokens_pos_neg_2)\n    tokens_pos_neg_11.append(tokens_pos_neg_1)\n    tokens_pos_neg_3=[]\n    tokens_pos_neg_2=[]\n    tokens_pos_neg_1=[]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(tokens_pos_neg_33)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word2idx_3 = {}\nword_idx_3 = 0\nfor k,w in enumerate(tokens_pos_neg_33):\n    for k1,w1 in enumerate(w):\n        if w1 not in word2idx_3:\n            word2idx_3[w1]=word_idx_3\n            word_idx_3 += 1\nprint(word2idx_3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(word2idx_3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(Ptrain)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Xtrain_3=[]\nfor k,w in enumerate(tokens_pos_neg_33):\n    l=[]\n    for k1,w1 in enumerate(w):\n        #print(w1)\n        #print(w1)\n        l.append(word2idx_3[w1])\n    Xtrain_3.append(l)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(Xtrain_3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X3=[]\nP3=[]\nfor x,z in zip(Xtrain_3,Ptrain):\n    if len(x)==len(z):\n        X3.append(x)\n        P3.append(z)\n        #selected_text_pos_neg_modified.append(s)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(X3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(P3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_data=np.zeros((2000,max_len+1,len(word2idx_3)),dtype='float32')\ntarget_data=np.zeros((2000,max_len+1,len(pos2idx)),dtype='float32')\nfor k,w in enumerate(X3[0:2000]):\n    for k1,w1 in enumerate(w):\n        input_data[k,k1,X3[k][k1]]=1\nfor k,w in enumerate(X3[0:2000]):\n    for k1,w1 in enumerate(w):\n        target_data[k,k1,P3[k][k1]]=1\nfrom keras.layers import SimpleRNN,Dense,Activation,TimeDistributed\nfrom keras.models import Sequential\nmodel3=Sequential()\nmodel3.add(SimpleRNN(50,input_shape=(max_len+1,len(word2idx_3)),return_sequences=True))\nmodel3.add(TimeDistributed(Dense(len(pos2idx),activation='softmax')))\nmodel3.compile(loss=\"categorical_crossentropy\",optimizer=\"adam\")\nmodel3.fit(input_data,target_data,batch_size=100,epochs=15)\nfor k2 in range(1,7):\n    input_data=np.zeros((2000,max_len+1,len(word2idx_3)),dtype='float32')\n    target_data=np.zeros((2000,max_len+1,len(pos2idx)),dtype='float32')\n    for k,w in enumerate(X3[k2*2000:k2*2000+2000]):\n        for k1,w1 in enumerate(w):\n            input_data[k,k1,X3[k+k2*2000][k1]]=1\n    for k,w in enumerate(X3[k2*2000:k2*2000+2000]):\n        for k1,w1 in enumerate(w):\n            target_data[k,k1,P3[k+k2*2000][k1]]=1\n    model3.fit(input_data,target_data,batch_size=100,epochs=15)\ninput_data=np.zeros((586,max_len+1,len(word2idx_3)),dtype='float32')\ntarget_data=np.zeros((586,max_len+1,len(pos2idx)),dtype='float32')\nk2=7\nfor k,w in enumerate(X3[k2*2000:k2*2000+586]):\n        for k1,w1 in enumerate(w):\n            input_data[k,k1,X3[k+k2*2000][k1]]=1\nfor k,w in enumerate(X3[k2*2000:k2*2000+586]):\n        for k1,w1 in enumerate(w):\n            target_data[k,k1,P3[k+k2*2000][k1]]=1\nmodel3.fit(input_data,target_data,batch_size=100,epochs=15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word2idx_2 = {}\nword_idx_2 = 0\nfor k,w in enumerate(tokens_pos_neg_22):\n    for k1,w1 in enumerate(w):\n        if w1 not in word2idx_2:\n            word2idx_2[w1]=word_idx_2\n            word_idx_2 += 1\nprint(word2idx_2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(word2idx_2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Xtrain_2=[]\nfor k,w in enumerate(tokens_pos_neg_22):\n    l=[]\n    for k1,w1 in enumerate(w):\n        #print(w1)\n        #print(w1)\n        l.append(word2idx_2[w1])\n    Xtrain_2.append(l)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X2=[]\nP2=[]\nfor x,z in zip(Xtrain_2,Ptrain):\n    if len(x)==len(z):\n        X2.append(x)\n        P2.append(z)\n        #selected_text_pos_neg_modified.append(s)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_data=np.zeros((2000,max_len+1,len(word2idx_2)),dtype='float32')\ntarget_data=np.zeros((2000,max_len+1,len(pos2idx)),dtype='float32')\nfor k,w in enumerate(X2[0:2000]):\n    for k1,w1 in enumerate(w):\n        input_data[k,k1,X2[k][k1]]=1\nfor k,w in enumerate(X2[0:2000]):\n    for k1,w1 in enumerate(w):\n        target_data[k,k1,P2[k][k1]]=1\nfrom keras.layers import SimpleRNN,Dense,Activation,TimeDistributed\nfrom keras.models import Sequential\nmodel2=Sequential()\nmodel2.add(SimpleRNN(50,input_shape=(max_len+1,len(word2idx_2)),return_sequences=True))\nmodel2.add(TimeDistributed(Dense(len(pos2idx),activation='softmax')))\nmodel2.compile(loss=\"categorical_crossentropy\",optimizer=\"adam\")\nmodel2.fit(input_data,target_data,batch_size=100,epochs=15)\nfor k2 in range(1,7):\n    input_data=np.zeros((2000,max_len+1,len(word2idx_2)),dtype='float32')\n    target_data=np.zeros((2000,max_len+1,len(pos2idx)),dtype='float32')\n    for k,w in enumerate(X2[k2*2000:k2*2000+2000]):\n        for k1,w1 in enumerate(w):\n            input_data[k,k1,X2[k+k2*2000][k1]]=1\n    for k,w in enumerate(X2[k2*2000:k2*2000+2000]):\n        for k1,w1 in enumerate(w):\n            target_data[k,k1,P2[k+k2*2000][k1]]=1\n    model2.fit(input_data,target_data,batch_size=100,epochs=15)\ninput_data=np.zeros((586,max_len+1,len(word2idx_2)),dtype='float32')\ntarget_data=np.zeros((586,max_len+1,len(pos2idx)),dtype='float32')\nk2=7\nfor k,w in enumerate(X2[k2*2000:k2*2000+586]):\n        for k1,w1 in enumerate(w):\n            input_data[k,k1,X2[k+k2*2000][k1]]=1\nfor k,w in enumerate(X2[k2*2000:k2*2000+586]):\n        for k1,w1 in enumerate(w):\n            target_data[k,k1,P2[k+k2*2000][k1]]=1\nmodel2.fit(input_data,target_data,batch_size=100,epochs=15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word2idx_1 = {}\nword_idx_1 = 0\nfor k,w in enumerate(tokens_pos_neg_11):\n    for k1,w1 in enumerate(w):\n        if w1 not in word2idx_1:\n            word2idx_1[w1]=word_idx_1\n            word_idx_1 += 1\nprint(word2idx_1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(word2idx_1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Xtrain_1=[]\nfor k,w in enumerate(tokens_pos_neg_11):\n    l=[]\n    for k1,w1 in enumerate(w):\n        #print(w1)\n        #print(w1)\n        l.append(word2idx_1[w1])\n    Xtrain_1.append(l)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X1=[]\nP1=[]\nfor x,z in zip(Xtrain_1,Ptrain):\n    if len(x)==len(z):\n        X1.append(x)\n        P1.append(z)\n        #selected_text_pos_neg_modified.append(s)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_data=np.zeros((2000,max_len+1,len(word2idx_1)),dtype='float32')\ntarget_data=np.zeros((2000,max_len+1,len(pos2idx)),dtype='float32')\nfor k,w in enumerate(X1[0:2000]):\n    for k1,w1 in enumerate(w):\n        input_data[k,k1,X1[k][k1]]=1\nfor k,w in enumerate(X1[0:2000]):\n    for k1,w1 in enumerate(w):\n        target_data[k,k1,P1[k][k1]]=1\nfrom keras.layers import SimpleRNN,Dense,Activation,TimeDistributed\nfrom keras.models import Sequential\nmodel1=Sequential()\nmodel1.add(SimpleRNN(50,input_shape=(max_len+1,len(word2idx_1)),return_sequences=True))\nmodel1.add(TimeDistributed(Dense(len(pos2idx),activation='softmax')))\nmodel1.compile(loss=\"categorical_crossentropy\",optimizer=\"adam\")\nmodel1.fit(input_data,target_data,batch_size=100,epochs=15)\nfor k2 in range(1,7):\n    input_data=np.zeros((2000,max_len+1,len(word2idx_1)),dtype='float32')\n    target_data=np.zeros((2000,max_len+1,len(pos2idx)),dtype='float32')\n    for k,w in enumerate(X1[k2*2000:k2*2000+2000]):\n        for k1,w1 in enumerate(w):\n            input_data[k,k1,X1[k+k2*2000][k1]]=1\n    for k,w in enumerate(X1[k2*2000:k2*2000+2000]):\n        for k1,w1 in enumerate(w):\n            target_data[k,k1,P1[k+k2*2000][k1]]=1\n    model1.fit(input_data,target_data,batch_size=100,epochs=15)\ninput_data=np.zeros((586,max_len+1,len(word2idx_1)),dtype='float32')\ntarget_data=np.zeros((586,max_len+1,len(pos2idx)),dtype='float32')\nk2=7\nfor k,w in enumerate(X1[k2*2000:k2*2000+586]):\n        for k1,w1 in enumerate(w):\n            input_data[k,k1,X1[k+k2*2000][k1]]=1\nfor k,w in enumerate(X1[k2*2000:k2*2000+586]):\n        for k1,w1 in enumerate(w):\n            target_data[k,k1,P1[k+k2*2000][k1]]=1\nmodel1.fit(input_data,target_data,batch_size=100,epochs=15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nlp = en_core_web_sm.load()\ndocs=[nlp(test_set.text[i]) for i in range(len(test_set.text))]\ntokens = [[token.text.lower() for token in doc] for doc in docs]\na=y2_predicted == 'negative' \nb=y2_predicted == 'positive'\nA=[]\nfor x,y in zip(a,b):\n    A.append(x or y)\ntokens_pos_neg_out=[]\n\nfor i,x in enumerate(A):\n    if x:\n        tokens_pos_neg_out.append(tokens[i])\nfor k,w in enumerate(tokens_pos_neg_out):\n    w.insert(0,'#')\n    w.append('#')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for k,w in enumerate(tokens_pos_neg_out):\n    for k1,w1 in enumerate(w):\n        if w1 not in word2idx:\n            word2idx[w1]=word_idx\n            word_idx += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Xtest=[]\nfor k,w in enumerate(tokens_pos_neg_out):\n    l=[]\n    for k1,w1 in enumerate(w):\n        #print(w1)\n        #print(w1)\n        l.append(word2idx[w1])\n    Xtest.append(l)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Xtest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokens_pos_neg_out_3=[]\nfor k,w in enumerate(tokens_pos_neg_out):\n    a=[]\n    for k1,w1 in enumerate(w):\n        a.append(w1[:3])\n    tokens_pos_neg_out_3.append(a)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokens_pos_neg_out_3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for k,w in enumerate(tokens_pos_neg_out_3):\n    for k1,w1 in enumerate(w):\n        if w1 not in word2idx_3:\n            word2idx_3[w1]=word_idx_3\n            word_idx_3 += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Xtest_3=[]\nfor k,w in enumerate(tokens_pos_neg_out_3):\n    l=[]\n    for k1,w1 in enumerate(w):\n        #print(w1)\n        #print(w1)\n        l.append(word2idx_3[w1])\n    Xtest_3.append(l)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Xtest_3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokens_pos_neg_out_2=[]\nfor k,w in enumerate(tokens_pos_neg_out):\n    a=[]\n    for k1,w1 in enumerate(w):\n        a.append(w1[:2])\n    tokens_pos_neg_out_2.append(a)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for k,w in enumerate(tokens_pos_neg_out_2):\n    for k1,w1 in enumerate(w):\n        if w1 not in word2idx_2:\n            word2idx_2[w1]=word_idx_2\n            word_idx_2 += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Xtest_2=[]\nfor k,w in enumerate(tokens_pos_neg_out_2):\n    l=[]\n    for k1,w1 in enumerate(w):\n        #print(w1)\n        #print(w1)\n        l.append(word2idx_2[w1])\n    Xtest_2.append(l)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Xtest_2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokens_pos_neg_out_1=[]\nfor k,w in enumerate(tokens_pos_neg_out):\n    a=[]\n    for k1,w1 in enumerate(w):\n        a.append(w1[:1])\n    tokens_pos_neg_out_1.append(a)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for k,w in enumerate(tokens_pos_neg_out_1):\n    for k1,w1 in enumerate(w):\n        if w1 not in word2idx_1:\n            word2idx_1[w1]=word_idx_1\n            word_idx_1 += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Xtest_1=[]\nfor k,w in enumerate(tokens_pos_neg_out_1):\n    l=[]\n    for k1,w1 in enumerate(w):\n        #print(w1)\n        #print(w1)\n        l.append(word2idx_1[w1])\n    Xtest_1.append(l)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Xtest_1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Xtest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cond=[]\nfor k,w in enumerate(Xtest):\n    if max(w)<19598:\n        cond.append(1)\n    else:\n        cond.append(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cond","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfor k,w in enumerate(Xtest_3):\n    if max(w)<4320 and cond[k]!=1:\n        cond[k]=2\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sum(np.array(cond)==2)+sum(np.array(cond)==1)+sum(np.array(cond)==3)+sum(np.array(cond)==4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for k,w in enumerate(Xtest_2):\n    if max(w)<1046 and cond[k]!=1 and cond[k]!=2 :\n        cond[k]=3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for k,w in enumerate(Xtest_1):\n    if max(w)<71 and cond[k]!=1 and cond[k]!=2 and cond[k]!=3:\n        cond[k]=4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(cond)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"P_theta_total=[]\nfor i in range(len(cond)):\n    if cond[i]==1:\n        output_seq=np.zeros((1,max_len+1,19598))\n        for k,w in enumerate(Xtest[i]):\n            output_seq[0,k,w]=1\n        probs=model.predict_proba(output_seq,verbose=0)\n        P_theta=[]\n        for i in range(len(Xtest[i])):\n            P_theta.append(list(probs[:,i,:][0]).index(max(probs[:,i,:][0])))\n            \n        P_theta_total.append(P_theta)\n    elif cond[i]==2:\n        output_seq=np.zeros((1,max_len+1,4320))\n        for k,w in enumerate(Xtest_3[i]):\n            output_seq[0,k,w]=1\n        probs=model3.predict_proba(output_seq,verbose=0)\n        P_theta=[]\n        for i in range(len(Xtest_3[i])):\n            P_theta.append(list(probs[:,i,:][0]).index(max(probs[:,i,:][0])))\n            \n        P_theta_total.append(P_theta)\n    elif cond[i]==3:\n        output_seq=np.zeros((1,max_len+1,1046))\n        for k,w in enumerate(Xtest_2[i]):\n            output_seq[0,k,w]=1\n        probs=model2.predict_proba(output_seq,verbose=0)\n        P_theta=[]\n        for i in range(len(Xtest_2[i])):\n            P_theta.append(list(probs[:,i,:][0]).index(max(probs[:,i,:][0])))\n            \n        P_theta_total.append(P_theta)\n    elif cond[i]==4:\n        output_seq=np.zeros((1,max_len+1,71))\n        for k,w in enumerate(Xtest_1[i]):\n            output_seq[0,k,w]=1\n        probs=model1.predict_proba(output_seq,verbose=0)\n        P_theta=[]\n        for i in range(len(Xtest_1[i])):\n            P_theta.append(list(probs[:,i,:][0]).index(max(probs[:,i,:][0])))\n            \n        P_theta_total.append(P_theta)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.array(P_theta_total[3])>0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Xtest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list(np.array(tokens_pos_neg_out[3])[np.array(P_theta_total[3])>0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"L=[]\nfor i in range(len(tokens_pos_neg_out)):\n    L.append(list(np.array(tokens_pos_neg_out[i])[np.array(P_theta_total[i])>0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(L)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"' '.join(L[3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"selected_text_predicted_pos_neg=[' '.join(str) for str in L]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"selected_text_predicted_pos_neg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_set.text[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sum(y2_predicted=='neutral')+len(L)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(y2_predicted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LL=[]\nk=0\nfor i in range(len(y2_predicted)):\n    if y2_predicted[i]=='neutral':\n        LL.append(test_set.text[i])\n    else:\n        LL.append(selected_text_predicted_pos_neg[k])\n        k=k+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LL","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission=pd.read_csv(\"/kaggle/input/tweet-sentiment-extraction/sample_submission.csv\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#`submission[\"selected_text\"]=LL","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = {'test_test':  test_set.text,\n        'extracted_text': LL,\n        'sentiment_predicted': y2_predicted\n        \n        }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = pd.DataFrame (data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LL","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X1=pd.DataFrame(X1_txt.toarray(), columns=vect1.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prob = log_reg1.predict_proba(X1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X2=pd.DataFrame(X2_txt.toarray(), columns=vect1.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_indexes_2=[]\nfor row in X2.itertuples():\n    feature_indexes_2.append(list(np.where(row==np.ones(len(row)))[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_indexes_2=[]\nfor row in X2.itertuples():\n    feature_indexes_2.append(list(np.where(row==np.ones(len(row)))[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_indexes_2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_indexes_rectified_2=[]\nfor x in feature_indexes_2:\n    x=np.array(x)-1\n    feature_indexes_rectified_2.append(list(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_indexes_rectified_2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features_2=[]\nfor k,w in enumerate(feature_indexes_rectified_2):\n    if len(w)>0:\n        features_2.append(list(np.array(vect1.get_feature_names())[w]))\n    else:\n        features_2.append([])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features_2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a=list(log_reg1.coef_[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"b=list(log_reg1.coef_[2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"e=zip(vect1.get_feature_names(),a,b)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f=[*e]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g={}\nfor w in f:\n    g[w[0]]=[w[1],w[2]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g['***']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"values_neg=[]\nvalues_pos=[]\nfor k,w in enumerate(features_2):\n    a=[]\n    b=[]\n    for k1,w1 in enumerate(w):\n        a.append(g[w1][0])\n        b.append(g[w1][1])\n    values_neg.append(a)\n    values_pos.append(b)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.array(values_neg[0])<0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list(np.array(features_2[0])[np.array(values_neg[0])>0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokens_positive=[]\ntokens_negative=[]\nfor k,w in enumerate(features_2):\n    tokens_negative.append(list(np.array(w)[np.array(values_neg[k])>0]))\n    tokens_positive.append(list(np.array(w)[np.array(values_pos[k])>0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"L1=[]\nL2=[]\nfor w,w1 in zip(tokens_negative,tokens_positive):\n    L1.append(' '.join(w) )\n    L2.append(' '.join(w1) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"L1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(LL)):\n    if len(LL[i])==0 and y2_predicted[i]=='negative':\n        LL[i]=L1[i]\n    elif len(LL[i])==0 and y2_predicted[i]=='positive':\n        LL[i]=L2[i]\n    elif len(LL[i])==0:\n        LL[i]='_'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LL","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y2_predicted[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission[\"selected_text\"]=LL","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}