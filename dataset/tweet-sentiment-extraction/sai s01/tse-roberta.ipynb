{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2020-10-25T18:52:09.295809Z","iopub.status.busy":"2020-10-25T18:52:09.294981Z","iopub.status.idle":"2020-10-25T18:52:18.509485Z","shell.execute_reply":"2020-10-25T18:52:18.508683Z"},"papermill":{"duration":9.243007,"end_time":"2020-10-25T18:52:18.509654","exception":false,"start_time":"2020-10-25T18:52:09.266647","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# importing libraries\n\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom transformers import *\nimport tokenizers\nprint('TF version',tf.__version__)\nimport os\nfrom tokenizers import BertWordPieceTokenizer\nfrom transformers import BertTokenizer, TFBertModel, BertConfig\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom transformers import BertTokenizer, TFBertForQuestionAnswering\nfrom sklearn.model_selection import train_test_split\nimport gc\nfrom keras.callbacks import ModelCheckpoint\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.execute_input":"2020-10-25T18:52:18.608237Z","iopub.status.busy":"2020-10-25T18:52:18.602882Z","iopub.status.idle":"2020-10-25T18:52:18.611234Z","shell.execute_reply":"2020-10-25T18:52:18.611675Z"},"papermill":{"duration":0.080641,"end_time":"2020-10-25T18:52:18.611801","exception":false,"start_time":"2020-10-25T18:52:18.53116","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"max_len = 128\ntrain_mode = False\n\n# Load the tokenizer\ntokenizer = tokenizers.ByteLevelBPETokenizer(vocab_file = '../input/tf-roberta/vocab-roberta-base.json',\n                             merges_file = '../input/tf-roberta/merges-roberta-base.txt',\n                             add_prefix_space = True,\n                             lowercase = True)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-25T18:52:18.671561Z","iopub.status.busy":"2020-10-25T18:52:18.670999Z","iopub.status.idle":"2020-10-25T18:52:18.846251Z","shell.execute_reply":"2020-10-25T18:52:18.846724Z"},"papermill":{"duration":0.214459,"end_time":"2020-10-25T18:52:18.846865","exception":false,"start_time":"2020-10-25T18:52:18.632406","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# loading train data.\nsentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}\ndata = pd.read_csv('../input/tweet-sentiment-extraction/train.csv').fillna('')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-25T18:52:18.899496Z","iopub.status.busy":"2020-10-25T18:52:18.898508Z","iopub.status.idle":"2020-10-25T18:52:18.932426Z","shell.execute_reply":"2020-10-25T18:52:18.931866Z"},"papermill":{"duration":0.064228,"end_time":"2020-10-25T18:52:18.932538","exception":false,"start_time":"2020-10-25T18:52:18.86831","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# removing empty rows\ndata['text'].replace('', np.nan, inplace=True)\ndata.dropna(subset=['text'], inplace=True)\ndata.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.021042,"end_time":"2020-10-25T18:52:18.974576","exception":false,"start_time":"2020-10-25T18:52:18.953534","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## Train Test CV split\n"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-25T18:52:19.02536Z","iopub.status.busy":"2020-10-25T18:52:19.024538Z","iopub.status.idle":"2020-10-25T18:52:19.03811Z","shell.execute_reply":"2020-10-25T18:52:19.037345Z"},"papermill":{"duration":0.042511,"end_time":"2020-10-25T18:52:19.03823","exception":false,"start_time":"2020-10-25T18:52:18.995719","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"x_train,x_test = train_test_split(data, test_size = 0.05, random_state=42)\nx_train,x_cv = train_test_split(x_train, test_size = 0.2, random_state = 42)\n\nprint(\"x_train shape is\", x_train.shape)\nprint(\"x_cv shape is\", x_cv.shape)\nprint(\"x_test shape is\", x_test.shape)\nx_train.reset_index(drop=True, inplace=True)\nx_cv.reset_index(drop=True, inplace=True)\nx_test.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.021512,"end_time":"2020-10-25T18:52:19.081558","exception":false,"start_time":"2020-10-25T18:52:19.060046","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## Data Preparation"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-25T18:52:19.148604Z","iopub.status.busy":"2020-10-25T18:52:19.147699Z","iopub.status.idle":"2020-10-25T18:52:19.154039Z","shell.execute_reply":"2020-10-25T18:52:19.15337Z"},"papermill":{"duration":0.05093,"end_time":"2020-10-25T18:52:19.154163","exception":false,"start_time":"2020-10-25T18:52:19.103233","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# https://www.kaggle.com/abhishek/bert-base-uncased-using-pytorch\n# Example to explain operations in text_process function\n\ntweet = 'WHY DO WE FALL? SO THAT WE CAN LEARN TO PICK OURSELVES BACK UP.'\nselected_text = 'LEARN TO PICK OURSELVES BACK UP'\nsentiment = 'positive'\nprint('text',tweet)\nprint('select_text:',selected_text)\nprint('sentiment:', sentiment)\n# idx0 and idx1 and start and end indices of select_text in tweet.\nidx0 = None\nidx1 = None\nst_len = len(selected_text)\n\n# with roberta 'hello' and ' hello' are different hence we need to add a space for beginning of tweet.\n\ntweet = \" \" + tweet\n\n\nfor i in range(len(tweet)):\n    if(tweet[i:i+st_len]==selected_text):\n        idx0 = i\n        idx1 = i + st_len -1\n        break\n\n# char_tartgets is of length tweet, assign indices with select text =1 \nchar_targets = [0]*len(tweet)\nfor i in range(len(tweet)):\n    if idx0 != None and idx1!=None:\n        if i>=idx0 and i<=idx1:\n            char_targets[i] = 1\n\nprint('char_targets:',char_targets)\n# encoding tweet using tokenizer, it returns ids(token for each word) and offsets(span of each word)\ntok_tweet = tokenizer.encode(tweet)\n\ninput_ids = tok_tweet.ids # word ids given by tokenizer stripping first[cls] and last token [sep]\noffsets = tok_tweet.offsets # offsets of the tweet \n\nprint('input_ids:',input_ids)\nprint('offsets:',offsets)\n# start index and end index of tweet words with select_text\ntargets_index = []\nfor i, (off1,off2) in enumerate(offsets):\n    if sum(char_targets[off1:off2])>0:\n        targets_index.append(i)       \ntarget_start = targets_index[0] \ntarget_end = targets_index[-1]\n\nprint('target_start:',target_start)\nprint('target_end:', target_end)\n\n# creating ids, token_type_ids, mask into bert format, changing target_start and target_end accordingly.\nids = [0] + input_ids  + [2,2] + [sentiment_id[sentiment]] + [2]\ntoken_type_ids = [0]*(len(ids))\nmask = [1] * len(ids)\ntarget_start+=1\ntarget_end+=1\noffsets = [(0,0)]*1 + offsets + [(0,0)]*4\n\n# padding \npadding_length = max_len - len(ids)\nif padding_length > 0:\n    ids = ids + ([0] * padding_length)\n    mask = mask + ([0] * padding_length)\n    token_type_ids = token_type_ids + ([0] * padding_length)\n    offsets = offsets + ([(0, 0)] * padding_length)\n","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-25T18:52:19.223705Z","iopub.status.busy":"2020-10-25T18:52:19.222881Z","iopub.status.idle":"2020-10-25T18:52:19.225408Z","shell.execute_reply":"2020-10-25T18:52:19.225902Z"},"papermill":{"duration":0.049485,"end_time":"2020-10-25T18:52:19.226013","exception":false,"start_time":"2020-10-25T18:52:19.176528","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# https://www.kaggle.com/abhishek/bert-base-uncased-using-pytorch\n\ndef text_process(tweet, sentiment, tokenizer, max_len, selected_text=None):\n    \n    \"\"\"\n    inputs:\n    tweets: text \n    sentiment: sentiment of the tweet\n    tokenizer: tokenizer\n    max_len: max length of ids, mask and token_type_ids (inputs of bert)\n    selected_text: selected_text (optional)\n    \n    operation:\n    \n    Given inputs it calculates  ids, mask , token_type_ids, offsets and target_start and target_end.\n    \n    outputs:\n    dictionary with keys as below,\n    ids: input tokens for roberta in format as [0] <text tokens> [2] [2] <sentiment> [2]\n    mask: array with length as max_len and has 1's in the indices of text and zeros elsewhere.\n    token_type_ids: all 0s , size max_len\n    target_start,target_end: begin and end of select_text (returned only when select_text is given)\n    offsets: offsets of text \n    \n    \"\"\"\n    \n    if selected_text!=None:\n        \n        tweet = \" \" + tweet\n        idx0 = None\n        idx1 = None\n        st_len = len(selected_text)\n        for i in range(len(tweet)):\n            if(tweet[i:i+st_len]==selected_text):\n                idx0 = i\n                idx1 = i + st_len -1\n                break\n\n        char_targets = [0]*len(tweet)\n\n        for i in range(len(tweet)):\n            if idx0 != None and idx1!=None:\n                if i>=idx0 and i<=idx1:\n                    char_targets[i] = 1\n\n        tok_tweet = tokenizer.encode(tweet)\n\n        input_ids = tok_tweet.ids\n        offsets = tok_tweet.offsets\n        \n        targets_index = []\n\n        for i, (off1,off2) in enumerate(offsets):\n            if sum(char_targets[off1:off2])>0:\n                targets_index.append(i)\n\n        target_start = targets_index[0] \n        target_end = targets_index[-1]\n        \n        \n        ids = [0] + input_ids  + [2,2] + [sentiment_id[sentiment]] + [2]\n        token_type_ids = [0]*(len(ids))\n        mask = [1] * len(ids)\n        target_start+=1\n        target_end+=1\n        offsets = [(0,0)]*1 + offsets + [(0,0)]*4\n\n        padding_length = max_len - len(ids)\n        if padding_length > 0:\n            ids = ids + ([0] * padding_length)\n            mask = mask + ([0] * padding_length)\n            token_type_ids = token_type_ids + ([0] * padding_length)\n            offsets = offsets + ([(0, 0)] * padding_length)\n            \n        return {\n            \n            'ids': ids,\n            'token_type_ids':token_type_ids,\n            'mask':mask,\n            'target_start':target_start,\n            'target_end':target_end,\n            'offsets':offsets\n        }\n    else:\n        \n        tweet = \" \" + tweet\n        tok_tweet = tokenizer.encode(tweet)\n        \n        input_ids = tok_tweet.ids\n        offsets = tok_tweet.offsets\n\n        ids = [0] + input_ids  + [2,2] + [sentiment_id[sentiment]] + [2]\n        token_type_ids = [0]*(len(ids))\n        mask = [1] * len(ids)\n        offsets = [(0,0)]*1 + offsets + [(0,0)]*4\n\n        padding_length = max_len - len(ids)\n        if padding_length > 0:\n            ids = ids + ([0] * padding_length)\n            mask = mask + ([0] * padding_length)\n            token_type_ids = token_type_ids + ([0] * padding_length)\n            offsets = offsets + ([(0, 0)] * padding_length)\n            \n        return {\n            \n            'ids': ids,\n            'token_type_ids':token_type_ids,\n            'mask':mask,\n            'offsets':offsets\n        }\n\n","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-25T18:52:19.281368Z","iopub.status.busy":"2020-10-25T18:52:19.278447Z","iopub.status.idle":"2020-10-25T18:52:24.394172Z","shell.execute_reply":"2020-10-25T18:52:24.39478Z"},"papermill":{"duration":5.14902,"end_time":"2020-10-25T18:52:24.397407","exception":false,"start_time":"2020-10-25T18:52:19.248387","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"rows = x_train.shape[0]\n\ntrain_ids = np.zeros((rows,max_len), dtype = 'int32')\ntrain_mask = np.zeros((rows,max_len), dtype = 'int32')\ntrain_type_ids = np.zeros((rows,max_len), dtype = 'int32')\ntrain_start_idx = np.zeros((rows,max_len), dtype = 'int32')\ntrain_end_idx = np.zeros((rows,max_len), dtype = 'int32')\n\nfor i in range(x_train.shape[0]):\n    \n    encoding = text_process(x_train.loc[i,'text'], x_train.loc[i,'sentiment'], tokenizer, max_len,x_train.loc[i,'selected_text'] )\n    \n    train_ids[i] = encoding['ids']\n    train_start_idx[i,encoding['target_start']] = 1\n    train_end_idx[i, encoding['target_end']] = 1\n    \n    train_type_ids[i] = encoding['token_type_ids']\n    train_mask[i] = encoding['mask']","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-25T18:52:24.621766Z","iopub.status.busy":"2020-10-25T18:52:24.620782Z","iopub.status.idle":"2020-10-25T18:52:24.637383Z","shell.execute_reply":"2020-10-25T18:52:24.638479Z"},"papermill":{"duration":0.112951,"end_time":"2020-10-25T18:52:24.638661","exception":false,"start_time":"2020-10-25T18:52:24.52571","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# checking\n\ni = 10\n\nencoding = text_process(x_train.loc[i,'text'], x_train.loc[i,'sentiment'], tokenizer, max_len,x_train.loc[i,'selected_text'] )\n\ntrain_ids[i] = encoding['ids']\n\n\ntrain_start_idx[i,encoding['target_start']] = 1\ntrain_end_idx[i, encoding['target_end']] = 1\n\ntrain_type_ids[i] = encoding['token_type_ids']\ntrain_mask[i] = encoding['mask']\n\ntweet = x_train.loc[i,'text']\nselect_text = x_train.loc[i,'selected_text']\n\ntarget_start = np.argmax(train_start_idx[i,])\ntarget_end = np.argmax(train_end_idx[i, ])\n\noffsets = encoding['offsets']\n\n\nprint('tweet:',tweet)\nprint('selected_text:',select_text)\nprint(tweet[offsets[target_start][0]:offsets[target_end][-1]])","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-25T18:52:24.724825Z","iopub.status.busy":"2020-10-25T18:52:24.723766Z","iopub.status.idle":"2020-10-25T18:52:25.972706Z","shell.execute_reply":"2020-10-25T18:52:25.974045Z"},"papermill":{"duration":1.300685,"end_time":"2020-10-25T18:52:25.974253","exception":false,"start_time":"2020-10-25T18:52:24.673568","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"rows = x_cv.shape[0]\n\ncv_ids = np.zeros((rows,max_len), dtype = 'int32')\ncv_mask = np.zeros((rows,max_len), dtype = 'int32')\ncv_type_ids = np.zeros((rows,max_len), dtype = 'int32')\ncv_start_idx = np.zeros((rows,max_len), dtype = 'int32')\ncv_end_idx = np.zeros((rows,max_len), dtype = 'int32')\n\nfor i in range(x_cv.shape[0]):\n    \n    encoding = text_process(x_cv.loc[i,'text'], x_cv.loc[i,'sentiment'], tokenizer, max_len,x_cv.loc[i,'selected_text'] )\n    \n    cv_ids[i] = encoding['ids']\n    cv_start_idx[i,encoding['target_start']] = 1\n    cv_end_idx[i, encoding['target_end']] = 1\n    \n    cv_type_ids[i] = encoding['token_type_ids']\n    cv_mask[i] = encoding['mask']","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-25T18:52:26.060501Z","iopub.status.busy":"2020-10-25T18:52:26.059615Z","iopub.status.idle":"2020-10-25T18:52:26.513676Z","shell.execute_reply":"2020-10-25T18:52:26.512591Z"},"papermill":{"duration":0.501802,"end_time":"2020-10-25T18:52:26.513797","exception":false,"start_time":"2020-10-25T18:52:26.011995","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"rows = x_test.shape[0]\n\ntest_ids = np.zeros((rows,max_len), dtype = 'int32')\ntest_mask = np.zeros((rows,max_len), dtype = 'int32')\ntest_type_ids = np.zeros((rows,max_len), dtype = 'int32')\ntest_start_idx = np.zeros((rows,max_len), dtype = 'int32')\ntest_end_idx = np.zeros((rows,max_len), dtype = 'int32')\n\nfor i in range(x_test.shape[0]):\n    \n    encoding = text_process(x_test.loc[i,'text'], x_test.loc[i,'sentiment'], tokenizer, max_len,x_test.loc[i,'selected_text'] )\n    \n    test_ids[i] = encoding['ids']\n    test_start_idx[i,encoding['target_start']] = 1\n    test_end_idx[i, encoding['target_end']] = 1\n    \n    test_type_ids[i] = encoding['token_type_ids']\n    test_mask[i] = encoding['mask']","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.023789,"end_time":"2020-10-25T18:52:26.561663","exception":false,"start_time":"2020-10-25T18:52:26.537874","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## Modelling"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-25T18:52:26.615952Z","iopub.status.busy":"2020-10-25T18:52:26.615172Z","iopub.status.idle":"2020-10-25T18:52:26.61819Z","shell.execute_reply":"2020-10-25T18:52:26.617546Z"},"papermill":{"duration":0.0331,"end_time":"2020-10-25T18:52:26.618309","exception":false,"start_time":"2020-10-25T18:52:26.585209","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Metric\ndef jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    \n    if (len(a)==0) & (len(b)==0): \n        return 0.5\n    \n    c = a.intersection(b)\n    \n    return float(len(c)) / (len(a) + len(b) - len(c))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Config\nconfig = RobertaConfig.from_pretrained('../input/tf-roberta/config-roberta-base.json')\nprint(config)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-25T18:52:26.676731Z","iopub.status.busy":"2020-10-25T18:52:26.673269Z","iopub.status.idle":"2020-10-25T18:52:26.679478Z","shell.execute_reply":"2020-10-25T18:52:26.679007Z"},"papermill":{"duration":0.037696,"end_time":"2020-10-25T18:52:26.679576","exception":false,"start_time":"2020-10-25T18:52:26.64188","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def build_model():\n    # Create Model\n       \n    ids = tf.keras.layers.Input((max_len,), dtype=tf.int32)\n    att = tf.keras.layers.Input((max_len,), dtype=tf.int32)\n    tok = tf.keras.layers.Input((max_len,), dtype=tf.int32)\n\n\n        \n    roberta = TFRobertaForQuestionAnswering.from_pretrained('../input/tf-roberta/pretrained-roberta-base.h5', config = config)\n    x = roberta(ids, attention_mask = att, token_type_ids = tok)\n\n    x1 = tf.keras.layers.Dropout(0.3)(x[0]) \n    x1 = tf.keras.layers.Activation('softmax')(x1)\n\n    x2 = tf.keras.layers.Dropout(0.3)(x[1]) \n    x2 = tf.keras.layers.Activation('softmax')(x2)\n\n    model = tf.keras.models.Model(inputs = [ids, att, tok], outputs=[x1, x2])\n    optimizer = tf.keras.optimizers.Adam(learning_rate = 3e-5)\n\n   # model.compile(loss = custom_loss, optimizer = optimizer)\n    model.compile(loss= 'categorical_crossentropy', optimizer=optimizer)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-25T18:52:26.738388Z","iopub.status.busy":"2020-10-25T18:52:26.737629Z","iopub.status.idle":"2020-10-25T18:52:26.740642Z","shell.execute_reply":"2020-10-25T18:52:26.740138Z"},"papermill":{"duration":0.034573,"end_time":"2020-10-25T18:52:26.740731","exception":false,"start_time":"2020-10-25T18:52:26.706158","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"filepath = \"/kaggle/working/best_model.h5\" \ncheckpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only = True, mode = 'auto', save_freq = 'epoch')\n","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-25T18:52:26.796006Z","iopub.status.busy":"2020-10-25T18:52:26.795253Z","iopub.status.idle":"2020-10-25T18:53:00.336169Z","shell.execute_reply":"2020-10-25T18:53:00.345952Z"},"papermill":{"duration":33.581315,"end_time":"2020-10-25T18:53:00.346169","exception":false,"start_time":"2020-10-25T18:52:26.764854","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"model = build_model()\n\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-25T18:53:00.565984Z","iopub.status.busy":"2020-10-25T18:53:00.564545Z","iopub.status.idle":"2020-10-25T18:53:00.569219Z","shell.execute_reply":"2020-10-25T18:53:00.56979Z"},"papermill":{"duration":0.198366,"end_time":"2020-10-25T18:53:00.569949","exception":false,"start_time":"2020-10-25T18:53:00.371583","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# clearing space\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-25T18:53:02.052396Z","iopub.status.busy":"2020-10-25T18:53:02.014361Z","iopub.status.idle":"2020-10-25T18:53:14.222961Z","shell.execute_reply":"2020-10-25T18:53:14.221986Z"},"papermill":{"duration":13.627851,"end_time":"2020-10-25T18:53:14.223077","exception":false,"start_time":"2020-10-25T18:53:00.595226","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Training or loading trained model for predictions.\nif train_mode:\n    model.fit([train_ids, train_mask, train_type_ids], [train_start_idx, train_end_idx], \n                      epochs = 3, \n                      batch_size = 16, \n                      verbose = True, \n                      callbacks = [checkpoint],\n                      validation_data = ([cv_ids,cv_mask,cv_type_ids], [cv_start_idx, cv_end_idx]),\n                      shuffle = True)\nelse:\n    \n    model.load_weights('../input/tse-best-roberta/best_model (6).h5')","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-25T18:53:14.280962Z","iopub.status.busy":"2020-10-25T18:53:14.279736Z","iopub.status.idle":"2020-10-25T18:53:14.282942Z","shell.execute_reply":"2020-10-25T18:53:14.282417Z"},"papermill":{"duration":0.033868,"end_time":"2020-10-25T18:53:14.28304","exception":false,"start_time":"2020-10-25T18:53:14.249172","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"rows = x_test.shape[0]\n\npreds_start = np.zeros((rows,max_len))\npreds_end = np.zeros((rows,max_len))","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-25T18:53:14.33906Z","iopub.status.busy":"2020-10-25T18:53:14.33808Z","iopub.status.idle":"2020-10-25T18:53:41.602511Z","shell.execute_reply":"2020-10-25T18:53:41.601955Z"},"papermill":{"duration":27.294084,"end_time":"2020-10-25T18:53:41.602623","exception":false,"start_time":"2020-10-25T18:53:14.308539","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"preds = model.predict([test_ids, test_mask, test_type_ids], verbose = True)\npreds_start += preds[0]\npreds_end += preds[1] ","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-25T18:53:41.833051Z","iopub.status.busy":"2020-10-25T18:53:41.748029Z","iopub.status.idle":"2020-10-25T18:53:41.9394Z","shell.execute_reply":"2020-10-25T18:53:41.940599Z"},"papermill":{"duration":0.29573,"end_time":"2020-10-25T18:53:41.940767","exception":false,"start_time":"2020-10-25T18:53:41.645037","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"\nscore = 0\nfor k in range(x_test.shape[0]):\n    \n        \n    \n        encoding = text_process(x_test.loc[k,'text'], x_test.loc[k,'sentiment'], tokenizer, max_len)\n        offsets = encoding['offsets']\n        #targets_start,targets_end = model.predict(encoding['ids'], encoding['mask'], encoding['token_type_ids'] )\n        targets_start = np.argmax(preds_start[k,])\n        targets_end = np.argmax(preds_end[k,])\n\n        pred = x_test.loc[k,'text'][offsets[targets_start][0]:offsets[targets_end][-1]]\n        score+=jaccard(x_test.loc[k,'selected_text'], pred)\n\n    \n    \nscore=score/x_test.shape[0]    \nprint('score on local test_data',score)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-25T18:53:42.038499Z","iopub.status.busy":"2020-10-25T18:53:42.035751Z","iopub.status.idle":"2020-10-25T18:53:42.063193Z","shell.execute_reply":"2020-10-25T18:53:42.061803Z"},"papermill":{"duration":0.080984,"end_time":"2020-10-25T18:53:42.063334","exception":false,"start_time":"2020-10-25T18:53:41.98235","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# prediction samples\n\nfor k in range(0,x_test.shape[0],100):\n    \n        encoding = text_process(x_test.loc[k,'text'], x_test.loc[k,'sentiment'], tokenizer, max_len)\n        offsets = encoding['offsets']\n        targets_start = np.argmax(preds_start[k,])\n        targets_end = np.argmax(preds_end[k,])\n\n        pred = x_test.loc[k,'text'][offsets[targets_start][0]:offsets[targets_end][-1]]\n        print('text:', x_test.text[k])\n        print('selected text:', x_test.selected_text[k])\n        print('sentiment:',x_test.sentiment[k] )\n        print('predicted:', pred)\n        print('jaccard_score:', jaccard(pred, x_test.loc[k,'selected_text']))\n        print('#########################################')\n    ","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.043943,"end_time":"2020-10-25T18:53:42.160171","exception":false,"start_time":"2020-10-25T18:53:42.116228","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## Test Predictions"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-25T18:53:42.256461Z","iopub.status.busy":"2020-10-25T18:53:42.255823Z","iopub.status.idle":"2020-10-25T18:53:42.287452Z","shell.execute_reply":"2020-10-25T18:53:42.287966Z"},"papermill":{"duration":0.084777,"end_time":"2020-10-25T18:53:42.2881","exception":false,"start_time":"2020-10-25T18:53:42.203323","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"x_test = pd.read_csv('../input/tweet-sentiment-extraction/test.csv')\nrows = x_test.shape[0]\npreds_start = np.zeros((rows,max_len))\npreds_end = np.zeros((rows,max_len))","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-25T18:53:42.382061Z","iopub.status.busy":"2020-10-25T18:53:42.381432Z","iopub.status.idle":"2020-10-25T18:53:43.009489Z","shell.execute_reply":"2020-10-25T18:53:43.008955Z"},"papermill":{"duration":0.678802,"end_time":"2020-10-25T18:53:43.009604","exception":false,"start_time":"2020-10-25T18:53:42.330802","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"rows = x_test.shape[0]\n\ntest_ids = np.zeros((rows,max_len), dtype = 'int32')\ntest_mask = np.zeros((rows,max_len), dtype = 'int32')\ntest_type_ids = np.zeros((rows,max_len), dtype = 'int32')\ntest_start_idx = np.zeros((rows,max_len), dtype = 'int32')\ntest_end_idx = np.zeros((rows,max_len), dtype = 'int32')\n\nfor i in range(x_test.shape[0]):\n    \n    encoding = text_process(x_test.loc[i,'text'], x_test.loc[i,'sentiment'], tokenizer, max_len )\n    test_ids[i] = encoding['ids']\n    test_type_ids[i] = encoding['token_type_ids']\n    test_mask[i] = encoding['mask']\n","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-25T18:53:43.104216Z","iopub.status.busy":"2020-10-25T18:53:43.103214Z","iopub.status.idle":"2020-10-25T18:54:38.212518Z","shell.execute_reply":"2020-10-25T18:54:38.212999Z"},"papermill":{"duration":55.160978,"end_time":"2020-10-25T18:54:38.213162","exception":false,"start_time":"2020-10-25T18:53:43.052184","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"preds = model.predict([test_ids, test_mask, test_type_ids], verbose = True)\npreds_start += preds[0]\npreds_end += preds[1] ","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-25T18:54:38.398501Z","iopub.status.busy":"2020-10-25T18:54:38.39786Z","iopub.status.idle":"2020-10-25T18:54:38.410981Z","shell.execute_reply":"2020-10-25T18:54:38.410441Z"},"papermill":{"duration":0.098321,"end_time":"2020-10-25T18:54:38.411087","exception":false,"start_time":"2020-10-25T18:54:38.312766","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('../input/tweet-sentiment-extraction/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-25T18:54:38.828973Z","iopub.status.busy":"2020-10-25T18:54:38.735677Z","iopub.status.idle":"2020-10-25T18:54:39.586613Z","shell.execute_reply":"2020-10-25T18:54:39.585365Z"},"papermill":{"duration":1.096526,"end_time":"2020-10-25T18:54:39.58676","exception":false,"start_time":"2020-10-25T18:54:38.490234","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"\nfor k in range(x_test.shape[0]):\n    \n    encoding = text_process(x_test.loc[k,'text'], x_test.loc[k,'sentiment'], tokenizer, max_len)\n    offsets = encoding['offsets']\n    targets_start = np.argmax(preds_start[k,])\n    targets_end = np.argmax(preds_end[k,])\n\n    pred = x_test.loc[k,'text'][offsets[targets_start][0]:offsets[targets_end][-1]]\n    submission.loc[k,'selected_text'] = pred\n\n","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-25T18:54:39.822735Z","iopub.status.busy":"2020-10-25T18:54:39.821622Z","iopub.status.idle":"2020-10-25T18:54:40.109565Z","shell.execute_reply":"2020-10-25T18:54:40.110711Z"},"papermill":{"duration":0.410564,"end_time":"2020-10-25T18:54:40.110903","exception":false,"start_time":"2020-10-25T18:54:39.700339","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}