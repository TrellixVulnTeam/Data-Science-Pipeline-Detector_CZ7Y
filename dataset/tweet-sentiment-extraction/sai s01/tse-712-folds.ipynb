{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2020-10-28T08:46:28.661272Z","iopub.status.busy":"2020-10-28T08:46:28.66042Z","iopub.status.idle":"2020-10-28T08:46:38.281759Z","shell.execute_reply":"2020-10-28T08:46:38.280992Z"},"papermill":{"duration":9.65624,"end_time":"2020-10-28T08:46:38.28191","exception":false,"start_time":"2020-10-28T08:46:28.62567","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# importing libraries\n\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom transformers import *\nimport tokenizers\nimport os\nfrom tokenizers import BertWordPieceTokenizer\nfrom transformers import BertTokenizer, TFBertModel, BertConfig\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom transformers import BertTokenizer, TFBertForQuestionAnswering\nfrom sklearn.model_selection import train_test_split\nimport gc\nfrom keras.callbacks import ModelCheckpoint\nfrom sklearn.model_selection import StratifiedKFold\nfrom keras.utils import plot_model\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.execute_input":"2020-10-28T08:46:38.458786Z","iopub.status.busy":"2020-10-28T08:46:38.457732Z","iopub.status.idle":"2020-10-28T08:46:38.461005Z","shell.execute_reply":"2020-10-28T08:46:38.460354Z"},"papermill":{"duration":0.149389,"end_time":"2020-10-28T08:46:38.461128","exception":false,"start_time":"2020-10-28T08:46:38.311739","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"max_len = 168\ntrain_mode = False\n\n# Load the tokenizer\ntokenizer = tokenizers.ByteLevelBPETokenizer(vocab_file = '../input/tf-roberta/vocab-roberta-base.json',\n                             merges_file = '../input/tf-roberta/merges-roberta-base.txt',\n                             add_prefix_space = True,\n                             lowercase = True)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-28T08:46:38.520584Z","iopub.status.busy":"2020-10-28T08:46:38.519829Z","iopub.status.idle":"2020-10-28T08:46:38.652588Z","shell.execute_reply":"2020-10-28T08:46:38.651986Z"},"papermill":{"duration":0.164452,"end_time":"2020-10-28T08:46:38.652714","exception":false,"start_time":"2020-10-28T08:46:38.488262","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# loading train data.\nsentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}\nx_train = pd.read_csv('../input/tweet-sentiment-extraction/train.csv').fillna('')\nx_train.head()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-28T08:46:38.722452Z","iopub.status.busy":"2020-10-28T08:46:38.7214Z","iopub.status.idle":"2020-10-28T08:46:38.755498Z","shell.execute_reply":"2020-10-28T08:46:38.754765Z"},"papermill":{"duration":0.073342,"end_time":"2020-10-28T08:46:38.755624","exception":false,"start_time":"2020-10-28T08:46:38.682282","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# removing empty rows\nx_train['text'].replace('', np.nan, inplace=True)\nx_train.dropna(subset=['text'], inplace=True)\nx_train.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.028754,"end_time":"2020-10-28T08:46:38.996354","exception":false,"start_time":"2020-10-28T08:46:38.9676","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## Data Preparation"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-28T08:46:39.082395Z","iopub.status.busy":"2020-10-28T08:46:39.061381Z","iopub.status.idle":"2020-10-28T08:46:39.090109Z","shell.execute_reply":"2020-10-28T08:46:39.089143Z"},"papermill":{"duration":0.065078,"end_time":"2020-10-28T08:46:39.090304","exception":false,"start_time":"2020-10-28T08:46:39.025226","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# https://www.kaggle.com/abhishek/bert-base-uncased-using-pytorch\n# Example to explain operations in text_process function\n\ntweet = 'WHY DO WE FALL? SO THAT WE CAN LEARN TO PICK OURSELVES BACK UP.'\nselected_text = 'LEARN TO PICK OURSELVES BACK UP'\nsentiment = 'positive'\nprint('text',tweet)\nprint('select_text:',selected_text)\nprint('sentiment:', sentiment)\n# idx0 and idx1 and start and end indices of select_text in tweet.\nidx0 = None\nidx1 = None\nst_len = len(selected_text)\n\n# with roberta 'hello' and ' hello' are different hence we need to add a space for beginning of tweet.\n\ntweet = \" \" + tweet\n\n\nfor i in range(len(tweet)):\n    if(tweet[i:i+st_len]==selected_text):\n        idx0 = i\n        idx1 = i + st_len -1\n        break\n\n# char_tartgets is of length tweet, assign indices with select text =1 \nchar_targets = [0]*len(tweet)\nfor i in range(len(tweet)):\n    if idx0 != None and idx1!=None:\n        if i>=idx0 and i<=idx1:\n            char_targets[i] = 1\n\nprint('char_targets:',char_targets)\n# encoding tweet using tokenizer, it returns ids(token for each word) and offsets(span of each word)\ntok_tweet = tokenizer.encode(tweet)\n\ninput_ids = tok_tweet.ids # word ids given by tokenizer stripping first[cls] and last token [sep]\noffsets = tok_tweet.offsets # offsets of the tweet \n\nprint('input_ids:',input_ids)\nprint('offsets:',offsets)\n# start index and end index of tweet words with select_text\ntargets_index = []\nfor i, (off1,off2) in enumerate(offsets):\n    if sum(char_targets[off1:off2])>0:\n        targets_index.append(i)       \ntarget_start = targets_index[0] \ntarget_end = targets_index[-1]\n\nprint('target_start:',target_start)\nprint('target_end:', target_end)\n\n# creating ids, token_type_ids, mask into bert format, changing target_start and target_end accordingly.\nids = [0] + input_ids  + [2,2] + [sentiment_id[sentiment]] + [2]\ntoken_type_ids = [0]*(len(ids))\nmask = [1] * len(ids)\ntarget_start+=1\ntarget_end+=1\noffsets = [(0,0)]*1 + offsets + [(0,0)]*4\n\n# padding \npadding_length = max_len - len(ids)\nif padding_length > 0:\n    ids = ids + ([0] * padding_length)\n    mask = mask + ([0] * padding_length)\n    token_type_ids = token_type_ids + ([0] * padding_length)\n    offsets = offsets + ([(0, 0)] * padding_length)\n","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-28T08:46:39.176875Z","iopub.status.busy":"2020-10-28T08:46:39.172638Z","iopub.status.idle":"2020-10-28T08:46:39.180727Z","shell.execute_reply":"2020-10-28T08:46:39.179621Z"},"papermill":{"duration":0.06116,"end_time":"2020-10-28T08:46:39.180843","exception":false,"start_time":"2020-10-28T08:46:39.119683","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# https://www.kaggle.com/abhishek/bert-base-uncased-using-pytorch\n\ndef text_process(tweet, sentiment, tokenizer, max_len, selected_text=None):\n    \n    \"\"\"\n    inputs:\n    tweets: text \n    sentiment: sentiment of the tweet\n    tokenizer: tokenizer\n    max_len: max length of ids, mask and token_type_ids (inputs of bert)\n    selected_text: selected_text (optional)\n    \n    operation:\n    \n    Given inputs it calculates  ids, mask , token_type_ids, offsets and target_start and target_end.\n    \n    outputs:\n    dictionary with keys as below,\n    ids: input tokens for roberta in format as [0] <text tokens> [2] [2] <sentiment> [2]\n    mask: array with length as max_len and has 1's in the indices of text and zeros elsewhere.\n    token_type_ids: all 0s , size max_len\n    target_start,target_end: begin and end of select_text (returned only when select_text is given)\n    offsets: offsets of text \n    \n    \"\"\"\n    \n    if selected_text!=None:\n        \n        tweet = \" \" + tweet\n        idx0 = None\n        idx1 = None\n        st_len = len(selected_text)\n        for i in range(len(tweet)):\n            if(tweet[i:i+st_len]==selected_text):\n                idx0 = i\n                idx1 = i + st_len -1\n                break\n\n        char_targets = [0]*len(tweet)\n\n        for i in range(len(tweet)):\n            if idx0 != None and idx1!=None:\n                if i>=idx0 and i<=idx1:\n                    char_targets[i] = 1\n\n        tok_tweet = tokenizer.encode(tweet)\n\n        input_ids = tok_tweet.ids\n        offsets = tok_tweet.offsets\n        \n        targets_index = []\n\n        for i, (off1,off2) in enumerate(offsets):\n            if sum(char_targets[off1:off2])>0:\n                targets_index.append(i)\n\n        target_start = targets_index[0] \n        target_end = targets_index[-1]\n        \n        \n        ids = [0] + input_ids  + [2,2] + [sentiment_id[sentiment]] + [2]\n        token_type_ids = [0]*(len(ids))\n        mask = [1] * len(ids)\n        target_start+=1\n        target_end+=1\n        offsets = [(0,0)]*1 + offsets + [(0,0)]*4\n\n        padding_length = max_len - len(ids)\n        if padding_length > 0:\n            ids = ids + ([0] * padding_length)\n            mask = mask + ([0] * padding_length)\n            token_type_ids = token_type_ids + ([0] * padding_length)\n            offsets = offsets + ([(0, 0)] * padding_length)\n            \n        return {\n            \n            'ids': ids,\n            'token_type_ids':token_type_ids,\n            'mask':mask,\n            'target_start':target_start,\n            'target_end':target_end,\n            'offsets':offsets\n        }\n    else:\n        \n        tweet = \" \" + tweet\n        tok_tweet = tokenizer.encode(tweet)\n        \n        input_ids = tok_tweet.ids\n        offsets = tok_tweet.offsets\n\n        ids = [0] + input_ids  + [2,2] + [sentiment_id[sentiment]] + [2]\n        token_type_ids = [0]*(len(ids))\n        mask = [1] * len(ids)\n        offsets = [(0,0)]*1 + offsets + [(0,0)]*4\n\n        padding_length = max_len - len(ids)\n        if padding_length > 0:\n            ids = ids + ([0] * padding_length)\n            mask = mask + ([0] * padding_length)\n            token_type_ids = token_type_ids + ([0] * padding_length)\n            offsets = offsets + ([(0, 0)] * padding_length)\n            \n        return {\n            \n            'ids': ids,\n            'token_type_ids':token_type_ids,\n            'mask':mask,\n            'offsets':offsets\n        }\n\n","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-28T08:46:39.251806Z","iopub.status.busy":"2020-10-28T08:46:39.250878Z","iopub.status.idle":"2020-10-28T08:46:51.311606Z","shell.execute_reply":"2020-10-28T08:46:51.310973Z"},"papermill":{"duration":12.101614,"end_time":"2020-10-28T08:46:51.311738","exception":false,"start_time":"2020-10-28T08:46:39.210124","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# creating ids,type_ids,mask , start_index and end_index for train set.\nrows = x_train.shape[0]\n\ntrain_ids = np.zeros((rows,max_len), dtype = 'int32')\ntrain_mask = np.zeros((rows,max_len), dtype = 'int32')\ntrain_type_ids = np.zeros((rows,max_len), dtype = 'int32')\ntrain_start_idx = np.zeros((rows,max_len), dtype = 'int32')\ntrain_end_idx = np.zeros((rows,max_len), dtype = 'int32')\n\nfor i in range(x_train.shape[0]):\n    \n    encoding = text_process(x_train.loc[i,'text'], x_train.loc[i,'sentiment'], tokenizer, max_len,x_train.loc[i,'selected_text'] )\n    \n    train_ids[i] = encoding['ids']\n    train_start_idx[i,encoding['target_start']] = 1\n    train_end_idx[i, encoding['target_end']] = 1\n    \n    train_type_ids[i] = encoding['token_type_ids']\n    train_mask[i] = encoding['mask']","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-28T08:46:51.384031Z","iopub.status.busy":"2020-10-28T08:46:51.381987Z","iopub.status.idle":"2020-10-28T08:46:51.386857Z","shell.execute_reply":"2020-10-28T08:46:51.386041Z"},"papermill":{"duration":0.047033,"end_time":"2020-10-28T08:46:51.387002","exception":false,"start_time":"2020-10-28T08:46:51.339969","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# checking\n\ni = 10\n\nencoding = text_process(x_train.loc[i,'text'], x_train.loc[i,'sentiment'], tokenizer, max_len,x_train.loc[i,'selected_text'] )\n\ntrain_ids[i] = encoding['ids']\n\n\ntrain_start_idx[i,encoding['target_start']] = 1\ntrain_end_idx[i, encoding['target_end']] = 1\n\ntrain_type_ids[i] = encoding['token_type_ids']\ntrain_mask[i] = encoding['mask']\n\ntweet = x_train.loc[i,'text']\nselect_text = x_train.loc[i,'selected_text']\n\ntarget_start = np.argmax(train_start_idx[i,])\ntarget_end = np.argmax(train_end_idx[i, ])\n\noffsets = encoding['offsets']\n\n\nprint('tweet:',tweet)\nprint('selected_text:',select_text)\nprint(tweet[offsets[target_start][0]:offsets[target_end][-1]])","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.028349,"end_time":"2020-10-28T08:46:52.114904","exception":false,"start_time":"2020-10-28T08:46:52.086555","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## Modelling"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-28T08:46:52.18112Z","iopub.status.busy":"2020-10-28T08:46:52.179073Z","iopub.status.idle":"2020-10-28T08:46:52.181891Z","shell.execute_reply":"2020-10-28T08:46:52.182458Z"},"papermill":{"duration":0.039732,"end_time":"2020-10-28T08:46:52.182596","exception":false,"start_time":"2020-10-28T08:46:52.142864","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Metric\ndef jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    \n    if (len(a)==0) & (len(b)==0): \n        return 0.5\n    \n    c = a.intersection(b)\n    \n    return float(len(c)) / (len(a) + len(b) - len(c))","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-28T08:46:52.244355Z","iopub.status.busy":"2020-10-28T08:46:52.243544Z","iopub.status.idle":"2020-10-28T08:46:52.252869Z","shell.execute_reply":"2020-10-28T08:46:52.253614Z"},"papermill":{"duration":0.043061,"end_time":"2020-10-28T08:46:52.253788","exception":false,"start_time":"2020-10-28T08:46:52.210727","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Config\nconfig = RobertaConfig.from_pretrained('../input/tf-roberta/config-roberta-base.json')\nprint(config)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-28T08:46:52.328682Z","iopub.status.busy":"2020-10-28T08:46:52.326823Z","iopub.status.idle":"2020-10-28T08:46:52.329395Z","shell.execute_reply":"2020-10-28T08:46:52.329864Z"},"papermill":{"duration":0.044864,"end_time":"2020-10-28T08:46:52.329983","exception":false,"start_time":"2020-10-28T08:46:52.285119","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# https://www.kaggle.com/c/tweet-sentiment-extraction/discussion/143281\ndef build_model():\n    # Create Model\n       \n    ids = tf.keras.layers.Input((max_len,), dtype=tf.int32)\n    att = tf.keras.layers.Input((max_len,), dtype=tf.int32)\n    tok = tf.keras.layers.Input((max_len,), dtype=tf.int32)\n\n    roberta = TFRobertaForQuestionAnswering.from_pretrained('../input/tf-roberta/pretrained-roberta-base.h5', config = config)\n    x = roberta(ids, attention_mask = att, token_type_ids = tok)\n\n    x1 = tf.keras.layers.Dropout(0.3)(x[0]) \n    x1 = tf.keras.layers.Activation('softmax')(x1)\n\n    x2 = tf.keras.layers.Dropout(0.3)(x[1]) \n    x2 = tf.keras.layers.Activation('softmax')(x2)\n\n    model = tf.keras.models.Model(inputs = [ids, att, tok], outputs=[x1, x2])\n    optimizer = tf.keras.optimizers.Adam(learning_rate = 3e-5)\n\n    model.compile(loss= 'categorical_crossentropy', optimizer=optimizer)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def lr_scheduler(epoch):\n    return 3e-5 * 0.2**epoch","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-28T08:46:52.462871Z","iopub.status.busy":"2020-10-28T08:46:52.462093Z","iopub.status.idle":"2020-10-28T08:47:09.49168Z","shell.execute_reply":"2020-10-28T08:47:09.492168Z"},"papermill":{"duration":17.066532,"end_time":"2020-10-28T08:47:09.492336","exception":false,"start_time":"2020-10-28T08:46:52.425804","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"\nmodel = build_model()\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plotting model\nplot_model(model, show_shapes = True)\n","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-28T08:47:09.72411Z","iopub.status.busy":"2020-10-28T08:47:09.722917Z","iopub.status.idle":"2020-10-28T08:47:09.727018Z","shell.execute_reply":"2020-10-28T08:47:09.72759Z"},"papermill":{"duration":0.200084,"end_time":"2020-10-28T08:47:09.727728","exception":false,"start_time":"2020-10-28T08:47:09.527644","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# clearing space\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-28T08:47:09.805807Z","iopub.status.busy":"2020-10-28T08:47:09.79561Z","iopub.status.idle":"2020-10-28T08:47:10.930566Z","shell.execute_reply":"2020-10-28T08:47:10.929789Z"},"papermill":{"duration":1.173452,"end_time":"2020-10-28T08:47:10.930694","exception":false,"start_time":"2020-10-28T08:47:09.757242","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Training or loading trained model for predictions.\n\n# https://www.kaggle.com/cdeotte/tensorflow-roberta-0-705\nfold_start = np.zeros((train_ids.shape[0],max_len))\nfold_end = np.zeros((train_ids.shape[0],max_len))\n\n# will train 5 models and combine(avg) their output to get final predictions.\nn_splits=5\nkfold = StratifiedKFold(n_splits=n_splits,shuffle=True,random_state=42) \n\nfor fold,(train_id,val_id) in enumerate(kfold.split(train_ids,x_train.sentiment.values)):\n\n    model = build_model()\n    \n    # callbacks\n    checkpoint = tf.keras.callbacks.ModelCheckpoint('roberta-%i.h5'%(fold), monitor = 'val_loss', verbose = 1, save_best_only = True,\n                                                          save_weights_only = True, mode = 'auto', save_freq = 'epoch')\n    reduce_lr_cb = tf.keras.callbacks.LearningRateScheduler(lr_scheduler)\n\n    if train_mode:\n\n        model.fit([train_ids[train_id,], train_mask[train_id,], train_type_ids[train_id,]], [train_start_idx[train_id,], train_end_idx[train_id,]], \n                          epochs = 4, \n                          batch_size = 16, \n                          verbose = True, \n                          callbacks = [checkpoint,reduce_lr_cb],\n                  validation_data = ([train_ids[val_id,], train_mask[val_id,], train_type_ids[val_id,]], [train_start_idx[val_id,], train_end_idx[val_id,]]),shuffle = True)\n        \n        model.load_weights('roberta-%i.h5'%(fold)) # loads best model.\n        \n    else:\n\n        model.load_weights(f'../input/roberta-folds/roberta-{fold}.h5')\n    # predictions on val data.\n    fold_start[val_id],fold_end[val_id] = model.predict([train_ids[val_id,],train_mask[val_id,],train_type_ids[val_id,]],verbose = 1)\n    \n    # score on val_data.\n    score = 0\n    for k in val_id:\n\n        if(x_train.loc[k,'sentiment']!='neutral'):\n\n            \n            encoding = text_process(x_train.loc[k,'text'], x_train.loc[k,'sentiment'], tokenizer, max_len)\n            offsets = encoding['offsets']\n\n            targets_start = np.argmax(fold_start[k,])\n            targets_end = np.argmax(fold_end[k,])\n            if targets_start<=targets_end:\n                pred = x_train.loc[k,'text'][offsets[targets_start][0]:offsets[targets_end][-1]]\n                score+=jaccard(x_train.loc[k,'selected_text'], pred)\n            else:\n                score+=jaccard(x_train.loc[k,'selected_text'], x_train.loc[k,'text'])\n\n        else:\n            score+=jaccard(x_train.loc[k,'selected_text'], x_train.loc[k,'text'])\n\n    score=score/len(val_id) \n    print(\"fold\",fold, \"jaccard_score:\",score)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.054539,"end_time":"2020-10-28T08:47:25.848242","exception":false,"start_time":"2020-10-28T08:47:25.793703","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## Test Predictions"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-28T08:47:25.969432Z","iopub.status.busy":"2020-10-28T08:47:25.968728Z","iopub.status.idle":"2020-10-28T08:47:25.984851Z","shell.execute_reply":"2020-10-28T08:47:25.984261Z"},"papermill":{"duration":0.080145,"end_time":"2020-10-28T08:47:25.984961","exception":false,"start_time":"2020-10-28T08:47:25.904816","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# reading test set.\nx_test = pd.read_csv('../input/tweet-sentiment-extraction/test.csv')\nrows = x_test.shape[0]\npreds_start = np.zeros((rows,max_len))\npreds_end = np.zeros((rows,max_len))","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-28T08:47:26.107543Z","iopub.status.busy":"2020-10-28T08:47:26.106782Z","iopub.status.idle":"2020-10-28T08:47:27.748116Z","shell.execute_reply":"2020-10-28T08:47:27.747228Z"},"papermill":{"duration":1.706899,"end_time":"2020-10-28T08:47:27.748271","exception":false,"start_time":"2020-10-28T08:47:26.041372","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# # creating ids,type_ids,mask , start_index and end_index for test set.\n\nrows = x_test.shape[0]\n\ntest_ids = np.zeros((rows,max_len), dtype = 'int32')\ntest_mask = np.zeros((rows,max_len), dtype = 'int32')\ntest_type_ids = np.zeros((rows,max_len), dtype = 'int32')\ntest_start_idx = np.zeros((rows,max_len), dtype = 'int32')\ntest_end_idx = np.zeros((rows,max_len), dtype = 'int32')\n\nfor i in range(x_test.shape[0]):\n    \n    encoding = text_process(x_test.loc[i,'text'], x_test.loc[i,'sentiment'], tokenizer, max_len )\n    test_ids[i] = encoding['ids']\n    test_type_ids[i] = encoding['token_type_ids']\n    test_mask[i] = encoding['mask']\n","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-28T08:47:27.865945Z","iopub.status.busy":"2020-10-28T08:47:27.865008Z","iopub.status.idle":"2020-10-28T08:47:52.528079Z","shell.execute_reply":"2020-10-28T08:47:52.526952Z"},"papermill":{"duration":24.723205,"end_time":"2020-10-28T08:47:52.528244","exception":false,"start_time":"2020-10-28T08:47:27.805039","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"for i in range(n_splits):\n    model.load_weights(f'../input/roberta-folds/roberta-{i}.h5')\n    preds = model.predict([test_ids, test_mask, test_type_ids], verbose = True)\n    preds_start += preds[0]/n_splits\n    preds_end += preds[1] /n_splits","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-28T08:47:52.731542Z","iopub.status.busy":"2020-10-28T08:47:52.730554Z","iopub.status.idle":"2020-10-28T08:47:52.739142Z","shell.execute_reply":"2020-10-28T08:47:52.73859Z"},"papermill":{"duration":0.111579,"end_time":"2020-10-28T08:47:52.739269","exception":false,"start_time":"2020-10-28T08:47:52.62769","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('../input/tweet-sentiment-extraction/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-28T08:47:53.183672Z","iopub.status.busy":"2020-10-28T08:47:52.980856Z","iopub.status.idle":"2020-10-28T08:47:54.183833Z","shell.execute_reply":"2020-10-28T08:47:54.18291Z"},"papermill":{"duration":1.344197,"end_time":"2020-10-28T08:47:54.183956","exception":false,"start_time":"2020-10-28T08:47:52.839759","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"\nfor k in range(x_test.shape[0]):\n    \n    if x_test.loc[k,'sentiment']!='neutral':\n        \n        encoding = text_process(x_test.loc[k,'text'], x_test.loc[k,'sentiment'], tokenizer, max_len)\n        offsets = encoding['offsets']\n        targets_start = np.argmax(preds_start[k,])\n        targets_end = np.argmax(preds_end[k,])\n        \n        if targets_start<=targets_end:\n            pred = x_test.loc[k,'text'][offsets[targets_start][0]:offsets[targets_end][-1]]\n            submission.loc[k,'selected_text'] = pred\n        else:\n            submission.loc[k,'selected_text'] = x_test.loc[k,'text']\n\n    else:\n        submission.loc[k,'selected_text'] = x_test.loc[k,'text']\n","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-28T08:47:54.384994Z","iopub.status.busy":"2020-10-28T08:47:54.384338Z","iopub.status.idle":"2020-10-28T08:47:54.629475Z","shell.execute_reply":"2020-10-28T08:47:54.628863Z"},"papermill":{"duration":0.347853,"end_time":"2020-10-28T08:47:54.62961","exception":false,"start_time":"2020-10-28T08:47:54.281757","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}