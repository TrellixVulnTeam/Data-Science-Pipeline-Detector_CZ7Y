{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# read datasets\nimport pandas as pd\ntrain_data = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv')\ntest_data = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv')\n\nprint('Train Dataset')\nprint(train_data.head())\nprint('Test Dataset')\nprint(test_data.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Text preprocessing\n\n# remove missing values\ntrain_data.dropna(axis = 0,inplace=True)\n\n# Removes punctuation and convert text to lower case.\nimport string\ndef remove_punctuation(text):\n    no_punct = \"\".join([c for c in text if c not in string.punctuation])\n    return no_punct\n\ntrain_data['s_text_clean'] = train_data['selected_text'].apply(str).apply(lambda x: remove_punctuation(x.lower()))\n\n# Breaks up entire string into a list of words based on a pattern specified by the Regular Expression\nfrom nltk.tokenize import RegexpTokenizer\ntokenizer = RegexpTokenizer(r'\\w+')  \ntrain_data['s_text_tokens'] = train_data['s_text_clean'].apply(str).apply(lambda x: tokenizer.tokenize(x))\n\n# Remove stopwords\nfrom nltk.corpus import stopwords\ndef remove_stopwords(text):\n    words = [w for w in text if (w not in stopwords.words('english') or w not in 'im')]\n    return words\n\ntrain_data['s_text_tokens_NOTstop'] = train_data['s_text_tokens'].apply(lambda x: remove_stopwords(x))\n\n# Lemmatization\nfrom nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\ndef word_lemmatizer(text):\n    lem_text = [lemmatizer.lemmatize(i) for i in text]\n    return lem_text\n\ntrain_data['s_text_lemma'] = train_data['s_text_tokens_NOTstop'].apply(lambda x: word_lemmatizer(x))\n\n# Stemming\nfrom nltk.stem.porter import PorterStemmer\nstemmer = PorterStemmer()\n\ndef word_stemmer(text):\n    stem_text = \" \".join([stemmer.stem(i) for i in text])\n    return stem_text\n\ntrain_data['s_text_stem'] = train_data['s_text_lemma'].apply(lambda x: word_stemmer(x))\n\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#BPE (Bype Pair Encoding) tokenizer is used for tokenizing text\nimport tokenizers\nimport numpy as np\nmax_len = 150\n\ntokenizer = tokenizers.ByteLevelBPETokenizer(\n    vocab_file = '/kaggle/input/roberta/vocab-roberta-base.json',\n    merges_file = '/kaggle/input/roberta/merges-roberta-base.txt',\n    lowercase =True,\n    add_prefix_space=True\n)\n\nsentiment_id = {'positive':tokenizer.encode('positive').ids[0],\n                'negative':tokenizer.encode('negative').ids[0],\n                'neutral':tokenizer.encode('neutral').ids[0]}\n\ntrain_data.reset_index(inplace=True)\n\n# input data formating for training\ntrain_shape = train_data.shape[0]\n\ninput_ids = np.ones((train_shape, max_len), dtype='int32')\nattention_mask = np.zeros((train_shape, max_len), dtype='int32')\ntoken_type_ids = np.zeros((train_shape, max_len), dtype='int32')\nstart_mask = np.zeros((train_shape, max_len), dtype='int32')\nend_mask = np.zeros((train_shape, max_len), dtype='int32')\n\nfor i in range(train_shape):\n    set1 = \" \"+\" \".join(train_data.loc[i,'text'].split())\n    set2 = \" \".join(train_data.loc[i,'selected_text'].split())\n    idx = set1.find(set2)\n    set2_loc = np.zeros((len(set1)))\n    set2_loc[idx:idx+len(set2)]=1\n    if set1[idx-1]==\" \":\n        set2_loc[idx-1]=1\n  \n    enc_set1 = tokenizer.encode(set1)\n\n    selected_text_token_idx=[]\n    for k,(a,b) in enumerate(enc_set1.offsets):\n        sm = np.sum(set2_loc[a:b]) \n        if sm > 0:\n            selected_text_token_idx.append(k)\n\n    senti_token = sentiment_id[train_data.loc[i,'sentiment']]\n    input_ids[i,:len(enc_set1.ids)+5] = [0]+enc_set1.ids+[2,2]+[senti_token]+[2] \n    attention_mask[i,:len(enc_set1.ids)+5]=1\n\n    if len(selected_text_token_idx) > 0:\n        start_mask[i,selected_text_token_idx[0]+1]=1\n        end_mask[i, selected_text_token_idx[-1]+1]=1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Categorical Cross Entropy with Label Smoothing\n# Label Smoothing is done to enhance accuracy\n\ndef custom_loss(y_true, y_pred):\n    loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred, from_logits = False, label_smoothing = 0.2)\n    loss = tf.reduce_mean(loss)\n    return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom sklearn.model_selection import StratifiedKFold\nfrom transformers import *\nimport tokenizers\nfrom keras.layers import Dense, Flatten, Conv1D, Dropout, Input\nfrom keras.models import Model, load_model\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\n\nos.environ['WANDB_MODE'] = 'dryrun'\n\ndef build_model():\n    ids = tf.keras.layers.Input((max_len,), dtype=tf.int32)\n    att = tf.keras.layers.Input((max_len,), dtype=tf.int32)\n    tok = tf.keras.layers.Input((max_len,), dtype=tf.int32) \n    \n    config_path = RobertaConfig.from_pretrained('/kaggle/input/roberta/config-roberta-base.json')\n    roberta_model = TFRobertaModel.from_pretrained('/kaggle/input/roberta/pretrained-roberta-base.h5', config=config_path)\n    x = roberta_model(ids, attention_mask=att, token_type_ids=tok)\n    \n    x1 = tf.keras.layers.Dropout(0.1)(x[0])\n    x1 = tf.keras.layers.Conv1D(128, 2,padding='same')(x1)\n    x1 = tf.keras.layers.LeakyReLU()(x1)\n    x1 = tf.keras.layers.Conv1D(64, 2,padding='same')(x1)\n    x1 = tf.keras.layers.Dense(1)(x1)\n    x1 = tf.keras.layers.Flatten()(x1)\n    x1 = tf.keras.layers.Activation('softmax')(x1)\n    \n    x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n    x2 = tf.keras.layers.Conv1D(128, 2, padding='same')(x2)\n    x2 = tf.keras.layers.LeakyReLU()(x2)\n    x2 = tf.keras.layers.Conv1D(64, 2, padding='same')(x2)\n    x2 = tf.keras.layers.Dense(1)(x2)\n    x2 = tf.keras.layers.Flatten()(x2)\n    x2 = tf.keras.layers.Activation('softmax')(x2)\n    \n    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5) \n    model.compile(loss=custom_loss, optimizer=optimizer)\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#input data formating for testing\ntest_shape = test_data.shape[0]\n\ninput_ids_t = np.ones((test_shape,max_len), dtype='int32')\nattention_mask_t = np.zeros((test_shape,max_len), dtype='int32')\ntoken_type_ids_t = np.zeros((test_shape,max_len), dtype='int32')\n\nfor i in range(test_shape):\n    set1 = \" \"+\" \".join(test_data.loc[i,'text'].split())\n    enc_set1 = tokenizer.encode(set1)\n\n    senti_token_t = sentiment_id[test_data.loc[i,'sentiment']]\n    input_ids_t[i,:len(enc_set1.ids)+5]=[0]+enc_set1.ids+[2,2]+[senti_token_t]+[2]\n    attention_mask_t[i,:len(enc_set1.ids)+5]=1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom transformers import TFRobertaModel\n\npreds_start= np.zeros((input_ids_t.shape[0],max_len))\npreds_end= np.zeros((input_ids_t.shape[0],max_len))\n\nmodel = build_model()\nmodel.load_weights('/kaggle/input/roberta/v4-roberta-4.h5')\npred = model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=1)\npred_start = pred[0]\npred_end = pred[1]\n  \nall = []\nfor k in range(input_ids_t.shape[0]):\n    a = np.argmax(pred_start[k,])\n    b = np.argmax(pred_end[k,])\n    if a>b: \n        st = test_data.loc[k,'text'] \n    else:\n        text1 = \" \"+\" \".join(test_data.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode(enc.ids[a-1:b])\n    all.append(st)\ntest_data['selected_text']=all\ntest_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data[['textID','selected_text']].to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}