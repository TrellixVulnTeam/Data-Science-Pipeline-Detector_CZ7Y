{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import os \n\nis_inference_flag = True\ntry:\n    tweet_models_dir = os.listdir('/kaggle/input/albert-xl-300/')\n    if len(tweet_models_dir) > 0: \n        is_inference_flag = True\nexcept:\n    is_inference_flag = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Inference flag status :', is_inference_flag)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if not is_inference_flag:\n    !git clone https://github.com/AnandAwasthi/Closed-domain-Question-Answering-fine-tune-Albert\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport pandas as pd\nimport numpy as np\nimport json\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv')\ntest_df = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv')\nsub_df = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/sample_submission.csv')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def jaccard(str1, str2):\n    a = str1.lower().split()\n    b = str2.lower().split()\n    c = set(a).intersection(set(b))\n    return float(len(c)) / (len(a) + len(b) - len(c))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test = train_test_split(train_df,test_size=0.10, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = np.array(X_train)\nval = np.array(X_test)\ntest = np.array(test_df)\nuse_cuda = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!mkdir -p data\n!mkdir -p data/models/albert","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n\"\"\"\nPrepare training data in QA-compatible format\n\"\"\"\n\n# Adpated from https://www.kaggle.com/cheongwoongkang/roberta-baseline-starter-simple-postprocessing\ndef find_all(input_str, search_str):\n    l1 = []\n    length = len(input_str)\n    index = 0\n    while index < length:\n        i = input_str.find(search_str, index)\n        if i == -1:\n            return l1\n        l1.append(i)\n        index = i + 1\n    return l1\n\ndef do_qa_train(train):\n    output = {}\n    output['version'] = 'v1.0'\n    output['data'] = []\n    paragraphs = []\n    for line in train:\n        context = line[1]\n\n        qas = []\n        question = line[-1]\n        qid = line[0]\n        answers = []\n        answer = line[2]\n        if type(answer) != str or type(context) != str or type(question) != str:\n            print(context, type(context))\n            print(answer, type(answer))\n            print(question, type(question))\n            continue\n        answer_starts = find_all(context, answer)\n        for answer_start in answer_starts:\n            answers.append({'answer_start': answer_start, 'text': answer.lower()})\n            break\n        qas.append({'question': question, 'id': qid, 'is_impossible': False, 'answers': answers})\n        paragraphs.append({'context': context.lower(), 'qas': qas})\n        #output['data'].append({'title': 'None', 'paragraphs': paragraphs})\n\n    output['data'].append({'title': 'None', 'paragraphs': paragraphs})\n    return output\n\nif not is_inference_flag:\n    qa_X_train = do_qa_train(train)\n    qa_X_test = do_qa_train(val)\n\n    with open('data/train.json', 'w') as outfile:\n        json.dump(qa_X_train, outfile)\n\n    with open('data/val.json', 'w') as outfile:\n        json.dump(qa_X_test, outfile)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"%%time\n\n\"\"\"\nPrepare testing data in QA-compatible format\n\"\"\"\n\n\ndef convert_test_qa_json(test):\n    output = {}\n    output['version'] = 'v1.0'\n    output['data'] = []\n    paragraphs = []\n    for line in test:\n        \n        context = line[1]\n        qas = []\n        question = line[-1]\n        qid = line[0]\n        if type(context) != str or type(question) != str:\n            print(context, type(context))\n            print(answer, type(answer))\n            print(question, type(question))\n            continue\n        answers = []\n        answers.append({'answer_start': 1000000, 'text': '__None__'})\n        qas.append({'question': question, 'id': qid, 'is_impossible': False, 'answers': answers})\n\n        paragraphs.append({'context': context.lower(), 'qas': qas})\n    \n    output['data'].append({'title': 'None', 'paragraphs': paragraphs})\n    return output\n\nqa_test = convert_test_qa_json(test)\n\nwith open('data/test.json', 'w') as outfile:\n    json.dump(qa_test, outfile)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a name=\"Training\" id=\"3\"></a> 3. Model Training ....\n\nUsing my training kernel out to save GPU time.<br>\nTo train uncomment train command."},{"metadata":{"trusted":true},"cell_type":"code","source":"if not is_inference_flag:\n    !export SQUAD_DIR=data \\\n    && python Closed-domain-Question-Answering-fine-tune-Albert/bsie/transformers/examples/run_squad.py \\\n      --model_type albert \\\n      --model_name_or_path albert-base-v2 \\\n      --do_train \\\n      --do_eval \\\n      --do_lower_case \\\n      --train_file $SQUAD_DIR/train.json \\\n      --predict_file $SQUAD_DIR/val.json \\\n      --per_gpu_train_batch_size 12 \\\n      --learning_rate 5e-5 \\\n      --num_train_epochs 1.0 \\\n      --max_seq_length 192 \\\n      --doc_stride 64 \\\n      --output_dir $SQUAD_DIR/models/albert \\\n      --save_steps 100000 \\\n      --threads 4 \\\n      --version_2_with_negative \\\n      --overwrite_output_dir","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a name=\"Infer\" id=\"4\"></a> 4. Model INFER"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport torch\nimport time\nfrom torch.utils.data import DataLoader, RandomSampler, SequentialSampler\nfrom collections import OrderedDict\nfrom transformers import (\n    AutoConfig,\n    AutoModelForQuestionAnswering,\n    AutoTokenizer,\n    squad_convert_examples_to_features\n)\n\nfrom transformers.data.processors.squad import SquadResult, SquadV2Processor, SquadExample\n\nfrom transformers.data.metrics.squad_metrics import compute_predictions_logits\n\nif not is_inference_flag:\n  model_name_or_path = \"data/models/\"\nelse:\n  model_name_or_path = '/kaggle/input/albert-xl-300/'\n\noutput_dir = \"\"\n\n# Config\nn_best_size = 1\nmax_answer_length = 254\ndo_lower_case = True\nnull_score_diff_threshold = 0.0\n\ndef to_list(tensor):\n    return tensor.detach().cpu().tolist()\n\n# Setup model\nconfig_class, model_class, tokenizer_class = (\n   AutoConfig, AutoModelForQuestionAnswering, AutoTokenizer)\nconfig = config_class.from_pretrained(model_name_or_path)\ntokenizer = tokenizer_class.from_pretrained(\n    model_name_or_path, do_lower_case=True)\nmodel = model_class.from_pretrained(model_name_or_path, config=config)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel.to(device)\n\nprocessor = SquadV2Processor()\n\ndef run_prediction(question_texts, context_text):\n    \"\"\"Setup function to compute predictions\"\"\"\n    if question_texts[0] != 'neutral':\n        examples = []\n\n        for i, question_text in enumerate(question_texts):\n            example = SquadExample(\n                qas_id=str(i),\n                question_text=question_text,\n                context_text=context_text,\n                answer_text=None,\n                start_position_character=None,\n                title=\"Predict\",\n                is_impossible=False,\n                answers=None,\n            )\n\n            examples.append(example)\n\n        features, dataset = squad_convert_examples_to_features(\n            examples=examples,\n            tokenizer=tokenizer,\n            max_seq_length=300,\n            doc_stride=128,\n            max_query_length=64,\n            is_training=False,\n            return_dataset=\"pt\",\n            threads=1,\n        )\n\n        eval_sampler = SequentialSampler(dataset)\n        eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=10)\n\n        all_results = []\n\n        for batch in eval_dataloader:\n            model.eval()\n            batch = tuple(t.to(device) for t in batch)\n\n            with torch.no_grad():\n                inputs = {\n                    \"input_ids\": batch[0],\n                    \"attention_mask\": batch[1],\n                    \"token_type_ids\": batch[2],\n                }\n\n                example_indices = batch[3]\n\n                outputs = model(**inputs)\n\n                for i, example_index in enumerate(example_indices):\n                    eval_feature = features[example_index.item()]\n                    unique_id = int(eval_feature.unique_id)\n\n                    output = [to_list(output[i]) for output in outputs]\n\n                    start_logits, end_logits = output\n                    result = SquadResult(unique_id, start_logits, end_logits)\n                    all_results.append(result)\n\n        output_prediction_file = \"predictions.json\"\n        output_nbest_file = \"nbest_predictions.json\"\n        output_null_log_odds_file = \"null_predictions.json\"\n\n        predictions = compute_predictions_logits(\n            examples,\n            features,\n            all_results,\n            n_best_size,\n            max_answer_length,\n            do_lower_case,\n            output_prediction_file,\n            output_nbest_file,\n            output_null_log_odds_file,\n            False,  # verbose_logging\n            True,  # version_2_with_negative\n            null_score_diff_threshold,\n            tokenizer,\n        )\n    else:\n        predictions = OrderedDict([(0, context_text)])\n\n    return predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Infering on trained model for Jaccard score\njaccard_scores = []\npredictions_x_test = []\nfor index, row in X_test.head(10).iterrows():\n    context = row['text']\n    selected_text = row['selected_text']\n    questions = [row['sentiment']]\n    preds_dict = run_prediction(questions, context)\n    for key in preds_dict.keys():\n        predicted_text = preds_dict[key] \n        predictions_x_test.append({'selected_text': selected_text,'predicted_text': predicted_text, 'sentiment': row['sentiment'], 'textID': row['textID']})\n        jaccard_score = jaccard(selected_text, predicted_text)\n        jaccard_scores.append(jaccard_score)\nprint('Jaccard Score', np.mean(jaccard_scores))\n\npredictions_x_test_df = pd.DataFrame.from_dict(predictions_x_test)\n\n\npredictions_x_test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Infering on trained model\npredictions = []\n\nfor index, row in test_df.iterrows():\n    context = row['text']\n    questions = [row['sentiment']]\n    preds_dict = run_prediction(questions, context)\n    for key in preds_dict.keys():\n        predicted_text = preds_dict[key] \n        predictions.append({'textID': row['textID'], 'selected_text': predicted_text})\n        \npredictions_df = pd.DataFrame.from_dict(predictions)\noutput_df = sub_df.merge(predictions_df, on ='textID')\npredictions_df.to_csv('submission.csv', index=False)\n\npredictions_df.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"35f23b0b120242468911c33a8b5510a1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"100%","description_tooltip":null,"layout":"IPY_MODEL_dfb243941dac47adbc9cec382812b49d","max":442,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7912cdfc974a4591bd2360b1dde639ab","value":442}},"6bfa26d293114f33a29ab6865456601d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_90f39f1e5cff496398d9ded537c1fc88","placeholder":"​","style":"IPY_MODEL_a5830897ce834e64b1ec6e0c7de5feaa","value":" 442/442 [00:44&lt;00:00,  9.97it/s]"}},"7912cdfc974a4591bd2360b1dde639ab":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"90f39f1e5cff496398d9ded537c1fc88":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"93c255b5ef664d7c91d4bf106abccbb2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a5830897ce834e64b1ec6e0c7de5feaa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"adb80f564ae444ec8d93aaec673f3159":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_35f23b0b120242468911c33a8b5510a1","IPY_MODEL_6bfa26d293114f33a29ab6865456601d"],"layout":"IPY_MODEL_93c255b5ef664d7c91d4bf106abccbb2"}},"dfb243941dac47adbc9cec382812b49d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}},"version_major":2,"version_minor":0}}},"nbformat":4,"nbformat_minor":4}