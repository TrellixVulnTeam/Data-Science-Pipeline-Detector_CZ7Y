{"cells":[{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://m.economictimes.com/thumb/msid-71590483,width-1200,height-900,resizemode-4,imgsize-178633/tweet.jpg\" width=\"500\" height=\"500\" align=\"center\"/>"},{"metadata":{},"cell_type":"markdown","source":"# About Competition: <br>\n\"My ridiculous dog is amazing.\" [sentiment: positive]\n\nWith all of the tweets circulating every second it is hard to tell whether the sentiment behind a specific tweet will impact a company, or a person's, brand for being viral (positive), or devastate profit because it strikes a negative tone. Capturing sentiment in language is important in these times where decisions and reactions are created and updated in seconds. But, which words actually lead to the sentiment description? In this competition you will need to pick out the part of the tweet (word or phrase) that reflects the sentiment."},{"metadata":{},"cell_type":"markdown","source":"### <font color='red'>If you find this kernel useful please consider upvoting it üòä which keeps me motivated for doing hard work and to produce more quality content.</font>"},{"metadata":{},"cell_type":"markdown","source":"# Competition metric: <br>\nThe metric in this competition is the word-level Jaccard score. <br>\nJaccard similarity or intersection over union is defined as size of intersection divided by size of union of two sets.The Jaccard Index, also known as the Jaccard similarity coefficient, is a statistic used in understanding the similarities between sample sets. The measurement emphasizes similarity between finite sample sets, and is formally defined as the size of the intersection divided by the size of the union of the sample sets. The mathematical representation of the index is written as: <br>\n![](https://images.deepai.org/glossary-terms/jaccard-index-9707615.jpg) <br>\nSimilar to the Jaccard Index, which is a measurement of similarity, the Jaccard distance measures dissimilarity between sample sets. The Jaccard distance is calculated by finding the Jaccard index and subtracting it from 1, or alternatively dividing the differences ny the intersection of the two sets. The formula for the Jaccard distance is represented as: <br>\n![](https://images.deepai.org/glossary-terms/jaccard-index-391304.jpg) <br>\n#### How does the Jaccard Index work? <br>\nBreaking down the formula, the Jaccard Index is essentially the number in both sets, divided by the number in either set, multiplied by 100. This will produce a percentage measurement of similarity between the two sample sets. Accordingly, to find the Jaccard distance, simply subtract the percentage value from 1. For example, if the similarity measurement is 35%, then the Jaccard distance (1 - .35) is .65 or 65%. <br>\n\n#### Let‚Äôs take example of two sentences: <br>\n**Sentence 1**: AI is our friend and it has been friendly <br>\n**Sentence 2**: AI and humans have always been friendly <br>\nIn order to calculate similarity using Jaccard similarity, we will first perform lemmatization to reduce words to the same root word. In our case, ‚Äúfriend‚Äù and ‚Äúfriendly‚Äù will both become ‚Äúfriend‚Äù, ‚Äúhas‚Äù and ‚Äúhave‚Äù will both become ‚Äúhas‚Äù. Drawing a Venn diagram of the two sentences we get: <br>\n![](https://miro.medium.com/max/579/1*u2ZZPh5er5YbmOg7k-s0-A.png)  <br>\nFor more : [read here](https://towardsdatascience.com/overview-of-text-similarity-metrics-3397c4601f50) and [here](https://deepai.org/machine-learning-glossary-and-terms/jaccard-index)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import math","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# math.floor(2.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Calculating Jaccard similarity using NLTK Library:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import nltk \n# w1 = set('AI is our friend and it has been friendly'.lower().split())\n# w2 = set('AI and humans have always been friendly'.lower().split())\n \n# print (\"Jaccard similarity of above two sentences is\",1-nltk.jaccard_distance(w1, w2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we have 0.33 which is size of intersection of the set divided by total size of set."},{"metadata":{"trusted":true},"cell_type":"code","source":"# w1 = set('Kaggle is awesome'.lower().split())\n# w2 = set('kaggle is great way of learning DS'.lower().split())\n# print(\"The Jaccard similarity is:\",1-nltk.jaccard_distance(w1, w2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Importing libraries:"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pyspark","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"!pip install \"/kaggle/input/chart-studio/chart_studio-1.0.0-py3-none-any.whl\"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom wordcloud import WordCloud, STOPWORDS\nimport spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom spacy.lang.en import English\nimport re\nimport string\n\nimport matplotlib.pyplot as plt\nfrom plotly import tools\nimport chart_studio.plotly as py\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\n\nimport os\nimport tokenizers\nimport torch\nimport transformers\nimport torch.nn as nn\nfrom tqdm import tqdm\n\n# import pyspark\n# from pyspark.sql import SparkSession\n# import pandas as pd\n\n# spark = SparkSession.builder.appName('pandasToSparkDF').getOrCreate()\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n# word level jaccard score: https://www.kaggle.com/c/tweet-sentiment-extraction/overview/evaluation\ndef jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import findspark\n# findspark.init()\nimport pyspark as ps\nimport warnings\nfrom pyspark.sql import SQLContext\n\nimport string\n\ntry:\n    # create SparkContext on all CPUs available: in my case I have 4 CPUs on my laptop\n    sc = ps.SparkContext()\n    sqlContext = SQLContext(sc)\n    print(\"Just created a SparkContext\")\nexcept ValueError:\n    warnings.warn(\"SparkContext already exists in this scope\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.sql import SparkSession\nMAX_MEMORY = \"85g\"\nspark = SparkSession(sc).builder \\\n    .master('local[*]') \\\n    .config(\"spark.driver.memory\", \"85g\") \\\n    .config(\"spark.executor.memory\", \"85g\") \\\n    .appName('my-cool-app') \\\n    .getOrCreate()\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Importing Data:"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# train = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv')\n# test = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv')\n# sub = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('/kaggle/input/tweet-sentiment-extraction/test.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sp_train = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('/kaggle/input/tweet-sentiment-extraction/train.csv')\nsp_test = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('/kaggle/input/tweet-sentiment-extraction/test.csv')\nsp_sub = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('/kaggle/input/tweet-sentiment-extraction/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sp_train = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/kaggle/input/tweet-sentiment-extraction/train.csv\") \n# sp_test = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/kaggle/input/tweet-sentiment-extraction/test.csv\") \n# sp_sub = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/kaggle/input/tweet-sentiment-extraction/sample_submission.csv\") \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_punctuations(text):\n    for punctuation in string.punctuation:\n        text = text.replace(punctuation, '')\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.sql.functions import regexp_replace, col\nimport re","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\npunc=re.escape('!\"#$%&\\*+,-./:;<=>?@[\\\\]^_`{|}~')\n# print(punc)\nsp_train = sp_train.withColumn('text', regexp_replace('text', '[' + punc +']', ''))\nsp_test = sp_test.withColumn('text', regexp_replace('text', '[' + punc +']', ''))\n\n# sp_train.take(5)\n# sp_test.take(5)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sp_train = sp_train.dropna()\nsp_test = sp_test.dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sp_train.take(5)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sp_test = sp_test.withColumn('selected_text', sp_test['text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sp_test.take(5)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Sentiment of text : {} \\nOur training text :\\n{}\\nSelected text which we need to predict:\\n{}'.format(sp_train.take(1)[0][3],sp_train.take(1)[0][2],sp_train.take(1)[0][1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import itertools\nsp_train = sp_train.rdd.map(lambda x: [item for item in (x.text, x.selected_text, x.sentiment)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sp_test = sp_test.rdd.map(lambda x: [item for item in (x.text, x.selected_text, x.sentiment)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(sp_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sp_train.take(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5.Model:"},{"metadata":{},"cell_type":"markdown","source":"In this notebook i am using Roberta model. <br>\nThe whole training stratergy was similar to which shown in this [video](https://www.youtube.com/watch?v=XaQ0CBlQ4cY). The few things that i changed is that i trained the model using KFold cross validation strategy. 20 Epochs for each fold total of 100 Epochs. I am only using the models which has better validation Jaccard score.<br>\n#### Credits : <br>\n* Author : [Abhishek Thakur](https://www.kaggle.com/abhishek) <br>\n* [Reference notebook](https://www.kaggle.com/abhishek/tweet-text-extraction-roberta-infer) <br>\n<br>\nI Strong recommend you to have a look at that notebook and upvote if you like it.\n\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"class config:\n    MAX_LEN = 128\n    BERT_PATH = \"../input/roberta-base/\"\n    MODEL_PATH = \"model.bin\"\n    TRAINING_FILE = \"../input/train.csv\"\n    TOKENIZER = tokenizers.ByteLevelBPETokenizer(\n        vocab_file=f\"{BERT_PATH}/vocab.json\", \n        merges_file=f\"{BERT_PATH}/merges.txt\", \n        lowercase=True,\n        add_prefix_space=True)\n    TRAIN_BATCH_SIZE = 64\n    VALID_BATCH_SIZE = 16\n    EPOCHS = 5\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN = 128\nVALID_BATCH_SIZE = 8\nBERT_PATH = \"../input/roberta-base/\"\nMODEL_PATH = \"model.bin\"\nTRAINING_FILE = \"../input/train.csv\"\nTOKENIZER = tokenizers.ByteLevelBPETokenizer(\n    vocab_file=f\"{BERT_PATH}/vocab.json\", \n    merges_file=f\"{BERT_PATH}/merges.txt\", \n    lowercase=True,\n    add_prefix_space=True\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TweetModel(nn.Module):\n    def __init__(self):\n        super(TweetModel, self).__init__()\n        self.bert = transformers.RobertaModel.from_pretrained(BERT_PATH)\n        self.l0 = nn.Linear(768, 2)\n    \n    def forward(self, ids, mask, token_type_ids):\n        sequence_output, pooled_output = self.bert(\n            ids, \n            attention_mask=mask\n        )\n        logits = self.l0(sequence_output)\n        start_logits, end_logits = logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n\n        return start_logits, end_logits","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def loss_fn(start_logits, end_logits, start_positions, end_positions):\n    loss_fct = nn.CrossEntropyLoss()\n    start_loss = loss_fct(start_logits, start_positions)\n    end_loss = loss_fct(end_logits, end_positions)\n    total_loss = (start_loss + end_loss)\n    return total_loss\ndef train_fn(data_loader, model, optimizer, device, scheduler=None):\n    model.train()\n    losses = utils.AverageMeter()\n    jaccards = utils.AverageMeter()\n\n    tk0 = tqdm(data_loader, total=len(data_loader))\n    \n    for bi, d in enumerate(tk0):\n\n        ids = d[\"ids\"]\n        token_type_ids = d[\"token_type_ids\"]\n        mask = d[\"mask\"]\n        targets_start = d[\"targets_start\"]\n        targets_end = d[\"targets_end\"]\n        sentiment = d[\"sentiment\"]\n        orig_selected = d[\"orig_selected\"]\n        orig_tweet = d[\"orig_tweet\"]\n        targets_start = d[\"targets_start\"]\n        targets_end = d[\"targets_end\"]\n        offsets = d[\"offsets\"]\n\n        ids = ids.to(device, dtype=torch.long)\n        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n        mask = mask.to(device, dtype=torch.long)\n        targets_start = targets_start.to(device, dtype=torch.long)\n        targets_end = targets_end.to(device, dtype=torch.long)\n\n        model.zero_grad()\n        outputs_start, outputs_end = model(\n            ids=ids,\n            mask=mask,\n            token_type_ids=token_type_ids,\n        )\n        loss = loss_fn(outputs_start, outputs_end, targets_start, targets_end)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n\n        outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n        outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n        jaccard_scores = []\n        for px, tweet in enumerate(orig_tweet):\n            selected_tweet = orig_selected[px]\n            tweet_sentiment = sentiment[px]\n            jaccard_score, _ = calculate_jaccard_score(\n                original_tweet=tweet,\n                target_string=selected_tweet,\n                sentiment_val=tweet_sentiment,\n                idx_start=np.argmax(outputs_start[px, :]),\n                idx_end=np.argmax(outputs_end[px, :]),\n                offsets=offsets[px]\n            )\n            jaccard_scores.append(jaccard_score)\n\n        jaccards.update(np.mean(jaccard_scores), ids.size(0))\n        losses.update(loss.item(), ids.size(0))\n        tk0.set_postfix(loss=losses.avg, jaccard=jaccards.avg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def calculate_jaccard_score(\n    original_tweet, \n    target_string, \n    sentiment_val, \n    idx_start, \n    idx_end, \n    offsets,\n    verbose=False):\n    \n    if idx_end < idx_start:\n        idx_end = idx_start\n    \n    filtered_output  = \"\"\n    for ix in range(idx_start, idx_end + 1):\n        filtered_output += original_tweet[offsets[ix][0]: offsets[ix][1]]\n        if (ix+1) < len(offsets) and offsets[ix][1] < offsets[ix+1][0]:\n            filtered_output += \" \"\n\n    if sentiment_val == \"neutral\" or len(original_tweet.split()) < 2:\n        filtered_output = original_tweet\n\n    jac = utils.jaccard(target_string.strip(), filtered_output.strip())\n    return jac, filtered_output\n\n\ndef eval_fn(data_loader, model, device):\n    model.eval()\n    losses = utils.AverageMeter()\n    jaccards = utils.AverageMeter()\n    \n    with torch.no_grad():\n        tk0 = tqdm(data_loader, total=len(data_loader))\n        for bi, d in enumerate(tk0):\n            ids = d[\"ids\"]\n            token_type_ids = d[\"token_type_ids\"]\n            mask = d[\"mask\"]\n            sentiment = d[\"sentiment\"]\n            orig_selected = d[\"orig_selected\"]\n            orig_tweet = d[\"orig_tweet\"]\n            targets_start = d[\"targets_start\"]\n            targets_end = d[\"targets_end\"]\n            offsets = d[\"offsets\"].numpy()\n\n            ids = ids.to(device, dtype=torch.long)\n            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n            mask = mask.to(device, dtype=torch.long)\n            targets_start = targets_start.to(device, dtype=torch.long)\n            targets_end = targets_end.to(device, dtype=torch.long)\n\n            outputs_start, outputs_end = model(\n                ids=ids,\n                mask=mask,\n                token_type_ids=token_type_ids\n            )\n            loss = loss_fn(outputs_start, outputs_end, targets_start, targets_end)\n            outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n            outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n            jaccard_scores = []\n            for px, tweet in enumerate(orig_tweet):\n                selected_tweet = orig_selected[px]\n                tweet_sentiment = sentiment[px]\n                jaccard_score, _ = calculate_jaccard_score(\n                    original_tweet=tweet,\n                    target_string=selected_tweet,\n                    sentiment_val=tweet_sentiment,\n                    idx_start=np.argmax(outputs_start[px, :]),\n                    idx_end=np.argmax(outputs_end[px, :]),\n                    offsets=offsets[px]\n                )\n                jaccard_scores.append(jaccard_score)\n\n            jaccards.update(np.mean(jaccard_scores), ids.size(0))\n            losses.update(loss.item(), ids.size(0))\n            tk0.set_postfix(loss=losses.avg, jaccard=jaccards.avg)\n    \n    print(f\"Jaccard = {jaccards.avg}\")\n    return jaccards.avg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda\")\nmodel = TweetModel()\nmodel.to(device)\nmodel = nn.DataParallel(model)\nmodel.load_state_dict(torch.load(\"../input/roberta-weights/roberta_model_1.bin\"))\nmodel.eval()\n\nmodel1 = TweetModel()\nmodel1.to(device)\nmodel1 = nn.DataParallel(model1)\nmodel1.load_state_dict(torch.load(\"../input/roberta-weights/roberta_model_2.bin\"))\nmodel1.eval()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TweetDataset:\n    def __init__(self, df):\n        self.sp_df = df\n        self.tokenizer = TOKENIZER\n        self.max_len = MAX_LEN\n    \n    def __len__(self):\n        return len(self.sp_df)\n    \n    def __getitem__(self, item):\n    \n        return {\n            'ids': torch.tensor(self.sp_df[item]['ids'], dtype=torch.long),\n            'mask': torch.tensor(self.sp_df[item]['mask'], dtype=torch.long),\n            'token_type_ids': torch.tensor(self.sp_df[item]['token_type_ids'], dtype=torch.long),\n            'targets_start': torch.tensor(self.sp_df[item]['targets_start'], dtype=torch.float),\n            'targets_end': torch.tensor(self.sp_df[item]['targets_end'], dtype=torch.float),\n            'padding_len': torch.tensor(self.sp_df[item]['padding_len'], dtype=torch.long),\n            'orig_tweet': self.sp_df[item]['orig_tweet'],\n            'orig_selected': self.sp_df[item]['orig_selected'],\n            'sentiment': self.sp_df[item]['sentiment']\n        }\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sp_train = sp_train.rdd.map(lambda x: [item for item in (x.text, x.selected_text, x.sentiment)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sp_test = sp_test.rdd.map(lambda x: [item for item in (x.text, x.selected_text, x.sentiment)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"   \ndef process(item):\n    MAX_LEN = 128\n    VALID_BATCH_SIZE = 8\n    BERT_PATH = \"../input/roberta-base/\"\n    MODEL_PATH = \"model.bin\"\n    TRAINING_FILE = \"../input/train.csv\"\n    TOKENIZER = tokenizers.ByteLevelBPETokenizer(\n        vocab_file=f\"{BERT_PATH}/vocab.json\", \n        merges_file=f\"{BERT_PATH}/merges.txt\", \n        lowercase=True,\n        add_prefix_space=True\n    )\n    tokenizer = TOKENIZER\n    max_len = MAX_LEN\n    tweet = \" \" + \" \".join(str(item[0]).split())\n    selected_text = \" \" + \" \".join(str(item[1]).split())\n\n    len_st = len(selected_text)\n    idx0 = -1\n    idx1 = -1\n    for ind in (i for i, e in enumerate(tweet) if e == selected_text[0]):\n        if tweet[ind: ind+len_st] == selected_text:\n            idx0 = ind\n            idx1 = ind + len_st\n            break\n\n    char_targets = [0] * len(tweet)\n    if idx0 != -1 and idx1 != -1:\n        for ct in range(idx0, idx1):\n            # if tweet[ct] != \" \":\n            char_targets[ct] = 1\n\n#     print(f\"char_targets: {char_targets}\")\n\n    tok_tweet = tokenizer.encode(tweet)\n    tok_tweet_tokens = tok_tweet.tokens\n    tok_tweet_ids = tok_tweet.ids\n    tok_tweet_offsets = tok_tweet.offsets\n\n    targets = [0] * len(tok_tweet_ids)\n    target_idx = []\n    for j, (offset1, offset2) in enumerate(tok_tweet_offsets):\n        if sum(char_targets[offset1: offset2]) > 0:\n            targets[j] = 1\n            target_idx.append(j)\n\n\n    targets_start = [0] * len(targets)\n    targets_end = [0] * len(targets)\n\n    non_zero = np.nonzero(targets)[0]\n    if len(non_zero) > 0:\n        targets_start[non_zero[0]] = 1\n        targets_end[non_zero[-1]] = 1\n\n    # check padding:\n    # <s> pos/neg/neu </s> </s> tweet </s>\n    if len(tok_tweet_tokens) > max_len - 5:\n        tok_tweet_tokens = tok_tweet_tokens[:max_len - 5]\n        tok_tweet_ids = tok_tweet_ids[:max_len - 5]\n        targets_start = targets_start[:max_len - 5]\n        targets_end = targets_end[:max_len - 5]\n\n\n    sentiment_id = {\n        'positive': 1313,\n        'negative': 2430,\n        'neutral': 7974\n    }\n\n    tok_tweet_ids = [0] + [sentiment_id[item[2]]] + [2] + [2] + tok_tweet_ids + [2]\n    targets_start = [0] + [0] + [0] + [0] + targets_start + [0]\n    targets_end = [0] + [0] + [0] + [0] + targets_end + [0]\n    token_type_ids = [0, 0, 0, 0] + [0] * (len(tok_tweet_ids) - 5) + [0]\n    mask = [1] * len(token_type_ids)\n\n    padding_length = max_len - len(tok_tweet_ids)\n\n    tok_tweet_ids = tok_tweet_ids + ([1] * padding_length)\n    mask = mask + ([0] * padding_length)\n    token_type_ids = token_type_ids + ([0] * padding_length)\n    targets_start = targets_start + ([0] * padding_length)\n    targets_end = targets_end + ([0] * padding_length)\n\n    return {\n        'ids': torch.tensor(tok_tweet_ids, dtype=torch.long),\n        'mask': torch.tensor(mask, dtype=torch.long),\n        'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n        'targets_start': torch.tensor(targets_start, dtype=torch.float),\n        'targets_end': torch.tensor(targets_end, dtype=torch.float),\n        'padding_len': torch.tensor(padding_length, dtype=torch.long),\n        'orig_tweet': item[0],\n        'orig_selected': item[1],\n        'sentiment': item[2]\n    }\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sp_train_processed = sp_train.map(process)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sp_train_processed.take(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sp_test_processed = sp_test.map(process)\n\n# sp_test_processed.take(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sp_test_processed.cache()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sp_train_processed.cache()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sp_test_processed.take(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sp_train_processed.take(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sp_test_processed.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sp_train_processed.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_processed_list  = sp_test_processed.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sp_test_processed.unpersist(True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_processed_list  = sp_train_processed.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sp_train_processed.unpersist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spark.stop()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dbfile = open('test_processed_list', 'ab') \n\n# source, destination \npickle.dump(test_processed_list, dbfile)                      \ndbfile.close() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dbfile = open('train_processed_list', 'ab') \n\n# source, destination \npickle.dump(train_processed_list, dbfile)                      \ndbfile.close() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ls","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dataset = TweetDataset(\n#         tweet=sp_test.select('text')#.values,\n#         sentiment=sp_test.select('sentiment')#.values,\n#         selected_text=sp_test.select('selected_text')#.values\n    df = test_processed_list)\n\ndata_loader = torch.utils.data.DataLoader(\n    test_dataset,\n    shuffle=False,\n    batch_size=VALID_BATCH_SIZE,\n    num_workers=1\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"def run(fold):\n#     dfx = pd.read_csv(config.TRAINING_FILE)\n\n    df_train = train_processed_list[:(len(train_processed_list)-math.floor(len(train_processed_list)*0.2))]\n    df_valid = train_processed_list[math.floor(len(train_processed_list)*0.2):]\n    \n    train_dataset = TweetDataset(\n    df = df_train\n    )\n\n\n    train_data_loader = torch.utils.data.DataLoader(\n    train_dataset,\n    shuffle=False,\n    batch_size=config.TRAIN_BATCH_SIZE,\n    num_workers=1\n    )\n\n    valid_dataset = TweetDataset(\n        df = df_valid\n    )\n\n    valid_data_loader = torch.utils.data.DataLoader(\n        valid_dataset,\n    shuffle=False,\n    batch_size=config.VALID_BATCH_SIZE,\n    num_workers=1\n    )\n\n    device = torch.device(\"cuda\")\n#     model_config = transformers.BertConfig.from_pretrained(config.BERT_PATH)\n#     model_config.output_hidden_states = True\n#     model = TweetModel(conf=model_config)\n    model = TweetModel()\n    model.to(device)\n\n    num_train_steps = int(len(df_train) / config.TRAIN_BATCH_SIZE * config.EPOCHS)\n    param_optimizer = list(model.named_parameters())\n    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n    optimizer_parameters = [\n        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n    ]\n    optimizer = AdamW(optimizer_parameters, lr=3e-5)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, \n        num_warmup_steps=0, \n        num_training_steps=num_train_steps\n    )\n\n    es = utils.EarlyStopping(patience=2, mode=\"max\")\n    print(f\"Training is Starting for fold={fold}\")\n    \n    # I'm training only for 3 epochs even though I specified 5!!!\n    for epoch in range(3):\n        train_fn(train_data_loader, model, optimizer, device, scheduler=scheduler)\n        jaccard = eval_fn(valid_data_loader, model, device)\n        print(f\"Jaccard Score = {jaccard}\")\n        es(jaccard, model, model_path=f\"model_{fold}.bin\")\n        if es.early_stop:\n            print(\"Early stopping\")\n            break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"run(fold=245)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda\")\nmodel = TweetModel()\nmodel.to(device)\nmodel = nn.DataParallel(model)\nmodel.load_state_dict(torch.load(\"../input/roberta-weights/roberta_model_245.bin\"))\nmodel.eval()\n\nmodel1 = TweetModel()\nmodel1.to(device)\nmodel1 = nn.DataParallel(model1)\nmodel1.load_state_dict(torch.load(\"../input/roberta-weights/roberta_model_245.bin\"))\nmodel1.eval()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for d in data_loader:\n#     print(d[\"ids\"])\n#     print(d[\"token_type_ids\"])\n#     print(d[\"mask\"])\n#     print(d[\"padding_len\"])\n#     print(d[\"sentiment\"])\n#     print(d[\"orig_selected\"])\n#     print(d[\"orig_tweet\"])\n#     print(d[\"targets_start\"])\n#     print(d[\"targets_end\"])\n\n#     print(i['orignal_iext'])\n#     pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_outputs = []\nfin_outputs_start = []\nfin_outputs_end = []\nfin_outputs_start1 = []\nfin_outputs_end1 = []\nfin_padding_lens = []\nfin_orig_selected = []\nfin_orig_sentiment = []\nfin_orig_tweet = []\nfin_tweet_token_ids = []\n\nwith torch.no_grad():\n    tk0 = tqdm(data_loader, total=len(data_loader))\n    for bi, d in enumerate(tk0):\n        ids = d[\"ids\"]\n        token_type_ids = d[\"token_type_ids\"]\n        mask = d[\"mask\"]\n        padding_len = d[\"padding_len\"]\n        sentiment = d[\"sentiment\"]\n        orig_selected = d[\"orig_selected\"]\n        orig_tweet = d[\"orig_tweet\"]\n        targets_start = d[\"targets_start\"]\n        targets_end = d[\"targets_end\"]\n\n        ids = ids.to(device, dtype=torch.long)\n        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n        mask = mask.to(device, dtype=torch.long)\n        targets_start = targets_start.to(device, dtype=torch.float)\n        targets_end = targets_end.to(device, dtype=torch.float)\n\n        outputs_start, outputs_end = model(\n            ids=ids,\n            mask=mask,\n            token_type_ids=token_type_ids\n        )\n        \n        outputs_start1, outputs_end1 = model1(\n            ids=ids,\n            mask=mask,\n            token_type_ids=token_type_ids\n        )\n        \n        \n\n        fin_outputs_start.append(torch.sigmoid(outputs_start).cpu().detach().numpy())\n        fin_outputs_end.append(torch.sigmoid(outputs_end).cpu().detach().numpy())\n        \n        fin_outputs_start1.append(torch.sigmoid(outputs_start1).cpu().detach().numpy())\n        fin_outputs_end1.append(torch.sigmoid(outputs_end1).cpu().detach().numpy())\n        \n        fin_padding_lens.extend(padding_len.cpu().detach().numpy().tolist())\n        fin_tweet_token_ids.append(ids.cpu().detach().numpy().tolist())\n\n        fin_orig_sentiment.extend(sentiment)\n        fin_orig_selected.extend(orig_selected)\n        fin_orig_tweet.extend(orig_tweet)\n\nfin_outputs_start = np.vstack(fin_outputs_start)\nfin_outputs_end = np.vstack(fin_outputs_end)\n\nfin_outputs_start1 = np.vstack(fin_outputs_start1)\nfin_outputs_end1 = np.vstack(fin_outputs_end1)\n\nfin_outputs_start = (fin_outputs_start + fin_outputs_start1) / 2\nfin_outputs_end = (fin_outputs_end + fin_outputs_end1) / 2\n\n\nfin_tweet_token_ids = np.vstack(fin_tweet_token_ids)\njaccards = []\nthreshold = 0.2\nfor j in range(fin_outputs_start.shape[0]):\n    target_string = fin_orig_selected[j]\n    padding_len = fin_padding_lens[j]\n    sentiment_val = fin_orig_sentiment[j]\n    original_tweet = fin_orig_tweet[j]\n\n    if padding_len > 0:\n        mask_start = fin_outputs_start[j, 4:-1][:-padding_len] >= threshold\n        mask_end = fin_outputs_end[j, 4:-1][:-padding_len] >= threshold\n        tweet_token_ids = fin_tweet_token_ids[j, 4:-1][:-padding_len]\n    else:\n        mask_start = fin_outputs_start[j, 4:-1] >= threshold\n        mask_end = fin_outputs_end[j, 4:-1] >= threshold\n        tweet_token_ids = fin_tweet_token_ids[j, 4:-1][:-padding_len]\n\n    mask = [0] * len(mask_start)\n    idx_start = np.nonzero(mask_start)[0]\n    idx_end = np.nonzero(mask_end)[0]\n    if len(idx_start) > 0:\n        idx_start = idx_start[0]\n        if len(idx_end) > 0:\n            idx_end = idx_end[0]\n        else:\n            idx_end = idx_start\n    else:\n        idx_start = 0\n        idx_end = 0\n\n    for mj in range(idx_start, idx_end + 1):\n        mask[mj] = 1\n\n    output_tokens = [x for p, x in enumerate(tweet_token_ids) if mask[p] == 1]\n\n    filtered_output = TOKENIZER.decode(output_tokens)\n    filtered_output = filtered_output.strip().lower()\n\n    if sentiment_val == \"neutral\" or len(original_tweet.split()) < 4:\n        filtered_output = original_tweet\n\n    all_outputs.append(filtered_output.strip())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample = pd.read_csv(\"../input/tweet-sentiment-extraction/sample_submission.csv\")\nsample.loc[:, 'selected_text'] = all_outputs\nsample.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sample","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Alot more to come.<font color='red'>If you like this kernel please consider Upvoting it</font>.I welcome suggestions to improve this kernel further.<br>"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}