{"cells":[{"metadata":{},"cell_type":"markdown","source":"## T5 Question Answering Inference (PyTorch)\n\n[T5](https://arxiv.org/abs/1910.10683) is a recent approach to do doing sequence to sequence modeling that specifically required input text, and output text, also called a *text-to-text* approach. I've been deeply interested in this model the moment I read about it.\n\nI believe that the combination of *text-to-text* as a universal interface for NLP tasks paired multi-task learning (single model learning multiple tasks) will have huge impact on how deep learning is applied in practice. This competition is my first attemt at utilizing T5 for a real world dataset so I hope it helps you guys use it for your own purposes!","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from transformers import T5Tokenizer, T5ForConditionalGeneration\nfrom tqdm import tqdm\nimport pandas as pd\nfrom torch.utils.data import DataLoader, SequentialSampler, TensorDataset\nimport torch\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode_file(tokenizer, data_path, max_length, pad_to_max_length=True, return_tensors=\"pt\"):\n    \"\"\"\n    Returns list[torch.tensor] of tokenized outputs from the input file separated per line\n    \"\"\"\n    examples = []\n    with open(data_path, \"r\") as f:\n        for text in f.readlines():\n            tokenized = tokenizer.batch_encode_plus(\n                [text], max_length=max_length, pad_to_max_length=pad_to_max_length, return_tensors=return_tensors,\n            )\n            # We keep dimension 0 as a singleton since `model.generate` requires dimensionality of (BS x SL)\n            examples.append(tokenized['input_ids']) # 1 x SL\n    return examples\n\ndef get_span_from_ids(input_ids, t5):\n    whole_input_str = tokenizer.decode(input_ids.squeeze())\n    input_str = whole_input_str.split('context: ')[-1]\n    question_str = whole_input_str.split('context: ')[0]\n    \n    # Return whole input string if neutral\n    if \"Which section was neutral?\" in whole_input_str:\n        #print('Neutral found! Returning input string ...')\n        return input_str.strip()\n    #print(input_str)\n    \n    # Predict\n    generated_ids = t5.generate(\n        input_ids=input_ids,\n        num_beams=1,\n        max_length=80,\n        repetition_penalty=2.5\n    ).squeeze()\n    predicted_span = tokenizer.decode(generated_ids)\n    # Make sure that the predicted span only has words contained in the context input\n    input_str_list = input_str.split()\n    predicted_span_list = predicted_span.split()\n    predicted_span_filtered = \" \".join([s for s in predicted_span_list if s in input_str_list])\n    return predicted_span_filtered\n\ndef process_span(pred_span, input_ids):\n    whole_input_str = tokenizer.decode(input_ids.squeeze())\n    input_str = whole_input_str.split('context:')[-1].strip()\n    question_str = whole_input_str.split('context:')[0].strip()\n    \n    if \"question: neutral\" in whole_input_str:\n        #print('Neutral found! Returning input string ...')\n        final_span = input_str\n    else:\n        input_str_list = input_str.split()\n        predicted_span_list = pred_span.split()\n        predicted_span_filtered = \" \".join([s for s in predicted_span_list if s in input_str_list])\n        # Simple heuristic given that blank answers are typically for short contexts\n        if predicted_span_filtered != '':\n            final_span = predicted_span_filtered\n        else:\n            final_span = input_str\n\n    return final_span.replace(' ‚Åá ', '`').replace('\"', '')\n\ndef get_span_from_ids_batch(input_ids, t5):\n    \"\"\"\n    Returns batch of predicted spans (str)\n    \"\"\"\n    generated_ids = t5.generate(\n        input_ids=input_ids,\n        num_beams=4,\n        max_length=80,\n        length_penalty=2,\n        early_stopping=True,\n        #repetition_penalty=2.5,\n    )\n    predicted_spans = [tokenizer.decode(ids) for ids in generated_ids]\n    return predicted_spans\n\ndef post_process(selected):\n    return \" \".join(set(selected.lower().split()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prepare data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv')#.iloc[:200]\nprocessed_input_test = (\"question: \" + test.sentiment + \" context: \" + test.text)\nprocessed_input_str_test = '\\n'.join(processed_input_test.values.tolist())\n\nwith open('../working/test.source', 'w') as f:\n    f.write(processed_input_str_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!head -20 ../working/test.source","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Read the model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = T5Tokenizer.from_pretrained('../input/t5-qa-training-short-question-pytorch/')\nt5 = T5ForConditionalGeneration.from_pretrained('../input/t5-5-epochs-sentiment-extraction/')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Read the data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Note we don't do any padding so no sequence length constraint is applied\ntest_input_ids = encode_file(tokenizer, '../working/test.source', 80, pad_to_max_length=True, return_tensors='pt')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(test_input_ids)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking max len\n#lens = [len(test_input_ids[i].squeeze()) for i in range(len(test_input_ids))]\n#max(lens)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_input_ids[6]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_input_ids[6].size()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Setup data as DataLoader","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_ids_tensor = torch.cat(test_input_ids).to(device)\ninput_dataset = TensorDataset(input_ids_tensor)\ninput_dataloader = DataLoader(\n    input_dataset,\n    batch_size=16,\n)\ninput_dataloader = iter(input_dataloader)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Sample prediction\n\nCareful, these go through the generator. Only run for testing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#generated_ids = t5.generate(\n#    input_ids=next(input_dataloader)[0],\n#    num_beams=1,\n#    max_length=80,\n#    #repetition_penalty=2.5\n#)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#tokenizer.decode(generated_ids[2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t5.to(device)\nt5.eval()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for param in t5.parameters():\n    param.requires_grad = False","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Make predictions\n\nFrom 2 hours on CPU, forward pass takes ~5min on GPU w/ batch size of 16","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds = []\nfor ex in tqdm(input_dataloader):\n    test_preds += get_span_from_ids_batch(ex[0], t5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Final processing\ntest_preds = [process_span(s, ids) for s, ids in zip(test_preds, test_input_ids)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#tokenizer.decode(tokenizer.encode('Cramps . . .'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(test_preds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Save predictions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv('../input/tweet-sentiment-extraction/sample_submission.csv')\nsub.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub['selected_text'] = test_preds\nsub.selected_text = sub.selected_text.map(post_process)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}