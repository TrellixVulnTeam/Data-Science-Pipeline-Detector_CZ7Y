{"cells":[{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://i.imgur.com/fOeJmu3.jpg\" width=\"600px\">","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\nHello everyone! In this project, I will try to finetune roBERTa base to predict the sentiment of a tweet. Capturing sentiment in language is important in these times where decisions and reactions are created and updated in seconds. Building machine learning models to understand the sentiment behind a tweet can help drive decisions by many parties and leverage the large amount of information on Twitter. \n\nI will use **PyTorch XLA** (PyTorch for TPUs) and **huggingface transformers** for this project.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Set up PyTorch-XLA\n\n* These few lines of code sets up PyTorch XLA for us.\n* We need PyTorch XLA to help us train PyTorch models on TPU.","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"!python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev\n!export XLA_USE_BF16=1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Install and import libraries\n\n* We will import several different packages and libraries required for different parts of the project. For example, we import <code>numpy</code> and <code>pandas</code> for data manipulation, <code>torch</code> and <code>torch_xla</code> for modeling, and <code>plotly</code> for visualization.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"!pip install -q colored\n!pip install -q transformers","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import os\nimport gc\nimport re\n\nimport time\nimport colored\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom colored import fg, bg, attr\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\n\nimport torch\nimport torch.nn as nn\nfrom torch.optim import Adam\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nfrom torch.multiprocessing import Pipe, Process\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data.distributed import DistributedSampler\n\nfrom tqdm.notebook import tqdm\nfrom sklearn.utils import shuffle\nfrom transformers import RobertaModel, RobertaTokenizer\n\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.parallel_loader as pl\nimport torch_xla.distributed.xla_multiprocessing as xmp\n\nfrom keras.utils import to_categorical\nfrom keras.preprocessing.sequence import pad_sequences as pad","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Define hyperparameters and load data\n\n* Here, we define the required hyperparameters such as the training batch size, learning rate, train/val split, etc.\n* We also load the training and tessting data required for the project using the read_csv function (from <code>pandas</code>)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"EPOCHS = 20\nSPLIT = 0.8\nMAXLEN = 48\nDROP_RATE = 0.3\nnp.random.seed(42)\n\nOUTPUT_UNITS = 3\nBATCH_SIZE = 384\nLR = (4e-5, 1e-2)\nROBERTA_UNITS = 768\nVAL_BATCH_SIZE = 384\nMODEL_SAVE_PATH = 'sentiment_model.pt'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.read_csv('../input/tweet-sentiment-extraction/test.csv')\ntrain_df = pd.read_csv('../input/tweet-sentiment-extraction/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Define PyTorch Dataset\n\n* Now we define a PyTorch Dataset which will help us feed data to the roBERTa model for training and inference.\n* We remove leading and trailing whitespaces using .strip(), tokenize the values using huggingface, and pad the tokens using keras.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class TweetDataset(Dataset):\n    def __init__(self, data, tokenizer):\n        self.data = data\n        self.text = data.text\n        self.tokenizer = tokenizer\n        self.sentiment = data.sentiment\n        self.sentiment_dict = {\"positive\": 0, \"neutral\": 1, \"negative\": 2}\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, i):\n        start, finish = 0, 2\n        pg, tg = 'post', 'post'\n        tweet = str(self.text[i]).strip()\n        tweet_ids = self.tokenizer.encode(tweet)\n\n        attention_mask_idx = len(tweet_ids) - 1\n        if start not in tweet_ids: tweet_ids = start + tweet_ids\n        tweet_ids = pad([tweet_ids], maxlen=MAXLEN, value=1, padding=pg, truncating=tg)\n\n        attention_mask = np.zeros(MAXLEN)\n        attention_mask[1:attention_mask_idx] = 1\n        attention_mask = attention_mask.reshape((1, -1))\n        if finish not in tweet_ids: tweet_ids[-1], attention_mask[-1] = finish, start\n            \n        sentiment = [self.sentiment_dict[self.sentiment[i]]]\n        sentiment = torch.FloatTensor(to_categorical(sentiment, num_classes=3))\n        return sentiment, torch.LongTensor(tweet_ids), torch.LongTensor(attention_mask)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Define roBERTa-base model\n\n* Now, we get to the interesting part: modeling! roBERTa base is a pretrained language model by Facebook AI.\n* We will use roBERTa with pretrained weights and add a (Dropout + Dense) head to to use it as a text classifier.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class Roberta(nn.Module):\n    def __init__(self):\n        super(Roberta, self).__init__()\n        self.softmax = nn.Softmax(dim=1)\n        self.drop = nn.Dropout(DROP_RATE)\n        self.roberta = RobertaModel.from_pretrained(model)\n        self.dense = nn.Linear(ROBERTA_UNITS, OUTPUT_UNITS)\n        \n    def forward(self, inp, att):\n        inp = inp.view(-1, MAXLEN)\n        _, self.feat = self.roberta(inp, att)\n        return self.softmax(self.dense(self.drop(self.feat)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Define tokenizer\n\n* Here we simply define the RobertaTokenizer from huggingface which we use to generate tokens from words.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = 'roberta-base'\ntokenizer = RobertaTokenizer.from_pretrained(model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Define cross entropy and accuracy\n\n* Here we implement categorical cross entropy and accuracy functions in PyTorch.\n* CEL is the loss function which is commonly used in classification tasks and helps us finetune roBERTa's weights.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def cel(inp, target):\n    _, labels = target.max(dim=1)\n    return nn.CrossEntropyLoss()(inp, labels)*len(inp)\n\ndef accuracy(inp, target):\n    inp_ind = inp.max(axis=1).indices\n    target_ind = target.max(axis=1).indices\n    return (inp_ind == target_ind).float().sum(axis=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train model on all 8 TPU cores\n\n* Now, we will train the roBERTa base model to classify tweet sentiments.\n* We define a simple training loop in PyTorch to train the model and validate it after each epoch.\n* We parallelize the training on all 8 TPU cores using <code>xmp.spawn</code> from PyTorch XLA (distributes training).\n* We aslo use <code>DistributedSampler</code> and <code>ParallelLoader</code> to parallelize data sampling and model training.","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"m = Roberta(); print(m)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"del m; gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def print_metric(data, batch, epoch, start, end, metric, typ):\n    t = typ, metric, \"%s\", data, \"%s\"\n    if typ == \"Train\": pre = \"BATCH %s\" + str(batch-1) + \"%s  \"\n    if typ == \"Val\": pre = \"\\nEPOCH %s\" + str(epoch+1) + \"%s  \"\n    time = np.round(end - start, 1); time = \"Time: %s{}%s s\".format(time)\n    fonts = [(fg(211), attr('reset')), (fg(212), attr('reset')), (fg(213), attr('reset'))]\n    xm.master_print(pre % fonts[0] + \"{} {}: {}{}{}\".format(*t) % fonts[1] + \"  \" + time % fonts[2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"global val_losses; global train_losses\nglobal val_accuracies; global train_accuracies\n\ndef train_fn(train_df):\n    train_df = shuffle(train_df)\n    train_df = train_df.reset_index(drop=True)\n\n    split = np.int32(SPLIT*len(train_df))\n    val_df, train_df = train_df[split:], train_df[:split]\n\n    val_df = val_df.reset_index(drop=True)\n    val_dataset = TweetDataset(val_df, tokenizer)\n    val_sampler = DistributedSampler(val_dataset, num_replicas=8,\n                                     rank=xm.get_ordinal(), shuffle=True)\n    \n    val_loader = DataLoader(val_dataset, batch_size=VAL_BATCH_SIZE,\n                            sampler=val_sampler, num_workers=0, drop_last=True)\n\n    train_df = train_df.reset_index(drop=True)\n    train_dataset = TweetDataset(train_df, tokenizer)\n    train_sampler = DistributedSampler(train_dataset, num_replicas=8,\n                                       rank=xm.get_ordinal(), shuffle=True)\n\n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n                              sampler=train_sampler, num_workers=0, drop_last=True)\n\n    device = xm.xla_device()\n    network = Roberta().to(device)\n    optimizer = Adam([{'params': network.dense.parameters(), 'lr': LR[1]},\n                      {'params': network.roberta.parameters(), 'lr': LR[0]}])\n\n    val_losses, val_accuracies = [], []\n    train_losses, train_accuracies = [], []\n    \n    start = time.time()\n    xm.master_print(\"STARTING TRAINING ...\\n\")\n\n    for epoch in range(EPOCHS):\n\n        batch = 1\n        network.train()\n        fonts = (fg(48), attr('reset'))\n        xm.master_print((\"EPOCH %s\" + str(epoch+1) + \"%s\") % fonts)\n\n        val_parallel = pl.ParallelLoader(val_loader, [device]).per_device_loader(device)\n        train_parallel = pl.ParallelLoader(train_loader, [device]).per_device_loader(device)\n        \n        for train_batch in train_parallel:\n            train_targ, train_in, train_att = train_batch\n            \n            network = network.to(device)\n            train_in = train_in.to(device)\n            train_att = train_att.to(device)\n            train_targ = train_targ.to(device)\n\n            train_preds = network.forward(train_in, train_att)\n            train_loss = cel(train_preds, train_targ.squeeze(dim=1))/len(train_in)\n            train_accuracy = accuracy(train_preds, train_targ.squeeze(dim=1))/len(train_in)\n\n            optimizer.zero_grad()\n            train_loss.backward()\n            xm.optimizer_step(optimizer)\n            \n            end = time.time()\n            batch = batch + 1\n            acc = np.round(train_accuracy.item(), 3)\n            print_metric(acc, batch, None, start, end, metric=\"acc\", typ=\"Train\")\n\n        val_loss, val_accuracy, val_points = 0, 0, 0\n\n        network.eval()\n        with torch.no_grad():\n            for val_batch in val_parallel:\n                targ, val_in, val_att = val_batch\n\n                targ = targ.to(device)\n                val_in = val_in.to(device)\n                val_att = val_att.to(device)\n                network = network.to(device)\n            \n                val_points += len(targ)\n                pred = network.forward(val_in, val_att)\n                val_loss += cel(pred, targ.squeeze(dim=1)).item()\n                val_accuracy += accuracy(pred, targ.squeeze(dim=1)).item()\n        \n        end = time.time()\n        val_loss /= val_points\n        val_accuracy /= val_points\n        acc = xm.mesh_reduce('acc', val_accuracy, lambda x: sum(x)/len(x))\n        print_metric(np.round(acc, 3), None, epoch, start, end, metric=\"acc\", typ=\"Val\")\n    \n        xm.master_print(\"\")\n        val_losses.append(val_loss); train_losses.append(train_loss.item())\n        val_accuracies.append(val_accuracy); train_accuracies.append(train_accuracy.item())\n\n    xm.master_print(\"ENDING TRAINING ...\")\n    xm.save(network.state_dict(), MODEL_SAVE_PATH); del network; gc.collect()\n\n    metric_names = ['val_loss_', 'train_loss_', 'val_acc_', 'train_acc_']\n    metric_lists = [val_losses, train_losses, val_accuracies, train_accuracies]\n    \n    for i, metric_list in enumerate(metric_lists):\n        for j, metric_value in enumerate(metric_list):\n            torch.save(metric_value, metric_names[i] + str(j) + '.pt')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"FLAGS = {}\ndef _mp_fn(rank, flags): train_fn(train_df)\nxmp.spawn(_mp_fn, args=(FLAGS,), nprocs=8, start_method='fork')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualize loss and accuracy over time\n\n* We now visualize how the loss and accuracy of the model change over time.\n* We can see that the model eventually converges to around 80% accuracy towards the end.","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"val_losses = [torch.load('val_loss_{}.pt'.format(i)) for i in range(EPOCHS)]\ntrain_losses = [torch.load('train_loss_{}.pt'.format(i)) for i in range(EPOCHS)]\nval_accuracies = [torch.load('val_acc_{}.pt'.format(i)) for i in range(EPOCHS)]\ntrain_accuracies = [torch.load('train_acc_{}.pt'.format(i)) for i in range(EPOCHS)]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig = go.Figure()\n\nfig.add_trace(go.Scatter(x=np.arange(1, len(val_losses)+1),\n                         y=val_losses, mode=\"lines+markers\", name=\"val\",\n                         marker=dict(color=\"hotpink\", line=dict(width=.5,\n                                                                color='rgb(0, 0, 0)'))))\n\nfig.add_trace(go.Scatter(x=np.arange(1, len(train_losses)+1),\n                         y=train_losses, mode=\"lines+markers\", name=\"train\",\n                         marker=dict(color=\"mediumorchid\", line=dict(width=.5,\n                                                                     color='rgb(0, 0, 0)'))))\n\nfig.update_layout(xaxis_title=\"Epochs\", yaxis_title=\"Cross Entropy\",\n                  title_text=\"Cross Entropy vs. Epochs\", template=\"plotly_white\", paper_bgcolor=\"#f0f0f0\")\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig = go.Figure()\n\nfig.add_trace(go.Scatter(x=np.arange(1, len(val_accuracies)+1),\n                         y=val_accuracies, mode=\"lines+markers\", name=\"val\",\n                         marker=dict(color=\"hotpink\", line=dict(width=.5,\n                                                                color='rgb(0, 0, 0)'))))\n\nfig.add_trace(go.Scatter(x=np.arange(1, len(train_accuracies)+1),\n                         y=train_accuracies, mode=\"lines+markers\", name=\"train\",\n                         marker=dict(color=\"mediumorchid\", line=dict(width=.5,\n                                                                     color='rgb(0, 0, 0)'))))\n\nfig.update_layout(xaxis_title=\"Epochs\", yaxis_title=\"Accuracy\",\n                  title_text=\"Accuracy vs. Epochs\", template=\"plotly_white\", paper_bgcolor=\"#f0f0f0\")\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load model\n\n* We now load the model to evaluate its performance.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"network = Roberta()\nnetwork.load_state_dict(torch.load('sentiment_model.pt'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Sample sentiment prediction\n\n* We will now see how the model performs on sample comments.\n* It appears to classify sentiment pretty accurately in these simple examples.","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"device = xm.xla_device()\nnetwork = network.to(device)\n\ndef predict_sentiment(tweet):\n    pg, tg = 'post', 'post'\n    tweet_ids = tokenizer.encode(tweet.strip())\n    sent = {0: 'positive', 1: 'neutral', 2: 'negative'}\n\n    att_mask_idx = len(tweet_ids) - 1\n    if 0 not in tweet_ids: tweet_ids = 0 + tweet_ids\n    tweet_ids = pad([tweet_ids], maxlen=MAXLEN, value=1, padding=pg, truncating=tg)\n\n    att_mask = np.zeros(MAXLEN)\n    att_mask[1:att_mask_idx] = 1\n    att_mask = att_mask.reshape((1, -1))\n    if 2 not in tweet_ids: tweet_ids[-1], att_mask[-1] = 2, 0\n    tweet_ids, att_mask = torch.LongTensor(tweet_ids), torch.LongTensor(att_mask)\n    return sent[np.argmax(network.forward(tweet_ids.to(device), att_mask.to(device)).detach().cpu().numpy())]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_sentiment(\"It does not look good now ...\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_sentiment(\"I want to know more about your product.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_sentiment(\"I have done something good today and so should you :D\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}