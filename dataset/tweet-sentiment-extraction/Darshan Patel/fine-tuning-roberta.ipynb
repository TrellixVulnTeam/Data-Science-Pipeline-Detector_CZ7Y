{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport warnings\nimport random\nimport torch \nfrom torch import nn\nimport torch.optim as optim\nimport tokenizers\nfrom transformers import RobertaModel, RobertaConfig\nfrom sklearn.model_selection import train_test_split\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def seed_everything(seed_value):\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    os.environ['PYTHONHASHSEED'] = str(seed_value)\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = True\n\nseed = 37\nseed_everything(seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nFine-tuning the library models for language modeling on a text file (GPT, GPT-2, BERT, RoBERTa).\nGPT and GPT-2 are fine-tuned using a causal language modeling (CLM) loss while BERT and RoBERTa are fine-tuned\nusing a masked language modeling (MLM) loss.\n\"\"\"\n\n\nimport argparse\nimport glob\nimport logging\nimport os\nimport pickle\nimport random\nimport re\nimport shutil\nfrom typing import Dict, List, Tuple\n\nimport numpy as np\nimport torch\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler, Sampler\nfrom torch.utils.data.distributed import DistributedSampler\nfrom tqdm import tqdm, trange\n\nfrom transformers import (\n    WEIGHTS_NAME,\n    AdamW,\n    PreTrainedModel,\n    PreTrainedTokenizer,\n    RobertaConfig,\n    RobertaForMaskedLM,\n    RobertaTokenizer,\n    get_linear_schedule_with_warmup,\n)\n\n\ntry:\n    from torch.utils.tensorboard import SummaryWriter\nexcept ImportError:\n    from tensorboardX import SummaryWriter\n\n\nlogger = logging.getLogger(__name__)\n\n\nMODEL_CLASSES = {\n    \"roberta\": (RobertaConfig, RobertaForMaskedLM, RobertaTokenizer),\n}\n\nclass SortishSampler(Sampler):\n    \"Go through the text data by order of length with a bit of randomness.\"\n\n    def __init__(self, data_source, key, bs:int):\n        self.data_source,self.key,self.bs = data_source,key,bs\n\n    def __len__(self) -> int: return len(self.data_source)\n\n    def __iter__(self):\n        idxs = np.random.permutation(len(self.data_source))\n        sz = self.bs*50\n        ck_idx = [idxs[i:i+sz] for i in range(0, len(idxs), sz)]\n        sort_idx = np.concatenate([sorted(s, key=self.key, reverse=True) for s in ck_idx])\n        sz = self.bs\n        ck_idx = [sort_idx[i:i+sz] for i in range(0, len(sort_idx), sz)]\n        max_ck = np.argmax([self.key(ck[0]) for ck in ck_idx])  # find the chunk with the largest key,\n        ck_idx[0],ck_idx[max_ck] = ck_idx[max_ck],ck_idx[0]     # then make sure it goes first.\n        sort_idx = np.concatenate(np.random.permutation(ck_idx[1:])) if len(ck_idx) > 1 else np.array([],dtype=np.int)\n        sort_idx = np.concatenate((ck_idx[0], sort_idx))\n        return iter(sort_idx)\n\nclass LineByLineTextDataset(Dataset):\n    def __init__(self, tokenizer: PreTrainedTokenizer, args, lines, block_size=128):\n        print(lines[0])\n        self.examples = tokenizer.batch_encode_plus(lines, max_length=block_size)[\"input_ids\"]\n                \n    def __len__(self):\n        return len(self.examples)\n\n    def __getitem__(self, i):\n        return torch.tensor(self.examples[i])\n\n\ndef load_and_cache_examples(args, tokenizer, lines):\n        return LineByLineTextDataset(tokenizer, args, lines=lines, block_size=args['block_size'])\n\ndef set_seed(args):\n    random.seed(args['seed'])\n    np.random.seed(args['seed'])\n    torch.manual_seed(args['seed'])\n    if args['n_gpu'] > 0:\n        torch.cuda.manual_seed_all(args['seed'])\n\ndef mask_tokens(inputs: torch.Tensor, tokenizer: PreTrainedTokenizer, args) -> Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\" Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original. \"\"\"\n    labels = inputs.clone()\n    probability_matrix = torch.full(labels.shape, args['mlm_probability'])\n    special_tokens_mask = [\n        tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n    ]\n    probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n    if tokenizer._pad_token is not None:\n        padding_mask = labels.eq(tokenizer.pad_token_id)\n        probability_matrix.masked_fill_(padding_mask, value=0.0)\n    masked_indices = torch.bernoulli(probability_matrix).bool()\n    labels[~masked_indices] = -100  # We only compute loss on masked tokens\n    \n    # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n    indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n    inputs[indices_replaced] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n\n    # 10% of the time, we replace masked input tokens with random word\n    indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n    random_words = torch.randint(len(tokenizer), labels.shape, dtype=torch.long)\n    inputs = inputs.type(torch.LongTensor)\n    inputs[indices_random] = random_words[indices_random]\n    # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n    return inputs, labels\n\n\ndef train(args, train_dataset, model: PreTrainedModel, tokenizer: PreTrainedTokenizer) -> Tuple[int, float]:\n    \"\"\" Train the model \"\"\"\n    \n    args['train_batch_size'] = args['per_gpu_train_batch_size'] * max(1, args['n_gpu'])\n\n    def collate(examples: List[torch.Tensor]):\n        if tokenizer._pad_token is None:\n            return pad_sequence(examples, batch_first=True)\n        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n    \n    train_sampler = SortishSampler(train_dataset, key=lambda t : len(train_dataset[t]), bs=args['train_batch_size'])\n    train_dataloader = DataLoader(\n        train_dataset, sampler=train_sampler, batch_size=args['train_batch_size'], collate_fn=collate\n    )\n\n    if args['max_steps'] > 0:\n        t_total = args['max_steps']\n        args['num_train_epochs'] = args['max_steps'] // (len(train_dataloader) // args['gradient_accumulation_steps']) + 1\n    else:\n        t_total = len(train_dataloader) // args['gradient_accumulation_steps'] * args['num_train_epochs']\n\n    # Prepare optimizer and schedule (linear warmup and decay)\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    optimizer_grouped_parameters = [\n        {\n            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n            \"weight_decay\": args['weight_decay'],\n        },\n        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n    ]\n    optimizer = AdamW(optimizer_grouped_parameters, lr=args['learning_rate'], eps=args['adam_epsilon'])\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, num_warmup_steps=args['warmup_steps'], num_training_steps=t_total\n    )\n\n    # Check if saved optimizer or scheduler states exist\n    if (\n        args['model_name_or_path']\n        and os.path.isfile(os.path.join(args['model_name_or_path'], \"optimizer.pt\"))\n        and os.path.isfile(os.path.join(args['model_name_or_path'], \"scheduler.pt\"))\n    ):\n        # Load in optimizer and scheduler states\n        optimizer.load_state_dict(torch.load(os.path.join(args['model_name_or_path'], \"optimizer.pt\")))\n        scheduler.load_state_dict(torch.load(os.path.join(args['model_name_or_path'], \"scheduler.pt\")))\n\n    # Train!\n    logger.info(\"***** Running training *****\")\n    logger.info(\"  Num examples = %d\", len(train_dataset))\n    logger.info(\"  Num Epochs = %d\", args['num_train_epochs'])\n    logger.info(\"  Instantaneous batch size per GPU = %d\", args['per_gpu_train_batch_size'])\n    logger.info(\n        \"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n        args['train_batch_size']\n        * args['gradient_accumulation_steps']\n        * (torch.distributed.get_world_size() if args['local_rank'] != -1 else 1),\n    )\n    logger.info(\"  Gradient Accumulation steps = %d\", args['gradient_accumulation_steps'])\n    logger.info(\"  Total optimization steps = %d\", t_total)\n\n    global_step = 0\n    epochs_trained = 0\n    steps_trained_in_current_epoch = 0\n    # Check if continuing training from a checkpoint\n    if args['model_name_or_path'] and os.path.exists(args['model_name_or_path']):\n        try:\n            # set global_step to gobal_step of last saved checkpoint from model path\n            checkpoint_suffix = args['model_name_or_path'].split(\"-\")[-1].split(\"/\")[0]\n            global_step = int(checkpoint_suffix)\n            epochs_trained = global_step // (len(train_dataloader) // args['gradient_accumulation_steps'])\n            steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args['gradient_accumulation_steps'])\n\n            logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n            logger.info(\"  Continuing training from epoch %d\", epochs_trained)\n            logger.info(\"  Continuing training from global step %d\", global_step)\n            logger.info(\"  Will skip the first %d steps in the first epoch\", steps_trained_in_current_epoch)\n        except ValueError:\n            logger.info(\"  Starting fine-tuning.\")\n\n    tr_loss, logging_loss = 0.0, 0.0\n\n    model_to_resize = model.module if hasattr(model, \"module\") else model  # Take care of distributed/parallel training\n    model_to_resize.resize_token_embeddings(len(tokenizer))\n\n    model.zero_grad()\n    train_iterator = trange(\n        epochs_trained, int(args['num_train_epochs']), desc=\"Epoch\", disable=args['local_rank'] not in [-1, 0]\n    )\n    set_seed(args)  # Added here for reproducibility\n    epoch_count=0\n    for _ in train_iterator:\n        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=True)\n        for step, batch in enumerate(epoch_iterator):\n\n            # Skip past any already trained steps if resuming training\n            if steps_trained_in_current_epoch > 0:\n                steps_trained_in_current_epoch -= 1\n                continue\n\n            inputs, labels = mask_tokens(batch, tokenizer, args) if args['mlm'] else (batch, batch)\n    \n            inputs = inputs.to(args['device'])\n            labels = labels.to(args['device'])\n            model.train()\n#             try:\n            outputs = model(inputs.type(torch.LongTensor).to(args['device']), masked_lm_labels=labels.type(torch.LongTensor).to(args['device'])) if args['mlm'] else model(inputs, labels=labels)\n#             except Exception as e:\n#                 print(e)\n#                 print(inputs, labels)\n#                 print(inputs.shape, labels.shape)\n#                 break\n                \n            loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n\n            if args['n_gpu'] > 1:\n                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n            if args['gradient_accumulation_steps'] > 1:\n                loss = loss / args['gradient_accumulation_steps']\n\n            \n            loss.backward()\n#             print(loss)\n            tr_loss += loss.item()\n            if (step + 1) % args['gradient_accumulation_steps'] == 0:\n                \n                torch.nn.utils.clip_grad_norm_(model.parameters(), args['max_grad_norm'])\n                optimizer.step()\n                scheduler.step()  # Update learning rate schedule\n                model.zero_grad()\n                global_step += 1\n#                 torch.cuda.empty_cache()\n                if args['local_rank'] in [-1, 0] and args['logging_steps'] > 0 and global_step % args['logging_steps'] == 0:\n                    # Log metrics\n                    if (\n                        args['local_rank'] == -1 and args['evaluate_during_training']\n                    ):  # Only evaluate when single GPU otherwise metrics may not average well\n                        results = evaluate(args, model, tokenizer)\n                        for key, value in results.items():\n                            tb_writer.add_scalar(\"eval_{}\".format(key), value, global_step)\n#                     tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n#                     tb_writer.add_scalar(\"loss\", (tr_loss - logging_loss) / args['logging_steps'], global_step)\n                    logging_loss = tr_loss\n\n            if args['max_steps'] > 0 and global_step > args['max_steps']:\n                epoch_iterator.close()\n                break\n        print('training loss --->' , (tr_loss - logging_loss) / args['logging_steps'])\n        epoch_count+=1\n#         output_dir = os.path.join(args['output_dir'], \"epoch_{}\".format(epoch_count))\n#         os.makedirs(output_dir, exist_ok=True)\n#         model_to_save = (\n#             model.module if hasattr(model, \"module\") else model\n#         )  # Take care of distributed/parallel training\n#         model_to_save.save_pretrained(output_dir)\n#         tokenizer.save_pretrained(output_dir)\n\n#         torch.save(args, os.path.join(output_dir, \"training_args['bin']\"))\n#         logger.info(\"Saving model checkpoint to %s\", output_dir)\n\n#         torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n#         torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n#         logger.info(\"Saving optimizer and scheduler states to %s\", output_dir)\n        if args['max_steps'] > 0 and global_step > args['max_steps']:\n            train_iterator.close()\n            break\n\n    return global_step, tr_loss / global_step\n\n\ndef evaluate(args, model: PreTrainedModel, tokenizer: PreTrainedTokenizer, eval_dataset, prefix=\"\") -> Dict:\n    # Loop to handle MNLI double evaluation (matched, mis-matched)\n    eval_output_dir = args['output_dir']\n\n    \n\n    if args['local_rank'] in [-1, 0]:\n        os.makedirs(eval_output_dir, exist_ok=True)\n\n    args['eval_batch_size'] = args['per_gpu_eval_batch_size'] * max(1, args['n_gpu'])\n    # Note that DistributedSampler samples randomly\n\n    def collate(examples: List[torch.Tensor]):\n        if tokenizer._pad_token is None:\n            return pad_sequence(examples, batch_first=True)\n        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n\n    eval_sampler = SequentialSampler(eval_dataset)\n    eval_dataloader = DataLoader(\n        eval_dataset, sampler=eval_sampler, batch_size=args['eval_batch_size'], collate_fn=collate\n    )\n\n    # multi-gpu evaluate\n    if args['n_gpu'] > 1:\n        model = torch.nn.DataParallel(model)\n\n    # Eval!\n    logger.info(\"***** Running evaluation {} *****\".format(prefix))\n    logger.info(\"  Num examples = %d\", len(eval_dataset))\n    logger.info(\"  Batch size = %d\", args['eval_batch_size'])\n    eval_loss = 0.0\n    nb_eval_steps = 0\n    model.eval()\n\n    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n        inputs, labels = mask_tokens(batch, tokenizer, args) if args['mlm'] else (batch, batch)\n        inputs = inputs.to(args['device'])\n        labels = labels.to(args['device'])\n        with torch.no_grad():\n            outputs = model(inputs.type(torch.LongTensor).to(args['device']), masked_lm_labels=labels.type(torch.LongTensor).to(args['device'])) if args['mlm'] else model(inputs, labels=labels)\n            lm_loss = outputs[0]\n            eval_loss += lm_loss.mean().item()\n        nb_eval_steps += 1\n\n    eval_loss = eval_loss / nb_eval_steps\n    perplexity = torch.exp(torch.tensor(eval_loss))\n\n    result = {\"perplexity\": perplexity, 'eval_loss':eval_loss}\n\n    output_eval_file = os.path.join(eval_output_dir, prefix, \"eval_results.txt\")\n    with open(output_eval_file, \"w\") as writer:\n        logger.info(\"***** Eval results {} *****\".format(prefix))\n        for key in sorted(result.keys()):\n            logger.info(\"  %s = %s\", key, str(result[key]))\n            writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n\n    return result\n\n\ndef main():\n    args = {}\n    args['should_continue'] = False\n    args['mlm_probability'] = 0.15\n    args['config_name'] = None\n    args['tokenizer_name'] = None\n    args['cache_dir'] = None\n    args['block_size'] = -1\n    args['do_train']=True\n    args['do_eval'] = True\n    args['evaluate_during_training'] = False\n    args['per_gpu_train_batch_size'] = 32\n    args['per_gpu_eval_batch_size'] = 32\n    args['gradient_accumulation_steps'] = 1\n    args['learning_rate'] = 5e-5\n    args['weight_decay'] = 0.1\n    args['adam_epsilon'] = 1e-6\n    args['max_grad_norm'] = 1.0\n    args['max_steps'] = -1\n    args['logging_steps'] = 500\n    args['seed'] = seed\n    args['custom_vocab_file'] = None\n    args['local_rank'] = -1\n    args['overwrite_output_dir'] = True\n    \n    args['output_dir'] = './roberta_finetuned/'\n    args['model_type'] = 'roberta'\n    args['model_name_or_path'] = 'roberta-base'\n    args['mlm'] = True\n    args['num_train_epochs'] = 4\n    args['warmup_steps'] = 1000\n    \n    \n    if args['model_type'] in [\"bert\", \"roberta\", \"distilbert\", \"camembert\"] and not args['mlm']:\n        raise ValueError(\n            \"BERT and RoBERTa-like models do not have LM heads but masked LM heads. They must be run using the --mlm \"\n            \"flag (masked language modeling).\"\n        )\n    if (\n        os.path.exists(args['output_dir'])\n        and os.listdir(args['output_dir'])\n        and args['do_train']\n        and not args['overwrite_output_dir']\n    ):\n        raise ValueError(\n            \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n                args['output_dir']\n            )\n        )\n\n    args['device'] = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    args['n_gpu'] = torch.cuda.device_count()\n    \n    config_class, model_class, tokenizer_class = MODEL_CLASSES[args['model_type']]\n\n    if args['config_name']:\n        config = config_class.from_pretrained(args['config_name'], cache_dir=args['cache_dir'])\n    elif args['model_name_or_path']:\n        config = config_class.from_pretrained(args['model_name_or_path'], cache_dir=args['cache_dir'])\n    else:\n        config = config_class()\n\n    if args['tokenizer_name']:\n        tokenizer = tokenizer_class.from_pretrained(args['tokenizer_name'], cache_dir=args['cache_dir'])\n    elif args['model_name_or_path']:\n        tokenizer = tokenizer_class.from_pretrained(args['model_name_or_path'], cache_dir=args['cache_dir'])\n    else:\n        raise ValueError(\n            \"You are instantiating a new {} tokenizer. This is not supported, but you can do it from another script, save it,\"\n            \"and load it from here, using --tokenizer_name\".format(tokenizer_class.__name__)\n        )\n    if args['block_size'] <= 0:\n        args['block_size'] = tokenizer.max_len_single_sentence\n        # Our input block size will be the max possible for the model\n    else:\n        args['block_size'] = min(args['block_size'], tokenizer.max_len_single_sentence)\n\n    if args['model_name_or_path']:\n        model = model_class.from_pretrained(\n            args['model_name_or_path'],\n            from_tf=bool(\".ckpt\" in args['model_name_or_path']),\n            config=config,\n            cache_dir=args['cache_dir'],\n        )\n        \n    else:\n        logger.info(\"Training new model from scratch\")\n        model = model_class(config=config)\n    \n    if args['tokenizer_name'] and 'bert' in args['tokenizer_name']:\n        print('using custom tokenizer')\n        with open('{}/vocab.txt'.format(args['tokenizer_name']),'r') as f:\n            custom_vocab = [line for line in f.read().splitlines()]\n        bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n        bert_org_path = bert_tokenizer.save_vocabulary('.')[0]\n        with open(bert_org_path,'r') as f:\n            bert_vocab = [line for line in f.read().splitlines()]\n        os.remove(bert_org_path)\n        bert_model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n        bert_model.resize_token_embeddings(len(custom_vocab))\n        model.resize_token_embeddings(len(custom_vocab))\n        bert_weights_vocab = {i:j for i,j in  zip(bert_vocab ,bert_model.state_dict()['bert.embeddings.word_embeddings.weight'])}\n        custom_weights_vocab = {i:j for i,j in zip(custom_vocab ,bert_model.state_dict()['bert.embeddings.word_embeddings.weight'])}\n        for i in custom_vocab:\n            try:\n                weight = bert_weights_vocab[i]\n                custom_weights_vocab[i] = weight\n            except KeyError:\n                pass\n        model.state_dict()['bert.embeddings.word_embeddings.weight'] = custom_weights_vocab\n        tokenizer = tokenizer_class.from_pretrained(args['tokenizer_name'], cache_dir=args['cache_dir'])\n        config.vocab_size = len(tokenizer)\n        del bert_model, bert_tokenizer\n    \n    if args['tokenizer_name'] and 'gpt-2' in args['tokenizer_name']:\n        import json\n        print('using custom tokenizer')\n        custom_vocab = list(json.load(open('{}/vocab.json'.format(args['tokenizer_name']))).keys())\n        bert_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n        bert_org_path = bert_tokenizer.save_vocabulary('.')[0]\n        bert_vocab = list(json.load(open(bert_org_path)).keys())\n        os.remove(bert_org_path)\n        bert_model = GPT2LMHeadModel.from_pretrained('gpt2')\n        bert_model.resize_token_embeddings(len(custom_vocab))\n        model.resize_token_embeddings(len(custom_vocab))\n        bert_weights_vocab = {i:j for i,j in  zip(bert_vocab ,bert_model.state_dict()['transformer.wte.weight'])}\n        custom_weights_vocab = {i:j for i,j in zip(custom_vocab ,bert_model.state_dict()['transformer.wte.weight'])}\n        for i in custom_vocab:\n            try:\n                weight = bert_weights_vocab[i]\n                custom_weights_vocab[i] = weight\n            except KeyError:\n                pass\n        model.state_dict()['transformer.wte.weight'] = custom_weights_vocab\n        tokenizer = tokenizer_class.from_pretrained(args['tokenizer_name'], cache_dir=args['cache_dir'])\n        tokenizer.add_special_tokens({'pad_token':'<pad>', 'unk_token':'<unk>', 'bos_token':'<cls>', 'eos_token':'<sep>'})\n        config.vocab_size = len(tokenizer)\n        del bert_model, bert_tokenizer\n    \n    if args['custom_vocab_file']:\n        logger.info(\"adding custom vocab from file : %s\", args['custom_vocab_file'])\n        import pickle\n        custom_vocab = pickle.load(open(args['custom_vocab_file'],'rb'))\n#         custom_vocab = ['Ġ'+i for i in custom_vocab]\n        tokenizer.add_tokens(custom_vocab)\n        if tokenizer._pad_token==None: tokenizer.add_special_tokens({'pad_token':'<pad>'})\n        model.resize_token_embeddings(len(tokenizer))\n        config.vocab_size = len(tokenizer)\n        model.lm_head.bias = torch.nn.Parameter(torch.nn.functional.pad(\n                model.lm_head.bias,\n                (0, model.get_output_embeddings().weight.shape[0]-model.lm_head.bias.shape[0]),\n                \"constant\",\n                0,\n            ))\n    logger.info(\"Vocab Size : %s\", len(tokenizer))\n    \n    \n    init_layers = [9, 10, 11]\n    dense_names = [\"query\", \"key\", \"value\", \"dense\"]\n    layernorm_names = [\"LayerNorm\"]\n    for name, module in model.named_parameters():\n        if any(f\".{i}.\" in name for i in init_layers):\n            if any(n in name for n in dense_names):\n                if \"bias\" in name:\n                    module.data.zero_()\n                elif \"weight\" in name:\n                    module.data.normal_(mean=0.0, std=0.02)\n            elif any(n in name for n in layernorm_names):\n                if \"bias\" in name:\n                    module.data.zero_()\n                elif \"weight\" in name:\n                    module.data.fill_(1.0)\n    \n    \n    \n    model.to(args['device'])\n\n    logger.info(\"Training/evaluation parameters %s\", args)\n    \n    df = pd.concat([pd.read_csv('../input/tweet-sentiment-extraction/train.csv'),pd.read_csv('../input/tweet-sentiment-extraction/test.csv')],ignore_index=True)\n    df = df.dropna(subset=['text'])\n    tr_df, ts_df = train_test_split(df, test_size=0.05, random_state = seed)\n    tr_lines = tr_df.text.tolist()\n    te_lines = ts_df.text.tolist()\n    del tr_df, ts_df\n    import gc\n    gc.collect()\n    eval_dataset = load_and_cache_examples(args, tokenizer, te_lines)\n    # Training\n    if args['do_train']:\n        test_text = 'this is sentence to encode, and this is wrgn'\n        print('original text: ',test_text)\n        print('tokenized: ', tokenizer.tokenize(test_text))\n        print('encoded: ', tokenizer.encode(test_text))\n        \n        \n        train_dataset = load_and_cache_examples(args, tokenizer, tr_lines)\n\n        global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n        logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n\n    # Saving best-practices: if you use save_pretrained for the model and tokenizer, you can reload them using from_pretrained()\n    if args['do_train']:\n        # Create output directory if needed\n        os.makedirs(args['output_dir'], exist_ok=True)\n\n        logger.info(\"Saving model checkpoint to %s\", args['output_dir'])\n        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n        # They can then be reloaded using `from_pretrained()`\n        model_to_save = (\n            model.module if hasattr(model, \"module\") else model\n        )  # Take care of distributed/parallel training\n        model_to_save.save_pretrained(args['output_dir'])\n        tokenizer.save_pretrained(args['output_dir'])\n\n        # Good practice: save your training arguments together with the trained model\n        torch.save(args, os.path.join(args['output_dir'], \"training_args.bin\"))\n\n        # Load a trained model and vocabulary that you have fine-tuned\n        model = model_class.from_pretrained(args['output_dir'])\n        tokenizer = tokenizer_class.from_pretrained(args['output_dir'])\n        model.to(args['device'])\n\n    # Evaluation\n    results = {}\n    if args['do_eval']:\n        checkpoints = [args['output_dir']]\n        for checkpoint in checkpoints:\n            global_step = checkpoint.split(\"-\")[-1] if len(checkpoints) > 1 else \"\"\n            prefix = checkpoint.split(\"/\")[-1] if checkpoint.find(\"checkpoint\") != -1 else \"\"\n\n            model = model_class.from_pretrained(checkpoint)\n            model.to(args['device'])\n            \n            result = evaluate(args, model, tokenizer,eval_dataset ,prefix=prefix)\n            result = dict((k + \"_{}\".format(global_step), v) for k, v in result.items())\n            results.update(result)\n\n    return results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"main()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}