{"cells":[{"metadata":{"id":"OrYYvt-7dhus"},"cell_type":"markdown","source":"Reference: https://www.kaggle.com/nkoprowicz/a-simple-solution-using-only-word-counts","execution_count":null},{"metadata":{"id":"hWZE0gVQf5sk"},"cell_type":"markdown","source":"### Load Packages","execution_count":null},{"metadata":{"id":"t-VmNJ41gPsD","outputId":"e6c2d741-631e-444b-9128-200d05f29a6c","trusted":false},"cell_type":"code","source":"#pip install afinn","execution_count":null,"outputs":[]},{"metadata":{"id":"f-YW0PCFdgXd","outputId":"28d590cc-9f44-4456-9d7f-2a148b8a12af","trusted":false},"cell_type":"code","source":"#pip install vaderSentiment","execution_count":null,"outputs":[]},{"metadata":{"id":"n6YmGxN3CeJF","outputId":"3057d013-67f6-483f-daa3-de30d8cfa201","trusted":false},"cell_type":"code","source":"#pip install shap","execution_count":null,"outputs":[]},{"metadata":{"id":"WjzY1DVlf4fF","trusted":false},"cell_type":"code","source":"#nltk packages\nimport nltk\nfrom nltk import word_tokenize\nfrom nltk.stem import WordNetLemmatizer \nfrom nltk.corpus import stopwords\nfrom string import punctuation\n\n#Pandas\nimport pandas as pd\nimport numpy as np\n#enable display of all columns in notebook\npd.options.display.max_columns = 999 \nnp.random.seed(12345)\n\n#re\nimport re\n\n#sklearn\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import r2_score\n\n#string\nimport string\n\n\n#sentiment packages\nfrom textblob import TextBlob\n\n#Used in creating ngrams\nimport math\n\n#xgboost\nimport shap\nimport xgboost as xgb\n\n#plt\nimport matplotlib.pyplot as plt\n#enables display of plots in notebook\n%matplotlib inline \n\n#stats\nfrom scipy import stats","execution_count":null,"outputs":[]},{"metadata":{"id":"X4Cz3Yyhgelv"},"cell_type":"markdown","source":"### Load Dataset","execution_count":null},{"metadata":{"id":"IbbX0XQFGJHR","outputId":"43393d8a-83c3-472f-de8b-c8b02f447de3","trusted":false},"cell_type":"code","source":"#from google.colab import drive\n#drive.mount(\"/content/gdrive\")","execution_count":null,"outputs":[]},{"metadata":{"id":"CykXzYXiGPdC","outputId":"7cca914f-1c19-441a-8bef-29ed2d86adfb","trusted":false},"cell_type":"code","source":" ##%cd /content/gdrive/My Drive/ML","execution_count":null,"outputs":[]},{"metadata":{"id":"8jvhSMaT0oPi","trusted":false},"cell_type":"code","source":"##train = pd.read_csv(\"train.csv\")\n##test = pd.read_csv(\"test.csv\")\n##sample_submission = pd.read_csv(\"sample_submission.csv\")\ntrain = pd.read_csv(\"/kaggle/input/tweet-sentiment-extraction/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/tweet-sentiment-extraction/test.csv\")\nsample_submission = pd.read_csv(\"/kaggle/input/tweet-sentiment-extraction/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"id":"t1fUTnhGlvmp"},"cell_type":"markdown","source":"Check Missing Values","execution_count":null},{"metadata":{"id":"vqVLYIaNlu3L","outputId":"bc224c4d-16bc-4517-e0a7-059b3eca7919","trusted":false},"cell_type":"code","source":"train[train['text'].isna()]","execution_count":null,"outputs":[]},{"metadata":{"id":"0l25uHWA-OS4","trusted":false},"cell_type":"code","source":"train.drop(314, inplace = True)\ntrain = train.reset_index(drop = True)","execution_count":null,"outputs":[]},{"metadata":{"id":"mL9EG-0KjIxU","outputId":"fe69150d-474e-4fb0-dbee-9da2b3470cdf","trusted":false},"cell_type":"code","source":"train.info()\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"1-sE63RUjNns"},"cell_type":"markdown","source":"- textID - unique ID for each piece of text\n- text - the text of the tweet\n- sentiment - the general sentiment of the tweet\n- selected_text - [train only] the text that supports the tweet's sentiment","execution_count":null},{"metadata":{"id":"Na4ReP5FjZBl","trusted":false},"cell_type":"code","source":"#Prepare column names for xgboost\ntrain.columns = train.columns.str.strip()\ntest.columns = test.columns.str.strip()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Define a function to calculate jaccard score\ndef jaccard(str1,str2):\n    a=str1.lower().split(\" \")\n    b=str2.lower().split(\" \")\n    c=set(a)&set(b)\n    prop=len(c)/(len(a)+len(b)-len(c))\n    return(prop)","execution_count":null,"outputs":[]},{"metadata":{"id":"EWD5OrOGX6rk","trusted":false},"cell_type":"code","source":"#To save time, remove neutral from the train set.\n# pos_train = train[train['sentiment'] == 'positive']\n# neg_train = train[train['sentiment'] == 'negative']\nneutral_train = train[train['sentiment'] == 'neutral']\ntrain = train[train['sentiment'] != 'neutral']\nneutral_test = test[test['sentiment'] == 'neutral']\ntest = test[test['sentiment'] != 'neutral']","execution_count":null,"outputs":[]},{"metadata":{"id":"ae3gU_R12GOr","outputId":"14c940b9-b0c8-4d6f-9d1c-3e3176dcb362","trusted":false},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"9EEMrUw2dwuD","outputId":"9edc829f-89e0-4f60-d4c7-62b6e0957baf","trusted":false},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Create jaccard score for texts in train set with neutral sentiment\nneutral_train.apply(lambda x: jaccard(x['text'], x['selected_text']), axis = 1).hist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The jaccard scores for texts in train set with neutral sentiment are close to 1, so we use original text to represent the forecasting result.","execution_count":null},{"metadata":{"id":"2lszdzpHa5vE"},"cell_type":"markdown","source":"Remove URL from text","execution_count":null},{"metadata":{"id":"BrL8NYEPa5IM","trusted":false},"cell_type":"code","source":"def remove_url(text):\n    url_pattern = re.compile(\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\", flags=re.UNICODE)\n    return url_pattern.sub(r'', text)","execution_count":null,"outputs":[]},{"metadata":{"id":"cjwWlzNGbUVJ","trusted":false},"cell_type":"code","source":"train['text']=train['text'].apply(lambda x:remove_url(x))\ntest['text']=test['text'].apply(lambda x:remove_url(x))","execution_count":null,"outputs":[]},{"metadata":{"id":"ph-EFszU468n"},"cell_type":"markdown","source":"Create features\n","execution_count":null},{"metadata":{"id":"v6aWTcNbDICP","outputId":"210a07b2-98b8-470e-ce74-b8268a9efe42","trusted":false},"cell_type":"code","source":"#Check the distribution of proportion of selected_text versus text_n_words in train set\ntrain['text_n_words'] = train['text'].apply(str).apply(lambda x: len(x.split(\" \")))\ntrain['sel_text_n_words'] = train['selected_text'].apply(str).apply(lambda x: len(x.split(\" \")))\ntrain['prop_sel_text_len'] = train['sel_text_n_words']/train['text_n_words']\ntrain['prop_sel_text_len'].hist()","execution_count":null,"outputs":[]},{"metadata":{"id":"5pvQTsy7HUNM","trusted":false},"cell_type":"code","source":"#Number of string in text\ndef find_str(text):\n  result = re.split('[!?,.]',text)\n  count = len(result)\n  return count\n\n#List of prepositions\nprep = ['about', 'below', 'excepting', 'off', 'toward', 'above', 'beneath', 'on', 'under', 'across', 'from','onto',\n'underneath', 'after','between', 'in', 'out', 'until', 'against', 'beyond' , 'outside', 'up' , 'along', 'but', 'inside','over',\n'upon', 'among','by','past', 'around', 'concerning', 'regarding', 'with', 'at', 'despite','into', 'since', 'within',\n'down', 'like', 'through','without', 'before', 'during', 'near', 'throughout', 'behind', 'except', 'of', 'to', 'for']\n\ndef preposition(sentence):\n  words = sentence.split()\n  prep_num = 0\n  for x in prep:\n      prep_num += words.count(x)\n  return prep_num","execution_count":null,"outputs":[]},{"metadata":{"id":"sN0B2H0KGv1W","trusted":false},"cell_type":"code","source":"#Create features for train set before running ngrams to save time\ntrain['text_sent_blob'] = train['text'].apply(str).apply(lambda x: TextBlob(x.lower()).sentiment.polarity)#Blog score\n##af = Afinn()\n##train['text_sent_afinn'] = train['text'].apply(str).apply(lambda x: af.score(x.lower()))#Afinn score\n##analyser = SentimentIntensityAnalyzer()\n##train['text_sent_varder'] = train['text'].apply(str).apply(lambda x: analyser.polarity_scores(x.lower())[\"compound\"])#Varder score\ntrain['text_n_str'] = train['text'].apply(str).apply(lambda x: find_str(x.lower()))#Number of string in text\ntrain['text_n_uq_words'] = train['text'].apply(str).apply(lambda x: len(np.unique(x.strip().split())))#Number of unique words\ntrain['text_n_uq_chars'] = train['text'].apply(str).apply(lambda x: len(np.unique(list(x.replace(\" \", \"\")))))#Number of unique characters\ntrain['text_n_prepositions'] = train['text'].apply(str).apply(lambda x: preposition(x.lower()))#Number of prepositions","execution_count":null,"outputs":[]},{"metadata":{"id":"RbQzSCJ7IgVR","outputId":"caeec55c-8e8b-49a6-a146-f5f0a6c8ab5f","trusted":false},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"sVle35SXLuEN","trusted":false},"cell_type":"code","source":"#Create features for test set\ntest['text_n_words'] = test['text'].apply(str).apply(lambda x: len(x.split(\" \")))\ntest['text_sent_blob'] = test['text'].apply(str).apply(lambda x: TextBlob(x.lower()).sentiment.polarity)#Blog score\n##af = Afinn()\n##test['text_sent_afinn'] = test['text'].apply(str).apply(lambda x: af.score(x.lower()))#Afinn score\n##analyser = SentimentIntensityAnalyzer()\n##test['text_sent_varder'] = test['text'].apply(str).apply(lambda x: analyser.polarity_scores(x.lower())[\"compound\"])#Varder score\ntest['text_n_str'] = test['text'].apply(str).apply(lambda x: find_str(x.lower()))#Number of string in text\ntest['text_n_uq_words'] = test['text'].apply(str).apply(lambda x: len(np.unique(x.strip().split())))#Number of unique words\ntest['text_n_uq_chars'] = test['text'].apply(str).apply(lambda x: len(np.unique(list(x.replace(\" \", \"\")))))#Number of unique characters\ntest['text_n_prepositions'] = test['text'].apply(str).apply(lambda x: preposition(x.lower()))#Number of prepositions","execution_count":null,"outputs":[]},{"metadata":{"id":"ItNIppkVLxl2","outputId":"18a6779d-8ae9-4213-f2b9-1af9b40d7aa1","trusted":false},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"0F-IAvjCFliC","trusted":false},"cell_type":"code","source":"#Create ngrams for a line\ndef create_ngrams(line):\n  words = line['text'].split()\n  # subsets = [words[i:j+1] for i in range(len(words)) for j in range(i,len(words))] #Create subset for whole train set\n  subsets = [words[i:j+1] for i in range(len(words)) for j in range(i,int(math.ceil(len(words)/2)))] #Create subset for only the rows with subsetsLen(ngrams)/Len(original text) <= 0.5\n  return subsets","execution_count":null,"outputs":[]},{"metadata":{"id":"f3HSGHFgCpmW","trusted":false},"cell_type":"code","source":"%%time\n#！！！！！！！！！！It takes 30 mins to run...\n#Create ngrams subsets for train set\ntrain_subsets = pd.DataFrame()\ntrain_temp = pd.DataFrame()\nfor i in range(len(train)):\n  ngrams_lines = create_ngrams(train.iloc[i])\n  train_temp = pd.DataFrame([train.iloc[i]]*(len(ngrams_lines)))#Create the new lines and \n  train_temp['ngram'] = list(map(lambda x: \" \".join(words for words in x),ngrams_lines))#Combine the new lines with their ngrams\n  train_subsets = train_subsets.append(train_temp,ignore_index=True)#Append new lines with ngrams to train_subsets","execution_count":null,"outputs":[]},{"metadata":{"id":"FURkYp37GVT4","trusted":false},"cell_type":"code","source":"train_temp = train\ntrain_temp['ngram'] = train_temp['text']\ntrain_subsets = train_subsets.append(train_temp,ignore_index=True) #Append original train set to get subsetsLen(ngrams)/Len(original text) = 1","execution_count":null,"outputs":[]},{"metadata":{"id":"R9eCD-eM_k9g","trusted":false},"cell_type":"code","source":"#Update the two columns created before\ntrain_subsets['sel_text_n_words'] = train_subsets['ngram'].apply(str).apply(lambda x: len(x.split(\" \")))\ntrain_subsets['prop_sel_text_len'] = train_subsets['sel_text_n_words']/train_subsets['text_n_words']","execution_count":null,"outputs":[]},{"metadata":{"id":"QEY32LYhKf7N","trusted":false},"cell_type":"code","source":"len(train_subsets)","execution_count":null,"outputs":[]},{"metadata":{"id":"7Zizv-R4_k_-","outputId":"6940b0fa-fec9-4349-ca0d-aead3c745e8a","trusted":false},"cell_type":"code","source":"train_subsets.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"O1tk0wldKzVE","trusted":false},"cell_type":"code","source":"train_subsets.tail()","execution_count":null,"outputs":[]},{"metadata":{"id":"Q8Nd274kJxUA","outputId":"db11d4a0-3cee-461a-b237-ed27d98ca8e3","trusted":false},"cell_type":"code","source":"%%time\ntest_subsets = pd.DataFrame()\ntest_temp = pd.DataFrame()\nfor i in range(len(test)):\n  ngrams_lines = create_ngrams(test.iloc[i])\n  test_temp = pd.DataFrame([test.iloc[i]]*(len(ngrams_lines)))#Create the new lines and \n  test_temp['ngram'] = list(map(lambda x: \" \".join(words for words in x),ngrams_lines))#Combine the new lines with their ngrams\n  test_subsets = test_subsets.append(test_temp,ignore_index=True)#Append new lines with ngrams to test_subsets","execution_count":null,"outputs":[]},{"metadata":{"id":"yUoaDTWNKRdl","trusted":false},"cell_type":"code","source":"test_temp = test\ntest_temp['ngram'] = test_temp['text']\ntest_subsets = test_subsets.append(test_temp,ignore_index=True) #Append original test set to get subsetsLen(ngrams)/Len(original text) = 1","execution_count":null,"outputs":[]},{"metadata":{"id":"KD8D-H_IK8TM","trusted":false},"cell_type":"code","source":"#Update the two columns created before\ntest_subsets['sel_text_n_words'] = test_subsets['ngram'].apply(str).apply(lambda x: len(x.split(\" \")))\ntest_subsets['prop_sel_text_len'] = test_subsets['sel_text_n_words']/test_subsets['text_n_words']","execution_count":null,"outputs":[]},{"metadata":{"id":"lAxL6SeysTOG","outputId":"9b4a52dd-b2e5-469c-a578-8d4bb615aaae","trusted":false},"cell_type":"code","source":"len(test_subsets)","execution_count":null,"outputs":[]},{"metadata":{"id":"A6WMGKt-K2hS","outputId":"9eab7547-89fc-4cf2-ba85-97ed9e4da8ad","trusted":false},"cell_type":"code","source":"test_subsets.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"E3Lqnnu7K5WU","outputId":"135e4fde-0f85-40f1-84f3-d68f8e4515fb","trusted":false},"cell_type":"code","source":"test_subsets.tail()","execution_count":null,"outputs":[]},{"metadata":{"id":"T9AIrD-VmIlN"},"cell_type":"markdown","source":"Create Features","execution_count":null},{"metadata":{"id":"y6agBesAlila","outputId":"519c0273-46be-48e6-b495-9a0cc8a27aba","trusted":false},"cell_type":"code","source":"%%time\n#It takes 6 mins\n\n#Jaccard score\ntrain_subsets['jaccard'] = train_subsets.apply(lambda x: jaccard(x['ngram'], x['selected_text']), axis = 1) # Create jaccard score for each row\n#Blob score\ntrain_subsets['sel_text_sent_blob'] = train_subsets['ngram'].apply(str).apply(lambda x: TextBlob(x.lower()).sentiment.polarity)\ntrain_subsets['dif_text_sent_blob'] = train_subsets['text_sent_blob'] - train_subsets['sel_text_sent_blob']\n#Afinn score\n##train_subsets['sel_text_sent_afinn'] = train_subsets['ngram'].apply(str).apply(lambda x: af.score(x.lower()))\n##train_subsets['dif_text_sent_afinn'] = train_subsets['text_sent_afinn'] - train_subsets['sel_text_sent_afinn']\n#Varder score\n##train_subsets['sel_text_sent_varder'] = train_subsets['ngram'].apply(str).apply(lambda x: analyser.polarity_scores(x.lower())[\"compound\"])\n##train_subsets['dif_text_sent_varder'] = train_subsets['text_sent_varder'] - train_subsets['sel_text_sent_varder']\n#Proportion of number of string of ngrams\ntrain_subsets['sel_text_n_str'] = train_subsets['ngram'].apply(str).apply(lambda x: find_str(x.lower()))\ntrain_subsets['prop_sel_text_n_str'] = train_subsets['sel_text_n_str'] / train_subsets['text_n_str']\n#Number of unique words of ngrams\ntrain_subsets['sel_text_n_uq_words'] = train_subsets['ngram'].apply(str).apply(lambda x: len(np.unique(x.strip().split())))\ntrain_subsets['prop_sel_text_n_uq_words'] =  train_subsets['sel_text_n_uq_words']/train_subsets['text_n_uq_words']\n#Number of unique characters of ngrams\ntrain_subsets['sel_text_n_uq_chars'] = train_subsets['ngram'].apply(str).apply(lambda x: len(np.unique(list(x.replace(\" \", \"\")))))\ntrain_subsets['prop_sel_text_n_uq_chars'] = train_subsets['sel_text_n_uq_chars']/train_subsets['text_n_uq_chars'] \n#Number of prepositions\ntrain_subsets['sel_text_n_prepositions'] = train_subsets['ngram'].apply(str).apply(lambda x: preposition(x.lower()))\ntrain_subsets['prop_sel_text_n_prepositions'] = train_subsets['sel_text_n_prepositions']/train_subsets['text_n_prepositions']","execution_count":null,"outputs":[]},{"metadata":{"id":"xh1xsOb9jt_T","outputId":"a3fc04a0-5ba4-448a-f118-23492d599bc1","trusted":false},"cell_type":"code","source":"train_subsets.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"GIqi44GBLF9K","outputId":"3370e873-775b-44ed-d58f-337624254731","trusted":false},"cell_type":"code","source":"%%time\n#It takes 1 mins\n\n#Blob score\ntest_subsets['sel_text_sent_blob'] = test_subsets['ngram'].apply(str).apply(lambda x: TextBlob(x.lower()).sentiment.polarity)\ntest_subsets['dif_text_sent_blob'] = test_subsets['text_sent_blob'] - test_subsets['sel_text_sent_blob']\n#Afinn score\n##test_subsets['sel_text_sent_afinn'] = test_subsets['ngram'].apply(str).apply(lambda x: af.score(x.lower()))\n##test_subsets['dif_text_sent_afinn'] = test_subsets['text_sent_afinn'] - test_subsets['sel_text_sent_afinn']\n#Varder score\n##test_subsets['sel_text_sent_varder'] = test_subsets['ngram'].apply(str).apply(lambda x: analyser.polarity_scores(x.lower())[\"compound\"])\n##test_subsets['dif_text_sent_varder'] = test_subsets['text_sent_varder'] - test_subsets['sel_text_sent_varder']\n#Proportion of number of string of ngrams\ntest_subsets['sel_text_n_str'] = test_subsets['ngram'].apply(str).apply(lambda x: find_str(x.lower()))\ntest_subsets['prop_sel_text_n_str'] = test_subsets['sel_text_n_str'] / test_subsets['text_n_str']\n#Number of unique words of ngrams\ntest_subsets['sel_text_n_uq_words'] = test_subsets['ngram'].apply(str).apply(lambda x: len(np.unique(x.strip().split())))\ntest_subsets['prop_sel_text_n_uq_words'] = test_subsets['sel_text_n_uq_words']/test_subsets['text_n_uq_words'] \n#Number of unique characters of ngrams\ntest_subsets['sel_text_n_uq_chars'] = test_subsets['ngram'].apply(str).apply(lambda x: len(np.unique(list(x.replace(\" \", \"\")))))\ntest_subsets['prop_sel_text_n_uq_chars'] = test_subsets['sel_text_n_uq_chars']/test_subsets['text_n_uq_chars']\n#Number of prepositions\ntest_subsets['sel_text_n_prepositions'] = test_subsets['ngram'].apply(str).apply(lambda x: preposition(x.lower()))\ntest_subsets['prop_sel_text_n_prepositions'] = test_subsets['sel_text_n_prepositions']/test_subsets['text_n_prepositions']","execution_count":null,"outputs":[]},{"metadata":{"id":"bpegX73vLJUw","outputId":"6fc7c06a-5b58-43b3-8b93-c717b9fd33bf","trusted":false},"cell_type":"code","source":"test_subsets.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_subsets.to_csv('train_subsets.csv',index = False)\ntest_subsets.to_csv('test_subsets.csv',index = False)","execution_count":null,"outputs":[]},{"metadata":{"id":"jGwiJclpAaRp"},"cell_type":"markdown","source":"## Xgboost","execution_count":null},{"metadata":{"id":"BgFQxFvUA_Oh","trusted":false},"cell_type":"code","source":"# %cd C:\\Users\\77548\\Desktop\\tweet-sentiment-extraction\n# train_subsets = pd.read_csv(\"train_subsets_without_url.csv\")\n# test_subsets = pd.read_csv(\"test_subsets_without_url.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_subsets_pos =  train_subsets[train_subsets['sentiment'] == 'positive']\ntest_subsets_pos =  test_subsets[test_subsets['sentiment'] == 'positive']\ntrain_subsets_neg =  train_subsets[train_subsets['sentiment'] == 'negative']\ntest_subsets_neg =  test_subsets[test_subsets['sentiment'] == 'negative']","execution_count":null,"outputs":[]},{"metadata":{"id":"3znH7XzH8kyr","trusted":false},"cell_type":"code","source":"train_subsets_pos['ID'] = train_subsets_pos.index + 1\ntest_subsets_pos['ID'] = test_subsets_pos.index + 1\ntrain_subsets_neg['ID'] = train_subsets_neg.index + 1\ntest_subsets_neg['ID'] = test_subsets_neg.index + 1","execution_count":null,"outputs":[]},{"metadata":{"id":"gQv4KmWXBgXn","trusted":false},"cell_type":"code","source":"def score_to_numeric(x):\n    if x=='negative':\n        return 0\n    if x=='positive':\n        return 1","execution_count":null,"outputs":[]},{"metadata":{"id":"DvD0tD8DBgai","trusted":false},"cell_type":"code","source":"train_subsets_pos['sentiment'] = train_subsets_pos['sentiment'].apply(score_to_numeric)\ntest_subsets_pos['sentiment'] = test_subsets_pos['sentiment'].apply(score_to_numeric)\ntrain_subsets_neg['sentiment'] = train_subsets_neg['sentiment'].apply(score_to_numeric)\ntest_subsets_neg['sentiment'] = test_subsets_neg['sentiment'].apply(score_to_numeric)","execution_count":null,"outputs":[]},{"metadata":{"id":"zUDPSyG7Bgcy","outputId":"89d71c6d-4a89-489e-8d10-8428c183e092","trusted":false},"cell_type":"code","source":"y = 'jaccard'\n# X = [name for name in train_subsets.columns if name not in [y, 'ID', 'textID','text', 'selected_text','ngram']\nX = [name for name in train_subsets.columns if name not in [y, 'ID', 'textID','text', 'selected_text','ngram']]\nprint('y =', y)\nprint('X =', X)","execution_count":null,"outputs":[]},{"metadata":{"id":"nVJgGOpfBgfI","outputId":"031b0f8b-5319-4ceb-8f2d-ec95021495ac","trusted":false},"cell_type":"code","source":"train_subsets_pos[X + [y]].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_subsets_neg[X + [y]].describe()","execution_count":null,"outputs":[]},{"metadata":{"id":"wJgr-ePc3wI3","outputId":"0e9d0bd7-6ec7-4a51-fe08-6e43bd5fd490","trusted":false},"cell_type":"code","source":"np.random.seed(12345) # set random seed for reproducibility\nsplit_ratio = 0.7     # 70%/30% train/test split\n\n# execute split_pos\nsplit_pos = np.random.rand(len(train_subsets_pos)) < split_ratio\ntrain_pos = train_subsets_pos[split_pos]\ntest_pos = train_subsets_pos[~split_pos]\n\n# summarize split_pos\nprint('Train_pos data rows = %d, columns = %d' % (train_pos.shape[0], train_pos.shape[1]))\nprint('Test_pos data rows = %d, columns = %d' % (test_pos.shape[0], test_pos.shape[1]))\n\n# execute split_neg\nsplit_neg = np.random.rand(len(train_subsets_neg)) < split_ratio\ntrain_neg = train_subsets_neg[split_neg]\ntest_neg = train_subsets_neg[~split_neg]\n\n# summarize split_neg\nprint('Train_neg data rows = %d, columns = %d' % (train_neg.shape[0], train_neg.shape[1]))\nprint('Test_neg data rows = %d, columns = %d' % (test_neg.shape[0], test_neg.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"id":"C1psMJZ8Bglj","trusted":false},"cell_type":"code","source":"dtrain_pos = xgb.DMatrix(train_pos[X], train_pos[y])\ndtest_pos = xgb.DMatrix(test_pos[X], test_pos[y])\ndtrain_neg = xgb.DMatrix(train_neg[X], train_neg[y])\ndtest_neg = xgb.DMatrix(test_neg[X], test_neg[y])","execution_count":null,"outputs":[]},{"metadata":{"id":"r7MzfvDeBgnh","outputId":"2ae67a25-9ce5-43df-c98c-3fb98589b697","trusted":false},"cell_type":"code","source":"#Negative dataset model training\nbase_y_pos = train_pos[y].mean()\n\n# tuning parameters\nparams = {\n    'objective': 'reg:linear', \n    'bagging_fraction': 0.8768575337571937,\n    'colsample_bytree': 0.9933592930641432,\n    'feature_fraction': 0.816825176108506,\n    'gamma': 0.05587328363633812,\n    'learning_rate': 0.19879098664834996,\n    'max_depth': 6,\n    'min_child_samples': 9,\n    'num_leaves': 7,\n    'reg_alpha': 0.11806338517600543,\n    'reg_lambda': 0.23269341544465222,\n    'subsample': 0.6,\n    'base_score': base_y_pos,                       # calibrate predictions to mean of y \n    'seed': 12345                               # set random seed for reproducibility\n}\n\n# watchlist is used for early stopping\nwatchlist_pos = [(dtrain_pos, 'train'), (dtest_pos, 'eval')]\n\n# train model\nxgb_model_pos = xgb.train(params,                   # set tuning parameters from above                   \n                      dtrain_pos,                   # training data\n                      1000,                     # maximum of 1000 iterations (trees)\n                      evals=watchlist_pos,          # use watchlist for early stopping \n                      early_stopping_rounds=50, # stop after 50 iterations (trees) without increase in rmse \n                      verbose_eval=True)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Negative dataset model training\nbase_y_neg = train_neg[y].mean()\n\n# tuning parameters\nparams = {\n    'objective': 'reg:linear', \n    'bagging_fraction': 0.8768575337571937,\n    'colsample_bytree': 0.9933592930641432,\n    'feature_fraction': 0.816825176108506,\n    'gamma': 0.05587328363633812,\n    'learning_rate': 0.19879098664834996,\n    'max_depth': 6,\n    'min_child_samples': 9,\n    'num_leaves': 7,\n    'reg_alpha': 0.11806338517600543,\n    'reg_lambda': 0.23269341544465222,\n    'subsample': 0.6,\n    'base_score': base_y_neg,                       # calibrate predictions to mean of y \n    'seed': 12345                               # set random seed for reproducibility\n}\n\n# watchlist is used for early stopping\nwatchlist_neg = [(dtrain_neg, 'train'), (dtest_neg, 'eval')]\n\n# train model\nxgb_model_neg = xgb.train(params,                   # set tuning parameters from above                   \n                      dtrain_neg,                   # training data\n                      1000,                     # maximum of 1000 iterations (trees)\n                      evals=watchlist_neg,          # use watchlist for early stopping \n                      early_stopping_rounds=50, # stop after 50 iterations (trees) without increase in rmse \n                      verbose_eval=True)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"NAEC9ctKBgpr","trusted":false},"cell_type":"code","source":"predictions_pos = xgb_model_pos.predict(dtest_pos)\npredictions_neg = xgb_model_neg.predict(dtest_neg)","execution_count":null,"outputs":[]},{"metadata":{"id":"MseDoicyB5mL","outputId":"b2eb507e-a9a2-47d2-a6d1-4bbb82b6b94e","trusted":false},"cell_type":"code","source":"xgb.plot_importance(xgb_model_pos,importance_type='weight')\nxgb.plot_importance(xgb_model_neg,importance_type='weight')","execution_count":null,"outputs":[]},{"metadata":{"id":"aqXzcLwz3iAf","trusted":false},"cell_type":"code","source":"dtest_subsets_pos = xgb.DMatrix(test_subsets_pos[X])\ndtest_subsets_neg = xgb.DMatrix(test_subsets_neg[X])","execution_count":null,"outputs":[]},{"metadata":{"id":"zE0miDZzRQzJ","trusted":false},"cell_type":"code","source":"prediction_test_pos = xgb_model_pos.predict(dtest_subsets_pos)\nprediction_test_neg = xgb_model_neg.predict(dtest_subsets_neg)","execution_count":null,"outputs":[]},{"metadata":{"id":"S6q38Sh-RQ1V","trusted":false},"cell_type":"code","source":"test_subsets_pos['Jaccard'] = prediction_test_pos\ntest_subsets_neg['Jaccard'] = prediction_test_neg","execution_count":null,"outputs":[]},{"metadata":{"id":"0oxMxzP1Rkru","outputId":"e267fb38-2867-4776-c34b-d550b3ac8173","trusted":false},"cell_type":"code","source":"test_subsets_pos.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test_subsets_neg.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"CKPr-nVwRQ3q","trusted":false},"cell_type":"code","source":"test_submission_pos = test_subsets_pos.sort_values('Jaccard', ascending=False).drop_duplicates(['textID'])\ntest_submission_neg = test_subsets_neg.sort_values('Jaccard', ascending=False).drop_duplicates(['textID'])","execution_count":null,"outputs":[]},{"metadata":{"id":"Gn4COyrLfTb9","trusted":false},"cell_type":"code","source":"test_submission_pos = test_submission_pos[['textID','ngram']]\ntest_submission_neg = test_submission_neg[['textID','ngram']]\ntest_submission_pos = test_submission_pos.rename(columns = {'ngram':'selected_text'})\ntest_submission_neg = test_submission_neg.rename(columns = {'ngram':'selected_text'})\ntest_submission = test_submission_pos.append(test_submission_neg)\nneutral_submission = neutral_test[['textID','text']]\nneutral_submission = neutral_submission.rename(columns = {'text':'selected_text'})\nsubmission = test_submission.append(neutral_submission,ignore_index=True)\nsample_submission = sample_submission.drop(columns=['selected_text'])\nsample_submission = pd.merge(sample_submission,submission,on='textID',how='left')","execution_count":null,"outputs":[]},{"metadata":{"id":"QIIHCj67fiV3","trusted":false},"cell_type":"code","source":"sample_submission.to_csv('submission.csv', index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}