{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Libraries","metadata":{}},{"cell_type":"code","source":"# 引入基本資料處理用函式庫\nimport numpy as np\nimport pandas as pd\nimport os\nimport warnings\nimport random\n\n# 引入 Pytorch 函式庫, 神經網路函式庫, Optimizer優化器 Loss function是要幫助我們判斷誤差值的，而Optimizer是要調整參數，來使Loss越小越好。\nimport torch \nfrom torch import nn\nimport torch.optim as optim\n\n# 資料集分割器, 供多重驗證模型使用\nfrom sklearn.model_selection import StratifiedKFold\n\n# 引入單字,單詞分割器\nimport tokenizers\n# 引入主要模型, RoBERTa (Robustly optimized BERT approach)\nfrom transformers import RobertaModel, RobertaConfig\n\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Seed","metadata":{}},{"cell_type":"code","source":"'''\n此區塊主要用於調整所有用到的函式庫使用同一個種子碼，\n確保程式及訓練過程及結果可以重現。確保亂數的值固定\n'''\ndef seed_everything(seed_value):\n    #調整 random, numpy, pytorch, python本體 的種子碼\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    os.environ['PYTHONHASHSEED'] = str(seed_value)\n    \n    # 若有 GPU 版本 Pytorch 可使用\n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = True\n\n# 設定種子碼為 42\nseed = 42\nseed_everything(seed)","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Loader","metadata":{}},{"cell_type":"markdown","source":"關於 `tokenizers.ByteLevelBPETokenizer` 可以參考 [網址](https://github.com/huggingface/tokenizers/blob/master/bindings/python/py_src/tokenizers/implementations/byte_level_bpe.py) <br>\n關於 `encoding.offsets` 可以參考 [網址](https://huggingface.co/docs/tokenizers/python/v0.10.0/api/reference.html?highlight=offsets#tokenizers.Encoding.offsets) <br>\n關於 `encoding.ids` 可以參考 [網址](https://huggingface.co/docs/tokenizers/python/v0.10.0/api/reference.html?highlight=offsets#tokenizers.Encoding.ids) <br>\n關於 `torch.utils.data.DataLoader` 可以參考 [網址](https://pytorch.org/docs/stable/data.html)","metadata":{}},{"cell_type":"code","source":"'''\n此區段為資料讀取使用，\n定義了 TweetDataset 這個 Class 及 Data 讀取相關的操作函式。\n'''\n# TweetDataset 繼承 torch.utils.data.Dataset, 可呼叫其定義的相關物件及函式\nclass TweetDataset(torch.utils.data.Dataset):\n    # 預設 TweetDataset 宣告時要讀入 df 及 max_len，max_len 預設為 96\n    def __init__(self, df, max_len=96):\n        # 賦予 class 內的變數值\n        self.df = df #dataframe\n        self.max_len = max_len\n        # 設定 labeled 為 df 中的 'selected_text'\n        self.labeled = 'selected_text' in df\n        ''' \n        使用 byte level version of the BPE 為語詞分割器，以下定義: 切割字串編碼\n        - vocab_file :轉換為對應的編碼通常频率越高的byte索引越小\n        - merges_file : 輸入的所有tokens轉化为merges.txt中對應的byte\n        - lowercase : 是否將所有文字轉成小寫\n        - add_prefix_space : 是否於第一個文字前加入空白\n        '''\n        self.tokenizer = tokenizers.ByteLevelBPETokenizer(\n            vocab_file='../input/roberta-base/vocab.json', \n            merges_file='../input/roberta-base/merges.txt', \n            lowercase=True,\n            add_prefix_space=True)\n\n    # 賦予此 Class 用 index 取值的能力， e.g. TweetDataset[1]\n    def __getitem__(self, index):\n        # 建立空的 dictionary\n        data = {}\n        # iloc:用index位置來取我們要的資料\n        row = self.df.iloc[index] \n        # 使用 class 函式 get_input_data 根據 index row 取值且放入剛剛的 data dictionary\n        ids, masks, tweet, offsets = self.get_input_data(row)\n        data['ids'] = ids\n        data['masks'] = masks #，由於 padding 會替不等長的句子們補0 ， 這時候利用masks就可以標註出非 0 的區域，也就是讓模型不被 padding 補的 0 影響判斷。\n        data['tweet'] = tweet\n        data['offsets'] = offsets #是一個表示 該單詞於句子的起始位置 結束位置的元組\n        \n        # 若 labeled 不為空集合則執行\n        if self.labeled:\n            # 使用 class 函式 get_target_idx, 額外針對目標取出 start_idx, end_idx \n            start_idx, end_idx = self.get_target_idx(row, tweet, offsets)\n            data['start_idx'] = start_idx\n            data['end_idx'] = end_idx\n            \n        # 回傳 data dictionary\n        return data\n    \n    # 定義針對此 class 呼叫 python 內建函式 len 的時候的回傳值\n    def __len__(self):\n        return len(self.df)\n    \n    # 傳入一列資料，回傳 ids, masks, tweet, offsets 四個變數\n    def get_input_data(self, row): \n        '''\n        在該列的 text 前先加上一個空格，變小寫後根據空字元分割單詞，再以空格連接單詞\n        e.g 'Some User\\tInput' => ['some','user','input'] -> 'some user input'\n        '''\n        tweet = \" \" + \" \".join(row.text.lower().split())\n        \n        '''\n        藉 tokenizer 將 tweet 編碼成 BERT 中所需要的編號，每個編號對應著一個『字』\n        \n        '''\n        encoding = self.tokenizer.encode(tweet)\n        # 這裡也將列資料中的 sentiment 文字編碼\n        sentiment_id = self.tokenizer.encode(row.sentiment).ids\n        # encoding.ids 會回傳\n        ids = [0] + sentiment_id + [2, 2] + encoding.ids + [2] \n        offsets = [(0, 0)] * 4 + encoding.offsets + [(0, 0)] \n        \n        # 確認 text 長度, 若不夠需補長\n        pad_len = self.max_len - len(ids)\n        if pad_len > 0:\n            ids += [1] * pad_len\n            offsets += [(0, 0)] * pad_len\n        \n        # 將 ids 轉為 pytorch 之 tensor\n        ids = torch.tensor(ids)\n        # 若 ids != 1 成立， masks 為 torch.tensor(1) , 否則 torch.tensor(0) #?\n        masks = torch.where(ids != 1, torch.tensor(1), torch.tensor(0))#?\n        # 將 offsets 轉為 pytorch 之 tensor\n        offsets = torch.tensor(offsets) \n        \n        return ids, masks, tweet, offsets\n    \n    '''\n    此資料集的目標是指出該列 Text 能夠判斷語氣的部份, \n    放置於 train 資料集的 selected_text 欄位\n    '''\n    def get_target_idx(self, row, tweet, offsets):\n        # 同上 text 處理方法\n        selected_text = \" \" +  \" \".join(row.selected_text.lower().split())\n        \n        # 取出 selected_text 的長度\n        len_st = len(selected_text) - 1\n        # 建立 text 之 index 用 #?\n        idx0 = None\n        idx1 = None\n        \n\n        # 在 e == selected_text[1] , 也就是與 selected_text 開頭的單詞相同的句子的集合內  enumerate=利用它可以同時獲得索引和值\n        for ind in (i for i, e in enumerate(tweet) if e == selected_text[1]):\n            # 若 \" \" + tweet[ind: ind+len_st] 的組合 和 selected_text 一樣\n            if \" \" + tweet[ind: ind+len_st] == selected_text:\n                # 設定 idx0 為起始點, idx1 為終止點\n                idx0 = ind\n                idx1 = ind + len_st - 1\n                break\n        \n        # 先以 len(tweet) 個 [0] 初始化 char_targets\n        char_targets = [0] * len(tweet)\n        # 若有成功取出 idx0 及 idx1\n        if idx0 != None and idx1 != None:\n            # 將 char_targets 對應 tweet 的 selected_text 位置 (idx0 ~ idx1 的範圍) 設為 1\n            for ct in range(idx0, idx1 + 1):\n                char_targets[ct] = 1\n\n        # 藉 offset 製造 target_idx 做訓練使用\n        target_idx = []\n        for j, (offset1, offset2) in enumerate(offsets):\n            # 若有發現 char_targets 中 範圍 offset1 至 offset2 的和大於 0 (代表有值)，\n            # 則將其 index 放入 target_idx\n            if sum(char_targets[offset1: offset2]) > 0:\n                target_idx.append(j)\n\n        # 起始 idx 為 target_idx 中第一個，終止 idx 則為最後一個\n        start_idx = target_idx[0]\n        end_idx = target_idx[-1]\n        \n        return start_idx, end_idx\n\n'''\n傳入 dataframe, 分割後之 train 及 val 對應的 idx, 及預設為 8 的 batch_size\n回傳有 train 及 val DataLoader 的 dictionary\n'''\ndef get_train_val_loaders(df, train_idx, val_idx, batch_size=8):\n    # 藉 train_idx 及 val_idx 將 dataframe 分割成訓練及驗證 dataframe\n    train_df = df.iloc[train_idx]\n    val_df = df.iloc[val_idx]\n\n    \n    train_loader = torch.utils.data.DataLoader(\n        TweetDataset(train_df), \n        batch_size=batch_size, \n        shuffle=True,  # 打亂排序 \n        num_workers=2, # 以兩個 子行程處理\n        drop_last=True) # 當資料集 batch 無法均分時，捨棄最後一個不完整的 batch\n\n    # 要注意不要打亂排序避免 idx 錯亂\n    val_loader = torch.utils.data.DataLoader(\n        TweetDataset(val_df), \n        batch_size=batch_size, \n        shuffle=False, \n        num_workers=2)\n    \n    # 用 dict 儲存兩個 Loader, 並且加上對應的 Key\n    dataloaders_dict = {\"train\": train_loader, \"val\": val_loader}\n\n    return dataloaders_dict\n\n'''\n傳入 dataframe, 及預設為 32 的 batch_size\n回傳 test 資料集使用的 Loader \n'''\ndef get_test_loader(df, batch_size=8):\n    loader = torch.utils.data.DataLoader(\n        TweetDataset(df), \n        batch_size=batch_size, \n        shuffle=False, # 找出答案用, 所以不打亂順序\n        num_workers=2)  # 以兩個 子行程 處理    \n    return loader","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model\nTransformers 的 RoBERTa 相關可以參考  [網址](https://huggingface.co/transformers/model_doc/roberta.html) <br>\nConfig 可以參考 [roberta-base/config.json](https://huggingface.co/roberta-base/resolve/main/config.json)","metadata":{}},{"cell_type":"code","source":"'''\nModel 基底繼承自 nn.Module神經網路模塊\n'''\nclass TweetModel(nn.Module):\n    def __init__(self):\n        super(TweetModel, self).__init__()\n        \n        # 以 pretrained (PyTorch 提供的預訓練模型) 的 config 初始化 RoBERTa configuration, 也將隱藏層的部分讀入\n        config = RobertaConfig.from_pretrained(\n            '../input/roberta-base/config.json', output_hidden_states=True)\n        # 讀入 pretrained 的 RobertaModel, 且以上面的 config 初始化\n        self.roberta = RobertaModel.from_pretrained(\n            '../input/roberta-base/pytorch_model.bin', config=config)\n        # 設置一個 dropout 層工具，會隨機關閉 50% 的神經元避免過擬合\n        self.dropout = nn.Dropout(0.5)\n        # 建立全連接層工具，此種全連接層傳入 12 個節點(參考 config)，輸出兩個節點\n        self.fc = nn.Linear(config.hidden_size, 2)\n        # 以 標準差為 0.02 之 normal distribution 初始化 fc 之權重\n        nn.init.normal_(self.fc.weight, std=0.02)\n        # 以 均值為 0 之 normal distribution 初始化 fc 之 bias\n        nn.init.normal_(self.fc.bias, 0)\n\n    # 定義向前傳播時的行為，會輸入 指定的 ids 及 attention_mask\n    def forward(self, input_ids, attention_mask):\n        # 用 hs 保留輸入 input_ids, attention_mask 進 roberta 的隱藏層狀態\n        _, _, hs = self.roberta(input_ids, attention_mask)\n        # 沿著維度 0 號 疊起 hidden state:在LSTM 的網路結構中，直接根據當前input 資料，得到的輸出稱為\n        x = torch.stack([hs[-1], hs[-2], hs[-3], hs[-4]])\\\n        # 沿著維度 0 取均值\n        x = torch.mean(x, 0)\n        # 利用上面 init 的 dropout 層工具建立 dropout\n        x = self.dropout(x)\n        # 利用上面 init 的全連接層工具建立 fc\n        x = self.fc(x)\n        # 沿著最後一個維度，一個一組進行分割\n        start_logits, end_logits = x.split(1, dim=-1)\n        # 將兩個結果的最後一個維度去除\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n        \n        # 回傳 logits (語氣句子起始位置及結束位置分布機率)\n        return start_logits, end_logits","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loss Function","metadata":{}},{"cell_type":"code","source":"'''\n建立 Loss Function 供訓練使用，\n基底是 CrossEntropy，但在此必須同時比對開頭位置及結束位置 ，CrossEntropy是在觀測預測的機率分佈與實際機率分布的誤差範圍\n所以程式將兩個的 CrossEntopyLoss 加起來計算。\n'''\ndef loss_fn(start_logits, end_logits, start_positions, end_positions):\n    ce_loss = nn.CrossEntropyLoss()\n    start_loss = ce_loss(start_logits, start_positions)\n    end_loss = ce_loss(end_logits, end_positions)    \n    total_loss = start_loss + end_loss\n    return total_loss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation Function\nJaccard index 可參考 [網址](https://zh.wikipedia.org/wiki/雅卡尔指数)","metadata":{}},{"cell_type":"code","source":"# 藉 start_idx, end_idx, offsets 取出 test 中的 selected_text\ndef get_selected_text(text, start_idx, end_idx, offsets):\n    selected_text = \"\"\n    for ix in range(start_idx, end_idx + 1):\n        # 先取出指定範圍\n        selected_text += text[offsets[ix][0]: offsets[ix][1]]\n        # 確認是否需要加上空白做辨識\n        if (ix + 1) < len(offsets) and offsets[ix][1] < offsets[ix + 1][0]:\n            selected_text += \" \"\n    return selected_text\n\n# 建立 evaluation function - Jaccard index, 又稱Intersection over Union=一種測量在特定資料集中檢測相應物體準確度的一個標準\ndef jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    # 取聯集分之交集\n    return float(len(c)) / (len(a) + len(b) - len(c))\n\n# 計算 jaccard_score\ndef compute_jaccard_score(text, start_idx, end_idx, start_logits, end_logits, offsets):\n    # 取出 機率最大的位置\n    start_pred = np.argmax(start_logits)\n    end_pred = np.argmax(end_logits)\n    \n    # 此區取出預測區段文字，第一個條件判斷出有可能是整句文字的狀況\n    if start_pred > end_pred:\n        pred = text\n    else:\n        pred = get_selected_text(text, start_pred, end_pred, offsets)\n    \n    # 取出正確對應語氣的文字\n    true = get_selected_text(text, start_idx, end_idx, offsets)\n    \n    # 計算 jaccard_score\n    return jaccard(true, pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training Function","metadata":{}},{"cell_type":"code","source":"'''\n訓練模型使用， 引入 Model, 訓練及驗證 dataloader, loss function , optimizer, 訓練回數, 檔案名稱\n最後會儲存訓練後的模型。\n'''\ndef train_model(model, dataloaders_dict, criterion, optimizer, num_epochs, filename):\n    # 使用 GPU\n    model.cuda()\n\n    # 根據訓練回數，每回訓練進行...\n    for epoch in range(num_epochs):\n        # 判斷當前階段\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()\n            else:\n                model.eval()\n                \n            # 預設 loss 及 jaccard 為 0\n            epoch_loss = 0.0\n            epoch_jaccard = 0.0\n            \n            # 取出當前階段(train 或 val) 所使用的資料集，資料若是 torch tensor，在 GPU 訓練要轉成 GPU 使用的 Tesnor\n            for data in (dataloaders_dict[phase]):\n                ids = data['ids'].cuda()\n                masks = data['masks'].cuda()\n                tweet = data['tweet']\n                offsets = data['offsets'].numpy()\n                start_idx = data['start_idx'].cuda()\n                end_idx = data['end_idx'].cuda()\n                \n                # 初始化 optimizer\n                optimizer.zero_grad()\n\n                with torch.set_grad_enabled(phase == 'train'):\n                    \n                    # 輸入 ids, masks 得到 model 輸出\n                    start_logits, end_logits = model(ids, masks)\n                    # 計算 loss\n                    loss = criterion(start_logits, end_logits, start_idx, end_idx)\n                    \n                    # 在訓練階段要反向傳播且讓 optimizer 進行梯度下降\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n                    \n                    # 計算各批訓練 loss 之總和，loss.item() 目的在於將 loss 取出成 python float 形式\n                    epoch_loss += loss.item() * len(ids)\n                    \n                    # 以下步驟目的在於將 tensor 從 gpu 拿回 cpu 並且轉成 numpy array\n                    # .cpu() 用於將 tensor 放回 cpu\n                    # .detach() 用於阻斷反向傳播\n                    # .numpy() 將 tensor 轉為 numpy array\n                    start_idx = start_idx.cpu().detach().numpy()\n                    end_idx = end_idx.cpu().detach().numpy()\n                    start_logits = torch.softmax(start_logits, dim=1).cpu().detach().numpy()\n                    end_logits = torch.softmax(end_logits, dim=1).cpu().detach().numpy()\n                    \n                    # 計算本回的總 jaccard 分數總合\n                    for i in range(len(ids)):                        \n                        jaccard_score = compute_jaccard_score(\n                            tweet[i],\n                            start_idx[i],\n                            end_idx[i],\n                            start_logits[i], \n                            end_logits[i], \n                            offsets[i])\n                        epoch_jaccard += jaccard_score\n            \n            # 平均 loss 及 jaccard\n            epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n            epoch_jaccard = epoch_jaccard / len(dataloaders_dict[phase].dataset)\n            \n            # 印出當前 Loss 及 jaccard\n            print('Epoch {}/{} | {:^5} | Loss: {:.4f} | Jaccard: {:.4f}'.format(\n                epoch + 1, num_epochs, phase, epoch_loss, epoch_jaccard))\n            \n    # 儲存模型\n    torch.save(model.state_dict(), filename)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"# 定義訓練過程中數據將被輪3次\nnum_epochs = 3\n# 每次批量訓練數量為 32\nbatch_size = 8\n# 建立 KFold 多重驗證訓練器，分十種資料集分布且要打亂排序\nskf = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# 讀入訓練用 csv\ntrain_df = pd.read_csv('../input/tweet-sentiment-extraction/train.csv')\n# 將 text 內容轉型為 string\ntrain_df['text'] = train_df['text'].astype(str)\n# 將 selected_text 內容轉型為 string\ntrain_df['selected_text'] = train_df['selected_text'].astype(str)\n\n# 將資料集以十種分布反覆進行訓練及驗證\nfor fold, (train_idx, val_idx) in enumerate(skf.split(train_df, train_df.sentiment), start=1): \n    print(f'Fold: {fold}')\n    # 每種資料集分布都會建立一個新 model\n    model = TweetModel()\n    # 使用 AdamW 為 optimizer, 學習率 3e-5, betas 分別為 0.9 及 0.999\n    optimizer = optim.AdamW(model.parameters(), lr=3e-5, betas=(0.9, 0.999))\n    # 呼叫 loss function\n    criterion = loss_fn\n    # 根據 train_idx 及 val_idx 的不同重新建立 data loader\n    dataloaders_dict = get_train_val_loaders(train_df, train_idx, val_idx, batch_size)\n    \n    # 呼叫模型進行訓練，儲存的 Model 名字為 (f'roberta_fold{fold}.pth')\n    train_model(\n        model, \n        dataloaders_dict,\n        criterion, \n        optimizer, \n        num_epochs,\n        f'roberta_fold{fold}.pth')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"%%time\n\n# 讀入測試(輸出答案)用 csv\ntest_df = pd.read_csv('../input/tweet-sentiment-extraction/test.csv')\n# 將 text 內容轉型為 string\ntest_df['text'] = test_df['text'].astype(str)\n# 取得 test 用 dataloader\ntest_loader = get_test_loader(test_df)\n\n# 初始化\npredictions = []\nmodels = []\n\n# 讀出每個 fold 訓練出的 Model 並且放到 models 中\nfor fold in range(skf.n_splits):\n    model = TweetModel()\n    model.cuda()\n    model.load_state_dict(torch.load(f'roberta_fold{fold+1}.pth'))\n    model.eval()\n    models.append(model)\n\nfor data in test_loader:\n    #資料若是 torch tensor，在 CPU 用要轉成 GPU 使用的 Tesnor\n    ids = data['ids'].cuda()\n    masks = data['masks'].cuda()\n    tweet = data['tweet']\n    offsets = data['offsets'].numpy()\n\n    start_logits = []\n    end_logits = []\n    # 運算出每個 fold 訓練下的輸出結果，並且放回 cpu，阻斷反向傳播，再轉成 numpy array\n    for model in models:\n        with torch.no_grad():\n            output = model(ids, masks)\n            start_logits.append(torch.softmax(output[0], dim=1).cpu().detach().numpy())\n            end_logits.append(torch.softmax(output[1], dim=1).cpu().detach().numpy())\n    # 沿著維度 0 號取平均\n    start_logits = np.mean(start_logits, axis=0)\n    end_logits = np.mean(end_logits, axis=0)\n    for i in range(len(ids)):    \n        start_pred = np.argmax(start_logits[i])\n        end_pred = np.argmax(end_logits[i])\n        # 取出預測區段文字，有可能是整句\n        if start_pred > end_pred:\n            pred = tweet[i]\n        else:\n            pred = get_selected_text(tweet[i], start_pred, end_pred, offsets[i])\n        # 放入 predictions\n        predictions.append(pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"# 讀入 submission 參考格式\nsub_df = pd.read_csv('../input/tweet-sentiment-extraction/sample_submission.csv')\n# 在答案區塊放入預測值\nsub_df['selected_text'] = predictions\n# 將語氣輔助詞縮短\nsub_df['selected_text'] = sub_df['selected_text'].apply(lambda x: x.replace('!!!!', '!') if len(x.split())==1 else x)\nsub_df['selected_text'] = sub_df['selected_text'].apply(lambda x: x.replace('..', '.') if len(x.split())==1 else x)\nsub_df['selected_text'] = sub_df['selected_text'].apply(lambda x: x.replace('...', '.') if len(x.split())==1 else x)\n# 將繳交答案用 dataframe 存成 csv, 不額外建立 index\nsub_df.to_csv('submission.csv', index=False)\n# 檢查用\nsub_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}