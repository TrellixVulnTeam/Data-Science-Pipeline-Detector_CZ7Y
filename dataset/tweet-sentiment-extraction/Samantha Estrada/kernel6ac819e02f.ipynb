{"cells":[{"metadata":{"_uuid":"b2054636-cb57-48fc-b302-f2f2bf5e6c7c","_cell_guid":"6e660bb7-894a-4107-956a-3cec81c635c1","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nimport pandas as pd \nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom nltk.corpus import stopwords\nfrom nltk import pos_tag, ngrams\nfrom nltk.corpus import sentiwordnet as swn, wordnet as wn\nfrom nltk.tokenize import word_tokenize\nimport nltk\nimport re\nnltk.download('stopwords')\n\nimport string\n\n\n# CountVectorizer will help calculate word counts\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Import the string dictionary that we'll use to remove punctuation\nimport string\n\n\n# Import datasets\ntrain = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv')\ntest = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv')\nsample = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/sample_submission.csv')\n# The row with index 13133 has NaN text, so remove it from the dataset\ntrain[train['text'].isna()]\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\ntrain.drop(314, inplace = True)\ntrain['text'] = train['text'].apply(lambda x: x.lower())\ntest['text'] = test['text'].apply(lambda x: x.lower())\n\n\nX_train, X_val = train_test_split(\n    train, train_size = 0.90, random_state = 0)\n\npos_train = X_train[X_train['sentiment'] == 'positive']\nneutral_train = X_train[X_train['sentiment'] == 'neutral']\nneg_train = X_train[X_train['sentiment'] == 'negative']\n\n#Include some text cleaning as sourced from https://www.kaggle.com/behcetsenturk/data-augmentation-thesaurus-synonyms-w-cleaning\ndef strip_links(text):\n    text = str(text)\n    line = re.findall(r'[\\w\\.-]+@[\\w\\.-]+(?<=#)\\w+[0-9]+',str(text))\n    for l in line:\n        text = text.replace(link[0], '')\n    return text\n\ntd = {\n    \"u\":\"you\",\n    \"ur\":\"you are\",\n    \"n\":\"and\",\n    \"aww\":\"cute\",\n    \"sooo\":\"so\",\n    \"r\":\"are\",\n    \"cuz\":\"because\",\n    \"til\":\"till\",\n    \"lil\":\",little\",\n    \"b\":\"be\",\n    \"ppl\":\"people\",\n    \"yay\":\"cheer\",\n    \"nite\":\"night\",\n    \"lmao\":\"haha\",\n    \"tho\":\"though\",\n    \"btw\":\"by the way\",\n    \"yr\":\"year\",\n    \"dm\":\"message\",\n    \"idk\":\"i do not know\",\n    \"outta\":\"out of\",\n    \"jus\":\"just\",\n    \"thru\":\"through\",\n    \"wtf\":\"what the fuck\",\n    \"wit\":\"with\",\n    \"gettin\":\"getting\",\n    \"dnt\":\"dont\",\n    \"mum\":\"mom\",\n    \"mums\":\"moms\",\n    \"hun\":\"honey\",\n    \"luv\":\"love\",\n    \"hrs\":\"hours\",\n    \"chillin\":\"chilling\",\n    \"abt\":\"about\",\n    \"tha\":\"that\",\n    \"ahh\":\"ah\",\n    \"feelin\":\"feeling\",\n\n    \"tho.\":\"though\",\n    \"w/\":\"with\",\n    \"u?\":\"you?\",\n    \"s\":\"is\",\n\n    \":O\":\"suprised\",\n    \":p\":\"lol\",\n    \"(:\":\":)\",\n    \":S\":\":(\"\n}\n\ndef cleaning_function(string):\n    # Take tweet clean and return\n    \n    cleaned_words = []\n    for word in string.split():\n        word = td.get(word, word)\n        cleaned_words.append(word)\n    \n    return \" \".join(cleaned_words)\n\n#clean up text\nfull_stops = stopwords.words('english')\ncv = CountVectorizer(max_df=0.95, min_df=2,max_features=3000,stop_words=stopwords.words('english'))\n\nX_train_cv = cv.fit_transform(X_train['text'])\nX_train_cv = strip_links(X_train_cv)\nX_train_cv = cleaning_function(X_train_cv)\n\nX_pos = cv.transform(pos_train['text'])\nX_neutral = cv.transform(neutral_train['text'])\nX_neg = cv.transform(neg_train['text'])\n\npos_count_df = pd.DataFrame(X_pos.toarray(), columns=cv.get_feature_names())\nneutral_count_df = pd.DataFrame(X_neutral.toarray(), columns=cv.get_feature_names())\nneg_count_df = pd.DataFrame(X_neg.toarray(), columns=cv.get_feature_names())\n\n# Create dictionaries of the words within each sentiment group, where the values are the proportions of tweets that \n# contain those words\n\npos_words = {}\nneutral_words = {}\nneg_words = {}\n\nfor k in cv.get_feature_names():\n    pos = pos_count_df[k].sum()\n    neutral = neutral_count_df[k].sum()\n    neg = neg_count_df[k].sum()\n    \n    pos_words[k] = pos/pos_train.shape[0]\n    neutral_words[k] = neutral/neutral_train.shape[0]\n    neg_words[k] = neg/neg_train.shape[0]\n    \n# We need to account for the fact that there will be a lot of words used in tweets of every sentiment.  \n# Therefore, we reassign the values in the dictionary by subtracting the proportion of tweets in the other \n# sentiments that use that word.\n\nneg_words_adj = {}\npos_words_adj = {}\nneutral_words_adj = {}\ni=0\nneg_sum = []\npos_sum = []\nneut_sum = []\nfor key, value in neg_words.items():\n    i += 1\n    if(neutral_words[key] == 0 and pos_words[key] == 0):\n#         print(\"Key: \", key, \" -- \", value, \"neutral: \", neutral_words[key], \"// pos: \", pos_words[key], \"// neg:\", neg_words[key])\n        neg_sum.append(neg_words[key])\n        neg_words_adj[key] = (np.sum(neg_sum))/i\n    else:\n        neg_words_adj[key] = neg_words[key] - (neutral_words[key] + pos_words[key])\n\nfor key, value in pos_words.items():\n#     print(\"Pos key: \",key, \" -- \", value)\n    if(neutral_words[key] == 0 and neg_words[key] == 0):\n        pos_sum.append(pos_words[key])\n        pos_words_adj[key] = (np.sum(pos_sum))/i\n    else:\n        pos_words_adj[key] = pos_words[key] - (neutral_words[key] + neg_words[key])\n\n\nfor key, value in neutral_words.items():\n    if(pos_words[key] == 0 and neg_words[key] == 0):\n        neut_sum.append(neutral_words[key])\n        neutral_words_adj[key] = (np.sum(neut_sum))/i\n    else:\n        neutral_words_adj[key] = neutral_words[key] - (neg_words[key] + pos_words[key])\n    \ndef calculate_selected_text(df_row, tol = 0):\n    \n    tweet = df_row['text']\n    sentiment = df_row['sentiment']\n    \n    if(sentiment == 'neutral'):\n        return tweet\n    \n    elif(sentiment == 'positive'):\n        dict_to_use = pos_words_adj # Calculate word weights using the pos_words dictionary\n    elif(sentiment == 'negative'):\n        dict_to_use = neg_words_adj # Calculate word weights using the neg_words dictionary\n        \n    words = tweet.split()\n    words_len = len(words)\n    subsets = [words[i:j+1] for i in range(words_len) for j in range(i,words_len)]\n    \n    scores = 0\n    selection_str = '' # This will be our choice\n    lst = sorted(subsets, key = len) # Sort candidates by length\n    \n    for i in range(len(subsets)):\n        \n        new_sum = 0 # Sum for the current substring\n#         # Calculate the sum of weights for each word in the substring\n        for p in range(len(lst[i])):\n            if(lst[i][p].translate(str.maketrans('','',string.punctuation)) in dict_to_use.keys()):\n                new_sum += dict_to_use[lst[i][p].translate(str.maketrans('','',string.punctuation))]\n\n        # If the sum is greater than the score, update our current selection\n        if(new_sum > 0):\n#             print(\"scores before, after: \", score, \" // \", new_sum)\n            scores = new_sum\n            selection_str = lst[i]\n#             tol = tol*5 # Increase the tolerance a bit each time we choose a selection\n\n    # If we didn't find good substrings, return the whole text\n    if(len(selection_str) == 0):\n        selection_str = words\n        \n    return ' '.join(selection_str)\n\npd.options.mode.chained_assignment = None\n\ntol = 0.001\n\nX_val['predicted_selection'] = ''\n\nfor index, row in X_val.iterrows():\n    \n    selected_text = calculate_selected_text(row, tol)\n    \n    X_val.loc[X_val['textID'] == row['textID'], ['predicted_selection']] = selected_text\n\n\ndef jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))\n\nX_val['jaccard'] = X_val.apply(lambda x: jaccard(x['selected_text'], x['predicted_selection']), axis = 1)\n\nprint('The jaccard score for the validation set is:', np.mean(X_val['jaccard']))\n\npos_tr = train[train['sentiment'] == 'positive']\nneutral_tr = train[train['sentiment'] == 'neutral']\nneg_tr = train[train['sentiment'] == 'negative']\ncv = CountVectorizer(max_df=0.95, min_df=2,\n                                     max_features=10000,\n                                     stop_words='english')\n\nfinal_cv = cv.fit_transform(train['text'])\n\nX_pos = cv.transform(pos_tr['text'])\nX_neutral = cv.transform(neutral_tr['text'])\nX_neg = cv.transform(neg_tr['text'])\n\npos_final_count_df = pd.DataFrame(X_pos.toarray(), columns=cv.get_feature_names())\nneutral_final_count_df = pd.DataFrame(X_neutral.toarray(), columns=cv.get_feature_names())\nneg_final_count_df = pd.DataFrame(X_neg.toarray(), columns=cv.get_feature_names())\npos_words = {}\nneutral_words = {}\nneg_words = {}\n\nfor k in cv.get_feature_names():\n    pos = pos_final_count_df[k].sum()\n    neutral = neutral_final_count_df[k].sum()\n    neg = neg_final_count_df[k].sum()\n    \n    pos_words[k] = pos/(pos_tr.shape[0])\n    neutral_words[k] = neutral/(neutral_tr.shape[0])\n    neg_words[k] = neg/(neg_tr.shape[0])\nneg_words_adj = {}\npos_words_adj = {}\nneutral_words_adj = {}\n\nfor key, value in neg_words.items():\n    neg_words_adj[key] = neg_words[key] - (neutral_words[key] + pos_words[key])\n    \nfor key, value in pos_words.items():\n    pos_words_adj[key] = pos_words[key] - (neutral_words[key] + neg_words[key])\n    \nfor key, value in neutral_words.items():\n    neutral_words_adj[key] = neutral_words[key] - (neg_words[key] + pos_words[key])\n\ntol = 0.001\n\nfor index, row in test.iterrows():\n    \n    selected_text = calculate_selected_text(row, tol)\n    \n    sample.loc[sample['textID'] == row['textID'], ['selected_text']] = selected_text\nsample.to_csv('submission.csv', index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}