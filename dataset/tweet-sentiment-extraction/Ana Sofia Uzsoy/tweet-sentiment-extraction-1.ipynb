{"cells":[{"metadata":{},"cell_type":"markdown","source":"This notebook is for tweet sentiment analysis- we are given the text of the tweet, as well as the sentiment, and are asked to generate the part of the tweet that embodies that sentiment.","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/tweet-sentiment-extraction/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(data.text), len(data.textID))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Clean text","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for idx, value in enumerate(data.text): ## remove hyperlinks\n    words = str(value).split()\n    words = [x for x in words if not x.startswith(\"http\")]\n    data[\"text\"][idx] = \" \".join(words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\n\ndef clean_text(dataset, field):\n    for index, strin in enumerate(dataset[field]):\n        if not strin:\n            strin = strin.lower()\n            strin = strin.replace(\"'\", \"\")\n            strin = strin.replace(\"\\n\", \"\")\n            strin = strin.strip()\n            strin = strin.replace('[{}]'.format(string.punctuation), '')\n            dataset[field][index] = strin\n\n\nclean_text(data, 'text')\nclean_text(data, 'selected_text')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(data.text), len(data.textID))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data[pd.notnull(data.selected_text)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data.text[data.textID == \"a88287bbda\"])\nprint(len(data.text), len(data.textID))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Separate into training and validation sets","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain, validation = train_test_split(data, test_size = 0.25)\nprint(len(data), len(train), len(validation))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"validation.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.text[train.textID == \"a88287bbda\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tokenize and create vocabulary","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_size = 10000\nembedding_dim = 16\nmax_length = 50\ntrunc_type='post'\npad_type='post'\noov_tok = \"<OOV>\"\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\ntokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\ntokenizer.fit_on_texts(train.text)\nword_index = tokenizer.word_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_sequences = tokenizer.texts_to_sequences(np.array(train.text))\ntraining_padded = pad_sequences(training_sequences,truncating=trunc_type, padding=pad_type)\n\nmax_length = len(training_padded[0])\n\nvalidation_sequences = tokenizer.texts_to_sequences(np.array(validation.text))\nvalidation_padded = pad_sequences(validation_sequences, padding=pad_type, maxlen = max_length)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prepare data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The padded sequences are going to be the inputs. The output will be an array of the same length, but with ones at the indexes of the words that we keep (that embody the sentiment), and zeros for the rest of the words.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"training_selected_sequences = tokenizer.texts_to_sequences(np.array(train.selected_text))\nvalidation_selected_sequences = tokenizer.texts_to_sequences(np.array(validation.selected_text))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_list(padded, sequence):\n    return np.array([1 if x in sequence else 0 for x in padded])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So for this tweet:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"training_padded[4]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These words are important:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"training_selected_sequences[4]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So the output array will look like this:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"get_list(training_padded[4], training_selected_sequences[4])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_y = np.array([get_list(i,j) for i,j in zip(training_padded, training_selected_sequences)])\nvalidate_y = np.array([get_list(i,j) for i,j in zip(validation_padded, validation_selected_sequences)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.array(train.sentiment).shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_padded.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To get the phrases back from the predicted arrays:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rev_word_index = {v: k for k, v in word_index.items()}\ndef get_phrase(array_x, array_y, index): \n    return np.array([rev_word_index[i] for i in array_x[index][array_y.astype(bool)[index]]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.text.values[4])\nprint(str(get_phrase(training_padded, train_y, 4)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x = np.copy(training_padded)\nvalidate_x = np.copy(validation_padded)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Dropout","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('dark_background')\n\ndef plot_graphs(history, string):\n  plt.plot(history.history[string])\n  plt.plot(history.history['val_'+string])\n  plt.xlabel(\"Epochs\")\n  plt.ylabel(string)\n  plt.legend([string, 'val_'+string])\n  plt.show()\n  \n# plot_graphs(history, \"accuracy\")\n# plot_graphs(history, \"loss\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"I was having some trouble figuring out how to incorporate the sentiment and the input text into the model, but I noticed that for neutral tweets, in almost every case the selected text is just the whole tweet. So, I decided to just go with that, and make two separate models, one for negative and one for positive tweets.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#!pip install -U keras-tuner","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_padded = np.array(training_padded)\nvalidation_padded = np.array(validation_padded)\ntrain_y = np.array(train_y)\nvalidate_y = np.array(validate_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_padded","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(np.array(training_padded)[(train.sentiment == \"positive\")].shape)\nprint(training_padded[(train.sentiment == \"neutral\")].shape)\ntraining_padded[(train.sentiment == \"negative\")].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_positive_x = training_padded[(train.sentiment == \"positive\")]\ntrain_neutral_x = training_padded[(train.sentiment == \"neutral\")]\ntrain_negative_x = training_padded[(train.sentiment == \"negative\")]\ntrain_positive_y = train_y[(train.sentiment == \"positive\")]\ntrain_neutral_y = train_y[(train.sentiment == \"neutral\")]\ntrain_negative_y = train_y[(train.sentiment == \"negative\")]\n\nvalidate_positive_x = validation_padded[(validation.sentiment == \"positive\")]\nvalidate_neutral_x = validation_padded[(validation.sentiment == \"neutral\")]\nvalidate_negative_x = validation_padded[(validation.sentiment == \"negative\")]\nvalidate_positive_y = validate_y[(validation.sentiment == \"positive\")]\nvalidate_neutral_y = validate_y[(validation.sentiment == \"neutral\")]\nvalidate_negative_y = validate_y[(validation.sentiment == \"negative\")]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tune and Train models","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"I read at this link: (https://medium.com/huggingface/multi-label-text-classification-using-bert-the-mighty-transformer-69714fa3fb3d) that this is the correct loss function to use for multilabel classification, which I think is what I'm doing here.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install -U keras-tuner","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import kerastuner\ndef build_model(hp):\n    model = Sequential()\n    model.add(Embedding(vocab_size, hp.Int('units', min_value = 5, max_value = 200, step = 25), input_length=max_length))\n    model.add(Dropout(0.5))\n    model.add(Bidirectional(LSTM(20)))\n    model.add(Dropout(0.5))\n    model.add(Dense(max_length, activation='softmax'))\n    model.compile(loss=loss, optimizer=tf.keras.optimizers.Adam(hp.Choice('learning_rate',\n                      values=[1e-2, 1e-3, 1e-4])), metrics=['accuracy'])\n    return model\n\ntuner = kerastuner.tuners.RandomSearch(\n    build_model,\n    objective='val_accuracy',\n    max_trials=5,\n    executions_per_trial=3)\n\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tuner.search(train_positive_x, train_positive_y, epochs = 40,verbose = 2,validation_data = (validate_positive_x, validate_positive_y), callbacks=[tf.keras.callbacks.EarlyStopping('val_loss', patience=6)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"positive_model = tuner.get_best_models()[0]\npositive_model.compile(loss=l, optimizer=tf.keras.optimizers.Adam(0.001), metrics=['accuracy'])\npositive_history = positive_model.fit(np.array(train_positive_x), np.array(train_positive_y), epochs=60, verbose=2,\n                    validation_data = (np.array(validate_positive_x), np.array(validate_positive_y)),callbacks=[tf.keras.callbacks.EarlyStopping('val_loss', patience=6)] )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# positive_model = Sequential()\n# positive_model.add(Embedding(vocab_size, 16, input_length=max_length))\n# positive_model.add(Dropout(0.5))\n# positive_model.add(Bidirectional(LSTM(20)))\n# positive_model.add(Dropout(0.5))\n# positive_model.add(Dense(max_length, activation='softmax'))\n# positive_model.compile(loss=l, optimizer=tf.keras.optimizers.Adam(0.001), metrics=['accuracy'])\n# positive_history = positive_model.fit(np.array(train_positive_x), np.array(train_positive_y), epochs=60, verbose=2,\n#                    validation_data = (np.array(validate_positive_x), np.array(validate_positive_y)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_graphs(positive_history, \"accuracy\")\nplot_graphs(positive_history, \"loss\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tuner.search(train_negative_x, train_negative_y, epochs = 40,verbose = 2,validation_data = (validate_negative_x, validate_negative_y), callbacks=[tf.keras.callbacks.EarlyStopping('val_loss', patience=6)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tuner.results_summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"negative_model = tuner.get_best_models()[0]\nnegative_model.compile(loss=l, optimizer=tf.keras.optimizers.Adam(0.001), metrics=['accuracy'])\nnegative_history = negative_model.fit(train_negative_x, train_negative_y, epochs=100, verbose=2,\n                   validation_data = (validate_negative_x, validate_negative_y), callbacks=[tf.keras.callbacks.EarlyStopping('val_loss', patience=6)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# negative_model = Sequential()\n# negative_model.add(Embedding(vocab_size, 16, input_length=max_length))\n# negative_model.add(Dropout(0.5))\n# negative_model.add(Bidirectional(LSTM(20)))\n# negative_model.add(Dropout(0.5))\n# negative_model.add(Dense(max_length, activation='softmax'))\n# negative_model.compile(loss=l, optimizer=tf.keras.optimizers.Adam(0.001), metrics=['accuracy'])\n# negative_history = negative_model.fit(train_negative_x, train_negative_y, epochs=60, verbose=2,\n#                    validation_data = (validate_negative_x, validate_negative_y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_graphs(negative_history, \"accuracy\")\nplot_graphs(negative_history, \"loss\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_neg_preds = [np.round(negative_model.predict(item[np.newaxis])) for item in validate_negative_x]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predicting for test set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv(\"../input/tweet-sentiment-extraction/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for idx, value in enumerate(test.text):\n    words = str(value).split()\n    words = [x for x in words if not x.startswith(\"http\")]\n    test[\"text\"][idx] = \" \".join(words)\ntest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_text(test, 'text')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_sequences = tokenizer.texts_to_sequences(np.array(test.text))\ntest_padded = pad_sequences(test_sequences,truncating=trunc_type, maxlen = max_length,padding=pad_type)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_phrase(array_x, array_y, index): \n    return np.array([rev_word_index[i] for i in array_x[index][array_y.astype(bool)[index]]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = []\nfor index, item in enumerate(test_padded):\n    if test.sentiment[index] == \"positive\":\n        p = np.round(positive_model.predict(item[np.newaxis]))\n        preds.append(p)\n    elif test.sentiment[index] == \"negative\":\n        p = np.round(negative_model.predict(item[np.newaxis]))\n        preds.append(p)\n    else:\n        #p = np.round(neutral_model.predict(item[np.newaxis]))\n        preds.append(test_padded[index].astype(bool).astype(int)[np.newaxis])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_phrase(array_x, array_y, index): \n    return np.array([rev_word_index[i] for i in array_x[index][array_y.astype(bool)[index][0]]if i != 0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test[\"prediction\"] = np.zeros(len(test))\n\nfor index, item in enumerate(preds):\n    test['prediction'][index] = str(get_phrase(test_padded, np.array(preds), index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.prediction[test.sentiment == \"neutral\"] = test.text[test.sentiment == \"neutral\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.prediction = test.prediction.str.replace(\"[\", \"\")\ntest.prediction = test.prediction.str.replace(\"]\", \"\")\ntest.prediction = test.prediction.str.replace(\"'\", \"\")\ntest.prediction = test.prediction.str.replace(\"<OOV>\", \"\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.prediction[(test.prediction) == ''] = test.text[(test.prediction) == '']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluation = test.textID.copy().to_frame()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluation['selected_text'] = test['prediction']\nevaluation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluation.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}