{"cells":[{"metadata":{"id":"ckehZhX67D21"},"cell_type":"markdown","source":"# **Twitter Sentiment Analysis**\n\nWith all of the tweets circulating every second it is hard to tell whether the sentiment behind a specific tweet will impact a company, or a person's, brand for being viral (positive), or devastate profit because it strikes a negative tone. Capturing sentiment in language is important in these times where decisions and reactions are created and updated in seconds.In this project I'll predict the words which actually lead to the sentiment description. "},{"metadata":{"id":"Pd59AOKE7JfR"},"cell_type":"markdown","source":"**Dataset:**\n\nEach row contains the text of a tweet and a sentiment label. In the training set you are provided with a word or phrase drawn from the tweet (selected_text) that encapsulates the provided sentiment."},{"metadata":{"id":"6JzzBjzT7NTA"},"cell_type":"markdown","source":"**Target:**\n\nTo predict the word or phrase from the tweet that exemplifies the provided sentiment. The word or phrase should include all characters within that span (i.e. including commas, spaces, etc.)"},{"metadata":{"id":"JYBWVr_N7Phe"},"cell_type":"markdown","source":"**Approach:**\nExploratory Data Analysis by Generating Meta Features and calculating Jaccard Similarity scores. Modeling using Named Entity Recognition(NER)\n\n"},{"metadata":{"id":"3yLDyV-BR-dN"},"cell_type":"markdown","source":"# **Importing Necessary Libraries and Modules**"},{"metadata":{"id":"w_h16Q0EA2tg","trusted":true},"cell_type":"code","source":"import re\nimport string\nimport numpy as np\nimport random\nimport pandas as pd\n\n#For data visualisation\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport plotly.offline\nimport plotly.graph_objects as go\nfrom plotly import graph_objs as go\nimport plotly.express as px\nimport plotly.figure_factory as ff\n\nfrom collections import Counter \n\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\n#For NLP\nimport nltk\nfrom nltk.corpus import stopwords\n\nfrom tqdm import tqdm\nimport os\nimport nltk\nimport spacy\nimport random\nfrom spacy.util import compounding\nfrom spacy.util import minibatch","execution_count":null,"outputs":[]},{"metadata":{"id":"o_yzZbfZesmZ"},"cell_type":"markdown","source":"**Creating helper Function**"},{"metadata":{"id":"Zg-vCUWte78t","trusted":true},"cell_type":"code","source":"#helper Function to generate random colors used to give different colors to plots.\ndef random_colours(number_of_colors):\n  colors=[]\n  for i in range(number_of_colors):\n    colors.append(\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)]))\n  return colors\n","execution_count":null,"outputs":[]},{"metadata":{"id":"BaAZWGSEeyDA"},"cell_type":"markdown","source":"# **Loading the Data**"},{"metadata":{"id":"sUAZSmoXgpJb","trusted":true},"cell_type":"code","source":"train=pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv')\ntest=pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv')\nss=pd.read_csv('/kaggle/input/tweet-sentiment-extraction/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"id":"RzkCV3WOeSg4","outputId":"60ce3bfc-655f-4be0-b02e-8af288dd351f","trusted":true},"cell_type":"code","source":"print(train.shape)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{"id":"LXbuRxgFIKDt"},"cell_type":"markdown","source":"**Filling missing values**"},{"metadata":{"id":"FIkV5h5eaFfq","outputId":"1be75c56-2aea-4fa8-868b-f40dee4d89d6","trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"id":"9JkcteATHSHO"},"cell_type":"markdown","source":"we have one null value in the train, as the text field for value is NAN."},{"metadata":{"id":"ZHqHd9_rHBiQ","trusted":true},"cell_type":"code","source":"#dropping row with null value\ntrain.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"mbFMtEfaHmW6","outputId":"f64a810f-b601-4e18-c3e3-4d4ceecbe68f","trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"id":"qNMEKbQwHrdr","outputId":"936c6c25-c800-4507-f469-0630aac4d150","trusted":true},"cell_type":"code","source":"test.info()","execution_count":null,"outputs":[]},{"metadata":{"id":"iA_fVt2U9aUQ"},"cell_type":"markdown","source":"test set contains no null value"},{"metadata":{"id":"jh9FCIwjIOuo"},"cell_type":"markdown","source":"# **Exploratory Data Analysis**"},{"metadata":{"id":"VWE-mGSrIGzU","outputId":"5081b1b3-49d5-4035-fdfb-8f1f06ec42cf","trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"uJQJnhc1IWgh","outputId":"7dddeab5-3ee7-48f9-db44-394eca7425ce","trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"id":"Uzkm1SrNdEK2"},"cell_type":"markdown","source":"**Analysing distribution of tweets**"},{"metadata":{"id":"YJkDHuuoIvl0","outputId":"31561500-3229-4ac1-fcec-e196c4e576b6","trusted":true},"cell_type":"code","source":"temp=train.groupby('sentiment').count()['text'].reset_index().sort_values(by='text',ascending=False)\ntemp.style.background_gradient(cmap='Purples')","execution_count":null,"outputs":[]},{"metadata":{"id":"fuYCzXibLFup","outputId":"06b6dcbc-4aaa-46dd-fefb-492645c1596d","trusted":true},"cell_type":"code","source":"#Funnel Chart for sentiments\nfig=go.Figure(go.Funnelarea(\n    text=temp.sentiment,\n    values=temp.text,\n    title={\"position\":\"top center\",\"text\":\"Funnel=Chart of Sentiment Distribution\"}\n))\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"bTJZwFTc_qOr"},"cell_type":"markdown","source":"Here, it's clear that we have more neutral tweets with almost equal no of negative and positive tweets"},{"metadata":{"id":"PNlGJvfveS-W"},"cell_type":"markdown","source":"# **Generating Meta-Features**"},{"metadata":{"id":"YdlVKbRaFMp3"},"cell_type":"markdown","source":"**Jaccard Similarity Scores between text and Selected_text**"},{"metadata":{"id":"Q0TZCD5Zjlr9","trusted":true},"cell_type":"code","source":"def jaccard(str1,str2):\n  a=set(str1.lower().split())\n  b=set(str2.lower().split())\n  c=a.intersection(b)\n  return(float(len(c))/(len(a) + len(b) - len(c)))","execution_count":null,"outputs":[]},{"metadata":{"id":"r_JzvzIP9yBs","trusted":true},"cell_type":"code","source":"results_jaccard=[]\nfor ind,row in train.iterrows():\n  sentence1=row.text\n  sentence2=row.selected_text\n  jaccard_score=jaccard(sentence1, sentence2)\n  results_jaccard.append([sentence1,sentence2,jaccard_score])","execution_count":null,"outputs":[]},{"metadata":{"id":"MIY_ijEI_GXW","trusted":true},"cell_type":"code","source":"jaccard=pd.DataFrame(results_jaccard,columns=[\"text\",\"selected_text\",\"jaccard_score\"])\ntrain=train.merge(jaccard,how='outer')","execution_count":null,"outputs":[]},{"metadata":{"id":"kV1v_RzlFUtb"},"cell_type":"markdown","source":"**Difference In Number of words in Selected_text and Text**"},{"metadata":{"id":"V-5pPTjtALO8","trusted":true},"cell_type":"code","source":"train['Num_words_ST']=train['selected_text'].apply(lambda x:len(str(x).split()))\ntrain['Num_words_text']=train['text'].apply(lambda x:len(str(x).split()))\ntrain['difference_in_words']=train['Num_words_text']-train['Num_words_ST']","execution_count":null,"outputs":[]},{"metadata":{"id":"OEy27mT0_pex","outputId":"9a66d030-e43a-4897-c3a3-91c4aa1d4dcf","trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"wnmiI45g1oJF"},"cell_type":"markdown","source":"# **Distribution of Meta Features**"},{"metadata":{"id":"H6Xa18JuFhlb"},"cell_type":"markdown","source":"**Distribution of Number of words**"},{"metadata":{"id":"aa84m6Nk_-Dr","outputId":"05c99359-fd5a-4673-98a2-61aaa2d01415","trusted":true},"cell_type":"code","source":"hist_data = [train['Num_words_ST'],train['Num_words_text']]\n\ngroup_labels = ['Selected_Text', 'Text']\n\n# Create distplot with custom bin_size\nfig = ff.create_distplot(hist_data, group_labels,show_curve=False)\nfig.update_layout(title_text='Distribution of Number Of words')\nfig.update_layout(\n    autosize=False,\n    width=900,\n    height=700,\n    paper_bgcolor=\"LightSteelBlue\",\n)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"CyWi5f72MiBN","outputId":"0677899e-db3d-4671-e8b1-6a89839e8e5f","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,6))\np1=sns.kdeplot(train['Num_words_ST'], shade=True, color=\"r\").set_title('Kernel Distributuion of Number of words')\np1=sns.kdeplot(train['Num_words_text'], shade=True, color=\"b\")","execution_count":null,"outputs":[]},{"metadata":{"id":"F8LE1TAqe9qg"},"cell_type":"markdown","source":"**Difference in number of words and jaccard_scores across different Sentiments**"},{"metadata":{"id":"3uQSsotzPWcl","outputId":"5a150375-2c75-4167-a944-cc3a5cc1de72","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,6))\np1=sns.kdeplot(train[train['sentiment']=='positive']['difference_in_words'], shade=True, color=\"b\").set_title('Kernel Distribution of Difference in no of words for positive and negative sentiments')\np2=sns.kdeplot(train[train['sentiment']=='negative']['difference_in_words'], shade=True, color=\"r\")","execution_count":null,"outputs":[]},{"metadata":{"id":"rq-kg9Qlfecm","outputId":"334e0b7c-64e1-4e81-9ec4-33b67f5dc87a","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,6))\nsns.distplot(train[train['sentiment']=='neutral']['difference_in_words'],kde=False).set_title('Kernel Distribution of Difference in no of words for neutral sentiments')","execution_count":null,"outputs":[]},{"metadata":{"id":"L-kdlb0D28Tg"},"cell_type":"markdown","source":"**Jaccered Scores across different Sentiments**"},{"metadata":{"id":"DZuS1vnwmR1h","outputId":"34f931d4-1c50-4da4-adeb-2d4f77559488","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,6))\np1=sns.kdeplot(train[train['sentiment']=='positive']['jaccard_score'], shade=True, color=\"b\").set_title('KDE of jaccard Scores across different Sentiments')\np2=sns.kdeplot(train[train['sentiment']=='negative']['jaccard_score'],shade=True, color=\"r\")\nplt.legend(labels=['positive','negative'])\n","execution_count":null,"outputs":[]},{"metadata":{"id":"ozV2HS6PLfhY","outputId":"69eeb8db-788c-4185-d322-47057b2e9e8e","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,5))\nsns.distplot(train[train['sentiment']=='neutral']['jaccard_score'],kde=False)","execution_count":null,"outputs":[]},{"metadata":{"id":"BsS7-SRp3o6A"},"cell_type":"markdown","source":"We can see some interesting trends here:\n\n*   Positive and negative tweets have high kurtosis and thus values are concentrated in two regions narrow and high density\n*   Neutral tweets have a low kurtosis value and their is bump in density near values of 1\n\n"},{"metadata":{"id":"NmTGbLLd4pRA","trusted":true},"cell_type":"code","source":"#Kurtosis is the measure of how peaked a distribution is and how much spread it is around that peak\n#Skewness measures how much a curve deviates from a normal distribution","execution_count":null,"outputs":[]},{"metadata":{"id":"pw4n4o604iSH"},"cell_type":"markdown","source":"# **Conclusion Of EDA**"},{"metadata":{"id":"I40M_1U7M2sO","outputId":"24c0fd87-c029-4ce4-d915-9b7993333b80","trusted":true},"cell_type":"code","source":"k=train[train['Num_words_text']<=2]\nk.groupby('sentiment').mean()['jaccard_score']","execution_count":null,"outputs":[]},{"metadata":{"id":"fYpG6k4OTchK","outputId":"f1d8e4b2-7bb3-4967-d7e6-7def25e84b30","trusted":true},"cell_type":"code","source":"k[k['sentiment']=='positive']","execution_count":null,"outputs":[]},{"metadata":{"id":"8Fn4p1xLlbf-"},"cell_type":"markdown","source":"# **Cleaning Data**"},{"metadata":{"id":"ppd7QQ6uT-LB","trusted":true},"cell_type":"code","source":"def clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = str(text).lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text","execution_count":null,"outputs":[]},{"metadata":{"id":"hGmygmSi-aQE","trusted":true},"cell_type":"code","source":"train['text']=train['text'].apply(lambda x:clean_text(x))\ntrain['selected_text']=train['selected_text'].apply(lambda x:clean_text(x))\n","execution_count":null,"outputs":[]},{"metadata":{"id":"GHN-aOww_hA0","outputId":"68f86625-1dce-4c51-dbf5-9d3c015b59fe","trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"Da2r-pyj_mel"},"cell_type":"markdown","source":"**Most Common words in our Target-Selected Text**"},{"metadata":{"id":"aliOkIgi_izF","outputId":"8a4ff177-2f20-4ce1-d616-690236768d4b","trusted":true},"cell_type":"code","source":"train['temp_list']=train['selected_text'].apply(lambda x:str(x).split())\ntop=Counter([item for sublist in train['temp_list'] for item in sublist])\ntemp=pd.DataFrame(top.most_common(20))\ntemp.columns=['Common_words','count']\ntemp.style.background_gradient(cmap='Blues')","execution_count":null,"outputs":[]},{"metadata":{"id":"d3FT8C4qOxya","outputId":"d309c15a-8342-4377-e21b-f600509b9260","trusted":true},"cell_type":"code","source":"fig=px.bar(temp, x=\"count\", y=\"Common_words\",title='Most Common Words in Selected Text', orientation='h',width=700, height=700, color='Common_words')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"6Al-Af5c6hG_","outputId":"20277237-0bb2-4a79-93ea-1d70316bd98a","trusted":true},"cell_type":"code","source":"nltk.download('stopwords')","execution_count":null,"outputs":[]},{"metadata":{"id":"9qTbY-LR68Tm"},"cell_type":"markdown","source":"**removing stopwords**"},{"metadata":{"id":"poT4KuNLPvIl","trusted":true},"cell_type":"code","source":"def remove_stopword(x):\n  return [y for y in x if y not in stopwords.words('english')]\ntrain['temp_list']=train['temp_list'].apply(lambda x:remove_stopword(x))","execution_count":null,"outputs":[]},{"metadata":{"id":"0Zwc3sOiTQ1Z","outputId":"a1554604-53ed-4015-e91e-58079a234d5c","trusted":true},"cell_type":"code","source":"top=Counter([item for sublist in train['temp_list'] for item in sublist])\ntemp=pd.DataFrame(top.most_common(20))\ntemp=temp.iloc[1:,:]\ntemp.columns=['Common_words','count']\ntemp.style.background_gradient(cmap='Purples')","execution_count":null,"outputs":[]},{"metadata":{"id":"8y8xcJ3yU0Sk"},"cell_type":"markdown","source":"**Tree of Most Common Words**"},{"metadata":{"id":"UmOTblk4UoPd","outputId":"9e10f87e-87aa-4c3b-dc88-2d6c3f0ea5af","trusted":true},"cell_type":"code","source":"fig = px.treemap(temp, path=['Common_words'], values='count',title='Tree of Most Common Words')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"HZK6EJ448Ba6"},"cell_type":"markdown","source":"**Most Common words in Text**"},{"metadata":{"id":"iuCpPnOGa_fz","trusted":true},"cell_type":"code","source":"train['temp_list1'] = train['text'].apply(lambda x:str(x).split()) #List of words in every row for text\ntrain['temp_list1'] = train['temp_list1'].apply(lambda x:remove_stopword(x)) #Removing Stopwords","execution_count":null,"outputs":[]},{"metadata":{"id":"D9zQipk9b3iA","outputId":"181d6a4a-5bc3-49f7-a45e-dec41f924b65","trusted":true},"cell_type":"code","source":"top = Counter([item for sublist in train['temp_list1'] for item in sublist])\ntemp = pd.DataFrame(top.most_common(25))\ntemp = temp.iloc[1:,:]\ntemp.columns = ['Common_words','count']\ntemp.style.background_gradient(cmap='Blues')","execution_count":null,"outputs":[]},{"metadata":{"id":"zToobTlzb9xf","outputId":"1f9e7b5f-092c-4891-ffcc-5f0542152d2f","trusted":true},"cell_type":"code","source":"fig = px.bar(temp, x=\"count\", y=\"Common_words\", title='Commmon Words in Text', orientation='h', \n             width=700, height=700,color='Common_words')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"rwcaP5IGeKBn"},"cell_type":"markdown","source":"**Most common words SentimentsWise**"},{"metadata":{"id":"KEZL7dfQcFCt","trusted":true},"cell_type":"code","source":"Positive_sent=train[train['sentiment']=='positive']\nNegative_sent=train[train['sentiment']=='negative']\nNeutral_sent=train[train['sentiment']=='neutral']","execution_count":null,"outputs":[]},{"metadata":{"id":"R6v_VeaKeljq","outputId":"fb55f9e7-84f2-4604-e60b-ada8e14b3f81","trusted":true},"cell_type":"code","source":"#MosT common positive words\ntop = Counter([item for sublist in Positive_sent['temp_list'] for item in sublist])\ntemp_positive = pd.DataFrame(top.most_common(20))\ntemp_positive.columns = ['Common_words','count']\ntemp_positive.style.background_gradient(cmap='Greens')","execution_count":null,"outputs":[]},{"metadata":{"id":"d37EYzqzewQj","outputId":"574bf81a-bdbb-4c63-f684-7cbad59a5242","trusted":true},"cell_type":"code","source":"fig = px.bar(temp_positive, x=\"count\", y=\"Common_words\", title='Most Commmon Positive Words', orientation='h', \n             width=700, height=700,color='Common_words')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"88hxP2BYe2dv","outputId":"b10a2e80-8344-4a96-c867-300fb5a4157c","trusted":true},"cell_type":"code","source":"#MosT common negative words\ntop = Counter([item for sublist in Negative_sent['temp_list'] for item in sublist])\ntemp_negative = pd.DataFrame(top.most_common(20))\ntemp_negative = temp_negative.iloc[1:,:]\ntemp_negative.columns = ['Common_words','count']\ntemp_negative.style.background_gradient(cmap='Reds')","execution_count":null,"outputs":[]},{"metadata":{"id":"1v2sZyVbe-Wr","outputId":"067a3b1a-1edd-4e4a-dec6-991743667b02","trusted":true},"cell_type":"code","source":"fig = px.treemap(temp_negative, path=['Common_words'], values='count',title='Tree Of Most Common Negative Words')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"u8yjZuGVfBjv","outputId":"337242da-59dc-4e8f-c51d-5cb28b943028","trusted":true},"cell_type":"code","source":"#MosT common Neutral words\ntop = Counter([item for sublist in Neutral_sent['temp_list'] for item in sublist])\ntemp_neutral = pd.DataFrame(top.most_common(20))\ntemp_neutral = temp_neutral.loc[1:,:]\ntemp_neutral.columns = ['Common_words','count']\ntemp_neutral.style.background_gradient(cmap='Reds')","execution_count":null,"outputs":[]},{"metadata":{"id":"ZT7Sj6lsfFlB","outputId":"6559ead4-6da9-4799-9023-568686483b3d","trusted":true},"cell_type":"code","source":"fig = px.bar(temp_neutral, x=\"count\", y=\"Common_words\", title='Most Commmon Neutral Words', orientation='h', \n             width=700, height=700,color='Common_words')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"4Su9J_I2fJzW","outputId":"70fdc507-d213-4311-82d9-bbb4ab8f7df0","trusted":true},"cell_type":"code","source":"fig = px.treemap(temp_neutral, path=['Common_words'], values='count',title='Tree Of Most Common Neutral Words')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"X29cC6Cy80wU"},"cell_type":"markdown","source":"1. We can see words like get,go,dont,got,u,cant,lol,like are common in all three segments . That's interesting because words like dont and cant are more of negative nature and words like lol are more of positive nature.Does this mean our data is incorrectly labelled , we will have more insights on this after N-gram analysis\n\n2. It will be interesting to see the word unique to different sentiments\n\n\n"},{"metadata":{"id":"87p9FvO1h8CD"},"cell_type":"markdown","source":"**Unique words in each Segment**"},{"metadata":{"id":"45VdqHELfPXs","trusted":true},"cell_type":"code","source":"raw_text=[word for word_list in train['temp_list1'] for word in word_list]","execution_count":null,"outputs":[]},{"metadata":{"id":"sp9Vn75D9J7h","trusted":true},"cell_type":"code","source":"def words_unique(sentiment,numwords,raw_words):\n    '''\n    Input:\n        segment - Segment category (ex. 'Neutral');\n        numwords - how many specific words do you want to see in the final result; \n        raw_words - list  for item in train_data[train_data.segments == segments]['temp_list1']:\n    Output: \n        dataframe giving information about the name of the specific ingredient and how many times it occurs in the chosen cuisine (in descending order based on their counts)..\n\n    '''\n    allother = []\n    for item in train[train.sentiment != sentiment]['temp_list1']:\n        for word in item:\n            allother .append(word)\n    allother  = list(set(allother ))\n    \n    specificnonly = [x for x in raw_text if x not in allother]\n    \n    mycounter = Counter()\n    \n    for item in train[train.sentiment == sentiment]['temp_list1']:\n        for word in item:\n            mycounter[word] += 1\n    keep = list(specificnonly)\n    \n    for word in list(mycounter):\n        if word not in keep:\n            del mycounter[word]\n    \n    Unique_words = pd.DataFrame(mycounter.most_common(numwords), columns = ['words','count'])\n    \n    return Unique_words","execution_count":null,"outputs":[]},{"metadata":{"id":"FaaLX3g--ACX"},"cell_type":"markdown","source":"**Positive Tweets**"},{"metadata":{"id":"_uwO1x_z9phv","outputId":"88537adc-e7fc-4368-9326-5c9f3edae731","trusted":true},"cell_type":"code","source":"Unique_Positive=words_unique('positive',20,raw_text)\nprint(\"The top 20 unique words in Positive Tweets are:\")\nUnique_Positive.style.background_gradient(cmap='Greens')","execution_count":null,"outputs":[]},{"metadata":{"id":"m96TelxN-wBN","outputId":"d870c826-e6b5-4afa-c50c-82a880fca8d3","trusted":true},"cell_type":"code","source":"fig=px.treemap(Unique_Positive,path=['words'],values='count',title='Tree Of Unique Positive Words')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"g3lv3xK-_0Fv"},"cell_type":"markdown","source":"**Negative Tweets**"},{"metadata":{"id":"0FGwOdll_bJS","outputId":"b374eec2-6559-464f-9b08-40502bde55ee","trusted":true},"cell_type":"code","source":"Unique_Negative= words_unique('negative', 10, raw_text)\nprint(\"The top 10 unique words in Negative Tweets are:\")\nUnique_Negative.style.background_gradient(cmap='Reds')","execution_count":null,"outputs":[]},{"metadata":{"id":"hvidYrsn_7j0","outputId":"f17c8e4a-4945-4963-da39-2db83ebe9211","trusted":true},"cell_type":"code","source":"from palettable.colorbrewer.qualitative import Pastel1_7\nplt.figure(figsize=(16,10))\nmy_circle=plt.Circle((0,0), 0.7, color='white')\nplt.rcParams['text.color'] = 'black'\nplt.pie(Unique_Negative['count'], labels=Unique_Negative.words, colors=Pastel1_7.hex_colors)\np=plt.gcf()\np.gca().add_artist(my_circle)\nplt.title('DoNut Plot Of Unique Negative Words')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"sosf7gn4AKZI","outputId":"92840359-aebb-40ff-a5b5-f7c40b70e562","trusted":true},"cell_type":"code","source":"Unique_Neutral= words_unique('neutral', 10, raw_text)\nprint(\"The top 10 unique words in Neutral Tweets are:\")\nUnique_Neutral.style.background_gradient(cmap='Oranges')","execution_count":null,"outputs":[]},{"metadata":{"id":"FrtDQCwXAZtI"},"cell_type":"markdown","source":"**WordClouds**"},{"metadata":{"id":"z-VGVVFIAN2v","trusted":true},"cell_type":"code","source":"def plot_wordcloud(text, mask=None, max_words=200, max_font_size=100, figure_size=(24.0,16.0), color = 'white',\n                   title = None, title_size=40, image_color=False):\n    stopwords = set(STOPWORDS)\n    more_stopwords = {'u', \"im\"}\n    stopwords = stopwords.union(more_stopwords)\n\n    wordcloud = WordCloud(background_color=color,\n                    stopwords = stopwords,\n                    max_words = max_words,\n                    max_font_size = max_font_size, \n                    random_state = 42,\n                    width=400, \n                    height=200,\n                    mask = mask)\n    wordcloud.generate(str(text))\n    \n    plt.figure(figsize=figure_size)\n    if image_color:\n        image_colors = ImageColorGenerator(mask);\n        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n        plt.title(title, fontdict={'size': title_size,  \n                                  'verticalalignment': 'bottom'})\n    else:\n        plt.imshow(wordcloud);\n        plt.title(title, fontdict={'size': title_size, 'color': 'black', \n                                  'verticalalignment': 'bottom'})\n    plt.axis('off');\n    plt.tight_layout()  \nd = '/kaggle/input/masks/masks-wordclouds/'","execution_count":null,"outputs":[]},{"metadata":{"id":"H2cy8KzIEhSn"},"cell_type":"markdown","source":"**Neutral Tweets**"},{"metadata":{"id":"fdAEQq_uAlge","outputId":"c385c82b-e76e-4c4f-abb8-ab2af16b5ebe","trusted":true},"cell_type":"code","source":"pos_mask = np.array(Image.open(d+ 'star.png'))\nplot_wordcloud(Neutral_sent.text,mask=pos_mask,color='white',max_font_size=100,title_size=30,title=\"WordCloud of Neutral Tweets\")","execution_count":null,"outputs":[]},{"metadata":{"id":"GDgdStChErfJ"},"cell_type":"markdown","source":"**Positive Tweets**"},{"metadata":{"id":"W1EgcYMsDl4m","outputId":"79f29595-128a-4ddd-dcdd-0584722d5bc0","trusted":true},"cell_type":"code","source":"plot_wordcloud(Positive_sent.text,mask=pos_mask,title=\"Word Cloud Of Positive tweets\",title_size=30)","execution_count":null,"outputs":[]},{"metadata":{"id":"yq8nnqBqE1-U"},"cell_type":"markdown","source":"**Negative Tweets**"},{"metadata":{"id":"PgeaJu5TEskb","outputId":"13bd75db-c5bf-432c-b7ec-ffde15beb9c3","trusted":true},"cell_type":"code","source":"plot_wordcloud(Negative_sent.text,mask=pos_mask,title=\"Word Cloud of Negative Tweets\",color='white',title_size=30)","execution_count":null,"outputs":[]},{"metadata":{"id":"fsW0znZAGuE4"},"cell_type":"markdown","source":"# **Modelling**"},{"metadata":{"id":"50F-i0AnRduz"},"cell_type":"markdown","source":"**Modelling the problem as NER**"},{"metadata":{"id":"GqvDn1L7S1FK"},"cell_type":"markdown","source":"Named Entity Recognition (NER) is a standard NLP problem which involves spotting named entities (people, places, organizations etc.) from a chunk of text, and classifying them into a predefined set of categories."},{"metadata":{"id":"MvJfi_wUTCAF"},"cell_type":"markdown","source":"We will be using spacy for creating our own customised NER model or models (seperate for each Sentiment)."},{"metadata":{"id":"rRvnutT_T-4r"},"cell_type":"markdown","source":"approach:\n1. using text as selected_text fro all neutral tweets due to their high jaccared similarity.\n2. using text as selected_text for all tweets having number of words less than 3 in text.\n3. training two different models for Positive and Negtive tweets.\n4. no preprocessing of data because the selected text contains raw text"},{"metadata":{"id":"qjgO3O5TE6dK","trusted":true},"cell_type":"code","source":"df_train=pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv')\ndf_test=pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv')\ndf_submission=pd.read_csv('/kaggle/input/tweet-sentiment-extraction/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"id":"eOQ4KbpxT85h","trusted":true},"cell_type":"code","source":"#Number Of words in main Text in train set\ndf_train['Num_words_text']=df_train['text'].apply(lambda x:len(str(x).split()))","execution_count":null,"outputs":[]},{"metadata":{"id":"UEmSYe3jVL_M","trusted":true},"cell_type":"code","source":"df_train=df_train[df_train['Num_words_text']>=3]","execution_count":null,"outputs":[]},{"metadata":{"id":"kqZwbppBb-vF"},"cell_type":"markdown","source":"**creating necessary functions**"},{"metadata":{"id":"V_JcTqbeVVDa","trusted":true},"cell_type":"code","source":"def save_model(output_dir, nlp, new_model_name):\n\n\n  ''' This Function Saves model to \n    given output directory'''\n\n  output_dir=f'./'\n  if output_dir is not None:\n    if not os.path.exists(output_dir):\n      os.makedirs(output_dir)\n    nlp.meta[\"name\"]=new_model_name\n    nlp.to_disk(output_dir)\n    print(\"Saved model to\",output_dir)  \n","execution_count":null,"outputs":[]},{"metadata":{"id":"hi2kI8JhX-xY","trusted":true},"cell_type":"code","source":"# pass model = nlp if you want to train on top of existing model \n\ndef train(train_data, output_dir, n_iter=20, model=None):\n    \"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\n    \"\"\n    if model is not None:\n        nlp = spacy.load(output_dir)  # load existing spaCy model\n        print(\"Loaded model '%s'\" % model)\n    else:\n        nlp = spacy.blank(\"en\")  # create blank Language class\n        print(\"Created blank 'en' model\")\n    \n    # create the built-in pipeline components and add them to the pipeline\n    # nlp.create_pipe works for built-ins that are registered with spaCy\n    if \"ner\" not in nlp.pipe_names:\n        ner = nlp.create_pipe(\"ner\")\n        nlp.add_pipe(ner, last=True)\n    # otherwise, get it so we can add labels\n    else:\n        ner = nlp.get_pipe(\"ner\")\n    \n    # add labels\n    for _, annotations in train_data:\n        for ent in annotations.get(\"entities\"):\n            ner.add_label(ent[2])\n\n    # get names of other pipes to disable them during training\n    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n    with nlp.disable_pipes(*other_pipes):  # only train NER\n        # sizes = compounding(1.0, 4.0, 1.001)\n        # batch up the examples using spaCy's minibatch\n        if model is None:\n            nlp.begin_training()\n        else:\n            nlp.resume_training()\n\n\n        for itn in tqdm(range(n_iter)):\n            random.shuffle(train_data)\n            batches = minibatch(train_data, size=compounding(4.0, 500.0, 1.001))    \n            losses = {}\n            for batch in batches:\n                texts, annotations = zip(*batch)\n                nlp.update(texts,  # batch of texts\n                            annotations,  # batch of annotations\n                            drop=0.5,   # dropout - make it harder to memorise data\n                            losses=losses, \n                            )\n            print(\"Losses\", losses)\n    save_model(output_dir, nlp, 'st_ner')\n","execution_count":null,"outputs":[]},{"metadata":{"id":"cm7Q0DRrY4TY","trusted":true},"cell_type":"code","source":"def get_model_out_path(sentiment):\n    '''\n    Returns Model output path\n    '''\n    model_out_path = None\n    if sentiment == 'positive':\n        model_out_path = './'\n    elif sentiment == 'negative':\n        model_out_path = './'\n    return model_out_path","execution_count":null,"outputs":[]},{"metadata":{"id":"NJczftoBb4Rp","trusted":true},"cell_type":"code","source":"def get_training_data(sentiment):\n    '''\n    Returns Trainong data in the format needed to train spacy NER\n    '''\n    train_data = []\n    for index, row in df_train.iterrows():\n        if row.sentiment == sentiment:\n            selected_text = row.selected_text\n            text = row.text\n            start = text.find(selected_text)\n            end = start + len(selected_text)\n            train_data.append((text, {\"entities\": [[start, end, 'selected_text']]}))\n    return train_data","execution_count":null,"outputs":[]},{"metadata":{"id":"ze-JJX-rkAD3"},"cell_type":"markdown","source":"**Training models for Positive and Negative tweets**"},{"metadata":{"id":"mCnwVLuEj3fO","outputId":"c72a54b9-7b2a-4343-f111-e827ef01d3c0","trusted":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"sentiment='positive'\ntrain_data=get_training_data(sentiment)\nmodel_path=get_model_out_path(sentiment)\ntrain(train_data, model_path, n_iter=3, model=None)","execution_count":null,"outputs":[]},{"metadata":{"id":"0I-yGj5hkmnb","outputId":"2999fb5f-2207-43a5-fb0f-ac938b03c5fe","trusted":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"sentiment='negative'\ntrain_data=get_training_data(sentiment)\nmodel_path=get_model_out_path(sentiment)\ntrain(train_data, model_path, n_iter=3, model=None)","execution_count":null,"outputs":[]},{"metadata":{"id":"sRnowGgeCxoF"},"cell_type":"markdown","source":"**Predicting with the trained Model**"},{"metadata":{"id":"Qg7OBgJxmHLa","trusted":true},"cell_type":"code","source":"def predict_entities(text, model):\n    doc = model(text)\n    ent_array = []\n    for ent in doc.ents:\n        start = text.find(ent.text)\n        end = start + len(ent.text)\n        new_int = [start, end, ent.label_]\n        if new_int not in ent_array:\n            ent_array.append([start, end, ent.label_])\n    selected_text = text[ent_array[0][0]: ent_array[0][1]] if len(ent_array) > 0 else text\n    return selected_text","execution_count":null,"outputs":[]},{"metadata":{"id":"K1dbxdEaDV0z","outputId":"661410a7-5c43-475e-da04-314c2c57ef2b","trusted":true},"cell_type":"code","source":"selected_texts = []\nMODELS_BASE_PATH = '../input/tse-spacy-model/models/'\n\nif MODELS_BASE_PATH is not None:\n    print(\"Loading Models  from \", MODELS_BASE_PATH)\n    model_pos = spacy.load(MODELS_BASE_PATH + 'model_pos')\n    model_neg = spacy.load(MODELS_BASE_PATH + 'model_neg')\n        \n    for index, row in df_test.iterrows():\n        text = row.text\n        output_str = \"\"\n        if row.sentiment == 'neutral' or len(text.split()) <= 2:\n            selected_texts.append(text)\n        elif row.sentiment == 'positive':\n            selected_texts.append(predict_entities(text, model_pos))\n        else:\n            selected_texts.append(predict_entities(text, model_neg))\n        \ndf_test['selected_text'] = selected_texts","execution_count":null,"outputs":[]},{"metadata":{"id":"uHd0ZyFqDbLs","outputId":"24c17ec6-5b7b-4503-9ade-0fee922a668a","trusted":true},"cell_type":"code","source":"df_submission['selected_text']=df_test['selected_text']\ndf_submission.to_csv(\"submission.csv\", index=False)\ndisplay(df_submission.head(10))","execution_count":null,"outputs":[]},{"metadata":{"id":"8MkodWWmRmc8"},"cell_type":"markdown","source":"Here we have the predicted word or phrase from the tweet that exemplifies the provided sentiment. The word or phrase includes all characters within that span (i.e. including commas, spaces, etc.). "},{"metadata":{},"cell_type":"markdown","source":"# **Please upvote my work if it could help! Thank you!**"},{"metadata":{"id":"0RntLXCHKnwo","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}