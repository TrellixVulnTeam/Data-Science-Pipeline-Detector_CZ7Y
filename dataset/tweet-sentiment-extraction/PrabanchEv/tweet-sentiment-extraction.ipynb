{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n# from keras.preprocessing.text import Tokenizer\n# from keras.preprocessing.sequence import pad_sequences\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n\nimport seaborn as sns\n# import the metrics class\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_auc_score,roc_curve,auc\n\nimport matplotlib.pyplot as plt\n\nimport numpy as np\nimport os\n\npd.set_option('display.max_colwidth', -1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import nltk\n# nltk.download('stopwords')\n# nltk.download('wordnet')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_PATH = '/kaggle/input/tweet-sentiment-extraction/'\ntrain = pd.read_csv(DATA_PATH+'train.csv')\ntest = pd.read_csv(DATA_PATH+'test.csv')\nsubmission_df = pd.read_csv(DATA_PATH+'sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings; warnings.simplefilter('ignore')\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\nfrom nltk.stem import WordNetLemmatizer\ntrain['text'] = train['text'].apply(lambda x: \" \".join(x.lower() for x in str(x).split()))\ntrain['text'] = train['text'].str.replace('[^\\w\\s]','')\nstop = stopwords.words('english')\nstop = stop + ['information','page','number','please']\n\n\ntrain['text'] = train['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\ntrain['text'] = train['text'].str.replace('\\d+', '')\n\nstemmer = SnowballStemmer('english')\ntrain['text'].apply(lambda x: \" \".join([stemmer.stem(word) for word in x.split()]))\n\nwordnet_lemmatizer = WordNetLemmatizer()\ntrain['text'] = train['text'].apply(lambda x: \" \".join([wordnet_lemmatizer.lemmatize(word) for word in x.split()]))\n\n\n# test\ntest['text'] = test['text'].apply(lambda x: \" \".join(x.lower() for x in str(x).split()))\ntest['text'] = test['text'].str.replace('[^\\w\\s]','')\nstop = stopwords.words('english')\nstop = stop + ['information','page','number','please']\n\n\ntest['text'] = test['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\ntest['text'] = test['text'].str.replace('\\d+', '')\n\nstemmer = SnowballStemmer('english')\ntest['text'].apply(lambda x: \" \".join([stemmer.stem(word) for word in x.split()]))\n\nwordnet_lemmatizer = WordNetLemmatizer()\ntest['text'] = test['text'].apply(lambda x: \" \".join([wordnet_lemmatizer.lemmatize(word) for word in x.split()]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from wordcloud import WordCloud, STOPWORDS\n# stopwords = set(STOPWORDS)\n\n\n# wordcloud = WordCloud(\n#                           background_color='white',\n#                           stopwords=stopwords,\n#                           max_words=200,\n#                           max_font_size=40, \n#                           random_state=42\n#                          ).generate(str(data[data.error==1]['text']))\n\n# print(wordcloud)\n# fig = plt.figure(1)\n# plt.imshow(wordcloud)\n# plt.axis('off')\n# plt.show()\n# fig.savefig(\"word1.png\", dpi=900)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from wordcloud import WordCloud, STOPWORDS\n# stopwords = set(STOPWORDS)\n\n\n# wordcloud = WordCloud(\n#                           background_color='white',\n#                           stopwords=stopwords,\n#                           max_words=200,\n#                           max_font_size=40, \n#                           random_state=42\n#                          ).generate(str(data[data.error==2]['text']))\n\n# print(wordcloud)\n# fig = plt.figure(1)\n# plt.imshow(wordcloud)\n# plt.axis('off')\n# plt.show()\n# fig.savefig(\"word1.png\", dpi=900)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# documents=[[word for word in document.text.split()] for document in data['text'].tolist()]\n# dictionary = corpora.Dictionary(documents)\n# n_items = len(dictionary)\n# corpus = [dictionary.doc2bow(text) for text in documents]\n# tfidf = models.TfidfModel(corpus)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting the dataset into the Training set and Test set\nfeatures = ['text']\ntarget = ['sentiment']\n\nX = train[features]\ny = train[target]\nfrom sklearn.model_selection import train_test_split\nX_train, val_X, train_y, val_y = train_test_split(X, y, test_size = 0.2, stratify=y, random_state = 0)\nprint(\"train test split done\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.externals import joblib\n# from components.classifier.text_utils import preprocess\nfrom gensim.corpora import Dictionary\nfrom gensim.models import TfidfModel\n\n## fill up the missing values\n## fill up the missing values\ntrain_X = X_train[\"text\"].fillna(\"_na_\")\nval_X = val_X[\"text\"].fillna(\"_na_\")\n\ntest_X = test[\"text\"].fillna(\"_na_\")\n\n\n# ## some config values \n# # # Get the tfidf vectors #\ntfidf_vec = TfidfVectorizer(stop_words='english', ngram_range=(1,3))\ntfidf_vec.fit_transform(train_X.values.tolist()+val_X.values.tolist()+test_X.values.tolist())\ntrain_tfidf = tfidf_vec.transform(train_X.values.tolist())\nval_tfidf = tfidf_vec.transform(val_X.values.tolist())\ntest_tfidf = tfidf_vec.transform(test_X.values.tolist())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\n\ndef plot_cm(y_true, y_pred, names_list, figsize=(10,10)):\n\n    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n    cm_sum = np.sum(cm, axis=1, keepdims=True)\n    cm_perc = cm / cm_sum.astype(float) * 100\n    annot = np.empty_like(cm).astype(str)\n    nrows, ncols = cm.shape\n\n    for i in range(nrows): \n        for j in range(ncols):\n            c = cm[i, j]\n            p = cm_perc[i, j]\n            if i == j:\n                s = cm_sum[i]\n                annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n            elif c == 0:\n                annot[i, j] = ''\n            else:\n                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n\n    cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n    cm.index.name = 'Actual'\n    cm.columns.name = 'Predicted'\n    sns.set(font_scale=1.25)\n    fig, ax = plt.subplots(figsize=figsize)\n    sns.heatmap(cm, cmap= \"Blues\", annot=annot, fmt='', ax=ax)\n\ndef get_metrics(validation_label,predicted_label,names_list):\n    cm = confusion_matrix(validation_label,predicted_label)\n    print(\"Accuracy:\",metrics.accuracy_score(validation_label,predicted_label))\n#     names_list = list(data['document_page_type'].unique())\n    print(metrics.classification_report(validation_label,predicted_label, target_names=names_list))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"names_list = list(train['sentiment'].unique())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# from sklearn.svm import LinearSVC\n# # model =  LinearSVC(C=3)\n# from sklearn.naive_bayes import MultinomialNB\n# from sklearn.externals import joblib\n# from sklearn.model_selection import GridSearchCV\n\n\n# from sklearn.model_selection import GridSearchCV\n# from sklearn import svm\n\n# model = svm.SVC(kernel='linear', probability=True)\n# grid_param = {\n#     'C': [1],\n#     'gamma': [0.001,0.1]\n# }\n\n# gd_sr = GridSearchCV(estimator=model,\n#                      param_grid=grid_param,\n#                      scoring='accuracy',\n#                      cv=2,\n#                      n_jobs=-1)\n\n# gd_sr.fit(train_tfidf, train_y)\n\n# y_pred_val_svm = gd_sr.predict(val_tfidf)\n\n# get_metrics(val_y, y_pred_val_svm, names_list)\n# plot_cm(val_y, y_pred_val_svm, names_list)\n# print(gd_sr.best_params_)\n# # plt.savefig(path + 'Fax Cover BInary-Apri-poc- SVM.png', dpi=300)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# y_pred_val_svm = gd_sr.predict_proba(val_tfidf)\n# y_pred_val_svm\n# get_metrics(val_y, y_pred_val_svm, names_list)\n# plot_cm(val_y, y_pred_val_svm, names_list)\n# print(gd_sr.best_params_)\n# pd.Series(y_pred_val_svm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.naive_bayes import MultinomialNB\nmodel = MultinomialNB(alpha=.01)\n\ngrid_param = {\n    'alpha': [.001]\n}\n\ngd_sr = GridSearchCV(estimator=model,\n                     param_grid=grid_param,\n                     scoring='accuracy',\n                     cv=2,\n                     n_jobs=-1)\n\ngd_sr.fit(train_tfidf, train_y)\n\ny_pred_val_nb = gd_sr.predict(val_tfidf)\n\nget_metrics(val_y, y_pred_val_nb, names_list)\nplot_cm(val_y, y_pred_val_nb, names_list)\nprint(gd_sr.best_params_)\n# plt.savefig(path + 'Fax Cover BInary-Apri-poc- NB.png', dpi=300)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":4}