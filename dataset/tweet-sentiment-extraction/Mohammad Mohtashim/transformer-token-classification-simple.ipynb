{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# !pip install --upgrade transformers \n# !pip install tokenizers==0.9.4","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# the OG \nimport numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom matplotlib import style\n\n\n# Sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import *\nfrom sklearn.utils.class_weight import compute_class_weight\n\n#NLP Specifics\nfrom nltk import word_tokenize\nimport nltk\nfrom tokenizers import BertWordPieceTokenizer\nfrom transformers import BertModel, BertConfig,TFBertForTokenClassification\nfrom transformers import (AdamW,get_linear_schedule_with_warmup,get_cosine_schedule_with_warmup,RobertaTokenizerFast,DistilBertTokenizerFast,PreTrainedTokenizerFast,AutoModel,\n                          DistilBertTokenizerFast,GPT2TokenizerFast,AutoTokenizer,BertTokenizer,TFBertModel,TFOpenAIGPTModel,OpenAIGPTTokenizer,DistilBertTokenizer, TFDistilBertModel,XLMTokenizer, TFXLMModel,TFBertForSequenceClassification,TFGPT2Model,TFXLMRobertaModel)\n\n#tensorflow\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.initializers import Constant\n\n#other imports\nimport urllib\nimport os\nimport gc\nfrom tqdm import tqdm\nimport re\nimport random\nfrom typing import Callable, List, Optional, Union","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train=pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv')\ndf_test=pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Some Bugs in Data-Needed Manual Filtering-if possible\ndf_train.iloc[18,2]='gonna'\nprint(df_train.iloc[32,2])\nprint(df_train.iloc[32,1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def max_text_len():\n    return max(df_train['text'].map(lambda x:len(x)))\nprint('The max len in train-set is : {}'.format(max_text_len()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Configuration\nBATCH_SIZE = 64\nEPOCHS=100\nLEARNING_RATE=1e-7\nBETA_1=0.9\nBETA_2=0.999\nAUTO = tf.data.experimental.AUTOTUNE\nsteps_per_epoch=df_train.shape[0]//BATCH_SIZE\nEARLY_STOP=tf.keras.callbacks.EarlyStopping(patience=10)\nMAX_LEN=150\nLOSS=tf.keras.losses.CategoricalCrossentropy()\nOPTIMIZER=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE,beta_1=BETA_1,beta_2=BETA_2)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class LossHistory(tf.keras.callbacks.Callback):\n    def on_train_begin(self, logs={}):\n        self.losses = []\n    def on_batch_end(self, batch, logs={}):\n        self.losses.append(logs.get('loss'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class preprocessing_data(object):\n    def __init__(self,tokenizer,MAXLEN=MAX_LEN,ATTENTION_MASK=True,PADDING=True):\n        self.tokenizer=tokenizer.from_pretrained('bert-base-uncased')\n        self.tokenizer._pad_token='[PAD]'\n        self.maxlen=MAXLEN\n        self.padding=PADDING\n        self.attention_mask=ATTENTION_MASK\n    def encode_text(self,text,sentiment):\n        if self.maxlen:\n            tokenize=self.tokenizer.encode_plus(text=text,text_pair=sentiment,max_length=self.maxlen,padding='max_length',return_attention_mask=self.attention_mask,\n                                               return_token_type_ids=True)\n        else:\n            tokenize=self.tokenizer.encode_plus(text=text,text_pair=sentiment,max_length=self.maxlen,padding=self.padding,return_attention_mask=self.attention_mask,\n                                               return_token_type_ids=True)\n        input_ids=tokenize['input_ids']\n        attention_mask=tokenize['attention_mask']\n        token_type_ids=tokenize['token_type_ids']\n\n        return (input_ids,attention_mask,token_type_ids)    \n    \n    def encode_labels(self,selected_text):\n        input_ids=self.tokenizer.encode_plus(text=selected_text)\n        return input_ids['input_ids']\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_dataset(data,tokenizer,train=True):\n    input_ids=[]\n    attention_mask=[]\n    token_type_ids=[]\n    labels=[]\n    final_dict={}\n    \n    print('Creating X- Data')\n    \n    preprocessor=preprocessing_data(tokenizer)\n    for text,sentiment in tqdm(zip(data['text'],data['sentiment'])):\n        instance=preprocessor.encode_text(text=text,sentiment=sentiment)\n        input_ids.append(instance[0])\n        attention_mask.append(instance[1])\n        token_type_ids.append(instance[2])\n    X_input_ids=np.array(input_ids)\n    X_attention_mask=np.array(attention_mask)\n    X_token_type_ids=np.array(token_type_ids)\n    \n    y_labels=[]\n    if train:\n        print('Creating Y-Labels')\n        for X_input_id,selected_text in tqdm(zip(X_input_ids,data['selected_text'])):\n            empty_list=np.zeros((X_input_ids.shape[1],))\n            selected_text_token_ids=preprocessor.encode_labels(selected_text)\n            ##Very Slow-Method but will have to run onetime-need to find a method of vectorization\n            for _id_ in selected_text_token_ids:\n                if _id_==101:\n                    pass\n                elif _id_==102:\n                    break\n                else:\n                    index=0\n                    for __id in  X_input_id:\n                        if __id==0 or __id==102:\n                            break\n                        else:\n                            if _id_==__id and empty_list[index]!=1:\n                                empty_list[index]=1\n                                break\n                            else:\n                                pass\n                        index+=1\n            y_labels.append(empty_list)\n#             first_id_index=np.where(X_input_id==first_id)[0][0]\n#             last_id_index=np.where(X_input_id==last_id)[-1][-1]\n#             empty_list[first_id_index:last_id_index+1]=1\n        y_labels=np.array(y_labels)\n        final_dict['y_labels']=y_labels\n    \n    final_dict['input_ids']=X_input_ids\n    final_dict['attention_mask']=X_attention_mask\n    final_dict['token_type_ids']=X_token_type_ids\n    \n    return final_dict\n    \n        \n    \n    \ndef tensorflow_dataset(dict_data,batch_size,train=True):\n    train_dict={}\n    valid_dict={}\n    if train:\n        X_input_ids=dict_data['input_ids']\n        X_attention_mask=dict_data['attention_mask']\n        X_token_type_ids=dict_data['token_type_ids']\n        y_labels=dict_data['y_labels']\n\n        X_train_input_ids,X_valid_input_ids=train_test_split(X_input_ids,test_size=0.05,random_state=42)\n        X_train_attention_mask,X_valid_attention_mask=train_test_split(X_attention_mask,test_size=0.05,random_state=42)\n        X_train_token_type_ids,X_valid_token_type_ids=train_test_split(X_token_type_ids,test_size=0.05,random_state=42)\n        y_train,y_valid=train_test_split(y_labels,test_size=0.05,random_state=42)\n        \n        \n        \n        train_dict['input_ids']=X_train_input_ids\n        train_dict['attention_mask']=X_train_attention_mask\n        train_dict['token_type_ids']=X_train_token_type_ids\n        \n        valid_dict['input_ids']=X_valid_input_ids\n        valid_dict['attention_mask']=X_valid_attention_mask\n        valid_dict['token_type_ids']=X_valid_token_type_ids\n        \n        \n        \n        train=tf.data.Dataset.from_tensor_slices((train_dict,y_train)).repeat().shuffle(1024).batch(batch_size).prefetch(AUTO)\n        valid=tf.data.Dataset.from_tensor_slices((valid_dict,y_valid)).batch(BATCH_SIZE).cache().prefetch(AUTO)\n\n    else:\n        return tf.data.Dataset((dict_data))\n    return train,valid,y_labels\ndict_data=preprocess_dataset(data=df_train,tokenizer=PreTrainedTokenizerFast)\ntrain,valid,y_train=tensorflow_dataset(dict_data,batch_size=32,train=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def TFBertForTokenClassification_Model():\n    # Initializing a BERT bert-base-uncased style configuration\n    configuration = BertConfig()\n    configuration.num_labels=2\n    \n    input_ids = tf.keras.layers.Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"input_ids\")\n    attention_mask=tf.keras.layers.Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"attention_mask\")\n    token_type_ids=tf.keras.layers.Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"token_type_ids\")\n    \n    # Initializing a model from the bert-base-uncased style configuration\n    model = TFBertForTokenClassification(configuration).from_pretrained('bert-base-cased')\n    \n    output=model([input_ids, attention_mask, token_type_ids],training=True,return_dict=True)\n    logit=output.logits \n    out=logit[:,:,1]\n    model = tf.keras.Model(inputs=[input_ids, attention_mask, token_type_ids],outputs=out)\n    return model\n\ndef compile_TFBertForTokenClassification_Model(optim):\n    model=TFBertForTokenClassification_Model()\n    model.compile(optimizer=optim,loss=LOSS,metrics='accuracy')\n    print(model.summary())\n    return model    \n\n\nmodel=compile_TFBertForTokenClassification_Model(OPTIMIZER)\nhistory = LossHistory()\nmodel.fit(\n      train,steps_per_epoch=steps_per_epoch,\n      epochs=EPOCHS,callbacks=[history,EARLY_STOP], validation_data=valid,\n)\n  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Bugging Terminal Lol xD\ntest_id=9202\npreprocessor=preprocessing_data(PreTrainedTokenizerFast)\nprint(df_train.iloc[test_id,:])\nprint(dict_data['input_ids'][test_id,:])\nempty_list=np.zeros((150,))\nprint(preprocessor.encode_labels(df_train.iloc[test_id,2]))\nprint(y_train[test_id,:])\nfor _id_ in preprocessor.encode_labels(df_train.iloc[test_id,2]):\n        if _id_==101:\n            pass\n        elif _id_==102:\n            break\n        else:\n            index=0\n            for __id in  dict_data['input_ids'][test_id,:]:\n                if __id==0 or __id==102:\n                    break\n                else:\n                    if _id_==__id and empty_list[index]!=1:\n                        empty_list[index]=1\n                        break\n                    else:\n                        pass\n                index+=1\nprint(empty_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}