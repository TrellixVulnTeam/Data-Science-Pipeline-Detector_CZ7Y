{"cells":[{"metadata":{},"cell_type":"markdown","source":"# EDA \n> This notebook contains preprocessing and visualization of the texts in the data.\n\n> For the visualization spacy, gensim and pyLDAvis is used.\n\n> Spacy: Spacy is an library which is used for advance natural processing. It also contains tokenization, Name Entity Recognition(NER), Parts of Speech tagging(POS), etc\n\n> Gensim: Gensim is used for unsupervised topic modelling and  finding similarity.\n\n> pyLDAviz: It is used to visualize the topic in the LDA model that has been fitted to large corpus. It creates clusters of similar topics. Two or more clusters overlapping means that they are similar. You can change the number of clusters if you want."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import the required tools\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nimport plotly.express as px\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.wordnet import WordNetLemmatizer\nimport spacy\nimport re\nimport pyLDAvis.gensim\nfrom wordcloud import WordCloud\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/tweet-sentiment-extraction/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for data imbalance\nsns.countplot(train[\"sentiment\"]);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for missing values\ntrain.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Remove punctuations and numbers from the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.dropna(inplace=True)\ntrain = train.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_punc(text):\n  cleaned_text = re.sub(r'[^\\w\\s]', '', text)\n  cleaned_text = re.sub(r'[0-9]', r'', cleaned_text)\n  return cleaned_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"cleaned_text\"] = np.vectorize(clean_punc)(train[\"text\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove stopwords\nstop_words = set(stopwords.words(\"english\"))\ntrain[\"cleaned_text\"] = train[\"cleaned_text\"].apply(lambda x : \" \".join([w.lower() for w in x.split() if w not in stop_words and len(w) > 3]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tokenize the tweet"},{"metadata":{"trusted":true},"cell_type":"code","source":"texts = train[\"cleaned_text\"].apply(lambda x : x.split())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Lemmatizing the text"},{"metadata":{"trusted":true},"cell_type":"code","source":"lemma = WordNetLemmatizer()\nlemming_texts = texts.apply(lambda x:[lemma.lemmatize(i) for i in x])\nlemming_texts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(lemming_texts)):\n  lemming_texts[i] = \" \".join(lemming_texts[i])\n\ntrain[\"cleaned_text\"] = lemming_texts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"positive_text = train[\"cleaned_text\"][train[\"sentiment\"]==\"positive\"].apply(lambda x : x.split())\nnegative_text = train[\"cleaned_text\"][train[\"sentiment\"]==\"negative\"].apply(lambda x: x.split())\nneutral_text = train[\"cleaned_text\"][train[\"sentiment\"]==\"neutral\"].apply(lambda x:x.split())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merge all the lists into one common list\npositive_text = sum(positive_text, [])\nnegative_text = sum(negative_text, [])\nneutral_text = sum(neutral_text, [])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the most common words in positive sentiment\nfreq_pos = nltk.FreqDist(positive_text)\npos_df = pd.DataFrame({\n    \"words\":list(freq_pos.keys()),\n    \"Count\":list(freq_pos.values())\n})\ncommon_pos = pos_df.nlargest(columns=\"Count\", n=30)\nfig = px.bar(common_pos, x=\"words\", y=\"Count\", labels={\"words\": \"Words\", \"Count\":\"Frequency\"})\nfig.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the most common words in negative sentiment\nfreq_neg = nltk.FreqDist(negative_text)\nneg_df = pd.DataFrame({\n    \"words\":list(freq_neg.keys()),\n    \"Count\":list(freq_neg.values())\n})\ncommon_neg = neg_df.nlargest(columns=\"Count\", n=30)\nfig = px.bar(common_neg, x=\"words\", y=\"Count\", labels={\"words\": \"Words\", \"Count\": \"Frequency\"})\nfig.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the most common words in neutral sentiment\nfreq_ntl = nltk.FreqDist(neutral_text)\nntl_df = pd.DataFrame({\n    \"words\":list(freq_ntl.keys()),\n    \"Count\":list(freq_ntl.values())\n})\ncommon_ntl = ntl_df.nlargest(columns=\"Count\", n=30)\nfig = px.bar(common_ntl, x=\"words\", y=\"Count\", labels={\"words\": \"Words\", \"Count\":\"Frequency\"})\nfig.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Wordcloud"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Most common words in the texts\nwords = \" \".join([text for text in train[\"cleaned_text\"]])\nwordclouds = WordCloud(width=900, height=600, random_state=42, max_font_size=110).generate(words)\n\nplt.figure(figsize=(10, 10))\nplt.imshow(wordclouds, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Wordcloud for positive words\npositive_words = \" \".join([word for word in train[\"cleaned_text\"][train[\"sentiment\"] == \"positive\"]])\nwordcloud_pos = WordCloud(width=900, height=700, random_state=42, max_font_size=100).generate(positive_words)\n\nplt.figure(figsize=(10, 10))\nplt.imshow(wordcloud_pos)\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Name Entity Recognition"},{"metadata":{"trusted":true},"cell_type":"code","source":"nlp = spacy.load(\"en_core_web_sm\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ners = []\nfor text in train[\"cleaned_text\"].values:\n  doc = nlp(text)\n\n  for entity in doc.ents:\n    ners.append((entity.text, entity.label_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"name_entity_df = pd.DataFrame(ners, columns = [\"Entity Name\", \"Entity Label\"])\nentity_df = name_entity_df.groupby(by=[\"Entity Name\", \"Entity Label\"]).size().sort_values(ascending=False).reset_index().rename(columns = {0: \"Frequency\"})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the first 50 entities\nfigure = px.bar(x=entity_df[\"Entity Label\"][:50], y=entity_df[\"Frequency\"][:50]) \nfigure.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Topic Modelling"},{"metadata":{},"cell_type":"markdown","source":"> Topic Modelling is the task of extracting the main topics from the documents.\n\n>For this task LDA is used. LDA is used to classify the text in a document to a particular topic"},{"metadata":{"trusted":true},"cell_type":"code","source":"texts = train[\"cleaned_text\"].apply(lambda x: x.split())\ntexts.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build the dictionary of words from the document\nfrom gensim import corpora\ndictionary = corpora.Dictionary(texts)\ncorpus = [dictionary.doc2bow(text) for text in texts]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build the LDA Model \nimport gensim\ntopics = 10\nlda = gensim.models.ldamodel.LdaModel(corpus, num_topics=topics, id2word=dictionary, passes=15)\nlda.save(\"LDA_model.gensim\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display = pyLDAvis.gensim.prepare(lda, corpus, dictionary)\npyLDAvis.display(display)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}