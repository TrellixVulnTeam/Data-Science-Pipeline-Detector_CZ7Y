{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nfrom tensorflow.keras import backend as k\nfrom tensorflow.keras import *\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport nltk\nnltk.download('wordnet')\nnltk.download('stopwords')\nnltk.download('punkt')\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\nimport re\nfrom string import punctuation\nfrom gensim.models import KeyedVectors\nfrom sklearn.model_selection import train_test_split\n# from emoji import *\n# import emoji\nimport functools\nimport string\nimport operator\nimport random\nrandom.seed(50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_text(text):\n    text = text.lower() \n    text = re.sub(r\"(@[a-z]*)\", \"<mention>\", text)#remove any word start with @\n    text = re.sub(r\"(&[a-z;]*)\", \"<none>\", text)#remove any word start with &\n    text = re.sub(r\"(#[a-z;]*)\", \"<hash>\", text)#remove any word start with #\n    text = re.sub(r\"(http|https|ftp|ftps)\\:\\/\\/[a-zA-Z0-9\\-\\.]+\\.[a-zA-Z]{2,3}(\\/\\S*)?\", \"<link>\", text)#remove LINKs\n    text = re.sub(r'https?://\\S+', '<link>', text) # remove https? links\n    text = re.sub(r\"(www.[a-z.\\/0-9]*)\", \"<link>\", text)#remove LINKs\n    return text\ndef Transfrom_text(textT,textS,type):\n    t = \" \".join(\"<tok>\" for i in range (len(textS.split())))\n    textT = textT.replace(textS,t)\n    return textT\ndef preprocess_text(df):\n    df['text_clean'] = df['text'].apply(lambda x: clean_text(x))\n    df['selected_text_clean'] = df['text'].apply(lambda x: clean_text(x))\n    df['target'] = pd.DataFrame([Transfrom_text(df['text'][i],df['selected_text'][i],df['sentiment'][i]) for i in range(len(df))])\n    return df ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv')\ntest_df = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv')\nprint(test_df.isnull().sum())\n\ntrain_df = train_df[train_df.sentiment != 'neutral']\ntrain_df = train_df.dropna()\ntrain_df = train_df.reset_index()\n\n# train_df.text = \"<startsent>\"+ \" \" + train_df.text+\" \"+\"<endsent>\"\n# test_df.text = \"<startsent>\"+\" \" + test_df.text+\" \"+\"<endsent>\"\n\ntrain = preprocess_text(train_df)\ntest = test_df\nprint(train_df.shape)\nprint(train.shape, test.shape)\nprint(train.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_glove():\n  tqdm.pandas()\n  f = open('/kaggle/input/glove840b300dtxt/glove.840B.300d.txt')\n#   f = open('/kaggle/input/glove6b300dtxt/glove.6B.300d.txt')\n    \n  embedding_values = {}\n  for line in tqdm(f):\n      value = line.split(' ')\n      word = value[0]\n      coef = np.array(value[1:],dtype = 'float32')\n      embedding_values[word] = coef\n  return embedding_values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fit_glove(embedding_values):\n  \n  all_embs = np.stack(embedding_values.values())\n  emb_mean,emb_std = all_embs.mean(), all_embs.std()\n  emb_mean,emb_std\n  \n  embedding_matrix = np.random.normal(emb_mean, emb_std, (vocab_size, 300))\n  OFV = []\n  for word,i in tqdm(token.word_index.items()):\n      values = embedding_values.get(re.sub(r\"[^A-Za-z]\", \"\", word))\n      if values is not None:\n          embedding_matrix[i] = values\n      else:\n        OFV.append(word)\n  print(len(OFV))\n#   print(\" | \".join(i for i in OFV))\n  return embedding_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t1 = pd.DataFrame({'A': test.text.values})\nt2 = pd.DataFrame({'A': train.text})\nt3 = pd.DataFrame({'A': train.target})\n# print(t2)\nall_tokens = pd.concat([t1,t2,t3],axis = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"token = Tokenizer(num_words=54000,filters='')\ntoken.fit_on_texts(all_tokens.A) \nvocab_size = len(token.word_index)+1\nvocab_size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reverse_input_char_index = token.index_word\ndef decode_sequence(train_pad_seq_x,input_seq):\n  decoded_sentence = \"\"\n  for i in range(len(input_seq)):\n    # print(input_seq[i])\n    if (input_seq[i] == 1 or input_seq[i] == 2):\n      if train_pad_seq_x[i] != 0:\n        sampled_char = reverse_input_char_index[train_pad_seq_x[i]]\n        decoded_sentence += sampled_char + \" \"\n  return decoded_sentence","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_values = load_glove()\nembedding_matrix = fit_glove(embedding_values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN = 35\nfor word,i in token.word_index.items():\n  if \"<tok>\" in word:\n    token.word_index[word] = token.word_index[\"<tok>\"]    \n    \ntrain_seq_x = token.texts_to_sequences(train.text)\ntrain_pad_seq_x = pad_sequences(train_seq_x,maxlen=MAX_LEN)\n\ntrain_seq_y = token.texts_to_sequences(train.target)\ntrain_pad_seq_y = pad_sequences(train_seq_y,maxlen=MAX_LEN)\n\n\ntrain_pad_seq_y[train_pad_seq_y != token.word_index['<tok>']] = 0\ntrain_pad_seq_y[train_pad_seq_y == token.word_index['<tok>']] = 1  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def validation():\n    test_pred = model.predict(X_train).round().astype(int)\n    avg = 0\n    for i in range(len(X_train)):  \n        st1 = decode_sequence(X_train[i],test_pred[i])\n        st2 = decode_sequence(X_train[i],y_train[i])\n        avg += jaccard(st1,st2)\n    print(\"Jac train sccore = \" , np.sum(avg) / len(X_train))\n\n    test_pred = model.predict(X_test).round().astype(int)\n    avg = 0\n    for i in range(len(X_test)):  \n        st1 = decode_sequence(X_test[i],test_pred[i])\n        st2 = decode_sequence(X_test[i],y_test[i])\n        avg += jaccard(st1,st2)\n    print(\"Jac valid sccore = \",np.sum(avg) / len(X_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"HIDDEN_DIM=256\n    \ninputs = Input(shape=(MAX_LEN, ), dtype='int32')\n\nembedding_layer = Embedding(vocab_size,300,weights = [embedding_matrix],trainable = False)\nencoder_LSTM_1 = Bidirectional(LSTM(HIDDEN_DIM,return_sequences=True,kernel_regularizer=regularizers.l2(0.01)))\n\ndense_layer_relu = TimeDistributed(Dense(64, activation='relu'))\ndense_layer_relu_1 = TimeDistributed(Dense(64, activation='relu'))\n\nDrop = TimeDistributed(Dropout(0.2))\ndense_layer = TimeDistributed(Dense(1, activation='sigmoid'))\n\nencoder_embedding = embedding_layer(inputs)\n\nEncoded_seq = encoder_LSTM_1(encoder_embedding)\n\noutputs = Drop(dense_layer_relu(Encoded_seq))\noutputs = Drop(dense_layer_relu_1(outputs))\n\noutputs = dense_layer(outputs)\n\nmodel = Model(inputs, outputs)\n    \nmodel.compile(optimizer='Adam', loss='binary_crossentropy',metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(train_pad_seq_x, train_pad_seq_y, test_size=0.2, random_state=50)\nfor i in range(4):\n    model.fit(X_train,y_train,batch_size=32,epochs=5,validation_data=(X_test,y_test),verbose=0)\n    validation()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(train_pad_seq_x,train_pad_seq_y,batch_size=32,epochs=2,verbose=0)\nvalidation()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred = model.predict(train_pad_seq_x).round().astype(int)\navg = 0\nfor i in range(len(train_pad_seq_x)):  \n    st1 = decode_sequence(train_pad_seq_x[i],test_pred[i])\n    st2 = decode_sequence(train_pad_seq_x[i],train_pad_seq_y[i])\n    avg += jaccard(st1,st2)\nprint(\"Jac all data sccore = \" , np.sum(avg) / len(train_pad_seq_x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/sample_submission.csv')\ntest_seq = token.texts_to_sequences(test.text)\ntest_pad_seq = pad_sequences(test_seq,maxlen=MAX_LEN)\ntest_pred = model.predict(test_pad_seq).round().astype(int)\nfor i in range(len(sub['selected_text'])):\n    if test.sentiment[i] == 'neutral' or len(test.text[i].split()) < 4:\n        sub['selected_text'][i] = test.text[i]\n    else:\n        sub['selected_text'][i] = decode_sequence(test_pad_seq[i],test_pred[i])\nsub.to_csv('submission.csv', index=False)\nsub.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}