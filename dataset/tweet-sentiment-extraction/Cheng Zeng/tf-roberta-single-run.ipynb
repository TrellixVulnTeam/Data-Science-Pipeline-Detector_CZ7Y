{"cells":[{"metadata":{},"cell_type":"markdown","source":"# TensorFlow roBERTa + CNN head","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Load  data and libraries","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd, numpy as np\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom sklearn.model_selection import StratifiedKFold\nfrom transformers import *\nimport tokenizers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_train():\n    train=pd.read_csv('../input/tweet-sentiment-extraction/train.csv')\n    train['text']=train['text'].astype(str)\n    train['selected_text']=train['selected_text'].astype(str)\n    return train\n\ndef read_test():\n    test=pd.read_csv('../input/tweet-sentiment-extraction/test.csv')\n    test['text']=test['text'].astype(str)\n    return test\n\ndef read_submission():\n    test=pd.read_csv('../input/tweet-sentiment-extraction/sample_submission.csv')\n    return test\n    \ntrain_df = read_train()\ntest_df = read_test()\nsubmission_df = read_submission()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED = 42\ntf.random.set_seed(SEED)\nnp.random.seed(SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    if (len(a)==0) & (len(b)==0): return 0.5\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data preproccesing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN = 96\n# MAX_LEN = 120\nPATH = '../input/tf-roberta/'\ntokenizer = tokenizers.ByteLevelBPETokenizer(\n    vocab_file=PATH+'vocab-roberta-base.json', \n    merges_file=PATH+'merges-roberta-base.txt', \n    lowercase=True,\n    add_prefix_space=True\n)\nsentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ct = train_df.shape[0]\ninput_ids = np.ones((ct,MAX_LEN),dtype='int32')\nattention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\ntoken_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\nstart_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\nend_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n\nfor k in range(train_df.shape[0]):\n    \n    # FIND OVERLAP\n    text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n    text2 = \" \".join(train_df.loc[k,'selected_text'].split())\n    idx = text1.find(text2)\n    chars = np.zeros((len(text1)))\n    chars[idx:idx+len(text2)]=1\n    if text1[idx-1]==' ': chars[idx-1] = 1 \n    enc = tokenizer.encode(text1) \n        \n    # ID_OFFSETS\n    offsets = []; idx=0\n    for t in enc.ids:\n        w = tokenizer.decode([t])\n        offsets.append((idx,idx+len(w)))\n        idx += len(w)\n    \n    # START END TOKENS\n    toks = []\n    for i,(a,b) in enumerate(offsets):\n        sm = np.sum(chars[a:b])\n        if sm>0: toks.append(i) \n        \n    s_tok = sentiment_id[train_df.loc[k,'sentiment']]\n    input_ids[k,:len(enc.ids)+3] = [0, s_tok] + enc.ids + [2]\n    attention_mask[k,:len(enc.ids)+3] = 1\n    if len(toks)>0:\n        start_tokens[k,toks[0]+2] = 1\n        end_tokens[k,toks[-1]+2] = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ct = test_df.shape[0]\ninput_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\nattention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\ntoken_type_ids_t = np.zeros((ct,MAX_LEN),dtype='int32')\n\nfor k in range(test_df.shape[0]):\n        \n    # INPUT_IDS\n    text1 = \" \"+\" \".join(test_df.loc[k,'text'].split())\n    enc = tokenizer.encode(text1)                \n    s_tok = sentiment_id[test_df.loc[k,'sentiment']]\n    input_ids_t[k,:len(enc.ids)+3] = [0, s_tok] + enc.ids + [2]\n    attention_mask_t[k,:len(enc.ids)+3] = 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"PAD_ID = 1\nLABEL_SMOOTHING = 0.15\n\ndef loss_fn(y_true, y_pred):\n    # adjust the targets for sequence bucketing\n    ll = tf.shape(y_pred)[1]\n    y_true = y_true[:, :ll]\n    loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred,\n        from_logits=False, label_smoothing=LABEL_SMOOTHING)\n    loss = tf.reduce_mean(loss)\n    return loss\n\ndef build_model(alpha=0.30):\n    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    padding = tf.cast(tf.equal(ids, PAD_ID), tf.int32)\n\n    lens = MAX_LEN - tf.reduce_sum(padding, -1)\n    max_len = tf.reduce_max(lens)\n    ids_ = ids[:, :max_len]\n    att_ = att[:, :max_len]\n    tok_ = tok[:, :max_len]\n\n    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n    x = bert_model(ids_,attention_mask=att_,token_type_ids=tok_)\n    \n    x1 = tf.keras.layers.Dropout(0.25)(x[0])\n    x1 = tf.keras.layers.Conv1D(768, 2,padding='same')(x1)\n    x1 = tf.keras.layers.LeakyReLU(alpha=alpha)(x1)\n    x1 = tf.keras.layers.Dense(1)(x1)\n    x1 = tf.keras.layers.Flatten()(x1)\n    x1 = tf.keras.layers.Activation('softmax')(x1)\n    \n    x2 = tf.keras.layers.Dropout(0.25)(x[0]) \n    x2 = tf.keras.layers.Conv1D(768, 2,padding='same')(x2)\n    x2 = tf.keras.layers.LeakyReLU(alpha=alpha)(x2)\n    x2 = tf.keras.layers.Dense(1)(x2)\n    x2 = tf.keras.layers.Flatten()(x2)\n    x2 = tf.keras.layers.Activation('softmax')(x2)\n\n    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n    optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5) \n    model.compile(loss=loss_fn, optimizer=optimizer)\n    \n    # this is required as `model.predict` needs a fixed size!\n    x1_padded = tf.pad(x1, [[0, 0], [0, MAX_LEN - max_len]], constant_values=0.)\n    x2_padded = tf.pad(x2, [[0, 0], [0, MAX_LEN - max_len]], constant_values=0.)\n    \n    padded_model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1_padded,x2_padded])\n    return model, padded_model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train\nWe will skip this stage and load already trained model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Inference","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\ndef load_weights(model, weight_fn):\n    with open(weight_fn, 'rb') as f:\n        weights = pickle.load(f)\n    model.set_weights(weights)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nn_splits = 10; DISPLAY = 1\npreds_start = np.zeros((n_splits, input_ids_t.shape[0],MAX_LEN))\npreds_end = np.zeros((n_splits, input_ids_t.shape[0],MAX_LEN))\n\nfor i in range(n_splits):\n    print('#'*25)\n    print('### MODEL %i'% (i+1))\n    print('#'*25)\n    \n    K.clear_session()\n    model, padded_model = build_model()\n    load_weights(padded_model, '../input/tse-seed88888/v0-roberta-%i.h5' %(i))\n    print('Predicting Test...')\n    preds = padded_model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n    preds_start[i,:,:] = preds[0]\n    preds_end[i,:,:] = preds[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"answer = [0.57657865, 0.61908779, 0.61798954, 0.61765775, 0.41941072, 0.37508611]\nstarts = [5,7,6,2,8,0]; ends = [5,7,8,6,2,9]; weights = answer\npreds_start_avg = np.average(preds_start[starts,], axis=0, weights=weights)\npreds_end_avg = np.average(preds_end[ends,], axis=0, weights=weights)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_word_number(x):\n    return len(x.selected_text.split())\n\ndef get_intersection_and_union(st1, st2):\n    set1 = set(st1.lower().split())\n    set2 = set(st2.lower().split())\n    return ' '.join(list(set1.intersection(set2))), ' '.join(list(set1.union(set2)))\n\ndef is_subset(st1, st2):\n    set1 = set(st1.lower().split())\n    set2 = set(st2.lower().split())\n    return set1.issubset(set2)\n\ndef get_the_word(st1, st2):\n    st1 = st1.lower()\n    st2 = st2.lower().split()\n    for _ in st2:\n        if st1 in _:\n            return _\n        \ndef get_diff(st1, st2):\n    set1 = set(st1.lower().split())\n    set2 = set(st2.lower().split())\n    diff = set2.difference(set1)\n    return ' '.join(list(diff))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\ndef post_process(x):\n    if x.startswith('.'):\n        x = re.sub(\"([\\.]+)\", '.', x, 1)\n    if len(x.split()) == 1:\n        x = x.replace('!!!!', '!')\n        x = x.replace('???', '?')\n        if x.endswith('...'):\n            x = x.replace('..', '.')\n            x = x.replace('...', '.')\n        return x\n    else:\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all = []\nall_bak = []\nall_com = []\nstarts, starts_bak, ends, ends_bak = [], [], [], []\nstart_diffs = []\nend_diffs = []\n\ncount = 0\ncount_abn = 0\ncount_abn_2 = 0\ncount_nm = 0\ncount_bak = 0\ncount_ori = 0\ncount_true = 0\ncount_zero = 0\nlength_true = []\n\nfor k in range(input_ids_t.shape[0]):\n    a, a_bak= np.argsort(preds_start_avg[k,])[::-1][:2]\n    b, b_bak = np.argsort(preds_end_avg[k,])[::-1][:2]\n    diff_start = abs(preds_start_avg[k,a_bak] -  preds_start_avg[k,a])\n    diff_end = abs(preds_end_avg[k,b_bak] -  preds_end_avg[k,b])\n    starts.append(a);starts_bak.append(a_bak);ends.append(b);ends_bak.append(b_bak)\n    start_diffs.append(diff_start)\n    end_diffs.append(diff_end)\n    full_text = test_df.loc[k,'text']\n    text1 = \" \"+\" \".join(full_text.split())\n    enc = tokenizer.encode(text1)\n    if a>b:\n        if a_bak <= b and a > b_bak:\n            st = tokenizer.decode(enc.ids[a_bak-2:b-1])\n        elif a_bak > b and a <= b_bak:\n            st = tokenizer.decode(enc.ids[a-2:b_bak-1])\n        elif a_bak <= b_bak:\n            st = tokenizer.decode(enc.ids[a_bak-2:b_bak-1])\n        else:\n            count_abn_2 += 1\n            st = full_text           \n    else:\n        st = tokenizer.decode(enc.ids[a-2:b-1])\n\n    all.append(st)\n        \n    if a_bak>b_bak:\n        st_bak = st\n    else:\n        st_bak = tokenizer.decode(enc.ids[a_bak-2:b_bak-1])\n    \n    all_bak.append(st_bak)\n    \n    m, n = 0.14, 0.14\n#     m, n = 0.001, 0.001\n    st_int, st_uni = get_intersection_and_union(st, st_bak)\n    if is_subset(st_bak, full_text) and is_subset(st, full_text):\n        count_nm += 1\n        if diff_start < m and diff_end < n:\n            count_abn += 1\n            st_com = st_uni\n        else:\n            st_com = st\n    elif is_subset(st, full_text):\n        count_ori += 1\n        st_com = st\n    else:\n        count_true += 1\n        length_true.append(len(st))\n        st_com = st\n#     st_com = full_text if test_df.loc[k,'sentiment'] == 'neutral' else st_com\n    st_com = post_process(st_com)\n    all_com.append(st_com)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from matplotlib import pyplot as plt\n# from collections import Counter\n# Counter(length_true).most_common(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_total = test_df.shape[0]\nprint(f\"Uncertain fraction: {100*count_abn/n_total: .2f}%\")      \nprint(f\"First Prob fraction: {100*count_ori/n_total: .2f}%\")      \nprint(f\"Legal First and Second Pred Fraction: {100*count_nm/n_total: .2f}%\")      \nprint(f\"First Prob not in original text fraction: {100*count_true/n_total: .2f}%\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mask_sentiment = test_df.sentiment != 'neutral'\nprint(np.quantile(np.array(start_diffs)[mask_sentiment], 0.344))\nprint(np.quantile(np.array(end_diffs)[mask_sentiment], 0.248))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df['selected_text'] = all_com\n# test_df['selected_text'] = test_df['selected_text'].apply(lambda x: x.replace('!!!!', '!') if len(x.split())==1 else x)\n# test_df['selected_text'] = test_df['selected_text'].apply(lambda x: x.replace('???', '?') if len(x.split())==1 else x)\n# test_df['selected_text'] = test_df['selected_text'].apply(lambda x: x.replace('..', '.') if len(x.split())==1 else x)\n# test_df['selected_text'] = test_df['selected_text'].apply(lambda x: x.replace('...', '.') if len(x.split())==1 else x)\ntest_df['is_subset'] = test_df.apply(lambda x: is_subset(x.selected_text, x.text), axis=1)\ntest_df[['textID','selected_text']].to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('max_colwidth', 200)\n# test_df.query(\"sentiment != 'neutral' and is_subset == False\").sample(25)\ntest_df.sample(25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(test_df.is_subset == 1).mean()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}