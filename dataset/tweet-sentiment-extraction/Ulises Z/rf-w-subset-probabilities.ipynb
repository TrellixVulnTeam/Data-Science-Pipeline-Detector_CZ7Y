{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport sklearn\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\nimport re","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Read in the data, and obtain original positive/negative sentiment labeled data for submission model","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"og_train = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv')\ntest = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv')\nsubmission = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/sample_submission.csv')\nog_train = og_train.dropna()\ntest = test.dropna()\n\nog_train = og_train.reset_index(drop=True)\ntest = test.reset_index(drop=True)\n\ntrain_og_pos = og_train[og_train['sentiment'] == 'positive'] \ntrain_og_neg = og_train[og_train['sentiment'] == 'negative']\n\ntrain_og_pos = train_og_pos.reset_index(drop=True)\ntrain_og_neg = train_og_neg.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Split train into train/val split new train on sentiment label","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain, val = train_test_split(og_train, test_size=0.25)\ntrain = train.reset_index(drop=True)\nval = val.reset_index(drop=True)\n\ntrain_pos = train[train['sentiment'] == 'positive']\ntrain_neg = train[train['sentiment'] == 'negative']\n\ntrain_pos = train_pos.reset_index(drop=True)\ntrain_neg = train_neg.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Preprocessing data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#lowercase the text\ntrain_pos_text = train_pos['text'].str.lower()\ntrain_neg_text = train_neg['text'].str.lower()\n\ntrain_pos_target_text = train_pos['selected_text'].str.lower()\ntrain_neg_target_text = train_neg['selected_text'].str.lower()\n\n\ntrain_og_pos_text = train_og_pos['text'].str.lower()\ntrain_og_neg_text = train_og_neg['text'].str.lower()\n\ntrain_og_pos_target_text = train_og_pos['selected_text'].str.lower()\ntrain_og_neg_target_text = train_og_neg['selected_text'].str.lower()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# removing special characters and numbers\ntrain_pos_text = train_pos_text.apply(lambda x : re.sub(\"[^a-z\\s]\",\"\",x) )\ntrain_neg_text = train_neg_text.apply(lambda x : re.sub(\"[^a-z\\s]\",\"\",x) )\n\ntrain_pos_target_text = train_pos_target_text.apply(lambda x : re.sub(\"[^a-z\\s]\",\"\",x) )\ntrain_neg_target_text = train_neg_target_text.apply(lambda x : re.sub(\"[^a-z\\s]\",\"\",x) )\n\n\ntrain_og_pos_text = train_og_pos_text.apply(lambda x : re.sub(\"[^a-z\\s]\",\"\",x) )\ntrain_og_neg_text = train_og_neg_text.apply(lambda x : re.sub(\"[^a-z\\s]\",\"\",x) )\n\ntrain_og_pos_target_text = train_og_pos_target_text.apply(lambda x : re.sub(\"[^a-z\\s]\",\"\",x) )\ntrain_og_neg_target_text = train_og_neg_target_text.apply(lambda x : re.sub(\"[^a-z\\s]\",\"\",x) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# removing stopwords\nfrom nltk.corpus import stopwords\nstopwords = set(stopwords.words(\"english\"))\n\ntrain_pos_text = train_pos_text.apply(lambda x : \" \".join(word for word in x.split() if word not in stopwords ))\ntrain_neg_text = train_neg_text.apply(lambda x : \" \".join(word for word in x.split() if word not in stopwords ))\n\ntrain_pos_target_text = train_pos_target_text.apply(lambda x : \" \".join(word for word in x.split() if word not in stopwords ))\ntrain_neg_target_text = train_neg_target_text.apply(lambda x : \" \".join(word for word in x.split() if word not in stopwords ))\n\n\ntrain_og_pos_text = train_og_pos_text.apply(lambda x : \" \".join(word for word in x.split() if word not in stopwords ))\ntrain_og_neg_text = train_og_neg_text.apply(lambda x : \" \".join(word for word in x.split() if word not in stopwords ))\n\ntrain_og_pos_target_text = train_og_pos_target_text.apply(lambda x : \" \".join(word for word in x.split() if word not in stopwords ))\ntrain_og_neg_target_text = train_og_neg_target_text.apply(lambda x : \" \".join(word for word in x.split() if word not in stopwords ))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Use spacy to featurize the data in the form [(word from sentence),(entire sentence)], label is presence/absence of the word in the selected text","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#create the matrix of featurized sentences from data using spacy \nimport spacy \nnlp = spacy.load('en_core_web_lg')\n\ndocument_pos = nlp.pipe(train_pos_text)\npos_vector = np.array([tweet.vector for tweet in document_pos])\n\ndocument_neg = nlp.pipe(train_neg_text)\nneg_vector = np.array([tweet.vector for tweet in document_neg])\n\n\ndocument_og_pos = nlp.pipe(train_og_pos_text)\nog_pos_vector = np.array([tweet.vector for tweet in document_og_pos])\n\ndocument_og_neg = nlp.pipe(train_og_neg_text)\nog_neg_vector = np.array([tweet.vector for tweet in document_og_neg])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#iterate through words of text and create word feature, and append sentence feature to word ft., label (Y) is whether or the not the single word is part of the selected text\ndef featurize(text, selected_text, corpus_vect):\n    labels = []\n    featurized_data = []\n    for i in range(len(text)):\n        sent_vect = corpus_vect[i]\n        target_text = selected_text[i]\n        for word in text[i].split():\n            word_vect = nlp(word).vector\n            ft_vect = np.concatenate((word_vect, sent_vect))\n            featurized_data.append(ft_vect.tolist())\n            if word in target_text:\n                labels.append(1)\n            else:\n                labels.append(0)\n    return (featurized_data, labels)\n\n(featurized_positive_X, featurized_positive_Y) = featurize(train_pos_text, train_pos_target_text, pos_vector)\n(featurized_negative_X, featurized_negative_Y) = featurize(train_neg_text, train_neg_target_text, neg_vector)\n\n\n(featurized_og_positive_X, featurized_og_positive_Y) = featurize(train_og_pos_text, train_og_pos_target_text, og_pos_vector)\n(featurized_og_negative_X, featurized_og_negative_Y) = featurize(train_og_neg_text, train_og_neg_target_text, og_neg_vector)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fit model with newly formed data (LR being compared with RF in validation set only, final submission made with RF)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nclf_pos = LogisticRegression(random_state=0, max_iter=1000).fit(featurized_positive_X, featurized_positive_Y)\nclf_neg = LogisticRegression(random_state=0, max_iter=1000).fit(featurized_negative_X, featurized_negative_Y)\n\n#clf_og_pos = LogisticRegression(random_state=0, max_iter=1000).fit(featurized_og_positive_X, featurized_og_positive_Y)\n#clf_og_neg = LogisticRegression(random_state=0, max_iter=1000).fit(featurized_og_negative_X, featurized_og_negative_Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nclf_pos_RF = RandomForestClassifier().fit(featurized_positive_X, featurized_positive_Y)\nclf_neg_RF = RandomForestClassifier().fit(featurized_negative_X, featurized_negative_Y)\n\nclf_og_pos_RF = RandomForestClassifier().fit(featurized_og_positive_X, featurized_og_positive_Y)\nclf_og_neg_RF = RandomForestClassifier().fit(featurized_og_negative_X, featurized_og_negative_Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#method to build the return string, uses the first word and last word of phrase to extract that portion from the original text (og_words)\ndef buildRetString(first_word, last_word, og_words):\n    retStr = '';\n    for i in range(len(og_words)):\n        og_word = og_words[i]\n        word = og_word.lower()\n        word = re.sub(\"[^a-z\\s]\",\"\", word)\n        if word not in stopwords:\n            if word != first_word:\n                continue;\n            else:\n                temp_og_word = og_word\n                temp_word = word\n                retStr += temp_og_word\n                i += 1\n                while temp_word != last_word:\n                    temp_og_word = og_words[i]\n                    retStr += (' ' + temp_og_word)\n                    temp_word = temp_og_word.lower()\n                    temp_word = re.sub(\"[^a-z\\s]\",\"\", temp_word)\n                    i += 1\n                return retStr","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Extract text for validation data and evaluate extraction using Jacard score for example 'selected_text' field\n* Running both LR and RF on validation data for comparison of classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#preprocess the validation data\nval_text = val['text'].str.lower()\nval_text = val_text.apply(lambda x : re.sub(\"[^a-z\\s]\",\"\",x) )\nval_text = val_text.apply(lambda x : \" \".join(word for word in x.split() if word not in stopwords ))\n#obtain feature vector for sentences\ndocument_val = nlp.pipe(val_text)\nval_vector = np.array([tweet.vector for tweet in document_val])\n\nselected_text_clf_probs = []\n\n#extra clfs\nselected_text_RF = []\n\n\nfor i in range(len(val_text)):\n    sent_vect = val_vector[i]\n    if val['sentiment'][i] != 'neutral' and len(val_text[i].split()) > 2:\n        temp_selected_text = []\n        probabilities_words = {}\n        \n        #extra clfs\n        probabilities_RF = {}        \n        \n        for word in val_text[i].split():\n            word_vect = nlp(word).vector\n            ft_vect = np.concatenate((word_vect, sent_vect))\n            if val['sentiment'][i] == 'positive':\n                clf = clf_pos\n                clf_RF = clf_pos_RF\n                \n            else:\n                clf = clf_neg\n                clf_RF = clf_neg_RF\n                \n            probability_class_1 = clf.predict_proba([ft_vect])[:, 1]\n            probabilities_words.update({word:(probability_class_1-0.5)})  \n            \n            #extra clfs                       \n            probability_class_1 = clf_RF.predict_proba([ft_vect])[:, 1]\n            probabilities_RF.update({word:(probability_class_1-0.5)})  \n\n        \n        words = val_text[i].split()\n        subsets = [words[m:j+1] for m in range(len(words)) for j in range(m,len(words))]\n        \n        \n        best_sum = 0;\n        best_index = -1\n        for j in range(len(subsets)):\n            current_sum = 0\n            for p in range(len(subsets[j])):\n                current_sum += probabilities_words.get(subsets[j][p])\n            if current_sum > best_sum:\n                best_sum = current_sum\n                best_index = j\n        if best_index != -1:\n            first_word = subsets[best_index][0]\n            last_word = subsets[best_index][len(subsets[best_index])-1]\n            og_words = val['text'][i].split()\n            retStr = buildRetString(first_word, last_word, og_words)  \n            #print(retStr)\n            selected_text_clf_probs.append(retStr)\n        else:\n            selected_text_clf_probs.append(val['text'][i])\n            \n        #extra clfs                 \n        best_sum = 0;\n        best_index = -1\n        for j in range(len(subsets)):\n            current_sum = 0\n            for p in range(len(subsets[j])):\n                current_sum += probabilities_RF.get(subsets[j][p])\n            if current_sum > best_sum:\n                best_sum = current_sum\n                best_index = j\n        if best_index != -1:\n            first_word = subsets[best_index][0]\n            last_word = subsets[best_index][len(subsets[best_index])-1]\n            og_words = val['text'][i].split()\n            retStr = buildRetString(first_word, last_word, og_words)  \n            #print(retStr)\n            selected_text_RF.append(retStr)\n        else:\n            selected_text_RF.append(val['text'][i])           \n\n        \n    else:\n        #neutral case\n        selected_text_clf_probs.append(val['text'][i])\n        \n        #extra clfs\n        selected_text_RF.append(val['text'][i])\n        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Compute Jacard score and extracting text using both LR and RF implementations","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#jacard score\ndef jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))\n\nground_truth = val['selected_text']\nprediction = pd.Series(selected_text_clf_probs)\njac_sum = 0\nfor i in range(len(prediction)):\n    jac_sum += jaccard(prediction[i], ground_truth[i])\n\nprint('score(val) LR: ', (1/len(prediction)) * jac_sum)\n\n#jacard score\ndef jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))\n\nground_truth = val['selected_text']\nprediction = pd.Series(selected_text_RF)\njac_sum = 0\nfor i in range(len(prediction)):\n    jac_sum += jaccard(prediction[i], ground_truth[i])\n\nprint('score(val) RF: ', (1/len(prediction)) * jac_sum)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Make predictions on selected text over test data, using the classifier trained over all training data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#preprocess the test data\ntest_text = test['text'].str.lower()\ntest_text = test_text.apply(lambda x : re.sub(\"[^a-z\\s]\",\"\",x) )\ntest_text = test_text.apply(lambda x : \" \".join(word for word in x.split() if word not in stopwords ))\n#obtain feature vector for sentences\ndocument_test = nlp.pipe(test_text)\ntest_vector = np.array([tweet.vector for tweet in document_test])\n\nselected_text_clf_probs_og = []\nfor i in range(len(test_text)):\n    sent_vect = test_vector[i]\n    if test['sentiment'][i] != 'neutral' and len(test_text[i].split()) > 2:\n        temp_selected_text = []\n        probabilities_words = {}\n        for word in test_text[i].split():\n            word_vect = nlp(word).vector\n            ft_vect = np.concatenate((word_vect, sent_vect))\n            if test['sentiment'][i] == 'positive':\n                clf = clf_og_pos_RF\n            else:\n                clf = clf_og_neg_RF\n            probability_class_1 = clf.predict_proba([ft_vect])[:, 1]\n            probabilities_words.update({word:(probability_class_1-0.5)})        \n        \n        words = test_text[i].split()\n        subsets = [words[m:j+1] for m in range(len(words)) for j in range(m,len(words))]\n        best_sum = 0;\n        best_index = -1\n        for j in range(len(subsets)):\n            current_sum = 0\n            for p in range(len(subsets[j])):\n                current_sum += probabilities_words.get(subsets[j][p])\n            if current_sum > best_sum:\n                best_sum = current_sum\n                best_index = j\n        if best_index != -1:\n            first_word = subsets[best_index][0]\n            last_word = subsets[best_index][len(subsets[best_index])-1]\n            og_words = test['text'][i].split()\n            retStr = buildRetString(first_word, last_word, og_words)  \n            selected_text_clf_probs_og.append(retStr)\n        else:\n            selected_text_clf_probs_og.append(test['text'][i])\n        \n    else:\n        #neutral case\n        selected_text_clf_probs_og.append(test['text'][i])\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Print the submission","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_series = pd.Series(selected_text_clf_probs_og)\nsubmission['selected_text'] = temp_series\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}