{"cells":[{"metadata":{"_uuid":"b0f52dfd-26f4-418d-94ba-7d491e369946","_cell_guid":"a55c8903-ea51-41cf-a76c-ce3543ca3c2a","trusted":true},"cell_type":"markdown","source":"# Load packages and scripts","execution_count":null},{"metadata":{"_uuid":"628a811b-af67-4965-8bff-033635f45c53","_cell_guid":"84e9ae4a-9ed0-4a01-b6e8-d48d7a031cf9","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# basic packages\nimport os, argparse\nimport json\nimport shutil\nimport warnings\nimport time\nimport psutil\nfrom pathlib import Path\nimport tqdm\nimport numpy as np\nimport pandas as pd\nimport re, gc\nfrom typing import Dict\nfrom collections import OrderedDict, defaultdict\n\n# torch related\nimport torch\nfrom torch import nn\nfrom torch.optim import Adam\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau, CyclicLR, StepLR, CosineAnnealingLR\nfrom torch.utils.data import DataLoader, WeightedRandomSampler\nimport torch.nn.functional as F\n# transformers & tokenizers\nimport transformers\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup\nfrom transformers import RobertaConfig, RobertaModel, RobertaTokenizer, AutoConfig, AutoModel, AutoTokenizer\nimport tokenizers\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom shutil import copyfile\n\nwarnings.filterwarnings(\"ignore\")\nwarnings.filterwarnings(\"ignore\",category=UserWarning)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"25816c94-636c-4e49-9aef-dbeb93da7a60","_cell_guid":"a43695ea-ff55-423e-b9f9-e0a035b9331e","trusted":true},"cell_type":"markdown","source":"# Model Inference Part I","execution_count":null},{"metadata":{"_uuid":"d4becff6-8080-4336-9d06-972016471e89","_cell_guid":"66933405-97c7-4100-8da9-43b6c89433b3","trusted":true},"cell_type":"code","source":"copyfile(src = \"../input/utils-v10/utilsv10.py\", dst = \"../working/utilsv10.py\")\ncopyfile(src = \"../input/utils-v10/dataset10.py\", dst = \"../working/dataset10.py\")\ncopyfile(src = \"../input/utils-v10/dataset11.py\", dst = \"../working/dataset11.py\")\n\nfrom utilsv10 import (binary_focal_loss, get_learning_rate, jaccard_list, get_best_pred, ensemble, ensemble_words,get_char_prob,\n                   load_model, save_model, set_seed, write_event, evaluate, get_predicts_from_token_logits)\n\nfrom dataset10 import TrainDataset, MyCollator\nfrom dataset11 import TrainDataset as TrainDataset11","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2275123d-5a5d-4aa6-aa04-5d46b406d074","_cell_guid":"1dc0bfb7-3ac4-418e-a631-c5ab591564a1","trusted":true},"cell_type":"markdown","source":"## Preapare data","execution_count":null},{"metadata":{"_uuid":"f5fe4d8a-ae79-4a57-8675-14090e8f13da","_cell_guid":"aa663282-288a-4016-aded-a2dd0de06c3b","trusted":true},"cell_type":"code","source":"df_pred = pd.read_csv('../input/tweet-sentiment-fast/test_all_post_finetune_0608.csv') #lb724\ndf_pred1 = pd.read_csv('../input/tweet-sentiment-fast/test_all_post_finetune_large.csv') #lb717\ndf_train = pd.read_csv('../input/tweet-sentiment-extraction/train.csv')\n\ndf_test = pd.read_csv('../input/tweet-sentiment-extraction/test.csv')\n#df_test = pd.read_csv('../input/tweet-sentiment-fast/test_hidden.csv')\ndf_test.loc[:, 'selected_text'] = df_test.text.values\ndf_test['text_clean'] = df_test['text'].apply(lambda x: \" \".join(x.split()))\n\ndf_full = pd.read_csv('/kaggle/input/complete-tweet-sentiment-extraction-data/tweet_dataset.csv')\ndf_full = df_full[df_full.text.notnull()].copy()\ndf_full['text_clean'] = df_full['text'].apply(lambda x: \" \".join(x.split()))\ndf_full = df_full.drop_duplicates(subset='text')\ndf_full = df_full[~df_full.aux_id.isin(df_train.textID)]\ndf_full.rename(columns={'sentiment': 'raw_sentiment'}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"495f0c59-a7e9-448d-93ee-877b21add4ee","_cell_guid":"5d7bcd7c-d8da-4785-8ce1-32aa7f64642b","trusted":true},"cell_type":"code","source":"def find_sentiment(textID, text, sentiment):\n    text_clean = \" \".join(text.split())\n    if textID in df_full.aux_id.values:\n        return df_full['raw_sentiment'].loc[df_full.aux_id==textID].values[0]\n    elif text in df_full.text.values:\n        return df_full['raw_sentiment'].loc[df_full.text==text].values[0]\n    elif text_clean in df_full.text_clean.values:\n        return df_full['raw_sentiment'].loc[df_full.text_clean==text_clean].values[0]\n    else:\n        return sentiment","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7e074941-04d3-4056-ab13-ae519160315d","_cell_guid":"a6a4c0cf-78d7-4592-b2fd-8d72da0eb87e","trusted":true},"cell_type":"code","source":"#%%time\n# find raw sentiment\ndf_test['raw_sentiment'] = df_test.apply(lambda x: find_sentiment(x['textID'], x['text'], x['sentiment']), axis=1)\nna_mask = df_test.raw_sentiment.isnull()\nprint(na_mask.sum())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4f8153db-ff54-4815-b899-fbc3d0baa711","_cell_guid":"699b4a6a-743c-4035-b632-e08faf5d8e3f","trusted":true},"cell_type":"markdown","source":"## Parse data","execution_count":null},{"metadata":{"_uuid":"6b3714bc-dc69-4777-a1ce-763ed2ab4d39","_cell_guid":"9eda1cfa-5f49-4653-941b-a39aec3a37d7","trusted":true},"cell_type":"code","source":"#test = pd.read_csv('../input/tweet-sentiment-extraction/test.csv')\ntest = df_test.copy()\ntokenizer = AutoTokenizer.from_pretrained('../input/roberta-base/', do_lower_case=False)\n\nclass Args:\n    post = True\n    tokenizer = tokenizer\n    offset = 4\n    batch_size = 4\n    workers = 0\nargs = Args()\n\n# v11\nclass Args11:\n    post = True\n    tokenizer = tokenizer\n    offset = 7\n    batch_size = 4\n    workers = 0\nargs11 = Args11()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4bc33b59-2a5b-4c47-b106-91271f2f3d6d","_cell_guid":"2bf1d1ef-dad9-4c62-a254-f0f2d5f703a0","trusted":true},"cell_type":"markdown","source":"## Model","execution_count":null},{"metadata":{"_uuid":"96fceb66-e339-4e33-a321-55b3289474ec","_cell_guid":"673cde9d-c98b-4af1-a995-1bd1760ecc47","trusted":true},"cell_type":"code","source":"class TweetModel(nn.Module):\n\n    def __init__(self, pretrain_path=None, dropout=0.2, config=None):\n        super(TweetModel, self).__init__()\n        if config is not None:\n            self.bert = AutoModel.from_config(config)\n        else:\n            config = AutoConfig.from_pretrained(pretrain_path, output_hidden_states=True)\n            self.bert = AutoModel.from_pretrained(\n                pretrain_path, cache_dir=None, config=config)\n        \n        self.cnn =  nn.Conv1d(self.bert.config.hidden_size*3, self.bert.config.hidden_size, 3, padding=1)\n\n        # self.rnn = nn.LSTM(self.bert.config.hidden_size, self.bert.config.hidden_size//2, num_layers=2,\n        #                     batch_first=True, bidirectional=True)\n        self.gelu = nn.GELU()\n\n        self.whole_head = nn.Sequential(OrderedDict([\n            ('dropout', nn.Dropout(0.1)),\n            ('l1', nn.Linear(self.bert.config.hidden_size*3, 256)),\n            ('act1', nn.GELU()),\n            ('dropout', nn.Dropout(0.1)),\n            ('l2', nn.Linear(256, 2))\n        ]))\n        self.se_head = nn.Linear(self.bert.config.hidden_size, 2)\n        self.inst_head = nn.Linear(self.bert.config.hidden_size, 2)\n        self.dropout = nn.Dropout(0.1)\n\n    def forward(self, inputs, masks, token_type_ids=None, input_emb=None):\n        _, pooled_output, hs = self.bert(\n            inputs, masks, token_type_ids=token_type_ids, inputs_embeds=input_emb)\n\n        seq_output = torch.cat([hs[-1],hs[-2],hs[-3]], dim=-1)\n\n        # seq_output = hs[-1]\n\n        avg_output = torch.sum(seq_output*masks.unsqueeze(-1), dim=1, keepdim=False)\n        avg_output = avg_output/torch.sum(masks, dim=-1, keepdim=True)\n        # +max_output\n        whole_out = self.whole_head(avg_output)\n\n        seq_output = self.gelu(self.cnn(seq_output.permute(0,2,1)).permute(0,2,1))\n        \n        se_out = self.se_head(self.dropout(seq_output))  #()\n        inst_out = self.inst_head(self.dropout(seq_output))\n        return whole_out, se_out[:, :, 0], se_out[:, :, 1], inst_out","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4bd98a9d-bbd3-4c5e-961a-f3192cce559e","_cell_guid":"3e4c0da5-d944-4b99-a762-a0af52158ece","trusted":true},"cell_type":"code","source":"def predict_wu(model: nn.Module, valid_df, valid_loader, args, progress=False) -> Dict[str, float]:\n    # run_root = Path('../experiments/' + args.run_root)\n    model.eval()\n    all_end_pred, all_whole_pred, all_start_pred, all_inst_out = [], [], [], []\n    if progress:\n        tq = tqdm.tqdm(total=len(valid_df))\n    with torch.no_grad():\n        for tokens, types, masks, _, _, _, _, _, _, _ in valid_loader:\n            if progress:\n                batch_size = tokens.size(0)\n                tq.update(batch_size)\n            masks = masks.cuda()\n            tokens = tokens.cuda()\n            types = types.cuda()\n            whole_out, start_out, end_out, inst_out = model(tokens, masks, types)\n            \n            all_whole_pred.append(torch.softmax(whole_out, dim=-1)[:,1].cpu().numpy())\n            inst_out = torch.softmax(inst_out, dim=-1)\n            for idx in range(len(start_out)):\n                length = torch.sum(masks[idx,:]).item()-1 # -1 for last token\n                all_start_pred.append(torch.softmax(start_out[idx, args.offset:length], axis=-1).cpu())\n                all_end_pred.append(torch.softmax(end_out[idx, args.offset:length], axis=-1).cpu())\n                all_inst_out.append(inst_out[idx,:,1].cpu())\n            assert all_start_pred[-1].dim()==1\n\n    all_whole_pred = np.concatenate(all_whole_pred)\n    \n    if progress:\n        tq.close()\n    return all_whole_pred, all_start_pred, all_end_pred, all_inst_out","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"da199e5e-d871-4d8e-ae8a-cb933ed467c5","_cell_guid":"2d2bb9e0-8253-4e89-a9e8-b4b0fc9782cb","trusted":true},"cell_type":"markdown","source":"## Predict","execution_count":null},{"metadata":{"_uuid":"43a8c126-991a-4b60-947e-b3662001d5b6","_cell_guid":"570fa2af-e87a-42a5-ab0c-973ad2ea3404","trusted":true},"cell_type":"code","source":"# convrt part I char-level probability to clean text version (an array of length 160)\ndef _convrt_prob_partI(text, prob, max_char_len=160):\n    clean_text = \" \".join(text.split())\n    new_prob = []\n    p1, p2 = 0, 0\n    while p1 < len(text):\n        if text[p1] not in [\" \", \"\\t\", \"\\xa0\"]:\n            if text[p1] == clean_text[p2]:\n                new_prob.append(prob[p1])\n                p1 += 1\n                p2 += 1\n        else:\n            if p1 + 1 < len(text) and text[p1+1] not in [\" \", \"\\t\", \"\\xa0\"]:\n                if clean_text[p2] == \" \":\n                    new_prob.append(prob[p1])\n                    p1 += 1\n                    p2 += 1\n                else:\n                    p1 += 1                   \n            else:\n                p1 += 1\n    if len(new_prob) < max_char_len:\n        new_prob = new_prob + (max_char_len - len(new_prob))*[0]\n    return new_prob","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7d886d71-75d9-4af2-a7ca-2c69936adc88","_cell_guid":"96364f08-c7fd-4d22-836f-951049fad4f9","trusted":true},"cell_type":"code","source":"def get_prediction_partI(weights_path, test, test_loader, args, output_name):\n    # load model\n    config = RobertaConfig.from_pretrained('../input/roberta-base', output_hidden_states=True)\n    model = TweetModel(config=config)\n\n    # 10-fold predict\n    all_whole_preds, all_start_preds, all_end_preds, all_inst_preds = [], [], [], []  \n    for fold in range(10):\n        load_model(model, f'{weights_path}/best-model-%d.pt' % fold)\n        model.cuda()\n        fold_whole_preds, fold_start_preds, fold_end_preds, fold_inst_preds = predict_wu(model, test, test_loader, args, progress=True)\n\n        all_whole_preds.append(fold_whole_preds)\n        all_start_preds.append(fold_start_preds)\n        all_end_preds.append(fold_end_preds)\n        all_inst_preds.append(fold_inst_preds)\n\n\n    all_whole_preds, all_start_preds, all_end_preds, all_inst_preds = ensemble(all_whole_preds, all_start_preds, all_end_preds, all_inst_preds, test)\n    word_preds, inst_word_preds, scores = get_predicts_from_token_logits(all_whole_preds, all_start_preds, all_end_preds, all_inst_preds, test, args)\n    # word_preds, inst_word_preds, scores = get_predicts_from_token_logits(fold_whole_preds, fold_start_preds, fold_end_preds, fold_inst_preds, test, args)\n    start_char_prob, end_char_prob = get_char_prob(all_start_preds, all_end_preds, test, args)\n    \n    test['start_char_prob'] = start_char_prob\n    test['end_char_prob'] = end_char_prob\n    test['selected_text'] = word_preds\n\n    test['prob_start'] = test.apply(lambda x: _convrt_prob_partI(x['text'], x['start_char_prob']), axis=1)\n    test['prob_end'] = test.apply(lambda x: _convrt_prob_partI(x['text'], x['end_char_prob']), axis=1)\n\n    test.to_pickle(f'{output_name}.pkl')\n    np.save(f\"start_{output_name}.npy\", np.array(test['prob_start'].tolist()))\n    np.save(f\"end_{output_name}.npy\", np.array(test['prob_end'].tolist()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"87a26da7-f1d5-4a75-b6ae-0b505ea08189","_cell_guid":"2b988161-3895-4c45-98cb-7bf01c8b854c","trusted":true},"cell_type":"markdown","source":"### V10 prediction","execution_count":null},{"metadata":{"_uuid":"1a4f4fa1-931f-4752-a045-5e5ee720f7b0","_cell_guid":"6b078b27-6e3d-443a-a069-838f9cb72a5e","trusted":true},"cell_type":"code","source":"collator = MyCollator()\ntest_set = TrainDataset(test, None, tokenizer=tokenizer, mode='test', offset=args.offset)\ntest_loader = DataLoader(test_set, batch_size=args.batch_size, shuffle=False, collate_fn=collator,\n                         num_workers=args.workers)\nget_prediction_partI(weights_path='../input/roberta-v10-10',\n                     test=test, \n                     test_loader=test_loader, \n                     args=args, \n                     output_name='output_v10',\n                    )\n\nmem = psutil.virtual_memory()\nprint(f'{mem.percent:5} - {mem.free/1024**3:10.2f} - {mem.available/1024**3:10.2f} - {mem.used/1024**3:10.2f}')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"970829a7-d14c-4b28-bedb-aa3ecc4030b5","_cell_guid":"2abf5b0f-4159-408c-9bf3-8e8e8a89d450","trusted":true},"cell_type":"markdown","source":"### V11 prediction","execution_count":null},{"metadata":{"_uuid":"42e73052-e2b8-443b-88e0-0b097cf66954","_cell_guid":"8c975dc6-d762-415c-b457-a488fe79d417","trusted":true},"cell_type":"code","source":"collator = MyCollator()\ntest_set = TrainDataset11(test, None, tokenizer=tokenizer, mode='test', offset=args11.offset)\ntest_loader = DataLoader(test_set, batch_size=args.batch_size, shuffle=False, collate_fn=collator,\n                         num_workers=args.workers)\nget_prediction_partI(weights_path='../input/roberta-v11-10',\n                     test=test, \n                     test_loader=test_loader, \n                     args=args11, \n                     output_name='output_v11',\n                    )\n\nmem = psutil.virtual_memory()\nprint(f'{mem.percent:5} - {mem.free/1024**3:10.2f} - {mem.available/1024**3:10.2f} - {mem.used/1024**3:10.2f}')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3c4e8542-7154-4b45-b8fb-72b8b131dddb","_cell_guid":"615bc3f8-cf63-41d5-9106-cc20724cf543","trusted":true},"cell_type":"markdown","source":"# Model Inference Part II","execution_count":null},{"metadata":{"_uuid":"5d980ad7-558e-41d2-b704-90e45fad9c9c","_cell_guid":"824867ec-94c1-4e6e-a711-538a7315c7a0","trusted":true},"cell_type":"code","source":"# import helper scripts\nfrom shutil import copyfile\n\n# copy our file into the working directory (make sure it has .py suffix)\ncopyfile(src = \"../input/tweet-sentiment/common.py\", dst = \"../working/common.py\")\ncopyfile(src = \"../input/tweet-sentiment/dataset.py\", dst = \"../working/dataset.py\")\ncopyfile(src = \"../input/tweet-sentiment/models.py\", dst = \"../working/models.py\")\ncopyfile(src = \"../input/tweet-sentiment/metrics.py\", dst = \"../working/metrics.py\")\ncopyfile(src = \"../input/tweet-sentiment/utils.py\", dst = \"../working/utils.py\")\ncopyfile(src = \"../input/tweet-sentiment/predict_fn.py\", dst = \"../working/predict_fn.py\")\ncopyfile(src = \"../input/tweet-sentiment/nlp_albumentations.py\", dst = \"../working/nlp_albumentations.py\")\ncopyfile(src = \"../input/tweet-sentiment/transform.py\", dst = \"../working/transform.py\")\ncopyfile(src = \"../input/tweet-sentiment/run_inference_kaggle.py\", dst = \"../working/run_inference_kaggle.py\")\n\nfrom dataset import process_data\nfrom dataset import TweetDataset_kaggle as TweetDataset\nfrom models import TweetModel, TweetModel_v2\nfrom common import *\nfrom metrics import *\nfrom utils import *\nfrom utils import _convrt_back\n\nset_seed(42)\n\n# # %% [code]\n# !python run_inference_kaggle.py --model_name='roberta_base' \\\n#                                 --model_path='../input/roberta-base/' \\\n#                                 --raw_sentiment=1\n\n# mem = psutil.virtual_memory()\n# print(f'{mem.percent:5} - {mem.free/1024**3:10.2f} - {mem.available/1024**3:10.2f} - {mem.used/1024**3:10.2f}')\n\n# # %% [code]\n# !python run_inference_kaggle.py --model_name='roberta_base_noRawSenti' \\\n#                                 --model_path='../input/roberta-base-bs32-v2-0608' \\\n#                                 --raw_sentiment=0\n\n# mem = psutil.virtual_memory()\n# print(f'{mem.percent:5} - {mem.free/1024**3:10.2f} - {mem.available/1024**3:10.2f} - {mem.used/1024**3:10.2f}')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7ae5f929-65d6-4a00-a6d9-1bd9f689ee7a","_cell_guid":"18b61e72-b80b-456e-822a-f431cee955de","trusted":true},"cell_type":"markdown","source":"## Helper functions & params","execution_count":null},{"metadata":{"_uuid":"e57e3fd9-472e-49ed-a32e-3cf8fdad840a","_cell_guid":"93ec38f7-d0f5-4fa1-b513-b85465f6b500","trusted":true},"cell_type":"code","source":"def get_model(model_name):\n    # return model and tokenizer\n    if model_name.startswith('roberta_base'):\n        model_info = {\n            'name': 'roberta-base',\n            'model_path': '../input/roberta-base' if ON_KAGGLE else 'roberta-base',\n            'from_pretrained': False if ON_KAGGLE else True,\n            'vocab_file': '../input/roberta-base/vocab.json',\n            'merges_file': '../input/roberta-base/merges.txt',\n        }\n    elif model_name.startswith('roberta_large'):\n        model_info = {\n            'name': 'roberta-large',\n            'model_path': '../input/roberta-large' if ON_KAGGLE else 'roberta-large',\n            'from_pretrained': False if ON_KAGGLE else True,\n            'vocab_file': '../input/roberta-large/vocab.json',\n            'merges_file': '../input/roberta-large/merges.txt',\n        }\n    else:\n        raise RuntimeError('%s is not implemented.' % model_name)\n\n    model = TweetModel_v2(model_info)\n    tokenizer = tokenizers.ByteLevelBPETokenizer(\n        vocab_file=model_info['vocab_file'],\n        merges_file=model_info['merges_file'],\n        lowercase=False,\n        add_prefix_space=True\n    )\n    return model, tokenizer","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d12eb71b-ffdb-4bb6-8a32-3e982876e314","_cell_guid":"84bc7dfd-8b1d-4ccf-ab3f-19cef069ff38","trusted":true},"cell_type":"markdown","source":"## Dataloader, model","execution_count":null},{"metadata":{"_uuid":"cb9c994c-89f2-4e5d-87e0-196d8a4aa0df","_cell_guid":"7fce52d3-4c28-47e4-9028-ed2ded767a13","trusted":true},"cell_type":"code","source":"N_FOLD = 10\nPOST_PROCESS = True\nparams = {\n    'models': [\n        #'roberta_base', \n        'roberta_base_noRawSenti', \n        'roberta_large',\n    ],\n    'batch_size': 4,\n    'workers': 1 if ON_KAGGLE else 8,\n    'max_len': 192, \n    'folds': list(x for x in range(N_FOLD)),\n    'limit': 0,\n}\n\npath_lib = {\n    'roberta_base': '../input/roberta-base/',\n    'roberta_base_noRawSenti': '../input/roberta-base-bs32-v2-0608',\n    'roberta_large': '../input/roberta-large-bs32-v2-0608',\n}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5a37afea-d775-4113-921a-c49cbf4904e3","_cell_guid":"635c973a-090a-48b3-a2cf-c709f812381d","trusted":true},"cell_type":"markdown","source":"## Prediction","execution_count":null},{"metadata":{"_uuid":"76d4f09e-074b-4e56-a8a9-439cd26e4df2","_cell_guid":"a90c98d2-9f42-4b06-8b6e-8a300ad060a6","trusted":true},"cell_type":"code","source":"def predict_II(model, data_loader, tokenizer):\n    start_probs, end_probs = [], []\n    with torch.no_grad():\n        for i, d in enumerate(tqdm.tqdm(data_loader, ascii=True)):\n            ids = d[\"ids\"]\n            token_type_ids = d[\"token_type_ids\"]\n            mask = d[\"mask\"]\n            sentiment = d[\"sentiment\"]\n            orig_selected = d[\"orig_selected\"]\n            orig_tweet = d[\"orig_tweet\"]\n            decode_selected = d[\"decode_selected\"]\n            raw_tweets = d[\"raw_tweet\"]\n            raw_selecteds = d[\"raw_selected_text\"]\n            text_span = d[\"text_span\"]\n            \n            ids = ids.cuda()\n            token_type_ids = token_type_ids.cuda()\n            mask = mask.cuda()\n            \n            start_idxs, end_idsx = [], []\n            outputs = model(ids=ids, mask=mask, token_type_ids=token_type_ids)\n            if len(outputs) == 2:\n                outputs_start, outputs_end = outputs\n                outputs_mask = None\n            elif len(outputs) == 3:\n                outputs_start, outputs_end, outputs_mask = outputs   \n            # probability\n            outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n            outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy() \n            \n            for px, tweet in enumerate(orig_tweet):\n                # char level probs\n                raw_tweet = raw_tweets[px]\n                span_start, span_end = text_span[0][px], text_span[1][px]\n                span_start, span_end = int(span_start), int(span_end)\n                token_ids = d[\"ids\"][px][span_start: span_end]\n                clean_text = \" \".join(raw_tweet.split())\n                _, prob_char = convrt_prob_char_level(clean_text, token_ids, outputs_start[px, span_start: span_end], tokenizer)\n                if len(prob_char) < 160: # padding\n                    prob_char += [0] * (160 - len(prob_char))\n                start_probs.append(prob_char)\n                _, prob_char = convrt_prob_char_level(clean_text, token_ids, outputs_end[px, span_start: span_end], tokenizer)\n                if len(prob_char) < 160: # padding\n                    prob_char += [0] * (160 - len(prob_char))\n                end_probs.append(prob_char)\n    start_probs = np.array(start_probs)\n    end_probs = np.array(end_probs)\n    return start_probs, end_probs","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bc95f75e-1f1a-49d7-b04c-8cee384ff65b","_cell_guid":"e3c54273-1e6d-4557-b417-058ee6b17cb1","trusted":true},"cell_type":"code","source":"# predict\nfor model_name in params['models']:\n    model, tokenizer = get_model(model_name)\n    if model_name.endswith(\"noRawSenti\"):\n        df_test_tmp = df_test.copy()\n        df_test_tmp['raw_sentiment'] = \"\"\n        tweet_dataset = TweetDataset(\n            df=df_test_tmp,\n            sentiment_weights=[1,1,1],\n            tokenizer=tokenizer,\n            mode='test',\n            lower_case=0,\n            max_len=params['max_len'],\n        )\n    else:\n        tweet_dataset = TweetDataset(\n            df=df_test,\n            sentiment_weights=[1,1,1],\n            tokenizer=tokenizer,\n            mode='test',\n            lower_case=0,\n            max_len=params['max_len'],\n        )      \n        \n    data_loader = DataLoader(\n        tweet_dataset,\n        batch_size=params['batch_size'],\n        num_workers=0,\n    )\n        \n    for i, fold in enumerate(params['folds']):\n        if model_name == \"roberta_base_noRawSenti\":\n            if fold < 5:\n                path = path_lib[model_name] + '-part2'\n            else:\n                path = path_lib[model_name]\n        elif model_name == \"roberta_large\":\n            if fold <= 2:\n                path = path_lib[model_name] + '-part1'\n            elif fold <= 5:\n                path = path_lib[model_name] + '-part2'\n            elif fold == 6:\n                path = path_lib[model_name] + '-part3-2'\n            elif fold == 7:\n                path = path_lib[model_name] + '-part3'\n            else:\n                path = path_lib[model_name] + '-part4'\n        else:\n            path = path_lib[model_name]\n        load_model(model, f\"{path}/best_jac_{fold}.pt\", multi2single=False)\n        model.cuda()\n        model.eval()\n        probs_start_pred, probs_end_pred = predict_II(model, data_loader, tokenizer)\n        if i == 0:\n            probs_start = probs_start_pred\n            probs_end = probs_end_pred\n        else:\n            probs_start += probs_start_pred\n            probs_end += probs_end_pred \n            \n    probs_start /= len(params['folds'])\n    probs_end /= len(params['folds'])\n\n    df_test['prob_start'] = probs_start.tolist()\n    df_test['prob_end'] = probs_end.tolist()\n    df_test.to_pickle(f'{model_name}.pkl')\n    np.save(f\"start_{model_name}.npy\", probs_start)\n    np.save(f\"end_{model_name}.npy\", probs_end)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"07dadec6-4db7-425f-b947-2aa18a074b84","_cell_guid":"9f10617a-41b9-44e5-92a7-ef01c64f2cc9","trusted":true},"cell_type":"markdown","source":"# Ensemble of different models","execution_count":null},{"metadata":{"_uuid":"1002ed7a-ebcd-4f85-9433-578b55f0f0f9","_cell_guid":"50f78ff6-af97-4ceb-b423-15bf68daf98c","trusted":true},"cell_type":"code","source":"# load .npy file from disk and ensemble char level probability\n\nmodel_names = [\n               #'roberta_base', \n               'roberta_base_noRawSenti',\n               'roberta_large',\n               'output_v10',\n               'output_v11',\n              ]\nfor i, model_name in enumerate(model_names):\n    prob_start_tmp = np.load(f'start_{model_name}.npy')\n    prob_end_tmp = np.load(f'end_{model_name}.npy')\n    if i == 0:\n        prob_start = prob_start_tmp\n        prob_end = prob_end_tmp\n    else:\n        prob_start += prob_start_tmp\n        prob_end += prob_end_tmp\n\nprob_start /= len(model_names)\nprob_end /= len(model_names)\n        \ndf_test['prob_start'] = prob_start.tolist()\ndf_test['prob_end'] = prob_end.tolist()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4648362a-e4a6-41f7-a110-7597685beefb","_cell_guid":"a85adf6a-9f3d-433e-a379-f36662ceb471","trusted":true},"cell_type":"markdown","source":"# Post processing","execution_count":null},{"metadata":{"_uuid":"84ddab4b-8433-41da-a7bd-211086a36735","_cell_guid":"308eb779-b0ac-4b92-ba93-1ff575608d70","trusted":true},"cell_type":"code","source":"def _get_pred_char(df, probs_start, probs_end):\n    df['start_idx'] = np.argmax(probs_start, axis=1)\n    df['end_idx'] = probs_end.shape[1] - np.argmax(probs_end[:, ::-1], axis=1) - 1\n    df['prob_start'] = probs_start.tolist()\n    df['prob_end'] = probs_end.tolist()\n    idxs = np.where(df.start_idx > df.end_idx)\n    \n    for idx in idxs[0]:\n        prob_start = df.prob_start.values[idx]\n        prob_end = df.prob_end.values[idx]\n        start_idx = df.start_idx.values[idx]\n        end_idx = df.end_idx.values[idx]\n        if prob_start[start_idx] > prob_end[end_idx] or end_idx == 0:\n            end_idx = len(prob_start) - np.argmax(prob_end[start_idx:][::-1]) - 1\n        else:\n            start_idx = np.argmax(prob_start[:end_idx])\n        df['start_idx'].iloc[idx] = start_idx     \n        df['end_idx'].iloc[idx] = end_idx    \n    #df.rename(columns={'selected_text': 'pred'}, inplace=True)\n    df['pred_char'] = df.apply(lambda x: x['text_clean'][x['start_idx']: x['end_idx']+1], axis=1)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ca1db0b4-097e-4e2a-9875-a0110c0d5e8a","_cell_guid":"d090b1bf-0720-4003-a444-d8d4cd1f8dc6","trusted":true},"cell_type":"code","source":"def post_neutral(df):\n    df['select_pt'] = df.apply(lambda x: len(x['pred_char'].strip())/len(x['text_clean']), axis=1).values\n\n    raw_sents = ['neutral', 'sadness', 'worry', 'happiness', 'love', 'enthusiasm']\n    mm = (df['sentiment'] == 'neutral') & (df['raw_sentiment'].isin(raw_sents))\n    mm = (mm |\\\n          ((df.select_pt > 0.85) & (df.sentiment.isin(['positive']))) |\\\n          ((df.select_pt > 0.85) & (df.sentiment.isin(['negative']))) |\\\n          ((df.select_pt < 0.2) & (df.sentiment.isin(['neutral']))))\n    \n    df['pred_exp'] = df['pred_char'].values\n    df['pred_exp'].loc[mm] = df['text_clean'].loc[mm].values\n    print(f\"# of modified samples: {np.sum(df['pred_exp'] != df['pred_char'])}\")\n    return df\n\ndef _post_shift_new(text, pred):\n    clean_text = \" \".join(text.split())\n    start_clean = clean_text.find(\" \".join(pred.split()))\n    \n    raw_pred = _convrt_back(text, pred, \"neutral\")\n    raw_pred = raw_pred.strip()\n    start = text.find(raw_pred)\n    end = start + len(raw_pred)\n    \n    extra_space = start - start_clean \n    \n    if start>extra_space and extra_space>0:\n        if extra_space==1:\n            if text[start-1] in [',','.','?','!'] and text[start-2]!=' ':\n                start -= 1\n        elif extra_space==2:\n            start -= extra_space\n            if text[end-1] in [',','.','!','?','*']:\n                end -= 1\n        else:\n            end -= (extra_space-2)\n            start -= extra_space\n    \n    pred = text[start:end]\n    if pred.count(\"'\") == 1:\n        if pred[0] == \"'\":\n            if text.find(pred) + len(pred) < len(text) and text[text.find(pred) + len(pred)] == \"'\":\n                pred += \"'\"\n        else:\n            if text.find(pred) - 1 >= 0 and text[text.find(pred) - 1] == \"'\":\n                pred = \"'\" + pred   \n                \n    return pred\n\ndef post_shift(df):\n    df['pred_final'] = df['pred_exp'].copy()\n    df['jac_text'] = df.apply(lambda x: jaccard(x['text'], x['pred_exp']), axis=1)\n    mask = (df.sentiment != 'neutral') & (df.start_idx != 0) & (df.jac_text != 1)\n    df['pred_final'].loc[mask] = df.apply(lambda x: _post_shift_new(x['text'], x['pred_exp']), axis=1).loc[mask].values\n    jac = df.apply(lambda x: jaccard(x['pred_exp'], x['pred_final']), axis=1)\n    print(f\"# of modified samples: {np.sum(jac != 1)}\")\n    return df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aeedec43-804d-404d-ab67-b8ec44ce1cab","_cell_guid":"90fbfcfa-aa99-404e-9e09-41c6e96c2732","trusted":true},"cell_type":"code","source":"# _get_pred_char bug ==> lb 717 (pred1)\n# POST_PROCESS bug ==> lb 0\nPOST_PROCESS=True\ntry:\n    df_test = _get_pred_char(df_test, np.array(df_test.prob_start.tolist()), np.array(df_test.prob_end.tolist()))\n    try:\n        if POST_PROCESS:\n            df_test = post_neutral(df_test)\n            df_test = post_shift(df_test)\n            df_test['selected_text'] = df_test['pred_final'].values\n        else:\n            df_test['selected_text'] = df_test['pred_char'].values\n    except:\n        df_test['selected_text'] = \" \"\nexcept:\n    df_test = pd.merge(df_test[['textID']], df_pred1, how='left', on='textID')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"46e35f0c-887e-4034-a4ce-22112ab6afdf","_cell_guid":"9952a025-077c-4d8f-9955-480135d632a6","trusted":true},"cell_type":"markdown","source":"# Save prediction","execution_count":null},{"metadata":{"_uuid":"c05f3dcd-070d-4111-8247-5472bc7abee4","_cell_guid":"9e27ebcf-1d2b-42ba-9d8d-fd830cf1acd4","trusted":true},"cell_type":"code","source":"df_test.to_csv(\"raw_prediction.csv\", index=False)\ndf_sub = df_test[['textID', 'selected_text']].copy()\ndf_sub.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ee82f25f-4cc8-4524-91b9-a4b642ad9904","_cell_guid":"d3e9d398-fcfd-4b6d-ad36-0737bb0767f8","trusted":true},"cell_type":"code","source":"df_sub.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f36c6310-9cfd-44f8-a0ae-a975ead9bb65","_cell_guid":"cabea972-e514-4b15-94e0-6ce8c67f9b84","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}