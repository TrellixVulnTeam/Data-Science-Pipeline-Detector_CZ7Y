{"cells":[{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Huggingface meets Fastai \n\n![](https://huggingface.co/landing/assets/transformers-docs/huggingface_logo.svg)\n![](https://docs.fast.ai/images/company_logo.png)"},{"metadata":{},"cell_type":"markdown","source":"This notebook shows training Bert model from `transformers` library  with `fastai` library interface. By doing so we get to use goodies like `lr_finder()`, `gradual_unfreezing`, `Callbacks`, `to_fp16()` and other customizations if necessary.\n\nSince finetuning requires loading pretrained models from the internet this notebook can't be submitted directly but required models or output of this notebook can be saved as a Kaggle dataset for further submission."},{"metadata":{"trusted":true},"cell_type":"code","source":"from fastai.core import *\nimport transformers; transformers.__version__","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"KAGGLE_WORKING = Path(\"/kaggle/working\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = Path(\"../input/tweet-sentiment-extraction/\")\ntrain_df = pd.read_csv(path/'train.csv')\ntest_df = pd.read_csv(path/'test.csv')\ntrain_df = train_df.dropna().reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### SQUAD Q/A Data Prep\n\nHere we are creating a SQUAD format dataset as we will leverage data prep utilities from `transformers` library. We could use SQUAD V1 or V2 for preparing data, but this dataset doesn't require `is_impossible` as it doesn't have any adversarial questions. Questions are coming from sentiment of the tweets; being either `positive` or `negative`. The idea has been taken from other kernels, so thanks!"},{"metadata":{"trusted":true},"cell_type":"code","source":"# an example for SQUAD json data format\nsquad_sample = {\n    \"version\": \"v2.0\",\n    \"data\": [\n        {\n            \"title\": \"Beyonc\\u00e9\",\n            \"paragraphs\": [\n                {\n                    \"qas\": [\n                        {\n                            \"question\": \"When did Beyonce start becoming popular?\",\n                            \"id\": \"56be85543aeaaa14008c9063\",\n                            \"answers\": [\n                                {\n                                    \"text\": \"in the late 1990s\",\n                                    \"answer_start\": 269\n                                }\n                            ],\n                            \"is_impossible\": False\n                        }\n                    ],\n                    \"context\": \"Beyonc\\u00e9 Giselle Knowles-Carter (/bi\\u02d0\\u02c8j\\u0252nse\\u026a/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyonc\\u00e9's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \\\"Crazy in Love\\\" and \\\"Baby Boy\\\".\"\n                }\n            ]\n        }\n    ]\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_answer_start(context, answer):\n    len_a = len(answer)\n    for i, _ in enumerate(context):\n        if context[i:i+len_a] == answer: return i\n    raise Exception(\"No overlapping segment found\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_qas_dict(text_id, context, answer, question):\n    qas_dict = {}\n    qas_dict['question'] = question\n    qas_dict['id'] = text_id\n    qas_dict['is_impossible'] = False\n    \n    if answer is None: \n        qas_dict['answers'] = []\n    else: \n        answer_start = get_answer_start(context, answer)\n        qas_dict['answers'] = [{\"text\":answer, \"answer_start\":answer_start}]\n    return qas_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_squad_from_df(df):\n    data_dicts = []\n    for _, row in df.iterrows():\n        text_id = row['textID']\n        context = row['text']\n        answer =  row['selected_text'] if 'selected_text' in row else None\n        question = row['sentiment']\n\n        qas_dict = generate_qas_dict(text_id, context, answer, question)\n        data_dict = {\"paragraphs\" : [{\"qas\" : [qas_dict], \"context\":context}]}\n        data_dict['title'] = text_id\n        data_dicts.append(data_dict)\n\n    return {\"version\": \"v2.0\", \"data\": data_dicts}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_no_neutral_df = train_df[train_df.sentiment != 'neutral'].reset_index(drop=True)\n# test_no_neutral_df = test_df[test_df.sentiment != 'neutral'].reset_index(drop=True)\n# train_no_neutral_df.shape, test_no_neutral_df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create KFold Validation\n\nFor training we will be using only positive and negative tweets, as neutral tweets have a score of `~0.97` when submitted as it is. Also, positive and negative tweets are balanced so I am here using a vanilla KFold cross validation approach. Trained models for all folds can be used for further ensembling if needed."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create 5 fold trn-val splits with only positive/negative tweets\nos.makedirs(\"squad_data\", exist_ok=True)\nkfold = KFold(5, shuffle=True, random_state=42)\nfold_idxs = list(kfold.split(train_df))\n\nfor i, (trn_idx, val_idx) in enumerate(fold_idxs[:1]):\n    _trn_fold_df = train_df.iloc[trn_idx]\n    _val_fold_df = train_df.iloc[val_idx]\n    train_squad_data = create_squad_from_df(_trn_fold_df)\n    valid_squad_data = create_squad_from_df(_val_fold_df)\n    with open(f\"squad_data/train_squad_data_{i}.json\", \"w\") as f: f.write(json.dumps(train_squad_data))\n    with open(f\"squad_data/valid_squad_data_{i}.json\", \"w\") as f: f.write(json.dumps(valid_squad_data))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # create for test \ntest_squad_data =  create_squad_from_df(test_df)\nwith open(\"squad_data/test_squad_data.json\", \"w\") as f: f.write(json.dumps(test_squad_data))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data\n\nSince we are training a BERT model we will use bert tokenizer for `bert-base-cased`. You can use `Auto*` classes from `transformers` library to load and train with any model available. For demonstration I am training with `foldnum=0`."},{"metadata":{"trusted":true},"cell_type":"code","source":"from fastai.text import *\nfrom transformers import (AutoTokenizer, AutoConfig, AutoModel, AutoModelForQuestionAnswering,\n                         RobertaTokenizer)\nfrom transformers.data.processors.squad import (SquadResult, SquadV1Processor, SquadV2Processor,\n                                                SquadExample, squad_convert_examples_to_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PRETRAINED_TYPE = 'roberta-base'\n# PRETRAINED_TYPE = 'distilbert-base-uncased'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PRETRAINED_TOK_PATH = Path(\"/kaggle/input/tse-fastai-squad-bert-pretrained/roberta-base-tokenizer/\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shutil.copytree(PRETRAINED_TOK_PATH, KAGGLE_WORKING/\"tokenizer_dir\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PRETRAINED_TOK_PATH = KAGGLE_WORKING/\"tokenizer_dir\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shutil.copyfile(str(PRETRAINED_TOK_PATH/\"tokenizer_config.json\"), str(PRETRAINED_TOK_PATH/\"config.json\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PRETRAINED_TOK_PATH.ls()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"processor = SquadV2Processor()\ntokenizer = RobertaTokenizer.from_pretrained(str(PRETRAINED_TOK_PATH))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_seq_length = 192\nmax_query_length = 10\n\ndef get_dataset(examples, is_training):\n    return squad_convert_examples_to_features(\n        examples=examples,\n        tokenizer=tokenizer,\n        doc_stride=200,\n        max_seq_length=max_seq_length,\n        max_query_length=10,\n        is_training=is_training,\n        return_dataset=\"pt\",\n        threads=defaults.cpus,\n    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SQUAD_Dataset(Dataset):\n    def __init__(self, dataset_tensors, examples, features, is_training=True):\n        self.dataset_tensors = dataset_tensors\n        self.examples = examples\n        self.features = features\n        self.is_training = is_training\n        \n        \n    def __getitem__(self, idx):\n        'fastai requires (xb, yb) to return'\n        'AutoModel handles loss computation in forward hence yb will be None'\n        input_ids = self.dataset_tensors[0][idx]\n        attention_mask = self.dataset_tensors[1][idx]\n        token_type_ids = self.dataset_tensors[2][idx]\n        xb = (input_ids, attention_mask, token_type_ids)\n        if self.is_training: \n            start_positions = self.dataset_tensors[3][idx]\n            end_positions = self.dataset_tensors[4][idx]\n            yb = [start_positions, end_positions]\n        else:\n            yb = 0\n        return xb, yb\n    \n    def __len__(self): return len(self.dataset_tensors[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PRETRAINED_PATH = Path(\"/kaggle/input/tse-fastai-squad-bert-pretrained/\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_fold_ds(foldnum):\n    data_dir = \"/kaggle/working/squad_data\"\n    train_filename = f\"train_squad_data_{foldnum}.json\"\n    valid_filename = f\"valid_squad_data_{foldnum}.json\"\n    test_filename = \"test_squad_data.json\"\n    \n    # tokenize\n    train_examples = processor.get_train_examples(data_dir, train_filename)\n    valid_examples = processor.get_train_examples(data_dir, valid_filename)\n    test_examples = processor.get_dev_examples(data_dir, test_filename)\n\n    # create tensor dataset\n    train_features, train_dataset = get_dataset(train_examples, True)\n    valid_features, valid_dataset = get_dataset(valid_examples, True)\n    test_features, test_dataset = get_dataset(test_examples, False)\n    \n    # create pytorch dataset\n    train_ds = SQUAD_Dataset(train_dataset.tensors, train_examples, train_features)\n    valid_ds = SQUAD_Dataset(valid_dataset.tensors, valid_examples, valid_features)\n    test_ds = SQUAD_Dataset(test_dataset.tensors, test_examples, test_features, False)\n    \n    return train_ds, valid_ds, test_ds    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model\n\nHere we have `ModelWrapper` to make model from `transformers` to work with `fastai` 's `Learner` class. Also, loss is computed within the model, for this we will use a `DummyLoss` to work with `Learner.fit()`"},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import AutoModelForPreTraining, RobertaModel, BertModel","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# MODEL_TYPE = 'distilbert'\nMODEL_TYPE = 'roberta'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class QAHead(Module): \n    def __init__(self, p=0.5):    \n        self.d0 = nn.Dropout(p)\n        self.l0 = nn.Linear(768, 256)\n        self.d1 = nn.Dropout(p)\n        self.l1 = nn.Linear(256, 2)        \n    def forward(self, x):\n        return self.l1(self.d1(self.l0(self.d0(x))))\n    \nclass TSEModel(Module):\n    def __init__(self, model): \n        self.sequence_model = model\n        self.head = QAHead()\n        \n    def forward(self, *xargs):\n        inp = {}\n        inp[\"input_ids\"] = xargs[0]\n        inp[\"attention_mask\"] = xargs[1]\n        inp[\"token_type_ids\"] = xargs[2]\n        if MODEL_TYPE in [\"xlm\", \"roberta\", \"distilbert\", \"camembert\"]: del inp[\"token_type_ids\"]\n    \n        sequence_output, _ = self.sequence_model(**inp)\n        start_logits, end_logits = self.head(sequence_output).split(1, dim=-1)\n        return (start_logits.squeeze(-1), end_logits.squeeze(-1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CELoss(Module):\n    \"single backward by concatenating both start and logits with correct targets\"\n    def __init__(self): self.loss_fn = nn.CrossEntropyLoss()\n    def forward(self, inputs, start_targets, end_targets):\n        start_logits, end_logits = inputs\n        \n        logits = torch.cat([start_logits, end_logits]).contiguous()\n        \n        targets = torch.cat([start_targets, end_targets]).contiguous()\n        \n        return self.loss_fn(logits, targets)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we define parameter group split points for `gradual unfreezing`. Idea is coming from [ULMFIT paper](https://arxiv.org/pdf/1801.06146.pdf)."},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_split_func(m): \n    n = len(m.sequence_model.encoder.layer) - 5\n    return (m.sequence_model.encoder.layer[n], m.head)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will choose start and end indexes for predictions such that sum of logits is maximum while satisfying `start_idx <= end_idx`"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_best_start_end_idxs(_start_logits, _end_logits):\n    best_logit = -1000\n    best_idxs = None\n    for start_idx, start_logit in enumerate(_start_logits):\n        for end_idx, end_logit in enumerate(_end_logits[start_idx:]):\n            logit_sum = (start_logit + end_logit).item()\n            if logit_sum > best_logit:\n                best_logit = logit_sum\n                best_idxs = (start_idx, start_idx+end_idx)\n    return best_idxs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_best_start_end_idxs_v2(_start_logits, _end_logits):\n    start_idx, end_idx = torch.argmax(_start_logits).item(), torch.argmax(_end_logits).item()\n    if start_idx > end_idx: end_idx = start_idx\n    return (start_idx, end_idx)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For each epoch we will calculate `JaccardScore` on validation set"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_answer_by_char_offset(context_text, char_to_word_offset, start_idx, end_idx, token_to_orig_map):\n    \n    start_offset_id = token_to_orig_map[start_idx] \n    end_offset_id = token_to_orig_map[end_idx]\n    \n    \n    return \"\".join([ct for ct, char_offs in zip(context_text, char_to_word_offset) if \n                                     (char_offs >= start_offset_id) & (char_offs <= end_offset_id)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class JaccardScore(Callback):\n    \"Stores predictions and targets to perform calculations on epoch end.\"\n    def __init__(self, valid_ds): \n        self.valid_ds = valid_ds\n        self.token_to_orig_map = [o.token_to_orig_map for o in valid_ds.features]\n        self.context_text = [o.context_text for o in valid_ds.examples]\n        self.answer_text = [o.answer_text for o in valid_ds.examples]\n        self.char_to_word_offset = [o.char_to_word_offset for o in valid_ds.examples]\n\n        self.offset_shift = min(self.token_to_orig_map[0].keys())\n        \n        \n    def on_epoch_begin(self, **kwargs):\n        self.jaccard_scores = []  \n        self.valid_ds_idx = 0\n        \n        \n    def on_batch_end(self, last_input:Tensor, last_output:Tensor, last_target:Tensor, **kwargs):\n        \n#         import pdb;pdb.set_trace()\n        \n        input_ids = last_input[0]\n        attention_masks = last_input[1].bool()\n        token_type_ids = last_input[2].bool()\n\n        start_logits, end_logits = last_output\n        \n        # mask select only context part\n        for i in range(len(input_ids)):\n            \n            if MODEL_TYPE == \"roberta\": \n                \n                _input_ids = input_ids[i].masked_select(attention_masks[i])\n                _start_logits = start_logits[i].masked_select(attention_masks[i])[4:-1] # ignore first 4 (non context) and last special token:2\n                _end_logits = end_logits[i].masked_select(attention_masks[i])[4:-1] # ignore first 4 (non context) and last special token:2\n                start_idx, end_idx = get_best_start_end_idxs(_start_logits, _end_logits)\n                start_idx, end_idx = start_idx + self.offset_shift, end_idx + self.offset_shift\n\n            \n            context_text = self.context_text[self.valid_ds_idx]\n            char_to_word_offset = self.char_to_word_offset[self.valid_ds_idx]\n            token_to_orig_map = self.token_to_orig_map[self.valid_ds_idx]\n            \n            _answer =  get_answer_by_char_offset(context_text, char_to_word_offset, start_idx, end_idx, token_to_orig_map)\n            _answer_text = self.answer_text[self.valid_ds_idx]\n            \n            score = jaccard(_answer, _answer_text)\n            self.jaccard_scores.append(score)\n\n            self.valid_ds_idx += 1\n            \n    def on_epoch_end(self, last_metrics, **kwargs):        \n        res = np.mean(self.jaccard_scores)\n        return add_metrics(last_metrics, res)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we are initialazing model, splitting parameter groups and putting model callback for mixed precision training. `bs=128` is a good choice for the GPU memory we have at hand."},{"metadata":{"trusted":true},"cell_type":"code","source":"from fastai.callbacks import *\n\ndef new_on_train_begin(self, **kwargs:Any)->None:\n    \"Initializes the best value.\"\n    if not hasattr(self, 'best'):\n        self.best = float('inf') if self.operator == np.less else -float('inf')\n\nSaveModelCallback.on_train_begin = new_on_train_begin","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Predict"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_ds, valid_ds, test_ds = get_fold_ds(0)\ndata = DataBunch.create(train_ds, valid_ds, test_ds, path=\".\", bs=128)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"config = AutoConfig.from_pretrained(PRETRAINED_PATH/\"roberta-base-config\")\nmodel = AutoModel.from_config(config)\ntse_model = TSEModel(model)\nlearner = Learner(data, tse_model, loss_func=CELoss(), path=PRETRAINED_PATH, model_dir=f\".\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"token_to_orig_map = [o.token_to_orig_map for o in test_ds.features]\ncontext_text = [o.context_text for o in test_ds.examples]\nchar_to_word_offset = [o.char_to_word_offset for o in test_ds.examples]\noffset_shift = min(token_to_orig_map[0].keys())\ntest_ds_idx = 0 \n\nMODEL_NAME = \"roberta\"\nfinal_answers = []\n\nwith torch.no_grad():        \n    for xb,yb in tqdm(learner.data.test_dl):\n        model0 = learner.load(f'models_fold_0/{MODEL_NAME}-qa-finetune').model.eval()\n        start_logits0, end_logits0 = to_cpu(model0(*xb))\n        start_logits0, end_logits0 = start_logits0.float(), end_logits0.float()\n        \n        model1 = learner.load(f'models_fold_1/{MODEL_NAME}-qa-finetune').model.eval()\n        start_logits1, end_logits1 = to_cpu(model1(*xb))\n        start_logits1, end_logits1 = start_logits1.float(), end_logits1.float()\n        \n        model2 = learner.load(f'models_fold_2/{MODEL_NAME}-qa-finetune').model.eval()        \n        start_logits2, end_logits2 = to_cpu(model2(*xb))\n        start_logits2, end_logits2 = start_logits2.float(), end_logits2.float()\n        \n        model3 = learner.load(f'models_fold_3/{MODEL_NAME}-qa-finetune').model.eval()\n        start_logits3, end_logits3 = to_cpu(model3(*xb))\n        start_logits3, end_logits3 = start_logits3.float(), end_logits3.float()\n        \n        model4 = learner.load(f'models_fold_4/{MODEL_NAME}-qa-finetune').model.eval()\n        start_logits4, end_logits4 = to_cpu(model4(*xb))\n        start_logits4, end_logits4 = start_logits4.float(), end_logits4.float()\n        \n        \n        input_ids = to_cpu(xb[0])\n        attention_masks = to_cpu(xb[1].bool())\n        token_type_ids = to_cpu(xb[2].bool())\n        \n        start_logits = (start_logits0 + start_logits1 + start_logits2 + start_logits3 + start_logits4) / 5\n        end_logits = (end_logits0 + end_logits1 + end_logits2 + end_logits3 + end_logits4) / 5\n        \n        # mask select only context part\n        for i in range(len(input_ids)):\n\n            _input_ids = input_ids[i].masked_select(attention_masks[i])\n            _start_logits = start_logits[i].masked_select(attention_masks[i])[4:-1] # ignore first 4 (non context) and last special token:2\n            _end_logits = end_logits[i].masked_select(attention_masks[i])[4:-1] # ignore first 4 (non context) and last special token:2\n            start_idx, end_idx = get_best_start_end_idxs(_start_logits, _end_logits)\n            start_idx, end_idx = start_idx + offset_shift, end_idx + offset_shift\n\n            _context_text = context_text[test_ds_idx]\n            _char_to_word_offset = char_to_word_offset[test_ds_idx]\n            _token_to_orig_map = token_to_orig_map[test_ds_idx]\n            \n            predicted_answer =  get_answer_by_char_offset(_context_text, _char_to_word_offset, start_idx, end_idx, _token_to_orig_map)\n            final_answers.append(predicted_answer)\n            \n            test_ds_idx += 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Submit"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df['selected_text'] = final_answers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predict same text if word count < 3\ntest_df['selected_text'] = test_df.apply(lambda o: o['text'] if len(o['text']) < 3 else o['selected_text'], 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# keep neutral as it is or not?\ntest_df['selected_text'] = test_df.apply(lambda o: o['text'] if o['sentiment'] == 'neutral' else o['selected_text'], 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subdf = test_df[['textID', 'selected_text']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subdf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subdf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## this shouldn't be necessary since evaluation code in Kaggle is fixed\n# def f(selected): return \" \".join(set(selected.lower().split()))\n# subdf.selected_text = subdf.selected_text.map(f)\nsubdf.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### fin"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Hope this is helpful to someone :)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}