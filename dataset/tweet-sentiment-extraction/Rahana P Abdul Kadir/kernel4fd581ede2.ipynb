{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Import libraries","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize\nimport spacy\nfrom tqdm import tqdm\n\nimport random\nfrom spacy.util import compounding\nfrom spacy.util import minibatch\nimport os\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Import the data file","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndf = pd.read_csv(\"/kaggle/input/tweet-sentiment-extraction/train.csv\")\ndf_test=pd.read_csv(\"/kaggle/input/tweet-sentiment-extraction/test.csv\")\ndf_submission=pd.read_csv(\"/kaggle/input/tweet-sentiment-extraction/sample_submission.csv\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking Null Values\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#drop null values\ndfnew=df.dropna()\ndfnew","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfnew.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfnew['cleaned_tweet']=dfnew['text'].replace(r'\\'|\\\"|\\,|\\.|\\?|\\+|\\-|\\/|\\=|\\(|\\)|\\n|\"','',regex=True)\ndfnew['cleaned_tweet']=dfnew['cleaned_tweet'].replace(\" \",\" \")\n\n\nwords_remove = [\"ax\",\"i\",\"you\",\"edu\",\"s\",\"t\",\"m\",\"subject\",\"can\",\"lines\",\"re\",\"what\", \"there\",\"all\",\"we\",\n                \"one\",\"the\",\"a\",\"an\",\"of\",\"or\",\"in\",\"for\",\"by\",\"on\",\"but\",\"is\",\"in\",\"a\",\"not\",\"with\",\"as\",\n                \"was\",\"if\",\"they\",\"are\",\"this\",\"and\",\"it\",\"have\",\"has\",\"from\",\"at\",\"my\",\"be\",\"by\",\"not\",\"that\",\"to\",\n                \"from\",\"com\",\"org\",\"like\",\"likes\",\"so\",\"said\",\"from\",\"what\",\"told\",\"over\",\"more\",\"other\",\n                \"have\",\"last\",\"with\",\"this\",\"that\",\"such\",\"when\",\"been\",\"says\",\"will\",\"also\",\"where\",\"why\",\n                \"would\",\"today\", \"in\", \"on\", \"you\", \"r\", \"d\", \"u\", \"hw\",\"wat\", \"oly\", \"s\", \"b\", \"ht\", \n                \"rt\", \"p\",\"the\",\"th\", \"n\", \"was\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfnew","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cleantext(dfnew,words_to_remove=words_remove):\n    # remove emoticons form the tweets\n    dfnew['cleaned_tweet'] = dfnew['cleaned_tweet'].replace(r'<ed>','', regex = True)\n    dfnew['cleaned_tweet'] = dfnew['cleaned_tweet'].replace(r'\\B<U+.*>|<U+.*>\\B|<U+.*>','', regex = True)\n    \n    # convert tweets to lowercase\n    dfnew['cleaned_tweet'] = dfnew['cleaned_tweet'].str.lower()\n    \n    #remove user mentions\n    dfnew['cleaned_tweet'] = dfnew['cleaned_tweet'].replace(r'^(@\\w+)',\"\", regex=True)\n    \n    #remove 'rt' in the beginning\n    dfnew['cleaned_tweet'] = dfnew['cleaned_tweet'].replace(r'^(rt @)',\"\", regex=True)\n    \n    #remove_symbols\n    dfnew['cleaned_tweet'] = dfnew['cleaned_tweet'].replace(r'[^a-zA-Z0-9]', \" \", regex=True)\n\n    #remove punctuations \n    dfnew['cleaned_tweet'] = dfnew['cleaned_tweet'].replace(r'[[]!\"#$%\\'()\\*+,-./:;<=>?^_`{|}]+',\"\", regex = True)\n\n    #remove_URL(x):\n    dfnew['cleaned_tweet'] = dfnew['cleaned_tweet'].replace(r'https.*$', \"\", regex = True)\n\n    #remove 'amp' in the text\n    dfnew['cleaned_tweet'] = dfnew['cleaned_tweet'].replace(r'amp',\"\", regex = True)\n    \n    #remove words of length 1 or 2 \n    dfnew['cleaned_tweet'] = dfnew['cleaned_tweet'].replace(r'\\b[a-zA-Z]{1,2}\\b','', regex=True)\n\n    #remove extra spaces in the tweet\n    dfnew['cleaned_tweet'] = dfnew['cleaned_tweet'].replace(r'^\\s+|\\s+$',\" \", regex=True)\n    \n    #remove stopwords and words_to_remove\n    stop_words = set(stopwords.words('english'))\n    mystopwords = [stop_words, \"via\", words_to_remove]\n    \n    dfnew['fully_cleaned_tweet'] = dfnew['cleaned_tweet'].apply(lambda x: ' '.join([word for word in x.split() if word not in mystopwords]))\n    \n\n    return dfnew\n    \n#get the processed tweets\ndfclean = cleantext(dfnew)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfclean.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfclean['tokenized_tweet'] = dfclean['fully_cleaned_tweet'].apply(word_tokenize)\ndfclean.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#if a word has a digit, remove that word\ndfclean['tokenized_tweet'] = dfclean['tokenized_tweet'].apply(lambda x: [y for y in x if not any(c.isdigit() for c in y)])\ndfclean.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfclean['no_words_ST']=dfclean['selected_text'].apply(lambda x:len(str(x).split()))\ndfclean['no_words_text']=dfclean['text'].apply(lambda x:len(str(x).split()))\ndfclean['diff_words']=dfclean['no_words_text'] - dfclean['no_words_ST']\ndfclean.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set values for various parameters\nnum_features = 100    # Word vector dimensionality                      \nmin_word_count = 1   # Minimum word count                        \nnum_threads = 4       # Number of threads to run in parallel\ncontext = 10          # Context window size   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialize and train the model (this will take some time)\nfrom gensim.models import word2vec\nprint(\"Training model...\")\nmodel = word2vec.Word2Vec(dfclean['tokenized_tweet'], workers=num_threads, \\\n            size=num_features, min_count = min_word_count, \\\n            window = context)\n# If you don't plan to train the model any further, calling \n# init_sims will make the model much more memory-efficient.\nmodel.init_sims(replace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find vector corresponding to each tweet\nimport numpy as np\nvocab = list(model.wv.vocab)\ndef sentence_vector(sentence, model):\n    nwords = 0\n    featureV = np.zeros(100, dtype=\"float32\")\n    for word in sentence:\n        if word not in vocab:\n            continue\n        featureV = np.add(featureV, model[word])\n        nwords = nwords + 1\n    if nwords > 0: \n        featureV = np.divide(featureV, nwords)\n    return featureV\n\ntweet_vector = dfclean['tokenized_tweet'].apply(lambda x: sentence_vector(x, model))  \n\ntweet_vector = tweet_vector.apply(pd.Series)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Tweet vector should vary from 0 to 1 (normalize the vector)\nfor x in range(len(tweet_vector)):\n    x_min = tweet_vector.iloc[x].min()\n    x_max = tweet_vector.iloc[x].max()\n    X  = tweet_vector.iloc[x]\n    i = 0\n    if (x_max - x_min) == 0:\n        for y in X:\n            tweet_vector.iloc[x][i] = (1/len(tweet_vector.iloc[x]))\n            i = i + 1\n    else:\n        for y in X:\n            tweet_vector.iloc[x][i] = ((y - x_min)/(x_max - x_min))\n            i = i + 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweet_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Cluster the narratives = opinions + expressions\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score, silhouette_samples\n\nrange_n_clusters = [4, 5, 6, 7, 8, 9, 10, 11]\nX = tweet_vector\nn_best_clusters = 0\nsilhouette_best = 0\nfor n_clusters in range_n_clusters:\n    \n    # Initialize the clusterer with n_clusters value and a random generator\n    # seed of 10 for reproducibility.\n    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n    cluster_labels = clusterer.fit_predict(X)\n\n    # The silhouette_score gives the average value for all the samples.\n    # This gives a perspective into the density and separation of the formed\n    # clusters\n    silhouette_avg = silhouette_score(X, cluster_labels)\n                                      #, sample_size = 5000)\n    print(\"For clusters =\", n_clusters,\n          \"The average silhouette_score is :\", silhouette_avg)\n    \n    if silhouette_avg > silhouette_best:\n        silhouette_best = silhouette_avg\n        n_best_clusters = n_clusters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_best_clusters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clusterS = KMeans(n_clusters= n_best_clusters , random_state=10)\ncluster_labels = clusterS.fit_predict(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Array of tweets, the corresponding cluster number, sentiment\nfinaldf = pd.DataFrame({'cl_num': cluster_labels,'fully_cleaned_tweet': dfclean['fully_cleaned_tweet'], 'cleaned_tweet': dfclean['cleaned_tweet'], 'tweet': dfclean['text'],'sentiment': dfclean['sentiment']})\nfinaldf = finaldf.sort_values(by=['cl_num'])\nfinaldf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfclean['cl_num'] = cluster_labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfOrdered = pd.DataFrame(dfclean)\n\n#Compute how many times a tweet has been 'retweeted' - that is, how many rows in dfOrdered are identical\ndfOrdered['tokenized_tweet'] = dfOrdered['tokenized_tweet'].apply(tuple)\ndfUnique = dfOrdered.groupby(['text', 'selected_text','cleaned_tweet', 'fully_cleaned_tweet', 'sentiment','tokenized_tweet', 'cl_num']).size().reset_index(name=\"freq\")\ndfUnique = dfUnique.sort_values(by=['cl_num'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfUnique['tokenized_tweet'] = dfUnique['tokenized_tweet'].apply(list)\ndfOrdered['tokenized_tweet'] = dfOrdered['tokenized_tweet'].apply(list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Discard the clusters with poor silhoutte score\n# Compute the silhouette scores for each sample\nsample_silhouette_values = silhouette_samples(X, cluster_labels)\n\npoor_cluster_indices = []\navg_cluster_sil_score = []\n\nfor i in range(n_best_clusters):\n# Aggregate the silhouette scores for samples belonging to\n# cluster i, and sort them\n        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n        avgscore = (np.mean(ith_cluster_silhouette_values))   #average silhouette score for each cluster\n        avg_cluster_sil_score = np.append(avg_cluster_sil_score, avgscore)\n        print('Cluster',i, ':', avgscore)\n        if avgscore < 0.2:\n            poor_cluster_indices = np.append(poor_cluster_indices, i)\n            \n        ith_cluster_silhouette_values.sort()\n        size_cluster_i = ith_cluster_silhouette_values.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#remove those rows where cluster value match poor_cluster_indices \navg_cluster_sil_score_final = []\ncluster_name = np.unique(dfOrdered['cl_num'])\n\nif (len(poor_cluster_indices)!=0):\n    n_final_clusters = n_best_clusters - len(poor_cluster_indices)\n    for i in poor_cluster_indices:\n        dfUnique = dfUnique[dfUnique['cl_num'] != i]\n    for j in cluster_name:\n        if j not in poor_cluster_indices:    \n            avg_cluster_sil_score_final = np.append(avg_cluster_sil_score_final, avg_cluster_sil_score[j])\n            \n    cluster_name = np.unique(dfUnique['cl_num'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfUnique['cl_num'] = abs(dfUnique['cl_num'])\ndfUnique = dfUnique.sort_values(by=['cl_num'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfUnique.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def save_model(output_dir, nlp, new_model):\n    ''' This Function Saves model to \n    given output directory'''\n    \n    output_dir = f'./working/{output_dir}'\n    if output_dir is not None:        \n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n        nlp.meta[\"name\"] = new_model\n        nlp.to_disk(output_dir)\n        print(\"Saved model :\", output_dir)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Returns Model output path\ndef model_out_path(sentiment):\n    out_path = None\n    if sentiment == 'positive':\n        out_path = 'models/model_positive'\n    elif sentiment == 'negative':\n        out_path = 'models/model_negative'\n    return out_path","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Load the model, set up the pipeline and train the entity recognizer.\n    \ndef train(train_data, output_dir, n_iter=20, model=None):\n  \n    \n    if model is not None:\n        nlp = spacy.load(output_dir)  # load existing spaCy model\n        print(\"Loaded model '%s'\" % model)\n    else:\n        nlp = spacy.blank(\"en\")  # create blank Language class\n        print(\"Created blank 'Named-Entity' model\")\n    \n    # create the built-in pipeline components and add them to the pipeline\n    \n    if \"ner\" not in nlp.pipe_names:\n        ner = nlp.create_pipe(\"ner\")\n        nlp.add_pipe(ner, last=True)\n    # otherwise, get it so we can add labels\n    else:\n        ner = nlp.get_pipe(\"ner\")\n    \n    # add labels\n    for _, annotations in train_data:\n        for entity in annotations.get(\"entities\"):\n            ner.add_label(entity[2])\n\n    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n    with nlp.disable_pipes(*other_pipes):  \n       \n        if model is None:\n            nlp.begin_training()\n        else:\n            nlp.resume_training()\n\n\n        for itn in tqdm(range(n_iter)):\n            random.shuffle(train_data)\n            batches = minibatch(train_data, size=compounding(4.0, 500.0, 1.0001))    \n            losses = {}\n            for batch in batches:\n                texts, annotations = zip(*batch)\n                nlp.update(texts,annotations,drop=0.5,losses=losses)\n            print(\"Losses\", losses)\n    save_model(output_dir, nlp, 'str-ner')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training models for positive and negative tweets\nsentiment = 'positive'\n\n#train_data = train_data(sentiment)\ntrain_data = []\nfor index, row in dfUnique.iterrows():\n    if row.sentiment == sentiment:\n        selected_text = row.selected_text\n        text = row.text\n        start = text.find(selected_text)\n        end = start + len(selected_text)\n        train_data.append((text, {\"entities\": [[start, end, 'selected_text']]}))\nmodel_path = model_out_path(sentiment)\n\ntrain(train_data, model_path, n_iter=5, model=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentiment = 'negative'\n\n#train_data = train_data(sentiment)\ntrain_data = []\nfor index, row in dfUnique.iterrows():\n    if row.sentiment == sentiment:\n        selected_text = row.selected_text\n        text = row.text\n        start = text.find(selected_text)\n        end = start + len(selected_text)\n        train_data.append((text, {\"entities\": [[start, end, 'selected_text']]}))\nmodel_path = model_out_path(sentiment)\n\ntrain(train_data, model_path, n_iter=5, model=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predicting with trained model\n\ndef entity_predict(text, model):\n    doc = model(text)\n    ent_array = []\n    for ent in doc.ents:\n        start = text.find(ent.text)\n        end = start + len(ent.text)\n        new_int = [start, end, ent.label_]\n        if new_int not in ent_array:\n            ent_array.append([start, end, ent.label_])\n    select_text = text[ent_array[0][0]: ent_array[0][1]] if len(ent_array) > 0 else text\n    return select_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"select_texts = []\nbase_path = './working/models/'\n\nif base_path is not None:\n    print(\"Loading: \", base_path)\n    model_positive = spacy.load(base_path + 'model_positive')\n    model_negative = spacy.load(base_path + 'model_negative')\n        \n    for index, row in df_test.iterrows():\n        text = row.text\n        output_str = \"\"\n        if row.sentiment == 'neutral' or len(text.split()) <= 2:\n            select_texts.append(text)\n        elif row.sentiment == 'positive':\n            select_texts.append(entity_predict(text, model_positive))\n        else:\n            select_texts.append(entity_predict(text, model_negative))\n        \ndf_test['selected_text'] = select_texts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_submission['selected_text'] = df_test['selected_text']\ndf_submission.to_csv(\"./working/submission.csv\", index=False)\ndisplay(df_submission.head(10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}