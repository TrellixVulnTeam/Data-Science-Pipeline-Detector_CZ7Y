{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Load  data and libraries","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd, numpy as np\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom sklearn.model_selection import StratifiedKFold\nfrom transformers import *\nimport tokenizers\nprint('TF version',tf.__version__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_train():\n    train=pd.read_csv('../input/tweet-sentiment-extraction/train.csv')\n    train['text']=train['text'].str.lower().astype(str)\n    train['selected_text']=train['selected_text'].str.lower().astype(str)\n    return train\n\ndef read_test():\n    test=pd.read_csv('../input/tweet-sentiment-extraction/test.csv')\n    test['text']=test['text'].str.lower().astype(str)\n    return test\n\ndef read_submission():\n    test=pd.read_csv('../input/tweet-sentiment-extraction/sample_submission.csv')\n    return test\n    \ntrain_df = read_train()\ntest_df = read_test()\nsubmission_df = read_submission()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t = train_df[:27000]\nv = train_df[27000:]\n\ntrain_df = t\nvalidation_df = v\n\nvalidation_df.reset_index(inplace=True,drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def jaccard(str1, str2): \n    a = set(str(str1).lower().split()) \n    b = set(str(str2).lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data preproccesing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN = 100\nPATH = '../input/tf-roberta/'\ntokenizer = tokenizers.ByteLevelBPETokenizer(\n    vocab_file=PATH+'vocab-roberta-base.json', \n    merges_file=PATH+'merges-roberta-base.txt', \n    lowercase=True,\n    add_prefix_space=True\n)\nsentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create train set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ct = train_df.shape[0]\ninput_ids = np.ones((ct,MAX_LEN),dtype='int32')\nattention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\ntoken_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\nstart_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\nend_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n\nfor k in range(train_df.shape[0]):\n    \n    # FIND OVERLAP\n    text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n    text2 = \" \".join(train_df.loc[k,'selected_text'].split())\n    idx = text1.find(text2)\n    chars = np.zeros((len(text1)))\n    chars[idx:idx+len(text2)]=1\n    if text1[idx-1]==' ': chars[idx-1] = 1 \n    enc = tokenizer.encode(text1) \n        \n    # ID_OFFSETS\n    offsets = []; idx=0\n    for t in enc.ids:\n        w = tokenizer.decode([t])\n        offsets.append((idx,idx+len(w)))\n        idx += len(w)\n    \n    # START END TOKENS\n    toks = []\n    for i,(a,b) in enumerate(offsets):\n        sm = np.sum(chars[a:b])\n        if sm>0: toks.append(i) \n        \n    s_tok = sentiment_id[train_df.loc[k,'sentiment']]\n    input_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n    attention_mask[k,:len(enc.ids)+5] = 1\n    if len(toks)>0:\n        start_tokens[k,toks[0]+1] = 1\n        end_tokens[k,toks[-1]+1] = 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create validation set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ct = validation_df.shape[0]\ninput_ids_v = np.ones((ct,MAX_LEN),dtype='int32')\nattention_mask_v = np.zeros((ct,MAX_LEN),dtype='int32')\ntoken_type_ids_v = np.zeros((ct,MAX_LEN),dtype='int32')\n\nfor k in range(validation_df.shape[0]):\n        \n    # INPUT_IDS\n    text1 = \" \"+\" \".join(validation_df.loc[k,'text'].split())\n    enc = tokenizer.encode(text1)                \n    s_tok = sentiment_id[validation_df.loc[k,'sentiment']]\n    input_ids_v[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n    attention_mask_v[k,:len(enc.ids)+5] = 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create test set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ct = test_df.shape[0]\ninput_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\nattention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\ntoken_type_ids_t = np.zeros((ct,MAX_LEN),dtype='int32')\n\nfor k in range(test_df.shape[0]):\n        \n    # INPUT_IDS\n    text1 = \" \"+\" \".join(test_df.loc[k,'text'].split())\n    enc = tokenizer.encode(text1)                \n    s_tok = sentiment_id[test_df.loc[k,'sentiment']]\n    input_ids_t[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n    attention_mask_t[k,:len(enc.ids)+5] = 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def scheduler(epoch):\n    return 3e-5 * 0.2**epoch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model():\n    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n\n    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n    x = bert_model(ids,attention_mask=att,token_type_ids=tok)\n    \n    \n    x1 = tf.keras.layers.Dropout(0.1)(x[0]) \n    x1 = tf.keras.layers.Conv1D(128, 2,padding='same')(x1)\n    x1 = tf.keras.layers.LeakyReLU()(x1)\n    x1 = tf.keras.layers.Conv1D(64, 2,padding='same')(x1)\n    x1 = tf.keras.layers.Dense(2)(x1)\n    x1 = tf.keras.layers.LeakyReLU()(x1)\n    x1 = tf.keras.layers.Dense(1)(x1)\n    x1 = tf.keras.layers.Flatten()(x1)\n    x1 = tf.keras.layers.Activation('softmax')(x1)\n    \n    x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n    x2 = tf.keras.layers.Conv1D(128, 2, padding='same')(x2)\n    x2 = tf.keras.layers.LeakyReLU()(x2)\n    x2 = tf.keras.layers.Conv1D(64, 2, padding='same')(x2)\n    x2 = tf.keras.layers.Dense(2)(x2)\n    x2 = tf.keras.layers.LeakyReLU()(x2)\n    x2 = tf.keras.layers.Dense(1)(x2)\n    x2 = tf.keras.layers.Flatten()(x2)\n    x2 = tf.keras.layers.Activation('softmax')(x2)\n\n    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n    model.compile(loss='binary_crossentropy', optimizer=optimizer)\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train\nWe will skip this stage and load already trained model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_splits = 4","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"jac = []; VER='v1'; DISPLAY=1 # USE display=1 FOR INTERACTIVE\noof_start = np.zeros((input_ids.shape[0],MAX_LEN))\noof_end = np.zeros((input_ids.shape[0],MAX_LEN))\n\nskf = StratifiedKFold(n_splits=n_splits,shuffle=True,random_state=777)\nfor fold,(idxT,idxV) in enumerate(skf.split(input_ids,train_df.sentiment.values)):\n\n    print('#'*25)\n    print('### FOLD %i'%(fold+1))\n    print('#'*25)\n    \n    K.clear_session()\n    model = build_model()\n        \n    reduce_lr = tf.keras.callbacks.LearningRateScheduler(scheduler)\n\n    sv = tf.keras.callbacks.ModelCheckpoint(\n        '%s-roberta-%i.h5'%(VER,fold), monitor='val_loss', verbose=1, save_best_only=True,\n        save_weights_only=True, mode='auto', save_freq='epoch')\n        \n    hist = model.fit([input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]], [start_tokens[idxT,], end_tokens[idxT,]], \n        epochs=4, batch_size=8, verbose=DISPLAY, callbacks=[sv, reduce_lr],\n        validation_data=([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]], \n        [start_tokens[idxV,], end_tokens[idxV,]]))\n    \n    print('Loading model...')\n    model.load_weights('./v1-roberta-%i.h5'%(fold))\n    \n    print('Predicting OOF...')\n    oof_start[idxV,],oof_end[idxV,] = model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n    \n    # DISPLAY FOLD JACCARD\n    all = []\n    for k in idxV:\n        start_sorted_indexes = np.argsort(-oof_start[k,], kind='quicksort', order=None)\n        end_sorted_indexes   = np.argsort(-oof_end[k,], kind='quicksort', order=None)\n\n        if start_sorted_indexes[0] <= end_sorted_indexes[0]:\n            a = start_sorted_indexes[0]\n            b = end_sorted_indexes[0]\n            text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n            #text1 = revert_clean(text1)\n            enc = tokenizer.encode(text1)\n            st = tokenizer.decode(enc.ids[a-1:b])\n            \n        elif start_sorted_indexes[0] <= end_sorted_indexes[1]:\n            a = start_sorted_indexes[0]\n            b = end_sorted_indexes[1]\n            text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n            #text1 = revert_clean(text1)\n            enc = tokenizer.encode(text1)\n            st = tokenizer.decode(enc.ids[a-1:b])\n\n        elif start_sorted_indexes[1] <= end_sorted_indexes[0]:\n            a = start_sorted_indexes[1]\n            b = end_sorted_indexes[0]\n            text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n            #text1 = revert_clean(text1)\n            enc = tokenizer.encode(text1)\n            st = tokenizer.decode(enc.ids[a-1:b])\n            \n        elif start_sorted_indexes[1] <= end_sorted_indexes[1]:\n            a = start_sorted_indexes[1]\n            b = end_sorted_indexes[1]\n            text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n            #text1 = revert_clean(text1)\n            enc = tokenizer.encode(text1)\n            st = tokenizer.decode(enc.ids[a-1:b])\n        \n        else:\n            st = train_df.loc[k,'text']\n            #st = revert_clean(st)\n\n        all.append(jaccard(st,train_df.loc[k,'selected_text']))\n    jac.append(np.mean(all))\n    print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all))\n    print()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Inference","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Predicting test","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\npreds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\nDISPLAY=1\nfor i in range(n_splits):\n    print('#'*25)\n    print('### MODEL %i'%(i+1))\n    print('#'*25)\n    \n    K.clear_session()\n    model = build_model()\n    model.load_weights('./v1-roberta-%i.h5'%(i))\n\n    print('Predicting Test...')\n    preds = model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n    preds_start += preds[0]/n_splits\n    preds_end += preds[1]/n_splits","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all = []\ncounter = 0\nfor k in range(input_ids_t.shape[0]):\n    start_sorted_indexes = np.argsort(-preds_start[k,], kind='quicksort', order=None)\n    end_sorted_indexes   = np.argsort(-preds_end[k,], kind='quicksort', order=None)\n\n    if start_sorted_indexes[0] <= end_sorted_indexes[0]:\n        a = start_sorted_indexes[0]\n        b = end_sorted_indexes[0]\n        text1 = \" \"+\" \".join(test_df.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode(enc.ids[a-1:b])\n\n    elif start_sorted_indexes[0] <= end_sorted_indexes[1]:\n        a = start_sorted_indexes[0]\n        b = end_sorted_indexes[1]\n        text1 = \" \"+\" \".join(test_df.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode(enc.ids[a-1:b])\n\n    elif start_sorted_indexes[1] <= end_sorted_indexes[0]:\n        a = start_sorted_indexes[1]\n        b = end_sorted_indexes[0]\n        text1 = \" \"+\" \".join(test_df.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode(enc.ids[a-1:b])\n\n    elif start_sorted_indexes[1] <= end_sorted_indexes[1]:\n        a = start_sorted_indexes[1]\n        b = end_sorted_indexes[1]\n        text1 = \" \"+\" \".join(test_df.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode(enc.ids[a-1:b])\n\n    else:\n        counter += 1\n        st = test_df.loc[k,'text']\n\n    all.append(st)\n    \nprint(counter, \" row cant predicted on test set\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df['selected_text'] = all\ntest_df[['textID','selected_text']].to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Predicting Train","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_preds_start = np.zeros((input_ids.shape[0],MAX_LEN))\ntrain_preds_end = np.zeros((input_ids.shape[0],MAX_LEN))\nDISPLAY=1\nfor i in range(n_splits):\n    print('#'*25)\n    print('### MODEL %i'%(i+1))\n    print('#'*25)\n    \n    K.clear_session()\n    model = build_model()\n    model.load_weights('./v1-roberta-%i.h5'%(i))\n\n    print('Predicting Train...')\n    train_preds = model.predict([input_ids,attention_mask,token_type_ids],verbose=DISPLAY)\n    train_preds_start += train_preds[0]/n_splits\n    train_preds_end += train_preds[1]/n_splits","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all = []\ncounter = 0\nfor k in range(input_ids.shape[0]):\n    start_sorted_indexes = np.argsort(-train_preds_start[k,], kind='quicksort', order=None)\n    end_sorted_indexes   = np.argsort(-train_preds_end[k,], kind='quicksort', order=None)\n\n    if start_sorted_indexes[0] <= end_sorted_indexes[0]:\n        a = start_sorted_indexes[0]\n        b = end_sorted_indexes[0]\n        text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode(enc.ids[a-1:b])\n\n    elif start_sorted_indexes[0] <= end_sorted_indexes[1]:\n        a = start_sorted_indexes[0]\n        b = end_sorted_indexes[1]\n        text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode(enc.ids[a-1:b])\n\n    elif start_sorted_indexes[1] <= end_sorted_indexes[0]:\n        a = start_sorted_indexes[1]\n        b = end_sorted_indexes[0]\n        text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode(enc.ids[a-1:b])\n\n    elif start_sorted_indexes[1] <= end_sorted_indexes[1]:\n        a = start_sorted_indexes[1]\n        b = end_sorted_indexes[1]\n        text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode(enc.ids[a-1:b])\n\n    else:\n        counter += 1\n        st = train_df.loc[k,'text']\n\n    all.append(st)\n\nprint(counter, \" row cant predicted on train set\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['predicted_text'] = all","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Predicting Validation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"validation_preds_start = np.zeros((input_ids_v.shape[0],MAX_LEN))\nvalidation_preds_end = np.zeros((input_ids_v.shape[0],MAX_LEN))\nDISPLAY=1\nfor i in range(n_splits):\n    print('#'*25)\n    print('### MODEL %i'%(i+1))\n    print('#'*25)\n    \n    K.clear_session()\n    model = build_model()\n    model.load_weights('./v1-roberta-%i.h5'%(i))\n\n    print('Predicting Validation...')\n    validation_preds = model.predict([input_ids_v,attention_mask_v,token_type_ids_v],verbose=DISPLAY)\n    validation_preds_start += validation_preds[0]/n_splits\n    validation_preds_end += validation_preds[1]/n_splits","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all = []\ncounter = 0\nfor k in range(input_ids_v.shape[0]):\n    start_sorted_indexes = np.argsort(-validation_preds_start[k,], kind='quicksort', order=None)\n    end_sorted_indexes   = np.argsort(-validation_preds_end[k,], kind='quicksort', order=None)\n\n    if start_sorted_indexes[0] <= end_sorted_indexes[0]:\n        a = start_sorted_indexes[0]\n        b = end_sorted_indexes[0]\n        text1 = \" \"+\" \".join(validation_df.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode(enc.ids[a-1:b])\n\n    elif start_sorted_indexes[0] <= end_sorted_indexes[1]:\n        a = start_sorted_indexes[0]\n        b = end_sorted_indexes[1]\n        text1 = \" \"+\" \".join(validation_df.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode(enc.ids[a-1:b])\n\n    elif start_sorted_indexes[1] <= end_sorted_indexes[0]:\n        a = start_sorted_indexes[1]\n        b = end_sorted_indexes[0]\n        text1 = \" \"+\" \".join(validation_df.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode(enc.ids[a-1:b])\n\n    elif start_sorted_indexes[1] <= end_sorted_indexes[1]:\n        a = start_sorted_indexes[1]\n        b = end_sorted_indexes[1]\n        text1 = \" \"+\" \".join(validation_df.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode(enc.ids[a-1:b])\n\n    else:\n        counter += 1\n        st = validation_df.loc[k,'text']\n\n    all.append(st)\n\nprint(counter, \" row cant predicted on validation set\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"validation_df['predicted_text'] = all","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Only roberta score on validation\n\nExpected value is ~0.711","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cumulativeJaccard = []\n\nfor index, row in validation_df.iterrows():\n    cumulativeJaccard.append(jaccard(row[\"predicted_text\"],row['selected_text']))\n    \nprint(\"Roberta Jaccard on validation = \", np.mean(cumulativeJaccard), \" Expected value is ~0.711\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preparing Data For NER","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"validation_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Prepare Train","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"text_se = []\nselected_text_se = []\nsentiment_se = []\nposition = []\nfor index, row in train_df.iterrows():\n    text = \" \"+\" \".join(row['text'].split())\n    selected_text = \" \".join(row[\"selected_text\"].split())\n    \n    selected_text_start_word = selected_text.split()[0]\n    selected_text_end_word = selected_text.split()[-1]\n    \n    selected_text_start_char = text.find(selected_text)\n    selected_text_end_char = selected_text_start_char + len(selected_text)\n    \n    # These words are from text not selected_text. So they can be different from selected_text words\n    selected_text_full_start_word = None\n    selected_text_full_end_word = None\n    \n    char_counter = 0\n    for word in row[\"text\"].split():\n        char_counter += len(word)+1\n        \n        if selected_text_start_char < char_counter:\n            # We find start word in text\n            selected_text_full_start_word = word\n            break\n    \n    char_counter = 0\n    for word in row[\"text\"].split():\n        char_counter += len(word)+1\n\n        if selected_text_end_char <= char_counter:\n            # We find start word in text\n            selected_text_full_end_word = word\n            break\n\n    if row[\"sentiment\"] == \"neutral\":\n        position.append(\"Start-Neutral\")\n        text_se.append(selected_text_full_start_word)\n        selected_text_se.append(selected_text_start_word)\n        position.append(\"End-Neutral\")\n        text_se.append(selected_text_full_end_word)\n        selected_text_se.append(selected_text_end_word)\n        sentiment_se.append(row[\"sentiment\"])\n        sentiment_se.append(row[\"sentiment\"])\n    else:\n        position.append(\"Start\")\n        text_se.append(selected_text_full_start_word)\n        selected_text_se.append(selected_text_start_word)\n        position.append(\"End\")\n        text_se.append(selected_text_full_end_word)\n        selected_text_se.append(selected_text_end_word)\n        sentiment_se.append(row[\"sentiment\"])\n        sentiment_se.append(row[\"sentiment\"])\n    \ntrain_df_se = pd.DataFrame(data={\"text_se\":text_se, \"selected_text_se\":selected_text_se, \"sentiment\":sentiment_se, \"position\":position})\n\ntrain_df_se","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Error Check\nfor index, row in train_df_se.iterrows():\n    if row[\"selected_text_se\"] not in row[\"text_se\"]:\n        print(\"Error at = \" + str(index), row[\"selected_text_se\"], row[\"text_se\"],)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Different words\ncounter = 0\nfor index, row in train_df_se.iterrows():\n    if row[\"selected_text_se\"] != row[\"text_se\"]:\n        counter += 1\n        print(\"Sentiment     =  \",row[\"sentiment\"].upper() + \"--\" +row[\"position\"])\n        print(\"Real word     =  \",row[\"text_se\"])\n        print(\"Selected word =  \",row[\"selected_text_se\"])\n        print(\"==========================================\")\nprint(\"Different word number in train set =\", counter)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Different word number in train set 3362, 346 word is **neutral**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Prepare Validation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"text_se = []\nselected_text_se = []\nsentiment_se = []\nposition = []\nfor index, row in validation_df.iterrows():\n    text = \" \"+\" \".join(row['text'].split())\n    selected_text = \" \".join(row[\"predicted_text\"].split())\n    \n    selected_text_start_word = selected_text.split()[0]\n    selected_text_end_word = selected_text.split()[-1]\n    \n    selected_text_start_char = text.find(selected_text)\n    selected_text_end_char = selected_text_start_char + len(selected_text)\n    \n    # These words are from text not selected_text. So they can be different from selected_text words\n    selected_text_full_start_word = None\n    selected_text_full_end_word = None\n    \n    char_counter = 0\n    for word in row[\"text\"].split():\n        char_counter += len(word)+1\n        \n        if selected_text_start_char < char_counter:\n            # We find start word in text\n            selected_text_full_start_word = word\n            break\n    \n    char_counter = 0\n    for word in row[\"text\"].split():\n        char_counter += len(word)+1\n\n        if selected_text_end_char <= char_counter:\n            # We find start word in text\n            selected_text_full_end_word = word\n            break\n\n    if row[\"sentiment\"] == \"neutral\":\n        position.append(\"Start-Neutral\")\n        text_se.append(selected_text_full_start_word)\n        selected_text_se.append(selected_text_start_word)\n        position.append(\"End-Neutral\")\n        text_se.append(selected_text_full_end_word)\n        selected_text_se.append(selected_text_end_word)\n        sentiment_se.append(row[\"sentiment\"])\n        sentiment_se.append(row[\"sentiment\"])\n    else:\n        position.append(\"Start\")\n        text_se.append(selected_text_full_start_word)\n        selected_text_se.append(selected_text_start_word)\n        position.append(\"End\")\n        text_se.append(selected_text_full_end_word)\n        selected_text_se.append(selected_text_end_word)\n        sentiment_se.append(row[\"sentiment\"])\n        sentiment_se.append(row[\"sentiment\"])\n    \nvalidation_df_se = pd.DataFrame(data={\"text_se\":text_se, \"selected_text_se\":selected_text_se, \"sentiment\":sentiment_se, \"position\":position})\n\nvalidation_df_se","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"validation_df_se.loc[96:98]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"validation_df.loc[48:49]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Error Check\nfor index, row in validation_df_se.iterrows():\n    if row[\"selected_text_se\"] not in row[\"text_se\"]:\n        print(\"Error at = \" + str(index), row[\"selected_text_se\"], row[\"text_se\"],)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Prepare Test","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"text_se = []\nselected_text_se = []\nsentiment_se = []\nposition = []\nfor index, row in test_df.iterrows():\n    text = \" \"+\" \".join(row['text'].split())\n    selected_text = \" \".join(row[\"selected_text\"].split())\n    \n    selected_text_start_word = selected_text.split()[0]\n    selected_text_end_word = selected_text.split()[-1]\n    \n    selected_text_start_char = text.find(selected_text)\n    selected_text_end_char = selected_text_start_char + len(selected_text)\n    \n    # These words are from text not selected_text. So they can be different from selected_text words\n    selected_text_full_start_word = None\n    selected_text_full_end_word = None\n    \n    char_counter = 0\n    for word in row[\"text\"].split():\n        char_counter += len(word)+1\n        \n        if selected_text_start_char < char_counter:\n            # We find start word in text\n            selected_text_full_start_word = word\n            break\n    \n    char_counter = 0\n    for word in row[\"text\"].split():\n        char_counter += len(word)+1\n\n        if selected_text_end_char <= char_counter:\n            # We find start word in text\n            selected_text_full_end_word = word\n            break\n\n    if row[\"sentiment\"] == \"neutral\":\n        position.append(\"Start-Neutral\")\n        text_se.append(selected_text_full_start_word)\n        selected_text_se.append(selected_text_start_word)\n        position.append(\"End-Neutral\")\n        text_se.append(selected_text_full_end_word)\n        selected_text_se.append(selected_text_end_word)\n        sentiment_se.append(row[\"sentiment\"])\n        sentiment_se.append(row[\"sentiment\"])\n    else:\n        position.append(\"Start\")\n        text_se.append(selected_text_full_start_word)\n        selected_text_se.append(selected_text_start_word)\n        position.append(\"End\")\n        text_se.append(selected_text_full_end_word)\n        selected_text_se.append(selected_text_end_word)\n        sentiment_se.append(row[\"sentiment\"])\n        sentiment_se.append(row[\"sentiment\"])\n    \ntest_df_se = pd.DataFrame(data={\"text_se\":text_se, \"selected_text_se\":selected_text_se, \"sentiment\":sentiment_se, \"position\":position})\n\ntest_df_se","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Error Check\nfor index, row in test_df_se.iterrows():\n    if row[\"selected_text_se\"] not in row[\"text_se\"]:\n        print(\"Error at = \" + str(index), row[\"selected_text_se\"], row[\"text_se\"],)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Different words\ncounter = 0\nfor index, row in test_df_se.iterrows():\n    if row[\"selected_text_se\"] != row[\"text_se\"]:\n        counter += 1\n        print(\"Sentiment     =  \",row[\"sentiment\"].upper() + \"--\" +row[\"position\"])\n        print(\"Real word     =  \",row[\"text_se\"])\n        print(\"Selected word =  \",row[\"selected_text_se\"])\n        print(\"==========================================\")\nprint(\"Different word number in test set =\", counter)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# NER","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nfrom nltk.corpus import stopwords\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport os\nimport nltk\nimport time\nimport spacy \nspacy.prefer_gpu()\nimport random\nfrom spacy.util import compounding\nfrom spacy.util import minibatch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def save_model(output_dir, nlp, new_model_name):\n    ''' This Function Saves model to \n    given output directory'''\n    \n    output_dir = f'../working/{output_dir}'\n    if output_dir is not None:        \n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n        nlp.meta[\"name\"] = new_model_name\n        nlp.to_disk(output_dir)\n        print(\"Saved model to\", output_dir)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(train_data, output_dir, n_iter=20, cont=False):\n    \"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\n    \"\"\n    if cont and os.path.exists(output_dir):\n        nlp = spacy.load(output_dir)  # load existing spaCy model\n        print(\"Loaded model '%s'\" % output_dir)\n    else:\n        nlp = spacy.blank(\"en\")  # create blank Language class\n        print(\"Created blank 'en' model\")\n    \n    # create the built-in pipeline components and add them to the pipeline\n    # nlp.create_pipe works for built-ins that are registered with spaCy\n    if \"ner\" not in nlp.pipe_names:\n        ner = nlp.create_pipe(\"ner\")\n        nlp.add_pipe(ner, last=True)\n    # otherwise, get it so we can add labels\n    else:\n        ner = nlp.get_pipe(\"ner\")\n    \n    # add labels\n    for _, annotations in train_data:\n        for ent in annotations.get(\"entities\"):\n            ner.add_label(ent[2])\n\n    # get names of other pipes to disable them during training\n    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n    with nlp.disable_pipes(*other_pipes):  # only train NER\n        # sizes = compounding(1.0, 4.0, 1.001)\n        # batch up the examples using spaCy's minibatch\n        if cont and os.path.exists(output_dir):\n            nlp.resume_training()\n        else:\n            nlp.begin_training()\n\n\n        for itn in tqdm(range(n_iter)):\n            random.shuffle(train_data)\n            batches = minibatch(train_data, size=compounding(4.0, 500.0, 1.001))    \n            losses = {}\n            for batch in batches:\n                texts, annotations = zip(*batch)\n                nlp.update(texts,  # batch of texts\n                            annotations,  # batch of annotations\n                            drop=0.5,   # dropout - make it harder to memorise data\n                            losses=losses, \n                            )\n            print(\"Losses\", losses)\n    save_model(output_dir, nlp, 'st_ner')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_model_out_path(position):\n    '''\n    Returns Model output path\n    '''\n    model_out_path = None\n    if position == 'Start':\n        model_out_path = 'models/model_Start'\n    elif position == 'End':\n        model_out_path = 'models/model_End'\n    return model_out_path","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_training_data(data, position):\n    '''\n    Returns Trainong data in the format needed to train spacy NER\n    '''\n    train_data = []\n    for index, row in data.iterrows():\n        if row.position == position:\n            selected_text = row.selected_text_se\n            text = row.text_se\n            start = text.find(selected_text)\n            end = start + len(selected_text)\n            train_data.append((text, {\"entities\": [[start, end, 'selected_text']]}))\n    return train_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"position = 'Start'\n\ntrain_data = get_training_data(train_df_se, position)\nmodel_path = get_model_out_path(position)\n\ntrain(train_data, model_path, n_iter=3, cont=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"position = 'End'\n\ntrain_data = get_training_data(train_df_se, position)\nmodel_path = get_model_out_path(position)\n\ntrain(train_data, model_path, n_iter=3, cont=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict_entities(text, model):\n    doc = model(text)\n    ent_array = []\n    for ent in doc.ents:\n        start = text.find(ent.text)\n        end = start + len(ent.text)\n        new_int = [start, end, ent.label_]\n        if new_int not in ent_array:\n            ent_array.append([start, end, ent.label_])\n    if len(ent_array) > 0:\n        selected_text = text[ent_array[0][0]: ent_array[0][1]]\n    else:\n        selected_text = text\n    return selected_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"selected_texts = []\nMODELS_BASE_PATH = '../working/models/'\n\nif MODELS_BASE_PATH is not None:\n    print(\"Loading Models  from \", MODELS_BASE_PATH)\n    model_Start = spacy.load(MODELS_BASE_PATH + 'model_Start')\n    model_End = spacy.load(MODELS_BASE_PATH + 'model_End')\n        \n    for index, row in validation_df_se.iterrows():\n        text = row.text_se            # ----------------------------------------------------------------------------------\n        output_str = \"\"\n        \n        if row.position == 'Start-Neutral' or row.position == 'End-Neutral':\n            selected_texts.append(text)\n        elif row.position == 'Start':\n            selected_texts.append(predict_entities(text, model_Start))\n        elif row.position == 'End':\n            selected_texts.append(predict_entities(text, model_End))\n        \nvalidation_df_se['ner_predicted_text'] = selected_texts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"validation_df_se","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Different words\ncounter = 0\nfor index, row in validation_df_se.iterrows():\n    if row[\"selected_text_se\"] != row[\"text_se\"]:\n        counter += 1\nprint(\"Different word count in validation set =\",counter)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"counter = 0\nfor index, row in validation_df_se.iterrows():\n    if row[\"text_se\"] == row[\"selected_text_se\"]:\n        counter+=1\nprint(\"Same word count in validation set =\",counter)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"counter = 0\nfor index, row in validation_df_se.iterrows():\n    if row[\"ner_predicted_text\"] != row[\"text_se\"]:\n        counter+=1\nprint(\"Changed word by NER in validation set =\",counter)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"counter = 0\nfor index, row in validation_df_se.iterrows():\n    if row[\"ner_predicted_text\"] != row[\"text_se\"]:\n        if row[\"ner_predicted_text\"] != row[\"selected_text_se\"]:\n            counter+=1\nprint(\"Changed word by NER in validation set but unsuccesfull changing=\",counter)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_entities(\"badly??!?!??!!?\", model_End)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for index, row in validation_df_se.iterrows():\n    if row[\"text_se\"] == \"badly??!?!??!!?\":\n        print(row)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for index, row in validation_df_se.iterrows():\n    if row[\"selected_text_se\"] != row[\"text_se\"]:\n        print(row[\"text_se\"], row[\"selected_text_se\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for index, row in validation_df.iterrows():\n     \n    predicted_words = row[\"predicted_text\"].split()\n    predicted_words[0] = validation_df_se[\"ner_predicted_text\"][2*index]\n    predicted_words[-1] = validation_df_se[\"ner_predicted_text\"][(2*index)+1]\n    \n    validation_df[\"predicted_text\"][index] = \" \".join(predicted_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"validation_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"totalJaccard = 0\nfor index, row in validation_df.iterrows():\n    predict = row.predicted_text\n    true = row.selected_text\n\n    j = jaccard(predict, true)\n\n    totalJaccard += j\n\nmeanJaccard = totalJaccard / len(validation_df['predicted_text'])\nprint(\"NER Validation results : \", meanJaccard)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df_se","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"selected_texts = []\nMODELS_BASE_PATH = '../working/models/'\n\nif MODELS_BASE_PATH is not None:\n    print(\"Loading Models  from \", MODELS_BASE_PATH)\n    model_Start = spacy.load(MODELS_BASE_PATH + 'model_Start')\n    model_End = spacy.load(MODELS_BASE_PATH + 'model_End')\n        \n    for index, row in test_df_se.iterrows():\n        text = row.text_se\n        output_str = \"\"\n        \n        if row.position == 'Start-Neutral' or row.position == 'End-Neutral':\n            selected_texts.append(text)\n        elif row.position == 'Start':\n            selected_texts.append(predict_entities(text, model_Start))\n        elif row.position == 'End':\n            selected_texts.append(predict_entities(text, model_End))\n        \ntest_df_se['ner_predicted_text'] = selected_texts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for index, row in test_df.iterrows():\n     \n    predicted_words = row[\"selected_text\"].split()\n    predicted_words[0] = test_df_se[\"ner_predicted_text\"][2*index]\n    predicted_words[-1] = test_df_se[\"ner_predicted_text\"][(2*index)+1]\n    \n    test_df[\"selected_text\"][index] = \" \".join(predicted_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df[['textID','selected_text']].to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}