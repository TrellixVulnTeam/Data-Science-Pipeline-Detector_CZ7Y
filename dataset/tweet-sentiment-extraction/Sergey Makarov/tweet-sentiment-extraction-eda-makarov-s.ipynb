{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Import libraries","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nimport warnings\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom nltk import ngrams\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nfrom wordcloud import WordCloud\nimport nltk\nimport re\nimport string\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport numpy as np  # linear algebra\nimport pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# We need to install a wide variety of libraries. For this we will install pandas, numpy, seaborn and matplotlib libraries.\nsns.set()\n\n\nwarnings.filterwarnings(\"ignore\")\n\n# Graphics in retina format are more sharp and legible\n%config InlineBackend.figure_format = 'retina'","metadata":{"execution":{"iopub.status.busy":"2021-11-23T12:25:06.195999Z","iopub.execute_input":"2021-11-23T12:25:06.196639Z","iopub.status.idle":"2021-11-23T12:25:06.223734Z","shell.execute_reply.started":"2021-11-23T12:25:06.196591Z","shell.execute_reply":"2021-11-23T12:25:06.222904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Reading the datasets","metadata":{}},{"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/tweet-sentiment-extraction/train.csv\")\ntest_data = pd.read_csv(\"/kaggle/input/tweet-sentiment-extraction/test.csv\")\ntrain_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-23T12:25:06.225566Z","iopub.execute_input":"2021-11-23T12:25:06.225813Z","iopub.status.idle":"2021-11-23T12:25:06.317582Z","shell.execute_reply.started":"2021-11-23T12:25:06.225783Z","shell.execute_reply":"2021-11-23T12:25:06.316699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.describe()","metadata":{"execution":{"iopub.status.busy":"2021-11-23T12:25:06.318772Z","iopub.execute_input":"2021-11-23T12:25:06.319016Z","iopub.status.idle":"2021-11-23T12:25:06.403321Z","shell.execute_reply.started":"2021-11-23T12:25:06.318987Z","shell.execute_reply":"2021-11-23T12:25:06.402499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. EDA\n### Missing Values treatment in the dataset","metadata":{}},{"cell_type":"code","source":"train_data.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-11-23T12:25:06.40527Z","iopub.execute_input":"2021-11-23T12:25:06.405498Z","iopub.status.idle":"2021-11-23T12:25:06.427074Z","shell.execute_reply.started":"2021-11-23T12:25:06.405472Z","shell.execute_reply":"2021-11-23T12:25:06.426196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We found one row is missing for text and selected_text, so we need to replace it or drop it.","metadata":{}},{"cell_type":"code","source":"# Dropping missing values\ntrain_data.dropna(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-23T12:25:06.428295Z","iopub.execute_input":"2021-11-23T12:25:06.428643Z","iopub.status.idle":"2021-11-23T12:25:06.457211Z","shell.execute_reply.started":"2021-11-23T12:25:06.428603Z","shell.execute_reply":"2021-11-23T12:25:06.45635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Distribution of the Sentiment Column","metadata":{}},{"cell_type":"code","source":"train_data['sentiment'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-11-23T12:25:06.459913Z","iopub.execute_input":"2021-11-23T12:25:06.460308Z","iopub.status.idle":"2021-11-23T12:25:06.479131Z","shell.execute_reply.started":"2021-11-23T12:25:06.460278Z","shell.execute_reply":"2021-11-23T12:25:06.47828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data['sentiment'].value_counts(normalize=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-23T12:25:06.480588Z","iopub.execute_input":"2021-11-23T12:25:06.481607Z","iopub.status.idle":"2021-11-23T12:25:06.497172Z","shell.execute_reply.started":"2021-11-23T12:25:06.48156Z","shell.execute_reply":"2021-11-23T12:25:06.496308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(data=train_data, x='sentiment',\n              order=train_data['sentiment'].value_counts().index)","metadata":{"execution":{"iopub.status.busy":"2021-11-23T12:25:06.498947Z","iopub.execute_input":"2021-11-23T12:25:06.499606Z","iopub.status.idle":"2021-11-23T12:25:06.802162Z","shell.execute_reply.started":"2021-11-23T12:25:06.499555Z","shell.execute_reply":"2021-11-23T12:25:06.801275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Examples of each sentiment","metadata":{}},{"cell_type":"code","source":"# Positive tweet\nprint('Positive Tweet example:',\n      train_data[train_data['sentiment'] == 'positive']['text'].values[0])\n\n# Negative tweet\nprint('Negative Tweet example:',\n      train_data[train_data['sentiment'] == 'negative']['text'].values[0])\n\n# Neutral tweet\nprint('Neutral Tweet example:',\n      train_data[train_data['sentiment'] == 'neutral']['text'].values[0])","metadata":{"execution":{"iopub.status.busy":"2021-11-23T12:25:06.803717Z","iopub.execute_input":"2021-11-23T12:25:06.804201Z","iopub.status.idle":"2021-11-23T12:25:06.838173Z","shell.execute_reply.started":"2021-11-23T12:25:06.804154Z","shell.execute_reply":"2021-11-23T12:25:06.837474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Text Data Preprocessing\nWe need to pre-process the data to get it all in a consistent format.We need to clean, tokenize and convert our data into a matrix. Let's create a function which will perform the following tasks on the text columns:\n\n- Tokenizes\n- Make text lowercase\n- Removes hyperlinks\n- Remove punctuation\n- Removes numbers\n- Removes useless words \"stopwords\"\n- Stemming/Lemmatization","metadata":{}},{"cell_type":"code","source":"stop_words = stopwords.words('english')\nstemmer = nltk.SnowballStemmer(\"english\")\n\n\ndef clean_text(text):\n    '''\n        Make text lowercase, remove text in square brackets,remove links,remove punctuation\n        and remove words containing numbers.\n    '''\n    text = str(text).lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?://\\S|www\\.\\S', '', text)  # remove urls\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation),\n                  '', text)  # remove punctuation\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n\n\ndef preprocess_data(text):\n    \"\"\"\n       Clean puntuation, urls, and so on, \n       removing stop-words and make stemming\n    \"\"\"\n    text = clean_text(text)\n    # Remove stop-words\n    text = ' '.join(word for word in text.split()\n                    if word not in stop_words)    # Remove stopwords\n    # Stemm all the words in the sentence\n    text = ' '.join(stemmer.stem(word) for word in text.split())\n    return text","metadata":{"execution":{"iopub.status.busy":"2021-11-23T12:25:06.840545Z","iopub.execute_input":"2021-11-23T12:25:06.841358Z","iopub.status.idle":"2021-11-23T12:25:06.85132Z","shell.execute_reply.started":"2021-11-23T12:25:06.841307Z","shell.execute_reply":"2021-11-23T12:25:06.850206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data['clean_text'] = train_data['text'].apply(preprocess_data)\ntrain_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-23T12:25:06.852733Z","iopub.execute_input":"2021-11-23T12:25:06.853016Z","iopub.status.idle":"2021-11-23T12:25:11.784713Z","shell.execute_reply.started":"2021-11-23T12:25:06.852987Z","shell.execute_reply":"2021-11-23T12:25:11.783836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert sentiment to numerical variable\ntrain_data['label'] = train_data.sentiment.map({'negative': 0,\n                                                'positive': 1,\n                                                'neutral': 2})\ntrain_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-23T12:25:11.786072Z","iopub.execute_input":"2021-11-23T12:25:11.786411Z","iopub.status.idle":"2021-11-23T12:25:11.805187Z","shell.execute_reply.started":"2021-11-23T12:25:11.786376Z","shell.execute_reply":"2021-11-23T12:25:11.804382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Analyzing Text Statistics\n\nWe can now do some statistical analysis to explore the data like:\n\n- Text length analysis\n- length for whole sentence\n- count of word in each sentence\n- word frequency analysis","metadata":{}},{"cell_type":"markdown","source":"### Text length analysis","metadata":{}},{"cell_type":"code","source":"train_data['text_n_chars'] = train_data.text.apply(\n    len)  # count all chars in each sentence\ntrain_data['text_n_words'] = train_data.text.apply(\n    lambda sent: len(sent.split()))  # count number of words in each sentence\ntrain_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-23T12:25:11.806724Z","iopub.execute_input":"2021-11-23T12:25:11.80722Z","iopub.status.idle":"2021-11-23T12:25:11.877731Z","shell.execute_reply.started":"2021-11-23T12:25:11.807185Z","shell.execute_reply":"2021-11-23T12:25:11.876804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The distribution of number of words for each sentiment\nsns.histplot(data=train_data, x='text_n_words',\n             hue='sentiment', multiple='stack')","metadata":{"execution":{"iopub.status.busy":"2021-11-23T12:25:11.878996Z","iopub.execute_input":"2021-11-23T12:25:11.87924Z","iopub.status.idle":"2021-11-23T12:25:12.803472Z","shell.execute_reply.started":"2021-11-23T12:25:11.87921Z","shell.execute_reply":"2021-11-23T12:25:12.802828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Most frequent words. In whole Text.","metadata":{}},{"cell_type":"code","source":"words = [word for sent in train_data['clean_text'] for word in sent.split()]\nwords[:10]  # words without sorting","metadata":{"execution":{"iopub.status.busy":"2021-11-23T12:25:12.804602Z","iopub.execute_input":"2021-11-23T12:25:12.805165Z","iopub.status.idle":"2021-11-23T12:25:12.848532Z","shell.execute_reply.started":"2021-11-23T12:25:12.805125Z","shell.execute_reply":"2021-11-23T12:25:12.847622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sort words descending order\nfreq_words = Counter(words)\nfreq_words_sorted = sorted(\n    freq_words.items(), key=lambda pair: pair[1], reverse=True)\nfreq_words_df = pd.DataFrame(\n    freq_words_sorted[:20], columns=['word', 'counts'])\nfreq_words_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-11-23T12:25:12.849803Z","iopub.execute_input":"2021-11-23T12:25:12.850041Z","iopub.status.idle":"2021-11-23T12:25:12.910235Z","shell.execute_reply.started":"2021-11-23T12:25:12.850011Z","shell.execute_reply":"2021-11-23T12:25:12.909143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12, 6))\nsns.barplot(data=freq_words_df, x='counts', y='word')\nplt.title('Top 20 words in whole text')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-23T12:25:12.912173Z","iopub.execute_input":"2021-11-23T12:25:12.91252Z","iopub.status.idle":"2021-11-23T12:25:13.371619Z","shell.execute_reply.started":"2021-11-23T12:25:12.912473Z","shell.execute_reply":"2021-11-23T12:25:13.370776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Frequent words for each sentiment","metadata":{}},{"cell_type":"code","source":"def freq_sentiment_words(text, sentiment, num):\n    '''\n        take the whole data, and return data which is have # of words in each sentiment has been passed\n    '''\n    words = [word for sent in text[text['sentiment'] == sentiment]\n             ['clean_text'] for word in sent.split()]\n    freq_words = Counter(words)\n    freq_words_sorted = sorted(\n        freq_words.items(), key=lambda pair: pair[1], reverse=True)\n    freq_words_df = pd.DataFrame(\n        freq_words_sorted[:num], columns=['word', 'counts'])\n    return freq_words_df\n\n\ndef plot_freq(data, st):\n    '''\n        take the data, and st refeere to kind of sentiment\n    '''\n    plt.figure(figsize=(12, 6))\n    sns.barplot(data=data, x='counts', y='word')\n    plt.title(f'Top 20 words in {st} sentiment')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-23T12:25:13.373344Z","iopub.execute_input":"2021-11-23T12:25:13.373648Z","iopub.status.idle":"2021-11-23T12:25:13.383632Z","shell.execute_reply.started":"2021-11-23T12:25:13.373609Z","shell.execute_reply":"2021-11-23T12:25:13.382711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# In Positive Sentiment\npositive_words = freq_sentiment_words(train_data, 'positive', 20)\nprint(positive_words.head())\nplot_freq(positive_words, 'positive')","metadata":{"execution":{"iopub.status.busy":"2021-11-23T12:25:13.385491Z","iopub.execute_input":"2021-11-23T12:25:13.385826Z","iopub.status.idle":"2021-11-23T12:25:13.879622Z","shell.execute_reply.started":"2021-11-23T12:25:13.385783Z","shell.execute_reply":"2021-11-23T12:25:13.878737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# In Negative Sentiment\nnegative_words = freq_sentiment_words(train_data, 'negative', 20)\nprint(negative_words.head())\nplot_freq(negative_words, 'negative')","metadata":{"execution":{"iopub.status.busy":"2021-11-23T12:25:13.881153Z","iopub.execute_input":"2021-11-23T12:25:13.881457Z","iopub.status.idle":"2021-11-23T12:25:14.357609Z","shell.execute_reply.started":"2021-11-23T12:25:13.881418Z","shell.execute_reply":"2021-11-23T12:25:14.356965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# In Neutral Sentiment\nneutral_words = freq_sentiment_words(train_data, 'neutral', 20)\nprint(neutral_words.head())\nplot_freq(neutral_words, 'neutral')","metadata":{"execution":{"iopub.status.busy":"2021-11-23T12:25:14.358964Z","iopub.execute_input":"2021-11-23T12:25:14.359619Z","iopub.status.idle":"2021-11-23T12:25:14.833407Z","shell.execute_reply.started":"2021-11-23T12:25:14.359579Z","shell.execute_reply":"2021-11-23T12:25:14.832323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Distribution of top n-grams","metadata":{}},{"cell_type":"code","source":"def get_top_n_gram(corpus, sentiment,  n_gram, top_n=None):\n    \"\"\"\n        Creates n-gram distribution and returns top n elements\n    \"\"\"\n    # list of splited senteces, which is just list of words\n    text = [word for sent in corpus[corpus['sentiment'] ==\n                                    sentiment]['clean_text'] for word in sent.split()]\n\n    grams = ngrams(text, n_gram)\n    grams = (' '.join(g) for g in grams)\n    num_of_grams = [words for words in grams]\n    freq_words = Counter(num_of_grams)\n    freq_words_sorted = sorted(\n        freq_words.items(), key=lambda pair: pair[1], reverse=True)\n    freq_words_df = pd.DataFrame(\n        freq_words_sorted[:top_n], columns=['word', 'counts'])\n    return freq_words_df[:top_n]","metadata":{"execution":{"iopub.status.busy":"2021-11-23T12:25:14.834867Z","iopub.execute_input":"2021-11-23T12:25:14.835186Z","iopub.status.idle":"2021-11-23T12:25:14.844535Z","shell.execute_reply.started":"2021-11-23T12:25:14.835142Z","shell.execute_reply":"2021-11-23T12:25:14.843833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Bi-Gram for positive sentiment\npositive_gram = get_top_n_gram(train_data, 'positive', 2, 20)\nprint(positive_gram.head())\nplot_freq(positive_gram, 'positive')","metadata":{"execution":{"iopub.status.busy":"2021-11-23T12:25:14.845718Z","iopub.execute_input":"2021-11-23T12:25:14.84661Z","iopub.status.idle":"2021-11-23T12:25:15.462944Z","shell.execute_reply.started":"2021-11-23T12:25:14.846568Z","shell.execute_reply":"2021-11-23T12:25:15.46202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Bi-Gram for negative sentiment\nnegative_gram = get_top_n_gram(train_data, 'negative', 2, 20)\nprint(negative_gram.head())\nplot_freq(negative_gram, 'negative')","metadata":{"execution":{"iopub.status.busy":"2021-11-23T12:25:15.464457Z","iopub.execute_input":"2021-11-23T12:25:15.464953Z","iopub.status.idle":"2021-11-23T12:25:16.025613Z","shell.execute_reply.started":"2021-11-23T12:25:15.464908Z","shell.execute_reply":"2021-11-23T12:25:16.024707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Bi-Gram for neutral sentiment\nnetutral_gram = get_top_n_gram(train_data, 'neutral', 2, 20)\nprint(netutral_gram.head())\nplot_freq(netutral_gram, 'neutral')","metadata":{"execution":{"iopub.status.busy":"2021-11-23T12:25:16.026881Z","iopub.execute_input":"2021-11-23T12:25:16.027118Z","iopub.status.idle":"2021-11-23T12:25:16.803327Z","shell.execute_reply.started":"2021-11-23T12:25:16.027088Z","shell.execute_reply":"2021-11-23T12:25:16.802341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can easily make tri-grams for sentiment using this function get_top_n_gram by passing n_gram = 3","metadata":{}},{"cell_type":"markdown","source":"### Word Cloud","metadata":{}},{"cell_type":"code","source":"# getting list of positive words\npositive_text_clean = train_data[train_data['sentiment']\n                                 == 'positive']['clean_text']\npositive_clean_words = [\n    word for words in positive_text_clean for word in words.split()]\npositive_clean_words[:10]","metadata":{"execution":{"iopub.status.busy":"2021-11-23T12:25:16.808341Z","iopub.execute_input":"2021-11-23T12:25:16.808825Z","iopub.status.idle":"2021-11-23T12:25:16.838387Z","shell.execute_reply.started":"2021-11-23T12:25:16.808762Z","shell.execute_reply":"2021-11-23T12:25:16.837407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# getting list of negative words\nnegative_text_clean = train_data[train_data['sentiment']\n                                 == 'negative']['clean_text']\nnegative_clean_words = [\n    word for words in negative_text_clean for word in words.split()]\nnegative_clean_words[:10]","metadata":{"execution":{"iopub.status.busy":"2021-11-23T12:25:16.839574Z","iopub.execute_input":"2021-11-23T12:25:16.839833Z","iopub.status.idle":"2021-11-23T12:25:16.867686Z","shell.execute_reply.started":"2021-11-23T12:25:16.839802Z","shell.execute_reply":"2021-11-23T12:25:16.866785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# getting list of neutral words\nneutral_text_clean = train_data[train_data['sentiment']\n                                == 'neutral']['clean_text']\nneutral_clean_words = [\n    word for words in neutral_text_clean for word in words.split()]\nneutral_clean_words[:10]","metadata":{"execution":{"iopub.status.busy":"2021-11-23T12:25:16.869419Z","iopub.execute_input":"2021-11-23T12:25:16.869763Z","iopub.status.idle":"2021-11-23T12:25:16.89891Z","shell.execute_reply.started":"2021-11-23T12:25:16.869704Z","shell.execute_reply":"2021-11-23T12:25:16.898055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=[30, 15])\nwordcloud1 = WordCloud(background_color='white',\n                       width=600,\n                       height=400).generate(\" \".join(positive_clean_words))\nax1.imshow(wordcloud1)\nax1.axis('off')\nax1.set_title('Positive text', fontsize=40)\n\nwordcloud2 = WordCloud(background_color='white',\n                       width=600,\n                       height=400).generate(\" \".join(negative_clean_words))\nax2.imshow(wordcloud2)\nax2.axis('off')\nax2.set_title('Negative text', fontsize=40)\n\nwordcloud3 = WordCloud(background_color='white',\n                       width=600,\n                       height=400).generate(\" \".join(neutral_clean_words))\nax3.imshow(wordcloud3)\nax3.axis('off')\nax3.set_title('Neutral text', fontsize=40)","metadata":{"execution":{"iopub.status.busy":"2021-11-23T12:25:16.900499Z","iopub.execute_input":"2021-11-23T12:25:16.900831Z","iopub.status.idle":"2021-11-23T12:25:22.795784Z","shell.execute_reply.started":"2021-11-23T12:25:16.900788Z","shell.execute_reply":"2021-11-23T12:25:22.793871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Naive model","metadata":{}},{"cell_type":"code","source":"def jaccard(str1, str2): #function for finding jaccard similarity for given two sentences\n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))","metadata":{"execution":{"iopub.status.busy":"2021-11-23T12:25:22.797568Z","iopub.execute_input":"2021-11-23T12:25:22.797857Z","iopub.status.idle":"2021-11-23T12:25:22.803604Z","shell.execute_reply.started":"2021-11-23T12:25:22.797824Z","shell.execute_reply":"2021-11-23T12:25:22.802493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/tweet-sentiment-extraction/train.csv\").fillna('')\ntrain_data['naive'] =  train_data['text']\njs=0\nfor i in range(train_data.shape[0]):\n    if i!=314: #as there is an null value and we replaced it with '',it will give zero length for sentence and raises zero error,so neglect that one sentence\n        js = js + jaccard(train_data.iloc[i,2],train_data.iloc[i,4])\nprint(\"final jaccard score for naive predictions:\",js/(train_data.shape[0]))","metadata":{"execution":{"iopub.status.busy":"2021-11-23T12:25:24.89417Z","iopub.execute_input":"2021-11-23T12:25:24.89458Z","iopub.status.idle":"2021-11-23T12:25:27.096637Z","shell.execute_reply.started":"2021-11-23T12:25:24.894535Z","shell.execute_reply":"2021-11-23T12:25:27.095695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data['text'] = train_data['text'].apply(preprocess_data)\njs=0\nfor i in range(train_data.shape[0]):\n    if i!=314: #as there is an null value and we replaced it with '',it will give zero length for sentence and raises zero error,so neglect that one sentence\n        js = js + jaccard(train_data.iloc[i,2],train_data.iloc[i,4])\nprint(\"final jaccard score for naive predictions:\",js/(train_data.shape[0]))","metadata":{"execution":{"iopub.status.busy":"2021-11-23T12:25:27.098206Z","iopub.execute_input":"2021-11-23T12:25:27.098618Z","iopub.status.idle":"2021-11-23T12:25:34.102666Z","shell.execute_reply.started":"2021-11-23T12:25:27.098563Z","shell.execute_reply":"2021-11-23T12:25:34.101773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion\n\nAs we can see from naive model, deep processing didn't improve final result for naive model.\nNaive model result we will use as reference for future research.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}