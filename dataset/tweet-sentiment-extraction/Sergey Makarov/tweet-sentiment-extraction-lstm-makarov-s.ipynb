{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Load libraries","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2021-11-08T14:58:42.862702Z","iopub.status.busy":"2021-11-08T14:58:42.861892Z","iopub.status.idle":"2021-11-08T14:58:42.867659Z","shell.execute_reply":"2021-11-08T14:58:42.867086Z","shell.execute_reply.started":"2021-11-08T14:58:42.862658Z"}}},{"cell_type":"code","source":"import datetime\nimport collections\nimport string\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nimport os\nfrom keras.layers import Concatenate, BatchNormalization, Bidirectional\nfrom keras.layers import SimpleRNN, Input, Dropout, Flatten\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.core import Dense, Activation\nfrom keras.layers.recurrent import LSTM\nimport tensorflow as tf\nfrom tensorflow.keras.models import load_model, Model\nfrom tensorflow.keras import regularizers, optimizers\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Sequential, load_model, Model\nfrom tensorflow.keras.preprocessing import sequence, text\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom fuzzywuzzy import fuzz\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn import preprocessing\nfrom tqdm import tqdm\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# load in all the modules we're going to need","metadata":{"execution":{"iopub.status.busy":"2021-11-15T08:49:00.692147Z","iopub.execute_input":"2021-11-15T08:49:00.692549Z","iopub.status.idle":"2021-11-15T08:49:04.413748Z","shell.execute_reply.started":"2021-11-15T08:49:00.692441Z","shell.execute_reply":"2021-11-15T08:49:04.412746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Functions","metadata":{}},{"cell_type":"code","source":"stop_words = stopwords.words('english')\nstemmer = nltk.SnowballStemmer(\"english\")\n\n\ndef read_train():\n    train = pd.read_csv(\n        '../input/tweet-sentiment-extraction/train.csv').dropna()\n    train['text'] = train['text']\n    train['selected_text'] = train['selected_text']\n    return train\n\n\ndef read_test():\n    test = pd.read_csv('../input/tweet-sentiment-extraction/test.csv').dropna()\n    test['text'] = test['text']\n    return test\n\n\ndef clean_text(text):\n    '''\n        Make text lowercase, remove text in square brackets,remove links,remove punctuation\n        and remove words containing numbers.\n    '''\n    text = str(text).lower()\n    pat = r\"[{}]\".format(string.punctuation) # create the pattern\n    text = re.sub(pat, ' ', text) # remove punctuation\n    text = re.sub('  +', ' ', text) # remove spaces\n    text = re.sub('https?://|www\\.', '', text)  # remove urls\n    # text = re.sub('<.*?>', '', text) \n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text) # remove digits\n    return text\n\n\ndef preprocess_data(text):\n    \"\"\"\n       Clean puntuation, urls, and so on, \n       removing stop-words and make stemming\n    \"\"\"\n    text = text if clean_text(text)==0 else clean_text(text)\n    # Remove stop-words\n    text = ' '.join(word for word in text.split()\n                    if word not in stop_words)    # Remove stopwords\n    # Stemm all the words in the sentence\n    # text = ' '.join(stemmer.stem(word) for word in text.split())\n    return text\n\n\ndef wrong_words(text, selected):\n    words = []\n    text = text.split()\n    selected = selected.split()\n    for i in selected:\n        if i not in text:\n            words.append(i)\n    if len(words) > 0:\n        return \" \".join(words)\n    else:\n        return '++++'\n\n\ndef remove_text(x):\n    selected = x[0]\n    spelling = x[1]\n    selected = selected.split()\n    selected.remove(spelling)\n    return \" \".join(selected)\n\n\ndef remove_text_end(x):\n    selected = x[0]\n    spelling = x[1]\n    selected = selected.split()\n    if selected[-1] == spelling:\n        selected.remove(spelling)\n    return \" \".join(selected)\n\n\ndef matching(x, tg):\n    text = x[0]\n    selected = x[1]\n    spelling = x[2]\n    text = text.split()\n    selected = selected.split()\n    spelling = spelling.split()\n    for s in spelling:\n        for t in text:\n            if s in selected:\n                if(fuzz.ratio(t, s) > tg):\n                    index = selected.index(s)\n                    selected[index] = t\n    return \" \".join(selected)\n\n\ndef start_index(x):\n    text = x[0]\n    selected = x[1]\n    text = text.split()\n    selected = selected.split()\n    try:\n        word = selected[0]\n        index = text.index(word)\n    except:\n        index = 0\n    return index\n\n\ndef end_index(x):\n    text = x[0]\n    selected = x[1]\n    start_index = x[2]\n    text = text.split()\n    selected = selected.split()\n    word = selected[-1]\n    try:\n        index = text.index(word, start_index)\n    except:\n        try:\n            index = text.index(word)\n        except:\n            index = len(text) - 1\n    return index\n\n\ndef jaccard(str1, str2):\n    a = set(str1.lower().split())\n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))\n\n\ndef get_text(x):\n    pred = []\n    text = x[0]\n    index = x[1]\n    text = text.split()\n    l = len(text)\n    for i in index:\n        if i < l:\n            pred.append(text[i])\n    return pred","metadata":{"execution":{"iopub.status.busy":"2021-11-15T08:49:04.415825Z","iopub.execute_input":"2021-11-15T08:49:04.4161Z","iopub.status.idle":"2021-11-15T08:49:04.445116Z","shell.execute_reply.started":"2021-11-15T08:49:04.416065Z","shell.execute_reply":"2021-11-15T08:49:04.444195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Load data","metadata":{}},{"cell_type":"code","source":"train = read_train()\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-15T08:49:04.446511Z","iopub.execute_input":"2021-11-15T08:49:04.446766Z","iopub.status.idle":"2021-11-15T08:49:04.576699Z","shell.execute_reply.started":"2021-11-15T08:49:04.446728Z","shell.execute_reply":"2021-11-15T08:49:04.575649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = read_test()\ntest.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-15T08:49:04.578863Z","iopub.execute_input":"2021-11-15T08:49:04.57915Z","iopub.status.idle":"2021-11-15T08:49:04.607167Z","shell.execute_reply.started":"2021-11-15T08:49:04.57911Z","shell.execute_reply":"2021-11-15T08:49:04.606156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Data processing","metadata":{}},{"cell_type":"markdown","source":"#### Clearing text","metadata":{}},{"cell_type":"code","source":"train['clean_text'] = train['text'].apply(preprocess_data)\ntrain['clean_selected'] = train['selected_text'].apply(preprocess_data)\ntest['clean_text'] = test['text'].apply(preprocess_data)","metadata":{"execution":{"iopub.status.busy":"2021-11-15T08:49:04.609118Z","iopub.execute_input":"2021-11-15T08:49:04.609389Z","iopub.status.idle":"2021-11-15T08:49:08.406554Z","shell.execute_reply.started":"2021-11-15T08:49:04.609356Z","shell.execute_reply":"2021-11-15T08:49:08.405531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Removing rows with blank Text or Selected text","metadata":{}},{"cell_type":"code","source":"print(train[train[\"clean_text\"] == ' ']['clean_text'].count())\nprint(train[train[\"clean_selected\"] == ' ']['clean_selected'].count())\ntrain.drop(train[train[\"clean_text\"] == ' '].index, inplace=True)\ntrain.drop(train[train[\"clean_selected\"] == ' '].index, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-15T08:49:08.408035Z","iopub.execute_input":"2021-11-15T08:49:08.408304Z","iopub.status.idle":"2021-11-15T08:49:08.451617Z","shell.execute_reply.started":"2021-11-15T08:49:08.40827Z","shell.execute_reply":"2021-11-15T08:49:08.450996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Working with spelling\n\nLets mark all rows, where selected_text contain symbols not exists in text and count rows with correct spelling","metadata":{}},{"cell_type":"code","source":"train['spelling'] = train.apply(\n    lambda x: wrong_words(x.clean_text, x.clean_selected), axis=1)\ntrain[train['spelling'] == '++++'].count()","metadata":{"execution":{"iopub.status.busy":"2021-11-15T08:49:08.452614Z","iopub.execute_input":"2021-11-15T08:49:08.453169Z","iopub.status.idle":"2021-11-15T08:49:09.321274Z","shell.execute_reply.started":"2021-11-15T08:49:08.453137Z","shell.execute_reply":"2021-11-15T08:49:09.320204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### One or two wrong symbol","metadata":{}},{"cell_type":"code","source":"train[train['spelling'].apply(lambda x: len(x)) <= 2]","metadata":{"execution":{"iopub.status.busy":"2021-11-15T08:49:09.322577Z","iopub.execute_input":"2021-11-15T08:49:09.322837Z","iopub.status.idle":"2021-11-15T08:49:09.365893Z","shell.execute_reply.started":"2021-11-15T08:49:09.322803Z","shell.execute_reply":"2021-11-15T08:49:09.364975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"and remove them","metadata":{}},{"cell_type":"code","source":"train['clean_selected'] = train[['clean_selected', 'spelling']].apply(\n    lambda x: remove_text(x) if len(x['spelling']) <= 2 else x['clean_selected'], axis=1)\ntrain['spelling'] = train.apply(\n    lambda x: wrong_words(x.clean_text, x.clean_selected), axis=1)\ntrain[train['spelling'] == '++++'].count()","metadata":{"execution":{"iopub.status.busy":"2021-11-15T08:49:09.367332Z","iopub.execute_input":"2021-11-15T08:49:09.367919Z","iopub.status.idle":"2021-11-15T08:49:10.749886Z","shell.execute_reply.started":"2021-11-15T08:49:09.367877Z","shell.execute_reply":"2021-11-15T08:49:10.748884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['len_cl_txt'] = train['clean_text'].apply(lambda x: len(x))\ntrain['len_cl_sel'] = train['clean_selected'].apply(lambda x: len(x))","metadata":{"execution":{"iopub.status.busy":"2021-11-15T08:49:10.7551Z","iopub.execute_input":"2021-11-15T08:49:10.755475Z","iopub.status.idle":"2021-11-15T08:49:10.803278Z","shell.execute_reply.started":"2021-11-15T08:49:10.755427Z","shell.execute_reply":"2021-11-15T08:49:10.801787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check lenght on text and selected text\ntrain[train['len_cl_txt'] < train['len_cl_sel']]","metadata":{"execution":{"iopub.status.busy":"2021-11-15T08:49:10.805047Z","iopub.execute_input":"2021-11-15T08:49:10.805407Z","iopub.status.idle":"2021-11-15T08:49:10.828947Z","shell.execute_reply.started":"2021-11-15T08:49:10.805361Z","shell.execute_reply":"2021-11-15T08:49:10.827728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.loc[train['len_cl_txt'] < train['len_cl_sel'], 'clean_selected'] = train.loc[train['len_cl_txt'] < train['len_cl_sel'], 'clean_text']\ntrain['spelling'] = train.apply(\n    lambda x: wrong_words(x.clean_text, x.clean_selected), axis=1)\nprint(train[train['spelling'] == '++++'].count())\ntrain['len_cl_txt'] = train['clean_text'].apply(lambda x: len(x))\ntrain['len_cl_sel'] = train['clean_selected'].apply(lambda x: len(x))\ntrain[train['len_cl_txt'] < train['len_cl_sel']]","metadata":{"execution":{"iopub.status.busy":"2021-11-15T08:49:10.830676Z","iopub.execute_input":"2021-11-15T08:49:10.831012Z","iopub.status.idle":"2021-11-15T08:49:11.76708Z","shell.execute_reply.started":"2021-11-15T08:49:10.830967Z","shell.execute_reply":"2021-11-15T08:49:11.766176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For other cases we will use library fuzzywuzzy, which allow obtain degree of matching two sentences","metadata":{"execution":{"iopub.execute_input":"2021-11-09T07:21:33.432652Z","iopub.status.busy":"2021-11-09T07:21:33.432261Z","iopub.status.idle":"2021-11-09T07:21:33.43904Z","shell.execute_reply":"2021-11-09T07:21:33.438056Z","shell.execute_reply.started":"2021-11-09T07:21:33.432606Z"}}},{"cell_type":"code","source":"print(fuzz.ratio('geeksforgeeks', 'geeksgeeks'))\nprint(fuzz.ratio('GeeksforGeeks', 'GeeksforGeeks'))","metadata":{"execution":{"iopub.status.busy":"2021-11-15T08:49:11.768553Z","iopub.execute_input":"2021-11-15T08:49:11.769119Z","iopub.status.idle":"2021-11-15T08:49:11.774713Z","shell.execute_reply.started":"2021-11-15T08:49:11.76908Z","shell.execute_reply":"2021-11-15T08:49:11.77382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"first step set matching level = 70% and look at result","metadata":{}},{"cell_type":"code","source":"\"\"\"train['selected_text'] = train[['text', 'selected_text', 'spelling']].apply(\n    lambda x: matching(x, 70) if x['spelling'] != '++++' else x['selected_text'], axis=1)\ntrain['spelling'] = train.apply(\n    lambda x: wrong_words(x.text, x.selected_text), axis=1)\ntrain[(train['spelling'] != '++++')]\"\"\"","metadata":{"execution":{"iopub.status.busy":"2021-11-15T08:49:11.776332Z","iopub.execute_input":"2021-11-15T08:49:11.776814Z","iopub.status.idle":"2021-11-15T08:49:11.790199Z","shell.execute_reply.started":"2021-11-15T08:49:11.776771Z","shell.execute_reply":"2021-11-15T08:49:11.789537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"next step - 55%","metadata":{}},{"cell_type":"code","source":"\"\"\"train['selected_text'] = train[['text', 'selected_text', 'spelling']].apply(\n    lambda x: matching(x, 55) if x['spelling'] != '++++' else x['selected_text'], axis=1)\ntrain['spelling'] = train.apply(\n    lambda x: wrong_words(x.text, x.selected_text), axis=1)\ntrain[(train['spelling'] != '++++')]\"\"\"","metadata":{"execution":{"iopub.status.busy":"2021-11-15T08:49:11.792005Z","iopub.execute_input":"2021-11-15T08:49:11.792222Z","iopub.status.idle":"2021-11-15T08:49:11.804061Z","shell.execute_reply.started":"2021-11-15T08:49:11.792192Z","shell.execute_reply":"2021-11-15T08:49:11.803374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And finally - 35%","metadata":{}},{"cell_type":"code","source":"train['clean_selected'] = train[['clean_text', 'clean_selected', 'spelling']].apply(\n    lambda x: matching(x, 35) if x['spelling'] != '++++' else x['clean_selected'], axis=1)\ntrain['spelling'] = train.apply(\n    lambda x: wrong_words(x.clean_text, x.clean_selected), axis=1)\ntrain[(train['spelling'] != '++++')]","metadata":{"execution":{"iopub.status.busy":"2021-11-15T08:49:11.804988Z","iopub.execute_input":"2021-11-15T08:49:11.805235Z","iopub.status.idle":"2021-11-15T08:49:13.204936Z","shell.execute_reply.started":"2021-11-15T08:49:11.805193Z","shell.execute_reply":"2021-11-15T08:49:13.203844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['clean_selected'] = train[['clean_selected', 'spelling']].apply(\n    lambda x: remove_text(x) if len(x['spelling']) <= 2 else x['clean_selected'], axis=1)\ntrain['spelling'] = train.apply(\n    lambda x: wrong_words(x.clean_text, x.clean_selected), axis=1)\ntrain[train['spelling'] == '++++'].count()","metadata":{"execution":{"iopub.status.busy":"2021-11-15T08:49:13.206469Z","iopub.execute_input":"2021-11-15T08:49:13.206739Z","iopub.status.idle":"2021-11-15T08:49:14.59555Z","shell.execute_reply.started":"2021-11-15T08:49:13.206706Z","shell.execute_reply":"2021-11-15T08:49:14.594466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets look at remaining part more carefully","metadata":{}},{"cell_type":"code","source":"train.loc[(train['spelling'] != '++++') & (train['sentiment'] == 'positive')]","metadata":{"execution":{"iopub.status.busy":"2021-11-15T08:49:14.597197Z","iopub.execute_input":"2021-11-15T08:49:14.597476Z","iopub.status.idle":"2021-11-15T08:49:14.623276Z","shell.execute_reply.started":"2021-11-15T08:49:14.59744Z","shell.execute_reply":"2021-11-15T08:49:14.622247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.loc[5189, 'clean_selected'] = 'fun'\ntrain.loc[6395, 'clean_selected'] = 'great'","metadata":{"execution":{"iopub.status.busy":"2021-11-15T08:49:14.624755Z","iopub.execute_input":"2021-11-15T08:49:14.625052Z","iopub.status.idle":"2021-11-15T08:49:14.639842Z","shell.execute_reply.started":"2021-11-15T08:49:14.625017Z","shell.execute_reply":"2021-11-15T08:49:14.638794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.loc[(train['spelling'] != '++++') & (train['sentiment'] == 'negative')]","metadata":{"execution":{"iopub.status.busy":"2021-11-15T08:49:14.64141Z","iopub.execute_input":"2021-11-15T08:49:14.641768Z","iopub.status.idle":"2021-11-15T08:49:14.677274Z","shell.execute_reply.started":"2021-11-15T08:49:14.641733Z","shell.execute_reply":"2021-11-15T08:49:14.676431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.loc[6113, 'clean_selected'] = 'going die'","metadata":{"execution":{"iopub.status.busy":"2021-11-15T08:49:14.678502Z","iopub.execute_input":"2021-11-15T08:49:14.678733Z","iopub.status.idle":"2021-11-15T08:49:14.685031Z","shell.execute_reply.started":"2021-11-15T08:49:14.678704Z","shell.execute_reply":"2021-11-15T08:49:14.684283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.loc[(train['spelling']!='++++') & (train['sentiment']=='neutral')]","metadata":{"execution":{"iopub.status.busy":"2021-11-15T08:49:14.686028Z","iopub.execute_input":"2021-11-15T08:49:14.686431Z","iopub.status.idle":"2021-11-15T08:49:14.716946Z","shell.execute_reply.started":"2021-11-15T08:49:14.686397Z","shell.execute_reply":"2021-11-15T08:49:14.71601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['clean_selected'] = train[['clean_text', 'clean_selected', 'spelling']].apply(\n    lambda x: matching(x, 35) if x['spelling'] != '++++' else x['clean_selected'], axis=1)\ntrain['clean_selected'] = train[['clean_selected', 'spelling']].apply(\n    lambda x: remove_text(x) if len(x['spelling']) == 1 else x['clean_selected'], axis=1)\ntrain['spelling'] = train.apply(\n    lambda x: wrong_words(x.clean_text, x.clean_selected), axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-11-15T08:49:14.718792Z","iopub.execute_input":"2021-11-15T08:49:14.719395Z","iopub.status.idle":"2021-11-15T08:49:16.664428Z","shell.execute_reply.started":"2021-11-15T08:49:14.71933Z","shell.execute_reply":"2021-11-15T08:49:16.6637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['clean_selected'] = train[['clean_text', 'clean_selected', 'spelling']].apply(\n    lambda x: x['clean_text'] if x['spelling'] != '++++' else x['clean_selected'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-11-15T08:49:16.6664Z","iopub.execute_input":"2021-11-15T08:49:16.667168Z","iopub.status.idle":"2021-11-15T08:49:17.194619Z","shell.execute_reply.started":"2021-11-15T08:49:16.667106Z","shell.execute_reply":"2021-11-15T08:49:17.193674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['spelling'] = train.apply(\n    lambda x: wrong_words(x.clean_text, x.clean_selected), axis=1)\ntrain[(train['spelling'] != '++++')]","metadata":{"execution":{"iopub.status.busy":"2021-11-15T08:49:17.196054Z","iopub.execute_input":"2021-11-15T08:49:17.197195Z","iopub.status.idle":"2021-11-15T08:49:18.045244Z","shell.execute_reply.started":"2021-11-15T08:49:17.197148Z","shell.execute_reply":"2021-11-15T08:49:18.044231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.reset_index(inplace=True)\ntrain.drop(['index'], inplace=True, axis=1)\ntrain","metadata":{"execution":{"iopub.status.busy":"2021-11-15T08:49:18.046673Z","iopub.execute_input":"2021-11-15T08:49:18.046919Z","iopub.status.idle":"2021-11-15T08:49:18.077697Z","shell.execute_reply.started":"2021-11-15T08:49:18.046889Z","shell.execute_reply":"2021-11-15T08:49:18.076774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.drop(train[train['clean_selected'].apply(\n    lambda x: len(x)) == 0].index, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-15T08:49:18.078935Z","iopub.execute_input":"2021-11-15T08:49:18.079163Z","iopub.status.idle":"2021-11-15T08:49:18.111279Z","shell.execute_reply.started":"2021-11-15T08:49:18.079135Z","shell.execute_reply":"2021-11-15T08:49:18.110355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['start_index'] = train[['clean_text', 'clean_selected']].apply(\n    lambda x: start_index(x), axis=1)\ntrain['end_index'] = train[['clean_text', 'clean_selected', 'start_index']].apply(\n    lambda x: end_index(x), axis=1)\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-15T08:49:18.11643Z","iopub.execute_input":"2021-11-15T08:49:18.117293Z","iopub.status.idle":"2021-11-15T08:49:19.115139Z","shell.execute_reply.started":"2021-11-15T08:49:18.117231Z","shell.execute_reply":"2021-11-15T08:49:19.114235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[train.start_index > train.end_index]","metadata":{"execution":{"iopub.status.busy":"2021-11-15T08:49:19.11693Z","iopub.execute_input":"2021-11-15T08:49:19.117221Z","iopub.status.idle":"2021-11-15T08:49:19.152407Z","shell.execute_reply.started":"2021-11-15T08:49:19.117183Z","shell.execute_reply":"2021-11-15T08:49:19.15143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train[train.start_index <= train.end_index]\ntrain[train.start_index > train.end_index]","metadata":{"execution":{"iopub.status.busy":"2021-11-15T08:49:19.153982Z","iopub.execute_input":"2021-11-15T08:49:19.154432Z","iopub.status.idle":"2021-11-15T08:49:19.175286Z","shell.execute_reply.started":"2021-11-15T08:49:19.154382Z","shell.execute_reply":"2021-11-15T08:49:19.174278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.reset_index(inplace=True)\ntrain.drop(['index'], inplace=True, axis=1)\nprint(train.shape)\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-15T08:49:19.176567Z","iopub.execute_input":"2021-11-15T08:49:19.176792Z","iopub.status.idle":"2021-11-15T08:49:19.210962Z","shell.execute_reply.started":"2021-11-15T08:49:19.176764Z","shell.execute_reply":"2021-11-15T08:49:19.209847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-15T08:49:19.212309Z","iopub.execute_input":"2021-11-15T08:49:19.212574Z","iopub.status.idle":"2021-11-15T08:49:19.227742Z","shell.execute_reply.started":"2021-11-15T08:49:19.212541Z","shell.execute_reply":"2021-11-15T08:49:19.226728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Max tweet length\ntext_split = train['clean_text'].apply(lambda x: len(str(x).split())).tolist()\nmax(text_split)","metadata":{"execution":{"iopub.status.busy":"2021-11-15T08:49:19.22915Z","iopub.execute_input":"2021-11-15T08:49:19.229944Z","iopub.status.idle":"2021-11-15T08:49:19.275739Z","shell.execute_reply.started":"2021-11-15T08:49:19.229887Z","shell.execute_reply":"2021-11-15T08:49:19.274939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_ = train.loc[train.sentiment != 'neutral'].reset_index(drop=True, inplace=False)\n\ny = np.zeros((train.shape[0], max(text_split)+1))\nfor i in range(train.shape[0]):\n    start = train['start_index'][i]\n    end = train['end_index'][i]\n    y[i][start:end+1] = 1","metadata":{"execution":{"iopub.status.busy":"2021-11-15T08:49:19.27714Z","iopub.execute_input":"2021-11-15T08:49:19.277653Z","iopub.status.idle":"2021-11-15T08:49:19.789713Z","shell.execute_reply.started":"2021-11-15T08:49:19.277618Z","shell.execute_reply":"2021-11-15T08:49:19.788905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in [1, 6, 11, 22]:\n    print(train['start_index'][i], train['end_index'][i], '\\n')\n    print(train['clean_text'][i], '\\n')\n    print(train['clean_selected'][i], '\\n')\n    print(y[i])\n    print(\"=\"*150)","metadata":{"execution":{"iopub.status.busy":"2021-11-15T08:49:19.791143Z","iopub.execute_input":"2021-11-15T08:49:19.791662Z","iopub.status.idle":"2021-11-15T08:49:19.808016Z","shell.execute_reply.started":"2021-11-15T08:49:19.791627Z","shell.execute_reply":"2021-11-15T08:49:19.807237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-15T08:49:19.809519Z","iopub.execute_input":"2021-11-15T08:49:19.809752Z","iopub.status.idle":"2021-11-15T08:49:19.816271Z","shell.execute_reply.started":"2021-11-15T08:49:19.809722Z","shell.execute_reply":"2021-11-15T08:49:19.815268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = train[['textID', 'clean_text', 'clean_selected', 'sentiment']]\nX_train, X_valid, y_train, y_valid = train_test_split(\n    X, y, test_size=0.15, random_state=42)\nprint(\"X_train shape \", X_train.shape, \"  X_test shape \", X_valid.shape)\nprint(\"y_train shape \", y_train.shape, \"  y_test shape \", y_valid.shape)","metadata":{"execution":{"iopub.status.busy":"2021-11-15T08:49:19.817904Z","iopub.execute_input":"2021-11-15T08:49:19.818185Z","iopub.status.idle":"2021-11-15T08:49:19.845187Z","shell.execute_reply.started":"2021-11-15T08:49:19.818142Z","shell.execute_reply":"2021-11-15T08:49:19.844322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train = np.expand_dims(y_train, -1)\ny_valid = np.expand_dims(y_valid, -1)\ny_train.shape, y_valid.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-15T08:49:19.846711Z","iopub.execute_input":"2021-11-15T08:49:19.846961Z","iopub.status.idle":"2021-11-15T08:49:19.853536Z","shell.execute_reply.started":"2021-11-15T08:49:19.846933Z","shell.execute_reply":"2021-11-15T08:49:19.852624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_text = X_train['clean_text'].values\nvalid_text = X_valid['clean_text'].values\ntrain_sentiment = X_train['sentiment'].values\nvalid_sentiment = X_valid['sentiment'].values","metadata":{"execution":{"iopub.status.busy":"2021-11-15T08:49:19.854931Z","iopub.execute_input":"2021-11-15T08:49:19.855942Z","iopub.status.idle":"2021-11-15T08:49:19.866938Z","shell.execute_reply.started":"2021-11-15T08:49:19.855895Z","shell.execute_reply":"2021-11-15T08:49:19.865811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Tokenizing","metadata":{}},{"cell_type":"code","source":"# using keras tokenizer here\ntoken1 = text.Tokenizer(num_words=None)\nmax_len_text = y_train.shape[1]-1\n\ntoken1.fit_on_texts(list(train_text))\ntrain_text = token1.texts_to_sequences(train_text)\nvalid_text = token1.texts_to_sequences(valid_text)\n\n\n# zero pad the sequences\ntrain_text = sequence.pad_sequences(\n    train_text, maxlen=max_len_text, padding='post')\nvalid_text = sequence.pad_sequences(\n    valid_text, maxlen=max_len_text, padding='post')\n\nword_index_text = token1.word_index\n# print(word_index_text)\nprint(train_text.shape, valid_text.shape)\n\n# using keras tokenizer here\ntoken2 = text.Tokenizer(num_words=None)\nmax_len_sentiment = 1\n\ntoken2.fit_on_texts(list(train_sentiment))\ntrain_sentiment = token2.texts_to_sequences(train_sentiment)\nvalid_sentiment = token2.texts_to_sequences(valid_sentiment)\n\n\n# zero pad the sequences\ntrain_sentiment = sequence.pad_sequences(\n    train_sentiment, maxlen=max_len_sentiment, padding='post')\nvalid_sentiment = sequence.pad_sequences(\n    valid_sentiment, maxlen=max_len_sentiment, padding='post')\n\nword_index_sentiment = token2.word_index\nprint(word_index_sentiment)\nprint(train_sentiment.shape, valid_sentiment.shape)","metadata":{"execution":{"iopub.status.busy":"2021-11-15T08:49:19.868508Z","iopub.execute_input":"2021-11-15T08:49:19.868802Z","iopub.status.idle":"2021-11-15T08:49:21.336326Z","shell.execute_reply.started":"2021-11-15T08:49:19.868767Z","shell.execute_reply":"2021-11-15T08:49:21.335179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Prepare embeddings","metadata":{}},{"cell_type":"code","source":"# load the GloVe vectors in a dictionary:\nembeddings_index = {}\nwith open('/kaggle/input/glove840b300dtxt/glove.840B.300d.txt') as f:\n    for line in tqdm(f):\n        values = line.split(' ')\n        word = values[0]\n        coefs = np.asarray([float(val) for val in values[1:]])\n        embeddings_index[word] = coefs\nprint('Found %s word vectors.' % len(embeddings_index))","metadata":{"execution":{"iopub.status.busy":"2021-11-15T08:49:21.33817Z","iopub.execute_input":"2021-11-15T08:49:21.339099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create an embedding matrix for the words we have in the dataset\nembedding_matrix_text = np.zeros((len(word_index_text) + 1, 300))\nfor word, i in tqdm(word_index_text.items()):\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix_text[i] = embedding_vector\n\nprint(embedding_matrix_text.shape)\n\n# create an embedding matrix for the words we have in the dataset\nembedding_matrix_sentiment = np.zeros((len(word_index_sentiment) + 1, 300))\nfor word, i in tqdm(word_index_sentiment.items()):\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix_sentiment[i] = embedding_vector\n\nprint(embedding_matrix_sentiment.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model","metadata":{}},{"cell_type":"code","source":"text_input = Input(shape=(max_len_text,), name='text_input')\nembd_text = Embedding(len(word_index_text)+1,  # embedding layer with glove vectors as embeddings\n                      300,\n                      weights=[embedding_matrix_text],\n                      input_length=max_len_text,\n                      trainable=False, mask_zero=True, name='embedding_text')(text_input)  # masking the input values with mask_zero= True\n\n\nsentiment_input = Input(shape=(max_len_sentiment,), name='sentiment_input')\nembd_sentiment = Embedding(len(word_index_sentiment)+1,  # embedding layer with glove vectors as embeddings\n                           300,\n                           weights=[embedding_matrix_sentiment],\n                           input_length=max_len_text,\n                           trainable=False, mask_zero=True, name='embedding_sentiment')(sentiment_input)  # masking the input values with mask_zero= True\n\n\ncon = Concatenate(axis=1)([embd_text, embd_sentiment])\n\nlstm = Bidirectional(LSTM(128, return_sequences=True,\n                          dropout=0.15, name='LSTM'))(con)  # lstm\n\n# dense layers with drop outs and batch normalization\nm = Dense(128, activation=\"relu\",\n          kernel_regularizer=regularizers.l2(0.0001))(lstm)\nm = Dropout(0.3)(m)\nm = BatchNormalization()(m)\noutput = Dense(1, activation='sigmoid', name='output')(m)\n\nmodel = Model(inputs=[text_input, sentiment_input], outputs=[output])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.utils.plot_model(\n    model, 'Model.png', show_shapes=True, show_layer_names=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"log_dir = os.path.join(\n    \"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\ntensorboard = tf.keras.callbacks.TensorBoard(\n    log_dir=log_dir, histogram_freq=1, write_graph=True, write_grads=True)\n\ncheckpoint_filepath = 'LSTM_model.h5'\nmodel_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_filepath, monitor='val_loss', save_best_only=True, verbose=1)\nadam = optimizers.Adam(0.001)\n\nmodel.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\ncallback = [model_checkpoint_callback, tensorboard]\n\nhistory = model.fit([train_text, train_sentiment],\n                    y_train,\n                    epochs=10,\n                    batch_size=64,\n                    validation_data=([valid_text, valid_sentiment], [y_valid]),\n                    verbose=1,\n                    callbacks=callback)","metadata":{"execution":{"iopub.status.idle":"2021-11-15T09:18:18.245105Z","shell.execute_reply.started":"2021-11-15T08:54:47.749183Z","shell.execute_reply":"2021-11-15T09:18:18.243889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(7, 4))\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'])\nplt.grid()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-15T09:18:18.248144Z","iopub.execute_input":"2021-11-15T09:18:18.249178Z","iopub.status.idle":"2021-11-15T09:18:18.550486Z","shell.execute_reply.started":"2021-11-15T09:18:18.249133Z","shell.execute_reply":"2021-11-15T09:18:18.549491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(7, 4))\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.ylabel('Accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'])\nplt.grid()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-15T09:18:18.551797Z","iopub.execute_input":"2021-11-15T09:18:18.552043Z","iopub.status.idle":"2021-11-15T09:18:18.796319Z","shell.execute_reply.started":"2021-11-15T09:18:18.55201Z","shell.execute_reply":"2021-11-15T09:18:18.795219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_pred = model.predict([train_text, train_sentiment])\ntrain_pred = np.squeeze(train_pred)\ntrain_pred = np.round(train_pred)\ntrain_pred.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-15T09:18:18.797784Z","iopub.execute_input":"2021-11-15T09:18:18.798049Z","iopub.status.idle":"2021-11-15T09:19:11.130511Z","shell.execute_reply.started":"2021-11-15T09:18:18.798015Z","shell.execute_reply":"2021-11-15T09:19:11.129545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = []\nfor vector in train_pred:\n    index = []\n    for i, value in enumerate(vector):\n        if value == 1:\n            index.append(i)\n    pred.append(np.array(index))\nprint(len(pred))","metadata":{"execution":{"iopub.status.busy":"2021-11-15T09:19:11.132816Z","iopub.execute_input":"2021-11-15T09:19:11.133627Z","iopub.status.idle":"2021-11-15T09:19:13.675606Z","shell.execute_reply.started":"2021-11-15T09:19:11.133576Z","shell.execute_reply":"2021-11-15T09:19:13.674578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train['prediction'] = pred\npred_text = X_train[['clean_text', 'prediction']].apply(\n    lambda x: get_text(x), axis=1)\nX_train['pred_text'] = pred_text\nX_train['pred_text'] = X_train['pred_text'].apply(lambda x: ' '.join(x))\nX_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-15T09:19:13.676889Z","iopub.execute_input":"2021-11-15T09:19:13.677144Z","iopub.status.idle":"2021-11-15T09:19:14.533383Z","shell.execute_reply.started":"2021-11-15T09:19:13.677111Z","shell.execute_reply":"2021-11-15T09:19:14.532537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train['jaccard'] = X_train.apply(\n    lambda x: jaccard(x.clean_selected, x.pred_text), axis=1)\nprint('Mean training Jaccard score:', np.mean(X_train['jaccard']))\nprint(\"=\"*150)\nprint('nMean jaccard score for positive sentiment tweets:', np.mean(\n    X_train[X_train['sentiment'] == 'positive']['jaccard']))\nprint(\"=\"*150)\nprint('Mean jaccard score for negative sentiment tweets', np.mean(\n    X_train[X_train['sentiment'] == 'negative']['jaccard']))\nprint(\"=\"*150)\nprint('Mean jaccard score for neutral sentiment tweets', np.mean(\n    X_train[X_train['sentiment'] == 'neutral']['jaccard']))","metadata":{"execution":{"iopub.status.busy":"2021-11-15T09:19:14.534751Z","iopub.execute_input":"2021-11-15T09:19:14.534989Z","iopub.status.idle":"2021-11-15T09:19:15.32722Z","shell.execute_reply.started":"2021-11-15T09:19:14.53496Z","shell.execute_reply":"2021-11-15T09:19:15.326128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid_pred = model.predict([valid_text, valid_sentiment])\nvalid_pred = np.squeeze(valid_pred)\nvalid_pred = np.round(valid_pred)\nvalid_pred.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-15T09:19:15.328669Z","iopub.execute_input":"2021-11-15T09:19:15.329115Z","iopub.status.idle":"2021-11-15T09:19:24.94263Z","shell.execute_reply.started":"2021-11-15T09:19:15.329064Z","shell.execute_reply":"2021-11-15T09:19:24.941494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = []\nfor vector in valid_pred:\n    index = []\n    for i, value in enumerate(vector):\n        if value == 1:\n            index.append(i)\n    pred.append(np.array(index))\nprint(len(pred))","metadata":{"execution":{"iopub.status.busy":"2021-11-15T09:19:24.944559Z","iopub.execute_input":"2021-11-15T09:19:24.944906Z","iopub.status.idle":"2021-11-15T09:19:25.41815Z","shell.execute_reply.started":"2021-11-15T09:19:24.94486Z","shell.execute_reply":"2021-11-15T09:19:25.417279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_valid['prediction'] = pred\npred_text = X_valid[['clean_text', 'prediction']].apply(\n    lambda x: get_text(x), axis=1)\nX_valid['pred_text'] = pred_text\nX_valid['pred_text'] = X_valid['pred_text'].apply(lambda x: ' '.join(x))\nX_valid.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-15T09:19:25.419522Z","iopub.execute_input":"2021-11-15T09:19:25.419795Z","iopub.status.idle":"2021-11-15T09:19:25.590174Z","shell.execute_reply.started":"2021-11-15T09:19:25.419753Z","shell.execute_reply":"2021-11-15T09:19:25.589267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_valid['jaccard'] = X_valid.apply(\n    lambda x: jaccard(x.clean_selected, x.pred_text), axis=1)\nprint('Mean training Jaccard score:', np.mean(X_valid['jaccard']))\nprint(\"=\"*150)\nprint('Mean jaccard score for positive sentiment tweets:', np.mean(\n    X_valid[X_valid['sentiment'] == 'positive']['jaccard']))\nprint(\"=\"*150)\nprint('Mean jaccard score for negative sentiment tweets', np.mean(\n    X_valid[X_valid['sentiment'] == 'negative']['jaccard']))\nprint(\"=\"*150)\nprint('Mean jaccard score for neutral sentiment tweets', np.mean(\n    X_valid[X_valid['sentiment'] == 'neutral']['jaccard']))","metadata":{"execution":{"iopub.status.busy":"2021-11-15T09:19:25.591517Z","iopub.execute_input":"2021-11-15T09:19:25.591776Z","iopub.status.idle":"2021-11-15T09:19:25.756286Z","shell.execute_reply.started":"2021-11-15T09:19:25.591744Z","shell.execute_reply":"2021-11-15T09:19:25.755192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = load_model('LSTM_model.h5')\ntest_text = test['clean_text'].values\ntest_sentiment = test['sentiment'].values\ntest_text = token1.texts_to_sequences(test_text)\ntest_text = sequence.pad_sequences(\n    test_text, maxlen=max_len_text, padding='post')\ntest_sentiment = token2.texts_to_sequences(test_sentiment)\ntest_sentiment = sequence.pad_sequences(\n    test_sentiment, maxlen=max_len_sentiment, padding='post')\n\ntest_pred = model.predict([test_text, test_sentiment])\ntest_pred = np.squeeze(test_pred)\ntest_pred = np.round(test_pred)\ntest_pred.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-15T09:19:25.758814Z","iopub.execute_input":"2021-11-15T09:19:25.759702Z","iopub.status.idle":"2021-11-15T09:19:35.747465Z","shell.execute_reply.started":"2021-11-15T09:19:25.759647Z","shell.execute_reply":"2021-11-15T09:19:35.746534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = []\nfor vector in test_pred:\n    index = []\n    for i, value in enumerate(vector):\n        if value == 1:\n            index.append(i)\n    pred.append(np.array(index))\nprint(len(pred))\n\ntest['prediction'] = pred\npred_text = test[['clean_text', 'prediction']].apply(lambda x: get_text(x), axis=1)\ntest['selected_text'] = pred_text\ntest['selected_text'] = test['selected_text'].apply(lambda x: ' '.join(x))\ntest.drop(['text', 'sentiment', 'prediction'], axis=1, inplace=True)\ntest = test[['textID', 'selected_text']]\ntest.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-15T09:19:35.749647Z","iopub.execute_input":"2021-11-15T09:19:35.750426Z","iopub.status.idle":"2021-11-15T09:19:36.226841Z","shell.execute_reply.started":"2021-11-15T09:19:35.750376Z","shell.execute_reply":"2021-11-15T09:19:36.225857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-15T09:19:36.228073Z","iopub.execute_input":"2021-11-15T09:19:36.228308Z","iopub.status.idle":"2021-11-15T09:19:36.254266Z","shell.execute_reply.started":"2021-11-15T09:19:36.22828Z","shell.execute_reply":"2021-11-15T09:19:36.253428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}