{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport utils\nimport transformers\nimport tokenizers\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup\nfrom tqdm.autonotebook import tqdm\nimport torch.nn.functional as F","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN = 160\nTRAIN_BATCH_SIZE = 32\nVALID_BATCH_SIZE = 8\nEPOCHS = 4\n\nROBERTA_PATH = \"../input/roberta-base\"\nTRAINING_FILE = \"../input/tweet-train-folds/train_folds.csv\"\n\nTOKENIZER = tokenizers.ByteLevelBPETokenizer(\n    vocab_file = \"../input/roberta-base/vocab.json\",\n    merges_file = \"../input/roberta-base/merges.txt\",\n    lowercase = True,\n    add_prefix_space = True\n)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"def process_data(tweet, selected_text, sentiment, tokenizer, max_len):\n    tweet = \" \" + \" \".join(str(tweet).split())    # 在推特文本的第一个词前加空格\n    selected_text = \" \" + \" \".join(str(selected_text).split())    # 在情感文本的第一个词前加空格\n\n    len_st = len(selected_text) - 1    # len(情感文本)-1：不考虑首空格\n    idx0 = -1    # 情感文本第一个词在推特文本中的索引\n    idx1 = -1    # 情感文本最后一个词在推特文本中的索引\n\n    # 确定情感文本范围（字符级）\n    for i in [index for index, c in enumerate(tweet) if c == selected_text[1]]:\n        if \" \" + tweet[i: i + len_st] == selected_text:    # 左闭右开\n            idx0 = i\n            idx1 = i + len_st - 1\n            break\n\n    # 将 情感文本对应的全部字符 在推特文本中的索引设为1（字符级）\n    char_targets = [0] * len(tweet)\n    if idx0 != -1 and idx1 != -1:\n        for i in range(idx0, idx1 + 1):    # 左闭右开\n            char_targets[i]= 1\n\n    tok_tweet = tokenizer.encode(tweet)    # 分词后的推特文本（词级）\n    # print(tok_tweet.tokens)\n    input_ids_orig = tok_tweet.ids    # 各词语id\n    tweet_offsets_orig = tok_tweet.offsets    # 各词语首尾跨度（左闭右开）\n    # print(tweet_offsets_orig)\n\n    # 情感文本对应的全部词 在推特文本中的索引（词级）\n    target_idx = []\n    for i, (offset1, offset2) in enumerate(tweet_offsets_orig):\n        # 情感文本对应的词\n        if sum(char_targets[offset1: offset2]) > 0:\n            target_idx.append(i)\n\n    # 情感文本的首尾索引\n    targets_start = target_idx[0]\n    targets_end = target_idx[-1]\n\n    # print(\"positive\", tokenizer.encode(\"positive\").ids)\n    # print(\"negative\", tokenizer.encode(\"negative\").ids)\n    # print(\"neutral\", tokenizer.encode(\"neutral\").ids)\n    sentiment_id = {\"positive\": 1313, \"negative\": 2430, \"neutral\": 7974}\n\n    # \"<s>\"：0，\"</s>\"：2\n    input_ids = [0] + [sentiment_id[sentiment]] + [2] + [0] + input_ids_orig + [2]\n\n    token_type_ids = [0] * 4 + [0] * (len(input_ids_orig) + 1)\n    mask = [1] * len(token_type_ids)\n\n    tweet_offsets = [(0, 0)] * 4 + tweet_offsets_orig + [(0, 0)]\n\n    targets_start += 4\n    targets_end += 4\n\n    padding_length = max_len - len(input_ids)\n    if padding_length > 0:\n        input_ids = input_ids + ([1] * padding_length)    # \"<pad>\"：1\n        mask = mask + ([0] * padding_length)    # padding部分设置为0\n        token_type_ids = token_type_ids + ([0] * padding_length)\n        tweet_offsets = tweet_offsets + ([(0, 0)] * padding_length)\n\n    return {\n        \"ids\": input_ids,    # 各词语id\n        \"mask\": mask,    # mask向量\n        \"token_type_ids\": token_type_ids,\n        \"targets_start\": targets_start,    # 情感文本首单词索引\n        \"targets_end\": targets_end,    # 情感文本尾单词索引\n        \"orig_tweet\": tweet,    # 推特文本\n        \"orig_selected\": selected_text,    # 情感文本\n        \"sentiment\": sentiment,    # 情感标签\n        \"offsets\": tweet_offsets    # 各词语首尾跨度\n    }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TweetDataset:\n    def __init__(self, tweet, sentiment, selected_text):\n        self.tweet = tweet    # 推特文本\n        self.sentiment = sentiment    # 情感标签\n        self.seleted_text = selected_text    # 情感文本\n        self.tokenizer = TOKENIZER    # 分词\n        self.max_len = MAX_LEN    # 最大长度\n\n    def __len__(self):\n        return len(self.tweet)\n\n    def __getitem__(self, item):\n        data = process_data(\n            self.tweet[item],\n            self.seleted_text[item],\n            self.sentiment[item],\n            self.tokenizer,\n            self.max_len\n        )\n\n        return {\n            \"ids\": torch.tensor(data[\"ids\"], dtype = torch.long),\n            \"mask\": torch.tensor(data[\"mask\"], dtype = torch.long),\n            \"token_type_ids\": torch.tensor(data[\"token_type_ids\"], dtype = torch.long),\n            \"targets_start\": torch.tensor(data[\"targets_start\"], dtype = torch.long),\n            \"targets_end\": torch.tensor(data[\"targets_end\"], dtype = torch.long),\n            \"orig_tweet\": data[\"orig_tweet\"],\n            \"orig_selected\": data[\"orig_selected\"],\n            \"sentiment\": data[\"sentiment\"],\n            \"offsets\": torch.tensor(data[\"offsets\"], dtype = torch.long)\n        }","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"hidden_size = 768\nclass TweetModel(transformers.BertPreTrainedModel):\n    def __init__(self, conf):\n        super(TweetModel, self).__init__(conf)\n        # 预训练的roberta模型\n        self.roberta = transformers.RobertaModel.from_pretrained(ROBERTA_PATH, config = conf)      \n        self.drop_out = nn.Dropout(0.5)        \n        self.w0 = nn.Linear(hidden_size * 6, hidden_size * 6)\n        self.w1 = nn.Linear(hidden_size * 6, hidden_size * 2) \n        self.w2 = nn.Parameter(torch.zeros((768 * 2,768 * 2),dtype=torch.float32))\n        self.w3 = nn.Linear(hidden_size * 6,hidden_size * 2)\n#         nn.init.xavier_normal_(self.w0.weight.data)\n        nn.init.xavier_normal_(self.w2)\n#         nn.init.xavier_normal_(self.w1.weight.data)\n#         nn.init.xavier_normal_(self.w3.weight.data)\n        # roberta-base隐藏状态的维度是768\n        self.lstm = nn.LSTM(input_size = hidden_size * 6, hidden_size = hidden_size, num_layers = 1, bidirectional = True, batch_first = True)\n        # 两维（情感文本首词概率，情感文本末词概率）\n        self.l0 = nn.Linear(hidden_size * 2, 2)\n#         nn.init.xavier_normal_(self.l0.weight.data)\n        self.tanh = torch.nn.Tanh()\n        self.softmax = torch.nn.Softmax(dim = -1)\n        self.relu = torch.nn.ReLU()\n        \n    def forward(self, ids, mask, token_type_ids):\n        # bert层数 x batch_size x 序列长度(160) x 768 = 13 x 24 x 160 x 768\n        _, _, out = self.roberta(ids, attention_mask = mask, token_type_ids = token_type_ids)\n\n        out = torch.cat((out[0],out[4], out[5], out[10], out[11], out[12]), dim=-1) # [24, 160, 768 * 13]\n        out = self.drop_out(out)\n        out = self.w0(out)\n        sentiment = out[:,1,:] # batch_size,hidden_size * 6\n        sentiment = self.w1(sentiment) # batch_size,hidden_size * 2\n        sentence = out[:,3:,:] # batch_size ,seq_len - 3,hidden_size * 6\n\n        sentence_out = self.w3(sentence)\n        sentence_out = torch.matmul(sentence_out,self.w2)\n        sentence_out = sentence_out.permute(1,0,2) # seq_len - 3,batch_size,hidden * 2\n        sentence_out = torch.mul(sentence_out,sentiment)\n        sentence_out = sentence_out.permute(1,0,2)\n        sentence_out = self.tanh(sentence_out)\n        logits = self.l0(sentence_out)\n        # batch_size x 序列长度(160) x 2 -> batch_size x 序列长度(160) x 1，batch_size x 序列长度(160) x 1\n        start_logits, end_logits = logits.split(1, dim = -1)\n\n        # batch_size x 序列长度(160)\n        start_logits = start_logits.squeeze(-1)\n\n        # batch_size x 序列长度(160)\n        end_logits = end_logits.squeeze(-1)\n\n        return start_logits, end_logits","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 损失函数\ndef loss_fn(start_logits, end_logits, start_positions, end_positions):\n    loss_fct = nn.CrossEntropyLoss()    # 交叉熵\n    # 情感文本第一个词的概率\n    start_loss = loss_fct(start_logits, start_positions - 3)\n    # 情感文本最后一个词的概率\n    end_loss = loss_fct(end_logits, end_positions - 3)\n    # 总损失\n    total_loss = start_loss + end_loss\n    return total_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 训练函数\ndef train_fn(data_loader, model, optimizer, device, scheduler = None):\n    model.train()    # 训练模式\n    losses = utils.AverageMeter()\n    jaccards = utils.AverageMeter()\n\n    tk0 = tqdm(data_loader, total = len(data_loader))\n    # 按batch读取\n    for bi, d in enumerate(tk0):\n        ids = d[\"ids\"].to(device, dtype = torch.long)\n        mask = d[\"mask\"].to(device, dtype = torch.long)\n        token_type_ids = d[\"token_type_ids\"].to(device, dtype = torch.long)\n\n        targets_start = d[\"targets_start\"].to(device, dtype = torch.long)\n        targets_end = d[\"targets_end\"].to(device, dtype = torch.long)\n\n        orig_tweet =  d[\"orig_tweet\"]\n        orig_selected = d[\"orig_selected\"]\n        sentiment =  d[\"sentiment\"]\n        offsets = d[\"offsets\"]\n\n        model.zero_grad()\n        # batch_size x 序列长度(192)，batch_size x 序列长度(192)\n        outputs_start, outputs_end = model(ids = ids, mask = mask, token_type_ids = token_type_ids)\n\n        loss = loss_fn(outputs_start, outputs_end, targets_start, targets_end)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n\n        outputs_start = torch.softmax(outputs_start, dim = 1).cpu().detach().numpy()\n        outputs_end = torch.softmax(outputs_end, dim = 1).cpu().detach().numpy()\n\n        jaccard_scores = []\n        for i, tweet in enumerate(orig_tweet):\n            selected_tweet = orig_selected[i]\n            tweet_sentiment = sentiment[i]\n            jaccard_score, _ = calculate_jaccard_score(\n                original_tweet = tweet,\n                target_string = selected_tweet,\n                sentiment_val = tweet_sentiment,\n                idx_start = np.argmax(outputs_start[i, :]) + 3,\n                idx_end = np.argmax(outputs_end[i, :]) + 3,\n                offsets = offsets[i]\n            )\n            jaccard_scores.append(jaccard_score)\n        jaccards.update(np.mean(jaccard_scores), ids.size(0))\n        losses.update(loss.item(), ids.size(0))\n        # 打印loss和jaccard\n        tk0.set_postfix(loss = losses.avg, jaccard = jaccards.avg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 评价指标jaccard\ndef calculate_jaccard_score(\n        original_tweet,\n        target_string,\n        sentiment_val,\n        idx_start,\n        idx_end,\n        offsets,\n        verbose = False\n):\n    if idx_end < idx_start:\n        idx_end = idx_start\n\n    filtered_output = \"\"\n    for ix in range(idx_start, idx_end + 1):\n            filtered_output += original_tweet[offsets[ix][0]: offsets[ix][1]]\n\n    jac = utils.jaccard(target_string, filtered_output)\n    return jac, filtered_output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 测试函数\ndef eval_fn(data_loader, model, device):\n    model.eval()    # 测试模式\n    losses = utils.AverageMeter()\n    jaccards = utils.AverageMeter()\n\n    with torch.no_grad():\n        tk0 = tqdm(data_loader, total = len(data_loader))\n        # 按batch读取\n        for bi, d in enumerate(tk0):\n            ids = d[\"ids\"].to(device, dtype = torch.long)\n            mask = d[\"mask\"].to(device, dtype = torch.long)\n            token_type_ids = d[\"token_type_ids\"].to(device, dtype = torch.long)\n\n            targets_start = d[\"targets_start\"].to(device, dtype = torch.long)\n            targets_end = d[\"targets_end\"].to(device, dtype = torch.long)\n\n            orig_tweet = d[\"orig_tweet\"]\n            orig_selected = d[\"orig_selected\"]\n            sentiment = d[\"sentiment\"]\n            offsets = d[\"offsets\"]\n\n            # batch_size x 序列长度(192)，batch_size x 序列长度(192)\n            outputs_start, outputs_end = model(ids = ids, mask = mask, token_type_ids = token_type_ids)\n\n            loss = loss_fn(outputs_start, outputs_end, targets_start, targets_end)\n\n            outputs_start = torch.softmax(outputs_start, dim = 1).cpu().detach().numpy()\n            outputs_end = torch.softmax(outputs_end, dim = 1).cpu().detach().numpy()\n\n            jaccard_scores = []\n            for i, tweet in enumerate(orig_tweet):\n                selected_tweet = orig_selected[i]\n                tweet_sentiment = sentiment[i]\n                jaccard_score, _ = calculate_jaccard_score(\n                    original_tweet = tweet,\n                    target_string = selected_tweet,\n                    sentiment_val = tweet_sentiment,\n                    idx_start = np.argmax(outputs_start[i, :]) + 3,\n                    idx_end = np.argmax(outputs_end[i, :]) + 3,\n                    offsets = offsets[i],\n                    verbose = False\n                )\n                jaccard_scores.append(jaccard_score)\n            jaccards.update(np.mean(jaccard_scores), ids.size(0))\n            losses.update(loss.item(), ids.size(0))\n            # 打印loss和jaccard\n            tk0.set_postfix(loss = losses.avg, jaccard = jaccards.avg)\n    print(\"Jaccard = \", jaccards.avg)\n    return jaccards.avg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\ndef run(fold):\n    dfx = pd.read_csv(TRAINING_FILE)\n\n    df_train = dfx[dfx.kfold != fold].reset_index(drop = True)\n    df_valid = dfx[dfx.kfold == fold].reset_index(drop = True)\n    \n    # 训练集\n    train_dataset = TweetDataset(\n        tweet = df_train.text.values,\n        sentiment = df_train.sentiment.values,\n        selected_text = df_train.selected_text.values\n    )\n    train_data_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size = TRAIN_BATCH_SIZE,\n        shuffle = True,\n        num_workers = 4,\n        drop_last = True\n    )\n    # 验证集\n    valid_dataset = TweetDataset(\n        tweet = df_valid.text.values,\n        sentiment = df_valid.sentiment.values,\n        selected_text = df_valid.selected_text.values\n    )\n    valid_data_loader = torch.utils.data.DataLoader(\n        valid_dataset,\n        batch_size = VALID_BATCH_SIZE,\n        shuffle = False,\n        num_workers = 2,\n        drop_last = True\n    )\n\n    device = torch.device(\"cuda\")\n    model_config = transformers.RobertaConfig.from_pretrained(ROBERTA_PATH)\n    model_config.output_hidden_states = True\n    model = TweetModel(conf = model_config)\n    model.to(device)\n\n    num_train_steps = int(len(df_train) / TRAIN_BATCH_SIZE * EPOCHS)\n    param_optimizer = list(model.named_parameters())\n    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n    optimizer_parameters = [\n        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n    ]\n    \n    optimizer = AdamW(optimizer_parameters, lr = 2e-5)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, \n        num_warmup_steps = 200, \n        num_training_steps = num_train_steps\n    )\n\n    es = utils.EarlyStopping(patience = 3, mode = \"max\")\n    print(\"Training is Starting for fold\", fold)\n    \n    for epoch in range(EPOCHS):\n        train_fn(train_data_loader, model, optimizer, device, scheduler = scheduler)\n        jaccard = eval_fn(valid_data_loader, model, device)\n        print(\"Jaccard Score = \", jaccard)\n        es(jaccard, model, model_path = f\"model_{fold}.bin\")\n        if es.early_stop:\n            print(\"Early stopping\")\n            break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"run(fold = 0)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"run(fold = 1)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"run(fold = 2)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"run(fold = 3)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"run(fold = 4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = pd.read_csv(\"../input/tweet-sentiment-extraction/test.csv\")\ndf_test.loc[:, \"selected_text\"] = df_test.text.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda\")\nmodel_config = transformers.RobertaConfig.from_pretrained(ROBERTA_PATH)\nmodel_config.output_hidden_states = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"model1 = TweetModel(conf = model_config)\nmodel1.to(device)\nmodel1.load_state_dict(torch.load(\"../input/qa-dot/model_0.bin\"))\nmodel1.eval()\n\nmodel2 = TweetModel(conf = model_config)\nmodel2.to(device)\nmodel2.load_state_dict(torch.load(\"../input/qa-dot/model_1.bin\"))\nmodel2.eval()\n\nmodel3 = TweetModel(conf = model_config)\nmodel3.to(device)\nmodel3.load_state_dict(torch.load(\"../input/qa-dot/model_2.bin\"))\nmodel3.eval()\n\nmodel4 = TweetModel(conf = model_config)\nmodel4.to(device)\nmodel4.load_state_dict(torch.load(\"../input/qa-dot/model_3.bin\"))\nmodel4.eval()\n\nmodel5 = TweetModel(conf = model_config)\nmodel5.to(device)\nmodel5.load_state_dict(torch.load(\"../input/qa-dot/model_4.bin\"))\nmodel5.eval()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_output = []\n\ntest_dataset = TweetDataset(\n        tweet = df_test.text.values,\n        sentiment = df_test.sentiment.values,\n        selected_text = df_test.selected_text.values\n)\ndata_loader = torch.utils.data.DataLoader(\n    test_dataset,\n    shuffle = False,\n    batch_size = VALID_BATCH_SIZE,\n    num_workers = 1\n)\n\nwith torch.no_grad():\n    tk0 = tqdm(data_loader, total = len(data_loader))\n    # 按batch读取\n    for bi, d in enumerate(tk0):\n        ids = d[\"ids\"].to(device, dtype = torch.long)\n        mask = d[\"mask\"].to(device, dtype = torch.long)\n        token_type_ids = d[\"token_type_ids\"].to(device, dtype = torch.long)\n\n        targets_start = d[\"targets_start\"].to(device, dtype = torch.long)\n        targets_end = d[\"targets_end\"].to(device, dtype = torch.long)\n\n        orig_tweet = d[\"orig_tweet\"]\n        orig_selected = d[\"orig_selected\"]\n        sentiment = d[\"sentiment\"]\n        offsets = d[\"offsets\"]\n\n        outputs_start1, outputs_end1 = model1(ids = ids, mask = mask, token_type_ids = token_type_ids)\n        outputs_start2, outputs_end2 = model2(ids = ids, mask = mask, token_type_ids = token_type_ids)\n        outputs_start3, outputs_end3 = model3(ids = ids, mask = mask, token_type_ids = token_type_ids)\n        outputs_start4, outputs_end4 = model4(ids = ids, mask = mask, token_type_ids = token_type_ids)\n        outputs_start5, outputs_end5 = model5(ids = ids, mask = mask, token_type_ids = token_type_ids)\n        \n        outputs_start1 = torch.softmax(outputs_start1, dim=1)\n        outputs_start2 = torch.softmax(outputs_start2, dim=1)\n        outputs_start3 = torch.softmax(outputs_start3, dim=1)\n        outputs_start4 = torch.softmax(outputs_start4, dim=1)\n        outputs_start5 = torch.softmax(outputs_start5, dim=1)\n            \n            \n        outputs_end1 = torch.softmax(outputs_end1, dim=1)\n        outputs_end2 = torch.softmax(outputs_end2, dim=1)\n        outputs_end3 = torch.softmax(outputs_end3, dim=1)\n        outputs_end4 = torch.softmax(outputs_end4, dim=1)\n        outputs_end5 = torch.softmax(outputs_end5, dim=1)\n        \n        outputs_start = (outputs_start1 + outputs_start2 + outputs_start3 + outputs_start4 + outputs_start5) / 5\n        outputs_end = (outputs_end1 + outputs_end2 + outputs_end3 + outputs_end4 + outputs_end5) / 5\n        \n        outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n        outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n\n        for i, tweet in enumerate(orig_tweet):\n            selected_tweet = orig_selected[i]\n            tweet_sentiment = sentiment[i]\n            _, output_sentence = calculate_jaccard_score(\n                original_tweet = tweet,\n                target_string = selected_tweet,\n                sentiment_val = tweet_sentiment,\n                idx_start = np.argmax(outputs_start[i, :]) + 3,\n                idx_end = np.argmax(outputs_end[i, :]) + 3,\n                offsets = offsets[i]\n            )\n            final_output.append(output_sentence)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def post_process(selected):\n    return \" \".join(set(selected.lower().split()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample = pd.read_csv(\"../input/tweet-sentiment-extraction/sample_submission.csv\")\nsample.loc[:, 'selected_text'] = final_output\nsample.selected_text = sample.selected_text.map(post_process)\nsample['selected_text'] = sample['selected_text'].apply(lambda x: x.replace('!!!!', '!') if len(x.split())==1 else x)\nsample['selected_text'] = sample['selected_text'].apply(lambda x: x.replace('..', '.') if len(x.split())==1 else x)\nsample['selected_text'] = sample['selected_text'].apply(lambda x: x.replace('...', '.') if len(x.split())==1 else x)\nsample.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}