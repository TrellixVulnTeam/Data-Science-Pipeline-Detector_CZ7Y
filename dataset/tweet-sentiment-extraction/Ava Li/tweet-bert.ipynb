{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport re\nimport string\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nimport copy\nimport math\nimport transformers\nfrom transformers import AdamW\nimport nltk","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/tweet-sentiment-extraction/train.csv\")\ntest = pd.read_csv(\"../input/tweet-sentiment-extraction/test.csv\")\nsample_submission = pd.read_csv(\"../input/tweet-sentiment-extraction/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_text(text):\n    text = text.lower()\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"text_raw\"]  = train['text']\ntrain['selected_text_raw'] = train['selected_text']\ntest['text_raw'] = test['text']\n\n\ntrain['text'] = train['text'].apply(lambda x:clean_text(x))\ntrain['selected_text'] = train['selected_text'].apply(lambda x:clean_text(x))\ntest['text'] = test['text'].apply(lambda x:clean_text(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_positive = train.loc[(train.sentiment == 'positive')]\ntrain_neutral = train.loc[(train.sentiment == 'neutral')]\ntrain_negative = train.loc[(train.sentiment == 'negative')]\n\ntest_positive = test.loc[(test.sentiment == 'positive')]\ntest_neutral = test.loc[(test.sentiment == 'neutral')]\ntest_negative = test.loc[(test.sentiment == 'negative')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_all = [train_positive, train_neutral, train_negative]\ntest_all = [test_positive, test_neutral, test_negative]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train_positive)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train_all)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_positive.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    if (len(a) + len(b) - len(c)) == 0:\n        return 1\n    return float(len(c)) / (len(a) + len(b) - len(c))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# class BertModel(torch.nn.Module):\n#     def __init__(self,UNCASED, outputSize, droupout, std):\n#         super(BertModel, self).__init__()\n#         self.config = transformers.BertConfig.from_pretrained(UNCASED, output_hidden_states=True)\n#         self.bert_model = transformers.BertModel.from_pretrained(UNCASED, config=self.config)\n#         self.drop_out = torch.nn.Dropout(droupout)\n#         self.con_model = torch.nn.Conv1d(in_channels=768, out_channels=outputSize, kernel_size=1)\n#         self.linear_model1 = torch.nn.Linear(768, 128)\n#         self.relu = torch.nn.ReLU()\n#         self.linear_model2 = torch.nn.Linear(128, outputSize)\n#         torch.nn.init.normal_(self.linear_model1.weight, std=std)\n#         torch.nn.init.normal_(self.linear_model2.weight, std=std)\n#         self.sigmoid = torch.nn.Sigmoid()\n\n#     def forward(self, input_ids, attention_mask):\n            \n#         _,_,hidden_states = self.bert_model(input_ids, attention_mask=attention_mask.float())\n#         embedding_output = hidden_states[0]\n#         attention_hidden_states = hidden_states[1:]\n#         summed_last_4_layers = torch.stack(attention_hidden_states[-4:]).sum(0)\n#         sentence_embedding = torch.mean(summed_last_4_layers, dim=1)\n\n#         out = self.drop_out(sentence_embedding)\n#         out = self.linear_model1(out)\n#         out = self.relu(out)\n#         out = self.linear_model2(out)\n#         out = self.sigmoid(out)\n#         return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def from_predicted_positon_to_text(predicted, threshold, padded_tokens, raw_text, tokenizer):  \n    # predicted, threshold, padded_tokens, tokenizer  = eval_predicted, threshold, X_train_all_input_ids[-num_of_val:,:], tokenizer\n    predicted[predicted >= threshold] = 1\n    predicted[predicted < threshold] = 0\n\n    tokens_matrix = []\n    decode_matrix = []\n    for i in range(padded_tokens.shape[0]):\n    #         print(padded_tokens[i])\n        decode = tokenizer.decode(padded_tokens[i])\n        tokens_matrix.append(tokenizer.tokenize(decode))\n        decode_matrix.append(\" \".join(tokenizer.tokenize(decode)))\n#     print(\"tokens_matrix:\", tokens_matrix)\n    #     print(\"tokens_matrix: \", tokens_matrix[:5])\n    ##################################################### Evaluation: Generate Token Number of Predicted Text  #####################################################\n\n    index_matrix = []\n    for i in range(len(predicted)):\n        index = [i for i,p in enumerate(predicted[i, :].tolist()) if p == 1]\n        index_matrix.append(index)\n    print(\"index_matrix\", index_matrix[:5])\n\n    ##################################################### Evaluation: Generate Token Number of First and Last Predicted Words  #####################################################\n\n    first_last_words = []\n    #     padded_tokens = X_val\n    for i, j in zip(tokens_matrix, index_matrix):\n        if 0 in j:\n            j.remove(0)\n        for k in j[::-1]:\n            if i[k] in [\"[SEP]\", \"[PAD]\"]:\n                j.remove(k)\n        if len(j) == 0:\n            first_last_words.append([\"\", \"\"])\n        else:\n            last = j[-1]\n            first = j[0]\n            if i[first] == i[last]:\n                first_last_words.append([\"\",i[last]])\n            else:\n                first_last_words.append([i[first],i[last]])\n\n    predicted_val_text = []\n    for f_l, s in zip(first_last_words, decode_matrix):\n        predicted_val_text.append(re.findall(f_l[0]+\".+\"+ f_l[1], s)[0].replace('[CLS]', '').replace('CLS]', '').replace('[SEP]', '').replace('[PAD]', '').replace('[S', '').replace('[P', '').replace('AD', '').replace(' ##', '').strip())\n    print(predicted_val_text[:5])\n    return predicted_val_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def from_predicted_positon_to_text_0(predicted, threshold, padded_tokens,raw_text, tokenizer): \n#     predicted, threshold, padded_tokens,raw_text, tokenizer  = eval_predicted, 0.5, X_train_all_input_ids[-num_of_val:,:],X_train_text_raw[-num_of_val:], tokenizer\n\n    predicted[predicted >= threshold] = 1\n    predicted[predicted < threshold] = 0\n\n    selected_tokens = torch.from_numpy(padded_tokens).to(device) * predicted\n    \n    predicted_tokens = []\n    for i in range(selected_tokens.shape[0]):\n        predicted_tokens.append(selected_tokens[i][selected_tokens[i] != 0])\n\n    predicted_text = []\n    for i in range(len(predicted_tokens)):\n        pt = tokenizer.decode(predicted_tokens[i]).replace(\"[CLS]\", \"\").replace(\"[SEP]\", \"\")\n        predicted_text.append(pt)\n    return predicted_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class BertModel(torch.nn.Module):\n    def __init__(self,UNCASED, outputSize, droupout, std):\n        super(BertModel, self).__init__()\n        self.config = transformers.BertConfig.from_pretrained(UNCASED, output_hidden_states=True)\n        self.bert_model = transformers.BertModel.from_pretrained(UNCASED, config=self.config)\n        self.drop_out = torch.nn.Dropout(droupout)\n        self.con_model1 = torch.nn.Conv1d(in_channels=768, out_channels=256, kernel_size=1)        \n        self.con_model2 = torch.nn.Conv1d(in_channels=256, out_channels=64, kernel_size=1)        \n        self.con_model3 = torch.nn.Conv1d(in_channels=64, out_channels=outputSize, kernel_size=1)\n\n\n#         self.linear_model = torch.nn.Linear(768, outputSize)\n        self.sigmoid = torch.nn.Sigmoid()\n#         torch.nn.init.normal_(self.linear_model.weight, std=std)\n\n    def forward(self, input_ids, attention_mask):\n            \n        last_hidden_states = self.bert_model(input_ids, attention_mask=attention_mask.float())\n        last_hidden_states = last_hidden_states[0].permute(0,2,1)\n#         print(torch.mean(last_hidden_states))\n        out = self.drop_out(last_hidden_states)\n        out = self.con_model1(out)\n        out = self.con_model2(out)\n        out = self.con_model3(out)\n        out = torch.sum(out, dim=2)\n        out = self.sigmoid(out)\n#         print(out[1])\n        return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# del model\ntorch.cuda.empty_cache()\n# 1 positive, 2 neutral, 3 negative\ncategory = [\"positive\", \"neutral\", \"negative\"]\ncolor = ['b', 'r', 'g']\nmax_sequence_length = 32\nval_frac = 0.25\nnum_of_val = 1000\nlearningRate = 3e-5 # 0.01\nmax_length = max_sequence_length\noutputSize = max_sequence_length\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# epochs = [3000, 2000, 5000]\nepochs = [10, 1, 12]\npredicted_text = {}\n\nVOCAB='/kaggle/input/bertbaseuncased/vocab.txt' # your path for model and vocab \nUNCASED='/kaggle/input/bertbaseuncased/'\nstd = 0.02\ndroupout = 0.1\nbatch_size = 16\nno_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\ntokens_dict = {}\n\n##################################################### For Loop #####################################################    \n\nfor i in range(len(train_all)):\n    if i == 1:\n        threshold = 0\n    else:\n        threshold = 0.5\n    ##################################################### Model Parameters #####################################################\n    tokenizer = transformers.BertTokenizer.from_pretrained(VOCAB)\n    \n#     for k, v in list(tokenizer.vocab.items()):\n#         if k[1:-1] not in [\"SEP\", \"MASK\", \"CLS\", \"PAD\"]:\n#             tokens_dict[k[1:-1]] = v\n            \n    model = BertModel(UNCASED, outputSize, droupout, std)\n    model.to(device)\n    criterion = torch.nn.MSELoss()\n    param_optimizer = list(model.named_parameters())\n    optimizer_parameters = [\n        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n    ]\n    optimizer = AdamW(optimizer_parameters, lr=learningRate)\n#     optimizer = torch.optim.Adam(model.parameters(), lr=learningRate)\n\n    \n    ##################################################### Input Preparation #####################################################\n    train_data = train_all[i]\n    test_data = test_all[i]\n    \n    ##################################################### Get Text, Selected Text #####################################################\n    X_train_text_raw = train_data.loc[:, \"text_raw\"].tolist()\n    y_train_text_raw = train_data.loc[:, \"selected_text_raw\"].tolist()\n    X_test_text_raw = test_data.loc[:, \"text_raw\"].tolist()\n    \n    \n    X_train_text = train_data.loc[:, \"text\"].tolist()\n    y_train_text = train_data.loc[:, \"selected_text\"].tolist()\n    X_test = test_data.loc[:, [\"textID\", \"text\"]]\n    ##################################################### Tokenizing #####################################################\n    X_train_tokens = [tokenizer.encode(t, add_special_tokens=True, max_length = max_length) for t in X_train_text]    \n    y_train_tokens = [tokenizer.encode(t, add_special_tokens=True, max_length = max_length) for t in y_train_text]\n    X_test_tokens = [tokenizer.encode(t, add_special_tokens=True, max_length = max_length) for t in X_test.text]\n    ##################################################### Padding #####################################################\n\n    X_train_all_input_ids = np.array([i + [0]*(max_length - len(i)) for i in X_train_tokens])\n    y_train_all_input_ids = np.array([i + [0]*(max_length - len(i)) for i in y_train_tokens])\n    X_test_input_ids = np.array([i + [0]*(max_length - len(i)) for i in X_test_tokens])\n    ##################################################### Attention Masks #####################################################\n    X_train_all_attention_mask = np.where(X_train_all_input_ids != 0, 1, 0)\n    y_train_all_attention_mask = np.where(y_train_all_input_ids != 0, 1, 0)\n    X_test_attention_mask = np.where(X_test_input_ids != 0, 1, 0)\n#     print(X_train_all_attention_mask[:5])\n    ##################################################### Convert Training Label to Position (0/1) #####################################################\n    \n    y_train_bool = []\n    for j in range(len(y_train_all_input_ids)):\n        a = [1 if x > 0 and x in y_train_all_input_ids[j,:] and (x not in [101, 102]) else 0 for x in X_train_all_input_ids[j,:].tolist()]\n        y_train_bool.append(a)\n    y_train_bool = np.array(y_train_bool)\n    \n    # print(y_train_bool[:5])\n    \n    ##################################################### Split into Train and Evaluation Data #####################################################\n    \n\n    num_of_val = num_of_val + (len(X_train_all_input_ids) - num_of_val)%batch_size\n    num_of_train = len(X_train_all_input_ids) - num_of_val\n\n#     num_of_train = math.floor(len(X_train_input_ids) * (1-val_frac))\n#     num_of_val = len(X_train_input_ids) - num_of_train   \n\n#     X_train = X_train_input_ids[:num_of_train,:]\n#     X_train_attention_mask = X_train_attention_mask[:num_of_train,:]\n#     y_train = y_train_bool[:num_of_train,:]\n\n    X_val = X_train_all_input_ids[-num_of_val:,:]\n    X_val_attention_mask = X_train_all_attention_mask[-num_of_val:,:]\n    y_val = y_train_bool[-num_of_val:,:]\n    \n    training_loss = []\n    \n        \n        ##################################################### Training #####################################################\n    model.train() \n    for epoch in range(epochs[i]):\n        for k_fold in range(int(num_of_train/batch_size)):\n            \n            X_train = X_train_all_input_ids[k_fold*batch_size:(k_fold+1)*batch_size,:]\n            X_train_attention_mask = X_train_all_attention_mask[k_fold*batch_size:(k_fold+1)*batch_size,:]\n            y_train = y_train_bool[k_fold*batch_size:(k_fold+1)*batch_size,:]\n            \n            # Converting inputs and labels to Variable\n            X_train =  Variable(torch.from_numpy(X_train).to(device))\n            X_train_attention_mask = Variable(torch.from_numpy(X_train_attention_mask).to(device))\n            y_train = Variable(torch.from_numpy(y_train).to(device))\n\n            outputs = model(X_train, X_train_attention_mask)\n            loss = criterion(outputs.float(), y_train.float())\n            training_loss.append(loss.item())\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n        \n            if k_fold%100 == 0:\n                print('{}-th category, total {} fold, now {}, epoch {}, loss {}'.format(i, int(num_of_train/batch_size), k_fold, epoch, loss.item()))\n\n    ##################################################### Drawing Training Loss #####################################################\n    \n    # draw training loss\n    plt.plot(range(len(training_loss)), training_loss, color[i], label='training loss')\n    \n    ##################################################### Evaluation:  Generate Selected Positions #####################################################\n    model.eval()\n    with torch.no_grad(): # we don't need gradients in the testing phase\n\n#         if torch.cuda.is_available():\n        X_val =  Variable(torch.from_numpy(X_val).to(device))\n        X_val_attention_mask =  Variable(torch.from_numpy(X_val_attention_mask).to(device))\n        y_val =  Variable(torch.from_numpy(y_val).to(device))\n\n        eval_predicted = model(X_val, X_val_attention_mask)\n    \n    predicted_val_text = from_predicted_positon_to_text(eval_predicted, threshold, X_train_all_input_ids[-num_of_val:,:], X_train_text_raw[-num_of_val:], tokenizer) \n\n    ##################################################### Jaccard Scores #####################################################\n    jaccard_score_list = []\n    for str1, str2 in zip(predicted_val_text, y_train_text[-num_of_val:]):\n        jaccard_score_list.append(jaccard(str1, str2))\n    result = pd.Series(jaccard_score_list)\n    print(result.describe())\n    \n    ##################################################### Prediction on Test Data #####################################################\n    \n    with torch.no_grad(): # we don't need gradients in the testing phase\n#         if torch.cuda.is_available():\n        X_test_input_ids_ =  Variable(torch.from_numpy(X_test_input_ids).to(device))\n        X_test_attention_mask_ =  Variable(torch.from_numpy(X_test_attention_mask).to(device))\n        \n        test_predicted = model(X_test_input_ids_, X_test_attention_mask_)\n        \n    predicted_test_text = from_predicted_positon_to_text(test_predicted, threshold, X_test_input_ids, X_test_text_raw, tokenizer) \n    for p, idx in zip(predicted_test_text, X_test.textID.tolist()):\n        predicted_text[idx] = p","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# num = -num_of_val\n# for i, j,k,l, m, n,o in zip(eval_predicted, predicted_val_text, X_train_text[num:], X_train_all_input_ids[num:],  y_train_text[num:], y_train_all_input_ids[num:], y_train_bool[num:,:]):\n#     print(\"i --- \", i)\n#     print(\"j --- \", j)\n#     print(\"k --- \", k)\n#     print(\"l --- \", l)\n#     print(\"m --- \", m)\n#     print(\"n --- \", n)\n#     print(\"o --- \", o)\n#     print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tokenizer.decode([2034 , 2028 , 2003 , 3976 , 3238  ,1010 , 2021 , 1996 , 3177 , 2080 , 2003,\n#   2307 , 1010 , 2205  ,1012 ,13970 ,12269 , 2000  ,3566   ])\n\n# tokenizer.encode(\"[PAD]\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission_df = pd.DataFrame()\nfor idx, row in sample_submission.iterrows():\n    row[\"selected_text\"] = predicted_text[row[\"textID\"]]\n    row_frame = row.to_frame().T\n    if sample_submission_df.empty:\n        sample_submission_df = row_frame\n    else:\n        sample_submission_df = sample_submission_df.append(row_frame)\ndisplay(sample_submission_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission_df.to_csv(\"submission.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}