{"cells":[{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"}},"source":"# BERT: Pre-Training of Bidirectional Tranformers for Language Understanding\n**see the full paper [here](https://arxiv.org/pdf/1810.04805.pdf)**"},{"cell_type":"markdown","metadata":{},"source":"## Architecture\nFor the architecture, the BERT paper referenced to the original\nimplementation of the multi-layer bidirectional\nTransformer encoder described in\n[Vaswani et al. (2017)](https://arxiv.org/pdf/1706.03762.pdf).\nThe Bert model has only one encoders stack.\nSo for this part, I am using the architecture described by the paper above.\n\n![architecture](https://tinyurl.com/y5ck5j7c)"},{"cell_type":"markdown","metadata":{"lines_to_next_cell":0,"pycharm":{"name":"#%% md\n"}},"source":"### Requirements\n**Note**: Don't forget to set the environment variable `CORPUS_SIZE`\nto set the size of corpus if it needed"},{"cell_type":"code","execution_count":null,"metadata":{"lines_to_end_of_cell_marker":2,"pycharm":{"name":"#%%\n"}},"outputs":[],"source":"\nimport math\nimport os\nimport random\nimport concurrent.futures\nimport re\nfrom pathlib import Path\n\nimport numpy\nimport pandas as pd\nimport seaborn as sns\nimport sentencepiece as spm\nimport torch\nimport torch.optim as optim\nfrom torch import nn\nfrom torch.nn import functional as f\nfrom torch.utils.data import Dataset, DataLoader\nimport spacy\n\n!python -m spacy download en_core_web_sm\n\n!pip install neptune-client\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nNEPTUNE_API_TOKEN = user_secrets.get_secret(\"NEPTUNE_API_TOKEN\")\nimport neptune\n\nCLS = 'CLS'\nMASK = 'MASK'\nSEP = 'SEP'\nPAD = 'PAD'\nUNK = 'UNK'\n\n# enable cuda if it exists\nif torch.cuda.is_available():\n    TORCH_DEVICE = \"cuda\"\nelse:\n    TORCH_DEVICE = \"cpu\"\n\ncurrent_device = torch.device(TORCH_DEVICE)"},{"cell_type":"markdown","metadata":{},"source":"### Bert Encoder Stacks\n* Bert takes as input a sequence of plain text tokens\n* the output is a representation vector of the size of the hidden layers\n* Bert is a stack of multi-layer bidirectional Transformer encoder"},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"}},"outputs":[],"source":"\n\nclass Bert(nn.Module):\n    # pylint: disable=too-many-arguments\n    def __init__(self, stack_size, voc_size,\n                 dim_model, mh_size, padding_idx=0):\n        super().__init__()\n        self.dim_model = dim_model\n        self.emb = nn.Embedding(\n            embedding_dim=dim_model,\n            num_embeddings=voc_size,\n            padding_idx=padding_idx\n        )\n        self.encoder_layer = nn.ModuleList()\n        for _ in range(stack_size):\n            self.encoder_layer.append(Encoder(dim_model, mh_size))\n\n    def forward(self, tokens):\n        mask = (tokens > 0).unsqueeze(1).repeat(1, tokens.size(1), 1).unsqueeze(1)\n        embeddings = self.emb(tokens)\n        pos_embedding = positional_enc(embeddings.shape[1], embeddings.shape[2],\n                                       self.emb.weight.device.type)\n        z_n = pos_embedding + embeddings * math.sqrt(self.dim_model)\n        for encoder in self.encoder_layer:\n            z_n = encoder(z_n, mask)\n        return z_n"},{"cell_type":"markdown","metadata":{},"source":"### Encoder\n\n* The encoder is composed of two modules. The first is the attention module\n and the second is the feed-forward network\nmodule.\n\n* this model is execute sequentially but the computation of each token\nis independent and could be compute concurrently"},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"}},"outputs":[],"source":"\nclass FeedForwardNetwork(nn.Module):\n    def __init__(self, dim_model):\n        super().__init__()\n        self.linear_1 = nn.Linear(dim_model, dim_model * 4)\n        self.linear_2 = nn.Linear(dim_model * 4, dim_model)\n\n    def forward(self, x_n):\n        out_l1 = f.relu(self.linear_1(x_n))\n        return self.linear_2(out_l1)\n\n\nclass Encoder(nn.Module):\n    def __init__(self, dim_model, mh_size):\n        super().__init__()\n        self.mh_att = MultiHeadAttention(mh_size, dim_model)\n        self.add_norm_l1 = AddNormalizeLayer(dim_model)\n        self.feed_forward_network = FeedForwardNetwork(dim_model)\n        self.add_norm_l2 = AddNormalizeLayer(dim_model)\n\n    def forward(self, x_n, mask):\n        z_n = self.mh_att(x_n, mask)\n        l1_out = self.add_norm_l1(x_n, z_n)\n        ffn_out = self.feed_forward_network(l1_out)\n        return self.add_norm_l2(l1_out, ffn_out)"},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"}},"source":"### Self Attention\n![attention](https://tinyurl.com/y47nyfeg)"},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"}},"outputs":[],"source":"class MultiHeadAttention(nn.Module):\n    def __init__(self, multi_head_size, dim_model):\n        super().__init__()\n        self.dim_model = dim_model\n        self.multi_head_size = multi_head_size\n        self.linear_o = nn.Linear(self.dim_model, self.dim_model)\n        self.query = nn.Linear(self.dim_model, self.dim_model)\n        self.key = nn.Linear(self.dim_model, self.dim_model)\n        self.value = nn.Linear(self.dim_model, self.dim_model)\n\n    def forward(self, tokens, mask):\n        batch_size = tokens.shape[0]\n        z_n = self.compute_attention(tokens, batch_size, mask)\n        return self.linear_o(z_n.transpose(1, 2).contiguous().view(batch_size, -1, self.dim_model))\n\n    def compute_attention(self, tokens, batch_size, mask):\n        d_k = self.dim_model // self.multi_head_size\n        query_mat = self.query(tokens).view(batch_size, -1, self.multi_head_size, d_k) \\\n            .transpose(2, 1)\n        key_mat = self.key(tokens).view(batch_size, -1, self.multi_head_size, d_k) \\\n            .transpose(2, 1)\n        value_mat = self.value(tokens).view(batch_size, -1, self.multi_head_size, d_k) \\\n            .transpose(2, 1)\n        scores = (query_mat.matmul(key_mat.transpose(-2, -1)) / math.sqrt(self.dim_model)) \\\n            .masked_fill(mask == 0, 1e-11)\n\n        return f.softmax(scores, dim=-1).matmul(value_mat)"},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"}},"source":"### Add & Normalize Layer"},{"cell_type":"code","execution_count":null,"metadata":{"lines_to_next_cell":2,"pycharm":{"name":"#%%\n"}},"outputs":[],"source":"\nclass AddNormalizeLayer(nn.Module):\n    def __init__(self, normalized_shape):\n        super().__init__()\n        self.layer_norm = nn.LayerNorm(normalized_shape)\n\n    def forward(self, residual_in, prev_res):\n        return residual_in + self.layer_norm(prev_res)"},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"}},"source":"### Positional Encoding"},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"}},"outputs":[],"source":"\n\ndef positional_enc(seq_len, model_dim, device=\"cpu\"):\n    pos_emb_vector = torch.empty(seq_len, model_dim).to(device)\n    for pos in range(seq_len):\n        for i_col in range(model_dim):\n            power_ind = 10000 ** ((2 * i_col) / model_dim)\n            if i_col % 2 == 0:\n                pos_emb_vector[pos, i_col] = math.sin(pos / power_ind)\n            else:\n                pos_emb_vector[pos, i_col] = math.cos(pos / power_ind)\n    return pos_emb_vector"},{"cell_type":"markdown","metadata":{},"source":"## Import CSV files"},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"}},"outputs":[],"source":"TRAIN_PATH = '../input/tweet-sentiment-extraction/train.csv'\nTEST_PATH = '../input/tweet-sentiment-extraction/test.csv'\nPR_TRAIN_PATH = './processed_train.csv'\nPR_TEST_PATH = './processed_test.csv'\nif not Path(PR_TRAIN_PATH).is_file():\n    train_csv = pd.read_csv(TRAIN_PATH, dtype={'text': 'string'})\n    test_dt = pd.read_csv(TEST_PATH, dtype={'text': 'string'})\nelse:\n    train_csv = pd.read_csv(PR_TRAIN_PATH, dtype={'text': 'string'})\n    test_dt = pd.read_csv(PR_TEST_PATH, dtype={'text': 'string'})"},{"cell_type":"markdown","metadata":{},"source":"### Cleaning and Normalization Step before Sentence Piece Training"},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"}},"outputs":[],"source":"if not Path(PR_TRAIN_PATH).is_file():\n    train_csv = train_csv.dropna()\n    train_csv = train_csv.reset_index(drop=True)\n    test_dt = test_dt.dropna()\n    test_dt = test_dt.reset_index(drop=True)\n    train_csv.head()"},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"}},"source":"## get a word tokenisation and lemmatization for each entry"},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"}},"outputs":[],"source":"nlp = spacy.load(\"en_core_web_sm\", disable=['ner', 'parser'])\n\n\ndef processing_text(entry, dataframe, df_idx):\n    text = entry['text'].lower().replace(\"`\", \"'\").strip()\n    text = ' '.join([token.text\n                     if token.lemma_ == \"-PRON-\" or '*' in token.text else token.lemma_\n    if not token.is_punct else '' for token in nlp(text)]).strip()\n    text = re.sub(r'http[s]?://\\S+', '[URL]', text)\n    dataframe.at[df_idx, 'text'] = re.sub(r'\\s\\s+', ' ', text)\n\n\ndef processing_df(dataframe, path):\n    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n        future_to_url = {executor.submit(processing_text, df_entry, dataframe, df_idx):\n                         df_entry for df_idx, df_entry in enumerate(dataframe.iloc)}\n    for _ in concurrent.futures.as_completed(future_to_url):\n        pass\n    dataframe.to_csv(path)\n\n\nif not Path(PR_TRAIN_PATH).is_file():\n    processing_df(test_dt, PR_TEST_PATH)\n    processing_df(train_csv, PR_TRAIN_PATH)\ntrain_csv.head()"},{"cell_type":"markdown","metadata":{},"source":"## Train & Initialize Sentence Piece"},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"}},"outputs":[],"source":"PATH = './tweet-sentiment-extraction'\nwith open(PATH + '.txt', 'w') as voc_txt:\n    for t in train_csv['text']:\n        voc_txt.write(t + '\\n')\nSPM_ARGS = \"\" \\\n           \"--input={0}.txt \" \\\n           \"--model_prefix={0} \" \\\n           \"--pad_id=0 \" \\\n           \"--unk_id=1 \" \\\n           \"--bos_id=2 \" \\\n           \"--eos_id=3 \" \\\n           \"--pad_piece={1} \" \\\n           \"--unk_piece={2} \" \\\n           \"--bos_piece={3} \" \\\n           \"--eos_piece={4}\" \\\n    .format(PATH, PAD, UNK, CLS, SEP)\nspm.SentencePieceTrainer.Train(SPM_ARGS)\nsp = spm.SentencePieceProcessor()\nsp.Load(PATH + '.model')\nprint(sp.EncodeAsPieces('this is a test'))\nprint(sp.EncodeAsIds('this is a test'))"},{"cell_type":"markdown","metadata":{},"source":"## Dataset : Analyze & Vectorization"},{"cell_type":"markdown","metadata":{},"source":"### resize the corpus if it needed"},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"}},"outputs":[],"source":"if \"CORPUS_SIZE\" in os.environ:\n    corpus_size = int(os.environ.get(\"CORPUS_SIZE\"))\n    train_csv = train_csv[:corpus_size]\n    test_dt = test_dt[:corpus_size]\nelse:\n    train_csv = train_csv[:100]\n    test_dt = test_dt[:10]\n"},{"cell_type":"markdown","metadata":{},"source":"### analysis"},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"}},"outputs":[],"source":"train_csv['sequence length'] = ''\nURL_COUNT = 0\nfor idx, d in enumerate(train_csv.iloc):\n    train_csv.at[idx, 'sequence length'] = len(sp.EncodeAsIds(d['text']))\nfor idx, d in enumerate(test_dt.iloc):\n    test_dt.at[idx, 'sequence length'] = len(sp.EncodeAsIds(d['text']))\nsns.set(font_scale=2)\nsns.displot(x='sequence length', data=train_csv, aspect=2, height=20)\nprint('number of entries containing a url : ' + str(URL_COUNT))\nprint('number of entries in train.csv : ' + str(len(train_csv)))"},{"cell_type":"markdown","metadata":{},"source":"### Filter the entries containing url and the less frequent length sequences"},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"}},"outputs":[],"source":"del train_csv['selected_text']\ntrain_csv = train_csv.drop(train_csv[train_csv['sequence length'].ge(35)].index)\ntrain_csv = train_csv.drop(train_csv[train_csv['sequence length'].le(5)].index)\ntrain_csv = train_csv.reset_index(drop=True)\nprint('number of entries in train.csv after filtering : ' + str(len(train_csv)))\nsns.displot(x='sequence length', data=train_csv, aspect=2, height=20)"},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"}},"source":"### split & create training, evaluation & test datasets"},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"tags":[]},"outputs":[],"source":"len_train_csv = len(train_csv)\nlen_test_df = len(test_dt)\ntotal_size = len_train_csv + len_test_df\n\ntrain_dt = train_csv.iloc[:int(len_train_csv * 70 / 100)]\neval_dt = train_csv.iloc[int(len_train_csv * 70 / 100):]\n\nprint(\n    \"\"\"size of train.csv file : {0}\nsize of test.csv file : {1}\ntotal size : {2}\n\nsize of train dataset : {3}\nsize of eval dataset : {4}\nsize of test dataset : {5}\n\"\"\".format(\n        len_train_csv,\n        len_test_df,\n        total_size,\n        len(train_dt),\n        len(eval_dt),\n        len(test_dt)))"},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"}},"source":"### Vectorizer"},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"tags":[]},"outputs":[],"source":"class TwitterDataset(Dataset):\n    def __init__(self, train_dataset, eval_dataset, test_dataset, sentence_piece):\n        self.train_dataset = train_dataset\n        self.eval_dataset = eval_dataset\n        self.current_dataset = self.train_dataset\n        self.test_dataset = test_dataset\n        self.sentence_piece = sentence_piece\n        self.st_voc = []\n        self.max_seq_len = int(pd.concat(\n            [train_dataset, eval_dataset, test_dataset])['sequence length'].max()) + 2\n        self.__init_sentiment_vocab()\n\n    def __init_sentiment_vocab(self):\n        self.st_voc = [UNK, *self.train_dataset['sentiment'].unique()]\n\n    def get_vocab_size(self):\n        return self.sentence_piece.vocab_size() + 1\n\n    def __getitem__(self, index):\n        return {\n            'vectorized_tokens': self.vectorize(self.current_dataset.iloc[index][\"text\"]),\n            \"sentiment_i\": self.get_sentiment_i(self.current_dataset.iloc[index][\"sentiment\"])\n        }\n\n    def __len__(self):\n        return len(self.current_dataset)\n\n    def switch_to_dataset(self, flag):\n        if flag == 'train':\n            self.current_dataset = self.train_dataset\n        elif flag == 'eval':\n            self.current_dataset = self.eval_dataset\n        elif flag == 'test':\n            self.current_dataset = self.test_dataset\n        else:\n            raise ValueError(\"this dataset doesn't exist !\")\n\n    # noinspection PyArgumentList\n    def vectorize(self, tokens):\n        vector = self.sentence_piece.EncodeAsIds(tokens)\n        return torch.LongTensor(\n            [sp.bos_id()] + vector + [sp.eos_id()] +\n            [self.get_pad()] * (self.max_seq_len - len(vector) - 2)\n        )\n\n    def get_mask(self):\n        return self.sentence_piece.vocab_size()\n\n    def get_pad(self):\n        return self.sentence_piece.pad_id()\n\n    def get_cls(self):\n        return self.sentence_piece.bos_id()\n\n    def get_sep(self):\n        return self.sentence_piece.eos_id()\n\n    def get_tokens(self, ids):\n        return ' '.join([self.sentence_piece.Decode(i) if i != self.get_mask()\n                         else MASK for i in ids.tolist()]).strip()\n\n    def get_sentiment_i(self, st_token):\n        return self.st_voc.index(st_token) if st_token in self.st_voc else self.st_voc.index(UNK)"},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"}},"source":"### Dataset Instantiation\n"},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"}},"outputs":[],"source":"twitter_dataset = TwitterDataset(train_dt, eval_dt, test_dt, sp)"},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"}},"source":"## Parameters"},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"}},"outputs":[],"source":"parameters = {\n    \"stack_size\": 6,\n    \"vocabulary_size\": twitter_dataset.get_vocab_size(),\n    \"bert_dim_model\": 256,\n    \"multi_heads\": 8,\n    \"pre_train_learning_rate\": 1e-4,\n    \"st_learning_rate\": 2e-5,\n    \"batch_size\": 1,\n    \"epochs\": 100,\n    \"device\": current_device,\n    \"corpus test size\": len(test_dt),\n    \"corpus train size\": len(train_csv),\n}"},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"}},"source":"## Model Instantiation and DataLoader\n"},{"cell_type":"code","execution_count":null,"metadata":{"lines_to_next_cell":2,"pycharm":{"name":"#%%\n"}},"outputs":[],"source":"bert = Bert(\n    stack_size=parameters[\"stack_size\"],\n    voc_size=parameters[\"vocabulary_size\"],\n    dim_model=parameters[\"bert_dim_model\"],\n    mh_size=parameters[\"multi_heads\"],\n    padding_idx=twitter_dataset.get_pad()\n).to(current_device)\n\nce_loss = nn.CrossEntropyLoss(ignore_index=twitter_dataset.get_pad()) \\\n    .to(current_device)\n\n\ndef generate_batches(dataset, batch_size, shuffle=True, drop_last=True, device=\"cpu\"):\n    \"\"\"\n    A generator function which wraps the PyTorch DataLoader. It will\n    ensure each tensor is on the write device location.\n    \"\"\"\n    data_loader = DataLoader(dataset=dataset, batch_size=batch_size,\n                             shuffle=shuffle, drop_last=drop_last)\n\n    for data_dict in data_loader:\n        data = {}\n        for name, _ in data_dict.items():\n            data[name] = data_dict[name].to(device)\n        yield data"},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"}},"source":"## Pre-Training & Fine-Tuning\nFor the Pre-Training, we use instead the RoBERTa learning method.\nWe use only one Pre-Training Task and we mask tokens dynamically.\nFor more details to the dynamic masking\nsee the original paper : https://arxiv.org/pdf/1907.11692.pdf"},{"cell_type":"markdown","metadata":{},"source":"### Masked LM method"},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"}},"outputs":[],"source":"# noinspection PyArgumentList\n\n\ndef generate_masked_lm(vector, dataset, mask_prob=.15, rnd_t_prob=.1, unchanged_prob=.1):\n    return torch.LongTensor([\n        replace_token(idx_token, dataset, rnd_t_prob, unchanged_prob)\n        if numpy.random.uniform() < mask_prob and is_not_markers(idx_token, dataset)\n        else idx_token\n        for idx_token in vector\n    ])"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"def is_not_markers(token, dataset):\n    return token not in [dataset.get_cls(), dataset.get_sep(),\n                         dataset.get_pad(), dataset.get_mask()]"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"def replace_token(token, dataset, rnd_t_prob, unchanged_prob):\n    prob = numpy.random.uniform()\n    if prob < rnd_t_prob:\n        return replace_by_another_id(token, dataset)\n    if rnd_t_prob < prob < unchanged_prob + rnd_t_prob:\n        return token\n    return dataset.get_mask()"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"def replace_by_another_id(index_token, dataset):\n    replaced_index_t = index_token\n    not_include_t = [\n        dataset.get_cls(),\n        dataset.get_sep(),\n        dataset.get_mask(),\n        dataset.get_pad(),\n        index_token\n    ]\n    while replaced_index_t in not_include_t:\n        replaced_index_t = random.choice(range(twitter_dataset.get_vocab_size()))\n    return replaced_index_t"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"def generate_batched_masked_lm(batched_vectors, dataset,\n                               mask_prob=.15, rnd_t_prob=.1, unchanged_prob=.1):\n    batched_masked_lm = [\n        generate_masked_lm(vector, dataset, mask_prob, rnd_t_prob, unchanged_prob)\n        for vector in batched_vectors\n    ]\n    return torch.stack(batched_masked_lm)"},{"cell_type":"markdown","metadata":{},"source":"### Pre-Training Classifier\na pre-training l_1 is needed to predict the masked token\nBert model give only a bi contextual representation of the sentence"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"class PreTrainingClassifier(nn.Module):\n    def __init__(self, zn_size, voc_size):\n        super().__init__()\n        self.l_1 = nn.Linear(zn_size, voc_size)\n\n    def forward(self, z_n):\n        return self.l_1(z_n)"},{"cell_type":"markdown","metadata":{},"source":"## Pre-Training Step\n### Training and Evaluation Loop"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"optimizer = optim.Adam(bert.parameters(), lr=parameters['pre_train_learning_rate'])"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"pre_train_classifier = PreTrainingClassifier(parameters['bert_dim_model'],\n                                             parameters['vocabulary_size']).to(current_device)"},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"}},"outputs":[],"source":"if \"TEST_ENV\" not in os.environ.keys():\n    neptune.init('smeoni/bert-impl', api_token=NEPTUNE_API_TOKEN)\n    neptune.create_experiment(name='bert-impl-experiment', params=parameters)\n    for epoch in range(parameters['epochs']):\n        # train loop\n        twitter_dataset.switch_to_dataset(\"train\")\n        for batch in generate_batches(twitter_dataset,\n                                      parameters['batch_size'],\n                                      device=parameters['device']):\n            x_obs = generate_batched_masked_lm(batch['vectorized_tokens'],\n                                               twitter_dataset).to(current_device)\n            y_target = batch['vectorized_tokens'].to(current_device)\n            # Step 1: Clear the gradients\n            bert.zero_grad()\n            # Step 2: Compute the forward pass of the model\n            bert_zn = bert(x_obs)\n            y_pred = pre_train_classifier(bert_zn)\n            # Step 3: Compute the loss value that we wish to optimize\n            loss = ce_loss(y_pred.reshape(-1, y_pred.shape[2]), y_target.reshape(-1))\n            # Step 4: Propagate the loss signal backward\n            loss.backward()\n            # Step 5: Trigger the optimizer to perform one update\n            optimizer.step()\n            neptune.log_metric('pre-train loss', loss.item())\n            observed_ids = torch.argmax(y_pred, dim=2)[-1]\n            RAW_TEXT_OBSERVED = sp.Decode([id_obv for id_obv in observed_ids.tolist()\n                                           if id_obv != twitter_dataset.get_mask()])\n            neptune.send_text('raw pre-train text observed', RAW_TEXT_OBSERVED)\n            RAW_TEXT_EXPECTED = sp.Decode(y_target[-1].tolist())\n            neptune.send_text('raw pre-train text expected', RAW_TEXT_EXPECTED)"},{"cell_type":"markdown","metadata":{},"source":"## Fine-Tuning Step\n### Fine-Tuning Classifier"},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"}},"outputs":[],"source":"class FineTuningClassifier(nn.Module):\n    def __init__(self, zn_size, st_voc_size, voc_size):\n        super().__init__()\n        self.l_1 = nn.Linear(zn_size, voc_size)\n        self.l_2 = nn.Linear(voc_size, st_voc_size)\n\n    def forward(self, z_n):\n        l1_out = f.relu((self.l_1(z_n)))\n        out = self.l_2(l1_out)\n        return out"},{"cell_type":"markdown","metadata":{},"source":"### Fine-Tuning Training Loop"},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"}},"outputs":[],"source":"fine_tuning_classifier = FineTuningClassifier(parameters['bert_dim_model'],\n                                              len(twitter_dataset.st_voc),\n                                              parameters['vocabulary_size']).to(current_device)\n\noptimizer = optim.Adam(bert.parameters(), lr=parameters['st_learning_rate'])\n\n\ndef no_learn_loop(corpus, model, no_learn_loss, dataset, no_learn_device):\n    dataset.switch_to_dataset(corpus)\n    # evaluation loop\n    for no_learn_batch in generate_batches(dataset, parameters['batch_size'],\n                                           device=no_learn_device):\n        no_learn_x_obs = generate_batched_masked_lm(no_learn_batch['vectorized_tokens'], dataset) \\\n            .to(no_learn_device)\n        no_learn_y_target = no_learn_batch['sentiment_i'].to(no_learn_device)\n        # Step 1: Compute the forward pass of the model\n        no_learn_zn = model(no_learn_x_obs)\n        no_learn_y_pred = fine_tuning_classifier(no_learn_zn[:, -1, :])\n        # Step 2: Compute the loss value that we wish to optimize\n        no_ll_res = no_learn_loss(no_learn_y_pred, no_learn_y_target.reshape(-1))\n\n        neptune.log_metric('sentiment ' + corpus + ' loss', no_ll_res.item())\n        neptune.send_text('sentiment ' + corpus + ' text',\n                          sp.Decode(x_obs[-1].tolist()))\n        neptune.send_text('sentiment' + corpus + ' observed',\n                          twitter_dataset.st_voc[torch.argmax(y_pred, dim=-1)[-1]])\n        neptune.send_text('sentiment ' + corpus + ' expected', twitter_dataset.st_voc[y_target[-1]])\n\n\nif \"TEST_ENV\" not in os.environ.keys():\n    for epoch in range(parameters['epochs']):\n        # train loop\n        twitter_dataset.switch_to_dataset(\"train\")\n        for batch in generate_batches(twitter_dataset,\n                                      parameters['batch_size'],\n                                      device=parameters['device']):\n            x_obs = batch['vectorized_tokens'].to(current_device)\n            y_target = batch['sentiment_i'].to(current_device)\n            # Step 1: Clear the gradients\n            bert.zero_grad()\n            # Step 2: Compute the forward pass of the model\n            bert_zn = bert(x_obs)\n            y_pred = fine_tuning_classifier(bert_zn[:, -1, :])\n            # Step 3: Compute the loss value that we wish to optimize\n            loss = ce_loss(y_pred, y_target.reshape(-1))\n            # Step 4: Propagate the loss signal backward\n            loss.backward()\n            # Step 5: Trigger the optimizer to perform one update\n            optimizer.step()\n            neptune.log_metric('sentiment train loss', loss.item())\n            neptune.send_text('sentiment train text', sp.Decode(x_obs[-1].tolist()))\n            neptune.send_text('sentiment train observed',\n                              twitter_dataset.st_voc[torch.argmax(y_pred, dim=-1)[-1]])\n            neptune.send_text('sentiment train expected',\n                              twitter_dataset.st_voc[y_target[-1]])\n\n        no_learn_loop('eval', bert, ce_loss, twitter_dataset, parameters['device'])"},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"}},"source":"### Fine-Tuning Test Loop"},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"}},"outputs":[],"source":"if \"TEST_ENV\" not in os.environ.keys():\n    no_learn_loop('test', bert, ce_loss, twitter_dataset, parameters['device'])\n    neptune.stop()"}],"metadata":{"jupytext":{"formats":"//notebook//ipynb,//src/main//py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":4}