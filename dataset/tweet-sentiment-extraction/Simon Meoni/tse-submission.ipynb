{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport os\nimport torch\nimport statistics\nimport pytorch_lightning as pl\nfrom torch import nn\nimport glob\nimport gc\nimport torch.nn.functional as F\nfrom transformers import RobertaTokenizer, AutoModel, AutoTokenizer, ElectraTokenizer, AutoModelForQuestionAnswering, RobertaForQuestionAnswering\nfrom pytorch_lightning.loggers import WandbLogger\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom pytorch_lightning import Trainer\nimport re","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-02T21:55:22.823392Z","iopub.execute_input":"2021-11-02T21:55:22.823719Z","iopub.status.idle":"2021-11-02T21:55:30.117634Z","shell.execute_reply.started":"2021-11-02T21:55:22.823647Z","shell.execute_reply":"2021-11-02T21:55:30.116696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Checkpoints","metadata":{}},{"cell_type":"code","source":"sort_ckpt = (lambda x: float(re.findall(r\"[0-9]\\.[0-9]*\", x)[0]))\nhyperparameters = {\n    \"batch_size\": 32,\n    \"tokenizer_config\": {\n        \"return_tensors\": \"pt\",\n        \"padding\": True,\n        \"max_length\": 512,\n        \"truncation\": True,\n    },        \n    \"roberta_base\": [\n        \"../input/tsecheckpoints/roberta-base-squad-2/roberta-base:v105/epoch_002_jaccard_0.737.ckpt\", \n        \"../input/tsecheckpoints/roberta-base-squad-2/roberta-base:v106/epoch_002_jaccard_0.739.ckpt\",\n        \"../input/tsecheckpoints/roberta-base-squad-2/roberta-base:v107/epoch_002_jaccard_0.744.ckpt\",\n        \"../input/tsecheckpoints/roberta-base-squad-2/roberta-base:v108/epoch_001_jaccard_0.742.ckpt\",\n        \"../input/tsecheckpoints/roberta-base-squad-2/roberta-base:v109/epoch_001_jaccard_0.742.ckpt\",\n        \n        \"../input/tsecheckpoints/roberta-base-lower-lr/roberta-base:v110/epoch_002_jaccard_0.728.ckpt\",\n        \"../input/tsecheckpoints/roberta-base-lower-lr/roberta-base:v111/epoch_001_jaccard_0.741.ckpt\",\n        \"../input/tsecheckpoints/roberta-base-lower-lr/roberta-base:v112/epoch_001_jaccard_0.738.ckpt\",\n        \"../input/tsecheckpoints/roberta-base-lower-lr/roberta-base:v113/epoch_001_jaccard_0.744.ckpt\",\n        \"../input/tsecheckpoints/roberta-base-lower-lr/roberta-base:v114/epoch_002_jaccard_0.733.ckpt\",\n        \n        ],\n    \n}","metadata":{"execution":{"iopub.status.busy":"2021-11-02T21:55:30.122684Z","iopub.execute_input":"2021-11-02T21:55:30.123013Z","iopub.status.idle":"2021-11-02T21:55:30.133093Z","shell.execute_reply.started":"2021-11-02T21:55:30.122975Z","shell.execute_reply":"2021-11-02T21:55:30.132104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = RobertaForQuestionAnswering.from_pretrained(\"roberta-base\")\n# model.save_pretrained('./roberta/base/model')\ntokenizer = RobertaTokenizer.from_pretrained('../input/tsepretrainedmodels/roberta/base/tokenizer', use_fast=True)\ntokenizer.save_pretrained('./roberta/base/tokenizer')","metadata":{"execution":{"iopub.status.busy":"2021-11-02T21:55:30.137188Z","iopub.execute_input":"2021-11-02T21:55:30.139313Z","iopub.status.idle":"2021-11-02T21:55:30.536336Z","shell.execute_reply.started":"2021-11-02T21:55:30.139275Z","shell.execute_reply":"2021-11-02T21:55:30.535635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset + Datamodule","metadata":{}},{"cell_type":"code","source":"class TSEDataset(Dataset):\n    def __init__(self, dataset):\n        self.dataset = dataset\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        example = self.dataset.iloc[idx]\n        return example.to_dict()\n\n\nclass TSEDataModule(pl.LightningDataModule):\n    \"\"\"\n    A DataModule implements 5 key methods:\n        - prepare_data (things to do on 1 GPU/TPU, not on every GPU/TPU in distributed mode)\n        - setup (things to do on every accelerator in distributed mode)\n        - train_dataloader (the training dataloader)\n        - val_dataloader (the validation dataloader(s))\n        - test_dataloader (the test dataloader(s))\n\n    This allows you to share a full dataset without explaining how to download,\n    split, transform and process the data.\n\n    Read the docs:\n        https://pytorch-lightning.readthedocs.io/en/latest/extensions/datamodules.html\n    \"\"\"\n\n    def __init__(\n        self,\n        data_dir: str = \"../input/tweet-sentiment-extraction/test.csv\",\n        train_val_test_split: float = 0.8,\n        batch_size: int = 32,\n        val_batch_size: int = 32,\n        num_workers: int = 0,\n        max_length: int = 256,\n        pin_memory: bool = False,\n        k_folds: int = 5,\n        current_fold: int = 0,\n    ):\n        super().__init__()\n\n        self.current_fold = current_fold\n        self.k_folds = k_folds\n        self.data_dir = data_dir\n        self.tokenizer = RobertaTokenizer.from_pretrained('../input/tsepretrainedmodels/roberta/base/tokenizer', use_fast=True)\n        self.train_val_split = train_val_test_split\n        self.batch_size = batch_size\n        self.val_batch_size = val_batch_size\n        self.num_workers = num_workers\n        self.pin_memory = pin_memory\n        self.max_length = max_length\n        self.data_test = None\n        self.full_dataset = None\n\n    def prepare_data(self):\n        csv = pd.read_csv(self.data_dir).dropna()\n        def preprocess_text(text):\n            text = re.sub(r\"http\\S+\", \"URL\", text)\n            text = re.sub(r\"www\\.[a-zA-Z].\\S+\", \"URL\", text)\n            text = str(text).replace(\"ï¿½\", \"`\")\n            text = text.replace(\"ï\", \"\")\n            text = text.replace(\"Aam   these\", \"these\")\n            text = text.replace(\"Aam  these\", \"these\")\n            text = text.replace(\"d stinks\", \"stinks\")\n            text = text.replace(\"Wave looks interesting. ht\", \"Wave looks interesting.\")\n            return text\n        for index, example in csv.iterrows():\n            context = preprocess_text(example[\"text\"])\n            question = example[\"sentiment\"] + \"?\"\n            csv.at[index, \"question\"] = question\n        self.full_dataset = TSEDataset(csv)\n\n    def setup(self, stage = None):\n        \"\"\"Load data. Set variables: self.data_train, self.data_val.\"\"\"\n        self.data_test = self.full_dataset\n\n\n    def test_dataloader(self):\n        return DataLoader(\n            dataset=self.data_test,\n            batch_size=self.val_batch_size,\n            num_workers=self.num_workers,\n            pin_memory=self.pin_memory,\n            collate_fn=self.collate_fn,\n        )\n\n    def collate_fn(self, batch):\n        collate = torch.utils.data.dataloader.default_collate(batch)\n        encodings = self.tokenizer(\n            collate[\"question\"],\n            collate[\"text\"],\n            truncation=True,\n            return_tensors=\"pt\",\n            padding=True,\n            max_length=self.max_length,\n        )\n        return encodings, collate[\"textID\"]","metadata":{"execution":{"iopub.status.busy":"2021-11-02T21:55:30.538375Z","iopub.execute_input":"2021-11-02T21:55:30.538744Z","iopub.status.idle":"2021-11-02T21:55:30.556582Z","shell.execute_reply.started":"2021-11-02T21:55:30.538705Z","shell.execute_reply":"2021-11-02T21:55:30.555782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"class TSEModel(pl.LightningModule):\n    \"\"\"\n    A LightningModule organizes your PyTorch code into 5 sections:\n        - Computations (init).\n        - Train loop (training_step)\n        - Validation loop (validation_step)\n        - Test loop (test_step)\n        - Optimizers (configure_optimizers)\n\n    Read the docs:\n        https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html\n    \"\"\"\n\n    def __init__(\n        self,\n    ):\n        super().__init__()\n\n        # this line ensures params passed to LightningModule will be saved to ckpt\n        # it also allows to access params with 'self.hparams' attribute\n        self.model = RobertaForQuestionAnswering.from_pretrained('../input/tsepretrainedmodels/roberta/base/model')\n        self.tokenizer = RobertaTokenizer.from_pretrained('../input/tsepretrainedmodels/roberta/base/tokenizer', use_fast=True)\n        self.outputs = {\"textID\": [], \"selected_text\": [], \"start\": [], \"end\": [], \"x\": []}\n\n    def forward(self, x):\n        return self.model(**x)\n\n    def step(self, batch):\n        outputs = self.forward(batch)\n        start = self.model(**batch).start_logits.argmax(dim=1)\n        end = self.model(**batch).end_logits.argmax(dim=1)\n        return outputs.loss, start, end, self.model(**batch).start_logits, self.model(**batch).end_logits\n\n    def convert_pred_response(self, x, start, end):\n        return [\n            self.tokenizer.convert_tokens_to_string(\n                self.tokenizer.convert_ids_to_tokens(\n                    input_id[start[index] : end[index] + 1], skip_special_tokens=True\n                )\n            )\n            for index, input_id in enumerate(x.input_ids)\n        ]\n\n    def test_step(self, batch, batch_idx: int):\n        x, _ = batch\n        loss, start, end, s_logit, e_logit = self.step(x)\n        preds = self.convert_pred_response(x, start, end)\n        self.outputs[\"textID\"] += batch[1]\n        self.outputs[\"selected_text\"] += preds\n        self.outputs[\"start\"] += s_logit\n        self.outputs[\"end\"] += e_logit  \n        self.outputs[\"x\"] += [input_id for input_id in x.input_ids]       ","metadata":{"execution":{"iopub.status.busy":"2021-11-02T21:55:30.557802Z","iopub.execute_input":"2021-11-02T21:55:30.558152Z","iopub.status.idle":"2021-11-02T21:55:30.57163Z","shell.execute_reply.started":"2021-11-02T21:55:30.558097Z","shell.execute_reply":"2021-11-02T21:55:30.570926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"examples_id = list(pd.read_csv(\"../input/tweet-sentiment-extraction/test.csv\")[\"textID\"])\ntargets = [[] for _ in range(len(examples_id))]\nstart=[[] for _ in range(len(examples_id))]\nend= [[] for _ in range(len(examples_id))]\nx= [[] for _ in range(len(examples_id))]\n\ntrainer = Trainer(gpus=1)\ndatamodule = TSEDataModule()\n\nfor model_checkpoints in hyperparameters['roberta_base']:\n    model = TSEModel().load_from_checkpoint(model_checkpoints)\n    trainer.test(model=model, datamodule=datamodule)\n    for index, target in enumerate(model.outputs['selected_text']):\n        targets[index].append(target)\n        start[index].append(model.outputs['start'][index])\n        end[index].append(model.outputs['end'][index])\n        x[index].append(model.outputs['x'][index])\n    del model\n    gc.collect()\n    torch.cuda.empty_cache()","metadata":{"_kg_hide-output":false,"execution":{"iopub.status.busy":"2021-11-02T21:55:30.572561Z","iopub.execute_input":"2021-11-02T21:55:30.574934Z","iopub.status.idle":"2021-11-02T21:59:02.546305Z","shell.execute_reply.started":"2021-11-02T21:55:30.574902Z","shell.execute_reply":"2021-11-02T21:59:02.545551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Bagging Ensemble Inference","metadata":{}},{"cell_type":"code","source":"bagging_targets = [[] for _ in range(len(examples_id))]\n\nfor s, e, x_el, b in zip(start, end, x, bagging_targets):\n    s_ = torch.mean(torch.stack(s), dim=0).argmax(dim=0)\n    e_ = torch.mean(torch.stack(e), dim=0).argmax(dim=0)\n    x_ = x_el[0]\n    b.append(tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(x_[s_ : e_ + 1], skip_special_tokens=True)))","metadata":{"execution":{"iopub.status.busy":"2021-11-02T21:59:02.54755Z","iopub.execute_input":"2021-11-02T21:59:02.547957Z","iopub.status.idle":"2021-11-02T21:59:04.543799Z","shell.execute_reply.started":"2021-11-02T21:59:02.547919Z","shell.execute_reply":"2021-11-02T21:59:04.543084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv(\"../input/tweet-sentiment-extraction/test.csv\")\ndf = pd.DataFrame(data={'textID':examples_id, 'selected_text': [target[0] for target in bagging_targets]})","metadata":{"execution":{"iopub.status.busy":"2021-11-02T22:11:13.876698Z","iopub.execute_input":"2021-11-02T22:11:13.877232Z","iopub.status.idle":"2021-11-02T22:11:14.202487Z","shell.execute_reply.started":"2021-11-02T22:11:13.877195Z","shell.execute_reply":"2021-11-02T22:11:14.201751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Post-Processing","metadata":{}},{"cell_type":"code","source":"for (_, col), (_, x_col) in zip(df.iterrows(), test.iterrows()):\n    if len(x_col[\"text\"].split()) <= 3:\n        col[\"selected_text\"] = x_col[\"text\"]\ndf.to_csv('./submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-02T22:32:37.613874Z","iopub.execute_input":"2021-11-02T22:32:37.614659Z","iopub.status.idle":"2021-11-02T22:32:38.101599Z","shell.execute_reply.started":"2021-11-02T22:32:37.614609Z","shell.execute_reply":"2021-11-02T22:32:38.100797Z"},"trusted":true},"execution_count":null,"outputs":[]}]}