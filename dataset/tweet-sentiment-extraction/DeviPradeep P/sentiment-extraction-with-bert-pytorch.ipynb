{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\n\nimport os\nfrom tqdm import tqdm ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv')\ntest_df = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv')\nss_df = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/sample_submission.csv')\nprint(train_df.shape, test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import transformers\nimport tokenizers\nimport torch\nimport torch.nn as nn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Config variables\nMAX_LEN = 128\nTRAIN_BATCH_SIZE = 32\nVALID_BATCH_SIZE = 16\nEPOCHS = 10\nBERT_PATH = '/kaggle/input/bert-base-uncased'\nMODEL_PATH = '/kaggle/working/model.h5'\nTRAINING_FILE = '/kaggle/input/tweet-sentiment-extraction/train.csv'\nTOKENIZER = tokenizers.BertWordPieceTokenizer(\n    os.path.join(BERT_PATH, 'vocab.txt'),\n    lowercase=True\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Evaluation metric\ndef jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating dataset\nclass TweetDataset:\n    def __init__(self, tweet, sentiment, selected_text):\n        self.tweet = tweet\n        self.sentiment = sentiment\n        self.selected_text = selected_text\n        self.max_len = MAX_LEN\n        self.tokenizer = TOKENIZER\n        \n    def __len__(self):\n        return len(self.sentiment)\n    \n    def __getitem__(self, idx):\n        tweet = \" \".join(self.tweet[idx].split())\n        selected_text = \" \".join(self.selected_text[idx].split())\n        \n        len_sel_txt = len(selected_text)\n        idx0 = 0\n        idx1 = 1\n        for ind in (i for i, e in enumerate(tweet) if e==selected_text[0]):\n            if tweet[ind : ind+len_sel_txt] == selected_text:\n                idx0 = ind\n                idx1 = ind+len_sel_txt-1\n                break\n        char_targets = [0] * len(tweet)\n        if idx0 != -1 and idx1 != -1:\n            for j in range(idx0, idx1+1):\n                if tweet[j] != \" \":\n                    char_targets[j] = 1\n        \n        tok_tweet = self.tokenizer.encode(tweet)\n        tok_tweet_tokens = tok_tweet.tokens\n        tok_tweet_ids = tok_tweet.ids\n        tok_tweet_offsets = tok_tweet.offsets[1:-1] #First and last are for [CLS] and [SEP]\n        \n        targets = [0] * (len(tok_tweet_tokens)-2)\n        for i, (offset1, offset2) in enumerate(tok_tweet_offsets):\n            if sum(char_targets[offset1:offset2])>0:\n                targets[i] = 1\n                \n        targets = [0] + targets + [0]\n        targets_start = [0] * len(targets)\n        targets_end = [0] * len(targets)\n        \n        non_zero = np.nonzero(targets)[0] #Indices of non-zero values\n        if len(non_zero) > 0:\n            targets_start[non_zero[0]] = 1\n            targets_end[non_zero[-1]] = 1\n        \n        mask = [1] * len(tok_tweet_ids)\n        token_type_ids = [0] * len(tok_tweet_ids)\n        \n        padding_len = self.max_len-len(tok_tweet_ids)\n        ids = tok_tweet_ids + [0]*padding_len\n        mask = mask + [0]*padding_len\n        token_type_ids = token_type_ids + [0]*padding_len\n        targets = targets + [0]*padding_len\n        targets_start = targets_start + [0]*padding_len\n        targets_end = targets_end + [0]*padding_len\n        \n        sentiment = [1,0,0]\n        if self.sentiment[idx] == \"positive\":\n            sentiment = [0,0,1]\n        if self.sentiment[idx] == \"negative\":\n            sentiment = [0,1,0]\n        \n        return {\n            \"ids\" : torch.tensor(ids, dtype=torch.long),\n            \"mask\" : torch.tensor(mask, dtype=torch.long),\n            \"token_type_ids\" : torch.tensor(token_type_ids, dtype=torch.long),\n            \"targets\" : torch.tensor(targets, dtype=torch.long),\n            \"targets_start\" : torch.tensor(targets_start, dtype=torch.long),\n            \"targets_end\" : torch.tensor(targets_end, dtype=torch.long),\n            \"tweet_tokens\" : \" \".join(tok_tweet_tokens),\n            \"orig_tweet\" : self.tweet[idx],\n            \"sentiment\" : torch.tensor(sentiment, dtype=torch.long),\n            \"orig_sentiment\" : self.sentiment[idx],\n            \"orig_selected_text\" : self.selected_text[idx]\n        }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model definition\nclass BERTBaseUncased(nn.Module):\n    def __init__(self):\n        super(BERTBaseUncased, self).__init__()\n        self.bert = transformers.BertModel.from_pretrained(BERT_PATH)\n        self.bert_drop = nn.Dropout(0.2)\n        self.l0 = nn.Linear(768, 2)\n    \n    def forward(self, ids, mask, token_type_ids):\n        sequence_output, pooled_output = self.bert(\n            ids,\n            mask,\n            token_type_ids\n        )\n        logits = self.l0(sequence_output)\n        #Splitting 768X2 tensor into 2 768X1 tensors\n        start_logit, end_logit = logits.split(1, dim=-1)\n        start_logit = start_logit.squeeze(-1)\n        end_logit = end_logit.squeeze(-1)\n        return start_logit, end_logit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def loss_fn(o1, o2, t1, t2):\n    l1 = nn.BCEWithLogitsLoss()(o1, t1)\n    l2 = nn.BCEWithLogitsLoss()(o2, t2)\n    return l1+l2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_fn(data_loader, model, optiizer, device, scheduler):\n    model.train()\n    losses = AverageMeter()\n    tk0 = tqdm(data_loader, total=len(data_loader))\n    for index, data in enumerate(tk0):\n        ids = data[\"ids\"]\n        mask = data[\"mask\"]\n        token_type_ids = data[\"token_type_ids\"]\n        targets_start = data[\"targets_start\"]\n        targets_end = data[\"targets_end\"]\n        \n        ids = ids.to(device, dtype=torch.long)\n        mask = mask.to(device, dtype=torch.long)\n        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n        targets_start = targets_start.to(device, dtype=torch.float)\n        targets_end = targets_end.to(device, dtype=torch.float)\n        \n        optimizer.zero_grad()\n        start, end = model(\n            ids,\n            mask,\n            token_type_ids\n        )\n        \n        loss = loss_fn(start, end, targets_start, targets_end)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        losses.update(loss.item(), ids.size(0))\n        tk0.set_postfix(loss=losses.avg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelse:\n    device = torch.device(\"cpu\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train, df_valid = train_test_split(train_df, test_size=0.2, random_state=42, stratify=train_df.sentiment.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = df_train.reset_index(drop=True)\ndf_valid = df_valid.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = TweetDataset(\n    tweet = df_train.text.values,\n    sentiment = df_train.sentiment.values, \n    selected_text = df_train.selected_text.values\n)\n\ntrain_dataloader = torch.utils.data.DataLoader(\n    train_dataset, \n    batch_size = TRAIN_BATCH_SIZE\n    #num_workers\n)\n\nvalid_dataset = TweetDataset(\n    tweet = df_valid.text.values,\n    sentiment = df_valid.sentiment.values, \n    selected_text = df_valid.selected_text.values\n)\n\nvalid_dataloader = torch.utils.data.DataLoader(\n    valid_dataset, \n    batch_size = VALID_BATCH_SIZE\n    #num_workers\n)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = BERTBaseUncased()\nmodel.to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"parameters = list(model.named_parameters())\nno_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n\noptimizer_params = [\n    {\n        'params' : [\n            p for n, p in parameters if not any(nd in n for nd in no_decay)\n        ],   \n        'weight_decay':0.001\n    }, #Check syntax\n    {'params' : [p for n, p in parameters if any(nd in n for nd in no_decay)], 'weight_decay':0.0}\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_train_steps = int(EPOCHS* len(df_train)/TRAIN_BATCH_SIZE)\noptimizer = transformers.AdamW(optimizer_params, lr=3e-5)\nscheduler = transformers.get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=0, #What is this?\n    num_training_steps = num_train_steps\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_jaccard = 0\nfor epoch in range(EPOCHS):\n    print(\"Epoch {}\".format(epoch))\n    train_fn(train_dataloader, model, optimizer, device, scheduler)\n#     jaccard = eval_fn(valid_dataloader, model, device)\n#     print(\"jaccard score : {}\".format(accuracy))\n#     if accuracy > best_acc:\n#         torch.save(model.state_dict(), f\"{MODEL_PATH}/model_{epoch}.pkl\")\n#         best_jaccard = jaccard","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}