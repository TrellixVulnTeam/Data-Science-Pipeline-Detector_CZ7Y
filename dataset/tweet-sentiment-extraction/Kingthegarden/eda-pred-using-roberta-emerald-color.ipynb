{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# --- CSS STYLE ---\nfrom IPython.core.display import HTML\ndef css_styling():\n    styles = open(\"../input/2020-cost-of-living/alerts.css\", \"r\").read()\n    return HTML(\"<style>\"+styles+\"</style>\")\ncss_styling()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-09T12:41:23.332244Z","iopub.execute_input":"2021-07-09T12:41:23.332703Z","iopub.status.idle":"2021-07-09T12:41:23.355745Z","shell.execute_reply.started":"2021-07-09T12:41:23.332618Z","shell.execute_reply":"2021-07-09T12:41:23.353855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #226D28\"><b>‚úÖ Load Green palette </b></p>","metadata":{}},{"cell_type":"code","source":"import seaborn as sns","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-09T12:41:24.127097Z","iopub.execute_input":"2021-07-09T12:41:24.127475Z","iopub.status.idle":"2021-07-09T12:41:24.598399Z","shell.execute_reply.started":"2021-07-09T12:41:24.127447Z","shell.execute_reply":"2021-07-09T12:41:24.597107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.color_palette(\"Greens\",  as_cmap=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-09T12:41:24.816567Z","iopub.execute_input":"2021-07-09T12:41:24.816935Z","iopub.status.idle":"2021-07-09T12:41:24.8409Z","shell.execute_reply.started":"2021-07-09T12:41:24.816907Z","shell.execute_reply":"2021-07-09T12:41:24.839492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Greens_palette = sns.color_palette(\"Greens\", 20)\nsns.palplot(Greens_palette)","metadata":{"execution":{"iopub.status.busy":"2021-07-09T12:52:45.072006Z","iopub.execute_input":"2021-07-09T12:52:45.072376Z","iopub.status.idle":"2021-07-09T12:52:45.2355Z","shell.execute_reply.started":"2021-07-09T12:52:45.072345Z","shell.execute_reply":"2021-07-09T12:52:45.234509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <p style=\"background-color:#008d62 ; font-family:Comic Sans MS; font-size:150%; text-align:center\"> üçÄ <b>Introduction</b> üçÄ\n\n<center><img src=\"https://junpyopark.github.io/assets/img/sentiment.png\"/ width=\"800\" height=\"700\" ></center>\n\n\n<div class=\"alert success-alert\">\n‚úî <b>General information about competitions : </b>\n\n<br>In this competition, we must predict whether phrases such as **\"Myridiculous dog is amazing\"** will have a positive or negative effect.\nVarious NLP frameworks exist, such as BERT, to make these predictions.\nMy notebook will introduce **basic and common data clearing methods to help predict NLP models.**\nI will also write modeling using RoBERTa, so please look forward to it!\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert success-alert\">\n‚úî <b>Understanding evaluation metrics : What's a jacquard? : </b>\n\n<br>Let's say there are two sets, A and B. An intersection is a set of elements that are common in two sets. That is, **the idea of jacquard similarity is that if we find the ratio of the intersection in the union, we can find the similarity of the two sets A and B.**\nJacquard similarity has a value **between 0 and 1**, if two sets are equal, a value of 1, and if there are no common elements in both sets, a value of 0. When J is the function to obtain jacquard similarity, the jacquard similarity function J is shown below\n\n<center><img src=\"data:image/png;base64,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\"/ width=\"500\" height=\"500\" ></center>\n\n</div>\n\n\n\n","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #008d62\"><b>‚úÖ Let me understand through a simple example </b></p>","metadata":{}},{"cell_type":"code","source":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))\n\ndoc1 = \"apple banana everyone like likey watch card holder\"\ndoc2 = \"apple banana coupon passport love you\"\n\nprint(jaccard(doc1, doc2))","metadata":{"execution":{"iopub.status.busy":"2021-07-09T12:41:28.419824Z","iopub.execute_input":"2021-07-09T12:41:28.420328Z","iopub.status.idle":"2021-07-09T12:41:28.429843Z","shell.execute_reply.started":"2021-07-09T12:41:28.420298Z","shell.execute_reply":"2021-07-09T12:41:28.428381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #008d62\"><b>Insights</b></p>\n\n><div class=\"alert success-alert\" role=\"alert\">‚úî <b>There are two words that appear in both document 1 and 2 in total: banana and apple. Now dividing the number of intersections by the number of assemblies calculates the jacquard similarity.</b></div>","metadata":{}},{"cell_type":"markdown","source":"# <p style=\"background-color:#008d62 ; font-family:Comic Sans MS; font-size:150%; text-align:center\"> üçÄ <b>Table of Contents</b> üçÄ\n\n<div class=\"alert success-alert\">\n    \n‚úî <b>Importing Neccesary Packages</b><br><br>\n\ni)  Most basic stuff for EDA, packages for text processing, Libraries for text preprocessing<br><br>\n  \nii) We will import basic libraries for data discovery and packages for the Bert model<br><br>\n  \n‚úî <b>Road Data</b><br><br>\n\ni)  We will need train.csv, test.csv<br><br>\n    \nii) These include text ID, text, sentiment, selected_text columns that can predict sentiment<br><br>\n    \n‚úî <b>EDA</b><br><br>\n\ni)  We will see how target values are distributed according to sentiment<br><br>\n    \nii) Let's look at the train and test data set in general<br><br>\n    \n‚úî <b>Cleaning Data</b><br><br>\n\ni)  Check for missing values, Removed urls, emojis and punctuations, Removed stopwords etc.<br><br>\n\nii) We proceed with data preprocessing for accurate predictive models<br><br>\n    \n‚úî <b>Visualizing the Data</b><br><br>\n\ni)  Tweet Lengths, Word Lengths, Word Counts, Most Common Words<br><br>\n    \nii) Increase overall understanding of data through visualization<br><br>\n    \n‚úî <b>N-gram Analysis</b><br><br>\n\ni)  An approach that considers only a few words and a sequence data representation approach<br><br>\n    \nii) Let's adjust n in n-gram and look at the changes<br><br>\n    \n‚úî <b>Wordclouds</b><br><br>\n    \n‚úî <b>Modelling using RoBERTa</b><br><br>\n \n‚úî <b>References</b>\n</div>","metadata":{}},{"cell_type":"markdown","source":"# <p style=\"background-color:#008d62 ; font-family:Comic Sans MS; font-size:150%; text-align:center\"> üçÄ <b>Data and Packages Improts</b> üçÄ","metadata":{}},{"cell_type":"code","source":"# Most basic stuff for EDA.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Core packages for text processing.\n\nimport string\nimport re\n\n# Libraries for text preprocessing.\n\nimport nltk\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\nnltk.download('wordnet')\nfrom nltk.corpus import stopwords, wordnet\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.probability import FreqDist\n\n# Loading some sklearn packaces for modelling.\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation, NMF\nfrom sklearn.metrics import f1_score, accuracy_score\n\n# Some packages for word clouds and NER.\n\nfrom wordcloud import WordCloud, STOPWORDS\nfrom collections import Counter, defaultdict\nfrom PIL import Image\nimport spacy\n!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz\nimport en_core_web_sm\n\n# Core packages for general use throughout the notebook.\n\nimport os\nimport random\nimport warnings\nimport time\nimport datetime\nfrom tqdm.autonotebook import tqdm\n\n# For customizing our plots.\n\nfrom matplotlib.ticker import MaxNLocator\nimport matplotlib.gridspec as gridspec\nimport matplotlib.patches as mpatches\n\n\n# Setting some options for general use.\n\nstop = set(stopwords.words('english'))\nplt.style.use('fivethirtyeight')\nsns.set(font_scale=1.5)\npd.options.display.max_columns = 250\npd.options.display.max_rows = 250\nwarnings.filterwarnings('ignore')\n\n# Modelling Packages\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F \nimport torch.optim as optim\nfrom sklearn import model_selection\nfrom sklearn.model_selection import StratifiedKFold\nimport transformers\nimport tokenizers\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup\nfrom tokenizers import ByteLevelBPETokenizer\nfrom transformers import RobertaModel, RobertaConfig\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom transformers import logging\nlogging.set_verbosity_warning()\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline\nimport matplotlib.pyplot as plt","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-07-09T12:49:54.177122Z","iopub.execute_input":"2021-07-09T12:49:54.177513Z","iopub.status.idle":"2021-07-09T12:50:07.655883Z","shell.execute_reply.started":"2021-07-09T12:49:54.177472Z","shell.execute_reply":"2021-07-09T12:50:07.65477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/tweet-sentiment-extraction/train.csv')\ntest = pd.read_csv('../input/tweet-sentiment-extraction/test.csv')","metadata":{"execution":{"iopub.status.busy":"2021-07-09T12:50:15.507237Z","iopub.execute_input":"2021-07-09T12:50:15.507807Z","iopub.status.idle":"2021-07-09T12:50:15.671678Z","shell.execute_reply.started":"2021-07-09T12:50:15.507752Z","shell.execute_reply":"2021-07-09T12:50:15.670539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Taking general look at the both datasets.\n\ndisplay(train.sample(3))\ndisplay(test.sample(3))","metadata":{"execution":{"iopub.status.busy":"2021-07-09T12:50:15.873783Z","iopub.execute_input":"2021-07-09T12:50:15.874165Z","iopub.status.idle":"2021-07-09T12:50:15.90071Z","shell.execute_reply.started":"2021-07-09T12:50:15.87413Z","shell.execute_reply":"2021-07-09T12:50:15.89915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking observation and feature numbers for train and test data.\n\nprint(train.shape)\nprint(test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-09T12:50:16.590686Z","iopub.execute_input":"2021-07-09T12:50:16.591095Z","iopub.status.idle":"2021-07-09T12:50:16.598672Z","shell.execute_reply.started":"2021-07-09T12:50:16.591064Z","shell.execute_reply":"2021-07-09T12:50:16.597278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #008d62\"><b>Insights</b></p>\n\n><div class=\"alert success-alert\" role=\"alert\">‚úî <b>The train and test data sets each have 4, 3 features, 2,7481 for train and 3,534 for test.</b></div>","metadata":{}},{"cell_type":"markdown","source":"# <p style=\"background-color:#008d62 ; font-family:Comic Sans MS; font-size:150%; text-align:center\"> üçÄ <b>Exploratory Data Analysis</b> üçÄ","metadata":{}},{"cell_type":"code","source":"# Displaying target distribution.\n\nGreens_palette_3 = sns.color_palette(\"Greens\", 3)\n\nfig, axes = plt.subplots(ncols=2, nrows=1, figsize=(10, 5), dpi=100)\nsns.countplot(train['sentiment'],palette = Greens_palette_3, ax=axes[0])\naxes[1].pie(train['sentiment'].value_counts(),\n            labels=['neutral', 'positive', 'negative'],\n            autopct='%1.2f%%',\n            shadow=True,\n            colors = Greens_palette_3,\n            explode=(0.05, 0.05, 0.05),\n            startangle=60)\nfig.suptitle('Twitter sentiment Extaction in Train', fontsize=20)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-09T12:50:17.843183Z","iopub.execute_input":"2021-07-09T12:50:17.843599Z","iopub.status.idle":"2021-07-09T12:50:18.207092Z","shell.execute_reply.started":"2021-07-09T12:50:17.843563Z","shell.execute_reply":"2021-07-09T12:50:18.205752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(ncols=2, nrows=1, figsize=(10, 5), dpi=100)\nsns.countplot(test['sentiment'],palette = Greens_palette_3,ax=axes[0])\naxes[1].pie(test['sentiment'].value_counts(),\n            labels=['neutral', 'positive', 'negative'],\n            autopct='%1.2f%%',\n            shadow=True,\n            colors = Greens_palette_3,\n            explode=(0.05, 0.05, 0.05),\n            startangle=60)\nfig.suptitle('Twitter sentiment Extaction in Test', fontsize=20)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-09T12:50:18.287571Z","iopub.execute_input":"2021-07-09T12:50:18.287891Z","iopub.status.idle":"2021-07-09T12:50:18.633686Z","shell.execute_reply.started":"2021-07-09T12:50:18.287831Z","shell.execute_reply":"2021-07-09T12:50:18.632275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <p style=\"background-color:#008d62 ; font-family:Comic Sans MS; font-size:150%; text-align:center\"> üçÄ <b>Data Cleaning</b> üçÄ\n\n<p style=\"font-family:Comic Sans MS; font-size:130%; color: #008d62; text-align:center;\"><b>Before visualizing the data through the basic Data Cleaning process, we will optimize it for \nanalysis purposes.</b></p>\n<br><br>\n \n<div class=\"alert success-alert\">\n    \n‚úî <b>The basic order is as follows : </b>\n    \n* Check for missing values\n\n* Removed urls, emojis and punctuations\n\n* Tokenized base text and selected_text\n\n* Lower cased clean text\n\n* Removed stopwords\n\n* Applied part of speech tags\n\n* Converted part of speeches to wordnet format\n\n* Applying word lemmatizer\n\n* Converted tokenized text to string again\n    \n</div>","metadata":{}},{"cell_type":"code","source":"# Check for missing values\n\ntrain.isnull().value_counts(), test.isnull().value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-07-09T12:50:20.046804Z","iopub.execute_input":"2021-07-09T12:50:20.047183Z","iopub.status.idle":"2021-07-09T12:50:20.081918Z","shell.execute_reply.started":"2021-07-09T12:50:20.047152Z","shell.execute_reply":"2021-07-09T12:50:20.080428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove missing values\n\ntrain.dropna(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-09T12:50:20.960123Z","iopub.execute_input":"2021-07-09T12:50:20.960507Z","iopub.status.idle":"2021-07-09T12:50:20.980613Z","shell.execute_reply.started":"2021-07-09T12:50:20.960461Z","shell.execute_reply":"2021-07-09T12:50:20.97933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #008d62\"><b>Insights</b></p>\n\n><div class=\"alert success-alert\" role=\"alert\">‚úî <b>One NAN exists in train data. Therefore, it removes it.</b></div>","metadata":{}},{"cell_type":"code","source":"# Some basic helper functions to clean text by punctuations.\n\ndef remove_URL(text):\n    url = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url.sub(r'', text)\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\n        '['\n        u'\\U0001F600-\\U0001F64F'  # emoticons\n        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n        u'\\U0001F1E0-\\U0001F1FF'  # flags (iOS)\n        u'\\U00002702-\\U000027B0'\n        u'\\U000024C2-\\U0001F251'\n        ']+',\n        flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\n\ndef remove_html(text):\n    html = re.compile(r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n    return re.sub(html, '', text)\n\ndef remove_comma(readData):\n    text = re.sub('[-=+,#/\\?:^$.@*\\\"‚Äª~&%„Üç!„Äè\\\\‚Äò|\\(\\)\\[\\]\\<\\>`\\'‚Ä¶„Äã]', '', readData)\n    return text\n\ndef remove_punct(text):\n    table = str.maketrans('', '', string.punctuation)\n    return text.translate(table)\n\n# Applying helper functions (train Text)\n\ntrain['text_clean'] = train['text'].apply(lambda x: remove_URL(x))\ntrain['text_clean'] = train['text_clean'].apply(lambda x: remove_punct(x))\ntrain['text_clean'] = train['text_clean'].apply(lambda x: remove_html(x))\ntrain['text_clean'] = train['text_clean'].apply(lambda x: remove_emoji(x))\ntrain['text_clean'] = train['text_clean'].apply(lambda x: remove_comma(x))                     \n\n# Applying helper functions (train Selected Text)\n\ntrain['ST_clean'] = train['selected_text'].apply(lambda x: remove_URL(x))\ntrain['ST_clean'] = train['ST_clean'].apply(lambda x: remove_punct(x))\ntrain['ST_clean'] = train['ST_clean'].apply(lambda x: remove_html(x))\ntrain['ST_clean'] = train['ST_clean'].apply(lambda x: remove_emoji(x))\ntrain['ST_clean'] = train['ST_clean'].apply(lambda x: remove_comma(x))\n\n# Applying helper functions (test Text)\n\ntest['text_clean'] = test['text'].apply(lambda x: remove_URL(x))\ntest['text_clean'] = test['text_clean'].apply(lambda x: remove_punct(x))\ntest['text_clean'] = test['text_clean'].apply(lambda x: remove_html(x))\ntest['text_clean'] = test['text_clean'].apply(lambda x: remove_emoji(x))\ntest['text_clean'] = test['text_clean'].apply(lambda x: remove_comma(x))      ","metadata":{"execution":{"iopub.status.busy":"2021-07-09T12:50:22.253427Z","iopub.execute_input":"2021-07-09T12:50:22.253876Z","iopub.status.idle":"2021-07-09T12:50:23.374031Z","shell.execute_reply.started":"2021-07-09T12:50:22.253834Z","shell.execute_reply":"2021-07-09T12:50:23.37293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-09T12:50:23.376098Z","iopub.execute_input":"2021-07-09T12:50:23.376513Z","iopub.status.idle":"2021-07-09T12:50:23.392389Z","shell.execute_reply.started":"2021-07-09T12:50:23.37648Z","shell.execute_reply":"2021-07-09T12:50:23.391224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-09T12:50:23.394576Z","iopub.execute_input":"2021-07-09T12:50:23.395276Z","iopub.status.idle":"2021-07-09T12:50:23.414996Z","shell.execute_reply.started":"2021-07-09T12:50:23.395224Z","shell.execute_reply":"2021-07-09T12:50:23.413373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #008d62\"><b>Insights</b></p>\n\n><div class=\"alert success-alert\" role=\"alert\">‚úî <b>You can see that emoji, url, etc. has been erased well.</b></div>","metadata":{}},{"cell_type":"code","source":"# Tokenizing the tweet base texts.\n\ntrain['tokenized_text'] = train['text_clean'].apply(word_tokenize)\ntrain['tokenized_ST'] = train['ST_clean'].apply(word_tokenize)\ntest['tokenized_text'] = test['text_clean'].apply(word_tokenize)\ndisplay(train.sample(5))\ndisplay(test.sample(5))","metadata":{"execution":{"iopub.status.busy":"2021-07-09T12:50:24.983424Z","iopub.execute_input":"2021-07-09T12:50:24.983931Z","iopub.status.idle":"2021-07-09T12:50:34.366869Z","shell.execute_reply.started":"2021-07-09T12:50:24.9839Z","shell.execute_reply":"2021-07-09T12:50:34.365666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lower casing clean text.\n\ntrain['lower_text'] = train['tokenized_text'].apply(\n    lambda x: [word.lower() for word in x])\n\ntrain['lower_ST'] = train['tokenized_ST'].apply(\n    lambda x: [word.lower() for word in x])\n\ntest['lower_text'] = test['tokenized_text'].apply(\n    lambda x: [word.lower() for word in x])\n\ndisplay(train.sample(3))\ndisplay(test.sample(3))","metadata":{"execution":{"iopub.status.busy":"2021-07-09T12:50:34.36876Z","iopub.execute_input":"2021-07-09T12:50:34.369216Z","iopub.status.idle":"2021-07-09T12:50:34.857947Z","shell.execute_reply.started":"2021-07-09T12:50:34.369161Z","shell.execute_reply":"2021-07-09T12:50:34.856369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #008d62\"><b>‚úÖ Why should I lower? </b></p>\n\n><div class=\"alert success-alert\" role=\"alert\">‚úî <b>This is because words can be saved by changing them to lowercase letters and reducing the number of words through subsequent processes.</b></div>","metadata":{}},{"cell_type":"code","source":"# Removing stopwords.\n\ntrain['stopwords_removed_text'] = train['lower_text'].apply(\n    lambda x: [word for word in x if word not in stop])\n\ntrain['stopwords_removed_ST'] = train['lower_ST'].apply(\n    lambda x: [word for word in x if word not in stop])\n\ntest['stopwords_removed_text'] = test['lower_text'].apply(\n    lambda x: [word for word in x if word not in stop])\n\ndisplay(train.sample(3))\ndisplay(test.sample(3))","metadata":{"execution":{"iopub.status.busy":"2021-07-09T12:50:34.860815Z","iopub.execute_input":"2021-07-09T12:50:34.861252Z","iopub.status.idle":"2021-07-09T12:50:35.04441Z","shell.execute_reply.started":"2021-07-09T12:50:34.861206Z","shell.execute_reply":"2021-07-09T12:50:35.043009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #008d62\"><b>‚úÖ What is Part-of-speech Tagging ? </b></p>\n\n><div class=\"alert success-alert\" role=\"alert\">‚úî <b>It refers to identifying and tagging the parts of the words in a sentence. It is output in the form of a tuple, and is output in the form of a (word, tag). where tags are POS tags.</b></div>\n\n<br><br>\n\n<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #008d62\"><b>‚úÖ The types of Part-of-speech Tag are shown in the table below : </b></p>\n\n<center><img src=\"https://mblogthumb-phinf.pstatic.net/MjAyMDA0MDZfMTUz/MDAxNTg2MTQzOTE2MDc3._q5jz1Y50qyH23mv5VsU_Vz_s6At_CnVQl-HbyL873wg.Bk1q6ZSCUOJ5rxy5yZGBKTaBpnVbnPdvu_A3a1vyzfEg.PNG.bycho211/1.png?type=w800\"/ width=\"500\" height=\"700\" ></center>","metadata":{}},{"cell_type":"code","source":"# Applying part of speech tags.\n\ntrain['pos_tags_text'] = train['stopwords_removed_text'].apply(nltk.tag.pos_tag)\ntrain['pos_tags_ST'] = train['stopwords_removed_ST'].apply(nltk.tag.pos_tag)\ntest['pos_tags_text'] = test['stopwords_removed_text'].apply(nltk.tag.pos_tag)\n\ndisplay(train.sample(3))\ndisplay(test.sample(3))","metadata":{"execution":{"iopub.status.busy":"2021-07-09T12:50:35.046388Z","iopub.execute_input":"2021-07-09T12:50:35.046943Z","iopub.status.idle":"2021-07-09T12:51:11.950091Z","shell.execute_reply.started":"2021-07-09T12:50:35.046897Z","shell.execute_reply":"2021-07-09T12:51:11.948994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #008d62\"><b>‚úÖ Why change to Wordnet format ? </b></p>\n\n><div class=\"alert success-alert\" role=\"alert\">‚úî <b>Wordnet is an ontology built by Princeton University about English words in the past, which outlines how they relate to each other. Rather than organizing individual meanings by word like a dictionary, organizing them around the relationship between words can increase their utilization.</b></div>","metadata":{}},{"cell_type":"code","source":"# Converting part of speeches to wordnet format.\n\ndef get_wordnet_pos(tag):\n    if tag.startswith('J'):\n        return wordnet.ADJ\n    elif tag.startswith('V'):\n        return wordnet.VERB\n    elif tag.startswith('N'):\n        return wordnet.NOUN\n    elif tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN\n\n\ntrain['wordnet_pos_text'] = train['pos_tags_text'].apply(\n    lambda x: [(word, get_wordnet_pos(pos_tag)) for (word, pos_tag) in x])\n\ntrain['wordnet_pos_ST'] = train['pos_tags_ST'].apply(\n    lambda x: [(word, get_wordnet_pos(pos_tag)) for (word, pos_tag) in x])\n\ntest['wordnet_pos_text'] = test['pos_tags_text'].apply(\n    lambda x: [(word, get_wordnet_pos(pos_tag)) for (word, pos_tag) in x])\n\ndisplay(train.sample(3))\ndisplay(test.sample(3))","metadata":{"execution":{"iopub.status.busy":"2021-07-09T12:51:11.953584Z","iopub.execute_input":"2021-07-09T12:51:11.954026Z","iopub.status.idle":"2021-07-09T12:51:14.551074Z","shell.execute_reply.started":"2021-07-09T12:51:11.953976Z","shell.execute_reply":"2021-07-09T12:51:14.549376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #008d62\"><b>‚úÖ Let's find out about Lemmatizing ! </b></p>\n\n><div class=\"alert success-alert\" role=\"alert\">‚úî <b>First of all, Stemming and Lemmatizing, which we commonly know, are similar, but there is a big difference.</b></div>\n\n<br>\n\n<center><img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQgUAJcKqcD-WlJa3rz6-5FInyAG_EpoXxHIQ&usqp=CAU\"/ ></center>\n\n<br>\n<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #008d62\"><b>Insights</b></p>\n\n><div class=\"alert success-alert\" role=\"alert\">‚úî <b>Stemming sometimes creates words that do not exist, and Lemmatizing sometimes creates words that do exist.That's what it's like.</b></div>\n\n<br>\n\n<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #008d62\"><b>Conclusion</b></p>\n\n><div class=\"alert success-alert\" role=\"alert\">‚úî <b>In summary, both Stemming and Lemmatizing are root extractions, which can be extracted from the dictionary, and Lemma can be extracted from the dictionary.</b></div>","metadata":{}},{"cell_type":"code","source":"# Applying word lemmatizer.\n\nwnl = WordNetLemmatizer()\n\ntrain['lemmatized_text'] = train['wordnet_pos_text'].apply(\n    lambda x: [wnl.lemmatize(word, tag) for word, tag in x])\n\ntrain['lemmatized_text'] = train['lemmatized_text'].apply(\n    lambda x: [word for word in x if word not in stop])\n\ntrain['lemma_str_text'] = [' '.join(map(str, l)) for l in train['lemmatized_text']]\n\ntrain['lemmatized_ST'] = train['wordnet_pos_ST'].apply(\n    lambda x: [wnl.lemmatize(word, tag) for word, tag in x])\n\ntrain['lemmatized_ST'] = train['lemmatized_ST'].apply(\n    lambda x: [word for word in x if word not in stop])\n\ntrain['lemmatized_str_ST'] = [' '.join(map(str, l)) for l in train['lemmatized_ST']]\n\ntest['lemmatized_text'] = test['wordnet_pos_text'].apply(\n    lambda x: [wnl.lemmatize(word, tag) for word, tag in x])\n\ntest['lemmatized_text'] = test['lemmatized_text'].apply(\n    lambda x: [word for word in x if word not in stop])\n\ntest['lemma_str_text'] = [' '.join(map(str, l)) for l in test['lemmatized_text']]\n\ndisplay(train.sample(3))\ndisplay(test.sample(3))","metadata":{"execution":{"iopub.status.busy":"2021-07-09T12:51:14.553044Z","iopub.execute_input":"2021-07-09T12:51:14.553495Z","iopub.status.idle":"2021-07-09T12:51:17.488723Z","shell.execute_reply.started":"2021-07-09T12:51:14.553432Z","shell.execute_reply":"2021-07-09T12:51:17.487651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #008d62\"><b>Insights</b></p>\n\n><div class=\"alert success-alert\" role=\"alert\">‚úî <b>All right! The data is complete as we wish. Now let's look at the visualization to see what insights are hidden.</b></div>","metadata":{}},{"cell_type":"markdown","source":"# <p style=\"background-color:#008d62 ; font-family:Comic Sans MS; font-size:150%; text-align:center\"> üçÄ <b>Visualizing the Data</b> üçÄ","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #008d62\"><b>‚úÖ Tweet Lengths </b>","metadata":{}},{"cell_type":"code","source":"# Creating a new feature for the visualization.\n# text_clean Lengths visualization.\ntrain['Character Count'] = train['text_clean'].apply(lambda x: len(str(x)))\n\n\ndef plot_dist3(df, feature, title):\n    # Creating a customized chart. and giving in figsize and everything.\n    fig = plt.figure(constrained_layout=True, figsize=(18, 8))\n    # Creating a grid of 3 cols and 3 rows.\n    grid = gridspec.GridSpec(ncols=3, nrows=3, figure=fig)\n\n    # Customizing the histogram grid.\n    ax1 = fig.add_subplot(grid[:2, :2])\n    # Set the title.\n    ax1.set_title('Histogram')\n    # plot the histogram.\n    sns.distplot(df.loc[:, feature],\n                 hist=True,\n                 kde=True,\n                 ax=ax1,\n                 color='#008d62')\n    ax1.set(ylabel='Frequency')\n    ax1.xaxis.set_major_locator(MaxNLocator(nbins=20))\n\n    # Customizing the ecdf_plot.\n    ax2 = fig.add_subplot(grid[2:, :2])\n    # Set the title.\n    ax2.set_title('Empirical CDF')\n    # Plotting the ecdf_Plot.\n    sns.distplot(df.loc[:, feature],\n                 ax=ax2,\n                 kde_kws={'cumulative': True},\n                 hist_kws={'cumulative': True},\n                 color='#008d62')\n    ax2.xaxis.set_major_locator(MaxNLocator(nbins=20))\n    ax2.set(ylabel='Cumulative Probability')\n\n    # Customizing the Box Plot.\n    ax3 = fig.add_subplot(grid[:, 2])\n    # Set title.\n    ax3.set_title('Box Plot')\n    # Plotting the box plot.\n    sns.boxplot(y=feature, data=df, ax=ax3, color='#008d62')\n    ax3.yaxis.set_major_locator(MaxNLocator(nbins=20))\n\n    plt.suptitle(f'{title}', fontsize=24)\n\n# Greens_palette_3 = sns.color_palette(\"Greens\", 3)\n# sns.palplot(Greens_palette_3)","metadata":{"execution":{"iopub.status.busy":"2021-07-09T12:51:26.242918Z","iopub.execute_input":"2021-07-09T12:51:26.24329Z","iopub.status.idle":"2021-07-09T12:51:26.28268Z","shell.execute_reply.started":"2021-07-09T12:51:26.243257Z","shell.execute_reply":"2021-07-09T12:51:26.281496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_dist3(train[train['sentiment'] == 'positive'], 'Character Count',\n           'Characters Per \"positive\" Tweet')","metadata":{"execution":{"iopub.status.busy":"2021-07-09T12:51:26.937374Z","iopub.execute_input":"2021-07-09T12:51:26.937977Z","iopub.status.idle":"2021-07-09T12:51:28.92937Z","shell.execute_reply.started":"2021-07-09T12:51:26.93793Z","shell.execute_reply":"2021-07-09T12:51:28.928267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_dist3(train[train['sentiment'] == 'neutral'], 'Character Count',\n           'Characters Per \"neutral\" Tweet')","metadata":{"execution":{"iopub.status.busy":"2021-07-09T12:51:28.931447Z","iopub.execute_input":"2021-07-09T12:51:28.932205Z","iopub.status.idle":"2021-07-09T12:51:30.751849Z","shell.execute_reply.started":"2021-07-09T12:51:28.932147Z","shell.execute_reply":"2021-07-09T12:51:30.750354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_dist3(train[train['sentiment'] == 'negative'], 'Character Count',\n           'Characters Per \"negative\" Tweet')","metadata":{"execution":{"iopub.status.busy":"2021-07-09T12:51:30.754332Z","iopub.execute_input":"2021-07-09T12:51:30.754893Z","iopub.status.idle":"2021-07-09T12:51:32.327662Z","shell.execute_reply.started":"2021-07-09T12:51:30.754845Z","shell.execute_reply":"2021-07-09T12:51:32.326589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ST_clean Lengths visualization.\n\ntrain['Character Count ST'] = train['ST_clean'].apply(lambda x: len(str(x)))","metadata":{"execution":{"iopub.status.busy":"2021-07-09T12:51:32.329831Z","iopub.execute_input":"2021-07-09T12:51:32.330316Z","iopub.status.idle":"2021-07-09T12:51:32.359775Z","shell.execute_reply.started":"2021-07-09T12:51:32.330268Z","shell.execute_reply":"2021-07-09T12:51:32.358536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_dist3(train[train['sentiment'] == 'positive'], 'Character Count ST',\n           'Characters Per \"positive\" Tweet')","metadata":{"execution":{"iopub.status.busy":"2021-07-09T12:51:32.361459Z","iopub.execute_input":"2021-07-09T12:51:32.362289Z","iopub.status.idle":"2021-07-09T12:51:34.040098Z","shell.execute_reply.started":"2021-07-09T12:51:32.362223Z","shell.execute_reply":"2021-07-09T12:51:34.038967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_dist3(train[train['sentiment'] == 'neutral'], 'Character Count ST',\n           'Characters Per \"neutral\" Tweet')","metadata":{"execution":{"iopub.status.busy":"2021-07-09T12:51:34.043358Z","iopub.execute_input":"2021-07-09T12:51:34.043696Z","iopub.status.idle":"2021-07-09T12:51:35.849508Z","shell.execute_reply.started":"2021-07-09T12:51:34.043664Z","shell.execute_reply":"2021-07-09T12:51:35.84839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_dist3(train[train['sentiment'] == 'negative'], 'Character Count ST',\n           'Characters Per \"negative\" Tweet')","metadata":{"execution":{"iopub.status.busy":"2021-07-09T12:51:35.851386Z","iopub.execute_input":"2021-07-09T12:51:35.852019Z","iopub.status.idle":"2021-07-09T12:51:37.589045Z","shell.execute_reply.started":"2021-07-09T12:51:35.851968Z","shell.execute_reply":"2021-07-09T12:51:37.587545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test data : text_clean Lengths visualization.\n\ntest['Character Count'] = test['text_clean'].apply(lambda x: len(str(x)))","metadata":{"execution":{"iopub.status.busy":"2021-07-09T12:51:37.603715Z","iopub.execute_input":"2021-07-09T12:51:37.609648Z","iopub.status.idle":"2021-07-09T12:51:37.631592Z","shell.execute_reply.started":"2021-07-09T12:51:37.609597Z","shell.execute_reply":"2021-07-09T12:51:37.63014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_dist3(test[test['sentiment'] == 'positive'], 'Character Count',\n           'Characters Per \"positive\" Tweet')","metadata":{"execution":{"iopub.status.busy":"2021-07-09T12:51:40.026367Z","iopub.execute_input":"2021-07-09T12:51:40.026787Z","iopub.status.idle":"2021-07-09T12:51:41.390213Z","shell.execute_reply.started":"2021-07-09T12:51:40.026755Z","shell.execute_reply":"2021-07-09T12:51:41.388861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_dist3(test[test['sentiment'] == 'neutral'], 'Character Count',\n           'Characters Per \"neutral\" Tweet')","metadata":{"execution":{"iopub.status.busy":"2021-07-09T12:51:41.392238Z","iopub.execute_input":"2021-07-09T12:51:41.392956Z","iopub.status.idle":"2021-07-09T12:51:42.514381Z","shell.execute_reply.started":"2021-07-09T12:51:41.392908Z","shell.execute_reply":"2021-07-09T12:51:42.51334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_dist3(test[test['sentiment'] == 'negative'], 'Character Count',\n           'Characters Per \"negative\" Tweet')","metadata":{"execution":{"iopub.status.busy":"2021-07-09T12:51:42.516767Z","iopub.execute_input":"2021-07-09T12:51:42.517233Z","iopub.status.idle":"2021-07-09T12:51:43.647444Z","shell.execute_reply.started":"2021-07-09T12:51:42.517187Z","shell.execute_reply":"2021-07-09T12:51:43.646408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #008d62\"><b>Insights</b></p>\n\n><div class=\"alert success-alert\" role=\"alert\">‚úî <b>Both train and test show similar distributions in positive, neutral, and negative.</b></div>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #008d62\"><b>‚úÖ Word Counts </b>","metadata":{}},{"cell_type":"code","source":"def plot_word_number_histogram(textne, textpo, textng):\n    \n    \"\"\"A function for comparing word counts\"\"\"\n\n    fig, axes = plt.subplots(ncols=3, nrows=1, figsize=(18, 6), sharey=True)\n    sns.distplot(textne.str.split().map(lambda x: len(x)), ax=axes[0], color='#008d62')\n    sns.distplot(textpo.str.split().map(lambda x: len(x)), ax=axes[1], color='#008d62')\n    sns.distplot(textng.str.split().map(lambda x: len(x)), ax=axes[2], color='#008d62')\n    \n    axes[0].set_xlabel('Word Count')\n    axes[0].set_ylabel('neutral')\n    axes[0].set_title('Reliable')\n    axes[1].set_xlabel('Word Count')\n    axes[1].set_title('positive')\n    axes[2].set_xlabel('Word Count')\n    axes[2].set_title('negative')\n    \n    fig.suptitle('Punctuations in tweets', fontsize=24, va='baseline')\n    \n    fig.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2021-07-09T12:51:43.844835Z","iopub.execute_input":"2021-07-09T12:51:43.845173Z","iopub.status.idle":"2021-07-09T12:51:43.855972Z","shell.execute_reply.started":"2021-07-09T12:51:43.845133Z","shell.execute_reply":"2021-07-09T12:51:43.854494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_word_number_histogram(train[train['sentiment'] == 'neutral']['text'],\n                           train[train['sentiment'] == 'positive']['text'],\n                           train[train['sentiment'] == 'negative']['text'])","metadata":{"execution":{"iopub.status.busy":"2021-07-09T12:51:44.436754Z","iopub.execute_input":"2021-07-09T12:51:44.437117Z","iopub.status.idle":"2021-07-09T12:51:45.696262Z","shell.execute_reply.started":"2021-07-09T12:51:44.437085Z","shell.execute_reply":"2021-07-09T12:51:45.69512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_word_number_histogram(train[train['sentiment'] == 'neutral']['selected_text'],\n                           train[train['sentiment'] == 'positive']['selected_text'],\n                           train[train['sentiment'] == 'negative']['selected_text'])","metadata":{"execution":{"iopub.status.busy":"2021-07-09T12:51:45.698307Z","iopub.execute_input":"2021-07-09T12:51:45.698772Z","iopub.status.idle":"2021-07-09T12:51:47.105042Z","shell.execute_reply.started":"2021-07-09T12:51:45.698726Z","shell.execute_reply":"2021-07-09T12:51:47.103946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_word_number_histogram(test[test['sentiment'] == 'neutral']['text'],\n                           test[test['sentiment'] == 'positive']['text'],\n                           test[test['sentiment'] == 'negative']['text'])","metadata":{"execution":{"iopub.status.busy":"2021-07-09T12:51:47.107281Z","iopub.execute_input":"2021-07-09T12:51:47.107775Z","iopub.status.idle":"2021-07-09T12:51:47.989661Z","shell.execute_reply.started":"2021-07-09T12:51:47.107724Z","shell.execute_reply":"2021-07-09T12:51:47.986821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #008d62\"><b>‚úÖ Word Lengths </b>","metadata":{}},{"cell_type":"code","source":"def plot_word_len_histogram(textne, textpo, textng):\n    \n    \"\"\"A function for comparing average word length\"\"\"\n    \n    fig, axes = plt.subplots(ncols=3, nrows=1, figsize=(18, 6), sharey=True)\n    sns.distplot(textne.str.split().apply(lambda x: [len(i) for i in x]).map(\n        lambda x: np.mean(x)),\n                 ax=axes[0], color='#008d62')\n    sns.distplot(textpo.str.split().apply(lambda x: [len(i) for i in x]).map(\n        lambda x: np.mean(x)),\n                 ax=axes[1], color='#008d62')\n    sns.distplot(textng.str.split().apply(lambda x: [len(i) for i in x]).map(\n        lambda x: np.mean(x)),\n                 ax=axes[2], color='#008d62')\n    \n    axes[0].set_xlabel('Word Count')\n    axes[0].set_ylabel('neutral')\n    axes[0].set_title('Reliable')\n    axes[1].set_xlabel('Word Count')\n    axes[1].set_title('positive')\n    axes[2].set_xlabel('Word Count')\n    axes[2].set_title('negative')\n    \n    fig.suptitle('Mean Word Lengths', fontsize=24, va='baseline')\n    fig.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2021-07-09T12:51:47.991747Z","iopub.execute_input":"2021-07-09T12:51:47.992203Z","iopub.status.idle":"2021-07-09T12:51:48.005007Z","shell.execute_reply.started":"2021-07-09T12:51:47.992158Z","shell.execute_reply":"2021-07-09T12:51:48.00328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_word_len_histogram(train[train['sentiment'] == 'neutral']['text'],\n                        train[train['sentiment'] == 'positive']['text'],\n                        train[train['sentiment'] == 'negative']['text'])","metadata":{"execution":{"iopub.status.busy":"2021-07-09T12:51:49.297112Z","iopub.execute_input":"2021-07-09T12:51:49.297511Z","iopub.status.idle":"2021-07-09T12:51:51.869076Z","shell.execute_reply.started":"2021-07-09T12:51:49.297464Z","shell.execute_reply":"2021-07-09T12:51:51.867741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_word_len_histogram(train[train['sentiment'] == 'neutral']['selected_text'],\n                        train[train['sentiment'] == 'positive']['selected_text'],\n                        train[train['sentiment'] == 'negative']['selected_text'])","metadata":{"execution":{"iopub.status.busy":"2021-07-09T12:51:53.287629Z","iopub.execute_input":"2021-07-09T12:51:53.288036Z","iopub.status.idle":"2021-07-09T12:51:55.25672Z","shell.execute_reply.started":"2021-07-09T12:51:53.287975Z","shell.execute_reply":"2021-07-09T12:51:55.255606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_word_len_histogram(test[test['sentiment'] == 'neutral']['text'],\n                        test[test['sentiment'] == 'positive']['text'],\n                        test[test['sentiment'] == 'negative']['text'])","metadata":{"execution":{"iopub.status.busy":"2021-07-09T12:51:55.258679Z","iopub.execute_input":"2021-07-09T12:51:55.259182Z","iopub.status.idle":"2021-07-09T12:51:56.438633Z","shell.execute_reply.started":"2021-07-09T12:51:55.259132Z","shell.execute_reply":"2021-07-09T12:51:56.437465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lis = [\n    train[train['sentiment'] == 'neutral']['lemma_str_text'],\n    train[train['sentiment'] == 'positive']['lemma_str_text'],\n    train[train['sentiment'] == 'negative']['lemma_str_text']\n]","metadata":{"execution":{"iopub.status.busy":"2021-07-09T12:51:56.759133Z","iopub.execute_input":"2021-07-09T12:51:56.759537Z","iopub.status.idle":"2021-07-09T12:51:56.797377Z","shell.execute_reply.started":"2021-07-09T12:51:56.759504Z","shell.execute_reply":"2021-07-09T12:51:56.796194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #008d62\"><b>Insights</b></p>\n\n><div class=\"alert success-alert\" role=\"alert\">‚úî <b>In Word counts, train and test show skwness.</b></div>","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(1, 2, figsize=(18, 8))\naxes = axes.flatten()\n\nfor i, j in zip(lis, axes):\n    try:\n        new = i.str.split()\n        new = new.values.tolist()\n        corpus = [word.lower() for i in new for word in i]\n        dic = defaultdict(int)\n        for word in corpus:\n            if word in stop:\n                dic[word] += 1\n\n        top = sorted(dic.items(), key=lambda x: x[1], reverse=True)[:15]\n        x, y = zip(*top)\n        df = pd.DataFrame([x, y]).T\n        df = df.rename(columns={0: 'Stopword', 1: 'Count'})\n        sns.barplot(x='Count', y='Stopword', data=df, palette='plasma', ax=j)\n        plt.tight_layout()\n    except:\n        plt.close()\n        print('No stopwords left in texts.')\n        break","metadata":{"execution":{"iopub.status.busy":"2021-07-09T12:51:57.696898Z","iopub.execute_input":"2021-07-09T12:51:57.697277Z","iopub.status.idle":"2021-07-09T12:51:57.799771Z","shell.execute_reply.started":"2021-07-09T12:51:57.697242Z","shell.execute_reply":"2021-07-09T12:51:57.798253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <p style=\"background-color:#008d62 ; font-family:Comic Sans MS; font-size:150%; text-align:center\"> üçÄ <b>N-gram Analysis</b> üçÄ","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #008d62\"><b>‚úÖ Most Common Words </b>","metadata":{}},{"cell_type":"code","source":"# Displaying most common words in train data.\n\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 8))\naxes = axes.flatten()\n\nfor i, j in zip(lis, axes):\n\n    new = i.str.split()\n    new = new.values.tolist()\n    corpus = [word for i in new for word in i]\n\n    counter = Counter(corpus)\n    most = counter.most_common()\n    x, y = [], []\n    for word, count in most[:20]:\n        if (word not in stop):\n            x.append(word)\n            y.append(count)\n\n    sns.barplot(x=y, y=x, palette= Greens_palette[::-1], ax=j)\naxes[0].set_title('neutral in Tweets')\naxes[1].set_title('positive in Tweets')\naxes[2].set_title('negative in Tweets')\n\naxes[0].set_xlabel('Count')\naxes[0].set_ylabel('Word')\naxes[1].set_xlabel('Count')\naxes[1].set_ylabel('Word')\naxes[2].set_xlabel('Count')\naxes[2].set_ylabel('Word')\n\nfig.suptitle('Most Common Unigrams TOP 20 in train data', fontsize=24, va='baseline')\nplt.tight_layout()\n\n\n\n# Displaying most common words in test data.\n\nlis_test = [\n    test[test['sentiment'] == 'neutral']['lemma_str_text'],\n    test[test['sentiment'] == 'positive']['lemma_str_text'],\n    test[test['sentiment'] == 'negative']['lemma_str_text']\n]\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 8))\naxes = axes.flatten()\n\nfor i, j in zip(lis_test, axes):\n\n    new = i.str.split()\n    new = new.values.tolist()\n    corpus = [word for i in new for word in i]\n\n    counter = Counter(corpus)\n    most = counter.most_common()\n    x, y = [], []\n    for word, count in most[:20]:\n        if (word not in stop):\n            x.append(word)\n            y.append(count)\n\n    sns.barplot(x=y, y=x, palette= Greens_palette[::-1], ax=j)\naxes[0].set_title('neutral in Tweets')\naxes[1].set_title('positive in Tweets')\naxes[2].set_title('negative in Tweets')\n\naxes[0].set_xlabel('Count')\naxes[0].set_ylabel('Word')\naxes[1].set_xlabel('Count')\naxes[1].set_ylabel('Word')\naxes[2].set_xlabel('Count')\naxes[2].set_ylabel('Word')\n\nfig.suptitle('Most Common Unigrams TOP 20 in test data', fontsize=24, va='baseline')\nplt.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2021-07-09T12:53:13.540823Z","iopub.execute_input":"2021-07-09T12:53:13.541244Z","iopub.status.idle":"2021-07-09T12:53:16.455586Z","shell.execute_reply.started":"2021-07-09T12:53:13.54121Z","shell.execute_reply":"2021-07-09T12:53:16.454169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #008d62\"><b>Insights</b></p>\n\n><div class=\"alert success-alert\" role=\"alert\">‚úî <b>Twenty Most Common Words appear similar in train and test.</b></div>","metadata":{}},{"cell_type":"code","source":"def ngrams(n, title):\n    \"\"\"A Function to plot most common ngrams\"\"\"\n    fig, axes = plt.subplots(1, 3, figsize=(18, 8))\n    axes = axes.flatten()\n    for i, j in zip(lis, axes):\n\n        new = i.str.split()\n        new = new.values.tolist()\n        corpus = [word for i in new for word in i]\n\n        def _get_top_ngram(corpus, n=None):\n            #getting top ngrams\n            vec = CountVectorizer(ngram_range=(n, n),\n                                  max_df=0.9,\n                                  stop_words='english').fit(corpus)\n            bag_of_words = vec.transform(corpus)\n            sum_words = bag_of_words.sum(axis=0)\n            words_freq = [(word, sum_words[0, idx])\n                          for word, idx in vec.vocabulary_.items()]\n            words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n            return words_freq[:15]\n\n        top_n_bigrams = _get_top_ngram(i, n)[:15]\n        x, y = map(list, zip(*top_n_bigrams))\n        sns.barplot(x=y, y=x, palette= Greens_palette[::-1], ax=j)\n        axes[0].set_title('neutral in Tweets')\n        axes[1].set_title('positive in Tweets')\n        axes[2].set_title('negative in Tweets')\n        \n        axes[0].set_xlabel('Count')\n        axes[0].set_ylabel('Words')\n        axes[1].set_xlabel('Count')\n        axes[1].set_ylabel('Words')\n        axes[2].set_xlabel('Count')\n        axes[2].set_ylabel('Words')\n        fig.suptitle(title, fontsize=24, va='baseline')\n        plt.tight_layout()\n        \n        \ndef ngrams_test(n, title):\n    \"\"\"A Function to plot most common ngrams\"\"\"\n    fig, axes = plt.subplots(1, 3, figsize=(18, 8))\n    axes = axes.flatten()\n    for i, j in zip(lis_test, axes):\n\n        new = i.str.split()\n        new = new.values.tolist()\n        corpus = [word for i in new for word in i]\n\n        def _get_top_ngram(corpus, n=None):\n            #getting top ngrams\n            vec = CountVectorizer(ngram_range=(n, n),\n                                  max_df=0.9,\n                                  stop_words='english').fit(corpus)\n            bag_of_words = vec.transform(corpus)\n            sum_words = bag_of_words.sum(axis=0)\n            words_freq = [(word, sum_words[0, idx])\n                          for word, idx in vec.vocabulary_.items()]\n            words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n            return words_freq[:15]\n\n        top_n_bigrams = _get_top_ngram(i, n)[:15]\n        x, y = map(list, zip(*top_n_bigrams))\n        sns.barplot(x=y, y=x, palette= Greens_palette[::-1], ax=j)\n        axes[0].set_title('neutral in Tweets')\n        axes[1].set_title('positive in Tweets')\n        axes[2].set_title('negative in Tweets')\n        \n        axes[0].set_xlabel('Count')\n        axes[0].set_ylabel('Words')\n        axes[1].set_xlabel('Count')\n        axes[1].set_ylabel('Words')\n        axes[2].set_xlabel('Count')\n        axes[2].set_ylabel('Words')\n        fig.suptitle(title, fontsize=24, va='baseline')\n        plt.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2021-07-09T12:53:19.372812Z","iopub.execute_input":"2021-07-09T12:53:19.373291Z","iopub.status.idle":"2021-07-09T12:53:19.395859Z","shell.execute_reply.started":"2021-07-09T12:53:19.373257Z","shell.execute_reply":"2021-07-09T12:53:19.393502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #008d62\"><b>‚úÖ Most Common Bigrams </b>","metadata":{}},{"cell_type":"code","source":"ngrams(2, 'Most Common Bigrams in train')\nngrams_test(2, 'Most Common Bigrams in test')","metadata":{"execution":{"iopub.status.busy":"2021-07-09T12:53:23.384008Z","iopub.execute_input":"2021-07-09T12:53:23.384378Z","iopub.status.idle":"2021-07-09T12:53:28.675922Z","shell.execute_reply.started":"2021-07-09T12:53:23.384347Z","shell.execute_reply":"2021-07-09T12:53:28.674894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #008d62\"><b>‚úÖ Most Common Trigram </b>","metadata":{}},{"cell_type":"code","source":"ngrams(3, 'Most Common Trigrams in train')\nngrams_test(3, 'Most Common Bigrams in test')","metadata":{"execution":{"iopub.status.busy":"2021-07-09T12:53:28.677999Z","iopub.execute_input":"2021-07-09T12:53:28.678425Z","iopub.status.idle":"2021-07-09T12:53:33.231846Z","shell.execute_reply.started":"2021-07-09T12:53:28.678382Z","shell.execute_reply":"2021-07-09T12:53:33.23076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #008d62\"><b>Insights</b></p>\n\n><div class=\"alert success-alert\" role=\"alert\">‚úî <b>In Bigrams and Trigrams, too, train and test had similar results.</b></div>","metadata":{}},{"cell_type":"markdown","source":"# <p style=\"background-color:#008d62 ; font-family:Comic Sans MS; font-size:150%; text-align:center\"> üçÄ <b>Word Cloud</b> üçÄ","metadata":{}},{"cell_type":"code","source":"mask = np.array(Image.open('../input/masksforwordclouds/twitter_mask3.jpg'))","metadata":{"execution":{"iopub.status.busy":"2021-07-09T12:53:36.567905Z","iopub.execute_input":"2021-07-09T12:53:36.568298Z","iopub.status.idle":"2021-07-09T12:53:36.596723Z","shell.execute_reply.started":"2021-07-09T12:53:36.568267Z","shell.execute_reply":"2021-07-09T12:53:36.595695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(1,3, figsize=(24,12))\nsentiment_list = np.unique(train['sentiment'])\n\nfor i, sentiment in zip(range(3), sentiment_list):\n    wc = WordCloud(background_color=\"white\", max_words = 2000, width = 1600, height = 800, mask=mask, colormap=\"Greens\").generate(\" \".join(train[train['sentiment']==sentiment]['lemma_str_text']))\n    \n    axes[i].text(0.5,1, \"{} text\".format(sentiment), fontweight=\"bold\", fontfamily='serif', fontsize=17)\n    axes[i].patch.set_alpha(0)\n    axes[i].axis('off')\n    axes[i].imshow(wc)\n\nfig.text(0.3,0.8,\"WordCloud by sentiment per selected text in train Tweets\", fontweight=\"bold\", fontfamily='serif', fontsize=22)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-09T12:53:37.293997Z","iopub.execute_input":"2021-07-09T12:53:37.294406Z","iopub.status.idle":"2021-07-09T12:53:44.991833Z","shell.execute_reply.started":"2021-07-09T12:53:37.294373Z","shell.execute_reply":"2021-07-09T12:53:44.990756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(1,3, figsize=(24,12))\nsentiment_list = np.unique(test['sentiment'])\n\nfor i, sentiment in zip(range(3), sentiment_list):\n    wc = WordCloud(background_color=\"white\", max_words = 2000, width = 1600, height = 800, mask=mask, colormap=\"Greens\").generate(\" \".join(test[test['sentiment']==sentiment]['lemma_str_text']))\n    \n    axes[i].text(0.5,1, \"{} text\".format(sentiment), fontweight=\"bold\", fontfamily='serif', fontsize=17)\n    axes[i].patch.set_alpha(0)\n    axes[i].axis('off')\n    axes[i].imshow(wc)\n\nfig.text(0.3,0.8,\"WordCloud by sentiment per selected text in test Tweets\", fontweight=\"bold\", fontfamily='serif', fontsize=22)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-09T12:53:44.993776Z","iopub.execute_input":"2021-07-09T12:53:44.994532Z","iopub.status.idle":"2021-07-09T12:53:50.742771Z","shell.execute_reply.started":"2021-07-09T12:53:44.99447Z","shell.execute_reply":"2021-07-09T12:53:50.741465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #008d62\"><b>Insights</b></p>\n\n><div class=\"alert success-alert\" role=\"alert\">‚úî <b>Word Cloud provides pretentious information.</b><br><br> ‚úî <b>We see the need for the elimination of words in train and test that negatively affect the analysis such as 'im', 'u'.</b><br></div>","metadata":{}},{"cell_type":"markdown","source":"# <p style=\"background-color:#008d62 ; font-family:Comic Sans MS; font-size:150%; text-align:center\"> üçÄ <b>Modelling using RoBERTa</b> üçÄ","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #008d62\"><b>‚úÖ What is RoBERTa ? </b>\n    \n><div class=\"alert success-alert\" role=\"alert\">‚úî <b>By adjusting the hyperparameters and learning data sizes of BERT, we are a model that shows the performance of post-BERT as well as that of post-BERT, which is comparable to that of post-BERT, or much better. In particular, RoBERTa demonstrates that by rebuilding BERT like this, the masked language model alone is sufficiently capable of competing with autoregressive language modeling such as XLNet.</b></div>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #008d62\"><b>‚úÖ What is the difference between BERT and RoBERTa? </b>\n    \n><div class=\"alert success-alert\" role=\"alert\">‚úî <b>Learning with longer, larger batch with more data</b><br><br>‚úî <b>Remove next sentence prediction objective</b><br><br>‚úî <b>Learning with longer sequences</b><br><br>‚úî <b>Change masking dynamically</b></div>\n\n<center><img src=\"https://vanche.github.io/assets/images/roberta_spanbert/roberta.png\"/ width=\"700\" height=\"500\" ></center>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #008d62\"><b>‚úÖ Feature Selection </b>\n    \n><div class=\"alert success-alert\" role=\"alert\">‚úî <b>Our EDA work was meaningful enough, but we decided to use existing 'text', 'selected_text' and 'sentiment' to find the output we wanted in the competition.</b></div>","metadata":{}},{"cell_type":"code","source":"# Train\ntrain_df = train[['textID','text','selected_text','sentiment']]\ntrain_df['text'] = train_df['text'].astype(str)\ntrain_df['selected_text'] = train_df['selected_text'].astype(str)\n\n# Test\ntest_df = test[['textID','text','sentiment']]\ntest_df['text'] = test_df['text'].astype(str)","metadata":{"execution":{"iopub.status.busy":"2021-07-09T12:54:00.270398Z","iopub.execute_input":"2021-07-09T12:54:00.270857Z","iopub.status.idle":"2021-07-09T12:54:00.302063Z","shell.execute_reply.started":"2021-07-09T12:54:00.270823Z","shell.execute_reply":"2021-07-09T12:54:00.300913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head(3)","metadata":{"execution":{"iopub.status.busy":"2021-07-09T12:54:01.072643Z","iopub.execute_input":"2021-07-09T12:54:01.073076Z","iopub.status.idle":"2021-07-09T12:54:01.086396Z","shell.execute_reply.started":"2021-07-09T12:54:01.073044Z","shell.execute_reply":"2021-07-09T12:54:01.085178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head(3)","metadata":{"execution":{"iopub.status.busy":"2021-07-09T12:54:01.582951Z","iopub.execute_input":"2021-07-09T12:54:01.583323Z","iopub.status.idle":"2021-07-09T12:54:01.598438Z","shell.execute_reply.started":"2021-07-09T12:54:01.58329Z","shell.execute_reply":"2021-07-09T12:54:01.596892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #008d62\"><b>‚úÖ Seed Setting </b>","metadata":{}},{"cell_type":"code","source":"if torch.cuda.is_available():    \n\n    # Tell PyTorch to use the GPU.  \n    \n    device = torch.device('cuda')    \n\n\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\n\n# If not...\n\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device('cpu')","metadata":{"execution":{"iopub.status.busy":"2021-07-09T12:54:02.732885Z","iopub.execute_input":"2021-07-09T12:54:02.733312Z","iopub.status.idle":"2021-07-09T12:54:02.80394Z","shell.execute_reply.started":"2021-07-09T12:54:02.733264Z","shell.execute_reply":"2021-07-09T12:54:02.802684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_seed(seed):\n    # Set random seed for reproducibility\n    \n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    \n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = True\n        \nseed = 777\nset_seed(seed)","metadata":{"execution":{"iopub.status.busy":"2021-07-09T12:54:03.272253Z","iopub.execute_input":"2021-07-09T12:54:03.272641Z","iopub.status.idle":"2021-07-09T12:54:03.280523Z","shell.execute_reply.started":"2021-07-09T12:54:03.272609Z","shell.execute_reply":"2021-07-09T12:54:03.279037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #008d62\"><b>‚úÖ Data Loader </b>","metadata":{}},{"cell_type":"code","source":"MAX_LENGTH = 100\nTRAIN_BATCH_SIZE = 32\nTEST_BATCH_SIZE = 16\nEPOCHS = 10","metadata":{"execution":{"iopub.status.busy":"2021-07-09T12:54:04.358957Z","iopub.execute_input":"2021-07-09T12:54:04.359344Z","iopub.status.idle":"2021-07-09T12:54:04.365142Z","shell.execute_reply.started":"2021-07-09T12:54:04.359311Z","shell.execute_reply":"2021-07-09T12:54:04.363477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PATH = \"../input/roberta-base\"\n\ntokenizer = ByteLevelBPETokenizer(\n            vocab = f'{PATH}/vocab.json', \n            merges= f'{PATH}/merges.txt', \n            add_prefix_space = True,\n            lowercase=True)\n\ntokenizer.enable_truncation(max_length=512)\n\n\nclass TextDataset(Dataset):\n    \n    def __init__(self, df, tokenizer, max_length):\n        #data loading\n        self.df = df\n        self.selected_text = \"selected_text\" in df\n        self.tokenizer = tokenizer\n        self.max_length = MAX_LENGTH\n        \n    def __len__(self):\n        #len(dataset) i.e., the total number of samples\n        return len(self.df)\n\n    \n    def get_data(self, row):\n        #processing the data\n        text = \" \"+\" \".join(row.text.lower().split()) # clean the text\n        encoded_input = self.tokenizer.encode(text) # the sentence to be encoded\n        \n        sentiment_id = {\n                'positive': 1313,\n                'negative': 2430,\n                'neutral': 7974\n            }  # stating the ids of the sentiment values \n        \n        #print ([list((i, encoded_input[i])) for i in range(len(encoded_input))])\n\n        \n        input_ids = [101] + [sentiment_id[row.sentiment]] + [102] + encoded_input.ids + [102]\n                 \n        #ID offsets       \n        offsets = [(0, 0)] * 3 + encoded_input.offsets + [(0, 0)]  # since first 3 are [CLS] ...sentiment tokens... [SEP]\n        \n        \n        pad_len = self.max_length - len(input_ids)    \n        if pad_len > 0:\n            input_ids += ([0] * pad_len)\n            offsets += ([(0, 0)] * pad_len)\n                       \n        input_ids = torch.tensor(input_ids, dtype=torch.long)\n        \n        # The mask has 1 for real tokens and 0 for padding tokens. Only real tokens are attended to.\n        masks = torch.where(input_ids != 0, torch.tensor(1), torch.tensor(0))  \n        \n        masks = torch.tensor(masks, dtype=torch.long)\n        offsets = torch.tensor(offsets, dtype=torch.long)\n                \n        return input_ids, masks, text, offsets\n    \n    def get_target_ids(self, row, text, offsets):\n        # preparing data only for the training\n        selected_text = \" \" + \" \".join(row.selected_text.lower().split())\n\n        string_len = len(selected_text) - 1\n        \n        idx0 = None\n        idx1 = None\n            \n        for ind in (position for position, line in enumerate(text) if line == selected_text[1]):\n            if \" \" + text[ind: ind+string_len] == selected_text:\n                idx0 = ind\n                idx1 = ind + string_len - 1\n                break\n                \n        char_targets = [0] * len(text)\n        if idx0 != None and idx1 != None:\n            for ct in range(idx0, idx1 + 1):\n                char_targets[ct] = 1\n                \n        # Start and end tokens\n        target_idx = []\n        for k, (offset1, offset2) in enumerate(offsets):\n            if sum(char_targets[offset1: offset2]) > 0:\n                try:\n                    target_idx.append(k)\n                except:\n                    continue\n\n        targets_start = target_idx[0]\n        targets_end = target_idx[-1]\n                \n        return selected_text, targets_start, targets_end\n    \n   \n    \n    def __getitem__(self, index): \n        \n        # addressing each row by its index\n        # dataset[index] i.e., generates one sample of data\n        data = {}\n        row = self.df.iloc[index]\n        \n        ids, masks, text, offsets = self.get_data(row)\n        data['ids'] = ids\n        data['masks'] = masks\n        data['text'] = text\n        data['offsets'] = offsets\n        data['sentiment'] = row.sentiment\n        \n        if self.selected_text:   \n            # checking if selected text exists\n            # This part only exists in the training\n            selected_text,start_index, end_index = self.get_target_ids(row, text, offsets)\n            data['start_index'] = start_index\n            data['end_index'] = end_index\n            data['selected_text'] = selected_text\n                \n        return data","metadata":{"execution":{"iopub.status.busy":"2021-07-09T12:54:04.840579Z","iopub.execute_input":"2021-07-09T12:54:04.840981Z","iopub.status.idle":"2021-07-09T12:54:05.033703Z","shell.execute_reply.started":"2021-07-09T12:54:04.840934Z","shell.execute_reply":"2021-07-09T12:54:05.03226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"e = TextDataset(train_df, tokenizer, MAX_LENGTH)\ne[1]","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-07-09T12:54:05.38465Z","iopub.execute_input":"2021-07-09T12:54:05.385006Z","iopub.status.idle":"2021-07-09T12:54:05.455212Z","shell.execute_reply.started":"2021-07-09T12:54:05.384974Z","shell.execute_reply":"2021-07-09T12:54:05.454174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #008d62\"><b>‚úÖ Modelling </b>","metadata":{}},{"cell_type":"code","source":"class TextModel(nn.Module):\n     \n    def __init__(self):\n        super(TextModel, self).__init__()\n        \n        # RoBERTa encoder \n        config = RobertaConfig.from_pretrained(\n            '../input/roberta-base/config.json', output_hidden_states=True)    \n        self.roberta = RobertaModel.from_pretrained(\n            '../input/roberta-base/pytorch_model.bin', config=config)\n\n        for param in self.roberta.parameters():\n            param.requires_grad = True\n    \n        self.drop0 = nn.Dropout(0.5)\n        self.l0 = nn.Linear(config.hidden_size * 2,config.hidden_size) \n        # Multiplied by 2 since the forward pass concatenates the last two hidden representation layers\n        \n        self.drop1 = nn.Dropout(config.hidden_dropout_prob)\n        self.l1 = nn.Linear(config.hidden_size, 2) \n        # The output will have two dimensions- start and end logits\n        \n        torch.nn.init.normal_(self.l0.weight, std=0.02)\n        torch.nn.init.normal_(self.l0.bias, 0)\n        \n    def forward(self, ids, masks): \n        # token_type_ids\n        # Return the hidden states from the RoBERTa backbone\n        # Type: torch tensor\n        last_hidden_state, pooled_output, hidden_states = self.roberta(input_ids=ids, attention_mask=masks, return_dict=False)\n        \n        # input_ids.shape and attention_mask.shape both will be of the size (batch size x seq length)\n        # print(last_hidden_state.shape) : torch.Size([24, 100, 768])\n        # But why 768? \n        # This is the number of hidden units in the feedforward-networks. We can verify that by checking the config.\n        # Concatenate the last two hidden states\n        out = torch.cat((hidden_states[-1], hidden_states[-2]), dim=-1)   \n        # out = torch.mean(out, 0) # take the mean along axis 0\n        \n        # adding dropouts and linear layers\n        out = self.drop0(out)\n        out = F.relu(self.l0(out))\n        out = self.drop1(out) \n        out = self.l1(out) \n        \n        # splitting the tensor into two logits \n        start_logits, end_logits = out.split(1, dim=-1)\n        \n        # dimension along which to split the tensor.\n        # Return a tensor with all the dimensions of input of size 1 removed, for both the logits.\n        start_logits = start_logits.squeeze()  \n        \n        # Squeezing a tensor removes the dimensions or axes that have a length of one\n        end_logits = end_logits.squeeze() \n        \n        return start_logits, end_logits\n\n# Training and validation dataloaders\ndef train_val_dataloaders(df, train_idx, val_idx, batch_size):\n    train_df = df.iloc[train_idx]\n    val_df = df.iloc[val_idx]\n\n    train_loader = torch.utils.data.DataLoader(\n        TextDataset(train_df, tokenizer, MAX_LENGTH), \n        batch_size=batch_size, \n        shuffle=True, \n        num_workers=0,   # to avoid multi-process, keep it at 0\n        drop_last=True)\n\n    val_loader = torch.utils.data.DataLoader(\n        TextDataset(val_df, tokenizer, MAX_LENGTH),\n        batch_size=batch_size, \n        shuffle=False, \n        num_workers=0)\n\n    dataloaders_dict = {\"train\": train_loader, \"val\": val_loader}\n\n    return dataloaders_dict\n\n# Test dataloader\ndef test_loader(df, batch_size=TEST_BATCH_SIZE):\n    loader = torch.utils.data.DataLoader(\n        TextDataset(test_df, tokenizer, MAX_LENGTH), \n        batch_size=batch_size, \n        shuffle=False, \n        num_workers=0)    \n    return loader","metadata":{"execution":{"iopub.status.busy":"2021-07-09T12:54:06.034956Z","iopub.execute_input":"2021-07-09T12:54:06.035382Z","iopub.status.idle":"2021-07-09T12:54:06.051613Z","shell.execute_reply.started":"2021-07-09T12:54:06.035351Z","shell.execute_reply":"2021-07-09T12:54:06.050009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #008d62\"><b>‚úÖ Loss Function </b>","metadata":{}},{"cell_type":"code","source":"def loss_function(start_logits, end_logits, start_positions, end_positions):\n    ce_loss = nn.CrossEntropyLoss()\n    start_loss = ce_loss(start_logits, start_positions)\n    end_loss = ce_loss(end_logits, end_positions)    \n    total_loss = start_loss + end_loss\n    return total_loss","metadata":{"execution":{"iopub.status.busy":"2021-07-09T12:54:06.793023Z","iopub.execute_input":"2021-07-09T12:54:06.793381Z","iopub.status.idle":"2021-07-09T12:54:06.799259Z","shell.execute_reply.started":"2021-07-09T12:54:06.793349Z","shell.execute_reply":"2021-07-09T12:54:06.797968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #008d62\"><b>‚úÖ Evaluation Function</b>","metadata":{}},{"cell_type":"code","source":"def get_selected_text(text, start_idx, end_idx, offsets):\n    selected_text = \"\"\n    for ix in range(start_idx, end_idx + 1):\n        selected_text += text[offsets[ix][0]: offsets[ix][1]]\n        if (ix + 1) < len(offsets) and offsets[ix][1] < offsets[ix + 1][0]:\n            selected_text += \" \"\n    return selected_text\n\ndef jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))\n\n\ndef find_jaccard_score(text, selected_text, sentiment, offsets, start_logits, end_logits): #start_idx, end_idx\n    start_pred = np.argmax(start_logits) # Predicted start index using argmax\n    end_pred = np.argmax(end_logits) # Predicted end index using argmax\n    if (end_pred <= start_pred) or sentiment == 'neutral' or len(text.split()) < 2:\n        enc = tokenizer.encode(text)\n        prediction = tokenizer.decode(enc.ids[start_pred-1:end_pred])   \n    else:\n        prediction = get_selected_text(text, start_pred, end_pred, offsets)\n    true = selected_text\n    \n    return jaccard(true, prediction), prediction","metadata":{"execution":{"iopub.status.busy":"2021-07-09T12:54:07.403608Z","iopub.execute_input":"2021-07-09T12:54:07.404039Z","iopub.status.idle":"2021-07-09T12:54:07.415254Z","shell.execute_reply.started":"2021-07-09T12:54:07.404008Z","shell.execute_reply":"2021-07-09T12:54:07.413746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #008d62\"><b>‚úÖ Training Function</b>","metadata":{}},{"cell_type":"code","source":"train_loss = []\nval_loss = []\njac_train = []\njac_val = []\n\ndef TrainModel(model, dataloaders_dict, optimizer, num_epochs, scheduler, device, filename): \n\n    # Set device as `cuda` (GPU)\n    model.to(device)\n    \n    for epoch in range(num_epochs):\n        for key in ['train', 'val']:\n            if key == 'train':\n                model.train()\n                dataloaders = dataloaders_dict['train']\n            else:\n                model.eval()\n                dataloaders = dataloaders_dict['val']\n\n            epoch_loss = 0.0\n            epoch_jaccard = 0.0\n            \n            # Set tqdm to add loading screen and set the length\n            loader = tqdm(dataloaders, total=len(dataloaders))\n            #print(len(dataloaders))\n                        \n            # loop over the data iterator, and feed the inputs to the network\n            # Train the model on each batch\n            for (idx, data) in enumerate(loader):\n                ids = data['ids']\n                masks = data['masks']\n                text = data['text']\n                offsets = data['offsets'].numpy()\n                start_idx = data['start_index']\n                end_idx = data['end_index']\n                sentiment = data['sentiment']\n\n                model.zero_grad()\n                optimizer.zero_grad()\n                \n                ids = ids.to(device, dtype=torch.long)\n                masks = masks.to(device, dtype=torch.long)\n                start_idx = start_idx.to(device, dtype=torch.long)\n                end_idx = end_idx.to(device, dtype=torch.long)\n\n                with torch.set_grad_enabled(key == 'train'): \n\n                    start_logits, end_logits = model(ids, masks) \n                    \n                    loss = loss_function(start_logits, end_logits, start_idx, end_idx)\n                    \n                    if key == 'train':\n                        if idx != 0: \n                            loss.backward() # Perform a backward pass to calculate the gradients\n                        optimizer.step() # Update parameters and take a step using the computed gradient\n                        scheduler.step() # Update learning rate schedule                        \n                        \n                        # Clip the norm of the gradients to 1.0.\n                        # This is to help prevent the \"exploding gradients\" problem.\n                        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  \n                        \n                    epoch_loss += loss.item() * len(ids)\n                    \n                    # Move logits to CPU\n                    # detaching these outputs so that the backward passes stop at this point\n                    start_idx = start_idx.cpu().detach().numpy()\n                    end_idx = end_idx.cpu().detach().numpy()\n                    start_logits = torch.softmax(start_logits, dim=1).cpu().detach().numpy()\n                    end_logits = torch.softmax(end_logits, dim=1).cpu().detach().numpy()\n                    \n                    selected_text = data['selected_text']\n                    \n                    filtered_sentences = []\n                    for i, t_data in enumerate(text):                 \n                        jaccard_score, filtered_output = find_jaccard_score(\n                            t_data,\n                            selected_text[i],\n                            sentiment[i],\n                            offsets[i],\n                            start_logits[i], \n                            end_logits[i])\n                        epoch_jaccard += jaccard_score\n                        filtered_sentences.append(filtered_output)\n            \n            # Calculate the average loss over the training data\n            epoch_loss = epoch_loss / len(dataloaders.dataset)\n            # Calculate the average jaccard score over the training data\n            epoch_jaccard = epoch_jaccard / len(dataloaders.dataset)\n            \n            print('Epoch {}/{} | {:^5} | Loss: {:.4f} | Jaccard: {:.4f}'.format(\n                epoch + 1, num_epochs, key, epoch_loss, epoch_jaccard))\n            \n            # Store the loss value for plotting the learning curve.\n            if key == 'train':\n                train_loss.append(epoch_loss)\n                jac_train.append(epoch_jaccard)\n                \n            else:\n                val_loss.append(epoch_loss)\n                jac_val.append(epoch_jaccard)\n    \n    torch.save(model.state_dict(), filename)","metadata":{"execution":{"iopub.status.busy":"2021-07-09T12:54:08.192649Z","iopub.execute_input":"2021-07-09T12:54:08.193035Z","iopub.status.idle":"2021-07-09T12:54:08.213283Z","shell.execute_reply.started":"2021-07-09T12:54:08.193005Z","shell.execute_reply":"2021-07-09T12:54:08.211744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #008d62\"><b>‚úÖ Training</b>","metadata":{}},{"cell_type":"code","source":"skf = StratifiedKFold(n_splits=3,shuffle=True,random_state=seed)","metadata":{"execution":{"iopub.status.busy":"2021-07-09T12:54:08.938092Z","iopub.execute_input":"2021-07-09T12:54:08.93851Z","iopub.status.idle":"2021-07-09T12:54:08.943954Z","shell.execute_reply.started":"2021-07-09T12:54:08.938454Z","shell.execute_reply":"2021-07-09T12:54:08.942619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for fold, (idxTrain, idxVal) in enumerate(skf.split(train_df, train_df.sentiment), start=1):\n    \n    print('#'*10)\n    print('# FOLD %i #'%(fold))\n    print('#'*10)\n    \n    model = TextModel()\n    optimizer = AdamW(model.parameters(), lr=3e-5, weight_decay=0.01, correct_bias=False)\n    dataloaders_dict = train_val_dataloaders(train_df, idxTrain, idxVal, batch_size=TRAIN_BATCH_SIZE)\n    num_training_steps = int(len(train_df) / EPOCHS * TRAIN_BATCH_SIZE)\n    \n    # default #use a linear scheduler with no warmup steps\n    scheduler = get_linear_schedule_with_warmup(\n                  optimizer,\n                  num_warmup_steps=0, \n                  num_training_steps=num_training_steps\n                )    \n    TrainModel(\n        model, \n        dataloaders_dict, \n        optimizer, \n        EPOCHS,\n        scheduler,\n        device,\n        f'bert_fold{fold}.pth')","metadata":{"execution":{"iopub.status.busy":"2021-07-09T12:54:09.228275Z","iopub.execute_input":"2021-07-09T12:54:09.228695Z","iopub.status.idle":"2021-07-09T15:32:14.350375Z","shell.execute_reply.started":"2021-07-09T12:54:09.228639Z","shell.execute_reply":"2021-07-09T15:32:14.349108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: Comic Sans MS; line-height: 1;font-size: 20px; letter-spacing: 1px;  color: #008d62\"><b>‚úÖ Inference</b>","metadata":{}},{"cell_type":"code","source":"%%time\n\nt_loader = test_loader(test_df)\npredictions = []\nmodels = []\nfor fold in range(skf.n_splits):\n    model = TextModel()\n    model.to(device)\n    model.load_state_dict(torch.load(f'./bert_fold{fold+1}.pth'))\n    model.eval()\n    models.append(model)\n\nloader = tqdm(t_loader, total=len(t_loader))\nfor (idx, data) in enumerate(loader):\n    ids = data['ids'].to(device)\n    masks = data['masks'].to(device)\n    text = data['text']\n    offsets = data['offsets'].numpy()\n\n    start_logits = []\n    end_logits = []\n    for model in models:\n        with torch.no_grad():\n            output = model(ids, masks)\n            start_logits.append(torch.softmax(output[0], dim=1).cpu().detach().numpy())\n            end_logits.append(torch.softmax(output[1], dim=1).cpu().detach().numpy())\n\n    start_logits = np.mean(start_logits, axis=0)\n    end_logits = np.mean(end_logits, axis=0)\n    \n    for i, t_data in enumerate(text):\n        start_pred = np.argmax(start_logits[i])\n        end_pred = np.argmax(end_logits[i])\n        if start_pred >= end_pred:\n            enc = tokenizer.encode(t_data)\n            prediction = tokenizer.decode(enc.ids[start_pred-1:end_pred])\n        else:\n            prediction = get_selected_text(t_data, start_pred, end_pred, offsets[i])\n        predictions.append(prediction)","metadata":{"execution":{"iopub.status.busy":"2021-07-09T15:32:14.352352Z","iopub.execute_input":"2021-07-09T15:32:14.352855Z","iopub.status.idle":"2021-07-09T15:33:04.180137Z","shell.execute_reply.started":"2021-07-09T15:32:14.35281Z","shell.execute_reply":"2021-07-09T15:33:04.178745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <p style=\"background-color:#008d62 ; font-family:Comic Sans MS; font-size:150%; text-align:center\"> üçÄ <b>Submission</b> üçÄ","metadata":{}},{"cell_type":"code","source":"sub_df = pd.read_csv('../input/tweet-sentiment-extraction/sample_submission.csv')\nsub_df['selected_text'] = predictions\n# post-processing trick\nsub_df['selected_text'] = sub_df['selected_text'].apply(lambda x: x.replace('!!!!', '!') if len(x.split())==1 else x)\nsub_df['selected_text'] = sub_df['selected_text'].apply(lambda x: x.replace('..', '.') if len(x.split())==1 else x)\nsub_df['selected_text'] = sub_df['selected_text'].apply(lambda x: x.replace('...', '.') if len(x.split())==1 else x)\nsub_df[['textID','selected_text']].to_csv('submission.csv',index=False)\npd.set_option('max_colwidth', 60)\n\nsub_df.sample(10)","metadata":{"execution":{"iopub.status.busy":"2021-07-09T15:33:04.182639Z","iopub.execute_input":"2021-07-09T15:33:04.183135Z","iopub.status.idle":"2021-07-09T15:33:04.445235Z","shell.execute_reply.started":"2021-07-09T15:33:04.183088Z","shell.execute_reply":"2021-07-09T15:33:04.44384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <p style=\"background-color:#008d62 ; font-family:Comic Sans MS; font-size:150%; text-align:center\"> üçÄ <b>References</b> üçÄ\n    \n<div class=\"alert success-alert\"><b>\n‚úî https://www.kaggle.com/abhishek/roberta-inference-5-folds/data <br><br>\n‚úî https://www.kaggle.com/shoheiazuma/tweet-sentiment-roberta-pytorch <br><br>\n‚úî https://www.kaggle.com/aditidutta/tweet-sentiment-extraction-pytorch <br><br>\n‚úî https://brunch.co.kr/@choseunghyek/7 <br><br>\n‚úî https://vanche.github.io/spanbert_roberta/ <b>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family:Comic Sans MS; font-size:180%; color: #008d62; text-align:center;\"><b>Pls, \"UPVOTE\" if this code helped ! üëÄ</b></p>","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}