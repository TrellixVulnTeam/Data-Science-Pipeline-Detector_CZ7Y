{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nimport re\n# Tutorial about Python regular expressions: https://pymotw.com/2/re/ import string\nfrom nltk.corpus import stopwords\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport os\nimport nltk\nimport spacy\nimport random\nfrom spacy.util import compounding\nfrom spacy.util import minibatch\nimport string\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.rcParams['figure.figsize']=10,6\nplt.rcParams['axes.grid']=True\nplt.gray()\n\nuse_cuda = True\npd.set_option('display.max_columns', None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/tweet-sentiment-extraction/train.csv\") \ntest = pd.read_csv(\"/kaggle/input/tweet-sentiment-extraction/test.csv\") \nsample_submission = pd.read_csv(\"/kaggle/input/tweet-sentiment-extraction/sample_submission.csv\") ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the shape of train and test data\nprint(train.shape)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking Missing value in the training set\nprint(train.isnull().sum())\n# Checking Missing Value in the testing set\nprint(test.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cr√©ation d'une fonction permettant de calculer le total de valeurs manquantes, le pourcentage et le type de \n ## chaque colonne \ndef missing_data(data):\n    total = data.isnull().sum()\n    percent = (data.isnull().sum()/data.isnull().count()*100)\n    tt = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    types = []\n    for col in data.columns:\n        dtype = str(data[col].dtype)\n        types.append(dtype)\n    tt['Types'] = types\n    return(np.transpose(tt))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_data(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_data(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"percent_sentiment = train.groupby('sentiment').count()\npercent_sentiment['percent'] = 100*(percent_sentiment['text']/train['sentiment'].count())\npercent_sentiment.reset_index(level=0, inplace=True)\npercent_sentiment","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Droping the row with missing values\ntrain.dropna(axis = 0, how ='any',inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Positive tweet\nprint(\"Positive Tweet example :\",train[train['sentiment']=='positive']['text'].values[0])\n#negative_text\nprint(\"Negative Tweet example :\",train[train['sentiment']=='negative']['text'].values[0])\n#neutral_text\nprint(\"Neutral tweet example  :\",train[train['sentiment']=='neutral']['text'].values[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of the Sentiment Column\ntrain['sentiment'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train data \nsns.countplot(x=train['sentiment'],data=train)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['sentiment'].value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax=plt.subplots(1,2,figsize=(13,5))\ntrain['sentiment'].value_counts().plot.pie(explode=[0,0.05,0.5],autopct='%1.1f%%',ax=ax[0],shadow=True)\nax[0].set_title('sentiment')\nax[0].set_ylabel('')\nsns.countplot('sentiment',data=train,ax=ax[1])\nax[1].set_title('sentiment')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.stem import PorterStemmer, SnowballStemmer\nfrom nltk import wordnet, pos_tag\nfrom nltk.tokenize import word_tokenize\nfrom nltk.util import ngrams\nfrom nltk.corpus import stopwords, wordnet as wn\nimport re\nimport string\n\n#Cleaning data\n\ndef clean_str(chaine):\n    chaine = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", chaine)     \n    chaine = re.sub(r\"\\'s\", \" \\'s\", chaine) \n    chaine = re.sub(r\"\\'ve\", \" \\'ve\", chaine) \n    chaine = re.sub(r\"n\\'t\", \" n\\'t\", chaine) \n    chaine = re.sub(r\"\\'re\", \" \\'re\", chaine) \n    chaine = re.sub(r\"\\'d\", \" \\'d\", chaine) \n    chaine = re.sub(r\"\\'ll\", \" \\'ll\", chaine) \n    chaine = re.sub(r\",\", \" , \", chaine) \n    chaine = re.sub(r\"!\", \" ! \", chaine) \n    chaine = re.sub(r\"\\(\", \" \\( \", chaine) \n    chaine = re.sub(r\"\\)\", \" \\) \", chaine) \n    chaine = re.sub(r\"\\?\", \" \\? \", chaine) \n    chaine = re.sub(r\"\\s{2,}\", \" \", chaine)\n    chaine = chaine.lower() #convert all text in lower case\n    chaine = chaine.replace(' +', ' ') # Remove double space\n    chaine = chaine.strip() # Remove trailing space at the beginning or end\n    chaine = chaine.replace('[^a-zA-Z]', ' ' )# Everything not a alphabet character replaced with a space\n    #words =  [word for word in chaine.split() if word not in [i for i in string.punctuation]] #Remove punctuations\n    words =  [word for word in chaine.split() if word.isalpha()] #droping numbers and punctuations\n    return ' '.join(words)\n\n#Tokenization and punctuation removing and stopwords\ndef tokeniZ_stopWords(chaine):\n    chaine = word_tokenize(chaine)\n    list_stopWords = set(stopwords.words('english'))\n    words = [word for word in chaine if word not in list_stopWords]\n    return words\n\n#Stemming \nps = PorterStemmer()\nsb = SnowballStemmer('english')\n\n#Lemmatization\ndef lemat_words(tokens_list):\n    from collections import defaultdict\n    tag_map = defaultdict(lambda : wn.NOUN)\n    tag_map['J'] = wn.ADJ\n    tag_map['V'] = wn.VERB\n    tag_map['R'] = wn.ADV\n    lemma_function = WordNetLemmatizer()\n    return [lemma_function.lemmatize(token, tag_map[tag[0]]) for token, tag in pos_tag(tokens_list)]\n    #for token, tag in pos_tag(tokens_list):\n     #   lemma = lemma_function.lemmatize(token, tag_map[tag[0]])\n\n# Define Ngrams function\ndef get_ngrams(text, n ):\n    n_grams = ngrams(word_tokenize(text), n)\n    return [ ' '.join(grams) for grams in n_grams]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## text","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Cleaning the train data \ntrain['text_clean'] = train['text'].apply(clean_str)\n\n#Tokenizing and stopwords removing\ntrain['tokeniZ_stopWords_text'] = train['text_clean'].apply(tokeniZ_stopWords)\n#Words Stemming\ntrain['stemming_text'] = [[ps.stem(word) for word in words] for words in train['tokeniZ_stopWords_text'] ]\ntrain['stemming_text_for_tfidf'] = [' '.join(words) for words in train['stemming_text']] \n\n#Words lemmatization\ntrain['lemmatize_text'] = train['tokeniZ_stopWords_text'].apply(lemat_words)\ntrain['lemmatize_text_for_tfidf'] = [' '.join(x) for x in train['lemmatize_text'] ]\n\n#Calcul longueur des commentaires\ntrain['text_lenght'] = train['text'].apply(len)\n\n#Calcul du nombre de ponctuation par question\nfrom string import punctuation\ntrain['number_punctuation_text'] = train['text'].apply(\n    lambda doc: len([word for word in str(doc) if word in punctuation])) \n\n#Number of unique words in the text\ntrain['number_of_Unique_words_text'] = train['text_clean'].apply([lambda x : len(set(str(x).split()))])\n\n#Number of stopwords in the text\nlist_stopWords = set(stopwords.words('english'))\ntrain['number_of_StopWords_text'] = train['text_clean'].apply(\n    lambda x : len([w for w in x.lower().split() if w in list_stopWords ]))\n\n\n#Number of upper case words\ntrain['number_of_uppercase_text'] = train['text'].apply(\n    lambda x : len([w for w in x.split() if w.isupper()]))\n\n\n#Average length of words in the text (whithout stop words)\ntrain['average_of_wordsLength_text'] = train['text_clean'].apply(\n    lambda x : np.mean([len(w) for w in x.split()]))\n\n#Number of words in the text\ntrain['number_of_words_text'] = train['text_clean'].apply([lambda x : len(str(x).split())])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## selected_text","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#Cleaning the train data \ntrain['selected_text_clean'] = train['selected_text'].apply(clean_str)\n\n#Tokenizing and stopwords removing\ntrain['tokeniZ_stopWords_text'] = train['selected_text_clean'].apply(tokeniZ_stopWords)\n\n#Words Stemming\ntrain['stemming_selected_text'] = [[ps.stem(word) for word in words] for words in train['tokeniZ_stopWords_text'] ]\ntrain['stemming_selected_text_for_tfidf'] = [' '.join(words) for words in train['stemming_selected_text']] \n\n#Words lemmatization\ntrain['lemmatize_selected_text'] = train['tokeniZ_stopWords_text'].apply(lemat_words)\ntrain['lemmatize_selected_text_for_tfidf'] = [' '.join(x) for x in train['lemmatize_selected_text'] ]\n\n\n#Calcul longueur des commentaires\ntrain['selected_text_lenght'] = train['selected_text'].apply(len)\n\n#Calcul du nombre de ponctuation par question\nfrom string import punctuation\ntrain['number_punctuation_selected_text'] = train['selected_text'].apply(\n    lambda doc: len([word for word in str(doc) if word in punctuation])) \n\n#Number of unique words in the text\ntrain['number_of_Unique_words_selected_text'] = train['selected_text_clean'].apply([lambda x : len(set(str(x).split()))])\n\n#Number of stopwords in the text\nlist_stopWords = set(stopwords.words('english'))\ntrain['number_of_StopWords_selected_text'] = train['selected_text_clean'].apply(\n    lambda x : len([w for w in x.lower().split() if w in list_stopWords ]))\n\n\n#Number of upper case words\ntrain['number_of_uppercase_selected_text'] = train['selected_text'].apply(\n    lambda x : len([w for w in x.split() if w.isupper()]))\n\n\n#Average length of words in the text (whithout stop words)\ntrain['average_of_wordsLength_selected_text'] = train['selected_text_clean'].apply(\n    lambda x : np.mean([len(w) for w in x.split()]))\n\n#Number of words in the text\ntrain['number_of_words_selected_text'] = train['selected_text_clean'].apply([lambda x : len(str(x).split())])\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's create three separate dataframes for positive, neutral and negative sentiments. \n#This will help in analyzing the text statistics separately for separate polarities.\n\npositive = train[train['sentiment']=='positive']\nnegative = train[train['sentiment']=='negative']\nneutral = train[train['sentiment']=='neutral']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sentence length analysis\n\nfig, ax = plt.subplots(1, 3, figsize=(15, 5))\nplt.subplot(1, 3, 1)\nplt.hist(positive['text_lenght'],bins=50,color='g')\nplt.title('Positive Text Length Distribution')\nplt.xlabel('text_lenght')\nplt.ylabel('count')\n\n\nplt.subplot(1, 3, 2)\nplt.hist(negative['text_lenght'],bins=50,color='r')\nplt.title('Negative Text Length Distribution')\nplt.xlabel('text_lenght')\nplt.ylabel('count')\n\n\nplt.subplot(1, 3, 3)\nplt.hist(neutral['text_lenght'],bins=50,color='y')\nplt.title('Neutral Text Length Distribution')\nplt.xlabel('text_lenght')\nplt.ylabel('count')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#source of code : https://medium.com/@cristhianboujon/how-to-list-the-most-common-words-from-text-corpus-using-scikit-learn-dad4d0cab41d\ndef get_top_n_words(corpus, n=None):\n    \"\"\"\n    List the top n words in a vocabulary according to occurrence in a text corpus.\n    \"\"\"\n    vec = CountVectorizer(stop_words = 'english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Distribution of top unigrams\npositive_unigrams = get_top_n_words(positive['text_clean'],20)\nnegative_unigrams = get_top_n_words(negative['text_clean'],20)\nneutral_unigrams = get_top_n_words(neutral['text_clean'],20)\n\ndf1 = pd.DataFrame(positive_unigrams, columns = ['Text' , 'count'])\ndf1.groupby('Text').sum()['count'].sort_values(ascending=True).plot(kind='barh',color='g')\nplt.ylabel('Count')\nplt.title('Top 20 unigrams in positve text')\nplt.show()\n\ndf2 = pd.DataFrame(negative_unigrams, columns = ['Text' , 'count'])\ndf2.groupby('Text').sum()['count'].sort_values(ascending=True).plot(kind='barh',color='red')\nplt.title('Top 20 unigram in Negative text')\nplt.show()\n\ndf3 = pd.DataFrame(neutral_unigrams, columns = ['Text' , 'count'])\ndf3.groupby('Text').sum()['count'].sort_values(ascending=True).plot(kind='barh',color='yellow')\nplt.title('Top 20 unigram in Neutral text')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_top_n_gram(corpus,ngram_range,n=None):\n    vec = CountVectorizer(ngram_range=ngram_range,stop_words = 'english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Distribution of top Bigrams\npositive_bigrams = get_top_n_gram(positive['text_clean'],(2,2),20)\nnegative_bigrams = get_top_n_gram(negative['text_clean'],(2,2),20)\nneutral_bigrams = get_top_n_gram(neutral['text_clean'],(2,2),20)\n\ndf1 = pd.DataFrame(positive_bigrams, columns = ['Text' , 'count'])\ndf1.groupby('Text').sum()['count'].sort_values(ascending=True).plot(kind='barh',color='g')\nplt.ylabel('Count')\nplt.title('Top 20 Bigrams in positve text')\nplt.show()\n\ndf2 = pd.DataFrame(negative_bigrams, columns = ['Text' , 'count'])\ndf2.groupby('Text').sum()['count'].sort_values(ascending=True).plot(kind='barh',color='red')\nplt.title('Top 20 Bigram in Negative text')\nplt.show()\n\ndf3 = pd.DataFrame(neutral_bigrams, columns = ['Text' , 'count'])\ndf3.groupby('Text').sum()['count'].sort_values(ascending=True).plot(kind='barh',color='yellow')\nplt.title('Top 20 Bigram in Neutral text')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finding top trigram\npositive_trigrams = get_top_n_gram(positive['text_clean'],(3,3),20)\nnegative_trigrams = get_top_n_gram(negative['text_clean'],(3,3),20)\nneutral_trigrams = get_top_n_gram(neutral['text_clean'],(3,3),20)\n\ndf1 = pd.DataFrame(positive_trigrams, columns = ['Text' , 'count'])\ndf1.groupby('Text').sum()['count'].sort_values(ascending=True).plot(kind='barh',color='g')\nplt.ylabel('Count')\nplt.title('Top 20 trigrams in positve text')\nplt.show()\n\ndf2 = pd.DataFrame(negative_trigrams, columns = ['Text' , 'count'])\ndf2.groupby('Text').sum()['count'].sort_values(ascending=True).plot(kind='barh',color='red')\nplt.title('Top 20 trigram in Negative text')\nplt.show()\n\ndf3 = pd.DataFrame(neutral_trigrams, columns = ['Text' , 'count'])\ndf3.groupby('Text').sum()['count'].sort_values(ascending=True).plot(kind='barh',color='yellow')\nplt.title('Top 20 trigram in Neutral text')\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Exploring the selected_text column\n\npositive_text = train[train['sentiment'] == 'positive']['selected_text']\nnegative_text = train[train['sentiment'] == 'negative']['selected_text']\nneutral_text = train[train['sentiment'] == 'neutral']['selected_text']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Positive text\nprint(\"Positive Text example :\",positive_text.values[0])\n#negative_text\nprint(\"Negative Tweet example :\",negative_text.values[0])\n#neutral_text\nprint(\"Neutral tweet example  :\",neutral_text.values[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preprocess Selected_text\n\npositive_text_clean = positive_text.apply(clean_str)\nnegative_text_clean = negative_text.apply(clean_str)\nneutral_text_clean = neutral_text.apply(clean_str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#source of code : https://medium.com/@cristhianboujon/how-to-list-the-most-common-words-from-text-corpus-using-scikit-learn-dad4d0cab41d\ndef get_top_n_words(corpus, n=None):\n    \"\"\"\n    List the top n words in a vocabulary according to occurrence in a text corpus.\n    \"\"\"\n    vec = CountVectorizer(stop_words = 'english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_words_in_positive_text = get_top_n_words(positive_text_clean)\ntop_words_in_negative_text = get_top_n_words(negative_text_clean)\ntop_words_in_neutral_text = get_top_n_words(neutral_text_clean)\n\np1 = [x[0] for x in top_words_in_positive_text[:20]]\np2 = [x[1] for x in top_words_in_positive_text[:20]]\n\n\nn1 = [x[0] for x in top_words_in_negative_text[:20]]\nn2 = [x[1] for x in top_words_in_negative_text[:20]]\n\n\nnu1 = [x[0] for x in top_words_in_neutral_text[:20]]\nnu2 = [x[1] for x in top_words_in_neutral_text[:20]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Top positive word\nsns.barplot(x=p1,y=p2,color = 'green')\nplt.xticks(rotation=45)\nplt.title('Top 20 Positive Word')\nplt.show()\n\nsns.barplot(x=n1,y=n2,color='red')\nplt.xticks(rotation=45)\nplt.title('Top 20 Negative Word')\nplt.show()\n\nsns.barplot(x=nu1,y=nu2,color='yellow')\nplt.xticks(rotation=45)\nplt.title('Top 20 Neutral Word')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Wordclouds\n# Wordclouds to see which words contribute to which type of polarity.\n\nfrom wordcloud import WordCloud\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=[30, 15])\nwordcloud1 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(positive_text_clean))\nax1.imshow(wordcloud1)\nax1.axis('off')\nax1.set_title('Positive text',fontsize=40);\n\nwordcloud2 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(negative_text_clean))\nax2.imshow(wordcloud2)\nax2.axis('off')\nax2.set_title('Negative text',fontsize=40);\n\nwordcloud3 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(neutral_text_clean))\nax3.imshow(wordcloud3)\nax3.axis('off')\nax3.set_title('Neutral text',fontsize=40)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://www.kaggle.com/ekhtiar/unintended-eda-with-tutorial-notes\ndef generate_word_cloud(df_data, text_col):\n    # convert stop words to sets as required by the wordcloud library\n    stop_words = set(stopwords.words(\"english\"))\n    \n    data_neutral = \" \".join(df_data.loc[df_data[\"sentiment\"]==\"neutral\", text_col].map(lambda x: str(x).lower()))\n    data_positive = \" \".join(df_data.loc[df_data[\"sentiment\"]==\"positive\", text_col].map(lambda x: str(x).lower()))\n    data_negative = \" \".join(df_data.loc[df_data[\"sentiment\"]==\"negative\", text_col].map(lambda x: str(x).lower()))\n\n    wc_neutral = WordCloud(max_font_size=100, max_words=100, background_color=\"white\", stopwords=stop_words).generate(data_neutral)\n    wc_positive = WordCloud(max_font_size=100, max_words=100, background_color=\"white\", stopwords=stop_words).generate(data_positive)\n    wc_negative = WordCloud(max_font_size=100, max_words=100, background_color=\"white\", stopwords=stop_words).generate(data_negative)\n\n    # draw the two wordclouds side by side using subplot\n    fig, ax = plt.subplots(1, 3, figsize=(20, 20))\n    ax[0].set_title(\"Neutral Wordcloud\" , fontsize=10)\n    ax[0].imshow(wc_neutral, interpolation=\"bilinear\")\n    ax[0].axis(\"off\")\n    \n    ax[1].set_title(\"Positive Wordcloud\", fontsize=10)\n    ax[1].imshow(wc_positive, interpolation=\"bilinear\")\n    ax[1].axis(\"off\")\n    \n    ax[2].set_title(\"Negative Wordcloud\", fontsize=10)\n    ax[2].imshow(wc_negative, interpolation=\"bilinear\")\n    ax[2].axis(\"off\")\n    plt.show()\n    \n    return [wc_neutral, wc_positive, wc_negative]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_text_wc = generate_word_cloud(train, \"text\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sel_text_wc = generate_word_cloud(train, \"selected_text\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_text_wc = generate_word_cloud(train, \"text\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_data(train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Target variable creating ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Target'] = train['sentiment'].apply(lambda x: 2 if x == 'positive' else 1 if x == 'neutral' else 0)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"percent_target = train.groupby('Target').count()\npercent_target['percent'] = 100*(percent_target['text']/train['Target'].count())\npercent_target.reset_index(level=0, inplace=True)\npercent_target","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Features engineering for training data ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[['text_lenght', 'number_punctuation_text', 'number_of_words_text',\n       'number_of_Unique_words_text', 'number_of_StopWords_text', 'number_of_uppercase_text',\n       'average_of_wordsLength_text']].sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_var=['text_lenght', 'number_punctuation_text', 'number_of_words_text',\n       'number_of_Unique_words_text', 'number_of_StopWords_text', 'number_of_uppercase_text','average_of_wordsLength_text']\ndef var_hist_global(df,X='Target',Y=list_var, Title='Features Engineering - Histograms', KDE=False):\n    fig, ((ax1, ax2),(ax3,ax4),(ax5,ax6),(ax6,ax7)) = plt.subplots(4, 2 ,figsize=(14,16))#, sharey=True )\n    aX = [ax1, ax2,ax3,ax4,ax5,ax6,ax6,ax7]\n    for i in range(len(list_var)):   \n        sns.distplot( df[list_var[i]][df[X]== 1 ].dropna(), label=\"Neutral\" , ax= aX[i], kde= KDE , color = 'red')           \n        sns.distplot( df[list_var[i]][df[X]== 0 ].dropna(), label=\"Negative\", ax= aX[i], kde= KDE , color = \"olive\")\n        sns.distplot( df[list_var[i]][df[X]== 2 ].dropna(), label=\"Positive\", ax= aX[i], kde= KDE , color = \"black\")\n    plt.legend()\n    plt.title(Title)\n    #plt.show()\n    plt.savefig(\"Features_Engineering_Histograms\")\n    \nvar_hist_global(df=train,X='Target',Y=list_var, KDE=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate number of obs per group & median to position labels\nlist_var = ['text_lenght', 'number_of_Unique_words_text', 'number_of_StopWords_text']\ndef violin_boxplott(df,X='Target',Y=list_var, Title='Features Engineering - Box plot'): \n    fig, (ax1, ax2 ,ax3) = plt.subplots(1,3 ,figsize=(14,8))#, sharey=True )\n    medians = train.groupby(['Target'])['text_lenght', 'number_of_Unique_words_text', 'number_of_StopWords_text'].median().values\n \n    sns.boxplot( y=list_var[0],  x=X , data = df, ax= ax1 , palette=['olive','red'])\n    sns.boxplot( y=list_var[1],  x=X , data = df, ax= ax2 , palette=['olive','red'])\n    sns.boxplot( y=list_var[2],  x=X , data = df, ax= ax3 , palette=['olive','red'])\n    #plt.title(Title)\n    plt.savefig(\"Features_Engineering_Boxplot\")\nviolin_boxplott(df=train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Word2Vec with preprocessiong questions (without stopwords) \nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument\n\nd2v_training_data = []\nfor i, doc in enumerate(train['stemming_selected_text']):\n    d2v_training_data.append(TaggedDocument(words=doc,tags=[i]))\n\n# ========== learning doc embeddings with doc2vec ==========\n\n# PV stands for 'Paragraph Vector'\n# PV-DBOW (distributed bag-of-words) dm=0\n\nd2v = Doc2Vec(d2v_training_data, vector_size=300, window=10, alpha=0.1, min_alpha=1e-4, dm=0, negative=1, epochs=10, min_count=2, workers=4)\nd2v_vecs = np.zeros((len(train['stemming_selected_text']), 300))\nfor i in range(len(train['stemming_selected_text'])):\n    d2v_vecs[i,:] = d2v.docvecs[i]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Word2Vec with lemmatize words\nd2v_training_data = []\nfor i, doc in enumerate(train['lemmatize_selected_text']):\n    d2v_training_data.append(TaggedDocument(words=doc,tags=[i]))\n\n# ========== learning doc embeddings with doc2vec ==========\n\n# PV stands for 'Paragraph Vector'\n# PV-DBOW (distributed bag-of-words) dm=0\n\nd2v = Doc2Vec(d2v_training_data, vector_size=200, window=5, alpha=0.1, min_alpha=1e-4, \n              dm=0, negative=1, epochs=10, min_count=2, workers=4)\nd2v_vecs_bigram = np.zeros((len(train['lemmatize_selected_text']), 200))\nfor i in range(len(train['lemmatize_selected_text'])):\n    d2v_vecs_bigram[i,:] = d2v.docvecs[i]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Testing data ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_data(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test data \nsns.countplot(x=test['sentiment'],data=train)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"percent_sentiment = test.groupby('sentiment').count()\npercent_sentiment['percent'] = 100*(percent_sentiment['text']/train['sentiment'].count())\npercent_sentiment.reset_index(level=0, inplace=True)\npercent_sentiment","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Positive tweet\nprint(\"Positive Tweet example :\",test[test['sentiment']=='positive']['text'].values[0])\n#negative_text\nprint(\"Negative Tweet example :\",test[test['sentiment']=='negative']['text'].values[0])\n#neutral_text\nprint(\"Neutral tweet example  :\",test[test['sentiment']=='neutral']['text'].values[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of the Sentiment Column\ntest['sentiment'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Cleaning the train data \ntest['text_clean_test'] = test['text'].apply(clean_str)\n\n#Tokenizing and stopwords removing\ntest['tokeniZ_stopWords_text_test'] = test['text_clean_test'].apply(tokeniZ_stopWords)\n#Words Stemming\ntest['stemming_text_test'] = [[ps.stem(word) for word in words] for words in test['tokeniZ_stopWords_text_test'] ]\ntest['stemming_text_for_tfidf_test'] = [' '.join(words) for words in test['stemming_text_test']] \n\n#Words lemmatization\ntest['lemmatize_text_test'] = test['tokeniZ_stopWords_text_test'].apply(lemat_words)\ntest['lemmatize_text_for_tfidf_test'] = [' '.join(x) for x in test['lemmatize_text_test'] ]\n\n#Calcul longueur des commentaires\ntest['text_lenght_test'] = test['text'].apply(len)\n\n#Calcul du nombre de ponctuation par question\nfrom string import punctuation\ntest['number_punctuation_text_test'] = test['text'].apply(\n    lambda doc: len([word for word in str(doc) if word in punctuation])) \n\n#Number of unique words in the text\ntest['number_of_Unique_words_text_test'] = test['text_clean_test'].apply([lambda x : len(set(str(x).split()))])\ntest\n#Number of stopwords in the text\nlist_stopWords = set(stopwords.words('english'))\ntest['number_of_StopWords_text_test'] = test['text_clean_test'].apply(\n    lambda x : len([w for w in x.lower().split() if w in list_stopWords ]))\n\n\n#Number of upper case words\ntest['number_of_uppercase_text_test'] = test['text'].apply(\n    lambda x : len([w for w in x.split() if w.isupper()]))\n\n\n#Average length of words in the text (whithout stop words)\ntest['average_of_wordsLength_text_test'] = test['text_clean_test'].apply(\n    lambda x : np.mean([len(w) for w in x.split()]))\n\n#Number of words in the text\ntest['number_of_words_text_test'] = test['text_clean_test'].apply([lambda x : len(str(x).split())])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's create three separate dataframes for positive, neutral and negative sentiments. \n#This will help in analyzing the text statistics separately for separate polarities.\n\npositive_test = test[test['sentiment']=='positive']\nnegative_test = test[test['sentiment']=='negative']\nneutral_test = test[test['sentiment']=='neutral']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sentence length analysis\n\nfig, ax = plt.subplots(1, 3, figsize=(15, 5))\nplt.subplot(1, 3, 1)\nplt.hist(positive_test['text_lenght_test'],bins=50,color='g')\nplt.title('Positive Text Length Distribution test data')\nplt.xlabel('text_lenght_test')\nplt.ylabel('count')\n\n\nplt.subplot(1, 3, 2)\nplt.hist(negative_test['text_lenght_test'],bins=50,color='r')\nplt.title('Negative Text Length Distribution  test data')\nplt.xlabel('text_lenght_test')\nplt.ylabel('count')\n\n\nplt.subplot(1, 3, 3)\nplt.hist(neutral_test['text_lenght_test'],bins=50,color='y')\nplt.title('Neutral Text Length Distribution  test data')\nplt.xlabel('text_lenght_test')\nplt.ylabel('count')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Distribution of top unigrams\npositive_test_unigrams = get_top_n_words(positive_test['text_clean_test'],20)\nnegative_test_unigrams = get_top_n_words(negative_test['text_clean_test'],20)\nneutral_test_unigrams = get_top_n_words(neutral_test['text_clean_test'],20)\n\ndf1 = pd.DataFrame(positive_test_unigrams, columns = ['Text' , 'count'])\ndf1.groupby('Text').sum()['count'].sort_values(ascending=True).plot(kind='barh',color='g')\nplt.ylabel('Count')\nplt.title('Top 20 unigrams in positve text')\nplt.show()\n\ndf2 = pd.DataFrame(negative_test_unigrams, columns = ['Text' , 'count'])\ndf2.groupby('Text').sum()['count'].sort_values(ascending=True).plot(kind='barh',color='red')\nplt.title('Top 20 unigram in Negative text')\nplt.show()\n\ndf3 = pd.DataFrame(neutral_test_unigrams, columns = ['Text' , 'count'])\ndf3.groupby('Text').sum()['count'].sort_values(ascending=True).plot(kind='barh',color='yellow')\nplt.title('Top 20 unigram in Neutral text')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Exploring the selected_text column\n\npositive_text_test = test[test['sentiment'] == 'positive']['text']\nnegative_text_test = test[test['sentiment'] == 'negative']['text']\nneutral_text_test = test[test['sentiment'] == 'neutral']['text']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preprocess Selected_text\n\npositive_text_clean_test = positive_text_test.apply(clean_str)\nnegative_text_clean_test = negative_text_test.apply(clean_str)\nneutral_text_clean_test = neutral_text_test.apply(clean_str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_words_in_positive_text_test = get_top_n_words(positive_text_clean_test)\ntop_words_in_negative_text_test = get_top_n_words(negative_text_clean_test)\ntop_words_in_neutral_text_test = get_top_n_words(neutral_text_clean_test)\n\np_test1 = [x[0] for x in top_words_in_positive_text_test[:20]]\np_test2 = [x[1] for x in top_words_in_positive_text_test[:20]]\n\n\nn_test1 = [x[0] for x in top_words_in_negative_text_test[:20]]\nn_test2 = [x[1] for x in top_words_in_negative_text_test[:20]]\n\n\nnu_test1 = [x[0] for x in top_words_in_neutral_text_test[:20]]\nnu_test2 = [x[1] for x in top_words_in_neutral_text_test[:20]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Top positive word\nsns.barplot(x=p_test1,y=p_test2,color = 'green')\nplt.xticks(rotation=45)\nplt.title('Top 20 Positive Word')\nplt.show()\n\nsns.barplot(x=n_test1,y=n_test2,color='red')\nplt.xticks(rotation=45)\nplt.title('Top 20 Negative Word')\nplt.show()\n\nsns.barplot(x=nu_test1,y=nu_test2,color='yellow')\nplt.xticks(rotation=45)\nplt.title('Top 20 Neutral Word')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Wordclouds\n# Wordclouds to see which words contribute to which type of polarity.\n\nfrom wordcloud import WordCloud\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=[30, 15])\nwordcloud1 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(positive_text_clean_test))\nax1.imshow(wordcloud1)\nax1.axis('off')\nax1.set_title('Positive text',fontsize=40);\n\nwordcloud2 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(negative_text_clean_test))\nax2.imshow(wordcloud2)\nax2.axis('off')\nax2.set_title('Negative text',fontsize=40);\n\nwordcloud3 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(neutral_text_clean_test))\nax3.imshow(wordcloud3)\nax3.axis('off')\nax3.set_title('Neutral text',fontsize=40)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_text_wc = generate_word_cloud(test, \"text\")\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['Target'] = test['sentiment'].apply(lambda x: 2 if x == 'positive' else 1 if x == 'neutral' else 0)\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test[['text_lenght_test', 'number_punctuation_text_test', 'number_of_words_text_test',\n       'number_of_Unique_words_text_test', 'number_of_StopWords_text_test', 'number_of_uppercase_text_test',\n       'average_of_wordsLength_text_test']].sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_var=['text_lenght_test', 'number_punctuation_text_test', 'number_of_words_text_test',\n       'number_of_Unique_words_text_test', 'number_of_StopWords_text_test', 'number_of_uppercase_text_test',\n       'average_of_wordsLength_text_test']\n\n    \nvar_hist_global(df=test,X='Target',Y=list_var, KDE=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature selection ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train =  train[['text_clean', 'stemming_text_for_tfidf', 'lemmatize_text_for_tfidf','tokeniZ_stopWords_text', 'stemming_text', 'lemmatize_text',\n                             'text_lenght', 'number_punctuation_text', 'number_of_StopWords_text', 'number_of_Unique_words_text', 'number_of_uppercase_text','average_of_wordsLength_text']]\n    \n\n\n\ny_train = train['Target']\n\nX_test = test[['text_clean_test','stemming_text_for_tfidf_test', 'lemmatize_text_for_tfidf_test', 'stemming_text_test', 'lemmatize_text_test', 'tokeniZ_stopWords_text_test', \n               'text_lenght_test', 'number_punctuation_text_test','number_of_Unique_words_text_test', 'number_of_StopWords_text_test', \n              'number_of_uppercase_text_test','average_of_wordsLength_text_test']]\n\ny_test = test['Target']\n\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Vectorization \n## Text Tf-Idf Vectorizer training data ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf_vectorizer = TfidfVectorizer(  ngram_range=(1,1), \n                                     analyzer='word',\n                                     stop_words='english', \n                                     lowercase=True, \n                                     max_df=0.9, # remove too frequent words\n                                     min_df=10, # remove too rare words\n                                     max_features = None, # max words in vocabulary, will keep most frequent words\n                                     binary=False #If True, all non zero counts are set to 1. This is useful for discrete probabilistic models that model binary events rather than integer counts.\n                                  )\n\n\n\n#Stemmed questions vectorzation\nX_text_tfidf_vectorizer_train = tfidf_vectorizer.fit_transform(X_train['stemming_text_for_tfidf'])\nX_text_tfidf_vectorizer_test = tfidf_vectorizer.transform(X_test['stemming_text_for_tfidf_test'])\n\n#Lemmentized questions vectorization\nX_text_tfidf_Lem_vect_train = tfidf_vectorizer.fit_transform(X_train['lemmatize_text_for_tfidf'])\nX_test_tfidf_Lem_vect_test = tfidf_vectorizer.transform(X_test['lemmatize_text_for_tfidf_test'])\n\n\n#bigram text vectorization\nbigram_vectorizer = TfidfVectorizer(  ngram_range=(1,2), \n                                     analyzer='word',\n                                     stop_words='english', \n                                     lowercase=True, \n                                     max_df=0.9, # remove too frequent words\n                                     min_df=10, # remove too rare words\n                                     max_features = None, # max words in vocabulary, will keep most frequent words\n                                     binary=False #If True, all non zero counts are set to 1. This is useful for discrete probabilistic models that model binary events rather than integer counts.\n                                  )\nX_text_bigram_vectorizer_train = bigram_vectorizer.fit_transform(X_train['stemming_text_for_tfidf'])\nX_text_bigram_vectorizer_test = bigram_vectorizer.transform(X_test['lemmatize_text_for_tfidf_test'])\n\n\n#T3gram questions vectorization\nt3gram_vectorizer = TfidfVectorizer(  ngram_range=(1,4), \n                                     analyzer='word',\n                                     stop_words='english', \n                                     lowercase=True, \n                                     max_df=0.9, # remove too frequent words\n                                     min_df=10, # remove too rare words\n                                     max_features = None, # max words in vocabulary, will keep most frequent words\n                                     binary=False #If True, all non zero counts are set to 1. This is useful for discrete probabilistic models that model binary events rather than integer counts.\n                                  )\nX_text_t3gram_vectorizer_train = t3gram_vectorizer.fit_transform(X_train['stemming_text_for_tfidf'])\nX_text_t3gram_vectorizer_test  = t3gram_vectorizer.transform(X_test['stemming_text_for_tfidf_test'])\n\n#Range single word to t3gram text vectorization\nst3gram_vectorizer = TfidfVectorizer(  ngram_range=(1,3), \n                                     analyzer='word',\n                                     stop_words='english', \n                                     lowercase=True, \n                                     max_df=0.9, # remove too frequent words\n                                     min_df=10, # remove too rare words\n                                     max_features = None, # max words in vocabulary, will keep most frequent words\n                                     binary=False #If True, all non zero counts are set to 1. This is useful for discrete probabilistic models that model binary events rather than integer counts.\n                                  )\nX_text_Singt3gram_vectorizer_train = st3gram_vectorizer.fit_transform(X_train['stemming_text_for_tfidf'])\nX_text_Singt3gram_vectorizer_test  = st3gram_vectorizer.transform(X_test['stemming_text_for_tfidf_test'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"selected_text_tfidf_vectorizer_train = tfidf_vectorizer.fit_transform(train['stemming_selected_text_for_tfidf'])\n\nselected_text_tfidf_Lem_vect_train = tfidf_vectorizer.fit_transform(train['lemmatize_selected_text_for_tfidf'])\n\nselected_text_bigram_vectorizer_train = bigram_vectorizer.fit_transform(train['stemming_selected_text_for_tfidf'])\n\nselected_text_t3gram_vectorizer_train = t3gram_vectorizer.fit_transform(train['stemming_selected_text_for_tfidf'])\nselected_text_Singt3gram_vectorizer_train = st3gram_vectorizer.fit_transform(train['stemming_selected_text_for_tfidf'])\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Text Word embedding - Doc2Vec ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Word2Vec with preprocessiong text (without stopwords) \nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument\n\nd2v_training_data = []\nfor i, doc in enumerate(X_train['stemming_text']):\n    d2v_training_data.append(TaggedDocument(words=doc,tags=[i]))\n\n# ========== learning doc embeddings with doc2vec ==========\n\n# PV stands for 'Paragraph Vector'\n# PV-DBOW (distributed bag-of-words) dm=0\n\nd2v = Doc2Vec(d2v_training_data, vector_size=300, window=10, alpha=0.1, min_alpha=1e-4, dm=0, negative=1, epochs=10, min_count=2, workers=4)\nd2v_vecs = np.zeros((len(X_train['stemming_text']), 300))\nfor i in range(len(X_train['stemming_text'])):\n    d2v_vecs[i,:] = d2v.docvecs[i]\n    \nd2v_test = np.zeros((len(X_test['stemming_text_test']), 300))\nfor i in range(len(X_test['stemming_text_test'])):\n    d2v_test[i,:] = d2v.infer_vector(X_test['stemming_text_test'].iloc[i])\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Word2Vec with lemmatize words\nd2v_training_data = []\nfor i, doc in enumerate(X_train['lemmatize_text']):\n    d2v_training_data.append(TaggedDocument(words=doc,tags=[i]))\n\n# ========== learning doc embeddings with doc2vec ==========\n\n# PV stands for 'Paragraph Vector'\n# PV-DBOW (distributed bag-of-words) dm=0\n\nd2v = Doc2Vec(d2v_training_data, vector_size=200, window=5, alpha=0.1, min_alpha=1e-4, \n              dm=0, negative=1, epochs=10, min_count=2, workers=4)\nd2v_vecs_bigram = np.zeros((len(X_train['lemmatize_text']), 200))\nfor i in range(len(X_train['lemmatize_text'])):\n    d2v_vecs_bigram[i,:] = d2v.docvecs[i]\n    \nd2v_test_bigram = np.zeros((len(X_test['lemmatize_text_test']), 200))\nfor i in range(len(X_test['lemmatize_text_test'])):\n    d2v_test_bigram[i,:] = d2v.infer_vector(X_test['lemmatize_text_test'].iloc[i])\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest, chi2, f_classif, mutual_info_classif, SelectPercentile\nfrom sklearn.pipeline import Pipeline\n\nfeatures = SelectKBest(mutual_info_classif,k=2).fit(X_train[['text_lenght', 'number_punctuation_text','number_of_Unique_words_text', 'number_of_StopWords_text', \n                                                             'number_of_uppercase_text','average_of_wordsLength_text']].fillna(0),y_train)\nindependance_test = np.zeros((6,2))\nfor idx,i in enumerate(['text_lenght', 'number_punctuation_text', 'number_of_Unique_words_text', 'number_of_StopWords_text', 'number_of_uppercase_text',\n                        'average_of_wordsLength_text']):\n    #independance_test[idx,0]= features.pvalues_[idx]\n    independance_test[idx,1]= features.scores_[idx]\n    #print (i,features.pvalues_[idx],features.scores_[idx])\n    #print('%s  %s'%(i,features.scores_[idx]))\n\n    \n    \nlist_var=['text_lenght', 'number_punctuation_text','number_of_Unique_words_text', 'number_of_StopWords_text', 'number_of_uppercase_text',\n          'average_of_wordsLength_text']\nindependance_df = pd.DataFrame({'Variables': list_var, 'p_values': independance_test[:,0], 'MI': independance_test[:,1]},index=None)\nindependance_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 10))\n_ = sns.heatmap(train[['text_lenght', 'number_punctuation_text','number_of_Unique_words_text', 'number_of_StopWords_text', 'number_of_uppercase_text','average_of_wordsLength_text']].corr()\n                ,cmap=\"YlGnBu\", annot=True, fmt=\".2f\")\nplt.savefig(\"Correlation Matrice\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modelisation ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#!pip install hpelm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import svm ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prediction ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score, StratifiedShuffleSplit, KFold\nrandom_state = 42\nkf = KFold(n_splits=2,random_state=random_state)\nn_iter= 50","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neural_network import MLPClassifier\nModel_final_MLPClassifier = MLPClassifier(random_state=random_state).fit(X_text_tfidf_vectorizer_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Predictions \n\nPredictions = Model_final_MLPClassifier.predict(X_text_tfidf_vectorizer_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission['selected_text'] = Predictions\nsample_submission['selected_text'] = sample_submission['selected_text'].apply(lambda x: x.replace('!!!!', '!') if len(x.split())==1 else x)\nsample_submission['selected_text'] = sample_submission['selected_text'].apply(lambda x: x.replace('..', '.') if len(x.split())==1 else x)\nsample_submission['selected_text'] = sample_submission['selected_text'].apply(lambda x: x.replace('...', '.') if len(x.split())==1 else x)\nsample_submission.to_csv('submission.csv', index=False)\nsample_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentiment_list = { 0: 'negative', \n                  2 : 'positive', \n                  1: 'neutral'}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sentiment_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsample_submission['Sentiment_pred'] = sample_submission['Sentiment_preds'].map(sentiment_list)\nsample_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission['text2'] = sample_submission[\"text\"].apply(lambda x: x.split())\nsample_submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text2 = sample_submission['text2']\ntext2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"text2 = [l[-int(Predictions.tolist()[ind]):] for ind, l in enumerate(text2)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text2[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission['text22'] = text2\nsample_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission['selected_text'] = sample_submission[\"text22\"].apply(lambda x: \" \".join(x))\nsample_submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission   = sample_submission[['textID', \"selected_text\"]]\nsubmission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}