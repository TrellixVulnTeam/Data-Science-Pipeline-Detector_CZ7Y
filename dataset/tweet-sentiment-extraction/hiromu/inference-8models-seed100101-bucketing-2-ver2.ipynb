{"cells":[{"metadata":{},"cell_type":"markdown","source":"# File Paths","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"##### Base-model paths #####\n## Main config based on hiromu's config\nMY_CONFIG_PATH = '../input/base-pre-cv072469/base_pre_cv072469/base_pre_cv072469_config_20200612_150232.pkl'\n\n\n## model_path & config of each models\nMODEL1_FOLDER = \"../input/large-cv0726033/RoBERTa-large-0726033 /\"\nMODEL1_CONFIG = \"config_20200611_000411.pkl\"\n\nMODEL2_FOLDER = \"../input/base-cv0722436/RoBERTa-base-0722436/\"\nMODEL2_CONFIG = \"config_20200610_230921.pkl\"\n\nMODEL3_FOLDER = \"../input/large-pre-cv072592/large_pre_cv072592/\"\nMODEL3_CONFIG = \"large_pre_cv072592_config_20200612_112512.pkl\"\n\nMODEL4_FOLDER = \"../input/base-pre-cv072469/base_pre_cv072469/\"\nMODEL4_CONFIG = \"base_pre_cv072469_config_20200612_150232.pkl\"\n\nMODEL5_FOLDER = \"../input/ooka-large-seed101/\"\nMODEL5_CONFIG = \"config_20200614_233735.pkl\"\n\nMODEL6_FOLDER = \"../input/ooka-base-seed101/ooka_base_seed101/\"\nMODEL6_CONFIG = \"config_20200612_183046.pkl\"\n\nMODEL7_FOLDER = \"../input/hiromu-large-seed101/\"\nMODEL7_CONFIG = \"config_20200614_134559.pkl\"\n\nMODEL8_FOLDER = \"../input/hiromu-base-seed101/hiromu_base_seed101/\"\nMODEL8_CONFIG = \"config_20200614_124718.pkl\"\n\n\n##### Reranking-model path #####\n## RoBERTa path\nROBERTA_PATH = \"../input/roberta-base\"\n\n## model_path\nREMODEL0 = \"../input/hiromu-reranking-cv07346/hiromu_reranking_cv0.7346/reranking_model_fold0.bin\"\nREMODEL1 = \"../input/hiromu-reranking-cv07346/hiromu_reranking_cv0.7346/reranking_model_fold1.bin\"\nREMODEL2 = \"../input/hiromu-reranking-cv07346/hiromu_reranking_cv0.7346/reranking_model_fold2.bin\"\nREMODEL3 = \"../input/hiromu-reranking-cv07346/hiromu_reranking_cv0.7346/reranking_model_fold3.bin\"\nREMODEL4 = \"../input/hiromu-reranking-cv07346/hiromu_reranking_cv0.7346/reranking_model_fold4.bin\"\n\nREMODEL5 = \"../input/oof-bucketing-fold0-3/reranking_model_fold0.bin\"\nREMODEL6 = \"../input/oof-bucketing-fold0-3/reranking_model_fold1.bin\"\nREMODEL7 = \"../input/oof-bucketing-fold0-3/reranking_model_fold2.bin\"\nREMODEL8 = \"../input/oof-bucketing-fold0-3/reranking_model_fold3.bin\"\nREMODEL9 = \"../input/oof-bucketing-fold4/reranking_model_fold4.bin\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# transformers==2.8.0 & tokenizers==0.5.2","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip uninstall transformers -y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip uninstall tokenizers -y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!mkdir -p /tmp/pip/cache-transformers/\n!cp ../input/transformers280/transformers-2.8.0-py3-none-any.whl /tmp/pip/cache-transformers/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!mkdir -p /tmp/pip/cache-tokenizers/\n!cp ../input/transformers280/tokenizers-0.5.2-cp37-cp37m-manylinux1_x86_64.whl /tmp/pip/cache-tokenizers/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install --no-index --find-links /tmp/pip/cache-tokenizers/ tokenizers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install --no-index --find-links /tmp/pip/cache-transformers/ transformers","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Inference of Base-model & Create candidates for test data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport sys\nsys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(\"__file__\")))+\"/input/tsepipeline/TSE-kaggle/input/src\")\n\nfrom utils.helper import seed_everything\nfrom config.roberta_config import INPUT_DIR, OUTPUT_DIR, ORI_DATA_DIR\nfrom processing.preprocess import preprocess_df\nfrom train.roberta_train import train\n\nimport numpy as np\nimport pandas as pd\npd.set_option(\"display.max_rows\", 1000)\npd.set_option(\"display.max_colwidth\", 1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SENTIMENT_COMB = {\n    'pos':['positive'],\n    'neg':['negative'],\n    'neu':['neutral'],\n    'png':['positive','negative'],\n    'pnu':['neutral','positive'],\n    'nng':['neutral','negative'],\n    'pnn':['positive','negative','neutral'],\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import glob\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, RandomSampler, SequentialSampler\nfrom transformers import BertTokenizer, get_cosine_schedule_with_warmup, get_linear_schedule_with_warmup\n\nfrom utils.helper import seed_everything, read_pickle, Logger\nfrom train.roberta_train import DEVICE\nfrom train.roberta_train import TweetDataset, TweetCollate, RoBERTaBaseUncased\nfrom train.roberta_train import calc_jaccard, create_valid, opt_th\nfrom train.roberta_train import jaccard_apply\nfrom train.roberta_train import valid, valid_total, ensemble, valid_02, valid_ensemble, create_test_candidates, create_test_candidates_ensemble","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Setting all seeds 100\nseed_everything(100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading main config\nMY_CONFIG = read_pickle(MY_CONFIG_PATH)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading Base-models","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# STR_NOW = '2020611_000411'\nCONFIG1 = read_pickle(MODEL1_FOLDER+MODEL1_CONFIG)\ntmp = pd.DataFrame([[_p] for _p in glob.glob(MODEL1_FOLDER+'*') if _p[-4:]=='.pth'], columns=['filepath'])\ntmp['model']      = tmp['filepath'].apply(lambda x: x.split('/')[-1])\ntmp['fold']       = tmp['model'].apply(lambda x: int(x.split('_')[4]))\ntmp['sentiment']  = tmp['model'].apply(lambda x: x.split('_')[3])\ntmp.sort_values('fold', inplace=True)\ntmp.reset_index(drop=True, inplace=True)\nfilepath1 = tmp.loc[tmp['sentiment']=='pnn', 'filepath'].tolist()\n\n# print(f'\\n\\n--- [START {STR_NOW}] {\"-\"*64}\\n')\nprint(CONFIG1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filepath1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# STR_NOW = '20200610_230921'\nCONFIG2 = read_pickle(MODEL2_FOLDER+MODEL2_CONFIG)\ntmp = pd.DataFrame([[_p] for _p in glob.glob(MODEL2_FOLDER+'*') if _p[-4:]=='.pth'], columns=['filepath'])\ntmp['model']      = tmp['filepath'].apply(lambda x: x.split('/')[-1])\ntmp['fold']       = tmp['model'].apply(lambda x: int(x.split('_')[4]))\ntmp['sentiment']  = tmp['model'].apply(lambda x: x.split('_')[3])\ntmp.sort_values('fold', inplace=True)\ntmp.reset_index(drop=True, inplace=True)\nfilepath2 = tmp.loc[tmp['sentiment']=='pnn', 'filepath'].tolist()\n\n# print(f'\\n\\n--- [START {STR_NOW}] {\"-\"*64}\\n')\nprint(CONFIG2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filepath2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# STR_NOW = '20200612_112512'\nCONFIG3 = read_pickle(MODEL3_FOLDER+MODEL3_CONFIG)\ntmp = pd.DataFrame([[_p] for _p in glob.glob(MODEL3_FOLDER+'*') if _p[-4:]=='.pth'], columns=['filepath'])\ntmp['model']      = tmp['filepath'].apply(lambda x: x.split('/')[-1])\ntmp['fold']       = tmp['model'].apply(lambda x: int(x.split('_')[7]))\ntmp['sentiment']  = tmp['model'].apply(lambda x: x.split('_')[6])\ntmp.sort_values('fold', inplace=True)\ntmp.reset_index(drop=True, inplace=True)\nfilepath3 = tmp.loc[tmp['sentiment']=='pnn', 'filepath'].tolist()\n\n# print(f'\\n\\n--- [START {STR_NOW}] {\"-\"*64}\\n')\nprint(CONFIG3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filepath3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# STR_NOW = '20200612_150232'\nCONFIG4 = read_pickle(MODEL4_FOLDER+MODEL4_CONFIG)\ntmp = pd.DataFrame([[_p] for _p in glob.glob(MODEL4_FOLDER+'*') if _p[-4:]=='.pth'], columns=['filepath'])\ntmp['model']      = tmp['filepath'].apply(lambda x: x.split('/')[-1])\ntmp['fold']       = tmp['model'].apply(lambda x: int(x.split('_')[7]))\ntmp['sentiment']  = tmp['model'].apply(lambda x: x.split('_')[6])\ntmp.sort_values('fold', inplace=True)\ntmp.reset_index(drop=True, inplace=True)\nfilepath4 = tmp.loc[tmp['sentiment']=='pnn', 'filepath'].tolist()\n\n# print(f'\\n\\n--- [START {STR_NOW}] {\"-\"*64}\\n')\nprint(CONFIG4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filepath4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# STR_NOW = '2020611_000411'\nCONFIG5 = read_pickle(MODEL5_FOLDER+MODEL5_CONFIG)\ntmp = pd.DataFrame([[_p] for _p in glob.glob(MODEL5_FOLDER+'*') if _p[-4:]=='.pth'], columns=['filepath'])\ntmp['model']      = tmp['filepath'].apply(lambda x: x.split('/')[-1])\ntmp['fold']       = tmp['model'].apply(lambda x: int(x.split('_')[4]))\ntmp['sentiment']  = tmp['model'].apply(lambda x: x.split('_')[3])\ntmp.sort_values('fold', inplace=True)\ntmp.reset_index(drop=True, inplace=True)\nfilepath5 = tmp.loc[tmp['sentiment']=='pnn', 'filepath'].tolist()\n\n# print(f'\\n\\n--- [START {STR_NOW}] {\"-\"*64}\\n')\nprint(CONFIG5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filepath5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# STR_NOW = '2020611_000411'\nCONFIG6 = read_pickle(MODEL6_FOLDER+MODEL6_CONFIG)\ntmp = pd.DataFrame([[_p] for _p in glob.glob(MODEL6_FOLDER+'*') if _p[-4:]=='.pth'], columns=['filepath'])\ntmp['model']      = tmp['filepath'].apply(lambda x: x.split('/')[-1])\ntmp['fold']       = tmp['model'].apply(lambda x: int(x.split('_')[4]))\ntmp['sentiment']  = tmp['model'].apply(lambda x: x.split('_')[3])\ntmp.sort_values('fold', inplace=True)\ntmp.reset_index(drop=True, inplace=True)\nfilepath6 = tmp.loc[tmp['sentiment']=='pnn', 'filepath'].tolist()\n\n# print(f'\\n\\n--- [START {STR_NOW}] {\"-\"*64}\\n')\nprint(CONFIG6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filepath6","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# STR_NOW = '2020611_000411'\nCONFIG7 = read_pickle(MODEL7_FOLDER+MODEL7_CONFIG)\ntmp = pd.DataFrame([[_p] for _p in glob.glob(MODEL7_FOLDER+'*') if _p[-4:]=='.pth'], columns=['filepath'])\ntmp['model']      = tmp['filepath'].apply(lambda x: x.split('/')[-1])\ntmp['fold']       = tmp['model'].apply(lambda x: int(x.split('_')[4]))\ntmp['sentiment']  = tmp['model'].apply(lambda x: x.split('_')[3])\ntmp.sort_values('fold', inplace=True)\ntmp.reset_index(drop=True, inplace=True)\nfilepath7 = tmp.loc[tmp['sentiment']=='pnn', 'filepath'].tolist()\n\n# print(f'\\n\\n--- [START {STR_NOW}] {\"-\"*64}\\n')\nprint(CONFIG7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filepath7","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# STR_NOW = '2020611_000411'\nCONFIG8 = read_pickle(MODEL8_FOLDER+MODEL8_CONFIG)\ntmp = pd.DataFrame([[_p] for _p in glob.glob(MODEL8_FOLDER+'*') if _p[-4:]=='.pth'], columns=['filepath'])\ntmp['model']      = tmp['filepath'].apply(lambda x: x.split('/')[-1])\ntmp['fold']       = tmp['model'].apply(lambda x: int(x.split('_')[4]))\ntmp['sentiment']  = tmp['model'].apply(lambda x: x.split('_')[3])\ntmp.sort_values('fold', inplace=True)\ntmp.reset_index(drop=True, inplace=True)\nfilepath8 = tmp.loc[tmp['sentiment']=='pnn', 'filepath'].tolist()\n\n# print(f'\\n\\n--- [START {STR_NOW}] {\"-\"*64}\\n')\nprint(CONFIG8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filepath8","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filepath1.extend(filepath5)\nfilepath2.extend(filepath6)\nfilepath3.extend(filepath7)\nfilepath4.extend(filepath8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filling the gap between hiromu's config and Y.O.'s config\nfor item in set(MY_CONFIG.keys()) - set(CONFIG1.keys()):\n    CONFIG1[item] = MY_CONFIG[item]\n    CONFIG2[item] = MY_CONFIG[item]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We don't use this JAC_TH, so any value is ok\nJAC_TH  = 0.46\n\n# Loading test data\n_, test_df = preprocess_df(ORI_DATA_DIR, use_sentiment=SENTIMENT_COMB[CONFIG1['USE_SENTIMENT']], show_head=False)\n# Creating candidates from ensemble result\npred_ensemble = create_test_candidates_ensemble(test_df, filepath1, filepath2, filepath3, filepath4, CONFIG1, CONFIG2, CONFIG3, CONFIG4, JAC_TH=JAC_TH, ratio=[0.4, 0.1, 0.3, 0.2, 0.4, 0.1, 0.3, 0.2], en_ratio=[0.5, 0.5], num=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Inference of Reranking-model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport tokenizers\nimport string\nimport torch\nimport transformers\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom tqdm import tqdm\nimport re\nfrom transformers import get_linear_schedule_with_warmup\n\nseed_everything(100)\n\n\nMAX_LEN = 256\nTOKENIZER = tokenizers.ByteLevelBPETokenizer(\n    vocab_file=f\"{ROBERTA_PATH}/vocab.json\",\n    merges_file=f\"{ROBERTA_PATH}/merges.txt\", \n    lowercase=True,\n    add_prefix_space=True\n)\ncand_nums = 5\nval_nums = 5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class RerankingDataset:\n    def __init__(self, tweet, sentiment, selected_text, jaccard, score):\n        self.tweet = tweet\n        self.sentiment = sentiment\n        self.selected_text = selected_text\n        self.tokenizer = TOKENIZER\n        self.max_len = MAX_LEN\n        self.jaccard = jaccard\n        self.score = score\n    \n    def __len__(self):\n        return len(self.tweet)\n    \n    def __getitem__(self, item):\n        tweet            = self.tweet[item]\n        selected_text    = self.selected_text[item]\n        sentiment        = self.sentiment[item]\n        jaccard          = self.jaccard[item]\n        score            = self.score[item]\n\n        # selected_text means candidate here\n        new_tweet = str(tweet) + \" </s> </s> \" + str(selected_text)\n        tok_tweet = TOKENIZER.encode(new_tweet)\n        input_ids_orig = tok_tweet.ids\n        \n        sentiment_id = {\n            'positive': 1313,\n            'negative': 2430,\n            'neutral': 7974\n        }\n        \n        input_ids = [0] + [sentiment_id[sentiment]] + [2] + [2] + input_ids_orig + [2]\n        token_type_ids = [0, 0, 0, 0] + [0] * (len(input_ids_orig) + 1)\n        mask = [1] * len(token_type_ids)\n        \n        return {\n            'orig_tweet'        : tweet,\n            'sentiment'         : sentiment,\n            'orig_selected'     : selected_text,\n            'jaccard'           : jaccard,\n            'score'             : score,\n            'ids'               : input_ids,\n            'mask'              : mask,\n            'token_type_ids'    : token_type_ids\n        }\n\n# SequenceBucketing\nclass RerankingCollate:\n    def __init__(self):\n        self.CONFIG = {}\n        self.CONFIG['BUCKET'] = True\n        self.CONFIG['MAX_LEN'] = MAX_LEN\n\n    def __call__(self, batch):\n        out = {\n                'orig_tweet'        : [],\n                'sentiment'         : [],\n                'orig_selected'     : [],\n                'jaccard'           : [],\n                'score'             : [],\n                'ids'               : [], # torch.longに型変換\n                'mask'              : [], # torch.longに型変換\n                'token_type_ids'    : [], # torch.longに型変換\n            }\n\n        for i in range(len(batch)):\n            for k, v in batch[i].items():\n                out[k].append(v)\n\n        # Deciding the number of padding\n        if self.CONFIG['BUCKET']:\n            max_pad = 0\n            for p in out['ids']:\n                if len(p)>max_pad:\n                    max_pad = len(p)\n        else:\n            max_pad = self.CONFIG['MAX_LEN']\n            \n        # Padding\n        for i in range(len(batch)):\n            tokenized_text = out['ids'][i]\n            token_type_ids = out['token_type_ids'][i]\n            mask           = out['mask'][i]\n            text_len       = len(tokenized_text)\n\n            out['ids'][i]            = (tokenized_text + [1]    *(max_pad - text_len))[:max_pad]\n            out['token_type_ids'][i] = (token_type_ids + [0]    *(max_pad - text_len))[:max_pad]\n            out['mask'][i]           = (mask           + [0]    *(max_pad - text_len))[:max_pad]\n\n        # torch.float\n        out['jaccard']        = torch.tensor(out['jaccard'], dtype=torch.float)\n        out['score']          = torch.tensor(out['score'], dtype=torch.float)\n        # torch.long\n        out['ids'] = torch.tensor(out['ids'], dtype=torch.long)\n        out['mask']           = torch.tensor(out['mask'], dtype=torch.long)\n        out['token_type_ids'] = torch.tensor(out['token_type_ids'], dtype=torch.long)\n\n        return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class myModel(transformers.BertPreTrainedModel):\n    def __init__(self, conf):\n        super(myModel, self).__init__(conf)\n        self.roberta = transformers.RobertaModel.from_pretrained(ROBERTA_PATH, config=conf)\n        self.dropout = nn.Dropout(0.1)\n        self.out_proj = nn.Linear(768*2, 1)\n    \n    def forward(self, ids, mask, token_type_ids):\n        o1, o2, _ = self.roberta(\n            ids,\n            attention_mask=mask,\n            token_type_ids=token_type_ids\n        )\n\n        apool = torch.mean(o1, 1)\n        mpool, _ = torch.max(o1, 1)\n        cat = torch.cat((apool, mpool), 1)\n\n        bo = self.dropout(cat)\n        p2 = self.out_proj(bo)\n        return p2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading Reranking-models","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda\")\nmodel_config = transformers.RobertaConfig.from_pretrained(ROBERTA_PATH)\nmodel_config.output_hidden_states = True\n\nmodel0 = myModel(conf=model_config)\nmodel0.to(device)\nmodel0.load_state_dict(torch.load(REMODEL0))\n\nmodel1 = myModel(conf=model_config)\nmodel1.to(device)\nmodel1.load_state_dict(torch.load(REMODEL1))\n\nmodel2 = myModel(conf=model_config)\nmodel2.to(device)\nmodel2.load_state_dict(torch.load(REMODEL2))\n\nmodel3 = myModel(conf=model_config)\nmodel3.to(device)\nmodel3.load_state_dict(torch.load(REMODEL3))\n\nmodel4 = myModel(conf=model_config)\nmodel4.to(device)\nmodel4.load_state_dict(torch.load(REMODEL4))\n\nmodel5 = myModel(conf=model_config)\nmodel5.to(device)\nmodel5.load_state_dict(torch.load(REMODEL5))\n\nmodel6 = myModel(conf=model_config)\nmodel6.to(device)\nmodel6.load_state_dict(torch.load(REMODEL6))\n\nmodel7 = myModel(conf=model_config)\nmodel7.to(device)\nmodel7.load_state_dict(torch.load(REMODEL7))\n\nmodel8 = myModel(conf=model_config)\nmodel8.to(device)\nmodel8.load_state_dict(torch.load(REMODEL8))\n\nmodel9 = myModel(conf=model_config)\nmodel9.to(device)\nmodel9.load_state_dict(torch.load(REMODEL9))\nprint('model load')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading test candidates data\ndev = pd.read_csv('/kaggle/working/test_candidate.csv')\ndev","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_dataset = RerankingDataset(\n    tweet=dev.tweet.values,\n    sentiment=dev.sentiment.values,\n    selected_text=dev.candidate.values,\n    jaccard=dev.jaccard.values,\n    score=dev.score.values\n)\nvalid_data_loader = torch.utils.data.DataLoader(\n    valid_dataset,\n    batch_size=val_nums,\n    collate_fn=RerankingCollate(),\n    num_workers=0,\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def eval(valid_data_loader):\n    model0.eval()\n    model1.eval()\n    model2.eval()\n    model3.eval()\n    model4.eval()\n    model5.eval()\n    model6.eval()\n    model7.eval()\n    model8.eval()\n    model9.eval()\n    all_jaccard = 0\n    all_num = 0\n    rets = []\n    with torch.no_grad():\n        for bi, d in enumerate(valid_data_loader):\n            ids = d[\"ids\"]\n            token_type_ids = d[\"token_type_ids\"]\n            mask = d[\"mask\"]\n            sentiment = d[\"sentiment\"]\n            orig_selected = d[\"orig_selected\"]\n            orig_tweet = d[\"orig_tweet\"]\n            jaccard = d[\"jaccard\"]\n            score = d[\"score\"]\n            ids = ids.to(device, dtype=torch.long)\n            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n            mask = mask.to(device, dtype=torch.long)\n            logits0 = model0(\n                ids=ids,\n                mask=mask,\n                token_type_ids=token_type_ids,\n            )\n            logits1 = model1(\n                ids=ids,\n                mask=mask,\n                token_type_ids=token_type_ids,\n            )\n            logits2 = model2(\n                ids=ids,\n                mask=mask,\n                token_type_ids=token_type_ids,\n            )\n            logits3 = model3(\n                ids=ids,\n                mask=mask,\n                token_type_ids=token_type_ids,\n            )\n            logits4 = model4(\n                ids=ids,\n                mask=mask,\n                token_type_ids=token_type_ids,\n            )\n            logits5 = model5(\n                ids=ids,\n                mask=mask,\n                token_type_ids=token_type_ids,\n            )\n            logits6 = model6(\n                ids=ids,\n                mask=mask,\n                token_type_ids=token_type_ids,\n            )\n            logits7 = model7(\n                ids=ids,\n                mask=mask,\n                token_type_ids=token_type_ids,\n            )\n            logits8 = model8(\n                ids=ids,\n                mask=mask,\n                token_type_ids=token_type_ids,\n            )\n            logits9 = model9(\n                ids=ids,\n                mask=mask,\n                token_type_ids=token_type_ids,\n            )\n            maxind = 0\n            maxjac = (logits0[0][0] + logits1[0][0] + logits2[0][0] + logits3[0][0] + logits4[0][0] + logits5[0][0] + logits6[0][0] + logits7[0][0] + logits8[0][0] + logits9[0][0]) / 10 + score[0] * 0.5\n            for i in range(1, val_nums):\n                tmpjac = (logits0[i][0] + logits1[i][0] + logits2[i][0] + logits3[i][0] + logits4[i][0] + logits5[i][0] + logits6[i][0] + logits7[i][0] + logits8[i][0] + logits9[i][0]) / 10 + score[i] * 0.5\n                # Choosing best candidate here\n                if maxjac < tmpjac:\n                    maxind = i\n                    maxjac = tmpjac\n            rets.append(orig_selected[maxind])\n    return rets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cand = eval(valid_data_loader)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Post-processing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\ndef pp(filtered_output, real_tweet):\n    filtered_output = ' '.join(filtered_output.split())\n    if len(real_tweet.split()) < 2:\n        filtered_output = real_tweet\n    else:\n        if len(filtered_output.split()) == 1:\n            if filtered_output.endswith(\"..\"):\n                if real_tweet.startswith(\" \"):\n                    st = real_tweet.find(filtered_output)\n                    fl = real_tweet.find(\"  \")\n                    if fl != -1 and fl < st:\n                        filtered_output = re.sub(r'(\\.)\\1{2,}', '', filtered_output)\n                    else:\n                        filtered_output = re.sub(r'(\\.)\\1{2,}', '.', filtered_output)\n                else:\n                    st = real_tweet.find(filtered_output)\n                    fl = real_tweet.find(\"  \")\n                    if fl != -1 and fl < st:\n                        filtered_output = re.sub(r'(\\.)\\1{2,}', '.', filtered_output)\n                    else:\n                        filtered_output = re.sub(r'(\\.)\\1{2,}', '..', filtered_output)\n                return filtered_output\n            if filtered_output.endswith('!!'):\n                if real_tweet.startswith(\" \"):\n                    st = real_tweet.find(filtered_output)\n                    fl = real_tweet.find(\"  \")\n                    if fl != -1 and fl < st:\n                        filtered_output = re.sub(r'(\\!)\\1{2,}', '', filtered_output)\n                    else:\n                        filtered_output = re.sub(r'(\\!)\\1{2,}', '!', filtered_output)\n                else:\n                    st = real_tweet.find(filtered_output)\n                    fl = real_tweet.find(\"  \")\n                    if fl != -1 and fl < st:\n                        filtered_output = re.sub(r'(\\!)\\1{2,}', '!', filtered_output)\n                    else:\n                        filtered_output = re.sub(r'(\\!)\\1{2,}', '!!', filtered_output)\n                return filtered_output\n\n        # Start with \" \"\n        if real_tweet.startswith(\" \"):\n            filtered_output = filtered_output.strip()\n            text_annotetor = ' '.join(real_tweet.split())\n            start = text_annotetor.find(filtered_output)\n            end = start + len(filtered_output)\n            start -= 0\n            end += 2\n            flag = real_tweet.find(\"  \")\n            if flag < start:\n                filtered_output = real_tweet[start:end]\n\n        # Not start with \" \", but contain \" {2,}\"\n        if \"  \" in real_tweet and not real_tweet.startswith(\" \"):\n            filtered_output = filtered_output.strip()\n            text_annotetor = re.sub(\" {2,}\", \" \", real_tweet)\n            start = text_annotetor.find(filtered_output)\n            end = start + len(filtered_output)\n            start -= 0\n            end += 2\n            flag = real_tweet.find(\"  \")\n            if flag < start:\n                filtered_output = real_tweet[start:end]\n    return filtered_output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_output = []\ntest = pd.read_csv(\"../input/tweet-sentiment-extraction/test.csv\")\ntweet = list(test.text.values)\nfor c, t in zip(cand, tweet):\n    final_output.append(pp(c, t))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample = pd.read_csv(\"../input/tweet-sentiment-extraction/sample_submission.csv\")\nsample.loc[:, 'selected_text'] = final_output\nsample","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}