{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport re\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/working'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n# import training and test data\ntrain_data = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv')\ntrain_data.head()\ntrain_data.dropna(inplace=True)\ntest_data = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv')\ntest_data.head()\n\n\n# we can't preprocess the text as selected text contains raw text\n# lets develop a custom NER model with the help of spacy, Named entities will be the selected text\n\nimport spacy\nfrom tqdm import tqdm\nimport plac\nimport random\nimport warnings\nfrom pathlib import Path\nfrom spacy.util import minibatch, compounding\n\n\n\n# create a function to get the training data for our model\n\ndef get_training_data(sentiment):\n    train_df = []\n    for index, row in train_data.iterrows():\n        if row.sentiment == sentiment:\n            selected_text = row.selected_text\n            text = row.text\n            start = text.find(selected_text)\n            end = start + len(selected_text)\n            train_df.append((text, {\"entities\": [[start, end, 'selected_text']]}))\n    return train_df    \n\n\ndef get_model_out_path(sentiment):\n    model_out_path = None\n    if sentiment == 'positive':\n        model_out_path = '/kaggle/working/model_pos'\n    elif sentiment == 'negative':\n        model_out_path = '/kaggle/working/model_neg'\n    elif sentiment == 'neutral':\n        model_out_path = '/kaggle/working/model_neu'\n    return model_out_path\n\n\ndef train(train_data, output_dir, n_iter=20, model=None):\n    \"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\n    \"\"\n    if model is not None:\n        nlp = spacy.load(output_dir)  # load existing spaCy model\n        print(\"Loaded model '%s'\" % model)\n    else:\n        nlp = spacy.blank(\"en\")  # create blank Language class\n        print(\"Created blank 'en' model\")\n    \n    # create the built-in pipeline components and add them to the pipeline\n    # nlp.create_pipe works for built-ins that are registered with spaCy\n    if \"ner\" not in nlp.pipe_names:\n        ner = nlp.create_pipe(\"ner\")\n        nlp.add_pipe(ner, last=True)\n    # otherwise, get it so we can add labels\n    else:\n        ner = nlp.get_pipe(\"ner\")\n    \n    # add labels\n    for _, annotations in train_data:\n        for ent in annotations.get(\"entities\"):\n            ner.add_label(ent[2])\n\n    # get names of other pipes to disable them during training\n    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n    with nlp.disable_pipes(*other_pipes):  # only train NER\n        # sizes = compounding(1.0, 4.0, 1.001)\n        # batch up the examples using spaCy's minibatch\n        if model is None:\n            nlp.begin_training()\n        else:\n            nlp.resume_training()\n\n        for itn in tqdm(range(n_iter)):\n            random.shuffle(train_data)\n            batches = minibatch(train_data, size=compounding(4.0, 500.0, 1.001))    \n            losses = {}\n            for batch in batches:\n                texts, annotations = zip(*batch)\n                nlp.update(\n                    texts,  # batch of texts\n                    annotations,  # batch of annotations\n                    drop=0.5,   # dropout - make it harder to memorise data\n                    losses=losses, \n                )\n            \n            print(\"Losses\", losses)\n    save_model(output_dir, nlp, 'st_ner')\n    \n\ndef save_model(output_dir, nlp, new_model_name):\n#    output_dir = f'/Users/DATA/Coding /Kaggle /tweet-sentiment-extraction/NER_models/'\n    output_dir=get_model_out_path(sentiment)\n    if output_dir is not None:        \n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n        nlp.meta[\"name\"] = new_model_name\n        nlp.to_disk(output_dir)\n        print(\"Saved model to\", output_dir)\n        \nsentiments=[ 'positive','negative','neutral']\n# training a model for each sentiment\nfor sentiment in sentiments:\n    train_df=get_training_data(sentiment)\n    model_path = get_model_out_path(sentiment)\n    train(train_df, model_path, n_iter=2, model=None)\n    \n\nTRAINED_MODELS_BASE_PATH = '/kaggle/working/' #path where models are saved\n\n\ndef predict_entities(text, model):\n    doc = model(text)\n    ent_array = []\n    for ent in doc.ents:\n        start = text.find(ent.text)\n        end = start + len(ent.text)\n        new_int = [start, end, ent.label_]\n        if new_int not in ent_array:\n            ent_array.append([start, end, ent.label_])\n    selected_text = text[ent_array[0][0]: ent_array[0][1]] if len(ent_array) > 0 else text\n    return selected_text  \n\ndef jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))\n\nif TRAINED_MODELS_BASE_PATH is not None:\n    print(\"Loading Models  from \", TRAINED_MODELS_BASE_PATH)\n    model_pos = spacy.load(TRAINED_MODELS_BASE_PATH + 'model_pos')\n    model_neg = spacy.load(TRAINED_MODELS_BASE_PATH + 'model_neg')\n    model_neu = spacy.load(TRAINED_MODELS_BASE_PATH + 'model_neu')\n    \n#    jaccard_score = 0\n#    for row in tqdm(train_data.itertuples(), total=train_data.shape[0]):\n#       text = row.text\n#        if row.sentiment == 'neutral':\n#            jaccard_score += jaccard(predict_entities(text, model_neu), row.selected_text)\n#            count +=1\n#            k=0\n        \n            \n#        elif row.sentiment == 'positive':\n#            jaccard_score += jaccard(predict_entities(text, model_pos), row.selected_text)\n#            count +=1\n#           k=0\n            \n#        else:\n#            jaccard_score += jaccard(predict_entities(text, model_neg), row.selected_text) \n#            count +=1\n#           k=0\n\n\n\n#print(f'Average Jaccard Score is {jaccard_score/train_data.shape[0]}') \n\ndef predict_on_test_df(text, sentiment):\n    if sentiment == 'neutral':\n        selected = predict_entities (text, model_neu)\n    elif sentiment == 'positive':\n        selected = predict_entities (text, model_pos)\n    else :\n        selected = predict_entities (text, model_neg)\n        \n    return(selected)\n\ntest_data['selected_text']= test_data.apply(lambda x: predict_on_test_df(x['text'], x['sentiment']), axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets do some EDA on the predicted selected text in the submission file\n\n#check and compare lengths of selected text in train and predicted dataset for each sentiment\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#import spacy\n#from spacy.lang.en import English","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#nlp=English()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# funtion to return number of words in a text (will count punctuation as a seperate word)\n#def tokens_in_text(text):\n#    doc=nlp(text)\n#    words= len(doc)\n#    return words\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#def list_of_tokens_in_text(text):\n#    doc=nlp(text)\n#    list_of_tokens= list(doc)\n#    return list_of_tokens\n\n#test_data['list_of_tokens_text']=test_data.apply(lambda x : list_of_tokens_in_text(x['text']),axis=1)\n#test_data['list_of_tokens_selected_text']=test_data.apply(lambda x : list_of_tokens_in_text(x['selected_text']),axis=1)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test_data['selected_text']= test_data.apply(lambda x: predict_on_test_df(x['text'], x['sentiment']), axis=1)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_data['tokens_in_selected_text']= train_data.apply(lambda x: tokens_in_text(x['selected_text']),axis=1)\n#test_data['tokens_in_selected_text']= test_data.apply(lambda x: tokens_in_text(x['selected_text']),axis=1)\n#test_data['tokens_in_text']= test_data.apply(lambda x: tokens_in_text(x['text']),axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create\n#avg_tokens_pos_test=test_data.groupby('sentiment')['tokens_in_selected_text'].mean()\n#avg_tokens_pos_train=train_data.groupby('sentiment')['tokens_in_selected_text'].mean()\n\n#print('avg_tokens_pos_test:{}\\n\\navg_tokens_pos_train: {}'.format(avg_tokens_pos_test,avg_tokens_pos_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#max_tokens_pos_test=test_data.groupby('sentiment')['tokens_in_selected_text'].max()\n#max_tokens_pos_train=train_data.groupby('sentiment')['tokens_in_selected_text'].max()\n#\n#print('max_tokens_pos_test:{}\\n\\nmax_tokens_pos_train: {}'.format(max_tokens_pos_test,max_tokens_pos_train))\n\n#min_tokens_pos_test=test_data.groupby('sentiment')['tokens_in_selected_text'].min()\n#min_tokens_pos_train=train_data.groupby('sentiment')['tokens_in_selected_text'].min()\n\n#print('min_tokens_pos_test:{}\\n\\min_tokens_pos_train: {}'.format(min_tokens_pos_test,min_tokens_pos_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_all(input_str, search_str):\n    l1 = []\n    length = len(input_str)\n    index = 0\n    while index < length:\n        i = input_str.find(search_str, index)\n        if i == -1:\n            return l1\n        l1.append(i)\n        index = i + 1\n    return l1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data['start_pos_of_selected_text_in_text']=test_data.apply(lambda x: find_all(x['text'],x['selected_text'])[0],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def one_word_before(text, selected_text,start_pos_of_selected_text_in_text):\n    try:\n        if start_pos_of_selected_text_in_text >2:\n            one_word = re.search('(\\S+?) '+ str(selected_text), text).groups()[0]\n            final_text= one_word + ' ' + selected_text\n            return final_text\n        else:\n            return selected_text\n    except:\n        return selected_text\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data['final_text']=test_data.apply(lambda x : one_word_before(x['text'],x['selected_text'],x['start_pos_of_selected_text_in_text']),axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_to_submit=test_data[['textID', 'final_text']]\ndata_to_submit.columns=['textID','selected_text']\ndata_to_submit.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}