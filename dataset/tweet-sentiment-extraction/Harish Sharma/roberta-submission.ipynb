{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport re\nimport string\nimport tokenizers\nimport transformers\n\nimport torch\nimport torch.nn as nn\n\nfrom sklearn import model_selection\nfrom torch.nn import functional as F\nfrom tqdm import tqdm\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"MAX_LEN = 192\nTEST_BATCH_SIZE = 32\nEPOCHS = 5\nROBERTA_PATH = \"../input/roberta-base\"\nTOKENIZER = tokenizers.ByteLevelBPETokenizer(\n    vocab_file=f\"{ROBERTA_PATH}/vocab.json\", \n    merges_file=f\"{ROBERTA_PATH}/merges.txt\", \n    lowercase=True,\n    add_prefix_space=True\n)\nDEVICE = 'cuda'\nTEST_PATH = '../input/tweet-sentiment-extraction/test.csv'\nFOLDS = 5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.read_csv(TEST_PATH)\ntest_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class AverageMeter:\n    \"\"\"\n    Computes and stores the average and current value\n    \"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\nclass EarlyStopping:\n    def __init__(self, patience=7, mode=\"max\", delta=0.001):\n        self.patience = patience\n        self.counter = 0\n        self.mode = mode\n        self.best_score = None\n        self.early_stop = False\n        self.delta = delta\n        if self.mode == \"min\":\n            self.val_score = np.Inf\n        else:\n            self.val_score = -np.Inf\n\n    def __call__(self, epoch_score, model, model_path):\n\n        if self.mode == \"min\":\n            score = -1.0 * epoch_score\n        else:\n            score = np.copy(epoch_score)\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(epoch_score, model, model_path)\n        elif score < self.best_score + self.delta:\n            self.counter += 1\n            print('EarlyStopping counter: {} out of {}'.format(self.counter, self.patience))\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(epoch_score, model, model_path)\n            self.counter = 0\n\n    def save_checkpoint(self, epoch_score, model, model_path):\n        if epoch_score not in [-np.inf, np.inf, -np.nan, np.nan]:\n            print('Validation score improved ({} --> {}). Saving model!'.format(self.val_score, epoch_score))\n            torch.save(model.state_dict(), model_path)\n        self.val_score = epoch_score\n\n\ndef jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TweetModel(transformers.BertPreTrainedModel):\n    def __init__(self, conf):\n        super(TweetModel, self).__init__(conf)\n        # Let's take pretrained weights for our model\n        config = transformers.RobertaConfig.from_pretrained(\n            '../input/roberta-base/config.json', output_hidden_states=True)    \n        self.roberta = transformers.RobertaModel.from_pretrained(\n            '../input/roberta-base/pytorch_model.bin', config=config)\n        self.drop_out = nn.Dropout(0.3)\n        # The final layer will have two output features for start and end indexes.\n        self.fc = nn.Linear(768, 2)\n#         nn.init.normal_(self.l0.weight, std = 0.02)\n        nn.init.normal_(self.fc.weight, std=0.02)\n        nn.init.normal_(self.fc.bias, 0)\n        \n    def forward(self, ids, mask):\n        _, _, out = self.roberta(\n            ids,\n            attention_mask = mask\n        )\n        out = torch.stack([out[-1], out[-2], out[-3], out[-4]])\n        out = torch.mean(out, 0)\n        out = self.drop_out(out)\n        logits = self.fc(out)\n        start_logits, end_logits = logits.split(1, dim = -1)\n        \n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n        \n        return start_logits, end_logits","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_data(tweet, sentiment, tokenizer, max_len):\n    '''\n    tweet - Tweet from which we have to perform sentiment extraction.\n    selected_text - Expected output of sentiment extraction\n    sentiment - sentiment to extract (positive, negative or neutral)\n    tokenizer - tokenizer to be used for creating tokens\n    max_len - max length of tweet.\n    '''\n    tweet = \" \" + \" \".join(str(tweet).split())\n#     selected_text = \" \" + \" \".join(str(selected_text).split())\n    \n#     len_st = len(selected_text) - 1\n#     idx0 = None\n#     idx1 = None\n    \n#     for ind in (i for i, e in enumerate(tweet) if e == selected_text[1]):\n#         if \" \" + tweet[ind: ind + len_st] == selected_text:\n#             idx0 = ind\n#             idx1 = ind + len_st - 1\n#             break\n            \n#     char_targets = [0] * len(tweet)\n#     if idx0 != None and idx1 != None:\n#         char_targets[idx0: idx1 + 1] = [1]*(idx1 + 1 - idx0)\n\n    tok_tweets = tokenizer.encode(tweet)\n    input_ids_orig = tok_tweets.ids\n    tweet_offsets = tok_tweets.offsets\n    \n#     target_idx = []\n    \n#     for j, (offset1, offset2) in enumerate(tweet_offsets):\n#         if sum(char_targets[offset1: offset2]) > 0:\n#             target_idx.append(j)\n            \n#     targets_start = target_idx[0]\n#     targets_end = target_idx[-1]\n\n    sentiment_id = {\n        'positive': 1313,\n        'negative': 2430,\n        'neutral': 7974\n    }\n    \n    input_ids = [0] + [sentiment_id[sentiment]] + [2] + [2] + input_ids_orig + [2]\n    token_type_ids = [0, 0, 0, 0] + [1] * (len(input_ids_orig) + 1)\n    mask = [1] * len(token_type_ids)\n    tweet_offsets = [(0, 0)] * 4 + tweet_offsets + [(0,0)]\n#     targets_start += 4\n#     targets_end += 4\n    \n    padding_length = max_len - len(input_ids)\n    if padding_length > 0:\n        input_ids = input_ids + ([1] * padding_length)\n        tweet_offsets = tweet_offsets + ([(0, 0)] * padding_length)\n        mask = mask + [0] * padding_length\n        token_type_ids += [0] * padding_length\n    \n    return {\n        'ids': input_ids,\n        'mask': mask,\n        'token_type_ids': token_type_ids,\n        'orig_tweet': tweet,\n        'sentiment': sentiment,\n        'offsets': tweet_offsets\n    }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TweetDataset:\n    def __init__(self, tweet, sentiment):\n        self.tweet = tweet\n        self.sentiment = sentiment\n        self.tokenizer = TOKENIZER\n        self.max_len = MAX_LEN\n        \n    def __len__(self):\n        return len(self.tweet)\n    \n    def __getitem__(self, item):\n        data = process_data(\n            self.tweet[item],\n            self.sentiment[item],\n            self.tokenizer,\n            self.max_len\n        )\n        \n        return {\n            'ids': torch.tensor(data['ids'], dtype = torch.long),\n            'mask': torch.tensor(data['mask'], dtype = torch.long),\n            'token_type_ids': torch.tensor(data['token_type_ids'], dtype = torch.long),\n            'orig_tweet': data['orig_tweet'],\n            'sentiment': data['sentiment'],\n            'offsets': torch.tensor(data['offsets'], dtype = torch.long)\n        }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def calculate_jaccard_score(\n    original_tweet, \n    sentiment_val, \n    idx_start, \n    idx_end, \n    offsets,\n    verbose=False):\n    \n    if idx_end < idx_start:\n        idx_end = idx_start\n    \n    filtered_output  = \"\"\n    if sentiment_val == \"neutral\" or len(original_tweet.split()) < 2:\n        filtered_output = original_tweet\n        \n    else:\n        for ix in range(idx_start, idx_end + 1):\n            filtered_output += original_tweet[offsets[ix][0]: offsets[ix][1]]\n            if (ix+1) < len(offsets) and offsets[ix][1] < offsets[ix+1][0]:\n                filtered_output += \" \"\n    return filtered_output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_config = transformers.RobertaConfig.from_pretrained(ROBERTA_PATH)\nmodel_config.output_hidden_states = True\n\nmodel1 = TweetModel(conf=model_config)\nmodel1.to(DEVICE)\nmodel1.load_state_dict(torch.load(\"../input/roberta-sentiment-extraction/model_0.bin\"))\nmodel1.eval()\n\nmodel2 = TweetModel(conf=model_config)\nmodel2.to(DEVICE)\nmodel2.load_state_dict(torch.load(\"../input/roberta-sentiment-extraction/model_1.bin\"))\nmodel2.eval()\n\nmodel3 = TweetModel(conf=model_config)\nmodel3.to(DEVICE)\nmodel3.load_state_dict(torch.load(\"../input/roberta-sentiment-extraction/model_2.bin\"))\nmodel3.eval()\n\nmodel4 = TweetModel(conf=model_config)\nmodel4.to(DEVICE)\nmodel4.load_state_dict(torch.load(\"../input/roberta-sentiment-extraction/model_3.bin\"))\nmodel4.eval()\n\nmodel5 = TweetModel(conf=model_config)\nmodel5.to(DEVICE)\nmodel5.load_state_dict(torch.load(\"../input/roberta-sentiment-extraction/model_4.bin\"))\nmodel5.eval()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def eval_fn(data_loader, device):\n    final_outputs = []\n    \n    with torch.no_grad():\n        tk0 = tqdm(data_loader, total = len(data_loader))\n        for bi, d in enumerate(tk0):\n            ids = d[\"ids\"]\n            mask = d[\"mask\"]\n            token_type_ids = d[\"token_type_ids\"]\n            orig_tweet = d[\"orig_tweet\"]\n            sentiment = d[\"sentiment\"]\n            offsets = d[\"offsets\"]\n\n            ids = ids.to(device, dtype=torch.long)\n            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n            mask = mask.to(device, dtype=torch.long)\n            \n            predicted_start1, predicted_end1 = model1(ids, mask)\n            predicted_start2, predicted_end2 = model2(ids, mask)\n            predicted_start3, predicted_end3 = model3(ids, mask)\n            predicted_start4, predicted_end4 = model4(ids, mask)\n            predicted_start5, predicted_end5 = model5(ids, mask)\n            \n            predicted_start = (predicted_start1 + predicted_start2 + predicted_start3 + predicted_start4 + predicted_start5) / 5\n            predicted_end = (predicted_end1 + predicted_end2 + predicted_end3 + predicted_end4 + predicted_end5) / 5\n            \n            predicted_start = torch.softmax(predicted_start, dim = 1).cpu().detach().numpy()\n            predicted_end = torch.softmax(predicted_end, dim = 1).cpu().detach().numpy()\n\n            for i, tweet in enumerate(orig_tweet):\n                tweet_sentiment = sentiment[i]\n                output_sentence = calculate_jaccard_score(\n                                    original_tweet = tweet,\n                                    sentiment_val = tweet_sentiment,\n                                    idx_start = np.argmax(predicted_start[i, :]),\n                                    idx_end = np.argmax(predicted_end[i, :]),\n                                    offsets = offsets[i]\n                                    )\n                final_outputs.append(output_sentence)\n    return final_outputs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict():\n    df_test = test_df\n    test_dataset = TweetDataset(\n                    tweet = df_test.text.values,\n                    sentiment = df_test.sentiment.values\n                    )\n    test_dataloader = torch.utils.data.DataLoader(\n                            test_dataset,\n                            batch_size = TEST_BATCH_SIZE,\n                            num_workers = 0\n                        )\n    \n    outputs = eval_fn(test_dataloader, DEVICE)\n    \n    sample = pd.read_csv(\"../input/tweet-sentiment-extraction/sample_submission.csv\")\n    sample.loc[:, 'selected_text'] = outputs\n    sample.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}