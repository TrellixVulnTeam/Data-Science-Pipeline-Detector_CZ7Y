{"cells":[{"metadata":{"id":"MV4ZIDskssN4"},"cell_type":"markdown","source":"## EDA and a Beginners BERT Model\n<img align=\"right\" width=\"142\" src=\"https://www.toonpool.com/user/3101/files/bert_409785.jpg\">I'm fairly new to both the realm of NLP and Kaggle competitions; new enough that in going through these notebooks, every other line of code had me looking like Bert over there. However, this competition proved to be not only a way to get my bearings *verrrry* but also a great introduction to the community arround Kaggle, and the immense amount of support participants are willing to give one another.  \n\nSince this was a journey from petty much ground zero to the lofty heights of almost (but not quite!) last on the leaderboard, I referenced some of the publically available kernels to help guide both the form and content of my analysis and model. There were three main sources i referenced:\n\n* https://www.kaggle.com/parulpandey/eda-and-preprocessing-for-bert\n* https://www.kaggle.com/ajinomoto132/starter-kernel-in-pytorch\n* https://www.kaggle.com/abhishek/bert-base-uncased-using-pytorch\n\nThis was also my first time using PyTorch! Although I'm already familiar with the Keras/Tensorflow libraries I decided to use Pytorch for this kernel, which was an experience in itself!\n\nEverything in this kernel is heavily commented and described, both for my own and for the reference of any future beginner. There were quite a few things that a seasoned Data Scientist would immediately recognize that sent me down a bit of a rabbithole for answers, and so any and all rabbitholes have been included in this kernel.\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Importing Libraries and Datasets","execution_count":null},{"metadata":{"id":"PtqsljAPkwv1","trusted":true},"cell_type":"code","source":"# Basic python functionality\nimport re\nimport string\nimport collections\nimport numpy as np\nimport pandas as pd\n\n# This is a great wrapper that displays a progress bar in the notebook\nfrom tqdm.notebook import tqdm\n\n# Plotting and visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(palette='deep', style='white')\nplt.rcParams['figure.figsize'] = [10, 8]\nplt.rcParams['figure.dpi'] = 100\nplt.rc(\"axes.spines\", top=False, right=False)\nfrom wordcloud import WordCloud, STOPWORDS\n\n# NLTK\nimport nltk\nfrom nltk.tokenize import word_tokenize, RegexpTokenizer\nfrom nltk.corpus import stopwords\n\n# Pytorch\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\n\n#Sci-Kit Learn\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import StratifiedKFold\n\n#Huggingface Transformers\nfrom transformers import BertTokenizer, BertConfig, BertModel, AdamW\nfrom transformers import get_linear_schedule_with_warmup\n\n# this allows multiple outputs to be displayed from a cell\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = 'all'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Loading in the Training and Test Datasets, and printing the size and a brief sample of the data to get a sense of its structure and nature.","execution_count":null},{"metadata":{"id":"CMxm5u5Rk4C_","outputId":"08047334-3024-49d0-d34d-0d814beb3752","trusted":true},"cell_type":"code","source":"# load data in Kaggle\ntrain = pd.read_csv('../input/tweet-sentiment-extraction/train.csv')\ntest = pd.read_csv('../input/tweet-sentiment-extraction/test.csv')\n\n# training dataset, sample of 10 entries\nprint('Training shape:', train.shape)\ntrain.sample(5)\n\n# test dataset, sample of 10 entries\nprint('Test shape:', test.shape)\ntest.sample(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Text Preprocessing\nTo prepare the text for an Exploratory Data Analysis (EDA), I'm going to run some basic cleaning and processing methods that remove unwanted characters and format it properly. \n\nFirst we'll start with finding any `null` values in the data and removing them.","execution_count":null},{"metadata":{"id":"iPL0yB9UqG56","outputId":"8b28ce64-2214-4324-a427-db61f18828f2","trusted":true},"cell_type":"code","source":"print(f'Training null Values:\\n{train.isnull().sum()}\\n')\nprint(f'Test null Values:\\n{test.isnull().sum()}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### First, I'm removing the null values in the training dataset using `pd.drop(no)`; looks like the testing set is already good.\nFYI, the argument `inplace=True` means that the operation is performmed directly to the dataframe itself.","execution_count":null},{"metadata":{"id":"6qSQdwX4mEg3","trusted":true},"cell_type":"code","source":"train.dropna(axis=0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Text Cleaning\n\nI know it's immensely useful but I still just get annoyed by Regular Expressions. Not sure why, exactly. But as you can see they lie at the heart of cleaning all the unwanted symbols, punctuation, hyperlinks and markdown from the text. The function below goes over some basic regex operations. For a bigger and better guide to all things regex, I highly recommend using [regex101.com](https://regex101.com/). They've got pretty much any application you could want covered there, as well as detailed explanations of how the expressions operate.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def text_preprocessing(text):\n    # makes text lowercase\n    text = text.lower()\n    # removes text within square brackets\n    text = re.sub('\\[.*?\\]', '', text)\n    # remove hyperlinks\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    # removes text within brackets\n    text = re.sub('<.*?>+', '', text)\n    # removes punctuation\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    # removes new line characters\n    text = re.sub('\\n', ' ', text)\n    # removes words with numbers in them\n    text = re.sub('\\w*\\d\\w*', '', text)\n    \n    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n    tokenized_text = tokenizer.tokenize(text)\n    stop_words = set(stopwords.words('english'))\n    no_stop_words = [w for w in tokenized_text if w not in stop_words]\n           \n    text = ' '.join(no_stop_words)\n    return text    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Cleaning the text and adding it to the training Dataframe","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train['clean text'] = train['text'].apply(lambda x: text_preprocessing(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Adding the length of the text and selected text to the training dataframe, as well as the word count for each.","execution_count":null},{"metadata":{"id":"ou-Is_S8899E","trusted":true},"cell_type":"code","source":"# tweet length\ntrain['text length'] = train['text'].apply(len)\ntrain['selected text length'] = train['selected_text'].apply(len)\n\n# tweet word count\ntrain['word count'] = train['text'].apply(lambda x: len(x.split()))\ntrain['selected word count'] = train['selected_text'].apply(lambda x: len(x.split()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Jaccard Similarities\n<img align=\"right\" src=\"https://images.deepai.org/glossary-terms/jaccard-index-452201.jpg\" alt=\"Visual representation of a Jaccard Similarity\" width=\"300\"> \n\nA Jaccard Similarity is a measure of the intersection of two sets as a percentage of the union of those sets. The image is probably a better indication of what a Jaccard Index represents. \n\nSo why are we calculating this? The `selected_text`is a subset of `text`, and by calculating the score we can again some insight into how much information is needed to determine the sentiment of a tweet.","execution_count":null},{"metadata":{"id":"8MZkV0wfKNlX","trusted":true},"cell_type":"code","source":"# creating a function for the Jaccard Similarity between two strings\ndef jaccard(a,b):\n    a = set(str(a).lower().split())\n    b = set(str(b).lower().split())\n    return len(a & b) / len(a | b)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### And finally adding the Jaccard Similarity to our training dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"jaccard_scores = []\ntext = train.text.iloc[0]\nfor i in range(len(train)):\n    j = jaccard(train.text.iloc[i],train.selected_text.iloc[i])\n    jaccard_scores.append(j)\ntrain['jaccard score'] = jaccard_scores","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### For the purposes of our EDA and not having to repeatedly split the data along \"positive\", \"negative\" and \"neutral\" lines, we'll go ahead and create a dataframe for each of these categories.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"positive = train[train['sentiment']=='positive']\nneutral = train[train['sentiment']=='neutral']\nnegative = train[train['sentiment']=='negative']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EDA","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"I started by plotting the distribution of `Positive`, `Negative` and `Neutral` tweets for both the train and test datasets. This ensures the samples aren't skewed towards a particular sentiment, as this could alter the output of our algorithm","execution_count":null},{"metadata":{"id":"4Bi6P9ddqvSs","outputId":"bee9d8a3-77d9-4b6a-c78e-ec155cc4b4e7","trusted":true},"cell_type":"code","source":"sns.countplot(train.sentiment)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(test.sentiment)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can also look at the difference in the normalized distributions of each; they all come out less than a percent.","execution_count":null},{"metadata":{"id":"UJCIq73j25uq","outputId":"213b349c-b0af-4db2-d52c-385a551f3dc4","trusted":true},"cell_type":"code","source":"p,neg, neu = train.sentiment.value_counts(normalize=True) - \\\ntest.sentiment.value_counts(normalize=True)\nprint(f'Difference in positive examples: {p*100}%')\nprint(f'Difference in negative examples: {neg*100}%')\nprint(f'Difference in neutral examples: {neu*100}%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Plotting the distributions of `text length` and `word count`","execution_count":null},{"metadata":{"id":"fV97m1-RPD15","outputId":"ca958332-3dc7-4deb-f0dd-62c8e7a436da","trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(3,1) \nsns.distplot(positive['text length'], bins=32, color='green', ax=axs[0])\nsns.distplot(neutral['text length'], bins=32, ax=axs[1])\nsns.distplot(negative['text length'], bins=32, color='red', ax=axs[2])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(3,1) \nsns.distplot(positive['word count'], bins=32, color='green', ax=axs[0])\nsns.distplot(neutral['word count'], bins=32, ax=axs[1])\nsns.distplot(negative['word count'], bins=32, color='red', ax=axs[2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Comparing the difference between the Text and the Selected text for each sentiment - this is an approximate way to visualize the Jaccard scores of each","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# plotting the similiairty between the positive words\nfig, axs = plt.subplots(figsize=(10, 4)) \nsns.kdeplot(positive['text length'], color='purple', shade=True)\nsns.kdeplot(positive['selected text length'], color='orange', shade=True)\nplt.title('Text length vs Selected Text Length: Positive')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plotting the similiairty between the negative words\nfig, axs = plt.subplots(figsize=(10, 4)) \nsns.kdeplot(negative['text length'], color='purple', shade=True)\nsns.kdeplot(negative['selected text length'], color='orange', shade=True)\nplt.title('Text length vs Selected Text Length: Negative')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plotting the similiairty between the neutral words\nfig, axs = plt.subplots(figsize=(10, 4)) \nsns.kdeplot(neutral['text length'], color='purple', shade=True)\nsns.kdeplot(neutral['selected text length'], color='orange', shade=True)\nplt.title('Similarity between Neutral Selected Text and Tweet')\nplt.title('Text length vs Selected Text Length: Neutral')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Comparing the selected text length between Positive and Negative Tweets","execution_count":null},{"metadata":{"_kg_hide-output":true,"id":"wUG3KsGtD357","outputId":"5ee91d38-4b91-46f6-e012-e35d9ecaec35","trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(figsize=(10, 4)) \nsns.kdeplot(positive['selected text length'], color='green', shade=True)\nsns.kdeplot(negative['selected text length'], color='red', shade=True)\nplt.title('Positive Selected Text vs. Negative Selected Text')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Comparing the selected text word count between Positive and Negative Tweets","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Plotting the Jaccard Scores of the positive and negative tweets","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(figsize=(10, 4), dpi=100) \nsns.kdeplot(positive['jaccard score'], color='green', shade=True)\nsns.kdeplot(negative['jaccard score'], color='red', shade=True)\nplt.title('Positive and Negative Jaccard Score Distributions')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Word Clouds!\n\nYay fun things with colors and sizes! Seriously though, I think these are a good tool for creating a sort of *gestalt* of the corpus","execution_count":null},{"metadata":{"id":"hkSNqJf-bqw2","trusted":true},"cell_type":"code","source":"# Word cloud\nwordcloud = WordCloud(background_color='white', colormap='viridis_r', height=1080, width=1080).generate(\"\".join(t for t in positive['selected_text']))\nplt.figure(figsize=(12,12))\nplt.imshow(wordcloud)\n\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wordcloud = WordCloud(background_color='white', colormap='inferno_r', height=1080, width=1080).generate(\"\".join(t for t in negative['selected_text']))\nplt.figure(figsize=(12,12))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Generating NGRAMS\nNGrams are an essential part of NLP, as they reveal the frequencies that series of words appear in a corpus. The `N` refers to the length of the phrase in question; Unigrams (1 word), Bigrams (2 words) and Trigrams (you guessed it - 3 words) are fairly common, though you could make the number as large as you want (provided its less than the length of the text, I suppose).\n\nAt first I tried to write my own algorithm for generating Ngrams but found it to be somewhat inaccurate and overly resource intensive, so I instead used the sources below to create a function that returns a list of ngrams.\n\n* https://medium.com/@cristhianboujon/how-to-list-the-most-common-words-from-text-corpus-using-scikit-learn-dad4d0cab41d\n* https://www.kaggle.com/parulpandey/eda-and-preprocessing-for-bert","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_ngrams(corpus, n, length):\n    # Convert a collection of text documents to a matrix of token counts \n    # The fit method learns a vocabulary dictionary of all tokens in the raw documents\n    vec = CountVectorizer(stop_words='english', ngram_range=(n,n)).fit(corpus)\n    \n    #bag_of_words a matrix where each row represents a specific text in corpus and each \n    # column represents a word in vocabulary, that is, all words found in corpus\n    bag_of_words = vec.transform(corpus)\n    \n    sum_words = bag_of_words.sum(axis=0)\n    word_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    word_freq =sorted(word_freq, key = lambda x: x[1], reverse=True)\n    \n    return word_freq[:length]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### I also created a function to plot them, because why not?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_ngram(corpus, n, length, color):\n    color = color + 's_d'\n    if n == 1:\n        n_gram = 'Unigrams'\n    elif n == 2:\n        n_gram = 'Bigrams'\n    elif n == 3:\n        n_gram = 'Trigrams'\n    else:\n        n_gram = str(n) + '-grams'\n        \n    df = pd.DataFrame()\n    df = pd.DataFrame(get_ngrams(corpus, n, length), columns=[n_gram, 'Count'])\n    \n    plot = sns.barplot(x='Count', y = n_gram, data=df, palette= color)\n\n    del df\n    \n    return fig, plot  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_ngram(positive['clean text'], 2, 20, 'Green')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_ngram(neutral['clean text'], 2, 20, 'Blue')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_ngram(negative['clean text'], 2, 20, 'Red')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_ngram(positive['selected_text'], 2, 20, 'Green')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_ngram(negative['selected_text'], 2, 20, 'Red')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## BERT Model \n<img src=\"https://huggingface.co/landing/assets/transformers-docs/huggingface_logo.svg\" align=\"left\" margin-right=\"10px\">Here we have the main dish, so to speak, the model itself using the (in)famous BERT transformer. BERT stands for 'Bidirectional Encoder Representation from Transformers, and to be honest I kind of wih they put the 'f' in the acronym. Either way, BERT performs exceedingly well on tasks such as this, and the kind folks over at [Huggingface](https://huggingface.co/transformers/model_doc/bert.html) went through the trouble of creating a PyTorch-friendly version of BERT, which, for the novice like yours truly, proved to be fairly straightforward to understand and implement.\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Setting up some basic parameters for the model:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_batch_size = 32\nvalidate_batch_size = 16\nepochs = 5\n!mkdir models\nmodel_dir = 'models/'\nmodel_path = 'models/bertbaseuncased.bin'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Huggingface Library offers some incredibly convenient ways for setting up your BERT model. The `config` files can be used and edited directly from their site, as well as the vocabulary used for tokenization.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"config = BertConfig.from_pretrained('bert-base-cased')\nconfig.output_hidden_states = True\nconfig.num_labels = 2\ntokenizer = BertTokenizer.from_pretrained('bert-base-cased')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The BERT model requires that all tensors it takes as inputs be padded to the same length. In order to do so, I calcualated the maximem length of the `text` from the training and test datasets and added some extra space, just in case.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define the maximum length\nmax_len = 0\n\nfor i in train['text']:\n    encoded_text = tokenizer.encode(str(i))\n    \n    max_len = max(max_len, len(encoded_text))\n    \nfor i in test['text']:\n    encoded_text = tokenizer.encode(str(i))\n    \n    max_len = max(max_len, len(encoded_text))\n\nmax_len +=4\nprint(f'The maximum length of a tweet in the train and test datasets is {max_len-4}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### PyTorch allows you to configure the device used; for this notebook I opted to use Kaggle's GPU to train the model, as using the CPU would've taken forever.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"if torch.cuda.is_available():\n    device = torch.device('cuda')\nelse:\n    device = torch.device('cpu')\n\nprint(f'Using {device}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating a class for the Model\nThe general approach when using PyTorch is to create a custom class for your model, using `nn.Module` as a base. I've commented a good deal to explain how and why the BERT model is constructed.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class BertBaseCased(nn.Module):\n    \n    def __init__(self, config):\n        super(BertBaseCased, self).__init__()\n        # Loading the model config file\n        self.config = config \n        # Setting the number of labels we want to train our model for\n        self.num_labels = config.num_labels \n        # The Bert Base the model is built on top of\n        self.base = 'bert-base-cased'\n        # Creates a BERT model instance\n        self.bert = BertModel(config = self.config)\n        # Creates a Linear Module the 768 hidden layer output from BERT and returns our predictions\n        self.classifier = nn.Linear(768, 2)\n        \n    def __str__(self):\n        return f'Bert Model using: {self.base}'\n        \n    def __repr__(self):\n        return f'Bert Model using: {self.base}'\n        \n    def forward(self, ids, mask, token_type_ids):\n        # This is the standard forward sequence common to PyTorch Modules\n        sequence_output, _, _ = self.bert(ids,\n                                   attention_mask=mask,\n                                   token_type_ids=token_type_ids)\n        # Takes the sequence output and transforms it to a prediction tensor\n        logits = self.classifier(sequence_output)\n        # Splitting the tensor output into our start and end positions of the selected text\n        start_logits, end_logits = logits.split(1, dim=-1)\n        \n        # Squeeze reduces the dimension of a tensor along the axis specified.\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n        \n        return start_logits, end_logits","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating a Class for the Training Data\nSince the data will need to be formatted in order to be processed by the BERT model, I'm also building a Class to hold the training data. It also proved useful for quickly testing and tweaking different elements of the model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class TweetDataset:\n    def __init__(self, tweet, sentiment, selected_text):\n        self.tweet = tweet\n        self.sentiment = sentiment\n        self.selected_text = selected_text\n        self.max_len = max_len\n        self.tokenizer = tokenizer\n        \n    def __len__(self):\n        return len(self.tweet)\n    \n    def __getitem__(self, item):\n        # This defines how any particular example from the training data is fed into the BERT model\n        \n        tweet = \" \".join(str(self.tweet[item]).split())\n        selected_text = \" \".join(str(self.selected_text[item]).split())\n        \n        len_selected_text = len(selected_text)\n        \n        # This creates a vector encoding of the text, and returns some additional features necessary for the model\n        encoded_tweet = self.tokenizer.encode_plus(self.sentiment[item],\n                                                   # Original Tweet\n                                                   tweet,\n                                                   # The Maximum Length\n                                                   max_length=max_len,\n                                                   # Pads each tensor to the max length with 0\n                                                   pad_to_max_length=True,\n                                                   # A binary mask identifying the different sequences in the model\n                                                   return_token_type_ids=True,\n                                                   # Tells the model which tokens to pay attention to\n                                                   return_attention_mask=True\n                                                  )\n        # Returns an encoded version of the selected_text\n        encoded_target = self.tokenizer.encode(selected_text, add_special_tokens=False)\n                \n        targets = [0] * max_len\n        target_start_ind = 0\n        target_end_ind = 0\n        \n        # Finds the start and end indices of the target text within the tweet. This will be the \n        # targets the BERT Model Optimizes for\n        for ind in (i for i, e in enumerate(encoded_tweet['input_ids']) if e == encoded_target[0]):\n            if encoded_tweet['input_ids'][ind:ind+len(encoded_target)] == encoded_target:\n                target_start_ind = ind\n                target_end_ind = ind + len(encoded_target) - 1\n                break\n        \n        target_start = [0] * max_len\n        target_start[target_start_ind] = 1\n        target_end = [0] * max_len\n        target_end[target_end_ind] = 1\n        \n        # creating a mask for the targets\n        for i in range(target_start_ind, target_end_ind+1):\n            targets[i] = 1\n                \n        return {\"ids\": torch.tensor(encoded_tweet['input_ids'], dtype=torch.long),\n            \"mask\": torch.tensor(encoded_tweet['attention_mask'], dtype=torch.long),\n            \"token_type_ids\": torch.tensor(encoded_tweet['token_type_ids'], dtype=torch.long),\n            \"targets\": torch.tensor(targets, dtype=torch.long),\n            \"targets_start\": torch.tensor(target_start, dtype=torch.long),\n            \"targets_end\": torch.tensor(target_end, dtype=torch.long),\n            \"orig_tweet\": self.tweet[item],\n            \"orig_sentiment\": self.sentiment[item],\n            \"orig_selected\": self.selected_text[item],\n        }","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The `AverageMeter` class was taken from this kernel; I was having trouble visualizing the average loss in PyTorch; looks like I wasn't the only one!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class AverageMeter:\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Creating the loss function using is the `Binary Cross Entropy with Logits Loss` class","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def loss_fn(o1, o2, t1, t2):\n    l1 = nn.BCEWithLogitsLoss()(o1, t1)\n    l2 = nn.BCEWithLogitsLoss()(o2, t2)\n    return l1 + l2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Since I plan on training the model using a variety of parameters, I've created a train function that contains all the basic steps for doing so.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(model, train_data_loader, optimizer, scheduler, results, epoch):\n    \n    model.train()\n    losses = AverageMeter()\n\n    tq = tqdm(train_data_loader)\n    for bi, batch in enumerate(tq):\n        ids = batch[\"ids\"]\n        token_type_ids = batch[\"token_type_ids\"]\n        mask = batch[\"mask\"]\n        targets_start = batch[\"targets_start\"]\n        targets_end = batch[\"targets_end\"]\n\n        ids = ids.to(device, dtype=torch.long)\n        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n        mask = mask.to(device, dtype=torch.long)\n        targets_start = targets_start.to(device, dtype=torch.float)\n        targets_end = targets_end.to(device, dtype=torch.float)\n\n\n        # o1 and o2 are the predicited start and end indices of the target phrase\n        o1, o2 = model(\n            ids=ids,\n            mask=mask,\n            token_type_ids=token_type_ids\n        )\n\n\n        loss = loss_fn(o1, o2, targets_start, targets_end)\n\n        # clears the gradient for each optimized tensor\n        optimizer.zero_grad()\n\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n\n        losses.update(loss.item(), ids.size(0))\n        tq.set_postfix(loss=losses.avg)\n        \n        # Appends the result every 10 batches\n        if bi % 10 == 0:\n            results.append([(bi)+(epoch*len(train_data_loader)), losses.avg])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training the Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating the Model\nmodel = BertBaseCased(config=config)\n# Sending the model to the device\nmodel.to(device)\nresults = []\n\n# The stratified KFold splits the data set 80/20 for the purpose of training and cross validation\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nfor fold, (train_idx, valid_idx) in enumerate(kf.split(train, \n                                                       train['sentiment'])):\n    print(f'Training Fold no: {fold+1}')\n    \n    # To save time and the GPU resources, just running a single fold\n    if fold == 0:\n\n        X_train = train.iloc[train_idx].reset_index(drop=True)\n        X_validate = train.iloc[valid_idx].reset_index(drop=True)\n\n        train_dataset = TweetDataset(tweet = X_train.text,\n                                    sentiment = X_train.sentiment,\n                                    selected_text = X_train.selected_text)\n\n        train_data_loader = DataLoader(train_dataset, \n                                       batch_size = train_batch_size)\n        \n        # I have not had time to set up the cross-validation method yet; as I mentioned PyTorch is new territorry for me, and\n        # this is another area I'm still figuring out...\n        validate_dataset = TweetDataset(tweet = X_validate.text,\n                                    sentiment = X_validate.sentiment,\n                                    selected_text = X_validate.selected_text)\n\n        validate_data_loader = DataLoader(validate_dataset, \n                                       batch_size = validate_batch_size)\n        \n        # Generating a list of the model parameters\n        model_params = list(model.named_parameters())\n\n        # These parameters are being excluded from the list given to the optimizer\n        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n\n        # Generates a list of dicts of parameters to feed into the optimizer; one that has a slight\n        # weight decay, and one with no decay\n        optimizer_params = [\n            {'params': [p for n, p in model_params if not any (nd in n for nd in no_decay)],\n            'weight_decay': 0.001}, #TWEAK LATER?\n            {'params': [p for n, p in model_params if any (nd in n for nd in no_decay)],\n            'weight_decay': 0.0}\n        ]\n\n        # total number of training steps for the learning rate\n        num_train_steps = int(len(X_train)/train_batch_size * epochs)\n        # Optimizer from HuggingFace to optimize the model weights\n        # https://huggingface.co/transformers/main_classes/optimizer_schedules.html#adamw\n        optimizer = AdamW(optimizer_params, lr=3e-5)\n        # Determines the schedule for the learning rate\n        # https://huggingface.co/transformers/main_classes/optimizer_schedules.html#learning-rate-schedules\n        scheduler = get_linear_schedule_with_warmup(optimizer,\n                                                   num_warmup_steps=0,\n                                                   num_training_steps=num_train_steps)\n\n        for epoch in range(epochs):\n            train_model(model, train_data_loader, optimizer, scheduler, results, epoch)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Saving the model\nI saved the model after it was done training so it could be used and tweaked in the future. I think this is a good practice, so as not to let all that GPU time go to waste.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.save(model.state_dict(), 'models/bert_base_uncased.bin')\nmodel.config.to_json_file('models/bert_base_cased_config.json')\ntokenizer.save_pretrained('models/')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Generating Predictions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sets the model in eval mode\nmodel.eval()\nmodel.to(device)\n\n#This is a quick way to add the final column needed to create out TweetDataset Object; we won't be using it\ntest.loc[:, \"selected_text\"] = test.text.values\n\nvalidate_dataset = TweetDataset(tweet = test.text,\n                            sentiment = test.sentiment,\n                                selected_text = test.selected_text\n                               )\n\nvalidate_data_loader = DataLoader(validate_dataset, \n                                  shuffle=False,\n                                  batch_size = validate_batch_size)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Inputting the test data into the model, and recording the outputs for each batch","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"outputs = []\nfin_outputs_start = []\nfin_outputs_end = []\nfin_token_ids = []\nfin_original_tweet = []\nfin_original_sentiment = []\nfin_original_selected = []\n\nwith torch.no_grad():\n    for bi, batch in enumerate(tqdm(validate_data_loader)):\n        ids = batch[\"ids\"]\n        token_type_ids = batch[\"token_type_ids\"]\n        mask = batch[\"mask\"]\n        original_tweet = batch['orig_tweet']\n        original_sentiment = batch['orig_sentiment']\n        original_selected = batch['orig_selected']\n\n        ids = ids.to(device, dtype=torch.long)\n        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n        mask = mask.to(device, dtype=torch.long)\n\n        outputs_start, outputs_end = model(\n            ids=ids,\n            mask=mask,\n            token_type_ids=token_type_ids\n        )\n\n        fin_outputs_start.append(torch.sigmoid(outputs_start).cpu().detach().numpy())\n        fin_outputs_end.append(torch.sigmoid(outputs_end).cpu().detach().numpy())\n        fin_original_tweet.extend(original_tweet)\n        fin_original_sentiment.extend(original_sentiment)\n        fin_token_ids.extend(ids.cpu().detach().numpy().tolist())\n        fin_original_selected.extend(original_selected)\n        \nfin_outputs_start = np.vstack(fin_outputs_start)\nfin_outputs_end = np.vstack(fin_outputs_end)\nfin_token_ids = np.vstack(fin_token_ids)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Decoding the Predictions\nThis is a slightly tricky part - decoding the predictions from the tensor to good ol' english. This was actually the part that took the longest, as there a lot of small mistakes and details that can be overlooked.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = []\nstart_idx = []\nend_idx = []\nthreshold = 0.3\n\nfor j in range(len(fin_token_ids)):\n    target_string = fin_original_selected[j]\n    tweet_tokens = fin_token_ids[j]\n    original_tweet = fin_original_tweet[j]\n    sentiment_val = fin_original_sentiment[j]\n    mask_start = fin_outputs_start[j]>=threshold\n    mask_end = fin_outputs_end[j]>=threshold\n\n    output_mask = [0] * len(mask_end)\n\n    idx_start = np.nonzero(mask_start)[0]\n    idx_end = np.nonzero(mask_end)[0]\n\n    if len(idx_start) > 0:\n        idx_start = idx_start[0]\n        if len(idx_end) > 0:\n            idx_end = idx_end[0]\n        else:\n            idx_end = idx_start   \n    else:\n        idx_start = 0\n        idx_end = 0\n\n    predicted_selected_text = []\n    for t in range(idx_start, idx_end+1):\n        predicted_selected_text.append(tweet_tokens[t])\n\n    predicted_selected_text = tokenizer.decode(predicted_selected_text)\n    predictions.append(predicted_selected_text)\n    \npredictions = np.vstack(predictions)    \n ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submitting the Predictions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('../input/tweet-sentiment-extraction/sample_submission.csv')\nsubmission.loc[:, 'selected_text'] = predictions\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}