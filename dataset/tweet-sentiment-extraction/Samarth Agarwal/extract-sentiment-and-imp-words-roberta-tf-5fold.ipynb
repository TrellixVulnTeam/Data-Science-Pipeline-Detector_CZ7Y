{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"## Predict both sentiment and imp words","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\npd.options.display.max_colwidth = -1\nfrom IPython.display import display\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nprint(tf.test.gpu_device_name())\n# See https://www.tensorflow.org/tutorials/using_gpu#allowing_gpu_memory_growth\n#config = tf.compat.v1.ConfigProto()\n#config.gpu_options.allow_growth = True\n\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.losses import BinaryCrossentropy, CategoricalCrossentropy\nfrom tensorflow.keras.initializers import TruncatedNormal\nfrom keras.utils.np_utils import to_categorical  \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nfrom sklearn.model_selection import train_test_split, KFold\n\nimport time\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import transformers\nprint(transformers.__version__)\nfrom transformers import TFRobertaModel, RobertaConfig\n\nimport tokenizers\nprint(tokenizers.__version__)\nfrom tokenizers import ByteLevelBPETokenizer\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def jaccard(str1, str2): \n    # Competition metric\n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def jaccard_txt_only(str1, str2): \n    \n    a = set([s for s in str1.lower().split() if re.fullmatch(pattern = r'\\W+',string=s) is None]) \n    b = set([s for s in str2.lower().split() if re.fullmatch(pattern = r'\\W+',string=s) is None]) \n    c = a.intersection(b)\n    #print(a,b,c)\n    if (len(a) + len(b) - len(c)) > 0:\n        return float(len(c)) / (len(a) + len(b) - len(c))\n    else:\n        return 0\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_epochs = 3\nnum_data_points = 30000\nmax_len = 96\n#validation_split = 0.2\nbatch_size_ = 32\ntrain_bert = True\nn_folds = 5\nlr = 3e-5\ngaussian_noise = 0.03","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#help(BertTokenizer.from_pretrained)\n#tokenizer = BertTokenizer.from_pretrained(\"/kaggle/input/bert-base-uncased/vocab.txt\")\n#dir(tokenizer)\n#help(ByteLevelBPETokenizer)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = ByteLevelBPETokenizer(vocab_file=\"/kaggle/input/roberta-base-tf/vocab.json\",\n                                  merges_file=\"/kaggle/input/roberta-base-tf/merges.txt\",\n                                  lowercase=True, \n                                  add_prefix_space=True,)\ntokenizer","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/tweet-sentiment-extraction/train.csv\")\ntrain = train.dropna().reset_index(drop=True)\n\ntrain[\"text\"] = train.text.str.strip().apply(lambda x: \" \" + \" \".join(x.lower().split()))\ntrain[\"selected_text\"] = train.selected_text.str.strip().apply(lambda x: \" \".join(x.lower().split()))\n\ntrain.head(5).T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# EDA\n\ntrain[\"ratio\"] = train.selected_text.str.strip().str.len() / train.text.str.strip().str.len()\ntrain[\"s_txt_len\"] = train.selected_text.str.strip().str.len()\ntrain[\"txt_len\"] = train.text.astype('str').apply(lambda x: len(x.split(\" \")))\n\nprint(\"*\"*30)\ndisplay(train.loc[(train.selected_text.str.len()<4) & (train.ratio < 1)])\n\nprint(\"*\"*30)\ndisplay(train.groupby(\"sentiment\").agg({\"ratio\":[\"count\",\"mean\"]}))\ndisplay(train[(train.sentiment == \"neutral\") & (train.ratio <= 1)].sort_values(\"ratio\").head(10))\n\nprint(\"*\"*30)\ndisplay(train.groupby(\"txt_len\").agg({\"ratio\":[\"count\",\"mean\"]})[:10])\ndisplay(train[(train.txt_len < 3) & (train.ratio < 1)].sort_values(\"ratio\").head(10))\n\nprint(\"*\"*30)\nprint(train.txt_len.describe())\n\ndel train[\"ratio\"], train[\"txt_len\"], train[\"s_txt_len\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train.text[train.text.str.contains(r\"https?[:/\\w+]\")]\ntrain[\"text_http\"] = train.text.str.contains(r\"(https?[:/\\w\\.]+)\")\ntrain[\"s_text_http\"] = train.selected_text.str.contains(r\"(https?[:/\\w\\.]+)\")\ndisplay(train.groupby(\"sentiment\").agg({\"text_http\":\"sum\", \"s_text_http\":\"sum\"}))\ndel train[\"s_text_http\"], train[\"text_http\"]\n\n#display(train[train.sentiment != \"neutral\"].text.str.extractall(r\"(https?[:/\\w\\.]+)\"))\n#display(train[train.sentiment != \"neutral\"].selected_text.str.extractall(r\"(https?[:/\\w\\.]+)\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def overlap(selected_txt, txt, offsets_, tok_len):\n    s = txt.index(selected_txt)\n    e = len(txt) -1 - txt[::-1].index(selected_txt[::-1])\n    #print(s,e)\n    start_token, end_token = 0,0\n    for i,o in enumerate(offsets_):\n        #if i < tok_len-1:\n        if (s>=o[0]) & (s < o[1]):\n            start_token = i\n        if (e>=o[0]) & (e <= o[1]):\n            end_token = i\n    return (start_token, end_token)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#x_.offsets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# \"what interview! leave me alone\"[16:31]\n# x_.offsets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# overlap('leave me alone', \"what interview! leave me alone\", x_.offsets, len(x_.ids))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#tokenizer.id_to_token(tokenizer.token_to_id(\"neutral\"))\nprint(tokenizer.id_to_token(tokenizer.encode(\"neutral\").ids[0]))\nroberta_sentiment_map = {\"neutral\":tokenizer.encode(\"neutral\").ids[0],\n                        \"positive\":tokenizer.encode(\"positive\").ids[0],\n                        \"negative\":tokenizer.encode(\"negative\").ids[0],\n                        \"sentiment\":tokenizer.encode(\"sentiment\").ids[0] }\nroberta_sentiment_map\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentiment_map = {\"neutral\":1,\n                \"positive\":2,\n                \"negative\":0,}\n\nrev_sentiment_map = {v:k for k, v in sentiment_map.items()}\nprint(rev_sentiment_map)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef Dataloader(textId, text, selected_text, sentiment, tokenizer, max_length):\n    \n    #tokenizer.enable_padding(pad_id = 1, max_length=max_len-4)\n    \n    x_input_ids, x_token_type_ids, x_attention_mask, x_tokens = [], [], [], []\n    y_s,y_e=[],[]\n    tok_len, offsets = [], []\n    y_sent = []\n    for i in range(len(text)):\n        #print(i)\n        #text[i] = \" \".join(text[i].split())\n        #selected_text[i] = \" \".join(selected_text[i].split())\n        \n        x_ = tokenizer.encode(text[i])\n\n        s_ind, e_ind = overlap(selected_text[i], text[i], x_.offsets, len(x_.ids))\n                               #len(np.where(x_.ids != 1)[0]))\n\n        start_vec = [0]*max_length\n        end_vec = [0]*max_length\n        start_vec[s_ind+4] = 1\n        end_vec[e_ind+4] = 1\n\n        x_input_ids.append([0] + [ roberta_sentiment_map[\"sentiment\"]] + [2]+ [2] + x_.ids + [2] + [1]*(max_length-len(x_.ids)-5))\n        #x_input_ids.append([0] + [ roberta_sentiment_map[sentiment[i]]] + [2]+ [2] + x_.ids + [2] + [1]*(max_length-len(x_.ids)-5))\n        x_token_type_ids.append([0]*(max_length))\n        x_attention_mask.append([1]*4 + x_.attention_mask + [1] + [0]*(max_length-len(x_.ids)-5))\n        tok_len.append( 5+len(x_.ids))\n        offsets.append([(0,0)]+[(0,0)]+[(0,0)]+[(0,0)] + x_.offsets+[(0,0)] + [(0,0)]*(max_length-len(x_.ids)-5))\n        x_tokens.append([\"<s>\"]+[\"sentiment\"]+[\"</s>\"]+[\"</s>\"]+x_.tokens+[\"</s>\"])\n        y_s.append((start_vec))\n        y_e.append((end_vec))\n        y_sent.append((sentiment_map[sentiment[i]]))\n\n    return {\"textIds\":np.asarray(textId), \n            \"input_ids\":np.asarray(x_input_ids), \n            \"token_type_ids\":np.asarray(x_token_type_ids), \n            \"attention_mask\":np.asarray(x_attention_mask), \n            \"start_vector\":np.asarray(y_s), \n            \"end_vector\":np.asarray(y_e),\n            \"tok_len\":np.asarray(tok_len),\n            \"sentiment\":np.asarray(sentiment),\n            \"text\":text.tolist(),\n            \"selected_text\":selected_text.tolist(),\n            \"offsets\":offsets,\n            \"tokens\":x_tokens,\n            \"y_sent\": to_categorical(np.asarray(y_sent),num_classes=3),\n           }\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"robertaConfig = RobertaConfig.from_pretrained(\"/kaggle/input/roberta-base-tf/config.json\", output_hidden_states=True)\n#robertaConfig[\"bos_token_id\"] = 0\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# https://huggingface.co/transformers/model_doc/bert.html#tfbertmodel\n\ndef model(Max_seq_length = max_len, train_bert = True):\n    \n    input_ids = layers.Input((Max_seq_length,), dtype = tf.int32)\n    token_type_ids = layers.Input((Max_seq_length,), dtype = tf.int32)\n    attention_mask = layers.Input((Max_seq_length,), dtype = tf.int32)\n    \n    tfbert = TFRobertaModel.from_pretrained(\"/kaggle/input/roberta-base-tf/tf_model.h5\",\n                                    config=robertaConfig)\n    tfbert.roberta.trainable = train_bert\n    \n    _, _, o3 = tfbert({\"input_ids\":input_ids, \n                       #\"token_type_ids\":token_type_ids, \n                       \"attention_mask\":attention_mask})\n    o1 = layers.Concatenate()([o3[-2], o3[-1]])\n    #o1 = o3[0]\n    o1 = layers.GaussianNoise(gaussian_noise)(o1)\n    #o1 = layers.Dropout(0.1)(o1)\n    \n    h1 = layers.Dense(1, activation = \"linear\", kernel_initializer=TruncatedNormal(stddev=0.02))(o1)\n    start_logits = tf.squeeze(h1, axis = -1, name = \"start_logits\")\n\n    h2 = layers.Dense(1, activation = \"linear\", kernel_initializer=TruncatedNormal(stddev=0.002))(o1)\n    end_logits = tf.squeeze(h2, axis = -1, name = \"end_logits\")\n    \n    h3 = layers.Dense(1, activation = \"linear\", kernel_initializer=TruncatedNormal(stddev=0.002))(o1)\n    sent_logits = tf.squeeze(h3, axis = -1)\n    sent_output = layers.Dense(3, activation = \"linear\", kernel_initializer=TruncatedNormal(stddev=0.002), name = \"sentiment_logits\")(sent_logits)\n\n    model = models.Model([input_ids, token_type_ids, attention_mask], [start_logits, end_logits, sent_output])\n    #print(model.summary())\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# #mod = model()\n# #del mod\n# import numpy as np\n# import pandas as pd\n# df = pd.DataFrame(np.random.randint(0, 3, (10, 2)), columns=list('ab'))\n# print(df)\n# df.apply(pd.Series.value_counts).T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = Adam(learning_rate=lr)\n#optimizer = mixed_precision.LossScaleOptimizer(optimizer, loss_scale='dynamic')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def pred_fn(model, data, threshold =0.1):\n\n    with tf.device('/gpu:0'):\n        pred_start, pred_end, pred_sent = model.predict((data[\"input_ids\"],\n                                              data[\"token_type_ids\"],\n                                              data[\"attention_mask\"]))\n        \n    return pred_start, pred_end, pred_sent\n\ndef decode_fn_basic(data, pred_start, pred_end, pred_sent, threshold =0.1, num_examples_to_show = 0):\n    \n    # For sentiment\n    setiment_pred_actual = pd.DataFrame({\"actual\": pd.Series(data[\"y_sent\"].argmax(axis=1)).map(rev_sentiment_map),\n                                         \"predicted\": pd.Series(tf.nn.softmax(pred_sent, axis=1).numpy().argmax(axis=1)).map(rev_sentiment_map)})\\\n                                        .assign(correct = lambda x: (x[\"actual\"] == x[\"predicted\"]).astype('int'))\n    print(\"\\nSentiment actual vs predictions count\")\n    print(pd.crosstab(setiment_pred_actual.actual, setiment_pred_actual.predicted))\n    print(\"\\nSentiment predictions accuracy\")\n    print(setiment_pred_actual.groupby(\"actual\")[\"correct\"].mean())\n    print(\"\\n\")\n    \n    predicted_text = []\n    current_text = []\n    outputs = []\n    for ix in range(len(pred_start)):\n        c_tok_len = data[\"tok_len\"][ix]\n        c_input_ids = data[\"input_ids\"][ix][4:c_tok_len-1]\n        c_offsets = data[\"offsets\"][ix][4:c_tok_len-1]\n        c_tokens = data[\"tokens\"][ix][4:c_tok_len-1]\n        c_pred_start = pred_start[ix,4:c_tok_len-1]\n        c_pred_end = pred_end[ix,4:c_tok_len-1]\n        c_pred_start_pos = tf.nn.softmax(c_pred_start).numpy().argmax()\n        c_pred_end_pos = tf.nn.softmax(c_pred_end).numpy().argmax()\n        if ix < num_examples_to_show:\n            print()\n            #print(c_input_ids)\n            print(tokenizer.decode(c_input_ids))\n            print(c_tokens)\n            print(c_pred_start)\n            print(c_pred_end)\n            print(c_pred_start_pos,c_pred_end_pos)\n            print(c_tokens[c_pred_start_pos:c_pred_end_pos+1])\n            print(c_offsets)\n        # Adjust for words that break into separate tokens\n        if c_pred_end_pos < c_pred_start_pos:\n            s_e_options = []\n            s_e_options.append(((c_pred_start_pos,c_pred_start_pos), \n                                c_pred_start[c_pred_start_pos] + c_pred_end[c_pred_start_pos] ))\n            s_e_options.append(((c_pred_end_pos,c_pred_end_pos), \n                                c_pred_start[c_pred_end_pos] + c_pred_end[c_pred_end_pos] ))\n            s_e_options.append(((0,len(c_pred_end)-1), \n                                c_pred_start[0] + c_pred_end[len(c_pred_end)-1] ))\n            if ix < num_examples_to_show:\n                print(\"c_pred_end_pos < c_pred_start_pos  - \", c_pred_start_pos, c_pred_end_pos)\n                print(sorted(s_e_options, key = lambda x: x[1], reverse = True))\n            \n            c_pred_start_pos, c_pred_end_pos = sorted(s_e_options, key = lambda x: x[1], reverse = True)[0][0]\n            \n            #c_pred_end_pos = c_pred_start_pos\n            \n        if (data[\"sentiment\"][ix] == \"neutral\") | ( len(data[\"text\"][ix].split()) < 2 ):\n            c_selected_text = data[\"text\"][ix]\n        else:\n            c_selected_text = data[\"text\"][ix][c_offsets[c_pred_start_pos][0]:c_offsets[c_pred_end_pos][1]]\n        if ix < num_examples_to_show:\n            print(c_pred_start_pos,c_pred_end_pos)\n            print(c_tokens[c_pred_start_pos:c_pred_end_pos+1])\n            print(data[\"sentiment\"][ix])\n            print(c_selected_text)\n        outputs.append(c_selected_text)\n    return outputs, setiment_pred_actual.predicted.values\n\n\ndef eval_fn(preds, labels):\n    j=[]\n    j_txt = []\n    for ix in range(len(preds)):\n        j.append( jaccard(preds[ix], labels[ix]) )\n        j_txt.append( jaccard_txt_only(preds[ix], labels[ix]) )\n        \n    return np.mean(j), np.mean(j_txt)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Train Model on 5-fold data**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read Test data\n\ntest = pd.read_csv(\"/kaggle/input/tweet-sentiment-extraction/test.csv\")\ntest[\"text\"] = test.text.str.strip().apply(lambda x: \" \" + x.lower())\ndisplay(test.loc[test.text.str.contains(\"   \")])\nprint(len(test))\n\ntest_data = Dataloader(test.textID.values, \n                       test.text.values, \n                       test.text.values, \n                       test.sentiment.values, \n                       tokenizer, \n                       max_len)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntest_data[\"y_sent\"]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create folds\nkf = KFold(n_splits = n_folds , shuffle = True, random_state=42)\nfold_pred = train.copy()\nfold_pred[\"pred_text\"] = \"\"\nfold_pred[\"pred_sentiment\"] = \"\"\n\ntest_pred_start = np.zeros((len(test),max_len), dtype=np.float32)\ntest_pred_end = np.zeros((len(test),max_len), dtype=np.float32)\n\nfor fold, (tr_idx, val_idx) in enumerate(kf.split(train[:num_data_points])):\n    best_score = 0\n    # Split data\n    print(\"\\n******************************* For Fold {} ***********************************\".format(fold+1))\n    train_data = Dataloader(train.iloc[tr_idx].textID.values, \n                               train.iloc[tr_idx].text.values, \n                               train.iloc[tr_idx].selected_text.values, \n                               train.iloc[tr_idx].sentiment.values, \n                               #np.asarray(pd.Series([\"sentiment\"]*train.iloc[tr_idx].shape[0])),\n                               tokenizer, \n                               max_len)\n\n    val_data = Dataloader(train.iloc[val_idx].textID.values, \n                               train.iloc[val_idx].text.values, \n                               train.iloc[val_idx].selected_text.values, \n                               train.iloc[val_idx].sentiment.values, \n                               #np.asarray(pd.Series([\"sentiment\"]*train.iloc[val_idx].shape[0])),\n                               tokenizer, \n                               max_len)\n\n    # Build and compile model\n    tweet_model = model(max_len, train_bert)\n    tweet_model.compile(optimizer= optimizer, \n                        loss = CategoricalCrossentropy(from_logits=True))\n    \n    # Sample weights for neutral\n    #sample_weights = np.ones(train_data[\"input_ids\"][:,1].shape)\n    #sample_weights[train_data[\"sentiment\"] == \"neutral\"] = 0.5\n    #sample_weights\n    \n    # Model training \n\n    with tf.device('/gpu:0'):\n        for e in range(num_epochs):\n            st = time.time()\n            tweet_model.fit((train_data[\"input_ids\"], train_data[\"token_type_ids\"], train_data[\"attention_mask\"]),\n                            (train_data[\"start_vector\"],train_data[\"end_vector\"],train_data[\"y_sent\"]),\n                            epochs=1,\n                            batch_size=batch_size_,\n                            shuffle=True,\n                            #sample_weight = [sample_weights,sample_weights],\n                            )\n            print(\"Time taken {} mins\".format(str((time.time()-st)/60)[:5]))\n            #break\n            val_pred_s,val_pred_e, val_pred_sent = pred_fn(tweet_model, val_data, threshold =0.1)\n            val_preds, val_sents = decode_fn_basic(val_data, val_pred_s, val_pred_e, val_pred_sent, threshold =0.1)\n            current_score, current_score_txt = eval_fn(val_preds, val_data[\"selected_text\"])\n            print(\"Jaccard score: {}\".format( current_score ))\n            print(\"Jaccard score (Alphanumeric only): {}\".format( current_score_txt ))\n            #break\n\n            if current_score > best_score: # For final fold epoch\n                best_score = current_score_txt\n                tweet_model.save_weights(\"/kaggle/working/\" + f'fold_{fold+1}.h5')\n                # Fill validation predictions\n                fold_pred.loc[val_idx, \"pred_text\"] = val_preds\n                fold_pred.loc[val_idx, \"pred_sentiment\"] = val_sents\n                # Fill test predictions\n                test_pred_s, test_pred_e, test_pred_sent = pred_fn(tweet_model, test_data, threshold =0.1)\n            print()\n        #break\n    test_pred_start = test_pred_start + test_pred_s / n_folds\n    test_pred_end = test_pred_end + test_pred_e / n_folds\n    #break\n\nprint(\"\\nFor full fold pred dataset - Jaccard score is - \")\nprint(eval_fn(fold_pred[\"pred_text\"].apply(lambda x: x.strip()), fold_pred[\"selected_text\"]))\n\n# Decode test predictions\ntest_preds, test_sents = decode_fn_basic(test_data, test_pred_start, test_pred_end, test_pred_sent, threshold = 0.1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fold_pred.to_csv('fold_pred.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfor v in range(10):\n    #print(key)\n    print(val_data[\"tokens\"][v])\n    #print(len(val_data[\"tokens\"][v]))\n    #print(\"attention length : {}\".format(sum(val_data[\"attention_mask\"][v])))\n    print(val_data[\"text\"][v])\n    print(val_data[\"selected_text\"][v])\n    print(val_data[\"start_vector\"][v].argmax())\n    print(val_data[\"end_vector\"][v].argmax())\n    print(val_data[\"tokens\"][v][val_data[\"start_vector\"][v].argmax():val_data[\"end_vector\"][v].argmax()+1])\n    print(val_data[\"offsets\"][v][4:val_data[\"end_vector\"][v].argmax()+2])\n    print(val_data[\"text\"][v][val_data[\"offsets\"][v][val_data[\"start_vector\"][v].argmax()][0]:val_data[\"offsets\"][v][val_data[\"end_vector\"][v].argmax()][1]+1])\n    print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Analysis of bad predictions \n# val_analysis = pd.DataFrame({\"text\":fold_pred[\"text\"], \n#                              \"pred_text\":fold_pred[\"pred_text\"], \n#                              \"selected_text\": fold_pred[\"selected_text\"],\n#                              \"sentiment\" : fold_pred[\"sentiment\"]})\n# #print(val_analysis.dtypes)\n# val_analysis[\"jaccard\"] = val_analysis.apply(lambda row: jaccard(row[\"pred_text\"], row[\"selected_text\"]), axis=1)\n\n# for sent in [\"positive\",\"negative\",\"neutral\"]:\n#     print(\"*\"*40 + sent + \"*\"*40)\n#     display(val_analysis[val_analysis.sentiment==sent][[\"selected_text\",\"pred_text\",\"jaccard\"]].sort_values(\"jaccard\",ascending =False).head(10))\n#     print()\n#     display(val_analysis[(val_analysis.sentiment==sent) & (val_analysis.jaccard < 1)][[\"selected_text\",\"pred_text\",\"jaccard\"]].sort_values(\"jaccard\",ascending =False).head(20))\n#     print()\n#     display(val_analysis[(val_analysis.sentiment==sent) ][[\"selected_text\",\"pred_text\",\"jaccard\"]].sort_values(\"jaccard\").head(10))\n#     print()\n#     display(val_analysis[(val_analysis.sentiment==sent) & (val_analysis.jaccard > 0.5) ][[\"selected_text\",\"pred_text\",\"jaccard\"]].sort_values(\"jaccard\").head(20))\n#     print()\n#     display(val_analysis[(val_analysis.sentiment==sent) & (val_analysis.jaccard > 0) ][[\"selected_text\",\"pred_text\",\"jaccard\"]].sort_values(\"jaccard\").head(20))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv(\"/kaggle/input/tweet-sentiment-extraction/sample_submission.csv\")\nsub[\"selected_text\"] = [t.strip() for t in test_preds]\nsub[\"pred_sentiment\"] = test_sents\n\n\nsub.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.to_csv('test_predictions.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}