{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd \nimport numpy as np\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom sklearn.model_selection import StratifiedKFold\nfrom keras.layers import Dense, Flatten, Conv1D, Dropout, Input\nfrom keras.models import Model, load_model\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom transformers import *\nimport tokenizers","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data_path = '../input/tweet-sentiment-extraction'\npath = '../input/roberta-base'\npath2 = '../input/tf-roberta'\nmax_len = 128\nvocab_file = path + '/vocab.json'\nmerges_file = path + '/merges.txt'\nconfig_file = path2 + '/config-roberta-base.json'\npretrained_file = path2 + '/pretrained-roberta-base.h5'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#tokenizer for roberta\ntokenizer = tokenizers.ByteLevelBPETokenizer(\n    vocab_file, \n    merges_file, \n    lowercase=True,\n    add_prefix_space=True\n)\nsentiment_id = {'positive':tokenizer.encode('positive').ids[0], \n                'negative':tokenizer.encode('negative').ids[0], \n                'neutral':tokenizer.encode('neutral').ids[0]}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = pd.read_csv(data_path + '/train.csv')\ntrain_data.dropna(axis = 0,inplace=True)\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = pd.read_csv(data_path + '/test.csv')\ntest_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Tokenizing the training data (input data formating for training)\n\ntrain_data.reset_index(inplace=True)\ntot_tw = train_data.shape[0]\n\ninput_ids = np.ones((tot_tw, max_len), dtype='int32')\nattention_mask = np.zeros((tot_tw, max_len), dtype='int32')\ntoken_type_ids = np.zeros((tot_tw, max_len), dtype='int32')\nstart_mask = np.zeros((tot_tw, max_len), dtype='int32')\nend_mask = np.zeros((tot_tw, max_len), dtype='int32')\n\nfor i in range(tot_tw):\n    set1 = \" \"+\" \".join(train_data.loc[i,'text'].split())\n    set2 = \" \".join(train_data.loc[i,'selected_text'].split())\n    idx = set1.find(set2)\n    set2_loc = np.zeros((len(set1)))\n    set2_loc[idx:idx+len(set2)]=1\n    if set1[idx-1]==\" \":\n        set2_loc[idx-1]=1\n  \n    enc_set1 = tokenizer.encode(set1)\n\n    selected_text_token_idx=[]\n    for k,(a,b) in enumerate(enc_set1.offsets):\n        sm = np.sum(set2_loc[a:b]) \n        if sm > 0:\n            selected_text_token_idx.append(k)\n\n    senti_token = sentiment_id[train_data.loc[i,'sentiment']]\n    input_ids[i,:len(enc_set1.ids)+5] = [0]+enc_set1.ids+[2,2]+[senti_token]+[2] \n    attention_mask[i,:len(enc_set1.ids)+5]=1\n\n    if len(selected_text_token_idx) > 0:\n        start_mask[i,selected_text_token_idx[0]+1]=1\n        end_mask[i, selected_text_token_idx[-1]+1]=1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Tokenizing the test data exactly the same way as the training data\n\ntot_test_tw = test_data.shape[0]\n\ninput_ids_t = np.ones((tot_test_tw,max_len), dtype='int32')\nattention_mask_t = np.zeros((tot_test_tw,max_len), dtype='int32')\ntoken_type_ids_t = np.zeros((tot_test_tw,max_len), dtype='int32')\n\nfor i in range(tot_test_tw):\n    set1 = \" \"+\" \".join(test_data.loc[i,'text'].split())\n    enc_set1 = tokenizer.encode(set1)\n\n    s_token = sentiment_id[test_data.loc[i,'sentiment']]\n    input_ids_t[i,:len(enc_set1.ids)+5]=[0]+enc_set1.ids+[2,2]+[s_token]+[2]\n    attention_mask_t[i,:len(enc_set1.ids)+5]=1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Metric Function\n# Categorical Cross Entropy with Label Smoothing\n# Label Smoothing is done to enhance accuracy\ndef custom_loss(y_true, y_pred):\n    loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred, from_logits = False, label_smoothing = 0.20)\n    loss = tf.reduce_mean(loss)\n    return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Function for building the model\ndef build_model():\n        ids = tf.keras.layers.Input((max_len,), dtype=tf.int32)\n        att = tf.keras.layers.Input((max_len,), dtype=tf.int32)\n        tok =  tf.keras.layers.Input((max_len,), dtype=tf.int32) \n\n        config_path = RobertaConfig.from_pretrained(config_file)\n        # I used a pre-trained model here\n        roberta_model = TFRobertaModel.from_pretrained(pretrained_file, config=config_path)\n        x = roberta_model(ids, attention_mask = att, token_type_ids=tok)\n        \n        x1 = tf.keras.layers.Dropout(0.05)(x[0])\n        x1 = tf.keras.layers.Conv1D(128, 2,padding='same')(x1)  #128 is the no. of filters and 2 is the kernel size of each filter\n        x1 = tf.keras.layers.LeakyReLU()(x1)\n        x1 = tf.keras.layers.Conv1D(64, 2,padding='same')(x1)\n        x1 = tf.keras.layers.Dense(1)(x1)\n        x1 = tf.keras.layers.Flatten()(x1)\n        x1 = tf.keras.layers.Activation('softmax')(x1)\n    \n        x2 = tf.keras.layers.Dropout(0.05)(x[0]) \n        x2 = tf.keras.layers.Conv1D(128, 2,padding='same')(x2)\n        x2 = tf.keras.layers.LeakyReLU()(x2)\n        x2 = tf.keras.layers.Conv1D(64, 2, padding='same')(x2)\n        x2 = tf.keras.layers.Dense(1)(x2)\n        x2 = tf.keras.layers.Flatten()(x2)\n        x2 = tf.keras.layers.Activation('softmax')(x2)\n\n\n        model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n        optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5) \n        model.compile(loss=custom_loss, optimizer=optimizer)\n\n        return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Training the model with 5 stratified kFolds\n\npred_start= np.zeros((input_ids_t.shape[0],max_len))\npred_end= np.zeros((input_ids_t.shape[0],max_len))\n\nskf = StratifiedKFold(n_splits=5,shuffle=True,random_state=777)\n\nfor i,(idxT,idxV) in enumerate(skf.split(input_ids,train_data.sentiment.values)):\n    print('--'*20)\n    print('-- FOLD %i --'%(i+1))\n    print('--'*20)\n    K.clear_session()\n    model = build_model()\n    '''\n    sv = tf.keras.callbacks.ModelCheckpoint(\n        'roberta-%i.h5'%(i), monitor='val_loss', verbose=1, save_best_only=True,\n        save_weights_only=True, mode='auto', save_freq='epoch')\n        \n    model.fit([input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]], [start_tokens[idxT,], end_tokens[idxT,]], \n        epochs=3, batch_size=32, verbose=DISPLAY, callbacks=[sv],\n        validation_data=([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]], \n        [start_tokens[idxV,], end_tokens[idxV,]]))\n    '''\n    model.load_weights('../input/tweet-model-data/roberta-%i.h5'%(i))\n    pred = model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=1)\n    pred_start = pred_start + (pred[0]/5)\n    pred_end = pred_end + (pred[1]/5) # 5 is the total no. of splits here","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all = []\nfor k in range(input_ids_t.shape[0]):\n    a = np.argmax(pred_start[k,])\n    b = np.argmax(pred_end[k,])\n    if a>b: \n        st = test_data.loc[k,'text'] \n    else:\n        text1 = \" \"+\" \".join(test_data.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode(enc.ids[a-1:b])\n    all.append(st)\ntest_data['selected_text']=all\ntest_data.head(20)\ntest_data[['textID','selected_text']].to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}