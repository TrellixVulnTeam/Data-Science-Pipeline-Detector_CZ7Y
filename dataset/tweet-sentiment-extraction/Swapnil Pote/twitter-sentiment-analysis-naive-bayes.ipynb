{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport re\nimport nltk\nimport string\n\nfrom nltk.corpus import stopwords","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_trn = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv')\ndf_test = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv')\n\nprint('Training data shape: ', df_trn.shape)\nprint('Testing data shape: ', df_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# First few rows of the training dataset\ndf_trn.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_trn['sentiment'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# First few rows of the testing dataset\ndf_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The columns denote the following:\n\n*     The textID of a tweet\n*     The text of a tweet\n*     The selected text which determines the polarity of the tweet\n*     sentiment of the tweet\n\nThe test dataset doesn't have the selected text column which needs to be identified.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Missing values in training set\ndf_trn.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Missing values in test set\ndf_test.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_trn[df_trn.text.isnull()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping missing values\ndf_trn.dropna(axis=0, inplace=True)\n\nprint('Training data shape: ', df_trn.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let's analyse and see how the Sentiment column looks like. I have only used the training dataset but the process will remain the same if we wish to to do it for the test dataset as well.\n\nLet's look at an example of each sentiment: Positive, negative and neutral","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Positive tweet\nprint(\"Positive Tweet example :\", df_trn[df_trn['sentiment']=='positive']['text'].values[0])\n#negative_text\nprint(\"Negative Tweet example :\", df_trn[df_trn['sentiment']=='negative']['text'].values[0])\n#neutral_text\nprint(\"Neutral tweet example  :\", df_trn[df_trn['sentiment']=='neutral']['text'].values[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of the Sentiment Column\n\ndf_trn['sentiment'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# It'll be better if we could get a relative percentage instead of the count.\n\ndf_trn['sentiment'].value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categories = df_trn['sentiment'].value_counts(normalize=True)\nplt.figure(figsize=(10,5))\nsns.barplot(categories.index, categories.values, alpha=0.8)\nplt.title('Distribution of Sentiment column in the training set')\nplt.ylabel('Percentage', fontsize=12)\nplt.xlabel('Category', fontsize=12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# It'll be better if we could get a relative percentage instead of the count.\n\ndf_test['sentiment'].value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categories = df_test['sentiment'].value_counts(normalize=True)\nplt.figure(figsize=(10,5))\nsns.barplot(categories.index, categories.values, alpha=0.8)\nplt.title('Distribution of Sentiment column in the testing set')\nplt.ylabel('Percentage', fontsize=12)\nplt.xlabel('Category', fontsize=12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before we start with any NLP project we need to pre-process the data to get it all in a consistent format.We need to clean, tokenize and convert our data into a matrix. Let's create a function which will perform the following tasks on the text columns:\n\n*     Make text lowercase,\n*     removes hyperlinks,\n*     remove punctuation\n*     removes numbers\n*     tokenizes\n*     removes stopwords","execution_count":null},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"sentences = ' '.join(x.lower() for x in df_trn['text'].values)\nprint(sentences)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"special_words = re.findall('[0-9a-z%s]+' % string.punctuation, sentences)\nprint(sorted(set(special_words)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_trn['text_clean'] = df_trn['text'].str.lower()\n\ndf_trn.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"df_trn['text_clean'] = df_trn['text_clean'].apply(lambda x: re.sub('https?://\\S+|www\\.\\S+', ' url ', x))\n\nclean_sentences = ' '.join(x for x in df_trn['text_clean'].values)\nspecial_words = re.findall('[0-9a-z%s]+' % string.punctuation, clean_sentences)\nprint(sorted(set(special_words)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"df_trn['text_clean'] = df_trn['text_clean'].apply(lambda x: re.sub('#[\\w\\d]+', ' hashtag ', x))\n\nclean_sentences = ' '.join(x for x in df_trn['text_clean'].values)\nspecial_words = re.findall('[0-9a-z%s]+' % string.punctuation, clean_sentences)\nprint(sorted(set(special_words)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"df_trn['text_clean'] = df_trn['text_clean'].apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), ' ', x))\n\nclean_sentences = ' '.join(x.lower() for x in df_trn['text_clean'].values)\nspecial_words = re.findall('[0-9a-z%s]+' % string.punctuation, clean_sentences)\nprint(sorted(set(special_words)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"df_trn['text_clean'] = df_trn['text_clean'].apply(lambda x: re.sub('\\n', ' ', x))\ndf_trn['text_clean'] = df_trn['text_clean'].apply(lambda x: re.sub('\\w*\\d\\w*', ' number ', x))\ndf_trn['text_clean'] = df_trn['text_clean'].apply(lambda x: re.sub('\\s+', ' ', x))\n\ndf_trn.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(stopwords.words('english'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(nltk.tokenize.word_tokenize(df_trn['text_clean'][5]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tk = nltk.tokenize.word_tokenize(df_trn['text_clean'][5])\nrs = []\nfor w in tk:\n    if w not in stopwords.words('english'):\n        rs.append(w)\n\nprint(df_trn['text_clean'][5])\nprint(' '.join(rs))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def text_preprocessing(text):\n    \"\"\"\n    Cleaning and parsing the text.\n\n    \"\"\"\n    tokenized_text = nltk.tokenize.word_tokenize(text)\n    remove_stopwords = [w for w in tokenized_text if w not in stopwords.words('english')]\n    combined_text = ' '.join(remove_stopwords)\n    return combined_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applying the cleaning function to both test and training datasets\ndf_trn['text_clean'] = df_trn['text_clean'].apply(str).apply(lambda x: text_preprocessing(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_trn.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of words\ndf_trn['word_count'] = df_trn['text'].apply(lambda x: len(str(x).split(\" \")))\ndf_trn[['text','word_count']].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of char\ndf_trn['char_count'] = df_trn['text'].str.len() ## this also includes spaces\ndf_trn[['text','char_count']].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pos = df_trn[df_trn['sentiment']=='positive']\nneg = df_trn[df_trn['sentiment']=='negative']\nneutral = df_trn[df_trn['sentiment']=='neutral']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categories = pos['char_count'].value_counts(normalize=True)\nplt.figure(figsize=(50, 25))\nsns.barplot(categories.index, categories.values, alpha=0.8)\nplt.title('Positive Text Length Distribution')\nplt.ylabel('Count', fontsize=20)\nplt.xlabel('Char length', fontsize=20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categories = neg['char_count'].value_counts(normalize=True)\nplt.figure(figsize=(50, 25))\nsns.barplot(categories.index, categories.values, alpha=0.8)\nplt.title('Negative Text Length Distribution')\nplt.ylabel('Count', fontsize=20)\nplt.xlabel('Char length', fontsize=20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categories = neutral['char_count'].value_counts(normalize=True)\nplt.figure(figsize=(50, 25))\nsns.barplot(categories.index, categories.values, alpha=0.8)\nplt.title('Neutral Text Length Distribution')\nplt.ylabel('Count', fontsize=20)\nplt.xlabel('Char length', fontsize=20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Average word length\n\ndef avg_word(sentence):\n    words = sentence.split()\n    return (sum(len(word) for word in words)/len(words))\n\ndf_trn['avg_word'] = df_trn['text'].apply(lambda x: avg_word(x))\ndf_trn[['text','avg_word']].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Stop words present in tweet\nstop = stopwords.words('english')\n\ndf_trn['stopwords'] = df_trn['text'].apply(lambda x: len([x for x in x.split() if x in stop]))\ndf_trn[['text','stopwords']].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hashtag present in tweet\n\ndf_trn['hastags'] = df_trn['text'].apply(lambda x: len([w for w in x.split() if w.startswith('#')]))\ndf_trn[['text','hastags']].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_trn['numerics'] = df_trn['text'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\ndf_trn[['text','numerics']].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}