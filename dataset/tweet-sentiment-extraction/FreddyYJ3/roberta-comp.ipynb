{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"max_seq_length = 128  # Your choice here.\nimport tensorflow_hub as hub\nimport tensorflow as tf\nimport math\nimport transformers\nfrom sklearn.model_selection import StratifiedKFold\nimport re\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN = 96\nPATH = '../input/tf-roberta/'\ntokenizer = transformers.RobertaTokenizer(\n    vocab_file=PATH+'vocab-roberta-base.json', \n    merges_file=PATH+'merges-roberta-base.txt', \n    lowercase=True,\n    add_prefix_space=True\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train=pd.read_csv('../input/tweet-sentiment-extraction/train.csv')\ntest=pd.read_csv('../input/tweet-sentiment-extraction/test.csv')\ntrain.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train.set_index('textID')\n#test.set_index('textID')\ntrain['text']=train['text'].fillna(\" \")\ntest['text']=test['text'].fillna(\" \")\ntrain['selected_text']=train['selected_text'].fillna(\" \")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stop_words = set(stopwords.words('english'))\nfor i in range(len(train['selected_text'])):\n    text=train['selected_text'][i]\n    text=text.strip()\n#     text=text.lower()\n#     text=re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))''', \" \", text)\n#     text=' '.join(re.sub(\"(@[A-Za-z0-9]+)\",\" \",text).split())\n    train['selected_text'][i]=text\ntrain.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x = train['text'].tolist()\n# train_x = np.array(train_x, dtype=object)[:, np.newaxis]\ntrain_y = train['selected_text'].tolist()\n\ntest_x = test['text'].tolist()\n# test_x = np.array(test_x, dtype=object)[:, np.newaxis]\n# test_y = test['selected_text'].tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for i in range(len(train_y)):\n#     train_y[i]=train_y[i].strip()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentiment=train['sentiment'].tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def map_example_to_dict(input_ids, attention_masks,type_ids):\n  return [\n      tf.convert_to_tensor(input_ids),\n      tf.convert_to_tensor(attention_masks),\n      tf.convert_to_tensor(type_ids)\n  ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_token=[]\ntest_token=[]\nfor i in range(len(train_x)):\n    train_token.append(tokenizer.encode_plus(train_x[i],pad_to_max_length=True,max_length=max_seq_length,return_token_type_ids=True))\nfor i in range(len(test_x)):\n    test_token.append(tokenizer.encode_plus(test_x[i],pad_to_max_length=True,max_length=max_seq_length,return_token_type_ids=True))\nprint(train_token[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_ids=[]\nattention_mask=[]\ntype_ids=[]\nfor i in train_token:\n    input_ids.append(tf.reshape(i['input_ids'],(-1,max_seq_length)))\n    attention_mask.append(tf.reshape(i['attention_mask'],(-1,max_seq_length)))\n    type_ids.append(tf.reshape(i['token_type_ids'],(-1,max_seq_length)))\ntrain_input=map_example_to_dict(input_ids,attention_mask,type_ids)\nprint(len(train_input[0]))\n#print(len(train_input[0][0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_ids=[]\nattention_mask=[]\ntype_ids=[]\nfor i in test_token:\n    input_ids.append(tf.reshape(i['input_ids'],(-1,max_seq_length)))\n    attention_mask.append(tf.reshape(i['attention_mask'],(-1,max_seq_length)))\n    type_ids.append(tf.reshape(i['token_type_ids'],(-1,max_seq_length)))\ntest_input=map_example_to_dict(input_ids,attention_mask,type_ids)\nprint(len(test_input[0]))\n#print(len(test_input[0][0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ids = train_input[0]\nmasks = train_input[1]\ntoken_ids=train_input[2]\n\nids = tf.reshape(ids, (-1, max_seq_length,))\nprint(\"Input ids shape: \", ids.shape)\nmasks = tf.reshape(masks, (-1, max_seq_length,))\nprint(\"Input Masks shape: \", masks.shape)\ntoken_ids = tf.reshape(token_ids, (-1, max_seq_length,))\nprint(\"Token Ids shape: \", token_ids.shape)\n\nids=ids.numpy()\nmasks = masks.numpy()\ntoken_ids=token_ids.numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_ids = test_input[0]\ntest_masks = test_input[1]\ntest_token_ids=test_input[2]\n\ntest_ids = tf.reshape(test_ids, (-1, max_seq_length,))\nprint(\"Input ids shape: \", test_ids.shape)\ntest_masks = tf.reshape(test_masks, (-1, max_seq_length,))\nprint(\"Input Masks shape: \", test_masks.shape)\ntest_token_ids = tf.reshape(test_token_ids, (-1, max_seq_length,))\nprint(\"Token Ids shape: \", test_token_ids.shape)\n\ntest_ids=test_ids.numpy()\ntest_masks = test_masks.numpy()\ntest_token_ids=test_token_ids.numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ct = train.shape[0]\nstart_tokens = np.zeros((ct,max_seq_length),dtype='int32')\nend_tokens = np.zeros((ct,max_seq_length),dtype='int32')\n\nfor k in range(train.shape[0]):\n    # FIND OVERLAP\n    text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n    text2 = \" \".join(train.loc[k,'selected_text'].split())\n    idx = text1.find(text2)\n    chars = np.zeros((len(text1)))\n    chars[idx:idx+len(text2)]=1\n    if text1[idx-1]==' ': chars[idx-1] = 1 \n    enc = tokenizer.encode(text1) \n        \n    # ID_OFFSETS\n    offsets = []; idx=0\n    for t in enc:\n        w = tokenizer.decode([t])\n        offsets.append((idx,idx+len(w)))\n        idx += len(w)\n    \n    # START END TOKENS\n    toks = []\n    for i,(a,b) in enumerate(offsets):\n        sm = np.sum(chars[a:b])\n        if sm>0: toks.append(i) \n            \n    if len(toks)>0:\n        start_tokens[k,toks[0]+1] = 1\n        end_tokens[k,toks[-1]+1] = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    if (len(a)==0) & (len(b)==0): return 0.5\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"config = transformers.RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\nbert_model = transformers.TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\nbert_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5),loss='categorical_crossentropy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\n\ndef save_weights(model, dst_fn):\n    weights = model.get_weights()\n    with open(dst_fn, 'wb') as f:\n        pickle.dump(weights, f)\n\n\ndef load_weights(model, weight_fn):\n    with open(weight_fn, 'rb') as f:\n        weights = pickle.load(f)\n    model.set_weights(weights)\n    return model\n\ndef loss_fn(y_true, y_pred):\n    # adjust the targets for sequence bucketing\n    ll = tf.shape(y_pred)[1]\n    y_true = y_true[:, :ll]\n    loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred,\n        from_logits=False, label_smoothing=LABEL_SMOOTHING)\n    loss = tf.reduce_mean(loss)\n    return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model():\n    input_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=np.int32)\n    attention_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=np.int32)\n    token_type = tf.keras.layers.Input(shape=(max_seq_length,), dtype=np.int32)\n    padding = tf.cast(tf.equal(ids, 1), tf.int32)\n    \n    lens = max_seq_length - tf.reduce_sum(padding, -1)\n    max_len_ = tf.reduce_max(lens)\n    ids_ = input_ids[:, :max_len_]\n    att_ = attention_mask[:, :max_len_]\n    tok_ = token_type[:, :max_len_]\n    bert_layer = bert_model(ids_, attention_mask=att_,token_type_ids=tok_)[0]\n\n    x1 = tf.keras.layers.Dropout(0.1)(bert_layer) \n    x1 = tf.keras.layers.Conv1D(768,2,padding='same')(x1)\n    x1 = tf.keras.layers.LeakyReLU()(x1)\n    x1 = tf.keras.layers.Conv1D(64, 2,padding='same')(x1)\n    x1 = tf.keras.layers.Dense(1)(x1)\n    x1 = tf.keras.layers.Flatten()(x1)\n    x1 = tf.keras.layers.Activation('softmax')(x1)\n\n    x2 = tf.keras.layers.Dropout(0.1)(bert_layer) \n    x2 = tf.keras.layers.Conv1D(768, 2,padding='same')(x2)\n    x2 = tf.keras.layers.LeakyReLU()(x2)\n    x2 = tf.keras.layers.Conv1D(64, 2, padding='same')(x2)\n    x2 = tf.keras.layers.Dense(1)(x2)\n    x2 = tf.keras.layers.Flatten()(x2)\n    x2 = tf.keras.layers.Activation('softmax')(x2)\n\n    model = tf.keras.Model(inputs=[input_ids, attention_mask,token_type], outputs=[x1,x2])\n    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5) \n    model.compile(loss=loss_fn, optimizer=optimizer)\n    \n    x1_padded = tf.pad(x1, [[0, 0], [0, max_seq_length - max_len_]], constant_values=0.)\n    x2_padded = tf.pad(x2, [[0, 0], [0, max_seq_length - max_len_]], constant_values=0.)\n    \n    padded_model = tf.keras.models.Model(inputs=[input_ids, attention_mask, token_type], outputs=[x1_padded,x2_padded])\n    return model,padded_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"earlystop_callback = tf.keras.callbacks.EarlyStopping(monitor='val_categorical_accuracy', min_delta=0.0, patience=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"jac = []; VER='v0'; DISPLAY=1 # USE display=1 FOR INTERACTIVE\noof_start = np.zeros((ids.shape[0],max_seq_length))\noof_end = np.zeros((ids.shape[0],max_seq_length))\npreds_start = np.zeros((test_ids.shape[0],max_seq_length))\npreds_end = np.zeros((test_ids.shape[0],max_seq_length))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EPOCHS = 3 # originally 3\nBATCH_SIZE = 32 # originally 32\nPAD_ID = 1\nSEED = 88888\nLABEL_SMOOTHING = 0.1\ntf.random.set_seed(SEED)\nnp.random.seed(SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\n\nskf = StratifiedKFold(n_splits=5,shuffle=True,random_state=random.randint(100,999))\nfor fold,(idxT,idxV) in enumerate(skf.split(ids,sentiment)):\n    \n    tf.keras.backend.clear_session()\n    model,padded_model = build_model()\n    sv = tf.keras.callbacks.ModelCheckpoint(\n        '%s-roberta-%i.h5'%(VER,fold), monitor='val_loss', verbose=1, save_best_only=True,\n        save_weights_only=True, mode='auto', save_freq='epoch')\n    \n    inpT = [ids[idxT,], masks[idxT,], token_ids[idxT,]]\n    targetT = [start_tokens[idxT,], end_tokens[idxT,]]\n    inpV = [ids[idxV,],masks[idxV,],token_ids[idxV,]]\n    targetV = [start_tokens[idxV,], end_tokens[idxV,]]\n    # sort the validation data\n    shuffleV = np.int32(sorted(range(len(inpV[0])), key=lambda k: (inpV[0][k] == 1).sum(), reverse=True))\n    inpV = [arr[shuffleV] for arr in inpV]\n    targetV = [arr[shuffleV] for arr in targetV]\n    weight_fn = '../input/roberta2conv1dmodel/%s-roberta-%i.h5'%(VER,fold)\n    #weight_fn = '%s-roberta-%i.h5'%(VER,fold)\n    for epoch in range(1, EPOCHS + 1):\n        # sort and shuffle: We add random numbers to not have the same order in each epoch\n        shuffleT = np.int32(sorted(range(len(inpT[0])), key=lambda k: (inpT[0][k] == PAD_ID).sum() + np.random.randint(-3, 3), reverse=True))\n        # shuffle in batches, otherwise short batches will always come in the beginning of each epoch\n        num_batches = math.ceil(len(shuffleT) / BATCH_SIZE)\n        batch_inds = np.random.permutation(num_batches)\n        shuffleT_ = []\n        for batch_ind in batch_inds:\n            shuffleT_.append(shuffleT[batch_ind * BATCH_SIZE: (batch_ind + 1) * BATCH_SIZE])\n        shuffleT = np.concatenate(shuffleT_)\n        # reorder the input data\n        inpT = [arr[shuffleT] for arr in inpT]\n        targetT = [arr[shuffleT] for arr in targetT]\n#         model.fit(inpT, targetT, \n#             epochs=epoch, initial_epoch=epoch - 1, batch_size=BATCH_SIZE, verbose=DISPLAY,\n#             validation_data=(inpV, targetV), shuffle=False)  # don't shuffle in `fit`\n#         save_weights(model, weight_fn)\n    \n    load_weights(model, weight_fn)\n    oof_start[idxV,],oof_end[idxV,] = padded_model.predict([ids[idxV,],masks[idxV,],token_ids[idxV,]],verbose=DISPLAY)\n    preds = padded_model.predict([test_ids,test_masks,test_token_ids],verbose=DISPLAY)\n    preds_start += preds[0]/skf.n_splits\n    preds_end += preds[1]/skf.n_splits\n\n    # DISPLAY FOLD JACCARD\n    all = []\n    for k in idxV:\n        a = np.argmax(oof_start[k,])\n        b = np.argmax(oof_end[k,])\n        if a>b: \n            text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n            enc = tokenizer.encode(text1)\n            st = tokenizer.decode(enc[a-1:])\n        else:\n            text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n            enc = tokenizer.encode(text1)\n            st = tokenizer.decode(enc[a-1:b])\n        all.append(jaccard(st,train.loc[k,'selected_text']))\n    jac.append(np.mean(all))\n    print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all))\n    print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('>>>> OVERALL CV Jaccard =',np.mean(jac))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all = []\nfor k in range(test_ids.shape[0]):\n    a = np.argmax(preds_start[k,])\n    b = np.argmax(preds_end[k,])\n    if a>b: \n        #st = test.loc[k,'text']\n        text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode(enc[b:a])\n    else:\n        text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        \n        st = tokenizer.decode(enc[a:b])\n        \n    all.append(st)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result=[]\nfor i in all:\n    result.append(\"%s\"%i)\nfor i in range(len(result)):\n    text=result[i]\n    text=text.lower()\n    text=re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))''', \" \", text)\n    text=' '.join(re.sub(\"(@[A-Za-z0-9]+)\",\" \",text).split())\n    tokenized=word_tokenize(text)\n    preprocessed=[]\n#     for j in tokenized:\n#         if j not in stop_words:\n#             preprocessed.append(j)\n#     text=' '.join(preprocessed)\n#    text=remove_all_consecutive(text)\n    alphanumeric=\"\"\n    for character in text:\n        if character not in [\"`\",\",\",'(',')','!',\"'\",'.','?']:\n            alphanumeric += character\n        else:\n            alphanumeric+=' '\n#     text=alphanumeric\n    text=text.strip()\n    result[i]=text\n\n#     #text=''.join(i for i in text if (not i.isdigit() or i==\" \"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['selected_text'] = result\ntest[['textID','selected_text']].to_csv('submission.csv',index=False)\npd.set_option('max_colwidth', 60)\ntest.sample(25)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}