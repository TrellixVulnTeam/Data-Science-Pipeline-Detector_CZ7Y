{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n#**Importing important packages**\n","metadata":{"id":"pD53i-v_ofS6"}},{"cell_type":"code","source":"import re\nimport nltk\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nnltk.download('wordnet')\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix, classification_report","metadata":{"id":"MBRr2WuZn5rT","outputId":"c3c8ff58-a000-4864-a517-a5d1161d08ec","execution":{"iopub.status.busy":"2021-07-22T22:44:40.982586Z","iopub.execute_input":"2021-07-22T22:44:40.983471Z","iopub.status.idle":"2021-07-22T22:44:40.994162Z","shell.execute_reply.started":"2021-07-22T22:44:40.983427Z","shell.execute_reply":"2021-07-22T22:44:40.992994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Loading Training Dataset**","metadata":{"id":"iPRFAIHpv3d_"}},{"cell_type":"code","source":"d_train = pd.read_csv('../input/tweet-sentiment-extraction/train.csv')\nd_train.info()\nprint('\\n-------------------------------------------------------------------------------------------------------\\n')\nd_train.head()","metadata":{"id":"zFwhxhnBwANH","outputId":"ed980417-a247-494f-b545-b12653572715","execution":{"iopub.status.busy":"2021-07-22T22:44:40.995917Z","iopub.execute_input":"2021-07-22T22:44:40.996334Z","iopub.status.idle":"2021-07-22T22:44:41.35094Z","shell.execute_reply.started":"2021-07-22T22:44:40.996297Z","shell.execute_reply":"2021-07-22T22:44:41.349806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As per the information about the following dataset, there is one null entry under ' text ' and ' selected_text ' coloumn, we have to remove that entire row.","metadata":{"id":"jB-zQMZrwQDg"}},{"cell_type":"code","source":"#checking for null values in dataset\nnull_row = pd.isnull(d_train['text'])\nd_train[null_row]","metadata":{"id":"68JZm5AMwjKF","outputId":"2ac28b5a-9429-4895-d574-a4c0df2f8189","execution":{"iopub.status.busy":"2021-07-22T22:44:41.353024Z","iopub.execute_input":"2021-07-22T22:44:41.353461Z","iopub.status.idle":"2021-07-22T22:44:41.371369Z","shell.execute_reply.started":"2021-07-22T22:44:41.353405Z","shell.execute_reply":"2021-07-22T22:44:41.370129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The row with index no. 314 have null entries.","metadata":{"id":"X7RJEmLyxBGJ"}},{"cell_type":"code","source":"#removing the values with null entries\nd_train.drop([d_train.index[314]] , inplace=True)\n\n#again getting info of dataset\nd_train.info()","metadata":{"id":"2Varo9pLw_2t","outputId":"caa616fe-befa-4b99-b103-bad7df2f6f59","execution":{"iopub.status.busy":"2021-07-22T22:44:41.373266Z","iopub.execute_input":"2021-07-22T22:44:41.373593Z","iopub.status.idle":"2021-07-22T22:44:41.409577Z","shell.execute_reply.started":"2021-07-22T22:44:41.373562Z","shell.execute_reply":"2021-07-22T22:44:41.408412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Hence now we can see that our data has no null entries.**","metadata":{"id":"IQyD0j99xTJs"}},{"cell_type":"code","source":"#storing important columns as tweets and sentiments\ntrain_tweets = d_train['text']\ntrain_sentiments = d_train['sentiment']\n\n#check for imbalance or balance dataset\ntrain_sentiments.value_counts()","metadata":{"id":"7xWJ6VpSxZIg","outputId":"1d34062e-3d06-4d4f-e569-5b8ef5a2c158","execution":{"iopub.status.busy":"2021-07-22T22:44:41.411275Z","iopub.execute_input":"2021-07-22T22:44:41.411903Z","iopub.status.idle":"2021-07-22T22:44:41.431477Z","shell.execute_reply.started":"2021-07-22T22:44:41.411851Z","shell.execute_reply":"2021-07-22T22:44:41.429965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Our dataset is also fairly balanced so we dont have to do upsampling or downsampling.**","metadata":{"id":"9ZaVOWmdyVM8"}},{"cell_type":"markdown","source":"# **Text Preprocessing(TRAINING DATA)**\nIt is the most important process for text classification problems as it decreases the the size of our dataset corpus be removing such words that do not add much value to our system and it also simplifies our data.\n---\n**We'll be doing that in following order**:\n\n\n1.   Every word of our data will be converted to lower case.\n2.   Removing all usernames, for eg. @abc.\n3.   Removing all the websites and URL's.\n4.   Replacing all the emojis by their respective emotion in words.\n5.   Removing all the alphnumeric words.\n6.   Replace all the alphabets of words that occur more than twice to two       times. For eg. 'reallly' to 'really'  \n7.   Removing stopwords.\n8.   Removing words of length of smaller than 2. \n9.   Lemmatization\n10.   Removing all the punctuation marks.\n","metadata":{"id":"IbHHMNwgzoYt"}},{"cell_type":"code","source":"semi_final_train_tweets = []\n#converting all the words of tweet into lower case \nfor tweet in train_tweets.values :    \n    tweet = tweet.lower()\n\n#removing all websites and URL's\n    tweet = re.sub(r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\" , '' , tweet) \n\n#removing usernames\n    tweet = re.sub('@[^\\s]+', '' , tweet)\n\n#replacing emojis\n    tweet = re.sub(r'(<3|:\\*)', ' smile ', tweet)\n    tweet = re.sub(r'(:\\s?D|:-D|x-?D|X-?D)', ' win ', tweet)\n    tweet = re.sub(r'(:\\s?\\)|:-\\)|\\(\\s?:|\\(-:|:\\'\\)|:O)', ' happy ', tweet)\n    tweet = re.sub(r'(;-?\\)|;-?D|\\(-?;|@-\\))', ' smile ', tweet)\n    tweet = re.sub(r'(:,\\(|:\\'\\(|:\"\\()', ' vampire ', tweet) \n    tweet = re.sub(r'(:\\s?\\(|:-\\(|\\)\\s?:|\\)-:|:-/|:-\\|)', ' sad ', tweet)\n\n#removing all the alphanumeric words\n    tweet = re.sub(\"[^a-zA-Z]\", \" \", tweet)\n\n#replacing consecutive letters\n    tweet = re.sub(r\"(.)\\1\\1+\", r\"\\1\\1\", tweet)\n\n    semi_final_train_tweets.append(tweet)\n            ","metadata":{"id":"wm8kIFvVyqi9","execution":{"iopub.status.busy":"2021-07-22T22:44:41.433518Z","iopub.execute_input":"2021-07-22T22:44:41.434379Z","iopub.status.idle":"2021-07-22T22:44:42.748457Z","shell.execute_reply.started":"2021-07-22T22:44:41.434325Z","shell.execute_reply":"2021-07-22T22:44:42.747619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Now comes the part of removing stopwords.The list of stopwords that NLTK provides has some words which should not be considered as stopwords because removing them sometimes changes the complete meaning of sentence. For eg. 'not' , 'against' etc etc. So we'll process after removing such words from NLTK stopwords list.**\n---\n\n","metadata":{"id":"jAXurdTIMqOP"}},{"cell_type":"code","source":"stop_w=list(stopwords.words('english'))\nstop_w.remove('not')\nstop_w.remove('against')\nstop_w.remove('doing')\nstop_w.remove('couldn')\nstop_w.remove('didn')\nstop_w.remove('doesn')\nstop_w.remove('hadn')\nstop_w.remove('hasn')\nstop_w.remove('haven')\nstop_w.remove('isn')\nstop_w.remove('mightn')\nstop_w.remove('mustn')\nstop_w.remove('needn')\nstop_w.remove('wasn')\nstop_w.remove('weren')\nstop_w.remove('won')\nstop_w.remove('wouldn')\n\n#defining lemmatizer\nlematizer = WordNetLemmatizer()","metadata":{"id":"V74yT-c_Mllu","execution":{"iopub.status.busy":"2021-07-22T22:44:42.751336Z","iopub.execute_input":"2021-07-22T22:44:42.751965Z","iopub.status.idle":"2021-07-22T22:44:42.762678Z","shell.execute_reply.started":"2021-07-22T22:44:42.751919Z","shell.execute_reply":"2021-07-22T22:44:42.761928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_train_tweets = []\n\nfor tweet in semi_final_train_tweets:\n    sentence = []   \n    tweet = tweet.split()\n    for each_word in tweet:\n        if len(each_word) >= 2:\n            if each_word not in stop_w:\n                each_word = lematizer.lemmatize(each_word)\n                sentence.append(each_word)\n        else:\n            continue\n             \n    string = ' '.join(sentence)   \n    final_train_tweets.append(string) ","metadata":{"id":"I_pMho1oNgZY","execution":{"iopub.status.busy":"2021-07-22T22:44:42.764354Z","iopub.execute_input":"2021-07-22T22:44:42.76496Z","iopub.status.idle":"2021-07-22T22:44:46.995239Z","shell.execute_reply.started":"2021-07-22T22:44:42.764914Z","shell.execute_reply":"2021-07-22T22:44:46.99424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_train_tweets[5]","metadata":{"id":"KtIsZtr4QM6R","outputId":"daa7d2fb-07e0-4306-c8ef-0d50ef816fd3","execution":{"iopub.status.busy":"2021-07-22T22:44:46.996626Z","iopub.execute_input":"2021-07-22T22:44:46.997313Z","iopub.status.idle":"2021-07-22T22:44:47.005905Z","shell.execute_reply.started":"2021-07-22T22:44:46.997266Z","shell.execute_reply":"2021-07-22T22:44:47.004593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Text preprocessing of been training data has finally been done. We had to follow the same process for test data also.","metadata":{"id":"by2bG8mAOMkZ"}},{"cell_type":"markdown","source":"# **Loading Test Dataset**","metadata":{"id":"mtHA3qMMO4rT"}},{"cell_type":"code","source":"d_test = pd.read_csv('../input/tweet-sentiment-extraction/test.csv')\nd_test.info()\nprint('\\n-------------------------------------------------------------------------------------------------------\\n')\nd_test.head()\ntest_tweets = d_test['text']\ntest_sentiments = d_test['sentiment']","metadata":{"id":"TMbC4D9MOx44","outputId":"81630b3c-9809-42d6-e498-ff6171183b84","execution":{"iopub.status.busy":"2021-07-22T22:44:47.007721Z","iopub.execute_input":"2021-07-22T22:44:47.008239Z","iopub.status.idle":"2021-07-22T22:44:47.055644Z","shell.execute_reply.started":"2021-07-22T22:44:47.008188Z","shell.execute_reply":"2021-07-22T22:44:47.054649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Test data has no null values.","metadata":{"id":"29I3IHaYPP-2"}},{"cell_type":"markdown","source":"# **Test Preprocessing(TEST DATA)**","metadata":{"id":"4zuUCYFcPnwt"}},{"cell_type":"code","source":"final_test_tweets = []\n \nfor tweet in test_tweets.values :    \n    tweet = tweet.lower()\n    tweet = re.sub(r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\" , '' , tweet) \n    tweet = re.sub('@[^\\s]+', '' , tweet)\n    tweet = re.sub(r'(<3|:\\*)', ' smile ', tweet)\n    tweet = re.sub(r'(:\\s?D|:-D|x-?D|X-?D)', ' win ', tweet)\n    tweet = re.sub(r'(:\\s?\\)|:-\\)|\\(\\s?:|\\(-:|:\\'\\)|:O)', ' happy ', tweet)\n    tweet = re.sub(r'(;-?\\)|;-?D|\\(-?;|@-\\))', ' smile ', tweet)\n    tweet = re.sub(r'(:,\\(|:\\'\\(|:\"\\()', ' vampire ', tweet) \n    tweet = re.sub(r'(:\\s?\\(|:-\\(|\\)\\s?:|\\)-:|:-/|:-\\|)', ' sad ', tweet)\n    tweet = re.sub(\"[^a-zA-Z]\", \" \", tweet)\n    tweet = re.sub(r\"(.)\\1\\1+\", r\"\\1\\1\", tweet)\n    sentence = []   \n    tweet = tweet.split()\n    for each_word in tweet:\n        if len(each_word) >= 2:\n            if each_word not in stop_w:\n                each_word = lematizer.lemmatize(each_word)\n                sentence.append(each_word)\n        else:\n            continue\n             \n    string = ' '.join(sentence)   \n    final_test_tweets.append(string) \n\nfinal_test_tweets[2]    ","metadata":{"id":"cZ8tGWewPSNN","outputId":"95ee4688-516a-4785-ea81-696e05bebac0","execution":{"iopub.status.busy":"2021-07-22T22:44:47.056982Z","iopub.execute_input":"2021-07-22T22:44:47.057301Z","iopub.status.idle":"2021-07-22T22:44:47.500101Z","shell.execute_reply.started":"2021-07-22T22:44:47.05727Z","shell.execute_reply":"2021-07-22T22:44:47.499156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **TF-IDF Vectorisation of Training and Test Data**","metadata":{"id":"fGIH9jzTQh7i"}},{"cell_type":"code","source":"#converting train tweets to vectors\ntf_idf_vect = TfidfVectorizer(ngram_range=(1,2))\nx_train = tf_idf_vect.fit(final_train_tweets)\nx_train = tf_idf_vect.transform(final_train_tweets)\n\n\n#converting test tweets to vectors\nx_test = tf_idf_vect.transform(final_test_tweets)","metadata":{"id":"HhHc1XlTQxkB","execution":{"iopub.status.busy":"2021-07-22T22:44:47.501517Z","iopub.execute_input":"2021-07-22T22:44:47.501819Z","iopub.status.idle":"2021-07-22T22:44:49.609424Z","shell.execute_reply.started":"2021-07-22T22:44:47.501789Z","shell.execute_reply":"2021-07-22T22:44:49.608039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Classification using Multinomial Naive Bayes**","metadata":{"id":"WDI8chdsSIpE"}},{"cell_type":"code","source":"classifier = MultinomialNB()\nclassifier = classifier.fit(x_train , train_sentiments)\npred_sentiments = classifier.predict(x_test)\naccuracy = accuracy_score(pred_sentiments , test_sentiments)\nprint('Accuracy of MultinomialNB is ',accuracy)\nprint('\\n')\nprint(classification_report(test_sentiments,pred_sentiments))","metadata":{"id":"xMyQvmZqSH8s","outputId":"00854061-51d5-4cad-cf0f-0be325756f5c","execution":{"iopub.status.busy":"2021-07-22T22:44:49.610789Z","iopub.execute_input":"2021-07-22T22:44:49.611121Z","iopub.status.idle":"2021-07-22T22:44:49.879401Z","shell.execute_reply.started":"2021-07-22T22:44:49.611089Z","shell.execute_reply":"2021-07-22T22:44:49.8781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plotting confusion_matrix of MultinomialNB\nprint('Confusion Matrix of MultinomialNB is given below:')\ncf_matrix = confusion_matrix(test_sentiments,pred_sentiments)\n#sns.heatmap(cf_matrix/np.sum(cf_matrix) , figsize=(8,8) , annot=True , fmt='0.2%')\nplt.subplots(figsize=(10,10))\nsns.set(font_scale=2)\nsns.heatmap(cf_matrix/np.sum(cf_matrix), annot=True , cmap='BuPu',linecolor='white' , linewidths=3)\nplt.xlabel(\"Predicted values\", fontdict = {'size':22})\nplt.ylabel(\"Actual values\"   , fontdict = {'size':22})\nplt.title (\"Confusion Matrix\", fontdict = {'size':22})","metadata":{"id":"1WYwghR6bf2H","outputId":"8af4b58d-3429-480e-d5b2-fd752232bfef","execution":{"iopub.status.busy":"2021-07-22T22:44:49.880995Z","iopub.execute_input":"2021-07-22T22:44:49.881417Z","iopub.status.idle":"2021-07-22T22:44:50.265951Z","shell.execute_reply.started":"2021-07-22T22:44:49.881385Z","shell.execute_reply":"2021-07-22T22:44:50.264935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Classification using Logistic Regression**","metadata":{"id":"XpMLRiuJADEI"}},{"cell_type":"code","source":"#we will use GridSearch to find for perfect hyperparameter\ntuned_parameters = [{'C': [10**-6,10**-4,10**-2 ,10**0 ,10**2,10**4,10**6 ]}]\nclassifier = RandomizedSearchCV(LogisticRegression() , tuned_parameters , cv=5)\nclassifier.fit(x_train , train_sentiments)","metadata":{"id":"kun1pI7yAQJB","outputId":"b4149ba9-9a84-438a-b99d-e0eaab827409","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_sentiments = classifier.predict(x_test)\naccuracy = accuracy_score(pred_sentiments , test_sentiments)\nprint('Accuracy of Logistic Regression is ',accuracy)\nprint('\\n')\nprint(classification_report(test_sentiments,pred_sentiments))","metadata":{"id":"1jptqq0yB4Ix","outputId":"3ca85224-649e-4632-f258-f0bf590c8e07","execution":{"iopub.status.busy":"2021-07-22T23:06:28.623889Z","iopub.execute_input":"2021-07-22T23:06:28.624268Z","iopub.status.idle":"2021-07-22T23:06:28.786861Z","shell.execute_reply.started":"2021-07-22T23:06:28.624209Z","shell.execute_reply":"2021-07-22T23:06:28.78558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plotting confusion_matrix of Logistic_Regression\nprint('Confusion Matrix of Logistic Regression is given below:')\ncf_matrix = confusion_matrix(test_sentiments,pred_sentiments)\nplt.subplots(figsize=(10,10))\nsns.set(font_scale=2)\nsns.heatmap(cf_matrix/np.sum(cf_matrix), annot=True , cmap='BuPu',linecolor='green' , linewidths=3)\nplt.xlabel(\"Predicted values\", fontdict = {'size':22})\nplt.ylabel(\"Actual values\"   , fontdict = {'size':22})\nplt.title (\"Confusion Matrix\", fontdict = {'size':22})","metadata":{"id":"L-tSngP_DpCW","outputId":"cd7506d2-cfe1-4515-ea9b-7576540ca58d","execution":{"iopub.status.busy":"2021-07-22T22:47:30.488362Z","iopub.execute_input":"2021-07-22T22:47:30.488635Z","iopub.status.idle":"2021-07-22T22:47:30.834544Z","shell.execute_reply.started":"2021-07-22T22:47:30.488609Z","shell.execute_reply":"2021-07-22T22:47:30.833257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Accuracy from MultinomialNB model = 63.15%**\n# **Accuracy from Logistic Regression model = 71.78%** \n# **Hence we can conclude that Logistic Regression is better model**","metadata":{"id":"EaooghJfD-ck"}}]}