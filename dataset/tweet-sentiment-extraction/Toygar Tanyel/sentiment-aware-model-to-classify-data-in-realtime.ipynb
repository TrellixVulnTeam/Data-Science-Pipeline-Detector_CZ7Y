{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Sentiment-aware Contextual Model for Tweets\n\nSentiment Analysis has an important role in today’s world especially for private companies\nwhich hold lots of data. The massive amount of data generated by Twitter present a unique\nopportunity for sentiment analysis. However, it is challenging to build an accurate predictive\nmodel to identify sentiments, which may lack sufficient context due to the length limit. In\naddition, sentimental and regular ones can be hard to separate because of word ambiguity. In\nthis notebook, I will be proposing the phases of text pre-processing, visual analysis and modeling.\n\n***I tried to keep code as simple as possible to remain understandable.***\n\nProposed **BERT-CNN-BiLSTM** learning pipeline, which consists of **three sequential modules**.<br />\nBERT produces competitive results, and can be considered as one of the new electricity of natural\nlanguage processing tasks such as sentiment analysis, named entity recognition (NER), and topic\nmodeling. The combination of CNN and BiLSTM models requires a particular design, since each\nmodel has a specific architecture and its own strengths:<br />\n• BERT is utilized to transform word tokens from the raw Tweet messages to contextual word\nembeddings.<br />\n• CNN is known for its ability to extract as many features as possible from the text.<br />\n• BiLSTM keeps the chronological order between words in a document, thus it has the ability\nto ignore unnecessary words using the delete gate.<br />\n\n___\n### Important\nThis notebook is prepared based on binary classification (negative-positive), and neutral added manually by using threshold. If you need process of multi-class classification which includes neutral from the beggining, feel free to check my other notebook I prepared for [Multi-class Classification](https://www.kaggle.com/toygarr/contextual-model-and-crawling-for-real-tweets).\n___\n#### _In last update, Error Analysis is added based on test set._\n\n**References:**<br />\n1) [A Sentiment-Aware Contextual Model for Real-Time Disaster Prediction Using Twitter Data](https://www.mdpi.com/1999-5903/13/7/163/htm) -> The idea comes from and really worth to check on, however, I made some changes.<br />\n2) [Automatic identification of eyewitness messages on twitter during disasters](https://reader.elsevier.com/reader/sd/pii/S0306457319303590?token=985D740724AEDB812611486EBAD3B68FA4393520D4DCD96FDADE4A642A9805D728945987C1BBBE0FDAA8EC3684E372C7&originRegion=eu-west-1&originCreation=20210920022341)<br />\n3) [BERT: Pre-training of Deep Bidirectional Transformers for Language\n               Understanding](http://arxiv.org/abs/1810.04805)<br />\n4) [Convolutional Neural Networks for Sentence Classification](http://arxiv.org/abs/1408.5882)<br />\n5) [Long-short Term Memory](https://www.researchgate.net/publication/13853244_Long_Short-term_Memory#fullTextFileContent)<br />\n6) [LMAES' Notebook](https://www.kaggle.com/lmasca/disaster-tweets-using-bert-embeddings-and-lstm)<br />\n7) [PAOLO RIPAMONTI's Notebook](https://www.kaggle.com/paoloripamonti/twitter-sentiment-analysis)\n\n## > If you find my work useful please don't forget to **Upvote!**  so it can reach more people.","metadata":{}},{"cell_type":"markdown","source":"___\n# Personal observations that I recommend to read before starting\n#### After trying lots of hyperparameters and different models on very different test distribution, adding things like extra dense layers, gives the models an appreciably robustness. Even though current model has no extra dense layer, those are my experiments to inform you to try for further improvement.\n\n#### Finding perfect hyperparameters are an actual issue after done preprocessing properly. We should not do every preprocessing transaction. I did some of them to show how to, however, generally traditional preprocessing affects texts in a really bad way to be learned by BERT or any contextual structures. We need to check and think how embedding models which we are going to use trained and why we need to clean that any specific property.\n#### For example: `version 11` has better accuracy and far better as randomly distributed custom real world sentiment detector. In `version 13`, I added stopwords removing function and it decreased validation score 84.49% to 80.76%. I did this experiment to show this important issue. We lose high level linguistic features if we remove everything from the real texts. The model lost most of its capability of finding actual sentiment of random real world sentences that you give random texts with prediction function as different distribution in `version 13`. \n\n#### Sentiment effect of words is highly changeable depending on the training phase. Even though the accuracy looks close for each experiments, the actual results which we can consider as totally different distributed real world sentences change this a lot.\n\n#### v12 is same with v13, just fixed some typos.\n___","metadata":{"execution":{"iopub.status.busy":"2022-02-19T09:53:21.706955Z","iopub.execute_input":"2022-02-19T09:53:21.7077Z","iopub.status.idle":"2022-02-19T09:53:21.713787Z","shell.execute_reply.started":"2022-02-19T09:53:21.707659Z","shell.execute_reply":"2022-02-19T09:53:21.713083Z"}}},{"cell_type":"code","source":"# few of the imports are just for checking while coding not included in the rest of notebook.\n\n# Most basic stuff for EDA.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Core packages for text processing.\nimport string\nimport re\n\n# Libraries for text preprocessing.\nimport nltk\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\nfrom nltk.corpus import stopwords\n\n\n# Loading some sklearn packaces for modelling.\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.decomposition import LatentDirichletAllocation, NMF # not actively using\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.model_selection import train_test_split\n\n# Utility\nimport logging\nimport itertools\n\n\n# Core packages for general use throughout the notebook.\nimport random\nimport warnings\nimport time\nimport datetime\n\n# For customizing our plots.\nfrom matplotlib.ticker import MaxNLocator\nimport matplotlib.gridspec as gridspec\nimport matplotlib.patches as mpatches\n\n# for build our model\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom transformers import BertTokenizer, TFBertModel\n\n# Setting some options for general use.\nimport os\nstop = set(stopwords.words('english'))\nplt.style.use('fivethirtyeight')\nsns.set(font_scale=1.5)\npd.options.display.max_columns = 250\npd.options.display.max_rows = 250\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-02-19T09:15:29.217284Z","iopub.execute_input":"2022-02-19T09:15:29.218059Z","iopub.status.idle":"2022-02-19T09:15:36.505334Z","shell.execute_reply.started":"2022-02-19T09:15:29.217942Z","shell.execute_reply":"2022-02-19T09:15:36.504401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# DATASET\nDATASET_COLUMNS = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\nDATASET_ENCODING = \"ISO-8859-1\"\n\n# SENTIMENT\nPOSITIVE = \"POSITIVE\"\nNEGATIVE = \"NEGATIVE\"\nNEUTRAL = \"NEUTRAL\"\nSENTIMENT_THRESHOLDS = (0.4, 0.7)\n\n# EXPORT\nKERAS_MODEL = \"model.h5\"","metadata":{"execution":{"iopub.status.busy":"2022-02-19T09:15:36.507199Z","iopub.execute_input":"2022-02-19T09:15:36.507492Z","iopub.status.idle":"2022-02-19T09:15:36.512891Z","shell.execute_reply.started":"2022-02-19T09:15:36.507455Z","shell.execute_reply":"2022-02-19T09:15:36.51209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Read Dataset\n\n### Dataset details:\nhttp://help.sentiment140.com the site is pretty old and most of the links are broken, however, check out for more detail.\n\nLatest availability of dataset: https://www.kaggle.com/kazanova/sentiment140\n\n@ONLINE {Sentiment140, <br />\n    author = \"Go, Alec and Bhayani, Richa and Huang, Lei\", <br />\n    title  = \"Twitter Sentiment Classification using Distant Supervision\", <br />\n    year   = \"2009\", <br />\n    url    = \"http://help.sentiment140.com/home\"\n}\n\n*According to the creators of the dataset:* \\\n\"Our approach was unique because our training data was automatically created, as opposed to having humans manual annotate tweets. In our approach, we assume that any tweet with positive emoticons, like :), were positive, and tweets with negative emoticons, like :(, were negative. We used the Twitter Search API to collect these tweets by using keyword search\".\n\n\nThe data is a CSV with emoticons removed. Data file format has 6 fields:\n* target: the polarity of the tweet (0 = negative, 4 = positive)<br/> -> We will insert (2 = neutral) manually using threshold.\n* ids: The id of the tweet\n* date: the date of the tweet\n* flag: The query (lyx). If there is no query, then this value is NO_QUERY.\n* user: the user that tweeted\n* text: the text of the tweet","metadata":{}},{"cell_type":"code","source":"# Read the data\ndf = pd.read_csv('../input/sentiment140/training.1600000.processed.noemoticon.csv', \n                 encoding = DATASET_ENCODING, names=DATASET_COLUMNS)","metadata":{"execution":{"iopub.status.busy":"2022-02-19T09:15:39.459431Z","iopub.execute_input":"2022-02-19T09:15:39.460111Z","iopub.status.idle":"2022-02-19T09:15:46.32603Z","shell.execute_reply.started":"2022-02-19T09:15:39.460076Z","shell.execute_reply":"2022-02-19T09:15:46.325341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Raw data\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-19T09:15:46.327205Z","iopub.execute_input":"2022-02-19T09:15:46.32753Z","iopub.status.idle":"2022-02-19T09:15:46.34804Z","shell.execute_reply.started":"2022-02-19T09:15:46.327504Z","shell.execute_reply":"2022-02-19T09:15:46.347188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Map target label to String\n* 0 -> NEGATIVE\n* 2 -> NEUTRAL\n* 4 -> POSITIVE\n\nWe prepare \"2\" for \"neutral\" label beforehand.","metadata":{}},{"cell_type":"code","source":"decode_map = {0: \"NEGATIVE\", 2: \"NEUTRAL\", 4: \"POSITIVE\"}\ndef decode_sentiment(label):\n    return decode_map[int(label)]","metadata":{"execution":{"iopub.status.busy":"2022-02-19T09:16:00.224209Z","iopub.execute_input":"2022-02-19T09:16:00.224521Z","iopub.status.idle":"2022-02-19T09:16:00.229339Z","shell.execute_reply.started":"2022-02-19T09:16:00.224487Z","shell.execute_reply":"2022-02-19T09:16:00.228405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ndf.target = df.target.apply(lambda x: decode_sentiment(x))","metadata":{"execution":{"iopub.status.busy":"2022-02-19T09:16:02.63554Z","iopub.execute_input":"2022-02-19T09:16:02.636203Z","iopub.status.idle":"2022-02-19T09:16:03.392097Z","shell.execute_reply.started":"2022-02-19T09:16:02.636156Z","shell.execute_reply":"2022-02-19T09:16:03.391161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cleaning Text\n\nSo basically what we will do here:\n\n* Remove urls, html tags and punctuations <br/>\n\nBased on this [paper](https://aclanthology.org/2020.pam-1.15.pdf), removing punctuations is important for BERT both in statistical and sentimental way. It significantly affects counting.\n\nOn the other hand, twitter data is a real mess so generally may not has obvious positive impact on the results. In our case we ignored the punctuations, but keep that paper in mind.","metadata":{}},{"cell_type":"code","source":"# Some basic helper functions to clean text by removing urls, emojis, html tags and punctuations.\ndef remove_stopwords(text):\n    tokens = []\n    for token in text.split():\n        if token not in stop:\n            tokens.append(token)\n    return \" \".join(tokens)\n\n\ndef remove_URL(text):\n    url = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url.sub(r'', text)\n\n\ndef remove_html(text):\n    html = re.compile(r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n    return re.sub(html, '', text)\n\n\ndef remove_punct(text):\n    table = str.maketrans('', '', string.punctuation)\n    return text.translate(table)\n\n# Applying helper functions\ndf['text_clean'] = df['text'].apply(lambda x: remove_stopwords(x))\ndf['text_clean'] = df['text_clean'].apply(lambda x: remove_URL(x))\ndf['text_clean'] = df['text_clean'].apply(lambda x: remove_html(x))\ndf['text_clean'] = df['text_clean'].apply(lambda x: remove_punct(x))","metadata":{"execution":{"iopub.status.busy":"2022-02-19T09:16:19.975554Z","iopub.execute_input":"2022-02-19T09:16:19.975869Z","iopub.status.idle":"2022-02-19T09:16:46.094224Z","shell.execute_reply.started":"2022-02-19T09:16:19.975836Z","shell.execute_reply":"2022-02-19T09:16:46.093442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-19T09:16:46.095448Z","iopub.execute_input":"2022-02-19T09:16:46.096042Z","iopub.status.idle":"2022-02-19T09:16:46.108901Z","shell.execute_reply.started":"2022-02-19T09:16:46.09601Z","shell.execute_reply":"2022-02-19T09:16:46.107956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualizing the Data","metadata":{}},{"cell_type":"code","source":"# Displaying target distribution.\n\nfig, axes = plt.subplots(ncols=2, nrows=1, figsize=(12, 4), dpi=70)\nsns.countplot(df['target'], ax=axes[0])\naxes[1].pie(df['target'].value_counts(),\n            labels=[NEGATIVE, POSITIVE],\n            autopct='%1.2f%%',\n            shadow=True,\n            explode=(0.05, 0),\n            startangle=60)\nfig.suptitle('Distribution of the Tweets', fontsize=24)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-19T09:17:36.613539Z","iopub.execute_input":"2022-02-19T09:17:36.613853Z","iopub.status.idle":"2022-02-19T09:17:38.958354Z","shell.execute_reply.started":"2022-02-19T09:17:36.613822Z","shell.execute_reply":"2022-02-19T09:17:38.957362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"________________","metadata":{}},{"cell_type":"markdown","source":"As a quick and easy observation, we can say that dataset has no imbalanced label problem. Negative and Positive labels are equal. The situation of equilibrium will let model to learn more accurate. However, should not be forgotten, dataset might has lots of mislabelled text due to way of collection which has only parameter as **\":)\" : positive** or **\":(\" : negative**. The problem is that lots of user may send \":)\" or \":(\" ironically.  \n\n> An example: \"u really like that thing damn :))))\"\n\nThis problem may decrease the accuracy, however, end of the day we're creating a sentiment-aware model depending on the words.","metadata":{}},{"cell_type":"markdown","source":"_____","metadata":{}},{"cell_type":"code","source":"# Creating a new feature for the visualization.\n\ndf['Character Count'] = df['text_clean'].apply(lambda x: len(str(x)))\n\n\ndef plot_dist3(df_x, feature, title):\n    # Creating a customized chart. and giving in figsize and everything.\n    fig = plt.figure(constrained_layout=True, figsize=(18, 8))\n    # Creating a grid of 3 cols and 3 rows.\n    grid = gridspec.GridSpec(ncols=3, nrows=3, figure=fig)\n\n    # Customizing the histogram grid.\n    ax1 = fig.add_subplot(grid[0, :2])\n    # Set the title.\n    ax1.set_title('Histogram')\n    # plot the histogram.\n    sns.distplot(df_x.loc[:, feature],\n                 hist=True,\n                 kde=True,\n                 ax=ax1,\n                 color='#e74c3c')\n    ax1.set(ylabel='Frequency')\n    ax1.xaxis.set_major_locator(MaxNLocator(nbins=20))\n\n    # Customizing the ecdf_plot.\n    ax2 = fig.add_subplot(grid[1, :2])\n    # Set the title.\n    ax2.set_title('Empirical CDF')\n    # Plotting the ecdf_Plot.\n    sns.distplot(df.loc[:, feature],\n                 ax=ax2,\n                 kde_kws={'cumulative': True},\n                 hist_kws={'cumulative': True},\n                 color='#e74c3c')\n    ax2.xaxis.set_major_locator(MaxNLocator(nbins=20))\n    ax2.set(ylabel='Cumulative Probability')\n\n    # Customizing the Box Plot.\n    ax3 = fig.add_subplot(grid[:, 2])\n    # Set title.\n    ax3.set_title('Box Plot')\n    # Plotting the box plot.\n    sns.boxplot(x=feature, data=df, orient='v', ax=ax3, color='#e74c3c')\n    ax3.yaxis.set_major_locator(MaxNLocator(nbins=25))\n\n    plt.suptitle(f'{title}', fontsize=24)","metadata":{"execution":{"iopub.status.busy":"2022-02-19T09:19:38.028924Z","iopub.execute_input":"2022-02-19T09:19:38.029315Z","iopub.status.idle":"2022-02-19T09:19:39.305352Z","shell.execute_reply.started":"2022-02-19T09:19:38.029278Z","shell.execute_reply":"2022-02-19T09:19:39.30445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_dist3(df[df['target'] == 'NEGATIVE'], 'Character Count',\n           'Characters Per \"NEGATIVE\" Tweet')","metadata":{"execution":{"iopub.status.busy":"2022-02-19T09:19:39.390105Z","iopub.execute_input":"2022-02-19T09:19:39.390398Z","iopub.status.idle":"2022-02-19T09:20:03.356908Z","shell.execute_reply.started":"2022-02-19T09:19:39.390366Z","shell.execute_reply":"2022-02-19T09:20:03.356056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_dist3(df[df['target'] == \"POSITIVE\"], 'Character Count',\n           'Characters Per \"POSITIVE\" Tweet')","metadata":{"execution":{"iopub.status.busy":"2022-02-19T09:20:03.358313Z","iopub.execute_input":"2022-02-19T09:20:03.358702Z","iopub.status.idle":"2022-02-19T09:20:26.833859Z","shell.execute_reply.started":"2022-02-19T09:20:03.358674Z","shell.execute_reply":"2022-02-19T09:20:26.833003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Setup environment to build model","metadata":{}},{"cell_type":"code","source":"os.environ[\"WANDB_API_KEY\"] = \"0\" ## to silence warning","metadata":{"execution":{"iopub.status.busy":"2022-02-19T09:20:26.834912Z","iopub.execute_input":"2022-02-19T09:20:26.835512Z","iopub.status.idle":"2022-02-19T09:20:26.839426Z","shell.execute_reply.started":"2022-02-19T09:20:26.835481Z","shell.execute_reply":"2022-02-19T09:20:26.838674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept ValueError:\n    strategy = tf.distribute.get_strategy() # for CPU and single GPU\n    print('Number of replicas:', strategy.num_replicas_in_sync)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-02-19T09:20:26.841383Z","iopub.execute_input":"2022-02-19T09:20:26.841777Z","iopub.status.idle":"2022-02-19T09:20:32.579186Z","shell.execute_reply.started":"2022-02-19T09:20:26.841735Z","shell.execute_reply":"2022-02-19T09:20:32.578415Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hyperparameters\nmax_length = 140 # max length of tweets in the dataset collection time\nbatch_size = 512 # huge batch_size is used because it affects training time significantly. /we have really big dataset/","metadata":{"execution":{"iopub.status.busy":"2022-02-19T09:20:32.58016Z","iopub.execute_input":"2022-02-19T09:20:32.580636Z","iopub.status.idle":"2022-02-19T09:20:32.584894Z","shell.execute_reply.started":"2022-02-19T09:20:32.580602Z","shell.execute_reply":"2022-02-19T09:20:32.58394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"____\nAgain we will use bert-base-uncased because we don't have proper written texts, mostly chaos. <br/>\nDo not need to consider cased characters for now. It would be more sensible if we were classifying text of news or papers etc. so we go through uncased bert.\n____","metadata":{}},{"cell_type":"code","source":"# Bert Tokenizer\nmodel_name = \"bert-base-uncased\"\ntokenizer = BertTokenizer.from_pretrained(model_name)","metadata":{"execution":{"iopub.status.busy":"2022-02-19T09:21:38.413917Z","iopub.execute_input":"2022-02-19T09:21:38.414345Z","iopub.status.idle":"2022-02-19T09:21:40.173735Z","shell.execute_reply.started":"2022-02-19T09:21:38.414317Z","shell.execute_reply":"2022-02-19T09:21:40.173084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Splitting the data\nWe have huge dataset which allows us taking only 1% of whole set for splitting to the development and the test set.\n\nTo be more clear, in the code below, first we take 1% of whole set as the test set. Afterwards, we take 1% of the rest of data which can be defined as 99% to create development set.","metadata":{}},{"cell_type":"code","source":"train_df, test = train_test_split(df, test_size=0.01, random_state=42)\nx_train, dev = train_test_split(train_df, test_size=0.01, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2022-02-19T09:21:48.842863Z","iopub.execute_input":"2022-02-19T09:21:48.843644Z","iopub.status.idle":"2022-02-19T09:21:52.222181Z","shell.execute_reply.started":"2022-02-19T09:21:48.843593Z","shell.execute_reply":"2022-02-19T09:21:52.221353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(x_train.shape)\nprint(test.shape)\nprint(dev.shape)","metadata":{"execution":{"iopub.status.busy":"2022-02-19T09:21:52.224323Z","iopub.execute_input":"2022-02-19T09:21:52.22464Z","iopub.status.idle":"2022-02-19T09:21:52.230598Z","shell.execute_reply.started":"2022-02-19T09:21:52.224599Z","shell.execute_reply":"2022-02-19T09:21:52.229749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"___\n\n### Data Decrease\nI will be taking 500k of the training data because it takes too much time to train with whole 1.5m different texts. However, you can try that to see how datasize affects the results.\n___","metadata":{}},{"cell_type":"code","source":"train = x_train[:500000]","metadata":{"execution":{"iopub.status.busy":"2022-02-19T09:21:59.339484Z","iopub.execute_input":"2022-02-19T09:21:59.340128Z","iopub.status.idle":"2022-02-19T09:21:59.345298Z","shell.execute_reply.started":"2022-02-19T09:21:59.340084Z","shell.execute_reply":"2022-02-19T09:21:59.344698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Label Encoder","metadata":{}},{"cell_type":"code","source":"labels = train.target.unique().tolist()\nlabels.append(NEUTRAL)\nlabels","metadata":{"execution":{"iopub.status.busy":"2022-02-19T09:22:02.00168Z","iopub.execute_input":"2022-02-19T09:22:02.002001Z","iopub.status.idle":"2022-02-19T09:22:02.050555Z","shell.execute_reply.started":"2022-02-19T09:22:02.001953Z","shell.execute_reply":"2022-02-19T09:22:02.049732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoder = LabelEncoder()\nencoder.fit(train.target.tolist())\n\ny_train = encoder.transform(train.target.tolist())\ny_test = encoder.transform(test.target.tolist())\ny_dev = encoder.transform(dev.target.tolist())\n\ny_train = y_train.reshape(-1,1)\ny_test = y_test.reshape(-1,1)\ny_dev = y_dev.reshape(-1,1)\n\nprint(\"y_train\",y_train.shape)\nprint(\"y_test\",y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-02-19T09:22:03.53397Z","iopub.execute_input":"2022-02-19T09:22:03.53429Z","iopub.status.idle":"2022-02-19T09:22:03.980491Z","shell.execute_reply.started":"2022-02-19T09:22:03.534261Z","shell.execute_reply":"2022-02-19T09:22:03.97961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def bert_encode(data):\n    tokens = tokenizer.batch_encode_plus(data, max_length=max_length, padding='max_length', truncation=True)\n    \n    return tf.constant(tokens['input_ids'])","metadata":{"execution":{"iopub.status.busy":"2022-02-19T09:22:06.437043Z","iopub.execute_input":"2022-02-19T09:22:06.437791Z","iopub.status.idle":"2022-02-19T09:22:06.44212Z","shell.execute_reply.started":"2022-02-19T09:22:06.437749Z","shell.execute_reply":"2022-02-19T09:22:06.441483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_encoded = bert_encode(train.text_clean)\ndev_encoded = bert_encode(dev.text_clean)\n\n\ntrain_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((train_encoded, y_train))\n    .shuffle(128)\n    .batch(batch_size)\n)\n\ndev_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((dev_encoded, y_dev))\n    .shuffle(128)\n    .batch(batch_size)\n)","metadata":{"execution":{"iopub.status.busy":"2022-02-19T09:22:09.540229Z","iopub.execute_input":"2022-02-19T09:22:09.540859Z","iopub.status.idle":"2022-02-19T09:26:00.545185Z","shell.execute_reply.started":"2022-02-19T09:22:09.54082Z","shell.execute_reply":"2022-02-19T09:26:00.542415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Proposed Model","metadata":{}},{"cell_type":"code","source":"def bert_model():\n\n    bert_encoder = TFBertModel.from_pretrained(model_name)\n    input_word_ids = tf.keras.Input(shape=(max_length,), dtype=tf.int32, name=\"input_ids\")\n    last_hidden_states = bert_encoder(input_word_ids)[0]\n    x = tf.keras.layers.SpatialDropout1D(0.2)(last_hidden_states)\n    x = tf.keras.layers.Conv1D(32, 3, activation='relu')(x)\n    x = tf.keras.layers.Bidirectional(LSTM(100, dropout=0.2, recurrent_dropout=0.2))(x)\n    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n    model = tf.keras.Model(input_word_ids, outputs)\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2022-02-19T09:26:00.552865Z","iopub.execute_input":"2022-02-19T09:26:00.554585Z","iopub.status.idle":"2022-02-19T09:26:00.586254Z","shell.execute_reply.started":"2022-02-19T09:26:00.55454Z","shell.execute_reply":"2022-02-19T09:26:00.583576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    model = bert_model()\n    adam_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n    model.compile(loss='binary_crossentropy',optimizer=adam_optimizer,metrics=['accuracy'])\n\n    model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-02-19T09:26:00.590462Z","iopub.execute_input":"2022-02-19T09:26:00.592731Z","iopub.status.idle":"2022-02-19T09:26:39.676547Z","shell.execute_reply.started":"2022-02-19T09:26:00.592585Z","shell.execute_reply":"2022-02-19T09:26:39.675515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.utils.plot_model(model, show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-19T09:26:39.679554Z","iopub.execute_input":"2022-02-19T09:26:39.680435Z","iopub.status.idle":"2022-02-19T09:26:40.772779Z","shell.execute_reply.started":"2022-02-19T09:26:39.68039Z","shell.execute_reply":"2022-02-19T09:26:40.771765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n\ncallbacks = [ ReduceLROnPlateau(monitor='val_loss', patience=5, cooldown=0),\n              EarlyStopping(monitor='val_acc', min_delta=1e-5, patience=5)]","metadata":{"execution":{"iopub.status.busy":"2022-02-19T09:26:40.774214Z","iopub.execute_input":"2022-02-19T09:26:40.774457Z","iopub.status.idle":"2022-02-19T09:26:40.780619Z","shell.execute_reply.started":"2022-02-19T09:26:40.774426Z","shell.execute_reply":"2022-02-19T09:26:40.779453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Start train\nhistory = model.fit(\n    train_dataset,\n    batch_size=batch_size,\n    epochs=3,\n    validation_data=dev_dataset,\n    verbose=1,\n    callbacks = callbacks)","metadata":{"execution":{"iopub.status.busy":"2022-02-19T09:27:46.193713Z","iopub.execute_input":"2022-02-19T09:27:46.194101Z","iopub.status.idle":"2022-02-19T09:45:43.864529Z","shell.execute_reply.started":"2022-02-19T09:27:46.194065Z","shell.execute_reply":"2022-02-19T09:45:43.863576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# SAVE MODEL WEIGHTS\nmodel.save_weights(f'sentiment_weights_v1.h5')","metadata":{"execution":{"iopub.status.busy":"2022-02-19T09:45:44.933172Z","iopub.execute_input":"2022-02-19T09:45:44.933536Z","iopub.status.idle":"2022-02-19T09:45:48.212551Z","shell.execute_reply.started":"2022-02-19T09:45:44.933506Z","shell.execute_reply":"2022-02-19T09:45:48.209412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# LOAD MODEL WEIGHTS\n#model.load_weights('../input/-THE PATH THAT YOU UPLOADED WEIGHTS ON KAGGLE-/sentiment_weights_v1.h5')\n\n#to be able to use weights you need to run same model again without fitting because you need model to get weights:) ","metadata":{"execution":{"iopub.status.busy":"2022-02-19T09:45:48.219074Z","iopub.execute_input":"2022-02-19T09:45:48.219974Z","iopub.status.idle":"2022-02-19T09:45:48.235681Z","shell.execute_reply.started":"2022-02-19T09:45:48.21983Z","shell.execute_reply":"2022-02-19T09:45:48.231555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_graphs(history, string):\n    plt.plot(history.history[string])\n    plt.plot(history.history['val_'+string])\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(string)\n    plt.legend([string, 'val_'+string])\n    plt.show()\n   \nplot_graphs(history, \"accuracy\")\nplot_graphs(history, \"loss\")","metadata":{"execution":{"iopub.status.busy":"2022-02-19T09:45:43.866477Z","iopub.execute_input":"2022-02-19T09:45:43.866735Z","iopub.status.idle":"2022-02-19T09:45:44.930217Z","shell.execute_reply.started":"2022-02-19T09:45:43.866704Z","shell.execute_reply":"2022-02-19T09:45:44.929209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predict Manually Before Using Test Data\n\nDecoder to be able to see results as labelled negative, positive or neutral <br/>\n**Recap:** The threshold was determined as (0.4, 0.7). <br/>\n<br/>\nIf you want to add neutral label, send include_neutral = **True** parameter after the given text.","metadata":{}},{"cell_type":"code","source":"def decode_sentiment(score, include_neutral=False):\n    if include_neutral:        \n        label = NEUTRAL\n        if score <= SENTIMENT_THRESHOLDS[0]:\n            label = NEGATIVE\n        elif score >= SENTIMENT_THRESHOLDS[1]:\n            label = POSITIVE\n\n        return label\n    else:\n        return NEGATIVE if score < 0.5 else POSITIVE","metadata":{"execution":{"iopub.status.busy":"2022-02-19T09:51:50.355961Z","iopub.execute_input":"2022-02-19T09:51:50.356865Z","iopub.status.idle":"2022-02-19T09:51:50.363047Z","shell.execute_reply.started":"2022-02-19T09:51:50.356812Z","shell.execute_reply":"2022-02-19T09:51:50.362129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(text, include_neutral=False):\n    start_at = time.time()\n    # Tokenize text\n    x_encoded = bert_encode([text])\n    # Predict\n    score = model.predict([x_encoded])[0]\n    # Decode sentiment\n    label = decode_sentiment(score, include_neutral=include_neutral)\n\n    return {\"label\": label, \"score\": float(score),\n       \"elapsed_time\": time.time()-start_at}  ","metadata":{"execution":{"iopub.status.busy":"2022-02-19T09:51:50.724582Z","iopub.execute_input":"2022-02-19T09:51:50.725385Z","iopub.status.idle":"2022-02-19T09:51:50.732066Z","shell.execute_reply.started":"2022-02-19T09:51:50.725344Z","shell.execute_reply":"2022-02-19T09:51:50.730945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict(\"I hate the economy\")","metadata":{"execution":{"iopub.status.busy":"2022-02-19T09:51:51.731261Z","iopub.execute_input":"2022-02-19T09:51:51.731563Z","iopub.status.idle":"2022-02-19T09:52:00.11415Z","shell.execute_reply.started":"2022-02-19T09:51:51.731531Z","shell.execute_reply":"2022-02-19T09:52:00.113097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict(\"I would prefer writing a crawler to create this dataset but i couldn't\", True)","metadata":{"execution":{"iopub.status.busy":"2022-02-19T09:52:00.115679Z","iopub.execute_input":"2022-02-19T09:52:00.115929Z","iopub.status.idle":"2022-02-19T09:52:00.485558Z","shell.execute_reply.started":"2022-02-19T09:52:00.1159Z","shell.execute_reply":"2022-02-19T09:52:00.484696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict(\"I LOVE NLP\")","metadata":{"execution":{"iopub.status.busy":"2022-02-19T09:52:00.487198Z","iopub.execute_input":"2022-02-19T09:52:00.487528Z","iopub.status.idle":"2022-02-19T09:52:00.865143Z","shell.execute_reply.started":"2022-02-19T09:52:00.487489Z","shell.execute_reply":"2022-02-19T09:52:00.864227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict(\"life is really strange isn't it? just the combination of laugh and cry\", True)","metadata":{"execution":{"iopub.status.busy":"2022-02-19T09:52:00.867027Z","iopub.execute_input":"2022-02-19T09:52:00.867287Z","iopub.status.idle":"2022-02-19T09:52:01.257497Z","shell.execute_reply.started":"2022-02-19T09:52:00.867258Z","shell.execute_reply":"2022-02-19T09:52:01.256694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict(\"ESL is the world's largest esports company, leading the industry across the most popular video games.\\\n        We're proud they've chosen us to help them deliver their launchers to gamers all over the world. Read the full review\")","metadata":{"execution":{"iopub.status.busy":"2022-02-19T09:52:01.258862Z","iopub.execute_input":"2022-02-19T09:52:01.259091Z","iopub.status.idle":"2022-02-19T09:52:01.646146Z","shell.execute_reply.started":"2022-02-19T09:52:01.259066Z","shell.execute_reply":"2022-02-19T09:52:01.645235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict(\"Excited to present a tutorial on 'Modular and Parameter-Efficient Fine-Tuning for NLP Models' \\\n        at #EMNLP2022 with @PfeiffJo & @licwu.\")","metadata":{"execution":{"iopub.status.busy":"2022-02-19T09:52:01.647622Z","iopub.execute_input":"2022-02-19T09:52:01.648075Z","iopub.status.idle":"2022-02-19T09:52:02.038165Z","shell.execute_reply.started":"2022-02-19T09:52:01.648043Z","shell.execute_reply":"2022-02-19T09:52:02.036895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict(\"Had a song stuck in my head. Thirty seconds later I'm listening to it, thanks to the internet,\\\n        and Apple/YouTube Music. In the bad old days I'd browse record stores for hours in the hope that the title might jog my memory.\\\n        It really is a wonderful time to be alive!\")","metadata":{"execution":{"iopub.status.busy":"2022-02-19T09:52:02.039825Z","iopub.execute_input":"2022-02-19T09:52:02.04064Z","iopub.status.idle":"2022-02-19T09:52:02.442801Z","shell.execute_reply.started":"2022-02-19T09:52:02.040602Z","shell.execute_reply":"2022-02-19T09:52:02.441957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict(\"i don't say this lightly - hemingway's life ended by suicide. His life was actually a loss\")","metadata":{"execution":{"iopub.status.busy":"2022-02-19T09:52:02.444325Z","iopub.execute_input":"2022-02-19T09:52:02.444921Z","iopub.status.idle":"2022-02-19T09:52:02.823031Z","shell.execute_reply.started":"2022-02-19T09:52:02.444887Z","shell.execute_reply":"2022-02-19T09:52:02.822087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict(\"these r not ur problems dear!!! these r ur x bf's commitng suicide\")","metadata":{"execution":{"iopub.status.busy":"2022-02-19T09:52:02.8244Z","iopub.execute_input":"2022-02-19T09:52:02.824605Z","iopub.status.idle":"2022-02-19T09:52:03.246872Z","shell.execute_reply.started":"2022-02-19T09:52:02.824581Z","shell.execute_reply":"2022-02-19T09:52:03.246075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict(\"i hve no idea about i love the uni or not\", True)","metadata":{"execution":{"iopub.status.busy":"2022-02-19T09:52:03.249041Z","iopub.execute_input":"2022-02-19T09:52:03.249276Z","iopub.status.idle":"2022-02-19T09:52:03.630374Z","shell.execute_reply.started":"2022-02-19T09:52:03.249246Z","shell.execute_reply":"2022-02-19T09:52:03.629743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict(\"For the third time in four years, the Warriors are champions once again.\\\nThis time, they wasted no time in the NBA Finals, dispatching LeBron James and the Cavs in four straight games.\\\nHere’s how they sealed the championship in Game 4. https://twitter.com/i/moments/1005197277663641600\")","metadata":{"execution":{"iopub.status.busy":"2022-02-19T09:52:03.631505Z","iopub.execute_input":"2022-02-19T09:52:03.631927Z","iopub.status.idle":"2022-02-19T09:52:04.009431Z","shell.execute_reply.started":"2022-02-19T09:52:03.631896Z","shell.execute_reply":"2022-02-19T09:52:04.008497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict(\"I found some old Reddit post in which one guy from english-speaking country complains that\\\nthe names in The Witcher books are 'too difficult' and non- intuitive for english speaker.\\\nMan, let me introduce you to 'The books werent written only/for english speakers.'' #witcher\")","metadata":{"execution":{"iopub.status.busy":"2022-02-19T09:52:18.769326Z","iopub.execute_input":"2022-02-19T09:52:18.769828Z","iopub.status.idle":"2022-02-19T09:52:19.16664Z","shell.execute_reply.started":"2022-02-19T09:52:18.769794Z","shell.execute_reply":"2022-02-19T09:52:19.165702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict(\"I forgot how cringy all the Slavic names sound read it English \\\nYOU'RE PRONOUNCING IT ALL WRONG MY EARS ARE HURTING AND I DON'T EVEN HAVE HEARING AIDS IN\")","metadata":{"execution":{"iopub.status.busy":"2022-02-19T09:52:20.081735Z","iopub.execute_input":"2022-02-19T09:52:20.082107Z","iopub.status.idle":"2022-02-19T09:52:20.479578Z","shell.execute_reply.started":"2022-02-19T09:52:20.082068Z","shell.execute_reply":"2022-02-19T09:52:20.478942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict(\"fun fact: ai cannot predict everything right\")","metadata":{"execution":{"iopub.status.busy":"2022-02-19T09:52:26.56002Z","iopub.execute_input":"2022-02-19T09:52:26.560871Z","iopub.status.idle":"2022-02-19T09:52:26.968069Z","shell.execute_reply.started":"2022-02-19T09:52:26.560812Z","shell.execute_reply":"2022-02-19T09:52:26.96721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict(\"brain is just machine\", True)","metadata":{"execution":{"iopub.status.busy":"2022-02-19T09:52:27.701408Z","iopub.execute_input":"2022-02-19T09:52:27.70198Z","iopub.status.idle":"2022-02-19T09:52:28.119843Z","shell.execute_reply.started":"2022-02-19T09:52:27.701946Z","shell.execute_reply":"2022-02-19T09:52:28.118921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test Results","metadata":{}},{"cell_type":"code","source":"test_encoded = bert_encode(test.text_clean)\n\ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(test_encoded)\n    .batch(batch_size)\n)\n\ny_pred = []\npredicted_tweets = model.predict(test_dataset, batch_size=batch_size)\npredicted_tweets_binary = tf.cast(tf.round(predicted_tweets), tf.int32).numpy().flatten()","metadata":{"execution":{"iopub.status.busy":"2022-02-19T10:00:32.151024Z","iopub.execute_input":"2022-02-19T10:00:32.151644Z","iopub.status.idle":"2022-02-19T10:00:59.003311Z","shell.execute_reply.started":"2022-02-19T10:00:32.151611Z","shell.execute_reply":"2022-02-19T10:00:59.002542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nscores = model.evaluate(test_encoded, y_test, batch_size=batch_size)\nprint()\nprint(\"ACCURACY:\",scores[1])\nprint(\"LOSS:\",scores[0])","metadata":{"execution":{"iopub.status.busy":"2022-02-19T10:00:59.005126Z","iopub.execute_input":"2022-02-19T10:00:59.005595Z","iopub.status.idle":"2022-02-19T10:01:07.936469Z","shell.execute_reply.started":"2022-02-19T10:00:59.005551Z","shell.execute_reply":"2022-02-19T10:01:07.93553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# To decrease our really bad guesses in old predict function","metadata":{}},{"cell_type":"markdown","source":"#### Creating function for removing everything from texts we give as random, and try on some wrong predicted ones again by new function","metadata":{}},{"cell_type":"code","source":"def decode_sentiment(score, include_neutral=False):\n    if include_neutral:        \n        label = NEUTRAL\n        if score <= SENTIMENT_THRESHOLDS[0]:\n            label = NEGATIVE\n        elif score >= SENTIMENT_THRESHOLDS[1]:\n            label = POSITIVE\n\n        return label\n    else:\n        return NEGATIVE if score < 0.5 else POSITIVE","metadata":{"execution":{"iopub.status.busy":"2022-02-19T10:06:31.456801Z","iopub.execute_input":"2022-02-19T10:06:31.457249Z","iopub.status.idle":"2022-02-19T10:06:31.46319Z","shell.execute_reply.started":"2022-02-19T10:06:31.457205Z","shell.execute_reply":"2022-02-19T10:06:31.462207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def improved_prediction(text, include_neutral=False):\n    start_at = time.time()\n    # Applying helper functions\n    text = remove_stopwords(text)\n    text = remove_URL(text)\n    text = remove_html(text)\n    text = remove_punct(text)\n    # Tokenize text\n    x_encoded = bert_encode([text])\n    # Predict\n    score = model.predict([x_encoded])[0]\n    # Decode sentiment\n    label = decode_sentiment(score, include_neutral=include_neutral)\n\n    return {\"label\": label, \"score\": float(score),\n       \"elapsed_time\": time.time()-start_at}","metadata":{"execution":{"iopub.status.busy":"2022-02-19T10:06:31.861861Z","iopub.execute_input":"2022-02-19T10:06:31.862189Z","iopub.status.idle":"2022-02-19T10:06:31.869538Z","shell.execute_reply.started":"2022-02-19T10:06:31.862155Z","shell.execute_reply":"2022-02-19T10:06:31.868578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"improved_prediction(\"life is really strange isn't it? just the combination of laugh and cry\", True)","metadata":{"execution":{"iopub.status.busy":"2022-02-19T10:06:33.0954Z","iopub.execute_input":"2022-02-19T10:06:33.095705Z","iopub.status.idle":"2022-02-19T10:06:33.48881Z","shell.execute_reply.started":"2022-02-19T10:06:33.095652Z","shell.execute_reply":"2022-02-19T10:06:33.488126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"improved_prediction(\"For the third time in four years, the Warriors are champions once again.\\\nThis time, they wasted no time in the NBA Finals, dispatching LeBron James and the Cavs in four straight games.\\\nHere’s how they sealed the championship in Game 4. https://twitter.com/i/moments/1005197277663641600\")","metadata":{"execution":{"iopub.status.busy":"2022-02-19T10:07:12.919875Z","iopub.execute_input":"2022-02-19T10:07:12.920189Z","iopub.status.idle":"2022-02-19T10:07:13.315062Z","shell.execute_reply.started":"2022-02-19T10:07:12.92016Z","shell.execute_reply":"2022-02-19T10:07:13.314013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"improved_prediction(\"brain is just machine\", True)","metadata":{"execution":{"iopub.status.busy":"2022-02-19T10:07:47.677453Z","iopub.execute_input":"2022-02-19T10:07:47.677773Z","iopub.status.idle":"2022-02-19T10:07:48.087663Z","shell.execute_reply.started":"2022-02-19T10:07:47.677737Z","shell.execute_reply":"2022-02-19T10:07:48.086827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Still problematic but less. Moreover, after these experiments we can say that not removing stopwords and punctuations gives more contextual information and robustness to BERT embeddings. Most of those are proved in papers and we see that by ourselves as well.","metadata":{}},{"cell_type":"code","source":"y_pred = [decode_sentiment(predicted_tweets) for predicted_tweets in scores]\ny_pred","metadata":{"execution":{"iopub.status.busy":"2022-02-19T10:01:07.937835Z","iopub.execute_input":"2022-02-19T10:01:07.938699Z","iopub.status.idle":"2022-02-19T10:01:07.945739Z","shell.execute_reply.started":"2022-02-19T10:01:07.938654Z","shell.execute_reply":"2022-02-19T10:01:07.945045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_confusion_matrix(cm, classes,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n\n    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title, fontsize=20)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=90, fontsize=16)\n    plt.yticks(tick_marks, classes, fontsize=16)\n\n    fmt = '.2f'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label', fontsize=12)\n    plt.xlabel('Predicted label', fontsize=12)","metadata":{"execution":{"iopub.status.busy":"2022-02-19T10:01:07.947709Z","iopub.execute_input":"2022-02-19T10:01:07.947926Z","iopub.status.idle":"2022-02-19T10:01:07.958955Z","shell.execute_reply.started":"2022-02-19T10:01:07.947901Z","shell.execute_reply":"2022-02-19T10:01:07.958267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cnf_matrix = confusion_matrix(y_test, predicted_tweets_binary)\nplt.figure(figsize=(6,6))\nplot_confusion_matrix(cnf_matrix, classes=train.target.unique(), title=\"Confusion matrix\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-19T10:01:07.960333Z","iopub.execute_input":"2022-02-19T10:01:07.960574Z","iopub.status.idle":"2022-02-19T10:01:08.266438Z","shell.execute_reply.started":"2022-02-19T10:01:07.960547Z","shell.execute_reply":"2022-02-19T10:01:08.265407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Comprehensive Report","metadata":{}},{"cell_type":"code","source":"print('Precision: %.4f' % precision_score(y_test, predicted_tweets_binary))\nprint('Recall: %.4f' % recall_score(y_test, predicted_tweets_binary))\nprint('Accuracy: %.4f' % accuracy_score(y_test, predicted_tweets_binary))\nprint('F1 Score: %.4f' % f1_score(y_test, predicted_tweets_binary))\nprint(classification_report(y_test, predicted_tweets_binary))","metadata":{"execution":{"iopub.status.busy":"2022-02-19T10:01:08.267641Z","iopub.execute_input":"2022-02-19T10:01:08.267889Z","iopub.status.idle":"2022-02-19T10:01:08.326903Z","shell.execute_reply.started":"2022-02-19T10:01:08.26786Z","shell.execute_reply":"2022-02-19T10:01:08.325938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Error Analysis","metadata":{}},{"cell_type":"markdown","source":"When you start looking at the inside of the data with predicted labels, you will realise that there are lots of mislabeled texts beforehand. Therefore, it is expected to not get really high accuracy. However, we can say that our model works pretty good against its competitors.","metadata":{}},{"cell_type":"code","source":"decode_map = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\ndef decode_sentiment(label):\n    return decode_map[int(label)]","metadata":{"execution":{"iopub.status.busy":"2022-02-19T10:01:08.328578Z","iopub.execute_input":"2022-02-19T10:01:08.32893Z","iopub.status.idle":"2022-02-19T10:01:08.334544Z","shell.execute_reply.started":"2022-02-19T10:01:08.328885Z","shell.execute_reply":"2022-02-19T10:01:08.333738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame(test.text, columns=[\"text\"])\ndf['ids'] = test.ids\ndf[\"actual\"] = test.target\ndf[\"predicted\"] = predicted_tweets_binary\ndf.predicted = df.predicted.apply(lambda x: decode_sentiment(x))\npd.set_option('display.max_columns', None)\npd.set_option('display.expand_frame_repr', False)\npd.set_option('max_colwidth', None)\nincorrect = df[df[\"actual\"] != df[\"predicted\"]]\nincorrect[10:20]","metadata":{"execution":{"iopub.status.busy":"2022-02-19T10:01:08.335553Z","iopub.execute_input":"2022-02-19T10:01:08.335768Z","iopub.status.idle":"2022-02-19T10:01:08.786296Z","shell.execute_reply.started":"2022-02-19T10:01:08.335745Z","shell.execute_reply":"2022-02-19T10:01:08.785347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"correct = df[df['actual'] == df['predicted']]\ncorrect.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-02-19T10:01:08.787496Z","iopub.execute_input":"2022-02-19T10:01:08.787832Z","iopub.status.idle":"2022-02-19T10:01:08.805889Z","shell.execute_reply.started":"2022-02-19T10:01:08.7878Z","shell.execute_reply":"2022-02-19T10:01:08.805165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fetching data from Twitter\nTo get started,\n\n* Import the twint package as follows.","metadata":{}},{"cell_type":"code","source":"!pip install twint\n!pip3 install --user --upgrade git+https://github.com/twintproject/twint.git@origin/master#egg=twint\nimport twint\nimport nest_asyncio","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-02-19T10:13:36.87132Z","iopub.execute_input":"2022-02-19T10:13:36.87168Z","iopub.status.idle":"2022-02-19T10:14:05.417321Z","shell.execute_reply.started":"2022-02-19T10:13:36.871642Z","shell.execute_reply":"2022-02-19T10:14:05.416366Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"c = twint.Config()\n\nc.Search = \"elonmusk\" #keyword for search\nc.Limit = 20 #limit of the number of tweets which will be extracted\nc.Store_csv = True \nc.Output = 'elonmusk_tweet_data.csv'\n\nnest_asyncio.apply()\ntwint.run.Search(c)","metadata":{"_kg_hide-output":false,"execution":{"iopub.status.busy":"2022-02-19T10:15:40.214889Z","iopub.execute_input":"2022-02-19T10:15:40.21528Z","iopub.status.idle":"2022-02-19T10:15:41.295534Z","shell.execute_reply.started":"2022-02-19T10:15:40.21524Z","shell.execute_reply":"2022-02-19T10:15:41.29406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## We stored the related tweets in the .csv or .json file which is really fast and cool\n\nSo how we will read from csv/json file to use for our purpose ? ","metadata":{}},{"cell_type":"code","source":"crawled_data = pd.read_csv(\"elonmusk_tweet_data.csv\")\n#crawled_data = pd.read_json(\"tweet_data.json\", lines=True)\npd.options.display.max_columns=36\ncrawled_data.head()","metadata":{"_kg_hide-output":false,"execution":{"iopub.status.busy":"2022-02-19T10:15:43.615666Z","iopub.execute_input":"2022-02-19T10:15:43.616214Z","iopub.status.idle":"2022-02-19T10:15:43.653865Z","shell.execute_reply.started":"2022-02-19T10:15:43.616176Z","shell.execute_reply":"2022-02-19T10:15:43.653085Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"_____\nAs you can see above we have lots of features which extracted by twint. However, we only need the \"tweet\" feature which includes the text data of tweets for our purpose.","metadata":{}},{"cell_type":"code","source":"# prediction of the first 15 extracted tweets\nfor i in range(15):\n    print(crawled_data[\"tweet\"][i])\n    print(improved_prediction(crawled_data[\"tweet\"][i]))\n    print(\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-02-19T10:15:48.956095Z","iopub.execute_input":"2022-02-19T10:15:48.956393Z","iopub.status.idle":"2022-02-19T10:15:54.843725Z","shell.execute_reply.started":"2022-02-19T10:15:48.956363Z","shell.execute_reply":"2022-02-19T10:15:54.842805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### If you want to know more about twint, you can checkout this Github link:\nhttps://github.com/twintproject/twint","metadata":{}}]}