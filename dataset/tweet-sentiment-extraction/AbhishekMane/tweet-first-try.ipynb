{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import f1_score\n\nimport pandas as pd\nimport numpy as np\nimport re\n\n# LSTM for sequence classification in the IMDB dataset\nimport numpy\n#import gensim\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.preprocessing import sequence\nfrom keras import preprocessing\nfrom keras.preprocessing.text import Tokenizer\n\nimport numpy\nfrom keras.datasets import imdb\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers.embeddings import Embedding\nfrom keras.preprocessing import sequence\n\n# Let's try TF-IDF, LSA, Clustering (Docs are sentiments)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\n\nimport nltk\nimport pandas as pd\nimport numpy as np\nimport lxml.html\nimport re\nimport collections\nimport networkx as nx\nimport matplotlib.pyplot as plt\nfrom scipy.spatial import distance\nimport random\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n\nimport lxml.html\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk import WordNetLemmatizer\nfrom nltk.stem.porter import PorterStemmer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#nltk.download('wordnet')\n#nltk.download('stopwords')\n#nltk.download('punkt')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tf_idf_train(df,col_name):\n    \n    vec = TfidfVectorizer(ngram_range=(1,1))\n    resp = vec.fit_transform(df[col_name])\n        \n    return resp,vec\n\n\n##############################################################################################\n\ndef tf_idf_test(df,col_name,vec):\n    \n    resp = vec.transform(df[col_name])\n    \n    return resp,vec\n\n#############################################################################################\n\ndef combine_text_by_sentiment(df):\n    \n    cols = ['sentiment','combined_text']\n    \n    df_tmp = df[['sentiment','selected_text']].copy ()\n    \n    #Creating empty dataframe \n    dfz1 = pd.DataFrame(columns=cols)\n    \n    #Add all text cols\n    #df_tmp['questions_title'] = (df['questions_title'] + ' ' + df['questions_body'] + ' ' + df['answers_body'])\n        \n    group = df_tmp.groupby('sentiment');i=0\n          \n    for nm,gr in group:\n        \n        tmp_str = ''\n        for each in gr['selected_text']:\n            tmp_str = tmp_str + ' ' + each\n        dfz1.loc[i] = [nm,tmp_str]\n                \n        i += 1\n            \n    return dfz1\n\n###############################################################################################\n\ndef remove_stop_words(df,col_name):\n    \n    #Lowercase all words\n    for idx,each in enumerate(df[col_name]):\n        df.iloc[idx,df.columns.get_loc(col_name)] = each.lower()\n    \n    #Remove stopwords\n    \n    for idx,each in enumerate(df[col_name]):\n        word_tokens = word_tokenize(each)\n        df.iloc[idx,df.columns.get_loc(col_name)] = \" \".join(select_stop_words(word_tokens))\n    \n    return df\n\n###############################################################################################\n\ndef remove_spl_chars(df0,col_name):\n    df = df0.copy()\n    \n    for idx,each in enumerate(df[col_name]):\n        word_tokens = word_tokenize(each)\n        tmp = []\n        \n        for each2 in word_tokens:\n            #Removing special characters - â€¢,! etc\n            word = re.sub('[^\\s\\w]','',each2)\n            tmp.append(word)\n\n        df.iloc[idx,df.columns.get_loc(col_name)] = \" \".join(tmp)\n\n    return df\n\n###############################################################################################\n\ndef lemma_text(df0,col_name):\n    df = df0.copy()\n    \n     #Lowercase all words\n    for idx,each in enumerate(df[col_name]):\n        df.iloc[idx,df.columns.get_loc(col_name)] = each.lower()\n    \n    #Lemmatize Text\n    lemmatizer = WordNetLemmatizer()\n\n    for idx,each in enumerate(df[col_name]):\n        word_tokens = word_tokenize(each)\n        tmp = []\n        \n        for each2 in word_tokens:\n            tmp.append(lemmatizer.lemmatize(each2))\n\n        df.iloc[idx,df.columns.get_loc(col_name)] = \" \".join(tmp)\n    \n    return df\n\n###############################################################################################\n\n\ndef remove_html(df,col_name):\n    \n    for idx, each in enumerate(df[col_name]):\n        #print(str(idx)+\":\"+col_name)\n        \n        df.iloc[idx,df.columns.get_loc(col_name)] = re.sub(r'http\\S+', '', each)\n    \n    return df\n\n###############################################################################################\n\n#Function obtained from https://www.programcreek.com/python/example/106181/sklearn.feature_extraction.stop_words.ENGLISH_STOP_WORDS and is open source\ndef select_stop_words(word_list):\n    \"\"\" Filter out cluster term names\"\"\"\n    st = PorterStemmer()\n    out = []\n    for word in word_list:\n        word_st = st.stem(word)\n        if len(word_st) <= 2 or\\\n                re.match('\\d+', word_st) or \\\n                re.match('[^a-zA-Z0-9]', word_st) or\\\n                word in ENGLISH_STOP_WORDS:\n            continue\n        out.append(word)\n    return out ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv')\ndf = df.dropna()\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2 = df.copy()\n\ncols = ['selected_text']\nfor each in cols:\n    #Function call to html tag removal\n    df2 = remove_html(df2,each)\n    \n    #Function to remove spl chars\n    df2 = remove_spl_chars(df2,each)\n    \n    #Function call to remove stop words\n    df2 = remove_stop_words(df2,each)\n    \n    #Function to lemmatie text\n    df2 = lemma_text(df2,each)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_1 = combine_text_by_sentiment(df2)\ndf_1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tr_resp, tr_vec = tf_idf_train(df_1,'combined_text')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#tr_resp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#tr_vec","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#tr_resp[0,100]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#tr_vec.get_feature_names()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_t = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv')\ndf_t = df_t.dropna()\ndf_t.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2_test = df_t.copy()\n#df2_test['ori_text'] = df2_test['text']\n    \ncols = ['text']\ndf2_test = remove_html(df2_test,cols[0])\ndf2_test = remove_spl_chars(df2_test,cols[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#tr_vec.get_feature_names().index('is')\ndef get_res_test(sample_txt,sentiment_txt):\n    \n    sentiment_id = df_1[df_1['sentiment'] == sentiment_txt].index\n    \n    sample_lst = []\n    for each in sample_txt.split():\n        try:\n            id = tr_vec.get_feature_names().index(each)\n            sample_lst.append(tr_resp[sentiment_id,id])\n        except:\n            sample_lst.append(0)\n\n    sample_txt_lst = sample_txt.split()\n    res = \"\"\n\n    start=-1\n    end=-1\n    for idx,each in enumerate(sample_lst):\n        if (each > 0):\n            start = idx\n            break\n\n            res = ' '.join([res,sample_txt_lst[idx]])\n\n    for idx in range(len(sample_lst)-1,0,-1):\n        if (sample_lst[idx] > 0):\n            end = idx+1\n            break\n\n    res = ' '.join(sample_txt_lst[start:end])\n    return res\n\n#print(sample_txt[sample_txt.find(sample_txt_lst[start]):sample_txt.find(sample_txt_lst[end])])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2_test['selected_text'] = df2_test.apply(lambda x: get_res_test(x.text, x.sentiment), axis=1)\ndf2_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2_test = df2_test.drop(columns=['text', 'sentiment'])\ndf2_test.to_csv('/kaggle/working/submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}