{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import nltk\nfrom tqdm import tqdm\nimport numpy as np\nimport pandas as pd\nimport re\n# from nltk.tokenize import word_tokenize\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport itertools\nimport matplotlib.pyplot as plt\n# nltk.download('punkt')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/tweet-sentiment-extraction/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/tweet-sentiment-extraction/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"모든전처리들 = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def 결측값제거(DATA):\n    \"\"\"\n    결측값을 데이터에서 제거해주는 함수. row를 날려버린다. \n    날린 row의 index를 출력한다\n    index를 다시 맞춰준다. \n    사용법은 DATA = 결측값제거(DATA)\n    \"\"\"\n    null_columns=DATA.columns[DATA.isnull().any()]\n    null_list = DATA[DATA.isnull().any(axis=1)][null_columns].index\n    for i in null_list:\n        print(\"결측값 row=\", i,\"제거!\")\n        DATA = DATA.drop(i, axis=0)\n    DATA.reset_index(inplace=True)\n    print(\"Empty Data Remove\")\n    return DATA","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"모든전처리들.append(결측값제거)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def 애스터리스크를_퍽으로_바꾸기(DATA):\n    \"\"\"\n    욕설을 아마 ****로 처리한 것 같은데, 데이터에서 나오는 모든 ****를 다 fuck로 바꿔준다.\n    \n    \"\"\"\n    tmp = DATA.columns\n    for i in [\"text\", \"selected_text\"]:\n        try:\n            DATA[i] = DATA[i].str.replace(\"****\", \"FUCK\", regex=False)\n        except:\n            pass\n    print(\"**** → fuck\")\n    return DATA","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"모든전처리들.append(애스터리스크를_퍽으로_바꾸기)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def URL제거기(DATA):\n    \"\"\"\n    문장에 URL이 있으면 찾아서 URLWASHERE 라는 단어로 바꿔준다\n    \"\"\"\n    #URL 제거 정규표현식\n    URL_Rex = \"(https?):\\/\\/([a-zA-Z0-9-\\.\\/~]+)+\"\n    targets = [\"text\", \"selected_text\"]\n    for i in targets:\n        try:\n            DATA[i] = DATA[i].str.replace(URL_Rex, \"URLWASHERE\", regex=True )\n        except:\n            pass\n    print(\"Replace All URL's to URLWASHERE\")\n    return DATA","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"모든전처리들.append(URL제거기)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def 알파벳만남기기(DATA):\n    \"\"\"알파벳이 아닌 글자를 다 날린다.\"\"\"\n    targets = [\"text\", \"selected_text\"]\n    for i in targets:\n        try:\n            DATA[i] = DATA[i].str.replace(\"[^a-zA-Z]\", \" \")\n        except:\n            pass\n    print(\"Remove all Characters EXCEPT Alphabet\")\n    return DATA","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"모든전처리들.append(알파벳만남기기)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def 소문자화(DATA):\n    \"\"\"다 소문자\"\"\"\n    targets = [\"text\", \"selected_text\"]\n    for i in targets:\n        try:\n            DATA[i] = DATA[i].str.lower()\n        except:\n            pass\n    print(\"Lower Case\")\n    return DATA","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"모든전처리들.append(소문자화)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def 세개연속된문자처리(DATA):\n    \"\"\"연속된 글자는 최대 2개 까지만 허용한다. coool -> cool \"\"\"\n    consecutive_characters = [\"aaa\",\"bbb\",\"ccc\",\"ddd\",\"eee\",\"fff\",\"ggg\",\"hhh\",\"iii\",\"jjj\",\"kkk\",\"lll\",\"mmm\",\n                              \"nnn\",\"ooo\",\"ppp\",\"qqq\",\"rrr\",\"sss\",\"ttt\",\"uuu\",\"vvv\",\"www\",\"xxx\",\"yyy\",\"zzz\"]\n\n    targets = [\"text\", \"selected_text\"]\n    for i in targets:\n        try:\n            for j in consecutive_characters:\n                DATA[i] = DATA[i].str.replace(j, j[:1])\n            for j in consecutive_characters:\n                DATA[i] = DATA[i].str.replace(j, j[:1])\n            for j in consecutive_characters:\n                DATA[i] = DATA[i].str.replace(j, j[:1])\n            for j in consecutive_characters:\n                DATA[i] = DATA[i].str.replace(j, j[:1])\n            for j in consecutive_characters:\n                DATA[i] = DATA[i].str.replace(j, j[:1])\n            for j in consecutive_characters:\n                DATA[i] = DATA[i].str.replace(j, j[:1])\n\n        except:\n            pass\n    \n    print(\"Remove consecutive characters more than 3. (eg:cooooooooool → cool) \")\n    return DATA","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"모든전처리들.append(세개연속된문자처리)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def 감성원핫인코딩(DATA):\n    tmp = pd.concat([DATA, pd.get_dummies(DATA.sentiment)], axis=1)\n    print(\"Generate One-Hot Encoded Sentiment column\")\n    return tmp ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"모든전처리들.append(감성원핫인코딩)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def 종합전처리기(DATA):\n    for i in 모든전처리들:\n        DATA = i(DATA)\n    return DATA","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = 종합전처리기(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = 종합전처리기(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(train[\"text\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"threshold = 3\ntotal_cnt = len(tokenizer.word_index) # 단어의 수\nrare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\ntotal_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\nrare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n\n# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\nfor key, value in tokenizer.word_counts.items():\n    total_freq = total_freq + value\n\n    # 단어의 등장 빈도수가 threshold보다 작으면\n    if(value < threshold):\n        rare_cnt = rare_cnt + 1\n        rare_freq = rare_freq + value\n\nprint('단어 집합(vocabulary)의 크기 :',total_cnt)\nprint('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\nprint(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\nprint(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_size = total_cnt - rare_cnt + 1 # 전체 단어 개수 중 빈도수 2이하인 단어 개수는 제거. 0번 패딩 토큰을 고려하여 +1\nprint('단어 집합의 크기 :',vocab_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer(vocab_size) \ntokenizer.fit_on_texts(train[\"text\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def 토큰화(DATA):\n    DATA[\"token\"] = tokenizer.texts_to_sequences(DATA[\"text\"])\n    return DATA","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = 토큰화(train)\ntest = 토큰화(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def 토큰화과정에서제거당한열제거기(DATA):\n    no_token_list = DATA[DATA[\"token\"].str.len()==0].index\n    for i in no_token_list:\n        DATA = DATA.drop(i, axis=0)\n    DATA.reset_index(inplace=True)       \n    print(\"토큰화 해서 사라진 값 \", i,\"제거!\")\n    return DATA","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = 토큰화과정에서제거당한열제거기(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = 토큰화과정에서제거당한열제거기(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def 패딩된토큰만들기(DATA):\n    DATA[\"padding_token\"]= pad_sequences(DATA['token'], maxlen=25).tolist()\n    return DATA","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = 패딩된토큰만들기(train)\ntest = 패딩된토큰만들기(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.layers import Embedding, Dense, LSTM\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(vocab_size, 100)) \nmodel.add(LSTM(128))\nmodel.add(Dense(3, activation='softmax'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n# mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'] )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train[[\"negative\",\"neutral\",\"positive\"]]\n\n# y = np.array(y.values, dtype=object)\ny = np.array(y.values)\n\ny[:3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train[\"padding_token\"]\n\n# X = np.array(X.values.tolist(), dtype=object)\nX = np.array(X.values.tolist())\nX","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nif \"best_model.h5\" in os.listdir():\n    model = load_model('best_model.h5') # 경고! 전처리 다시 했으면 이 파일을 삭제해줘야 재학습함. 있으면 말고. \nelse:\n    history = model.fit(X, y, epochs=20, batch_size=64, validation_split=0.2, callbacks=[mc], workers=0)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\ndef sequence_to_text(list_of_indices):\n    # Looking up words in dictionary\n    words = [reverse_word_map.get(letter) for letter in list_of_indices]\n    return(words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def 셀렉티드생성기(DATA):\n    \"\"\"\n    예시문장 = 단어1 단어2 단어3 단어4 단어5 단어6 단어7 \n    이 있으면,\n    확률평가(예시문장) = (부정확률, 중립확률, 긍정확률)\n    이 나온다.\n    \n    이 중, 예시문장에서 i~j 번 단어를 뽑은 예시문장 tmp를 만들고 tmp를 확률평가한 뒤,\n    (i는 1에서7 사이의 정수, j는 i+1에서 7 사이의 정수)\n    \n    전체 문장의 감성이 부정일 경우, 최대 부정 확률을 나타내는 i,j 값을 찾는다.\n    전체 문장의 감성이 긍정일 경우, 최대 긍정 확률을 나타내는 i,j 값을 찾는다.\n    전체 문장의 감성이 중립일 경우, 최대 중립 확률을 나타내는 i,j 값을 찾는다.\n    \n    이 단어i ~ 단어j 가 센티멘트를 나타내는 selected_text 이다!\n    \n    \"\"\"\n    total_selected_text = []\n    for 개별트윗 in tqdm(DATA[\"padding_token\"].tolist()):\n        셀렉티드 = [x for x in 개별트윗 if x is not 0]\n        개별트윗문장평가값 =  model.predict([개별트윗])\n        \n        \n        for i in range(24):\n            if 개별트윗[i] ==0:\n                continue\n            for j in range(i+1, 25, 3):\n                패딩된개별트윗부분 = pad_sequences([개별트윗[i:j]], maxlen=25).tolist()\n                if max(패딩된개별트윗부분[0]) == 0: # None만 있으면 제낀다.\n                    continue\n                predict = model.predict(패딩된개별트윗부분)\n                inx = predict[0].argmax()\n                \n                if inx == 개별트윗문장평가값[0].argmax(): # 그 부분의 평가가 문장 전체의 평가랑 일치하며\n                    if np.max(predict[0]) >= np.max(개별트윗문장평가값[0]): # 그 부분이 더 높은 평가값을 보일 경우\n                        셀렉티드 = 개별트윗[i:j]\n        \n        \n        결과텍스트 = '\"'+\" \".join(sequence_to_text(셀렉티드))+'\"'\n        결과텍스트 = 결과텍스트.replace(\"urlwashere\",\"\")\n        total_selected_text.append(결과텍스트)\n        \n    DATA[\"selected_text\"] = total_selected_text\n    return DATA            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_file = 셀렉티드생성기(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_file","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_file=submission_file.drop([\"level_0\",\"index\", \"text\",\"sentiment\", \"negative\",\"neutral\", \"positive\", \"token\", \"padding_token\" ], axis=1)\nsubmission_file","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_file.to_csv(\"submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}