{"cells":[{"metadata":{"id":"334bTDN5fRkU"},"cell_type":"markdown","source":"# Sentiment Extraction -- Simple Neural Network"},{"metadata":{"id":"JzTnHqTkfxM7"},"cell_type":"markdown","source":"       Input                BiLSTM               BiLSTM   Dense        Targets\n       FastText  Embedding  w/ dropout                    Output       Output\n                                      Merge               Layer\n    +----------+  +-----+  +-----+    inputs     +-----+  +-------+  +---------+\n    |   tweets |->|embed|->|bidir|----+       +->|bidir|->|softmax|->|start_idx|\n    +----------+  +-----+  +-----+    |       |  +-----+  +-------+  +---------+\n    vecs (142,1)                   +--------+ |  \n                                   | concat |-+ \n                                   +--------+ |  \n    +----------+  +-----+  +-----+    |       |  +-----+  +-------+  +---------+\n    |sentiments|->|embed|->|bidir|----+       +->|bidir|->|softmax|->|end_idx  |\n    +----------+  +-----+  +-----+               +-----+  +-------+  +---------+\n    vecs (8,1)\n\n    [57 82 14 76 18 4 26 0] <=> ['n', 'e', 'u', 't', 'r', 'a', 'l'] \n\n    Embedding: Turns positive integers (indexes) into dense vectors of fixed size:\n    eg. [[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]]"},{"metadata":{"id":"IfGTJ7trHP9n"},"cell_type":"markdown","source":"Источник модели:\n\nhttps://github.com/nstsj/compling_nlp_hse_course/blob/master/notebooks/Question_answering.ipynb"},{"metadata":{"id":"C_Rb8EJDWsev","outputId":"a2678371-ab90-4b02-99a8-3ec54eaec383","executionInfo":{"status":"ok","timestamp":1587713753271,"user_tz":-180,"elapsed":3053,"user":{"displayName":"Alex Skornyakov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhvDGL0gITiWuZeBk8WuqiFgNjo86fRwcdf3MuZHg=s64","userId":"00614715397981539727"}},"trusted":true},"cell_type":"code","source":"import os\nfrom time import time\n\nimport pandas as pd\nimport numpy as np\n\n# for cross validation\nfrom sklearn.model_selection import StratifiedKFold\n\n# for neural network model\nfrom keras.models import Model\nfrom keras.layers import Embedding, Dropout\nfrom keras.layers import LSTM, Bidirectional, Dense, Input\nfrom keras.layers import concatenate\nfrom keras import backend as K\nfrom keras.preprocessing.sequence import pad_sequences\n\nimport keras\n\n# for fast-text vectorization\nimport gensim\n\nimport warnings\nwarnings.simplefilter(action='ignore')","execution_count":null,"outputs":[]},{"metadata":{"id":"bruYs0jJf26d","trusted":true},"cell_type":"code","source":"# config\nDATA_DIR = '../input/tweet-sentiment-extraction'\nTRAIN_DATA_FILE = 'train.csv'\nTEST_DATA_FILE = 'test.csv'\nSUBMISSION_FILE = 'submission.csv'\n\nRANDOM_STATE = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! ls ../input/traininglstmcheckpoints2","execution_count":null,"outputs":[]},{"metadata":{"id":"HyoSsKH7gCFG"},"cell_type":"markdown","source":"## Load Data"},{"metadata":{"id":"LI4ALYanKkAE","trusted":true},"cell_type":"code","source":"train_data = pd.read_csv(os.path.join(DATA_DIR, TRAIN_DATA_FILE)).fillna('')\ntest_data = pd.read_csv(os.path.join(DATA_DIR, TEST_DATA_FILE)).fillna('')","execution_count":null,"outputs":[]},{"metadata":{"id":"CzR_AbpDCCIL"},"cell_type":"markdown","source":"## FastText Model"},{"metadata":{"id":"1XgP9NnWoyVf","trusted":true},"cell_type":"code","source":"corpus = list(train_data['text']) \ncorpus.extend(list(test_data['text']))\ncorpus.extend(['neutral', 'positive', 'negative'])","execution_count":null,"outputs":[]},{"metadata":{"id":"-jvwT8o_mSkj","trusted":true},"cell_type":"code","source":"DIM = 200  # fast-text vector size\nft = gensim.models.FastText(corpus, size=DIM, sg=1)","execution_count":null,"outputs":[]},{"metadata":{"id":"BzfnICaymiFo","trusted":true},"cell_type":"code","source":"# Vocabulary\nvocab = set()\n\nfor context in corpus:\n    vocab.update(context)\n\nid2word = {i+1:word for i, word in enumerate(vocab)}\nword2id = {word:i for i, word in id2word.items()}\n\nembeddings = np.zeros((len(vocab) + 1, DIM))\n\nfor i in range(1, len(vocab)+1):\n    try:\n        embeddings[i] = ft[id2word[i]]\n    except KeyError:\n        embeddings[i] = np.random.rand((DIM))","execution_count":null,"outputs":[]},{"metadata":{"id":"8_TaKmrT7_cc"},"cell_type":"markdown","source":"## Training Data"},{"metadata":{"id":"ezxl5eolZa7L","trusted":true},"cell_type":"code","source":"contexts_train = list(train_data['text'])\nquestions_train = list(train_data['sentiment'])\n\nstarts = []\nends = []\nfor text, selected_text in zip(train_data['text'], train_data['selected_text']):\n  start = text.find(selected_text)\n  starts.append(start)\n  ends.append(start + len(selected_text))\n\nstarts = np.array(starts)  \nends = np.array(ends) \n\nMAX_LEN = max([len(c) for c in corpus])\nMAX_LEN_Q = max([len(c) for c in ['neutral', 'positive', 'negative']])  # anything wrong here?\n\n# Context\ncontexts_le = [[word2id[word] for word in context] for context in contexts_train]\nX_train_context = pad_sequences(contexts_le, MAX_LEN, padding='post')\n\n# Questions\nquestions_le = [[word2id[word] for word in question] for question in questions_train]\nX_train_question = pad_sequences(questions_le, MAX_LEN_Q, padding='post')","execution_count":null,"outputs":[]},{"metadata":{"id":"3IqHKiZf8c8k"},"cell_type":"markdown","source":"## Testing Data"},{"metadata":{"id":"SaVUcByXYTvS","trusted":true},"cell_type":"code","source":"contexts_test = list(test_data['text'])\nquestions_test = list(test_data['sentiment'])\n\ncontexts_le_test = [[word2id.get(word, 0) for word in context] for context in contexts_test]\nX_test_context = pad_sequences(contexts_le_test, MAX_LEN, padding='post')\n\nquestions_le_test = [[word2id.get(word, 0) for word in question] for question in questions_test]\nX_test_question = pad_sequences(questions_le_test, MAX_LEN_Q, padding='post')","execution_count":null,"outputs":[]},{"metadata":{"id":"xhmnK7A9tsUM"},"cell_type":"markdown","source":"## NN Model"},{"metadata":{"id":"YufgGGMpr6Yd","trusted":true},"cell_type":"code","source":"def create_model():\n    \"\"\"Return a model.\"\"\"\n    vocab_size = len(vocab)+1\n\n    # Input 1 - Tweets \n    context_input = Input(shape=(MAX_LEN, ), name='context_input')\n    emb_c = Embedding(input_dim=vocab_size,\n                      output_dim=200,\n                      weights=[embeddings], \n                      input_length=MAX_LEN,\n                      trainable=False)(context_input)\n\n    lstm_out_c = Bidirectional(LSTM(50,  return_sequences=True,))(emb_c)\n    drop_1 = Dropout(0.1)(lstm_out_c)\n\n    # Input 2 - Sentiment labels\n    ques_input = Input(shape=(MAX_LEN_Q, ), name='ques_input')\n    emb_q = Embedding(input_dim=vocab_size,\n                      output_dim=200,\n                      weights=[embeddings], \n                      input_length=MAX_LEN_Q, \n                      trainable=False)(ques_input)\n    lstm_out_q = Bidirectional(LSTM(50,return_sequences=True,) )(emb_q)\n    drop_2 = Dropout(0.1)(lstm_out_q)\n\n    # Merge inputs\n    merge_layer = concatenate([drop_1, drop_2], axis=1)\n    biLSTM_s = Bidirectional(LSTM(10,))(merge_layer)\n    biLSTM_e = Bidirectional(LSTM(10,))(merge_layer)\n\n    # Output 1\n    softmax_1 = Dense(MAX_LEN,\n                      activation='softmax',\n                      name='start')(biLSTM_s)\n\n    # Output 2\n    softmax_2 = Dense(MAX_LEN,\n                      activation='softmax',\n                      name='end')(biLSTM_e)\n\n    model = Model(inputs=[context_input, ques_input],\n                  outputs=[softmax_1, softmax_2]\n                  )\n\n    model.compile(optimizer='adam',\n                  loss='sparse_categorical_crossentropy',\n                  metrics=['accuracy']\n                  )\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"id":"SQfiShVFVrMu","outputId":"fe01503f-a2a0-436a-f46b-06bc812d047a","executionInfo":{"status":"ok","timestamp":1587713770232,"user_tz":-180,"elapsed":19459,"user":{"displayName":"Alex Skornyakov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhvDGL0gITiWuZeBk8WuqiFgNjo86fRwcdf3MuZHg=s64","userId":"00614715397981539727"}},"trusted":true},"cell_type":"code","source":"model = create_model()\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"id":"ScWlR4v5wBat"},"cell_type":"markdown","source":"## Evaluation Metric"},{"metadata":{"id":"tIyRtHUUv_yO","trusted":true},"cell_type":"code","source":"def jaccard(top_selected):\n    str1, str2 = top_selected\n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    if (len(a) == 0) & (len(b) == 0):\n        return 0.5\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))","execution_count":null,"outputs":[]},{"metadata":{"id":"CC3V5HkgvJeT"},"cell_type":"markdown","source":"## Train & Evaluate Model"},{"metadata":{"id":"DIW5eNMJr_d1","outputId":"12a7efc5-bc41-4674-ae4b-1d028b5aec9c","executionInfo":{"status":"ok","timestamp":1587746094591,"user_tz":-180,"elapsed":1218802,"user":{"displayName":"Alex Skornyakov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhvDGL0gITiWuZeBk8WuqiFgNjo86fRwcdf3MuZHg=s64","userId":"00614715397981539727"}},"trusted":true},"cell_type":"code","source":"RESUME = False  # FALSE DURING SUBMISSION\ntest_starts_pred = np.zeros((X_test_context.shape[0], MAX_LEN))\ntest_ends_pred = np.zeros((X_test_context.shape[0], MAX_LEN))\n\njac = []  # container for Jaccard scores per fold\n\n# cp_dir = os.path.join('drive', 'My Drive', 'training_lstm_checkpoints')  # in Colab\ncp_dir = '../input/training-lstm-checkpoints'\ncp_dir = '../input/traininglstmcheckpoints2'\n# Commented out because Kaggle raises OSError: [Errno 30] Read-only file system: '../input/training_lstm_checkpoints'\n# try:\n#     os.mkdir(cp_dir)\n# except FileExistsError:\n#     pass\n\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n\nt0 = time()\nfor fold, (train_idx, test_idx) in enumerate(skf.split(train_data['text'], train_data['sentiment'])):\n    print('>' * 20)\n    print(f'>>> FOLD {fold + 1}')\n    print('>' * 20)\n\n    K.clear_session()\n    # TO-DO: need to load the latest checkpoint.\n    # Is re-loading the model contrary to the idea of k-fold validation?\n    if RESUME:\n        try:\n            del model\n        except NameError:\n            pass\n        model = keras.models.load_model(os.path.join(cp_dir, '0-lstm.ckpt'))\n    else:\n        model = create_model()\n\n    cp_callback = keras.callbacks.ModelCheckpoint(os.path.join(cp_dir, f'{fold}-lstm.ckpt'),\n                                                     monitor='val_loss',\n                                                     verbose=1,\n                                                     save_best_only=True,\n                                                     save_weights_only=False,\n                                                     mode='auto',\n                                                     period=1\n                                                     )\n\n  \n    training_data=({'context_input': X_train_context[train_idx],\n                    'ques_input': X_train_question[train_idx]}, \n                    {'start': starts[train_idx],\n                      'end': ends[train_idx]})\n\n    validation_data=({'context_input': X_train_context[test_idx],\n                      'ques_input': X_train_question[test_idx]}, \n                    {'start': starts[test_idx],\n                      'end': ends[test_idx]})\n\n    # COMMENT OUT DURING SUBMISSION\n    #############################################################\n#     model.fit(training_data[0],\n#               training_data[1], \n#               batch_size=1024,  # 32\n#               epochs=2,\n#               verbose=1,\n#               shuffle=True,\n#               callbacks=[cp_callback],\n#               validation_data=(validation_data[0],\n#                               validation_data[1]\n#                               )\n#               )\n    #############################################################\n    \n    print('Loading model...')\n    model.load_weights(os.path.join(cp_dir, f'{fold}-lstm.ckpt'))\n\n    print('Predicting validation data...')\n    preds = model.predict(validation_data[0], verbose=1)\n    starts_pred = preds[0].argmax(axis=1)\n    ends_pred = preds[1].argmax(axis=1)\n\n    print('Predicting Kaggle test data...')\n    test_data_dict = {'context_input': X_test_context,\n                  'ques_input': X_test_question}\n\n    test_preds = model.predict(test_data_dict, verbose=1)\n    # test_starts_pred += test_preds[0].argmax(axis=1)/skf.n_splits\n    # test_ends_pred += test_preds[1].argmax(axis=1)/skf.n_splits\n    test_starts_pred += test_preds[0]/skf.n_splits\n    test_ends_pred += test_preds[1]/skf.n_splits\n\n    print('Evaluating...')\n    res = pd.DataFrame()\n    res['pred_starts'] = starts_pred\n    res['pred_ends'] = ends_pred\n    \n    columns = ['text', 'sentiment', 'selected_text']\n    res[columns] = train_data[columns].loc[test_idx].reset_index(drop=True)\n\n    res['pred_select'] = res[['text', 'pred_starts', 'pred_ends']].apply(lambda x: x[0][x[1]:x[2]], axis=1)\n    # Handle cases where start >= end\n    condition = res['pred_starts'] >= res['pred_ends']\n    res.loc[:, 'pred_select'][condition] = res.loc[:, 'text'][condition]\n\n    res['score'] = res[['pred_select', 'selected_text']].apply(jaccard, axis=1)\n\n    print(res.groupby('sentiment')['score'].mean())\n    mean_jac = res['score'].mean()\n    print(f\"Mean score in Fold {fold + 1}: {mean_jac}\")\n\n    jac.append(mean_jac)\n\nprint(f'Done in {(time() - t0)} seconds')\n\ntotal_score = np.mean(jac)\nprint('>' * 10)\nprint(f'Total Jaccard score for 5 folds = {total_score}')","execution_count":null,"outputs":[]},{"metadata":{"id":"hzDwztfvThcH"},"cell_type":"markdown","source":"## Kaggle submission"},{"metadata":{"id":"VhukA1jSQfvQ","outputId":"93f4e77c-c12b-4564-dd24-8d7e6495e312","executionInfo":{"status":"ok","timestamp":1587746418338,"user_tz":-180,"elapsed":794,"user":{"displayName":"Alex Skornyakov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhvDGL0gITiWuZeBk8WuqiFgNjo86fRwcdf3MuZHg=s64","userId":"00614715397981539727"}},"trusted":true},"cell_type":"code","source":"temp_df = pd.DataFrame()\ntemp_df['pred_starts'] = test_starts_pred.argmax(axis=1)\ntemp_df['pred_ends'] = test_ends_pred.argmax(axis=1)\n\n# columns = ['text', 'sentiment']\ntemp_df['text'] = test_data['text']\nprint(temp_df.head())\ntemp_df['selected_text'] = temp_df[[\n                          'text', \n                          'pred_starts', \n                          'pred_ends']\n                                ].apply(lambda x: x[0][x[1]:x[2]], axis=1)\n\n# Handle cases where start >= end\ncondition = temp_df['pred_starts'] >= temp_df['pred_ends']\ntemp_df.loc[:, 'selected_text'][condition] = temp_df.loc[:, 'text'][condition]\n\nsubmission_df = pd.DataFrame() \nsubmission_df['textID'] = test_data['textID']\nsubmission_df['selected_text'] = temp_df['selected_text']\nsubmission_df.to_csv(SUBMISSION_FILE, index = False)","execution_count":null,"outputs":[]},{"metadata":{"id":"N5DY1idNvQS6","outputId":"07a1a35a-da3a-4290-e739-9580a32fe3f4","executionInfo":{"status":"ok","timestamp":1587746419876,"user_tz":-180,"elapsed":497,"user":{"displayName":"Alex Skornyakov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhvDGL0gITiWuZeBk8WuqiFgNjo86fRwcdf3MuZHg=s64","userId":"00614715397981539727"}},"trusted":true},"cell_type":"code","source":"print(temp_df['pred_starts'].unique())\ntemp_df['pred_ends'].unique()","execution_count":null,"outputs":[]},{"metadata":{"id":"6n3Fvk5MSmq_","outputId":"34158d1c-84b6-4ad8-90a9-051b6f7cc0cf","executionInfo":{"status":"ok","timestamp":1587746421557,"user_tz":-180,"elapsed":562,"user":{"displayName":"Alex Skornyakov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhvDGL0gITiWuZeBk8WuqiFgNjo86fRwcdf3MuZHg=s64","userId":"00614715397981539727"}},"trusted":true},"cell_type":"code","source":"test_data.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"IwdmM1T8iW3C","outputId":"90233b5b-91f7-4b7f-a697-f2a74928b7ee","executionInfo":{"status":"ok","timestamp":1587746424780,"user_tz":-180,"elapsed":917,"user":{"displayName":"Alex Skornyakov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhvDGL0gITiWuZeBk8WuqiFgNjo86fRwcdf3MuZHg=s64","userId":"00614715397981539727"}},"trusted":true},"cell_type":"code","source":"submission_df.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"z6v3pdE0wCS1","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predictions -vs- Tweets by Length"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(test_data['text'].map(len), bins=40)\nplt.hist(submission_df['selected_text'].map(len), alpha=0.8, bins=40)\nplt.title('Lengths of Tweets -vs- Predicted Substrings in testing data\\nNeural Network')\nplt.xlabel('Char length')\nplt.ylabel('How often')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predictions -vs- Tweets by string length"},{"metadata":{"trusted":true},"cell_type":"code","source":"# plt.hist(res['text'].map(len), bins=40)\nplt.hist(res['selected_text'].map(len), alpha=0.8, bins=40)\n# plt.title('Lengths of Tweets -vs- Selected Substrings in training data\\nNeural Network')\nplt.title('Distribution of Lengths of TRUE Substrings in Training Data')\nplt.xlabel('Char length')\nplt.ylabel('How often')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plt.hist(res['text'].map(len), bins=40)\nplt.hist(res['pred_select'].map(len), alpha=0.8, bins=40)\n# plt.title('Lengths of Tweets -vs- Predicted Substrings in training data\\nNeural Network')\nplt.title('Distribution of Lengths of PREDICTED Substrings in Training Data')\nplt.xlabel('Char length')\nplt.ylabel('How often')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plt.hist(test_data['text'].map(len),  alpha=1, bins=40)\nplt.hist(submission_df['selected_text'].map(len), alpha=0.8, bins=40)\nplt.title('Distribution of Lengths of PREDICTED Substrings in Testing Data')\nplt.xlabel('Char length')\nplt.ylabel('How often')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"colab":{"name":"dl_simple_nn.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1G3Aqg9CVopQwaHButhKArdHNyPE0ZYoa","authorship_tag":"ABX9TyNsbUnX/RjxHG/6s5uBxlyX"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":4}