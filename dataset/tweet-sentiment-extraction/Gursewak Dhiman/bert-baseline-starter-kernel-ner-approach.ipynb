{"cells":[{"metadata":{},"cell_type":"markdown","source":"# About this kernel\n### This kernel is Just exploring the possibilities of using BERT in this competition as NER approach.\n### This is just a starter kernel, alot can be improved from here.\n\n### Inspired from this @akensert's kernel [bert-base-tf2-0-now-huggingface-transformer](https://www.kaggle.com/akensert/bert-base-tf2-0-now-huggingface-transformer)\n### These kernels were very helpful [tensorflow-roberta-0-705](https://www.kaggle.com/cdeotte/tensorflow-roberta-0-705) and [bert-base-uncased-using-pytorch](https://www.kaggle.com/abhishek/bert-base-uncased-using-pytorch)\n### <font color='red'>If you find this kernel helpful please upvote ðŸ˜Š. (Don't Just Fork Only)</font>"},{"metadata":{},"cell_type":"markdown","source":"# Import the necessary libraries"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import os\nimport gc\nimport numpy as np \nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport tokenizers\nfrom transformers import BertTokenizer,BertConfig,TFBertModel\nfrom tqdm import tqdm\n\ntqdm.pandas()\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"DATA_PATH = '/kaggle/input/tweet-sentiment-extraction/'\ntrain_df = pd.read_csv(DATA_PATH+'train.csv')\ntest_df = pd.read_csv(DATA_PATH+'test.csv')\nsubmission_df = pd.read_csv(DATA_PATH+'sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class config:\n    MAX_LEN = 128\n    TRAIN_BATCH_SIZE = 64\n    VALID_BATCH_SIZE = 32\n    TEST_BATCH_SIZE = 32\n    EPOCHS = 5\n    BERT_CONFIG = \"/kaggle/input/bertconfig/bert-base-uncased-config.json\" \n    BERT_PATH = \"/kaggle/input/bert-base-uncased-huggingface-transformer/\"\n    TOKENIZER = tokenizers.BertWordPieceTokenizer(\n        f\"{BERT_PATH}/bert-base-uncased-vocab.txt\", \n        lowercase=True)\n    SAVEMODEL_PATH = '/kaggle/input/tftweetfinetuned/finetuned_bert.h5'\n    THRESHOLD = 0.4","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create Targets"},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_data(tweet, selected_text, tokenizer):\n    len_st = len(selected_text)\n    idx0 = None\n    idx1 = None\n\n    for ind in (i for i, e in enumerate(tweet) if e == selected_text[0]):\n        if tweet[ind: ind+len_st] == selected_text:\n            idx0 = ind\n            idx1 = ind + len_st\n            break\n\n    char_targets = [0] * len(tweet)\n    if idx0 != None and idx1 != None:\n        for ct in range(idx0, idx1):\n            char_targets[ct] = 1\n            \n    tok_tweet = tokenizer.encode(tweet)\n    input_ids_orig = tok_tweet.ids\n    tweet_offsets = tok_tweet.offsets\n\n    target_idx = []\n    for j, (offset1, offset2) in enumerate(tweet_offsets):\n        if sum(char_targets[offset1: offset2]) > 0:\n            target_idx.append(j)\n            \n    targets = [0] * len(input_ids_orig)\n    for idx in target_idx:\n        targets[idx] = 1\n    return targets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['targets'] = train_df.progress_apply(lambda row: process_data(\n                                                                    str(row['text']), \n                                                                    str(row['selected_text']),\n                                                                    config.TOKENIZER),\n                                                                    axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## pad all the targets\ntrain_df['targets'] = train_df['targets'].apply(lambda x :x + [0] * (config.MAX_LEN-len(x)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Convert into Bert Format"},{"metadata":{"trusted":true},"cell_type":"code","source":"def _convert_to_transformer_inputs(text, tokenizer, max_sequence_length):\n    inputs = tokenizer.encode(text)\n    input_ids =  inputs.ids\n    input_masks = inputs.attention_mask\n    input_segments = inputs.type_ids\n    padding_length = max_sequence_length - len(input_ids)\n    padding_id = 0\n    input_ids = input_ids + ([padding_id] * padding_length)\n    input_masks = input_masks + ([0] * padding_length)\n    input_segments = input_segments + ([0] * padding_length)\n    return [input_ids, input_masks, input_segments]\n\ndef compute_input_arrays(df, tokenizer, max_sequence_length):\n    input_ids, input_masks, input_segments = [], [], []\n    for _, instance in tqdm(df.iterrows()):\n        ids, masks, segments= _convert_to_transformer_inputs(str(instance.text),tokenizer, max_sequence_length)\n        input_ids.append(ids)\n        input_masks.append(masks)\n        input_segments.append(segments)\n        \n    return [np.asarray(input_ids, dtype=np.int32), \n            np.asarray(input_masks, dtype=np.int32), \n            np.asarray(input_segments, dtype=np.int32)]\n\ndef compute_output_arrays(df, columns):\n    return np.asarray(df[columns].values.tolist())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"outputs = compute_output_arrays(train_df,'targets')\ninputs = compute_input_arrays(train_df, config.TOKENIZER, config.MAX_LEN)\ntest_inputs = compute_input_arrays(test_df, config.TOKENIZER, config.MAX_LEN)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model():\n    ids = tf.keras.layers.Input((config.MAX_LEN,), dtype=tf.int32)\n    mask = tf.keras.layers.Input((config.MAX_LEN,), dtype=tf.int32)\n    attn = tf.keras.layers.Input((config.MAX_LEN,), dtype=tf.int32)\n    bert_conf = BertConfig() \n    #bert_conf.output_hidden_states = True\n    bert_model = TFBertModel.from_pretrained(config.BERT_PATH+'/bert-base-uncased-tf_model.h5', config=bert_conf)\n    \n    output = bert_model(ids, attention_mask=mask, token_type_ids=attn)\n    \n    out = tf.keras.layers.Dropout(0.1)(output[0]) \n    out = tf.keras.layers.Conv1D(1,1)(out)\n    out = tf.keras.layers.Flatten()(out)\n    out = tf.keras.layers.Activation('sigmoid')(out)\n    model = tf.keras.models.Model(inputs=[ids, mask, attn], outputs=out)\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"K.clear_session()\nmodel = create_model()\noptimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\nmodel.compile(loss='binary_crossentropy', optimizer=optimizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if not os.path.exists(config.SAVEMODEL_PATH):\n    model.fit(inputs,outputs, epochs=config.EPOCHS, batch_size=config.TRAIN_BATCH_SIZE)\n    model.save_weights(f'finetuned_bert.h5')\nelse:\n    model.load_weights(config.SAVEMODEL_PATH)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = model.predict(test_inputs, batch_size=32, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Will change to higher threshold in upcoming versions\nthreshold = config.THRESHOLD\npred = np.where(predictions>threshold,1,0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def decode_tweet(original_tweet,idx_start,idx_end,offsets):\n    filtered_output  = \"\"\n    for ix in range(idx_start, idx_end + 1):\n        filtered_output += original_tweet[offsets[ix][0]: offsets[ix][1]]\n        if (ix+1) < len(offsets) and offsets[ix][1] < offsets[ix+1][0]:\n            filtered_output += \" \"\n    return filtered_output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"outputs = []\nfor test_idx in range(test_df.shape[0]):\n    indexes = list(np.where(pred[test_idx]==1)[0])\n    text = str(test_df.loc[test_idx,'text'])\n    encoded_text = config.TOKENIZER.encode(text)\n    if len(indexes)>0:\n        start = indexes[0]\n        end =  indexes[-1]\n    else:  ### if we found nothing above threshold\n        start = 0\n        end = len(encoded_text.ids) - 1\n    if end >= len(encoded_text.ids): ## -1 for SEP token at last\n        end = len(encoded_text.ids) - 1\n    if start>end: \n        selected_text = test_df.loc[test_idx,'text']\n    else:\n        selected_text = decode_tweet(text,start,end,encoded_text.offsets)\n    outputs.append(selected_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df['selected_text'] = outputs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def replacer(row):\n    if row['sentiment'] == 'neutral' or len(row['text'].split())<2:\n        return row['text']\n    else:\n        return row['selected_text']\ntest_df['selected_text'] = test_df.apply(replacer,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head(100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df['selected_text'] = test_df['selected_text']\nsubmission_df.to_csv('submission.csv',index=False)\npd.set_option('max_colwidth', 80)\nsubmission_df.sample(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":4}