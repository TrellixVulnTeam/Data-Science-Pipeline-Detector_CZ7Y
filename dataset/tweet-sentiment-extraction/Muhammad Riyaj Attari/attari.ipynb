{"cells":[{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"0\"></a>\n# [Tweet Sentiment Extraction](https://www.kaggle.com/c/tweet-sentiment-extraction)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Overview\n\nIn this notebook, I analyze and visualize the outliers of the NLP solution from very good notebook \"[TSE2020] RoBERTa (CNN) & Random Seed Distribution\"(https://www.kaggle.com/khoongweihao/tse2020-roberta-cnn-random-seed-distribution) using the functions from my notebook [NLP - EDA, Bag of Words, TF IDF, GloVe, BERT](https://www.kaggle.com/vbmokin/nlp-eda-bag-of-words-tf-idf-glove-bert) including PCA processing, Kmeans clustering, WordCloud and others. More over I try to improve the original solution.\n\nAdd chapters \"**Subtext analysis**\" and \"**Metric analysis**\" from the commit 10.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Results of analysis:\n1. Outlier analysis of the best solutions on basic roBERTa - pls. see https://www.kaggle.com/c/tweet-sentiment-extraction/discussion/155419\n2. Analysis of the predictions with the worst score=0 from roBERTa - pls. see https://www.kaggle.com/c/tweet-sentiment-extraction/discussion/155616\n3. New (commit 22): **analysis of 3 or more repetitions of characters in words**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Acknowledgements\n* [NLP - EDA, Bag of Words, TF IDF, GloVe, BERT](https://www.kaggle.com/vbmokin/nlp-eda-bag-of-words-tf-idf-glove-bert)\n* [COVID-19 (Week5) Global Forecasting - EDA&ExtraTR](https://www.kaggle.com/vbmokin/covid-19-week5-global-forecasting-eda-extratr)\n* [TSE2020] RoBERTa (CNN) & Random Seed Distribution (https://www.kaggle.com/khoongweihao/tse2020-roberta-cnn-random-seed-distribution)\n* Chris Deotte's post: https://www.kaggle.com/c/tweet-sentiment-extraction/discussion/142404#809872\n* [Faster (2x) TF roBERTa](https://www.kaggle.com/seesee/faster-2x-tf-roberta)\n* Many thanks to Chris Deotte for his TF roBERTa dataset at https://www.kaggle.com/cdeotte/tf-roberta\n* https://www.kaggle.com/abhishek/roberta-inference-5-folds","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"0.1\"></a>\n## Table of Contents\n\n1. [Import libraries](#1)\n1. [Download data & FE](#2)\n1. [Model tuning](#3)\n   - [My upgrade of parameters](#3.1)\n   - [Model training](#3.2)\n1. [Submission](#4)\n1. [Outlier analysis](#5)\n    - [Training prediction result visualization](#5.1)\n    - [WordCloud](#5.2)\n    - [Subtext analysis](#5.3)\n    - [Metric analysis](#5.4)\n    - [PCA visualization](#5.5)\n    - [Clustering](#5.6)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 1. Import libraries <a class=\"anchor\" id=\"1\"></a>\n\n[Back to Table of Contents](#0.1)","execution_count":null},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib\nimport matplotlib.patches as mpatches\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport plotly.express as px\nimport seaborn as sns; sns.set(style='white')\nfrom mpl_toolkits.mplot3d import Axes3D\n\nfrom wordcloud import WordCloud\nfrom sklearn.decomposition import PCA, TruncatedSVD\nimport math\nimport pickle\n\nfrom scipy.cluster import hierarchy\nfrom scipy.spatial.distance import pdist\nfrom sklearn.manifold import TSNE\nfrom sklearn.cluster import KMeans\n\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom transformers import *\nimport tokenizers\nfrom sklearn.model_selection import StratifiedKFold\n\npd.set_option('max_colwidth', 40)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Download data & FE <a class=\"anchor\" id=\"2\"></a>\n\n[Back to Table of Contents](#0.1)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Code from notebook https://www.kaggle.com/khoongweihao/tse2020-roberta-cnn-random-seed-distribution?scriptVersionId=34448972","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN = 96\nPATH = '../input/tf-roberta/'\ntokenizer = tokenizers.ByteLevelBPETokenizer(\n    vocab_file=PATH+'vocab-roberta-base.json', \n    merges_file=PATH+'merges-roberta-base.txt', \n    lowercase=True,\n    add_prefix_space=True\n)\nEPOCHS = 3 # originally 3\nBATCH_SIZE = 32 # originally 32\nPAD_ID = 1\nSEED = 88888\nLABEL_SMOOTHING = 0.1\ntf.random.set_seed(SEED)\nnp.random.seed(SEED)\nsentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}\ntrain = pd.read_csv('../input/tweet-sentiment-extraction/train.csv').fillna('')\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"test = pd.read_csv('../input/tweet-sentiment-extraction/test.csv').fillna('')\n\nct = test.shape[0]\ninput_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\nattention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\ntoken_type_ids_t = np.zeros((ct,MAX_LEN),dtype='int32')\n\nfor k in range(test.shape[0]):\n        \n    # INPUT_IDS\n    text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n    enc = tokenizer.encode(text1)                \n    s_tok = sentiment_id[test.loc[k,'sentiment']]\n    input_ids_t[k,:len(enc.ids)+3] = [0, s_tok] + enc.ids + [2]\n    attention_mask_t[k,:len(enc.ids)+3] = 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Model tuning <a class=\"anchor\" id=\"3\"></a>\n\n[Back to Table of Contents](#0.1)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 3.1. My upgrade of parameters <a class=\"anchor\" id=\"3.1\"></a>\n\n[Back to Table of Contents](#0.1)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Dropout_new = 0.15     # originally 0.1\nn_split = 5            # originally 5\nlr = 3e-5              # originally 3e-5","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Previous successful commits","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Commit 3 (with original parameters)\n\n* Dropout_new = 0.1\n* n_split = 5\n* lr = 3e-5\n\nLB = 0.711","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Commit 5\n\n* Dropout_new = 0.15\n* n_split = 5\n* lr = 3e-5\n\nLB = 0.713","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Commit 6\n\n* Dropout_new = 0.15\n* n_split = 7\n* lr = 3e-5\n\nLB = 0.709","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Commit 7\n\n* Dropout_new = 0.15\n* n_split = 5\n* lr = 4e-5\n\nLB = 0.709","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Commit 8\n\n* Dropout_new = 0.15\n* n_split = 5\n* lr = 2e-5\n\nLB = 0.712","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Commit 9\n\n* Dropout_new = 0.15\n* n_split = 5\n* lr = 3e-5\n* LeakyReLU_alpha=0.05\n\nLB = 0.711","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Commit 10\n\n* Dropout_new = 0.15\n* n_split = 5\n* lr = 3e-5\n* LeakyReLU_alpha=0.3\n\nLB = 0.711","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Commit 12\n\n* Dropout_new = 0.15\n* n_split = 5\n* lr = 3e-5\n* SEED = 42\n\nLB = 0.711","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Commit 13\n\n* Dropout_new = 0.16\n* n_split = 5\n* lr = 3e-5\n\nLB = 0.711","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Commit 14\n\n* Dropout_new = 0.15\n* n_split = 5\n* lr = 3e-5\n\n**LB = 0.715 (the best)**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Commit 15\n\n* Dropout_new = 0.16\n* n_split = 5\n* lr = 3e-5\n* SEED = 777\n\nLB = 0.710","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Commit 17\n\n* Dropout_new = 0.15\n* n_split = 5\n* lr = 1e-5\n\nLB = 0.709","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Commit 18\n\n* Dropout_new = 0.15\n* n_split = 5\n* lr = 3e-5\n* BATCH_SIZE = 24      # originally 32\n\nLB = 0.704","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Commit 19\n\n* Dropout_new = 0.125\n* n_split = 5\n* lr = 3e-5\n\nLB = 0.711","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Commit 20\n\n* Dropout_new = 0.15\n* n_split = 5\n* lr = 1e-4\n\nLB = 0.709","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Commit 21\n\n* Dropout_new = 0.15\n* n_split = 5\n* lr = 1e-4\n* num_cnn2 = 96          # originally 64\n\nLB = 0.712","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 3.2. Model training <a class=\"anchor\" id=\"3.2\"></a>\n\n[Back to Table of Contents](#0.1)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Code from notebook https://www.kaggle.com/khoongweihao/tse2020-roberta-cnn-random-seed-distribution?scriptVersionId=34448972\n\n**Upgrade:** add prediction for training data for Outlier analysis and parameters tuning","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import pickle\n\ndef save_weights(model, dst_fn):\n    weights = model.get_weights()\n    with open(dst_fn, 'wb') as f:\n        pickle.dump(weights, f)\n\n\ndef load_weights(model, weight_fn):\n    with open(weight_fn, 'rb') as f:\n        weights = pickle.load(f)\n    model.set_weights(weights)\n    return model\n\ndef loss_fn(y_true, y_pred):\n    # adjust the targets for sequence bucketing\n    ll = tf.shape(y_pred)[1]\n    y_true = y_true[:, :ll]\n    loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred,\n        from_logits=False, label_smoothing=LABEL_SMOOTHING)\n    loss = tf.reduce_mean(loss)\n    return loss\n\n\ndef build_model():\n    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    padding = tf.cast(tf.equal(ids, PAD_ID), tf.int32)\n\n    lens = MAX_LEN - tf.reduce_sum(padding, -1)\n    max_len = tf.reduce_max(lens)\n    ids_ = ids[:, :max_len]\n    att_ = att[:, :max_len]\n    tok_ = tok[:, :max_len]\n\n    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n    x = bert_model(ids_,attention_mask=att_,token_type_ids=tok_)\n    \n    x1 = tf.keras.layers.Dropout(Dropout_new)(x[0])\n    x1 = tf.keras.layers.Conv1D(768, 2,padding='same')(x1)\n    x1 = tf.keras.layers.LeakyReLU()(x1)\n    x1 = tf.keras.layers.Conv1D(64, 2,padding='same')(x1)\n    x1 = tf.keras.layers.Dense(1)(x1)\n    x1 = tf.keras.layers.Flatten()(x1)\n    x1 = tf.keras.layers.Activation('softmax')(x1)\n    \n    x2 = tf.keras.layers.Dropout(Dropout_new)(x[0]) \n    x2 = tf.keras.layers.Conv1D(768, 2,padding='same')(x2)\n    x2 = tf.keras.layers.LeakyReLU()(x2)\n    x2 = tf.keras.layers.Conv1D(64, 2, padding='same')(x2)\n    x2 = tf.keras.layers.Dense(1)(x2)\n    x2 = tf.keras.layers.Flatten()(x2)\n    x2 = tf.keras.layers.Activation('softmax')(x2)\n\n    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n    optimizer = tf.keras.optimizers.Adam(learning_rate=lr) \n    model.compile(loss=loss_fn, optimizer=optimizer)\n    \n    # this is required as `model.predict` needs a fixed size!\n    x1_padded = tf.pad(x1, [[0, 0], [0, MAX_LEN - max_len]], constant_values=0.)\n    x2_padded = tf.pad(x2, [[0, 0], [0, MAX_LEN - max_len]], constant_values=0.)\n    \n    padded_model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1_padded,x2_padded])\n    return model, padded_model","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    if (len(a)==0) & (len(b)==0): return 0.5\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"ct = train.shape[0]\ninput_ids = np.ones((ct,MAX_LEN),dtype='int32')\nattention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\ntoken_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\nstart_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\nend_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n\nfor k in range(train.shape[0]):\n    \n    # FIND OVERLAP\n    text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n    text2 = \" \".join(train.loc[k,'selected_text'].split())\n    idx = text1.find(text2)\n    chars = np.zeros((len(text1)))\n    chars[idx:idx+len(text2)]=1\n    if text1[idx-1]==' ': chars[idx-1] = 1 \n    enc = tokenizer.encode(text1) \n        \n    # ID_OFFSETS\n    offsets = []; idx=0\n    for t in enc.ids:\n        w = tokenizer.decode([t])\n        offsets.append((idx,idx+len(w)))\n        idx += len(w)\n    \n    # START END TOKENS\n    toks = []\n    for i,(a,b) in enumerate(offsets):\n        sm = np.sum(chars[a:b])\n        if sm>0: toks.append(i) \n        \n    s_tok = sentiment_id[train.loc[k,'sentiment']]\n    input_ids[k,:len(enc.ids)+3] = [0, s_tok] + enc.ids + [2]\n    attention_mask[k,:len(enc.ids)+3] = 1\n    if len(toks)>0:\n        start_tokens[k,toks[0]+2] = 1\n        end_tokens[k,toks[-1]+2] = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\njac = []; VER='v0'; DISPLAY=1 # USE display=1 FOR INTERACTIVE\noof_start = np.zeros((input_ids.shape[0],MAX_LEN))\noof_end = np.zeros((input_ids.shape[0],MAX_LEN))\npreds_start_train = np.zeros((input_ids.shape[0],MAX_LEN))\npreds_end_train = np.zeros((input_ids.shape[0],MAX_LEN))\npreds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\npreds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\n\nskf = StratifiedKFold(n_splits=n_split,shuffle=True,random_state=SEED)\nfor fold,(idxT,idxV) in enumerate(skf.split(input_ids,train.sentiment.values)):\n\n    print('#'*25)\n    print('### FOLD %i'%(fold+1))\n    print('#'*25)\n    \n    K.clear_session()\n    model, padded_model = build_model()\n        \n    #sv = tf.keras.callbacks.ModelCheckpoint(\n    #    '%s-roberta-%i.h5'%(VER,fold), monitor='val_loss', verbose=1, save_best_only=True,\n    #    save_weights_only=True, mode='auto', save_freq='epoch')\n    inpT = [input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]]\n    targetT = [start_tokens[idxT,], end_tokens[idxT,]]\n    inpV = [input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]]\n    targetV = [start_tokens[idxV,], end_tokens[idxV,]]\n    # sort the validation data\n    shuffleV = np.int32(sorted(range(len(inpV[0])), key=lambda k: (inpV[0][k] == PAD_ID).sum(), reverse=True))\n    inpV = [arr[shuffleV] for arr in inpV]\n    targetV = [arr[shuffleV] for arr in targetV]\n    weight_fn = '%s-roberta-%i.h5'%(VER,fold)\n    for epoch in range(1, EPOCHS + 1):\n        # sort and shuffle: We add random numbers to not have the same order in each epoch\n        shuffleT = np.int32(sorted(range(len(inpT[0])), key=lambda k: (inpT[0][k] == PAD_ID).sum() + np.random.randint(-3, 3), reverse=True))\n        # shuffle in batches, otherwise short batches will always come in the beginning of each epoch\n        num_batches = math.ceil(len(shuffleT) / BATCH_SIZE)\n        batch_inds = np.random.permutation(num_batches)\n        shuffleT_ = []\n        for batch_ind in batch_inds:\n            shuffleT_.append(shuffleT[batch_ind * BATCH_SIZE: (batch_ind + 1) * BATCH_SIZE])\n        shuffleT = np.concatenate(shuffleT_)\n        # reorder the input data\n        inpT = [arr[shuffleT] for arr in inpT]\n        targetT = [arr[shuffleT] for arr in targetT]\n        model.fit(inpT, targetT, \n            epochs=epoch, initial_epoch=epoch - 1, batch_size=BATCH_SIZE, verbose=DISPLAY, callbacks=[],\n            validation_data=(inpV, targetV), shuffle=False)  # don't shuffle in `fit`\n        save_weights(model, weight_fn)\n\n    print('Loading model...')\n    # model.load_weights('%s-roberta-%i.h5'%(VER,fold))\n    load_weights(model, weight_fn)\n\n    print('Predicting OOF...')\n    oof_start[idxV,],oof_end[idxV,] = padded_model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n    \n    print('Predicting all Train for Outlier analysis...')\n    preds_train = padded_model.predict([input_ids,attention_mask,token_type_ids],verbose=DISPLAY)\n    preds_start_train += preds_train[0]/skf.n_splits\n    preds_end_train += preds_train[1]/skf.n_splits\n\n    print('Predicting Test...')\n    preds = padded_model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n    preds_start += preds[0]/skf.n_splits\n    preds_end += preds[1]/skf.n_splits\n    \n    # DISPLAY FOLD JACCARD\n    all = []\n    for k in idxV:\n        a = np.argmax(oof_start[k,])\n        b = np.argmax(oof_end[k,])\n        if a>b: \n            st = train.loc[k,'text'] # IMPROVE CV/LB with better choice here\n        else:\n            text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n            enc = tokenizer.encode(text1)\n            st = tokenizer.decode(enc.ids[a-2:b-1])\n        all.append(jaccard(st,train.loc[k,'selected_text']))\n    jac.append(np.mean(all))\n    print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all))\n    print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('>>>> OVERALL 5Fold CV Jaccard =',np.mean(jac))\nprint(jac) # Jaccard CVs","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Submission <a class=\"anchor\" id=\"4\"></a>\n\n[Back to Table of Contents](#0.1)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Code from notebook https://www.kaggle.com/khoongweihao/tse2020-roberta-cnn-random-seed-distribution?scriptVersionId=34448972","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"all = []\nfor k in range(input_ids_t.shape[0]):\n    a = np.argmax(preds_start[k,])\n    b = np.argmax(preds_end[k,])\n    if a>b: \n        st = test.loc[k,'text']\n    else:\n        text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode(enc.ids[a-2:b-1])\n    all.append(st)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['selected_text'] = all\ntest[['textID','selected_text']].to_csv('submission.csv',index=False)\ntest.sample(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Outlier analysis <a class=\"anchor\" id=\"5\"></a>\n\n[Back to Table of Contents](#0.1)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 5.1. Training prediction result visualization <a class=\"anchor\" id=\"5.1\"></a>\n\n[Back to Table of Contents](#0.1)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualization training prediction results\nall = []\nstart = []\nend = []\nstart_pred = []\nend_pred = []\nfor k in range(input_ids.shape[0]):\n    a = np.argmax(preds_start_train[k,])\n    b = np.argmax(preds_end_train[k,])\n    start.append(np.argmax(start_tokens[k]))\n    end.append(np.argmax(end_tokens[k]))        \n    if a>b:\n        st = train.loc[k,'text']\n        start_pred.append(0)\n        end_pred.append(len(st))\n    else:\n        text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode(enc.ids[a-2:b-1])\n        start_pred.append(a)\n        end_pred.append(b)\n    all.append(st)\ntrain['start'] = start\ntrain['end'] = end\ntrain['start_pred'] = start_pred\ntrain['end_pred'] = end_pred\ntrain['selected_text_pred'] = all\ntrain.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def metric_tse(df,col1,col2):\n    # Calc metric of tse-competition - according to https://www.kaggle.com/c/tweet-sentiment-extraction/overview/evaluation\n    return df.apply(lambda x: jaccard(x[col1],x[col2]),axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analytics\ntrain = train.replace({'sentiment': {'negative': -1, 'neutral': 0, 'positive': 1}})\ntrain['len_text'] = train['text'].str.len()\ntrain['len_selected_text'] = train['selected_text'].str.len()\ntrain['diff_num'] = train['end']-train['start']\ntrain['share'] = train['len_selected_text']/train['len_text']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prediction analytics\ntrain['selected_text_pred'] = train['selected_text_pred'].map(lambda x: x.lstrip(' '))\ntrain['len_selected_text_pred'] = train['selected_text_pred'].str.len()\ntrain['diff_num_pred'] = train['end_pred']-train['start_pred']\ntrain['share_pred'] = train['len_selected_text_pred']/train['len_text']\n# len_equal\ntrain['len_equal'] = 0\ntrain.loc[(train['start'] == train['start_pred']) & (train['end'] == train['end_pred']), 'len_equal'] = 1\n# metric\ntrain['metric'] = metric_tse(train,'selected_text','selected_text_pred')\n# res\ntrain['res'] = 0\ntrain.loc[train['metric'] == 1, 'res'] = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def rep_3chr(text):\n    # Checks if there are 3 or more repetitions of characters in words\n    chr3 = 0\n    for word in text.split():\n        for c in set(word):\n            if word.rfind(c+c+c) > -1:\n                chr3 = 1                \n    return chr3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analysis of 3 or more repetitions of characters in words\ntrain['text_chr3'] = train['text'].apply(rep_3chr)\ntrain['selected_text_chr3'] = train['selected_text'].apply(rep_3chr)\ntrain['selected_text_pred_chr3'] = train['selected_text_pred'].apply(rep_3chr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# result\ncol_interesting = ['sentiment', 'len_text', 'text_chr3', 'selected_text', 'len_selected_text', 'diff_num', 'share', \n                   'selected_text_chr3', 'selected_text_pred', 'len_selected_text_pred', 'diff_num_pred', 'share_pred',\n                   'selected_text_pred_chr3', 'len_equal', 'metric', 'res']\ntrain[col_interesting].head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Total metric =',train['metric'].mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Long 'selected text' are not predicted correctly (get too long)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Outlier\ntrain_outlier = train[train['res'] == 0].reset_index(drop=True)\ntrain_outlier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_outlier.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sh_out = str(round(len(train_outlier)*100/len(train),1))\nprint('Number of outliers is ' + sh_out + '% from training data')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Good prediction\ntrain_good = train[train['res'] == 1].reset_index(drop=True)\ntrain_good","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_good.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Share of all data')\ntrain[['share', 'share_pred']].hist(bins=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Share of outlier data')\ntrain_outlier[['share', 'share_pred']].hist(bins=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"The main problem is in the predicting of the longest and shortest selected_text which are most or least different from the given text","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Only one word in 'selected_text'\ntrain_outlier[train_outlier['diff_num']==0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_good[train_good['diff_num']==0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 'selected_text' = 'text'\ntrain_outlier[train_outlier['share']==1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_good[train_good['share']==1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Only one word in 'text'\ntrain_outlier[train_outlier[\"text\"].str.find(' ') == -1].head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train_outlier[train_outlier[\"text\"].str.find(' ') == -1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_good[train_good[\"text\"].str.find(' ') == -1].head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train_good[train_good[\"text\"].str.find(' ') == -1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Text from a single word almost always processes correctly","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 5.2. WordCloud <a class=\"anchor\" id=\"5.2\"></a>\n\n[Back to Table of Contents](#0.1)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Using my notebook https://www.kaggle.com/vbmokin/nlp-eda-bag-of-words-tf-idf-glove-bert","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def plot_word_cloud(x, col):\n    corpus=[]\n    for k in x[col].str.split():\n        for i in k:\n            corpus.append(i)\n    plt.figure(figsize=(12,8))\n    word_cloud = WordCloud(\n                              background_color='black',\n                              max_font_size = 80\n                             ).generate(\" \".join(corpus[:50]))\n    plt.imshow(word_cloud)\n    plt.axis('off')\n    plt.show()\n    return corpus[:50]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# All training text\nprint('Word Cloud for all text in training data')\ntrain_all = plot_word_cloud(train, 'text')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_all","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# All test\nprint('Word Cloud for all text in test')\ntest_all = plot_word_cloud(test, 'text')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_all","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# All training selected_text\nprint('Word Cloud for selected_text in training data')\ntrain_selected_text = plot_word_cloud(train, 'selected_text')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_selected_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Oitlier WordCloud\nprint('Word Cloud for Outliers')\noutlier_max = plot_word_cloud(train_outlier, 'selected_text')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"outlier_max","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Worst oitlier WordCloud\nprint('Word Cloud for the 100 worst outliers')\noutlier_max100 = plot_word_cloud(train_outlier.nsmallest(100, 'metric', keep='all'), 'selected_text')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"outlier_max100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Worst oitlier WordCloud\nprint('Word Cloud for the 1000 worst outliers')\noutlier_max1000 = plot_word_cloud(train_outlier.nsmallest(1000, 'metric', keep='all'), 'selected_text')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"outlier_max1000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Good prediction WordCloud\nprint('Word Cloud for good prediction')\ngood_max = plot_word_cloud(train_good, 'selected_text')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"good_max","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5.3. Subtext analysis <a class=\"anchor\" id=\"5.3\"></a>\n\n[Back to Table of Contents](#0.1)","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def subtext_analysis(col, subtext, df1, str1, df2, str2):\n    # Calc statistics as table for subtext in the df1[col] (smaller) in compare to df2[col] (bigger) \n    \n    result = pd.DataFrame(columns = ['subtext', str1, str2, 'share,%'])\n    if (len(df1) > 0) and (len(df2) > 0):\n        for i in range(len(subtext)):\n            result.loc[i,'subtext'] = subtext[i]\n            num1 = len(df1[df1[col].str.find(subtext[i]) > -1])\n            result.loc[i, str1] = num1\n            num2 = len(df2[df2[col].str.find(subtext[i]) > -1])\n            result.loc[i, str2] = num2\n            result.loc[i,'share,%'] = round(num1*100/num2,1) if num2 != 0 else 0\n    print('Number of all data is', len(df2))\n    display(result.sort_values(by=['share,%', str1], ascending=False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def subtext_analysis_one_df(col, subtext, df, str):\n    # Calc statistics as table for subtext in the df[col]\n    \n    result = pd.DataFrame(columns = ['subtext', str, 'share of all,%'])\n    num_all = len(df)\n    if (num_all > 0):\n        for i in range(len(subtext)):\n            result.loc[i,'subtext'] = subtext[i]\n            num = len(df[df[col].str.find(subtext[i]) > -1])\n            result.loc[i, str] = num\n            result.loc[i,'share of all,%'] = round(num*100/num_all,1)\n    print('Number of all data is', len(df))\n    display(result.sort_values(by='share of all,%', ascending=False))    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subtext_test = ['SAD', 'bullying', 'Uh', 'oh', 'onna', 'fun', 'addicted', 'Power', 'well', 'unhappy', 'funny', 'Tears', 'Fears', 'sleeeeepy', ' ', ',', '?', '!' ,'!!', '!!!', ':/', '...', 'http', '****']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subtext_analysis(\"selected_text\", subtext_test, train_outlier, 'train_outliers', train, 'train_all')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are problems in processing: \"!\", \"!!\", \"!!!\", \":/\", \"...\", \"http\" etc.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"subtext_analysis(\"selected_text\", subtext_test, train_good, 'train_good', train, 'train_all')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subtext_analysis_one_df(\"selected_text\", subtext_test, train, 'test_all')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subtext_analysis_one_df(\"selected_text\", subtext_test, test, 'test_all')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['text_chr3'] = test['text'].apply(rep_3chr)\ntest.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5.4. Metric analysis <a class=\"anchor\" id=\"5.4\"></a>\n\n[Back to Table of Contents](#0.1)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Metric of prediction for training data')\ntrain[['metric']].hist(bins=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Metric of prediction for outliers of training data')\ntrain_outlier[['metric']].hist(bins=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_outlier1 = train_outlier.nsmallest(1000, 'metric', keep='all')\ntrain_outlier2 = train_outlier.nsmallest(2000, 'metric', keep='all')\ntrain_outlier3 = train_outlier.nsmallest(3000, 'metric', keep='all')\ntrain_outlier5 = train_outlier.nsmallest(5000, 'metric', keep='all')\ntrain_outlier8 = train_outlier.nsmallest(8000, 'metric', keep='all')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subtext_analysis(\"selected_text\", subtext_test, train_outlier1, 'in worst 1000 outliers', train_outlier, 'in all outliers')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subtext_analysis(\"selected_text\", subtext_test, train_outlier2, 'in worst 2000 outliers', train_outlier, 'in all outliers')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subtext_analysis(\"selected_text\", subtext_test, train_outlier3, 'in worst 3000 outliers', train_outlier, 'in all outliers')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subtext_analysis(\"selected_text\", subtext_test, train_outlier5, 'in worst 5000 outliers', train_outlier, 'in all outliers')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subtext_analysis(\"selected_text\", subtext_test, train_outlier8, 'in worst 8000 outliers', train_outlier, 'in all outliers')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_outlier1.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_outlier2.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_outlier3.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_outlier5.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_outlier8.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Histograms of interesting features in training data\ncol_hist = ['sentiment', 'start', 'end', 'start_pred', 'end_pred', 'len_text', 'len_selected_text', \n            'text_chr3', 'selected_text_chr3', 'selected_text_pred_chr3', 'metric']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Statistics for 1000 worst outliers')\nfig = plt.figure(figsize = (8,8))\nax = fig.gca()\ntrain_outlier1[col_hist].hist(ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Statistics for 2000 worst outliers')\nfig = plt.figure(figsize = (8,8))\nax = fig.gca()\ntrain_outlier2[col_hist].hist(ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Statistics for 3000 worst outliers')\nfig = plt.figure(figsize = (8,8))\nax = fig.gca()\ntrain_outlier3[col_hist].hist(ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Statistics for 5000 worst outliers')\nfig = plt.figure(figsize = (8,8))\nax = fig.gca()\ntrain_outlier5[col_hist].hist(ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Statistics for 8000 worst outliers')\nfig = plt.figure(figsize = (8,8))\nax = fig.gca()\ntrain_outlier8[col_hist].hist(ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5.5. PCA visualization <a class=\"anchor\" id=\"5.5\"></a>\n\n[Back to Table of Contents](#0.1)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Using my notebook https://www.kaggle.com/vbmokin/nlp-eda-bag-of-words-tf-idf-glove-bert","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_LSA(test_data, test_labels, savepath=\"PCA_demo.csv\", plot=True, title=None):\n        lsa = TruncatedSVD(n_components=2)\n        lsa.fit(test_data)\n        lsa_scores = lsa.transform(test_data)\n        color_mapper = {label:idx for idx,label in enumerate(set(test_labels))}\n        color_column = [color_mapper[label] for label in test_labels]\n        colors = ['orange','blue']\n        if plot:\n            plt.title(title)\n            plt.scatter(lsa_scores[:,0], lsa_scores[:,1], s=8, alpha=.8, c=test_labels, cmap=matplotlib.colors.ListedColormap(colors))\n            orange_patch = mpatches.Patch(color='orange', label='Good')\n            blue_patch = mpatches.Patch(color='blue', label='Outlier')\n            plt.legend(handles=[orange_patch, blue_patch], prop={'size': 30})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(16, 16))          \nplot_LSA(preds_start_train, train['res'], title='Predicted start places of selected text in training data')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(16, 16))          \nplot_LSA(preds_end_train, train['res'], title='Predicted end places of selected text in training data')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are a number of clear patterns that allow us to hope that we can improve the solution.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 5.6. Clustering <a class=\"anchor\" id=\"5.6\"></a>\n\n[Back to Table of Contents](#0.1)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Using my notebook https://www.kaggle.com/vbmokin/covid-19-week5-global-forecasting-eda-extratr","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = train[['sentiment', 'start', 'end', 'start_pred', 'end_pred', 'len_text', 'len_selected_text', 'diff_num', 'share', 'metric', 'res']].dropna()\ndata","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Thanks to https://www.kaggle.com/kashnitsky/topic-7-unsupervised-learning-pca-and-clustering\ninertia = []\npca = PCA(n_components=2)\n# fit X and apply the reduction to X \nx_3d = pca.fit_transform(data)\nfor k in range(1, 8):\n    kmeans = KMeans(n_clusters=k, random_state=1).fit(x_3d)\n    inertia.append(np.sqrt(kmeans.inertia_))\nplt.plot(range(1, 8), inertia, marker='s');\nplt.xlabel('$k$')\nplt.ylabel('$J(C_k)$');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Thanks to https://www.kaggle.com/arthurtok/a-cluster-of-colors-principal-component-analysis\n# Set a 3 KMeans clustering\nkmeans = KMeans(n_clusters=5, random_state=0)\n# Compute cluster centers and predict cluster indices\nX_clustered = kmeans.fit_predict(x_3d)\nLABEL_COLOR_MAP = {0 : 'r',\n                   1 : 'g',\n                   2 : 'b',\n                   3 : 'y',\n                   4 : 'c'}\n\nlabel_color = [LABEL_COLOR_MAP[l] for l in X_clustered]\nplt.figure(figsize = (7,7))\nplt.scatter(x_3d[:,0],x_3d[:,1], c= label_color, alpha=0.9)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are a number of clear clusters that allow us to hope that we can improve the solution.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"I hope you find this kernel useful and enjoyable.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Your comments and feedback are most welcome.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"[Go to Top](#0)","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}