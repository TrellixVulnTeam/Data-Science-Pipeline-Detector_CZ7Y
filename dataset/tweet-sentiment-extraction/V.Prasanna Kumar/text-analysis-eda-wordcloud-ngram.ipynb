{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Acknowledgements \n*For some visuvalization* \n## Base Kernal : https://www.kaggle.com/tanulsingh077/twitter-sentiment-extaction-analysis-eda-and-model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Please upvote the kernal if you like it and gained some information from this kernal","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Lets gets started ","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\npd.set_option('display.max_colwidth', -1)\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib import rcParams\nimport seaborn as sns\nimport nltk\nfrom nltk.corpus import stopwords\nstop = stopwords.words('english')\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nfrom collections import Counter\nimport plotly.express as px\nimport plotly.figure_factory as ff\nimport re\nimport string","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/tweet-sentiment-extraction/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/tweet-sentiment-extraction/test.csv\")\nsubmission = pd.read_csv(\"/kaggle/input/tweet-sentiment-extraction/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train data has one nan value in text and selected_text feature","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.sentiment.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rcParams[\"figure.figsize\"] = 15,13\ntrain.sentiment.value_counts().plot(kind=\"pie\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rcParams[\"figure.figsize\"] = 15,10\nsns.countplot(x=train[\"sentiment\"],data=train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The above graph shows that Neutral label is more than positive and negative","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Full data Analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_word_cloud(text):\n    wordcloud = WordCloud(\n        width = 3000,\n        height = 2000,\n        background_color = 'black').generate(str(text))\n    fig = plt.figure(\n        figsize = (40, 30),\n        facecolor = 'k',\n        edgecolor = 'k')\n    plt.imshow(wordcloud, interpolation = 'bilinear')\n    plt.axis('off')\n    plt.tight_layout(pad=0)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = str(text).lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['text'] = train['text'].apply(lambda x:clean_text(x))\ntrain['selected_text'] = train['selected_text'].apply(lambda x:clean_text(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Before Pre-Processing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train['temp_list'] = train['selected_text'].apply(lambda x:str(x).split())\ntop = Counter([item for sublist in train['temp_list'] for item in sublist])\ntemp = pd.DataFrame(top.most_common(20))\ntemp.columns = ['Common_words','count']\ntemp.style.background_gradient(cmap='Blues')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.bar(temp, x=\"count\", y=\"Common_words\", title='Commmon Words in Selected Text', orientation='h', width=700, height=700,color='Common_words')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.treemap(temp, path=['Common_words'], values='count',title='Tree of Most Common Words')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## After Pre-Processing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_stopword(x):\n    return [w for w in x if not w in stop]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['temp_list'] = train['temp_list'].apply(lambda x:remove_stopword(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top = Counter([item for sublist in train['temp_list'] for item in sublist])\ntemp = pd.DataFrame(top.most_common(20))\ntemp.columns = ['Common_words','count']\ntemp.style.background_gradient(cmap='Purples')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.treemap(temp, path=['Common_words'], values='count',title='Tree of Most Common Words')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_bigram = (pd.Series(nltk.ngrams(train[\"selected_text\"], 2)).value_counts())[:20]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_bigram.sort_values().plot.barh(color='blue', width=.9, figsize=(12, 8))\nplt.title('20 Most Frequently Occuring Bigrams')\nplt.ylabel('Bigram')\nplt.xlabel('# of Occurances')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Wordcloud of train data for first 100 data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_text = train.selected_text[:100].values\ngenerate_word_cloud(train_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_trigram = pd.Series(nltk.ngrams(train[\"text\"], 3)).value_counts()[:20]\n# train_trigram.sort_values().plot.barh(color='blue', width=.9, figsize=(12, 8))\n# plt.title('20 Most Frequently Occuring Bigrams')\n# plt.ylabel('Trigram')\n# plt.xlabel('# of Occurances')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Seperating the data into different data frame based on the labels ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"positive_train = train[train[\"sentiment\"]==\"positive\"]\nnegative_train = train[train[\"sentiment\"]==\"negative\"]\nneutral_train = train[train[\"sentiment\"]==\"neutral\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tokenizeandstopwords(text):\n    tokens = nltk.word_tokenize(text)\n    # taken only words (not punctuation)\n    token_words = [w for w in tokens if w.isalpha()]\n    meaningful_words = [w for w in token_words if not w in stop]\n    joined_words = ( \" \".join(meaningful_words))\n    return joined_words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"positive_train[\"selected_text\"] = positive_train[\"selected_text\"].apply(clean_text)\nnegative_train[\"selected_text\"] = negative_train[\"selected_text\"].apply(clean_text)\nneutral_train[\"selected_text\"] = neutral_train[\"selected_text\"].apply(clean_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"positive_train[\"selected_text\"] = positive_train[\"selected_text\"].apply(tokenizeandstopwords)\nnegative_train[\"selected_text\"] = negative_train[\"selected_text\"].apply(tokenizeandstopwords)\nneutral_train[\"selected_text\"] = neutral_train[\"selected_text\"].apply(tokenizeandstopwords)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Positive data Analysis","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## N-gram Analysis of positive data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"positive_bigram = (pd.Series(nltk.ngrams(positive_train[\"selected_text\"], 2)).value_counts())[:25]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"positive_bigram.sort_values().plot.barh(color='blue', width=.9, figsize=(12, 8))\nplt.title('25 Most Frequently Occuring Bigrams')\nplt.ylabel('Bigram')\nplt.xlabel('# of Occurances')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## most important or common positive words","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"positive_train['temp_list'] = positive_train['selected_text'].apply(lambda x:str(x).split())\npositive_train['temp_list'] = positive_train['temp_list'].apply(lambda x:remove_stopword(x))\npositive_top = Counter([item for sublist in positive_train['temp_list'] for item in sublist])\npositive_temp = pd.DataFrame(positive_top.most_common(20))\npositive_temp.columns = ['Common_words','count']\npositive_temp.style.background_gradient(cmap='Blues')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## tree map view of common words using plotyly","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.treemap(positive_temp, path=['Common_words'], values='count',title='Tree of Most Common Positive Words')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## positive wordcloud of 100 data points","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"positive_text = positive_train.selected_text[:100].values\ngenerate_word_cloud(positive_text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Negative Data Analysis","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Most important or common negative words","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"negative_train['temp_list'] = negative_train['selected_text'].apply(lambda x:str(x).split())\nnegative_train['temp_list'] = negative_train['temp_list'].apply(lambda x:remove_stopword(x))\nnegative_top = Counter([item for sublist in negative_train['temp_list'] for item in sublist])\nnegative_temp = pd.DataFrame(negative_top.most_common(20))\nnegative_temp.columns = ['Common_words','count']\nnegative_temp.style.background_gradient(cmap='Blues')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tree map view of most common words","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.treemap(negative_temp, path=['Common_words'], values='count',title='Tree of Most Common  Negative Words')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## N-gram analysis of negative words","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"negative_bigram = (pd.Series(nltk.ngrams(negative_train[\"selected_text\"], 2)).value_counts())[:25]\nnegative_bigram.sort_values().plot.barh(color='blue', width=.9, figsize=(12, 8))\nplt.title('25 Most Frequently Occuring Bigrams')\nplt.ylabel('Bigram')\nplt.xlabel('# of Occurances')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Wordcloud Analysis of Negative words for 100 data points","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"negative_text = negative_train.selected_text[:100].values\ngenerate_word_cloud(negative_text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Neutral Data Analysis","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Most important or common words in neutral data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"neutral_train['temp_list'] = neutral_train['selected_text'].apply(lambda x:str(x).split())\nneutral_train['temp_list'] = neutral_train['temp_list'].apply(lambda x:remove_stopword(x))\nneutral_top = Counter([item for sublist in neutral_train['temp_list'] for item in sublist])\nneutral_temp = pd.DataFrame(neutral_top.most_common(20))\nneutral_temp.columns = ['Common_words','count']\nneutral_temp.style.background_gradient(cmap='Blues')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tree map view of Most common words in Neutral Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.treemap(neutral_temp, path=['Common_words'], values='count',title='Tree of Most Common Neutral Words')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## N-gram Analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"neutral_bigram = (pd.Series(nltk.ngrams(neutral_train[\"selected_text\"], 2)).value_counts())[:25]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"neutral_bigram.sort_values().plot.barh(color='blue', width=.9, figsize=(12, 8))\nplt.title('25 Most Frequently Occuring Bigrams')\nplt.ylabel('Bigram')\nplt.xlabel('# of Occurances')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Word cloud Analysis of Neutral data for 100 data points","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"neutral_text = neutral_train.selected_text[:100].values\ngenerate_word_cloud(neutral_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np             # for algebric functions\nimport pandas as pd            # to handle dataframes\nimport os                      # to import files \n#!pip install transformers\nimport transformers            # Transformers (pytorch-transformers /pytorch-pretrained-bert) provides general-purpose architectures (BERT, RoBERTa,..)\nimport tokenizers              # A tokenizer is in charge of preparing the inputs for a model. \nimport string                  \nimport torch                   # pytorch\nimport torch.nn as nn   \nfrom torch.nn import functional as F\nfrom tqdm import tqdm          # TQDM is a progress bar library\nimport re                      # regular expression\nimport json\nimport requests","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN = 192\nTRAIN_BATCH_SIZE = 32\nVALID_BATCH_SIZE = 8\nEPOCHS = 5\nROBERTA_PATH = 'roberta-base'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n# pre_voc_file = transformers.RobertaTokenizer.pretrained_vocab_files_map\n# merges_file  = pre_voc_file.get('merges_file').get(ROBERTA_PATH)\n# vocab_file = pre_voc_file.get('vocab_file').get(ROBERTA_PATH)\n# model_bin = transformers.modeling_roberta.ROBERTA_PRETRAINED_MODEL_ARCHIVE_MAP.get(ROBERTA_PATH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# json_f = requests.get(vocab_file) \n# txt_f = requests.get(merges_file) \n# mod_bin = requests.get(model_bin)\n\n# data = json_f.json()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# with open('vocab.json', 'w') as f: json.dump(data, f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# open('merge.txt', 'wb').write(txt_f.content) \n# open('model.bin', 'wb').write(mod_bin.content)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TOKENIZER = tokenizers.ByteLevelBPETokenizer(vocab_file=f\"../input/roberta-vocab-file/vocab.json\", \n#                                              merges_file=f\"../input/roberta-vocab-file/merge.txt\", \n#                                              lowercase=True,\n#                                              add_prefix_space=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# class TweetModel(transformers.BertPreTrainedModel):\n#     def __init__(self, conf):\n#         super(TweetModel, self).__init__(conf)\n#         self.roberta = transformers.RobertaModel.from_pretrained(\"roberta-base\", config=conf)\n#         self.drop_out = nn.Dropout(0.1)\n#         self.l0 = nn.Linear(768 * 2, 2)\n#         torch.nn.init.normal_(self.l0.weight, std=0.02)\n    \n#     def forward(self, ids, mask, token_type_ids):\n#         _, _, out = self.roberta(\n#             ids,\n#             attention_mask=mask,\n#             token_type_ids=token_type_ids\n#         )\n\n#         out = torch.cat((out[-1], out[-2]), dim=-1)\n#         out = self.drop_out(out)\n#         logits = self.l0(out)\n\n#         start_logits, end_logits = logits.split(1, dim=-1)\n\n#         start_logits = start_logits.squeeze(-1)\n#         end_logits = end_logits.squeeze(-1)\n\n#         return start_logits, end_logits\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def process_data(tweet, selected_text, sentiment, tokenizer, max_len):\n#     tweet = \" \" + \" \".join(str(tweet).split())\n#     selected_text = \" \" + \" \".join(str(selected_text).split())\n\n#     len_st = len(selected_text) - 1\n#     idx0 = None\n#     idx1 = None\n\n#     for ind in (i for i, e in enumerate(tweet) if e == selected_text[1]):\n#         if \" \" + tweet[ind: ind+len_st] == selected_text:\n#             idx0 = ind\n#             idx1 = ind + len_st - 1\n#             break\n\n#     char_targets = [0] * len(tweet)\n#     if idx0 != None and idx1 != None:\n#         for ct in range(idx0, idx1 + 1):\n#             char_targets[ct] = 1\n    \n#     tok_tweet = tokenizer.encode(tweet)\n#     input_ids_orig = tok_tweet.ids\n#     tweet_offsets = tok_tweet.offsets\n    \n#     target_idx = []\n#     for j, (offset1, offset2) in enumerate(tweet_offsets):\n#         if sum(char_targets[offset1: offset2]) > 0:\n#             target_idx.append(j)\n    \n#     targets_start = target_idx[0]\n#     targets_end = target_idx[-1]\n\n#     sentiment_id = {\n#         'positive': 1313,\n#         'negative': 2430,\n#         'neutral': 7974\n#     }\n    \n#     input_ids = [0] + [sentiment_id[sentiment]] + [2] + [2] + input_ids_orig + [2]\n#     token_type_ids = [0, 0, 0, 0] + [0] * (len(input_ids_orig) + 1)\n#     mask = [1] * len(token_type_ids)\n#     tweet_offsets = [(0, 0)] * 4 + tweet_offsets + [(0, 0)]\n#     targets_start += 4\n#     targets_end += 4\n\n#     padding_length = max_len - len(input_ids)\n#     if padding_length > 0:\n#         input_ids = input_ids + ([1] * padding_length)\n#         mask = mask + ([0] * padding_length)\n#         token_type_ids = token_type_ids + ([0] * padding_length)\n#         tweet_offsets = tweet_offsets + ([(0, 0)] * padding_length)\n    \n#     return {\n#         'ids': input_ids,\n#         'mask': mask,\n#         'token_type_ids': token_type_ids,\n#         'targets_start': targets_start,\n#         'targets_end': targets_end,\n#         'orig_tweet': tweet,\n#         'orig_selected': selected_text,\n#         'sentiment': sentiment,\n#         'offsets': tweet_offsets\n#     }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# class TweetDataset:\n#     def __init__(self, tweet, sentiment, selected_text):\n#         self.tweet = tweet\n#         self.sentiment = sentiment\n#         self.selected_text = selected_text\n#         self.tokenizer = TOKENIZER\n#         self.max_len = MAX_LEN\n    \n#     def __len__(self):\n#         return len(self.tweet)\n\n#     def __getitem__(self, item):\n#         data = process_data(\n#             self.tweet[item], \n#             self.selected_text[item], \n#             self.sentiment[item],\n#             self.tokenizer,\n#             self.max_len\n#         )\n\n#         return {\n#             'ids': torch.tensor(data[\"ids\"], dtype=torch.long),\n#             'mask': torch.tensor(data[\"mask\"], dtype=torch.long),\n#             'token_type_ids': torch.tensor(data[\"token_type_ids\"], dtype=torch.long),\n#             'targets_start': torch.tensor(data[\"targets_start\"], dtype=torch.long),\n#             'targets_end': torch.tensor(data[\"targets_end\"], dtype=torch.long),\n#             'orig_tweet': data[\"orig_tweet\"],\n#             'orig_selected': data[\"orig_selected\"],\n#             'sentiment': data[\"sentiment\"],\n#             'offsets': torch.tensor(data[\"offsets\"], dtype=torch.long)\n#         }\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def calculate_jaccard_score(\n#     original_tweet, \n#     target_string, \n#     sentiment_val, \n#     idx_start, \n#     idx_end, \n#     offsets,\n#     verbose=False):\n    \n#     if idx_end < idx_start:\n#         idx_end = idx_start\n    \n#     filtered_output  = \"\"\n#     for ix in range(idx_start, idx_end + 1):\n#         filtered_output += original_tweet[offsets[ix][0]: offsets[ix][1]]\n#         if (ix+1) < len(offsets) and offsets[ix][1] < offsets[ix+1][0]:\n#             filtered_output += \" \"\n\n#     if sentiment_val == \"neutral\" or len(original_tweet.split()) < 2:\n#         filtered_output = original_tweet\n\n#     if sentiment_val != \"neutral\" and verbose == True:\n#         if filtered_output.strip().lower() != target_string.strip().lower():\n#             print(\"********************************\")\n#             print(f\"Output= {filtered_output.strip()}\")\n#             print(f\"Target= {target_string.strip()}\")\n#             print(f\"Tweet= {original_tweet.strip()}\")\n#             print(\"********************************\")\n\n#     jac = 0\n#     return jac, filtered_output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df_test = pd.read_csv(\"../input/tweet-sentiment-extraction/test.csv\")\n# df_test.loc[:, \"selected_text\"] = df_test.text.values\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# device = torch.device(\"cuda\")\n# model_config = transformers.RobertaConfig.from_pretrained('../input/roberta-vocab-file/config.json')  # to download from internet\n# model_config.output_hidden_states = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TweetDataset(tweet=df_test.text.values,\n#              sentiment=df_test.sentiment.values,\n#              selected_text=df_test.selected_text.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model = TweetModel(conf=model_config)\n# model.to(device)\n# model.eval()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# final_output = []\n# test_dataset = TweetDataset(\n#         tweet=df_test.text.values,\n#         sentiment=df_test.sentiment.values,\n#         selected_text=df_test.selected_text.values\n#     )\n\n# data_loader = torch.utils.data.DataLoader(\n#     test_dataset,\n#     shuffle=False,\n#     batch_size=VALID_BATCH_SIZE,\n#     num_workers=0\n# )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# with torch.no_grad():\n#     tk0 = tqdm(data_loader, total=len(data_loader))\n#     for bi, d in enumerate(tk0):\n#         ids = d[\"ids\"]\n#         token_type_ids = d[\"token_type_ids\"]\n#         mask = d[\"mask\"]\n#         sentiment = d[\"sentiment\"]\n#         orig_selected = d[\"orig_selected\"]\n#         orig_tweet = d[\"orig_tweet\"]\n#         targets_start = d[\"targets_start\"]\n#         targets_end = d[\"targets_end\"]\n#         offsets = d[\"offsets\"].numpy()\n\n#         ids            = ids.to(device, dtype=torch.long)\n#         token_type_ids = token_type_ids.to(device, dtype=torch.long)\n#         mask           = mask.to(device, dtype=torch.long)\n#         targets_start  = targets_start.to(device, dtype=torch.long)\n#         targets_end    = targets_end.to(device, dtype=torch.long)\n\n#         outputs_start1, outputs_end1 = model(\n#             ids=ids,\n#             mask=mask,\n#             token_type_ids=token_type_ids\n#         )\n\n#         outputs_start = outputs_start1\n#         outputs_end = outputs_end1\n        \n#         outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n#         outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n#         jaccard_scores = []\n#         for px, tweet in enumerate(orig_tweet):\n#           selected_tweet = orig_selected[px]\n#           tweet_sentiment = sentiment[px]\n#           _, output_sentence = calculate_jaccard_score(original_tweet=tweet,\n#                                                        target_string=selected_tweet,\n#                                                        sentiment_val=tweet_sentiment,\n#                                                        idx_start=np.argmax(outputs_start[px, :]),\n#                                                        idx_end=np.argmax(outputs_end[px, :]),\n#                                                        offsets=offsets[px])\n#           final_output.append(output_sentence)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sample = pd.read_csv(\"../input/tweet-sentiment-extraction/sample_submission.csv\")\n# sample.loc[:, 'selected_text'] = final_output\n# sample.to_csv(\"submission.csv\", index=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class AverageMeter:\n    \"\"\"\n    Computes and stores the average and current value\n    \"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\nclass EarlyStopping:\n    def __init__(self, patience=7, mode=\"max\", delta=0.001):\n        self.patience = patience\n        self.counter = 0\n        self.mode = mode\n        self.best_score = None\n        self.early_stop = False\n        self.delta = delta\n        if self.mode == \"min\":\n            self.val_score = np.Inf\n        else:\n            self.val_score = -np.Inf\n\n    def __call__(self, epoch_score, model, model_path):\n\n        if self.mode == \"min\":\n            score = -1.0 * epoch_score\n        else:\n            score = np.copy(epoch_score)\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(epoch_score, model, model_path)\n        elif score < self.best_score + self.delta:\n            self.counter += 1\n            print('EarlyStopping counter: {} out of {}'.format(self.counter, self.patience))\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(epoch_score, model, model_path)\n            self.counter = 0\n\n    def save_checkpoint(self, epoch_score, model, model_path):\n        if epoch_score not in [-np.inf, np.inf, -np.nan, np.nan]:\n            print('Validation score improved ({} --> {}). Saving model!'.format(self.val_score, epoch_score))\n            torch.save(model.state_dict(), model_path)\n        self.val_score = epoch_score\n\n\ndef jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport torch\nimport pandas as pd\nimport torch.nn as nn\nimport numpy as np\nimport torch.nn.functional as F\nfrom torch.optim import lr_scheduler\n\nfrom sklearn import model_selection\nfrom sklearn import metrics\nimport transformers\nimport tokenizers\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup\nfrom tqdm.autonotebook import tqdm\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class config:\n    MAX_LEN = 128\n    TRAIN_BATCH_SIZE = 64\n    VALID_BATCH_SIZE = 16\n    EPOCHS = 5\n    BERT_PATH = \"../input/bert-base-uncased/\"\n    MODEL_PATH = \"model.bin\"\n    TRAINING_FILE = \"../input/tweet-train-folds/train_folds.csv\"\n    TOKENIZER = tokenizers.BertWordPieceTokenizer(\n        f\"{BERT_PATH}/vocab.txt\", \n        lowercase=True\n    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ndef process_data(tweet, selected_text, sentiment, tokenizer, max_len):\n    len_st = len(selected_text)\n    idx0 = None\n    idx1 = None\n    for ind in (i for i, e in enumerate(tweet) if e == selected_text[0]):\n        if tweet[ind: ind+len_st] == selected_text:\n            idx0 = ind\n            idx1 = ind + len_st - 1\n            break\n\n    char_targets = [0] * len(tweet)\n    if idx0 != None and idx1 != None:\n        for ct in range(idx0, idx1 + 1):\n            char_targets[ct] = 1\n    \n    tok_tweet = tokenizer.encode(tweet)\n    input_ids_orig = tok_tweet.ids[1:-1]\n    tweet_offsets = tok_tweet.offsets[1:-1]\n    \n    target_idx = []\n    for j, (offset1, offset2) in enumerate(tweet_offsets):\n        if sum(char_targets[offset1: offset2]) > 0:\n            target_idx.append(j)\n    \n    targets_start = target_idx[0]\n    targets_end = target_idx[-1]\n\n    sentiment_id = {\n        'positive': 3893,\n        'negative': 4997,\n        'neutral': 8699\n    }\n    \n    input_ids = [101] + [sentiment_id[sentiment]] + [102] + input_ids_orig + [102]\n    token_type_ids = [0, 0, 0] + [1] * (len(input_ids_orig) + 1)\n    mask = [1] * len(token_type_ids)\n    tweet_offsets = [(0, 0)] * 3 + tweet_offsets + [(0, 0)]\n    targets_start += 3\n    targets_end += 3\n\n    padding_length = max_len - len(input_ids)\n    if padding_length > 0:\n        input_ids = input_ids + ([0] * padding_length)\n        mask = mask + ([0] * padding_length)\n        token_type_ids = token_type_ids + ([0] * padding_length)\n        tweet_offsets = tweet_offsets + ([(0, 0)] * padding_length)\n    \n    return {\n        'ids': input_ids,\n        'mask': mask,\n        'token_type_ids': token_type_ids,\n        'targets_start': targets_start,\n        'targets_end': targets_end,\n        'orig_tweet': tweet,\n        'orig_selected': selected_text,\n        'sentiment': sentiment,\n        'offsets': tweet_offsets\n    }\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TweetDataset:\n    def __init__(self, tweet, sentiment, selected_text):\n        self.tweet = tweet\n        self.sentiment = sentiment\n        self.selected_text = selected_text\n        self.tokenizer = config.TOKENIZER\n        self.max_len = config.MAX_LEN\n    \n    def __len__(self):\n        return len(self.tweet)\n\n    def __getitem__(self, item):\n        data = process_data(\n            self.tweet[item], \n            self.selected_text[item], \n            self.sentiment[item],\n            self.tokenizer,\n            self.max_len\n        )\n\n        return {\n            'ids': torch.tensor(data[\"ids\"], dtype=torch.long),\n            'mask': torch.tensor(data[\"mask\"], dtype=torch.long),\n            'token_type_ids': torch.tensor(data[\"token_type_ids\"], dtype=torch.long),\n            'targets_start': torch.tensor(data[\"targets_start\"], dtype=torch.long),\n            'targets_end': torch.tensor(data[\"targets_end\"], dtype=torch.long),\n            'orig_tweet': data[\"orig_tweet\"],\n            'orig_selected': data[\"orig_selected\"],\n            'sentiment': data[\"sentiment\"],\n            'offsets': torch.tensor(data[\"offsets\"], dtype=torch.long)\n        }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nclass TweetModel(transformers.BertPreTrainedModel):\n    def __init__(self, conf):\n        super(TweetModel, self).__init__(conf)\n        self.bert = transformers.BertModel.from_pretrained(config.BERT_PATH, config=conf)\n        self.drop_out = nn.Dropout(0.1)\n        self.l0 = nn.Linear(768 * 2, 2)\n        torch.nn.init.normal_(self.l0.weight, std=0.02)\n    \n    def forward(self, ids, mask, token_type_ids):\n        _, _, out = self.bert(\n            ids,\n            attention_mask=mask,\n            token_type_ids=token_type_ids\n        )\n\n        out = torch.cat((out[-1], out[-2]), dim=-1)\n        out = self.drop_out(out)\n        logits = self.l0(out)\n\n        start_logits, end_logits = logits.split(1, dim=-1)\n\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n\n        return start_logits, end_logits\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ndef loss_fn(start_logits, end_logits, start_positions, end_positions):\n    loss_fct = nn.CrossEntropyLoss()\n    start_loss = loss_fct(start_logits, start_positions)\n    end_loss = loss_fct(end_logits, end_positions)\n    total_loss = (start_loss + end_loss)\n    return total_loss\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_fn(data_loader, model, optimizer, device, scheduler=None):\n    model.train()\n    losses = AverageMeter()\n    jaccards = AverageMeter()\n\n    tk0 = tqdm(data_loader, total=len(data_loader))\n    \n    for bi, d in enumerate(tk0):\n\n        ids = d[\"ids\"]\n        token_type_ids = d[\"token_type_ids\"]\n        mask = d[\"mask\"]\n        targets_start = d[\"targets_start\"]\n        targets_end = d[\"targets_end\"]\n        sentiment = d[\"sentiment\"]\n        orig_selected = d[\"orig_selected\"]\n        orig_tweet = d[\"orig_tweet\"]\n        targets_start = d[\"targets_start\"]\n        targets_end = d[\"targets_end\"]\n        offsets = d[\"offsets\"]\n\n        ids = ids.to(device, dtype=torch.long)\n        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n        mask = mask.to(device, dtype=torch.long)\n        targets_start = targets_start.to(device, dtype=torch.long)\n        targets_end = targets_end.to(device, dtype=torch.long)\n\n        model.zero_grad()\n        outputs_start, outputs_end = model(\n            ids=ids,\n            mask=mask,\n            token_type_ids=token_type_ids,\n        )\n        loss = loss_fn(outputs_start, outputs_end, targets_start, targets_end)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n\n        outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n        outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n        jaccard_scores = []\n        for px, tweet in enumerate(orig_tweet):\n            selected_tweet = orig_selected[px]\n            tweet_sentiment = sentiment[px]\n            jaccard_score, _ = calculate_jaccard_score(\n                original_tweet=tweet,\n                target_string=selected_tweet,\n                sentiment_val=tweet_sentiment,\n                idx_start=np.argmax(outputs_start[px, :]),\n                idx_end=np.argmax(outputs_end[px, :]),\n                offsets=offsets[px]\n            )\n            jaccard_scores.append(jaccard_score)\n\n        jaccards.update(np.mean(jaccard_scores), ids.size(0))\n        losses.update(loss.item(), ids.size(0))\n        tk0.set_postfix(loss=losses.avg, jaccard=jaccards.avg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def calculate_jaccard_score(\n    original_tweet, \n    target_string, \n    sentiment_val, \n    idx_start, \n    idx_end, \n    offsets,\n    verbose=False):\n    \n    if idx_end < idx_start:\n        idx_end = idx_start\n    \n    filtered_output  = \"\"\n    for ix in range(idx_start, idx_end + 1):\n        filtered_output += original_tweet[offsets[ix][0]: offsets[ix][1]]\n        if (ix+1) < len(offsets) and offsets[ix][1] < offsets[ix+1][0]:\n            filtered_output += \" \"\n\n    if sentiment_val == \"neutral\" or len(original_tweet.split()) < 2:\n        filtered_output = original_tweet\n\n    jac = jaccard(target_string.strip(), filtered_output.strip())\n    return jac, filtered_output\n\n\ndef eval_fn(data_loader, model, device):\n    model.eval()\n    losses = AverageMeter()\n    jaccards = AverageMeter()\n    \n    with torch.no_grad():\n        tk0 = tqdm(data_loader, total=len(data_loader))\n        for bi, d in enumerate(tk0):\n            ids = d[\"ids\"]\n            token_type_ids = d[\"token_type_ids\"]\n            mask = d[\"mask\"]\n            sentiment = d[\"sentiment\"]\n            orig_selected = d[\"orig_selected\"]\n            orig_tweet = d[\"orig_tweet\"]\n            targets_start = d[\"targets_start\"]\n            targets_end = d[\"targets_end\"]\n            offsets = d[\"offsets\"].numpy()\n\n            ids = ids.to(device, dtype=torch.long)\n            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n            mask = mask.to(device, dtype=torch.long)\n            targets_start = targets_start.to(device, dtype=torch.long)\n            targets_end = targets_end.to(device, dtype=torch.long)\n\n            outputs_start, outputs_end = model(\n                ids=ids,\n                mask=mask,\n                token_type_ids=token_type_ids\n            )\n            loss = loss_fn(outputs_start, outputs_end, targets_start, targets_end)\n            outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n            outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n            jaccard_scores = []\n            for px, tweet in enumerate(orig_tweet):\n                selected_tweet = orig_selected[px]\n                tweet_sentiment = sentiment[px]\n                jaccard_score, _ = calculate_jaccard_score(\n                    original_tweet=tweet,\n                    target_string=selected_tweet,\n                    sentiment_val=tweet_sentiment,\n                    idx_start=np.argmax(outputs_start[px, :]),\n                    idx_end=np.argmax(outputs_end[px, :]),\n                    offsets=offsets[px]\n                )\n                jaccard_scores.append(jaccard_score)\n\n            jaccards.update(np.mean(jaccard_scores), ids.size(0))\n            losses.update(loss.item(), ids.size(0))\n            tk0.set_postfix(loss=losses.avg, jaccard=jaccards.avg)\n    \n    print(f\"Jaccard = {jaccards.avg}\")\n    return jaccards.avg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run(fold):\n    dfx = pd.read_csv(config.TRAINING_FILE)\n\n    df_train = dfx[dfx.kfold != fold].reset_index(drop=True)\n    df_valid = dfx[dfx.kfold == fold].reset_index(drop=True)\n    \n    train_dataset = TweetDataset(\n        tweet=df_train.text.values,\n        sentiment=df_train.sentiment.values,\n        selected_text=df_train.selected_text.values\n    )\n\n    train_data_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=config.TRAIN_BATCH_SIZE,\n        num_workers=4\n    )\n\n    valid_dataset = TweetDataset(\n        tweet=df_valid.text.values,\n        sentiment=df_valid.sentiment.values,\n        selected_text=df_valid.selected_text.values\n    )\n\n    valid_data_loader = torch.utils.data.DataLoader(\n        valid_dataset,\n        batch_size=config.VALID_BATCH_SIZE,\n        num_workers=2\n    )\n\n    device = torch.device(\"cuda\")\n    model_config = transformers.BertConfig.from_pretrained(config.BERT_PATH)\n    model_config.output_hidden_states = True\n    model = TweetModel(conf=model_config)\n    model.to(device)\n\n    num_train_steps = int(len(df_train) / config.TRAIN_BATCH_SIZE * config.EPOCHS)\n    param_optimizer = list(model.named_parameters())\n    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n    optimizer_parameters = [\n        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n    ]\n    optimizer = AdamW(optimizer_parameters, lr=3e-5)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, \n        num_warmup_steps=0, \n        num_training_steps=num_train_steps\n    )\n\n    es = EarlyStopping(patience=2, mode=\"max\")\n    print(f\"Training is Starting for fold={fold}\")\n    \n    # I'm training only for 3 epochs even though I specified 5!!!\n    for epoch in range(3):\n        train_fn(train_data_loader, model, optimizer, device, scheduler=scheduler)\n        jaccard = eval_fn(valid_data_loader, model, device)\n        print(f\"Jaccard Score = {jaccard}\")\n        es(jaccard, model, model_path=f\"model_{fold}.bin\")\n        if es.early_stop:\n            print(\"Early stopping\")\n            break\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"run(fold=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"run(fold=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"run(fold=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"run(fold=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"run(fold=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = pd.read_csv(\"../input/tweet-sentiment-extraction/test.csv\")\ndf_test.loc[:, \"selected_text\"] = df_test.text.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ndevice = torch.device(\"cuda\")\nmodel_config = transformers.BertConfig.from_pretrained(config.BERT_PATH)\nmodel_config.output_hidden_states = True\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model1 = TweetModel(conf=model_config)\nmodel1.to(device)\nmodel1.load_state_dict(torch.load(\"model_0.bin\"))\nmodel1.eval()\n\nmodel2 = TweetModel(conf=model_config)\nmodel2.to(device)\nmodel2.load_state_dict(torch.load(\"model_1.bin\"))\nmodel2.eval()\n\nmodel3 = TweetModel(conf=model_config)\nmodel3.to(device)\nmodel3.load_state_dict(torch.load(\"model_2.bin\"))\nmodel3.eval()\n\nmodel4 = TweetModel(conf=model_config)\nmodel4.to(device)\nmodel4.load_state_dict(torch.load(\"model_3.bin\"))\nmodel4.eval()\n\nmodel5 = TweetModel(conf=model_config)\nmodel5.to(device)\nmodel5.load_state_dict(torch.load(\"model_4.bin\"))\nmodel5.eval()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_output = []\n\ntest_dataset = TweetDataset(\n        tweet=df_test.text.values,\n        sentiment=df_test.sentiment.values,\n        selected_text=df_test.selected_text.values\n)\n\ndata_loader = torch.utils.data.DataLoader(\n    test_dataset,\n    shuffle=False,\n    batch_size=config.VALID_BATCH_SIZE,\n    num_workers=1\n)\n\nwith torch.no_grad():\n    tk0 = tqdm(data_loader, total=len(data_loader))\n    for bi, d in enumerate(tk0):\n        ids = d[\"ids\"]\n        token_type_ids = d[\"token_type_ids\"]\n        mask = d[\"mask\"]\n        sentiment = d[\"sentiment\"]\n        orig_selected = d[\"orig_selected\"]\n        orig_tweet = d[\"orig_tweet\"]\n        targets_start = d[\"targets_start\"]\n        targets_end = d[\"targets_end\"]\n        offsets = d[\"offsets\"].numpy()\n\n        ids = ids.to(device, dtype=torch.long)\n        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n        mask = mask.to(device, dtype=torch.long)\n        targets_start = targets_start.to(device, dtype=torch.long)\n        targets_end = targets_end.to(device, dtype=torch.long)\n\n        outputs_start1, outputs_end1 = model1(\n            ids=ids,\n            mask=mask,\n            token_type_ids=token_type_ids\n        )\n        \n        outputs_start2, outputs_end2 = model2(\n            ids=ids,\n            mask=mask,\n            token_type_ids=token_type_ids\n        )\n        \n        outputs_start3, outputs_end3 = model3(\n            ids=ids,\n            mask=mask,\n            token_type_ids=token_type_ids\n        )\n        \n        outputs_start4, outputs_end4 = model4(\n            ids=ids,\n            mask=mask,\n            token_type_ids=token_type_ids\n        )\n        \n        outputs_start5, outputs_end5 = model5(\n            ids=ids,\n            mask=mask,\n            token_type_ids=token_type_ids\n        )\n        outputs_start = (\n            outputs_start1 \n            + outputs_start2 \n            + outputs_start3 \n            + outputs_start4 \n            + outputs_start5\n        ) / 5\n        outputs_end = (\n            outputs_end1 \n            + outputs_end2 \n            + outputs_end3 \n            + outputs_end4 \n            + outputs_end5\n        ) / 5\n        \n        outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n        outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n\n        for px, tweet in enumerate(orig_tweet):\n            selected_tweet = orig_selected[px]\n            tweet_sentiment = sentiment[px]\n            _, output_sentence = calculate_jaccard_score(\n                original_tweet=tweet,\n                target_string=selected_tweet,\n                sentiment_val=tweet_sentiment,\n                idx_start=np.argmax(outputs_start[px, :]),\n                idx_end=np.argmax(outputs_end[px, :]),\n                offsets=offsets[px]\n            )\n            final_output.append(output_sentence)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def post_process(selected):\n    return \" \".join(set(selected.lower().split()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample = pd.read_csv(\"../input/tweet-sentiment-extraction/sample_submission.csv\")\nsample.loc[:, 'selected_text'] = final_output\nsample.selected_text = sample.selected_text.map(post_process)\nsample.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}