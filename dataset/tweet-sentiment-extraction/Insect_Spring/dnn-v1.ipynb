{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nnp.random.seed(42)\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom keras.preprocessing.text import text_to_word_sequence\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers.embeddings import Embedding\n\nfrom keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\nfrom keras.layers import RepeatVector, Dense, Activation, Lambda, Dropout, BatchNormalization\nfrom keras.optimizers import Adam\nfrom keras.utils import to_categorical\nfrom keras.models import load_model, Model\nimport keras.backend as K\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\n\nimport random\nfrom tqdm import tqdm\nfrom babel.dates import format_date\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"ROOT = '/kaggle/'\nINPUT_ROOT = ROOT + 'input/'\ntrain_data = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv')\ntest_data = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv')\nsubmission_data = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/sample_submission.csv')\npd.set_option('display.max_colwidth', None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def phrase_start_finder(s1, s2):\n    if s2 not in s1:\n        raise ValueError('s2 not substring of s1')\n    start = s1.find(s2)\n    return len(s1[:start].strip().split(' '))\n\ndef phrase_end_finder(s1, s2):\n    if s2 not in s1:\n        raise ValueError('s2 not substring of s1')\n    return phrase_start_finder(s1, s2) + len(s2.strip().split(' ')) - 1\n\ndef start_finder(row):\n    \"\"\"\n    Returns starting position of the phrase in the string when split by spaces.\n    \"\"\"\n    return phrase_start_finder(row['prepended_text'], row['selected_text'])\n    \n\ndef end_finder(row):\n    \"\"\"\n    Returns ending position of the phrase in the string when split by spaces.\n    \"\"\"\n    return phrase_end_finder(row['prepended_text'], row['selected_text'])\n\nprint(phrase_start_finder(\"bear is awesome but monkey is cool too\", \"awesome but monkey\"))\nphrase_end_finder(\"bear is awesome but monkey is cool too\", \"awesome but monkey\")\n\n# phrase_end_finder(\"bear is awesome but monkey is cool too\", \"not found\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.dropna(inplace=True)\ntrain_data['prepended_text'] = train_data['sentiment'] + ': ' + train_data['text']\ntrain_data['prepended_text'] = train_data['prepended_text'].astype('str')\ntrain_data['start'] = train_data.apply(start_finder, axis=1)\ntrain_data['end'] = train_data.apply(end_finder, axis=1)\nTx = max(train_data.apply(lambda row : len(row['prepended_text'].split()), axis=1)) + 10# start and end of sentence words\nprint(Tx)\nprint(train_data.dtypes)\n# Largest tweet has 34 words. Will truncate any tweets longer than this and pad the ones smaller than this\ntrain_data.head(10)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_unique_words(data, column_name='prepended_text'):\n    \"\"\"\n    Find and returns a set containing all unique words from a dataframe.\n    \"\"\"\n    unique_words = []\n    for index, row in data.iterrows():\n        words = str(row['prepended_text']).split(' ')\n        for word in words:\n            unique_words.append(word)\n    \n    return unique_words\n\n# Unit test\ndf = pd.DataFrame(data={'prepended_text': [\n    'positive: this world; was great', \n    'negative: until COVID-19 happened',\n    'neutral: now it is so-so'\n    ]})\nfind_unique_words(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_embeddings(filter_words=None):\n    \"\"\"\n    Loads glove embeddings. If filter_words is provided then the embedding dict returned only contains the filtered \n    words. Use this option if you only need a smaller set of words.\n    \"\"\"\n    embeddings_index = dict()\n    f = open(INPUT_ROOT + 'glove-twitter/glove.twitter.27B.50d.txt')\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        if word == 'unk':\n            embeddings_index[word] = coefs\n            continue\n        embeddings_index[word] = coefs\n    f.close()\n    print('Loaded %s word vectors.' % len(embeddings_index))\n    return embeddings_index\n\n# Loaded 1193514 word vectors.\nembeddings = load_embeddings(find_unique_words(train_data))\nembeddings['morning']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert_sentence_to_embedding(sentence, embeddings, max_sentence_len=Tx):\n    # TODO: Confirm that 'unk' is the unknown word\n    unknown_word = 'unk'\n    output = np.zeros((max_sentence_len, 50))\n    words = sentence.split(' ')\n    for j, word in enumerate(words):\n        if word in embeddings:\n            embedding = embeddings[word]\n        else:\n            embedding = embeddings[unknown_word]\n        output[j] = embedding\n    return output.reshape([max_sentence_len*50])\n\ndef convert_input_to_embeddings(data, embeddings, label='prepended_text', max_sentence_len=Tx):\n    unknown_word = 'unk'\n    output = np.zeros((data.shape[0], max_sentence_len, 50))\n    i = 0\n    for _, row in data.iterrows():\n        sentence = ' '.join(row[label].split())\n        words = sentence.split(' ')\n        for j, word in enumerate(words):\n            if word in embeddings:\n                embedding = embeddings[word]\n            else:\n                embedding = embeddings[unknown_word]\n            output[i, j] = np.array(embedding)\n        i = i+1\n    return output\n\ndef convert_to_vector(data, max_sentence_len=Tx):\n    label_vector = np.zeros((data.shape[0], max_sentence_len))\n    for i in range(len(data)):\n        for j in range(max_sentence_len):\n            if j >= data[i][0] and j <= data[i][1]:\n                label_vector[i][j] = 1\n    return label_vector\n\n                \nX_train, X_test, Y_train, Y_test = train_test_split(\n    train_data[['prepended_text']], \n    train_data[['start', 'end', 'selected_text']].iloc[:, :].values,\n    test_size=0.15,\n    random_state=42\n)\n    \ntrain_text = convert_input_to_embeddings(X_train, embeddings)\ntest_text = convert_input_to_embeddings(X_test, embeddings)\ntrain_labels = convert_to_vector(Y_train)\ntest_labels = convert_to_vector(Y_test)\n\ntrain_text = train_text.reshape((train_text.shape[0], Tx*50))\ntest_text = test_text.reshape((test_text.shape[0], Tx*50))\n\nprint(train_text.shape)\nprint(test_text.shape)\nprint(train_labels.shape)\nprint(test_labels.shape)\n# m samples, Tx denotes max size of a sentence, 50 denotes embeddings.\n# Note that it is common for last few embeddings to be 0.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def dnn(input_shape, output_shape=Tx):\n    \"\"\"\n    Shape of one input: Tx, embeddings_size\n    \"\"\"\n    X_input = Input(input_shape)\n    \n    X = Dense(128, activation='relu', name='fc1')(X_input)\n    \n    X = Dense(128, activation='relu', name='fc2')(X)\n    X = Dropout(0.3)(X)\n    X = BatchNormalization()(X)\n    X = Dense(64, activation='relu', name='fc3')(X)\n    X = Dropout(0.3)(X)\n    X = BatchNormalization()(X)\n    X = Dense(64, activation='relu', name='fc4')(X)\n    X = Dropout(0.15)(X)\n    X = BatchNormalization()(X)\n    X = Dense(output_shape , activation='sigmoid', name='fc_last')(X)\n    \n    model = Model(inputs=X_input, outputs=X, name=\"DNN for sentimend extraction\")\n    return model\n\nmodel = dnn([Tx*50])\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(train_text, train_labels, epochs=8, batch_size=64, validation_data=(test_text, test_labels))\n\n# Epoch 100/100\n# 23358/23358 [==============================] - 2s 99us/step - loss: 0.0704 - accuracy: 0.9701 - val_loss: 0.5517 \n#             - val_accuracy: 0.8561","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_input = \"positive: today is a wonderful day but not so cold\"\ntest_embedding = convert_sentence_to_embedding(test_input, embeddings)\nprint(test_embedding.shape)\ntest_embedding = test_embedding.reshape(1, -1)\nans = model.predict(test_embedding)\nprint(ans)\n\nmodel.evaluate(test_text, test_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_sentiment_segment(sentence, arr, threshold=0.05):\n    \"\"\"\n    Takes a sentence and an array of scores and and computes the sentiment using scores based on the theshold.\n    \n    sentence: Original full sentence\n    arr: An array containing scores for each word\n    \"\"\"\n    words = sentence.split(' ')\n    length = len(words)\n    ans_i = 0\n    ans_j = 0\n    for i in range(1, length):\n        for j in range(i, length):\n            # Check if sentence from word i to j has score > threshold\n            if len(sentence[i:j+1].strip()) == 0 or len(arr[i:j+1]) == 0:\n                continue\n            if min(arr[i:j+1]) > threshold and ans_j - ans_i < j - i:\n                ans_i = i\n                ans_j = j\n    return ' '.join(words[ans_i:ans_j+1]).strip()\n                \nfind_sentiment_segment(\n    \"positive: This is an amazingly amazing day for skiing\",\n    [0.002, 0.02, 0.011, 0.32, 0.323, 0.21, 0.15, 0.02, 0.002, 0.003], threshold=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_output = model.predict(test_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluate one single example.\nprint(test_output.shape)\nprint(test_labels.shape)\nprint(test_output[0])\nprint(test_labels[0])\nprint(X_test.iloc[0].prepended_text)\nprint('Y_test')\nprint(Y_test[0])\nsentiment_output = find_sentiment_segment(X_test.iloc[0].prepended_text, test_output[0], 0.000000000000001)\nsentiment_output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate_iou(test_outputs, X_test, Y_test, threshold=1e-06):\n    print(\"For threshold: \" + str(threshold))\n    print(Y_test.shape)\n    assert len(test_outputs) == len(Y_test)\n    assert len(test_outputs) == len(Y_test)\n    ans = 0.\n    labels_list = []\n    predictions_list = []\n    def jaccard(str1, str2): \n        a = set(str1.lower().split()) \n        b = set(str2.lower().split())\n        c = a.intersection(b)\n        return float(len(c)) / (len(a) + len(b) - len(c))\n    for i in range(len(test_outputs)):\n        sentiment_output = find_sentiment_segment(X_test.iloc[i].prepended_text, test_outputs[i], threshold)\n        jaccard_score = jaccard(Y_test[i][2], sentiment_output)\n        labels_list.append(Y_test[i][2])\n        predictions_list.append(sentiment_output)\n        ans = ans + jaccard_score\n    ans = ans/len(test_outputs)\n    print(\"Resulting iou: \" + str(ans))\n    return ans, labels_list, predictions_list\n\ndef find_optimum_threshold():    \n    ans = 0\n    ret = 0\n    responses = []\n    for threshold in [0.000000005, 0.0000005, 0.0005, 0.005, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6]:\n#     for threshold in [0.000000005]:\n        iou, l_list, p_list = evaluate_iou(test_output, X_test, Y_test, threshold)\n        labels_predictions = pd.DataFrame()\n        labels_predictions['labels'] = l_list\n        labels_predictions['predictions'] = p_list\n        labels_predictions.to_csv(str(threshold) + 'latest.csv', index=False)\n        responses.append((threshold, iou))\n        if iou > ans:\n            ans = iou\n            ret = threshold\n        threshold = threshold * 2\n    return ans, ret, responses\n            \n# ans, _, responses = find_optimum_threshold()\n# print(ans)\n# print(responses)\n\n\n# For threshold: 5e-08\n# Resulting iou: 0.5925133028085846\n    \nprint('completed')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint(X_test.shape)\nprint(test_output.shape)\nprint(Y_test.shape)\nprint(test_output[0])\nprint(Y_test[0])\nX_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluate on test data\n\ntest_data['prepended_text'] = test_data['sentiment'] + ': ' + test_data['text']\ntest_data['prepended_text'] = test_data['prepended_text'].astype('str')\n\ntest_unique_words = find_unique_words(test_data)\ntest_embeddings = load_embeddings(test_unique_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data_embeddings = convert_input_to_embeddings(test_data, test_embeddings, label='prepended_text')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data_embeddings = test_data_embeddings.reshape(-1, Tx*50)\ntest_predictions = model.predict(test_data_embeddings)\ntest_predictions[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert_output_to_sentences(test_predictions, test_data):\n#     assert len(test_predictions) == len(test_data)\n    output = []\n    for i in range(len(test_data)):\n        output.append(find_sentiment_segment(test_data.iloc[i].prepended_text, test_predictions[i], 0.005))\n    return np.array(output)\n\noutput = convert_output_to_sentences(test_predictions, test_data)\noutput = pd.DataFrame(output)\noutput.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import csv\nresult = test_data.join(output)\nresult = result[['textID', 0]]\nresult.rename(columns={0:'selected_text'}, inplace=True)\n# result['selected_text'] = '\"' + result['selected_text'] + '\"'\n# result.to_csv('submission.csv',quoting = csv.QUOTE_NONE,quotechar=\"\",escapechar = ',', index=False)\nresult.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result[result['selected_text'] == ''].index\n# df[df['column_name'] == ''].index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result[['selected_text']].to_numpy()\nfinal = []\nfor i in range(len(result)):\n    final.append(result.iloc[i].selected_text)\nfinal[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/tweet-sentiment-extraction/sample_submission.csv\")\ndf['selected_text'] = final\ndf['selected_text'] = df['selected_text'].apply(lambda x: x.replace('!!!!', '!') if len(x.split())==1 else x)\ndf['selected_text'] = df['selected_text'].apply(lambda x: x.replace('..', '.') if len(x.split())==1 else x)\ndf['selected_text'] = df['selected_text'].apply(lambda x: x.replace('...', '.') if len(x.split())==1 else x)\n\ndf.to_csv('submission.csv', index=False)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}