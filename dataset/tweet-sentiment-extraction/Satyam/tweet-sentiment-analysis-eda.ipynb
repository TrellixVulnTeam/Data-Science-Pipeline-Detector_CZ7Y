{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **Tweet Sentiment Analysis EDA**\n\nThis is my first NLP Kernal. I am beginner in this domain. I have tried doing some basic EDA.\n\n\n**References:**\n\nhttps://www.kaggle.com/tanulsingh077/twitter-sentiment-extaction-analysis-eda-and-model\n\nhttps://www.kaggle.com/parulpandey/basic-preprocessing-and-eda"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#Importing Libararies\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Reading Files\ndf_train = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv')\ndf_test = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv')\ndf_ss = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking size\nprint(df_train.size)\nprint(df_test.size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking Shape\nprint(df_train.shape)\nprint(df_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Some basic info about data\nprint(df_train.info())\nprint(df_test.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Cehcking the null values\ndf_train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Removing the null values\ndf_train.dropna(inplace=True)\ndf_train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Visualization of distribution of sentiments\nf, axes = plt.subplots(ncols=2 ,figsize=(15, 5))\nsns.countplot(df_train['sentiment'],ax=axes[0]).set_title('Sentiment')\nsns.countplot(df_test['sentiment'],ax=axes[1]).set_title('Sentiment')\n\n#Percenatge wise distribution of sentiments\nprint(df_train['sentiment'].value_counts(normalize=True))\nprint(df_test['sentiment'].value_counts(normalize=True))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df_train\nax = sns.countplot(y=\"sentiment\", data=df )\nplt.title('Distribution of  Configurations')\nplt.xlabel('Number of Axles')\n\ntotal = len(df['sentiment'])\nfor p in ax.patches:\n        percentage = '{:.1f}%'.format(100 * p.get_width()/total)\n        x = p.get_x() + p.get_width() + 0.02\n        y = p.get_y() + p.get_height()/2\n        ax.annotate(percentage, (x, y))\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df_test\nax = sns.countplot(y=\"sentiment\", data=df )\nplt.title('Distribution of  Configurations')\nplt.xlabel('Number of Axles')\n\ntotal = len(df['sentiment'])\nfor p in ax.patches:\n        percentage = '{:.1f}%'.format(100 * p.get_width()/total)\n        x = p.get_x() + p.get_width() + 0.02\n        y = p.get_y() + p.get_height()/2\n        ax.annotate(percentage, (x, y))\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Text Preprocessing\n#Text preprocessing helper functions\nimport nltk\nimport re\nimport string\nfrom nltk.corpus import stopwords\n\ndef clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n\n\ndef text_preprocessing(text):\n    \"\"\"\n    Cleaning and parsing the text.\n\n    \"\"\"\n    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n    nopunc = clean_text(text)\n    tokenized_text = tokenizer.tokenize(nopunc)\n    remove_stopwords = [w for w in tokenized_text if w not in stopwords.words('english')]\n    combined_text = ' '.join(tokenized_text)\n    return combined_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applying the cleaning function to both test and training datasets\ndf_train['text_clean'] = df_train['text'].apply(str).apply(lambda x: text_preprocessing(x))\ndf_test['text_clean'] = df_train['text'].apply(str).apply(lambda x: text_preprocessing(x))\n\ndf_train['selected_text_clean'] = df_train['selected_text'].apply(str).apply(lambda x: text_preprocessing(x))\ndf_test['selected_text_clean'] = df_train['selected_text'].apply(str).apply(lambda x: text_preprocessing(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Adding new Columns\ndf_train['text_len'] = df_train['text'].astype(str).apply(len)\ndf_train['Num_words_ST'] = df_train['selected_text'].apply(lambda x:len(str(x).split())) #Number Of words in Selected Text\ndf_train['Num_word_text'] = df_train['text'].apply(lambda x:len(str(x).split())) #Number Of words in main text\ndf_train['difference_in_words'] = df_train['Num_word_text'] - df_train['Num_words_ST'] #Difference in Number of words text and Selected Text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Text length distribution Visualization\nf, axes = plt.subplots(nrows=3 ,figsize=(15, 12))\nsns.distplot(df_train[df_train['sentiment'] == 'positive'][\"text_len\"],ax=axes[0],bins=75, color=\"#009c1a\", axlabel=False).set_title('Positive Sentiment')\nsns.distplot(df_train[df_train['sentiment'] == 'negative'][\"text_len\"],ax=axes[1],bins=75, color=\"#e3170a\", axlabel=False).set_title('Negative Sentiment')\nsns.distplot(df_train[df_train['sentiment'] == 'neutral'][\"text_len\"],ax=axes[2],bins=75, color=\"#011f4b\", axlabel=False).set_title('Neutral Sentiment')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Color palettes\ncolors = [\"#e9d758\", \"#ff8552\", \"#338e8e\", \"#e6e6e6\", \"#39393a\"]\nRed = [\"#e3170a\", \"#e04135\", \"#d69e9a\", \"#d6c2c0\", \"#d6cecd\"]\nBlue = [\"#011f4b\",\"#03396c\",\"#005b96\",\"#6497b1\",\"#b3cde0\"]\nGreen = [\"#009c1a\",\"#22b600\",\"#26cc00\",\"#7be382\",\"#d2f2d4\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Number of words in Text  Visualization\nf, axes = plt.subplots(nrows=3 ,figsize=(15, 10))\nsns.countplot(df_train[df_train['sentiment'] == 'positive'][\"Num_word_text\"],ax=axes[0], palette=Green)\nsns.countplot(df_train[df_train['sentiment'] == 'negative'][\"Num_word_text\"],ax=axes[1], palette=Red)\nsns.countplot(df_train[df_train['sentiment'] == 'neutral'][\"Num_word_text\"],ax=axes[2], palette=Blue)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Number of words in Selected Text  Visualization\nf, axes = plt.subplots(nrows=3 ,figsize=(15, 10))\nsns.countplot(df_train[df_train['sentiment'] == 'positive'][\"Num_words_ST\"],ax=axes[0], color=\"#009c1a\")\nsns.countplot(df_train[df_train['sentiment'] == 'negative'][\"Num_words_ST\"],ax=axes[1], color=\"#e3170a\")\nsns.countplot(df_train[df_train['sentiment'] == 'neutral'][\"Num_words_ST\"],ax=axes[2], color=\"#011f4b\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Number of words in Text and Selected Text Visulaization\nfig = plt.figure(figsize=(20, 5))\nsns.kdeplot(df_train[\"Num_words_ST\"],shade=True, color=\"#011f4b\")\nsns.kdeplot(df_train[\"Num_word_text\"], shade=True, color=\"#e3170a\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Common Words from Text_Clean\n\nfrom collections import Counter\ntemp_list = df_train['text_clean'].apply(lambda x:str(x).split())\ntop = Counter([item for sublist in temp_list for item in sublist])\ntemp = pd.DataFrame(top.most_common(20))\ntemp.columns = ['Common_words','count']\ntemp.style.background_gradient(cmap=\"bone_r\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Common Words from selected_text_clean\n\nfrom collections import Counter\ntemp_list = df_train['selected_text_clean'].apply(lambda x:str(x).split())\ntop = Counter([item for sublist in temp_list for item in sublist])\ntemp = pd.DataFrame(top.most_common(20))\ntemp.columns = ['Common_words','count']\ntemp.style.background_gradient(cmap='bone_r')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Sentiment Wise Common Words in text_clean**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#positive\nfrom collections import Counter\ntemp_list = df_train[df_train['sentiment']=='positive']['text_clean'].apply(lambda x:str(x).split())\ntop = Counter([item for sublist in temp_list for item in sublist])\ntemp1 = pd.DataFrame(top.most_common(20))\ntemp1.columns = ['Common_words','count']\ntemp1.style.background_gradient(cmap='Greens')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#neagtive\ntemp_list = df_train[df_train['sentiment']=='negative']['text_clean'].apply(lambda x:str(x).split())\ntop = Counter([item for sublist in temp_list for item in sublist])\ntemp1 = pd.DataFrame(top.most_common(20))\ntemp1.columns = ['Common_words','count']\ntemp1.style.background_gradient(cmap='Reds')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#neagtive\ntemp_list = df_train[df_train['sentiment']=='neutral']['text_clean'].apply(lambda x:str(x).split())\ntop = Counter([item for sublist in temp_list for item in sublist])\ntemp1 = pd.DataFrame(top.most_common(20))\ntemp1.columns = ['Common_words','count']\ntemp1.style.background_gradient(cmap='Blues')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Wordcloud**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Wordcloud\nf, ax = plt.subplots(nrows=3 ,figsize=(20,35))\n\n#Positive\nfrom wordcloud import WordCloud\ncloud1 = WordCloud(width=1440, height=1080).generate(str(df_train[df_train['sentiment']=='positive'][\"text_clean\"]))\nax[0].imshow(cloud1)\nax[0].axis('off')\nax[0].set_title('Positive',fontsize=25);\n\n#Neutral\ncloud2 = WordCloud(width=1440, height=1080).generate(str(df_train[df_train['sentiment']=='neutral'][\"text_clean\"]))\nax[1].imshow(cloud2)\nax[1].axis('off')\nax[1].set_title('Neutral',fontsize=25);\n\n#Negative\ncloud3 = WordCloud(width=1440, height=1080).generate(str(df_train[df_train['sentiment']=='negative'][\"text_clean\"]))\nax[2].imshow(cloud3)\nax[2].axis('off')\nax[2].set_title('Negative',fontsize=25);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#\npos = df_train[df_train['sentiment']=='positive']\nneg = df_train[df_train['sentiment']=='negative']\nneutral = df_train[df_train['sentiment']=='neutral']\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**N-GRAMS**"},{"metadata":{"trusted":true},"cell_type":"code","source":"##source of code : https://medium.com/@cristhianboujon/how-to-list-the-most-common-words-from-text-corpus-using-scikit-learn-dad4d0cab41d\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n\ndef get_top_n_words(corpus, n=None):\n    \"\"\"\n    List the top n words in a vocabulary according to occurrence in a text corpus.\n    \"\"\"\n    vec = CountVectorizer(stop_words = 'english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Unigram"},{"metadata":{"trusted":true},"cell_type":"code","source":"#for word, freq in top_unigrams:\n    #print(word, freq)\n\npos_unigrams = get_top_n_words(pos['text_clean'],30)\nneg_unigrams = get_top_n_words(neg['text_clean'],30)\nneutral_unigrams = get_top_n_words(neutral['text_clean'],30)    \n    \nplt.subplots(figsize=(15,5))\ndf1 = pd.DataFrame(pos_unigrams, columns = ['Text' , 'count'])\ndf1.groupby('Text').sum()['count'].sort_values(ascending=True).plot(\n    kind='bar',color='#009c1a', title='Top 20 Unigrams in positve text')\n\nplt.subplots(figsize=(15,5))\ndf2 = pd.DataFrame(neg_unigrams, columns = ['Text' , 'count'])\ndf2.groupby('Text').sum()['count'].sort_values(ascending=True).plot(\n     kind='bar',  color='#e3170a',title='Top 20 Unigrams in negative text')\n\nplt.subplots(figsize=(15,5))\ndf3 = pd.DataFrame(neutral_unigrams, columns = ['Text' , 'count'])\ndf3.groupby('Text').sum()['count'].sort_values(ascending=True).plot(\n     kind='bar',color=\"#011f4b\" ,title='Top 20 Unigrams in neutral text')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"BiGrams"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_top_n_gram(corpus,ngram_range,n=None):\n    vec = CountVectorizer(ngram_range=ngram_range,stop_words = 'english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pos_bigrams = get_top_n_gram(pos['text_clean'],(2,2),30)\nneg_bigrams = get_top_n_gram(neg['text_clean'],(2,2),30)\nneutral_bigrams = get_top_n_gram(neutral['text_clean'],(2,2),30)\n\n\n#for word, freq in top_bigrams:\n    #print(word, freq)\n    \nplt.subplots(figsize=(15,5))\ndf1 = pd.DataFrame(pos_bigrams, columns = ['Text' , 'count'])\ndf1.groupby('Text').sum()['count'].sort_values(ascending=True).plot(\n    kind='bar',color='#009c1a', title='Top 20 Bigrams in positve text')\n   \n    \nplt.subplots(figsize=(15,5))\ndf2 = pd.DataFrame(neg_bigrams, columns = ['Text' , 'count'])\ndf2.groupby('Text').sum()['count'].sort_values(ascending=True).plot(\n     kind='bar',  color='#e3170a',title='Top 20 Bigrams in negative text')\n\nplt.subplots(figsize=(15,5))\ndf3 = pd.DataFrame(neutral_bigrams, columns = ['Text' , 'count'])\ndf3.groupby('Text').sum()['count'].sort_values(ascending=True).plot(\n     kind='bar',color=\"#011f4b\" ,title='Top 20 Bigrams in neutral text')\n\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}