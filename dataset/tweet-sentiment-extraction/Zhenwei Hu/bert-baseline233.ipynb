{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom typing import List, Tuple, Dict, Set\nfrom collections import Counter\nfrom time import time\nimport random\nimport json\nimport math\nimport nltk\nfrom matplotlib import pyplot as plt\n\nimport torchtext\nfrom torchtext import data, datasets\nfrom torch.utils.data.dataset import random_split\n\nimport os\n\"\"\"\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\"\"\"\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"batch_size = 8\nn_epochs = 2\nmax_context_len = 150\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nprint(\"device: \", device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def divide_dataset(__train, factor, fields):\n    train_size = int(len(__train) * factor)\n    valid_size = len(__train) - train_size\n    valid_indices = list(range(train_size+valid_size))\n    random.shuffle(valid_indices)\n    train_indices = valid_indices[:train_size]\n    valid_indices = valid_indices[train_size:]\n    train = [__train[idx] for idx in train_indices]\n    valid = [__train[idx] for idx in valid_indices]\n    train = data.Dataset(train, fields)\n    valid = data.Dataset(valid, fields)\n    return train, valid\n    \ndef is_whitespace(c):\n    if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" \\\n    or ord(c) == 0x202F:\n        return True\n    return False\n\ndef character_filter(c):\n    if c == \"\\t\": return \"\"\n    if ord(c)<128: return c\n    if c in \"≠•∞™ˈʃʊʁʁiʁɑ̃ʃɔ.̃ºª¶§¡£¢ç\": return \"z\"\n    if c in \"àáâãäåæ\": return \"a\"\n    if c in \"èéêë\": return \"e\"\n    if c in \"ìíîï\": return \"i\"\n    if c in \"òóôõöōŏő\": return \"o\"\n    if c in \"ùúûü\": return \"u\"\n\n    if c in \"ÀÁÂÃÄÅ\": return \"A\"\n    if c in \"ÈÉÊË\": return \"E\"\n    if c in \"ÌÍÎÏ\": return \"I\" \n    if c in \"ÒÓÔÕÖŌŎŐ\": return \"O\"\n    if c in \"ÙÚÛÜ\": return \"U\"\n\n    return \"z\"\ndef postproc(text, x):\n    ret = []\n    for sent in text:\n        ret.append(tokenizer.convert_tokens_to_ids(sent))\n    return ret","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_SQuAD(file, fields, train):\n    input_data = pd.read_csv(file)\n    examples = []\n    word_to_char_list = []\n    context_raw_test = []\n    \n    for idx, row in input_data.iterrows():\n       # print(row['text'], )\n        context_raw = row['text']\n        question_raw = row['sentiment']\n        if type(context_raw) != str:\n            print(\"data\", idx, \"is illegal\")\n            continue\n        context_raw = context_raw.strip()\n        if not train:\n            context_raw_test.append(context_raw)\n        context_raw = \"\".join(map(character_filter, context_raw))\n\n        pt = 0\n        char_to_word = [-1]*len(context_raw)\n        word_to_char = []\n        context  = tokenizer.tokenize(context_raw)\n        question = tokenizer.tokenize(question_raw)\n        \n        word_to_char = [-1]*(len(question)+2)\n        if (len(context)>max_context_len): \n            print(\"data\", idx, \":too long\")\n            continue\n        if unk_token in context:\n            print(\"data\", idx, \"tokenize with [UNK]\", \":discard\")\n            continue\n        for idx, word in enumerate(context):\n            if model_name == \"bert\":\n                if word[0:2]==\"##\":\n                    word = word[2:]\n            if model_name == \"roberta\":\n                if word[0]==\"Ġ\":\n                    word = word[1:]\n            if model_name == \"xlnet\":\n                if word[0]==\"▁\":\n                    word = word[1:]\n            if pt>=len(context_raw):\n                print(context_raw)\n                print(context)\n                print(idx, word)\n            while is_whitespace(context_raw[pt]): \n                pt+=1\n                if pt>=len(context_raw):\n                    print(context_raw)\n                    print(context)\n                    print(idx, word)\n                    \n\n            if word == '\"':\n                if context_raw[pt:pt+2] != \"``\" and context_raw[pt:pt+2] != \"''\":\n                    print(context_raw)\n                    print(pt)\n                    print(context_raw[pt:pt+len(word)], word)\n                char_to_word[pt:pt+2]= [idx]*len(word)\n                word_to_char.append(pt)\n                pt = pt + 2\n            else:\n                if context_raw[pt:pt+len(word)].lower() != word.lower():\n                    print(context_raw)\n                    print(pt)\n                    print(context_raw[pt:pt+len(word)], word)\n                char_to_word[pt:pt+len(word)]= [idx]*len(word)\n                word_to_char.append(pt)\n                pt = pt + len(word)\n        word_to_char.append(pt)\n        \n             \n        if train:    \n            answer_text = row['selected_text']\n            answer_text = \"\".join(map(character_filter, answer_text))\n            answer = tokenizer.tokenize(answer_text)\n            \n            raw_start = context_raw.find(answer_text)\n            assert(raw_start>=0)\n            answer_start= char_to_word[raw_start]\n            answer_end  = char_to_word[raw_start + len(answer_text) - 1]\n  \n            \"\"\"\n            for i in range(len(context)):\n                flag = True\n                for j, w in enumerate(answer):\n                    if context[i+j] != w:\n                        flag = False\n                        break\n                if flag:\n                    answer_start = i\n                    answer_end   = i + len(answer) - 1\n                    break\n            \"\"\"\n                    \n            assert(answer_end>=0 and answer_start>=0)\n            answer_start += len(question)+2\n            answer_end   += len(question)+2\n        else:\n            answer_start = -1\n            answer_end   = -1\n            assert(len(word_to_char) == len(question) + len(context) + 3)\n            word_to_char_list.append(word_to_char)\n            \n\n        example = data.Example.fromlist([\n            [cls_token]+question+[sep_token]+ context+[sep_token], \n            answer_start, \n            answer_end],\n            fields\n        )\n        examples.append(example)\n\n    if train:\n        return data.Dataset(examples, fields)\n    else:\n        return data.Dataset(examples, fields), context_raw_test, word_to_char_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import BertTokenizer   , BertForQuestionAnswering   , BertConfig\nfrom transformers import XLNetTokenizer  , XLNetForQuestionAnsweringSimple  , XLNetConfig\nfrom transformers import RobertaTokenizer, RobertaModel               , RobertaConfig\n\nmodel_name = \"roberta\"\nPATH = \"/kaggle/input/huggingfacetransformermodels/model_classes/\"\n\nif model_name == \"xlnet\":\n    tmp = PATH + model_name + \"/\" + model_name + \"-large-cased-\"\n    Tokenizer =  XLNetTokenizer\n    TPATH = tmp + \"tokenizer/\"\n    model = XLNetForQuestionAnsweringSimple.from_pretrained(tmp+\"pytorch-model/\").to(device)\n\nclass QAModel(nn.Module):\n    def __init__(self, model):\n        super().__init__()\n        self.model = model\n        self.conv = nn.Conv1d(1024, 128, 3, padding=1)\n        self.dropout = nn.Dropout(0.1)\n        self.fc = nn.Linear(128, 2)\n    def forward(self, x):\n        out = self.model(x)[0].transpose(1, 2)\n        out = self.dropout(out) # (32, 1024, L)\n        out = F.relu(self.conv(out)).transpose(1, 2)\n        out = self.fc(out)\n        return out[:,:,0], out[:,:,1]\n    \nif model_name == \"roberta\":\n    QA_PATH = '/kaggle/input/roberta-transformers-pytorch/roberta-large/'\n    Tokenizer = RobertaTokenizer\n    TPATH = QA_PATH\n    \n    model = RobertaModel.from_pretrained(QA_PATH).to(device)\n    model = QAModel(model).to(device)\n    \nif model_name == \"bert\":\n    QA_PATH = '/kaggle/input/bertlargewholewordmaskingfinetunedsquad/'\n    prefix = \"bert-large-uncased-whole-word-masking-\"\n    Tokenizer = BertTokenizer\n    TPATH = QA_PATH + prefix + 'vocab.txt'\n    \n    config = BertConfig.from_pretrained(\n        QA_PATH + prefix + 'finetuned-squad-config.json')\n    model = BertForQuestionAnswering.from_pretrained(\n        QA_PATH + prefix + 'finetuned-squad-pytorch_model.bin', config=config).to(device)\n\ntokenizer = Tokenizer.from_pretrained(TPATH)\ncls_token = tokenizer.cls_token\nsep_token = tokenizer.sep_token\npad_token = tokenizer.pad_token\nunk_token = tokenizer.unk_token","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CONTEXT     = data.Field(batch_first=True, use_vocab=False, postprocessing=postproc, pad_token=pad_token)#, preprocessing=preproc)\nLABEL       = data.Field(batch_first=True, use_vocab=False, sequential=False)\n# make splits for data\nfields = [\n    (\"text\", CONTEXT),\n    (\"answer_start\" , LABEL),\n    (\"answer_end\"   , LABEL),\n]\n\nprint(\"read train data...\")\n__train = read_SQuAD(\"/kaggle/input/tweet-sentiment-extraction/train.csv\", fields, True)\ntrain, valid = divide_dataset(__train, 0.9, fields)\n\nprint(\"read test data...\")\ntest, context_raw_test, word_to_char_list = \\\n    read_SQuAD(\"/kaggle/input/tweet-sentiment-extraction/test.csv\", fields, False)\n\ntrain_size = len(train)\nvalid_size = len(valid)\ntest_size  = len(test)\nprint(\"read finish\")\n\nprint(\"train size :\", train_size)\nprint(\"valid size :\", valid_size)\nprint(\"test size :\" , test_size)\n\n# make iterator for splits\ntrain_iter = data.BucketIterator(train, batch_size=batch_size, train=True, sort_key=lambda x:len(x.text), device=device)\nvalid_iter = data.BucketIterator(valid, batch_size=batch_size, train=False,sort_key=lambda x:len(x.text), device=device)\ntest_iter  = data.Iterator(test , batch_size=1, train=False, sort=False, device=device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def data_check():\n    for i in range(5):\n        print(train[i].text)\n        print(train[i].text[train[i].answer_start: train[i].answer_end+1])\n\n    train_length_freqs = Counter([len(example.text) for example in train])\n    print(\"train: min len :\",min(train_length_freqs),\"max len :\",max(train_length_freqs))\n    \n    test_length_freqs = Counter([len(example.text) for example in test])\n    print(\"test: min len :\",min(test_length_freqs),\"max len :\",max(test_length_freqs))\n    \n    \"\"\"\n  #  plt.hist([len(example.text) for example in train], log=True, bins=20, range=(0, 120))\n  #  plt.savefig(\"/kaggle/working/train_len.png\")\n    plt.hist([len(example.text) for example in test] , log=True, bins=20, range=(0, 120))\n   # plt.savefig(\"/kaggle/working/full_len.png\")\n    \n  #  plt.hist([example.answer_end-example.answer_start+1 for example in train],\n   #          log=True, bins=20, range=(0, 120))\n  #  plt.savefig(\"/kaggle/working/train_answer_len.png\")\n    plt.hist([example.answer_end-example.answer_start+1 for example in test],\n             log=True, bins=20, range=(0, 120))\n  #  plt.savefig(\"/kaggle/working/full_len.png\")\n    \"\"\"\ndata_check()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MRCMetrics(object):\n    def __init__(self, criterion):\n        self.loss = 0\n        self.size = 0\n        self.F1 = 0\n        self.EM = 0\n        self.ja = 0\n        self.criterion = criterion\n    def update(self, st_prob, ed_prob, st, ed):\n        st_pred = st_prob.argmax(-1)\n        ed_pred = ed_prob.argmax(-1)\n        size = st_pred.shape[0]\n\n        zeros = torch.zeros_like(st_pred, dtype=torch.float)\n  \n        F1 = torch.max(zeros  , zeros+torch.min(ed_pred, ed) - torch.max(st_pred, st)+1)*2 \\\n            /torch.max(zeros+2, zeros+ed_pred+ed-st_pred-st+2)\n        ja = torch.max(zeros  , zeros+torch.min(ed_pred, ed) - torch.max(st_pred, st)+1) \\\n            /torch.max(zeros  , zeros+torch.max(ed_pred, ed) - torch.min(st_pred, st)+1)\n\n        loss = self.criterion(st_prob, st) + criterion(ed_prob, ed)*size\n      #      print(\"grad:\", loss_t.requires_grad)\n    #        print(F1.shape, F1_t.shape)\n     #       print(F1.dtype, F1_t.dtype)\n        EM = ((st_pred == st)*(ed_pred == ed)).to(torch.float)\n        self.F1 += F1.sum().item()\n        self.EM += EM.sum().item()\n        self.ja += ja.sum().item()\n        self.loss += loss.sum().item()\n        self.size += size\n        return loss/size\n    def __getitem__(self, key):\n        return getattr(self, key)/self.size\n    def __str__(self):\n        return (\"loss: %.4f, EM: %.4f, F1: %.4f, ja: %.4f\" %(self[\"loss\"], self[\"EM\"], self[\"F1\"], self[\"ja\"]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_num = sum(p.numel() for p in model.parameters())\nprint('Total:', total_num)\n\ncriterion = torch.nn.CrossEntropyLoss().to(device)\nif model_name == \"bert\":\n    lr = 1e-5\nelif model_name == \"roberta\":\n    lr = 1e-5\nelif model_name == \"xlnet\":\n    lr = 6e-6\nclip_grad_value = 5\noptimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=3e-3)\n\n#scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch=-1)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.2, patience=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_check():\n    batch = next(iter(valid_iter))\n    print(batch.text[0])\n    \n    st_prob, ed_prob = model(batch.text)\n    #out = model(batch.text)\n    #print(out)\n    #print(out.shape)\n    print(st_prob.shape)\n    \"\"\"\n    print(st_prob[0])\n    print(ed_prob[0])\n    print(batch.answer_start[0])\n    print(batch.answer_end[0])\n    \"\"\"\n    metrics = MRCMetrics(criterion)\n    loss = metrics.update(st_prob, ed_prob, batch.answer_start, batch.answer_end)\n    print(loss)\n\n    optimizer.zero_grad()\n    #loss.backward()\n    print(metrics)\n#model_check()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_epoch(data_iter, train):\n    # Train the model\n    metrics = MRCMetrics(criterion)\n    num_iter = len(data_iter)\n    if train:\n        model.train()\n    else:\n        model.eval()\n    for i, batch in enumerate(data_iter):\n        text = batch.text.to(device)\n        answer_start = batch.answer_start.to(device)\n        answer_end   = batch.answer_end  .to(device)\n   #     print(context.shape, question.shape)\n   #     print(answer_start.shape)\n        \n        if train:\n            st_prob, ed_prob = model(text)\n            loss = metrics.update(st_prob, ed_prob, answer_start, answer_end)\n            loss.backward()\n        #    nn.utils.clip_grad_norm_(model.parameters(), clip_grad_value)\n            optimizer.step()\n            optimizer.zero_grad()\n            \n\n            if ((i+1)%400==0): \n                scheduler.step(metrics[\"ja\"])\n                print(i, \": train\",  metrics, \"lr:\", optimizer.param_groups[0][\"lr\"])\n            \"\"\"\n            if loss.item()<loss_min:\n                loss_min = loss.item()\n                loss_count = 0\n            else:\n                loss_count += 1\n            if loss_count >= 300:\n                scheduler.step()\n                loss_count = 0\n                print(\"scheduler\")\n            \"\"\"\n        else:\n            with torch.no_grad():\n                st_prob, ed_prob = model(text)\n                loss = metrics.update(st_prob, ed_prob, answer_start, answer_end)\n\n    return metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for epoch in range(n_epochs):\n    start_time = time()\n    train_metrics = run_epoch(train_iter, True)\n    valid_metrics = run_epoch(valid_iter, False)\n\n    secs = int(time() - start_time)\n    print(\"epoch\", epoch,\"finished in \"+str(secs)+\"s\")\n    print(\"train:\", train_metrics)\n    print(\"valid:\", valid_metrics)\n    # train loss: 0.83, EM: 0.53, F1: 0.74, ja: 0.69","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def decode(st_prob, ed_prob, word_to_char):\n    global cnt\n    if st_prob.argmax().item() > ed_prob.argmax().item():\n        cnt = cnt + 1\n        print(\"empty out \", cnt)\n    n = st_prob.shape[1]-1\n    mx = -12345\n    start, end = -1, -1\n    for i in range(n):\n        for j in range(i, n):\n            if word_to_char[i] < 0: continue\n            if word_to_char[j] < 0: continue\n            logit = st_prob[0,i].item() + ed_prob[0,j].item()\n            if logit > mx:\n                mx = logit\n                start, end = i, j\n    return start, end\n# 80 zero","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.eval()\nout = []\ncnt = 0\nfor batch, context_raw, word_to_char in zip(test_iter, context_raw_test, word_to_char_list):\n    text = batch.text.to(device)\n    with torch.no_grad():\n        st_prob, ed_prob = model(text)\n        assert(st_prob.shape[0]==1)\n        \n        start = st_prob.argmax().item()\n        end   = ed_prob.argmax().item()\n      #  print(start, end)\n        start, end = decode(st_prob, ed_prob, word_to_char)\n        answer= context_raw[word_to_char[start]: word_to_char[end+1]]\n        out.append(answer)\n       # print(answer)\n        \"\"\"\n        if i%100==0: \n            print(i)\n            print(phrase)\n            print(tokens)\n            print(string)\n        \"\"\"\nsample = pd.read_csv(\"/kaggle/input/tweet-sentiment-extraction/sample_submission.csv\")\nsample.loc[:, 'selected_text'] = out\nsample.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}