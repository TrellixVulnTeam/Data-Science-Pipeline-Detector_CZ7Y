{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport re\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nfrom tokenizers import BertWordPieceTokenizer\nfrom transformers import BertTokenizer, TFBertModel, BertConfig","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_len = 100\n\nconfiguration = BertConfig.from_pretrained(f'/kaggle/input/bert-tensorflow/bert-base-uncased-config.json')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save the slow pretrained tokenizer\nslow_tokenizer = BertTokenizer.from_pretrained('/kaggle/input/bertbaseuncased/vocab.txt')\nsave_path = \"bert-base-uncased/\"\nif not os.path.exists(save_path):\n    os.makedirs(save_path)\nslow_tokenizer.save_pretrained(save_path)\n\n# Load the fast tokenizer from saved file\ntokenizer = BertWordPieceTokenizer('/kaggle/input/bertbaseuncased/vocab.txt', lowercase=True)\n#tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SquadTest:\n    def __init__(self, text, sentiment):\n        self.text = text\n        self.sentiment = sentiment\n        \n    def preprocess(self):        \n        text = self.text\n        sentiment = self.sentiment\n\n        text = str(text)\n        sentiment = str(sentiment)\n                  \n        # Tokenize context\n        tokenized_context = tokenizer.encode(text)       \n        \n        # Tokenize question\n        tokenized_question = tokenizer.encode(sentiment)\n\n        # Create inputs\n        input_ids = tokenized_context.ids + tokenized_question.ids[1:]\n        token_type_ids = [0] * len(tokenized_context.ids) + [1] * len(\n            tokenized_question.ids[1:]\n        )\n        attention_mask = [1] * len(input_ids)\n\n        # Pad and create attention masks.\n        # Skip if truncation is needed\n        padding_length = max_len - len(input_ids)\n        if padding_length > 0:  # pad\n            input_ids = input_ids + ([0] * padding_length)\n            attention_mask = attention_mask + ([0] * padding_length)\n            token_type_ids = token_type_ids + ([0] * padding_length)\n        elif padding_length < 0:  # skip\n            self.skip = True\n            return\n\n\n        self.input_ids = input_ids\n        self.token_type_ids = token_type_ids\n        self.attention_mask = attention_mask\n\ndef create_test(raw_data):\n    squad_examples = []\n    for i in range(raw_data.shape[0]):\n        text = raw_data[\"text\"][i]\n        sentiment = raw_data[\"sentiment\"][i] \n        squad_eg = SquadTest(text, sentiment)\n            \n        squad_eg.preprocess()\n        squad_examples.append(squad_eg)\n        \n    return squad_examples\n\ndef create_inputs_test(squad_examples):\n    dataset_dict = {\n        \"input_ids\": [],\n        \"token_type_ids\": [],\n        \"attention_mask\": [],\n    }\n    for item in squad_examples:\n        for key in dataset_dict:\n            dataset_dict[key].append(getattr(item, key))\n    for key in dataset_dict:\n        dataset_dict[key] = np.array(dataset_dict[key])\n\n    x = [\n        dataset_dict[\"input_ids\"],\n        dataset_dict[\"token_type_ids\"],\n        dataset_dict[\"attention_mask\"],\n    ]\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"base_path = '/kaggle/input/bert-tensorflow/bert-base-uncased-tf_model.h5'\n\ndef create_model(path = base_path):\n    ## BERT encoder\n    encoder = TFBertModel.from_pretrained(path, config=configuration)\n\n    ## QA Model\n    input_ids = layers.Input(shape=(max_len,), dtype=tf.int32)\n    token_type_ids = layers.Input(shape=(max_len,), dtype=tf.int32)\n    attention_mask = layers.Input(shape=(max_len,), dtype=tf.int32)\n    embedding = encoder(\n        input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask\n    )[0]\n\n    start_logits = layers.Dense(1, name=\"start_logit\", use_bias=False)(embedding)\n    start_logits = layers.Flatten()(start_logits)\n\n    end_logits = layers.Dense(1, name=\"end_logit\", use_bias=False)(embedding)\n    end_logits = layers.Flatten()(end_logits)\n\n    start_probs = layers.Activation(keras.activations.softmax)(start_logits)\n    end_probs = layers.Activation(keras.activations.softmax)(end_logits)\n\n    model = keras.Model(\n        inputs=[input_ids, token_type_ids, attention_mask],\n        outputs=[start_probs, end_probs],\n    )\n    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n    optimizer = keras.optimizers.Adam(lr=5e-5)\n    model.compile(optimizer=optimizer, loss=[loss, loss])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def ans_convert(start_scores, end_scores, input_ids, tokenizer):\n     # ======== Reconstruct Answer ========\n    # Find the tokens with the highest `start` and `end` scores.\n    answer_start = np.argmax(start_scores)\n    answer_end = np.argmax(end_scores)\n\n    # Get the string versions of the input tokens.\n    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n\n    # Start with the first token.\n    answer = tokens[answer_start]\n\n    # Select the remaining answer tokens and join them with whitespace.\n    for i in range(answer_start + 1, answer_end + 1):\n\n        # If it's a subword token, then recombine it with the previous token.\n        if tokens[i][0:2] == '##':\n            answer += tokens[i][2:]\n\n        # Otherwise, add a space then the token.\n        else:\n            answer += ' ' + tokens[i]\n            \n    #print(str(answer_start), \" - \", str(answer_end))\n    return answer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.read_csv(f'/kaggle/input/tweet-sentiment-extraction/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df['text'] = test_df['text'].str.strip()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_examples = create_test(test_df)\nx_test = create_inputs_test(test_examples)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = create_model(base_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.load_weights(f'/kaggle/input/bert-model-weights/bert_model_weights.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred2 = model.predict(x_test, batch_size=16)\nans = [ans_convert(pred2[0][i],pred2[1][i],x_test[0][i],slow_tokenizer) for i in range(len(x_test[0]))] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_sub = test_df.copy()\ntest_sub['selected_text'] = ans\ntest_sub = test_sub[['textID','selected_text']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def replace_blank (text):\n    punc = '''!()-[]{};:'\"\\, <>./?@#$%^&*_~'''\n    for i in text:  \n        if i in punc:  \n            text = text.replace(\" \" +i, i)  \n    \n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_sub['selected_text'] = test_sub['selected_text'].apply(replace_blank)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_sub['selected_text'] = '\"' + test_sub['selected_text']+'\"'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_sub.head(50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_sub.to_csv('submission.csv',index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}