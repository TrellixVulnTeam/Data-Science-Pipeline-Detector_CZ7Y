{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#https://keras.io/examples/nlp/text_extraction_with_bert/#train-and-evaluate","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport re","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow import keras\nfrom tensorflow.keras import layers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tokenizers import BertWordPieceTokenizer\nfrom transformers import BertTokenizer, TFBertModel, BertConfig","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from kaggle_datasets import KaggleDatasets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_len = 300","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"configuration = BertConfig.from_pretrained(f'/kaggle/input/bert-tensorflow/bert-base-uncased-config.json')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save the slow pretrained tokenizer\nslow_tokenizer = BertTokenizer.from_pretrained(f'/kaggle/input/bertbaseuncased/vocab.txt')\nsave_path = \"bert-base-uncased/\"\nif not os.path.exists(save_path):\n    os.makedirs(save_path)\nslow_tokenizer.save_pretrained(save_path)\n\n# Load the fast tokenizer from saved file\ntokenizer = BertWordPieceTokenizer(f'/kaggle/input/bertbaseuncased/vocab.txt', lowercase=True)\n#tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(f'/kaggle/input/tweet-sentiment-extraction/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.read_csv(f'/kaggle/input/tweet-sentiment-extraction/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['text'] = train_df['text'].str.strip()\ntrain_df['selected_text'] = train_df['selected_text'].str.strip()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SquadExample:\n    def __init__(self, text, sentiment, selected_text, start_char_idx):\n        self.text = text\n        self.sentiment = sentiment\n        self.selected_text = selected_text\n        self.start_char_idx = start_char_idx\n        self.skip = False\n        \n    def preprocess(self):\n        \n        text = self.text\n        sentiment = self.sentiment\n        selected_text = self.selected_text\n        start_char_idx = self.start_char_idx\n\n        text = str(text)\n        sentiment = str(sentiment)\n        selected_text = str(selected_text)\n        \n        # Find end character index of answer in context\n        end_char_idx = start_char_idx + len(str(selected_text))\n        \n                \n        if end_char_idx > len(text):\n            self.skip = True\n            return\n        \n        \n        # Mark the character indexes in context that are in answer\n        is_char_in_ans = [0] * len(text)\n        for idx in range(start_char_idx, end_char_idx):\n            is_char_in_ans[idx] = 1\n            \n        # Tokenize context\n        tokenized_context = tokenizer.encode(text)\n\n        # Find tokens that were created from answer characters\n        ans_token_idx = []\n        for idx, (start, end) in enumerate(tokenized_context.offsets):\n            if sum(is_char_in_ans[start:end]) > 0:\n                ans_token_idx.append(idx)\n        \n        if len(ans_token_idx) == 0:\n            self.skip = True\n            return\n        \n        \n        # Find start and end token index for tokens from answer\n        start_token_idx = ans_token_idx[0]\n        end_token_idx = ans_token_idx[-1]\n\n        # Tokenize question\n        tokenized_question = tokenizer.encode(sentiment)\n\n        # Create inputs\n        input_ids = tokenized_context.ids + tokenized_question.ids[1:]\n        token_type_ids = [0] * len(tokenized_context.ids) + [1] * len(\n            tokenized_question.ids[1:]\n        )\n        attention_mask = [1] * len(input_ids)\n\n        # Pad and create attention masks.\n        # Skip if truncation is needed\n        padding_length = max_len - len(input_ids)\n        if padding_length > 0:  # pad\n            input_ids = input_ids + ([0] * padding_length)\n            attention_mask = attention_mask + ([0] * padding_length)\n            token_type_ids = token_type_ids + ([0] * padding_length)\n        elif padding_length < 0:  # skip\n            self.skip = True\n            return\n\n\n        self.input_ids = input_ids\n        self.token_type_ids = token_type_ids\n        self.attention_mask = attention_mask\n        self.start_token_idx = start_token_idx\n        self.end_token_idx = end_token_idx\n        self.context_token_to_char = tokenized_context.offsets\n\ndef create_examples(raw_data):\n    squad_examples = []\n    for i in range(raw_data.shape[0]):\n        text = raw_data[\"text\"][i]\n        sentiment = raw_data[\"sentiment\"][i]\n        selected_text = raw_data[\"selected_text\"][i]\n        start_char_idx = str(raw_data[\"text\"][i]).find(str(raw_data[\"selected_text\"][i]))\n        squad_eg = SquadExample(text, sentiment, selected_text, start_char_idx)\n        squad_eg.preprocess()\n        squad_examples.append(squad_eg)\n        \n    return squad_examples\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_inputs_targets(squad_examples):\n    dataset_dict = {\n        \"input_ids\": [],\n        \"token_type_ids\": [],\n        \"attention_mask\": [],\n        \"start_token_idx\": [],\n        \"end_token_idx\": [],\n    }\n    for item in squad_examples:\n        if item.skip == False:\n            for key in dataset_dict:\n                dataset_dict[key].append(getattr(item, key))\n    for key in dataset_dict:\n        dataset_dict[key] = np.array(dataset_dict[key])\n\n    x = [\n        dataset_dict[\"input_ids\"],\n        dataset_dict[\"token_type_ids\"],\n        dataset_dict[\"attention_mask\"],\n    ]\n    y = [dataset_dict[\"start_token_idx\"], dataset_dict[\"end_token_idx\"]]\n    return x, y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SquadTest:\n    def __init__(self, text, sentiment):\n        self.text = text\n        self.sentiment = sentiment\n        \n    def preprocess(self):        \n        text = self.text\n        sentiment = self.sentiment\n\n        text = str(text)\n        sentiment = str(sentiment)\n                  \n        # Tokenize context\n        tokenized_context = tokenizer.encode(text)       \n        \n        # Tokenize question\n        tokenized_question = tokenizer.encode(sentiment)\n\n        # Create inputs\n        input_ids = tokenized_context.ids + tokenized_question.ids[1:]\n        token_type_ids = [0] * len(tokenized_context.ids) + [1] * len(\n            tokenized_question.ids[1:]\n        )\n        attention_mask = [1] * len(input_ids)\n\n        # Pad and create attention masks.\n        # Skip if truncation is needed\n        padding_length = max_len - len(input_ids)\n        if padding_length > 0:  # pad\n            input_ids = input_ids + ([0] * padding_length)\n            attention_mask = attention_mask + ([0] * padding_length)\n            token_type_ids = token_type_ids + ([0] * padding_length)\n        elif padding_length < 0:  # skip\n            self.skip = True\n            return\n\n\n        self.input_ids = input_ids\n        self.token_type_ids = token_type_ids\n        self.attention_mask = attention_mask\n\ndef create_test(raw_data):\n    squad_examples = []\n    for i in range(raw_data.shape[0]):\n        text = raw_data[\"text\"][i]\n        sentiment = raw_data[\"sentiment\"][i] \n        squad_eg = SquadTest(text, sentiment)\n            \n        squad_eg.preprocess()\n        squad_examples.append(squad_eg)\n        \n    return squad_examples\n\ndef create_inputs_test(squad_examples):\n    dataset_dict = {\n        \"input_ids\": [],\n        \"token_type_ids\": [],\n        \"attention_mask\": [],\n    }\n    for item in squad_examples:\n        for key in dataset_dict:\n            dataset_dict[key].append(getattr(item, key))\n    for key in dataset_dict:\n        dataset_dict[key] = np.array(dataset_dict[key])\n\n    x = [\n        dataset_dict[\"input_ids\"],\n        dataset_dict[\"token_type_ids\"],\n        dataset_dict[\"attention_mask\"],\n    ]\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_examples = create_examples(train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"{len(train_examples)} training points created.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, y_train = create_inputs_targets(train_examples)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(y_train[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"base_path = '/kaggle/input/bert-tensorflow/bert-base-uncased-tf_model.h5'\n\ndef create_model(path = base_path):\n    ## BERT encoder\n    encoder = TFBertModel.from_pretrained(path, config=configuration)\n\n    ## QA Model\n    input_ids = layers.Input(shape=(max_len,), dtype=tf.int32)\n    token_type_ids = layers.Input(shape=(max_len,), dtype=tf.int32)\n    attention_mask = layers.Input(shape=(max_len,), dtype=tf.int32)\n    embedding = encoder(\n        input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask\n    )[0]\n\n    start_logits = layers.Dense(1, name=\"start_logit\", use_bias=False)(embedding)\n    start_logits = layers.Flatten()(start_logits)\n\n    end_logits = layers.Dense(1, name=\"end_logit\", use_bias=False)(embedding)\n    end_logits = layers.Flatten()(end_logits)\n\n    start_probs = layers.Activation(keras.activations.softmax)(start_logits)\n    end_probs = layers.Activation(keras.activations.softmax)(end_logits)\n\n    model = keras.Model(\n        inputs=[input_ids, token_type_ids, attention_mask],\n        outputs=[start_probs, end_probs],\n    )\n    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n    optimizer = keras.optimizers.Adam(lr=5e-5)\n    model.compile(optimizer=optimizer, loss=[loss, loss])\n    return model\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nuse_tpu = True\nif use_tpu:\n    # Create distribution strategy\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\n    # Create model\n    with strategy.scope():\n        model = create_model(base_path)\nelse:\n    model = create_model(base_path)\n\nmodel.summary()\n\"\"\"\nmodel = create_model(base_path)\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#save_locally = tf.saved_model.SaveOptions(experimental_io_device='/job:localhost')\n#model.save('./model', options=save_locally) # saving in Tensorflow's \"saved model\" format","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(\n    x_train,\n    y_train,\n    epochs=5, \n    verbose=2,\n    batch_size=16\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def ans_convert(start_scores, end_scores, input_ids, tokenizer):\n     # ======== Reconstruct Answer ========\n    # Find the tokens with the highest `start` and `end` scores.\n    answer_start = np.argmax(start_scores)\n    answer_end = np.argmax(end_scores)\n\n    # Get the string versions of the input tokens.\n    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n\n    # Start with the first token.\n    answer = tokens[answer_start]\n\n    # Select the remaining answer tokens and join them with whitespace.\n    for i in range(answer_start + 1, answer_end + 1):\n\n        # If it's a subword token, then recombine it with the previous token.\n        if tokens[i][0:2] == '##':\n            answer += tokens[i][2:]\n\n        # Otherwise, add a space then the token.\n        else:\n            answer += ' ' + tokens[i]\n            \n    #print(str(answer_start), \" - \", str(answer_end))\n    return answer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df['text'] = test_df['text'].str.strip()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_examples = create_test(test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test = create_inputs_test(test_examples)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred2 = model.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ans = [ans_convert(pred2[0][i],pred2[1][i],x_test[0][i],slow_tokenizer) for i in range(len(x_test[0]))] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_sub = test_df.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_sub['selected_text'] = ans","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_sub = test_sub[['textID','selected_text']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_sub.to_csv('submission.csv',index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_sub.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save_weights('bert_model_weights.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}