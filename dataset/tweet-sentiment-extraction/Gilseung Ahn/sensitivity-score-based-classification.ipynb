{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Preparation\nImporting module and data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"scrolled":true},"cell_type":"code","source":"import numpy as np\nimport nltk\nfrom nltk.tokenize import word_tokenize\nimport pandas as pd\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nimport re\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import *","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/tweet-sentiment-extraction/train.csv\")\ntest_df = pd.read_csv(\"../input/tweet-sentiment-extraction/test.csv\")\nsubmission_df = pd.read_csv(\"../input/tweet-sentiment-extraction/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"train_df['text'] = train_df['text'].str.lower()\ntest_df['text'] = test_df['text'].str.lower()\ntrain_df['selected_text'] = train_df['selected_text'].str.lower()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))\n\ndef evaluation(actual_list, pred_list):\n    score = 0\n    for (actual, pred) in zip(actual_list, pred_list):\n        score += jaccard(actual, pred)\n    return score / len(pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"lm = WordNetLemmatizer()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Problem Definition\nProblem is to find start point s, and end point of a document, where selected document is document[s:e]\n\n\n\n\n"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"def Remove_Special_Char(text):\n    text = re.sub('[-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》]', '', text) \n    return text","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Make sentiment score dictionary"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# remove special characters from selected text \ntrain_df['selected_text'] = train_df['selected_text'].astype(str).apply(Remove_Special_Char)\nsentiment_value_counts = train_df['sentiment'].value_counts()\n\n# initialize sentiment score dictionary\nsentiment_score = {'neutral': dict(), \n                  'positive': dict(),\n                  'negative': dict()}\n\n# generate selected text corpus\nselected_text_corpus = train_df['selected_text'].values.flatten()\nselected_text_corpus = np.array(selected_text_corpus, dtype = str)\nselected_text_corpus = ' '.join(selected_text_corpus)\n\nfor selected_text, sentiment in zip(train_df['selected_text'], train_df['sentiment']):\n    word_list_in_selected_text = word_tokenize(selected_text)    \n    for word in word_list_in_selected_text:\n        lemmatized_word = lm.lemmatize(word)\n        if lemmatized_word in sentiment_score[sentiment].keys():\n            sentiment_score[sentiment][lemmatized_word] += 1\n        else:\n            sentiment_score[sentiment][lemmatized_word] = 1\n                \nfor sentiment in sentiment_score.keys():\n    expected_value = sentiment_value_counts[sentiment] / sum(sentiment_value_counts)\n    for word in sentiment_score[sentiment].keys():\n        word_frequency = sentiment_score['positive'].get(word, 0) + sentiment_score['neutral'].get(word, 0) + sentiment_score['negative'].get(word, 0)\n        actual_value = sentiment_score[sentiment][word] / word_frequency\n        sentiment_score[sentiment][word] = actual_value - expected_value","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"train_df['text'] = train_df['text'].astype(str).apply(Remove_Special_Char)\ntrain_df['selected_text'] = train_df['selected_text'].astype(str).apply(Remove_Special_Char)\n\ntrain_df['tokend_text'] = train_df['text'].apply(word_tokenize)\ntrain_df['tokend_selected_text'] = train_df['selected_text'].apply(word_tokenize)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"def find_neighbor(t, window_size):\n    T = np.arange(0, 100)\n    return np.argsort(np.abs(T - t))[:window_size]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"def generate_dataset(df, window_size):\n    X = []; Y = []\n    for tokend_text, tokend_selected_text, sentiment in zip(df['tokend_text'], df['tokend_selected_text'], df['sentiment']):\n        try:\n            s, e = [(i, i+len(tokend_selected_text)) for i in range(len(tokend_text)) if tokend_text[i:i+len(tokend_selected_text)] == tokend_selected_text][0] #s: start point of tokend_selected_text in tokend_text // e: end point of tokend_selected_text in tokend_text\n        except:\n            s, e = (0, 0)\n        y = [0] * s + [1] * (e-s) + [0] * (len(tokend_text) - e)\n        x = []\n        for word in tokend_text:\n            lemmatized_word = lm.lemmatize(word)\n            x.append(sentiment_score[sentiment].get(word, 0))        \n        \n        x = np.array(x)\n        y = np.array(y)\n        for t in range(len(x)):            \n            neighbor = find_neighbor(t, window_size)\n            try:\n                X.append(x[neighbor])\n                Y.append(y[t])\n            except:\n                pass\n\n    return X, Y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"X, Y = generate_dataset(df = train_df, window_size = 3)\nmodel = SVC(kernel = 'linear').fit(X, Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"def make_prediction(test_df, model, window_size):\n    test_df['text'] = test_df['text'].astype(str).apply(Remove_Special_Char)\n    test_df['tokend_text'] = test_df['text'].apply(word_tokenize)\n    result = []\n    for tokend_text, sentiment in zip(test_df['tokend_text'], test_df['sentiment']):\n        x = []\n        for word in tokend_text:\n            lemmatized_word = lm.lemmatize(word)\n            x.append(sentiment_score[sentiment].get(word, 0))\n\n        X = []\n        x = np.array(x)\n        for t in range(len(x)):            \n            neighbor = find_neighbor(t, window_size)\n            try:\n                X.append(x[neighbor])\n            except:\n                pass        \n        \n        try:\n            pred_Y = model.predict(X)\n            pred_sentence = ''\n            for (word, y) in zip(tokend_text, pred_Y):\n                if y == 1:\n                    pred_sentence += word + ' '            \n            while pred_sentence[-1] == ' ':\n                pred_sentence = pred_sentence[:-1]\n            result.append(pred_sentence)\n        except:\n            result.append('')\n    \n    return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"result = make_prediction(test_df, model, window_size = 3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"submission_df['selected_text'] = result\nsubmission_df.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}