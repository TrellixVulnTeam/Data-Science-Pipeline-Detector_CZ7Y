{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\n*THIS NOTEBOOK IS FOR ACADEMIC PURPOSES ONLY*\n\n**This notebook is for the Natural Language Processing Mid-Term Test 3 (M3) submission on behalf of the following Students :**\n* Vishesh Gupta - J018\n* Hasan Naqvi - J032\n* Atharv Navander - J033\n\n*(Studying in B.Tech Data Science Stream. 3rd Year, Semester 6 - Mukesh Patel School of Technology Management and Engineering, NMIMS)*\n\n# **Approach to Problem Statement**\n1. Conducting Exploratory Data Analysis (EDA) on training data\n2. Creating an algorithm to extract high polarity words using Sentiment Intensity Analyzer\n3. Checking prediction accuracy on Training Data\n4. Predicting words on Texting Data and submitting file"},{"metadata":{},"cell_type":"markdown","source":"# Importing Libraries"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport scipy.io\nfrom array import *\nimport numpy as np\nimport re\nimport nltk\nfrom nltk.corpus import wordnet,stopwords\nfrom nltk.tokenize import word_tokenize\nSTOPWORDS = set(stopwords.words('english'))\nfrom nltk.stem import WordNetLemmatizer\nlem=WordNetLemmatizer()\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom PIL import Image\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set(style=\"whitegrid\")\nimport collections","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Importing Training and Testing Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv')\ntest = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv')\nsample = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Training Data shape is:', train.shape)\nprint('Testing Data shape is:',test.shape)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating Helper Functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Function to get unique list elements\ndef unique_list(l):\n    ulist = []\n    [ulist.append(x) for x in l if x not in ulist]\n    return ulist\n\n#Function to obtain Jaccard Index. It is a statistic used in understanding the similarities between 2 texts.\ndef jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))\n\n#Function to make wordcloud    \ndef MakeCloud(array , title = 'Word Cloud' , w = 16 , h = 13, my_mask='null',my_colormap='null',border_size=0):\n    plt.figure(figsize=(w,h))\n    if my_mask is not 'null':\n        wc = WordCloud(background_color=\"black\",stopwords=STOPWORDS, max_words=10000, \n                       max_font_size= 40, mask = my_mask,contour_width=border_size, contour_color='white')\n    else:\n        wc = WordCloud(background_color=\"black\",stopwords=STOPWORDS, max_words=10000, \n                       max_font_size= 40, contour_width=border_size, contour_color='white')\n    wc.generate(\" \".join(array))\n    if my_colormap is not 'null':\n        plt.imshow(wc.recolor( colormap= my_colormap , random_state=17), alpha=0.98)\n    else:\n        plt.imshow(wc)\n    plt.axis(\"off\")\n    plt.title(title)\n    plt.show()\n\n#Function that tags our tokens for lemmatizing\ndef get_wordnet_pos(word):\n    tag = nltk.pos_tag([word])[0][1][0].upper()\n    tag_dict = {\"J\": wordnet.ADJ,\n                \"N\": wordnet.NOUN,\n                \"V\": wordnet.VERB,\n                \"R\": wordnet.ADV}\n    return tag_dict.get(tag, wordnet.NOUN)\n\n#Function that pipelines and prepares data for Prediction Models\ndef pipeline(text):\n    text = str(text).strip()\n    text = re.sub(r'http\\S+', '', text)\n    stop_free = ' '.join([word for word in text.lower().split() if ((word not in STOPWORDS))])\n    punc_free=re.sub('[^a-zA-Z]', \" \", str(stop_free))\n    text = ' '.join(lem.lemmatize(word, get_wordnet_pos(word)) for word in nltk.word_tokenize(punc_free))\n    return text\n\n#Function to create a bar plot with 2 axis representing 2 kinds of data\ndef BPlot(feature_1,feature_2,dataframe) :\n    sns.barplot(x=feature_1, y=feature_2 , data=dataframe)\n    \n#EDA function to get wordcount of a column in dataset\ndef CommonWords(text ,show = True , kk=10) : \n    all_words = []\n\n    for i in range(text.shape[0]) : \n        this_phrase = list(text)[i]\n        for word in this_phrase.split() : \n            all_words.append(word)\n    common_words = collections.Counter(all_words).most_common()\n    k=0\n    word_list =[]\n    for word, i in common_words : \n        if not word.lower() in  STOPWORDS :\n            if show : \n                print(f'The word    {word}   is repeated   {i}  times')\n            word_list.append(word)\n            k+=1\n        if k==kk : \n            break\n            \n    return word_list","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating our Prediction Algorithm"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Function to obtain words with maximum polarity\ndef choosing_selectedword(df_process):\n    train_data = df_process['text']\n    train_data_sentiment = df_process['sentiment']\n    #This list will contain our predictions for the dataframe\n    selected_text_processed = []\n    #Initializing our Sentiment Analyzer\n    analyser = SentimentIntensityAnalyzer()\n    for j in range(0 , len(train_data)):\n        #Removing hyperlink from tweets\n        text = re.sub(r'http\\S+', '', str(train_data.iloc[j]))\n        #If the sentiment is neutral, we return the text as it is. Because the words labelled neutral will have low polarity\n        if(train_data_sentiment.iloc[j] == \"neutral\" or len(text.split()) < 2):\n            selected_text_processed.append(str(text))\n        if(train_data_sentiment.iloc[j] == \"positive\" and len(text.split()) >= 2):\n            #Tokenizing the text\n            aa = re.split(' ', text)\n            #This string will contain our high polarity words of each text\n            ss_arr = \"\"\n            polar = 0\n            #Looking for high polarity tokens\n            for qa in range(0,len(aa)):\n                score = analyser.polarity_scores(aa[qa])\n                if score['compound'] >polar:\n                    polar = score['compound']\n                    ss_arr = aa[qa]\n            #If we find high polarity words, we return the ss_arr string containing high polarity words, else we return the initial text\n            if len(ss_arr) != 0:\n                selected_text_processed.append(ss_arr)   \n            if len(ss_arr) == 0:\n                selected_text_processed.append(text)\n        if(train_data_sentiment.iloc[j] == \"negative\"and len(text.split()) >= 2):\n            #Tokenizing the text\n            aa = re.split(' ', text)\n            #This string will contain our high polarity words of each text\n            ss_arr = \"\"\n            polar = 0\n            #Looking for high polarity tokens\n            for qa in range(0,len(aa)):\n                score = analyser.polarity_scores(aa[qa])\n                if score['compound'] <polar:\n                    polar = score['compound']\n                    ss_arr = aa[qa]\n            #If we find high polarity words, we return the ss_arr string containing high polarity words, else we return the initial text\n            if len(ss_arr) != 0:\n                selected_text_processed.append(ss_arr)   \n            if len(ss_arr) == 0:\n                selected_text_processed.append(text)  \n    return selected_text_processed","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis (EDA)\nWe will first pre-process (pipeline) words from text column to get text in suitable format for our EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Pipelining\ntrain['pipelined_text']=train['text'].apply(pipeline)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Seeing distribution of rows on the basis of sentiment\ntemp = train.groupby('sentiment').count()['text'].reset_index().sort_values(by='text',ascending=False)\ntemp.style.background_gradient(cmap='Blues')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Seeing graph of rows v/s sentiment\nBPlot(train['sentiment'].value_counts().index , train['sentiment'].value_counts().values,train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Seeing the word frequency in the texts of dataset\nAllCommon = CommonWords(train['pipelined_text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Importing images that will act as masks for our wordclouds\npos = np.array(Image.open('/kaggle/input/tweet-sentiment-extraction-masks/happy.jpg'))\nneu = np.array(Image.open('/kaggle/input/tweet-sentiment-extraction-masks/neutral.jpg'))\nneg = np.array(Image.open('/kaggle/input/tweet-sentiment-extraction-masks/sad.jpg'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating 3 arrays which contain all texts from each author respectively. \n#They are used further for wordcloud represnting each author.\npositive = train[train.sentiment==\"positive\"][\"pipelined_text\"].values\nnegative = train[train.sentiment==\"negative\"][\"pipelined_text\"].values\nneutral = train[train.sentiment==\"neutral\"][\"pipelined_text\"].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating a wordcloud for Positive Sentiment.\nMakeCloud(positive,title=\"WordCloud for Sentiment : Positive\",my_mask=pos,my_colormap='Reds')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating a wordcloud for Neutral Sentiment.\nMakeCloud(neutral,title=\"WordCloud for Sentiment : Neutral\",my_mask=neu,my_colormap='Greys')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating a wordcloud for Negative Sentiment.\nMakeCloud(negative,title=\"WordCloud for Sentiment : Negative\",my_mask=neg,my_colormap='Blues')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Implementing Prediction Algorithm on Training and Testing Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fetching predictions for train data\nselected_text_train = choosing_selectedword(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking accuracy of our Sentiment Analyser on Training Set\ntrain_selected_data = train['selected_text']\naverage = 0;\n#Fetching Jaccard Scores\nfor i in range(0,len(train_selected_data)):\n    ja_s = jaccard(str(selected_text_train[i]),str(train_selected_data[i]))\n    average = ja_s+average\nprint('Training Data accuracey')\nprint(average/len(selected_text_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fetching predictions for test data\nselected_text_test = choosing_selectedword(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating Final Submission File"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Preparing dataframe in the form of sample submissions\ntest_textid = test['textID']\ntext_id_list = []\nfor kk in range(0,len(test_textid)):\n    text_id_list.append(test_textid.iloc[kk])\nfinal_result = pd.DataFrame({'textID':text_id_list,'selected_text':selected_text_test})\nfinal_result.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating submission file from dataframe\nfinal_result.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}