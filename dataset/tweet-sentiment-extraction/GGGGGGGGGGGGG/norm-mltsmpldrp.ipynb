{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import tokenizers\nimport torch\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN = 128\nTRAIN_BATCH_SIZE = 32\nVALID_BATCH_SIZE=32\nEPOCHS=10\nBERT_PATH='bert-base-uncased'\nTRAINING_FILE = '/kaggle/input/tweet-sentiment-extraction/train.csv'\nSAMPLE_FILE ='/kaggle/input/tweet-sentiment-extraction/sample_submission.csv'\nclass config:\n    MAX_LEN = 128\n    TRAIN_BATCH_SIZE = 64\n    VALID_BATCH_SIZE = 16\n    EPOCHS = 5\n    BERT_PATH = \"../input/bert-base-uncased/\"\n    MODEL_PATH = \"model.bin\"\n    #TRAINING_FILE = \"../input/tweet-train-folds/train_folds.csv\"\n    TOKENIZER = tokenizers.BertWordPieceTokenizer(\n        f\"{BERT_PATH}/vocab.txt\", \n        lowercase=True\n    )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\nfrom fastprogress import master_bar, progress_bar\nfrom functools import partial\nfrom fastprogress.fastprogress import format_time\nimport re\nfrom typing import *\ndef param_getter(m): return m.parameters()\ndef listify(o):\n    if o is None : return []\n    if isinstance(o,list): return o\n    if isinstance(o,str): return [o]\n    if isinstance(o,Iterable): return list(o)\n    return [o]\nclass DataBunch():\n    def __init__(self, train_dl, valid_dl, c=None):\n        self.train_dl,self.valid_dl,self.c = train_dl,valid_dl,c\n        \n    @property\n    def train_ds(self): return self.train_dl.dataset\n        \n    @property\n    def valid_ds(self): return self.valid_dl.dataset\n_camel_re1 = re.compile('(.)([A-Z][a-z]+)')\n_camel_re2 = re.compile('([a-z0-9])([A-Z])')\ndef camel2snake(name):\n    s1 = re.sub(_camel_re1, r'\\1_\\2', name)\n    return re.sub(_camel_re2, r'\\1_\\2', s1).lower()\n\nclass Callback():\n    _order=0\n    def set_runner(self, run): self.run=run\n    def __getattr__(self, k): return getattr(self.run, k)\n    \n    @property\n    def name(self):\n        name = re.sub(r'Callback$', '', self.__class__.__name__)\n        return camel2snake(name or 'callback')\n    \n    def __call__(self, cb_name):\n        f = getattr(self, cb_name, None)\n        if f and f(): return True\n        return False\nclass AvgStatsCallback(Callback):\n    def __init__(self, metrics):\n        self.train_stats,self.valid_stats = AvgStats(metrics,True),AvgStats(metrics,False)\n        \n    def begin_epoch(self):\n        self.train_stats.reset()\n        self.valid_stats.reset()\n        \n    def after_loss(self):\n        #if not self.in_train:\n            stats =  self.valid_stats if not self.in_train else self.train_stats\n            with torch.no_grad(): stats.accumulate(self.run)\n    \n    def after_epoch(self):\n        print(self.train_stats)\n        print(self.valid_stats)\n        \nclass Recorder(Callback):\n    def begin_fit(self):\n        self.lrs = [[] for _ in self.opt.param_groups]\n        self.losses = []\n\n    def after_batch(self):\n        if not self.in_train: return\n        for pg,lr in zip(self.opt.param_groups,self.lrs): lr.append(pg['lr'])\n        self.losses.append(self.loss.detach().cpu())        \n\n    def plot_lr  (self, pgid=-1): plt.plot(self.lrs[pgid])\n    def plot_loss(self, skip_last=0): plt.plot(self.losses[:len(self.losses)-skip_last])\n        \n    def plot(self, skip_last=0, pgid=-1):\n        losses = [o.item() for o in self.losses]\n        lrs    = self.lrs[pgid]\n        n = len(losses)-skip_last\n        plt.xscale('log')\n        plt.plot(lrs[:n], losses[:n])\n\nclass ParamScheduler(Callback):\n    _order=1\n    def __init__(self, pname, sched_funcs): self.pname,self.sched_funcs = pname,sched_funcs\n        \n    def begin_fit(self):\n        if not isinstance(self.sched_funcs, (list,tuple)):\n            self.sched_funcs = [self.sched_funcs] * len(self.opt.param_groups)\n\n    def set_param(self):\n        assert len(self.opt.param_groups)==len(self.sched_funcs)\n        for pg,f in zip(self.opt.param_groups,self.sched_funcs):\n            pg[self.pname] = f(self.n_epochs/self.epochs)\n            \n    def begin_batch(self): \n        if self.in_train: self.set_param()\n\n\nclass LR_Find(Callback):\n    _order=1\n    def __init__(self, max_iter=100, min_lr=1e-6, max_lr=10):\n        self.max_iter,self.min_lr,self.max_lr = max_iter,min_lr,max_lr\n        self.best_loss = 1e9\n        \n    def begin_batch(self): \n        if not self.in_train: return\n        pos = self.n_iter/self.max_iter\n        lr = self.min_lr * (self.max_lr/self.min_lr) ** pos\n        for pg in self.opt.param_groups: pg['lr'] = lr\n            \n    def after_step(self):\n        if self.n_iter>=self.max_iter or self.loss>self.best_loss*10:\n            raise CancelTrainException()\n        if self.loss < self.best_loss: self.best_loss = self.loss\nclass TrainEvalCallback(Callback):\n    def begin_fit(self):\n        self.run.n_epochs=0.\n        self.run.n_iter=0\n    \n    def after_batch(self):\n        if not self.in_train: return\n        self.run.n_epochs += 1./self.iters\n        self.run.n_iter   += 1\n        \n    def begin_epoch(self):\n        self.run.n_epochs=self.epoch\n        self.model.train()\n        self.run.in_train=True\n\n    def begin_validate(self):\n        self.model.eval()\n        self.run.in_train=False\n\nclass CancelTrainException(Exception): pass\nclass CancelEpochException(Exception): pass\nclass CancelBatchException(Exception): pass\n\nclass Learner():\n    def __init__(self, model, data, loss_func, optimizer, lr=1e-2, splitter=param_getter,\n                 cbs=None, cb_funcs=None):\n        self.model,self.data,self.loss_func,self.lr,self.splitter = model,data,loss_func,lr,splitter\n        self.in_train,self.logger,self.opt = False,print,optimizer\n        \n        # NB: Things marked \"NEW\" are covered in lesson 12\n        # NEW: avoid need for set_runner\n        self.cbs = []\n        self.add_cb(TrainEvalCallback())\n        self.add_cbs(cbs)\n        self.add_cbs(cbf() for cbf in listify(cb_funcs))\n\n    def add_cbs(self, cbs):\n        for cb in listify(cbs): self.add_cb(cb)\n            \n    def add_cb(self, cb):\n        cb.set_runner(self)\n        setattr(self, cb.name, cb)\n        self.cbs.append(cb)\n\n    def remove_cbs(self, cbs):\n        for cb in listify(cbs): self.cbs.remove(cb)\n            \n    def one_batch(self, i, xb, yb):\n        try:\n            self.iter = i\n            self.xb,self.yb = xb,yb;                        self('begin_batch')\n            self.pred = self.model(self.xb);                self('after_pred')\n            self.loss = self.loss_func(self.pred, self.yb); self('after_loss')\n            if not self.in_train: return\n            self.loss.backward();                           self('after_backward')\n            self.opt.step();                                self('after_step')\n            self.opt.zero_grad()\n        except CancelBatchException:                        self('after_cancel_batch')\n        finally:                                            self('after_batch')\n\n    def all_batches(self):\n        self.iters = len(self.dl)\n        try:\n            for i,(xb,yb) in enumerate(self.dl): self.one_batch(i, xb, yb)\n        except CancelEpochException: self('after_cancel_epoch')\n\n    def do_begin_fit(self, epochs):\n        self.epochs,self.loss = epochs,torch.tensor(0.)\n        self('begin_fit')\n\n    def do_begin_epoch(self, epoch):\n        self.epoch,self.dl = epoch,self.data.train_dl\n        return self('begin_epoch')\n\n    def fit(self, epochs, cbs=None, reset_opt=False):\n        # NEW: pass callbacks to fit() and have them removed when done\n        self.add_cbs(cbs)\n        # NEW: create optimizer on fit(), optionally replacing existing\n        #if reset_opt or not self.opt: self.opt = self.opt_func(self.splitter(self.model), lr=self.lr)\n            \n        try:\n            self.do_begin_fit(epochs)\n            for epoch in range(epochs):\n                if not self.do_begin_epoch(epoch): self.all_batches()\n\n                with torch.no_grad(): \n                    self.dl = self.data.valid_dl\n                    if not self('begin_validate'): self.all_batches()\n                self('after_epoch')\n            \n        except CancelTrainException: self('after_cancel_train')\n        finally:\n            self('after_fit')\n            self.remove_cbs(cbs)\n\n    ALL_CBS = {'begin_batch', 'after_pred', 'after_loss', 'after_backward', 'after_step',\n        'after_cancel_batch', 'after_batch', 'after_cancel_epoch', 'begin_fit',\n        'begin_epoch', 'begin_validate', 'after_epoch',\n        'after_cancel_train', 'after_fit'}\n    \n    def __call__(self, cb_name):\n        res = False\n        assert cb_name in self.ALL_CBS\n        for cb in sorted(self.cbs, key=lambda x: x._order): res = cb(cb_name) and res\n        return res\ndef annealer(f):\n    def _inner(start,end): return partial(f,start,end)\n    return _inner\n@annealer\ndef sched_lin(start,end,pos): return start + pos*(end-start)\n@annealer\ndef sched_cos(start,end,pos):return start+(1+math.cos(math.pi*(1-pos)))*(end-start)/2\n@annealer\ndef sched_no(start,end,pos): return start\n@annealer\ndef sched_exp(start,end,pos): return start*(end/start)**pos\n\ndef cos_1cycle_anneal(start,high,end):\n    return [sched_cos(start,high),sched_cos(high,end)]\ndef combine_scheds(pcts, scheds):\n    assert sum(pcts) == 1.\n    pcts = torch.tensor([0] + listify(pcts))\n    assert torch.all(pcts >= 0)\n    pcts = torch.cumsum(pcts, 0)\n    def _inner(pos):\n        idx = (pos >= pcts).nonzero().max()\n        actual_pos = (pos-pcts[idx]) / (pcts[idx+1]-pcts[idx])\n        return scheds[idx](actual_pos)\n    return _inner\nclass CudaCallback(Callback):\n    def begin_fit(self): self.model.cuda()\n    def begin_batch(self): \n        if type(self.run.xb) is dict:\n            for key in self.run.xb.keys():\n                self.run.xb[key]=self.run.xb[key].cuda()\n        if type(self.run.yb) is dict:\n            for key in self.run.yb.keys():\n                if type(self.run.yb[key]) is not list:\n                    self.run.yb[key]=self.run.yb[key].cuda()\nclass AvgStats():\n    def __init__(self,metrics,in_train): self.metrics,self.in_train = listify(metrics),in_train\n    def  reset(self):\n        self.tot_loss,self.count =0.,0.\n        self.tot_mets = [0.]*len(self.metrics)\n    @property\n    def all_stats(self):return [self.tot_loss.item()]+ self.tot_mets\n    @property\n    def avg_stats(self): return [o/self.count for o in self.all_stats]\n\n    def __repr__(self):\n        if not self.count: return ''\n        return f\"{'train' if self.in_train else 'valid'}: {self.avg_stats}\"\n    def accumulate(self,run):\n        bn = run.xb[list(run.xb.keys())[0]].shape[0]\n        self.tot_loss+=run.loss*bn\n        self.count+=bn\n        if not self.in_train:\n            for i,m in enumerate(self.metrics):\n                self.tot_mets[i]+= m(run.pred,run.yb)*bn\nclass ProgressCallback(Callback):\n    _order=-1\n    def begin_fit(self):\n        self.mbar = master_bar(range(self.epochs))\n        self.mbar.on_iter_begin()\n        self.run.logger = partial(self.mbar.write, table=True)\n        \n    def after_fit(self): self.mbar.on_iter_end()\n    def after_batch(self): self.pb.update(self.iter)\n    def begin_epoch   (self): self.set_pb()\n    def begin_validate(self): self.set_pb()\n    def after_loss(self): self.pb.comment='loss= ' +str(self.loss.item())[:5]+' lr '+str(self.opt.param_groups[0]['lr'])[:9]\n\n    def set_pb(self):\n        self.pb = progress_bar(self.dl, parent=self.mbar)\n        self.mbar.update(self.epoch)\nclass EarlyStopingCallback(Callback):\n    _order=-1\n    def __init__(self,iters=1,path='bert.pth'):\n        self.iters=iters\n    \n        self.bad_metrics =iters \n        self.path=path\n        '''if path: \n            print('>>>>')\n            torch.save(self.model.state_dict(),path) \n            print('END')'''\n    def begin_fit(self):\n        self.best_metric=[0]\n    def after_epoch(self):\n        mean_metric=self.avg_stats.valid_stats.avg_stats[1]\n        if mean_metric>self.best_metric:\n            self.bad_metrics=self.iters\n            self.best_metric=mean_metric\n            if self.path:\n                print('Saving..... ',mean_metric)\n                torch.save(self.run.model.state_dict(),self.path) \n        else:\n            self.bad_metrics-=1\n            if self.bad_metrics==0:\n                self.run.model.load_state_dict(torch.load(self.path))\n                raise CancelTrainException()\n\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"config.TOKENIZER.decode([103])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TweetDataset:\n    def __init__(self,tweet,sentiment,selected_text,MAX_LEN,TOKENIZER):\n        self.tweet = tweet\n        self.sentiment = sentiment\n        self.selected_text = selected_text\n        self.max_len = MAX_LEN\n        self.tokenizer = TOKENIZER\n    def __len__(self):\n        return len(self.tweet)\n    def __getitem__(self,item):\n        tweet = self.tweet[item]\n        selected_text =self.selected_text[item]\n        len_sel_text = len(selected_text)\n        idx0 = -1\n        idx1 = -1\n        for ind in (i for i,e in enumerate(tweet) if e == selected_text[0]):\n            if tweet[ind:ind+len_sel_text] == selected_text:\n                idx0=ind\n                idx1=ind+len_sel_text-1\n                break\n        char_targets = [0]*len(tweet)\n        if idx0 != -1 and idx1 !=-1:\n            for j in range(idx0,idx1+1):\n                    char_targets[j] = 1\n        sentiment_id = {\n        'positive': 3893,\n        'negative': 4997,\n        'neutral': 8699}\n        \n        tok_tweet =self.tokenizer.encode(tweet)\n        tok_tweet_ids = tok_tweet.ids[1:-1]\n        tok_tweet_offsets = tok_tweet.offsets[1:-1]\n        #print(tok_tweet_ids,' IDS')\n        t=True\n        targets = []\n        for j , (offset1,offset2) in enumerate(tok_tweet_offsets):\n            if sum(char_targets[offset1:offset2])>0:\n                targets.append(j)\n        \n        #print(targets,'<<<<<<<')\n        #print(non_zero)\n        token_type_ids = [0]*len(tok_tweet_ids)\n        token_type_ids=[0]*3 + [1]*len(tok_tweet_ids + [102])\n        tok_tweet_ids=[101] + [sentiment_id[self.sentiment[item]]] + [102] + tok_tweet_ids + [102] \n        tweet_offsets = [(0, 0)] * 3 + tok_tweet_offsets + [(0, 0)]\n        mask = [1]*len(tok_tweet_ids)\n        padding_len = self.max_len - len(tok_tweet_ids)\n\n        if padding_len>1:\n            ids = tok_tweet_ids+ [0]*padding_len\n            mask = mask + [0]*padding_len\n            token_type_ids += [0]*(padding_len)\n            tweet_offsets = tweet_offsets + ([(0, 0)] * padding_len)\n        #print(targets[0],targets[-1],res_targets[(targets[0]+3+1):(targets[-1]+3)])\n        #res_targets[(targets[0]+3+1):(targets[-1]+3)]=0\n        #print('------')\n        return {\n            'ids':torch.tensor(ids,dtype=torch.long),                       \n            'mask':torch.tensor(mask,dtype=torch.long),\n            'token_type_ids':torch.tensor(token_type_ids,dtype=torch.long),\n            'padding_len':torch.tensor(padding_len,dtype=torch.long),\n            },{'tweet_offsets':torch.tensor(tweet_offsets,dtype=torch.long),\n            'orig_selected':self.selected_text[item],\n            'sentiment':self.sentiment[item],\n            'orig_tweet': self.tweet[item],\n            'targets_start':torch.tensor(targets[0]+3,dtype=torch.long),#torch.tensor(targets_start,dtype=torch.float),\n            'targets_end':torch.tensor(targets[-1]+3,dtype=torch.long)}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv(TRAINING_FILE).dropna().reset_index(drop=True)\ndset = TweetDataset(tweet = df.text.values,\n                    sentiment = df.sentiment.values,\n                    selected_text = df.selected_text.values,\n                   MAX_LEN=MAX_LEN,\n                   TOKENIZER=config.TOKENIZER)\n#dset[12538]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"skf = StratifiedKFold(n_splits=5)\nids=skf.get_n_splits(df['text'], df['sentiment'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['kfold']=0\nfor ids,(train_index, test_index) in enumerate(skf.split(df['text'], df['sentiment'])):\n    df['kfold'].loc[test_index]=ids","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from transformers import *\nimport torch.nn as nn\nimport transformers\nimport torch\nfrom sklearn import model_selection\nfrom transformers.modeling_bert import BertPreTrainedModel\nimport scipy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class BERTBaseUncased(transformers.BertPreTrainedModel):\n    def __init__(self, conf,drop_sample=5):\n        super(BERTBaseUncased, self).__init__(conf)\n        self.bert = transformers.BertModel.from_pretrained(config.BERT_PATH, config=conf)\n        self.drop_out = nn.Dropout(0.5)\n        self.drop = nn.Dropout(0.2)\n        #self.lstm = nn.LSTM(768,768,bidirectional=True)\n        self.l0 = nn.Linear(768 * 2, 2)\n        torch.nn.init.normal_(self.l0.weight, std=0.02)\n        self.drop_sample=drop_sample\n    \n    def forward(self, x):\n\n        _,out1,out= self.bert(\n            x['ids'],\n            attention_mask=x['mask'],\n            token_type_ids=x['token_type_ids']\n        )\n        out = torch.cat((out1.unsqueeze(1).repeat(1,128,1),out[-1]), dim=-1)\n        #out,_=self.lstm(self.drop(out[-1].permute(1,0,2)),(self.drop(out1.unsqueeze(0).repeat(2,1,1)),self.drop(out1.unsqueeze(0).repeat(2,1,1))))\n        #out=out.permute(1,0,2)\n        #================\n        res_start=[]\n        res_end=[]\n        if self.training:\n            for i in range(self.drop_sample):\n                outt = self.drop_out(out)\n                logits = self.l0(outt)\n\n                start_logits, end_logits = logits.split(1, dim=-1)\n\n                start_logits = start_logits.squeeze(-1)\n                end_logits = end_logits.squeeze(-1)\n                res_start.append(start_logits)\n                res_end.append(end_logits)\n            return res_start, res_end\n        else:\n            outt = self.drop_out(out)\n            logits = self.l0(outt)\n\n            start_logits, end_logits = logits.split(1, dim=-1)\n\n            start_logits = start_logits.squeeze(-1)\n            end_logits = end_logits.squeeze(-1)\n            return start_logits,end_logits","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.randint(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def loss_fn(outputs,targets):\n    loss_=0\n    if type(outputs[0]) is list:\n        for i in range(len(outputs[0])):\n            loss_+= nn.CrossEntropyLoss()(outputs[0][i],targets['targets_start']) + nn.CrossEntropyLoss()(outputs[1][i],targets['targets_end'])\n        loss_=loss_/len(outputs[0])    \n    else:\n        loss_=nn.CrossEntropyLoss()(outputs[0],targets['targets_start']) + nn.CrossEntropyLoss()(outputs[1],targets['targets_end'])\n    return loss_\ndef jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))\ndef calculate_jaccard_score(original_tweet, target_string, sentiment_val, idx_start, idx_end, offsets,\n    verbose=False):\n    if idx_end < idx_start:\n        idx_end = idx_start\n    \n    filtered_output  = \"\"\n    for ix in range(idx_start, idx_end + 1):\n        filtered_output += original_tweet[offsets[ix][0]: offsets[ix][1]]\n        if (ix+1) < len(offsets) and offsets[ix][1] < offsets[ix+1][0]:\n            filtered_output += \" \"\n\n    if sentiment_val == \"neutral\" or len(original_tweet.split()) < 2:\n        filtered_output = original_tweet\n\n    jac = jaccard(target_string, filtered_output)\n    return jac, filtered_output\ndef metric(outputs,target):\n    tweet,selected_tweet,tweet_sentiment,offsets =target['orig_tweet'],target['orig_selected'],target['sentiment'],target['tweet_offsets'].cpu().numpy()    \n    outputs_start,outputs_end = outputs[0].softmax(dim=1).detach().cpu().numpy(),outputs[1].softmax(dim=1).detach().cpu().numpy()\n    jac=[]\n\n    for ind in range(len(tweet)):\n            jac.append(calculate_jaccard_score(tweet[ind], selected_tweet[ind], tweet_sentiment[ind], outputs_start[ind].argmax(),outputs_end[ind].argmax(), offsets[ind])[0])   \n    return np.mean(jac)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import  tqdm_notebook\ndef step(data_loader,model,optimizer,device,scheduler=None,is_train=False,metric=metric):\n    model.train() if is_train else model.eval()\n    losses = []\n    metrics=[]\n    tk0 = tqdm_notebook(data_loader,total=len(data_loader))\n    for i,d in enumerate(tk0):\n        ids = d['ids'].to(device)\n        mask = d['mask'].to(device)\n        token_type_ids =d['token_type_ids'].to(device)\n        targets_start = d['targets_start'].to(device)\n        targets_end   = d['targets_end'].to(device)\n        optimizer.zero_grad()\n        outputs = model(ids,mask,token_type_ids)\n        loss = loss_fn(outputs,[targets_start,targets_end])\n        if is_train:\n            loss.backward()\n            optimizer.step()\n            if scheduler is not None:\n                scheduler.step()\n        else:\n            metrics.append(metric((d['orig_tweet'],d['orig_selected'],d['sentiment'],outputs[0],outputs[1],d['tweet_offsets'])))\n        losses.append(loss.item())  \n        tk0.set_postfix(loss=losses[-1])\n\n    return losses,metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN_BATCH_SIZE=2\nMAX_LEN=128\nEPOCHS = 2\ndfx=pd.read_csv(TRAINING_FILE).dropna().reset_index(drop=True)\nsample =pd.read_csv(SAMPLE_FILE)\ndf_train,df_valid = model_selection.train_test_split(dfx,random_state=42,test_size=0.2)\ndf_train = df_train.reset_index(drop=True)\ndf_valid = df_valid.reset_index(drop=True)\n\ntarget_cols = list(sample.drop('textID',axis=1).columns)\ntrain_targets = df_train[target_cols].values\nvalid_targets = df_valid[target_cols].values\n\ntrain_dataset = TweetDataset(df_train.text.values,df_train.sentiment.values,df_train.selected_text.values,MAX_LEN,config.TOKENIZER)\ntrain_loader = torch.utils.data.DataLoader(train_dataset,batch_size=TRAIN_BATCH_SIZE,shuffle=True)\n\nvalid_dataset = TweetDataset(df_valid.text.values,df_valid.sentiment.values,df_valid.selected_text.values,MAX_LEN,config.TOKENIZER)\nvalid_loader = torch.utils.data.DataLoader(valid_dataset,batch_size=16,shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = DataBunch(train_loader,valid_loader)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_config = transformers.BertConfig.from_pretrained(config.BERT_PATH)\nmodel_config.output_hidden_states = True\nmodel = BERTBaseUncased(conf=model_config)\nparam_optimizer = list(model.named_parameters())\nno_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\noptimizer_parameters = [\n        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n    ]\noptimizer = transformers.AdamW(optimizer_parameters, lr=1e-4/2)#'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sched = combine_scheds([0.3, 0.7], [sched_cos(1e-4/2,1e-4), sched_cos(1e-4, 1e-4/5)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''cbfs = [Recorder,\n        partial(AvgStatsCallback,metric),\n        CudaCallback,\n       ProgressCallback,\n       partial(EarlyStopingCallback,1,'model')]\n        #partial(ParamScheduler, 'lr', sched)]'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''def get_learner(model,opt,data, loss_func,\n                cb_funcs=None):\n    return Learner(model, data, loss_func, opt,cb_funcs=cb_funcs)\n\nlearn = get_learner(model,optimizer,data,loss_fn, cb_funcs=cbfs)'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#learn.fit(4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.kfold.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef get_learner(model,opt,data, loss_func,\n                cb_funcs=None):\n    return Learner(model, data, loss_func, opt,cb_funcs=cb_funcs)\nfrom tqdm import tqdm_notebook\ndef run_fold(k):\n    TRAIN_BATCH_SIZE=32\n    MAX_LEN=128\n    \n    sample =pd.read_csv(SAMPLE_FILE)\n    df_train,df_valid = df[df.kfold!=k],df[df.kfold==k]\n    df_train = df_train.reset_index(drop=True)\n    df_valid = df_valid.reset_index(drop=True)\n\n    target_cols = list(sample.drop('textID',axis=1).columns)\n    train_targets = df_train[target_cols].values\n    valid_targets = df_valid[target_cols].values\n\n    #tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n    train_dataset = TweetDataset(df_train.text.values,df_train.sentiment.values,df_train.selected_text.values,MAX_LEN,config.TOKENIZER)\n    train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=TRAIN_BATCH_SIZE,shuffle=True)\n    valid_dataset = TweetDataset(df_valid.text.values,df_valid.sentiment.values,df_valid.selected_text.values,MAX_LEN,config.TOKENIZER)\n    valid_loader = torch.utils.data.DataLoader(valid_dataset,batch_size=16,shuffle=False)\n    data = DataBunch(train_loader,valid_loader)\n    \n    model_config = transformers.BertConfig.from_pretrained(config.BERT_PATH)\n    model_config.output_hidden_states = True\n    model = BERTBaseUncased(conf=model_config)\n    params = list(model.named_parameters())\n    def is_backbone(n):\n        return \"bert\" in n\n    lrr=2e-5\n    optimizer_grouped_parameters = [\n            {\"params\": [p for n, p in params if is_backbone(n)], \"lr\": lrr},\n            {\"params\": [p for n, p in params if not is_backbone(n)], \"lr\":lrr * 100},\n        ]\n\n    optimizer = torch.optim.AdamW(\n            optimizer_grouped_parameters, lr=lrr, weight_decay=0\n        )\n    #cbfs = [Recorder,partial(AvgStatsCallback,metric),CudaCallback,ProgressCallback,partial(EarlyStopingCallback,1)]\n    #sched = combine_scheds([0.3, 0.7], [sched_cos(1e-4/2,1e-4), sched_cos(1e-4, 1e-4/3)])\n    cbfs = [Recorder,\n        partial(AvgStatsCallback,metric),\n        CudaCallback,\n        ProgressCallback,\n        #partial(ParamScheduler, 'lr', sched),\n        partial(EarlyStopingCallback,1)]\n    learn = get_learner(model,optimizer,data,loss_fn, cb_funcs=cbfs)\n    \n    learn.fit(10)    \n    test = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv')\n    test['selected_text']=test['text'].apply(lambda x:x)\n    dset = TweetDataset(tweet = test.text.values,\n                    sentiment = test.sentiment.values,\n                    selected_text = test.selected_text.values,\n                   MAX_LEN=MAX_LEN,\n                   TOKENIZER=config.TOKENIZER)\n    res_start=[]\n    res_end=[]\n    bs=16\n    for x,y in tqdm_notebook(torch.utils.data.DataLoader(dset,batch_size=bs,shuffle=False)):\n        for key in x.keys():\n            x[key]=x[key].cuda()\n        output = learn.model(x)\n        res_start.append(output[0].detach().cpu().numpy())\n        res_end.append(output[1].detach().cpu().numpy())\n    return np.concatenate(res_start),np.concatenate(res_end)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start1,end1=run_fold(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start2,end2=run_fold(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start3,end3=run_fold(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start4,end4=run_fold(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = (start1+start2+start3+start4)/4\nend = (end1+end2+end3+end4)/4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for x,y in dset:\n    print(y)\n    break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv')\ntest['selected_text']=test['text'].apply(lambda x:x)\ndset = TweetDataset(tweet = test.text.values,\n                    sentiment = test.sentiment.values,\n                    selected_text = test.selected_text.values,\n                   MAX_LEN=MAX_LEN,\n                   TOKENIZER=config.TOKENIZER)\nfinal_output=[]\ni=0\nfor x,y in dset:\n                ids = x[\"ids\"]\n                token_type_ids = x[\"token_type_ids\"]\n                mask =x[\"mask\"]\n                sentiment = y[\"sentiment\"]\n                orig_selected = y[\"orig_selected\"]\n                orig_tweet = y[\"orig_tweet\"]\n                targets_start = y[\"targets_start\"]\n                targets_end = y[\"targets_end\"]\n                offsets = y[\"tweet_offsets\"].numpy()\n                selected_tweet = orig_selected\n                tweet_sentiment = sentiment\n                _, output_sentence = calculate_jaccard_score(\n                    original_tweet=orig_tweet,\n                    target_string=selected_tweet,\n                    sentiment_val=tweet_sentiment,\n                    idx_start=np.argmax(start[i]),\n                    idx_end=np.argmax(end[i]),\n                    offsets=offsets\n                )\n                final_output.append(output_sentence)\n                i+=1\n                ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['out']=final_output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def post_process(selected):\n    return \" \".join(set(selected.lower().split()))\nsample = pd.read_csv(\"../input/tweet-sentiment-extraction/sample_submission.csv\")\nsample.loc[:, 'selected_text'] = final_output\nsample.selected_text = sample.selected_text.map(post_process)\nsample.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}