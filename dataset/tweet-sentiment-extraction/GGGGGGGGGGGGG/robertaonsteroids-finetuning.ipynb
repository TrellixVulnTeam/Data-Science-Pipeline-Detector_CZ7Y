{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install torchcontrib\nimport numpy as np\nimport pandas as pd\nimport os\nimport warnings\nimport random\nimport torch \nfrom torch import nn\nimport torch.optim as optim\nfrom sklearn.model_selection import StratifiedKFold\nimport tokenizers\nfrom transformers import RobertaModel, RobertaConfig\nfrom torchcontrib.optim import SWA\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed_value):\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    os.environ['PYTHONHASHSEED'] = str(seed_value)\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = True\n\nseed = 42\nseed_everything(seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN = 128\nTRAIN_BATCH_SIZE = 32\nVALID_BATCH_SIZE=32\nEPOCHS=10\nBERT_PATH='bert-base-uncased'\nTRAINING_FILE = '/kaggle/input/tweet-sentiment-extraction/train.csv'\nSAMPLE_FILE ='/kaggle/input/tweet-sentiment-extraction/sample_submission.csv'\nclass config:\n    MAX_LEN = 128\n    TRAIN_BATCH_SIZE = 32\n    VALID_BATCH_SIZE = 16\n    EPOCHS = 5\n    BERT_PATH = \"../input/roberta-base\"\n    #TRAINING_FILE = \"../input/tweet-train-folds/train_folds.csv\"\n    '''TOKENIZER = tokenizers.BertWordPieceTokenizer(\n        f\"{BERT_PATH}/vocab.txt\", \n        lowercase=True\n    )'''\n    TOKENIZER = tokenizers.ByteLevelBPETokenizer(\n    vocab_file=f\"{BERT_PATH}/vocab.json\", \n    merges_file=f\"{BERT_PATH}/merges.txt\", \n    lowercase=True,\n    add_prefix_space=True\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\nfrom fastprogress import master_bar, progress_bar\nfrom functools import partial\nfrom fastprogress.fastprogress import format_time\nimport re\nfrom typing import *\ndef param_getter(m): return m.parameters()\ndef listify(o):\n    if o is None : return []\n    if isinstance(o,list): return o\n    if isinstance(o,str): return [o]\n    if isinstance(o,Iterable): return list(o)\n    return [o]\nclass DataBunch():\n    def __init__(self, train_dl, valid_dl, c=None):\n        self.train_dl,self.valid_dl,self.c = train_dl,valid_dl,c\n        \n    @property\n    def train_ds(self): return self.train_dl.dataset\n        \n    @property\n    def valid_ds(self): return self.valid_dl.dataset\n_camel_re1 = re.compile('(.)([A-Z][a-z]+)')\n_camel_re2 = re.compile('([a-z0-9])([A-Z])')\ndef camel2snake(name):\n    s1 = re.sub(_camel_re1, r'\\1_\\2', name)\n    return re.sub(_camel_re2, r'\\1_\\2', s1).lower()\n\nclass Callback():\n    _order=0\n    def set_runner(self, run): self.run=run\n    def __getattr__(self, k): return getattr(self.run, k)\n    \n    @property\n    def name(self):\n        name = re.sub(r'Callback$', '', self.__class__.__name__)\n        return camel2snake(name or 'callback')\n    \n    def __call__(self, cb_name):\n        f = getattr(self, cb_name, None)\n        if f and f(): return True\n        return False\nclass AvgStatsCallback(Callback):\n    def __init__(self, metrics):\n        self.train_stats,self.valid_stats = AvgStats(metrics,True),AvgStats(metrics,False)\n        \n    def begin_epoch(self):\n        self.train_stats.reset()\n        self.valid_stats.reset()\n        \n    def after_loss(self):\n        #if not self.in_train:\n            stats =  self.valid_stats if not self.in_train else self.train_stats\n            with torch.no_grad(): stats.accumulate(self.run)\n    \n    def after_epoch(self):\n        print(self.train_stats)\n        print(self.valid_stats)\n        \nclass Recorder(Callback):\n    def begin_fit(self):\n        self.lrs = [[] for _ in self.opt.param_groups]\n        self.losses = []\n\n    def after_batch(self):\n        if not self.in_train: return\n        for pg,lr in zip(self.opt.param_groups,self.lrs): lr.append(pg['lr'])\n        self.losses.append(self.loss.detach().cpu())        \n\n    def plot_lr  (self, pgid=-1): plt.plot(self.lrs[pgid])\n    def plot_loss(self, skip_last=0): plt.plot(self.losses[:len(self.losses)-skip_last])\n        \n    def plot(self, skip_last=0, pgid=-1):\n        losses = [o.item() for o in self.losses]\n        lrs    = self.lrs[pgid]\n        n = len(losses)-skip_last\n        plt.xscale('log')\n        plt.plot(lrs[:n], losses[:n])\n\nclass ParamScheduler(Callback):\n    _order=1\n    def __init__(self, pname, sched_funcs): self.pname,self.sched_funcs = pname,sched_funcs\n        \n    def begin_fit(self):\n        if not isinstance(self.sched_funcs, (list,tuple)):\n            self.sched_funcs = [self.sched_funcs] * len(self.opt.param_groups)\n\n    def set_param(self):\n        assert len(self.opt.param_groups)==len(self.sched_funcs)\n        for pg,f in zip(self.opt.param_groups,self.sched_funcs):\n            pg[self.pname] = f(self.n_epochs/self.epochs)\n            \n    def begin_batch(self): \n        if self.in_train: self.set_param()\n\n\nclass LR_Find(Callback):\n    _order=1\n    def __init__(self, max_iter=100, min_lr=1e-6, max_lr=10):\n        self.max_iter,self.min_lr,self.max_lr = max_iter,min_lr,max_lr\n        self.best_loss = 1e9\n        \n    def begin_batch(self): \n        if not self.in_train: return\n        pos = self.n_iter/self.max_iter\n        lr = self.min_lr * (self.max_lr/self.min_lr) ** pos\n        for pg in self.opt.param_groups: pg['lr'] = lr\n            \n    def after_step(self):\n        if self.n_iter>=self.max_iter or self.loss>self.best_loss*10:\n            raise CancelTrainException()\n        if self.loss < self.best_loss: self.best_loss = self.loss\nclass TrainEvalCallback(Callback):\n    def begin_fit(self):\n        self.run.n_epochs=0.\n        self.run.n_iter=0\n    \n    def after_batch(self):\n        if not self.in_train: return\n        self.run.n_epochs += 1./self.iters\n        self.run.n_iter   += 1\n        \n    def begin_epoch(self):\n        self.run.n_epochs=self.epoch\n        self.model.train()\n        self.run.in_train=True\n\n    def begin_validate(self):\n        self.model.eval()\n        self.run.in_train=False\n\nclass CancelTrainException(Exception): pass\nclass CancelEpochException(Exception): pass\nclass CancelBatchException(Exception): pass\n\nclass Learner():\n    def __init__(self, model, data, loss_func, optimizer, lr=1e-2, splitter=param_getter,\n                 cbs=None, cb_funcs=None):\n        self.model,self.data,self.loss_func,self.lr,self.splitter = model,data,loss_func,lr,splitter\n        self.in_train,self.logger,self.opt = False,print,optimizer\n        \n        # NB: Things marked \"NEW\" are covered in lesson 12\n        # NEW: avoid need for set_runner\n        self.cbs = []\n        self.add_cb(TrainEvalCallback())\n        self.add_cbs(cbs)\n        self.add_cbs(cbf() for cbf in listify(cb_funcs))\n\n    def add_cbs(self, cbs):\n        for cb in listify(cbs): self.add_cb(cb)\n            \n    def add_cb(self, cb):\n        cb.set_runner(self)\n        setattr(self, cb.name, cb)\n        self.cbs.append(cb)\n\n    def remove_cbs(self, cbs):\n        for cb in listify(cbs): self.cbs.remove(cb)\n            \n    def one_batch(self, i, xb, yb):\n        try:\n            self.iter = i\n            self.xb,self.yb = xb,yb;                        self('begin_batch')\n            self.pred = self.model(self.xb);                self('after_pred')\n            self.loss = self.loss_func(self.pred, self.yb); self('after_loss')\n            if not self.in_train: return\n            self.loss.backward();                           self('after_backward')\n            self.opt.step();                                self('after_step')\n            self.opt.zero_grad()\n            \n\n        except CancelBatchException:                        self('after_cancel_batch')\n        finally:                                            self('after_batch')\n\n    def all_batches(self):\n        self.iters = len(self.dl)\n        try:\n            for i,(xb,yb) in enumerate(self.dl): self.one_batch(i, xb, yb)\n        except CancelEpochException: self('after_cancel_epoch')\n\n    def do_begin_fit(self, epochs):\n        self.epochs,self.loss = epochs,torch.tensor(0.)\n        self('begin_fit')\n\n    def do_begin_epoch(self, epoch):\n        self.epoch,self.dl = epoch,self.data.train_dl\n        return self('begin_epoch')\n\n    def fit(self, epochs, cbs=None, reset_opt=False):\n        # NEW: pass callbacks to fit() and have them removed when done\n        self.add_cbs(cbs)\n        # NEW: create optimizer on fit(), optionally replacing existing\n        #if reset_opt or not self.opt: self.opt = self.opt_func(self.splitter(self.model), lr=self.lr)\n            \n        try:\n            self.do_begin_fit(epochs)\n            for epoch in range(epochs):\n                if not self.do_begin_epoch(epoch): self.all_batches()\n\n                with torch.no_grad(): \n                    self.dl = self.data.valid_dl\n                    if not self('begin_validate'): self.all_batches()\n                self('after_epoch')\n            \n        except CancelTrainException: self('after_cancel_train')\n        finally:\n            self('after_fit')\n            self.remove_cbs(cbs)\n\n    ALL_CBS = {'begin_batch', 'after_pred', 'after_loss', 'after_backward', 'after_step',\n        'after_cancel_batch', 'after_batch', 'after_cancel_epoch', 'begin_fit',\n        'begin_epoch', 'begin_validate', 'after_epoch',\n        'after_cancel_train', 'after_fit'}\n    \n    def __call__(self, cb_name):\n        res = False\n        assert cb_name in self.ALL_CBS\n        for cb in sorted(self.cbs, key=lambda x: x._order): res = cb(cb_name) and res\n        return res\ndef annealer(f):\n    def _inner(start,end): return partial(f,start,end)\n    return _inner\n@annealer\ndef sched_lin(start,end,pos): return start + pos*(end-start)\n@annealer\ndef sched_cos(start,end,pos):return start+(1+math.cos(math.pi*(1-pos)))*(end-start)/2\n@annealer\ndef sched_no(start,end,pos): return start\n@annealer\ndef sched_exp(start,end,pos): return start*(end/start)**pos\n\ndef cos_1cycle_anneal(start,high,end):\n    return [sched_cos(start,high),sched_cos(high,end)]\ndef combine_scheds(pcts, scheds):\n    assert sum(pcts) == 1.\n    pcts = torch.tensor([0] + listify(pcts))\n    assert torch.all(pcts >= 0)\n    pcts = torch.cumsum(pcts, 0)\n    def _inner(pos):\n        idx = (pos >= pcts).nonzero().max()\n        actual_pos = (pos-pcts[idx]) / (pcts[idx+1]-pcts[idx])\n        return scheds[idx](actual_pos)\n    return _inner\nclass CudaCallback(Callback):\n    def begin_fit(self): self.model.cuda()\n    def begin_batch(self): \n        if type(self.run.xb) is dict:\n            for key in self.run.xb.keys():\n                self.run.xb[key]=self.run.xb[key].cuda()\n        if type(self.run.yb) is dict:\n            for key in self.run.yb.keys():\n                if type(self.run.yb[key]) is not list:\n                    self.run.yb[key]=self.run.yb[key].cuda()\nclass AvgStats():\n    def __init__(self,metrics,in_train): self.metrics,self.in_train = listify(metrics),in_train\n    def  reset(self):\n        self.tot_loss,self.count =0.,0.\n        self.tot_mets = [0.]*len(self.metrics)\n    @property\n    def all_stats(self):return [self.tot_loss.item()]+ self.tot_mets\n    @property\n    def avg_stats(self): return [o/self.count for o in self.all_stats]\n\n    def __repr__(self):\n        if not self.count: return ''\n        return f\"{'train' if self.in_train else 'valid'}: {self.avg_stats}\"\n    def accumulate(self,run):\n        bn = run.xb[list(run.xb.keys())[0]].shape[0]\n        self.tot_loss+=run.loss*bn\n        self.count+=bn\n        if not self.in_train:\n            for i,m in enumerate(self.metrics):\n                self.tot_mets[i]+= m(run.pred,run.yb)*bn\nclass ProgressCallback(Callback):\n    _order=-1\n    def begin_fit(self):\n        self.mbar = master_bar(range(self.epochs))\n        self.mbar.on_iter_begin()\n        self.run.logger = partial(self.mbar.write, table=True)\n        \n    def after_fit(self): self.mbar.on_iter_end()\n    def after_batch(self): self.pb.update(self.iter)\n    def begin_epoch   (self): self.set_pb()\n    def begin_validate(self): self.set_pb()\n    def after_loss(self): self.pb.comment='loss= ' +str(self.loss.item())[:5]+' lr '+str(self.opt.param_groups[0]['lr'])[:9]\n\n    def set_pb(self):\n        self.pb = progress_bar(self.dl, parent=self.mbar)\n        self.mbar.update(self.epoch)\nclass EarlyStopingCallback(Callback):\n    _order=-1\n    def __init__(self,iters=1,path='bert.pth'):\n        self.iters=iters\n    \n        self.bad_metrics =iters \n        self.path=path\n        '''if path: \n            print('>>>>')\n            torch.save(self.model.state_dict(),path) \n            print('END')'''\n    def begin_fit(self):\n        self.best_metric=[0]\n    def after_epoch(self):\n        mean_metric=self.avg_stats.valid_stats.avg_stats[1]\n        if mean_metric>self.best_metric:\n            self.bad_metrics=self.iters\n            self.best_metric=mean_metric\n            if self.path:\n                print('Saving..... ',mean_metric)\n                torch.save(self.run.model.state_dict(),self.path) \n        else:\n            self.bad_metrics-=1\n            if self.bad_metrics==0:\n                self.run.model.load_state_dict(torch.load(self.path))\n                raise CancelTrainException()\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from albumentations.core.transforms_interface import DualTransform, BasicTransform\nclass NLPTransform(BasicTransform):\n    \"\"\" Transform for nlp task.\"\"\"\n    LANGS = {\n        'en': 'english',\n        'it': 'italian', \n        'fr': 'french', \n        'es': 'spanish',\n        'tr': 'turkish', \n        'ru': 'russian',\n        'pt': 'portuguese'\n    }\n\n    @property\n    def targets(self):\n        return {\"data\": self.apply}\n    \n    def update_params(self, params, **kwargs):\n        if hasattr(self, \"interpolation\"):\n            params[\"interpolation\"] = self.interpolation\n        if hasattr(self, \"fill_value\"):\n            params[\"fill_value\"] = self.fill_value\n        return params\n\n    def get_sentences(self, text, lang='en'):\n        return sent_tokenize(text, self.LANGS.get(lang, 'english'))\nclass ExcludeNumbersTransform(NLPTransform):\n    \"\"\" exclude any numbers \"\"\"\n    def __init__(self, always_apply=False, p=0.5):\n        super(ExcludeNumbersTransform, self).__init__(always_apply, p)\n\n    def apply(self, data, **params):\n        text,sent, lang = data\n        text = re.sub(r'[0-9]', '', text)\n        text = re.sub(r'\\s+', ' ', text)\n        \n        sent = re.sub(r'[0-9]', '', sent)\n        sent = re.sub(r'\\s+', ' ', sent)\n        return text,sent, lang\nclass ExcludeHashtagsTransform(NLPTransform):\n    \"\"\" Exclude any hashtags with # \"\"\"\n    def __init__(self, always_apply=False, p=0.5):\n        super(ExcludeHashtagsTransform, self).__init__(always_apply, p)\n\n    def apply(self, data, **params):\n        text,sent, lang = data\n        text = re.sub(r'#[\\S]+\\b', '', text)\n        text = re.sub(r'\\s+', ' ', text)\n        \n        sent = re.sub(r'#[\\S]+\\b', '', sent)\n        sent = re.sub(r'\\s+', ' ', sent)\n        \n        return text,sent, lang\nclass ExcludeUsersMentionedTransform(NLPTransform):\n    \"\"\" Exclude @users \"\"\"\n    def __init__(self, always_apply=False, p=0.5):\n        super(ExcludeUsersMentionedTransform, self).__init__(always_apply, p)\n\n    def apply(self, data, **params):\n        text, sent,lang = data\n        text = re.sub(r'@[\\S]+\\b', '', text)\n        text = re.sub(r'\\s+', ' ', text)\n        \n        sent = re.sub(r'@[\\S]+\\b', '', sent)\n        sent = re.sub(r'\\s+', ' ', sent)\n        \n        return text, sent,lang\nclass ExcludeUrlsTransform(NLPTransform):\n    \"\"\" Exclude urls \"\"\"\n    def __init__(self, always_apply=False, p=0.5):\n        super(ExcludeUrlsTransform, self).__init__(always_apply, p)\n\n    def apply(self, data, **params):\n        text,sent, lang = data\n        text = re.sub(r'https?\\S+', '', text)\n        text = re.sub(r'\\s+', ' ', text)\n        \n        sent = re.sub(r'https?\\S+', '', sent)\n        sent = re.sub(r'\\s+', ' ', sent)\n        return text,sent, lang","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import albumentations\n\ndef get_train_transforms(is_train):\n    if is_train:\n        return albumentations.Compose([\n            ExcludeNumbersTransform(p=0.0),\n            ExcludeHashtagsTransform(p=0.0),\n            ExcludeUsersMentionedTransform(p=0.0),\n            ExcludeUrlsTransform(p=0.0),\n        ])\n    else:\n        return albumentations.Compose([\n        ExcludeNumbersTransform(p=0),\n        ExcludeHashtagsTransform(p=0),\n        ExcludeUsersMentionedTransform(p=0),\n        ExcludeUrlsTransform(p=0),\n    ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.randint(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nclass TweetDataset:\n    def process_data(self,tweet, selected_text, sentiment, tokenizer, max_len):\n        if self.is_train and np.random.rand()>0.2:\n            new_df = self.selected_text[self.sentiment==sentiment]\n            ids=np.random.randint(len(new_df))\n            tweet=tweet[:tweet.index(selected_text)]+' '+new_df[ids]+\" \"+tweet[(tweet.index(selected_text)+len(selected_text)):]\n            selected_text=new_df[ids]\n        tweet = \" \" + \" \".join(str(tweet).split())\n        selected_text = \" \" + \" \".join(str(selected_text).split())\n\n        len_st = len(selected_text) - 1\n        idx0 = None\n        idx1 = None\n        #print(tweet,selected_text)\n        for ind in (i for i, e in enumerate(tweet) if e == selected_text[1]):\n            if \" \" + tweet[ind: ind+len_st] == selected_text:\n                idx0 = ind\n                idx1 = ind + len_st - 1\n                break\n\n        char_targets = [0] * len(tweet)\n        if idx0 != None and idx1 != None:\n            for ct in range(idx0, idx1 + 1):\n                char_targets[ct] = 1\n\n        tok_tweet = tokenizer.encode(tweet)\n        input_ids_orig = tok_tweet.ids\n        tweet_offsets = tok_tweet.offsets\n\n        target_idx = []\n        for j, (offset1, offset2) in enumerate(tweet_offsets):\n            if sum(char_targets[offset1: offset2]) > 0:\n                target_idx.append(j)\n        #print(target_idx)\n\n        targets_start = target_idx[0]\n        targets_end = target_idx[-1]\n\n        sentiment_id = {\n            'positive': 1313,\n            'negative': 2430,\n            'neutral': 7974\n        }\n\n        input_ids = [0] + [sentiment_id[sentiment]] + [2] + [2] + input_ids_orig + [2]\n        token_type_ids = [0, 0, 0, 0] + [0] * (len(input_ids_orig) + 1)\n        mask = [1] * len(token_type_ids)\n        tweet_offsets = [(0, 0)] * 4 + tweet_offsets + [(0, 0)]\n        targets_start += 4\n        targets_end += 4\n\n        padding_length = max_len - len(input_ids)\n        if padding_length > 0:\n            input_ids = input_ids + ([1] * padding_length)\n            mask = mask + ([0] * padding_length)\n            token_type_ids = token_type_ids + ([0] * padding_length)\n            tweet_offsets = tweet_offsets + ([(0, 0)] * padding_length)\n        return {'ids': torch.tensor(input_ids, dtype=torch.long),\n            'mask': torch.tensor(mask, dtype=torch.long),\n            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long)},{'targets_start': torch.tensor(targets_start, dtype=torch.long),\n            'targets_end': torch.tensor(targets_end, dtype=torch.long),\n            'orig_tweet': tweet,\n            'orig_selected': selected_text,\n            'sentiment': sentiment,\n            'tweet_offsets': torch.tensor(tweet_offsets, dtype=torch.long)}\n    def __init__(self,tweet,sentiment,selected_text,MAX_LEN,TOKENIZER,is_train):\n        self.tweet = tweet\n        self.sentiment = sentiment\n        self.selected_text = selected_text\n        self.max_len = MAX_LEN\n        self.tokenizer = TOKENIZER\n        self.is_train=is_train\n        self.transforms=get_train_transforms(is_train)\n    def __len__(self):\n        return len(self.tweet)\n    def __getitem__(self,item):\n        sent = self.tweet[item]\n        selected_text=self.selected_text[item]\n        if self.sentiment[item]!='neutral':\n            sent,selected_text,_=self.transforms(data=(sent,selected_text, 'en'))['data']\n        return  self.process_data(\n            sent, \n            selected_text, \n            self.sentiment[item],\n            self.tokenizer,\n            self.max_len\n        )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv(TRAINING_FILE).dropna().reset_index(drop=True)\n\n\n#-------\ndf.loc[2940]['text']=df.loc[2940]['text'][1:][:2]+\" \"+df.loc[2940]['text'][1:][2:]\ndf.loc[2940]['selected_text']=df.loc[2940]['selected_text'][1:][:2]+\" \"+df.loc[2940]['selected_text'][1:][2:]\ndf.loc[8444]['text']=df.loc[8444]['text'][1:2]+' '+ df.loc[8444]['text'][2:6]+' '+df.loc[8444]['text'][6:8]+' '+df.loc[8444]['text'][8:10]+' '+df.loc[8444]['text'][10:16]+\" \"+df.loc[8444]['text'][16:18]+' '+df.loc[8444]['text'][18:]\ndf.loc[8444]['selected_text']=df.loc[8444]['selected_text'][1:2]+' '+ df.loc[8444]['selected_text'][2:6]+' '+df.loc[8444]['selected_text'][6:8]+' '+df.loc[8444]['selected_text'][8:10]+' '+df.loc[8444]['selected_text'][10:16]+\" \"+df.loc[8444]['selected_text'][16:18]\n\ndf.loc[8718]['text']= df.loc[8718]['text'].replace('#itsucks','it sucks')\ndf.loc[8718]['selected_text']=df.loc[8718]['selected_text'].replace('#itsucks','it sucks')\n\n\n#-----------\ndf.loc[10531]['text']= df.loc[10531]['text'].replace('#fail','fail')\ndf.loc[10531]['selected_text']=df.loc[10531]['selected_text'].replace('#fail','fail')\n\ndf.loc[10576]['text']=df.loc[10576]['text'].replace('#liesboystell','lies boys tell')\ndf.loc[10576]['selected_text']=df.loc[10576]['selected_text'].replace('liesboystell','lies boys tell')\n\ndf.loc[11744]['text']=df.loc[11744]['text'].replace('#shortcakefail','shortcake fail')\ndf.loc[11744]['selected_text']='shortcake fail'\n\ndf.loc[14838]['text'] = df.loc[14838]['text'].replace('was boring but had to eat nonetheless',' was boring but had to eat nonetheless' )\n\ndf.loc[14934]['text']=df.loc[14934]['text'].replace('#sad',' sad')\ndf.loc[14934]['selected_text']='sad'\n\ndf.loc[15565]['text'] = df.loc[15565]['text'][1:]\n\ndf.loc[17299]['text']=df.loc[17299]['text'].replace('#fail',' fail')\ndf.loc[17299]['selected_text']='fail'\n\ndf.loc[17418]['text']=df.loc[17418]['text'].replace('#itsucks',' it sucks')\ndf.loc[17418]['selected_text']='it sucks'\nids=18516\ndf.loc[ids]['text']=df.loc[ids]['text'].replace('#yourock',' you rock')\ndf.loc[ids]['selected_text']='you rock'\nids=18626\ndf.loc[ids]['text']=df.loc[ids]['text'].replace('#HappyMothersDay',' Happy Mothers Day')\ndf.loc[ids]['selected_text']='Happy Mothers Day'\n\nids=21559\ndf.loc[ids]['text']=df.loc[ids]['text'].replace('#FAIL',' fail')\ndf.loc[ids]['selected_text']='fail'\n\nids=21954\ndf.loc[ids]['text']=df.loc[ids]['text'].replace('#FRUSTRADED :@',' FRUSTRADED')\n\nids=22587\ndf.loc[ids]['selected_text']='thank you so much'\n\nids=23672\ndf.loc[ids]['text']=df.loc[ids]['text'].replace('#NO WAYYY I couldn`t `tune in`',' no way i couldn`t tune in')\ndf.loc[ids]['selected_text']=' no way i couldn`t tune in'\n\nids=23741\ndf.loc[ids]['text']=df.loc[ids]['text'].replace('#excited',' excited')\ndf.loc[ids]['selected_text']='excited'\nids=24681\ndf.loc[ids]['selected_text']='Wave looks interesting'\nids=26093\ndf.loc[ids]['text']=df.loc[ids]['text'].replace('#heartbreak',' heartbreak')\ndf.loc[ids]['selected_text']='heart break'\n#========\ndf.loc[7926]['selected_text']=df.loc[7926]['selected_text'][2:] +' you'\n\ndf.loc[166]['selected_text']=df.loc[166]['selected_text'][2:]\ndf.loc[251]['selected_text']=df.loc[251]['selected_text'][2:]\ndf.loc[1986]['selected_text']=df.loc[1986]['selected_text'][2:]\n\ndf.loc[706]['selected_text']=df.loc[706]['selected_text'][2:]\ndf.loc[6112]['selected_text']=df.loc[6112]['selected_text'][6:]\ndf.loc[11678]['selected_text']=df.loc[11678]['selected_text'][2:]\ndf.loc[12038]['selected_text']=df.loc[12038]['selected_text'][2:]\ndf.loc[23923]['selected_text']=df.loc[23923]['selected_text'][2:]\ndf.drop([26686,25690,24681,25690,26686,2269,697,1032],inplace=True)\ndf=df.dropna().reset_index(drop=True)\ndf.drop([6056,21591,23425,23669,26088],inplace=True)\ndf=df.dropna().reset_index(drop=True)\n\ndset = TweetDataset(tweet = df.text.values,\n                    sentiment = df.sentiment.values,\n                    selected_text = df.selected_text.values,\n                   MAX_LEN=MAX_LEN,\n                   TOKENIZER=config.TOKENIZER,\n                   is_train=True)\n#dset[12538]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"skf = StratifiedKFold(n_splits=5)\nids=skf.get_n_splits(df['text'], df['sentiment'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['kfold']=0\nfor ids,(train_index, test_index) in enumerate(skf.split(df['text'], df['sentiment'])):\n    df['kfold'].loc[test_index]=ids","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from transformers import *\nimport torch.nn as nn\nimport transformers\nimport torch\nfrom sklearn import model_selection\nfrom transformers.modeling_bert import BertPreTrainedModel\nimport scipy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TweetModel(nn.Module):\n    def __init__(self):\n        super(TweetModel, self).__init__()\n        \n        config = RobertaConfig.from_pretrained(\n            '../input/roberta-base/config.json', output_hidden_states=True)    \n        self.roberta = RobertaModel.from_pretrained(\n            '../input/roberta-base/pytorch_model.bin', config=config)\n        self.fc = nn.Linear(config.hidden_size, 2)\n        nn.init.normal_(self.fc.weight, std=0.02)\n        nn.init.normal_(self.fc.bias, 0)\n\n    def forward(self, x):\n        _, _, hs = self.roberta(x['ids'], x['mask'])\n         \n        x = torch.stack([hs[-1], hs[-2], hs[-3]])\n        x = torch.mean(x, 0)\n        perm = None\n        '''if self.training:\n            perm=torch.randperm(x.size()[0])\n            y=x[perm,:,:]\n            x = self.fc(0.9*x+0.1*y.detach())\n        else:'''\n        x = self.fc(x)\n        start_logits, end_logits = x.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n                \n        return start_logits, end_logits,perm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\n\n\nclass LabelSmoothSoftmaxCEV1(nn.Module):\n    '''\n    This is the autograd version, you can also try the LabelSmoothSoftmaxCEV2 that uses derived gradients\n    '''\n\n    def __init__(self, lb_smooth=0.1, reduction='mean', ignore_index=-100):\n        super(LabelSmoothSoftmaxCEV1, self).__init__()\n        self.lb_smooth = lb_smooth\n        self.reduction = reduction\n        self.lb_ignore = ignore_index\n        self.log_softmax = nn.LogSoftmax(dim=1)\n\n    def forward(self, logits, label):\n        '''\n        args: logits: tensor of shape (N, C, H, W)\n        args: label: tensor of shape(N, H, W)\n        '''\n        # overcome ignored label\n        logits = logits.float() # use fp32 to avoid nan\n        with torch.no_grad():\n            num_classes = logits.size(1)\n            label = label.clone().detach()\n            ignore = label == self.lb_ignore\n            n_valid = (ignore == 0).sum()\n            label[ignore] = 0\n            lb_pos, lb_neg = 1. - self.lb_smooth, self.lb_smooth / num_classes\n            label = torch.empty_like(logits).fill_(\n                lb_neg).scatter_(1, label.unsqueeze(1), lb_pos).detach()\n\n        logs = self.log_softmax(logits)\n        loss = -torch.sum(logs * label, dim=1)\n        loss[ignore] = 0\n        if self.reduction == 'mean':\n            loss = loss.sum() / n_valid\n        if self.reduction == 'sum':\n            loss = loss.sum()\n\n        return loss\n    \ndef loss_fn(outputs,targets):    \n    loss_=0\n    '''if outputs[2] is not None:\n        loss_+=0.9*(nn.CrossEntropyLoss()(outputs[0],targets['targets_start']) + nn.CrossEntropyLoss()(outputs[1],targets['targets_end']))\n        loss_+=0.1*(nn.CrossEntropyLoss()(outputs[0][outputs[2]],targets['targets_start']) + nn.CrossEntropyLoss()(outputs[1][outputs[2]],targets['targets_end']))\n    else:'''\n    loss_=nn.CrossEntropyLoss()(outputs[0],targets['targets_start']) + nn.CrossEntropyLoss()(outputs[1],targets['targets_end'])\n    return loss_\ndef jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))\ndef calculate_jaccard_score(original_tweet, target_string, sentiment_val, idx_start, idx_end, offsets,\n    verbose=False):\n    if idx_end < idx_start:\n        idx_end = idx_start\n    \n    filtered_output  = \"\"\n    for ix in range(idx_start, idx_end + 1):\n        filtered_output += original_tweet[offsets[ix][0]: offsets[ix][1]]\n        if (ix+1) < len(offsets) and offsets[ix][1] < offsets[ix+1][0]:\n            filtered_output += \" \"\n\n    if sentiment_val == \"neutral\" or len(original_tweet.split()) < 2:\n        filtered_output = original_tweet\n\n    jac = jaccard(target_string, filtered_output)\n    return jac, filtered_output\ndef metric(outputs,target):\n    tweet,selected_tweet,tweet_sentiment,offsets =target['orig_tweet'],target['orig_selected'],target['sentiment'],target['tweet_offsets'].cpu().numpy()    \n    outputs_start,outputs_end = outputs[0].softmax(dim=1).detach().cpu().numpy(),outputs[1].softmax(dim=1).detach().cpu().numpy()\n    jac=[]\n\n    for ind in range(len(tweet)):\n            jac.append(calculate_jaccard_score(tweet[ind], selected_tweet[ind], tweet_sentiment[ind], outputs_start[ind].argmax(),outputs_end[ind].argmax(), offsets[ind])[0])   \n    return np.mean(jac)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import  tqdm_notebook\ndef step(data_loader,model,optimizer,device,scheduler=None,is_train=False,metric=metric):\n    model.train() if is_train else model.eval()\n    losses = []\n    metrics=[]\n    tk0 = tqdm_notebook(data_loader,total=len(data_loader))\n    for i,d in enumerate(tk0):\n        ids = d['ids'].to(device)\n        mask = d['mask'].to(device)\n        token_type_ids =d['token_type_ids'].to(device)\n        targets_start = d['targets_start'].to(device)\n        targets_end   = d['targets_end'].to(device)\n        optimizer.zero_grad()\n        outputs = model(ids,mask,token_type_ids)\n        loss = loss_fn(outputs,[targets_start,targets_end])\n        if is_train:\n            loss.backward()\n            optimizer.step()\n            \n            if scheduler is not None:\n                scheduler.step()\n        else:\n            metrics.append(metric((d['orig_tweet'],d['orig_selected'],d['sentiment'],outputs[0],outputs[1],d['tweet_offsets'])))\n        losses.append(loss.item())  \n        tk0.set_postfix(loss=losses[-1])\n\n    return losses,metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN_BATCH_SIZE=2\nMAX_LEN=128\nEPOCHS = 2\ndfx=pd.read_csv(TRAINING_FILE).dropna().reset_index(drop=True)\nsample =pd.read_csv(SAMPLE_FILE)\ndf_train,df_valid = model_selection.train_test_split(dfx,random_state=42,test_size=0.2)\ndf_train = df_train.reset_index(drop=True)\ndf_valid = df_valid.reset_index(drop=True)\n\ntarget_cols = list(sample.drop('textID',axis=1).columns)\ntrain_targets = df_train[target_cols].values\nvalid_targets = df_valid[target_cols].values\n\ntrain_dataset = TweetDataset(df_train.text.values,df_train.sentiment.values,df_train.selected_text.values,MAX_LEN,config.TOKENIZER,is_train=True)\ntrain_loader = torch.utils.data.DataLoader(train_dataset,batch_size=TRAIN_BATCH_SIZE,shuffle=True)\n\nvalid_dataset = TweetDataset(df_valid.text.values,df_valid.sentiment.values,df_valid.selected_text.values,MAX_LEN,config.TOKENIZER,is_train=False)\nvalid_loader = torch.utils.data.DataLoader(valid_dataset,batch_size=16,shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset[0][0]['ids']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#config.TOKENIZER.decode(list(train_dataset[0][0]['ids'])),list(train_dataset[0][1]['orig_selected'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = DataBunch(train_loader,valid_loader)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for x,y in valid_loader:\n    break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sched = combine_scheds([0.3, 0.7], [sched_cos(1e-4/2,1e-4), sched_cos(1e-4, 1e-4/5)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''cbfs = [Recorder,\n        partial(AvgStatsCallback,metric),\n        CudaCallback,\n       ProgressCallback,\n       partial(EarlyStopingCallback,1,'model')]\n        #partial(ParamScheduler, 'lr', sched)]'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''def get_learner(model,opt,data, loss_func,\n                cb_funcs=None):\n    return Learner(model, data, loss_func, opt,cb_funcs=cb_funcs)\n\nlearn = get_learner(model,optimizer,data,loss_fn, cb_funcs=cbfs)'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#learn.fit(4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.kfold.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef get_learner(model,opt,data, loss_func,\n                cb_funcs=None):\n    return Learner(model, data, loss_func, opt,cb_funcs=cb_funcs)\nfrom tqdm import tqdm_notebook\ndef run_fold(k):\n    TRAIN_BATCH_SIZE=64\n    MAX_LEN=128\n    \n    sample =pd.read_csv(SAMPLE_FILE)\n    df_train,df_valid = df[df.kfold!=k],df[df.kfold==k]\n    df_train=df_train[df_train.sentiment!='neutral']\n    df_train = df_train.reset_index(drop=True)\n    df_valid = df_valid.reset_index(drop=True)\n\n    target_cols = list(sample.drop('textID',axis=1).columns)\n    train_targets = df_train[target_cols].values\n    valid_targets = df_valid[target_cols].values\n\n    #tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n    train_dataset = TweetDataset(df_train.text.values,df_train.sentiment.values,df_train.selected_text.values,MAX_LEN,config.TOKENIZER,is_train=True)\n    train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=TRAIN_BATCH_SIZE,shuffle=True)\n    valid_dataset = TweetDataset(df_valid.text.values,df_valid.sentiment.values,df_valid.selected_text.values,MAX_LEN,config.TOKENIZER,is_train=False)\n    valid_loader = torch.utils.data.DataLoader(valid_dataset,batch_size=16,shuffle=False)\n    data = DataBunch(train_loader,valid_loader)\n    \n    model = TweetModel()\n    params = list(model.named_parameters())\n    optimizer = optim.AdamW(model.parameters(), lr=3e-5, betas=(0.9, 0.999))\n    #cbfs = [Recorder,partial(AvgStatsCallback,metric),CudaCallback,ProgressCallback,partial(EarlyStopingCallback,1)]\n    #sched = combine_scheds([0.3, 0.7], [sched_cos(1e-4/2,1e-4), sched_cos(1e-4, 1e-4/3)])\n    cbfs = [Recorder,\n        partial(AvgStatsCallback,metric),\n        CudaCallback,\n        ProgressCallback,\n        #partial(ParamScheduler, 'lr', sched),\n        partial(EarlyStopingCallback,3,'model_'+str(k))]\n    learn = get_learner(model,optimizer,data,loss_fn, cb_funcs=cbfs)\n    \n    learn.fit(8)    \n    test = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv')\n    test['selected_text']=test['text'].apply(lambda x:x)\n    dset = TweetDataset(tweet = test.text.values,\n                    sentiment = test.sentiment.values,\n                    selected_text = test.selected_text.values,\n                   MAX_LEN=MAX_LEN,\n                   TOKENIZER=config.TOKENIZER,\n                       is_train=False)\n    res_start=[]\n    res_end=[]\n    bs=16\n    for x,y in tqdm_notebook(torch.utils.data.DataLoader(dset,batch_size=bs,shuffle=False)):\n        for key in x.keys():\n            x[key]=x[key].cuda()\n        output = learn.model(x)\n        res_start.append(output[0].detach().cpu().numpy())\n        res_end.append(output[1].detach().cpu().numpy())\n    return np.concatenate(res_start),np.concatenate(res_end)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start1,end1=run_fold(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start2,end2=run_fold(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start3,end3=run_fold(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start4,end4=run_fold(4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = (start1+start2+start3+start4)/4\nend = (end1+end2+end3+end4)/4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv')\ntest['selected_text']=test['text'].apply(lambda x:x)\ndset = TweetDataset(tweet = test.text.values,\n                    sentiment = test.sentiment.values,\n                    selected_text = test.selected_text.values,\n                   MAX_LEN=MAX_LEN,\n                   TOKENIZER=config.TOKENIZER,\n                   is_train=False)\nfinal_output=[]\ni=0\nfor x,y in dset:\n                ids = x[\"ids\"]\n                token_type_ids = x[\"token_type_ids\"]\n                mask =x[\"mask\"]\n                sentiment = y[\"sentiment\"]\n                orig_selected = y[\"orig_selected\"]\n                orig_tweet = y[\"orig_tweet\"]\n                targets_start = y[\"targets_start\"]\n                targets_end = y[\"targets_end\"]\n                offsets = y[\"tweet_offsets\"].numpy()\n                selected_tweet = orig_selected\n                tweet_sentiment = sentiment\n                _, output_sentence = calculate_jaccard_score(\n                    original_tweet=orig_tweet,\n                    target_string=selected_tweet,\n                    sentiment_val=tweet_sentiment,\n                    idx_start=np.argmax(start[i]),\n                    idx_end=np.argmax(end[i]),\n                    offsets=offsets\n                )\n                final_output.append(output_sentence)\n                i+=1\n                ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['out']=final_output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def post_process(selected):\n    return \" \".join(set(selected.lower().split()))\nsample = pd.read_csv(\"../input/tweet-sentiment-extraction/sample_submission.csv\")\nsample.loc[:, 'selected_text'] = final_output\nsample.selected_text = sample.selected_text.map(post_process)\nsample.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}