{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Story:\n\nI don't comment much for kernels but for this kernel I will.\n\nFirst of all I am very thankful to: @akensert, @ajinomoto132 and @adityaecdrid who helped me a lot in finding the mistake I was making. It was due to them that I was able to find the mistake in my training code.\n\n@akensert shared a kernel with data-processing similar to mine but a different model and loss function.\nThis kernel was written using Tensorflow. You can checkout the kernel here: https://www.kaggle.com/akensert/complete-tf2-1-mixed-precision-implementation\n\nPlease upvote @akensert's kernel mentioned above! :)\n\nThus, my plan began to replicate the same score in pytorch. Previously I was using BCEWithLogitsLoss. The TF kernel used Cross Entropy loss. This was one of the major differences. Another major difference was using the last two hidden states instead of just the last one.\n\nSo, I tried and failed.\n\nI then tried again, cleaned up my code, started comparing line-by-line and failed again.\n\nAfter 2 days of frustration, I made a discussion post asking for help: https://www.kaggle.com/c/tweet-sentiment-extraction/discussion/141019\n\nThen came @ajinomoto132. He mentioned he had replicated the model and gladly shared his code to help me out!!! His code can be found here: https://www.kaggle.com/ajinomoto132/starter-kernel-in-pytorch .\n\nPlease upvote @ajinomoto132's kernel mentioned above! :)\n\nAfter a few more hours of struggle, I was able to find the mistake I was doing. It was a stupid mistake of not using `.from_pretrained` when using BertModel. Quite stupid I would say.\n\nSince the community helped me so much, I am releasing the fixed version of my code which is also much cleaner than the previous versions of my code.\n\nI love this community! Thank you for all the help!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# All the important imports","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nPytorch:\n1. 分析数量，dataset\n2. 构建DataLoader\n3. 构建模型BERT\n    3.1 导入预训练的模型 transformers第三方库使用BERT\n    3.2 接BERT后面的模型\n    3.3 构建loss\n    3.4 构建optimizer，EarlyStop\n4. 训练\n5. 测试\n'''\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\n\npd.set_option('display.max_columns', None)\nimport transformers\nfrom transformers import AdamW\nimport tokenizers\nimport torch\nimport torch.nn as nn\n\npath = '../input/tweet-sentiment-extraction/'\ndf_train = pd.read_csv(os.path.join(path, 'train.csv'))\n\nclass EarlyStopping:\n    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n        \"\"\"\n        Args:\n            patience (int): How long to wait after last time validation loss improved.\n                            Default: 7\n            verbose (bool): If True, prints a message for each validation loss improvement.\n                            Default: False\n            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n                            Default: 0\n            path (str): Path for the checkpoint to be saved to.\n                            Default: 'checkpoint.pt'\n            trace_func (function): trace print function.\n                            Default: print\n        \"\"\"\n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.val_loss_min = np.Inf\n        self.delta = delta\n        self.path = path\n        self.trace_func = trace_func\n    def __call__(self, val_loss, model):\n\n        score = -val_loss\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n        elif score < self.best_score + self.delta:\n            self.counter += 1\n            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n            self.counter = 0\n\n    def save_checkpoint(self, val_loss, model):\n        '''Saves model when validation loss decrease.'''\n        if self.verbose:\n            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n        torch.save(model.state_dict(), self.path)\n        self.val_loss_min = val_loss\n\n\nclass config:\n    MAX_LEN = 128\n    TRAIN_BATCH_SIZE = 48\n    EPOCH = 3\n    BERT_PATH = '../input/bert-base-uncased/'\n    MODEL_PATH = 'model.bin'\n    TOKENIZER = tokenizers.BertWordPieceTokenizer(os.path.join(BERT_PATH, 'vocab.txt'), lowercase=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\n1. 定义DataLoader\n'''\n\n\nclass TweetDataset:\n    def __init__(self, tweet, selected_text, sentiment):\n        self.tweet = tweet\n        self.seleted_text = selected_text\n        self.sentiment = sentiment\n        self.tokenizer = config.TOKENIZER\n\n    def __len__(self):\n        return len(self.tweet)\n\n    def __getitem__(self, item):\n        tweet = str(self.tweet[item])\n        selected_text = str(self.seleted_text[item])\n        sentimen = str(self.sentiment[item])\n        '''\n        BERT模型需要的输入数据的格式：\n        1) ids，把text数据转换成index方式。tokenizer\n        2) mask\n        3) token_type_ids\n        '''\n        # 1）selected_text的位置找到，label的值\n        idx0 = None\n        idx1 = None\n        for i, text in enumerate(tweet):\n            if text == selected_text[0] and tweet[i:i + len(selected_text)] == selected_text:\n                idx0 = i\n                idx1 = i + len(selected_text) - 1\n                break\n\n        char_targets = [0] * len(tweet)\n        char_targets[idx0:idx1 + 1] = [1] * len(selected_text)\n\n        tok_tweet = self.tokenizer.encode(tweet)\n        input_ids_orig = tok_tweet.ids[1:-1]\n        tweet_offset = tok_tweet.offsets[1:-1]\n        target_idx = []\n        for j, (offset1, offset2) in enumerate(tweet_offset):\n            if sum(char_targets[offset1:offset2]) > 0:\n                target_idx.append(j)\n        target_start = target_idx[0]\n        target_end = target_idx[-1]\n\n        '''\n        进行ids, mask, token_types数据清洗\n        查看ids方法\n        print(config.TOKENIZER.encode('[CLS] [SEP]').ids)\n        '''\n        sentiment_id = {\n            'positive': 3893,\n            'negative': 4997,\n            'neutral': 8699\n        }\n        input_ids = [101] + [sentiment_id[sentimen]] + [102] + input_ids_orig\n        token_type_ids = [0, 0, 0] + [1] * (len(input_ids) - 3)\n        mask = [1] * len(input_ids)\n        tweet_offset = [(0, 0)] * 3 + tweet_offset\n        target_start += 3\n        target_end += 3\n\n        # padding\n        padding_length = config.MAX_LEN - len(input_ids)\n        if padding_length > 0:\n            input_ids = input_ids + [0] * padding_length\n            token_type_ids = token_type_ids + [0] * padding_length\n            mask = mask + [0] * padding_length\n            tweet_offset = tweet_offset + ([(0, 0)] * padding_length)\n        return {\n            'ids': torch.tensor(input_ids, dtype=torch.long),\n            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n            'mask': torch.tensor(mask, dtype=torch.long),\n            'target_start': torch.tensor(target_start, dtype=torch.long),\n            'target_end': torch.tensor(target_end, dtype=torch.long),\n            'sentiment':sentimen,\n            'tweet': tweet,\n            'selected': selected_text,\n            'tweet_offset': torch.tensor(tweet_offset, dtype=torch.long)\n        }\n\n\n# 测试用例\n\n'''\n构建BERT模型：\ntransformers.BertModel 模型\n1. 导入模型\n    1) 导入配置\n    2）导入模型\n\n2.BERT模型训练参数\nhttps://www.cnblogs.com/deep-deep-learning/p/12792041.html\n    1) ids: word的编码\n    2) mask：指定哪些词用作self-attention\n    3) token_types_ids：区分两个句子的编码\n\n3.BERT模型输出：\nsequence_output, pooled_output, (), ()\n    1) sequence_output: 输出序列，所有word的embedding [batch, length, embedding]\n    2）pooled_output：CLS的embedding输出 [batch, embedding]\n    3) hidden_states(model_hidden_states=True): 13 * [batch, length, embedding] \n    4) attentions\n'''\n\n\nclass Tweet(transformers.BertPreTrainedModel):\n    def __init__(self, conf):\n        super(Tweet, self).__init__(conf)\n        self.bert = transformers.BertModel.from_pretrained(pretrained_model_name_or_path=config.BERT_PATH, config=conf)\n        for param in self.bert.parameters():\n            param.requires_grad = True\n        self.drop_out = nn.Dropout(0.1)\n        self.l0 = nn.Linear(768 * 2, 2)\n\n    def forward(self, ids, mask, token_type_ids):\n        t1, t2, out = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids)\n        out = torch.cat((out[-1], out[-2]), dim=-1)\n        out = self.drop_out(out)\n        logist = self.l0(out)\n        start_logists, end_logists = logist.split(1, dim=-1)\n        start_logists, end_logists = start_logists.squeeze(-1), end_logists.squeeze(-1)\n        # start_logists, end_logists = torch.softmax(start_logists.squeeze(-1), dim=-1), torch.softmax(end_logists.squeeze(-1), dim=-1)\n        return start_logists, end_logists\n\n\ndef loss_fn(start_logists, end_logists, start_positions, end_positions):\n    loss_fct = nn.CrossEntropyLoss()\n    start_loss = loss_fct(start_logists, start_positions)\n    end_loss = loss_fct(end_logists, end_positions)\n    loss = start_loss + end_loss\n    return loss\n\n\n'''\n构建optimizer，EarlyStopping\noptimizer_parameters参数的设计：\n1. 根据name选择不同的参数值\n2. 根据模型选择不同的参数值\n'''\n\n# 定义Early Stopping\n\nes = EarlyStopping(path='checkpoint.pt')\n\nfrom torch.utils.data import DataLoader\n\n# # 1. 构建数据集\n# train_dataloader = DataLoader(TweetDataset(tweet=df_train['text'],sentiment=df_train['sentiment'],selected_text=df_train['selected_text']),batch_size=config.TRAIN_BATCH_SIZE)\n# # 2. 构建模型\n# model_config = transformers.BertConfig.from_pretrained(config.BERT_PATH)\n# model_config.output_hidden_states = True\n# m = Tweet(model_config)\n# '''\n# 测试\n# '''\n# for t1 in train_dataloader:\n#     m(t1['ids'], t1['mask'], t1['token_type_ids'])\n\n'''\n开始训练\n'''\ndevice = torch.device(\"cuda\")\ntrain_dataloader = DataLoader(TweetDataset(tweet=df_train['text'],\n                                           selected_text=df_train['selected_text'],\n                                           sentiment=df_train['sentiment']),\n                              batch_size=config.TRAIN_BATCH_SIZE)\nmodel_config = transformers.BertConfig.from_pretrained(config.BERT_PATH)\nmodel_config.output_hidden_states = True\nmodel = Tweet(conf=model_config).to(device)\n\nparam_optimizer = list(model.named_parameters())\nno_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\noptimizer_parameters = [\n    {'params': [p for n, p in param_optimizer if not any(i in n for i in no_decay)], 'weight_decay': 0.01, 'lr': 3e-5},\n    {'params': [p for n, p in param_optimizer if any(i in n for i in no_decay)], 'weight_decay': 0.00, 'lr': 5e-5}\n]\n# optimizer_parameters = [\n#     {'params':model.bert.parameters(),'weight_decay':0.01,'lr':3e-5},\n#     {'params':model.l0.parameters(),'lr':1e-3}\n# ]\n\noptimizer = AdamW(optimizer_parameters, lr=5e-5)\n# 调整learning rate\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n                                                       factor=0.1,\n                                                       patience=3,\n                                                       eps=1e-8)\n\nfrom tqdm.autonotebook import tqdm\n\n\ndef jaccard(str1, str2):\n    a = set(str1.lower().split())\n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))\n\n\ndef calculate_jaccard_score(tweet, orig_selected, start_logist, end_logist, sentiment, offset):\n    if start_logist > end_logist:\n        start_logist = end_logist\n    logist_selected = tweet[offset[start_logist][0]: offset[end_logist][1]]\n    # print('\\n')\n    # print('offset:', offset)\n    # print('num：s', start_logist, 'num：s', end_logist)\n    # print('text: ', orig_selected, 'text1', logist_selected)\n    if sentiment == 'neutral' or len(orig_selected.split()) < 2:\n        logist_selected = tweet\n    return jaccard(orig_selected, logist_selected)\n\ndef main():\n    for i in range(config.EPOCH):\n        tk0 = tqdm(train_dataloader, total=len(train_dataloader))\n        losses = 0\n        for i, data in enumerate(tk0):\n            start_logists, end_logists = model(data['ids'].to(device), data['mask'].to(device), data['token_type_ids'].to(device))\n            loss = loss_fn(start_logists, end_logists, data['target_start'].to(device), data['target_end'].to(device))\n            losses += loss.item() * data['ids'].shape[0]\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            scheduler.step(loss)\n            '''\n            计算jaccard相似度 \n            '''\n            jaccard_scores = []\n            output_start = torch.argmax(start_logists, dim=-1)\n            output_end = torch.argmax(end_logists, dim=-1)\n            for p_i, tweet in enumerate(data['tweet']):\n                jaccard_scores = calculate_jaccard_score(tweet, data['selected'][p_i], output_start[p_i], output_end[p_i], data['sentiment'][p_i],data['tweet_offset'][p_i])\n            tk0.set_postfix({'loss': loss.item(), 'jaccard_scores': np.mean(jaccard_scores)})\n        losses = losses / len(df_train)\n        scheduler.step(losses)\n        es(losses, model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"main()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\n1. 定义DataLoader\n'''\n\n\nclass TweetTestDataset:\n    def __init__(self, tweet, sentiment):\n        self.tweet = tweet\n        self.sentiment = sentiment\n        self.tokenizer = config.TOKENIZER\n\n    def __len__(self):\n        return len(self.tweet)\n\n    def __getitem__(self, item):\n        tweet = str(self.tweet[item])\n        sentimen = str(self.sentiment[item])\n        tok_tweet = self.tokenizer.encode(tweet)\n        input_ids_orig = tok_tweet.ids[1:-1]\n        tweet_offset = tok_tweet.offsets[1:-1]\n        '''\n        进行ids, mask, token_types数据清洗\n        查看ids方法\n        print(config.TOKENIZER.encode('[CLS] [SEP]').ids)\n        '''\n        sentiment_id = {\n            'positive': 3893,\n            'negative': 4997,\n            'neutral': 8699\n        }\n        input_ids = [101] + [sentiment_id[sentimen]] + [102] + input_ids_orig\n        token_type_ids = [0, 0, 0] + [1] * (len(input_ids) - 3)\n        mask = [1] * len(input_ids)\n        tweet_offset = [(0, 0)] * 3 + tweet_offset\n\n        # padding\n        padding_length = config.MAX_LEN - len(input_ids)\n        if padding_length > 0:\n            input_ids = input_ids + [0] * padding_length\n            token_type_ids = token_type_ids + [0] * padding_length\n            mask = mask + [0] * padding_length\n            tweet_offset = tweet_offset + ([(0, 0)] * padding_length)\n        return {\n            'ids': torch.tensor(input_ids, dtype=torch.long),\n            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n            'mask': torch.tensor(mask, dtype=torch.long),\n            'sentiment':sentimen,\n            'tweet': tweet,\n            'tweet_offset': torch.tensor(tweet_offset, dtype=torch.long)\n        }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv(os.path.join(path, 'test.csv'))\ntrain_dataloader = DataLoader(TweetTestDataset(tweet=df_train['text'],\n                                           sentiment=df_train['sentiment']),\n                              batch_size=config.TRAIN_BATCH_SIZE)\n# model_config = transformers.BertConfig.from_pretrained(config.BERT_PATH)\n# model_config.output_hidden_states = True\n# model = Tweet(conf=model_config).to(device)\n# model.load_state_dict(torch.load('./output/checkpoint.pt'))\n\ntk0 = tqdm(train_dataloader, total=len(train_dataloader))\n\njaccard_scores = []\nlogist_selecteds = []\nmodel.eval()\nfor i, data in enumerate(tk0):\n    start_logists, end_logists = model(data['ids'].to(device), data['mask'].to(device), data['token_type_ids'].to(device))\n    '''\n    计算jaccard相似度\n    '''\n    output_start = torch.argmax(start_logists, dim=-1)\n    output_end = torch.argmax(end_logists, dim=-1)\n    for p_i, tweet in enumerate(data['tweet']):\n        start_logist = output_start[p_i]\n        end_logist = output_end[p_i]\n        offset = data['tweet_offset'][p_i]\n        logist_selected = tweet[offset[start_logist][0]: offset[end_logist][1]]\n        logist_selecteds.append(logist_selected)\n\n\ndf_train['selected_text'] = logist_selecteds\ndf_train[['textID','selected_text']].to_csv('submission.csv',index=False)\npd.set_option('max_colwidth', 60)\ndf_train.sample(25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}