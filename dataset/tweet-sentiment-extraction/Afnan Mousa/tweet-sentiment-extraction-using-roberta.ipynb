{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importing Necesseties","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n#Credit: https://www.kaggle.com/cdeotte/tensorflow-roberta-0-705/notebook\n\n#Goal of this notebook: How to tokenize the data, create question answer targets, and how to build a custom question answer head for RoBERTa\n# in TensorFlow. Note that HuggingFace transformers don't have a TFRobertaForQuestionAnswering so we must make our own from TFRobertaModel.\n\n# Here's a pro tip for people using TPU. Start each fold loop with-\n# tf.tpu.experimental.initialize_tpu_system(tpu)\n# This will prevent the TPU from running out of memory during 5 Fold.\n\n#v5: got .706 score with max_len 192","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-17T21:51:57.173541Z","iopub.execute_input":"2021-09-17T21:51:57.174011Z","iopub.status.idle":"2021-09-17T21:51:57.203354Z","shell.execute_reply.started":"2021-09-17T21:51:57.173968Z","shell.execute_reply":"2021-09-17T21:51:57.202436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nimport string\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport nltk\nfrom nltk.corpus import stopwords\nfrom tqdm import tqdm\nimport spacy\nimport random\nfrom spacy.util import compounding\nfrom spacy.util import minibatch","metadata":{"execution":{"iopub.status.busy":"2021-09-17T21:51:57.205126Z","iopub.execute_input":"2021-09-17T21:51:57.205712Z","iopub.status.idle":"2021-09-17T21:52:00.007494Z","shell.execute_reply.started":"2021-09-17T21:51:57.205674Z","shell.execute_reply":"2021-09-17T21:52:00.006633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom sklearn.model_selection import StratifiedKFold\nfrom transformers import *\nimport tokenizers\nprint('TF version',tf.__version__)","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2021-09-17T21:52:00.010871Z","iopub.execute_input":"2021-09-17T21:52:00.011127Z","iopub.status.idle":"2021-09-17T21:52:05.259116Z","shell.execute_reply.started":"2021-09-17T21:52:00.0111Z","shell.execute_reply":"2021-09-17T21:52:05.25815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Make functions to reading the Data","metadata":{}},{"cell_type":"code","source":"def load_data_of_train():\n    train=pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv')\n    train['text'] = train['text'].astype(str) #ensuring data type is string to avoid any error\n    train['selected_text'] = train['selected_text'].astype(str)\n    return train\n\ndef load_data_of_test():\n    test = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv')\n#     \n    return test\n\ndef load_data_of_submission():\n    sub = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/sample_submission.csv')\n    return sub","metadata":{"execution":{"iopub.status.busy":"2021-09-17T21:52:05.260822Z","iopub.execute_input":"2021-09-17T21:52:05.2614Z","iopub.status.idle":"2021-09-17T21:52:05.268862Z","shell.execute_reply.started":"2021-09-17T21:52:05.261357Z","shell.execute_reply":"2021-09-17T21:52:05.267862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# quick check of the datasets\ntrain_data= load_data_of_train()\ntrain_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-17T21:52:05.273349Z","iopub.execute_input":"2021-09-17T21:52:05.273648Z","iopub.status.idle":"2021-09-17T21:52:05.429446Z","shell.execute_reply.started":"2021-09-17T21:52:05.273622Z","shell.execute_reply":"2021-09-17T21:52:05.428516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data['sentiment'].unique()","metadata":{"execution":{"iopub.status.busy":"2021-09-17T21:52:05.432863Z","iopub.execute_input":"2021-09-17T21:52:05.433223Z","iopub.status.idle":"2021-09-17T21:52:05.443834Z","shell.execute_reply.started":"2021-09-17T21:52:05.433187Z","shell.execute_reply":"2021-09-17T21:52:05.440974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.sentiment.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-09-17T21:52:05.44632Z","iopub.execute_input":"2021-09-17T21:52:05.446689Z","iopub.status.idle":"2021-09-17T21:52:05.462218Z","shell.execute_reply.started":"2021-09-17T21:52:05.446652Z","shell.execute_reply":"2021-09-17T21:52:05.46139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.info()","metadata":{"execution":{"iopub.status.busy":"2021-09-17T21:52:05.463676Z","iopub.execute_input":"2021-09-17T21:52:05.464111Z","iopub.status.idle":"2021-09-17T21:52:05.485849Z","shell.execute_reply.started":"2021-09-17T21:52:05.464073Z","shell.execute_reply":"2021-09-17T21:52:05.484688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.dropna(inplace=True) #The dropna() function is used to remove missing values.","metadata":{"execution":{"iopub.status.busy":"2021-09-17T21:52:05.487488Z","iopub.execute_input":"2021-09-17T21:52:05.488015Z","iopub.status.idle":"2021-09-17T21:52:05.507075Z","shell.execute_reply.started":"2021-09-17T21:52:05.487977Z","shell.execute_reply":"2021-09-17T21:52:05.506333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.info()","metadata":{"execution":{"iopub.status.busy":"2021-09-17T21:52:05.510202Z","iopub.execute_input":"2021-09-17T21:52:05.510471Z","iopub.status.idle":"2021-09-17T21:52:05.52805Z","shell.execute_reply.started":"2021-09-17T21:52:05.510447Z","shell.execute_reply":"2021-09-17T21:52:05.527065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data = load_data_of_test()\nsample_submission_data = load_data_of_submission() ","metadata":{"execution":{"iopub.status.busy":"2021-09-17T21:52:05.529558Z","iopub.execute_input":"2021-09-17T21:52:05.529908Z","iopub.status.idle":"2021-09-17T21:52:05.56644Z","shell.execute_reply.started":"2021-09-17T21:52:05.529872Z","shell.execute_reply":"2021-09-17T21:52:05.565794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.info()","metadata":{"execution":{"iopub.status.busy":"2021-09-17T21:52:05.568133Z","iopub.execute_input":"2021-09-17T21:52:05.568484Z","iopub.status.idle":"2021-09-17T21:52:05.576543Z","shell.execute_reply.started":"2021-09-17T21:52:05.56845Z","shell.execute_reply":"2021-09-17T21:52:05.57569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission_data.info()","metadata":{"execution":{"iopub.status.busy":"2021-09-17T21:52:05.57807Z","iopub.execute_input":"2021-09-17T21:52:05.578707Z","iopub.status.idle":"2021-09-17T21:52:05.592737Z","shell.execute_reply.started":"2021-09-17T21:52:05.578672Z","shell.execute_reply":"2021-09-17T21:52:05.592063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The Size of train&test ","metadata":{}},{"cell_type":"code","source":"print(\"The size of Train is:\",train_data.shape)\nprint(\"The size of Test is:\",test_data.shape)","metadata":{"execution":{"iopub.status.busy":"2021-09-17T21:52:05.596362Z","iopub.execute_input":"2021-09-17T21:52:05.596674Z","iopub.status.idle":"2021-09-17T21:52:05.605963Z","shell.execute_reply.started":"2021-09-17T21:52:05.59665Z","shell.execute_reply":"2021-09-17T21:52:05.605012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def jaccard (text1, text2):\n    a = set(text1.split())\n    b = set(text2.split())\n    intresection = a.intersection(b)\n    IOU =(float) (len(intresection))/(len(a)+len(b)-len(intresection))\n    return IOU","metadata":{"execution":{"iopub.status.busy":"2021-09-17T21:52:05.607571Z","iopub.execute_input":"2021-09-17T21:52:05.608234Z","iopub.status.idle":"2021-09-17T21:52:05.61524Z","shell.execute_reply.started":"2021-09-17T21:52:05.608196Z","shell.execute_reply":"2021-09-17T21:52:05.614538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"jaccard_list=[]\ndef calc_jaccard():\n    for row in train_data.itertuples():\n        jaccard_list.append(jaccard (row.text, row.selected_text))\n    return jaccard_list;","metadata":{"execution":{"iopub.status.busy":"2021-09-17T21:52:05.618154Z","iopub.execute_input":"2021-09-17T21:52:05.618629Z","iopub.status.idle":"2021-09-17T21:52:05.625858Z","shell.execute_reply.started":"2021-09-17T21:52:05.618595Z","shell.execute_reply":"2021-09-17T21:52:05.625069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"jac = calc_jaccard()\ntrain_data['jaccard'] = jac","metadata":{"execution":{"iopub.status.busy":"2021-09-17T21:52:05.628119Z","iopub.execute_input":"2021-09-17T21:52:05.628744Z","iopub.status.idle":"2021-09-17T21:52:05.811918Z","shell.execute_reply.started":"2021-09-17T21:52:05.628708Z","shell.execute_reply":"2021-09-17T21:52:05.811275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data","metadata":{"execution":{"iopub.status.busy":"2021-09-17T21:52:05.813122Z","iopub.execute_input":"2021-09-17T21:52:05.813473Z","iopub.status.idle":"2021-09-17T21:52:05.832648Z","shell.execute_reply.started":"2021-09-17T21:52:05.813439Z","shell.execute_reply":"2021-09-17T21:52:05.831942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,6))\nsns.distplot(train_data[train_data['sentiment']=='neutral']['jaccard'],kde=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-17T21:52:05.83422Z","iopub.execute_input":"2021-09-17T21:52:05.834753Z","iopub.status.idle":"2021-09-17T21:52:06.081416Z","shell.execute_reply.started":"2021-09-17T21:52:05.834715Z","shell.execute_reply":"2021-09-17T21:52:06.080331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,6))\np1=sns.kdeplot(train_data[train_data['sentiment']=='positive']['jaccard'], shade=True, color=\"b\").set_title('Jaccard Scores across different Sentiments')\np2=sns.kdeplot(train_data[train_data['sentiment']=='negative']['jaccard'], shade=True, color=\"r\")\nplt.legend(labels=['positive','negative'])","metadata":{"execution":{"iopub.status.busy":"2021-09-17T21:52:06.082856Z","iopub.execute_input":"2021-09-17T21:52:06.083214Z","iopub.status.idle":"2021-09-17T21:52:06.272834Z","shell.execute_reply.started":"2021-09-17T21:52:06.083177Z","shell.execute_reply":"2021-09-17T21:52:06.272008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data['no_words'] = train_data['text'].apply(lambda x:len(str(x).split()))\nlessThanThree = train_data[train_data['no_words']<=2]","metadata":{"execution":{"iopub.status.busy":"2021-09-17T21:52:06.274275Z","iopub.execute_input":"2021-09-17T21:52:06.27468Z","iopub.status.idle":"2021-09-17T21:52:06.327652Z","shell.execute_reply.started":"2021-09-17T21:52:06.274638Z","shell.execute_reply":"2021-09-17T21:52:06.326918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lessThanThree[lessThanThree['sentiment']=='negative']","metadata":{"execution":{"iopub.status.busy":"2021-09-17T21:52:06.330379Z","iopub.execute_input":"2021-09-17T21:52:06.330674Z","iopub.status.idle":"2021-09-17T21:52:06.350705Z","shell.execute_reply.started":"2021-09-17T21:52:06.330646Z","shell.execute_reply":"2021-09-17T21:52:06.349841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_train=load_data_of_train()\nfinal_test=load_data_of_test()\nfinal_sample_submission=load_data_of_submission()\n\nfinal_train","metadata":{"execution":{"iopub.status.busy":"2021-09-17T21:52:06.352165Z","iopub.execute_input":"2021-09-17T21:52:06.352503Z","iopub.status.idle":"2021-09-17T21:52:06.452486Z","shell.execute_reply.started":"2021-09-17T21:52:06.352474Z","shell.execute_reply":"2021-09-17T21:52:06.451419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_train['no_words'] = final_train['text'].apply(lambda x:len(str(x).split()))\nfinal_train = final_train[final_train['no_words']>=3]","metadata":{"execution":{"iopub.status.busy":"2021-09-17T21:52:06.454496Z","iopub.execute_input":"2021-09-17T21:52:06.455484Z","iopub.status.idle":"2021-09-17T21:52:06.543395Z","shell.execute_reply.started":"2021-09-17T21:52:06.455408Z","shell.execute_reply":"2021-09-17T21:52:06.542358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_train","metadata":{"execution":{"iopub.status.busy":"2021-09-17T21:52:06.5488Z","iopub.execute_input":"2021-09-17T21:52:06.54918Z","iopub.status.idle":"2021-09-17T21:52:06.582169Z","shell.execute_reply.started":"2021-09-17T21:52:06.549141Z","shell.execute_reply":"2021-09-17T21:52:06.581387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_LEN = 96 #try max_len=192 for longer training otherwise use 96\nPATH = ''\ntokenizer = tokenizers.ByteLevelBPETokenizer(\n    vocab_file='../input/tf-roberta/vocab-roberta-base.json', \n    merges_file='../input/tf-roberta/merges-roberta-base.txt', \n    lowercase=True,\n    add_prefix_space=True\n)\n# tokenizer.encode('positive').ids\n# tokenizer.encode('negative').ids\n# tokenizer.encode('neutral').ids\n\n#encoded values of  a particular sentiment\nsentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974} ","metadata":{"execution":{"iopub.status.busy":"2021-09-17T21:52:06.586098Z","iopub.execute_input":"2021-09-17T21:52:06.586698Z","iopub.status.idle":"2021-09-17T21:52:06.783338Z","shell.execute_reply.started":"2021-09-17T21:52:06.586655Z","shell.execute_reply":"2021-09-17T21:52:06.781381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def definitions(Count,flag):\n    d = dict()\n    d['input_ids'] = np.ones((Count,MAX_LEN),dtype='int32')\n    d['attention_mask'] = np.zeros((Count,MAX_LEN),dtype='int32')\n    d['token_type_ids'] = np.zeros((Count,MAX_LEN),dtype='int32')\n    if(flag):\n        d['start_tokens'] = np.zeros((Count,MAX_LEN),dtype='int32')\n        d['end_tokens'] = np.zeros((Count,MAX_LEN),dtype='int32')\n    return d","metadata":{"execution":{"iopub.status.busy":"2021-09-17T21:52:06.785099Z","iopub.execute_input":"2021-09-17T21:52:06.787165Z","iopub.status.idle":"2021-09-17T21:52:06.796753Z","shell.execute_reply.started":"2021-09-17T21:52:06.787118Z","shell.execute_reply":"2021-09-17T21:52:06.795899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# required step to transform data into RoBERTa format\n# print(final_train.shape[0])\ncount_row = final_train.shape[0]\n# 1 for tokens and 0 for padding \ndata_definitions = definitions(count_row, True)\ninput_ids = data_definitions['input_ids']\nattention_mask = data_definitions['attention_mask']\ntoken_type_ids = data_definitions['token_type_ids']\nstart_tokens = data_definitions['start_tokens']\nend_tokens = data_definitions['end_tokens']","metadata":{"execution":{"iopub.status.busy":"2021-09-17T21:52:06.79816Z","iopub.execute_input":"2021-09-17T21:52:06.79869Z","iopub.status.idle":"2021-09-17T21:52:06.811744Z","shell.execute_reply.started":"2021-09-17T21:52:06.798651Z","shell.execute_reply":"2021-09-17T21:52:06.8108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"iterate=0\n# the K represent the index and i represent the data of row\n\nfor k,col in final_train.iterrows():  \n    \n    \n    # FIND OVERLAP\n    text1 = \" \"+\" \".join(col['text'].split())\n    #print(\"text1\",text1)\n    text2 = \" \".join(col['selected_text'].split()) #final_train.loc[k,'selected_text'].split()\n    #print(\"text2\",text2)\n    idx = text1.find(text2)\n    chars = np.zeros((len(text1)))\n    chars[idx:idx+len(text2)]=1\n    if text1[idx-1]==' ': chars[idx-1] = 1 \n    enc = tokenizer.encode(text1) \n        \n    # ID_OFFSETS\n    offsets = []; idx=0\n    for t in enc.ids:\n        w = tokenizer.decode([t])\n        offsets.append((idx,idx+len(w)))\n        idx += len(w)\n    #print(\"offset\",offsets)\n    # START END TOKENS\n    toks = [] #store the index of word which common between text and select_text\n    for i,(a,b) in enumerate(offsets):\n        sm = np.sum(chars[a:b])\n        if sm>0: toks.append(i) \n    #print(\"toks\",toks)\n    \n    s_tok = sentiment_id[final_train.loc[k]['sentiment']] #store the type of sentiment for each row      #final_train.loc[k,'sentiment']\n    #print(\"s_tok\",s_tok)\n    input_ids[iterate][:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n    attention_mask[iterate][:len(enc.ids)+5] = 1\n    if len(toks)>0:\n        start_tokens[iterate][toks[0]+1] = 1\n        end_tokens[iterate][toks[-1]+1] = 1\n    iterate = iterate + 1","metadata":{"execution":{"iopub.status.busy":"2021-09-17T21:52:06.813081Z","iopub.execute_input":"2021-09-17T21:52:06.813654Z","iopub.status.idle":"2021-09-17T21:52:30.380694Z","shell.execute_reply.started":"2021-09-17T21:52:06.813617Z","shell.execute_reply":"2021-09-17T21:52:30.379803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Above code we combine those two column values (text & sentiment) together using [2,2], instead [2, 0] because-\nin HuggingFace tokenizer, RoBERTa tokenization accepts the output like: [0] + ? + [2,2] + ? + [2]","metadata":{}},{"cell_type":"code","source":"# tokenize the test data also as we did above for train data\ncount_row = final_test.shape[0]\n\ndata_definitions = definitions(count_row,False)\ninput_ids_t = data_definitions['input_ids']\ntoken_type_ids_t = data_definitions['token_type_ids']\nattention_mask_t = data_definitions['attention_mask']\n\nfor k,col in final_test.iterrows():\n        \n    # INPUT_IDS\n    text1 = \" \"+\" \".join(col['text'].split()) #test_df.loc[k,'text']\n    enc = tokenizer.encode(text1)                \n    s_tok = sentiment_id[col['sentiment']]\n    #print(\"s_tok\",s_tok)\n    input_ids_t[k][:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n    attention_mask_t[k][:len(enc.ids)+5] = 1","metadata":{"execution":{"iopub.status.busy":"2021-09-17T21:52:30.38219Z","iopub.execute_input":"2021-09-17T21:52:30.382547Z","iopub.status.idle":"2021-09-17T21:52:31.698254Z","shell.execute_reply.started":"2021-09-17T21:52:30.382512Z","shell.execute_reply":"2021-09-17T21:52:31.697343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We use a pretrained roBERTa base model and add a custom question answer head. First tokens are input into bert_model and we use BERT's first output, i.e. x[0] below. These are embeddings of all input tokens and have shape (batch_size, MAX_LEN, 768). Next we apply tf.keras.layers.Conv1D(filters=1, kernel_size=1) and transform the embeddings into shape (batch_size, MAX_LEN, 1). We then flatten this and apply softmax, so our final output from x1 has shape (batch_size, MAX_LEN). These are one hot encodings of the start tokens indicies (for selected_text). And x2 are the end tokens indicies.","metadata":{}},{"cell_type":"code","source":"# build a RoBERTa model\ndef build_model():\n    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n\n    config = RobertaConfig.from_pretrained('../input/tf-roberta/config-roberta-base.json')\n    bert_model = TFRobertaModel.from_pretrained('../input/tf-roberta/pretrained-roberta-base.h5',config=config)\n    x = bert_model(ids,attention_mask=att,token_type_ids=tok)\n    \n    \n    x1 = tf.keras.layers.Dropout(0.1)(x[0]) \n    x1 = tf.keras.layers.Conv1D(128, 2,padding='same')(x1)\n    x1 = tf.keras.layers.LeakyReLU()(x1)\n    x1 = tf.keras.layers.Conv1D(64, 2,padding='same')(x1)\n    x1 = tf.keras.layers.Dense(1)(x1)\n    x1 = tf.keras.layers.Flatten()(x1)\n    x1 = tf.keras.layers.Activation('softmax')(x1)\n    \n    x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n    x2 = tf.keras.layers.Conv1D(128, 2, padding='same')(x2)\n    x2 = tf.keras.layers.LeakyReLU()(x2)\n    x2 = tf.keras.layers.Conv1D(64, 2, padding='same')(x2)\n    x2 = tf.keras.layers.Dense(1)(x2)\n    x2 = tf.keras.layers.Flatten()(x2)\n    x2 = tf.keras.layers.Activation('softmax')(x2)\n\n    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n    model.compile(loss='binary_crossentropy', optimizer=optimizer)\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-09-17T21:52:31.699642Z","iopub.execute_input":"2021-09-17T21:52:31.69999Z","iopub.status.idle":"2021-09-17T21:52:31.71626Z","shell.execute_reply.started":"2021-09-17T21:52:31.699954Z","shell.execute_reply":"2021-09-17T21:52:31.715199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Uncomment below cell if you want to train the model.\nHere we train with 5 Stratified KFolds (based on sentiment stratification). \nEach fold, the best model weights are saved and then reloaded before oof prediction and test prediction. \nTherefore you can run this code offline and upload your 5 fold models to a private Kaggle dataset. \nThen run this notebook and comment out the line model.fit(). \nInstead your notebook will load your model weights from offline training in the line model.load_weights(). \nUpdate this to have the correct path. Also make sure you change the KFold seed below to match your offline training. \nThen this notebook will proceed to use your offline models to predict oof and predict test.","metadata":{}},{"cell_type":"markdown","source":"Use inference in below cell if yo don't want to run above cell.\nOtherwise comment/neglect below cell.\nHere I am using my trained models with max len 192 got from above cell.","metadata":{}},{"cell_type":"code","source":"%%time\nn_splits = 5\npreds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\npreds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\nDISPLAY=1\nfor i in range(5):\n    print('#'*25)\n    print('### MODEL %i'%(i+1))\n    print('#'*25)\n    \n    K.clear_session()\n    model = build_model()\n    model.load_weights('/kaggle/input/model4/v4-roberta-%i.h5'%i)\n#     model.load_weights('/kaggle/input/roberta-trained-model-by-prateekg/v5-roberta-%i.h5'%i)\n\n    print('Predicting Test...')\n    preds = model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n    preds_start += preds[0]/n_splits\n    preds_end += preds[1]/n_splits","metadata":{"execution":{"iopub.status.busy":"2021-09-17T21:52:31.720719Z","iopub.execute_input":"2021-09-17T21:52:31.720975Z","iopub.status.idle":"2021-09-17T21:55:31.771097Z","shell.execute_reply.started":"2021-09-17T21:52:31.72095Z","shell.execute_reply":"2021-09-17T21:55:31.770244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# make submission file\nall = []\nfor k in range(input_ids_t.shape[0]):\n    a = np.argmax(preds_start[k,])\n    b = np.argmax(preds_end[k,])\n    if a>b: \n        st = final_test.loc[k,'text']\n    else:\n        text1 = \" \"+\" \".join(final_test.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode(enc.ids[a-1:b])\n    all.append(st)","metadata":{"execution":{"iopub.status.busy":"2021-09-17T21:55:31.772775Z","iopub.execute_input":"2021-09-17T21:55:31.773122Z","iopub.status.idle":"2021-09-17T21:55:32.325463Z","shell.execute_reply.started":"2021-09-17T21:55:31.773084Z","shell.execute_reply":"2021-09-17T21:55:32.324497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_test['selected_text'] = all\nfinal_test[['textID','selected_text']].to_csv('submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-17T21:55:32.32677Z","iopub.execute_input":"2021-09-17T21:55:32.327108Z","iopub.status.idle":"2021-09-17T21:55:32.519581Z","shell.execute_reply.started":"2021-09-17T21:55:32.327072Z","shell.execute_reply":"2021-09-17T21:55:32.518678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}