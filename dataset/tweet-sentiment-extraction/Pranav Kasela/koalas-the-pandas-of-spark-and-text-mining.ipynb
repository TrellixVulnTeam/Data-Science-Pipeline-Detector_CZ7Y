{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Intro to Koalas for Apache Spark 3.0.0\n<center>\n<table>\n    <tr>\n<td><img src=\"https://raw.githubusercontent.com/databricks/koalas/master/icons/koalas-logo.png\" alt=\"Koalas\" width=\"100\"/></td>\n<td><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/f1/Heart_coraz%C3%B3n.svg/1200px-Heart_coraz%C3%B3n.svg.png\" alt=\"Love\" width=\"100\"/></td>\n<td><img src=\"https://miro.medium.com/max/3494/1*Qw6DOZnKIGXzkxa7iZk2uQ.png\" alt=\"Love\" height=\"300\" width=\"200\"/></td>\n    </tr>\n</table>    \n</center>\n\nApache Spark has not been used as much as other dataframe libraries such as pandas mostly because of it's difficult and different API for the spark dataframe, but it is an powerful instrument for Big Data that each data scientist must know. Koalas is a library created by databricks to improve and make data scientist more productive by mapping Pandas's API on top of Spark, Pandas is, as of today, the most used DataFrame implementation in python.\n\nIn this notebook I will try to introduce Koalas, Pandas-like API for Apache Spark, and I will be using spark 3 (recentely released on 18 June), hitting two objectives with one stone.\nOf course there are some differences and I will try to cover them as well, Koalas is meant to be an introduction for new users, once you are in Spark world you might want to explore their APIs too.\n\n<center><h2 style=\"color:red\"> UPVOTE if you like this kernel! :)</h2></center>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Installing and Importing the libraries\nFirst thing first we need to upgrade the pyspark versione to 3.0.0 and install the koalas library, which is pretty easy, and we don't need to manage the java version problem because spark 3 now is compatible also with Java 11 (the one used by kaggle)!!","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"!java -version","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"!pip install --upgrade --quiet pyspark==3.0.0\n\n!pip install --quiet koalas","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Import the libraries here and in the next cell I configure the Spark Session, as you can see I commented out the conf part, it's becuase I use the default configurations, but wanted to show that you can configure them, all you need to add is `.config(conf=conf)` in the SparkSession object creation. You can change the driver IP from `local[*]` to it's IP, btw `*` indicated to use all available processors it can be changed to a number if you don't want it, for example `local[4]` will use only 4 processors.","execution_count":null},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":false},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \npd.set_option('display.max_colwidth', 20)\n\nimport databricks.koalas as ks\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom pyspark.sql import SQLContext, SparkSession\nfrom pyspark.sql.types import *\nimport pyspark.sql.functions as sqlF\nfrom pyspark import SparkContext, SparkConf","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"code","source":"#conf = SparkConf().setAll([('spark.executor.memory', '5g'), \n#                           ('spark.driver.memory','5g'),\n#                           ('spark.driver.maxResultSize','0')])\n\n\nspark = SparkSession \\\n            .builder.master('local[*]')\\\n            .appName(\"TutorialApp\")\\\n            .getOrCreate()\n\nsqlContext = SQLContext(sparkContext=spark.sparkContext, \n                        sparkSession=spark)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Reading is pretty easy with all three APIs, but you can already start seeing that koalas one is practically identical to the pandas API, while the spark one seems to be a little with more verbose.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"train_pandas = pd.read_csv(\"../input/tweet-sentiment-extraction/train.csv\")\ntrain_koalas = ks.read_csv(\"../input/tweet-sentiment-extraction/train.csv\", escape=\"_\")\n\ntrain_spark  = spark.read.csv(\"../input/tweet-sentiment-extraction/train.csv\",\n                             inferSchema=\"true\", header=\"true\", escape=\"_\")\n\n# another ways of attaching koalas api to spark dataframe\n#train_koalas = train_spark.to_koalas()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#This will be useful to show similarity between pandas and koalas dataframes\nclass display(object):\n    \"\"\"Display HTML representation of multiple objects\"\"\"\n    template = \"\"\"<div style=\"float: left; padding: 10px;\">\n    <p style='font-family:\"Courier New\", Courier, monospace'>{0}</p>{1}\n    </div>\"\"\"\n    def __init__(self, *args):\n        self.args = args\n        \n    def _repr_html_(self):\n        return '\\n'.join(self.template.format(a, eval(a)._repr_html_())\n                         for a in self.args)\n    \n    def __repr__(self):\n        return '\\n\\n'.join(a + '\\n' + repr(eval(a))\n                           for a in self.args)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Basic Operations\n\n## Show/head\n\nHere we see how to print the first 5 rows of the dataframe, you can see that in spark we use `.show(n_rows)` with with koalas and pandas we use the usual `.head(n_rows)`, while the output format is different between spark and the other two, the actual rows can change between koalas and padas when the dataframe is splitted into chunks (it might show any top 5 rows).","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"print(\"train_spark.show(5)\")\ntrain_spark.show(5)\ndisplay(\"train_koalas.head(5)\",\"train_pandas.head(5)\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## dtypes and columns\nThe dtypes for spark is different from the other two, spark doesn't have an object dtype for columns while koalas is very similar to pandas in this case.\nYou can also see the result of `.columns` is different between spark and the other two.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"print(\"Spark API:\")\nprint(train_spark.dtypes)\nprint()\nprint(\"Koalas API:\")\nprint(train_koalas.dtypes)\nprint()\nprint(\"Pandas API:\")\nprint(train_pandas.dtypes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(\"Spark API:\")\nprint(train_spark.columns)\nprint()\nprint(\"Koalas API:\")\nprint(train_koalas.columns)\nprint()\nprint(\"Pandas API:\")\nprint(train_pandas.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## sort_index and sort_values\nLet's see how sorting works, both index and value wise.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"print(\"SPARK doesn't use any index so there is no sort by index!\")\ndisplay(\"train_koalas.sort_index(ascending=False).head(5)\", \"train_pandas.sort_index(ascending=False).head(5)\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(\"train_spark.sort('text', ascending=False).show(5)\")\ntrain_spark.sort(\"text\", ascending=False).show(5)\ndisplay(\"train_koalas.sort_values(by='text',ascending=False).head(5)\", \n        \"train_pandas.sort_values(by='text',ascending=False).head(5)\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## GroupBy\nGroupby has the same syntax in almost all three cases, ","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"print(\"train_spark.groupBy('sentiment').count().orderBy(sqlF.col('count').desc()).show()\")\ntrain_spark.groupBy(\"sentiment\").count()\\\n    .orderBy(sqlF.col(\"count\").desc())\\\n    .show()\ndisplay(\"train_koalas.groupby('sentiment')[['textID']].count().sort_values('textID', ascending=False)\",\n        \"train_pandas.groupby('sentiment')[['textID']].count().sort_values('textID', ascending=False)\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## NaN values\nHere is one big difference, pandas removes the NaN values during the groupBy while spark doesn't. Personally I think it's better to keep the NaNs and remove them on demand. Let's see if there are any NaN and remove them.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"#to drop\nspark_dropna  = train_spark.na.drop()\nkoalas_dropna = train_koalas.dropna()\npandas_dropna = train_pandas.dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#to fill\nspark_fillna  = train_spark.na.fill('missing text')\nkoalas_fillna = train_koalas.fillna('missing text')\npandas_fillna = train_pandas.fillna('missing text')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(\"INITIALLY\")\ntrain_spark.filter(train_spark.text.isNull()).show()\nprint(\"AFTER DROP NA\")\nspark_dropna.filter(spark_dropna.text.isNull()).show()\nprint(\"AFTER FILL NA\")\nspark_fillna.filter(spark_fillna.text == \"missing text\").show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# SQL on Spark\nOne of the greates advantage of spark is that if you know SQL very well you can \"convert\" the dataframe to a tempView in SQL and use SQL queries to perform data transformation etc. we repeat some of the comand above using SQL: ","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# First Create a tempView\ntrain_spark.createOrReplaceTempView(\"train\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"spark.sql(\"SELECT * FROM train\").show(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"spark.sql(\"\"\"\n            SELECT sentiment, count(*) AS total \n            FROM train \n            GROUP BY sentiment \n            ORDER BY total DESC\n          \"\"\").show(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"spark.sql(\"\"\"\n            SELECT * \n            FROM train \n            WHERE text IS NULL\n          \"\"\").show(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Plots\nLet's see how to plot using koalas, it's pretty easy for mostly all kind of plot, you transform the data for the necessary plot and with the `.plot()` using the `kind=type_of_plot` plot what you need.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = train_koalas.groupby('sentiment')['textID'].count()\n\ndata.plot(kind=\"bar\", figsize=(5,4))\nplt.ylabel(\"Frequency\")\nplt.xticks(rotation=45)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If you want to use any other library, you can still to it, assuming the data to be a pandas dataframe or series, just use `.to_numpy()` method to convert the result in numpy array which can be plotted. Overall pretty easy in my opinion :).\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x=data.index.to_numpy(), y=data.to_numpy())\nplt.ylabel(\"Frequency\")\nplt.xlabel(\"Sentiment\")\nplt.xticks(rotation=45)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Some WordCloud\nEvery One wants to do them!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import wordcloud\nimport re\nwords = koalas_dropna.to_spark().rdd.flatMap(lambda x: re.split(\"\\s+\",x[2]))\\\n                  .map(lambda word: (word, 1))\\\n                  .reduceByKey(lambda a, b: a + b)\n\nschema = StructType([StructField(\"words\", StringType(), True),\n                 StructField(\"count\", IntegerType(), True)])\n\nwords_df = sqlContext.createDataFrame(words, schema=schema)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Total number of words:\")\nwords_df.groupBy().sum(\"count\").show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"words_df.groupBy('words')\\\n        .agg(sqlF.mean(\"count\")/195177)\\\n        .orderBy(sqlF.desc(\"(avg(count) / 195177)\"))\\\n        .show(50)\n\nword_cloud = words_df.orderBy(sqlF.desc(\"count\"))\\\n                     .limit(200)\\\n                     .toPandas()\\\n                     .set_index('words')\\\n                     .T\\\n                     .to_dict('records')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wc = wordcloud.WordCloud(background_color=\"white\", max_words=200)\nwc.generate_from_frequencies(dict(*word_cloud))\n\nplt.figure(figsize=(15,10))\nplt.imshow(wc, interpolation='bilinear')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Some Text Mining :)\nIt's pretty simple we will create a very simple model using TF-IDF with unigrams (given the data quantity, if we ahd more data usually a bigram or trigram model works better.) of the data and perform a holdout validation performance check using a LinearSVC with OneVsRest technique for multiclass classification. We will see also how the Pipelines in spark works, we will convert the koalas dataframe to spark dataframe for the training using `df.to_spark()` where df is the koalas dataframe. You can also use other libraries at this point, to convert from koalas or spark to pandas just use `df.toPandas()`, there are way to speed up the latter process, may be will show it some other time.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"from pyspark.ml.feature import HashingTF, IDF, RegexTokenizer, NGram\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import CountVectorizer, StringIndexer\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml.classification import LinearSVC, OneVsRest","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"tokenizer = RegexTokenizer(inputCol=\"selected_text\", outputCol=\"words\", pattern=\"\\\\W\")\nngram = NGram(inputCol=\"words\", outputCol=\"n-gram\").setN(1) #Unigram\ntf = CountVectorizer(inputCol=\"n-gram\", outputCol=\"tf\")\nidf = IDF(inputCol=\"tf\", outputCol=\"features\", minDocFreq=3)\nindexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\nindexer = StringIndexer(inputCol=\"sentiment\", outputCol=\"sentiment_index\")\n\npipeline = Pipeline(stages=[tokenizer, ngram, tf, idf, indexer])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"tf_idf = pipeline.fit(spark_dropna)\ntraining_data = tf_idf.transform(koalas_dropna.to_spark()).select(\"features\",\"sentiment_index\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train, valid = training_data.randomSplit([0.7, 0.3], seed=41)\nsvc = LinearSVC()\n\nclassifierMod = OneVsRest(classifier=svc, featuresCol=\"features\",\n                         labelCol=\"sentiment_index\")\n\nmodel = classifierMod.fit(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"valid_prediction = model.transform(valid)\ntrain_prediction = model.transform(train)\n\n\nevaluator = MulticlassClassificationEvaluator(labelCol=\"sentiment_index\", \n                                              predictionCol=\"prediction\")\nprint(\"Train Accuracy achieved:\",round(evaluator.evaluate(train_prediction.select(\"sentiment_index\",\"prediction\"), {evaluator.metricName: \"accuracy\"}),3))\nprint(\"Valid Accuracy achieved:\",round(evaluator.evaluate(valid_prediction.select(\"sentiment_index\",\"prediction\"), {evaluator.metricName: \"accuracy\"}),3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We achieved a 81% accuracy on validation set in this very simple model, without any hyper parameter optimization or feature selection, the model is overfitting on the trainset there is difference of almost 11% in terms of accuracy between the training and validation, but still not a bad model!","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"#close the spark session when done\nspark.stop()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusions\n\nHere we saw how can spark be used in the Text Mining Sector with a very simple model, there are some external library like spark-nlp (not yet updated for spark 3, but works very well on  spark 2.4.x) to do some advanced stuff as well such as bert embedding, WordNet embeddings etc. This is the first version of the spark tutorial, will update this notebook in future to include some more cool stuff such as plots with Koalas API, more complex examples some more ML with may be some Parameter optimization and Cross Validation.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}