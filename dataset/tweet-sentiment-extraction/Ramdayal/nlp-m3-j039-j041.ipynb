{"cells":[{"metadata":{"id":"-zSPvJq1erbS"},"cell_type":"markdown","source":"# Importing Relevant Libraries"},{"metadata":{"id":"F4c6lh-qfweI","outputId":"5f79722c-678b-4727-c86f-ef873520a0ad","trusted":true},"cell_type":"code","source":"import re\nimport string\nimport numpy as np \nimport random\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom plotly import graph_objs as go\nimport plotly.express as px\nimport plotly.figure_factory as ff\nfrom collections import Counter\n\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\n\nimport nltk\nfrom nltk.corpus import stopwords\n\nfrom tqdm import tqdm\nimport os\nimport nltk\nimport spacy\nimport random\nfrom spacy.util import compounding\nfrom spacy.util import minibatch\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n","execution_count":null,"outputs":[]},{"metadata":{"id":"qI4POEqhfnKT"},"cell_type":"markdown","source":"# Analyzing the Data"},{"metadata":{"id":"qJZjmz9QfnRt","trusted":true},"cell_type":"code","source":"traindata = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"id":"4TOTNbg4ej-A","outputId":"f914fd8b-a41d-40b7-fb6c-333f0824e5cb","trusted":true},"cell_type":"code","source":"traindata.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"_4B9xAUDgV3P","outputId":"fa2a38a0-2a7d-4628-ff14-f9e9f28ff766","trusted":true},"cell_type":"code","source":"traindata.info()","execution_count":null,"outputs":[]},{"metadata":{"id":"jQPt2OAtgieN","trusted":true},"cell_type":"code","source":"traindata.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"zvBkTn5Tg0Qc","outputId":"241accd7-e1a1-453a-bba0-0ea1af9ff1ed","trusted":true},"cell_type":"code","source":"traindata['sentiment'].unique()","execution_count":null,"outputs":[]},{"metadata":{"id":"D7h5GAM7hKxR","outputId":"33e24632-49e7-4d4f-a690-3e0327ab10e8","trusted":true},"cell_type":"code","source":"traindata['sentiment'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"id":"0g1zcmq3hkhl","trusted":true},"cell_type":"code","source":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))","execution_count":null,"outputs":[]},{"metadata":{"id":"sRlDBCYwkTMm","trusted":true},"cell_type":"code","source":"results_jaccard=[]\n\nfor ind,row in traindata.iterrows():\n    sentence1 = row.text\n    sentence2 = row.selected_text\n\n    jaccard_score = jaccard(sentence1,sentence2)\n    results_jaccard.append([sentence1,sentence2,jaccard_score])","execution_count":null,"outputs":[]},{"metadata":{"id":"S6Wwu2rnkTPm","trusted":true},"cell_type":"code","source":"jaccard = pd.DataFrame(results_jaccard,columns=[\"text\",\"selected_text\",\"jaccard_score\"])\ntraindata = traindata.merge(jaccard,how='outer')","execution_count":null,"outputs":[]},{"metadata":{"id":"Qnj2VRPokTTe","trusted":true},"cell_type":"code","source":"traindata['Num_words_ST'] = traindata['selected_text'].apply(lambda x:len(str(x).split())) #Number Of words in Selected Text\ntraindata['Num_word_text'] = traindata['text'].apply(lambda x:len(str(x).split())) #Number Of words in main text\ntraindata['difference_in_words'] = traindata['Num_word_text'] - traindata['Num_words_ST'] #Difference in Number of words text and Selected Text","execution_count":null,"outputs":[]},{"metadata":{"id":"FNYoKo-SkTYE","outputId":"40a9a7bb-986d-430a-df4d-9c6795940b46","trusted":true},"cell_type":"code","source":"traindata.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"SyrucsWyPh6I"},"cell_type":"markdown","source":"# Data Cleaning"},{"metadata":{"id":"2kxO8NqakTSw","trusted":true},"cell_type":"code","source":"def clean_text(text):\n\n    text = str(text).lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    \n    return text","execution_count":null,"outputs":[]},{"metadata":{"id":"6WiKveTIpBaR","trusted":true},"cell_type":"code","source":"traindata['text'] = traindata['text'].apply(lambda x:clean_text(x))\ntraindata['selected_text'] = traindata['selected_text'].apply(lambda x:clean_text(x))","execution_count":null,"outputs":[]},{"metadata":{"id":"hC-9Iu9EpKVP","outputId":"9db595e6-fbe2-416f-b0c8-deaa3a648240","trusted":true},"cell_type":"code","source":"traindata.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"WHcalS-Fo4Zw"},"cell_type":"markdown","source":"# Visualizing the Data"},{"metadata":{"id":"h2bc1aYfP54A"},"cell_type":"markdown","source":"## Most Common Words in Selected Text"},{"metadata":{"id":"tYGTHz3Zp85e","outputId":"ba84c452-31ef-4b6d-f10a-7c12ca72b3c0","trusted":true},"cell_type":"code","source":"traindata['temp_list_ST'] = traindata['selected_text'].apply(lambda x:str(x).split())\ntop = Counter([item for sublist in traindata['temp_list_ST'] for item in sublist])\ntemp = pd.DataFrame(top.most_common(20))\ntemp.columns = ['Common_words','count']\ntemp.style.background_gradient(cmap='Reds')","execution_count":null,"outputs":[]},{"metadata":{"id":"gBvihe-ru780","trusted":true},"cell_type":"code","source":"def remove_stopword(x):\n    return [y for y in x if y not in stopwords.words('english')]\ntraindata['temp_list_ST'] = traindata['temp_list_ST'].apply(lambda x:remove_stopword(x))","execution_count":null,"outputs":[]},{"metadata":{"id":"fKfX2ZHgw_Hb","outputId":"4180510e-3a27-4c21-8de2-1662c6835229","trusted":true},"cell_type":"code","source":"top = Counter([item for sublist in traindata['temp_list_ST'] for item in sublist])\ntemp = pd.DataFrame(top.most_common(20))\ntemp = temp.iloc[1:,:]\ntemp.columns = ['Common_words','count']\ntemp.style.background_gradient(cmap='Purples')","execution_count":null,"outputs":[]},{"metadata":{"id":"X-qpD3aVQjgy"},"cell_type":"markdown","source":"## Most Common Words in Main Text"},{"metadata":{"id":"i-59SQTLzOwF","trusted":true},"cell_type":"code","source":"traindata['temp_list_T'] = traindata['text'].apply(lambda x:str(x).split())\ntraindata['temp_list_T'] = traindata['temp_list_T'].apply(lambda x:remove_stopword(x))","execution_count":null,"outputs":[]},{"metadata":{"id":"VUBXkPCd4Eud","outputId":"134684d5-bb1d-4ed8-8784-3e792777afe2","trusted":true},"cell_type":"code","source":"top = Counter([item for sublist in traindata['temp_list_T'] for item in sublist])\ntemp = pd.DataFrame(top.most_common(25))\ntemp = temp.iloc[1:,:]\ntemp.columns = ['Common_words','count']\ntemp.style.background_gradient(cmap='Blues')","execution_count":null,"outputs":[]},{"metadata":{"id":"bTNzKZPQQsp4"},"cell_type":"markdown","source":"## Most Common Words Sentiment Wise"},{"metadata":{"id":"67cZAS8D75rS","trusted":true},"cell_type":"code","source":"Positive_sent = traindata[traindata['sentiment']=='positive']\nNegative_sent = traindata[traindata['sentiment']=='negative']\nNeutral_sent = traindata[traindata['sentiment']=='neutral']","execution_count":null,"outputs":[]},{"metadata":{"id":"5EgnvcVI94gm","outputId":"a3a8f485-4e67-466f-9550-91fef7a01f15","trusted":true},"cell_type":"code","source":"#MosT common positive words\ntop = Counter([item for sublist in Positive_sent['temp_list_ST'] for item in sublist])\ntemp_positive = pd.DataFrame(top.most_common(20))\ntemp_positive.columns = ['Common_words','count']\ntemp_positive.style.background_gradient(cmap='Greens')","execution_count":null,"outputs":[]},{"metadata":{"id":"QwK109Fm-OJi","outputId":"7f2950ca-a0ae-4aab-dc6b-6d1d7b08e4c5","trusted":true},"cell_type":"code","source":"#MosT common negative words\ntop = Counter([item for sublist in Negative_sent['temp_list_ST'] for item in sublist])\ntemp_negative = pd.DataFrame(top.most_common(20))\ntemp_negative = temp_negative.iloc[1:,:]\ntemp_negative.columns = ['Common_words','count']\ntemp_negative.style.background_gradient(cmap='Reds')","execution_count":null,"outputs":[]},{"metadata":{"id":"qzHyzfyX94jf","outputId":"524b29b2-b495-439a-c884-6689feab575a","trusted":true},"cell_type":"code","source":"#MosT common Neutral words\ntop = Counter([item for sublist in Neutral_sent['temp_list_ST'] for item in sublist])\ntemp_neutral = pd.DataFrame(top.most_common(20))\ntemp_neutral = temp_neutral.loc[1:,:]\ntemp_neutral.columns = ['Common_words','count']\ntemp_neutral.style.background_gradient(cmap='Blues')","execution_count":null,"outputs":[]},{"metadata":{"id":"lbyGYypWQ11v"},"cell_type":"markdown","source":"## Unique Words Sentiment Wise"},{"metadata":{"id":"DJqalvoU94mO","trusted":true},"cell_type":"code","source":"raw_text = [word for word_list in traindata['temp_list_T'] for word in word_list]","execution_count":null,"outputs":[]},{"metadata":{"id":"kesmDC2T-4vF","outputId":"5cc2d1d5-75a9-4424-eb08-1653d04bf042","trusted":true},"cell_type":"code","source":"raw_text[:20]","execution_count":null,"outputs":[]},{"metadata":{"id":"SnfOJaAq94ox","trusted":true},"cell_type":"code","source":"def words_unique(sentiment,numwords,raw_words):\n\n    allother = []\n    for item in traindata[traindata.sentiment != sentiment]['temp_list_T']:\n        for word in item:\n            allother.append(word)\n    allother = list(set(allother))\n    \n    specificnonly = [x for x in raw_text if x not in allother]\n    \n    mycounter = Counter()\n    \n    for item in traindata[traindata.sentiment == sentiment]['temp_list_T']:\n        for word in item:\n            mycounter[word] += 1\n    keep = list(specificnonly)\n    \n    for word in list(mycounter):\n        if word not in keep:\n            del mycounter[word]\n    \n    Unique_words = pd.DataFrame(mycounter.most_common(numwords), columns = ['words','count'])\n    \n    return Unique_words","execution_count":null,"outputs":[]},{"metadata":{"id":"B4DLrwAP94rn","outputId":"8cba0d72-936c-4c1b-d70f-9d8485c32543","trusted":true},"cell_type":"code","source":"Unique_Positive= words_unique('positive', 20, raw_text)\nprint(\"The top 20 unique words in Positive Tweets are:\")\nUnique_Positive.style.background_gradient(cmap='Greens')","execution_count":null,"outputs":[]},{"metadata":{"id":"xmu8ttKM94ul","outputId":"2bcf1681-f98e-4701-ced4-57c6242a4748","trusted":true},"cell_type":"code","source":"Unique_Negative= words_unique('negative', 10, raw_text)\nprint(\"The top 10 unique words in Negative Tweets are:\")\nUnique_Negative.style.background_gradient(cmap='Reds')","execution_count":null,"outputs":[]},{"metadata":{"id":"3jqG9Vvv94w6","outputId":"f510dd61-a582-4468-bb6a-3a1ebc182d5c","trusted":true},"cell_type":"code","source":"Unique_Neutral= words_unique('neutral', 10, raw_text)\nprint(\"The top 10 unique words in Neutral Tweets are:\")\nUnique_Neutral.style.background_gradient(cmap='Blues')","execution_count":null,"outputs":[]},{"metadata":{"id":"PvcpAbUUBAty"},"cell_type":"markdown","source":"# Model"},{"metadata":{"id":"dN2_c-i6BCy8","trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv')\ndf_test = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv')\ndf_submission = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"id":"Dikrtnc9BOoR","trusted":true},"cell_type":"code","source":"df_train['Num_words_text'] = df_train['text'].apply(lambda x:len(str(x).split()))","execution_count":null,"outputs":[]},{"metadata":{"id":"kU6y76KiBOrW","trusted":true},"cell_type":"code","source":"df_train = df_train[df_train['Num_words_text']>=3]","execution_count":null,"outputs":[]},{"metadata":{"id":"wyLlWSvoBOy1","outputId":"b07be36a-4ef4-4005-f09b-c26e206c10d8","trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"iNNndcdmBO1o","trusted":true},"cell_type":"code","source":"def save_model(output_dir, nlp, new_model_name):\n    \n    output_dir = f'/kaggle/working/{output_dir}'\n    if output_dir is not None:        \n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n        nlp.meta[\"name\"] = new_model_name\n        nlp.to_disk(output_dir)\n        print(\"Saved model to\", output_dir)","execution_count":null,"outputs":[]},{"metadata":{"id":"Bv1CPO1GK4Lw","trusted":true},"cell_type":"code","source":"def train(train_data, output_dir, n_iter=20, model=None):\n\n    if model is not None:\n        nlp = spacy.load(output_dir)  # load existing spaCy model\n        print(\"Loaded model '%s'\" % model)\n    else:\n        nlp = spacy.blank(\"en\")  # create blank Language class\n        print(\"Created blank 'en' model\")\n    \n    if \"ner\" not in nlp.pipe_names:\n        ner = nlp.create_pipe(\"ner\")\n        nlp.add_pipe(ner, last=True)\n    # otherwise, get it so we can add labels\n    else:\n        ner = nlp.get_pipe(\"ner\")\n    \n    # add labels\n    for _, annotations in train_data:\n        for ent in annotations.get(\"entities\"):\n            ner.add_label(ent[2])\n\n    # get names of other pipes to disable them during training\n    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n    with nlp.disable_pipes(*other_pipes):  # only train NER\n        # sizes = compounding(1.0, 4.0, 1.001)\n        # batch up the examples using spaCy's minibatch\n        if model is None:\n            nlp.begin_training()\n        else:\n            nlp.resume_training()\n\n\n        for itn in tqdm(range(n_iter)):\n            random.shuffle(train_data)\n            batches = minibatch(train_data, size=compounding(4.0, 500.0, 1.001))    \n            losses = {}\n            for batch in batches:\n                texts, annotations = zip(*batch)\n                nlp.update(texts,  # batch of texts\n                            annotations,  # batch of annotations\n                            drop=0.5,   # dropout - make it harder to memorise data\n                            losses=losses, \n                            )\n            print(\"Losses\", losses)\n    save_model(output_dir, nlp, 'st_ner')","execution_count":null,"outputs":[]},{"metadata":{"id":"uIxxG-ccBO5C","trusted":true},"cell_type":"code","source":"def get_model_out_path(sentiment):\n\n    model_out_path = None\n    if sentiment == 'positive':\n        model_out_path = 'models/model_pos'\n    elif sentiment == 'negative':\n        model_out_path = 'models/model_neg'\n    return model_out_path","execution_count":null,"outputs":[]},{"metadata":{"id":"fT5bjNHyBtXY","trusted":true},"cell_type":"code","source":"def get_training_data(sentiment):\n\n    train_data = []\n    for index, row in df_train.iterrows():\n        if row.sentiment == sentiment:\n            selected_text = row.selected_text\n            text = row.text\n            start = text.find(selected_text)\n            end = start + len(selected_text)\n            train_data.append((text, {\"entities\": [[start, end, 'selected_text']]}))\n    return train_data","execution_count":null,"outputs":[]},{"metadata":{"id":"K9tM2Cp2RJys"},"cell_type":"markdown","source":"## Training the Model for Positive Sentiment"},{"metadata":{"id":"9eKKWU7tBw1j","outputId":"8eea2df1-85f6-4a79-e336-252750e68756","trusted":true},"cell_type":"code","source":"sentiment = 'positive'\n\ntrain_data = get_training_data(sentiment)\nmodel_path = get_model_out_path(sentiment)\ntrain(train_data, model_path, n_iter=3, model=None)","execution_count":null,"outputs":[]},{"metadata":{"id":"i9nLHXC3RQu8"},"cell_type":"markdown","source":"## Training the Model for Negative Sentiment"},{"metadata":{"id":"2bT_k6tyMQHi","outputId":"93f36e41-6acf-4786-bc53-b14689bb1f9e","trusted":true},"cell_type":"code","source":"sentiment = 'negative'\n\ntrain_data = get_training_data(sentiment)\nmodel_path = get_model_out_path(sentiment)\ntrain(train_data, model_path, n_iter=3, model=None)","execution_count":null,"outputs":[]},{"metadata":{"id":"TBMcl12yRfU6"},"cell_type":"markdown","source":"## Predicting"},{"metadata":{"id":"rm0XsUy1MaQF","trusted":true},"cell_type":"code","source":"def predict_entities(text, model):\n    doc = model(text)\n    ent_array = []\n    for ent in doc.ents:\n        start = text.find(ent.text)\n        end = start + len(ent.text)\n        new_int = [start, end, ent.label_]\n        if new_int not in ent_array:\n            ent_array.append([start, end, ent.label_])\n    selected_text = text[ent_array[0][0]: ent_array[0][1]] if len(ent_array) > 0 else text\n    return selected_text","execution_count":null,"outputs":[]},{"metadata":{"id":"sszXApMSSL2B"},"cell_type":"markdown","source":"# Final Submission"},{"metadata":{"id":"p2mWjUHmNA6d","outputId":"3f5d7202-1f45-459e-9c67-a64ace25364a","trusted":true},"cell_type":"code","source":"selected_texts = []\nMODELS_BASE_PATH = '/kaggle/working/models/'\n\nif MODELS_BASE_PATH is not None:\n    print(\"Loading Models  from \", MODELS_BASE_PATH)\n    model_pos = spacy.load(MODELS_BASE_PATH + 'model_pos')\n    model_neg = spacy.load(MODELS_BASE_PATH + 'model_neg')\n        \n    for index, row in df_test.iterrows():\n        text = row.text\n        output_str = \"\"\n        if row.sentiment == 'neutral' or len(text.split()) <= 2:\n            selected_texts.append(text)\n        elif row.sentiment == 'positive':\n            selected_texts.append(predict_entities(text, model_pos))\n        else:\n            selected_texts.append(predict_entities(text, model_neg))\n        \ndf_test['selected_text'] = selected_texts","execution_count":null,"outputs":[]},{"metadata":{"id":"ZBvUu0CqNRsB","outputId":"f6297ca5-ac79-4894-e01f-d90467bcf2a9","trusted":true},"cell_type":"code","source":"df_submission['selected_text'] = df_test['selected_text']\ndf_submission.to_csv(\"submission.csv\", index=False)\ndisplay(df_submission.head(10))","execution_count":null,"outputs":[]}],"metadata":{"colab":{"name":"NLP_M3.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":4}