{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom nltk.corpus import stopwords\nimport numpy as np\nimport sys\nimport string\nimport re\nfrom nltk.stem import WordNetLemmatizer \n\n#global adjustable variables:\ninputs = 0\n\n# train_size = 0.90 #percentage of training data utilized\n# random_state = 0\n# max_features = 10000\n# max_df = 0.95\n# min_df = 2\n#from the slides, J(A,B) = |A ∩ B| / |A ∪ B| = |A ∩ B| / (|A| + |B| - |A ∩ B|)\ndef jaccard(A, B): \n    a = set(A.lower().split()) \n    b = set(B.lower().split())\n    c = a.intersection(b)\n\n    j_value = float(len(c)) / (len(a) + len(b) - len(c))\n    return j_value\n\n#This function simply returns the number of sentiments for the entire training dataset.\ndef total_number_of_sentiments():\n\t#load in the data\n\tdf_reviews = pd.read_csv(\"train.csv\")\n\t#seperate positives, neutrals, negatives into an array call sentiments\n\tsentiments = df_reviews.groupby('sentiment')['textID'].nunique()\n\t\n\t#put them into variables\n\ttotal_pos = sentiments[0]\n\ttotal_neutral = sentiments[1]\n\ttotal_neg = sentiments[2]\n\treturn total_pos, total_neutral, total_neg\n\n# For program train_data, I have referenced Nick Koprowicz from his kaggle noteboook \n# (https://www.kaggle.com/nkoprowicz/a-simple-solution-using-only-word-counts)\n# on his section for displaying the count vectorization vocabulary in an effort to make the results cleaner.\n# His setup method is straight forward and very easy to adjust with my added global variable adjustments.\n\ndef load_and_train(train_size=None, random_state=None, max_feat=None, max_d=None, min_d=None):\n\tif train_size is None:\n\t\ttrain_size = 0.90\n\tif random_state is None:\n\t\trandom_state = 0\n\tif max_feat is None:\n\t\tmax_feat = 10000\n\tif max_d is None:\n\t\tmax_d= 0.95\n\tif min_d is None:\n\t\tmin_d = 2\n\t#Loads the values into variables\n\ttrain = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv')\n\ttest = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv')\n\tsample = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/sample_submission.csv')\n\t#drops the NaN\n\ttrain[train['text'].isna()]\n\ttrain = train.dropna()\n\ttrain[train['text'].isna()]\n\n\t#################################################################\n\t# (These attempts did not assist in prediction value)\n\t# Attempt on filtering out junk characters as done in Implementation Assignment 2\n\t################################################################\n\t# filters = '!\"\\'#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n\t# translate_dict = dict((c, \" \") for c in filters)\n\t# translate_map = str.maketrans(translate_dict)\n\t# train['text'] = train['text'].str.translate(translate_map)\n\t################################################################\n\t#Filtering out http://\n\t# filters = 'http://'\n\t# translate_dict = dict((c, \" \") for c in filters)\n\t# translate_map = str.maketrans(translate_dict)\n\t# train['text'] = train['text'].str.translate(translate_map)\n\n\t# Referencing Rajaram's Kaggle (https://www.kaggle.com/rajaram1988/ignored-stop-words-using-only-word-counts) to \n\t# implement filtering out web links\n\t###############################################################\n\n\n\ttrain['text'] = train['text'].map(lambda x: re.sub('\\\\n',' ',str(x)))\n    \n\t# remove any text starting with User... \n\ttrain['text'] = train['text'].map(lambda x: re.sub(\"\\[\\[User.*\",'',str(x)))\n\t    \n\t# remove IP addresses or user IDs\n\ttrain['text'] = train['text'].map(lambda x: re.sub(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",'',str(x)))\n\t    \n\t#remove http links in the text\n\ttrain['text'] = train['text'].map(lambda x: re.sub(\"(http://.*?\\s)|(http://.*)\",'',str(x)))\n\t###############################################################\n\n\n\t#Gets rid of case sensitivity so words like hi vs HI are the same.\n\ttrain['text'] = train['text'].str.lower()\n\ttest['text'] = test['text'].str.lower()\n\t\n\t#splits the training data into what the values were above\n\tX_train, X_val = train_test_split(train, train_size = train_size, random_state = random_state)\n\n\t#simply organizes the sentiments into categories of positive, neutral, negative\n\tpos_train = X_train[X_train['sentiment'] == 'positive']\n\tneutral_train = X_train[X_train['sentiment'] == 'neutral']\n\tneg_train = X_train[X_train['sentiment'] == 'negative']\n\n\tcv = CountVectorizer(max_df=max_d, min_df=min_d, max_features=max_feat, stop_words='english')\n\n\tX_train_cv = cv.fit_transform(X_train['text'].values.astype('U'))\n\n\tX_pos = cv.transform(pos_train['text'])\n\tX_neutral = cv.transform(neutral_train['text'])\n\tX_neg = cv.transform(neg_train['text'])\n\n\tpos_count_df = pd.DataFrame(X_pos.toarray(), columns=cv.get_feature_names())\n\tneutral_count_df = pd.DataFrame(X_neutral.toarray(), columns=cv.get_feature_names())\n\tneg_count_df = pd.DataFrame(X_neg.toarray(), columns=cv.get_feature_names())\n\n\tpos_words = {}\n\tneutral_words = {}\n\tneg_words = {}\n\n\tfor k in cv.get_feature_names():\n\t    pos = pos_count_df[k].sum()\n\t    neutral = neutral_count_df[k].sum()\n\t    neg = neg_count_df[k].sum()\n\t    \n\t    pos_words[k] = pos/pos_train.shape[0]\n\t    neutral_words[k] = neutral/neutral_train.shape[0]\n\t    neg_words[k] = neg/neg_train.shape[0]\n\t    \n\t# We need to account for the fact that there will be a lot of words used in tweets of every sentiment.  \n\t# Therefore, we reassign the values in the dictionary by subtracting the proportion of tweets in the other \n\t# sentiments that use that word.\n\n\tneg_words_adj = {}\n\tpos_words_adj = {}\n\tneutral_words_adj = {}\n\n\tfor key, value in neg_words.items():\n\t    neg_words_adj[key] = neg_words[key] - (neutral_words[key] + pos_words[key])\n\n\tfor key, value in pos_words.items():\n\t    pos_words_adj[key] = pos_words[key] - (neutral_words[key] + neg_words[key])\n\t    \n\tfor key, value in neutral_words.items():\n\t    neutral_words_adj[key] = neutral_words[key] - (neg_words[key] + pos_words[key])\n\n\treturn X_val, pos_words_adj, neg_words_adj, neutral_words_adj, train, test, sample\n# For program calculation, I have referenced Nick Koprowicz from his kaggle noteboook \n# (https://www.kaggle.com/nkoprowicz/a-simple-solution-using-only-word-counts)\n# on his section for calculating the text. The way of calculating the text cannot vary and his method was clean\n# in implementing the positive and negative word weights. \ndef calculate_selected_text(df_row, pos_words_adj, neg_words_adj, neutral_words_adj, tol):\n    \n    tweet = df_row['text']\n    sentiment = df_row['sentiment']\n    \n    if(sentiment == 'neutral'):\n        return tweet\n    \n    elif(sentiment == 'positive'):\n        dict_to_use = pos_words_adj # Calculate word weights using the pos_words dictionary\n    elif(sentiment == 'negative'):\n        dict_to_use = neg_words_adj # Calculate word weights using the neg_words dictionary\n        \n    words = tweet.split()\n    words_len = len(words)\n    subsets = [words[i:j+1] for i in range(words_len) for j in range(i,words_len)]\n    \n    score = 0\n    selection_str = '' # This will be our choice\n    lst = sorted(subsets, key = len) # Sort candidates by length\n    \n    #Calculating the \n    for i in range(len(subsets)):\n        \n        new_sum = 0 # Sum for the current substring\n        \n        # Calculate the sum of weights for each word in the substring\n        for p in range(len(lst[i])):\n            if(lst[i][p].translate(str.maketrans('','',string.punctuation)) in dict_to_use.keys()):\n            \tnew_sum += dict_to_use[lst[i][p].translate(str.maketrans('','',string.punctuation))]\n            \tafter_character = 0\n            \tbefore_character = 0\n            if (inputs == 1):  # If there are inputs.\n                    # Example of the for loop: \"This guy cool\"\n\n                    # Calculate the second word to impact the first word:\n                if (p + 1 < len(lst[i]) and p - 1 > 0):\n\n                    # Ex: Word selected at lst[i][p] = \"guy\"\n                    # after_character = \"cool\"\n                    # before_character = \"This\"\n\n                    after_character = lst[i][p + 1].translate(\n                        str.maketrans('', ''.string.punctuation)) in dict_to_use.keys()\n                    before_character = lst[i][p - 1].translate(\n                        str.maketrans('', ''.string.punctuation)) in dict_to_use.keys()\n\n                    # Calculate the weight of each of the words individually and determine which sum is greater.\n                    if (abs(after_character) > abs(before_character)):  # If the weight of the word is more\n                        new_sum += dict_to_use[lst[i][p].translate(\n                            str.maketrans('', '', string.punctuation))] + after_character\n\n                    elif (abs(after_characvter) < abs(before_character)):  # If the weight of the other word is more\n                        new_sum += dict_to_use[lst[i][p].translate(\n                            str.maketrans('', '', string.punctuation))] + before_character\n\n                    else:  # if the weight is 0 (edge words in the tweet) #otherwise its on the edge\n                        new_sum += dict_to_use[lst[i][p].translate(str.maketrans('', '', string.punctuation))]\n        # If the sum is greater than the score, update our current selection\n        if(new_sum > score + tol):\n            score = new_sum\n            selection_str = lst[i]\n            tol = tol*5 # Increase the tolerance a bit each time we choose a selection\n\n    # If we didn't find good substrings, return the whole text\n    if(len(selection_str) == 0):\n        selection_str = words\n        \n    return ' '.join(selection_str)\n\n# For program calculation, I have referenced Nick Koprowicz from his kaggle noteboook \n# (https://www.kaggle.com/nkoprowicz/a-simple-solution-using-only-word-counts)\n# on his section for calculating the jaccard. The way of calculating the text cannot vary and his method was clean\n# in implementing the positive and negative word weights. \n\ndef calculation(train_size=None, random_state=None, max_features=None, max_df=None, min_df=None):\n\tif train_size is None:\n\t\ttrain_size = 0.90\n\tif random_state is None:\n\t\trandom_state = 0\n\tif max_features is None:\n\t\tmax_feat = 10000\n\tif max_df is None:\n\t\tmax_df= 0.95\n\tif min_df is None:\n\t\tmin_df = 2\n\tX_val, pos_words_adj, neg_words_adj, neutral_words_adj = load_and_train(train_size, random_state, max_features, max_df, min_df)\n\n\tpd.options.mode.chained_assignment = None\n\ttol = 0.001\n\n\tX_val['predicted_selection'] = ''\n\n\tfor index, row in X_val.iterrows():\n\t    \n\t    selected_text = calculate_selected_text(row, pos_words_adj, neg_words_adj, neutral_words_adj, 3)\n\t    \n\t    X_val.loc[X_val['textID'] == row['textID'], ['predicted_selection']] = selected_text\n\n\tX_val['jaccard'] = X_val.apply(lambda x: jaccard(x['selected_text'], x['predicted_selection']), axis = 1)\n\tprint(\"For train_size: \", train_size)\n\tprint('The jaccard score for the validation set is:', np.mean(X_val['jaccard']))\n\treturn np.mean(X_val['jaccard'])\n\ndef run_output(train, test, sample):\n\tpos_tr = train[train['sentiment'] == 'positive']\n\tneutral_tr = train[train['sentiment'] == 'neutral']\n\tneg_tr = train[train['sentiment'] == 'negative']\n\tcv = CountVectorizer(max_df=0.95, min_df=2,\n                                     max_features=10000,\n                                     stop_words='english')\n\tfinal_cv = cv.fit_transform(train['text'])\n\n\tX_pos = cv.transform(pos_tr['text'])\n\tX_neutral = cv.transform(neutral_tr['text'])\n\tX_neg = cv.transform(neg_tr['text'])\n\n\tpos_final_count_df = pd.DataFrame(X_pos.toarray(), columns=cv.get_feature_names())\n\tneutral_final_count_df = pd.DataFrame(X_neutral.toarray(), columns=cv.get_feature_names())\n\tneg_final_count_df = pd.DataFrame(X_neg.toarray(), columns=cv.get_feature_names())\n\n\tpos_words = {}\n\tneutral_words = {}\n\tneg_words = {}\n\n\tfor k in cv.get_feature_names():\n\t    pos = pos_final_count_df[k].sum()\n\t    neutral = neutral_final_count_df[k].sum()\n\t    neg = neg_final_count_df[k].sum()\n\t    \n\t    pos_words[k] = pos/(pos_tr.shape[0])\n\t    neutral_words[k] = neutral/(neutral_tr.shape[0])\n\t    neg_words[k] = neg/(neg_tr.shape[0])\n\n\tneg_words_adj = {}\n\tpos_words_adj = {}\n\tneutral_words_adj = {}\n\n\tfor key, value in neg_words.items():\n\t    neg_words_adj[key] = neg_words[key] - (neutral_words[key] + pos_words[key])\n\t    \n\tfor key, value in pos_words.items():\n\t    pos_words_adj[key] = pos_words[key] - (neutral_words[key] + neg_words[key])\n\t    \n\tfor key, value in neutral_words.items():\n\t    neutral_words_adj[key] = neutral_words[key] - (neg_words[key] + pos_words[key])\n\n\ttol = 0.001\n\n\tfor index, row in test.iterrows():\n\t    \n\t    selected_text = calculate_selected_text(row, pos_words_adj, neg_words_adj, neutral_words_adj, tol)\n\t    \n\t    sample.loc[sample['textID'] == row['textID'], ['selected_text']] = selected_text\n\n\n\tsample.to_csv('submission.csv', index = False)\n\ndef run_multiple_times():\n\tbest = 0\n\t# for train_size in np.arange(0.1, 0.9, 0.1): #doesn't do much\n\t# \ttemp = calculation(train_size)\n\t# \tif best < temp:\n\t# \t\tbest = temp\n\t# print(\"best train size is: \", best)\n\t# for max_features in np.arange(10000,20000,2500):\n\t# \ttemp = calculation(None, None, max_features)\n\t# \tif best < temp:\n\t# \t\tbest = temp\n\t# print(\"best max_feature size is: \", best)\n\t# calculation()\n\tX_val, pos_words_adj, neg_words_adj, neutral_words_adj, train, test, sample = load_and_train()\n\trun_output(train, test, sample)\nrun_multiple_times()\n# load_and_split_data()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}