{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/tweet-sentiment-extraction/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data[\"words_text\"] = [ str(x).split() for x in train_data.text ]\ntrain_data[\"words_selected_text\"] = [ str(x).split() for x in train_data.selected_text ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n\ndef clean(row):\n    row = row.replace('.', ' ')\n    row = row.replace(',', '')\n    row = row.replace(\"'\", \"\")\n    row = re.sub(\"\\d+\", \"<NUM>\", row)\n    row = re.sub(\"\\*+\", \"<CURSE>\", row)\n    row = re.sub(\"^@.*\", \"<USER>\", row)\n    row = re.sub(\"^#.*\", \"<HASH>\", row)\n    row = re.sub(\"^((https|http|ftp|file)?:\\/\\/).*\", \"<LINK>\", row)\n    row = re.sub(\"[0-9]+:[0-9]+(am|AM|pm|PM)?\", \"<DATE>\", row)\n    row = row.lower().strip()\n    return row.split()\ntrain_data[\"words_text\"] = train_data.text.apply(lambda row: clean(str(row)))\ntrain_data[\"words_selected_text\"] = train_data.selected_text.apply(lambda row: clean(str(row)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Spelling correction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from spellchecker import SpellChecker\n# spell = SpellChecker()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from spellchecker import SpellChecker\n# spell = SpellChecker()\n\n\n# def spelling_correction(row) : \n    \n#     constant = [\"<curse>\", \"<num>\", \"<user>\", \"<hash>\", '<link>', '<date>']\n#     temp = [ spell.correction(word) if word not in constant else word for word in row ]\n    \n#     return temp\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_data[\"words_text\"] = [ spelling_correction(row) for row in train_data.words_text ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_data[\"words_selected_text\"] = [ spelling_correction(row) for row in train_data.words_selected_text ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_data.to_csv(\"/kaggle/input/tweet-sentiment-extraction/spell_correct_train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Making indices","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data2 = pd.read_csv(\"/kaggle/input/spell-correct/spell_correct_train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data2.words_text[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data2.words_text[0][0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### so first we need to convert this to literal\n\"['id','have','if']\" -> ['id','have','if']","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import ast\n\ntrain_data2.words_text = [ ast.literal_eval(str(x)) for x in train_data2.words_text ]\ntrain_data2.words_selected_text = [ ast.literal_eval(str(x)) for x in train_data2.words_selected_text]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train_data2['Unnamed: 0']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train_data2['Unnamed: 0.1']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data2.words_text[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data2.words_text[0][0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import difflib as diff\n\n\n# def first_matching_index(text,selected_text) :\n#     try :\n#         return  text.index(diff.get_close_matches(selected_text[0],text)[0])\n#     except :\n#         return  None\n        \n\n# def last_matching_index(text,selected_text) :\n#     length = len(selected_text)\n#     try : \n#         return text.index(diff.get_close_matches(selected_text[length-1],text)[0])\n#     except :\n#         return None\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import difflib as diff\n\ndef matching_index_search(text,selected,index):\n    text = list(text)\n    selected = list(selected)\n    return text.index(diff.get_close_matches(selected[index],text,cutoff=0)[0])\ntrain_data2[\"start_indices\"] = train_data2.apply(lambda x: matching_index_search(x.words_text,x.words_selected_text,0),axis=1)\ntrain_data2[\"end_indices\"] = train_data2.apply(lambda x: matching_index_search(x.words_text,x.words_selected_text,-1),axis=1)\ntrain_data2.head()\ndata = pd.read_csv(\"/kaggle/input/temp-file/try_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#temp1 = [ first_matching_index(x.text_split,x.selected_text_split) for x in  train_data2 ]\n\n# train_dataCp = train_data2.copy()\n\n# train_dataCp[\"start_indices\"] = train_data2.apply(lambda x : first_matching_index(x.text_split,x.selected_text_split), axis = 1 )\n\n# train_dataCp[\"end_indices\"] = train_data2.apply(lambda x : last_matching_index(x.text_split,x.selected_text_split), axis = 1 )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data2.iloc[49]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize = (30,10))\nsns.heatmap(train_data2.isnull())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_dataCp.drop([\"initial_indice\"],axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fig = plt.figure(figsize = (30,10))\n# sns.heatmap(train_dataCp.isnull())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# null_start_indices = train_dataCp[train_dataCp['start_indices'].isnull()].index.tolist()\n# null_end_indices = train_dataCp[train_dataCp['end_indices'].isnull()].index.tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# (len(null_start_indices),len(null_start_indices))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data2.iloc[49]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# len([ x  for x in  null_end_indices if x not in null_start_indices])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data2.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### need to remove those data which has initial index greater then final","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataCp = train_data2[ train_data2.start_indices <= train_data2.end_indices ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataCp.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataCp.to_csv(\"range_data.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Tokenization","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nimport pandas as pd \nimport ast\nimport tensorflow\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport pickle","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data3 =   train_dataCp.copy() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data3.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_data3[\"words_text\"] = train_data3.words_text.apply(lambda x: ast.literal_eval(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data3[\"words_text\"][0][0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dictionary = []\nfor words in train_data3.words_text :\n    dictionary.extend(words)\n    \ndictionary = [ word for word in dictionary if word.isalnum() ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"whole_text = \" \".join(dictionary)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokens = nltk.word_tokenize(whole_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(len(tokens),len(dictionary))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer(num_words=20000,oov_token=\"<OOV>\")\n\ntokenizer.fit_on_texts(train_data3.words_text)\ntokenized_text = tokenizer.texts_to_sequences(train_data3.words_text)\ntokenized_selected_text = tokenizer.texts_to_sequences(train_data3.words_selected_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(tokenizer.word_index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer.word_index[\"this\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pad_token_text = pad_sequences(tokenized_text,padding = \"post\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pad_token_text[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(pad_token_text[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pad_token_selected_text = pad_sequences(tokenized_selected_text,padding=\"post\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(pad_token_text).to_csv(\"pad_token_data.csv\",header=None,index=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data3.to_csv(\"tokenized_form.csv\",index=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('tokenizer.pickle', 'wb') as handle:\n    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model training","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport tensorflow\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, Bidirectional, GRU, Dense, Dropout, BatchNormalization, Flatten\nfrom tensorflow.keras.regularizers import l2, l1, l1_l2\nfrom tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import jaccard_similarity_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"tokenized_form.csv\")\ntargets = df[[\"start_indices\",\"end_indices\"]]\ntargets.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training = pd.read_csv(\"pad_token_data.csv\",header= None)\ntraining.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(training.values, targets.values, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Baseline(vocab_size):\n    model = Sequential([\n        Embedding(vocab_size, 128, input_length=33),\n        Bidirectional(GRU(128, return_sequences=True, dropout=0.8, recurrent_dropout=0.8)),\n        Bidirectional(GRU(128,return_sequences=True, dropout=0.8, recurrent_dropout=0.8)),\n        BatchNormalization(),\n        Dense(64, activation='elu',kernel_regularizer=l1_l2()),\n        Dropout(0.8),\n        Dense(2, activation='elu'),\n        Flatten(),\n        Dense(2, activation='elu')\n\n    ])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab = 20000\nmodel = Baseline(vocab)\nes = EarlyStopping(patience=5)\nmcp_save = ModelCheckpoint('tweet_sentiment_model.hdf5', save_best_only=True, monitor='val_mse')\nmodel.compile(loss=\"mse\",optimizer=\"adam\",metrics=['mse',\"mae\"])\nmodel.summary()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## finally submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, Bidirectional, GRU, Dense, Dropout, BatchNormalization, Flatten\nfrom tensorflow.keras.regularizers import l2, l1, l1_l2\nfrom tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import jaccard_similarity_score\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport re\nimport numpy as np\nimport pickle","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_test4 = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv')\nwith open('tokenizer.pickle', 'rb') as handle:\n    tokenizer = pickle.load(handle)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean(row):\n    row = row.replace('.', ' ')\n    row = row.replace(',', '')\n    row = row.replace(\"'\", \"\")\n    row = re.sub(\"\\d+\", \"<NUM>\", row)\n    row = re.sub(\"\\*+\", \"<CURSE>\", row)\n    row = re.sub(\"^@.*\", \"<USER>\", row)\n    row = re.sub(\"^#.*\", \"<HASH>\", row)\n    row = re.sub(\"^((https|http|ftp|file)?:\\/\\/).*\", \"<LINK>\", row)\n    row = re.sub(\"[0-9]+:[0-9]+(am|AM|pm|PM)?\", \"<DATE>\", row)\n    row = row.lower().strip()\n    return row.split()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ndata_test4[\"test_text_split\"] = data_test4.text.apply(lambda row: clean(str(row)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_tokenized_text = tokenizer.texts_to_sequences(data_test4.test_text_split)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pad_token_text = pad_sequences(test_tokenized_text,maxlen=33, padding = \"post\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pad_token_text[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Baseline(vocab_size):\n    model = Sequential([\n        Embedding(vocab_size, 128, input_length=33),\n        Bidirectional(GRU(128, return_sequences=True, dropout=0.8, recurrent_dropout=0.8)),\n        Bidirectional(GRU(128,return_sequences=True, dropout=0.8, recurrent_dropout=0.8)),\n        BatchNormalization(),\n        Dense(64, activation='elu',kernel_regularizer=l1_l2()),\n        Dropout(0.8),\n        Dense(2, activation='elu'),\n        Flatten(),\n        Dense(2, activation='elu')\n\n    ])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Baseline(20000)\nmodel.load_weights(\"/kaggle/input/tweeter-model/tweet_sentiment_model.hdf5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = model.predict(test_pad_token_text)\nresults","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = np.round(results)\nresults","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sum(results[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_test4[\"final_split\"] = data_test4.text.apply(lambda x: x.split())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_selected_text(split_text,indices):\n    try:\n        return \" \".join(split_text[int(indices[0][0]):int(indices[0][1])])\n    except:\n        return \" \".join(split_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_test4[\"selected_text\"] = data_test4.apply(lambda x: add_selected_text(x.test_text_split,results), axis=1)\nfig = plt.figure(figsize = (30,10))\nsns.heatmap(data_test4.isnull())\ndata_fn = data_test4.copy()\ndata_test4 = data.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#data_test4[\"selected_text\"] = data_test4.apply(lambda x: add_selected_text(x.test_text_split,results), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_test4.to_csv(\"submission.csv\",index=None,columns=[\"textID\",\"selected_text\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# data_test4.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fig = plt.figure(figsize = (30,10))\n# sns.heatmap(data_test4.isnull())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}