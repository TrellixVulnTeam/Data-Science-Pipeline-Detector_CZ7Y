{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Overview\n\nThis project is based on the dataset available at https://www.kaggle.com/c/tweet-sentiment-extraction/overview which is composed of about 20k tweets to train sentiment predictors.\n\nThis notebook will guide you through the process of tweets cleaning (a very basic NLP task when dealing with text data), training a few Deep Learning models with different architectures and finally inferencing on test text."},{"metadata":{},"cell_type":"markdown","source":"# Imports"},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nimport matplotlib.pyplot as plt\nimport string\nfrom nltk.corpus import stopwords\nimport nltk\nfrom nltk.corpus import wordnet\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize.treebank import TreebankWordDetokenizer\nfrom collections import Counter\nfrom wordcloud import WordCloud\nfrom nltk.corpus import stopwords\nimport nltk\nfrom gensim.utils import simple_preprocess\nfrom nltk.corpus import stopwords\nimport gensim\nfrom sklearn.model_selection import train_test_split\nimport spacy\nimport pickle\nimport warnings\nwarnings.filterwarnings('ignore')\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt \nimport tensorflow as tf\nimport keras\nimport numpy as np\nimport pandas as pd\nprint('Done')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data importing"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data exploration"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's get the dataset lenght\nlen(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Is there any other different value than neutral, negative and positive?\ntrain['sentiment'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#How's distributed the dataset? Is it biased?\ntrain.groupby('sentiment').nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data cleaning\n\nEven when the dataset is a little bit biased, we'll keep it this way because the differences are not significant."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's keep only the columns that we're going to use\ntrain = train[['selected_text','sentiment']]\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Is there any null value?\ntrain[\"selected_text\"].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's fill the only null value.\ntrain[\"selected_text\"].fillna(\"No content\", inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The next steps about data cleaning will be:\n\n* Remove URLs from the tweets\n* Tokenize text\n* Remove emails\n* Remove new lines characters\n* Remove distracting single quotes\n* Remove all punctuation signs\n* Lowercase all text\n* Detokenize text\n* Convert list of texts to Numpy array"},{"metadata":{"trusted":true},"cell_type":"code","source":"def depure_data(data):\n    \n    #Removing URLs with a regular expression\n    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n    data = url_pattern.sub(r'', data)\n\n    # Remove Emails\n    data = re.sub('\\S*@\\S*\\s?', '', data)\n\n    # Remove new line characters\n    data = re.sub('\\s+', ' ', data)\n\n    # Remove distracting single quotes\n    data = re.sub(\"\\'\", \"\", data)\n        \n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = []\n#Splitting pd.Series to list\ndata_to_list = train['selected_text'].values.tolist()\nfor i in range(len(data_to_list)):\n    temp.append(depure_data(data_to_list[i]))\nlist(temp[:5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sent_to_words(sentences):\n    for sentence in sentences:\n        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n        \n\ndata_words = list(sent_to_words(temp))\n\nprint(data_words[:10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(data_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def detokenize(text):\n    return TreebankWordDetokenizer().detokenize(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = []\nfor i in range(len(data_words)):\n    data.append(detokenize(data_words[i]))\nprint(data[:5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = np.array(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Label encoding\n\nAs the dataset is categorical, we need to convert the sentiment labels from Neutral, Negative and Positive to a float type that our model can understand. To achieve this task, we'll implement the to_categorical method from Keras."},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = np.array(train['sentiment'])\ny = []\nfor i in range(len(labels)):\n    if labels[i] == 'neutral':\n        y.append(0)\n    if labels[i] == 'negative':\n        y.append(1)\n    if labels[i] == 'positive':\n        y.append(2)\ny = np.array(y)\nlabels = tf.keras.utils.to_categorical(y, 3, dtype=\"float32\")\ndel y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data sequencing and splitting\n\nWe'll implement the Keras tokenizer as well as its pad_sequences method to transform our text data into 3D float data, otherwise our neural networks won't be able to be trained on it."},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras import layers\nfrom keras.optimizers import RMSprop,Adam\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras import regularizers\nfrom keras import backend as K\nfrom keras.callbacks import ModelCheckpoint\nmax_words = 5000\nmax_len = 200\n\ntokenizer = Tokenizer(num_words=max_words)\ntokenizer.fit_on_texts(data)\nsequences = tokenizer.texts_to_sequences(data)\ntweets = pad_sequences(sequences, maxlen=max_len)\nprint(tweets)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Splitting the data\nX_train, X_test, y_train, y_test = train_test_split(tweets,labels, random_state=0)\nprint (len(X_train),len(X_test),len(y_train),len(y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model building\n\nAlright, in the next cells I'll guide you through the process of building 3 Recurrent Neural Networks. I'll implement sequential models from the Keras API to achieve this task. Essentially, I'll start with a single layer **LSTM** network which is known by achieving good results in NLP tasks when the dataset is relatively small (I could have started with a SimpleRNN which is even simpler, but to be honest it's actually not deployed in production environments because it is too simple - however I'll leave it commented in case you want to know it's built). The next one will be a Bidirectional LSTM model, a more complex one and this particular one is known to achieve great metrics when talking about text classification. To go beyond the classic NLP approach, finally we'll implement a very unusual model: a Convolutional 1D network, known as well by delivering good metrics when talking about NLP. If everything goes ok, we should get the best results with the BidRNN, let's see what happens.\n\nLet's get hands on:"},{"metadata":{},"cell_type":"markdown","source":"## SimpleRNN model (Bonus)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#model0 = Sequential()\n#model0.add(layers.Embedding(max_words, 15))\n#model0.add(layers.SimpleRNN(15))\n#model0.add(layers.Dense(3,activation='softmax'))\n\n\n#model0.compile(optimizer='rmsprop',loss='categorical_crossentropy', metrics=['accuracy'])\n#Implementing model checkpoins to save the best metric and do not lose it on training.\n#checkpoint0 = ModelCheckpoint(\"best_model0.hdf5\", monitor='val_accuracy', verbose=1,save_best_only=True, mode='auto', period=1,save_weights_only=False)\n#history = model0.fit(X_train, y_train, epochs=5,validation_data=(X_test, y_test),callbacks=[checkpoint0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Single LSTM layer model"},{"metadata":{"trusted":true},"cell_type":"code","source":"model1 = Sequential()\nmodel1.add(layers.Embedding(max_words, 20))\nmodel1.add(layers.LSTM(15,dropout=0.5))\nmodel1.add(layers.Dense(3,activation='softmax'))\n\n\nmodel1.compile(optimizer='rmsprop',loss='categorical_crossentropy', metrics=['accuracy'])\n#Implementing model checkpoins to save the best metric and do not lose it on training.\ncheckpoint1 = ModelCheckpoint(\"best_model1.hdf5\", monitor='val_accuracy', verbose=1,save_best_only=True, mode='auto', period=1,save_weights_only=False)\nhistory = model1.fit(X_train, y_train, epochs=70,validation_data=(X_test, y_test),callbacks=[checkpoint1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Bidirectional LTSM model"},{"metadata":{"trusted":true},"cell_type":"code","source":"model2 = Sequential()\nmodel2.add(layers.Embedding(max_words, 40, input_length=max_len))\nmodel2.add(layers.Bidirectional(layers.LSTM(20,dropout=0.6)))\nmodel2.add(layers.Dense(3,activation='softmax'))\nmodel2.compile(optimizer='rmsprop',loss='categorical_crossentropy', metrics=['accuracy'])\n#Implementing model checkpoins to save the best metric and do not lose it on training.\ncheckpoint2 = ModelCheckpoint(\"best_model2.hdf5\", monitor='val_accuracy', verbose=1,save_best_only=True, mode='auto', period=1,save_weights_only=False)\nhistory = model2.fit(X_train, y_train, epochs=70,validation_data=(X_test, y_test),callbacks=[checkpoint2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1D Convolutional model\n\nBefore diving into this model, I know by prior experience that it tends to overfit extremely fast on small datasets. In this sense, just will implement it to show you how to do it in case it's of your interest."},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import regularizers\nmodel3 = Sequential()\nmodel3.add(layers.Embedding(max_words, 40, input_length=max_len))\nmodel3.add(layers.Conv1D(20, 6, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=2e-3, l2=2e-3),bias_regularizer=regularizers.l2(2e-3)))\nmodel3.add(layers.MaxPooling1D(5))\nmodel3.add(layers.Conv1D(20, 6, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=2e-3, l2=2e-3),bias_regularizer=regularizers.l2(2e-3)))\nmodel3.add(layers.GlobalMaxPooling1D())\nmodel3.add(layers.Dense(3,activation='softmax'))\nmodel3.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['acc'])\ncheckpoint3 = ModelCheckpoint(\"best_model3.hdf5\", monitor='val_accuracy', verbose=1,save_best_only=True, mode='auto', period=1,save_weights_only=False)\nhistory = model3.fit(X_train, y_train, epochs=70,validation_data=(X_test, y_test),callbacks=[checkpoint3])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If you check the val_accuracy metric in the training logs you won't find better score than the one achieved by the BidRNN. Again, the previous model is not the best for this task becaue is majorly used for short translation tasks, but the good thing to notice is its speed to train.\n\nLet's move on."},{"metadata":{},"cell_type":"markdown","source":"# Best model validation\n(Before final commit, the best model obtained was the BidRNN)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's load the best model obtained during training\nbest_model = keras.models.load_model(\"best_model2.hdf5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_loss, test_acc = best_model.evaluate(X_test, y_test, verbose=2)\nprint('Model accuracy: ',test_acc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = best_model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Confusion matrix\n\nAlright, we all know the accuracy is not a good metric to measure how well a model is. That's the reason why I like to always see its confusion matrix, that way I have a better understanding of its classification and generalization ability. Let's plot it."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nmatrix = confusion_matrix(y_test.argmax(axis=1), np.around(predictions, decimals=0).argmax(axis=1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nconf_matrix = pd.DataFrame(matrix, index = ['Neutral','Negative','Positive'],columns = ['Neutral','Negative','Positive'])\n#Normalizing\nconf_matrix = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\nplt.figure(figsize = (15,15))\nsns.heatmap(conf_matrix, annot=True, annot_kws={\"size\": 15})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again, the model's score is very poor, but keep in mind it hasn't gone through hyperparameter tuning. Let's see how it performs on some test text."},{"metadata":{"trusted":true},"cell_type":"code","source":"sentiment = ['Neutral','Negative','Positive']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sequence = tokenizer.texts_to_sequences(['this experience has been the worst , want my money back'])\ntest = pad_sequences(sequence, maxlen=max_len)\nsentiment[np.around(best_model.predict(test), decimals=0).argmax(axis=1)[0]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sequence = tokenizer.texts_to_sequences(['this data science article is the best ever'])\ntest = pad_sequences(sequence, maxlen=max_len)\nsentiment[np.around(best_model.predict(test), decimals=0).argmax(axis=1)[0]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sequence = tokenizer.texts_to_sequences(['i hate youtube ads, they are annoying'])\ntest = pad_sequences(sequence, maxlen=max_len)\nsentiment[np.around(best_model.predict(test), decimals=0).argmax(axis=1)[0]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sequence = tokenizer.texts_to_sequences(['i really loved how the technician helped me with the issue that i had'])\ntest = pad_sequences(sequence, maxlen=max_len)\nsentiment[np.around(best_model.predict(test), decimals=0).argmax(axis=1)[0]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preparing model for AWS SageMaker"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Saving weights and tokenizer so we can reduce training time on SageMaker\n\n# serialize model to JSON\nmodel_json = best_model.to_json()\nwith open(\"model.json\", \"w\") as json_file:\n    json_file.write(model_json)\n# serialize weights to HDF5\nbest_model.save_weights(\"model-weights.h5\")\nprint(\"Model saved\")\n\n# saving tokenizer\nwith open('tokenizer.pickle', 'wb') as handle:\n    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\nprint('Tokenizer saved')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We've reached the end of this notebook. I just wanted to highlight a few things before let you go.\n\nAs you could see, very simple networks can achieve fantastic results. To go beyond, always the best approach is to build a model that underfit the data, then optimize it to overfit and finally start tuning your hyperparameters to achieve the metric that the business needs to reach. The way you tune the model is up to you, there's no magic formula for it, but adding regularization always works, as well as dropout. \n\nIf you have any doubt, please feel free to comment :)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}