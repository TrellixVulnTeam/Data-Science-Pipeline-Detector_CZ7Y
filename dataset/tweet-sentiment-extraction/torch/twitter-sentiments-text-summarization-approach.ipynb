{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# %tensorflow_version 2.x\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n%matplotlib inline\n%config InlineBackend.figure_format = \"retina\"\n\nimport unicodedata\nimport re\nimport numpy as np\nimport pandas as pd\nimport io\nimport os\nimport time\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold, StratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/tweet-sentiment-extraction/train.csv')\ntest = pd.read_csv('../input/tweet-sentiment-extraction/test.csv')\nsample = pd.read_csv('../input/tweet-sentiment-extraction/sample_submission.csv')\n\ntrain = train.dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def unicode_to_ascii(s):\n    return ''.join(c for c in unicodedata.normalize('NFD', s)\n      if unicodedata.category(c) != 'Mn')\n\n\ndef preprocess_sentence(w):\n    w = '<start> ' + w + ' <end>'\n    return w","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_dataset(data):\n    text = train.text.apply(lambda x: preprocess_sentence(x))\n    summary = train.selected_text.apply(lambda x: preprocess_sentence(x))\n    test_text = test.text.apply(lambda x: preprocess_sentence(x))\n    return text, summary, test_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def max_length(tensor):\n    return max(len(t) for t in tensor)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tokenize(lang):\n    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n      filters=''\n    )\n    lang_tokenizer.fit_on_texts(lang)\n    tensor = lang_tokenizer.texts_to_sequences(lang)\n    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n    return tensor, lang_tokenizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_dataset(data, num_examples=None):\n    text, summary, test = create_dataset(data)\n    new_text = pd.concat([text,test],axis=0)\n    new_text_tensor, text_tokenizer = tokenize(new_text)\n    text_tensor, test_tensor = new_text_tensor[:len(text)],new_text_tensor[len(text):]\n    summary_tensor, summary_tokenizer = tokenize(summary)\n    return text_tensor, text_tokenizer, summary_tensor, summary_tokenizer, test_tensor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_tensor, text_tokenizer, summary_tensor, summary_tokenizer, test_tensor = load_dataset(train)\nmax_length_text, max_length_summary = max_length(text_tensor), max_length(summary_tensor)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(text_tensor, summary_tensor, test_size=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert(lang, tensor):\n    for t in tensor:\n        if t!=0:\n            print (\"%d ----> %s\" % (t, lang.index_word[t]))\n\nprint (\"Input Text; index to word mapping\")\nconvert(text_tokenizer, input_tensor_train[1])\nprint ()\nprint (\"Target Summary; index to word mapping\")\nconvert(summary_tokenizer, target_tensor_train[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BUFFER_SIZE = len(input_tensor_train)\nBATCH_SIZE = 64\nsteps_per_epoch = len(input_tensor_train)//BATCH_SIZE\nembedding_dim = 256\nunits = 1024\nvocab_inp_size = len(text_tokenizer.word_index)+1\nvocab_tar_size = len(summary_tokenizer.word_index)+1\n\ndataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).cache().shuffle(BUFFER_SIZE)\ndataset = dataset.batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"example_input_batch, example_target_batch = next(iter(dataset))\nexample_input_batch.shape, example_target_batch.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Encoder(tf.keras.Model):\n    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n        super(Encoder, self).__init__()\n        self.batch_sz = batch_sz\n        self.enc_units = enc_units\n        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n        self.gru = tf.keras.layers.GRU(self.enc_units,\n                                       return_sequences=True,\n                                       return_state=True,\n                                       recurrent_initializer='glorot_uniform')\n\n    def call(self, x, hidden):\n        x = self.embedding(x)\n        output, state = self.gru(x, initial_state = hidden)\n        return output, state\n\n    def initialize_hidden_state(self):\n        return tf.zeros((self.batch_sz, self.enc_units))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n\n# sample input\nsample_hidden = encoder.initialize_hidden_state()\nsample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\nprint ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\nprint ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class BahdanauAttention(tf.keras.layers.Layer):\n    def __init__(self, units):\n        super(BahdanauAttention, self).__init__()    \n        self.W1 = tf.keras.layers.Dense(units)\n        self.W2 = tf.keras.layers.Dense(units)\n        self.V = tf.keras.layers.Dense(1)\n    \n    def call(self, query, values):\n        query_with_time_axis = tf.expand_dims(query, 1)\n        score = self.V(tf.nn.tanh(\n            self.W1(query_with_time_axis) + self.W2(values)))\n        attention_weights = tf.nn.softmax(score, axis=1)\n        context_vector = attention_weights * values\n        context_vector = tf.reduce_sum(context_vector, axis=1)\n        return context_vector, attention_weights","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"attention_layer = BahdanauAttention(1)\nattention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n\nprint(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\nprint(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Decoder(tf.keras.Model):\n    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n        super(Decoder, self).__init__()\n        self.batch_sz = batch_sz\n        self.dec_units = dec_units\n        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n        self.gru = tf.keras.layers.GRU(self.dec_units,\n                                       return_sequences=True,\n                                       return_state=True,\n                                       recurrent_initializer='glorot_uniform')\n        self.fc = tf.keras.layers.Dense(vocab_size)\n        self.attention = BahdanauAttention(self.dec_units)\n\n    def call(self, x, hidden, enc_output):\n        context_vector, attention_weights = self.attention(hidden, enc_output)\n        x = self.embedding(x)\n        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n        output, state = self.gru(x)\n        output = tf.reshape(output, (-1, output.shape[2]))\n        x = self.fc(output)\n        return x, state, attention_weights","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n\nsample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n                                      sample_hidden, sample_output)\n\nprint ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = tf.keras.optimizers.RMSprop()\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n    from_logits=True, reduction='none')\n\ndef loss_function(real, pred):\n    mask = tf.math.logical_not(tf.math.equal(real, 0))\n    loss_ = loss_object(real, pred)\n\n    mask = tf.cast(mask, dtype=loss_.dtype)\n    loss_ *= mask\n\n    return tf.reduce_mean(loss_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint_dir = './training_checkpoints'\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\ncheckpoint = tf.train.Checkpoint(optimizer=optimizer,\n                                 encoder=encoder,\n                                 decoder=decoder)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@tf.function\ndef train_step(inp, targ, enc_hidden):\n    loss = 0\n\n    with tf.GradientTape() as tape:\n        enc_output, enc_hidden = encoder(inp, enc_hidden)\n\n        dec_hidden = enc_hidden\n\n        dec_input = tf.expand_dims([summary_tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n\n        for t in range(1, targ.shape[1]):\n            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n            loss += loss_function(targ[:, t], predictions)\n            dec_input = tf.expand_dims(targ[:, t], 1)\n\n    batch_loss = (loss / int(targ.shape[1]))\n\n    variables = encoder.trainable_variables + decoder.trainable_variables\n\n    gradients = tape.gradient(loss, variables)\n\n    optimizer.apply_gradients(zip(gradients, variables))\n\n    return batch_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EPOCHS = 15\n\nfor epoch in range(EPOCHS):\n    start = time.time()\n\n    enc_hidden = encoder.initialize_hidden_state()\n    total_loss = 0\n\n    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n        batch_loss = train_step(inp, targ, enc_hidden)\n        total_loss += batch_loss\n\n        if batch % 100 == 0:\n            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n                                                   batch,\n                                                   batch_loss.numpy()))\n    if (epoch + 1) % 2 == 0:\n        checkpoint.save(file_prefix = checkpoint_prefix)\n\n    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n                                  total_loss / steps_per_epoch))\n    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate(sentence):\n    attention_plot = np.zeros((max_length_summary, max_length_text))\n\n    sentence = preprocess_sentence(sentence)\n    inputs = [text_tokenizer.word_index[i] for i in sentence.lower().split()]\n    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n                                                         maxlen=max_length_text,\n                                                         padding='post')\n    inputs = tf.convert_to_tensor(inputs)\n\n    result = ''\n\n    hidden = [tf.zeros((1, units))]\n    enc_out, enc_hidden = encoder(inputs, hidden)\n\n    dec_hidden = enc_hidden\n    dec_input = tf.expand_dims([summary_tokenizer.word_index['<start>']], 0)\n\n    for t in range(max_length_summary):\n        predictions, dec_hidden, attention_weights = decoder(dec_input,\n                                                             dec_hidden,\n                                                             enc_out)\n\n        # storing the attention weights to plot later on\n        attention_weights = tf.reshape(attention_weights, (-1, ))\n        attention_plot[t] = attention_weights.numpy()\n\n        predicted_id = tf.argmax(predictions[0]).numpy()\n\n        result += summary_tokenizer.index_word[predicted_id] + ' '\n\n        if summary_tokenizer.index_word[predicted_id] == '<end>':\n            return result, sentence, attention_plot\n\n        # the predicted ID is fed back into the model\n        dec_input = tf.expand_dims([predicted_id], 0)\n\n    return result, sentence, attention_plot","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# function for plotting the attention weights\nimport matplotlib.ticker as ticker\ndef plot_attention(attention, sentence, predicted_sentence):\n    fig = plt.figure(figsize=(10,10))\n    ax = fig.add_subplot(1, 1, 1)\n    ax.matshow(attention, cmap='viridis')\n\n    fontdict = {'fontsize': 14}\n\n    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n\n    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def summarize(sentence):\n    result, sentence, attention_plot = evaluate(sentence)\n    return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample['selected_text']  = test.text.apply(lambda x: summarize(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample['selected_text'] = sample['selected_text'].apply(lambda x: x.replace(\"<end>\",\"\"))\nsample.to_csv('submission.csv',index=False)\nsample","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}