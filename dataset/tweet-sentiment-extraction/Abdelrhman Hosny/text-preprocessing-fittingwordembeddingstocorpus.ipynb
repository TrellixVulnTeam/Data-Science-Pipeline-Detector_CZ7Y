{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom nltk.tokenize import word_tokenize\nimport gensim.downloader\nimport re","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-19T10:32:07.830134Z","iopub.execute_input":"2021-07-19T10:32:07.830921Z","iopub.status.idle":"2021-07-19T10:32:10.028114Z","shell.execute_reply.started":"2021-07-19T10:32:07.830795Z","shell.execute_reply":"2021-07-19T10:32:10.026761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is an adaptation from [Notebook link](https://www.kaggle.com/christofhenkel/how-to-preprocessing-when-using-embeddings) applied to this dataset.\n\nThis aims to make sure that there are word embeddings that correspond to our corpus\n\nWhat we will try to fix is that sometimes due to different ways of writing and embedding styles, embeddings aren't found for words","metadata":{}},{"cell_type":"markdown","source":"Example:\n\nIf the embedding dictionary $x$ has the embedding for the word 'me' but not the word 'Me', we inspect that and apply the preprocessing we need to match these two words","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv')\ntrain_df.sample(n=3)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T10:32:10.030567Z","iopub.execute_input":"2021-07-19T10:32:10.031087Z","iopub.status.idle":"2021-07-19T10:32:10.209717Z","shell.execute_reply.started":"2021-07-19T10:32:10.031029Z","shell.execute_reply":"2021-07-19T10:32:10.208712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First we check for missing data","metadata":{}},{"cell_type":"code","source":"train_df.info()","metadata":{"execution":{"iopub.status.busy":"2021-07-19T10:32:10.212048Z","iopub.execute_input":"2021-07-19T10:32:10.212522Z","iopub.status.idle":"2021-07-19T10:32:10.24611Z","shell.execute_reply.started":"2021-07-19T10:32:10.212473Z","shell.execute_reply":"2021-07-19T10:32:10.244712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since there was only 1 row missing information, we removed it as it had no text which made it hard to use","metadata":{}},{"cell_type":"code","source":"train_df.dropna(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T10:32:10.248492Z","iopub.execute_input":"2021-07-19T10:32:10.2489Z","iopub.status.idle":"2021-07-19T10:32:10.292951Z","shell.execute_reply.started":"2021-07-19T10:32:10.248867Z","shell.execute_reply":"2021-07-19T10:32:10.291982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Helper functions to track our preprocessing progress** ","metadata":{}},{"cell_type":"code","source":"def gen_count_matrix(text) -> dict():\n    \"\"\"Return a dictionary that has the number of times each word appeared in text.\n    \n    Args:\n        text: Array of strings that contains the sentences(tweets)\n            that are in our corpus\n    \n    Returns:\n        count: Dictionary containing the number of times each word was mentioned in our corpus\n        example:\n        \n        count = {\n        'you': 123,\n        'are': 320,\n        'awesome': 100\n        }\n        \n        count['you'] # 123\n        means that you was mentioned 123 times in our corpus.\n    \"\"\"\n    count = {}\n    for i, sentence in enumerate(tqdm(text)):\n        for word in word_tokenize(sentence):\n            count.setdefault(word,0)\n            count[word] += 1\n    \n    return count\n            \n\ndef check_coverage(count, embedding):\n    \"\"\"Check how much text and words are covered by the embedding.\n    \n    Args:\n        count: A dict that is generated from the function gen_count_matrix\n            similar to CountVectorizer in sklearn\n        embedding: A gensim KeyedVector that has word embeddings\n        \n    Returns:\n        oov: A dict that shows the words that are not in the embeddings(vocabulary)\n            and how many times each one occurred.\n            short for out of vocabulary.\n        known: A dict that shows the words that are in the embeddings(vocabulary)\n            and how many times each one occurred. \"\"\"\n    \n    oov = {}\n    known = {}\n    for word in tqdm(count.keys()):\n        if word in embedding.index_to_key:\n            known.setdefault(word, 0)\n            known[word] += count[word]\n        else:\n            oov.setdefault(word,0)\n            oov[word] += count[word]\n    \n    print('Word % in embedding : {} %'.format( round( len(known) / (len(known)+len(oov)) * 100, 2)))\n    print('Text % in embedding : {} %'.format( round( sum(known.values()) / (sum(known.values()) + sum(oov.values())) * 100, 2)))\n    return oov, known\n            \ndef sort_dict_values(oov):\n    \"\"\"Sorts dictionary by values.\n    \"\"\"\n    return dict(sorted(oov.items(), key=lambda item: item[1], reverse=True))\n    ","metadata":{"execution":{"iopub.status.busy":"2021-07-19T10:32:10.294338Z","iopub.execute_input":"2021-07-19T10:32:10.294859Z","iopub.status.idle":"2021-07-19T10:32:10.307004Z","shell.execute_reply.started":"2021-07-19T10:32:10.294806Z","shell.execute_reply":"2021-07-19T10:32:10.306088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Preprocessing**\n\n## **Make sure corpus matches pretrained embeddings** \n**We make sure that our corpus matches with our pretrained word vectors**","metadata":{}},{"cell_type":"code","source":"text = [sentence.strip() for sentence in train_df['text']]\ntwitter_vectors = gensim.downloader.load('glove-twitter-100') # Loading word embeddings","metadata":{"execution":{"iopub.status.busy":"2021-07-19T10:32:10.308344Z","iopub.execute_input":"2021-07-19T10:32:10.308875Z","iopub.status.idle":"2021-07-19T10:37:06.566733Z","shell.execute_reply.started":"2021-07-19T10:32:10.308824Z","shell.execute_reply":"2021-07-19T10:37:06.565396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**We can see that we have 37k words in our corpus and that**\n- **40% of our words are covered** \n- **80% of text is covered in embeddings**","metadata":{}},{"cell_type":"code","source":"count = gen_count_matrix(text)\nprint('Number of unique words in our corpus ', len(count))\noov , known = check_coverage(count, twitter_vectors)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T10:37:06.568686Z","iopub.execute_input":"2021-07-19T10:37:06.569159Z","iopub.status.idle":"2021-07-19T10:45:31.221458Z","shell.execute_reply.started":"2021-07-19T10:37:06.569109Z","shell.execute_reply":"2021-07-19T10:45:31.219899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Lower case , numbers & links**\n**From previous examples done for this, I know that glove-twitter embeddings has all lowercase embeddings and you can notice that in the oov dictionary**\n\nThis leads me to preprocessing my data by making all text lowercase and removing links and replacing numbers with ##","metadata":{}},{"cell_type":"code","source":"sort_dict_values(oov)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-07-19T10:45:31.2235Z","iopub.execute_input":"2021-07-19T10:45:31.223936Z","iopub.status.idle":"2021-07-19T10:45:31.324583Z","shell.execute_reply.started":"2021-07-19T10:45:31.223887Z","shell.execute_reply":"2021-07-19T10:45:31.32343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**When training glove, the numbers were changed so that**\n   - 20 -> ##, (2 numbers -> ##)\n   - 2015 -> ####, (4 numbers -> ####)\n   - 200 -> ###, (3 numbers -> ###) \n   - 315135135 -> ##### (5 or more numbers -> #####)\n\nI saw this from another kaggle notebook that I'll try to link","metadata":{}},{"cell_type":"code","source":"def text_preproc_1(tweets):\n    '''Removes links and numbers and turns all text to lowercase.'''\n    \n    x = [ re.sub(r\"https?:(\\/\\/t\\.co\\/([A-Za-z0-9]|[A-Za-z]){10})\", \"\", tweet) for tweet in tweets]\n    x = [ word.lower() for word in x]\n    x = [ re.sub(r'[0-9]{5,}', '#####', word) for word in x ]\n    x = [ re.sub(r'[0-9]{4}', '####', word) for word in x ]\n    x = [ re.sub(r'[0-9]{3}', '###', word) for word in x ]\n    x = [ re.sub(r'[0-9]{2}', '##', word) for word in x ]\n    return x","metadata":{"execution":{"iopub.status.busy":"2021-07-19T10:45:31.328314Z","iopub.execute_input":"2021-07-19T10:45:31.328864Z","iopub.status.idle":"2021-07-19T10:45:31.337795Z","shell.execute_reply.started":"2021-07-19T10:45:31.328811Z","shell.execute_reply":"2021-07-19T10:45:31.336417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**When we applied our preprocessing, The number of words decreased tremendously and we got more coverage**\n\n**This happened as**\n- Each group of numbers (2 numbers, 3 numbers ...etc) are now represented as one\n  and are also covered in our embeddings\n- Links are all removed from our corpus as they are just noise that increased the size of the corpus (one could argue that they should be replaced with a token \\<LNK\\> for example but we don't have enough text to retrain the embeddings)\n- The words 'You' and 'you' were considered different words and the former wasn't counted towards the words that were in the embedding.\n","metadata":{}},{"cell_type":"code","source":"cleaned_tweets = text_preproc_1(text)\ncount = gen_count_matrix(cleaned_tweets)\nprint('Number of unique words in our corpus : ', len(count))\noov , known = check_coverage(count, twitter_vectors)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T10:45:31.340072Z","iopub.execute_input":"2021-07-19T10:45:31.340411Z","iopub.status.idle":"2021-07-19T10:50:02.958084Z","shell.execute_reply.started":"2021-07-19T10:45:31.340373Z","shell.execute_reply":"2021-07-19T10:50:02.957231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sort_dict_values(oov)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-07-19T10:50:02.959501Z","iopub.execute_input":"2021-07-19T10:50:02.959812Z","iopub.status.idle":"2021-07-19T10:50:03.011941Z","shell.execute_reply.started":"2021-07-19T10:50:02.95978Z","shell.execute_reply":"2021-07-19T10:50:03.010652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Contractions**\n**A lot of the out of vocabulary items are actually contractions**\n**Contraction** is something like how will not is written as won\\`t and while both are correct the word embedding doesn\\`t understand the latter","metadata":{}},{"cell_type":"code","source":"mappings = {\n        'twas': 'it was', \"it`s\": 'it is',\n        \"could`ve\": 'could have', \"it`ll\": 'it will',\n        \"they`ll\": 'they will', \"he`ll\": 'he will',\n        \"we`ll\": 'we will', \"i`m\": 'i am',\n        \"don`t\": 'do not', \"can`t\": 'can not',\n        'i`ll': 'i will', 'that`s': 'that is',\n        'didn`t': 'did not', 'i`ve': 'i have',\n        'won`t': 'will not', 'doesn`t': 'does not',\n        'he`s': 'he is', 'isn`t': 'is not',\n        'i`d': 'i would', 'haven`t': 'have not',\n        'we`re': 'we are', 'wasn`t': 'was not',\n        'she`s': 'she is', 'there`s': 'there is',\n        'couldn`t': 'could not','they`re': 'they are',\n        'what`s': 'what is', 'w/': 'with',\n        'you`ll': 'you will', 'we`ll': 'we will',\n        'aren`t': 'are not', 'you`ve': 'you have',\n        'wouldn`t': 'would not', 'let`s': 'let us',\n        'it`ll': 'it will', '2day': 'today',\n        'how`s': 'how is', 'b4': 'before',\n        'y`all': 'you all', '2nite': 'tonight',\n        'you`d': 'you would', 'ya`ll': 'you all',\n        'who`s': 'who is', 'hasn`t': 'has not',\n        'where`s': 'where is', 'here`s': 'here is',\n        'shouldn`t': 'should not', 'we`ve': 'we have',\n        'weren`t': 'were not', 'w/o': 'without',\n        '`cause': 'because', 'b/c': 'because',\n        '2moro': 'tomorrow', 'hadn`t': 'had not',\n        'he`ll': 'he will', 'we`d': 'we would',\n        'they`ve': 'they have', 'gr8': 'great',\n        'would`ve': 'would have', '2morrow': 'tomorrow',\n    }","metadata":{"execution":{"iopub.status.busy":"2021-07-19T10:50:03.014019Z","iopub.execute_input":"2021-07-19T10:50:03.014468Z","iopub.status.idle":"2021-07-19T10:50:03.025407Z","shell.execute_reply.started":"2021-07-19T10:50:03.014429Z","shell.execute_reply":"2021-07-19T10:50:03.023916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def text_preproc_2(tweets, mapping,map_contractions=False):\n    x = [ re.sub(r\"https?:(\\/\\/t\\.co\\/([A-Za-z0-9]|[A-Za-z]){10})\", \"\", tweet) for tweet in tweets]\n    x = [ tweet.lower() for tweet in x]\n    x = [ re.sub(r'[0-9]{5,}', '#####', word) for word in x ]\n    x = [ re.sub(r'[0-9]{4}', '####', word) for word in x ]\n    x = [ re.sub(r'[0-9]{3}', '###', word) for word in x ]\n    x = [ re.sub(r'[0-9]{2}', '##', word) for word in x ]\n    if(map_contractions):\n        for key, value in mapping.items():\n            x = [sentence.replace(key, value)  for sentence in x]\n    return x","metadata":{"execution":{"iopub.status.busy":"2021-07-19T10:50:03.026956Z","iopub.execute_input":"2021-07-19T10:50:03.027297Z","iopub.status.idle":"2021-07-19T10:50:03.042586Z","shell.execute_reply.started":"2021-07-19T10:50:03.027246Z","shell.execute_reply":"2021-07-19T10:50:03.041691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**We can see that the contractions improved our coverage by 3%**\n\nHowever, If you look at the competitions rules and submissions, you'll find that submitting 'do not' instead of 'don't' would make you lose points so, we will make the default to not do this and then if we need to improve the performance, we can work around it to revert the contractions at prediction time.","metadata":{}},{"cell_type":"code","source":"cleaned_tweets = text_preproc_2(text, mappings,True)\ncount = gen_count_matrix(cleaned_tweets)\nprint('Number of words in our corpus : ', len(count))\noov , known = check_coverage(count, twitter_vectors)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T10:50:03.044183Z","iopub.execute_input":"2021-07-19T10:50:03.044523Z","iopub.status.idle":"2021-07-19T10:54:33.329216Z","shell.execute_reply.started":"2021-07-19T10:50:03.044488Z","shell.execute_reply":"2021-07-19T10:54:33.328376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def check_tweets_containing(search_word, tweets, num_tweets=7):\n    num_printed = 0\n    for sentence in cleaned_tweets:\n        if num_printed == num_tweets:\n            break\n        for word in word_tokenize(sentence):\n            if word == search_word:\n                print(sentence)\n                num_printed += 1\n                continue","metadata":{"execution":{"iopub.status.busy":"2021-07-19T11:02:18.831618Z","iopub.execute_input":"2021-07-19T11:02:18.831917Z","iopub.status.idle":"2021-07-19T11:02:18.837759Z","shell.execute_reply.started":"2021-07-19T11:02:18.831885Z","shell.execute_reply":"2021-07-19T11:02:18.836791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sort_dict_values(oov)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-07-19T10:54:33.33726Z","iopub.execute_input":"2021-07-19T10:54:33.337687Z","iopub.status.idle":"2021-07-19T10:54:33.39513Z","shell.execute_reply.started":"2021-07-19T10:54:33.337637Z","shell.execute_reply":"2021-07-19T10:54:33.394421Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"check_tweets_containing('..', cleaned_tweets)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T11:02:18.839078Z","iopub.execute_input":"2021-07-19T11:02:18.83939Z","iopub.status.idle":"2021-07-19T11:02:24.821336Z","shell.execute_reply.started":"2021-07-19T11:02:18.839359Z","shell.execute_reply":"2021-07-19T11:02:24.820469Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"check_tweets_containing(\"'you\", cleaned_tweets)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T10:54:40.843641Z","iopub.execute_input":"2021-07-19T10:54:40.843955Z","iopub.status.idle":"2021-07-19T10:54:48.087522Z","shell.execute_reply.started":"2021-07-19T10:54:40.843921Z","shell.execute_reply":"2021-07-19T10:54:48.086177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Dots & Quotations**\n### **Dots**\n**You can see that a lot of the words are written normally but have two or three dots before/after them**\n\n### **Quotations**\nSome quotations are right next to the word which causes the words to look like the text above\n\n\nWe can try removing these dots and see how that affects our coverage","metadata":{}},{"cell_type":"code","source":"def text_preproc_3(tweets, mapping, map_contractions=False):\n    x = [ re.sub(r\"https?:(\\/\\/t\\.co\\/([A-Za-z0-9]|[A-Za-z]){10})\", \"\", tweet) for tweet in tweets]\n    x = [ tweet.lower() for tweet in x]\n    x = [ re.sub(r'[0-9]{5,}', '#####', word) for word in x ]\n    x = [ re.sub(r'[0-9]{4}', '####', word) for word in x ]\n    x = [ re.sub(r'[0-9]{3}', '###', word) for word in x ]\n    x = [ re.sub(r'[0-9]{2}', '##', word) for word in x ]\n    x = [ re.sub(r'[.]{1,}', '', word) for word in x ]    \n    if(map_contractions):\n        for key, value in mapping.items():\n            x = [sentence.replace(key, value)  for sentence in x]\n    x = [ word.replace(\"'\",'') for word in x ]\n    return x\n","metadata":{"execution":{"iopub.status.busy":"2021-07-19T10:54:48.090358Z","iopub.execute_input":"2021-07-19T10:54:48.090659Z","iopub.status.idle":"2021-07-19T10:54:48.100841Z","shell.execute_reply.started":"2021-07-19T10:54:48.090631Z","shell.execute_reply":"2021-07-19T10:54:48.099764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"They didn't have any significant improvement to our model so we will leave them out as unlike contractions there is no way to reverse them, so they might ruin our submissions","metadata":{}},{"cell_type":"code","source":"cleaned_tweets = text_preproc_3(text, mappings)\ncount = gen_count_matrix(cleaned_tweets)\nprint('Number of words in our corpus : ', len(count))\noov , known = check_coverage(count, twitter_vectors)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T10:54:48.102294Z","iopub.execute_input":"2021-07-19T10:54:48.102675Z","iopub.status.idle":"2021-07-19T10:59:16.0583Z","shell.execute_reply.started":"2021-07-19T10:54:48.10264Z","shell.execute_reply":"2021-07-19T10:59:16.056964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sort_dict_values(oov)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-07-19T10:59:16.059825Z","iopub.execute_input":"2021-07-19T10:59:16.060133Z","iopub.status.idle":"2021-07-19T10:59:16.110375Z","shell.execute_reply.started":"2021-07-19T10:59:16.0601Z","shell.execute_reply":"2021-07-19T10:59:16.109184Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Final text processing function**\n\nThis is the same as the function text_preproc_2","metadata":{}},{"cell_type":"code","source":"def text_preproc(tweets, mapping,map_contractions=False):\n    x = [ re.sub(r\"https?:(\\/\\/t\\.co\\/([A-Za-z0-9]|[A-Za-z]){10})\", \"\", tweet) for tweet in tweets]\n    x = [ tweet.lower() for tweet in x]\n    x = [ re.sub(r'[0-9]{5,}', '#####', word) for word in x ]\n    x = [ re.sub(r'[0-9]{4}', '####', word) for word in x ]\n    x = [ re.sub(r'[0-9]{3}', '###', word) for word in x ]\n    x = [ re.sub(r'[0-9]{2}', '##', word) for word in x ]\n    if(map_contractions):\n        for key, value in mapping.items():\n            x = [sentence.replace(key, value)  for sentence in x]\n    return x","metadata":{"execution":{"iopub.status.busy":"2021-07-19T10:59:16.113608Z","iopub.execute_input":"2021-07-19T10:59:16.11399Z","iopub.status.idle":"2021-07-19T10:59:16.12303Z","shell.execute_reply.started":"2021-07-19T10:59:16.113955Z","shell.execute_reply":"2021-07-19T10:59:16.121352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test Data","metadata":{}},{"cell_type":"markdown","source":"Here we will test the preprocessing that we made on the testset, for the first time to make sure we are not overfitting the train dataset.","metadata":{}},{"cell_type":"code","source":"test_df = pd.read_csv('../input/tweet-sentiment-extraction/test.csv')\ntest_text = [sentence.strip() for sentence in test_df['text']]","metadata":{"execution":{"iopub.status.busy":"2021-07-19T10:59:16.125053Z","iopub.execute_input":"2021-07-19T10:59:16.125673Z","iopub.status.idle":"2021-07-19T10:59:16.162621Z","shell.execute_reply.started":"2021-07-19T10:59:16.125617Z","shell.execute_reply":"2021-07-19T10:59:16.161808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**We applied the check coverage function 3 times**\n1. **No preprocessing**\n   - We had 55.3% of words in corpus are also in embedding\n   - We had 80.59% of words in corpus are also in embedding\n2. **Preprocessing (no contractions)**\n   - We had 79.37% of words in corpus are also in embedding\n   - We had 92.75% of words in corpus are also in embedding\n3. **Preprocessing (with contractions)**\n   - We had 79.99% of words in corpus are also in embedding\n   - We had 95.11% of words in corpus are also in embedding","metadata":{}},{"cell_type":"markdown","source":"We can see that these results are very similar to our training set which means we didn't overfit the train data.\n\nHowever, This data came from twitter and due to the character limit people write a certain way so this may not work as well on text from other places.","metadata":{}},{"cell_type":"code","source":"count = gen_count_matrix(test_text)\nprint('Number of unique words in our corpus : ', len(count))\noov , known = check_coverage(count, twitter_vectors)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T10:59:16.163788Z","iopub.execute_input":"2021-07-19T10:59:16.16425Z","iopub.status.idle":"2021-07-19T11:00:52.79549Z","shell.execute_reply.started":"2021-07-19T10:59:16.164208Z","shell.execute_reply":"2021-07-19T11:00:52.793629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_cleaned_tweets = text_preproc(test_text,mappings)\ncount = gen_count_matrix(test_cleaned_tweets)\nprint('Number of unique words in our corpus : ', len(count))\noov , known = check_coverage(count, twitter_vectors)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T11:00:52.796856Z","iopub.execute_input":"2021-07-19T11:00:52.797137Z","iopub.status.idle":"2021-07-19T11:01:36.427038Z","shell.execute_reply.started":"2021-07-19T11:00:52.79711Z","shell.execute_reply":"2021-07-19T11:01:36.426044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_cleaned_tweets = text_preproc(test_text,mappings, map_contractions=True)\ncount = gen_count_matrix(test_cleaned_tweets)\nprint('Number of unique words in our corpus : ', len(count))\noov , known = check_coverage(count, twitter_vectors)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T11:01:36.428522Z","iopub.execute_input":"2021-07-19T11:01:36.428839Z","iopub.status.idle":"2021-07-19T11:02:18.829003Z","shell.execute_reply.started":"2021-07-19T11:01:36.428805Z","shell.execute_reply":"2021-07-19T11:02:18.8282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}