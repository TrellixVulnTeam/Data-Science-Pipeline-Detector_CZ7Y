{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction"},{"metadata":{},"cell_type":"markdown","source":"## Problem Statement\nWith all of the tweets circulating every second it is hard to tell whether the sentiment behind a specific tweet will impact a company, or a person's, brand for being viral (positive), or devastate profit because it strikes a negative tone. Capturing sentiment in language is important in these times where decisions and reactions are created and updated in seconds. But, which words actually lead to the sentiment description? In this competition you will need to pick out the part of the tweet (word or phrase) that reflects the sentiment.\n**The objective in this competition is to construct a model that can do the same - look at the labeled sentiment for a given tweet and figure out what word or phrase best supports it.**\n\nE.g. given a statement \"**Sooo SAD I will miss you here in San Diego!!!**\" and given that the sentiment of the statement is \"**negative**\", we need to find that \"**Sooo SAD**\" is the phrase that best supports the negative sentiment\n\n![](https://www.kdnuggets.com/images/sentiment-fig-1-689.jpg)"},{"metadata":{},"cell_type":"markdown","source":"## Data Provided\nWe have the following files:\n* train.csv - the training set\n* test.csv - the test set\n* sample_submission.csv - a sample submission file in the correct format"},{"metadata":{},"cell_type":"markdown","source":"## Evaluation Metric\nThe metric in this competition is the word-level Jaccard score. \n\nE.g.\n\nSentence 1: AI is our friend and it has been friendly\n\nSentence 2: AI and humans have always been friendly\n\nIn order to calculate similarity using Jaccard similarity, we will first perform lemmatization to reduce words to the same root word. In our case, ‚Äúfriend‚Äù and ‚Äúfriendly‚Äù will both become ‚Äúfriend‚Äù, ‚Äúhas‚Äù and ‚Äúhave‚Äù will both become ‚Äúhas‚Äù.\n\nA ‚ãÇ B = ( AI, has, been, friend, and)\n\nA ‚ãÉ B = ( AI, has, been, friend, and, our, is, it, human, always)\n\nFor the above two sentences, we get Jaccard similarity of 5/(5+3+2) = 0.5 \n\nA Python implementation is given below:\n```\ndef jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))\n```"},{"metadata":{},"cell_type":"markdown","source":"## Content\n1. Loading Necessary Libraries\n2. Reading the dataset\n3. Summary Statistics\n4. Exploring Jaccard similarity \n5. Analysing Most preferred n-grams"},{"metadata":{},"cell_type":"markdown","source":"If you find this kernel useful,consider giving an upvote üôè"},{"metadata":{},"cell_type":"markdown","source":"# 1. Loading Necessary Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"# !pip install pyspellchecker","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n#For plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# text processing libraries\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\n# from spellchecker import SpellChecker\n\n# sklearn \nfrom sklearn import model_selection\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2.Reading the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/tweet-sentiment-extraction/train.csv\")\ntest_data = pd.read_csv(\"/kaggle/input/tweet-sentiment-extraction/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Summary Statistics"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The train data has {} rows and {} columns\".format(train_data.shape[0],train_data.shape[1]))\nprint(\"The test data has {} rows and {} columns\".format(test_data.shape[0],test_data.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have 1 missing text in training data and there is no point in keeping that. So, we will remove that. Test data has no missing values."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.dropna(inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#There are in total 27K training rows\ntrain_data.sentiment.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#There are in total 3.5K test rows\ntest_data.sentiment.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4))\n\npd.value_counts(train_data['sentiment']).plot(kind = 'pie', ax=ax1,autopct='%1.1f%%')\npd.value_counts(test_data['sentiment']).plot(kind ='pie', ax=ax2,autopct='%1.1f%%')\nax1.set_title(\"Train Data\")\nax2.set_title(\"Test Data\")\nplt.suptitle(\"Percentage Distribution of sentiments across train and test data\",fontweight = \"bold\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that the test data has exactly the same distribution as the train data which was shocking to me but I rechecked it  \tü§™.\nNeutral tweets are the highest (~41%), followed by positive tweets(~31%) and the lowest is negative tweets (~28%)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Text Length\ntrain_data[\"text_len\"] = train_data[\"text\"].astype(str).apply(len)\ntest_data[\"text_len\"] = test_data[\"text\"].astype(str).apply(len)\ntrain_data[\"selected_text_len\"] = train_data[\"selected_text\"].astype(str).apply(len)\n\n#Word Count (before preprocessing)\ntrain_data[\"text_wc\"] = train_data[\"text\"].apply(lambda x: len(str(x).split()))\ntest_data[\"text_wc\"] = test_data[\"text\"].apply(lambda x: len(str(x).split()))\ntrain_data[\"selected_text_wc\"] = train_data[\"selected_text\"].apply(lambda x: len(str(x).split()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 4))\nsns.kdeplot(train_data.text_len, shade=True, ax= ax1)\nsns.kdeplot(test_data.text_len, shade=True, ax = ax1)\nsns.kdeplot(train_data.text_wc, shade=True, ax= ax2)\nsns.kdeplot(test_data.text_wc, shade=True, ax = ax2)\nax1.set_title(\"Text Length\")\nax2.set_title(\"Text Word Count\")\nax1.legend(['Train Data','Test Data'])\nax2.legend(['Train Data','Test Data'])\nplt.suptitle(\"Comparison of text lengths and word counts of the train and test data \",fontweight = \"bold\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What we see:\n* We see that the train data and test data have a similar length and word count distribution meaning that they are carefully distributed\n* The Text length of both train and test data tends to show a bi-modal distribution rather than a proper normal distribution meaning it has two peaks (one at 40 letters and one at 125 letters).However the lower peak is higher meaning the texts are generally smaller \n* The word count of train and test data shows lesser bimodal behaviour than text length and the most words are between 8 to 10 words count bucket"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (ax) = plt.subplots(3, 2, figsize=(10, 10))\nsns.kdeplot(train_data[train_data.sentiment==\"neutral\"].text_len, shade=True, ax= ax[0][0])\nsns.kdeplot(test_data[test_data.sentiment==\"neutral\"].text_len, shade=True, ax = ax[0][0])\nsns.kdeplot(train_data[train_data.sentiment==\"neutral\"].text_wc, shade=True, ax= ax[0][1])\nsns.kdeplot(test_data[test_data.sentiment==\"neutral\"].text_wc, shade=True, ax = ax[0][1])\nax[0][0].set_title(\"Neutral Text Length\")\nax[0][1].set_title(\"Neutral Text Word Count\")\nax[0][0].legend(['Train Data','Test Data'])\nax[0][1].legend(['Train Data','Test Data'])\n\nsns.kdeplot(train_data[train_data.sentiment==\"positive\"].text_len, shade=True, ax= ax[1][0])\nsns.kdeplot(test_data[test_data.sentiment==\"positive\"].text_len, shade=True, ax = ax[1][0])\nsns.kdeplot(train_data[train_data.sentiment==\"positive\"].text_wc, shade=True, ax= ax[1][1])\nsns.kdeplot(test_data[test_data.sentiment==\"positive\"].text_wc, shade=True, ax = ax[1][1])\nax[1][0].set_title(\"Positive Text Length\")\nax[1][1].set_title(\"Positive Text Word Count\")\nax[1][0].legend(['Train Data','Test Data'])\nax[1][1].legend(['Train Data','Test Data'])\n\nsns.kdeplot(train_data[train_data.sentiment==\"negative\"].text_len, shade=True, ax= ax[2][0])\nsns.kdeplot(test_data[test_data.sentiment==\"negative\"].text_len, shade=True, ax = ax[2][0])\nsns.kdeplot(train_data[train_data.sentiment==\"negative\"].text_wc, shade=True, ax= ax[2][1])\nsns.kdeplot(test_data[test_data.sentiment==\"negative\"].text_wc, shade=True, ax = ax[2][1])\nax[2][0].set_title(\"Negative Text Length\")\nax[2][1].set_title(\"Negative Text Word Count\")\nax[2][0].legend(['Train Data','Test Data'])\nax[2][1].legend(['Train Data','Test Data'])\n\nplt.suptitle(\"Comparison of text lengths and word counts of the train and test data for each sentiment\",fontweight = \"bold\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What we get:\n* This graph takes the graph above it of overall train and test comparison of text length and word count a step forward and we see the comparison accross sentiments and we actually more differences here\n* Negative sentences show a lesser bimodal behaviour in test data in both length and word count \n* Negative and Positive sentences in test data tend to show lesser peak in the higher bucket than train data. Opposite is the case of neutral text"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 4))\nsns.kdeplot(train_data[train_data.sentiment==\"positive\"].text_len, shade=True,color = \"g\", ax= ax1)\nsns.kdeplot(train_data[train_data.sentiment==\"negative\"].text_len, shade=True,color = \"r\", ax= ax1)\nsns.kdeplot(train_data[train_data.sentiment==\"neutral\"].text_len, shade=True,color = \"y\", ax= ax1)\nsns.kdeplot(train_data[train_data.sentiment==\"positive\"].selected_text_len, shade=True,color = \"g\", ax= ax2)\nsns.kdeplot(train_data[train_data.sentiment==\"negative\"].selected_text_len, shade=True,color = \"r\", ax= ax2)\nsns.kdeplot(train_data[train_data.sentiment==\"neutral\"].selected_text_len, shade=True, color = \"y\", ax= ax2)\nax1.set_title(\"Text lengths\")\nax2.set_title(\"Selected text lengths\")\nax1.legend(['positive','negative','neutral'])\nax2.legend(['positive','negative','neutral'])\nplt.suptitle(\"Comparison of length of different sentiments for actual text and selected text\",fontweight = \"bold\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What we get:\n* For actual texts, neutral sentences have more shorter sentences than positive and negative sentences. The second peak towards the higher lengths is the most for positive sentences followed by negative and neutral sentences in that order.\n* The selected text graph is the more interesting one and opposite to that of the actual texts, the positive sentences have the highest peak towards shorter lengths followed by negative sentences. Neutral sentences again shows a bimodal distribution again."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (ax1, ax2,ax3) = plt.subplots(1, 3, figsize=(16, 4))\nsns.kdeplot(train_data[train_data.sentiment==\"positive\"].text_len, shade=True,color = \"g\", ax= ax1)\nsns.kdeplot(train_data[train_data.sentiment==\"positive\"].selected_text_len, shade=True,color = \"b\", ax= ax1)\n\nsns.kdeplot(train_data[train_data.sentiment==\"negative\"].text_len, shade=True,color = \"r\", ax= ax2)\nsns.kdeplot(train_data[train_data.sentiment==\"negative\"].selected_text_len, shade=True,color = \"m\", ax= ax2)\n\nsns.kdeplot(train_data[train_data.sentiment==\"neutral\"].text_len, shade=True,color = \"y\", ax= ax3)\nsns.kdeplot(train_data[train_data.sentiment==\"neutral\"].selected_text_len, shade=True, color = \"c\", ax= ax3)\n\nax1.set_title(\"positive\")\nax2.set_title(\"negative\")\nax3.set_title(\"neutral\")\nplt.suptitle(\"Comparison of text lengths and selected text lengths across different sentiments\",fontweight = \"bold\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What we get:\n* Positive and negative show the same behaviour as earlier as selected sentences tend be smaller and actual texts tend to vary from small to long sentences\n* Neutral sentences have almost the same distribution for actual text lengths and selected text length, which is also verified by this\n[discussion](https://www.kaggle.com/c/tweet-sentiment-extraction/discussion/138520) where the topic author says \"Almost 97 % Jaccard Similarity in train data \"text\" and \"selectedtext\". In conclusion maybe we can use neutral \"text\" as it is for \"selectedtext\" in test data submission.\" which is good news as we already saw that 41% of dataset is neutral. Also, we need to surely check this"},{"metadata":{},"cell_type":"markdown","source":"# 4. Exploring Jaccard similarity"},{"metadata":{"trusted":true},"cell_type":"code","source":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['jaccard_actual_selected'] = train_data.apply(lambda row:jaccard(row[\"text\"],row[\"selected_text\"]),axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.groupby('sentiment')['jaccard_actual_selected'].mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What we get:\n* We can replicate the same as the discussion, ~98% of the neutral sentences have the same sentence as their actual sentence and selected sentences\n* Also we have some of the positive and negative sentences as same actual and selected sentences, probably that would be texts of 1-2 words"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,(ax1,ax2,ax3)=plt.subplots(1,3,figsize=(15,3))\nsns.distplot(train_data.loc[train_data['sentiment']==\"negative\",\"jaccard_actual_selected\"],kde = False,ax=ax1)\nsns.distplot(train_data.loc[train_data['sentiment']==\"neutral\",\"jaccard_actual_selected\"],kde = False,ax=ax2)\nsns.distplot(train_data.loc[train_data['sentiment']==\"positive\",\"jaccard_actual_selected\"],kde = False,ax=ax3)\nax1.set_title(\"Negative\")\nax2.set_title(\"Neutral\")\nax3.set_title(\"Positive\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What we see:\n* For Negative texts, the jaccard similarity is either at the lower side 0.1 - 0.2 which mean 1 out of 5 words from the text is in the selected text or at the higher side all the words from the text is in the selected text. We have a similar trend in Positive tweets.\n* For Neutral texts, most of the sentences are same for the actual text and selected text. "},{"metadata":{},"cell_type":"markdown","source":"# 5. Analysing Most preferred n-grams"},{"metadata":{},"cell_type":"markdown","source":"Following is an example of N-Grams where we look at n=1 (unigram or one word), n = 2 (bigram or two words) and n = 3 (trigram pr three words)\n![N-Grams](https://devopedia.org/images/article/219/7356.1569499094.png)\nSo, in this part we look at most commonly occuring unigrams, bigrams and trigrams in the train and test dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# spell = SpellChecker()\n# def correct_spellings(text):\n#     corrected_text = []\n#     misspelled_words = spell.unknown(text.split())\n#     for word in text.split():\n#         if word in misspelled_words:\n#             corrected_text.append(spell.correction(word))\n#         else:\n#             corrected_text.append(word)\n#     return \" \".join(corrected_text)\n        \n# text = \"corect me plese\"\n# correct_spellings(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# text preprocessing helper functions\n\ndef clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n\n\ndef text_preprocessing(text):\n    \"\"\"\n    Cleaning and parsing the text.\n\n    \"\"\"\n    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n    nopunc = clean_text(text)\n    tokenized_text = tokenizer.tokenize(nopunc)\n    #remove_stopwords = [w for w in tokenized_text if w not in stopwords.words('english')]\n    combined_text = ' '.join(tokenized_text)\n    return combined_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applying the cleaning function to both test and training datasets\ntrain_data['text_clean'] = train_data['text'].apply(str).apply(lambda x: text_preprocessing(x))\ntest_data['text_clean'] = test_data['text'].apply(str).apply(lambda x: text_preprocessing(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#source of code : https://medium.com/@cristhianboujon/how-to-list-the-most-common-words-from-text-corpus-using-scikit-learn-dad4d0cab41d\ndef get_top_n_gram(corpus,ngram_range,n):\n    vec = CountVectorizer(ngram_range=ngram_range,stop_words = 'english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pos_unigrams = get_top_n_gram(train_data.loc[train_data['sentiment']==\"positive\",'text_clean'],(1,1),20)\nneg_unigrams = get_top_n_gram(train_data.loc[train_data['sentiment']==\"negative\",'text_clean'],(1,1),20)\nneutral_unigrams = get_top_n_gram(train_data.loc[train_data['sentiment']==\"neutral\",'text_clean'],(1,1),20)\n\npos_bigrams = get_top_n_gram(train_data.loc[train_data['sentiment']==\"positive\",'text_clean'],(2,2),20)\nneg_bigrams = get_top_n_gram(train_data.loc[train_data['sentiment']==\"negative\",'text_clean'],(2,2),20)\nneutral_bigrams = get_top_n_gram(train_data.loc[train_data['sentiment']==\"neutral\",'text_clean'],(2,2),20)\n\npos_trigrams = get_top_n_gram(train_data.loc[train_data['sentiment']==\"positive\",'text_clean'],(3,3),20)\nneg_trigrams = get_top_n_gram(train_data.loc[train_data['sentiment']==\"negative\",'text_clean'],(3,3),20)\nneutral_trigrams = get_top_n_gram(train_data.loc[train_data['sentiment']==\"neutral\",'text_clean'],(3,3),20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_pos_unigram = pd.DataFrame(pos_unigrams,columns = [\"word\",\"count\"])\ndf_neg_unigram = pd.DataFrame(neg_unigrams,columns = [\"word\",\"count\"])\ndf_neutral_unigram = pd.DataFrame(neutral_unigrams,columns = [\"word\",\"count\"])\n\ndf_pos_bigrams = pd.DataFrame(pos_bigrams,columns = [\"words\",\"count\"])\ndf_neg_bigrams = pd.DataFrame(neg_bigrams,columns = [\"words\",\"count\"])\ndf_neutral_bigrams = pd.DataFrame(neutral_bigrams,columns = [\"words\",\"count\"])\n\ndf_pos_trigrams = pd.DataFrame(pos_trigrams,columns = [\"words\",\"count\"])\ndf_neg_trigrams = pd.DataFrame(neg_trigrams,columns = [\"words\",\"count\"])\ndf_neutral_trigrams = pd.DataFrame(neutral_trigrams,columns = [\"words\",\"count\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,(ax1,ax2,ax3)=plt.subplots(1,3,figsize=(15,5))\nsns.barplot(x=\"count\",y = \"word\",data = df_neutral_unigram,ax=ax1,color = \"yellow\")\nsns.barplot(x=\"count\",y = \"words\",data = df_neutral_bigrams,ax=ax2,color = \"gold\")\nsns.barplot(x=\"count\",y = \"words\",data = df_neutral_trigrams,ax=ax3,color = \"goldenrod\")\nax1.set_title(\"Uni-gram\")\nax2.set_title(\"Bi-gram\")\nax3.set_title(\"Tri-gram\")\nplt.suptitle(\"Most preferred N-grams in Neutral tweets\",fontweight = \"bold\")\nplt.tight_layout()\nplt.subplots_adjust(top=0.85)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,(ax1,ax2,ax3)=plt.subplots(1,3,figsize=(15,5))\nsns.barplot(x=\"count\",y = \"word\",data = df_neg_unigram,ax=ax1,color = \"tomato\")\nsns.barplot(x=\"count\",y = \"words\",data = df_neg_bigrams,ax=ax2,color = \"red\")\nsns.barplot(x=\"count\",y = \"words\",data = df_neg_trigrams,ax=ax3,color = \"maroon\")\nax1.set_title(\"Uni-gram\")\nax2.set_title(\"Bi-gram\")\nax3.set_title(\"Tri-gram\")\nplt.suptitle(\"Most preferred N-grams in Negative tweets\",fontweight = \"bold\")\nplt.tight_layout()\nplt.subplots_adjust(top=0.85)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,(ax1,ax2,ax3)=plt.subplots(1,3,figsize=(15,5))\nsns.barplot(x=\"count\",y = \"word\",data = df_pos_unigram,ax=ax1,color = \"lime\")\nsns.barplot(x=\"count\",y = \"words\",data = df_pos_bigrams,ax=ax2,color = \"green\")\nsns.barplot(x=\"count\",y = \"words\",data = df_pos_trigrams,ax=ax3,color = \"darkgreen\")\nax1.set_title(\"Uni-gram\")\nax2.set_title(\"Bi-gram\")\nax3.set_title(\"Tri-gram\")\nplt.suptitle(\"Most preferred N-grams in Positive tweets\",fontweight = \"bold\")\nplt.tight_layout()\nplt.subplots_adjust(top=0.85)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Highly motivated by :\n* [https://www.kaggle.com/parulpandey/eda-and-preprocessing-for-bert](https://www.kaggle.com/parulpandey/eda-and-preprocessing-for-bert)\n* [https://www.kaggle.com/tanulsingh077/twitter-sentiment-extaction-analysis-eda-and-model](https://www.kaggle.com/tanulsingh077/twitter-sentiment-extaction-analysis-eda-and-model)\n* [https://www.kaggle.com/shahules/complete-eda-baseline-model-0-708-lb](https://www.kaggle.com/shahules/complete-eda-baseline-model-0-708-lb)"},{"metadata":{},"cell_type":"markdown","source":"*Do an upvote if you think this was helpful* üò¨"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}