{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import argparse\nimport json\nimport os\nimport random\nimport re\nimport shutil\nfrom collections import OrderedDict, defaultdict\nfrom functools import partial\nfrom pathlib import Path\nfrom typing import Dict\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport tqdm\n\n\nfrom sklearn.model_selection import GroupKFold\nfrom torch import nn\nfrom torch.nn import functional as F\n\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\nfrom transformers import RobertaConfig, RobertaModel, RobertaTokenizer, AutoConfig, AutoModel, AutoTokenizer\nfrom transformers.optimization import (AdamW, get_cosine_schedule_with_warmup,\n                                       get_linear_schedule_with_warmup,\n                                       get_cosine_with_hard_restarts_schedule_with_warmup)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from shutil import copyfile\ncopyfile(src = \"../input/utils-v10/utilsv10.py\", dst = \"../working/utilsv10.py\")\ncopyfile(src = \"../input/utils-v10/dataset10.py\", dst = \"../working/dataset10.py\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from utilsv10 import (binary_focal_loss, get_learning_rate, jaccard_list, get_best_pred, ensemble, ensemble_words,get_char_prob,\n                   load_model, save_model, set_seed, write_event, evaluate, get_predicts_from_token_logits)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from dataset10 import TrainDataset, MyCollator","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Parse data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('../input/tweet-sentiment-extraction/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained('../input/roberta-base/', do_lower_case=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Args:\n    post = True\n    tokenizer = tokenizer\n    offset = 4\n    batch_size = 32\n    workers = 1\nargs = Args()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"collator = MyCollator()\ntest_set = TrainDataset(test, None, tokenizer=tokenizer, mode='test', offset=args.offset)\ntest_loader = DataLoader(test_set, batch_size=args.batch_size, shuffle=False, collate_fn=collator,\n                                 num_workers=args.workers)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class TweetModel(nn.Module):\n\n    def __init__(self, pretrain_path=None, dropout=0.2, config=None):\n        super(TweetModel, self).__init__()\n        if config is not None:\n            self.bert = AutoModel.from_config(config)\n        else:\n            config = AutoConfig.from_pretrained(pretrain_path, output_hidden_states=True)\n            self.bert = AutoModel.from_pretrained(\n                pretrain_path, cache_dir=None, config=config)\n        \n        self.cnn =  nn.Conv1d(self.bert.config.hidden_size*3, self.bert.config.hidden_size, 3, padding=1)\n\n        # self.rnn = nn.LSTM(self.bert.config.hidden_size, self.bert.config.hidden_size//2, num_layers=2,\n        #                     batch_first=True, bidirectional=True)\n        self.gelu = nn.GELU()\n\n        self.whole_head = nn.Sequential(OrderedDict([\n            ('dropout', nn.Dropout(0.1)),\n            ('l1', nn.Linear(self.bert.config.hidden_size*3, 256)),\n            ('act1', nn.GELU()),\n            ('dropout', nn.Dropout(0.1)),\n            ('l2', nn.Linear(256, 2))\n        ]))\n        self.se_head = nn.Linear(self.bert.config.hidden_size, 2)\n        self.inst_head = nn.Linear(self.bert.config.hidden_size, 2)\n        self.dropout = nn.Dropout(0.1)\n\n    def forward(self, inputs, masks, token_type_ids=None, input_emb=None):\n        _, pooled_output, hs = self.bert(\n            inputs, masks, token_type_ids=token_type_ids, inputs_embeds=input_emb)\n\n        seq_output = torch.cat([hs[-1],hs[-2],hs[-3]], dim=-1)\n\n        # seq_output = hs[-1]\n\n        avg_output = torch.sum(seq_output*masks.unsqueeze(-1), dim=1, keepdim=False)\n        avg_output = avg_output/torch.sum(masks, dim=-1, keepdim=True)\n        # +max_output\n        whole_out = self.whole_head(avg_output)\n\n        seq_output = self.gelu(self.cnn(seq_output.permute(0,2,1)).permute(0,2,1))\n        \n        se_out = self.se_head(self.dropout(seq_output))  #()\n        inst_out = self.inst_head(self.dropout(seq_output))\n        return whole_out, se_out[:, :, 0], se_out[:, :, 1], inst_out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(model: nn.Module, valid_df, valid_loader, args, progress=False) -> Dict[str, float]:\n    # run_root = Path('../experiments/' + args.run_root)\n    model.eval()\n    all_end_pred, all_whole_pred, all_start_pred, all_inst_out = [], [], [], []\n    if progress:\n        tq = tqdm.tqdm(total=len(valid_df))\n    with torch.no_grad():\n        for tokens, types, masks, _, _, _, _, _, _, _ in valid_loader:\n            if progress:\n                batch_size = tokens.size(0)\n                tq.update(batch_size)\n            masks = masks.cuda()\n            tokens = tokens.cuda()\n            types = types.cuda()\n            whole_out, start_out, end_out, inst_out = model(tokens, masks, types)\n            \n            all_whole_pred.append(torch.softmax(whole_out, dim=-1)[:,1].cpu().numpy())\n            inst_out = torch.softmax(inst_out, dim=-1)\n            for idx in range(len(start_out)):\n                length = torch.sum(masks[idx,:]).item()-1 # -1 for last token\n                all_start_pred.append(torch.softmax(start_out[idx, args.offset:length], axis=-1).cpu())\n                all_end_pred.append(torch.softmax(end_out[idx, args.offset:length], axis=-1).cpu())\n                all_inst_out.append(inst_out[idx,:,1].cpu())\n            assert all_start_pred[-1].dim()==1\n\n    all_whole_pred = np.concatenate(all_whole_pred)\n    \n    if progress:\n        tq.close()\n    return all_whole_pred, all_start_pred, all_end_pred, all_inst_out","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## predict","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"config = RobertaConfig.from_pretrained('../input/roberta-base', output_hidden_states=True)\nmodel = TweetModel(config=config)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_whole_preds, all_start_preds, all_end_preds, all_inst_preds = [], [], [], []\n\n    \nfor fold in range(10):\n    load_model(model, '../input/roberta-v10-10/best-model-%d.pt' % fold)\n    model.cuda()\n    fold_whole_preds, fold_start_preds, fold_end_preds, fold_inst_preds = predict(model, test, test_loader, args, progress=True)\n\n    all_whole_preds.append(fold_whole_preds)\n    all_start_preds.append(fold_start_preds)\n    all_end_preds.append(fold_end_preds)\n    all_inst_preds.append(fold_inst_preds)\n\n\nall_whole_preds, all_start_preds, all_end_preds, all_inst_preds = ensemble(all_whole_preds, all_start_preds, all_end_preds, all_inst_preds, test)\nword_preds, inst_word_preds, scores = get_predicts_from_token_logits(all_whole_preds, all_start_preds, all_end_preds, all_inst_preds, test, args)\n# word_preds, inst_word_preds, scores = get_predicts_from_token_logits(fold_whole_preds, fold_start_preds, fold_end_preds, fold_inst_preds, test, args)\nstart_char_prob, end_char_prob = get_char_prob(all_start_preds, all_end_preds, test, args)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['start_char_prob'] = start_char_prob\ntest['end_char_prob'] = end_char_prob","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['selected_text'] = word_preds\n# test.loc[replace_idx, 'selected_text'] = test.loc[replace_idx, 'text']\ndef f(selected):\n    return \" \".join(set(selected.lower().split()))\n# test.selected_text = test.selected_text.map(f)\ntest[['textID','selected_text']].to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}