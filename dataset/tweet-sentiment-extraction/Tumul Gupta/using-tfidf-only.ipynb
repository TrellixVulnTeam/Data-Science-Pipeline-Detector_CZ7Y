{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os, re, string\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\nsample = pd.read_csv(\"/kaggle/input/tweet-sentiment-extraction/sample_submission.csv\")\ntrain_data = pd.read_csv(\"/kaggle/input/tweet-sentiment-extraction/train.csv\")\ntest_data = pd.read_csv(\"/kaggle/input/tweet-sentiment-extraction/test.csv\")\n\ntrain_pos=train_data.loc[train_data['sentiment'] == 'positive']\ntrain_neg=train_data.loc[train_data['sentiment'] == 'negative']\ntrain_nut=train_data.loc[train_data['sentiment'] == 'neutral']\ntrain_pos.shape[0]+train_neg.shape[0] + train_nut.shape[0],train_data.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def clean(text):\n    for i in range(2):\n        #Make text lowercase, remove text in square brackets,remove links,remove punctuation\n        #and remove words containing numbers.\n        text = str(text).lower()\n        text = re.sub('\\[.*?\\]', '', text)\n        text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n        text = re.sub('<.*?>+', '', text)\n        text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n        text = re.sub('\\n', '', text)\n        text = re.sub('\\w*\\d\\w*', '', text)\n        return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grouped_cleaned=[clean(train_pos['text'].str.cat(sep=' ')),\n          clean(train_neg['text'].str.cat(sep=' ')),\n          clean(train_nut['text'].str.cat(sep=' '))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialize CountVectorize\ncv=CountVectorizer(ngram_range=(1, 3),stop_words='english')\ncv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# CountVectorize fit (apply CountVectorize to the documents) assign a number to each word in alphabetical order\nfitted_words=cv.fit(grouped_cleaned)\nfitted_words.vocabulary_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# count the number of instances of each word in each document\ncounted_fitted_words=cv.transform(grouped_cleaned)\npd.DataFrame(counted_fitted_words.todense(),columns=cv.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialise the tfidfTransformer\ntfidf=TfidfTransformer()\ntfidf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#find idf values for each word\nidf_words=tfidf.fit(counted_fitted_words)\npd.DataFrame(idf_words.idf_,index=cv.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find the TF IDF for each word in all the documents\nTFidf_words=tfidf.transform(counted_fitted_words)\ndf_TFidf_words=pd.DataFrame(TFidf_words.todense(),columns=cv.get_feature_names())\ndf_TFidf_words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(clean(test_data['text']))\ncleaned_test_data=test_data.copy()\nfor row in range(len(test_data)):\n    cleaned_test_data['text'][row]=clean(test_data['text'][row])\ncleaned_test_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission=cleaned_test_data.copy()\nsubmission.insert(2,'selected_text',['']*len(test_data))\nsubmission.head()\n\nfor row in range(len(submission)):\n    cv_row=CountVectorizer(ngram_range=(1, 3))\n    try:\n        cv_row_count=cv_row.fit([submission['text'][row]]) #or use .iloc[row,col] col 0 = ID, 1 = text, 2 = sentiment\n        ngrams_row = list(cv_row_count.vocabulary_)\n        tfidf_word_score=df_TFidf_words.reindex(columns=ngrams_row,fill_value=0)\n        if submission['sentiment'][row]=='neutral':    \n            submission['selected_text'][row]=submission['text'][row]  #neutral is 3rd (2) row in grouped\n\n        elif submission['sentiment'][row]=='positive':\n            submission['selected_text'][row]=tfidf_word_score.loc[0,:].idxmax(axis=0) #positive is 1st row (0) in grouped\n        elif submission['sentiment'][row]=='negative':\n            submission['selected_text'][row]=tfidf_word_score.loc[1,:].idxmax(axis=0) #negative is 2nd row (1) in grouped\n    except:\n        print('row ',row,' broken')\nsubmission\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', columns=['textID','selected_text'], index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}