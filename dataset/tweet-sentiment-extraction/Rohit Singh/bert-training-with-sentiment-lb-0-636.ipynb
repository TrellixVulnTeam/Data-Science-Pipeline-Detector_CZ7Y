{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Note: If you will train this model for 10 EPOCHs, you will easily get a LB: 0.636"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport tokenizers\nimport string\nimport torch\nimport transformers\nimport torch.nn as nn\nfrom tqdm import tqdm\nimport re\n\nfrom sklearn import model_selection\nfrom sklearn import metrics\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1. BERT Config Code"},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN = 128\nTRAIN_BATCH_SIZE = 40\nVALID_BATCH_SIZE = 16\nEPOCHS = 2\n\n\nBERT_PATH = \"../input/bert-base-uncased/\" # running from run.sh\n\n# MODEL_PATH = \"models/bert_w_sent/model.bin\"\nTRAINING_FILE = \"../input/tweet-sentiment-extraction/train.csv\"\n\nTOKENIZER = tokenizers.BertWordPieceTokenizer(\n    os.path.join(BERT_PATH, 'vocab.txt'),\n    lowercase=True\n)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Dataset code\n\nHere i have also use sentiment feature along with text feature. "},{"metadata":{"trusted":true},"cell_type":"code","source":"class TweetDataSet:\n    def __init__(self, tweet, sentiment, selected_text):\n        self.tweet = tweet\n        self.sentiment = sentiment\n        self.selected_text = selected_text\n        self.max_len = MAX_LEN\n        self.tokenizer = TOKENIZER\n\n    def __len__(self):\n        return len(self.tweet)\n\n    def __getitem__(self, item):\n        # removes all extra spaces in between words\n        tweet = \" \".join(str(self.tweet[item]).split())\n        selected_text = \" \".join(str(self.selected_text[item]).split())\n\n        start_ind = tweet.find(selected_text) # same as idx0\n        end_ind = start_ind + len(selected_text) # same as idx1\n        # print(start_ind, end_ind)\n\n        len_selected_text = len(selected_text)\n        idx0 = -1\n        idx1 = -1\n        # replace this with your find code\n\n        for ind in (i for i, e in enumerate(tweet) if e == selected_text[0]):\n            if tweet[ind: ind + len_selected_text] == selected_text:\n                idx0 = ind\n                idx1 = ind + len_selected_text - 1\n                break\n        # print(idx0, idx1)\n\n        char_targets = [0] * len(tweet)\n        # [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0]\n        if idx0 != -1 and idx1 != -1:\n            for j in range(idx0, idx1 + 1):\n                if tweet[j] != \" \":\n                    char_targets[j] = 1\n        # [0,0,0,0,1,1,1,0,1,1,1,0,1,1,1,0,0,,0,0,0,0,0,0] 0 between 1's denotes space\n        # tok_tweet = self.tokenizer.encode(tweet)\n        tok_tweet = self.tokenizer.encode(sequence=self.sentiment[item], pair=tweet)\n        \n        tok_tweet_tokens = tok_tweet.tokens\n        tok_tweet_ids = tok_tweet.ids\n        # with sentiment # CLS, 0,0,1, SEP\n        tok_tweet_offsets = tok_tweet.offsets[3:-1] # as first and last tokens are always cls and sep\n        targets = [0] * (len(tok_tweet_tokens) - 4) # -4 for cls and sep and sentiment and cls\n        # targets = [0] * (len(tok_tweet_tokens) - 2) # -2 for cls and sep\n\n        if self.sentiment[item] == \"positive\" or self.sentiment[item] == \"negative\":\n            sub_minus = 8\n        else:\n            sub_minus = 7\n\n        # [0,0,0,0,0,0,0]\n        for j, (offset1, offset2) in enumerate(tok_tweet_offsets):\n            # if sum(char_targets[offset1:offset2]) > 0: # this is done to encounter cases where characters in between word comes in selected text | to encounter partial match\n            if sum(char_targets[offset1 - sub_minus :offset2 - sub_minus]) > 0:\n                targets[j] = 1\n        # [0,0,1,1,1,0,0]\n        # from here on we don't need char_targets, we only need targets\n        targets = [0] + [0] + [0] + targets + [0] # cls + sentiment + cls + targets +  sep\n        targets_start = [0] * len(targets)\n        targets_end = [0] * len(targets)\n        non_zero = np.nonzero(targets)[0] \n        if len(non_zero) > 0:\n            targets_start[non_zero[0]] = 1\n            targets_end[non_zero[-1]] = 1\n\n        mask = [1] * len(tok_tweet_ids)\n        # token_type_ids = [0] * len(tok_tweet_ids)\n        token_type_ids = [0] * 3 + [1] * (len(tok_tweet_ids) - 3)\n        \n        padding_len = self.max_len - len(tok_tweet_ids)\n        ids = tok_tweet_ids + ([0] * padding_len)\n        mask = mask + ([0] * padding_len)\n        token_type_ids = token_type_ids + ([0] * padding_len)\n        targets = targets + ([0] * padding_len)\n        targets_start = targets_start + ([0] * padding_len)\n        targets_end = targets_end + ([0] * padding_len)\n\n        sentiment = [1, 0, 0]\n        if self.sentiment[item] == \"positive\":\n            sentiment = [0, 0, 1]\n        if self.sentiment[item] == \"negative\":\n            sentiment = [0, 1, 0]\n\n        return {\n            \"ids\": torch.tensor(ids, dtype=torch.long),\n            \"mask\": torch.tensor(mask, dtype=torch.long),\n            \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n            \"targets\": torch.tensor(targets, dtype=torch.long),\n            \"targets_start\": torch.tensor(targets_start, dtype=torch.long),\n            \"targets_end\": torch.tensor(targets_end, dtype=torch.long),\n            \"padding_len\": torch.tensor(padding_len, dtype=torch.long),\n            \"tweet_tokens\": \" \".join(tok_tweet_tokens),\n            \"orig_tweet\": self.tweet[item],\n            \"sentiment\": torch.tensor(sentiment, dtype=torch.long),\n            \"orig_sentiment\": self.sentiment[item],\n            \"orig_selected\": self.selected_text[item]\n        }","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Train, Eval and loss function"},{"metadata":{"trusted":true},"cell_type":"code","source":"class AverageMeter():\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\ndef jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def loss_fn(o1, o2, t1, t2):\n    l1 = nn.BCEWithLogitsLoss()(o1, t1)\n    l2 = nn.BCEWithLogitsLoss()(o2, t2)\n    return l1 + l2\n\n\ndef train_fn(data_loader, model, optimizer, device, scheduler):\n    model.train()\n    losses = AverageMeter()\n    tk0 = tqdm(data_loader, total=len(data_loader))\n    for bi, d in enumerate(tk0):\n        ids = d[\"ids\"]\n        token_type_ids = d[\"token_type_ids\"]\n        mask = d[\"mask\"]\n        targets_start = d[\"targets_start\"]\n        targets_end = d[\"targets_end\"]\n\n        ids = ids.to(device, dtype=torch.long)\n        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n        mask = mask.to(device, dtype=torch.long)\n        targets_start = targets_start.to(device, dtype=torch.float)\n        targets_end = targets_end.to(device, dtype=torch.float)\n      \n        optimizer.zero_grad()\n        o1, o2 = model(\n            ids = ids,\n            mask = mask,\n            token_type_ids = token_type_ids\n        )\n\n        loss = loss_fn(o1, o2, targets_start, targets_end)\n        loss.backward()\n        optimizer.step()\n        losses.update(loss.item(), ids.size(0))\n        tk0.set_postfix(loss=losses.avg)\n\n\n\ndef eval_fn(data_loader, model, device):\n    model.eval()\n    fin_output_start = []\n    fin_output_end = []\n    fin_padding_lens = []\n    fin_tweet_tokens = []\n    fin_orig_sentiment = []\n    fin_orig_selected = []\n    fin_orig_tweet = []\n\n    losses = AverageMeter()\n    tk0 = tqdm(data_loader, total=len(data_loader))\n    for bi, d in enumerate(tk0):\n        ids = d[\"ids\"]\n        token_type_ids = d[\"token_type_ids\"]\n        mask = d[\"mask\"]\n        targets_start = d[\"targets_start\"]\n        targets_end = d[\"targets_end\"]\n\n        tweet_tokens = d[\"tweet_tokens\"]\n        padding_len = d[\"padding_len\"]\n        # sentiment = d[\"sentiment\"]\n        orig_sentiment = d[\"orig_sentiment\"]\n        orig_selected = d[\"orig_selected\"]\n        orig_tweet = d[\"orig_tweet\"]\n\n\n        ids = ids.to(device, dtype=torch.long)\n        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n        mask = mask.to(device, dtype=torch.long)\n        targets_start = targets_start.to(device, dtype=torch.float)\n        targets_end = targets_end.to(device, dtype=torch.float)\n      \n        o1, o2 = model(\n            ids = ids,\n            mask = mask,\n            token_type_ids = token_type_ids\n        )\n\n        fin_output_start.append(torch.sigmoid(o1).cpu().detach().numpy())\n        fin_output_end.append(torch.sigmoid(o2).cpu().detach().numpy())\n        fin_padding_lens.extend(padding_len.cpu().detach().numpy().tolist())\n\n        fin_tweet_tokens.extend(tweet_tokens)\n        fin_orig_sentiment.extend(orig_sentiment)\n        fin_orig_selected.extend(orig_selected)\n        fin_orig_tweet.extend(orig_tweet)\n\n\n    fin_output_start = np.vstack(fin_output_start)\n    fin_output_end = np.vstack(fin_output_end)\n\n    threshold = 0.3\n    jaccards = []\n    for j in range(len(fin_tweet_tokens)):\n        target_string = fin_orig_selected[j]\n        tweet_tokens = fin_tweet_tokens[j]\n        padding_len = fin_padding_lens[j]\n        original_tweet = fin_orig_tweet[j]\n        sentiment = fin_orig_sentiment[j]\n\n        if padding_len > 0:\n            mask_start = fin_output_start[j, 3:][:-padding_len] >= threshold\n            mask_end = fin_output_end[j,3:][:-padding_len] >= threshold\n        else:\n            mask_start = fin_output_start[j, 3:] >= threshold\n            mask_end = fin_output_end[j,3:] >= threshold\n\n        mask = [0] * len(mask_start)\n        idx_start = np.nonzero(mask_start)[0]\n        idx_end = np.nonzero(mask_end)[0]\n\n        if len(idx_start) > 0:\n            idx_start = idx_start[0]\n            if len(idx_end) > 0:\n                idx_end = idx_end[0]\n            else:\n                idx_end = idx_start\n        else:\n            idx_start = 0\n            idx_end = 0\n\n        for mj in range(idx_start, idx_end + 1):\n            mask[mj] = 1\n\n        output_tokens = [x for p, x in enumerate(tweet_tokens.split()[3:]) if mask[p] == 1]\n        output_tokens = [x for x in output_tokens if x not in (\"[CLS]\", \"[SEP]\")]\n\n        final_output = \"\"\n        for ot in output_tokens:\n            if ot.startswith(\"##\"):\n                final_output = final_output + ot[2:]\n            elif len(ot) == 1 and ot in string.punctuation:\n                final_output = final_output + ot\n            else:\n                final_output = final_output + \" \" + ot\n        final_output = final_output.strip()\n        # if sentiment == \"neutral\" or len(original_tweet.split()) < 4:\n        if sentiment == \"neutral\" or len(original_tweet.split()) < 4:\n            final_output = original_tweet\n        jac = jaccard(target_string.strip(), final_output.strip())\n        jaccards.append(jac)\n    mean_jac = np.mean(jaccards)\n    return mean_jac\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Creating Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"class BERTBaseUncased(nn.Module):\n    def __init__(self):\n        super(BERTBaseUncased, self).__init__()\n        self.bert = transformers.BertModel.from_pretrained(BERT_PATH)\n        self.bert_drop = nn.Dropout(0.3)\n        self.l0 = nn.Linear(768, 2)\n    \n    def forward(self, ids, mask, token_type_ids, sentiment=None):\n        # not using sentiment for now\n        sequence_output, pooled_output = self.bert(\n            ids, \n            attention_mask=mask,\n            token_type_ids=token_type_ids\n        )\n        # sequence_output will have (batch_size, num_tokens, 768)\n        logits = self.l0(sequence_output)\n        # logits (batch_size, num_tokens, 2)\n        start_logits, end_logits = logits.split(1, dim=-1) # (batch_size, num_tokens, 1), (batch_size, num_tokens, 1)\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n        # (batch_size, num_tokens), (batch_size, num_tokens)\n       \n        return start_logits, end_logits","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Training Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"def run():\n\n    dfx = pd.read_csv(TRAINING_FILE).dropna().reset_index(drop=True)\n    print(dfx.shape)\n\n    df_train, df_valid = model_selection.train_test_split(\n        dfx,\n        test_size=0.1,\n        random_state=42,\n        stratify=dfx.sentiment.values\n    )\n\n    df_train = df_train.reset_index(drop=True)\n    df_valid = df_valid.reset_index(drop=True)\n\n    train_dataset = TweetDataSet(\n        tweet=df_train.text.values,\n        sentiment=df_train.sentiment.values,\n        selected_text=df_train.selected_text.values\n    )\n    \n\n    train_data_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=TRAIN_BATCH_SIZE,\n        num_workers=0\n    )\n\n    # print(train_dataset[0])\n\n    valid_dataset = TweetDataSet(\n        tweet=df_valid.text.values,\n        sentiment=df_valid.sentiment.values,\n        selected_text=df_valid.selected_text.values\n    )\n\n    valid_data_loader = torch.utils.data.DataLoader(\n        valid_dataset,\n        batch_size=VALID_BATCH_SIZE,\n        num_workers=0\n    )\n\n    device = torch.device(\"cuda\")\n    model = BERTBaseUncased()\n    model.to(device)\n    \n    param_optimizer = list(model.named_parameters())\n    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n    optimizer_parameters = [\n        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n    ]\n\n    num_train_steps = int(len(df_train) / TRAIN_BATCH_SIZE * EPOCHS)\n    optimizer = AdamW(optimizer_parameters, lr=3e-5)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=0,\n        num_training_steps=num_train_steps\n    )\n\n    if torch.cuda.device_count() > 1:\n        model = nn.DataParallel(model)\n\n    best_jaccard = 0\n    for epoch in range(EPOCHS):\n        print(f\"Epoch {epoch}/{EPOCHS}\")\n        train_fn(train_data_loader, model, optimizer, device, scheduler)\n        jaccard = eval_fn(valid_data_loader, model, device)\n\n        if jaccard > best_jaccard:\n            torch.save(model.state_dict(), 'model.bin')\n            best_jaccard = jaccard\n        print(f\"Jaccard Score = {jaccard}, best jaccard Score = {best_jaccard}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"run()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For inference related part please use abhishek's awesome kernel\n\nhttps://www.kaggle.com/abhishek/text-extraction-using-bert-w-sentiment-inference"},{"metadata":{},"cell_type":"markdown","source":"### Help taken from following sources:\n\n1. Abhishek's awesome youtube video [here](https://www.youtube.com/watch?v=XaQ0CBlQ4cY)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}