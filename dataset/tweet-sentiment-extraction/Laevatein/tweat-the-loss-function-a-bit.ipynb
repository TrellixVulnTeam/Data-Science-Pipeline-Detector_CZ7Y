{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Start from CrossEntropyLoss"},{"metadata":{},"cell_type":"markdown","source":"In [abhishek](https://www.kaggle.com/abhishek)'s great baseline notebook, the loss function used is the `CrossEntropyLoss`.  \n\nHowever, one drawback of `CrossEntropyLoss` if that it doesn't care \"the position of the error\".   \n\nWe can see from a simple example.   \n\nLet's say we got a sentence of length 5 (index starts from 0), and the correct answer is position 3.   \n\nThe first prediction has its highest probability at position 4, while the second prediction put it at position 0."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"inputs = torch.Tensor([[0.1, 0.1, 0.1, 0.1, 0.8]]).float() # pred as 4\ntargets = torch.Tensor([3]).long()\n\nloss_func = torch.nn.CrossEntropyLoss()\nprint(loss_func(inputs, targets))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inputs = torch.Tensor([[0.8, 0.1, 0.1, 0.1, 0.1]]).float() # pred as 0\ntargets = torch.Tensor([3]).long()\n\nloss_func = torch.nn.CrossEntropyLoss()\nprint(loss_func(inputs, targets))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see, the loss is the same, but 4 is much closer to 3 than 0. Then how can we optimise the loss function a bit?"},{"metadata":{},"cell_type":"markdown","source":"# Penalty based on the position gap"},{"metadata":{},"cell_type":"markdown","source":"Now let's add more penalty when our \"argmax prediction\" is far away from our target.   \n\nThis function also allows you to add different penalty to \"argmax prediction\" before/after the target."},{"metadata":{"trusted":true},"cell_type":"code","source":"def pos_weight(pred_tensor, pos_tensor, neg_weight=1, pos_weight=1):\n    # neg_weight for when pred position < target position\n    # pos_weight for when pred position > target position\n    gap = torch.argmax(pred_tensor, dim=1) - pos_tensor\n    gap = gap.type(torch.float32)\n    return torch.where(gap < 0, -neg_weight * gap, pos_weight * gap)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inputs = torch.Tensor([[0.1, 0.1, 0.1, 0.1, 0.8]]).float() # pred as 4\ntargets = torch.Tensor([3]).long()\n\npos_weight(inputs, targets, 1, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inputs = torch.Tensor([[0.8, 0.1, 0.1, 0.1, 0.1]]).float() # pred as 0\ntargets = torch.Tensor([3]).long()\n\npos_weight(inputs, targets, 1, 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The larger the gap is, the more penalty it'll get."},{"metadata":{},"cell_type":"markdown","source":"# Combine them"},{"metadata":{},"cell_type":"markdown","source":"Now it's time to combine them. A simply tweak on the original `loss_fn` is all we need.  \n\nYou can either multiple `loss_fct` with `pos_weight`, or squeeze `pos_weight` first and then add them together."},{"metadata":{"trusted":true},"cell_type":"code","source":"def loss_fn(start_logits, end_logits, start_positions, end_positions):\n    loss_fct = nn.CrossEntropyLoss(reduce='none') # do reduction later\n    \n    start_loss = loss_fct(start_logits, start_positions) * pos_weight(start_logits, start_positions, 1, 1)\n    end_loss = loss_fct(end_logits, end_positions) * pos_weight(end_logits, end_positions, 1, 1)\n    \n    start_loss = torch.mean(start_loss)\n    end_loss = torch.mean(end_loss)\n    \n    total_loss = (start_loss + end_loss)\n    return total_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# argmax pred for the start is 3, target is 1\n# argmax pred for the end is 3, target is 3\nstart = torch.Tensor([[0.1, 0.1, 0.1, 0.8, 0.1]]).float()\nstart_target = torch.Tensor([1]).long()\n\nend = torch.Tensor([[0.1, 0.1, 0.1, 0.8, 0.1]]).float()\nend_target = torch.Tensor([3]).long()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_fn(start, end, start_target, end_target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# argmax pred for the start is 2, target is 1\n# argmax pred for the end is 3, target is 3\nstart = torch.Tensor([[0.1, 0.1, 0.8, 0.1, 0.1]]).float()\nstart_target = torch.Tensor([1]).long()\n\nend = torch.Tensor([[0.1, 0.1, 0.1, 0.8, 0.1]]).float()\nend_target = torch.Tensor([3]).long()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_fn(start, end, start_target, end_target)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see, the loss for the second one is smaller, which also aligns with the jaccard."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}