{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-whitegrid')\nimport seaborn as sns\n\nfrom itertools import islice\nfrom collections import Counter\nimport re\n\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom scipy.stats import pearsonr\nimport wordcloud\nfrom textblob import TextBlob\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def jaccard(chain_one, chain_two):\n    \"\"\"\n        Returns the jaccard simalarity between two strings of text 0<=jaccard<=1\n    chain_one: type string\n    chain_two: type string\n    \"\"\"\n    chain_one = chain_one.split()\n    chain_two = chain_two.split()\n    \n    anb = set(chain_one).intersection(set(chain_two))\n    \n    \n    return len(anb)/(len(chain_one) + len(chain_two) - len(anb) + .01)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_text(text):\n    text = re.sub(r\"[^a-zA-Z]\",\" \",text)\n    text =  re.sub(r\"\\b\\w{1}\\b\", \"\", text)\n    text = text.lower()\n    text = re.sub(r\"http\", \"\",text)\n    \n    tokens = text.split()\n    \n    text = \" \".join([word for word in tokens if word not in ENGLISH_STOP_WORDS])\n    \n    return text\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def visualize_most_common_words(frame, text_col_name = \"text\", class_column = None, return_top = 10, cleanse_text = False):\n    all_words = list()\n    colors = ['orange','red', 'green']\n    \n    if cleanse_text:\n        frame[text_col_name] = frame[text_col_name].fillna('').apply(lambda x: clean_text(x))\n    \n        \n    if not class_column:\n        for _, row in frame.fillna('').iterrows():\n            for word in row[text_col_name].split():\n                all_words.append(word)\n        words_dict = Counter(all_words).most_common(return_top)\n        \n        words_dict = dict(words_dict)\n        \n        \n        \n        fig, ax =plt.subplots(nrows = 1, ncols = 1, figsize = (12,7))\n        \n        ax.barh(list(words_dict.keys()), list(words_dict.values()), edgecolor = 'black', color = 'steelblue')\n    else:\n\n        classes = frame[class_column].unique()\n        fig, ax =plt.subplots(nrows = 1, ncols = classes.size, figsize = (20,9))\n        \n        for i, class_ in enumerate(classes):\n            for _, row in frame[frame[class_column] == class_].fillna('').iterrows():\n                for word in row[text_col_name].split():\n                    all_words.append(word)\n            words_dict = Counter(all_words).most_common(return_top)\n\n            words_dict = dict(words_dict)\n\n            ax[i].barh(list(words_dict.keys()), list(words_dict.values()), edgecolor = 'black', color = colors[i])\n            ax[i].set_title(class_)\n            \n            all_words = list()\n        \n                ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def visualize_text_lenght(frame, text_col_name = \"text\", class_column = None, cleanse_text = False, separated_plots = True):\n    colors = ['orange','red', 'green']\n    \n    if cleanse_text:\n        frame[text_col_name] = frame[text_col_name].fillna('').apply(lambda x: clean_text(x))\n        \n    frame[\"doc_lenght\"] = frame[text_col_name].fillna('').apply(lambda x: len(x.split()))\n    \n    avg_length = np.mean(frame.doc_lenght)\n    \n    if not separated_plots:\n        fig, ax = plt.subplots(nrows=1, ncols=1, figsize = (12,7))\n        if not class_column:\n            sns.kdeplot(frame[\"doc_lenght\"],color = 'orange', alpha = .5, shade = True, ax = ax)\n            ax.set_title(f\"Documents lenght distribution\\nMean words by document: {int(avg_length)}\")\n            ax.set_xlabel(\"Number of words\")\n        else:\n            for color, class_ in zip(colors, frame[class_column].unique()):\n                sns.kdeplot(frame[frame[class_column] == class_][\"doc_lenght\"],color = color, alpha = .5, shade = True, ax = ax, label = class_)\n            ax.set_title(f\"Documents lenght distribution by class\\nMean words by document: {int(avg_length)}\")\n            ax.set_xlabel(\"Number of words\")\n    else:\n        classes = frame[class_column].unique()\n        nrows = 1\n        ncols = classes.size\n        fig, ax = plt.subplots(nrows = nrows, ncols = ncols, figsize = (20,7))\n        for i, class_ in enumerate(classes):\n            avg_length = np.mean(frame[frame[class_column] == class_].doc_lenght)\n            sns.kdeplot(frame[frame[class_column] == class_][\"doc_lenght\"],color = colors[i], alpha = .5, shade = True, ax = ax[i], label = class_)\n            ax[i].set_title(f\"Class: {class_}\\nMean words by document: {int(avg_length)}\")\n            ax[i].set_xlabel(\"Number of words\")\n            \n            \n            \n                                                       \n                                                       \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def visualize_text_vs_selected_lenght(frame, text_col_name = \"text\", sel_text_col_name = \"selected_text\", class_column = None, cleanse_text = False, separated_plots = True):\n    colors = ['orange','red', 'green']\n    if cleanse_text:\n        frame[text_col_name] = frame[text_col_name].fillna('').apply(lambda x: clean_text(x))\n        frame[sel_text_col_name] = frame[sel_text_col_name].fillna('').apply(lambda x: clean_text(x))\n    frame[\"doc_lenght\"] = frame[text_col_name].fillna('').apply(lambda x: len(x.split()))\n    frame[\"sel_doc_lenght\"] = frame[sel_text_col_name].fillna('').apply(lambda x: len(x.split()))\n    \n    text_corr = pearsonr(frame[\"doc_lenght\"], frame[\"sel_doc_lenght\"])[0]\n    \n    if not separated_plots:\n        fig, ax = plt.subplots(nrows=1, ncols=1, figsize = (12,7))\n        if not class_column:\n            ax.scatter(frame[\"doc_lenght\"], frame[\"sel_doc_lenght\"],facecolors = 'none',edgecolor = 'orange')\n            ax.set_title(f\"Documents lenght distribution\\Correlation for words in selected and original text: {round(text_corr, 3)}\")\n            ax.set_xlabel(\"Text words lenght\")\n            ax.set_ylabel(\"Selected text words lenght\")\n        else:\n            for color, class_ in zip(colors, frame[class_column].unique()):\n                ax.scatter(frame[\"doc_lenght\"], frame[\"sel_doc_lenght\"],facecolors = 'none',c = frame[class_column].map({'positive':'green','negative':'red','neutral':'orange'}))\n            ax.set_title(f\"Documents lenght distribution by class\\nCorrelation for words in selected and original text: {round(text_corr, 3)}\")\n            ax.set_xlabel(\"Text words lenght\")\n            ax.set_ylabel(\"Selected text words lenght\")\n    else:\n        classes = frame[class_column].unique()\n        nrows = 1\n        ncols = classes.size\n        fig, ax = plt.subplots(nrows = nrows, ncols = ncols, figsize = (20,7))\n        for i, class_ in enumerate(classes):\n            text_corr = pearsonr(frame[frame[class_column] == class_][\"doc_lenght\"], frame[frame[class_column] == class_][\"sel_doc_lenght\"])[0]\n            ax[i].scatter(frame[frame[class_column] == class_][\"doc_lenght\"], frame[frame[class_column] == class_][\"sel_doc_lenght\"],facecolors = 'none',color = colors[i])\n            ax[i].set_title(f\"Class: {class_}\\nMean words by document: {round(text_corr,3)}\")\n            ax[i].set_xlabel(\"Text words lenght\")\n            ax[i].set_ylabel(\"Selected text words lenght\")\n            \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def visualize_jaccard_similarity(frame, document_one_col_name = \"text\", document_two_col_name = \"selected_text\", class_column = None, cleanse_text = False, separated_plots = True):\n    colors = ['orange','red', 'green']\n    if cleanse_text:\n        frame[document_one_col_name] = frame[document_one_col_name].fillna('').apply(lambda x: clean_text(x))\n        frame[document_two_col_name] = frame[document_two_col_name].fillna('').apply(lambda x: clean_text(x))\n    frame[\"jaccard_similarity\"] = frame.apply(lambda x: jaccard(x[document_one_col_name] ,x[document_two_col_name]), axis = 1)\n    \n    \n    avg_js = np.mean(frame.jaccard_similarity)\n    \n    if not separated_plots:\n        fig, ax = plt.subplots(nrows=1, ncols=1, figsize = (12,7))\n        if not class_column:\n            sns.kdeplot(frame[\"jaccard_similarity\"],color = 'orange', alpha = .5, shade = True, ax = ax)\n            ax.set_title(f\"Documents lenght distribution\\nMean words by document: {int(avg_js)}\")\n            ax.set_xlabel(\"Number of words\")\n        else:\n            for color, class_ in zip(colors, frame[class_column].unique()):\n                sns.kdeplot(frame[frame[class_column] == class_][\"jaccard_similarity\"],color = color, alpha = .5, shade = True, ax = ax, label = class_)\n            ax.set_title(f\"Documents Jaccard similarityon by class\\nJaccard similarity by document: {int(avg_js)}\")\n            ax.set_xlabel(\"Jaccard similarity\")\n    else:\n        classes = frame[class_column].unique()\n        nrows = 1\n        ncols = classes.size\n        fig, ax = plt.subplots(nrows = nrows, ncols = ncols, figsize = (20,7))\n        for i, class_ in enumerate(classes):\n            avg_js = np.mean(frame[frame[class_column] == class_].jaccard_similarity)\n            sns.kdeplot(frame[frame[class_column] == class_][\"jaccard_similarity\"],color = colors[i], alpha = .5, shade = True, ax = ax[i], label = class_)\n            ax[i].set_title(f\"Class: {class_}\\nJaccard similarity: {round(avg_js, 3)}\")\n            ax[i].set_xlabel(\"Jaccard similarity\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_word_cloud(frame, text_col_name = 'text', class_column = None):\n    colors = ['Oranges','Reds', 'Greens']\n    if not class_column:\n        wc = wordcloud.WordCloud(\n            width= 900,\n            height= 500,\n            max_words = 500,\n            max_font_size= 100,\n            relative_scaling= .5,\n            colormap = 'Oranges',\n            normalize_plurals = True,\n            stopwords = ENGLISH_STOP_WORDS\n        ).generate(frame[text_col_name].to_string())\n        \n        plt.figure(figsize = (12,7))\n        plt.imshow(wc, interpolation= 'bilinear')\n        plt.axis(\"off\")\n    else:\n\n        classes = frame[class_column].unique()\n        fig, ax =plt.subplots(nrows = 1, ncols = classes.size, figsize = (20,20))\n        \n        for i, class_ in enumerate(classes):\n            wc = wordcloud.WordCloud(\n            width= 900,\n            height= 500,\n            max_words = 500,\n            max_font_size= 100,\n            relative_scaling= .5,\n            colormap = colors[i],\n            normalize_plurals = True,\n            stopwords = ENGLISH_STOP_WORDS\n        ).generate(frame[frame[class_column] == class_][text_col_name].to_string())\n        \n            ax[i].imshow(wc, interpolation= 'bilinear')\n            ax[i].axis(\"off\")\n            ax[i].set_title(class_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv')\ntest = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.sample(7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train[train.text.notnull()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.sentiment.value_counts(normalize = True).plot.bar(edgecolor = 'black', color = 'steelblue')\nplt.title(\"Class distribution\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## From these plots we can see that the number of words that define neutral documents is much higher than positive and negative ones, so It might imply a most difficult task for finfing the correct words in positives and negatives since for neutrals, the lenght of selected text is closer to the original one (see scatter plot below)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"visualize_text_lenght(train, cleanse_text = True, separated_plots= True, class_column= 'sentiment', text_col_name= 'selected_text')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"visualize_jaccard_similarity(train, class_column= 'sentiment', separated_plots= True, cleanse_text= False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## This plot show the relation ship between original text number of words and selected words by class\n### As we can see there is no linear relationship between those two","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"visualize_text_vs_selected_lenght(train, class_column= 'sentiment', separated_plots= True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## By examin the most frequent terms by class we can notice that there are wprds with a \"positive context\" that defined both negative and positive texts. Say for example the word like","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"visualize_most_common_words(train, cleanse_text= True, class_column= 'sentiment', return_top= 20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"make_word_cloud(train, class_column= 'sentiment')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Initial modeling\nFor this innitial approach I'm going to use Texblob to measure the polarity of each word and we are going to keep the n most important for each class","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}