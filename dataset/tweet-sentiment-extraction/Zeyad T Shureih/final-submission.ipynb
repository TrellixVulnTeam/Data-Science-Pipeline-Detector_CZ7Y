{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport numpy as np  # linear algebra\nimport pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import svm\nimport matplotlib.pyplot as plt\nimport re #regex\nimport nltk\nimport string\n# nltk.download('stopwords')\nfrom nltk.corpus import stopwords\nimport seaborn as sns\nfrom scipy.sparse import coo_matrix\n\ntrain_data = pd.read_csv('../input/tweet-sentiment-extraction/train.csv')\ntest_data = pd.read_csv('../input/tweet-sentiment-extraction/test.csv')\nsample = pd.read_csv('../input/tweet-sentiment-extraction/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"code","source":"# Pre-processing step - from https://www.kaggle.com/rajaram1988/ignored-stop-words-using-only-word-counts\n\n# drop entries with no text\ntrain_data = train_data.dropna()\ntrain_data[train_data['text'].isna()]\ntest_data = test_data.dropna()\n\n# convert text to lowercase\ntrain_data['text'] = train_data['text'].map(lambda x: x.lower())\ntest_data['text'] = test_data['text'].map(lambda x: x.lower())\n\n# remove '\\\\n'\ntrain_data['text'] = train_data['text'].map(lambda x: re.sub('\\\\n', ' ', str(x)))\ntest_data['text'] = test_data['text'].map(lambda x: re.sub('\\\\n', ' ', str(x)))\n\n# remove any text starting with User...\ntrain_data['text'] = train_data['text'].map(lambda x: re.sub(\"\\[\\[User.*\", '', str(x)))\ntest_data['text'] = test_data['text'].map(lambda x: re.sub(\"\\[\\[User.*\", '', str(x)))\n\n# remove IP addresses or user IDs\ntrain_data['text'] = train_data['text'].map(lambda x: re.sub(\n    \"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\", '', str(x)))\ntest_data['text'] = test_data['text'].map(lambda x: re.sub(\n    \"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\", '', str(x)))\n\n#remove http links in the text\ntrain_data['text'] = train_data['text'].map(lambda x: re.sub(\n    \"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\", '', str(x)))\ntest_data['text'] = test_data['text'].map(lambda x: re.sub(\n    \"(http://.*?\\s)|(http://.*)\", '', str(x)))\n\n# create a list of stop words and add custom stop words from the data set\nstop_words = set(stopwords.words(\"english\"))\nprint(\"# of stop words before join\", len(stop_words))\n# get 20 most common words and their counts\nmost_common = pd.Series(\n    ' '.join(train_data['text']).split()).value_counts()[:20]\n\n# add most commmon words to stop_words\nstop_words = stop_words.union(most_common.keys())\nprint(\"# of stop words after join\", len(stop_words))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_val = train_test_split(train_data, train_size=0.8, random_state=0)\n\n# set aside positive/negative/neutral tweets\npositive_tweets = x_train[x_train['sentiment'] == 'positive']\nnegative_tweets = x_train[x_train['sentiment'] == 'negative']\nneutral_tweets = x_train[x_train['sentiment'] == 'neutral']\n\n# get lengths of 'selected_text' for non-neutral tweets\npos_selected_lengths = positive_tweets['selected_text'].map(lambda x: len(x.split()))\nneg_selected_lengths = negative_tweets['selected_text'].map(lambda x: len(x.split()))\n\n# plot 'selected_text' lengths against frequency in sentiment\nplt.figure(figsize=(12, 6))\np1 = sns.kdeplot(pos_selected_lengths, shade=True, color=\"b\").set_title(\n    'Selected Text lengths across Positive and Negative Sentiments')\np2 = sns.kdeplot(neg_selected_lengths, shade=True, color=\"r\")\nplt.legend(labels=['positive', 'negative'])\nplt.show()\nplt.clf()\n# based on this plot we can see that selected_text is more frequently shorter in positive tweets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get lengths of 'text' for non-neutral tweets\npos_lengths = positive_tweets['text'].map(lambda x: len(x.split()))\nneg_lengths = negative_tweets['text'].map(lambda x: len(x.split()))\n\n# let's look at the lengths of positive vs negative tweets\nplt.figure(figsize=(12,6))\np1 = sns.kdeplot(pos_lengths, shade=True, color=\"b\").set_title(\n    'Text Lengths across Positive and Negative Sentiments')\np2 = sns.kdeplot(neg_lengths, shade=True, color=\"r\")\nplt.legend(labels=['positive', 'negative'])\nplt.show()\nplt.clf()\n# this doesn't seem all that useful. tweet lengths are distributed evenly in positive and negative tweets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create feature vectors that include ngrams of size max_ngram \n# so we can select a feature that is a word or phrase to be our 'selected_text'\nmax_ngram = max(pos_selected_lengths) if max(pos_selected_lengths) > \\\n    max(neg_selected_lengths) else max(neg_selected_lengths)\nmin_ngram = min(pos_selected_lengths) if min(pos_selected_lengths) > \\\n    min(neg_selected_lengths) else min(neg_selected_lengths)\n# this ended up being fruitless","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer = CountVectorizer(\n    max_df=0.85,\n    min_df=2,\n    stop_words=stop_words,\n    max_features=10000,\n)\n\n# let's remove all neutral tweets from x_train so that we can train the cvm properly\nnon_neutral = x_train[x_train['sentiment'] != 'neutral']\n\n# fit the vectorizer to the non_neutral training data\ntrain_vectors = vectorizer.fit_transform(non_neutral['text'])\nx_pos = vectorizer.transform(positive_tweets['text'])\nx_neg = vectorizer.transform(negative_tweets['text'])\n\ntfidf_transformer = TfidfTransformer(smooth_idf=True, use_idf=True)\ntfidf_vectors = tfidf_transformer.fit_transform(train_vectors)\n\nsupportVector = svm.SVC(kernel='linear')\nsupportVector.fit(tfidf_vectors, non_neutral['sentiment'])\n\nweights_dict = {}\nfeatures = vectorizer.get_feature_names()\nfor i in range(len(features)):\n    feature = features[i]\n    weights_dict[feature] = supportVector.coef_[0, i]\n    \n# get bag of words and weights of key words/phrases in negative tweets\nweights_list = [(word, weights_dict[word])\n              for word in features]\nweights_sorted = sorted(weights_list, key=lambda x: x[1],\n                    reverse=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get the top 50 words and plot them \ntop_50_words = weights_sorted[:50]\nweight_top_df = pd.DataFrame(top_50_words)\nweight_top_df.columns = [\"Word\", \"Weight\"]\n\nsns.set(rc={'figure.figsize': (13, 8)})\ng = sns.barplot(x=\"Word\", y=\"Weight\", data=weight_top_df)\ng.set_xticklabels(g.get_xticklabels(), rotation=60)\nplt.show()\nplt.clf()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get the bottom 50 words and plot them\nbot_50_words = weights_sorted[len(weights_sorted) - 50:]\nweight_bot_df = pd.DataFrame(bot_50_words)\nweight_bot_df.columns = [\"Word\", \"Weight\"]\n\nsns.set(rc={'figure.figsize': (13, 8)})\ng = sns.barplot(x=\"Word\", y=\"Weight\", data=weight_bot_df)\ng.set_xticklabels(g.get_xticklabels(), rotation=60)\nplt.show()\nplt.clf()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# it looks like positive words have a negative weight and negative words have a positive weight\n# so we should calculate selected_text based on sentiment\ninv_weights_dict = {}\nfor key in weights_dict.keys():\n    inv_weights_dict[key] = weights_dict[key] * -1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def calc_selected_text(x, tol, a):\n    tweet = x['text']\n    sentiment = x['sentiment']\n\n    if sentiment == 'neutral':\n        return tweet\n    if sentiment == 'positive':\n        weights = weights_dict\n    if sentiment == 'negative':\n        weights = inv_weights_dict\n\n    text = tweet.split()\n    text_len = len(text)\n\n    subsets = [text[i:j+1]\n               for i in range(text_len) for j in range(i, text_len)]\n\n    subsets = sorted(subsets, key=len)\n\n    score = 0\n    selected = ''\n    for i in range(len(subsets)):\n        subtr_sum = 0\n\n        for p in range(len(subsets[i])):\n            words_in_substr = subsets[i][p].translate(\n                str.maketrans('', '', string.punctuation))\n            if(words_in_substr in weights.keys()):\n                # We noticed that our selected strings were ~375% longer than they should be, so we implemented a \"cost function\"\n                # to encourage smaller strings\n                subtr_sum += weights[words_in_substr] - a * (len(words_in_substr) / text_len)\n\n        #tol = tol*5 # Increase the tolerance a bit each time we choose a selection\n        if(subtr_sum > score + tol):\n            score = subtr_sum\n            selected = subsets[i]\n\n    if len(selected) == 0:\n        selected = text\n\n    return ' '.join(selected)\n\n# from https://www.kaggle.com/rajaram1988/ignored-stop-words-using-only-word-counts\ndef jaccard(str1, str2):\n    a = set(str1.lower().split())\n    b = set(str2.lower().split())\n    # print(\"{} - {}\".format(str1, str2))\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# to prevent warnings appearing in the console\npd.options.mode.chained_assignment = None\n\n# some parameters for the text selector\ntol = 0.0015\na = 5\nprint(\"tol = {}\".format(tol))\nprint(\"a = {}\".format(a))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make predictions on training set\nx_train['prediction'] = ''\nfor key, row in x_train.iterrows():\n    selected = calc_selected_text(row, tol, a)\n\n    x_train.loc[x_train['textID'] == row['textID'], ['prediction']] = selected\n\nx_train['jaccard'] = x_train.apply(\n    lambda x: jaccard(x['selected_text'], x['prediction']), axis=1)\n\nprint('Jaccard for training set = ', np.mean(x_train['jaccard']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make predictions on validation set\nx_val['prediction'] = ''\nfor key, row in x_val.iterrows():\n    selected = calc_selected_text(row, tol, a)\n\n    x_val.loc[x_val['textID'] == row['textID'], ['prediction']] = selected\n\nx_val['jaccard'] = x_val.apply(\n    lambda x: jaccard(x['selected_text'], x['prediction']), axis=1)\n\nprint('Jaccard for validation set = ', np.mean(x_val['jaccard']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make final submission\ntest_data['prediction'] = ''\nfor index, row in test_data.iterrows():\n    selected_text = calc_selected_text(row, tol, a)\n\n    sample.loc[sample['textID'] == row['textID'], ['selected_text']] = selected_text\n\nsample.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}