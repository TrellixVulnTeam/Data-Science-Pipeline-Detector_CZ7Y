{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"gpu_info = !nvidia-smi\ngpu_info = '\\n'.join(gpu_info)\nif gpu_info.find('failed') >= 0:\n  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n  print('and then re-execute this cell.')\nelse:\n  print(gpu_info)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import collections\nimport os\nimport random\nimport re\nimport string\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport nltk.corpus # for stopwords\nimport numpy as np\nimport pandas as pd\n\n# visualization lib \nimport matplotlib.pyplot as plt \nfrom PIL import Image\nfrom plotly import graph_objs, express, figure_factory\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport seaborn as sns\n\n# ml\nimport spacy.util\nimport tensorflow as tf\nimport torch\nimport torch.nn as nn\nimport tokenizers\nimport transformers\nfrom sklearn.model_selection import StratifiedKFold\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('TF version', tf.__version__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv')\ntest = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv')\nsubmission = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/sample_submission.csv') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.shape)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# selected_text가 모두 text 데이터의 sub 인지 확인\nlen(train.apply(lambda x:x.selected_text in x.text, axis=1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = train.groupby('sentiment').count()['text'].reset_index().sort_values(by='text', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp.style.background_gradient(cmap='Purples')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 6))\nsns.countplot(x='sentiment', data=train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = graph_objs.Figure(graph_objs.Funnelarea(\n                    text = temp.sentiment,\n                    values = temp.text,\n                    title = {\n                        \"position\": \"top center\",\n                        \"text\": \"Funnel-Chart of Sentiment Distribution\"\n                    }\n                ))\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# selected_text와 text Jaccard Similiarity로 유사도 추출하기\ndef jaccard(str1, str2):\n    a = set(str1.lower().split())\n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nresult_jaccard = []\n\nfor idx, row in train.iterrows():\n    sent1 = row.text\n    sent2 = row.selected_text\n    \n    jaccard_score = jaccard(sent1, sent2)\n    result_jaccard.append([sent1, sent2, jaccard_score])\n    \njaccard = pd.DataFrame(result_jaccard, columns=[\"text\", \"selected_text\", \"jaccard_score\"])\ntrain = train.merge(jaccard, how='outer')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['num_word_selected'] = train['selected_text'].apply(lambda x: len(str(x).split()))\ntrain['num_word_text'] = train['text'].apply(lambda x: len(str(x).split()))\ntrain['difference_in_words'] = train['num_word_text'] - train['num_word_selected']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# selected text와 text의 word 개수 비교\nhist_data = [train['num_word_selected'], train['num_word_text']]\n\nfig, axes = plt.subplots(figsize=(12, 6))\nsns.countplot(train['num_word_selected'], ax=axes, color='blue', alpha=0.3, label='selected_text')\nsns.countplot(train['num_word_text'], ax=axes, color='red', alpha=0.3, label='text')\naxes.legend()\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# positive, negative 의 selected_text와 text의 단어 개수 차이 \nplt.figure(figsize=(12, 6))\np1 = sns.kdeplot(\n        train[train['sentiment'] == 'positive']['difference_in_words'],\n        shade=True,\n        color='b',\n        label='positive').set_title('Kernel Distribution of Difference in Number of words')\np2 = sns.kdeplot(\n        train[train['sentiment'] == 'negative']['difference_in_words'],\n        shade=True,\n        color='r',\n        label='positive')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# neutral의 selected_text와 text의 단어개수 차이 \nplt.figure(figsize=(12, 6))\nsns.distplot(train[train['sentiment'] == 'neutral']['difference_in_words'], kde=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# positive, negative의 jaccard score 차이\nplt.figure(figsize=(12, 6))\np1 = sns.distplot(\n        train[train['sentiment'] == 'positive']['jaccard_score'],\n#         shade=True,\n        color='b',\n        label='positive'\n        ).set_title('KDE of Jaccard Scores across different Sentiments')\np2 = sns.distplot(\n        train[train['sentiment'] == 'negative']['jaccard_score'],\n#         shade=True,\n        color='r',\n        label='negative')\nplt.legend(labels=['positive', 'negative'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 6))\np1 = sns.kdeplot(\n        train[train['sentiment'] == 'positive']['jaccard_score'],\n        shade=True,\n        color='b',\n        label='positive'\n        ).set_title('KDE of Jaccard Scores across different Sentiments')\np2 = sns.kdeplot(\n        train[train['sentiment'] == 'negative']['jaccard_score'],\n        shade=True,\n        color='r',\n        label='negative')\nplt.legend(labels=['positive', 'negative'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN = 96","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modeling  \n- roBerta\n- Bart"},{"metadata":{},"cell_type":"markdown","source":"### roBERTa (tensorflow)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN = 96\nPATH = '../input/tf-roberta/'\ntokenizer = tokenizers.ByteLevelBPETokenizer(\n                vocab = PATH + 'vocab-roberta-base.json',\n                merges = PATH + 'merges-roberta-base.txt',\n                lowercase = True,\n                add_prefix_space = True\n            )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unique_sentiment = train.sentiment.unique()\nprint(unique_sentiment)\nsentiment_id = collections.defaultdict(int)\nfor idx, sentiment in enumerate(unique_sentiment):\n    sentiment_id[sentiment] = idx\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shape0 = train.shape[0]\ninput_ids = np.ones((shape0, MAX_LEN), dtype='int32')\nattention_mask = np.zeros((shape0, MAX_LEN), dtype='int32')\ntoken_type_ids = np.zeros((shape0, MAX_LEN), dtype='int32')\nstart_tokens = np.zeros((shape0, MAX_LEN), dtype='int32')\nend_tokens = np.zeros((shape0, MAX_LEN), dtype='int32')\n\nfor k in range(shape0):\n    # text2에서 text1의 위치를 찾아 chars에 해당 위치에 1이라고 mark\n    text1 = \" \" + \" \".join(train.loc[k, 'text'].split())\n    text2 = \" \".join(train.loc[k, 'selected_text'].split())\n    idx = text1.find(text2)\n    chars = np.zeros((len(text1)))\n    chars[idx: idx + len(text2)] = 1\n    if text1[idx - 1] == ' ':\n        chars[idx - 1] = 1\n    # text1 문장 tokenizer encoding하여 저장 \n    enc = tokenizer.encode(text1)\n#     print(enc)\n    # offset에 enc에 저장되어 있는 각 단어 길이 저장 (단어 맨처음 char에 ' '포함되어 카운팅됨)\n    offsets = []\n    idx = 0\n    for t in enc.ids:\n        w = tokenizer.decode([t])\n#         print(w)\n        offsets.append((idx, idx + len(w)))\n        idx += len(w)\n        \n    # 단어 인덱싱\n    toks = []\n#     print(chars)\n#     print(text2)\n    for i, (a, b) in enumerate(offsets):\n        sm = np.sum(chars[a:b])\n        if sm>0: toks.append(i)\n#         print(chars[a:b], sm)\n#     print(toks)\n#     print('')\n#     print('')\n    s_tok = sentiment_id[train.loc[k, 'sentiment']]\n#     print('sentiment_id: ', s_tok)\n#     print('before input_ids: ', input_ids)\n    input_ids[k, :len(enc.ids) + 5] = [0] + enc.ids + [2, 2] + [s_tok] + [2]\n#     print('after input_ids: ', input_ids)\n#     print('before attention_mask: ', attention_mask)\n    attention_mask[k, :len(enc.ids)+5] = 1\n#     print('after attention_mask: ', attention_mask)\n    if len(toks) > 0:\n        start_tokens[k, toks[0]+1] = 1\n        end_tokens[k, toks[-1]+1] = 1\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train['text'][1])\nprint(input_ids[1])\nprint(attention_mask[1])\nprint(sentiment_id[1])\nprint(start_tokens[1])\nprint(end_tokens[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"enc = tokenizer(train['text'][1])\nenc\n# print(tokenizer.decode(enc['input_ids']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_roberta():\n    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    \n    config = transformers.RobertaConfig.from_pretrained(PATH + 'config-roberta-base.json')\n    bert_model = transformers.TFRobertaModel.from_pretrained(PATH + 'pretrained-roberta-base.h5', config=config)\n    \n    x = bert_model(ids, attention_mask=att, token_type_ids=tok)\n    \n    x1 = tf.keras.layers.Dropout(0.1)(x[0])\n    x1 = tf.keras.layers.Conv1D(1,1)(x1)\n    x1 = tf.keras.layers.Flatten()(x1)\n    x1 = tf.keras.layers.Activation('softmax')(x1)\n    \n    x2 = tf.keras.layers.Dropout(0.1)(x[0])\n    x2 = tf.keras.layers.Conv1D(1,1)(x2)\n    x2 = tf.keras.layers.Flatten()(x2)\n    x2 = tf.keras.layers.Activation('softmax')(x2)\n    \n    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1, x2])\n    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n    \n    return model\n\n\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = build_roberta()\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"VER = 'v0'\nDISPLAY=1\nval_start = np.zeros((input_ids.shape[0], MAX_LEN))\nval_end = np.zeros((input_ids.shape[0], MAX_LEN))\n\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=777)\nbest_model = 0\nhistory = []\nfor fold, (idx_train, idx_val) in enumerate(skf.split(input_ids, train.sentiment.values)):\n    print(fold, (idx_train, idx_val))\n    tf.keras.backend.clear_session()\n    model = build_roberta()\n    sv = tf.keras.callbacks.ModelCheckpoint(\n            '/kaggle/working/%s-roberta-%i.h5'%(VER,fold),\n            monitor='val_loss',\n            verbose=1,\n            save_best_only=True,\n            save_weight_only=True,\n            mode='auto',\n            save_freq='epoch'\n            )\n    \n    history.append(\n        model.fit([input_ids[idx_train,], attention_mask[idx_train,], token_type_ids[idx_train, ]], [start_tokens[idx_train,], end_tokens[idx_train,]],\n                  epochs=3,\n                  batch_size=32,\n                  verbose=1,\n                  callbacks=[sv],\n                  validation_data=([input_ids[idx_val,], attention_mask[idx_val,], token_type_ids[idx_val,]], [start_tokens[idx_val,], end_tokens[idx_val,]])))\n      \n    model.load_weights('/kaggle/working/%s-roberta-%i.h5'%(VER,fold))\n    val_start[idx_val,], val_end[idx_val,] = model.predict(\n                                [input_ids[idx_val,],attention_mask[idx_val,],token_type_ids[idx_val,]],verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history[0].history['loss'])\nplt.plot(history[0].history['val_loss'])\nplt.title('Model Loss')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **BART**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import BartModel, BartConfig\n# Initializing a BART facebook/bart-large style configuration\nconfiguration = BartConfig()\n# Initializing a model from the facebook/bart-large style configuration\nmodel = BartModel(configuration)\n# Accessing the model configuration\nconfiguration = model.config","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import BartTokenizer, BartModel\nimport torch\ntokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\nmodel = BartModel.from_pretrained('facebook/bart-large')\ninputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\noutputs = model(**inputs)\nlast_hidden_states = outputs.last_hidden_state\n\ntokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\nmodel = BartModel.from_pretrained('facebook/bart-large')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import BartTokenizer, BartModel, AdamW\nimport torch.nn as nn\n\n\nclass BartQA(nn.Module):\n    def __init__(self, bart, config):\n        super(BartQA, self).__init__()\n        self.bart = bart\n        self.qa = nn.Linear(config.hidden_size, 2)\n        self.softmax = nn.LogSoftmax(dim=1)\n\n    def forward(self, input_ids, attention_mask):\n        output = self.bart(input_ids, attention_mask=attention_mask)\n        qa_output = self.qa(output[0])\n        print('output[0].shape: ', output[0].shape)\n        print('qa_output.shape: ', qa_output.shape)\n        start, end = qa_output.split(1, dim=-1)\n        print('start.shape, end.shape: ', start.shape, end.shape)\n        start_logits = start.squeeze(-1)\n        end_logit = end.squeeze(-1)\n        print('start_logits.shape, end_logit.shape: ', start_logits.shape, end_logit.shape)\n        return start_logits, end_logit\n\n\n\nbartqa = BartQA(model, model.config)\nbartqa.to('cuda')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import  TensorDataset, DataLoader","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_ids_tensor = torch.tensor(input_ids, dtype=torch.long)\nattention_mask_tensor = torch.tensor(attention_mask, dtype=torch.float32)\nstart_tokens_tensor = torch.tensor(start_tokens, dtype=torch.long)\nend_tokens_tensor = torch.tensor(end_tokens, dtype=torch.long)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = TensorDataset(input_ids_tensor, attention_mask_tensor, start_tokens_tensor, end_tokens_tensor)\ntrain_dataloader = DataLoader(train_data, shuffle=True, batch_size=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = 'cuda'\n\nmodel.train()\ntotal_loss = 0\ntotal_preds = []\nfor step, batch in enumerate(train_dataloader):\n    # progress update after every 50 batches.\n    if step % 50 == 0 and not step == 0:\n        print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n\n    batch = [r.to(device) for r in batch]\n    sent_id, mask, start, end = batch\n    print(len(sent_id), len(mask), len(start), len(end))\n    model.zero_grad()\n\n    preds = bartqa(sent_id, mask)\n    print(preds[0].shape, start.shape)\n    print(preds[1].shape, end.shape)\n    loss_fct = nn.CrossEntropyLoss(ignore_index=1)\n    start_loss = loss_fct(preds[0], start)\n    end_loss = loss_fct(preds[1], end)\n    loss = (start_loss + end_loss) / 2\n    total_losss = total_loss + loss.item()\n\n    loss.backward()\n\n    nn.utils.clip_grad_norm(model.paramters(), 1.0)\n\n    optimizer.step()\n\n    preds = preds.detach().cpu().numpy()\n\n    total_preds.append(preds)\n\n    avg_loss = total_loss / len(train_dataloader)\n\n    total_preds = np.concatenate(total_preds, axis=0)\n\n    print(avg_loss, total_preds)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}