{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import transformers\n# from transformers import AlbertTokenizer, AlbertModel\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nimport math\n\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nfrom tqdm import tqdm_notebook\nimport re\nimport os, sys\nimport random","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed_value):\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    os.environ['PYTHONHASHSEED'] = str(seed_value)\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = True\n\nseed = 42\nseed_everything(seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.environ['CUDA_LAUNCH_BLOCKING'] = '1'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\nWith all of the tweets circulating every second it is hard to tell whether the sentiment behind a specific tweet will impact a company, or a person's, brand for being viral (positive), or devastate profit because it strikes a negative tone. Capturing sentiment in language is important in these times where decisions and reactions are created and updated in seconds. But, which words actually lead to the sentiment description, In this notebook I will pick out the part of the tweet (word or phrase) that reflects the sentiment.\n\nThank you to [Abhishek Thakur](https://www.kaggle.com/c/tweet-sentiment-extraction/discussion/144352) who shared an alternative tokenizer with offsets / spans in ALBERT and I learned a lot of great kernels from him in this competition.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Inspect Data\n### Load dataset and check data distribution","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv')\ntest = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(train))\ntrain = train.dropna()\nprint(len(train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute_jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    if (len(a)==0) & (len(b)==0): \n        return 0.5\n    else:\n        return round(float(len(c)) / (len(a) + len(b) - len(c)), 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"compute_jaccard(train.text[3], train.selected_text[3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add histogram data\ndist_score_pos = []\ndist_score_neg = []\ndist_score_neu = []\n\ngroup_labels = ['positive', 'negative', 'neutral']\n\nfor label in group_labels:\n    for t, s_t in zip(train.text[train.sentiment == label], train.selected_text[train.sentiment == label]):\n        score = compute_jaccard(t, s_t)\n        if label == 'positive':\n            dist_score_pos.append(score)\n        elif label == 'negative':\n            dist_score_neg.append(score)\n        else:\n            dist_score_neu.append(score)\n\n# Group data together\nhist_data = [dist_score_pos, dist_score_neg, dist_score_neu]\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = make_subplots(rows=3, cols=1)\nrows_c = 1\nfor h_data, g_label in zip(hist_data, group_labels):\n    fig.append_trace(go.Histogram(x=h_data, name=g_label), rows_c, 1)\n    rows_c += 1\n\nfig.update_layout(title='Distribution jaccard score each sentiment')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure()\nrows_c = 1\nfor h_data, g_label in zip(hist_data, group_labels):\n    if rows_c == 3:\n        continue\n    fig.add_trace(go.Histogram(x=h_data, name=g_label))\n    rows_c += 1\n\n# Overlay both histograms\nfig.update_layout(barmode='overlay', title='Distribution jaccard score in positive and negative sentiment')\n# Reduce opacity to see both histograms\nfig.update_traces(opacity=0.5)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dist_score_neu.count(1) / len(dist_score_neu)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"jaccard score in negative and positive sentiment have a pretty similar score. positive sentiment more lower score than negative and neutral sentiment have 1.0 or perfect jaccard score in 90% of neutral data, which means text and selected text are no different in 90%","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Prepare Dataloader\n### preprocessing and prepare dataloader to transformer input","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# use this command if u want to download sentencepiece model from kernel\n# !wget -O \"/kaggle/working/albert-base-v2-spiece.model\" \"https://s3.amazonaws.com/models.huggingface.co/bert/albert-base-v2-spiece.model\"\n# !wget -O \"/kaggle/working/sentencepiece_pb2.py\" \"https://raw.githubusercontent.com/google/sentencepiece/master/python/sentencepiece_pb2.py\"\n# and use import sentencepiece_pb2.py to generate offsets / spans\n# !wget https://github.com/google/sentencepiece/blob/master/python/sentencepiece_python_module_example.ipynb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sys.path.append('/kaggle/input/sentencepiecepb2')\nimport sentencepiece as spm\nimport sentencepiece_pb2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path_spiece_model = '/kaggle/input/albert-model/albert-base-v2-spiece.model'\npath_albert_config = '/kaggle/input/albert-model/albert-base-v2-config.json'\npath_albert_model = '/kaggle/input/albert-model/albert-base-v2-pytorch_model.bin'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sp = spm.SentencePieceProcessor()\nsp.load(path_spiece_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = transformers.AlbertTokenizer.from_pretrained(path_spiece_model, do_lower_case=True)\ntokenizer.tokenize(\"Test tokenizer\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sp.encode_as_pieces(\"Test tokenizer\".lower())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class OffsetTokenizer():\n    def __init__(self, path_model=path_spiece_model):\n        self.spt = sentencepiece_pb2.SentencePieceText()\n        self.sp = spm.SentencePieceProcessor()\n        self.sp.load(path_model)\n        \n    def encode(self, text, return_tokens=False, lower=True):\n        if lower:\n            text = text.lower()\n        offset = []\n        ids = []\n        self.spt.ParseFromString(self.sp.encode_as_serialized_proto(text))\n        \n        for piece in self.spt.pieces:\n            offset.append((piece.begin, piece.end))\n            ids.append(piece.id)\n            \n        if return_tokens:\n            return sp.encode_as_pieces(text), ids, offset\n        else:\n            return ids, offset\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"o_tokenizer = OffsetTokenizer()\no_tokenizer.encode(\"Test tokenizer\", return_tokens=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# spt = sentencepiece_pb2.SentencePieceText()\n# spt.ParseFromString(sp.encode_as_serialized_proto(text))\n# a = tokenizer.encode(text, add_special_tokens=False)\n# b = tokenizer.tokenize(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"albert_config = transformers.AlbertConfig.from_pretrained(path_albert_config)\nalbert_config.output_hidden_states=True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def pad_or_truncate(list_num, max_len, SEP_ID, is_input_ids=False):\n    if len(list_num) < max_len:\n        list_num = list_num + [0] * (max_len - len(list_num))\n    else:\n        if is_input_ids:\n            list_num = list_num[:max_len-1] + SEP_ID\n        else:\n            return list_num[:max_len]\n    return list_num","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def issubset_sequence(s_text_ids, text_ids):\n    s_text_ids = [str(i) for i in s_text_ids]\n    ptn_s_text_ids = r'\\b' + r'\\b \\b'.join(s_text_ids) + r'\\b'\n    text_ids = [str(i) for i in text_ids]\n    text_ids = ' '.join(text_ids)\n    \n    if re.search(ptn_s_text_ids, text_ids):\n        return True\n    else:\n        return False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def data_extractor(df, data_test=False, max_len=128, random=True):\n    \"\"\"\n    Processes the tweet and outputs the features necessary for model training and inference\n    \"\"\"\n    count = 0\n    # ALBERT ids\n    sentiment_id = {'positive' : 2221,\n                    'neutral' : 8387,\n                    'negative' : 3682}\n    CLS_ID = tokenizer.encode('[CLS]', add_special_tokens=False)\n    SEP_ID = tokenizer.encode('[SEP]', add_special_tokens=False)\n    \n    all_data = []\n    if random:\n        df = df.sample(frac=1, random_state=42)\n    texts = df.text.tolist()\n    sentiments = df.sentiment.tolist()\n    \n    if not data_test:\n        sel_texts = df.selected_text.tolist()\n        for idx, row in df.iterrows():\n            start_target, end_target = 0, 0\n            text = ' '+ ' '.join( row.text.lower().split() )\n            s_text = ' '+ ' '.join( row.selected_text.lower().split() )\n            sentiment = row.sentiment.lower().strip()\n            text_ids, offsets = o_tokenizer.encode(text)\n            s_text_ids, _ = o_tokenizer.encode(s_text)\n\n            # fixing bug selection text\n            for i in range(len(s_text_ids)):\n                if issubset_sequence(s_text_ids, text_ids):\n                    break\n                else:\n                    s_text_ids = s_text_ids[1:]\n\n            if len(s_text_ids) == 0:\n                s_text_ids = tokenizer.encode(s_text, add_special_tokens=False)\n                for i in range(len(s_text_ids)):\n                    if issubset_sequence(s_text_ids, text_ids):\n                        break\n                    else:\n                        s_text_ids = s_text_ids[:-1]\n\n            for i in range(len(text_ids)):\n                if text_ids[i:i+len(s_text_ids)] == s_text_ids:\n                    # +3 from cls_id, sentiment and sep_id and -1 for end_target\n                    start_target, end_target = i+3, i+len(s_text_ids)+3-1 \n                    if (start_target - end_target) == 1:\n                        end_target += 1\n#                         print(text, '__|__', s_text)\n#                         print(text_ids)\n#                         print(start_target, end_target)\n#                         print(s_text_ids)\n#                         raise ValueError('tes')\n                    elif end_target-3 > len(text_ids):\n                        print(text, '__|__', s_text)\n                        print(text_ids)\n                        print(start_target, end_target)\n                        print(s_text_ids)\n                        raise ValueError('tes, end_target > length text ids')\n                    break\n\n            input_ids = CLS_ID + [sentiment_id[sentiment]] + SEP_ID + text_ids + SEP_ID\n            token_type_ids = [0] * 3 + [1] * (len(text_ids)+1)\n            attention_mask = [1] * len(input_ids)\n\n            input_ids = pad_or_truncate(input_ids, max_len, SEP_ID, is_input_ids=True)\n            token_type_ids = pad_or_truncate(token_type_ids, max_len, SEP_ID)\n            attention_mask = pad_or_truncate(attention_mask, max_len, SEP_ID)\n            \n            if len(offsets) < max_len:\n                offsets = offsets + ( [(0,0)] * (max_len - len(offsets)) )\n            else:\n                offsets = offsets[:max_len]\n\n            if start_target == 0 or end_target == 0:\n                print(text, '_|_', s_text)\n                print(tokenizer.tokenize(text), '_|_', tokenizer.tokenize(s_text))\n                print(text_ids)\n                print(s_text_ids)\n                raise ValueError('tes')\n\n            albert_input = { 'input_ids':torch.tensor(input_ids).to(device),\n                            'token_type_ids':torch.tensor(token_type_ids).to(device),\n                            'attention_mask':torch.tensor(attention_mask).to(device),\n                            'start_targets':torch.tensor(start_target).to(device),\n                            'end_targets':torch.tensor(end_target).to(device),\n                            'offsets':torch.tensor(offsets).to(device),\n                            'original_texts':text, \n                            'sentiments':sentiment, \n            }\n            all_data.append(albert_input)\n    else:\n        for idx, row in df.iterrows():\n            text = ' '+ ' '.join( row.text.lower().split() )\n            sentiment = row.sentiment.strip()\n            text_ids, offsets = o_tokenizer.encode(text)\n\n            input_ids = CLS_ID + [sentiment_id[sentiment]] + SEP_ID + text_ids + SEP_ID\n            token_type_ids = [0] * 3 + [1] * (len(text_ids)+1)\n            attention_mask = [1] * len(input_ids)\n\n            input_ids = pad_or_truncate(input_ids, max_len, SEP_ID, is_input_ids=True)\n            token_type_ids = pad_or_truncate(token_type_ids, max_len, SEP_ID)\n            attention_mask = pad_or_truncate(attention_mask, max_len, SEP_ID)\n            if len(offsets) < max_len:\n                offsets = offsets + ( [(0,0)] * (max_len - len(offsets)) )\n            else:\n                offsets = offsets[:max_len]\n\n            albert_input = { 'input_ids':torch.tensor(input_ids).to(device),\n                            'token_type_ids':torch.tensor(token_type_ids).to(device),\n                            'attention_mask':torch.tensor(attention_mask).to(device),\n                            'offsets':torch.tensor(offsets).to(device),\n                            'original_texts':text, \n                            'sentiments':sentiment, \n                            \n            }\n            all_data.append(albert_input)\n            \n    return all_data\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_dataloader(albert_inputs, split=True, val_size=0.2, batch_size=32):\n    if split:\n        val_size = math.floor(len(albert_inputs)*val_size)\n        train_data = albert_inputs[val_size:]\n        val_data = albert_inputs[:val_size]\n        train_dataloader = DataLoader(train_data, batch_size=batch_size)\n        val_dataloader = DataLoader(val_data, batch_size=batch_size)\n\n        return train_dataloader, val_dataloader\n    else:\n        return DataLoader(albert_inputs, batch_size=batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"albert_inputs = data_extractor(train)\ntrain_dataloader, val_dataloader = generate_dataloader(albert_inputs, split=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"albert_inputs[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len ( (albert_inputs[0]['offsets'].sum(1) > 0).nonzero() ) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# class TweetSelectionModel(transformers.AlbertPreTrainedModel):\nclass TweetSelectionModel(transformers.AlbertPreTrainedModel):\n    def __init__(self, conf, hidden_dim=768):\n        super(TweetSelectionModel, self).__init__(conf, hidden_dim)\n        self.albert = transformers.AlbertModel.from_pretrained(path_albert_model, \n                                                               config=conf, from_tf=False)\n        self.drop_out = nn.Dropout(0.3)\n        self.linear = nn.Linear(hidden_dim*2, 2)\n        torch.nn.init.normal_(self.linear.weight, std=0.02)\n    \n    def forward(self, ids, token_type_ids, mask):\n        # config.output hiddenstates = True\n        _, _, out = self.albert(\n            ids,\n            token_type_ids=token_type_ids,\n            attention_mask=mask,\n        )\n\n        out = torch.cat((out[-1], out[-2]), dim=-1)\n#         out = torch.cat((out[:,-1,:], out[:,-2,:]), dim=-1)\n#         print('concat', out.shape)\n        out = self.drop_out(out)\n        logits = self.linear(out)\n\n        start_logits, end_logits = logits.split(1, dim=-1)\n\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n\n        return start_logits, end_logits\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def loss_fn(start_logits, end_logits, start_positions, end_positions):\n    \"\"\"\n    Return the sum of the cross entropy losses for both the start and end logits\n    \"\"\"\n    loss_fct = nn.CrossEntropyLoss()\n    start_loss = loss_fct(start_logits, start_positions)\n    end_loss = loss_fct(end_logits, end_positions)\n    total_loss = (start_loss + end_loss)\n    return total_loss\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def eval(data, start_outputs, end_outputs):\n    \"\"\"evaluation data in batch\"\"\"\n    scores = []\n    input_ids = data['input_ids']\n    start_targets = data['start_targets']\n    end_targets = data['end_targets']\n    offsets = data['offsets']\n    texts = data['original_texts']\n    sentiments = data['sentiments']\n    c_wrong_end = 0\n    start_outputs = torch.argmax(start_outputs, dim=1)\n    end_outputs = torch.argmax(end_outputs, dim=1)\n    # looping batch size\n    for text, sentiment, offset, s_target, e_target, s_output, e_output in zip(texts, sentiments, offsets, start_targets, \n                                                                               end_targets, start_outputs, end_outputs):\n\n        s_target, e_target, s_output, e_output = s_target.item()-3, e_target.item()-3, max(s_output.item()-3, 0), max(e_output.item()-3, 0)\n        len_offset = len ( (offset.sum(1) > 0).nonzero() )\n        offset = offset.tolist()\n    \n        # handle overlap offsets\n        if s_output > e_output:\n            if sentiment == 'neutral':\n                e_output = len_offset\n            else:\n                s_output = e_output\n            c_wrong_end += 1\n        elif e_output > len_offset:\n            e_output = len_offset\n\n        s_char, e_char, s_char_output, e_char_output = offset[s_target][0], offset[e_target][1], offset[s_output][0], offset[e_output][1]\n        s_text = text[s_char:e_char]\n        pred_s_text = text[s_char_output:e_char_output]\n        \n        # I can't remove this print codes for debug, LOL\n#         print('======')\n#         print('text and pred text')\n#         print(s_text)\n#         print(pred_s_text)\n#         print(s_char, e_char, s_char_output, e_char_output)\n#         print(s_target, e_target, s_output, e_output)\n#         print(text)\n#         print(o_tokenizer.encode(text))\n#         print(tokenizer.encode(text, add_special_tokens=False))\n#         print(offset[:15])\n        \n        score_jaccard = compute_jaccard(s_text, pred_s_text)\n        scores.append(score_jaccard)\n    \n    return c_wrong_end, round( sum(scores) / len(scores), 2 )\n        \n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def eval_dataloader(model, dataloader):\n    model.eval()\n    total_c_wrong_end, jaccard_mean_score, loss_score = 0, 0, 0\n    for data in dataloader:\n        input_ids = data['input_ids'].to(device)\n        token_type_ids = data['token_type_ids'].to(device)\n        attention_mask = data['attention_mask'].to(device)\n        start_targets = data['start_targets'].to(device)\n        end_targets = data['end_targets'].to(device)\n\n        with torch.no_grad():\n            start_outputs, end_outputs = model(input_ids, token_type_ids, attention_mask)\n\n            loss = loss_fn(start_outputs, end_outputs, start_targets, end_targets)\n\n            c_wrong_end, jaccard_score = eval(data, start_outputs, end_outputs)\n            total_c_wrong_end += c_wrong_end\n\n            if loss_score == 0:\n                loss_score = loss.item()\n            else:\n                loss_score = round( (loss_score + loss.item() ) / 2, 3 )\n\n            if jaccard_mean_score == 0:\n                jaccard_mean_score = jaccard_score\n            else:\n                jaccard_mean_score = round( (jaccard_score + jaccard_mean_score) / 2, 3 )\n    \n    return total_c_wrong_end, loss_score, jaccard_mean_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(train_dataloader, val_dataloader, model, epochs=3):\n    \n    model = model.to(device)\n    optimizer = transformers.AdamW(model.parameters(), weight_decay=0.001, lr=1e-5)\n    \n    for i in range(epochs):\n        model.train()\n        jaccard_mean_score, loss_score = 0, 0\n        total_c_wrong_end = 0\n        for data in train_dataloader:\n            input_ids = data['input_ids'].to(device)\n            token_type_ids = data['token_type_ids'].to(device)\n            attention_mask = data['attention_mask'].to(device)\n            start_targets = data['start_targets'].to(device)\n            end_targets = data['end_targets'].to(device)\n            \n            model.zero_grad()\n            start_outputs, end_outputs = model(input_ids, token_type_ids, attention_mask)\n\n            loss = loss_fn(start_outputs, end_outputs, start_targets, end_targets)\n            loss.backward()\n            optimizer.step()\n\n            c_wrong_end, jaccard_score = eval(data, start_outputs, end_outputs)\n            total_c_wrong_end += c_wrong_end\n            if loss_score == 0:\n                loss_score = loss.item()\n            else:\n                loss_score = round( (loss_score + loss.item() ) / 2, 3 )\n\n            if jaccard_mean_score == 0:\n                jaccard_mean_score = jaccard_score\n            else:\n                jaccard_mean_score = round( (jaccard_score + jaccard_mean_score) / 2, 3 )\n            \n        val_total_c_wrong_end, val_loss, val_jaccard_mean_score = eval_dataloader(model, val_dataloader)\n        print('epoch:', i, end='  ')\n        print('train end < start:', str(total_c_wrong_end) + '/' + str(len(train_dataloader.dataset)), end='  ')\n        print('val end < start:', str(val_total_c_wrong_end) + '/' + str(len(val_dataloader.dataset)), end='  ')\n        print('train loss:', loss_score, end='  ')\n        print('val loss:', val_loss, end='  ')\n        print('train jaccard:', jaccard_score, end='  ')\n        print('val jaccard:', val_jaccard_mean_score, end='  ')\n        print()\n        \n#         torch.save({'model':model.state_dict(), 'optim':optimizer.state_dict()}, '/kaggle/working/albert_sentiment_extraction.pt')\n    \n    return model\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = TweetSelectionModel(albert_config)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nmodel = train_model(train_dataloader, val_dataloader, model, epochs=5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Test","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# test\nmodel.eval()\nmodel.to(device)\ndata = next(iter(val_dataloader))\nstart, stop = model(data['input_ids'], data['token_type_ids'], data['attention_mask'])\nprint( start.argmax(1), torch.argmax(stop, dim=1) )\nprint( data['start_targets'], data['end_targets'] )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_albert_input = data_extractor(test, data_test=True, random=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict_submission(df, model, device='cpu'):\n    test_albert_input = data_extractor(df, data_test=True, random=False)\n    dataloader = generate_dataloader(test_albert_input, split=False)\n    model.eval()\n    model.to(device)\n    predicts = []\n    s_outputs, e_outputs, a_s_outputs, a_e_outputs, s_chars, e_chars = [], [], [], [], [], [] # for evaluate\n    for data in dataloader:\n        input_ids = data['input_ids'].to(device)\n        token_type_ids = data['token_type_ids'].to(device)\n        attention_mask = data['attention_mask'].to(device)\n        offsets = data['offsets'].to(device)\n        texts = data['original_texts']\n        sentiments = data['sentiments']\n\n        with torch.no_grad():\n            start_outputs, end_outputs = model(input_ids, token_type_ids, attention_mask)\n            start_outputs, end_outputs = start_outputs.argmax(1), end_outputs.argmax(1)\n            \n            for text, sentiment, s_output, e_output, offset in zip(texts, sentiments, start_outputs, end_outputs, offsets):\n                s_output, e_output = max(s_output.item()-3, 0), max(e_output.item()-3, 0)\n                len_offset = len ( (offset.sum(1) > 0).nonzero() )\n\n                s_outputs.append(s_output)\n                e_outputs.append(e_output)\n                \n                offset = offset.tolist()\n                if s_output > e_output:\n                    if sentiment == 'neutral':\n                        e_output = len_offset\n                    else:\n                        s_output = e_output\n                elif e_output > len_offset:\n                    e_output = len_offset\n\n                s_char_output, e_char_output = offset[s_output][0], offset[e_output][1]\n                predict = text[s_char_output:e_char_output]\n                predicts.append(predict)\n                s_chars.append(s_char_output)\n                e_chars.append(e_char_output)\n                a_s_outputs.append(s_output)\n                a_e_outputs.append(e_output)\n    return s_outputs, e_outputs, a_s_outputs, a_e_outputs, s_chars, e_chars, predicts              ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_, _, _, _, _, _, predicts = predict_submission(test, model, device=device)\nlen(predicts), len(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicts[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicts = [p.strip() for p in predicts]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['selected_text'] = predicts\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test[['textID','selected_text']].to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Post Inspect Data\n### check and investigate data train with result of phrase predictions, char targets and index targets","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_colwidth', -1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_albert_inputs = data_extractor(train, random=False)\nori_s_targets, ori_e_targets = [], []\nfor d in test_albert_inputs:\n    ori_s_targets.append( d['start_targets'].item() )\n    ori_e_targets.append( d['end_targets'].item() )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['ori_s_target'] = ori_s_targets\ntrain['ori_e_target'] = ori_e_targets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train[:5000] # just evaluate for some data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s_outputs, e_outputs, a_s_outputs, a_e_outputs, s_chars, e_chars, t_predicts = predict_submission(train, model, device=device)\ntrain['predict'] = t_predicts\ntrain['s_target'] = s_outputs\ntrain['e_target'] = e_outputs\ntrain['a_s_target'] = s_outputs\ntrain['a_e_target'] = e_outputs\ntrain['s_char'] = s_chars\ntrain['e_char'] = e_chars","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train.sentiment == 'positive'][50:100]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# t_p = train[train.sentiment == 'negative']['predict'].values\n# t_s = train[train.sentiment == 'negative']['selected_text'].values\n# scores = []\n# for p, s in zip(t_p, t_s):\n#     score = compute_jaccard(p, s)\n#     scores.append(score)\n# print(np.mean(scores))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}