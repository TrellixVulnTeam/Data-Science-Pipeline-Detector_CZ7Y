{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# DATA.PY","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport string\nfrom tqdm import tqdm\nimport time\nimport gc\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_data(text):\n    regular_punct = list(string.punctuation)\n    for punc in regular_punct:\n        text = text.replace(punc, f\" {punc} \")\n    return text\n\ndef drop_empty_rows(df):\n    nan_value = float(\"NaN\")\n    df.replace(\"\", nan_value, inplace=True)\n\ndef data_process(path):\n    train_df = pd.read_csv(os.path.join(path, \"train.csv\"))\n    test_df = pd.read_csv(os.path.join(path, \"test.csv\"))\n\n    \"\"\"\n        Train: textID, text, selected_text, sentiment\n        Test: textID, text, sentiment\n    \"\"\"\n    ## Train\n    train_df[\"text\"] = train_df[\"text\"].astype(str).str.lower()\n    train_df[\"selected_text\"] = train_df[\"selected_text\"].astype(str).str.lower()\n    train_df[\"sentiment\"] = train_df[\"sentiment\"].astype(str).str.lower()\n    drop_empty_rows(train_df)\n\n    ## Test\n    test_df[\"text\"] = test_df[\"text\"].astype(str).str.lower()\n    test_df[\"sentiment\"] = test_df[\"sentiment\"].astype(str).str.lower()\n    drop_empty_rows(test_df)\n\n    return train_df, test_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# METRICS.PY","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\n\ndef jaccard(str1, str2):\n    a = set(str1.lower().split())\n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))\n\ndef jaccard_distance(y_true, y_pred, smooth=100):\n    y_true = tf.cast(y_true, tf.float32)\n    y_pred = tf.cast(y_pred, tf.float32)\n    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n    sum_ = K.sum(K.abs(y_true) + K.abs(y_pred), axis=-1)\n    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n    return (1 - jac) * smooth","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# SIMPLE_GRU.PY","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport string\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, Embedding, GRU, Bidirectional, Concatenate\nfrom tensorflow.keras.layers import GlobalMaxPooling1D, GlobalAveragePooling1D, Reshape, SpatialDropout1D\nfrom tensorflow.keras.preprocessing import text, sequence\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(embedding_matrix, params):\n    inputs = Input(shape=(params[\"input_size\"],))\n    x = Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1], weights=[embedding_matrix], trainable=False)(inputs)\n    x = SpatialDropout1D(0.2)(x)\n    x = Bidirectional(GRU(params[\"gru_units\"], return_sequences=True))(x)\n    x = Concatenate()([\n        GlobalMaxPooling1D()(x),\n        GlobalAveragePooling1D()(x)\n    ])\n    x = Dense(params[\"gru_units\"], activation=\"relu\")(x)\n    y1 = Dense(params[\"input_size\"], activation=\"softmax\", name=\"y1\")(x)\n    y2 = Dense(params[\"input_size\"], activation=\"softmax\", name=\"y2\")(x)\n    model = Model(inputs=inputs, outputs=[y1, y2])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {}\nparams[\"batch_size\"] = 128\nparams[\"gru_units\"] = 256\nparams[\"epochs\"] = 100\nparams[\"input_size\"] = 32\nparams[\"embed_dim\"] = 300\nparams[\"num_words\"] = 70000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# EMBEDDINGS","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls ../input/embeddings","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_glove(word_index):\n    EMBEDDING_FILE = '../input/embeddings/glove-840B-300d.txt'\n    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n\n    all_embs = np.stack(embeddings_index.values())\n    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n    embed_size = all_embs.shape[1]\n\n    # word_index = tokenizer.word_index\n    nb_words = max_features= len(word_index)+1\n    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n    for word, i in word_index.items():\n        if i >= max_features: continue\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n            \n    return embedding_matrix \n    \ndef load_fasttext(word_index):    \n    EMBEDDING_FILE = '../input/embeddings/wiki-news-1M-300d.vec'\n    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100)\n\n    all_embs = np.stack(embeddings_index.values())\n    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n    embed_size = all_embs.shape[1]\n\n    # word_index = tokenizer.word_index\n    nb_words = max_features= len(word_index)+1\n    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n    for word, i in word_index.items():\n        if i >= max_features: continue\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n\n    return embedding_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_path = \"../input/tweet-sentiment-extraction/\"\ntrain_df, test_df = data_process(dataset_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Data from train\ntrain_text = train_df[\"text\"].apply(lambda x: clean_data(x)).values\ntrain_selected_text = train_df[\"selected_text\"].apply(lambda x: clean_data(x)).values\ntrain_sentiment = train_df[\"sentiment\"].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Data from test\ntest_text = test_df[\"text\"].apply(lambda x: clean_data(x)).values\ntest_sentiment = test_df[\"sentiment\"].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##\ntrain_size = len(train_text)\ntest_size = len(test_text)\nprint(f\"Train data: {train_size} - Test data: {test_size}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = text.Tokenizer(num_words=params[\"num_words\"], filters=\"\")\ntotal_text = list(train_text) + list(test_text)\ntokenizer.fit_on_texts(total_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_train_len = len(train_text)\nx_train = []\ny1_train = []\ny2_train = []\n\nfor i in range(total_train_len):\n    text1 = train_text[i].strip()\n    text2 = train_selected_text[i].strip()\n\n    idx1 = text1.find(text2)\n    idx2 = idx1 + len(text2) - 1\n\n    x = tokenizer.texts_to_sequences([text1])\n    y1 = np.zeros((len(text1)))\n    y2 = np.zeros((len(text1)))\n\n    y1[idx1] = 1\n    y2[idx2] = 1\n    #y[idx1:idx2] = 1.0\n\n    # text3 = text1[idx1:idx2]\n\n    x = sequence.pad_sequences(x, maxlen=params[\"input_size\"])[0]\n    y1 = sequence.pad_sequences([y1], maxlen=params[\"input_size\"])[0]\n    y2 = sequence.pad_sequences([y2], maxlen=params[\"input_size\"])[0]\n\n    x_train.append(x)\n    y1_train.append(y1)\n    y2_train.append(y2)\n\n    # print(text2)\n    # print(text3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\nstart_time = time.time()\nword_index = tokenizer.word_index\nembedding_matrix_1 = load_glove(word_index)\nembedding_matrix_2 = load_fasttext(word_index)\n\ntotal_time = (time.time() - start_time)/60.0\nprint(\"Took {0} minutes\".format(total_time))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_mean = np.mean([embedding_matrix_1, embedding_matrix_2], axis = 0)\ndel embedding_matrix_1\ndel embedding_matrix_2\n\nprint(np.shape(embedding_mean))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = build_model(embedding_mean, params)\nmodel.compile(loss=jaccard_distance, optimizer=\"adam\")\nmodel.summary()\n\nx_train = np.array(x_train)\ny1_train = np.array(y1_train)\ny2_train = np.array(y2_train)\n\ncallbacks = [\n    ReduceLROnPlateau(patience=5, monitor=\"val_loss\", factor=0.1),\n    EarlyStopping(patience=10, monitor=\"val_loss\")\n]\n\nmodel.fit(\n    x_train,\n    [y1_train, y2_train],\n    batch_size=params[\"batch_size\"],\n    epochs=params[\"epochs\"],\n    callbacks=callbacks\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_x = tokenizer.texts_to_sequences(test_text)\ntest_x = sequence.pad_sequences(test_x, maxlen=params[\"input_size\"])\nprint(len(test_x))\ntest_y1, test_y2 = model.predict(test_x)\nprint(len(test_y1), len(test_y2))\n\nanswers = []\nfor i in range(len(test_text)):\n    start = np.argmax(test_y1[i])\n    end = np.argmax(test_y2[i])\n    ans = test_text[i][start:end]\n\n    regular_punct = list(string.punctuation)\n    for punc in regular_punct:\n        ans = ans.replace(f\" {punc} \", f\"{punc}\")\n    answers.append(ans)\n\nsubmission = pd.read_csv(f\"{dataset_path}/sample_submission.csv\")\nsubmission['selected_text'] = answers\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}