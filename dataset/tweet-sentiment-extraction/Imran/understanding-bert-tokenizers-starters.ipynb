{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **BERT Base Uncased Exploration**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"> Hello All, Hope all doing good.\n> \n> The goal of this notebook is to understand how BERT tokenizers look like and little deep dive into that. \n> \n> This notebook was created for my learning and sharing the same. I will keep updating this notebook to understand different tokenizers.\n> \n> Thanks to ChrisMc and many more which led me to understand BERT more closer. ","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import torch\nfrom pytorch_pretrained_bert import BertTokenizer\n\n# Load pre-trained model tokenizer (vocabulary)\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **BERT Base uncased - Deep dive** # \n\n\n1. This vocab has around 30k tokens with around 768 features for each tokens\n2. Below is how vocab looks like\n\n   1. [PAD]\n   2. [unused0] - [unused98]\n   3. [UNK]\n   4. [CLS]\n   5. [SEP]\n   6. [MASK]\n   7. [unused99] - [unused993]\n   8. Single characters (from '!' till '~')\n   9. Whole words and subwords (starts from 'the', Reason the comes is by frequency of occurance while BERT Training)\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"with open(\"vocab.txt\", 'w') as f:\n    for token in tokenizer.vocab.keys():\n        print (token)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Single Character Tokens**# \n\n1. We can notice it has Numbers, Alphabets, special characters, roman letters, etc.\n2. We can see it also covered languages English, Tamil, hindi, Chinese\n3. We have around 997 tokens covers Single letters","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"single_chars = []\nfor token in tokenizer.vocab.keys():\n    if len(token) == 1:\n        single_chars.append(token)\n    \nprint (single_chars)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (\"Length of single_chars:\", len(single_chars))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Single Character Tokens with # **# \n\nThis is as same as previous Single char what we saw. all characters will be prefixed with # to help tokenize subwords. \n\n1. We can notice it has Numbers, Alphabets, special characters, roman letters, etc.\n2. We can see it also covered languages English, Tamil, hindi, Chinese\n3. We have around 997 tokens covers Single letters","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"single_chars_with_hash = []\nfor token in tokenizer.vocab.keys():\n    if len(token) == 3 and token[0:2] == '##':\n        single_chars_with_hash.append(token)\n    \nprint (single_chars_with_hash)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (\"Length of single_chars_with_hash:\", len(single_chars_with_hash))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Words in Corpus**# \n\nBERT uncased have around 23209 words and rest goes into subwords\n\nWe can see it contains years, states, countries, etc\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"words_in_corpus= []\nfor token in tokenizer.vocab.keys():\n    if len(token) > 2 and token[0:2] != '##':\n        words_in_corpus.append(token)\n    \nprint (words_in_corpus)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (\"How many words in the corpus:\", len(words_in_corpus))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Subwords **# \n\nBERT typically break the words into different subwords for more granularity. e.g. if we see the word doing, it will break into do + ing. \n\nWe can see the tokens and its weights given for the \"do\" and \"ing\" hash prefixed. \n\nBelow example, we can see few subwords as \"##ion\", \"##fully\", etc. We can also see some numbers also been treated as subwords\n\nWe have around 5828 subwords in this corpus. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"words_in_corpus_with_subwords= []\nfor token in tokenizer.vocab.keys():\n    if len(token) > 2 and token[0:2] == '##':\n        words_in_corpus_with_subwords.append(token)\n    \nprint (words_in_corpus_with_subwords)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (\"How many sub words in the corpus:\", len(words_in_corpus_with_subwords))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Corpus Analyis**# \n\nLooks like we have the longest word is of length 18 and on an average length being 11-14","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lentgh_of_words_in_corpus= []\nfor token in tokenizer.vocab.keys():\n    if len(token) > 2 and token[0:2] != '##':\n        lentgh_of_words_in_corpus.append(len(token))\n    \nprint (lentgh_of_words_in_corpus)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.plot(lentgh_of_words_in_corpus)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}