{"cells":[{"metadata":{},"cell_type":"markdown","source":"# About the competition"},{"metadata":{},"cell_type":"markdown","source":"*This is slighly modified problem than the sentiment analysis. Here we have to extract the sentiment sentences which cause the specific nature of senytence i.e.** which line of the sentences implies whether the sentiment is positive or negative.** This seems a little more intresting problem than the classical problem of sentiment classification.*"},{"metadata":{},"cell_type":"markdown","source":"Without further delay let's dive in completely."},{"metadata":{},"cell_type":"markdown","source":"* # Evaluation Metric"},{"metadata":{"trusted":true},"cell_type":"code","source":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well this metric is quite easy to understand from the code give above. If you don't get it below is one example for your refrence"},{"metadata":{},"cell_type":"markdown","source":"A = {0,1,2,5,6}\n\nB = {0,2,3,4,5,7,9}\n\nSolution: \n                 \n                 = J(A,B) = |A∩B| / |A∪B| \n\n                 = |{0,2,5}| / |{0,1,2,3,4,5,6,7,9}|\n                 \n                 = 3/9 = 0.33."},{"metadata":{},"cell_type":"markdown","source":"For more please refer to this :\nhttps://www.statisticshowto.datasciencecentral.com/jaccard-index/"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"# Let't get our guns ready \n***(Import Libraries)***"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \n\n# text processing libraries\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\n\n# XGBoost\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\n\n# sklearn \nfrom sklearn import model_selection\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import f1_score\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom sklearn.model_selection import GridSearchCV,StratifiedKFold,RandomizedSearchCV\n\n# matplotlib and seaborn for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# File system manangement\nimport os\n\n# Suppress warnings \nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reading all datasets available\n\nWe have total of three csv file : train , test and submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Training data\ndf_train = pd.read_csv('../input/tweet-sentiment-extraction/train.csv')\ndf_test = pd.read_csv('../input/tweet-sentiment-extraction/test.csv')\ndf_sub = pd.read_csv('../input/tweet-sentiment-extraction/sample_submission.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we can see select_text column is missing in the testing dataset ! "},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_train.shape,df_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our dataset is of nominal size for training "},{"metadata":{},"cell_type":"markdown","source":"# We gonna go all the way down,guys!!"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.isnull().sum()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Its strange that out of 27k rows only **1 text and 1 selected_text** is empty. \n\nThis make me curious. Let's dig a bit more"},{"metadata":{"trusted":true},"cell_type":"code","source":"null_columns=df_train.columns[df_train.isnull().any()]\n# print all rows with atleast one null values\nprint(df_train[df_train.isnull().any(axis=1)][null_columns])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well we both nan value is in same column. I think we have all right to dump it ;)"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.dropna(axis=0, how='any', thresh=None, subset=None, inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Moving on to test dataset.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Nothing to worry about. ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Contains positive tweets \n# Here we have a glimpse of positive cases\ndf_pos = df_train[df_train['sentiment']=='positive']\ndf_pos['text'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_neg = df_train[df_train['sentiment']=='negative']\ndf_neg['text'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_neu = df_train[df_train['sentiment']=='neutral']\ndf_neu['text'].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data distribution based on Sentiment Column"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now lets check out whether tha dataset is distributed equally or not","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['sentiment'].value_counts()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So the dataset which we have got is uneven with more more neutral text than positive and negative."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(df_train['sentiment'].value_counts().index,df_train['sentiment'].value_counts(),palette='rocket')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets check out for tes dataset what is the proportions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test['sentiment'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(df_test['sentiment'].value_counts().index,df_test['sentiment'].value_counts(),palette='rocket')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From my intution we can see the test dataset is in same proportion !!"},{"metadata":{},"cell_type":"markdown","source":"# No preprocessing! Let's do this bare-handed"},{"metadata":{},"cell_type":"markdown","source":"From what I have understand this problem the sentiment extraction does not need any text preprocessing as even the extracted sentence do have stopwords and all this!"},{"metadata":{},"cell_type":"markdown","source":"This is need for something special!! And wkt i.e. BERT( for now ;) )"},{"metadata":{},"cell_type":"markdown","source":"# BERT is HERE!!"},{"metadata":{},"cell_type":"markdown","source":"I will be using pytorch and Hugging face transformer for this task. Its quite easy to implement and use."},{"metadata":{},"cell_type":"markdown","source":"##  Install huggingface transformers library \n\nPlease be sure to keep internet on!"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load Fine-Tuned BERT-large\n"},{"metadata":{},"cell_type":"markdown","source":"For Question Answering we use the BertForQuestionAnswering class from the transformers library."},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import BertForQuestionAnswering\n\nmodel = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import BertTokenizer\n\ntokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Demo shot : Just in case for understanding"},{"metadata":{},"cell_type":"markdown","source":"## For Easy-Pissy reproducible code we can use this amazing function"},{"metadata":{"trusted":true},"cell_type":"code","source":"def answer_question(question, answer_text):\n    '''\n    Takes a `question` string and an `answer_text` string (which contains the\n    answer), and identifies the words within the `answer_text` that are the\n    answer. Prints them out.\n    '''\n    # ======== Tokenize ========\n    # Apply the tokenizer to the input text, treating them as a text-pair.\n    input_ids = tokenizer.encode(question, answer_text)\n\n    # Report how long the input sequence is.\n    #print('Query has {:,} tokens.\\n'.format(len(input_ids)))\n\n    # ======== Set Segment IDs ========\n    # Search the input_ids for the first instance of the `[SEP]` token.\n    sep_index = input_ids.index(tokenizer.sep_token_id)\n\n    # The number of segment A tokens includes the [SEP] token istelf.\n    num_seg_a = sep_index + 1\n\n    # The remainder are segment B.\n    num_seg_b = len(input_ids) - num_seg_a\n\n    # Construct the list of 0s and 1s.\n    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n\n    # There should be a segment_id for every input token.\n    assert len(segment_ids) == len(input_ids)\n\n    # ======== Evaluate ========\n    # Run our example question through the model.\n    start_scores, end_scores = model(torch.tensor([input_ids]), # The tokens representing our input text.\n                                    token_type_ids=torch.tensor([segment_ids])) # The segment IDs to differentiate question from answer_text\n\n    # ======== Reconstruct Answer ========\n    # Find the tokens with the highest `start` and `end` scores.\n    answer_start = torch.argmax(start_scores)\n    answer_end = torch.argmax(end_scores)\n\n    # Get the string versions of the input tokens.\n    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n\n    # Start with the first token.\n    answer = tokens[answer_start]\n\n    # Select the remaining answer tokens and join them with whitespace.\n    for i in range(answer_start + 1, answer_end + 1):\n        \n        # If it's a subword token, then recombine it with the previous token.\n        if tokens[i][0:2] == '##':\n            answer += tokens[i][2:]\n        \n        # Otherwise, add a space then the token.\n        else:\n            answer += ' ' + tokens[i]\n\n    return answer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text ='Spent the entire morning in a meeting w/ a vendor, and my boss was not happy w/ them. Lots of fun.I had other plans for my morning'\nquestion = 'What text is neutral?'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ans = answer_question(question, text)\nprint(ans)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Voila! This does amazingly well!!"},{"metadata":{},"cell_type":"markdown","source":"## Since this is a pretrained model we don't need to train model on the text.So we will directly apply the dataset\n\nThis may seem bizarre to few people but it's kinda cool to try."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialise the text\ndf_train['Bert_answers'] = ''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We will ask BERT three questions"},{"metadata":{"trusted":true},"cell_type":"code","source":"positive_question = 'What text is positive ?' # In case of positive sentiment\nnegative_question = 'What text is negative ?'  # In case of negative sentiment\nneutral_question = 'What text is neutral?' # In case of neutral sentiment","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"\ndf_test['selected_text'] = ''\ni = 0\nwhile(i!=df_test.shape[0]):\n    if (df_test['sentiment'].iloc[i]== 'positive'):\n        df_test['selected_text'].iloc[i] = answer_question(positive_question,df_test['text'][i])\n    elif (df_test['sentiment'].iloc[i]== 'negative'):\n        df_test['selected_text'].iloc[i] = answer_question(negative_question,df_test['text'][i])\n    else :\n        df_test['selected_text'].iloc[i] = answer_question(neutral_question,df_test['text'][i])\n    print(df_test['selected_text'].iloc[i])\n    print(i)\n    i = i+1\n    \n     \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = df_test.drop(['text','sentiment'],axis = 1)\ndf_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.set_index('textID',inplace = True)\ndf_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.to_csv('submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## This was gonna take foreever to see.We need to amp up our cpu power which is rather impossible"},{"metadata":{"trusted":true},"cell_type":"code","source":"# So we try to make efficent code","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def soc_iter(sentiment,text):\n#     if (sentiment == 'positive'):\n#         result = answer_question(positive_question,text)\n#     elif (sentiment == 'negative'):\n#         result = answer_question(negative_question,text)\n#     else :\n#         result = answer_question(neutral_question,text)\n        \n    ","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# %%timeit\n# df_test['selected_text'] = ''\n# draw_series = []\n# for index, row in df_test.iterrows():\n#     draw_series.append(soc_iter(row['sentiment'],row['text']))\n#     print(index)\n    \n# df_test['selected_text'] = draw_series","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can see this method even though 321 times faster but still is gonna take an hour to complete","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}