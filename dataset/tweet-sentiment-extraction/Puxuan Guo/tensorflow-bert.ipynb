{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# TensorFlow BERT","metadata":{}},{"cell_type":"markdown","source":"# Load Libraries, Data, Tokenizer","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport tokenizers\nfrom transformers import BertTokenizer, BertConfig, TFBertModel\nfrom tqdm import tqdm\ntqdm.pandas()","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_LEN = 128\nBERT_CONFIG = '/kaggle/input/bertconfig/bert-base-uncased-config.json'\nBERT_PATH = '/kaggle/input/bert-base-uncased-huggingface-transformer//bert-base-uncased-tf_model.h5'\nTOKENIZER = tokenizers.BertWordPieceTokenizer(\"/kaggle/input/bert-base-uncased-huggingface-transformer//bert-base-uncased-vocab.txt\", lowercase=True)\n\ntrain = pd.read_csv('../input/tweet-sentiment-extraction/train.csv').fillna('')\ntest = pd.read_csv('../input/tweet-sentiment-extraction/test.csv').fillna('')\nsubmission = pd.read_csv('../input/tweet-sentiment-extraction/sample_submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training Data","metadata":{}},{"cell_type":"code","source":"def process_data(tweet, selected_text, tokenizer):\n    len_st = len(selected_text)\n    idx0 = None\n    idx1 = None\n    \n    for ind in (i for i, e in enumerate(tweet) if e == selected_text[0]):\n        if tweet[ind: ind+len_st] == selected_text:\n            idx0 = ind\n            idx1 = ind + len_st\n            break\n    \n    char_targets = [0] * len(tweet)\n    if idx0 != None and idx1 != None:\n        for ct in range(idx0, idx1):\n            char_targets[ct] = 1\n    tok_tweet = tokenizer.encode(tweet)\n    input_ids_orig = tok_tweet.ids\n    tweet_offsets = tok_tweet.offsets\n\n    target_idx = []\n    for j, (offset1, offset2) in enumerate(tweet_offsets):\n        if sum(char_targets[offset1: offset2]) > 0:\n            target_idx.append(j)\n    targets = [0] * len(input_ids_orig)\n    for idx in target_idx:\n        targets[idx] = 1\n    return targets\n\ndef convert_to_transformer_inputs(text, tokenizer, max_length):\n    inputs = tokenizer.encode(text)\n    input_ids =  inputs.ids\n    input_masks = inputs.attention_mask\n    input_segments = inputs.type_ids\n    padding_length = max_length - len(input_ids)\n    padding_id = 0\n    input_ids = input_ids + ([padding_id] * padding_length)\n    input_masks = input_masks + ([0] * padding_length)\n    input_segments = input_segments + ([0] * padding_length)\n    return [input_ids, input_masks, input_segments]\n\ntrain['targets'] = train.progress_apply(lambda row: process_data(str(row['text']), str(row['selected_text']),TOKENIZER),axis=1)\ntrain['targets'] = train['targets'].apply(lambda x :x + [0] * (MAX_LEN-len(x)))\n\ninput_ids, input_masks, input_segments = [], [], []\nfor _, instance in tqdm(train.iterrows()):\n    ids, masks, segments= convert_to_transformer_inputs(str(instance.text),TOKENIZER, MAX_LEN)\n    input_ids.append(ids)\n    input_masks.append(masks)\n    input_segments.append(segments)\ninputs = [np.asarray(input_ids, dtype=np.int32), np.asarray(input_masks, dtype=np.int32), np.asarray(input_segments, dtype=np.int32)]\noutputs = np.asarray(train['targets'].values.tolist())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test Data","metadata":{}},{"cell_type":"code","source":"input_ids, input_masks, input_segments = [], [], []\nfor _, instance in tqdm(test.iterrows()):\n    ids, masks, segments= convert_to_transformer_inputs(str(instance.text),TOKENIZER, MAX_LEN)\n    input_ids.append(ids)\n    input_masks.append(masks)\n    input_segments.append(segments)\ntest_inputs = [np.asarray(input_ids, dtype=np.int32), np.asarray(input_masks, dtype=np.int32), np.asarray(input_segments, dtype=np.int32)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Build BERT Model","metadata":{}},{"cell_type":"code","source":"def build_model():\n    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    mask = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    attn = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    bert_conf = BertConfig() \n    bert_model = TFBertModel.from_pretrained(BERT_PATH, config=bert_conf)\n    \n    output = bert_model(ids, attention_mask=mask, token_type_ids=attn)\n    \n    out = tf.keras.layers.Dropout(0.1)(output[0]) \n    out = tf.keras.layers.Conv1D(1,1)(out)\n    out = tf.keras.layers.Flatten()(out)\n    out = tf.keras.layers.Activation('softmax')(out)\n    model = tf.keras.models.Model(inputs=[ids, mask, attn], outputs=out)\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train BERT Model","metadata":{}},{"cell_type":"code","source":"K.clear_session()\nmodel = build_model()\noptimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\nmodel.compile(loss='categorical_crossentropy', optimizer=optimizer)\nmodel.fit(inputs,outputs, epochs=16, batch_size=32)\nmodel.save_weights(f'finetuned_bert.h5')\npredictions = model.predict(test_inputs, batch_size=32, verbose=1)\npred = np.where(predictions>0.4, 1,0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def decode(tweet,idx_start,idx_end,offsets):\n    output  = \"\"\n    for ix in range(idx_start, idx_end + 1):\n        output += tweet[offsets[ix][0]: offsets[ix][1]]\n        if (ix+1) < len(offsets) and offsets[ix][1] < offsets[ix+1][0]:\n            output += \" \"\n    return output\n\nfinal = []\nfor test_idx in range(test.shape[0]):\n    indexes = list(np.where(pred[test_idx]==1)[0])\n    text = str(test.loc[test_idx,'text'])\n    encoded_text = TOKENIZER.encode(text)\n    if len(indexes)>0:\n        start_tokens = indexes[0]\n        end_tokens =  indexes[-1]\n    else: \n        start_tokens = 0\n        end_tokens = len(encoded_text.ids) - 1\n    if start_tokens>end_tokens: \n        selected_text = test.loc[test_idx,'text']\n    else:\n        selected_text = decode(text,start_tokens,end_tokens,encoded_text.offsets)\n    final.append(selected_text)\n    \ntest['selected_text'] = final\nsubmission['selected_text'] = test['selected_text']\nsubmission.to_csv('submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}