{"cells":[{"metadata":{"_cell_guid":"51d41d53-1196-41a8-9eb6-b1d63ab9e5af","_uuid":"95355177-aa2b-4933-9b1e-195cbf5fefda","trusted":true},"cell_type":"code","source":"# !pip uninstall tensorflow\n# !pip install --ignore-installed --upgrade tensorflow-gpu\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import StratifiedKFold\nimport tensorflow as tf\nimport tokenizers\nfrom transformers import RobertaTokenizer, RobertaConfig, TFRobertaPreTrainedModel\nfrom transformers.modeling_tf_roberta import TFRobertaMainLayer,TFRobertaModel\nfrom transformers.modeling_tf_utils import get_initializer\nimport tensorflow.keras.backend as K\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n    \nprint('TF version',tf.__version__)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a744502e-4545-43f5-b1b6-d90dc3f8912a","_uuid":"016ccbcd-3c8a-47b5-bfe8-409585a476ce","trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv').fillna('')\ntest = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv').fillna('')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6905008d-c477-47dc-ba5a-6f4e86e9952d","_uuid":"e1e2bfc7-b1c4-4586-a6d9-a4ac08eb458a","trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ae7660c6-fc44-4db5-991f-6ff23f4b40ec","_uuid":"27f9a880-0731-4f5c-8acc-c16c0c91b9f7","trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"be94abd4-28a7-4fb7-917c-2340ea69006d","_uuid":"db0ce4da-dbdd-4aa4-a7a4-440f722d8f73","trusted":true},"cell_type":"code","source":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    if (len(a)==0) & (len(b)==0): return 0.5\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4e6b7ab8-6d77-4bd9-bec8-e1eee99807c5","_uuid":"61ac77cc-159f-413c-850e-b748a1973c1d","trusted":true},"cell_type":"code","source":"MAX_LEN = 96\nPATH = '../input/tf-roberta/'\ntokenizer = tokenizers.ByteLevelBPETokenizer(\n    vocab_file=PATH+'vocab-roberta-base.json', \n    merges_file=PATH+'merges-roberta-base.txt', \n    lowercase=True,\n    add_prefix_space=True\n)\nsentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2509b74f-ce59-4f0f-87af-b7cb4e1103ee","_uuid":"4e7a70c3-a81b-447e-a13d-d5ff2f2e5550","trusted":true},"cell_type":"code","source":"ct = train.shape[0]\ninput_ids = np.ones((ct,MAX_LEN),dtype='int32')\nattention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\ntoken_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\nstart_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\nend_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n\nfor k in range(train.shape[0]):\n    \n    # FIND OVERLAP\n    text1 = \" \"+\" \".join(str(train.loc[k,'text']).split()) # TODO achar o float\n    text2 = \" \".join(str(train.loc[k,'selected_text']).split())\n    idx = text1.find(text2)\n    chars = np.zeros((len(text1)))\n    chars[idx:idx+len(text2)]=1\n    if text1[idx-1]==' ': chars[idx-1] = 1 \n    enc = tokenizer.encode(text1) \n        \n    # ID_OFFSETS\n    offsets = []; idx=0\n    for t in enc.ids:\n        w = tokenizer.decode([t])\n        offsets.append((idx,idx+len(w)))\n        idx += len(w)\n    \n    # START END TOKENS\n    toks = []\n    for i,(a,b) in enumerate(offsets):\n        sm = np.sum(chars[a:b])\n        if sm>0: toks.append(i) \n        \n    s_tok = sentiment_id[train.loc[k,'sentiment']]\n    input_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n    attention_mask[k,:len(enc.ids)+5] = 1\n    if len(toks)>0:\n        start_tokens[k,toks[0]+1] = 1\n        end_tokens[k,toks[-1]+1] = 1","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e4a2ad1c-2162-40ba-b8b2-bb9498ec5119","_uuid":"f33bb2f2-46c7-45e9-9104-929a621635d5","trusted":true},"cell_type":"code","source":"ct = test.shape[0]\ninput_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\nattention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\ntoken_type_ids_t = np.zeros((ct,MAX_LEN),dtype='int32')\n\nfor k in range(test.shape[0]):\n        \n    # INPUT_IDS\n    text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n    enc = tokenizer.encode(text1)                \n    s_tok = sentiment_id[test.loc[k,'sentiment']]\n    input_ids_t[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n    attention_mask_t[k,:len(enc.ids)+5] = 1","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"04e89458-5dbf-4ddb-8a84-95f4fbf4a229","_uuid":"6a3a88c8-23a3-40ea-a77a-c881ce418392","trusted":true},"cell_type":"code","source":"def build_model():\n    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n\n    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n    x = bert_model(ids,attention_mask=att,token_type_ids=tok)\n\n    x1 = tf.keras.layers.Dropout(0.1)(x[0]) \n    x1 = tf.keras.layers.Conv1D(128, 2,padding='same')(x1)\n    x1 = tf.keras.layers.LeakyReLU()(x1)\n    x1 = tf.keras.layers.Conv1D(64, 2,padding='same')(x1)\n    x1 = tf.keras.layers.Dense(1)(x1)\n#     x1 = tf.keras.layers.Bidirectional(\n#             tf.keras.layers.LSTM(\n#                 units=48,\n#                 dropout=0.3,\n#                 return_sequences=False\n#             )\n#         )(x1)\n    x1 = tf.keras.layers.Flatten()(x1)\n \n    x1 = tf.keras.layers.Activation('softmax')(x1)\n    \n    x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n    x2 = tf.keras.layers.Conv1D(128, 2, padding='same')(x2)\n    x2 = tf.keras.layers.LeakyReLU()(x2)\n    x2 = tf.keras.layers.Conv1D(64, 2, padding='same')(x2)\n    x2 = tf.keras.layers.Dense(1)(x2)\n#     x2 = tf.keras.layers.Bidirectional(\n#             tf.keras.layers.LSTM(\n#                 units=48,\n#                 dropout=0.3,\n#                 return_sequences=False\n#             )\n#         )(x2) \n    x2 = tf.keras.layers.Flatten()(x2)\n    \n    x2 = tf.keras.layers.Activation('softmax')(x2)\n\n\n#     x1 = tf.keras.layers.Bidirectional(\n#             tf.keras.layers.LSTM(\n#                 units=128,\n#                 dropout=0.3,\n#                 return_sequences=True\n#             )\n#         )(x[0])\n\n#     x1 = tf.keras.layers.Bidirectional(\n#             tf.keras.layers.LSTM(\n#                 units=128,\n#                 dropout=0.3,\n#                 return_sequences=False\n#             )\n#         )(x1)    \n#     x1 = tf.keras.layers.Dense(128, activation='relu')(x1)\n#     x1 = tf.keras.layers.Dropout(0.3)(x1)\n#     x1 = tf.keras.layers.Flatten()(x1)\n#     x1 = tf.keras.layers.Dense(1, activation='softmax')(x1)\n    \n    \n#     x2 = tf.keras.layers.Bidirectional(\n#             tf.keras.layers.LSTM(\n#                 units=128,\n#                 dropout=0.3,\n#                 return_sequences=True\n#             )\n#         )(x[0])\n\n#     x2 = tf.keras.layers.Bidirectional(\n#             tf.keras.layers.LSTM(\n#                 units=128,\n#                 dropout=0.3,\n#                 return_sequences=False\n#             )\n#         )(x2)    \n#     x2 = tf.keras.layers.Dense(128, activation='relu')(x2)\n#     x2 = tf.keras.layers.Dropout(0.3)(x2)\n#     x2 = tf.keras.layers.Flatten()(x2)\n#     x2 = tf.keras.layers.Dense(1, activation='softmax')(x2)\n    \n    \n    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1, x2])\n    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def lr(epoch):\n    return 3e-5 * 0.2**epoch\n\n# schedule = tf.keras.callbacks.LearningRateScheduler(lr, verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"28a994f9-5f17-475c-9f64-c3436c15e679","_uuid":"f41f4fdb-dc74-4a59-87c0-e370e63f252d","trusted":true},"cell_type":"code","source":"jac = []; VER='v0'; DISPLAY=2 # USE display=1 FOR INTERACTIVE\noof_start = np.zeros((input_ids.shape[0],MAX_LEN))\noof_end = np.zeros((input_ids.shape[0],MAX_LEN))\npreds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\npreds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\nFOLDS = 5\n\nskf = StratifiedKFold(n_splits=FOLDS,shuffle=True,random_state=777)\nfor fold,(idxT,idxV) in enumerate(skf.split(input_ids,train.sentiment.values)):\n\n    print('#'*25)\n    print('### FOLD %i'%(fold+1))\n    print('#'*25)\n    \n    K.clear_session()\n    model = build_model()\n#     print(model.summary())\n        \n#     sv = tf.keras.callbacks.ModelCheckpoint(\n#         '%s-roberta-%i.h5'%(VER,fold), monitor='val_loss', verbose=1, save_best_only=True,\n#         save_weights_only=True, mode='auto', save_freq='epoch')\n \n#     model.fit([input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]], [start_tokens[idxT,], end_tokens[idxT,]], \n#         epochs=5, batch_size=60, verbose=DISPLAY, callbacks=[sv], validation_data=([input_ids[idxV,],\n#         attention_mask[idxV,],token_type_ids[idxV,]], [start_tokens[idxV,], end_tokens[idxV,]]))\n    \n    print('Loading model...')\n    model.load_weights('/kaggle/input/robertaweights1/%s-roberta-%i.h5'%(VER,fold))#\n    \n    print('Predicting OOF...')\n    oof_start[idxV,],oof_end[idxV,] = model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],\n                                                    verbose=DISPLAY)\n    \n    print('Predicting Test...')\n    preds = model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n    preds_start += preds[0]/skf.n_splits\n    preds_end += preds[1]/skf.n_splits\n    \n    all = []\n    for k in idxV:\n        a = np.argmax(oof_start[k,])\n        b = np.argmax(oof_end[k,])\n        if a>b: \n            st = train.loc[k,'text'] # IMPROVE CV/LB with better choice here\n        else:\n            text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n            enc = tokenizer.encode(text1)\n            st = tokenizer.decode(enc.ids[a-1:b])\n        all.append(jaccard(st,train.loc[k,'selected_text']))\n    jac.append(np.mean(all))\n    print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all))\n    print()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4496e9d3-ed8c-4fd8-a319-9212464d85b6","_uuid":"2e21136d-0e36-481d-983c-136087ee1643","trusted":true},"cell_type":"code","source":"print('>>>> OVERALL 5Fold CV Jaccard =',np.mean(jac))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7feff427-9056-4c08-8e5c-170f6732aa2c","_uuid":"37eb1e83-be9b-4f5a-9098-d229d466f8a3","trusted":true},"cell_type":"code","source":"all = []\nfor k in range(input_ids_t.shape[0]):\n    a = np.argmax(preds_start[k,])\n    b = np.argmax(preds_end[k,])\n    if a>b: \n        st = test.loc[k,'text']\n    else:\n        text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode(enc.ids[a-1:b])\n    all.append(st)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0e25826e-fda4-445e-9185-1dc97297ce62","_uuid":"e9280210-84a2-4ed3-8f57-aeb73e4976a4","trusted":true},"cell_type":"code","source":"test['selected_text'] = ['\"{}\"'.format(x) for x in all]\ntest[['textID','selected_text']].to_csv('submission.csv',index=False)\npd.set_option('max_colwidth', 60)\ntest.sample(25)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"51ce4113-fbc4-4403-a22a-fabf1e065ddc","_uuid":"5e2dfa8a-1d05-41a4-aab2-411983fa07f3","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":4}