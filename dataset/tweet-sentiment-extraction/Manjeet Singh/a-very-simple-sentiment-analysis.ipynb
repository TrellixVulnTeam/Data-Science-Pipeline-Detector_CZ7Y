{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd \nimport numpy as np\nimport re\n\n# Import the string dictionary that we'll use to remove punctuation\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\n\n# sklearn \nfrom sklearn import model_selection\n# CountVectorizer will help calculate word counts\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n\n# File system manangement\nimport os","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import datasets\ntrain = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv')\ntest = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv')\nsample = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.dropna(axis = 0, how ='any',inplace=True) ;","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Positive tweet\nprint(\"Positive Tweet example :\",train[train['sentiment']=='positive']['text'].values[0])\n#negative_text\nprint(\"Negative Tweet example :\",train[train['sentiment']=='negative']['text'].values[0])\n#neutral_text\nprint(\"Neutral tweet example  :\",train[train['sentiment']=='neutral']['text'].values[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['sentiment'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['sentiment'].value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n\n\ndef text_preprocessing(text):\n    \"\"\"\n    Cleaning and parsing the text.\n\n    \"\"\"\n    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n    nopunc = clean_text(text)\n    tokenized_text = tokenizer.tokenize(nopunc)\n    #remove_stopwords = [w for w in tokenized_text if w not in stopwords.words('english')]\n    combined_text = ' '.join(tokenized_text)\n    return combined_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['text_clean'] = train['text'].apply(str).apply(lambda x: text_preprocessing(x))\ntest['text_clean'] = test['text'].apply(str).apply(lambda x: text_preprocessing(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['text_len'] = train['text_clean'].astype(str).apply(len)\ntrain['text_word_count'] = train['text_clean'].apply(lambda x: len(str(x).split()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['text'] = train['text'].apply(lambda x: x.lower())\ntest['text'] = test['text'].apply(lambda x: x.lower())\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_val = train_test_split(\n    train, train_size = 0.80, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pos_train = X_train[X_train['sentiment'] == 'positive']\nneutral_train = X_train[X_train['sentiment'] == 'neutral']\nneg_train = X_train[X_train['sentiment'] == 'negative']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use CountVectorizer to get the word counts within each dataset\n\ncv = CountVectorizer(max_df=0.95, min_df=2,\n                                     max_features=10000,\n                                     stop_words='english')\n\nX_train_cv = cv.fit_transform(X_train['text'])\n\nX_pos = cv.transform(pos_train['text'])\nX_neutral = cv.transform(neutral_train['text'])\nX_neg = cv.transform(neg_train['text'])\n\npos_count_df = pd.DataFrame(X_pos.toarray(), columns=cv.get_feature_names())\nneutral_count_df = pd.DataFrame(X_neutral.toarray(), columns=cv.get_feature_names())\nneg_count_df = pd.DataFrame(X_neg.toarray(), columns=cv.get_feature_names())\n\n# Create dictionaries of the words within each sentiment group, where the values are the proportions of tweets that \n# contain those words\n\npos_words = {}\nneutral_words = {}\nneg_words = {}\n\nfor k in cv.get_feature_names():\n    pos = pos_count_df[k].sum()\n    neutral = neutral_count_df[k].sum()\n    neg = neg_count_df[k].sum()\n    \n    pos_words[k] = pos/pos_train.shape[0]\n    neutral_words[k] = neutral/neutral_train.shape[0]\n    neg_words[k] = neg/neg_train.shape[0]\n    \n# We need to account for the fact that there will be a lot of words used in tweets of every sentiment.  \n# Therefore, we reassign the values in the dictionary by subtracting the proportion of tweets in the other \n# sentiments that use that word.\n\nneg_words_adj = {}\npos_words_adj = {}\nneutral_words_adj = {}\n\nfor key, value in neg_words.items():\n    neg_words_adj[key] = neg_words[key] - (neutral_words[key] + pos_words[key])\n    \nfor key, value in pos_words.items():\n    pos_words_adj[key] = pos_words[key] - (neutral_words[key] + neg_words[key])\n    \nfor key, value in neutral_words.items():\n    neutral_words_adj[key] = neutral_words[key] - (neg_words[key] + pos_words[key])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def calculate_selected_text(df_row, tol = 0):\n    \n    tweet = df_row['text']\n    sentiment = df_row['sentiment']\n    \n    if(sentiment == 'neutral'):\n        return tweet\n    \n    elif(sentiment == 'positive'):\n        dict_to_use = pos_words_adj # Calculate word weights using the pos_words dictionary\n    elif(sentiment == 'negative'):\n        dict_to_use = neg_words_adj # Calculate word weights using the neg_words dictionary\n        \n    words = tweet.split()\n    words_len = len(words)\n    subsets = [words[i:j+1] for i in range(words_len) for j in range(i,words_len)]\n    \n    score = 0\n    selection_str = '' # This will be our choice\n    lst = sorted(subsets, key = len) # Sort candidates by length\n    \n    \n    for i in range(len(subsets)):\n        \n        new_sum = 0 # Sum for the current substring\n        \n        # Calculate the sum of weights for each word in the substring\n        for p in range(len(lst[i])):\n            if(lst[i][p].translate(str.maketrans('','',string.punctuation)) in dict_to_use.keys()):\n                new_sum += dict_to_use[lst[i][p].translate(str.maketrans('','',string.punctuation))]\n            \n        # If the sum is greater than the score, update our current selection\n        if(new_sum > score + tol):\n            score = new_sum\n            selection_str = lst[i]\n            #tol = tol*5 # Increase the tolerance a bit each time we choose a selection\n\n    # If we didn't find good substrings, return the whole text\n    if(len(selection_str) == 0):\n        selection_str = words\n        \n    return ' '.join(selection_str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.options.mode.chained_assignment = None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tol = 0.001\n\nX_val['predicted_selection'] = ''\n\nfor index, row in X_val.iterrows():\n    \n    selected_text = calculate_selected_text(row, tol)\n    \n    X_val.loc[X_val['textID'] == row['textID'], ['predicted_selection']] = selected_text","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Understanding the Evaluation Metric\n-\n-Jaccard Score is a measure of how similar/dissimilar two sets are. The higher the score, the more similar the two strings. The idea is to find the number of common tokens and divide it by the total number of unique tokens","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_val['jaccard'] = X_val.apply(lambda x: jaccard(x['selected_text'], x['predicted_selection']), axis = 1)\n\nprint('The jaccard score for the validation set is:', np.mean(X_val['jaccard']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pos_tr = train[train['sentiment'] == 'positive']\nneutral_tr = train[train['sentiment'] == 'neutral']\nneg_tr = train[train['sentiment'] == 'negative']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv = CountVectorizer(max_df=0.95, min_df=2,\n                                     max_features=10000,\n                                     stop_words='english')\n\nfinal_cv = cv.fit_transform(train['text'])\n\nX_pos = cv.transform(pos_tr['text'])\nX_neutral = cv.transform(neutral_tr['text'])\nX_neg = cv.transform(neg_tr['text'])\n\npos_final_count_df = pd.DataFrame(X_pos.toarray(), columns=cv.get_feature_names())\nneutral_final_count_df = pd.DataFrame(X_neutral.toarray(), columns=cv.get_feature_names())\nneg_final_count_df = pd.DataFrame(X_neg.toarray(), columns=cv.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pos_words = {}\nneutral_words = {}\nneg_words = {}\n\nfor k in cv.get_feature_names():\n    pos = pos_final_count_df[k].sum()\n    neutral = neutral_final_count_df[k].sum()\n    neg = neg_final_count_df[k].sum()\n    \n    pos_words[k] = pos/(pos_tr.shape[0])\n    neutral_words[k] = neutral/(neutral_tr.shape[0])\n    neg_words[k] = neg/(neg_tr.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"neg_words_adj = {}\npos_words_adj = {}\nneutral_words_adj = {}\n\nfor key, value in neg_words.items():\n    neg_words_adj[key] = neg_words[key] - (neutral_words[key] + pos_words[key])\n    \nfor key, value in pos_words.items():\n    pos_words_adj[key] = pos_words[key] - (neutral_words[key] + neg_words[key])\n    \nfor key, value in neutral_words.items():\n    neutral_words_adj[key] = neutral_words[key] - (neg_words[key] + pos_words[key])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tol = 0.001\n\nfor index, row in test.iterrows():\n    \n    selected_text = calculate_selected_text(row, tol)\n    \n    sample.loc[sample['textID'] == row['textID'], ['selected_text']] = selected_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample.to_csv('submission.csv', index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}