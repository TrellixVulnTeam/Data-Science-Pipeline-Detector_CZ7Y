{"cells":[{"metadata":{},"cell_type":"markdown","source":"In this notebook we will use gradient boosting for sentence extraction. In the first part we change our target to binary classification. We will predict does the text completely corresponds to a sentiment.  ","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.metrics.pairwise import cosine_distances as cosine\nfrom catboost import CatBoostClassifier, Pool, cv\n\nimport string\nimport re\n\nimport nltk\nfrom nltk.corpus import stopwords\nimport hunspell\n\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn import metrics\nfrom sklearn.model_selection import KFold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/tweet-sentiment-extraction/train.csv')\ntest = pd.read_csv('../input/tweet-sentiment-extraction/test.csv')\nsample_sub = pd.read_csv('../input/tweet-sentiment-extraction/sample_submission.csv')\n\ntrain = train.drop(314)\ntrain.index = np.arange(27480)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We clean up texts and add some information about it.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"table_empty = str.maketrans({key: None for key in (string.punctuation + ' ')})\ntable_space = str.maketrans({key: ' ' for key in string.punctuation})\n\nhobj = hunspell.HunSpell('/usr/share/hunspell/en_US.dic', '/usr/share/hunspell/en_US.aff')\n\ngood_smile = ['=P', '<3', ':)', '(:', ';)', '=D', ';D', 'xD', ':-D', '=}', '=]',\n              '(=', '^-^', ':-*', ';-)', ':]', ':>', '*-*', '^.^', '^^']\nbad_smile = ['D:', ':(', ':[', '=(', ' :/ ', '):', 'D:', ')=', '>=[', \":'(\",\n             ':-/', ':`(', ':-o', ':|', ':-/', ':@', ':[', ':-|', ':o', '={']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def TextCleaner(text):\n    text = re.sub(r'https?:\\/\\/\\S*', 'huperlink', text) #huperlink\n    text = re.sub('[_@]\\S*', 'username', text) #username\n    text = re.sub('\\s', ' ', text) # '\\t', '\\n' to ' '\n    text = re.sub('[ì,í,î,ï]', 'i', text)\n    text = re.sub('[À,Á,Â,Ã,Ä,Å]', 'A', text)\n    text = re.sub(' w/o', ' without', text)\n    text = re.sub(' b/c', ' because', text)\n    text = re.sub(' w/', ' with', text)\n    text = re.sub(' n ', ' and ', text) \n    text = re.sub(' u ', ' you ', text)\n    text = re.sub(' r ', ' are ', text)\n    text = re.sub(' u ', ' you ', text)\n    text = re.sub(' U ', ' You ', text)\n    text = re.sub(' ppl', ' peolpe ', text)\n    text = re.sub(' pls ', ' please ', text)\n    text = re.sub(' coz ', ' cause ', text)\n    text = re.sub(' cuz ', ' cause ', text)\n    text = re.sub(' cos ', ' cause ', text)\n    text = re.sub(' wat ', ' what ', text)\n    text = re.sub(' \\*\\*\\*\\* ', ' fuck ', text)\n    text = re.sub(' +', ' ', text)\n    text = re.sub(\"`\", \"'\", text)\n    text = ''.join(map(WordCleaner, \n                       re.split('(\\W+)', text.lower().strip())))\n    return text\n\ndef replace(text):\n    st = text\n    for char in set(text):\n        if char in string.punctuation:\n            continue\n        pattern = char + '{2,}'\n        st = re.sub(pattern, char, st) \n    return st\ndef replace2(text):\n    st = text\n    for char in set(text):\n        if char in string.punctuation:\n            continue\n        pattern = char + '{3,}'\n        st = re.sub(pattern, char+char, st) \n    return st\n\ndef WordCleaner(word):\n    if len(word.translate(table_empty)) == 0 or hobj.spell(word):\n        return word\n    if hobj.spell(re.sub('0', 'o', word)):\n        return re.sub('0', 'o', word)\n    if hobj.spell(replace(word)):\n        return replace(word)\n    if hobj.spell(replace2(word)):\n        return replace2(word)\n    some_ideas = hobj.suggest(word)\n    if (len(word) > 3 and len(some_ideas) > 0 and len(some_ideas[0].split()) < 2 \n        and nltk.edit_distance(word, some_ideas[0]) < 2):\n        return some_ideas[0]\n    return word\n\ndef character_filter(c):\n    if c == \"\\t\": return \" \"\n    if ord(c)<128: return c\n    if c in \"≠•∞™ˈʃʊʁʁiʁɑ̃ʃɔ.̃ºª¶§¡£¢ç\": return \"z\"\n    if c in \"àáâãäåæ\": return \"a\"\n    if c in \"èéêë\": return \"e\"\n    if c in \"ìíîï\": return \"i\"\n    if c in \"òóôõöōŏő\": return \"o\"\n    if c in \"ùúûü\": return \"u\"\n\n    if c in \"ÀÁÂÃÄÅ\": return \"A\"\n    if c in \"ÈÉÊË\": return \"E\"\n    if c in \"ÌÍÎÏ\": return \"I\" \n    if c in \"ÒÓÔÕÖŌŎŐ\": return \"O\"\n    if c in \"ÙÚÛÜ\": return \"U\"\n\n    return \"z\"\n\ndef text_filter(text):\n    return \"\".join(map(character_filter, text))\n\n\ndef IsOk(string):\n    if hobj.spell(string) != -1:\n        return True\n    if hobj.spell(re.sub('0', 'o', string)) != -1:\n        return True\n    if hobj.spell(replace(string)) != -1:\n        return True\n    if hobj.spell(replace2(string)) != -1:\n        return True\n    for r in range(3,len(string)-3):\n        if hobj.spell(string[:r]) and IsOk(string[r:]):\n            return True\n    some_ideas = hobj.suggest(string)\n    if len(some_ideas) > 0 and nltk.edit_distance(string, some_ideas[0]) < 2:\n        return True\n    return False\n\ndef WordTransform(word):\n    word = text_filter(word)\n    if len(word) == 0 or word[0] == '@' or word[0] == '_' or word.find('http') != -1:\n        return ''\n    a = word.find('****') \n    if a != -1:\n        word = word[:a] + 'fuck' + word[:(a+4)]\n    word = word.translate(table_empty).lower()\n    if len(word) == 0 or hobj.spell(word):\n        return word\n    if hobj.spell(re.sub('0', 'o', word)):\n        return re.sub('0', 'o', word)\n    if hobj.spell(replace(word)):\n        return replace(word)\n    if hobj.spell(replace2(word)):\n        return replace2(word)\n    for r in range(3,len(word)-3):\n        if hobj.spell(word[:r]) and IsOk(word[r:]):\n            return word[:r] + ' ' + WordTransform(word[r:])\n    some_ideas = hobj.suggest(word)\n    if len(some_ideas) > 0 and nltk.edit_distance(word, some_ideas[0]) < 2:\n        return some_ideas[0]\n    return word","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['good'] = np.array(list(map(lambda text: sum([text.count(smile) for smile in good_smile]), \n                  train['text'])))\ntrain['bad'] = np.array(list(map(lambda text: sum([text.count(smile) for smile in bad_smile]), \n                  train['text'])))\ntest['good'] = np.array(list(map(lambda text: sum([text.count(smile) for smile in good_smile]), \n                  test['text'])))\ntest['bad'] = np.array(list(map(lambda text: sum([text.count(smile) for smile in bad_smile]), \n                  test['text'])))\n\ntrain['cleaned_text'] = list(map(TextCleaner, train['text']))\n                             \ntest['cleaned_text'] = list(map(TextCleaner, test['text']))\n\ntrain['exclamation'] = np.array(list(map(lambda x: x.count('!'), train['text'])))\ntest['exclamation'] = np.array(list(map(lambda x: x.count('!'), test['text'])))\n\ntrain['CapsLock'] = np.array(list(map(lambda text: np.array(list(map(str.isupper, text))).mean(), train['text'])))\ntest['CapsLock'] = np.array(list(map(lambda text: np.array(list(map(str.isupper, text))).mean(), test['text'])))\n\ntrain['lenght'] = np.array(list(map(len, train['cleaned_text'])))\ntest['lenght'] = np.array(list(map(len, test['cleaned_text'])))\n\ntrain['missed'] = np.array(list(map(lambda x: len(x) - x.count(' '),\n                  train['text']))) - np.array(list(map(lambda x: len(x) - x.count(' '), \n                                                       train['cleaned_text'])))\ntest['missed'] = np.array(list(map(lambda x: len(x) - x.count(' '),\n                  test['text']))) - np.array(list(map(lambda x: len(x) - x.count(' '), \n                                                       test['cleaned_text'])))\n\ntrain['url'] = np.array(list(map(lambda x: x.find('http') != -1, train['text']))).astype(int)\ntest['url'] = np.array(list(map(lambda x: x.find('http') != -1, test['text']))).astype(int)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Our target\n\nIn 'take_all' column we mark by '1' twits which are fully selected to needed sentiment, and by '0' others.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def count_len(text):\n    words = text.split(' ')\n    return len(list(filter(lambda word: len(word.translate(table_empty)) > 1 and word.count('http://') == 0 and\n                    word[0] != '@' and word[0] != '_' and word[0] != '#',\n                    words))) + 1\n\nfraction = np.array([[count_len(x['selected_text']),\n                     count_len(x['text'])] for _, x in train.iterrows()])\n\ntrain['take_all'] = (fraction[:,0] >= fraction[:,1]).astype(int)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's look on head of train:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Using of a pretrained embedding","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's use pretrained embeddings to predict, which sentences have positive, negative or neutral sentiment. We will add this prediction to dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"bert_clean_embed = np.load('../input/embedded-twits/clean_embed.npy')\nbert_selected_embed = np.load('../input/embedded-twits/selected_embed.npy')\nbert_test_embed = np.load('../input/embedded-twits/test_embed.npy')\n\nembed_target =((train['sentiment'] == 'positive').astype(int) + 2 * (train['sentiment'] == 'negative').astype(int)).values\n\nlda = LinearDiscriminantAnalysis(solver='eigen')\nlda.fit(bert_selected_embed, embed_target)\n\ntrain = pd.concat([train, \n                   pd.DataFrame(lda.transform(bert_clean_embed), \n                                columns=['lda_bert_0', 'lda_bert_1', ])], \n                  axis=1, sort=False)\n\ntest = pd.concat([test, \n                   pd.DataFrame(lda.transform(bert_test_embed), \n                                columns=['lda_bert_0', 'lda_bert_1', ])], \n                  axis=1, sort=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"((lda.predict(bert_clean_embed) ==  embed_target) == train['take_all']).mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"col = ['cleaned_text', \n       'sentiment',\n       'exclamation', \n       'CapsLock',\n       'missed',\n       'lenght',\n       'good',\n       'bad',\n       'lda_bert_0', 'lda_bert_1',\n       'url',\n      ]\n\ncv_dataset = Pool(data=train[col],\n                  label=train['take_all'],\n                  cat_features=['sentiment'],\n                  text_features=['cleaned_text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rkf = KFold(n_splits=5, random_state=42)\nfor cv_train, cv_test in rkf.split(np.arange(train.shape[0])):\n    cat = CatBoostClassifier(n_estimators=500,\n                         max_depth = 8,\n                         task_type = 'GPU',\n                         verbose = 0\n                        )\n    cat.fit(train.loc[cv_train, col], train.loc[cv_train, 'take_all'],\n        cat_features=['sentiment'],\n        text_features=['cleaned_text'])\n    print('accuracy: ' + str((cat.predict(train.loc[cv_test, col]) == train.loc[cv_test, 'take_all']).mean()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In next part we'll choose parts of the texts with corresponded sentiment.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## What words we will take to our prediction","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's predict other target, do we need to take this word to prediction?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def rr(df, target):\n    for _, x in df.iterrows():\n        for word in re.split('(\\W+)', x['text'].strip()):\n            y = x.copy()\n            y['place'] = x['text'].find(word)\n            y['word'] = WordTransform(word).lower()\n            if len(y['word'].translate(table_empty)) < 3:\n                continue\n            y['missed'] = len(word) - len(y['word'])\n            y['CapsLock'] = sum(list(map(str.isupper, y['word'])))\n            if target:\n                y['target'] = int(x['selected_text'].find(word) != -1)\n                del y['selected_text']\n            del y['text']\n            yield y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"selected_train = pd.DataFrame(rr(train, True))\nselected_test = pd.DataFrame(rr(test, False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from collections import Counter\n#words_train = Counter(selected_train['word'])\n#words_test = Counter(selected_test['word'])\n#embed_train = dict(zip(list(words_train), lda.transform(model.encode(list(words_train), batch_size=128))))\n#embed_test = dict(zip(list(words_test), lda.transform(model.encode(list(words_test), batch_size=128))))\n#selected_train['bert_0'] = [embed_train[word][0] for word in selected_train['word']]\n#selected_train['bert_1'] = [embed_train[word][1] for word in selected_train['word']]\n#selected_test['bert_0'] = [embed_test[word][0] for word in selected_test['word']]\n#selected_test['bert_1'] = [embed_test[word][1] for word in selected_test['word']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat = CatBoostClassifier(n_estimators=2000,\n                         max_depth = 8,\n                         thread_count=6,\n                         task_type = 'GPU',\n                         verbose = 0)\n\ncol = ['cleaned_text', 'sentiment',\n                            'CapsLock',\n                            'missed', \n                            'lenght',\n      #                     'bert_0', 'bert_1',\n      ]\n\ncat.fit(selected_train[col], \n        selected_train['target'],\n        cat_features=['sentiment',],\n        text_features=['cleaned_text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sentiment_ext(i, df):\n    #emb = lda.transform(np.array(model.encode(df.loc[i,'cleaned_text'].split(), batch_size=16)))\n    tt = pd.concat([df.iloc[[i]][col]*emb.shape[0]])\n    #tt['bert_0'] = emb[:,0]\n    #tt['bert_1'] = emb[:,1]\n    res = cat.predict_proba(tt)[:,1]\n    words = df.loc[i, 'text'].split()\n    answer = ''\n    if (res < 0.5).mean() == 1:\n        answer = words[np.argmax(res)]\n    else:\n        good = np.where(res >= 0.5)[0]\n        answer = ' '.join(words[i] for i in range(good[0], good[-1] + 1))\n    return answer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['selected_text'] = [sentiment_ext(i, test) for i in range(test.shape[0])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test[['textID','selected_text']].to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}