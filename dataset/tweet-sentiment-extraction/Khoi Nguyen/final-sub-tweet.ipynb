{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import os\nimport warnings\nimport sys\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")\nimport pandas as pd\nfrom tqdm import tqdm\ntqdm.pandas()\nfrom tqdm import tqdm_notebook as tqdm\nfrom torch import nn\nimport copy\nimport transformers\nprint(transformers.__version__)\nfrom transformers.modeling_t5 import *\nimport json\nimport numpy as np\nimport pickle\nimport os\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\nfrom transformers import *\nimport torch\n# import matplotlib.pyplot as plt\nimport torch.utils.data\nimport torch.nn.functional as F\nfrom torch.optim import Adagrad, Adamax\nfrom transformers.modeling_utils import *\nfrom scipy.special import softmax\nimport argparse\nimport itertools\nfrom collections import OrderedDict\nfrom fuzzywuzzy import fuzz\nimport operator\nfrom transformers.tokenization_bert import BasicTokenizer\nfrom tqdm import tqdm_notebook as tqdm\nimport regex as re","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls -halt ../input/bart-drop-head","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import *\nfrom transformers.modeling_bart import PretrainedBartModel\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\n\nimport logging\nimport math\nimport os\n\nimport torch\nfrom torch import nn\nfrom torch.nn import CrossEntropyLoss, MSELoss\n\nclass PoolerStartLogits(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.hidden_size, 1)\n\n    def forward(self, hidden_states, p_mask=None):\n        x = self.dense(hidden_states).squeeze(-1)\n\n        if p_mask is not None:\n            if next(self.parameters()).dtype == torch.float16:\n                x = x * (1 - p_mask) - 65500 * p_mask\n            else:\n                x = x * (1 - p_mask) - 1e30 * p_mask\n\n        return x\n\n\nclass PoolerEndLogits(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense_0 = nn.Linear(config.hidden_size*2, config.hidden_size)\n        self.activation = nn.Tanh()\n        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps if hasattr(config,\"layer_norm_eps\") else 1e-5)\n        self.dense_1 = nn.Linear(config.hidden_size, 1)\n\n    def forward(self, hidden_states, start_states=None, start_positions=None, p_mask=None):\n        assert (\n            start_states is not None or start_positions is not None\n        ), \"One of start_states, start_positions should be not None\"\n        if start_positions is not None:\n            slen, hsz = hidden_states.shape[-2:]\n            start_positions = start_positions[:, None, None].expand(-1, -1, hsz)  # shape (bsz, 1, hsz)\n            start_states = hidden_states.gather(-2, start_positions)  # shape (bsz, 1, hsz)\n            start_states = start_states.expand(-1, slen, -1)  # shape (bsz, slen, hsz)\n\n        x = self.dense_0(torch.cat([hidden_states, start_states], dim=-1))\n        x = self.activation(x)\n        x = self.LayerNorm(x)\n        x = self.dense_1(x).squeeze(-1)\n\n        if p_mask is not None:\n            if next(self.parameters()).dtype == torch.float16:\n                x = x * (1 - p_mask) - 65500 * p_mask\n            else:\n                x = x * (1 - p_mask) - 1e30 * p_mask\n\n        return x\n\nclass RobertaForSentimentExtraction(BertPreTrainedModel):\n    config_class = RobertaConfig\n    base_model_prefix = \"roberta\"\n\n    def __init__(self, config):\n        super().__init__(config)\n\n        self.roberta = RobertaModel(config)\n#         self.transformer = self.roberta\n        self.start_logits = PoolerStartLogits(config)\n        self.end_logits = PoolerEndLogits(config)\n        self.init_weights()\n\n    def forward(\n        self, beam_size=1, drop_head=True,\n        input_ids=None,attention_mask=None,token_type_ids=None,input_mask=None,position_ids=None,\n        head_mask=None,inputs_embeds=None,start_positions=None,end_positions=None,p_mask=None\n    ):\n        outputs = self.roberta(\n            input_ids,\n            attention_mask=attention_mask,\n            # token_type_ids=None,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n        )\n        if drop_head:\n            hidden_states = outputs[2][-3]\n        else:\n            hidden_states = outputs[0]\n#        hidden_states = torch.cat((outputs[2][-1],outputs[2][-2], outputs[2][-3], outputs[2][-4]),-1)\n        start_logits = self.start_logits(hidden_states, p_mask=p_mask)\n\n        outputs = outputs[1:]  # Keep mems, hidden states, attentions if there are in it\n\n        if start_positions is not None and end_positions is not None:\n            # If we are on multi-GPU, let's remove the dimension added by batch splitting\n            for x in (start_positions, end_positions):\n                if x is not None and x.dim() > 1:\n                    x.squeeze_(-1)\n\n            # during training, compute the end logits based on the ground truth of the start position\n            end_logits = self.end_logits(hidden_states, start_positions=start_positions, p_mask=p_mask)\n\n            loss_fct = CrossEntropyLoss()\n\n            start_loss = loss_fct(start_logits, start_positions)\n            end_loss = loss_fct(end_logits, end_positions)\n            total_loss = (start_loss + end_loss) / 2\n            outputs = total_loss\n\n        else:\n            # during inference, compute the end logits based on beam search\n            bsz, slen, hsz = hidden_states.size()\n            start_log_probs = F.softmax(start_logits, dim=-1)  # shape (bsz, slen)\n#            start_log_probs = F.sigmoid(start_logits)\n\n            start_top_log_probs, start_top_index = torch.topk(\n                start_log_probs, beam_size, dim=-1\n            )  # shape (bsz, start_n_top)\n            start_top_index_exp = start_top_index.unsqueeze(-1).expand(-1, -1, hsz)  # shape (bsz, start_n_top, hsz)\n            start_states = torch.gather(hidden_states, -2, start_top_index_exp)  # shape (bsz, start_n_top, hsz)\n            start_states = start_states.unsqueeze(1).expand(-1, slen, -1, -1)  # shape (bsz, slen, start_n_top, hsz)\n\n            hidden_states_expanded = hidden_states.unsqueeze(2).expand_as(\n                start_states\n            )  # shape (bsz, slen, start_n_top, hsz)\n            p_mask = p_mask.unsqueeze(-1) if p_mask is not None else None\n            end_logits = self.end_logits(hidden_states_expanded, start_states=start_states, p_mask=p_mask)\n            end_log_probs = F.softmax(end_logits, dim=1)  # shape (bsz, slen, start_n_top)\n#            end_log_probs = F.sigmoid(end_logits)\n\n            end_top_log_probs, end_top_index = torch.topk(\n                end_log_probs, beam_size, dim=1\n            )  # shape (bsz, end_n_top, start_n_top)\n            end_top_log_probs = end_top_log_probs.view(-1, beam_size * beam_size)\n            end_top_index = end_top_index.view(-1, beam_size * beam_size)\n\n            outputs = start_top_log_probs, start_top_index, end_top_log_probs, end_top_index\n\n        return outputs\n\nclass BartForSentimentExtraction(PretrainedBartModel):\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.start_logits = PoolerStartLogits(config)\n        self.end_logits = PoolerEndLogits(config)\n        self.model = BartModel(config)\n#         self.transformer = self.model\n        self.init_weights()\n\n    def forward(\n        self, beam_size=1, drop_head=True,\n        input_ids=None,attention_mask=None,token_type_ids=None,input_mask=None,position_ids=None,\n        head_mask=None,inputs_embeds=None,start_positions=None,end_positions=None,p_mask=None\n    ):\n        outputs = self.model(\n            input_ids,\n            attention_mask=attention_mask,\n        )\n        \n        if drop_head:\n            hidden_states = outputs[1][-3]\n        else:\n            hidden_states = outputs[0]\n#        hidden_states = torch.cat((outputs[2][-1],outputs[2][-2], outputs[2][-3], outputs[2][-4]),-1)\n        start_logits = self.start_logits(hidden_states, p_mask=p_mask)\n\n        outputs = outputs[1:]  # Keep mems, hidden states, attentions if there are in it\n\n        if start_positions is not None and end_positions is not None:\n            # If we are on multi-GPU, let's remove the dimension added by batch splitting\n            for x in (start_positions, end_positions):\n                if x is not None and x.dim() > 1:\n                    x.squeeze_(-1)\n\n            # during training, compute the end logits based on the ground truth of the start position\n            end_logits = self.end_logits(hidden_states, start_positions=start_positions, p_mask=p_mask)\n\n            loss_fct = CrossEntropyLoss()\n\n            start_loss = loss_fct(start_logits, start_positions)\n            end_loss = loss_fct(end_logits, end_positions)\n            total_loss = (start_loss + end_loss) / 2\n            outputs = total_loss\n\n        else:\n            # during inference, compute the end logits based on beam search\n            bsz, slen, hsz = hidden_states.size()\n            start_log_probs = F.softmax(start_logits, dim=-1)  # shape (bsz, slen)\n\n            start_top_log_probs, start_top_index = torch.topk(\n                start_log_probs, beam_size, dim=-1\n            )  # shape (bsz, start_n_top)\n            start_top_index_exp = start_top_index.unsqueeze(-1).expand(-1, -1, hsz)  # shape (bsz, start_n_top, hsz)\n            start_states = torch.gather(hidden_states, -2, start_top_index_exp)  # shape (bsz, start_n_top, hsz)\n            start_states = start_states.unsqueeze(1).expand(-1, slen, -1, -1)  # shape (bsz, slen, start_n_top, hsz)\n\n            hidden_states_expanded = hidden_states.unsqueeze(2).expand_as(\n                start_states\n            )  # shape (bsz, slen, start_n_top, hsz)\n            p_mask = p_mask.unsqueeze(-1) if p_mask is not None else None\n            end_logits = self.end_logits(hidden_states_expanded, start_states=start_states, p_mask=p_mask)\n            end_log_probs = F.softmax(end_logits, dim=1)  # shape (bsz, slen, start_n_top)\n\n            end_top_log_probs, end_top_index = torch.topk(\n                end_log_probs, beam_size, dim=1\n            )  # shape (bsz, end_n_top, start_n_top)\n            end_top_log_probs = end_top_log_probs.view(-1, beam_size * beam_size)\n            end_top_index = end_top_index.view(-1, beam_size * beam_size)\n\n            outputs = start_top_log_probs, start_top_index, end_top_log_probs, end_top_index\n\n        return outputs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 64\nbeam_size = 3\nmax_sequence_length = 128","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"def find_best_combinations(start_top_log_probs, start_top_index, end_top_log_probs, end_top_index, valid_start= 0, valid_end=512):\n    best = (valid_start, valid_end - 1)\n    best_score = -9999\n    for i in range(len(start_top_log_probs)):\n        for j in range(end_top_log_probs.shape[0]):\n            if valid_start <= start_top_index[i] < valid_end and valid_start <= end_top_index[j,i] < valid_end and start_top_index[i] < end_top_index[j,i]:\n                score = start_top_log_probs[i] * end_top_log_probs[j,i]\n                if score > best_score:\n                    best = (start_top_index[i],end_top_index[j,i])\n                    best_score = score\n    return best\n\ndef jaccard(str1, str2):\n    a = set(str1.lower().split())\n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    try:\n        return float(len(c)) / (len(a) + len(b) - len(c))\n    except:\n        return 0\n\ndef fuzzy_match(x,y,weights=None):\n    l1 = len(x.split())\n    matches = dict()\n    x_ = x.split()\n    if type(y) is str:\n        y = [y]\n    for curr_length in range(l1 + 1):\n        for i in range(l1 + 1 - curr_length):\n            sub_x = ' '.join(x_[i:i+curr_length])\n            if sub_x not in matches:\n                matches[sub_x] = np.average([fuzz.ratio(sub_x,y_) for y_ in y],weights=weights)\n    if len(matches) == 0:\n        return None, x\n    return matches, sorted(matches.items(), key=operator.itemgetter(1))[-1][0]\n    \ndef ensemble_v0(context, predictions,weights=None):\n    context = context.split()\n    scores = dict()\n    for i,j in itertools.combinations(range(len(context) + 1),r=2):\n        curr_context = ' '.join(context[i:j])\n        scores[curr_context] = np.average([jaccard(curr_context, p) for p in predictions],weights=weghts)\n    best_score = np.max([val for val in scores.values()])\n    has_tie = np.sum([val == best_score for val in scores.values()]) > 1\n    if not has_tie:\n        for key, val in scores.items():\n            if val == best_score:\n                return key\n    else:\n        keys = [key for key, val in scores.items() if val == best_score]\n#         return keys[np.argmax([jaccard(key,predictions[0]) for key in keys])]\n        return keys[np.argmax([len(key.split()) for key in keys])]\n\ndef ensemble(context, predictions,slow_mode=False,weights=None):\n    starts = []\n    ends = []\n    for p in predictions:\n        if p in context:\n            start = context.index(p)\n            starts.append(start)\n            ends.append(start+len(p))\n    if len(starts) == 0:\n        print(context)\n        return ensemble_v0(context, predictions)\n    scores = dict()\n    context = context[np.min(starts):np.max(ends)]\n    for i,j in itertools.combinations(range(len(context) + 1),r=2):\n        if slow_mode or len(context.split()) == 1 or \\\n            (not ((i > 0 and context[i-1].isalnum() and context[i].isalnum()) or (j < len(context) and j > 1 and context[j-1].isalnum() and context[j].isalnum()))):\n            curr_context = context[i:j].strip()\n            scores[curr_context] = np.average([jaccard(curr_context, p) for p in predictions],weights=weights)\n    for pred in predictions:\n        if pred not in scores:\n            scores[pred] = np.average([jaccard(pred, p) for p in predictions],weights=weights)\n    best_score = np.max([val for val in scores.values()])\n    has_tie = np.sum([val == best_score for val in scores.values()]) > 1\n    if not has_tie:\n        for key, val in scores.items():\n            if val == best_score:\n                return key\n    else:\n        keys = [key for key, val in scores.items() if val == best_score]\n#         return keys[np.argmax([jaccard(key,predictions[0]) for key in keys])]\n        return keys[np.argmax([len(key.split()) for key in keys])]\n\nbasic_tokenizer = BasicTokenizer(do_lower_case=False)\ndef fix_spaces(t):\n    for i,item in enumerate(t):\n        re_res = re.search('\\s+$', item)\n        if bool(re_res) & (i < len(t)-1):\n            sp = re_res.span()\n            t[i+1] = t[i][sp[0]:] + t[i+1]\n            t[i] = t[i][:sp[0]]\n    return t\ndef roberta_tokenize_v2(tokenizer, line):\n    tokenized_line = []\n    line2 = basic_tokenizer._run_split_on_punc(line)\n    line2 = fix_spaces(line2)\n    for item in line2:\n        sub_word_tokens = tokenizer.tokenize(item)\n        tokenized_line += sub_word_tokens\n    return tokenized_line\n\n\ndef convert_lines(tokenizer, df, max_sequence_length = 512):\n    pad_token_idx = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n    outputs = np.zeros((len(df), max_sequence_length))\n    type_outputs = np.zeros((len(df), max_sequence_length))\n    position_outputs = np.zeros((len(df), 2))\n    extracted = []\n    for idx, row in tqdm(df.iterrows(), total=len(df)): \n        input_ids_0 = tokenizer.convert_tokens_to_ids(roberta_tokenize_v2(tokenizer, row.sentiment)) \n        input_ids_1 = tokenizer.convert_tokens_to_ids(roberta_tokenize_v2(tokenizer, row.text)) \n        input_ids = [tokenizer.cls_token_id, ]+ input_ids_0 +  [tokenizer.sep_token_id,] +input_ids_1 + [tokenizer.sep_token_id, ]\n        token_type_ids = [0,]*(len(input_ids_0) + 1) + [1,]*(len(input_ids_1) + 2)\n        if len(input_ids) > max_sequence_length: \n#            input_ids = input_ids[:max_sequence_length//2] + input_ids[-max_sequence_length//2:] \n            input_ids = input_ids[:max_sequence_length]\n            input_ids[-1] = tokenizer.sep_token_id\n            token_type_ids = token_type_ids[:max_sequence_length]\n        else:\n            input_ids = input_ids + [pad_token_idx, ]*(max_sequence_length - len(input_ids))\n            token_type_ids = token_type_ids + [pad_token_idx, ]*(max_sequence_length - len(token_type_ids))\n        assert len(input_ids) == len(token_type_ids)\n        outputs[idx,:max_sequence_length] = np.array(input_ids)\n        type_outputs[idx,:] = token_type_ids\n    return outputs, type_outputs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test_df = pd.read_csv(\"./data/test_holdout.csv\")\ntest_df = pd.read_csv(\"../input/tweet-sentiment-extraction/test.csv\")\ntest_df[\"sep_text\"] = test_df.text.apply(lambda x: \" \".join(x.split()).lower())\ndef get_predictions(x_test, x_type_test, model, is_xlnet=False,drop_head=True):\n    all_start_top_log_probs = None\n    all_start_top_index = None\n    all_end_top_log_probs = None\n    all_end_top_index = None\n    test_dataset = torch.utils.data.TensorDataset(torch.tensor(x_test,dtype=torch.long), torch.tensor(x_type_test,dtype=torch.long))\n    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n    with torch.no_grad():\n        pbar = tqdm(enumerate(test_loader),total=len(test_loader),leave=False)\n        for i, items in pbar:\n            x_batch, x_type_batch = items\n            attention_mask = x_batch != tokenizer.pad_token_id\n            p_mask = torch.zeros(x_batch.shape,dtype=torch.float32)\n            p_mask[x_batch == tokenizer.pad_token_id] = 1.0\n            p_mask[x_batch == tokenizer.cls_token_id] = 1.0\n            if is_xlnet:\n                attention_mask = attention_mask.float()\n                p_mask[:,:2] = 1.0\n            else:\n                p_mask[:,:3] = 1.0\n            start_top_log_probs, start_top_index, end_top_log_probs, end_top_index = model(input_ids=x_batch.cuda(), attention_mask=attention_mask.cuda(), \\\n                                                token_type_ids=x_type_batch.cuda(), beam_size=beam_size, p_mask=p_mask.cuda(), drop_head=drop_head)\n            start_top_log_probs = start_top_log_probs.detach().cpu().numpy()\n            start_top_index = start_top_index.detach().cpu().numpy()\n            end_top_log_probs = end_top_log_probs.detach().cpu().numpy()\n            end_top_index = end_top_index.detach().cpu().numpy()\n\n            all_start_top_log_probs = start_top_log_probs if all_start_top_log_probs is None else np.concatenate([all_start_top_log_probs, start_top_log_probs])\n            all_start_top_index = start_top_index if all_start_top_index is None else np.concatenate([all_start_top_index, start_top_index])\n            all_end_top_log_probs = end_top_log_probs if all_end_top_log_probs is None else np.concatenate([all_end_top_log_probs, end_top_log_probs])\n            all_end_top_index = end_top_index if all_end_top_index is None else np.concatenate([all_end_top_index, end_top_index])\n\n    return all_start_top_log_probs,all_start_top_index,all_end_top_log_probs,all_end_top_index\n\ndef load_and_fix_state(model_path):\n    state_dict = torch.load(model_path)\n    new_state_dict = OrderedDict()\n    for k, v in state_dict.items():\n        name = k[7:] # remove `module.`\n        new_state_dict[name] = v\n    return new_state_dict\n\ndef get_selected_texts(x_test, tokenizer, offset=3, is_xlnet=False):\n    selected_texts = []\n    for i_, x in tqdm(enumerate(x_test),total=len(x_test)):\n        real_length = np.sum(x != tokenizer.pad_token_id)\n        if is_xlnet:\n            real_length -= 1\n        best_start, best_end = find_best_combinations(all_start_top_log_probs[i_], all_start_top_index[i_], \\\n                                                        all_end_top_log_probs[i_].reshape(beam_size,beam_size), all_end_top_index[i_].reshape(beam_size,beam_size), \\\n                                                        valid_start = offset, valid_end = real_length)\n        selected_text = tokenizer.decode([w for w in x[best_start:best_end] if w != tokenizer.pad_token_id],clean_up_tokenization_spaces=False)\n        selected_texts.append(selected_text if selected_text in test_df.loc[i_].text \n                              else fuzzy_match(test_df.loc[i_].text, selected_text)[-1])\n    return selected_texts\n\ndef check_corrs(model_name):\n    corrs = np.zeros((5,5))\n    for i in range(5):\n        for j in range(5):\n            if corrs[j,i] == 0:\n                corrs[i,j] = np.mean([jaccard(x,y) for x,y in zip(all_preds[f\"{model_name}_{i}\"], all_preds[f\"{model_name}_{j}\"])])\n                corrs[j,i] = corrs[i,j]\n    return corrs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_preds = dict()\n\n!mkdir configs\n!cp ../input/roberta-large-quest/roberta-large-vocab.json ./configs/vocab.json\n!cp ../input/bart-large/config.json ./configs/config.json\n!cp ../input/roberta-large-quest/roberta-large-merges.txt ./configs/merges.txt\n\ntokenizer = BartTokenizer.from_pretrained('./configs', do_lower_case=False)\nconfig = BartConfig.from_pretrained('./configs',do_lower_case=False, output_hidden_states=True)\nmodel = BartForSentimentExtraction(config)\nmodel.cuda()\nX_test, X_type_test = convert_lines(tokenizer, test_df, max_sequence_length= max_sequence_length)\ndirs = [\"bart-drop-head\",\"bart-drop-head-2\",\"tweet-all\"]\n\nfor data_dir in dirs:\n    for fn in os.listdir(f\"../input/{data_dir}\"):\n        if \"bart_\" in fn:\n            drop_head = data_dir != \"tweet-all\"\n            print(fn,drop_head)\n            model.load_state_dict(torch.load(f\"../input/{data_dir}/{fn}\"), strict=True)\n            model.eval()\n            all_start_top_log_probs,all_start_top_index,all_end_top_log_probs,all_end_top_index = get_predictions(X_test, X_type_test, model)\n            selected_texts = get_selected_texts(X_test, tokenizer)\n            print(selected_texts[:10])\n            all_preds[f\"{fn}\"] = [x if type(x) is str else \"\" for x in selected_texts]\n!rm -rf configs\n\n!mkdir configs\n!cp ../input/roberta-base-quest/roberta-base-vocab.json ./configs/vocab.json\n!cp ../input/roberta-base-quest/roberta-base-config.json ./configs/config.json\n!cp ../input/roberta-base-quest/roberta-base-merges.txt ./configs/merges.txt\ntokenizer = RobertaTokenizer.from_pretrained(f'./configs/', do_lower_case=False)\nconfig = RobertaConfig.from_pretrained('./configs',do_lower_case=False, output_hidden_states=True)\nmodel = RobertaForSentimentExtraction(config)\nmodel.cuda()\nX_test, X_type_test = convert_lines(tokenizer, test_df, max_sequence_length= max_sequence_length)\ndirs = [\"roberta-base-drop-head\",\"tweet-all\"]\nfor data_dir in dirs:\n    for fn in os.listdir(f\"../input/{data_dir}\"):\n        if \"roberta_\" in fn:\n            drop_head = data_dir != \"tweet-all\"\n            print(fn,drop_head)\n            model.load_state_dict(torch.load(f\"../input/{data_dir}/{fn}\"), strict=True)\n            model.eval()\n            all_start_top_log_probs,all_start_top_index,all_end_top_log_probs,all_end_top_index = get_predictions(X_test, X_type_test, model)\n            selected_texts = get_selected_texts(X_test, tokenizer)\n            print(selected_texts[:10])\n            all_preds[f\"{fn}\"] = [x if type(x) is str else \"\" for x in selected_texts]\n!rm -rf configs\n\n!mkdir configs\n!cp ../input/roberta-large-quest/roberta-large-vocab.json ./configs/vocab.json\n!cp ../input/roberta-large-quest/roberta-large-config.json ./configs/config.json\n!cp ../input/roberta-large-quest/roberta-large-merges.txt ./configs/merges.txt\ntokenizer = RobertaTokenizer.from_pretrained(f'./configs/', do_lower_case=False)\nconfig = RobertaConfig.from_pretrained('./configs',do_lower_case=False, output_hidden_states=True)\nmodel = RobertaForSentimentExtraction(config)\nmodel.cuda()\nX_test, X_type_test = convert_lines(tokenizer, test_df, max_sequence_length= max_sequence_length)\ndirs = [\"roberta-large-drop-head\",\"roberta-large-drop-head-2\",\"tweet-all\"]\nfor data_dir in dirs:\n    for fn in os.listdir(f\"../input/{data_dir}\"):\n        if \"roberta-large_\" in fn:\n            drop_head = data_dir != \"tweet-all\"\n            print(fn,drop_head)\n            model.load_state_dict(torch.load(f\"../input/{data_dir}/{fn}\"), strict=True)\n            model.eval()\n            all_start_top_log_probs,all_start_top_index,all_end_top_log_probs,all_end_top_index = get_predictions(X_test, X_type_test, model)\n            selected_texts = get_selected_texts(X_test, tokenizer)\n            print(selected_texts[:10])\n            all_preds[f\"{fn}\"] = [x if type(x) is str else \"\" for x in selected_texts]\n!rm -rf configs\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model_list = [key for key in all_preds.keys()]\n# print(model_list)\n# all_vals = [all_preds[model_name] for model_name in model_list]  #+ dieter_preds\n# print(len(all_vals))\n# ensembled = []\n# sep_texts = test_df.text.values\n# # sep_texts = test_df.text.apply(lambda x: \" \".join(x.split()).lower())\n# for i in tqdm(range(len(test_df))):\n#     predictions = [val[i] for val in all_vals]\n#     slow_mode = False\n#     ensembled.append(ensemble(sep_texts[i], predictions, slow_mode=slow_mode))\n    \n# sub_df = pd.read_csv(\"../input/valid-sub/submission.csv\")\n# print(np.mean([jaccard(x,y) for x,y in zip(sub_df.selected_text, ensembled)]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport torch\nfrom torch import nn\nfrom tqdm import tqdm\nimport os\nimport gc\n\ndef get_features(line, tokenizer, sentiment,  span=None):\n    \n    MAX_LEN = 114\n    MAX_CHAR = 146\n    pad_token_id = 1\n    sep_token_id = 2\n    cls_token_id = 0\n    \n    encoding = tokenizer.encode(line)\n    offsets = np.array(encoding.offsets)\n    token_lenghts = list(np.diff(offsets, axis =1)[:,0])\n    \n    #handle unnatural long chars\n    while sum(token_lenghts)>(MAX_CHAR-4):\n        token_lenghts = token_lenghts[:-1]\n    \n    rep_vec = [1,1,1] + token_lenghts + [1]\n    sentiment_id = tokenizer.encode(sentiment).ids\n    \n\n    input_ids = [cls_token_id] + sentiment_id + [sep_token_id] + encoding.ids[:MAX_LEN-4] + [sep_token_id]\n    attention_mask = [1] * len(input_ids)\n    \n    #perform padding\n    attention_padding = [0] * (MAX_LEN - len(input_ids))\n    padding = [pad_token_id] * (MAX_LEN - len(input_ids))\n    rep_vec_padding = [0] * (MAX_LEN - len(rep_vec) - 1)\n    rep_vec_padding += [MAX_CHAR - sum(rep_vec_padding) - sum(rep_vec)]\n\n    \n    input_ids.extend(padding)\n    attention_mask.extend(attention_padding)\n    rep_vec.extend(rep_vec_padding)\n    \n    assert len(rep_vec) == MAX_LEN\n    assert sum(rep_vec) == MAX_CHAR\n    assert len(input_ids) == MAX_LEN\n    assert len(attention_mask) == MAX_LEN\n\n    \n    \n    features = {'input_ids': np.array(input_ids,dtype=int),\n                      'attention_mask': np.array(attention_mask,dtype=int),\n                'rep_vec':np.array(rep_vec,dtype=int),\n                      #'token_type_ids': np.array(token_type_ids,dtype=int),\n                      #'token_idx2word_idx':np.array(token_idx2word_idx,dtype=int)\n                      }\n    \n    if span is not None:\n        features['target'] = np.array(span,dtype=int) + (3,3)\n\n    else:\n        features['target'] = np.array((-1,-1),dtype=int)\n        \n    return features\n\nfrom torch.utils.data import Dataset, DataLoader, SequentialSampler, RandomSampler\n\nclass TEDataset(Dataset):\n    \n    def __init__(self,text,tokenizer,sentiments, labels=None, verbose=False):\n        \n        if labels is None:\n            self.features = [get_features(text[i], tokenizer,sentiments[i], span=None) for i in tqdm(range(len(text)),disable= 1-verbose)]\n        \n        else:\n            self.features = [get_features(text[i], tokenizer,sentiments[i], span=labels[i]) for i in tqdm(range(len(text)),disable= 1-verbose)]\n        \n    def __len__(self):\n        return len(self.features)\n    \n    def __getitem__(self, index):\n\n        return self.features[index]\n    \ndef test_collate(batch):\n\n    input_dict = {}\n    target_dict = {}\n    \n    for key in ['input_ids','attention_mask','rep_vec']:#,'token_idx2word_idx','token_type_ids']:\n        input_dict[key] = torch.from_numpy(np.stack([b[key] for b in batch])).long()\n    for key in ['target']:#,'token_idx2word_idx','token_type_ids']:\n        input_dict[key] = [None,None]\n    \n    for key in ['target']:\n        target_dict[key] = torch.from_numpy(np.stack([b[key] for b in batch])).long()\n    \n    return input_dict, target_dict\nimport tokenizers\n\ntokenizer = tokenizers.ByteLevelBPETokenizer(\n            vocab_file='/kaggle/input/roberta-base-quest/roberta-base-vocab.json', \n            merges_file='/kaggle/input/roberta-base-quest/roberta-base-merges.txt', \n            lowercase=True,\n            add_prefix_space=False)\ntest = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv')\ntest['text'] = test['text'].astype(str)#.apply(split_join)\ntest['text'] = test['text'].apply(lambda x: x.replace('ï¿½','<i>'))\ntest_text = test['text'].values\ntest_sentiments = test['sentiment'].values\nte_ds = TEDataset(test_text,tokenizer,test_sentiments)\nte_dl = DataLoader(te_ds,sampler=SequentialSampler(te_ds), collate_fn=test_collate, batch_size=256)\nfrom transformers.configuration_roberta import RobertaConfig\nfrom transformers.modeling_roberta import RobertaModel\nfrom transformers.configuration_bart import BartConfig\nfrom transformers.modeling_bart import BartModel\n\n\n\nclass TERobertaBase(nn.Module):\n\n    def __init__(self):\n        super(TERobertaBase, self).__init__()\n\n        config = RobertaConfig.from_json_file('/kaggle/input/roberta-base-quest/roberta-base-config.json')\n        self.transformer = RobertaModel(config)\n        self.dropout = torch.nn.Dropout(0)\n        hdz = self.transformer.config.hidden_size\n        \n        self.rnn_start0 = nn.GRU(hdz, hdz, num_layers=4, batch_first=True, bidirectional=True)\n        self.act_start0 = nn.ReLU()\n        self.rnn_end0 = nn.GRU(hdz, hdz, num_layers=4, batch_first=True, bidirectional=True)\n        self.act_end0 = nn.ReLU()\n        \n        self.head_start = nn.Sequential(nn.Linear(self.transformer.config.hidden_size,1))\n        self.head_end = nn.Sequential(nn.Linear(self.transformer.config.hidden_size,1))\n        self.beam_size = 1\n\n    def forward(self, input_dict):\n        beam_size = self.beam_size\n        input_ids= input_dict['input_ids']\n        #token_type_ids = input_dict['token_type_ids']\n\n        attention_mask = input_dict['attention_mask']\n        # targets = input_dict['target']\n        rep_vec = input_dict['rep_vec']\n        outputs = self.transformer(input_ids,\n                            attention_mask=attention_mask,\n                            #token_type_ids=token_type_ids\n                                   )\n\n        hidden_states = outputs[0]\n        \n        bs = hidden_states.shape[0]\n        hidden_states = torch.stack([torch.repeat_interleave(hidden_states[k],rep_vec[k], axis = 0) for k in range(bs)])\n        hs_start, _ = self.rnn_start0(hidden_states)\n        hs_start = hs_start.reshape(hs_start.shape[0],hs_start.shape[1],hs_start.shape[2]//2,2).sum(-1)\n        hs_start = self.act_start0(hs_start)\n        \n        hs_end, _ = self.rnn_end0(hidden_states)\n        hs_end = hs_end.reshape(hs_end.shape[0],hs_end.shape[1],hs_end.shape[2]//2,2).sum(-1)\n        hs_end = self.act_end0(hs_end)\n        \n\n        start_logits = self.head_start(hs_start).squeeze(-1)\n        end_logits = self.head_start(hs_end).squeeze(-1)\n\n        output_dict = {'start_logits':start_logits,\n                   'end_logits':end_logits\n                   }\n     \n        return output_dict\n\n    \nclass TERobertaLarge(nn.Module):\n\n    def __init__(self):\n        super(TERobertaLarge, self).__init__()\n\n        config = RobertaConfig.from_json_file('/kaggle/input/roberta-large-quest/roberta-large-config.json')\n        self.transformer = RobertaModel(config)\n        self.dropout = torch.nn.Dropout(0)\n        hdz = self.transformer.config.hidden_size\n        \n        self.rnn_start0 = nn.GRU(hdz, hdz, num_layers=2, batch_first=True, bidirectional=True)\n        self.act_start0 = nn.ReLU()\n        self.rnn_end0 = nn.GRU(hdz, hdz, num_layers=2, batch_first=True, bidirectional=True)\n        self.act_end0 = nn.ReLU()\n        \n        self.head_start = nn.Sequential(nn.Linear(self.transformer.config.hidden_size,1))\n        self.head_end = nn.Sequential(nn.Linear(self.transformer.config.hidden_size,1))\n        self.beam_size = 1\n\n    def forward(self, input_dict):\n        beam_size = self.beam_size\n        input_ids= input_dict['input_ids']\n        #token_type_ids = input_dict['token_type_ids']\n\n        attention_mask = input_dict['attention_mask']\n        #targets = input_dict['target']\n        rep_vec = input_dict['rep_vec']\n        outputs = self.transformer(input_ids,\n                            attention_mask=attention_mask,\n                            #token_type_ids=token_type_ids\n                                   )\n\n        hidden_states = outputs[0]\n        \n        bs = hidden_states.shape[0]\n        hidden_states = torch.stack([torch.repeat_interleave(hidden_states[k],rep_vec[k], axis = 0) for k in range(bs)])\n        hs_start, _ = self.rnn_start0(hidden_states)\n        hs_start = hs_start.reshape(hs_start.shape[0],hs_start.shape[1],hs_start.shape[2]//2,2).mean(-1)\n        hs_start = self.act_start0(hs_start)\n        \n        hs_end, _ = self.rnn_end0(hidden_states)\n        hs_end = hs_end.reshape(hs_end.shape[0],hs_end.shape[1],hs_end.shape[2]//2,2).mean(-1)\n        hs_end = self.act_end0(hs_end)\n        \n        start_logits = self.head_start(hs_start).squeeze(-1)\n        end_logits = self.head_start(hs_end).squeeze(-1)\n\n        output_dict = {'start_logits':start_logits,\n                   'end_logits':end_logits\n                   }\n\n        return output_dict\n    \n    \nclass TEBartLarge(nn.Module):\n\n    def __init__(self):\n        super(TEBartLarge, self).__init__()\n\n        config = BartConfig.from_json_file('/kaggle/input/bart-large/config.json')\n        self.transformer = BartModel(config)\n        self.dropout = torch.nn.Dropout(0)\n        hdz = self.transformer.config.hidden_size\n        \n        self.rnn_start0 = nn.GRU(hdz, hdz, num_layers=3, batch_first=True, bidirectional=True)\n        self.act_start0 = nn.ReLU()\n        self.rnn_end0 = nn.GRU(hdz, hdz, num_layers=3, batch_first=True, bidirectional=True)\n        self.act_end0 = nn.ReLU()\n        \n        self.head_start = nn.Sequential(nn.Linear(self.transformer.config.hidden_size,1))\n        self.head_end = nn.Sequential(nn.Linear(self.transformer.config.hidden_size,1))\n\n        self.beam_size = 1\n\n    def forward(self, input_dict):\n        beam_size = self.beam_size\n        input_ids= input_dict['input_ids']\n        #token_type_ids = input_dict['token_type_ids']\n\n        attention_mask = input_dict['attention_mask']\n        #targets = input_dict['target']\n        rep_vec = input_dict['rep_vec']\n        outputs = self.transformer(input_ids,\n                            attention_mask=attention_mask,\n                            #token_type_ids=token_type_ids\n                                   )\n\n        hidden_states = outputs[0]\n        \n        bs = hidden_states.shape[0]\n        hidden_states = torch.stack([torch.repeat_interleave(hidden_states[k],rep_vec[k], axis = 0) for k in range(bs)])\n        hs_start, _ = self.rnn_start0(hidden_states)\n        hs_start = hs_start.reshape(hs_start.shape[0],hs_start.shape[1],hs_start.shape[2]//2,2).sum(-1)\n        hs_start = self.act_start0(hs_start)\n        \n        hs_end, _ = self.rnn_end0(hidden_states)\n        hs_end = hs_end.reshape(hs_end.shape[0],hs_end.shape[1],hs_end.shape[2]//2,2).sum(-1)\n        hs_end = self.act_end0(hs_end)\n        \n\n        start_logits = self.head_start(hs_start).squeeze(-1)\n\n        end_logits = self.head_start(hs_end).squeeze(-1)\n\n        output_dict = {'start_logits':start_logits,\n                   'end_logits':end_logits\n                   }\n     \n            \n\n        return output_dict\ndef dict_unravel(batch):\n        input_dict, label_dict = batch\n        input_dict2 = {k: input_dict[k].cuda() for k in input_dict if not k == 'target'}\n#         input_dict2['target'] = input_dict['target']\n        label_dict = {k: label_dict[k].cuda() for k in label_dict}\n\n        return input_dict2, label_dict\n\n\ndef get_logits(dl, model):\n    with torch.no_grad():\n\n        all_start_logits = []\n        all_end_logits = []\n        for batch in tqdm(dl):\n            input_dict, target_dict = dict_unravel(batch)\n            outs = model.forward(input_dict)\n            start_logits = outs['start_logits']\n            end_logits = outs['end_logits']\n            all_start_logits += [start_logits.detach().cpu().numpy()]\n            all_end_logits += [end_logits.detach().cpu().numpy()]\n           \n        all_start_logits = np.concatenate(all_start_logits)\n        all_end_logits = np.concatenate(all_end_logits)\n\n    return all_start_logits, all_end_logits\nweights_path = '/kaggle/input/roberta-base-38-full-fp16-weights/'\nmodel = TERobertaBase().cuda().eval()\n\ntest_start_logits = []\ntest_end_logits = []\nfor FOLD in [0,1,2,3,4]:\n    model.load_state_dict(torch.load(weights_path + f'fold{FOLD}.pt'))\n    test_start_logits_, test_end_logits_ = get_logits(te_dl, model)\n    test_start_logits += [test_start_logits_]\n    test_end_logits += [test_end_logits_]\n\nweights_path = '/kaggle/input/roberta-base-38-pseudo-full-fp16-weights/'\nfor FOLD in [0,1,2,3,4]:\n    model.load_state_dict(torch.load(weights_path + f'fold{FOLD}.pt'))\n    test_start_logits_, test_end_logits_ = get_logits(te_dl, model)\n    test_start_logits += [test_start_logits_]\n    test_end_logits += [test_end_logits_]\n    \ndel model\ngc.collect()\n\nweights_path = '/kaggle/input/roberta-large-3-full-fp16-weights/'\nmodel = TERobertaLarge().cuda().eval()\n\nfor FOLD in [0,1,2,3,4]:\n    model.load_state_dict(torch.load(weights_path + f'fold{FOLD}.pt'))\n    test_start_logits_, test_end_logits_ = get_logits(te_dl, model)\n    test_start_logits += [test_start_logits_]\n    test_end_logits += [test_end_logits_]\n    \nweights_path = '/kaggle/input/roberta-large-3-full-pseudo-weights/'\nfor FOLD in [0,1,2,3,4]:\n    model.load_state_dict(torch.load(weights_path + f'fold{FOLD}.pt'))\n    test_start_logits_, test_end_logits_ = get_logits(te_dl, model)\n    test_start_logits += [test_start_logits_]\n    test_end_logits += [test_end_logits_]\n\ndel model\ngc.collect()\n\nweights_path = '/kaggle/input/bart-large-2-full-fp16-weights/'\nmodel = TEBartLarge().cuda().eval()\n\nfor FOLD in [0,1,2,3,4]:\n    model.load_state_dict(torch.load(weights_path + f'fold{FOLD}.pt'))\n    test_start_logits_, test_end_logits_ = get_logits(te_dl, model)\n    test_start_logits += [test_start_logits_]\n    test_end_logits += [test_end_logits_]\n    \nweights_path = '/kaggle/input/bart-large-2-full-pseudo-weights/'\nfor FOLD in [0,1,2,3,4]:\n    model.load_state_dict(torch.load(weights_path + f'fold{FOLD}.pt'))\n    test_start_logits_, test_end_logits_ = get_logits(te_dl, model)\n    test_start_logits += [test_start_logits_]\n    test_end_logits += [test_end_logits_]\n    \ndel model\ngc.collect()\n\ndef find_best_combinations(start_logits,end_logits, valid_start= 3, valid_end=512):\n    \n    best = (valid_start, valid_end-1)\n    best_score = -9999\n    best_ = np.argmax(start_logits), np.argmax(end_logits)\n    if (valid_start <= best_[0] < valid_end) and (valid_start <= best_[1] < valid_end) and (best_[0] < best_[1]):\n        return best_\n    for i in range(valid_start, valid_end):\n        for j in range(i + 1, valid_end):\n            score = start_logits[i] + end_logits[j]\n            if score > best_score:\n                best = (i,j)\n                best_score = score\n    return best\n\ndef get_preds(all_start_logits, all_end_logits, valid_start= 3, valid_end=512 ):\n    \n    all_start_logits = all_start_logits\n    \n    start_preds = []\n    end_preds = []\n    for start_logits,end_logits in zip(all_start_logits, all_end_logits):\n        start, end = find_best_combinations(start_logits,end_logits, valid_start, valid_end)\n        start_preds += [start]\n        end_preds += [end]\n    \n    start_preds = np.array(start_preds)\n    end_preds = np.array(end_preds)\n    return start_preds, end_preds\n\nstart_logits = np.mean(test_start_logits, axis = 0)\nend_logits = np.mean(test_end_logits, axis = 0)\ntest_start_preds, test_end_preds = get_preds(start_logits, end_logits, valid_start= 3, valid_end=146 )\ntest_pred_text = []\nfor i in range(len(test_text)):\n    test_pred_text += [test_text[i][test_start_preds[i]-3:test_end_preds[i]-3]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_vals2 = []\nfor i in tqdm(range(len(test_start_logits))):\n    test_start_preds, test_end_preds = get_preds(test_start_logits[i], test_end_logits[i], valid_start= 3, valid_end=146 )\n    curr_val = []\n    for i in range(len(test_text)):\n        curr_val += [test_text[i][test_start_preds[i]-3:test_end_preds[i]-3]]\n    all_vals2.append(curr_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_vals = [val for val in all_preds.values()] + all_vals2\nprint(len(all_vals))\nensembled = []\nsep_texts = test_df.text.values\nfor i in tqdm(range(len(test_df))):\n    if \"  \" in sep_texts[i]:\n        ensembled.append(test_pred_text[i])\n    else:\n        predictions = [val[i] for val in all_vals]\n        ensembled.append(ensemble(sep_texts[i], predictions, slow_mode=True))\n#     predictions = [val[i] for val in all_vals]\n#     ensembled.append(ensemble(sep_texts[i], predictions, slow_mode=True))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if len(all_vals) == 60:\n    test_df[\"selected_text\"] = ensembled\n    test_df[[\"textID\",\"selected_text\"]].to_csv(\"submission.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!head -n10 submission.csv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}