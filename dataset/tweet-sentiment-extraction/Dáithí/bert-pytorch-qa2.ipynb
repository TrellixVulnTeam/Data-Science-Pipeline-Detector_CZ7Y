{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport random\nimport torch\nimport transformers\nfrom transformers import BertConfig, BertTokenizer, BertForQuestionAnswering,AdamW\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df=pd.read_csv(\"/kaggle/input/tweet-sentiment-extraction/test.csv\")\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"tokenizer = transformers.BertTokenizer.from_pretrained(\"/kaggle/input/bert-base-uncased/\",model_max_len=48,\n                                                       padding_side=\"right\",cls_token=\"[CLS]\",eos_token=\"[SEP]\",pad_token=\"[PAD]\")\n\nconfig = BertConfig.from_pretrained('/kaggle/input/bertlargewholewordmaskingfinetunedsquad/bert-large-uncased-whole-word-masking-finetuned-squad-config.json', \n                                    output_hidden_states=True)\n\nmodel = BertForQuestionAnswering.from_pretrained('/kaggle/input/bertlargewholewordmaskingfinetunedsquad/bert-large-uncased-whole-word-masking-finetuned-squad-pytorch_model.bin',\n                                                 config=config)\n\n\ntrain_df=pd.read_csv(\"/kaggle/input/tweet-sentiment-extraction/train.csv\")\ntrain_df=train_df.dropna()\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed_value):\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    os.environ['PYTHONHASHSEED'] = str(seed_value)\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = True\n\nseed = 42\nseed_everything(seed)\n\n\ndef create_input(q,c,a):\n    \n    def get_idxs(text_ids,selected_text_ids):\n        len_text_ids,len_selected_text_ids=len(text_ids),len(selected_text_ids)\n        for i in range( len_text_ids - len_selected_text_ids + 1):\n            start_idx,end_idx=i,i+len_selected_text_ids\n            sub_list=text_ids[start_idx:end_idx]\n            if sub_list==selected_text_ids:\n                return start_idx,end_idx\n    \n    \n    MAX_LEN=84\n    input_seqA,token_idsA,masksA,spA,epA=[],[],[],[],[]\n    count,fails=0,0\n    for i,_ in enumerate(q):\n        count+=1\n        cls,sep=[101],[102]\n        q_encoded=tokenizer.encode(q[i],add_special_tokens=False,pad_to_max_length=False)\n        c_encoded=tokenizer.encode(c[i],add_special_tokens=False,pad_to_max_length=False)\n        a_encoded=tokenizer.encode(a[i],add_special_tokens=False,pad_to_max_length=False)\n        \n        len_c=len(c_encoded)\n        try:\n            start_idx,end_idx=get_idxs(c_encoded,a_encoded)\n            \n            input_seq=cls+q_encoded+sep+ c_encoded+sep\n            token_ids=[0,0,0]+[1]*len_c+[1]\n\n            padd=[0]*( MAX_LEN - len(input_seq) )\n\n            mask_ids=[1]*len(input_seq) + padd\n            input_seq=input_seq + padd\n            token_ids=token_ids + padd\n\n            #print(\"{} {} {}\".format(len(mask_ids),len(input_seq),len(token_ids)))\n\n            if len(token_ids)+len(input_seq)+len(mask_ids)==(84*3):\n                input_seqA.append(input_seq)\n                token_idsA.append(token_ids)\n                masksA.append(mask_ids)\n                spA.append(start_idx+3)\n                epA.append(end_idx+3)\n\n            else:\n                fails+=1\n            \n        except:\n            fails+=1\n\n    print(\"fails: {}, % fails {}\".format( fails, round( fails/count,2) ) )\n    \n    return np.array(input_seqA),np.array(token_idsA),np.array(masksA),np.array(spA),np.array(epA)\n\n        \n    \n# iseq,ti,m=create_input(q,c,a)\n\n# print(\"{} :: {}\".format(len(iseq[0]),iseq[0]) )\n# print(\"{} :: {}\".format(len(ti[0]),ti[0]) )\n# print(\"{} :: {}\".format(len(m[0]),m[0]) )\n\nclass Bert_Classification_Data(torch.utils.data.Dataset):\n    \"\"\"\n    What should a dataset look like to pytorch?\n    \"\"\"\n    \n    def __init__(self,q,c,a,create_input=create_input):\n        \n        eseq,ti,m,sp,ep=create_input(q,c,a)\n        \n        self.input_seq=torch.tensor(eseq, device=\"cuda\",dtype=torch.long)\n        self.token_ids=torch.tensor(ti, device=\"cuda\",dtype=torch.long)\n        self.masks=torch.tensor(m,device=\"cuda\",dtype=torch.float32)\n        self.start_positions=torch.tensor( sp ,dtype=torch.long,device=\"cuda\"  )\n        self.end_positions=torch.tensor( ep,dtype=torch.long, device=\"cuda\"  )\n        \n    def __len__(self):\n        return len(self.input_seq)\n    \n    def __getitem__(self,index):\n        return self.input_seq[index],self.token_ids[index],self.masks[index],self.start_positions[index],self.end_positions[index]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"q,c,a=train_df[\"sentiment\"].values,train_df[\"text\"].values,train_df[\"selected_text\"].values\ndataset=Bert_Classification_Data(q,c,a)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Test","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndata_loader=train_loader=torch.utils.data.DataLoader(dataset=dataset,batch_size=128,shuffle=True)\n\nout=next(iter(data_loader))\n\n\ninput_ids=out[0][0]\nsi,ei=out[3][0].item(),out[4][0].item()\n\nprint(si,ei)\n\ns=tokenizer.decode( input_ids[si:ei] )\nprint(s)\nout=train_df.where(train_df[\"selected_text\"]==s).dropna(how=\"all\")\nout\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ntorch.set_grad_enabled(True)\n\n#push to gpu\nmodel.to(\"cuda\")\n\n#initialize model for training\nmodel.train()\n\ntrain_loader=torch.utils.data.DataLoader(dataset=dataset,batch_size=36,shuffle=True)\noptimizer=AdamW(model.parameters(),lr=2e-5,eps=1e-8)\n\nEPOCHS=10\ntotal_loss,score=[],[]\nfor epoch in range(EPOCHS):\n    for batch in train_loader:\n        input_seq,input_ids,masks,sp,ep=batch\n\n        loss = model(input_seq, token_type_ids=input_ids, start_positions=sp, end_positions=ep )[0]\n\n        optimizer.zero_grad() #zero out the previous gradiant\n        loss.backward() #calculates gradients (backpropagation)\n        optimizer.step() #updates the weights\n\n        print(loss.item())\n        total_loss.append( loss.item() )\n    \nplt.plot(total_loss)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Test Eval","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ndef jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    \n    #print(a,b,c)\n    \n    return float(len(c)) / (len(a) + len(b) - len(c))\n    \n    \n\na,q,c=train_df[\"selected_text\"].values,train_df[\"sentiment\"].values,train_df[\"text\"].values\n\ntorch.set_grad_enabled(False)\nmodel.eval()\n\naccuracy,sent=[],[]\nfor i,_ in enumerate(q):\n    \n    if q[i]!=\"neutral\":\n        encoding = tokenizer.encode_plus(q[i], c[i])\n\n        input_ids, token_type_ids = encoding[\"input_ids\"], encoding[\"token_type_ids\"]\n\n        #start_scores, end_scores = model(torch.tensor([input_ids],device=\"cuda\"), token_type_ids=torch.tensor([token_type_ids],device=\"cuda\"))\n        out = model( torch.tensor([input_ids],device=\"cuda\"), token_type_ids=torch.tensor([token_type_ids], device=\"cuda\") )\n\n        start_scores, end_scores= out[0],out[1]\n\n        all_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n\n        answer = ' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)])\n        answer=answer.replace(' ##', '')\n        answer=answer.replace(\" ` \",\"`\")\n        answer=answer.replace(\" , \",\", \")\n        answer=answer.replace(\" ,\",\", \")\n        answer=answer.replace(\" .\",\".\")\n        answer=answer.replace(\" !\",\"!\")\n        answer=answer.replace(\" ?\",\"?\")\n        answer=answer.replace(\"* * * *\",\"****\")\n        answer=answer.replace(\"... \",\"...\")\n        answer=answer.replace(\"< 3\",\"<3\")\n        answer=answer.replace(\" )\",\")\")\n        answer=answer.replace(\"( \",\"(\")\n        \n        \n        accu_=jaccard(answer,a[i]) \n                \n        accuracy.append( accu_ )\n        \n        if accu_ <0.6:\n            sent.append( [answer,a[i],c[i]] )\n    \n    elif q[i]==\"neutral\":\n        accu_=jaccard(a[i],c[i]) \n        accuracy.append( accu_ )\n        \nacc=np.sum(np.array(accuracy))/len(accuracy)\n\nprint(acc)\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    \n    print(a,b,c)\n    \n    return float(len(c)) / (len(a) + len(b) - len(c))\n\n\nprint(len(sent))\nfor j,i in enumerate(sent):\n    print(i)\n    print(a[j])\n        \n        \nprint(sent[2][0])\n\n#print(jaccard(sent[10][0],sent[10][1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Eval","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df=pd.read_csv(\"/kaggle/input/tweet-sentiment-extraction/test.csv\")\ntid,q,c=test_df[\"textID\"],test_df[\"sentiment\"].values,test_df[\"text\"].values\n\n\ntorch.set_grad_enabled(False)\nmodel.eval()\n\nanswers=[]\nfor i,_ in enumerate(q):\n    \n    if q[i]!=\"neutral\":\n        encoding = tokenizer.encode_plus(q[i], c[i])\n\n        input_ids, token_type_ids = encoding[\"input_ids\"], encoding[\"token_type_ids\"]\n\n        #start_scores, end_scores = model(torch.tensor([input_ids],device=\"cuda\"), token_type_ids=torch.tensor([token_type_ids],device=\"cuda\"))\n        out = model( torch.tensor([input_ids],device=\"cuda\"), token_type_ids=torch.tensor([token_type_ids], device=\"cuda\") )\n\n        start_scores, end_scores= out[0],out[1]\n\n        all_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n\n        answer = ' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)])\n        answer=answer.replace(' ##', '')\n        answer=answer.replace(\" ` \",\"`\")\n        answer=answer.replace(\" , \",\", \")\n        answer=answer.replace(\" ,\",\", \")\n        answer=answer.replace(\" .\",\".\")\n        answer=answer.replace(\" !\",\"!\")\n        answer=answer.replace(\" ?\",\"?\")\n        answer=answer.replace(\"* * * *\",\"****\")\n        answer=answer.replace(\"... \",\"...\")\n        answer=answer.replace(\"< 3\",\"<3\")\n        answer=answer.replace(\" )\",\")\")\n        answer=answer.replace(\"( \",\"(\")\n\n        #print(c[i])\n        #print(answer)\n        answers.append([tid[i],'\"'+answer+'\"'])\n    \n    elif q[i]==\"neutral\":\n        answers.append([tid[i],'\"'+c[i]+'\"'])\n        \n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndf=pd.DataFrame(data=answers,columns=[\"textID\",\"selected_text\"])\n\nprint(df[0:30])\n\ndf.to_csv(\"submission.csv\",index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}