{"cells":[{"metadata":{},"cell_type":"markdown","source":"# RoBERTa using PyTorch\n## This a RoBERTa version of @abhishek's [BERT Base Uncased using PyTorch](https://www.kaggle.com/abhishek/bert-base-uncased-using-pytorch)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# All the important imports","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"VERSION = 20200515\n!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py --version $VERSION --apt-packages libomp5 libopenblas-dev","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!export XLA_USE_BF16=1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch_xla.core.xla_model as xm\nimport torch_xla.distributed.parallel_loader as pl\nimport torch_xla.distributed.xla_multiprocessing as xmp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport torch\nimport pandas as pd\nimport torch.nn as nn\nimport numpy as np\nimport torch.nn.functional as F\nfrom torch.optim import lr_scheduler\n\nfrom sklearn import model_selection\nfrom sklearn import metrics\nimport transformers\nimport tokenizers\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup,get_cosine_with_hard_restarts_schedule_with_warmup\nfrom tqdm.autonotebook import tqdm\nimport utils","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ROBERTA_PATH = \"../input/roberta-base\"\nTOKENIZER = tokenizers.ByteLevelBPETokenizer(\n    vocab_file=f\"{ROBERTA_PATH}/vocab.json\", \n    merges_file=f\"{ROBERTA_PATH}/merges.txt\", \n    lowercase=True,\n    add_prefix_space=True\n)\nTRAINING_FILE = \"../input/tweet-train-folds-v2/train_folds.csv\"\nMAX_LEN = 192\nTRAIN_BATCH_SIZE = 32\nVALID_BATCH_SIZE = 8\nEPOCHS = 5","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Processing","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"def process_data(tweet, selected_text, sentiment, tokenizer, max_len):\n    \"\"\"\n    Processes the tweet and outputs the features necessary for model training and inference.\n    \n    Note: there are some differences between this and the BERT version (bert-case-uncased), mostly due to differences in token codes and special tokens\n    \"\"\"\n    tweet = \" \" + \" \".join(str(tweet).split())\n    selected_text = \" \" + \" \".join(str(selected_text).split())\n\n    len_st = len(selected_text) - 1\n    idx0 = None\n    idx1 = None\n\n    for ind in (i for i, e in enumerate(tweet) if e == selected_text[1]):\n        if \" \" + tweet[ind: ind+len_st] == selected_text:\n            idx0 = ind\n            idx1 = ind + len_st - 1\n            break\n\n    char_targets = [0] * len(tweet)\n    if idx0 != None and idx1 != None:\n        for ct in range(idx0, idx1 + 1):\n            char_targets[ct] = 1\n    \n    tok_tweet = tokenizer.encode(tweet)\n    input_ids_orig = tok_tweet.ids\n    tweet_offsets = tok_tweet.offsets\n    \n    target_idx = []\n    for j, (offset1, offset2) in enumerate(tweet_offsets):\n        if sum(char_targets[offset1: offset2]) > 0:\n            target_idx.append(j)\n    \n    targets_start = target_idx[0]\n    targets_end = target_idx[-1]\n\n    sentiment_id = {\n        'positive': 1313,\n        'negative': 2430,\n        'neutral': 7974\n    }\n    \n    input_ids = [0] + [sentiment_id[sentiment]] + [2] + [2] + input_ids_orig + [2]\n    token_type_ids = [0, 0, 0, 0] + [0] * (len(input_ids_orig) + 1)\n    mask = [1] * len(token_type_ids)\n    tweet_offsets = [(0, 0)] * 4 + tweet_offsets + [(0, 0)]\n    targets_start += 4\n    targets_end += 4\n\n    padding_length = max_len - len(input_ids)\n    if padding_length > 0:\n        input_ids = input_ids + ([1] * padding_length)\n        mask = mask + ([0] * padding_length)\n        token_type_ids = token_type_ids + ([0] * padding_length)\n        tweet_offsets = tweet_offsets + ([(0, 0)] * padding_length)\n    \n    return {\n        'ids': input_ids,\n        'mask': mask,\n        'token_type_ids': token_type_ids,\n        'targets_start': targets_start,\n        'targets_end': targets_end,\n        'orig_tweet': tweet,\n        'orig_selected': selected_text,\n        'sentiment': sentiment,\n        'offsets': tweet_offsets\n    }","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data loader","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class TweetDataset:\n    \"\"\"\n    Dataset which stores the tweets and returns them as processed features\n    \"\"\"\n    def __init__(self, tweet, sentiment, selected_text):\n        self.tweet = tweet\n        self.sentiment = sentiment\n        self.selected_text = selected_text\n        self.tokenizer = TOKENIZER\n        self.max_len = MAX_LEN\n    \n    def __len__(self):\n        return len(self.tweet)\n\n    def __getitem__(self, item):\n        data = process_data(\n            self.tweet[item], \n            self.selected_text[item], \n            self.sentiment[item],\n            self.tokenizer,\n            self.max_len\n        )\n        targets_start_oh = torch.zeros(192)\n        targets_start_oh[data[\"targets_start\"]] = 1\n        \n        targets_end_oh = torch.zeros(192)\n        targets_end_oh[data[\"targets_end\"]] = 1\n        \n        # Return the processed data where the lists are converted to `torch.tensor`s\n        return {\n            'ids': torch.tensor(data[\"ids\"], dtype=torch.long),\n            'mask': torch.tensor(data[\"mask\"], dtype=torch.long),\n            'token_type_ids': torch.tensor(data[\"token_type_ids\"], dtype=torch.long),\n            'targets_start': torch.tensor(data[\"targets_start\"], dtype=torch.long),\n            'targets_end': torch.tensor(data[\"targets_end\"], dtype=torch.long),\n            'orig_tweet': data[\"orig_tweet\"],\n            'orig_selected': data[\"orig_selected\"],\n            'sentiment': data[\"sentiment\"],\n            'offsets': torch.tensor(data[\"offsets\"], dtype=torch.long),\n            'targets_start_oh': targets_start_oh,\n            'targets_end_oh':targets_end_oh\n        }","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The Model","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"class TweetModel(transformers.BertPreTrainedModel):\n    \"\"\"\n    Model class that combines a pretrained bert model with a linear later\n    \"\"\"\n    def __init__(self, conf):\n        super(TweetModel, self).__init__(conf)\n        # Load the pretrained BERT model\n        self.roberta = transformers.RobertaModel.from_pretrained(ROBERTA_PATH, config=conf)\n        # Set 10% dropout to be applied to the BERT backbone's output\n        self.dropouts = nn.ModuleList([nn.Dropout(0.1) for _ in range(5)])\n        # 768 is the dimensionality of bert-base-uncased's hidden representations\n        # Multiplied by 2 since the forward pass concatenates the last two hidden representation layers\n        # The output will have two dimensions (\"start_logits\", and \"end_logits\")\n        self.l0 = nn.Linear(768 * 2, 2)\n        torch.nn.init.normal_(self.l0.weight, std=0.02)\n    \n    def forward(self, ids, mask, token_type_ids):\n        # Return the hidden states from the BERT backbone\n        _, _, out = self.roberta(\n            ids,\n            attention_mask=mask,\n            token_type_ids=token_type_ids\n        ) # bert_layers x bs x SL x (768 * 2)\n\n        # Concatenate the last two hidden states\n        # This is done since experiments have shown that just getting the last layer\n        # gives out vectors that may be too taylored to the original BERT training objectives (MLM + NSP)\n        # Sample explanation: https://bert-as-service.readthedocs.io/en/latest/section/faq.html#why-not-the-last-hidden-layer-why-second-to-last\n        out = torch.cat((out[-1], out[-2]), dim=-1) # bs x SL x (768 * 2)\n        # Apply 10% dropout to the last 2 hidden states\n        for i,dropout in enumerate(self.dropouts):\n            if i == 0:\n                out_sum = dropout(out)\n            else:\n                out_sum += dropout(out)\n        out = out_sum/len(self.dropouts)\n        # The \"dropped out\" hidden vectors are now fed into the linear layer to output two scores\n        logits = self.l0(out) # bs x SL x 2\n\n        # Splits the tensor into start_logits and end_logits\n        # (bs x SL x 2) -> (bs x SL x 1), (bs x SL x 1)\n        start_logits, end_logits = logits.split(1, dim=-1)\n\n        start_logits = start_logits.squeeze(-1) # (bs x SL)\n        end_logits = end_logits.squeeze(-1) # (bs x SL)\n\n        return start_logits, end_logits","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loss Function","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def loss_fn(start_logits, end_logits, start_positions, end_positions):\n    \"\"\"\n    Return the sum of the cross entropy losses for both the start and end logits\n    \"\"\"\n    loss_fct = nn.CrossEntropyLoss()\n    start_loss = loss_fct(start_logits, start_positions)\n    end_loss = loss_fct(end_logits, end_positions)\n    total_loss = (start_loss + end_loss)\n    return total_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def jac_transform(start_logits,end_logits,softmax=True,T=1):\n    if softmax:\n        start_logits = torch.softmax(start_logits/T,dim=1)\n        end_logits = torch.softmax(end_logits/T,dim=1)\n    start_cum = torch.cumsum(start_logits,dim=1)\n    end_cum = torch.cumsum(end_logits,dim=1)\n    end = end_cum - end_logits\n    return F.relu(start_cum - end)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def dice_loss(pred, target):\n    \"\"\"This definition generalize to real valued pred and target vector.\nThis should be differentiable.\n    pred: tensor with first dimension as batch\n    target: tensor with first dimension as batch\n    \"\"\"\n\n    smooth = 1.\n\n    # have to use contiguous since they may from a torch.view op\n    iflat = pred.contiguous().view(-1)\n    tflat = target.contiguous().view(-1)\n    intersection = (iflat * tflat).sum()\n\n    A_sum = torch.sum(iflat * tflat)\n    B_sum = torch.sum(tflat * tflat)\n    \n    return 1 - ((2. * intersection + smooth) / (A_sum + B_sum + smooth) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def jaccard_loss(output_start,output_end,target_start,target_end, smooth=1e-10):\n    start_cum = torch.cumsum(output_start,dim=1)\n    end_cum = torch.cumsum(output_end,dim=1)\n    end = end_cum - output_end\n    pred = F.relu(start_cum-end)\n    \n    start_cum_t = torch.cumsum(target_start,dim=1)\n    end_cum_t = torch.cumsum(target_end,dim=1)\n    end_t = end_cum_t - target_end\n    target = F.relu(start_cum_t-end_t)\n    \n    I = (pred * target).sum(axis=1, keepdim=True)\n    P = pred.sum(axis=1, keepdim=True)\n    T = target.sum(axis=1, keepdim=True)\n    U = P + T - I \n    IOU = (I+smooth) / (U + smooth)\n    \n    a = torch.max(start_cum,start_cum_t)\n    b = torch.min(end_cum,end_cum_t)\n    b = F.relu(b - output_end - target_end)\n    Ac = F.relu((a - b).sum(axis=1,keepdim=True))\n    giou = IOU - (Ac - U+smooth)/(Ac+smooth)\n    \n    loss_giou = 1 - giou\n    \n    return loss_giou.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_fn(vals):\n    return sum(vals) / len(vals)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training Function","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_fn(data_loader, model, optimizer, device,epoch,num_batches,scheduler=None):\n    \"\"\"\n    Trains the bert model on the twitter data\n    \"\"\"\n    # Set model to training mode (dropout + sampled batch norm is activated)\n    model.train()\n\n    # Set tqdm to add loading screen and set the length\n    tk0 = tqdm(data_loader, total=num_batches, desc=\"Training\", disable=not xm.is_master_ordinal())\n    # Train the model on each batch\n    for bi, d in enumerate(tk0):\n\n        ids = d[\"ids\"]\n        token_type_ids = d[\"token_type_ids\"]\n        mask = d[\"mask\"]\n        targets_start = d[\"targets_start\"]\n        targets_end = d[\"targets_end\"]\n        sentiment = d[\"sentiment\"]\n        orig_selected = d[\"orig_selected\"]\n        orig_tweet = d[\"orig_tweet\"]\n        targets_start = d[\"targets_start\"]\n        targets_end = d[\"targets_end\"]\n        offsets = d[\"offsets\"]\n        targets_start_oh = d['targets_start_oh']\n        targets_end_oh = d[\"targets_end_oh\"]\n\n        # Move ids, masks, and targets to gpu while setting as torch.long\n        ids = ids.to(device, dtype=torch.long)\n        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n        mask = mask.to(device, dtype=torch.long)\n        targets_start = targets_start.to(device, dtype=torch.long)\n        targets_end = targets_end.to(device, dtype=torch.long)\n        targets_start_oh = targets_start_oh.to(device,dtype=torch.float32)\n        targets_end_oh = targets_end_oh.to(device,dtype=torch.float32)\n        # Reset gradients\n        model.zero_grad()\n        # Use ids, masks, and token types as input to the model\n        # Predict logits for each of the input tokens for each batch\n        outputs_start, outputs_end = model(\n            ids=ids,\n            mask=mask,\n            token_type_ids=token_type_ids,\n        ) # (bs x SL), (bs x SL)\n        # Calculate batch loss based on CrossEntropy\n#         loss = loss_fn(outputs_start, outputs_end, targets_start, targets_end)\n        loss = loss_fn(outputs_start,outputs_end,targets_start,targets_end)\n        outputs_start = torch.softmax(outputs_start,dim=1)\n        outputs_end = torch.softmax(outputs_end,dim=1)\n        loss2 = jaccard_loss(outputs_start,outputs_end,targets_start_oh,targets_end_oh)\n        total_loss = 0.2*loss + loss2\n        total_loss.backward()\n        # Adjust weights based on calculated gradients\n#         optimizer.step()\n        xm.optimizer_step(optimizer)\n#         # Update scheduler\n        scheduler.step()\n        \n        # Update the jaccard score and loss\n        # For details, refer to `AverageMeter` in https://www.kaggle.com/abhishek/utils\n        print_loss = xm.mesh_reduce('loss_reduce', loss, reduce_fn)\n        print_loss2 = xm.mesh_reduce('loss_reduce', loss2, reduce_fn)\n        # Print the average loss and jaccard score at the end of each batch\n        tk0.set_postfix(loss=print_loss.item(),loss2=print_loss2.item())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluation Functions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def calculate_jaccard_score(\n    original_tweet, \n    target_string, \n    sentiment_val, \n    idx_start, \n    idx_end, \n    offsets,\n    verbose=False):\n    \"\"\"\n    Calculate the jaccard score from the predicted span and the actual span for a batch of tweets\n    \"\"\"\n    \n    # A span's start index has to be greater than or equal to the end index\n    # If this doesn't hold, the start index is set to equal the end index (the span is a single token)\n    if idx_end < idx_start:\n        idx_end = idx_start\n    \n    # Combine into a string the tokens that belong to the predicted span\n    filtered_output  = \"\"\n    for ix in range(idx_start, idx_end + 1):\n        filtered_output += original_tweet[offsets[ix][0]: offsets[ix][1]]\n        # If the token is not the last token in the tweet, and the ending offset of the current token is less\n        # than the beginning offset of the following token, add a space.\n        # Basically, add a space when the next token (word piece) corresponds to a new word\n        if (ix+1) < len(offsets) and offsets[ix][1] < offsets[ix+1][0]:\n            filtered_output += \" \"\n\n    # Set the predicted output as the original tweet when the tweet's sentiment is \"neutral\", or the tweet only contains one word\n    if len(original_tweet.split()) < 2:\n        filtered_output = original_tweet\n\n    # Calculate the jaccard score between the predicted span, and the actual span\n    # The IOU (intersection over union) approach is detailed in the utils module's `jaccard` function:\n    # https://www.kaggle.com/abhishek/utils\n    jac = utils.jaccard(target_string.strip(), filtered_output.strip())\n    return jac, filtered_output\n\n\ndef eval_fn(data_loader, model, device,num_batches,out_word=None):\n    \"\"\"\n    Evaluation function to predict on the test set\n    \"\"\"\n    # Set model to evaluation mode\n    # I.e., turn off dropout and set batchnorm to use overall mean and variance (from training), rather than batch level mean and variance\n    # Reference: https://github.com/pytorch/pytorch/issues/5406\n    model.eval()\n    losses = utils.AverageMeter()\n    jaccards = utils.AverageMeter()\n    outputs = []\n    # Turns off gradient calculations (https://datascience.stackexchange.com/questions/32651/what-is-the-use-of-torch-no-grad-in-pytorch)\n    with torch.no_grad():\n        tk0 = tqdm(data_loader, total=num_batches, desc=\"evalutation\", disable=not xm.is_master_ordinal())\n        # Make predictions and calculate loss / jaccard score for each batch\n        for bi, d in enumerate(tk0):\n            ids = d[\"ids\"]\n            token_type_ids = d[\"token_type_ids\"]\n            mask = d[\"mask\"]\n            sentiment = d[\"sentiment\"]\n            orig_selected = d[\"orig_selected\"]\n            orig_tweet = d[\"orig_tweet\"]\n            targets_start = d[\"targets_start\"]\n            targets_end = d[\"targets_end\"]\n            offsets = d[\"offsets\"].cpu().numpy()\n            \n            # Move ids, masks, and targets to gpu while setting as torch.long\n            ids = ids.to(device, dtype=torch.long)\n            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n            mask = mask.to(device, dtype=torch.long)\n            targets_start = targets_start.to(device, dtype=torch.long)\n            targets_end = targets_end.to(device, dtype=torch.long)\n\n            # Predict logits for start and end indexes\n            outputs_start, outputs_end = model(\n                ids=ids,\n                mask=mask,\n                token_type_ids=token_type_ids\n            )\n            # Calculate loss for the batch\n            loss = loss_fn(outputs_start, outputs_end, targets_start, targets_end)\n            # Apply softmax to the predicted logits for the start and end indexes\n            # This converts the \"logits\" to \"probability-like\" scores\n            outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n            outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n            # Calculate jaccard scores for each tweet in the batch\n            jaccard_scores = []\n            for px, tweet in enumerate(orig_tweet):\n                selected_tweet = orig_selected[px]\n                tweet_sentiment = sentiment[px]\n                jaccard_score, output = calculate_jaccard_score(\n                    original_tweet=tweet,\n                    target_string=selected_tweet,\n                    sentiment_val=tweet_sentiment,\n                    idx_start=np.argmax(outputs_start[px, :]),\n                    idx_end=np.argmax(outputs_end[px, :]),\n                    offsets=offsets[px]\n                )\n                jaccard_scores.append(jaccard_score)\n                if out_word != None:\n                    outputs.append(output)\n            # Update running jaccard score and loss\n            if out_word == None:\n                print_loss = xm.mesh_reduce('loss_reduce', loss, reduce_fn)\n            else:\n                print_loss = 'none'\n            # Print the average loss and jaccard score at the end of each batch\n            \n            tk0.set_postfix(loss=print_loss)\n            jaccards.update(np.mean(jaccard_scores), ids.size(0))\n            losses.update(loss.item(), ids.size(0))\n    if out_word != None:\n        return outputs\n    return jaccards.avg","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# model_config = transformers.RobertaConfig.from_pretrained(ROBERTA_PATH)\n# # Output hidden states\n# # This is important to set since we want to concatenate the hidden states from the last 2 BERT layers\n# model_config.output_hidden_states = True\n# # Instantiate our model with `model_config`\n\n# MX = TweetModel(conf=model_config)\n# MX","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# MX.roberta.encoder.layer[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch_xla.distributed.parallel_loader as pl\n# Load pretrained RoBERTa\nmodel_config = transformers.RobertaConfig.from_pretrained(ROBERTA_PATH)\n# Output hidden states\n# This is important to set since we want to concatenate the hidden states from the last 2 BERT layers\nmodel_config.output_hidden_states = True\n# Instantiate our model with `model_config`\n\nMX = TweetModel(conf=model_config)\nfrom torch.optim import SGD\n# Read training csv\n\ndef run(fold):\n    \"\"\"\n    Train model for a speciied fold\n    \"\"\"\n    dfx = pd.read_csv(TRAINING_FILE)\n    # Set train validation set split\n    df_train = dfx[dfx.kfold != fold].reset_index(drop=True)\n    df_valid = dfx[dfx.kfold == fold].reset_index(drop=True)\n    device = xm.xla_device()\n    model = MX.to(device)\n    # Instantiate TweetDataset with training data\n    train_dataset = TweetDataset(\n        tweet=df_train.text.values,\n        sentiment=df_train.sentiment.values,\n        selected_text=df_train.selected_text.values\n    )\n    \n    train_sampler = torch.utils.data.distributed.DistributedSampler(\n      train_dataset,\n      num_replicas=xm.xrt_world_size(),\n      rank=xm.get_ordinal(),\n      shuffle=True\n    )\n    \n    # Instantiate DataLoader with `train_dataset`\n    # This is a generator that yields the dataset in batches\n    train_data_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=TRAIN_BATCH_SIZE,\n        num_workers=2,\n        sampler= train_sampler,\n        drop_last=True\n    )\n\n    # Instantiate TweetDataset with validation data\n    valid_dataset = TweetDataset(\n        tweet=df_valid.text.values,\n        sentiment=df_valid.sentiment.values,\n        selected_text=df_valid.selected_text.values\n    )\n    \n    valid_sampler = torch.utils.data.distributed.DistributedSampler(\n      valid_dataset,\n      num_replicas=xm.xrt_world_size(),\n      rank=xm.get_ordinal(),\n      shuffle=False\n    )\n\n    # Instantiate DataLoader with `valid_dataset`\n    valid_data_loader = torch.utils.data.DataLoader(\n        valid_dataset,\n        batch_size=VALID_BATCH_SIZE,\n        num_workers=2,\n        sampler= valid_sampler\n    )\n\n    # Calculate the number of training steps\n    num_train_steps = int(\n        len(df_train) / TRAIN_BATCH_SIZE / xm.xrt_world_size() * EPOCHS\n    )\n    # Get the list of named parameters\n    param_optimizer = list(model.named_parameters())\n    # Specify parameters where weight decay shouldn't be applied\n    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n    # Define two sets of parameters: those with weight decay, and those without\n    optimizer_parameters = [\n        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n    ]\n    # Instantiate AdamW optimizer with our two sets of parameters, and a learning rate of 3e-5\n    optimizer = AdamW(\n        optimizer_parameters, \n        lr=0.2*3e-5 * xm.xrt_world_size()\n    )\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=85,\n        num_training_steps=num_train_steps\n    )\n\n    # Apply early stopping with patience of 2\n    # This means to stop training new epochs when 2 rounds have passed without any improvement\n    es = utils.EarlyStopping(patience=2, mode=\"max\")\n    xm.master_print(f\"Training is Starting for fold={fold}\")\n    num_batches = int(len(df_train) / (TRAIN_BATCH_SIZE * xm.xrt_world_size()))\n    # I'm training only for 3 epochs even though I specified 5!!!\n    best_jac = 0\n    for epoch in range(EPOCHS):\n        para_loader = pl.ParallelLoader(train_data_loader, [device])\n        train_fn(para_loader.per_device_loader(device), model, optimizer,device, epoch,num_batches,scheduler=scheduler)\n        para_loader = pl.ParallelLoader(valid_data_loader, [device])\n        jac = eval_fn(\n            para_loader.per_device_loader(device), \n            model, \n            device,\n            num_batches\n        )\n        jac = xm.mesh_reduce('jac_reduce', jac, reduce_fn)\n        if jac>best_jac:\n            xm.save(model.state_dict(),f'model_{fold}.bin')\n        xm.master_print(f\"Jaccard Score = {jac}\")\n    return jac","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def _mp_fn(rank, flags,fold):\n    torch.set_default_tensor_type('torch.FloatTensor')\n    a = run(fold)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"FLAGS={}\nxmp.spawn(_mp_fn, args=(FLAGS,0), nprocs=8, start_method='fork')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"FLAGS={}\nxmp.spawn(_mp_fn, args=(FLAGS,1), nprocs=8, start_method='fork')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"FLAGS={}\nxmp.spawn(_mp_fn, args=(FLAGS,2), nprocs=8, start_method='fork')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"FLAGS={}\nxmp.spawn(_mp_fn, args=(FLAGS,3), nprocs=8, start_method='fork')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"FLAGS={}\nxmp.spawn(_mp_fn, args=(FLAGS,4), nprocs=8, start_method='fork')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def final_evaluate_fn(fold):\n    device = xm.xla_device()\n    model = MX.to(device)\n    model.load_state_dict(torch.load(f'model_{fold}.bin'))\n    dfx = pd.read_csv(TRAINING_FILE)\n    df_valid = dfx[dfx.kfold == fold].reset_index(drop=True)\n    valid_dataset = TweetDataset(\n        tweet=df_valid.text.values,\n        sentiment=df_valid.sentiment.values,\n        selected_text=df_valid.selected_text.values\n    )\n    valid_data_loader = torch.utils.data.DataLoader(\n        valid_dataset,\n        batch_size=VALID_BATCH_SIZE,\n        num_workers=1,\n    )\n    num_batches = len(valid_data_loader)\n    output = eval_fn(\n        valid_data_loader, \n        model, \n        device,\n        num_batches,\n        out_word=True\n    )\n    df_valid.loc[:, 'selected_text_out'] = output\n    df_valid.to_csv(f'fold_{fold}.csv')\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(5):\n    final_evaluate_fn(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}