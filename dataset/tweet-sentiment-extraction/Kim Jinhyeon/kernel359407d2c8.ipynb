{"cells":[{"metadata":{"_uuid":"8f4c5c4e-1616-4b92-bdb2-e92dce05c54d","_cell_guid":"a8a029d9-2911-488e-a794-eb37a22ebec7","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n\n# Input data files are available in the read-only \"../input/\" directory\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\nimport csv\nimport re\n\nimport torch\nfrom tqdm.auto import tqdm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport time\n\nfrom transformers import BertForQuestionAnswering\n\nmodel_name = 'bert-base-uncased'\npretrained_path = f'../input/pretrainedbert/{model_name}'\nmodel = BertForQuestionAnswering.from_pretrained(pretrained_path).cuda()\n\ntimestamp = int(time.time())\ncheckpoint = f'checkpoint/{timestamp}'\nos.makedirs(checkpoint, exist_ok=True)\n\ndata_dir = '../input/tweet-sentiment-extraction'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Hyperparameter","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"NUM_EPOCH = 4\nBATCH_SIZE = 32\nLEARNING_RATE = 5e-5","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tokenizer","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import BertTokenizer\n\nspecial_tokens = ['<POS>', '<NEG>', '<NEU>', '<LINK>', '<MENTION>']\n\ntokenizer = BertTokenizer.from_pretrained(pretrained_path)\ntokenizer.add_tokens(special_tokens)\n\nmodel.resize_token_embeddings(len(tokenizer))\ntokenizer.tokenize('<NEU>')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"link_pattern = r'http.*?(?=[\\s)]|$)'\nmention_pattern = r'@\\w+(?!@)'\nbroken_pattern = 'ï¿½'\n\n\ndef normalize(string):\n    string = string.replace(broken_pattern, '`')\n    string = re.sub(link_pattern, '<LINK>', string)\n    string = re.sub(mention_pattern, '<MENTION>', string)\n    return string","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def unnormalize(predicted, original):\n    predicted = re.sub(r\"n['`]t\\b\", r\" not \", predicted)\n    predicted = re.sub(r'(\\W)', r' \\1 ', predicted)\n    pattern = re.escape(predicted)\n    pattern = pattern.replace(r\"\\ not\\ \", r\"\\ n('|`|o)t\\ \")\n    pattern = pattern.replace('`', f'(?:`|{broken_pattern})')\n    pattern = pattern.replace(r'<\\ link\\ >', link_pattern).replace(r'<\\ mention\\ >', mention_pattern)\n    pattern = re.sub(r'(\\\\ )+', r'\\\\s*', pattern)\n    match = re.search(pattern, original, re.IGNORECASE)\n    return match and match[0].strip()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preprocess","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def _trim(token):\n    return token[2:] if token.startswith('##') else token\n\ndef subfinder(haystack, needle):\n    if not needle:\n        return\n    length = len(needle)\n    for i, token in enumerate(haystack[:len(haystack)-length+1]):\n        if _trim(token) == _trim(needle[0]) and haystack[i+1:i + length] == needle[1:]:\n            return i, i + length - 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentiment_table = {'positive': '<POS>', 'negative': '<NEG>', 'neutral': '<NEU>'}\n\ndef _tokenize(s):\n    return tokenizer.tokenize(normalize(s))\n\ndef prepare_input_token(text, selected, sentiment):\n    selected_tokens = _tokenize(selected)\n    if selected:\n        before, after = text.split(selected, 1)\n        text = _tokenize(before) + selected_tokens + _tokenize(after)\n    else:\n        text = _tokenize(text)\n    sentiment = sentiment_table[sentiment]\n    return [sentiment] + text, selected_tokens","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LENGTH = 128","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_train_data():\n    with open(f'{data_dir}/train.csv', 'r', newline='') as f:\n        reader = csv.reader(f)\n        print(next(reader))\n\n        for textid, text, selected, sentiment in reader:\n            tokens, selected_tokens = prepare_input_token(text, selected, sentiment)\n            span = subfinder(tokens, selected_tokens)\n            if not span:\n                continue  # discard\n\n            ids = tokenizer.encode(tokens,\n                                   pad_to_max_length=True,\n                                   max_length=MAX_LENGTH,\n                                   truncation_strategy='do_not_truncate')\n            yield torch.tensor(ids), torch.tensor(span) + 1  # [BOS]\n\ntrain_data = list(load_train_data())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\n\nrandom.shuffle(train_data)\n\ntotal = len(train_data)\ndev_size = total // 21\ndev_data = train_data[:dev_size]\ntrain_data = train_data[dev_size:]\ntotal, total - dev_size, dev_size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ntrain_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\ndev_loader = DataLoader(dev_data, batch_size=BATCH_SIZE, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Metric","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn.functional as F\n\ndef token_jaccard(spans):\n    '''spans: a Tensor of shape [num_spans, batch_size, 2]'''\n    earlier_starts, later_starts = spans[..., 0].min(dim=0)[0], spans[..., 0].max(dim=0)[0]\n    earlier_ends, later_ends = spans[..., 1].min(dim=0)[0], spans[..., 1].max(dim=0)[0]\n    intersections = F.relu(earlier_ends - later_starts + 1).float()\n    unions = later_ends - earlier_starts + 1\n    return (intersections / unions).mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import AdamW\n\noptimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n_criterion = torch.nn.CrossEntropyLoss()\n\ndef calculate_loss(logits, spans):\n    _, _, hidden_size = logits.shape\n    return _criterion(logits.view(-1, hidden_size), spans.t().flatten())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(model, input_ids):\n    pad_mask = (input_ids == 0)\n    logits = model(input_ids)\n    logits = torch.stack(logits, dim=0)  # [2, B, T]\n    logits[:, pad_mask] = float('-inf')\n    return logits","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_step(batch):\n    input_ids, spans = (item.cuda() for item in batch)\n    optimizer.zero_grad()\n    logits = predict(model, input_ids)\n    loss = calculate_loss(logits, spans)\n    loss.backward()\n    optimizer.step()\n    return loss.item()\n\ndef eval_step(batch):\n    input_ids, spans = (item.cuda() for item in batch)\n    logits = predict(model, input_ids)\n    loss = calculate_loss(logits, spans)\n\n    output = logits.argmax(dim=-1).t()  # [B, 2]\n    score = token_jaccard(torch.stack([spans, output], dim=0))\n    return loss.item(), score.item()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def update_average(acc, cur, i):\n    return (acc * i + cur) / (i + 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_losses = []\neval_losses = []\neval_scores = []\n\nbest_score = 0.0\nfor epoch in range(NUM_EPOCH):\n    loss_avg = 0.0\n    model.train()\n    pbar = tqdm(train_loader)\n    for i, batch in enumerate(pbar):\n        loss = train_step(batch)\n        loss_avg = update_average(loss_avg, loss, i)\n        pbar.set_description(f'Train loss {loss_avg:.4f}')\n    train_losses.append(loss_avg)\n\n    loss_avg, score_avg = 0.0, 0.0\n    model.eval()\n    with torch.no_grad():\n        pbar = tqdm(dev_loader)\n        for i, batch in enumerate(pbar):\n            loss, score = eval_step(batch)\n            loss_avg = update_average(loss_avg, loss, i)\n            score_avg = update_average(score_avg, score, i)\n            pbar.set_description(f'Eval loss {loss_avg:.4f} score {score_avg:.4f}')\n    eval_losses.append(loss_avg)\n    eval_scores.append(score_avg)\n\n    if best_score < score_avg:\n        model.save_pretrained(checkpoint)\n        best_score = score_avg\n\ntrain_losses, eval_losses, eval_scores","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Test","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"del model\nmodel = BertForQuestionAnswering.from_pretrained(checkpoint).cuda()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_bank = {}\nwith open(f'{data_dir}/test.csv', 'r', newline='') as f:\n    reader = csv.reader(f)\n    print(next(reader))\n    for textid, text, sentiment in reader:\n        test_bank[textid] = (text, sentiment)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def infer_start_end(logits):  # [2, 1, T]\n    logits = logits.squeeze(1)  # [2, T]\n    eos_pos = (logits[0, :] != float('-inf')).sum() - 1\n    if eos_pos <= 2:\n        return logits.argmax(dim=-1)\n    probs = torch.nn.functional.softmax(logits, dim=-1)  # [2, T]\n    start, end = probs[0, 2:eos_pos], probs[1, 2:eos_pos]\n    joint_probs = start.unsqueeze(1) * end.unsqueeze(0)  # [T, T]\n    pos = torch.triu(joint_probs).argmax()\n    start_pos, end_pos = (pos // (eos_pos - 2) + 2), (pos % (eos_pos - 2) + 2)\n    return start_pos, end_pos","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def submission(textid):\n    text, sentiment = test_bank[textid]\n    tokens, _ = prepare_input_token(text, '', sentiment)\n    input_ids = tokenizer.encode(tokens, pad_to_max_length=True)\n    logits = predict(model, torch.tensor([input_ids]).cuda())  # [2, 1, T]\n    start, end = infer_start_end(logits)\n    prediction = tokenizer.decode(input_ids[start:end+1])\n    return unnormalize(prediction, text), prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"error_cases = {}\nwith torch.no_grad():\n    with open(f'{data_dir}/sample_submission.csv', 'r') as f, open(f'submission.csv', 'w') as g:\n        g.write(next(f))\n        for line in tqdm(f, total=3534):\n            textid = line[:10]\n            output, prediction = submission(textid)\n            if output is None:\n                error_cases[textid] = prediction\n                output = test_bank[textid][0]  # echo back\n            g.write(f'{textid},\"{output}\"\\n')\nlen(error_cases)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('error_cases.txt', 'w') as f:\n    f.write('\\n'.join(f'{textid},{prediction}' for textid, prediction in error_cases.items()))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}