{"cells":[{"metadata":{"_uuid":"35b637ff-897b-4a87-9265-dc70b219adcd","_cell_guid":"f8dc566c-bb0f-42d4-bbeb-a85977e8d9d8","trusted":true},"cell_type":"code","source":"import csv\nimport sys\nimport cv2\nimport os\nimport random\nimport re\nimport nltk\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom numpy import array\nfrom numpy import asarray\nfrom numpy import zeros\nfrom nltk.corpus import stopwords\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import defaultdict\nfrom keras import backend as keras\nfrom keras.models import *\nfrom keras.layers import *\nfrom keras.optimizers import *\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.callbacks import CSVLogger\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Activation, Dropout, Flatten, Dense, Conv2D, MaxPooling2D,Embedding,LSTM\nfrom keras.callbacks import ModelCheckpoint, LearningRateScheduler\nfrom keras.preprocessing.text import one_hot\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import GlobalMaxPooling1D\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.utils import to_categorical\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.layers  import Bidirectional\n\ntf.test.gpu_device_name()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install '/kaggle/input/wheeel/keras_self_attention-0.46.0-py3-none-any.whl'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#!pip install keras-self-attention","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#!ls /root/.cache/pip/wheels/ec/f7/48/30de93f8333298bad9202aab9b04db0cfd58dcd379b5a5ef1c","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#!mv /root/.cache/pip/wheels/ec/f7/48/30de93f8333298bad9202aab9b04db0cfd58dcd379b5a5ef1c/* /kaggle/working/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#!ls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras_self_attention import SeqSelfAttention","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e2e42e51-092d-4a03-87c8-c9089ce12df5","_cell_guid":"91d5e558-416e-41f0-84f5-9665277581d0","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nddir=[]\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        ddir.append(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"148f2444-ce41-4477-a4d3-f21c8af4e046","_cell_guid":"674b38d5-1f1c-4fda-9a45-887dafb5c631","trusted":true},"cell_type":"code","source":"ddir","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"be891d96-8a53-4ea6-90da-39e3a3b2b5a2","_cell_guid":"196c18e6-454f-4140-921e-c0aba661bfa0","trusted":true},"cell_type":"code","source":"def preprocess_train(train):\n  #preprocessing for the train dataset\n  train['text'] = train['text'].fillna('')\n  train['selected_text'] = train['selected_text'].fillna('')\n  train['sentiment']=train['sentiment'].replace('neutral',0)\n  train['sentiment']=train['sentiment'].replace('positive',1)\n  train['sentiment']=train['sentiment'].replace('negative',1)\n  #sns.countplot(x='sentiment', data=train)\n  #plt.show()\n  return train\ndef preprocess_test(test):\n  #preprocessing for the train dataset\n  test['text'] = test['text'].fillna('')\n  test['sentiment']=test['sentiment'].replace('neutral',0)\n  test['sentiment']=test['sentiment'].replace('positive',1)\n  test['sentiment']=test['sentiment'].replace('negative',1)\n  #sns.countplot(x='sentiment', data=test)\n  #plt.show()\n  return test","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"304abf6b-22bf-4ae3-8756-08d312842da0","_cell_guid":"0254fe0b-a82e-4cd3-b0d3-ff6c05a9bb87","trusted":true},"cell_type":"code","source":"embedsize=200\ndef embed(tokenizer):\n  embeddings_dictionary = dict()\n  glove_file = open('/kaggle/input/glove-global-vectors-for-word-representation/glove.twitter.27B.200d.txt', encoding=\"utf8\")\n\n  for line in glove_file:\n      records = line.split()\n      word = records[0]\n      vector_dimensions = asarray(records[1:], dtype='float32')\n      embeddings_dictionary [word] = vector_dimensions\n  glove_file.close()\n\n  #embeddings_dictionary\n  embedding_matrix = zeros((vocab_size, embedsize))\n  for word, index in tokenizer.word_index.items():\n      embedding_vector = embeddings_dictionary.get(word)\n      if embedding_vector is not None:\n          embedding_matrix[index] = embedding_vector\n  return embedding_matrix\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#analysing\ntrain_original=pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv')\ntrain_original=preprocess_train(train_original)\n\ntest_original=pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv')\ntest_original=preprocess_test(test_original)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer(num_words=500000,filters='')\ntokenizer.fit_on_texts(train_original['text'])\nmaxlen=35\ntrain_original['Text_Sequences'] = pd.Series(tokenizer.texts_to_sequences(train_original['text']))\ntrain_original['Text_Sequences_padded']  = pad_sequences(train_original['Text_Sequences'] , padding='post', maxlen=maxlen).tolist()\n\ntest_original['Text_Sequences'] = pd.Series(tokenizer.texts_to_sequences(test_original['text']))\ntest_original['Text_Sequences_padded']  = pad_sequences(test_original['Text_Sequences'] , padding='post', maxlen=maxlen).tolist()\n\ntrain_original['Selected_Text_Sequences']=pd.Series(tokenizer.texts_to_sequences(train_original['selected_text']))\ntrain_original['Selected_Text_Sequences_padded'] = pad_sequences(train_original['Selected_Text_Sequences'], padding='post', maxlen=maxlen).tolist()\n\nY_train_per_word=np.zeros((train_original['Text_Sequences_padded'].shape[0],maxlen))\nidx=0\nfor sentence in train_original['Text_Sequences_padded']:\n  idx2=0\n  for word in sentence:\n    #print(word)\n    if (word != 0) and (train_original['sentiment'][idx]!=0):\n      if (word==train_original['Text_Sequences_padded'][idx][idx2]):\n        Y_train_per_word[idx][idx2]= train_original['sentiment'][idx]\n        idx2=idx2+1\n  idx=idx+1\ntrain_original['Y_labeled']=Y_train_per_word.tolist()\n#train_original['jaccard_distance']=jaccard_score(train_original['Text_Sequences_padded'],train_original['Selected_Text_Sequences_padded'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_temp=(train_original[train_original['sentiment']!=0]['Text_Sequences_padded'])\nX_train=np.array([np.array(xi) for xi in X_train_temp])\nY_train_temp=train_original[train_original['sentiment']!=0]['Y_labeled']\nY_train=np.array([np.array(xi) for xi in Y_train_temp])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_size = len(tokenizer.word_index) + 1\nembedding_matrix=embed(tokenizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def jaccard_distance(y_true, y_pred, smooth=100):\n    \"\"\" Calculates mean of Jaccard distance as a loss function \"\"\"\n    intersection = tf.reduce_sum(y_true * y_pred, axis=(1))\n    sum_ = tf.reduce_sum(y_true + y_pred, axis=(1))\n    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n    jd =  (1 - jac) * smooth\n    return tf.reduce_mean(jd)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def jaccard_score(y_true, y_pred, smooth=100):\n    \"\"\" Calculates mean of Jaccard distance as a loss function \"\"\"\n    arr1=np.array([np.array(xi) for xi in y_true])\n    arr1=(arr1>0).astype('int')\n    arr2=np.array([np.array(xi) for xi in y_pred])\n    arr2=(arr2>0).astype('int')\n    intersection = np.sum(np.multiply(arr1 , arr2), axis=(1))\n    sum_ =np.sum(arr1,axis=1)+np.sum(arr2,axis=1)\n    jac = (intersection + smooth) / (sum_ - intersection + smooth) #score\n    jd =  (1 - jac) * smooth #distance\n    return (jd)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#not stateful\\\nimport tensorflow.keras\nimport keras_self_attention \nimport keras\nfrom keras_self_attention import SeqSelfAttention \nfrom keras.layers import Activation, Dropout, Flatten, Dense,Conv1D, Conv2D, MaxPooling2D,Embedding,LSTM,Bidirectional\nX_train1, X_test1, y_train1, y_test1 = train_test_split(X_train, Y_train, test_size=0.20, random_state=42)\nmodel1 = keras.Sequential()\nembedding_layer = Embedding(vocab_size, embedsize, weights=[embedding_matrix], input_length=maxlen , trainable=False)\nmodel1.add(embedding_layer)\nmodel1.add(Dropout(0.2))\n#model1.add((Dense(35, activation='relu')))\n#model1.add(Bidirectional(LSTM(128 ,input_shape=(maxlen,100),return_sequences=True)))\nmodel1.add(LSTM(128,input_shape=(maxlen,embedsize), return_sequences=True))\nmodel1.add(Conv1D(64,(3),padding='same',activation='relu'))\nmodel1.add(Dropout(0.2))\nmodel1.add(Conv1D(16,(3),padding='same',activation='relu'))\nmodel1.add(Dropout(0.2))\nmodel1.add(Conv1D(1,(3),padding='same',activation='relu'))\n#model1.add(SeqSelfAttention(attention_activation='sigmoid'))\n#model1.add(Reshape((128,maxlen, 1)))\n#model1.add(Dropout(0.2))\nmodel1.add((Dense(1, activation='sigmoid')))\nmodel1.compile(optimizer='adam', loss=jaccard_distance)\nprint(model1.summary())\n\nY_train = Y_train.reshape(Y_train.shape[0], Y_train.shape[1], 1)\ny_train1 = y_train1.reshape(y_train1.shape[0], y_train1.shape[1], 1)\ny_test1 = y_test1.reshape(y_test1.shape[0], y_test1.shape[1], 1)\n#history = model1.fit(X_train1, y_train1, batch_size=128, epochs=110, verbose=1, validation_data=(X_test1, y_test1))\n#history = model1.fit(X_train, Y_train, batch_size=128, epochs=120, verbose=1)\nmodel1.load_weights('/kaggle/input/lstm-and-convolution/save_model_lstm_with_conv.h5')\nscore = model1.evaluate(X_test1, y_test1, verbose=1)\nprint(score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model1.fit(X_train, Y_train, batch_size=128, epochs=15, verbose=1, validation_data=(X_test1, y_test1))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model1.save_weights('/kaggle/working/save_model_lstm_with_conv.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def postprocessing1(res2):\n  resres2=np.zeros((res2.shape))\n  i=0\n  for item in res2:\n    arr=np.where(item==True)[0]\n    length=len(np.where(item==True)[0])\n    if length >0:\n      first=arr[0]\n      last=arr[length-1]\n      for j in range(first,last+1):\n        resres2[i][j]=1\n    i=i+1\n  return resres2\n\ndef postprocessing1_1(res2):\n  resres2=np.zeros((res2.shape))\n  i=0\n  for item in res2:\n    arr=np.where(item==True)[0]\n    length=len(np.where(item==True)[0])\n    if length >0:\n      first=arr[0]\n      last=arr[length-1]\n      for item2 in arr:\n        resres2[i][item2]=1\n    i=i+1\n  return resres2\n\n\ndef postprocessing2(X_TEST,TEST_SENTIMENT,RES_Y_TEST):\n  X_submission=np.copy(X_TEST)\n  i=0\n  for item in RES_Y_TEST:\n    if TEST_SENTIMENT[i]!=0:\n      j=0\n      for word in item:\n        if word==0 and X_TEST[i][j]!=0:\n          X_submission[i][j]=0 \n        j=j+1\n    i=i+1\n  return X_submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred=model1.predict(X_test1)\nres=y_pred>0.5\nres=postprocessing1(res)\njac_des=jaccard_score(res,y_test1)\nprint(\"mean\",np.mean(jac_des))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_temp=(test_original['Text_Sequences_padded'])\nX_TEST=np.array([np.array(xi) for xi in X_test_temp])\nY_test_temp=test_original['sentiment']\nTEST_SENTIMENT=np.array([np.array(xi) for xi in Y_test_temp])\nY_TEST=model1.predict(X_TEST)\nRES_Y_TEST=Y_TEST>0.5\nRES_Y_TEST=postprocessing1_1(RES_Y_TEST)\nX_submission=postprocessing2(X_TEST,TEST_SENTIMENT,RES_Y_TEST)\nX_submission_text=tokenizer.sequences_to_texts(X_submission)\n#X_submission_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"i=0\nfor item in TEST_SENTIMENT:\n  if item==0:\n    X_submission_text[i]=test_original['text'][i]\n  i=i+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas import DataFrame\ndf = DataFrame({'textID': test_original['textID'], 'selected_text': X_submission_text})\ndf.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}