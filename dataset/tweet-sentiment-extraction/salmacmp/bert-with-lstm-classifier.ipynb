{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import csv\nimport sys\nimport cv2\nimport os\nimport random\nimport re\nimport nltk\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom numpy import array\nfrom numpy import asarray\nfrom numpy import zeros\nfrom nltk.corpus import stopwords\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom keras import backend as keras\nfrom keras.models import *\nfrom keras.layers import *\nfrom keras.optimizers import *\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.callbacks import CSVLogger\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Activation, Dropout, Flatten, Dense, Conv2D, MaxPooling2D,Embedding,LSTM\nfrom keras.callbacks import ModelCheckpoint, LearningRateScheduler\nfrom keras.preprocessing.text import one_hot\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import GlobalMaxPooling1D\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.utils import to_categorical\nfrom tensorflow.keras.layers import Embedding\n\ntf.test.gpu_device_name()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#!conda install -y gdown","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#import gdown\n#url = 'https://drive.google.com/uc?export=download&id=1-NxLIxP1FZm9T-eC9u22ZvOqbTSnYTSi'\n#output = 'model_w.h5'\n#gdown.download(url, output, quiet=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install '/kaggle/input/transformerswhlfiles/transformers-2.11.0-py3-none-any.whl'\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_train(train):\n  #preprocessing for the train dataset\n  train['text'] = train['text'].fillna('')\n  train['selected_text'] = train['selected_text'].fillna('')\n  train['sentiment']=train['sentiment'].replace('neutral',0)\n  train['sentiment']=train['sentiment'].replace('positive',1)\n  train['sentiment']=train['sentiment'].replace('negative',2)\n  #sns.countplot(x='sentiment', data=train)\n  #plt.show()\n  return train\ndef preprocess_test(test):\n  #preprocessing for the train dataset\n  test['text'] = test['text'].fillna('')\n  test['sentiment']=test['sentiment'].replace('neutral',0)\n  test['sentiment']=test['sentiment'].replace('positive',1)\n  test['sentiment']=test['sentiment'].replace('negative',2)\n  #sns.countplot(x='sentiment', data=test)\n  #plt.show()\n  return test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#analysing\ntrain_original=pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv')\ntrain_original=preprocess_train(train_original)\n\ntest_original=pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv')\ntest_original=preprocess_test(test_original)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import BertTokenizer\ntokenizer = BertTokenizer.from_pretrained('../input/saved-pretrains')\nvocabulary = tokenizer.get_vocab()\nprint(list(vocabulary.keys())[5000:5020])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_length = 35\nbatch_size = 6","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert_example_to_feature(review):\n  # combine step for tokenization, WordPiece vector mapping, adding special tokens as well as truncating reviews longer than the max length\n  return tokenizer.encode_plus(review, \n                add_special_tokens = True, # add [CLS], [SEP]\n                max_length = max_length, # max length of the text that can go to BERT\n                pad_to_max_length = True, # add [PAD] tokens\n                return_attention_mask = True, # add attention mask to not focus on pad tokens\n              )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode_examples_train(ds):\n  input_ids_list = []\n  token_type_ids_list = []\n  attention_mask_list = []\n  label_list = []\n  all_list=[]\n  for review,selected,sent in (ds[['text','selected_text','sentiment']]).itertuples(index=False):\n    bert_input = convert_example_to_feature(review)\n    input_ids_list=(bert_input['input_ids'])\n    token_type_ids_list=(bert_input['token_type_ids'])\n    attention_mask_list=(bert_input['attention_mask'])\n    bertselect=convert_example_to_feature(selected )\n    label_list.append([sent*x for x in bertselect['attention_mask']])\n    all_list.append([input_ids_list,token_type_ids_list,attention_mask_list])\n  return all_list,label_list\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode_examples_test(ds):\n  input_ids_list = []\n  token_type_ids_list = []\n  attention_mask_list = []\n  all_list=[]\n  for review,sent in (ds[['text','sentiment']]).itertuples(index=False):\n    bert_input = convert_example_to_feature(review)\n    input_ids_list=(bert_input['input_ids'])\n    token_type_ids_list=(bert_input['token_type_ids'])\n    attention_mask_list=(bert_input['attention_mask'])\n    all_list.append([input_ids_list,token_type_ids_list,attention_mask_list])\n  return all_list\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# train dataset\nds_train_encoded,labels_train = encode_examples_train(train_original)\n\n# test dataset\nds_test_encoded = encode_examples_test(test_original)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def jaccard_distance(y_true, y_pred, smooth=100):\n    \"\"\" Calculates mean of Jaccard distance as a loss function \"\"\"\n    intersection = tf.reduce_sum(y_true * y_pred, axis=(1))\n    sum_ = tf.reduce_sum(y_true + y_pred, axis=(1))\n    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n    jd =  (1 - jac) * smooth\n    return tf.reduce_mean(jd)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def jaccard_score(y_true, y_pred, smooth=100):\n    \"\"\" Calculates mean of Jaccard distance as a loss function \"\"\"\n    arr1=np.array([np.array(xi) for xi in y_true])\n    arr1=(arr1>0).astype('int')\n    arr2=np.array([np.array(xi) for xi in y_pred])\n    arr2=(arr2>0).astype('int')\n    intersection = np.sum(np.multiply(arr1 , arr2), axis=(1))\n    sum_ =np.sum(arr1,axis=1)+np.sum(arr2,axis=1)\n    jac = (intersection + smooth) / (sum_ - intersection + smooth) #score\n    jd =  (1 - jac) * smooth #distance\n    return (jd)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import TFBertModel,BertConfig\nimport tensorflow as tf\n\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.layers import Dense,Conv2D,Reshape,LSTM,MaxPooling2D,Flatten,Conv1D,MaxPooling1D,Dropout\n\nlearning_rate = 2e-5\n\nall_ins = Input(shape = (3,max_length,), dtype=tf.int32)\nids = all_ins[:,0,:]\natt =  all_ins[:,1,:]\ntok =  all_ins[:,2,:]\n\n'''\n    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n'''\n#config = BertConfig.from_pretrained('kaggle/input/saved-pretrains/config.json')\nconfig = BertConfig() # print(config) to see settings\nconfig.output_hidden_states = False # \nbert = TFBertModel.from_pretrained('/kaggle/input/bert-base-uncased/bert-base-uncased-tf_model.h5',config=config)([ids,att,tok])\n\n#bert = TFBertModel.from_pretrained('bert-base-uncased')([ids,att,tok]) \nbert = bert[0]\nlstmlayer=LSTM(128,input_shape=(35,768), return_sequences=True)(bert)\n\nconv1=Conv1D(64,(3),padding='same',activation='relu')(lstmlayer)\ndrop1=Dropout(0.5)(conv1)\nconv2=Conv1D(32,(3),padding='same',activation='relu')(drop1)\ndrop2=Dropout(0.5)(conv2)\nconv3=Conv1D(16,(3),padding='same',activation='relu')(drop2)\nclassifier = Dense(3, activation='softmax')(conv3)\n\nmodel = Model(all_ins, outputs=classifier)\nloss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\nmetric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\noptimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=1e-08)\nmodel.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=[metric])\nmodel.summary()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#model.load_weights('/kaggle/input/bert-classifier/save_model_w.h5')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yt=np.array(labels_train)\nprint(yt.shape)\n#y= yt.reshape(yt.shape[0], yt.shape[1], 1)\n#y= yt.reshape(yt.shape[0], 1)\n#bert_history = model.fit( ds_train_encoded,labels_train,batch_size=128,epochs=18)\nmodel.load_weights('/kaggle/input/bert-with-lstm-classifier/save_model_bert_with_cnn.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bert_history = model.fit( ds_train_encoded,labels_train,batch_size=128,epochs=6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save_weights('/kaggle/working/save_model_bert_with_cnn.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#url = 'https://drive.google.com/uc?export=download&id=1GR-R-rr8gKx2luKRlDs5af4o1t-YVDIp'\n#output = 'berty.h5'\n#gdown.download(url, output, quiet=False)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_TEST=model.predict(ds_train_encoded[10:20])\npred=np.argmax(Y_TEST,axis=-1)\nprint(pred)\nprint(labels_train[0:10])\njac_des=jaccard_score(pred,labels_train[10:20])\nprint(jac_des)\nprint(np.mean(jac_des))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_TEST_blind=model.predict(ds_test_encoded)\nY_TEST_blind_pred=np.argmax(Y_TEST_blind,axis=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#convert the output\noutput=[]\noutput_decoded=[]\nsent_list=[]\ni=0\nfor item in ds_test_encoded:\n  j=0\n  temparr=[]\n  sent=0\n  for item2 in item[0]:\n    if Y_TEST_blind_pred[i][j]>0:\n      if (item2!=102) and (item2!=101): \n        temparr.append(item2)\n      sent=Y_TEST_blind_pred[i][j]\n    j=j+1\n  output.append(temparr.copy())\n  if (sent!=0) and test_original['sentiment'][i]!=0 :\n    output_decoded.append(tokenizer.decode(temparr))\n  else:\n    output_decoded.append(test_original['text'][i])\n  sent_list.append(sent)\n  i=i+1\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"z=0\nprint(test_original['text'][z])\nprint(ds_test_encoded[z][0])\nprint(Y_TEST_blind_pred[z])\nprint(output[z])\nprint(output_decoded[z])\nprint(sent_list[z])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas import DataFrame\ndf = DataFrame({'textID': test_original['textID'], 'selected_text': output_decoded})\ndf.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}