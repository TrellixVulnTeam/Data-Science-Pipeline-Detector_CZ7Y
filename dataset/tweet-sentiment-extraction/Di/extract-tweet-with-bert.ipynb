{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input, Conv1D, Flatten, Dropout, Activation, LeakyReLU\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model,load_model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow_hub as hub\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.model_selection import StratifiedKFold\nimport tensorflow.keras.backend as K\n\nfrom shutil import copyfile\n#copy our file into the working directory (make sure it has .py suffix)\ncopyfile(src = \"../input/bert-tokenization/bert_tokenization.py\", dst = \"../working/bert_tokenization.py\")\n\nimport bert_tokenization\n#from transformers import BertTokenizer","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2021-07-19T16:37:46.464215Z","iopub.execute_input":"2021-07-19T16:37:46.464536Z","iopub.status.idle":"2021-07-19T16:37:52.07354Z","shell.execute_reply.started":"2021-07-19T16:37:46.464508Z","shell.execute_reply":"2021-07-19T16:37:52.072553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Import libraries that we need","metadata":{}},{"cell_type":"markdown","source":"# Data Overview","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv')\ndf_test = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv')\ndf_train.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-07-19T16:37:52.075431Z","iopub.execute_input":"2021-07-19T16:37:52.07582Z","iopub.status.idle":"2021-07-19T16:37:52.23814Z","shell.execute_reply.started":"2021-07-19T16:37:52.075775Z","shell.execute_reply":"2021-07-19T16:37:52.237444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.dropna(how='any',axis=0,inplace=True)\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-19T16:37:52.239571Z","iopub.execute_input":"2021-07-19T16:37:52.23992Z","iopub.status.idle":"2021-07-19T16:37:52.272681Z","shell.execute_reply.started":"2021-07-19T16:37:52.239883Z","shell.execute_reply":"2021-07-19T16:37:52.271678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-07-19T16:37:52.274648Z","iopub.execute_input":"2021-07-19T16:37:52.275087Z","iopub.status.idle":"2021-07-19T16:37:52.284872Z","shell.execute_reply.started":"2021-07-19T16:37:52.275043Z","shell.execute_reply":"2021-07-19T16:37:52.283734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-19T16:37:52.2893Z","iopub.execute_input":"2021-07-19T16:37:52.290045Z","iopub.status.idle":"2021-07-19T16:37:52.302361Z","shell.execute_reply.started":"2021-07-19T16:37:52.290001Z","shell.execute_reply":"2021-07-19T16:37:52.30129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"colors = sns.color_palette()\nplt.subplot(211)\nsentiment_num_1 = df_train['sentiment'].value_counts()\nsentiment_num_1.plot(kind='bar',figsize=(10,10),color=colors[0],rot=0)\nplt.title('Sentiment Distribution for Train Data')\n\nplt.subplot(212)\nsentiment_num_2 = df_test['sentiment'].value_counts()\nsentiment_num_2.plot(kind='bar',figsize=(10,10),color=colors[1],rot=0)\nplt.title('Sentiment Distribution for Test Data')\n\nplt.tight_layout(pad =3)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-19T16:37:52.305834Z","iopub.execute_input":"2021-07-19T16:37:52.306231Z","iopub.status.idle":"2021-07-19T16:37:52.664951Z","shell.execute_reply.started":"2021-07-19T16:37:52.306192Z","shell.execute_reply":"2021-07-19T16:37:52.663314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Simple visualization for data distribution (The sentiment distributions for train and test data are almost same)","metadata":{}},{"cell_type":"code","source":"df_train['word_cnt_full_texts'] = df_train['text'].apply(lambda x: len(x.split()))\ndf_train['word_cnt_sel_texts'] = df_train['selected_text'].apply(lambda x: len(x.split()))\nfig,axes = plt.subplots(nrows=3,ncols=1,figsize=(8,20))\nfor i,s in enumerate(['positive','negative','neutral']):\n    sns.distplot(df_train[df_train.sentiment==s]['word_cnt_full_texts'],\n                 bins=20, color='skyblue', label='full texts', ax=axes[i])\n    sns.distplot(df_train[df_train.sentiment==s]['word_cnt_sel_texts'],\n                 bins=20, color='red', label='sel texts', ax=axes[i])\n    axes[i].legend(fontsize=14)\n    axes[i].set_title('%s: full texts length vs selected texts length'%(s),fontsize=15,fontweight='bold')","metadata":{"execution":{"iopub.status.busy":"2021-07-19T16:37:52.669457Z","iopub.execute_input":"2021-07-19T16:37:52.670017Z","iopub.status.idle":"2021-07-19T16:37:53.581446Z","shell.execute_reply.started":"2021-07-19T16:37:52.669963Z","shell.execute_reply":"2021-07-19T16:37:53.580407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It looks like for neutral tweets, slected texts and full texts are almost the same. While for positive and negative tweets, selected texts are only a small part of full texts.","metadata":{}},{"cell_type":"markdown","source":"# Bert","metadata":{}},{"cell_type":"markdown","source":"First I take a look at how the tokenizer works","metadata":{}},{"cell_type":"code","source":"bert_layer = hub.KerasLayer('../input/berthub', trainable=True)\nvocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = bert_tokenization.FullTokenizer(vocab_file, do_lower_case)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T16:37:53.582776Z","iopub.execute_input":"2021-07-19T16:37:53.583136Z","iopub.status.idle":"2021-07-19T16:38:06.90831Z","shell.execute_reply.started":"2021-07-19T16:37:53.583101Z","shell.execute_reply":"2021-07-19T16:38:06.907177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df_train['text'][1]) \nprint(tokenizer.tokenize(df_train['text'][1]))\nprint(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(df_train['text'][1])))","metadata":{"execution":{"iopub.status.busy":"2021-07-19T16:38:06.91219Z","iopub.execute_input":"2021-07-19T16:38:06.912506Z","iopub.status.idle":"2021-07-19T16:38:06.92388Z","shell.execute_reply.started":"2021-07-19T16:38:06.912467Z","shell.execute_reply":"2021-07-19T16:38:06.922752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df_train['selected_text'][1])\nprint(tokenizer.tokenize(df_train['selected_text'][1]))\nprint(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(df_train['selected_text'][1])))","metadata":{"execution":{"iopub.status.busy":"2021-07-19T16:38:06.925929Z","iopub.execute_input":"2021-07-19T16:38:06.926648Z","iopub.status.idle":"2021-07-19T16:38:06.936283Z","shell.execute_reply.started":"2021-07-19T16:38:06.926601Z","shell.execute_reply":"2021-07-19T16:38:06.935088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(tokenizer.convert_tokens_to_ids(['[CLS]']))\nprint(tokenizer.convert_tokens_to_ids(['[SEP]']))","metadata":{"execution":{"iopub.status.busy":"2021-07-19T16:38:06.938669Z","iopub.execute_input":"2021-07-19T16:38:06.939092Z","iopub.status.idle":"2021-07-19T16:38:06.945547Z","shell.execute_reply.started":"2021-07-19T16:38:06.939049Z","shell.execute_reply":"2021-07-19T16:38:06.944538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Encode train and test data","metadata":{}},{"cell_type":"markdown","source":"Then I segment encoding part step by step","metadata":{}},{"cell_type":"markdown","source":"Step 1. Decompose full texts into three parts: texts before selected texts, selected texts and texts after selected texts","metadata":{}},{"cell_type":"markdown","source":"I use the second sentence as an example","metadata":{}},{"cell_type":"code","source":"train_texts = df_train['text']\ntrain_sel_texts = df_train['selected_text']\ntrain_sentiment = df_train['sentiment']\ntexts = list(train_texts)\nsentiments = list(train_sentiment)\nsel_texts = list(train_sel_texts)\nlen(sel_texts),len(texts),len(sentiments)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T16:38:06.947565Z","iopub.execute_input":"2021-07-19T16:38:06.947961Z","iopub.status.idle":"2021-07-19T16:38:06.971236Z","shell.execute_reply.started":"2021-07-19T16:38:06.947916Z","shell.execute_reply":"2021-07-19T16:38:06.970356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentiments[1],texts[1],sel_texts[1]","metadata":{"execution":{"iopub.status.busy":"2021-07-19T16:38:06.972873Z","iopub.execute_input":"2021-07-19T16:38:06.973244Z","iopub.status.idle":"2021-07-19T16:38:06.979973Z","shell.execute_reply.started":"2021-07-19T16:38:06.973207Z","shell.execute_reply":"2021-07-19T16:38:06.979097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_idx = texts[1].find(sel_texts[1])\nend_idx = start_idx + len(sel_texts[1])-1                \nsentiment = sentiments[1]\nfull_text_1 = tokenizer.tokenize(texts[1][:start_idx])\nfull_text_2 = tokenizer.tokenize(texts[1][start_idx:end_idx+1])\nfull_text_3 = tokenizer.tokenize(texts[1][end_idx+1:])\nsentiment,full_text_1,full_text_2,full_text_3","metadata":{"execution":{"iopub.status.busy":"2021-07-19T16:38:06.981516Z","iopub.execute_input":"2021-07-19T16:38:06.982064Z","iopub.status.idle":"2021-07-19T16:38:06.994269Z","shell.execute_reply.started":"2021-07-19T16:38:06.982025Z","shell.execute_reply":"2021-07-19T16:38:06.992956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Step 2. tokenize three input arrays: input_ids,attention_masks,type_ids","metadata":{}},{"cell_type":"code","source":"max_len=150","metadata":{"execution":{"iopub.status.busy":"2021-07-19T16:38:06.995941Z","iopub.execute_input":"2021-07-19T16:38:06.996321Z","iopub.status.idle":"2021-07-19T16:38:07.0052Z","shell.execute_reply.started":"2021-07-19T16:38:06.996285Z","shell.execute_reply":"2021-07-19T16:38:07.004039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_tokens = ['[CLS]'] + [sentiment] + ['[SEP]'] + full_text_1+ full_text_2 + full_text_3 +['[SEP]']\npad_len = max_len - len(input_tokens)\nvalid_input_ids = tokenizer.convert_tokens_to_ids(input_tokens)\ninput_ids = valid_input_ids + [0]*pad_len\nattention_masks = [1]*len(valid_input_ids) + [0]*pad_len\ntype_ids = [0]*3 + [1]*(len(valid_input_ids)-3)+[0]*pad_len\nprint(input_tokens)\nprint(input_ids)\nprint(attention_masks)\nprint(type_ids)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T16:38:07.006751Z","iopub.execute_input":"2021-07-19T16:38:07.007193Z","iopub.status.idle":"2021-07-19T16:38:07.01976Z","shell.execute_reply.started":"2021-07-19T16:38:07.00715Z","shell.execute_reply":"2021-07-19T16:38:07.018449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Step 3. tokenize two output arrays: start and end","metadata":{}},{"cell_type":"code","source":"start_tokens = [0]*(len(full_text_1)+3)+[1]+[0]*(max_len-len(full_text_1)-4)\nend_tokens = [0]*(len(full_text_1)+len(full_text_2)+2)+[1]+[0]*(max_len-len(full_text_1)-len(full_text_2)-3)\nprint(start_tokens)\nprint(end_tokens)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T16:38:07.021781Z","iopub.execute_input":"2021-07-19T16:38:07.022229Z","iopub.status.idle":"2021-07-19T16:38:07.033509Z","shell.execute_reply.started":"2021-07-19T16:38:07.022184Z","shell.execute_reply":"2021-07-19T16:38:07.032151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check if all the arrays have the same length","metadata":{}},{"cell_type":"code","source":"len(input_ids),len(attention_masks),len(type_ids),len(start_tokens),len(end_tokens)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T16:38:07.035562Z","iopub.execute_input":"2021-07-19T16:38:07.036525Z","iopub.status.idle":"2021-07-19T16:38:07.044163Z","shell.execute_reply.started":"2021-07-19T16:38:07.036488Z","shell.execute_reply":"2021-07-19T16:38:07.043037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Wrap the step-by-step encoding methods","metadata":{}},{"cell_type":"code","source":"def bert_encode_train(sentiments, texts, sel_texts, tokenizer, max_len =512):\n    all_input_ids = []\n    all_masks = []\n    all_type_ids = []\n    all_start_tokens = []\n    all_end_tokens = []\n    \n    \n    for i in range(len(texts)):\n        \n        start_idx = texts[i].find(sel_texts[i])\n        end_idx = start_idx + len(sel_texts[i])-1                \n        sentiment = sentiments[i]\n        full_text_1 = tokenizer.tokenize(texts[i][:start_idx])\n        full_text_2 = tokenizer.tokenize(texts[i][start_idx:end_idx+1])\n        full_text_3 = tokenizer.tokenize(texts[i][end_idx+1:])\n        \n        input_tokens = ['[CLS]'] + [sentiment] + ['[SEP]'] + full_text_1+ full_text_2 + full_text_3 +['[SEP]']\n        pad_len = max_len - len(input_tokens)\n        valid_input_ids = tokenizer.convert_tokens_to_ids(input_tokens)\n        input_ids = valid_input_ids + [0]*pad_len\n        attention_masks = [1]*len(valid_input_ids) + [0]*pad_len\n        type_ids = [0]*3 + [1]*(len(valid_input_ids)-3) + [0]*pad_len\n        #type_ids = [0]*len(input_ids)\n        \n        start_tokens = [0]*(len(full_text_1)+3)+[1]+[0]*(max_len-len(full_text_1)-4)\n        end_tokens = [0]*(len(full_text_1)+len(full_text_2)+2)+[1]+[0]*(max_len-len(full_text_1)-len(full_text_2)-3)\n        \n        all_input_ids.append(input_ids)\n        all_masks.append(attention_masks)\n        all_type_ids.append(type_ids)\n        all_start_tokens.append(start_tokens) \n        all_end_tokens.append(end_tokens)\n        \n    return np.array(all_input_ids), np.array(all_masks), np.array(all_type_ids),np.array(all_start_tokens),np.array(all_end_tokens)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T16:38:07.046141Z","iopub.execute_input":"2021-07-19T16:38:07.046675Z","iopub.status.idle":"2021-07-19T16:38:07.06591Z","shell.execute_reply.started":"2021-07-19T16:38:07.046495Z","shell.execute_reply":"2021-07-19T16:38:07.064786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check max length after bert encoding","metadata":{}},{"cell_type":"code","source":"max_len = 0\nfor i in range(df_train.shape[0]+1):\n    try:\n        tokens = tokenizer.tokenize(df_train['text'][i])\n        input_ids = tokenizer.convert_tokens_to_ids(['[CLS]']+list(df_train.loc[i,'sentiment'])+\n                                       ['[SEP]']+tokens+['[SEP]'])\n        max_len = max(max_len, len(input_ids))\n    except:\n        pass\n\nprint('Max length for training data: ', max_len)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T16:38:07.067414Z","iopub.execute_input":"2021-07-19T16:38:07.068053Z","iopub.status.idle":"2021-07-19T16:38:17.02875Z","shell.execute_reply.started":"2021-07-19T16:38:07.06801Z","shell.execute_reply":"2021-07-19T16:38:17.027866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_len = 0\nfor i in range(df_train.shape[0]+1):\n    try:\n        tokens = tokenizer.tokenize(df_test['text'][i])\n        input_ids = tokenizer.convert_tokens_to_ids(['[CLS]']+list(df_test.loc[i,'sentiment'])+\n                                       ['[SEP]']+tokens+['[SEP]'])\n        max_len = max(max_len, len(input_ids))\n    except:\n        pass\n\nprint('Max length for test data: ', max_len)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T16:38:17.030044Z","iopub.execute_input":"2021-07-19T16:38:17.030555Z","iopub.status.idle":"2021-07-19T16:38:19.09419Z","shell.execute_reply.started":"2021-07-19T16:38:17.030514Z","shell.execute_reply":"2021-07-19T16:38:19.093271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since the word count of full texts and selected texts for neutral text is almost the same, I only use positive and negative texts as training and test data. ","metadata":{}},{"cell_type":"code","source":"train_texts = df_train[df_train['sentiment']!='neutral']['text']\ntrain_sel_texts = df_train[df_train['sentiment']!='neutral']['selected_text']\ntrain_sentiment = df_train[df_train['sentiment']!='neutral']['sentiment']\n#train_texts = df_train['text']\n#train_sel_texts = df_train['selected_text']\n#train_sentiment = df_train['sentiment']\nfull_texts = list(train_texts)\nsentiments = list(train_sentiment)\nsel_texts = list(train_sel_texts)\n\ntrain_input = bert_encode_train(sentiments,full_texts,sel_texts,tokenizer, max_len =150)[:3]\ntrain_labels = bert_encode_train(sentiments,full_texts,sel_texts,tokenizer, max_len =150)[3:]","metadata":{"execution":{"iopub.status.busy":"2021-07-19T16:38:19.097955Z","iopub.execute_input":"2021-07-19T16:38:19.098318Z","iopub.status.idle":"2021-07-19T16:38:35.964235Z","shell.execute_reply.started":"2021-07-19T16:38:19.098282Z","shell.execute_reply":"2021-07-19T16:38:35.963223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def bert_encode_test(sentiments, texts, tokenizer, max_len =512):\n    all_input_ids = []\n    all_masks = []\n    all_type_ids = []\n\n    for i in range(len(texts)):\n        text = tokenizer.tokenize(texts[i])               \n        sentiment = sentiments[i]\n        input_tokens = ['[CLS]'] + [sentiment] + ['[SEP]'] + text +['[SEP]']\n        pad_len = max_len - len(input_tokens)\n        valid_input_ids = tokenizer.convert_tokens_to_ids(input_tokens)\n        input_ids = valid_input_ids + [0]*pad_len\n        attention_masks = [1]*len(valid_input_ids) + [0]*pad_len\n        type_ids = [0]*3 + [1]*(len(valid_input_ids)-3) + [0]*pad_len\n        #type_ids = [0]*len(input_ids)\n        \n        all_input_ids.append(input_ids)\n        all_masks.append(attention_masks)\n        all_type_ids.append(type_ids)\n      \n    return np.array(all_input_ids), np.array(all_masks), np.array(all_type_ids)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T16:38:35.965598Z","iopub.execute_input":"2021-07-19T16:38:35.965995Z","iopub.status.idle":"2021-07-19T16:38:35.978083Z","shell.execute_reply.started":"2021-07-19T16:38:35.965957Z","shell.execute_reply":"2021-07-19T16:38:35.976786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_texts = df_test[df_test['sentiment']!='neutral']['text']\ntest_sentiment = df_test[df_test['sentiment']!='neutral']['sentiment']\n#test_texts = df_test['text']\n#test_sentiment = df_test['sentiment']\nfull_texts_test = list(test_texts)\nsentiments_test = list(test_sentiment)\n\ntest_input = bert_encode_test(sentiments_test, full_texts_test, tokenizer, max_len =150)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T16:38:35.979557Z","iopub.execute_input":"2021-07-19T16:38:35.979947Z","iopub.status.idle":"2021-07-19T16:38:36.853367Z","shell.execute_reply.started":"2021-07-19T16:38:35.979907Z","shell.execute_reply":"2021-07-19T16:38:36.85259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Build Bert Model with CNN head","metadata":{}},{"cell_type":"code","source":"# K.clear_session()","metadata":{"execution":{"iopub.status.busy":"2021-07-19T16:38:36.854659Z","iopub.execute_input":"2021-07-19T16:38:36.855032Z","iopub.status.idle":"2021-07-19T16:38:36.859251Z","shell.execute_reply.started":"2021-07-19T16:38:36.854994Z","shell.execute_reply":"2021-07-19T16:38:36.858296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_bert(bert_layer, max_len =512):\n    adam = Adam(lr=3e-5)\n    main_input = Input(shape =(max_len,), dtype =tf.int32)\n    input_word_ids = Input(shape = (max_len,),dtype =tf.int32)\n    input_mask = Input(shape = (max_len,),dtype =tf.int32)\n    input_type_ids = Input(shape = (max_len,),dtype =tf.int32)\n    \n    clf_output = bert_layer([input_word_ids, input_mask, input_type_ids])\n    #pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, input_type_ids])\n    #clf_output = sequence_output[1]\n    \n    out1 = Dropout(0.1)(clf_output[1])\n    out1 = Conv1D(filters=1, kernel_size=1) (out1)\n    out1 = Flatten()(out1)\n    out1 = Activation('softmax')(out1)\n    \n    out2 = Dropout(0.1)(clf_output[1])\n    out2 = Conv1D(filters=1, kernel_size=1) (out2)\n    out2 = Flatten()(out2)\n    out2 = Activation('softmax')(out2)\n    \n    model = Model(inputs = [input_word_ids, input_mask, input_type_ids], outputs =[out1,out2])\n    model.compile(optimizer=Adam(lr=3e-5) ,loss = 'categorical_crossentropy')\n    print(model.summary())\n    return model\n\n\nmodel = build_bert(bert_layer,max_len=150)\nfilepath='best_weight.hdf5'\ncheckpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\nmodel.fit(train_input, train_labels, epochs =3, batch_size = 16, callbacks=[checkpoint], validation_split=0.2)\n#for layer in model.layers:\n    #print(layer.output_shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T16:38:36.861151Z","iopub.execute_input":"2021-07-19T16:38:36.86162Z","iopub.status.idle":"2021-07-19T16:53:02.991465Z","shell.execute_reply.started":"2021-07-19T16:38:36.861581Z","shell.execute_reply":"2021-07-19T16:53:02.990645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def jaccard_similarity(str1,str2):\n    a = set(str1.lower().split())\n    b = set(str2.lower().split())\n    if not a and not b:\n        return 0.5 \n    c = a.intersection(b)\n    return float(len(c)/(len(a)+len(b)-len(c)))","metadata":{"execution":{"iopub.status.busy":"2021-07-19T16:53:02.994546Z","iopub.execute_input":"2021-07-19T16:53:02.994822Z","iopub.status.idle":"2021-07-19T16:53:03.001763Z","shell.execute_reply.started":"2021-07-19T16:53:02.994784Z","shell.execute_reply":"2021-07-19T16:53:03.0009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"jaccard_similarity(' Sooo SAD I will miss you here in San Diego!!!','Sooo SAD')","metadata":{"execution":{"iopub.status.busy":"2021-07-19T16:53:03.002887Z","iopub.execute_input":"2021-07-19T16:53:03.003401Z","iopub.status.idle":"2021-07-19T16:53:03.014071Z","shell.execute_reply.started":"2021-07-19T16:53:03.003371Z","shell.execute_reply":"2021-07-19T16:53:03.013014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predict and Submission","metadata":{}},{"cell_type":"markdown","source":"When it comes to the submission, selected texts of neutral texts are assigned with full texts.","metadata":{}},{"cell_type":"code","source":"best_model = load_model('./best_weight.hdf5',custom_objects={'KerasLayer':bert_layer})\npred_start,pred_end = model.predict(test_input)\nresults = []\nfor k in range(test_input[0].shape[0]):\n    a = np.argmax(pred_start[k])\n    b = np.argmax(pred_end[k])\n    \n    if a>b:\n        sel_text = full_texts_test[k]\n    else:\n        sel_text = ' '.join(tokenizer.convert_ids_to_tokens(test_input[0][k,a:b+1]))\n        \n    results.append(sel_text)\n\n#google fulltokenizer will generate meaingless punction ##   \nresults = [x.replace(' ##','') for x in results]\n    \nfor k in range(df_test.shape[0]):\n    if df_test.loc[k, 'sentiment'] == 'neutral':\n        df_test.loc[k, 'selected_text'] = df_test.loc[k, 'text']\n\ndf_test.loc[df_test['sentiment']!='neutral','selected_text'] = results\n#df_test['selected_text'] = results\noutput = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/sample_submission.csv')\noutput['selected_text'] = df_test['selected_text']","metadata":{"execution":{"iopub.status.busy":"2021-07-19T17:05:56.123166Z","iopub.execute_input":"2021-07-19T17:05:56.123568Z","iopub.status.idle":"2021-07-19T17:06:20.076593Z","shell.execute_reply.started":"2021-07-19T17:05:56.123534Z","shell.execute_reply":"2021-07-19T17:06:20.075831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output.to_csv('submission.csv',index=False,header=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T16:53:27.904496Z","iopub.execute_input":"2021-07-19T16:53:27.904846Z","iopub.status.idle":"2021-07-19T16:53:28.09257Z","shell.execute_reply.started":"2021-07-19T16:53:27.904809Z","shell.execute_reply":"2021-07-19T16:53:28.091779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output['text'] = df_test['text']\noutput['sentiment'] = df_test['sentiment']\noutput.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T17:07:26.640343Z","iopub.execute_input":"2021-07-19T17:07:26.640681Z","iopub.status.idle":"2021-07-19T17:07:26.658855Z","shell.execute_reply.started":"2021-07-19T17:07:26.64065Z","shell.execute_reply":"2021-07-19T17:07:26.657909Z"},"trusted":true},"execution_count":null,"outputs":[]}]}