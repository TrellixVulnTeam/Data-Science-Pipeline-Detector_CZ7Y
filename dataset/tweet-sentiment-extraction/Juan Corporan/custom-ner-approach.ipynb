{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/tweet-sentiment-extraction/train.csv\")\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv(\"/kaggle/input/tweet-sentiment-extraction/test.csv\")\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Preparation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"full = train.append(test)\nfull.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full = full.reset_index(drop=True)\nfull[\"train\"] = 0\nfull.loc[train.index,\"train\"] = 1\nfull.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full = full.loc[(full.text.notna())] \nfull.text = full.text.str.strip()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full[\"Has_BW\"] = full.text.str.contains(\"\\*\\*\\*\").astype(int)\nfull.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full[\"clean_text\"] = full[\"text\"]\nfull.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\ndef clean_text(text):\n    lower = text.lower()\n    no_links = re.sub(\"((http|https)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#]+\\.([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#])*\",\"\",lower)\n    no_alpha_num = re.sub(\"[^a-zA-Z\\s\\*]\",\"\", no_links)\n    no_bw = no_alpha_num.replace(\"**\",\"*n\").replace(\"*n*n\",\"*n\")\n    no_extra_spaces =no_bw.replace(\"  \",\" \")\n    return no_bw\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full.clean_text = full.clean_text.apply(clean_text)\nfull.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full.text = full.text.str.replace(\"((http|https)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#]+\\.([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#])*\",\"\")\nfull.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modelling","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train = full.loc[full.train == 1]\ntest = full.loc[full.train == 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"WRONG_DATA\"] = train.apply(lambda x: x[\"selected_text\"] not in x[\"text\"],axis=1).astype(int)\ntrain.loc[train.clean_text == '','WRONG_DATA'] = 1\ntrain = train.loc[train.WRONG_DATA == 0]\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"selected_clean_text\"] = train.selected_text.apply(clean_text)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.loc[train.selected_clean_text != \"\"]\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_len = full.text.str.split().apply(len).max()\nmax_len","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert_text(texts):\n    bio_texts_df = pd.DataFrame([])\n    for text in texts:\n        text_id_sentiment_data = list()\n        selected_text = text['selected_clean_text']\n        clean_text = text['clean_text']\n        split_text = clean_text.split(selected_text)\n        for t in [split_text[0],selected_text,split_text[1]]:\n            if t != '':\n                for w in t.split():\n                    text_id_sentiment_data.append({\n                            'word':w,\n                            'target':1 if t == selected_text else 0,\n                            'textID': text['textID']\n                        })\n        bio_texts_df = bio_texts_df.append(pd.DataFrame(text_id_sentiment_data))\n    return bio_texts_df.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_converted_train = convert_text(train.to_dict(orient='records'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_text_ids = data_converted_train.textID.drop_duplicates().sample(frac=0.9)\ntest_text_ids = data_converted_train.loc[~(data_converted_train.textID.isin(train_text_ids))].textID.drop_duplicates()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_converted_train_full = data_converted_train.merge(train[[\"textID\",\"sentiment\"]])\ndata_converted_train_full.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nimport numpy as np \nnp.random.seed(777)\ndef create_sentiment_data(data,sentiment):\n    sentiment_data = data.loc[data.sentiment == sentiment]\n    train_text_ids = sentiment_data.textID.drop_duplicates().sample(frac=0.9).sort_values()\n    test_text_ids = sentiment_data.loc[~(sentiment_data.textID.isin(train_text_ids))].textID.drop_duplicates().sort_values()\n    agg_func = lambda s: [(w, t) for w, t in zip(s[\"word\"].values.tolist(),s[\"target\"].values.tolist())]\n    train_sentiment_data = sentiment_data.loc[sentiment_data.textID.isin(train_text_ids)].sort_values(by='textID')\n    test_sentiment_data = sentiment_data.loc[sentiment_data.textID.isin(test_text_ids)].sort_values(by='textID')\n    \n    train_grouped_sentences = train_sentiment_data.groupby(\"textID\").apply(agg_func)\n    train_sentences = [s for s in train_grouped_sentences]\n    train_raw_sentences = [\" \".join([w[0] for w in s]) for s in train_sentences]\n    \n    test_grouped_sentences = test_sentiment_data.groupby(\"textID\").apply(agg_func)\n    test_sentences = [s for s in test_grouped_sentences]\n    test_raw_sentences = [\" \".join([w[0] for w in s]) for s in test_sentences]\n    \n    vocab_length = 10000\n    entity_text_tokenizer = Tokenizer(vocab_length)\n    entity_text_tokenizer.fit_on_texts(train_raw_sentences)\n    train_embedded_sentences = entity_text_tokenizer.texts_to_sequences(train_raw_sentences)\n    train_padded_sentences = pad_sequences(train_embedded_sentences, max_len, padding='post')  \n\n    test_embedded_sentences = entity_text_tokenizer.texts_to_sequences(test_raw_sentences)\n    test_padded_sentences = pad_sequences(test_embedded_sentences, max_len, padding='post')\n    \n    \n    train_tags = train_sentiment_data.target.unique()\n    n_tags = len(train_tags)\n    tags2index = {t:i for i,t in enumerate(train_tags)}\n    train_target_tags = [[tags2index[w[1]] for w in s] for s in train_sentences]\n    train_target_tags = pad_sequences(maxlen=max_len, sequences=train_target_tags, padding=\"post\", value=0)\n    \n    test_tags = test_sentiment_data.target.unique()\n    n_tags = len(test_tags)\n    tags2index = {t:i for i,t in enumerate(test_tags)}\n    test_target_tags = [[tags2index[w[1]] for w in s] for s in test_sentences]\n    test_target_tags = pad_sequences(maxlen=max_len, sequences=test_target_tags, padding=\"post\", value=0)\n    \n    return entity_text_tokenizer,train_padded_sentences,test_padded_sentences,train_target_tags,test_target_tags,train_text_ids,test_text_ids","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"positive_tokenizer,train_positive_sentences,test_positive_sentences,train_positive_tags,test_positive_tags,train_pos_ids,test_pos_ids = create_sentiment_data(data_converted_train_full,\"positive\")\ntrain_text_ids.iloc[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow.keras as keras\nfrom tensorflow.keras.layers import Concatenate\nfrom tensorflow.keras import Model, Input\nfrom tensorflow.keras.backend import clear_session\nimport gc\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nimport sklearn.metrics as sklm\nfrom tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, Lambda,Attention,GlobalAveragePooling1D,Conv1D,GlobalMaxPooling1D","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_length = 10000\nn_tags=2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_text = Input(shape=(max_len,))\nembedding  = Embedding(vocab_length, 32, input_length=max_len)(input_text)\nlstm_1 = Bidirectional(LSTM(units=32, return_sequences=True,\n                       recurrent_dropout=0.2, dropout=0.2))(embedding)\nlstm_2 = Bidirectional(LSTM(units=32, return_sequences=True,\n                           recurrent_dropout=0.2, dropout=0.2))(lstm_1)\nmerge = Concatenate()([lstm_1,lstm_2])\noutput = TimeDistributed(Dense(n_tags, activation=\"softmax\"))(merge)\nmodel = Model(input_text, output)\nmodel.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(train_positive_sentences, train_positive_tags, epochs=5, verbose=1,batch_size=64)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install seqeval","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from seqeval.metrics import precision_score, recall_score, f1_score, classification_report\ntest_pred = model.predict(np.array(test_positive_sentences))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def pred2label(pred):\n    out = []\n    for pred_i in pred:\n        out_i = []\n        for p in pred_i:\n            p_i = np.argmax(p)\n            out_i.append(str(p_i))\n        out.append(out_i)\n    return out\ndef test2label(pred):\n    out = []\n    for pred_i in pred:\n        out_i = []\n        for p in pred_i:\n            out_i.append(str(p))\n        out.append(out_i)\n    return out\n    \npred_labels = pred2label(test_pred)\ntest_labels = test2label(test_positive_tags)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_labels[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_labels[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(test_labels, pred_labels))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_texts = positive_tokenizer.sequences_to_texts(test_positive_sentences)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_texts[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def pred_to_selected_text(texts,preds):\n    selected_texts = list()\n    for index,text in enumerate(texts):\n        pred_selected_text = preds[index]\n        words = text.split()\n        prediction = \" \".join([words[i] for i,label in enumerate(pred_selected_text) if label == 1 and i < len(words)])\n        selected_texts.append(prediction)\n    return selected_texts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pos_ids.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.loc[train.textID == \"9339ee8e0b\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.text.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_to_selected_text(test_texts,test_positive_tags)[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}