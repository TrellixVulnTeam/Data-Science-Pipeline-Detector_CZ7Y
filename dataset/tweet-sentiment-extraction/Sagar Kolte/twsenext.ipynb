{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Credits\nI trained my LSTM using the very clear and concise example in this notebook(By Peter Nagy):\n\n- https://www.kaggle.com/ngyptr/lstm-sentiment-analysis-keras\n\nI learned how to use Word2Vec embeddings in my LSTM using the instructions in this notebook(user:lystdo):\n\n- https://www.kaggle.com/lystdo/lstm-with-word2vec-embeddings\n\nMy First course on using word embeddings in python came from here:\n\n- https://ai.intelligentonlinetools.com/ml/text-vectors-word-embeddings-word2vec/\n\nA great trivial example of a LSTM implemetation, get the important stuff straight:\n\nhttps://www.youtube.com/watch?v=iMIWee_PXl8\n\nAdding an embedding layer to your LSTM in Keras\n\nhttps://www.youtube.com/watch?v=8h8Z_pKyifM\n\n## Reference material for Transformers and BERT\n\nThis is a great lecture by Pascal Pupoart as a first introduction to Transformers and Multi Headed Attention.\n\n- https://youtu.be/OyFJWRnt_AY\n\nThis article by Jay Alammar is a great step by step documentation of the vector transformations that happen in a transformer.\n\n- http://jalammar.github.io/illustrated-transformer/\n\nThis is a tutorial on using BERT as a classifier in the NLP context.\n\nhttps://towardsdatascience.com/bert-for-dummies-step-by-step-tutorial-fb90890ffe03\n\n\n","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \npd.options.display.max_colwidth = 2000\nfrom scipy import stats\nimport seaborn as sns\nimport math\nfrom nltk.util import ngrams\nfrom nltk.util import everygrams\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n#---------------------------------------->Sentiment Analyser Libraries\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils.np_utils import to_categorical\nfrom sklearn.model_selection import GridSearchCV\nfrom keras.wrappers.scikit_learn import KerasClassifier\nimport re\nfrom gensim.models import Word2Vec\nfrom nltk.tokenize import word_tokenize\n#---------------------------------------->Sentiment Analyser Libraries","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# install\n!pip install pytorch-pretrained-bert pytorch-nlp\n\n# BERT imports\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom pytorch_pretrained_bert import BertTokenizer, BertConfig\nfrom pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\nfrom tqdm import tqdm, trange\nimport pandas as pd\nimport io\nimport numpy as np\nimport matplotlib.pyplot as plt\n#% matplotlib inline\n\n# specify GPU device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nn_gpu = torch.cuda.device_count()\ntorch.cuda.get_device_name(0)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Prepare the data for training","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv')\n# Keeping only the neccessary columns\ndata1 = df_train[['text','sentiment']]\ndata1['n_sentiment'] = data1.apply(lambda row: 'p' if row['sentiment']=='positive' else ('n' if row['sentiment']=='negative' else 'neu'), axis = 1)#--->Making the sentiment numeric\ndata1 = data1[['text','n_sentiment']]\ndata1.columns=['text','sentiment']\ndata2 = df_train[['selected_text','sentiment']]\ndata2.columns=['text','sentiment']\ndata2['n_sentiment'] = data2.apply(lambda row: 'vp' if row['sentiment']=='positive' else ('vn' if row['sentiment']=='negative' else 'neu'), axis = 1)#--->Making the sentiment numeric\ndata2 = data2[['text','n_sentiment']]\ndata2.columns=['text','sentiment']\ndata =  pd.concat([data1,data2])#-------------------------------------------------------------------------->Final Training Data\ndata['text'] = data['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',str(x))))#------------------------->Cleaning the tweets\nprint(len(data))\ndata['text_len'] = data.apply(lambda row:len(row['text']),axis=1)\ndata = data[data['text_len']!=0]\nprint(len(data))\n\n#Tokenize the data\nmax_fatures = 2000\n#tokenizer = Tokenizer(num_words=max_fatures,split=' ')\ntokenizer = Tokenizer(split=' ')\ntokenizer = Tokenizer(split=' ')\ntokenizer.fit_on_texts(data['text'].values)\nword_index = tokenizer.word_index\nX = tokenizer.texts_to_sequences(data['text'].values)\nprint(len(X[2]))\nX = pad_sequences(X)\nprint(len(X[2]))\n\n#Split into training and test\nY = pd.get_dummies(data['sentiment']).values\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.33, random_state = 42)\n#print(X_train.shape,Y_train.shape)\n#print(X_test.shape,Y_test.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#prepare the data for Bert\n\ntext_bert = list(data['text'])#-------------------->List of sentences\nsenti_bert = list(data['sentiment'])#-------------->List of Labels\nsenti_dict = {'neu':0,'p':1,'vp':2,'n':3,'vn':4}#-->Dictionary to convert labels to integers\nf = lambda st:senti_dict[st]#---------------------->Function to assign label to the integer\nsenti_bert = [f(t) for t in senti_bert]#----------->Converting labels to integers\n# add special tokens for BERT to work properly\ntext_bert = [\"[CLS] \" + t + \" [SEP]\" for t in text_bert]#-->Preparing sentences to be consumed by BERT\n\n\n# Tokenize with BERT tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)#---->Creating a Tokenizer\ntokenized_texts = [tokenizer.tokenize(sent) for sent in text_bert]#-------------------->Tokenizing the sentences\nprint (\"Tokenize the first sentence:\")\nprint (tokenized_texts[0])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set the maximum sequence length. \nMAX_LEN = 128#----------------------------->Is this the Max length of the sentences in words?\n# Pad our input tokens\n#Each word is converted into an integer and padding is applied\ninput_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")#------->Each word is converted into an integer and padding is applied\n# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n#This is just a repeat of the above code.\ninput_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\ninput_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create attention masks\nattention_masks = []#----------------------------------------->We need to understand what attention masks do.\n# Create a mask of 1s for each token followed by 0s for padding\nfor seq in input_ids:\n  seq_mask = [float(i>0) for i in seq]#----------------------->Creates a list of True/False values True everywhere except for the padding\n  attention_masks.append(seq_mask)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use train_test_split to split our data into train and validation sets for training\n#This is self explanatory\ntrain_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, senti_bert, \n                                                            random_state=2018, test_size=0.1)\ntrain_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n                                             random_state=2018, test_size=0.1)\n                                             \n# Convert all of our data into torch tensors, the required datatype for our model\ntrain_inputs = torch.tensor(train_inputs)#------------------------------------------------->What are torch tensors?\nvalidation_inputs = torch.tensor(validation_inputs)\ntrain_labels = torch.tensor(train_labels)\nvalidation_labels = torch.tensor(validation_labels)\ntrain_masks = torch.tensor(train_masks)\nvalidation_masks = torch.tensor(validation_masks)\n\n# Select a batch size for training. \nbatch_size = 32#---------------------------------------------------------------------------->Need to understand what batch size is.\n\n# Create an iterator of our data with torch DataLoader#------------------------------------->What is an iterator here? \ntrain_data = TensorDataset(train_inputs, train_masks, train_labels)#------------------------>What does TensorDataset do?\ntrain_sampler = RandomSampler(train_data)#-------------------------------------------------->What does RandomSampler do?\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)#---->What is a DataLoader?\nvalidation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\nvalidation_sampler = SequentialSampler(validation_data)#------------------------------------>What is a SequentialSampler?\nvalidation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top. \n\nmodel = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",num_labels=5)#--->Declaring the Model\nmodel.cuda()#------------------------------------------------------------------------------>What does .cuda() do?","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# BERT fine-tuning parameters\nparam_optimizer = list(model.named_parameters())\nno_decay = ['bias', 'gamma', 'beta']\noptimizer_grouped_parameters = [\n    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n     'weight_decay_rate': 0.01},\n    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n     'weight_decay_rate': 0.0}\n]\n\noptimizer = BertAdam(optimizer_grouped_parameters,\n                     lr=2e-5,\n                     warmup=.1)\n\n# Function to calculate the accuracy of our predictions vs labels\ndef flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n  \n# Store our loss and accuracy for plotting\ntrain_loss_set = []\n# Number of training epochs \nepochs = 4\n\n# BERT training loop\nfor _ in trange(epochs, desc=\"Epoch\"):  \n  \n  ## TRAINING\n  \n  # Set our model to training mode\n  model.train()  \n  # Tracking variables\n  tr_loss = 0\n  nb_tr_examples, nb_tr_steps = 0, 0\n  # Train the data for one epoch\n  for step, batch in enumerate(train_dataloader):\n    # Add batch to GPU\n    batch = tuple(t.to(device) for t in batch)\n    # Unpack the inputs from our dataloader\n    b_input_ids, b_input_mask, b_labels = batch\n    # Clear out the gradients (by default they accumulate)\n    optimizer.zero_grad()\n    # Forward pass\n    loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n    train_loss_set.append(loss.item())    \n    # Backward pass\n    loss.backward()\n    # Update parameters and take a step using the computed gradient\n    optimizer.step()\n    # Update tracking variables\n    tr_loss += loss.item()\n    nb_tr_examples += b_input_ids.size(0)\n    nb_tr_steps += 1\n  print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n      \n  ## VALIDATION\n\n  # Put model in evaluation mode\n  model.eval()\n  # Tracking variables \n  eval_loss, eval_accuracy = 0, 0\n  nb_eval_steps, nb_eval_examples = 0, 0\n  # Evaluate data for one epoch\n  for batch in validation_dataloader:\n    # Add batch to GPU\n    batch = tuple(t.to(device) for t in batch)\n    # Unpack the inputs from our dataloader\n    b_input_ids, b_input_mask, b_labels = batch\n    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n    with torch.no_grad():\n      # Forward pass, calculate logit predictions\n      logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)    \n    # Move logits and labels to CPU\n    logits = logits.detach().cpu().numpy()\n    label_ids = b_labels.to('cpu').numpy()\n    tmp_eval_accuracy = flat_accuracy(logits, label_ids)    \n    eval_accuracy += tmp_eval_accuracy\n    nb_eval_steps += 1\n  print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n\n# plot training performance\nplt.figure(figsize=(15,8))\nplt.title(\"Training loss\")\nplt.xlabel(\"Batch\")\nplt.ylabel(\"Loss\")\nplt.plot(train_loss_set)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_bert = list(data['text'])#-------------------->List of sentences\nsenti_bert = list(data['sentiment'])#-------------->List of Labels\nsenti_dict = {'neu':0,'p':1,'vp':2,'n':3,'vn':4}#-->Dictionary to convert labels to integers\nf = lambda st:senti_dict[st]#---------------------->Function to assign label to the integer\nsenti_bert = [f(t) for t in senti_bert]#----------->Converting labels to integers\n\n\n# load test data\nsentences = [\"[CLS] \" + query + \" [SEP]\" for query in text_bert]\nlabels = senti_bert\n\n# tokenize test data\ntokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\nMAX_LEN = 128\n# Pad our input tokens\ninput_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\ninput_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\ninput_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n# Create attention masks\nattention_masks = []\n# Create a mask of 1s for each token followed by 0s for padding\nfor seq in input_ids:\n  seq_mask = [float(i>0) for i in seq]\n  attention_masks.append(seq_mask) \n\n# create test tensors\nprediction_inputs = torch.tensor(input_ids)\nprediction_masks = torch.tensor(attention_masks)\nprediction_labels = torch.tensor(labels)\nbatch_size = 32  \nprediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\nprediction_sampler = SequentialSampler(prediction_data)\nprediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n\n\n## Prediction on test set\n# Put model in evaluation mode\nmodel.eval()\n# Tracking variables \npredictions , true_labels = [], []\n# Predict \nfor batch in prediction_dataloader:\n  # Add batch to GPU\n  batch = tuple(t.to(device) for t in batch)\n  # Unpack the inputs from our dataloader\n  b_input_ids, b_input_mask, b_labels = batch\n  # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n  with torch.no_grad():\n    # Forward pass, calculate logit predictions\n    logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n  # Move logits and labels to CPU\n  logits = logits.detach().cpu().numpy()\n  label_ids = b_labels.to('cpu').numpy()  \n  # Store predictions and true labels\n  predictions.append(logits)\n  true_labels.append(label_ids)\n  \n# Import and evaluate each test batch using Matthew's correlation coefficient\nfrom sklearn.metrics import matthews_corrcoef\nmatthews_set = []\nfor i in range(len(true_labels)):\n  matthews = matthews_corrcoef(true_labels[i],\n                 np.argmax(predictions[i], axis=1).flatten())\n  matthews_set.append(matthews)\n  \n# Flatten the predictions and true values for aggregate Matthew's evaluation on the whole dataset\nflat_predictions = [item for sublist in predictions for item in sublist]\nflat_predictions = np.argmax(flat_predictions, axis=1).flatten()\nflat_true_labels = [item for sublist in true_labels for item in sublist]\n\nprint('Classification accuracy using BERT Fine Tuning: {0:0.2%}'.format(matthews_corrcoef(flat_true_labels, flat_predictions)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for batch in prediction_dataloader:\n    print(len(batch[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Peparing the Embedding Matrix","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def int_to_word(word_index,i):\n    index_word_items=word_index.items()\n    j=[item for item in index_word_items if item[1]==i]\n    return j[0][0]\n\ndef get_emb_mat(data,l_input_vec):\n    tokenizer = Tokenizer(split=' ')\n    tokenizer.fit_on_texts(data['text'].values)\n    word_index = tokenizer.word_index#---------------------------------------->Needed for embedding matrix\n    sentences = [word_tokenize(text) for text in list(data['text'])]\n    #Get a list of tokenized sentences\n    w2vmodel = Word2Vec(sentences, min_count=1, size=l_input_vec)#------------->Needs tokenized sentences\n    vocab = w2vmodel.wv.vocab.keys()\n    vocab_dict = w2vmodel.wv.vocab\n    wordsInVocab = len(vocab)\n\n    nb_words = len(word_index)+1\n\n    embedding_matrix = np.zeros((nb_words, l_input_vec))#----------------------->Needs word_index and Word2Vec model\n    for word, i in word_index.items():\n        if word in vocab:\n            embedding_matrix[i] = w2vmodel.wv.word_vec(word)\n    print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n    return embedding_matrix,vocab_dict,w2vmodel\n\ndef train_nn(model,X_train,Y_train,batch_size,epochs):\n    #batch_size = 34\n    #batch_size = 20\n    model.fit(X_train, Y_train, epochs = 12, batch_size=batch_size, verbose = 2)\n    return model\n\n\ndef get_emb_word_vectors(word_index,model,l_input_vec,i):\n    try:\n        word = int_to_word(word_index,i)\n        vec = model[word]\n        return vec\n    except:\n        return [0 for i in range(l_input_vec)]\n\n#w2vmodel = get_emb_mat(data,100)[2]\n#get_emb_word_vectors(word_index,w2vmodel,100,1)\n\ndef get_emb_sentence_vectors(word_index,model,l_input_vec,X):\n    g = lambda l:[get_emb_word_vectors(word_index,model,l_input_vec,i) for i in l]\n    X_emb = [g(k) for k in X]\n    return X_emb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Parameters","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"l_input_vec = 100\nlstm_out = 196\nbatch_size = 20\nepochs = 12\nspatial_drp = 0.4\ndrp = 0.2\nrec_drp = 0.2\ndense_out = np.shape(Y)[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape[1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Declaring the LSTM Model and training it.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix = get_emb_mat(data,l_input_vec)[0]\nembed_dim = l_input_vec\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(len(embedding_matrix), embed_dim ,weights=[embedding_matrix],input_length = 34,trainable=False))\nmodel.add(SpatialDropout1D(spatial_drp))\nmodel.add(LSTM(lstm_out, dropout=drp, recurrent_dropout=rec_drp))\nmodel.add(Dense(dense_out,activation='softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\nprint(model.summary())\ntrain_nn(model,X_train,Y_train,batch_size,epochs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"    \ndef sortSecond(val): \n    return val[1] \n\ndef get_ngrams(doc,n):\n    tokens = doc.split()\n    all_ngrams = ngrams(tokens, n)\n    return all_ngrams\n\ndef get_everygrams(doc):\n    tokens = doc.split()\n    every_grams = list(everygrams(tokens))\n    every_grams = [' '.join(k) for k in every_grams]\n    return every_grams\n\n\ndef lstm_sent(text,sent):\n    twt = [text]\n    #vectorizing the tweet by the pre-fitted tokenizer instance\n    twt = tokenizer.texts_to_sequences(twt)\n    #padding the tweet to have exactly the same shape as `embedding_2` input\n    twt = pad_sequences(twt, maxlen=34, dtype='int32', value=0)\n    #print(twt)\n    sentiment_n = model.predict(twt,batch_size=1,verbose = 2)\n    #sentiment = translate(flatten(list(sentiment_n[0])))\n    sentiment = translate(list(sentiment_n[0]),sent)\n    return sentiment,sentiment_n\n\n    \ndef vader_sent(sentence,a):\n    sid_obj = SentimentIntensityAnalyzer()\n    sentiment_dict = sid_obj.polarity_scores(sentence)\n    if sentiment_dict['compound'] >= a :\n        return \"positive\" \n    elif sentiment_dict['compound'] <= a*(-1) :\n        return \"negative\"\n    else :\n        return \"neutral\"\n    \ndef vader_sent_num(sentence):\n    sid_obj = SentimentIntensityAnalyzer()\n    sentiment_dict = sid_obj.polarity_scores(sentence)\n    sent_num = sentiment_dict['compound']\n    return sent_num\n\ndef get_sig_ngram(sentence,sentiment,perc):\n    n = int(math.ceil(len(sentence.split())*perc))\n    g = lambda sentiment:max if sentiment =='positive' else min\n    #sentence = custom_spell(str(sentence))\n    ngrams = get_ngrams(sentence,n)\n    sent_dict = [(' '.join(k),vader_sent_num(' '.join(k))) for k in ngrams]\n    prominent_sent = g(sentiment)([v[1] for v in sent_dict])\n    prom_ngrams = [ntup for ntup in sent_dict if ntup[1]==prominent_sent]\n    return prom_ngrams[0][0]\n\ndef get_sig_ngram_inv(sentence,sentiment,perc):\n    n = int(math.ceil(len(sentence.split())*perc))\n    g = lambda sentiment:max if sentiment =='positive' else min\n    #sentence = custom_spell(str(sentence))\n    ngrams = get_ngrams(sentence,n)\n    sent_dict = [(' '.join(k),vader_sent_num(' '.join(k))) for k in ngrams]\n    prominent_sent = g(sentiment)([v[1] for v in sent_dict])\n    prom_ngrams = [ntup for ntup in sent_dict if ntup[1]==prominent_sent]\n    return sent_dict\n\n\n\ndef jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv')\ndf_test = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#These functions translate the ratings from vectors to the original categories\ndef flatten(l):\n    m = max(l)\n    g = lambda i,m: 1 if i>=m else 0\n    l = list(map(g,l,[m]*len(l)))\n    return l\ndef translate(l,sent):\n    s_categories = ['n','neu','p','vn','vp']\n    g = lambda l: l[s_categories.index(sent)]\n    #sentiment = s_categories[g(l)]\n    sentiment = g(l)\n    return sentiment\n\ndef get_jaccard(df,rows):\n    \n    df1=df.sample(rows)\n    #print(df1)\n    g = lambda senti:'vp' if senti=='positive' else 'vn'\n    df1['algo_text'] = df1.apply(lambda row:str(row['text']) if row['sentiment']=='neutral' else get_f_text(str(row['text']),g(row['sentiment'])),axis=1)\n    df1['jaccard'] = df1.apply(lambda row:jaccard(str(row['selected_text']),str(row['algo_text'])),axis=1)\n    score = np.average(list(df1['jaccard']))\n    return score,df1\ndef get_jaccard_thresh(df,rows,perc,thresh):\n    df1=df.sample(rows)\n    \n    df1['algo_text'] = df1.apply(lambda row:row['text'] if row['sentiment']=='neutral' else get_sig_ngram(row['text'],row['sentiment'],perc),axis=1)\n    df1['jaccard'] = df1.apply(lambda row:jaccard(str(row['selected_text']),str(row['algo_text'])),axis=1)\n    score = np.average(list(df1['jaccard']))\n    df2 = df1[df1['jaccard']<thresh] \n    return score,df2\n\ndef get_sel_text(text,sent):\n    f = lambda x: re.sub('[^a-zA-z0-9\\s]',' ',x)\n    #g = lambda x: str(x).lower()\n    #text = g(text)\n    text = f(text)\n    e = get_everygrams(text)\n    lis = [(i,lstm_sent(i,sent)[0]) for i in e]\n    #lis = [i for i in lis if i[1]==sent]\n    return lis\n\ndef get_f_text(text,sent):\n    t_list = get_sel_text(text,sent)\n    m = max([t[1] for t in t_list])\n    f = [t for t in t_list if t[1]>=m]\n    return f[0][0]\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score,df1 = get_jaccard(df_train,1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 = df1[df1['sentiment']!='neutral']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.sort_values('jaccard').tail(30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}