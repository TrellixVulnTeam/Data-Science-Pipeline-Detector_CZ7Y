{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-10T23:22:19.434541Z","iopub.execute_input":"2021-10-10T23:22:19.434818Z","iopub.status.idle":"2021-10-10T23:22:19.442984Z","shell.execute_reply.started":"2021-10-10T23:22:19.434774Z","shell.execute_reply":"2021-10-10T23:22:19.441881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Step1: Let us start by taking input of the training set and exploring it. Like the null values and the distribution.","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/tweet-sentiment-extraction/train.csv\")\ntrain_df.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-10-10T23:40:41.125658Z","iopub.execute_input":"2021-10-10T23:40:41.125986Z","iopub.status.idle":"2021-10-10T23:40:41.222351Z","shell.execute_reply.started":"2021-10-10T23:40:41.125949Z","shell.execute_reply":"2021-10-10T23:40:41.221229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here text is the full length tweet, sentiment is the classification and selected_text is the text that leads to the given sentiment.","metadata":{}},{"cell_type":"code","source":"train_df.info()","metadata":{"execution":{"iopub.status.busy":"2021-10-10T23:22:24.127052Z","iopub.execute_input":"2021-10-10T23:22:24.127349Z","iopub.status.idle":"2021-10-10T23:22:24.156138Z","shell.execute_reply.started":"2021-10-10T23:22:24.127318Z","shell.execute_reply":"2021-10-10T23:22:24.154498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Step2: It can be seen that there is a row with null values, since it is a single row, it can be deleted.","metadata":{}},{"cell_type":"code","source":"train_df.dropna(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-10-10T23:40:44.99736Z","iopub.execute_input":"2021-10-10T23:40:44.997668Z","iopub.status.idle":"2021-10-10T23:40:45.022582Z","shell.execute_reply.started":"2021-10-10T23:40:44.997637Z","shell.execute_reply":"2021-10-10T23:40:45.021433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Step3: Now to find the relation between the text and the selected_text, we can compute the jacquard similarity between the 2 texts. It is nothing but the size of intersection divided by the size of union of the 2 texts.","metadata":{}},{"cell_type":"code","source":"def jacquard_f(text1, text2):\n    text1 = set(text1.lower().split())\n    text2 = set(text2.lower().split())\n    inter = text1.intersection(text2)\n    return len(inter)/(len(text1) + len(text2) - len(inter))\n    ","metadata":{"execution":{"iopub.status.busy":"2021-10-10T23:39:50.861668Z","iopub.execute_input":"2021-10-10T23:39:50.86202Z","iopub.status.idle":"2021-10-10T23:39:50.869368Z","shell.execute_reply.started":"2021-10-10T23:39:50.861981Z","shell.execute_reply":"2021-10-10T23:39:50.868224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"jacquard_values = [] \nfor ind, row in train_df.iterrows():\n    s1 = row.text\n    s2 = row.selected_text\n    jacquard_values.append([s1, s2, jacquard_f(s1, s2)])\njacquard = pd.DataFrame(jacquard_values, columns=[\"text\",\"selected_text\",\"jac\"])\ntrain_df = train_df.merge(jacquard, how=\"outer\",on=\"text\")\ntrain_df.head(3)","metadata":{"execution":{"iopub.status.busy":"2021-10-10T23:40:47.474096Z","iopub.execute_input":"2021-10-10T23:40:47.475165Z","iopub.status.idle":"2021-10-10T23:40:49.898187Z","shell.execute_reply.started":"2021-10-10T23:40:47.475093Z","shell.execute_reply":"2021-10-10T23:40:49.897055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Step4: Let us plot the jacqauard similarity for the different categories of classification i.e. neutral and positive. We use the kde plot, that is the kernel distribution estimate plot. It is similar to histogram.\n","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\np1=sns.kdeplot(train_df[train_df['sentiment']=='positive']['jac'], shade=True, color=\"r\")\np2=sns.kdeplot(train_df[train_df['sentiment']=='negative']['jac'], shade=True, color=\"b\")\np3=sns.kdeplot(train_df[train_df['sentiment']=='neutral']['jac'], shade=True, color=\"g\")","metadata":{"execution":{"iopub.status.busy":"2021-10-10T23:40:52.110218Z","iopub.execute_input":"2021-10-10T23:40:52.110514Z","iopub.status.idle":"2021-10-10T23:40:52.628279Z","shell.execute_reply.started":"2021-10-10T23:40:52.110485Z","shell.execute_reply":"2021-10-10T23:40:52.627167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It can be seen that there are peaks towards 1, i.e. high similarity between text and selected text. We can device a method working on these texts that have high similarity, as selected text would be similar to given text. One particular case would be when there are less number of words in text, there is high chances that selected text would be same as text.","metadata":{}},{"cell_type":"markdown","source":"Step4: To work on texts with less words, we need a column that tells the number of words used.","metadata":{}},{"cell_type":"code","source":"train_df['num_words_text']= train_df['text'].apply(lambda x: len(str(x).split()))","metadata":{"execution":{"iopub.status.busy":"2021-10-10T23:40:59.395506Z","iopub.execute_input":"2021-10-10T23:40:59.395874Z","iopub.status.idle":"2021-10-10T23:40:59.454503Z","shell.execute_reply.started":"2021-10-10T23:40:59.395841Z","shell.execute_reply":"2021-10-10T23:40:59.453579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Step5: Let us see the jacquard similarity for texts with few words","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"less_three = train_df[train_df['num_words_text']<=2]\nless_three.groupby('sentiment').mean()['jac']\nless_three.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-10-10T23:41:00.674706Z","iopub.execute_input":"2021-10-10T23:41:00.675849Z","iopub.status.idle":"2021-10-10T23:41:00.694716Z","shell.execute_reply.started":"2021-10-10T23:41:00.675808Z","shell.execute_reply":"2021-10-10T23:41:00.694039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It can be seen that text and selected_text is the same. Now lets see what about tweets with more than 3 words. Let us clean the text, it is very important to get good models. Junk data will reduce efficiency of the models.","metadata":{}},{"cell_type":"code","source":"from nltk.corpus import stopwords\nstopword = stopwords.words('english')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\n#\nimport string\ndef clean_text(text):\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    text = text.split()\n    words = [t for t in text if t not in stopword]\n    return words\ntrain_df['list_words'] = train_df['text'].apply(lambda x:clean_text(x))\ntrain_df['list_words_selected'] = train_df['selected_text_x'].apply(lambda x:clean_text(x))","metadata":{"execution":{"iopub.status.busy":"2021-10-10T23:44:58.769911Z","iopub.execute_input":"2021-10-10T23:44:58.770191Z","iopub.status.idle":"2021-10-10T23:45:01.379503Z","shell.execute_reply.started":"2021-10-10T23:44:58.770162Z","shell.execute_reply":"2021-10-10T23:45:01.378408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2021-10-10T23:40:10.476442Z","iopub.execute_input":"2021-10-10T23:40:10.476705Z","iopub.status.idle":"2021-10-10T23:40:10.483006Z","shell.execute_reply.started":"2021-10-10T23:40:10.476675Z","shell.execute_reply":"2021-10-10T23:40:10.481677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['list_words'].head(3)","metadata":{"execution":{"iopub.status.busy":"2021-10-10T23:45:11.48933Z","iopub.execute_input":"2021-10-10T23:45:11.489853Z","iopub.status.idle":"2021-10-10T23:45:11.499275Z","shell.execute_reply.started":"2021-10-10T23:45:11.489775Z","shell.execute_reply":"2021-10-10T23:45:11.498314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Step6: Let us obtain the most common words in the selected  text and text, as they might play greater role in the process","metadata":{}},{"cell_type":"code","source":"from collections import Counter\n#train_df['list_words']=train_df['selected_text_x'].apply(lambda x:str(x).split())\ntop_words_text = Counter([item for sublist in train_df['list_words'] for item in sublist])\n#train_df['list_words_text']=train_df['text'].apply(lambda x:str(x).split())\ntop_words_selected_text = Counter([item for sublist in train_df['list_words_selected'] for item in sublist])","metadata":{"execution":{"iopub.status.busy":"2021-10-10T23:57:14.121247Z","iopub.execute_input":"2021-10-10T23:57:14.121665Z","iopub.status.idle":"2021-10-10T23:57:14.230264Z","shell.execute_reply.started":"2021-10-10T23:57:14.121631Z","shell.execute_reply":"2021-10-10T23:57:14.229224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Step7: now let us see the common words for each caetegory i.e. positive and negative","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"positives = train_df[train_df['sentiment']=='positive']\nnegatives = train_df[train_df['sentiment']=='negative']\nneutrals = train_df[train_df['sentiment']=='neutral']","metadata":{"execution":{"iopub.status.busy":"2021-10-10T23:57:16.456119Z","iopub.execute_input":"2021-10-10T23:57:16.456441Z","iopub.status.idle":"2021-10-10T23:57:16.498957Z","shell.execute_reply.started":"2021-10-10T23:57:16.45641Z","shell.execute_reply":"2021-10-10T23:57:16.497917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top = Counter([item for sublist in positives['list_words'] for item in sublist])\ntemp_positive = pd.DataFrame(top.most_common(20))\ntemp_positive.columns = ['Common_words','count']\ntemp_positive","metadata":{"execution":{"iopub.status.busy":"2021-10-10T23:57:26.344829Z","iopub.execute_input":"2021-10-10T23:57:26.345111Z","iopub.status.idle":"2021-10-10T23:57:26.395112Z","shell.execute_reply.started":"2021-10-10T23:57:26.345084Z","shell.execute_reply":"2021-10-10T23:57:26.394015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top = Counter([item for sublist in negatives['list_words'] for item in sublist])\ntemp_negative = pd.DataFrame(top.most_common(20))\ntemp_negative.columns = ['Common_words','count']\ntemp_negative","metadata":{"execution":{"iopub.status.busy":"2021-10-10T23:45:50.661629Z","iopub.execute_input":"2021-10-10T23:45:50.662388Z","iopub.status.idle":"2021-10-10T23:45:50.702893Z","shell.execute_reply.started":"2021-10-10T23:45:50.662343Z","shell.execute_reply":"2021-10-10T23:45:50.701729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top = Counter([item for sublist in neutrals['list_words'] for item in sublist])\ntemp_neutral = pd.DataFrame(top.most_common(20))\ntemp_neutral.columns = ['Common_words','count']\ntemp_neutral","metadata":{"execution":{"iopub.status.busy":"2021-10-10T23:46:17.84866Z","iopub.execute_input":"2021-10-10T23:46:17.849074Z","iopub.status.idle":"2021-10-10T23:46:17.894597Z","shell.execute_reply.started":"2021-10-10T23:46:17.849036Z","shell.execute_reply":"2021-10-10T23:46:17.893331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.express as px\nfig = px.treemap(temp_positive, path=['Common_words'], values='count',title='Common Postive Words')\nfig.show()\nfig = px.treemap(temp_negative, path=['Common_words'], values='count',title='Common Negative Words')\nfig.show()\nfig = px.treemap(temp_neutral, path=['Common_words'], values='count',title='Common Neutral Words')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-10T23:49:53.802145Z","iopub.execute_input":"2021-10-10T23:49:53.802416Z","iopub.status.idle":"2021-10-10T23:49:54.028285Z","shell.execute_reply.started":"2021-10-10T23:49:53.80239Z","shell.execute_reply":"2021-10-10T23:49:54.027254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"However, it can be seen that, some common words are across all the categories, it would be more meaningful if we could have words that are specific to certain categories, so that they can strongly determine the sentiment","metadata":{}},{"cell_type":"code","source":"def get_unique_words(sentiment,numwords,raw_words):\n    #Get unique words belonging to categories other than given sentiment\n    other_words = []\n    for item in train_df[train_df.sentiment != sentiment]['list_words']:\n        for word in item:\n            other_words.append(word)\n    other_words= list(set(other_words))\n    category_words = [x for x in raw_words if x not in other_words]\n    newcounter = Counter()\n    for item in train_df[train_df.sentiment == sentiment]['list_words']:\n        for word in item:\n            newcounter[word] += 1\n    keep = list(category_words)\n    for word in list(newcounter):\n        if word not in keep:\n            del newcounter[word]\n    unique_words = pd.DataFrame(newcounter.most_common(numwords), columns = ['words','count'])\n    return unique_words","metadata":{"execution":{"iopub.status.busy":"2021-10-11T00:06:12.112052Z","iopub.execute_input":"2021-10-11T00:06:12.112412Z","iopub.status.idle":"2021-10-11T00:06:12.122264Z","shell.execute_reply.started":"2021-10-11T00:06:12.112377Z","shell.execute_reply":"2021-10-11T00:06:12.121189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"raw_text = [word for word_list in train_df['list_words'] for word in word_list]\nunique_positive= get_unique_words('positive', 20, raw_text)\nunique_negative= get_unique_words('negative', 20, raw_text)\nunique_neutral= get_unique_words('neutral', 20, raw_text)\nunique_positive","metadata":{"execution":{"iopub.status.busy":"2021-10-11T00:18:05.989513Z","iopub.execute_input":"2021-10-11T00:18:05.989944Z","iopub.status.idle":"2021-10-11T00:20:43.320631Z","shell.execute_reply.started":"2021-10-11T00:18:05.989895Z","shell.execute_reply":"2021-10-11T00:20:43.319796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Step8: Given these strong words, let us train models for the given task. First let us try NER i.e. Names Entity recognition. Let us begin by creating a model for positive sentiments. Our data must be converted to entities in order to model it as a NER problem. In particular, we consider only those data for which number of words>3. For the rest, the selected text can be considered as text.","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv')\ntest_df = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv')\ntrain_df['text_length'] = train_df['text'].apply(lambda x:len(str(x).split())) \ntrain_df = train_df[train_df['text_length']>=3]","metadata":{"execution":{"iopub.status.busy":"2021-10-11T00:24:17.70898Z","iopub.execute_input":"2021-10-11T00:24:17.709523Z","iopub.status.idle":"2021-10-11T00:24:17.883749Z","shell.execute_reply.started":"2021-10-11T00:24:17.709476Z","shell.execute_reply":"2021-10-11T00:24:17.88235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def format_data(sentiment):\n    formatted_data = []\n    for index, row in train_df.iterrows():\n        if row.sentiment == sentiment:\n            selected_text = row.selected_text\n            text = row.text\n            start = text.find(selected_text)\n            end = start + len(selected_text)\n            formatted_data.append((text, {\"entities\": [[start, end, 'selected_text']]}))\n    return formatted_data","metadata":{"execution":{"iopub.status.busy":"2021-10-11T00:24:42.025046Z","iopub.execute_input":"2021-10-11T00:24:42.025329Z","iopub.status.idle":"2021-10-11T00:24:42.03216Z","shell.execute_reply.started":"2021-10-11T00:24:42.025301Z","shell.execute_reply":"2021-10-11T00:24:42.03146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Step9: We train 2 nlp pipeline that performs NER for positive texts and negative texts","metadata":{}},{"cell_type":"code","source":"def train(train_data, output_path, n_iter=20, model=None):\n    if model is not None:\n        nlp = spacy.load(output_path) \n    else:\n        nlp = spacy.blank(\"en\")\n    \n    if \"ner\" not in nlp.pipe_names:\n        ner = nlp.create_pipe(\"ner\")\n        nlp.add_pipe(ner, last=True)\n    else:\n        ner = nlp.get_pipe(\"ner\")\n    \n    for _, annotations in train_data:\n        for ent in annotations.get(\"entities\"):\n            ner.add_label(ent[2])\n\n    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n    with nlp.disable_pipes(*other_pipes): \n        if model is None:\n            nlp.begin_training()\n        else:\n            nlp.resume_training()\n\n\n        for itn in tqdm(range(n_iter)):\n            random.shuffle(train_data)\n            batches = minibatch(train_data, size=compounding(4.0, 500.0, 1.001))    \n            losses = {}\n            for batch in batches:\n                texts, annotations = zip(*batch)\n                nlp.update(texts,  # batch of texts\n                            annotations,  # batch of annotations\n                            drop=0.5,   # dropout - make it harder to memorise data\n                            losses=losses, \n                            )\n            print(\"Losses\", losses)\n    nlp.meta[\"name\"] = \"st_ner\"\n    nlp.to_disk(output_path)","metadata":{"execution":{"iopub.status.busy":"2021-10-11T00:36:43.738847Z","iopub.execute_input":"2021-10-11T00:36:43.739177Z","iopub.status.idle":"2021-10-11T00:36:43.752521Z","shell.execute_reply.started":"2021-10-11T00:36:43.739146Z","shell.execute_reply":"2021-10-11T00:36:43.751105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import spacy\nfrom tqdm import tqdm\nimport random\nfrom spacy.util import minibatch, compounding\nsentiment = 'positive'\ntrain_data_positive = format_data(sentiment)\ntrain(train_data_positive, 'positive', n_iter=3, model=None)","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentiment = 'negative'\ntrain_data_negative = format_data(sentiment)\ntrain(train_data_negative, 'negative', n_iter=3, model=None)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Step 10: Using the trained model, we predict the named entity selected_text for each text","metadata":{}},{"cell_type":"code","source":"def predict_entities(text, model):\n    doc = model(text)\n    ent_array = []\n    for ent in doc.ents:\n        start = text.find(ent.text)\n        end = start + len(ent.text)\n        new_int = [start, end, ent.label_]\n        if new_int not in ent_array:\n            ent_array.append([start, end, ent.label_])\n    selected_text = text[ent_array[0][0]: ent_array[0][1]] if len(ent_array) > 0 else text\n    return selected_text","metadata":{"execution":{"iopub.status.busy":"2021-10-11T00:45:47.120571Z","iopub.execute_input":"2021-10-11T00:45:47.121449Z","iopub.status.idle":"2021-10-11T00:45:47.129835Z","shell.execute_reply.started":"2021-10-11T00:45:47.1214Z","shell.execute_reply":"2021-10-11T00:45:47.128718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted_selected_text = []\nmodel_pos = spacy.load('positive')\nmodel_neg = spacy.load('negative')\n        \nfor index, row in test_df.iterrows():\n    text = row.text\n    output_str = \"\"\n    if row.sentiment == 'neutral' or len(text.split()) <= 2:\n        predicted_selected_text.append(text)\n    elif row.sentiment == 'positive':\n        predicted_selected_text.append(predict_entities(text, model_pos))\n    else:\n        predicted_selected_text.append(predict_entities(text, model_neg))\n        \ntest_df['selected_text'] = predicted_selected_text","metadata":{"execution":{"iopub.status.busy":"2021-10-11T00:47:10.73557Z","iopub.execute_input":"2021-10-11T00:47:10.735866Z","iopub.status.idle":"2021-10-11T00:47:21.046276Z","shell.execute_reply.started":"2021-10-11T00:47:10.735837Z","shell.execute_reply":"2021-10-11T00:47:21.045226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Step 11: we submit the predicted selected texts!!","metadata":{}},{"cell_type":"code","source":"submission_df = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/sample_submission.csv')\nsubmission_df['selected_text'] = test_df['selected_text']\nsubmission_df.to_csv(\"submission.csv\", index=False)\nsubmission_df.head(10)\n","metadata":{"execution":{"iopub.status.busy":"2021-10-11T00:48:42.133129Z","iopub.execute_input":"2021-10-11T00:48:42.133418Z","iopub.status.idle":"2021-10-11T00:48:42.18389Z","shell.execute_reply.started":"2021-10-11T00:48:42.133389Z","shell.execute_reply":"2021-10-11T00:48:42.182764Z"},"trusted":true},"execution_count":null,"outputs":[]}]}