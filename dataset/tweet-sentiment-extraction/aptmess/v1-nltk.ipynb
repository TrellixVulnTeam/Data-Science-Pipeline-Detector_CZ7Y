{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.sentiment.vader import SentimentIntensityAnalyzer\n\nimport re\nimport nltk\nimport string\nimport operator","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.preprocessing import StandardScaler\nfrom nltk.tokenize import WordPunctTokenizer\nfrom bokeh.io import output_notebook\nfrom gensim.models import Word2Vec\nfrom sklearn.manifold import TSNE\nfrom nltk.corpus import stopwords\nfrom tqdm import tqdm_notebook\nfrom copy import deepcopy\nfrom array import *\n\nimport re\nimport nltk\nimport gensim\nimport string\nimport operator\nimport scipy.io\nimport itertools\nimport numpy as np\nimport pandas as pd\nimport bokeh.models as bm, bokeh.plotting as pl","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/tweet-sentiment-extraction/train.csv')\ntest = pd.read_csv('../input/tweet-sentiment-extraction/test.csv')\ntrain  = train.dropna()\ntest = test.dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def text_prepare(text):\n    #Удалить стопслова.\n    stopWords = set(stopwords.words('english'))\n    for stopWord in stopWords:\n        text = re.sub(r'\\b{}\\b'.format(stopWord), '', text)\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_perm(original):\n    result = []\n    words = original.split(' ')\n    n = len(words)\n    for i in range(n):\n        for j in range(i + 1, n + 1):\n            # Append tuple(original[i:j]) if that's what you are looking for\n            result.append(words[i:j])\n            result[-1] = ' '.join(result[-1])\n    return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['permutations'] = train['text'].apply(get_perm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def jaccard(str1, str2):\n    a = set(str1.lower().split())\n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))\n\ndef choosing_selectedword(df_process,n):\n    train_data = df_process['text']\n    train_data_sentiment = df_process['sentiment']\n    selected_text_processed = []\n    analyser = SentimentIntensityAnalyzer()\n    for j in range(0 , len(train_data)):\n        text = re.sub(r'http\\S+', '', str(train_data.iloc[j]))\n        if(train_data_sentiment.iloc[j] == \"neutral\" or len(text.split()) < n):\n            selected_text_processed.append(str(text))\n        if(train_data_sentiment.iloc[j] == \"positive\" and len(text.split()) >=n):\n            aa = re.split(' ', text)\n            ss_arr = \"\"\n            polar = 0\n            for qa in range(0,len(aa)):\n                score = analyser.polarity_scores(aa[qa])\n                if score['compound'] >polar:\n                    polar = score['compound']\n                    ss_arr = aa[qa]\n            if len(ss_arr) != 0:\n                selected_text_processed.append(ss_arr)   \n            if len(ss_arr) == 0:\n                selected_text_processed.append(text)\n        if(train_data_sentiment.iloc[j] == \"negative\"and len(text.split()) >= n):\n            aa = re.split(' ', text)\n        \n            ss_arr = \"\"\n            polar = 0\n            for qa in range(0,len(aa)):\n                score = analyser.polarity_scores(aa[qa])\n                if score['compound'] < polar:\n                    polar = score['compound']\n                    ss_arr = aa[qa]\n            if len(ss_arr) != 0:\n                selected_text_processed.append(ss_arr)   \n            if len(ss_arr) == 0:\n                selected_text_processed.append(text)  \n    return selected_text_processed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result_train = choosing_selectedword(train,4)\ntrain_selected_data = train['selected_text']\naverage = 0;\nfor i in range(0,len(train_selected_data)):\n    ja_s = jaccard(str(result_train[i]),str(train_selected_data.iloc[i]))\n    average = ja_s+average\nprint('Training Data accuracy')\nprint(average/len(result_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['len_w'] = t","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.where(train['len_w']>0)[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.sentiment2 = train.sentiment.replace(to_replace = pd.unique(train.sentiment),value = [0,1,2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.sentiment2.values[np.where(train['len_w']>0)[0]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[(train.len_w>0)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[(train.len_w>0) &(train.sentiment == 'neutral')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(t)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t = np.array(list(map(len,result_train))) - np.array(list(map(len,train.selected_text)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sizes = 100\ntokenizer = WordPunctTokenizer()\n#traint_tokenized = [tokenizer.tokenize(line.lower()) for line in new_test]\ntraint_tokenized = [tokenizer.tokenize(line.lower()) for line in train.text]\nwv_embeddings = Word2Vec(traint_tokenized, # data for model to train on\n                 size=sizes,         # embedding vector size     \n                         # consider words that occured at least 5 times\n                 window=5, min_count = 3).wv   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result_test = choosing_selectedword(test,4)\nindex = test.textID\nsubmisstion = pd.DataFrame(columns = ['textID','selected_text'], data ={'textID':index,'selected_text':result_test})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def question_to_vec(question, embeddings, dim=300):\n    \"\"\"\n        question: строка\n        embeddings: наше векторное представление\n        dim: размер любого вектора в нашем представлении\n        \n        return: векторное представление для вопроса\n    \"\"\"\n    words = question.split(' ') #your code\n    # убрать знак вопроса, если он есть\n    n_known = 0\n    result = np.array([0] * dim, dtype=float)\n    \n    for word in words:\n        if word in embeddings:\n            result += embeddings[word] #your code\n            n_known += 1\n            \n    if n_known != 0:\n        return result / n_known #your code\n    else:\n        return result\n    \ndef text_prepare(text):\n    \"\"\"\n        text: a string\n        \n        return: modified string\n    \"\"\"\n    # Перевести символы в нижний регистр\n    #text = text.lower() #your code\n    \n    \n    # Заменить символы пунктуации на пробелы\n    #text = re.sub(r'[{}]'.format(string.punctuation), ' ', text)\n    \n    #Удалить \"плохие\" символы\n    #text = re.sub('[^A-Za-z0-9 ]', '', text)\n    \n    ##Удалить стопслова.\n    #stopWords = set(stopwords.words('english'))\n    #for stopWord in stopWords:\n    #    text = re.sub(r'\\b{}\\b'.format(stopWord), '', text)\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"quora_vectors_emb = []\nfor num in train.text:\n    q, *ex = num\n    quora_vectors_emb.append(question_to_vec(q, wv_embeddings,100)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"quora_tokenized = train.text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import operator\ndef find_closest_questions(question, k=5):\n    \"\"\"\n    function that finds closest questions from dataset given question\n    args:\n        question: question, preprocessed using text_prepare \n        k: how many nearest questions to find\n    \"\"\"\n\n    vec_question = question_to_vec(question,wv_embeddings,100).reshape(1,-1)\n    dist_s = cosine_similarity(quora_vectors_emb, vec_question)[:,0]\n    sort_dist_s = sorted(dist_s)[::-1][:k]\n    sorted_questions = deepcopy(np.array(quora_tokenized)[dist_s.argsort()[::-1]])[:k]\n    sort_dict = dict(zip(sorted_questions,sort_dist_s))\n    sorted_d = sorted(sort_dict.items(), key=operator.itemgetter(1),reverse = True)\n    return sorted_d","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"traintext = result_train\nnew_test = deepcopy(traintext)\nfor i in tqdm_notebook(range(len(traintext))):\n    new_test[i] = text_prepare(traintext[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_selected_data = train['selected_text']\naverage = 0;\nfor i in range(0,len(train_selected_data)):\n    ja_s = jaccard(str(new_test[i]),str(train_selected_data.iloc[i]))\n    average = ja_s+average\nprint('Training Data accuracy')\nprint(average/len(new_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"find_closest_questions(result_train[0],k=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submisstion.to_csv('submission.csv', index = False)\nprint('done')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}