{"cells":[{"metadata":{},"cell_type":"markdown","source":"# TensorFlow roBERTa + CNN head - LB   v2"},{"metadata":{},"cell_type":"markdown","source":"Hello everyone! \n\n1. 1. 1. This kernel is based on [Al-Kharba Kiram](https://www.kaggle.com/al0kharba/tensorflow-roberta-0-712/output).  \n\n "},{"metadata":{},"cell_type":"markdown","source":"# Load  data and libraries"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import pandas as pd, numpy as np\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom sklearn.model_selection import StratifiedKFold\nfrom transformers import *\nimport tokenizers\nprint('TF version',tf.__version__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_train():\n    train=pd.read_csv('../input/tweet-sentiment-extraction/train.csv')\n    train['text']=train['text'].astype(str)\n    train['selected_text']=train['selected_text'].astype(str)\n    return train\n\ndef read_test():\n    test=pd.read_csv('../input/tweet-sentiment-extraction/test.csv')\n    test['text']=test['text'].astype(str)\n    return test\n\ndef read_submission():\n    test=pd.read_csv('../input/tweet-sentiment-extraction/sample_submission.csv')\n    return test\n    \ntrain_df = read_train()\ntest_df = read_test()\nsubmission_df = read_submission()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def jaccard_improve(str1, str2): \n    str1=str1.lower()\n    str2=str2.lower()    \n    index=str1.find(str2) \n    text1=str1[:index]\n    #print(text1)\n    text2=str1[index:].replace(str2,'')\n    words1=text1.split()\n    words2=text2.split()\n    #print(words1[-3:])\n\n    if len(words1)>len(words2):\n        words1=words1[-3:]\n        mod_text=\" \".join(words1)+\" \"+ str2\n    else:\n        words2=words2[0:2]\n        mod_text=str2+\" \"+\" \".join(words2)\n    return mod_text ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def jaccard(str1, str2): \n    a = set(str(str1).lower().split())  \n    b = set(str(str2).lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(train_df))\n#train_df1=train_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['selected_text_mod']=train_df['selected_text']\ntrain_df['mod']=0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data preproccesing"},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN = 96\nPATH = '../input/tf-roberta/'\ntokenizer = tokenizers.ByteLevelBPETokenizer(\n    vocab_file=PATH+'vocab-roberta-base.json', \n    merges_file=PATH+'merges-roberta-base.txt', \n    lowercase=True,\n    add_prefix_space=True\n)\nsentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ct = train_df.shape[0]\ninput_ids = np.ones((ct,MAX_LEN),dtype='int32')\nattention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\ntoken_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\nstart_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\nend_tokens = np.zeros((ct,MAX_LEN),dtype='int32')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(input_ids.shape)\nprint(attention_mask.shape)\nprint(token_type_ids.shape)\nprint(start_tokens.shape)\nprint(end_tokens.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ct = train_df.shape[0]\ninput_ids = np.ones((ct,MAX_LEN),dtype='int32')\nattention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\ntoken_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\nstart_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\nend_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n\nfor k in range(train_df.shape[0]):\n    \n    # FIND OVERLAP\n    text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n    text2 = \" \".join(train_df.loc[k,'selected_text'].split())\n    idx = text1.find(text2)\n    chars = np.zeros((len(text1)))\n    chars[idx:idx+len(text2)]=1\n    if text1[idx-1]==' ': chars[idx-1] = 1 \n    enc = tokenizer.encode(text1) \n        \n    # ID_OFFSETS\n    offsets = []; idx=0\n    for t in enc.ids:\n        w = tokenizer.decode([t])\n        offsets.append((idx,idx+len(w)))\n        idx += len(w)\n    \n    # START END TOKENS\n    toks = []\n    for i,(a,b) in enumerate(offsets):\n        sm = np.sum(chars[a:b])\n        if sm>0: toks.append(i) \n        \n    s_tok = sentiment_id[train_df.loc[k,'sentiment']]\n    input_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n    attention_mask[k,:len(enc.ids)+5] = 1\n    if len(toks)>0:\n        start_tokens[k,toks[0]+1] = 1\n        end_tokens[k,toks[-1]+1] = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ct = test_df.shape[0]\ninput_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\nattention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\ntoken_type_ids_t = np.zeros((ct,MAX_LEN),dtype='int32')\n\nfor k in range(test_df.shape[0]):\n        \n    # INPUT_IDS\n    text1 = \" \"+\" \".join(test_df.loc[k,'text'].split())\n    enc = tokenizer.encode(text1)                \n    s_tok = sentiment_id[test_df.loc[k,'sentiment']]\n    input_ids_t[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n    attention_mask_t[k,:len(enc.ids)+5] = 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"def scheduler(epoch):\n    return 3e-5 * 0.2**epoch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model():\n    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n\n    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n    x = bert_model(ids,attention_mask=att,token_type_ids=tok)\n    \n    \n    x1 = tf.keras.layers.Dropout(0.1)(x[0]) \n    x1 = tf.keras.layers.Conv1D(128, 2,padding='same')(x1)\n    x1 = tf.keras.layers.LeakyReLU()(x1)\n    #x1 = tf.keras.layers.ReLU()(x1)\n    x1 = tf.keras.layers.Conv1D(64, 2,padding='same')(x1)\n    x1 = tf.keras.layers.Dense(1)(x1)\n    x1 = tf.keras.layers.Flatten()(x1)\n    x1 = tf.keras.layers.Activation('softmax')(x1)\n    \n    x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n    x2 = tf.keras.layers.Conv1D(128, 2, padding='same')(x2)\n    x2 = tf.keras.layers.LeakyReLU()(x2)\n    #x2 = tf.keras.layers.ReLU()(x2)\n    x2 = tf.keras.layers.Conv1D(64, 2, padding='same')(x2)\n    x2 = tf.keras.layers.Dense(1)(x2)\n    x2 = tf.keras.layers.Flatten()(x2)\n    x2 = tf.keras.layers.Activation('softmax')(x2)\n\n    \n    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)    \n    model.compile(loss='binary_crossentropy', optimizer=optimizer)\n\n    return model\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train\nWe will skip this stage and load already trained model"},{"metadata":{"trusted":true},"cell_type":"code","source":"n_splits = 5","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Inference"},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\npreds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\nDISPLAY=1\nfor i in range(5):\n    print('#'*25)\n    print('### MODEL %i'%(i+1))\n    print('#'*25)\n    \n    K.clear_session()\n    model = build_model()\n    #model.load_weights('../input/m6aprila/v6-roberta-%i.h5'%i)\n    model.load_weights('../input/model8/v8-roberta-%i.h5'%i)\n\n    #model.load_weights('v5-roberta-%i.h5'%i)\n\n    print('Predicting Test...')\n    preds = model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n    preds_start += preds[0]/n_splits\n    preds_end += preds[1]/n_splits","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all = []\nfor k in range(input_ids_t.shape[0]):\n    a = np.argmax(preds_start[k,])\n    b = np.argmax(preds_end[k,])\n    if a>b: \n        st = test_df.loc[k,'text']\n    else:\n        text1 = \" \"+\" \".join(test_df.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode(enc.ids[a-1:b])          \n        st1=st.strip()\n        if st1=='****' or  st1=='****!' or st1=='****!' or st1=='****!' or st1=='****,' or  st1=='****,' or  st1=='****.' or st1=='****.':\n            #print(st1.strip())\n            #print(text1)\n            st=text1\n        elif st1=='(good':   \n            st='good'\n        elif st1=='__joy':   \n            st='joy'           \n    all.append(st)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df['selected_text'] = all","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk import word_tokenize\nfrom nltk.corpus import stopwords\n\nstop = set(stopwords.words('english'))\nsentence = \"this is a foo bar sentence\"\n\nlist = []\n\nfor sentence, sentiment  in zip(test_df['selected_text'], test_df['sentiment']):\n    if sentiment == 'positive' or sentiment == 'negative':\n        filtered_sentence = [i for i in word_tokenize(sentence.lower()) if i not in stop]\n        list.append(' '.join(filtered_sentence))\n    else:\n        filtered_sentence = sentence\n        list.append(filtered_sentence)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df['selected_text'] = pd.Series(list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df[['textID','selected_text']].to_csv('submission.csv',index=False)\n ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":4}