{"cells":[{"metadata":{"id":"gMTwmzff4m86"},"cell_type":"markdown","source":"In This Kernal we are going to train model using predefined RoBert BERT Model, not with given data.\nThis kernal is created from this [kernal](https://www.kaggle.com/abhishek/roberta-inference-5-folds)"},{"metadata":{"id":"mdAw_WP8FFCS"},"cell_type":"markdown","source":"# Imports\n"},{"metadata":{"id":"tiHsRPY0Xpus","trusted":true},"cell_type":"code","source":"import numpy as np             # for algebric functions\nimport pandas as pd            # to handle dataframes\nimport os                      # to import files \n#!pip install transformers\nimport transformers            # Transformers (pytorch-transformers /pytorch-pretrained-bert) provides general-purpose architectures (BERT, RoBERTa,..)\nimport tokenizers              # A tokenizer is in charge of preparing the inputs for a model. \nimport string                  \nimport torch                   # pytorch\nimport torch.nn as nn   \nfrom torch.nn import functional as F\nfrom tqdm import tqdm          # TQDM is a progress bar library\nimport re                      # regular expression\nimport json\nimport requests","execution_count":null,"outputs":[]},{"metadata":{"id":"y0Lm0krrjyh9"},"cell_type":"markdown","source":"# Packages used in this kernal:"},{"metadata":{"id":"ikJKQQYFhz2G"},"cell_type":"markdown","source":"\n\n* Transformers\n\n  Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet…) for Natural Language Understanding (NLU) and Natural Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between TensorFlow 2.0 and PyTorch.\n\n  This is the documentation of our [repository transformers](https://huggingface.co/transformers/index.html).\n\n\n* Tokenizers\n  \n  A tokenizer is in charge of preparing the inputs for a model. The library comprise tokenizers for all the models. Most of the tokenizers are available in two flavors: a full python implementation and a “Fast” implementation based on the Rust library tokenizers.\n\n  For more Info: [Tokenizers from transformers](https://huggingface.co/transformers/main_classes/tokenizer.html)\n\n* Pytorch (torch)\n\n  PyTorch is an open source machine learning library based on the Torch library, used for applications such as computer vision and natural language processing. It is primarily developed by Facebook's AI Research lab.\n\n  For PyTorch Beginner Tutorial:\n  * https://www.kaggle.com/anandsubbu007/pytorch-basics-tutorial-1 \n  * https://www.kaggle.com/anandsubbu007/pytorch-autograd-tutorial-2 \n  * https://www.kaggle.com/anandsubbu007/deep-nn-pytorch-tutorial-3 \n  * https://www.kaggle.com/anandsubbu007/cnn-cifar10-pytorch-tutorial-4\n\n  "},{"metadata":{"id":"sHM3EANE1zXV"},"cell_type":"markdown","source":"For Roberta Vocablary file: \n* https://huggingface.co/transformers/_modules/transformers/modeling_roberta.html\n\nFor Roberta PreTrainedModel File:\n* https://huggingface.co/transformers/_modules/transformers/tokenization_roberta.html"},{"metadata":{"id":"FiBGsNcHgeft","trusted":true},"cell_type":"code","source":"MAX_LEN = 192\nTRAIN_BATCH_SIZE = 32\nVALID_BATCH_SIZE = 8\nEPOCHS = 5\nROBERTA_PATH = 'roberta-base'","execution_count":null,"outputs":[]},{"metadata":{"id":"61lePkcSlARi"},"cell_type":"markdown","source":"# Downloading Vocab & Merge File from Roberta"},{"metadata":{"id":"vqL6W_aCdx9V","trusted":true},"cell_type":"code","source":"pre_voc_file = transformers.RobertaTokenizer.pretrained_vocab_files_map\nmerges_file  = pre_voc_file.get('merges_file').get('roberta-base')\nvocab_file = pre_voc_file.get('vocab_file').get('roberta-base')\nmodel_bin = transformers.modeling_roberta.ROBERTA_PRETRAINED_MODEL_ARCHIVE_MAP.get('roberta-base')","execution_count":null,"outputs":[]},{"metadata":{"id":"RSDBwTKZe5mE","trusted":true},"cell_type":"code","source":"#pre_voc_file","execution_count":null,"outputs":[]},{"metadata":{"id":"MR8ITIJWgldz","outputId":"2ed2dbf8-369c-4cd1-cf7e-5ae295d2e1e0","trusted":true},"cell_type":"code","source":"# Download Vocab file & Merge file\njson_f = requests.get(vocab_file)\ntxt_f = requests.get(merges_file)\nmod_bin = requests.get(model_bin)\n\ndata = json_f.json()\n#saving json vocab file\nwith open('vocab.json', 'w') as f:\n    json.dump(data, f)\n#saving merge.txt file\nopen('merge.txt', 'wb').write(txt_f.content)\nopen('model.bin', 'wb').write(mod_bin.content)","execution_count":null,"outputs":[]},{"metadata":{"id":"IC7zDhVpj8oX","trusted":true},"cell_type":"code","source":"TOKENIZER = tokenizers.ByteLevelBPETokenizer(vocab_file=f\"vocab.json\", \n                                             merges_file=f\"merge.txt\", \n                                             lowercase=True,\n                                             add_prefix_space=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"E8VujYcnrOZx","trusted":true},"cell_type":"code","source":"class TweetModel(transformers.BertPreTrainedModel):\n    def __init__(self, conf):\n        super(TweetModel, self).__init__(conf)\n        self.roberta = transformers.RobertaModel.from_pretrained(ROBERTA_PATH, config=conf)\n        self.drop_out = nn.Dropout(0.1)\n        self.l0 = nn.Linear(768 * 2, 2)\n        torch.nn.init.normal_(self.l0.weight, std=0.02)\n    \n    def forward(self, ids, mask, token_type_ids):\n        _, _, out = self.roberta(\n            ids,\n            attention_mask=mask,\n            token_type_ids=token_type_ids\n        )\n\n        out = torch.cat((out[-1], out[-2]), dim=-1)\n        out = self.drop_out(out)\n        logits = self.l0(out)\n\n        start_logits, end_logits = logits.split(1, dim=-1)\n\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n\n        return start_logits, end_logits","execution_count":null,"outputs":[]},{"metadata":{"id":"yOijGe1Bj5Ki"},"cell_type":"markdown","source":"## Roberta Model"},{"metadata":{"id":"eKJUJBU2j-97"},"cell_type":"markdown","source":"The RoBERTa model was proposed in RoBERTa: A Robustly Optimized BERT Pretraining Approach by Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov. It is based on Google’s BERT model released in 2018.\n\nWe are using [Roberta Model](https://huggingface.co/transformers/model_doc/roberta.html#robertamodel) In this kernal. For more models [Click Here](https://huggingface.co/models)\n\n"},{"metadata":{"id":"bTqouBQsosPW","trusted":true},"cell_type":"code","source":"def process_data(tweet, selected_text, sentiment, tokenizer, max_len):\n    tweet = \" \" + \" \".join(str(tweet).split())\n    selected_text = \" \" + \" \".join(str(selected_text).split())\n\n    len_st = len(selected_text) - 1\n    idx0 = None\n    idx1 = None\n\n    for ind in (i for i, e in enumerate(tweet) if e == selected_text[1]):\n        if \" \" + tweet[ind: ind+len_st] == selected_text:\n            idx0 = ind\n            idx1 = ind + len_st - 1\n            break\n\n    char_targets = [0] * len(tweet)\n    if idx0 != None and idx1 != None:\n        for ct in range(idx0, idx1 + 1):\n            char_targets[ct] = 1\n    \n    tok_tweet = tokenizer.encode(tweet)\n    input_ids_orig = tok_tweet.ids\n    tweet_offsets = tok_tweet.offsets\n    \n    target_idx = []\n    for j, (offset1, offset2) in enumerate(tweet_offsets):\n        if sum(char_targets[offset1: offset2]) > 0:\n            target_idx.append(j)\n    \n    targets_start = target_idx[0]\n    targets_end = target_idx[-1]\n\n    sentiment_id = {\n        'positive': 1313,\n        'negative': 2430,\n        'neutral': 7974\n    }\n    \n    input_ids = [0] + [sentiment_id[sentiment]] + [2] + [2] + input_ids_orig + [2]\n    token_type_ids = [0, 0, 0, 0] + [0] * (len(input_ids_orig) + 1)\n    mask = [1] * len(token_type_ids)\n    tweet_offsets = [(0, 0)] * 4 + tweet_offsets + [(0, 0)]\n    targets_start += 4\n    targets_end += 4\n\n    padding_length = max_len - len(input_ids)\n    if padding_length > 0:\n        input_ids = input_ids + ([1] * padding_length)\n        mask = mask + ([0] * padding_length)\n        token_type_ids = token_type_ids + ([0] * padding_length)\n        tweet_offsets = tweet_offsets + ([(0, 0)] * padding_length)\n    \n    return {\n        'ids': input_ids,\n        'mask': mask,\n        'token_type_ids': token_type_ids,\n        'targets_start': targets_start,\n        'targets_end': targets_end,\n        'orig_tweet': tweet,\n        'orig_selected': selected_text,\n        'sentiment': sentiment,\n        'offsets': tweet_offsets\n    }\n\n\nclass TweetDataset:\n    def __init__(self, tweet, sentiment, selected_text):\n        self.tweet = tweet\n        self.sentiment = sentiment\n        self.selected_text = selected_text\n        self.tokenizer = TOKENIZER\n        self.max_len = MAX_LEN\n    \n    def __len__(self):\n        return len(self.tweet)\n\n    def __getitem__(self, item):\n        data = process_data(\n            self.tweet[item], \n            self.selected_text[item], \n            self.sentiment[item],\n            self.tokenizer,\n            self.max_len\n        )\n\n        return {\n            'ids': torch.tensor(data[\"ids\"], dtype=torch.long),\n            'mask': torch.tensor(data[\"mask\"], dtype=torch.long),\n            'token_type_ids': torch.tensor(data[\"token_type_ids\"], dtype=torch.long),\n            'targets_start': torch.tensor(data[\"targets_start\"], dtype=torch.long),\n            'targets_end': torch.tensor(data[\"targets_end\"], dtype=torch.long),\n            'orig_tweet': data[\"orig_tweet\"],\n            'orig_selected': data[\"orig_selected\"],\n            'sentiment': data[\"sentiment\"],\n            'offsets': torch.tensor(data[\"offsets\"], dtype=torch.long)\n        }","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluation Model"},{"metadata":{"id":"kd7MYT5fvpLN","trusted":true},"cell_type":"code","source":"def calculate_jaccard_score(\n    original_tweet, \n    target_string, \n    sentiment_val, \n    idx_start, \n    idx_end, \n    offsets,\n    verbose=False):\n    \n    if idx_end < idx_start:\n        idx_end = idx_start\n    \n    filtered_output  = \"\"\n    for ix in range(idx_start, idx_end + 1):\n        filtered_output += original_tweet[offsets[ix][0]: offsets[ix][1]]\n        if (ix+1) < len(offsets) and offsets[ix][1] < offsets[ix+1][0]:\n            filtered_output += \" \"\n\n    if sentiment_val == \"neutral\" or len(original_tweet.split()) < 2:\n        filtered_output = original_tweet\n\n    if sentiment_val != \"neutral\" and verbose == True:\n        if filtered_output.strip().lower() != target_string.strip().lower():\n            print(\"********************************\")\n            print(f\"Output= {filtered_output.strip()}\")\n            print(f\"Target= {target_string.strip()}\")\n            print(f\"Tweet= {original_tweet.strip()}\")\n            print(\"********************************\")\n\n    jac = 0\n    return jac, filtered_output\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Importing Test Data"},{"metadata":{"id":"VlWT_PeGvrCl","trusted":true},"cell_type":"code","source":"df_test = pd.read_csv(\"../input/tweet-sentiment-extraction/test.csv\")\ndf_test.loc[:, \"selected_text\"] = df_test.text.values","execution_count":null,"outputs":[]},{"metadata":{"id":"MMI4tdngvscb","trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda\")\nmodel_config = transformers.RobertaConfig.from_pretrained('roberta-base')             # to download from internet\nmodel_config.output_hidden_states = True","execution_count":null,"outputs":[]},{"metadata":{"id":"3fI_9rrqQMQl","outputId":"52745fda-5132-4588-f27c-e585a11cd4d0","trusted":true},"cell_type":"code","source":"TweetDataset(tweet=df_test.text.values,\n             sentiment=df_test.sentiment.values,\n             selected_text=df_test.selected_text.values)","execution_count":null,"outputs":[]},{"metadata":{"id":"05qTImep2o9N","outputId":"16b52bb8-5bf9-47f3-8ab7-9740f7a0c461","trusted":true},"cell_type":"code","source":"model = TweetModel(conf=model_config)\nmodel.to(device)\nmodel.eval()\nprint(\"k\")","execution_count":null,"outputs":[]},{"metadata":{"id":"393346rq3sMP","trusted":true},"cell_type":"code","source":"final_output = []","execution_count":null,"outputs":[]},{"metadata":{"id":"wbANVbn33rrT","trusted":true},"cell_type":"code","source":"test_dataset = TweetDataset(\n        tweet=df_test.text.values,\n        sentiment=df_test.sentiment.values,\n        selected_text=df_test.selected_text.values\n    )\n\ndata_loader = torch.utils.data.DataLoader(\n    test_dataset,\n    shuffle=False,\n    batch_size=VALID_BATCH_SIZE,\n    num_workers=0\n)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prediction"},{"metadata":{"id":"Fg2fjT3a4r4J","outputId":"069bedd6-8a14-41ca-f0a0-a4580a20211a","trusted":true},"cell_type":"code","source":"with torch.no_grad():\n    tk0 = tqdm(data_loader, total=len(data_loader))\n    for bi, d in enumerate(tk0):\n        ids = d[\"ids\"]\n        token_type_ids = d[\"token_type_ids\"]\n        mask = d[\"mask\"]\n        sentiment = d[\"sentiment\"]\n        orig_selected = d[\"orig_selected\"]\n        orig_tweet = d[\"orig_tweet\"]\n        targets_start = d[\"targets_start\"]\n        targets_end = d[\"targets_end\"]\n        offsets = d[\"offsets\"].numpy()\n\n        ids            = ids.to(device, dtype=torch.long)\n        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n        mask           = mask.to(device, dtype=torch.long)\n        targets_start  = targets_start.to(device, dtype=torch.long)\n        targets_end    = targets_end.to(device, dtype=torch.long)\n\n        outputs_start1, outputs_end1 = model(\n            ids=ids,\n            mask=mask,\n            token_type_ids=token_type_ids\n        )\n\n        outputs_start = outputs_start1\n        outputs_end = outputs_end1\n        \n        outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n        outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n        jaccard_scores = []\n        for px, tweet in enumerate(orig_tweet):\n          selected_tweet = orig_selected[px]\n          tweet_sentiment = sentiment[px]\n          _, output_sentence = calculate_jaccard_score(original_tweet=tweet,\n                                                       target_string=selected_tweet,\n                                                       sentiment_val=tweet_sentiment,\n                                                       idx_start=np.argmax(outputs_start[px, :]),\n                                                       idx_end=np.argmax(outputs_end[px, :]),\n                                                       offsets=offsets[px])\n          final_output.append(output_sentence)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"id":"bKluzXnQ3tsj","trusted":true},"cell_type":"code","source":"sample = pd.read_csv(\"../input/tweet-sentiment-extraction/sample_submission.csv\")\nsample.loc[:, 'selected_text'] = final_output\nsample.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"id":"IxlrpmSGWUfN","trusted":true},"cell_type":"code","source":"sample.head()","execution_count":null,"outputs":[]}],"metadata":{"colab":{"name":"NLP Tweet Extraxtion","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":4}