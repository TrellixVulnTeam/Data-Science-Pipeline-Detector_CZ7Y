{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd, numpy as np\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom sklearn.model_selection import StratifiedKFold\nfrom transformers import *\nimport tokenizers\nprint('TF version',tf.__version__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_test():\n    test=pd.read_csv('../input/tweet-sentiment-extraction/test.csv')\n    test['text']=test['text'].astype(str)\n    return test\n\ntest_df = read_test()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN = 160\nPATH = '../input/tf-roberta/'\ntokenizer = tokenizers.ByteLevelBPETokenizer(\n    vocab_file=PATH+'vocab-roberta-base.json', \n    merges_file=PATH+'merges-roberta-base.txt', \n    lowercase=True,\n    add_prefix_space=True\n)\nsentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}\n\nct = test_df.shape[0]\ninput_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\nattention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\ntoken_type_ids_t = np.zeros((ct,MAX_LEN),dtype='int32')\n\nfor k in range(test_df.shape[0]):\n    # INPUT_IDS\n    text1 = \" \"+\" \".join(test_df.loc[k,'text'].split())\n    enc = tokenizer.encode(text1)                \n    s_tok = sentiment_id[test_df.loc[k,'sentiment']]\n    input_ids_t[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n    attention_mask_t[k,:len(enc.ids)+5] = 1\n    \ndef jacc(y_true, y_pred, smooth=100):\n    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n    sum_ = K.sum(K.abs(y_true) + K.abs(y_pred), axis=-1)\n    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n    return (1 - jac) * smooth\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model():\n    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n\n    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n    x = bert_model(ids,attention_mask=att,token_type_ids=tok)\n    \n    x1 = tf.keras.layers.Dropout(0.1)(x[0]) \n    x1 = tf.keras.layers.Conv1D(256, 5,padding='same')(x1)\n    x1 = tf.keras.layers.LeakyReLU()(x1)\n    x1 = tf.keras.layers.Conv1D(128, 5,padding='same')(x1)\n    x1 = tf.keras.layers.LeakyReLU()(x1)\n    x1 = tf.keras.layers.Conv1D(64, 5,padding='same')(x1)\n    x1 = tf.keras.layers.LeakyReLU()(x1)\n    x1 = tf.keras.layers.Dense(5)(x1)\n    x1 = tf.keras.layers.LeakyReLU()(x1)\n    x1 = tf.keras.layers.Dense(1)(x1)\n    x1 = tf.keras.layers.Flatten()(x1)\n    x1 = tf.keras.layers.Activation('softmax')(x1)\n    \n    x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n    x2 = tf.keras.layers.Conv1D(256, 5, padding='same')(x2)\n    x2 = tf.keras.layers.LeakyReLU()(x2)\n    x2 = tf.keras.layers.Conv1D(128, 5, padding='same')(x2)\n    x2 = tf.keras.layers.LeakyReLU()(x2)\n    x2 = tf.keras.layers.Conv1D(64, 5, padding='same')(x2)\n    x2 = tf.keras.layers.LeakyReLU()(x2)\n    x2 = tf.keras.layers.Dense(5)(x2)\n    x2 = tf.keras.layers.LeakyReLU()(x2)\n    x2 = tf.keras.layers.Dense(1)(x2)\n    x2 = tf.keras.layers.Flatten()(x2)\n    x2 = tf.keras.layers.Activation('softmax')(x2)\n\n    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n    model.compile(loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0.15), optimizer=optimizer)\n    return model\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_splits=5\n\npreds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\npreds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\nDISPLAY=1\nfor i in range(5):\n    print('#'*25)\n    print('### MODEL %i'%(i+1))\n    print('#'*25)\n\n    K.clear_session()\n    model = build_model()\n    model.load_weights('/kaggle/input/len160/v4-'+'positive'+'-%i.h5'%i)\n    print('Predicting Test...')\n    preds = model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n    preds_start += preds[0]/n_splits\n    preds_end += preds[1]/n_splits","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all = []\nfor k in range(input_ids_t.shape[0]):\n    a = np.argmax(preds_start[k,])\n    b = np.argmax(preds_end[k,])\n    if a>b: \n        st = test_df.loc[k,'text']\n    else:\n        text1 = \" \"+\" \".join(test_df.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode(enc.ids[a-1:b])\n    all.append(st.strip())\n\n\n\ntest_df['selected_text'] = all\ntest_df[['textID','selected_text']].to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}