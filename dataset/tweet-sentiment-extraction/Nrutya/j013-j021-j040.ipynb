{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"id":"Bksa2sS1VUK4"},"cell_type":"markdown","source":"#Nrutya Doshi J013 \n#Rishabh Jain J021\n#Reuben Rapose J040 "},{"metadata":{"id":"n8Dna3YTKV0k"},"cell_type":"markdown","source":"Importing important libraries"},{"metadata":{"id":"VChyDOHiY_R2","trusted":true},"cell_type":"code","source":"import pandas as pd \nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"id":"pcXxuoXiKk8D"},"cell_type":"markdown","source":"Importing dataset"},{"metadata":{"id":"1cp5iSzxZPH8","trusted":true},"cell_type":"code","source":"# Import datasets\ntrain = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv')\ntest = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv')\nsample = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"id":"B2CcCW7HK4Et"},"cell_type":"markdown","source":"Calculating & removing na values"},{"metadata":{"id":"-155BGCqb8N5","outputId":"8226e4aa-dc63-4a20-bf9c-feabe3dc9804","trusted":true},"cell_type":"code","source":"# Calculating number of na values in dataframe\ntrain.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"id":"eS1DyPtjZ5WZ","trusted":true},"cell_type":"code","source":"# Droping na values\ntrain.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"Hgh_Rym_cXtm","outputId":"7e4aff90-053a-4fcf-c92a-6ca4f6589f27","trusted":true},"cell_type":"code","source":"# Now no na values\ntrain.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"id":"mWjWR2sEc36Z","outputId":"13fbcf42-71ff-4dbd-d9d9-8c31bccabdb8","trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"EJsU6ijLK-_b"},"cell_type":"markdown","source":"Making a wordcloud representation"},{"metadata":{"id":"1wVXknY_GI0p","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"cExRlQxLFllA"},"cell_type":"code","source":"def plotWordClouds(df_text,sentiment):\n    text = \" \".join(str(tmptext) for tmptext in df_text)\n    text = text.lower()\n    wordcloud = WordCloud(\n        background_color='white',\n        stopwords=stopwords,\n        max_words=300,\n        max_font_size=40, \n        scale=3,\n        random_state=1 # chosen at random by flipping a coin; it was heads\n    ).generate(text)\n  \n    # plot the WordCloud image                        \n    plt.figure(figsize = (8, 8), facecolor = None) \n    plt.imshow(wordcloud) \n    plt.axis(\"off\") \n    plt.tight_layout(pad = 0) \n    plt.title('WordCloud - ' + sentiment)\n    plt.show()         \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"2ZvBYxFtFllD","outputId":"51278aa8-9b5d-4b93-e380-63ffd4456987"},"cell_type":"code","source":"subtext = train[train['sentiment']=='positive']['selected_text']\nfrom wordcloud import WordCloud, STOPWORDS\nstopwords = set(STOPWORDS) \nplotWordClouds(subtext,'positive')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"NhEDFr2CFllI","outputId":"8c014293-0ac1-4aa7-e95b-036b36b0f310"},"cell_type":"code","source":"subtext = train[train['sentiment']=='neutral']['selected_text']\nplotWordClouds(subtext,'neutral')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"Vv0jFAdvFllL","outputId":"82eeb223-1711-4405-cf18-4ffd4b2881e1"},"cell_type":"code","source":"subtext = train[train['sentiment']=='negative']['selected_text']\nplotWordClouds(subtext,'negative')","execution_count":null,"outputs":[]},{"metadata":{"id":"coHRMNhjLFNG"},"cell_type":"markdown","source":"Pre-processing the data"},{"metadata":{"id":"htbA-crHLL2w"},"cell_type":"markdown","source":"Converting the train & test data to lowercase"},{"metadata":{"id":"UofJOXegjZ4l","trusted":true},"cell_type":"code","source":"# Make all the text lowercase - casing doesn't matter when \n# we choose our selected text.\ntrain['text'] = train['text'].apply(lambda x: x.lower())\ntest['text'] = test['text'].apply(lambda x: x.lower())","execution_count":null,"outputs":[]},{"metadata":{"id":"Bb3TUBuLLYE8"},"cell_type":"markdown","source":"Taking only words that contain alphabets in train data i.e. removing punctuations & numbers"},{"metadata":{"id":"w7ztv4zQSPwj","trusted":true},"cell_type":"code","source":"import re","execution_count":null,"outputs":[]},{"metadata":{"id":"EebLaa4OhoOk","trusted":true},"cell_type":"code","source":"def text_data(str): # taking only the text data\n  str_new = \" \".join(re.findall(\"[a-zA-Z]+\", str))\n  return (str_new)","execution_count":null,"outputs":[]},{"metadata":{"id":"uL436HFEhsQH","trusted":true},"cell_type":"code","source":"train['text'] = train['text'].apply(lambda x: text_data(x))\ntrain['selected_text'] = train['selected_text'].apply(lambda x: text_data(x))","execution_count":null,"outputs":[]},{"metadata":{"id":"wEllLKkKiYDn","outputId":"8bdb3fcb-799f-48a2-f0e0-6328781a4e86","trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"tX2AkJvrLmy0"},"cell_type":"markdown","source":"Splitting the train data into train & validation set as we want to see how the mdethod we created would would on the final test data"},{"metadata":{"id":"8VO39E6XaIT8","trusted":true},"cell_type":"code","source":"# Make training/test split\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_val = train_test_split(train, train_size = 0.9, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"id":"cLGXSyVhioyA","outputId":"2fbab2a8-d76a-4fa5-d516-e88502264d0d","trusted":true},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"cIGEu6gEMAuh"},"cell_type":"markdown","source":"Cleaning the X_train data"},{"metadata":{"id":"wn167NE9aqv4","outputId":"0d66b689-7809-4eba-c050-8c8ac491b25a","trusted":true},"cell_type":"code","source":"import nltk\nnltk.download(\"all\")","execution_count":null,"outputs":[]},{"metadata":{"id":"Qg5DmR-XMLJM"},"cell_type":"markdown","source":"Tokenization"},{"metadata":{"id":"kDbPDejfjrBn","trusted":true},"cell_type":"code","source":"def tokenize(str_new): # tokenization\n  tokens = []\n  tokens = nltk.word_tokenize(str_new)\n  return (tokens)","execution_count":null,"outputs":[]},{"metadata":{"id":"OwJLVql_kJIS","trusted":true},"cell_type":"code","source":"X_train['text'] = X_train['text'].apply(lambda x: tokenize(x))\nX_train['selected_text'] = X_train['selected_text'].apply(lambda x: tokenize(x))","execution_count":null,"outputs":[]},{"metadata":{"id":"9r4zLjiBMNLu"},"cell_type":"markdown","source":"Removing stopwords"},{"metadata":{"id":"hQiw5ZIOg0lG","trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\ndef remove(tokens): # removing stopwords & punctuations\n  removed = []\n  stopword = stopwords.words('english') \n  for w in tokens:\n    if (w not in stopword):\n      removed.append(w)\n  return (removed)","execution_count":null,"outputs":[]},{"metadata":{"id":"N5RDPKtwj_KM","trusted":true},"cell_type":"code","source":"X_train['text'] = X_train['text'].apply(lambda x: remove(x))\nX_train['selected_text'] = X_train['selected_text'].apply(lambda x: remove(x))","execution_count":null,"outputs":[]},{"metadata":{"id":"Aazj07RAMQJD"},"cell_type":"markdown","source":"POS tagging"},{"metadata":{"id":"JDtOjUAimt6W","trusted":true},"cell_type":"code","source":"def tagging(removed):\n  pos = []\n  pos.append(nltk.pos_tag(removed))\n  return (pos)","execution_count":null,"outputs":[]},{"metadata":{"id":"lBQO7no-mt48","trusted":true},"cell_type":"code","source":"X_train['text'] = X_train['text'].apply(lambda x: tagging(x))\nX_train['selected_text'] = X_train['selected_text'].apply(lambda x: tagging(x))","execution_count":null,"outputs":[]},{"metadata":{"id":"5cO36imIMZJ7"},"cell_type":"markdown","source":"Lemmatization"},{"metadata":{"id":"dJpVPn8qmtzn","trusted":true},"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet","execution_count":null,"outputs":[]},{"metadata":{"id":"SK_1UjyWmtnf","trusted":true},"cell_type":"code","source":"def lemmatization(pos):\n  lem = WordNetLemmatizer()\n  lemma = []\n  for word in pos:\n    for w in word:\n      pos_value = \"\"\n      if (w[1].startswith('J')):\n        pos_value = wordnet.ADJ\n      elif (w[1].startswith('V')):\n        pos_value = wordnet.VERB\n      elif (w[1].startswith('N')):\n        pos_value = wordnet.NOUN\n      elif (w[1].startswith('R')):\n        pos_value = wordnet.ADV \n      else:\n        continue\n      lemma.append(lem.lemmatize(w[0],pos_value))\n  return (lemma)","execution_count":null,"outputs":[]},{"metadata":{"id":"PCqxbfyamtvJ","trusted":true},"cell_type":"code","source":"X_train['text'] = X_train['text'].apply(lambda x: lemmatization(x))\nX_train['selected_text'] = X_train['selected_text'].apply(lambda x: lemmatization(x))","execution_count":null,"outputs":[]},{"metadata":{"id":"Fd0F_EhGMfDa"},"cell_type":"markdown","source":"Converting list to string"},{"metadata":{"id":"teyZkw6Ukkj0","trusted":true},"cell_type":"code","source":"def to_string(lemma): # converting the data from list to string format for tfidf\n  str1 = ' '.join([elem for elem in lemma]) \n  return (str1)","execution_count":null,"outputs":[]},{"metadata":{"id":"ktFORoGBnjMv","trusted":true},"cell_type":"code","source":"X_train['text'] = X_train['text'].apply(lambda x: to_string(x))\nX_train['selected_text'] = X_train['selected_text'].apply(lambda x: to_string(x))","execution_count":null,"outputs":[]},{"metadata":{"id":"uLh3CLpzkUtu","outputId":"94a24ca1-ea90-424f-9f51-afa226af09fc","trusted":true},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"L02swZQLMk48"},"cell_type":"markdown","source":"Creating different dataset for different sentiments"},{"metadata":{"id":"BUx2-qPIa0F1","trusted":true},"cell_type":"code","source":"pos_train = X_train[X_train['sentiment'] == 'positive'] # contains all positive\nneutral_train = X_train[X_train['sentiment'] == 'neutral']\nneg_train = X_train[X_train['sentiment'] == 'negative']","execution_count":null,"outputs":[]},{"metadata":{"id":"DzB8OzrMMskH"},"cell_type":"markdown","source":"Using count vectorizer to convert words to numbers"},{"metadata":{"id":"o9E7Uc1ua3gV","trusted":true},"cell_type":"code","source":"# CountVectorizer will help calculate word counts\nfrom sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer(analyzer='word',max_df=0.95,stop_words='english')","execution_count":null,"outputs":[]},{"metadata":{"id":"6iN9ovgeM31n","trusted":true},"cell_type":"code","source":"X_train_cv = cv.fit_transform(X_train['text']) # has only text data","execution_count":null,"outputs":[]},{"metadata":{"id":"7MgJsRcxM3yj","trusted":true},"cell_type":"code","source":"X_pos = cv.transform(pos_train['text'])\nX_neutral = cv.transform(neutral_train['text'])\nX_neg = cv.transform(neg_train['text'])","execution_count":null,"outputs":[]},{"metadata":{"id":"ATNpsDjzM3wh","trusted":true},"cell_type":"code","source":"pos_count_df = pd.DataFrame(X_pos.toarray(), columns=cv.get_feature_names())\nneutral_count_df = pd.DataFrame(X_neutral.toarray(), columns=cv.get_feature_names())\nneg_count_df = pd.DataFrame(X_neg.toarray(), columns=cv.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"id":"qmNE401TNALl"},"cell_type":"markdown","source":"We caculate a score for each word, which is like a tf-idf score"},{"metadata":{"id":"GpgCHIcENski"},"cell_type":"markdown","source":"Term frequency value is calculated, here for positive, pos contains the number of times a word occurs in the positive sentiment (datframe), pos_train.shape[0] will denote the number of documents "},{"metadata":{"id":"6HLk8lXVa61-","trusted":true},"cell_type":"code","source":"# Create dictionaries of the words within each sentiment group, where the values are the proportions of tweets that contain those words\n\npos_words = {}\nneutral_words = {}\nneg_words = {}","execution_count":null,"outputs":[]},{"metadata":{"id":"w_QG7IOBNfet","trusted":true},"cell_type":"code","source":"for k in cv.get_feature_names(): # for every word \n    pos = pos_count_df[k].sum() # number of times the positive word occurs\n    neutral = neutral_count_df[k].sum()\n    neg = neg_count_df[k].sum()\n    pos_words[k] = pos/pos_train.shape[0] # pos_train.shape[0] - total positive documents, therefore positive word/ positive documents, term frequency\n    neutral_words[k] = neutral/neutral_train.shape[0]\n    neg_words[k] = neg/neg_train.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"id":"cgqkqJXqOSjx"},"cell_type":"markdown","source":"Inverse document frequency is calculated here, for positive, the tf of negative & neutral is subtracted from positive, to determine how positive a word is"},{"metadata":{"id":"evPDaR06Nfji","trusted":true},"cell_type":"code","source":"# We need to account for the fact that there will be a lot of words used in tweets of every sentiment.  \n# Therefore, we reassign the values in the dictionary by subtracting the proportion of tweets in the other \n# sentiments that use that word.\npos_words_adj = {}\nneutral_words_adj = {}\nneg_words_adj = {}","execution_count":null,"outputs":[]},{"metadata":{"id":"k4EiJbPuNlBv","trusted":true},"cell_type":"code","source":"for key, value in pos_words.items():\n    pos_words_adj[key] = pos_words[key] - (neutral_words[key] + neg_words[key]) # this is basically idf, which means how positive is the word, does it occur in negative & neutral\n    \nfor key, value in neutral_words.items():\n    neutral_words_adj[key] = neutral_words[key] - (neg_words[key] + pos_words[key])\n\nfor key, value in neg_words.items():\n    neg_words_adj[key] = neg_words[key] - (neutral_words[key] + pos_words[key])","execution_count":null,"outputs":[]},{"metadata":{"id":"tNPGkM8CO4SV"},"cell_type":"markdown","source":"Here we determine the answer for the X_val, we make subset of words such that 1 gram, 2 gram, ..., then we calculate the score for the validation input using the scores for each word calculated in the previous cell, the subset with the highest score is the answer"},{"metadata":{"id":"sK991TFruHQ0","trusted":true},"cell_type":"code","source":"def calculate_selected_text(df_row, tol = 0):\n    \n    tweet = df_row['text']\n    sentiment = df_row['sentiment']\n    \n    if(sentiment == 'neutral'):\n        return tweet \n    \n    elif(sentiment == 'positive'):\n        dict_to_use = pos_words_adj # Calculate word weights using the pos_words dictionary\n    elif(sentiment == 'negative'):\n        dict_to_use = neg_words_adj # Calculate word weights using the neg_words dictionary\n        \n    words = tweet.split()\n    words_len = len(words)\n    subsets = [words[i:j+1] for i in range(words_len) for j in range(i,words_len)] # taking combination of words 1 gram, 2 gram, ..., with 1st word, 2nd word\n        \n    score = 0\n    selection_str = '' # This will be our choice\n    lst = sorted(subsets, key = len) # Sort candidates by length for each subset\n    \n    \n    for i in range(len(subsets)): # for all possiblities\n        \n        new_sum = 0 # Sum for the current substring\n        \n        # Calculate the sum of weights for each word in the substring\n        for p in range(len(lst[i])): # go length wise\n            if(lst[i][p] in dict_to_use.keys()):\n                new_sum += dict_to_use[lst[i][p]]\n                \n            \n        # If the sum is greater than the score, update our current selection\n        if(new_sum > score + tol): \n            score = new_sum\n            selection_str = lst[i]\n            #tol = tol*5 # Increase the tolerance a bit each time we choose a selection\n\n    # If we didn't find good substrings, return the whole text\n    if(len(selection_str) == 0):\n        selection_str = words                   \n        \n    return ' '.join(selection_str)","execution_count":null,"outputs":[]},{"metadata":{"id":"DDqtLahvbR3z","trusted":true},"cell_type":"code","source":"tol = 0.0015\n\nX_val['predicted_selection'] = ''\n\nfor index, row in X_val.iterrows(): # rows in X validation set\n    \n    selected_text = calculate_selected_text(row, tol)\n    \n    X_val.loc[X_val['textID'] == row['textID'], ['predicted_selection']] = selected_text","execution_count":null,"outputs":[]},{"metadata":{"id":"tcjxeL1kQ-7O"},"cell_type":"markdown","source":"We calculate the jaccard score to see how accurate is our prediction, we basically see the similarity"},{"metadata":{"id":"2JKIgFDNbUUk","trusted":true},"cell_type":"code","source":"def jaccard(str1, str2): # to see similarity\n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c)) ","execution_count":null,"outputs":[]},{"metadata":{"id":"SasIdQUfbWfS","outputId":"8e6864a3-d8e8-402b-8e94-c37312a8037b","trusted":true},"cell_type":"code","source":"X_val['jaccard'] = X_val.apply(lambda x: jaccard(x['selected_text'], x['predicted_selection']), axis = 1)\n\nprint('The jaccard score for the validation set is:', np.mean(X_val['jaccard']))","execution_count":null,"outputs":[]},{"metadata":{"id":"RaE66kw1bbAl","outputId":"ff30eb78-9781-4bcd-855f-2785af7dbfdb","trusted":true},"cell_type":"code","source":"X_val","execution_count":null,"outputs":[]},{"metadata":{"id":"Edn1RiHdRKWf"},"cell_type":"markdown","source":"Now we fit the model on our entire train data & calculate the prediction for test data "},{"metadata":{"id":"Z6e4dvxubdpH","trusted":true},"cell_type":"code","source":"pos_tr = train[train['sentiment'] == 'positive']\nneutral_tr = train[train['sentiment'] == 'neutral']\nneg_tr = train[train['sentiment'] == 'negative']","execution_count":null,"outputs":[]},{"metadata":{"id":"o8m_Xb5eRjrH"},"cell_type":"markdown","source":"Count Vectorizer"},{"metadata":{"id":"zULKBd3Obf5A","trusted":true},"cell_type":"code","source":"cv = CountVectorizer(analyzer='word',max_df=0.95, min_df=2,stop_words='english')\n\nfinal_cv = cv.fit_transform(train['text'])\n\nX_pos = cv.transform(pos_tr['text'])\nX_neutral = cv.transform(neutral_tr['text'])\nX_neg = cv.transform(neg_tr['text'])\n\npos_final_count_df = pd.DataFrame(X_pos.toarray(), columns=cv.get_feature_names())\nneutral_final_count_df = pd.DataFrame(X_neutral.toarray(), columns=cv.get_feature_names())\nneg_final_count_df = pd.DataFrame(X_neg.toarray(), columns=cv.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"id":"Fmji8gewRmU7"},"cell_type":"markdown","source":"Score for each word (TF-IDF value)"},{"metadata":{"id":"OEWBENikbh1a","trusted":true},"cell_type":"code","source":"pos_words = {}\nneutral_words = {}\nneg_words = {}\n\nfor k in cv.get_feature_names():\n    pos = pos_final_count_df[k].sum()\n    neutral = neutral_final_count_df[k].sum()\n    neg = neg_final_count_df[k].sum()\n    \n    pos_words[k] = pos/(pos_tr.shape[0])\n    neutral_words[k] = neutral/(neutral_tr.shape[0])\n    neg_words[k] = neg/(neg_tr.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"id":"4ji9FIkWbj6D","trusted":true},"cell_type":"code","source":"neg_words_adj = {}\npos_words_adj = {}\nneutral_words_adj = {}\n\nfor key, value in neg_words.items():\n    neg_words_adj[key] = neg_words[key] - (neutral_words[key] + pos_words[key])\n    \nfor key, value in pos_words.items():\n    pos_words_adj[key] = pos_words[key] - (neutral_words[key] + neg_words[key])\n    \nfor key, value in neutral_words.items():\n    neutral_words_adj[key] = neutral_words[key] - (neg_words[key] + pos_words[key])","execution_count":null,"outputs":[]},{"metadata":{"id":"aUK-R3l_R1Re"},"cell_type":"markdown","source":"Prediction"},{"metadata":{"id":"BHWWozTfboT6","trusted":true},"cell_type":"code","source":"tol = 0.001\n\nfor index, row in test.iterrows():\n    \n    selected_text = calculate_selected_text(row, tol)\n    \n    sample.loc[sample['textID'] == row['textID'], ['selected_text']] = selected_text","execution_count":null,"outputs":[]},{"metadata":{"id":"JJWDSQHGbrpG","outputId":"24975128-50d9-4c21-f6f0-8945c7ec2536","trusted":true},"cell_type":"code","source":"sample.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"jPwG9cSrR66q"},"cell_type":"markdown","source":"Writing the prediction in the submission.csv file"},{"metadata":{"id":"xZJNKF5Qbo7Y","trusted":true},"cell_type":"code","source":"sample.to_csv('submission.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"id":"x3XgSix5bq-I","trusted":false},"cell_type":"code","source":"","execution_count":0,"outputs":[]}],"metadata":{"colab":{"name":"NLP_M3.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":4}