{"cells":[{"metadata":{},"cell_type":"markdown","source":"**In this Notebook I will apply basic NLP techniques which includes regular expressions to remove unwanted characters from a text, lammetizaion,stemming , removing stop words, part of speech tagging and visualisation and understanding patterns**"},{"metadata":{},"cell_type":"markdown","source":"Some Basics:\n    Regular Expressions:\n    A regular expression (sometimes called a rational expression) is a sequence of characters that define a search pattern, mainly for use in pattern matching with strings, or string matching, i.e. “find and replace”-like operations.(Wikipedia).\n\nRegular expressions are a generalized way to match patterns with sequences of characters. It is used in every programming language like C++, Java and Python.\n\nWhat is a regular expression and what makes it so important?\nRegex are used in Google analytics in URL matching in supporting search and replace in most popular editors like Sublime, Notepad++, Brackets, Google Docs and Microsoft word.\n\n\n\nExample :  Regular expression for an email address :\n^([a-zA-Z0-9_\\-\\.]+)@([a-zA-Z0-9_\\-\\.]+)\\.([a-zA-Z]{2,5})$ \n\nHow to write regular expression ?\nFor that you need to learn various rule for the same. \nYou may refer below link:\n   [NTL by Dan Jurafsky](http://youtu.be/3Dt_yh1mf_U)\n   \n POS tagging:\n It is used to identify the part of speech of a word in text. NLTK povide methods for pos tagging. A comprehancive list of POS tags short list is as below:\n  will use POS tagging ( which is used for tagging a word for its part of speech as per engilsh grammer) for filtering words which can be used as features for classification of tweets\n\n\nStop words:\nit , the , a etc words generaly dont help in finding sentiments hence these are generaly being removed from the text. These are called stop words. \n\nPlease refer the link given above to undestand basics of natural language programming\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#POS tagging code\n'''CC coordinating conjunction\nCD cardinal digit\nDT determiner\nEX existential there (like: \"there is\" ... think of it like \"there exists\")\nFW foreign word\nIN preposition/subordinating conjunction\nJJ adjective 'big'\nJJR adjective, comparative 'bigger'\nJJS adjective, superlative 'biggest'\nLS list marker 1)\nMD modal could, will\nNN noun, singular 'desk'\nNNS noun plural 'desks'\nNNP proper noun, singular 'Harrison'\nNNPS proper noun, plural 'Americans'\nPDT predeterminer 'all the kids'\nPOS possessive ending parent's\nPRP personal pronoun I, he, she\nPRP$ possessive pronoun my, his, hers\nRB adverb very, silently,\nRBR adverb, comparative better\nRBS adverb, superlative best\nRP particle give up\nTO to go 'to' the store.\nUH interjection errrrrrrrm\nVB verb, base form take\nVBD verb, past tense took\nVBG verb, gerund/present participle taking\nVBN verb, past participle taken\nVBP verb, sing. present, non-3d take\nVBZ verb, 3rd person sing. present takes\nWDT wh-determiner which\nWP wh-pronoun who, what\nWP$ possessive wh-pronoun whose\nWRB wh-abverb where, when'''\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# General Libraries\nimport nltk\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\n# specific for data preproressing and visualization\nfrom nltk.tokenize import sent_tokenize,word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom wordcloud import WordCloud,STOPWORDS \nfrom statistics import mode\n# classifiers\nfrom nltk.classify.scikitlearn import SklearnClassifier\nfrom sklearn.naive_bayes import MultinomialNB,GaussianNB,BernoulliNB\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC,LinearSVC,NuSVC\nfrom nltk.classify import ClassifierI","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"TweetData =  pd.read_csv(\"/kaggle/input/tweet-sentiment-extraction/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TweetData.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have total 27486 rows but text and selected text is missing for one row Lets check the same"},{"metadata":{"trusted":true},"cell_type":"code","source":"TweetData.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(TweetData.isna().any())\nTweetData[TweetData.isnull().any(axis=1)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets Remove the Empty Text row"},{"metadata":{"trusted":true},"cell_type":"code","source":"TweetData.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TweetData.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets see the Distributions of Sentiments"},{"metadata":{"trusted":true},"cell_type":"code","source":"# the count of sentiments in absolute value as we as percentage form\n\nplt.figure(figsize=(12,6))\nplt.subplot(1,2,1)\nsns.countplot(TweetData['sentiment'], facecolor=(0, 0, 0, 0),\n                   linewidth=5,\n                   edgecolor=sns.color_palette(\"muted\", 3))\nplt.subplot(1,2,2)\n(TweetData['sentiment'].value_counts()/len(TweetData)*100).plot(kind=\"bar\", rot=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets count the words in each tweet and add columns for it\ndef wordcount(text):\n    words = word_tokenize(text)\n    return len(words)\n\nTweetData['Text_words'] = TweetData['text'].apply(lambda x : wordcount(x))\nTweetData['Select_text_words']= TweetData['selected_text'].apply(lambda x : wordcount(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TweetData.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(style='whitegrid', rc={\"grid.linewidth\": 0.2})\nsns.set_context(\"paper\", font_scale=2)\nplt.figure(figsize=(12,6),dpi=100)\nplt.subplot(1,2,1)\nsns.distplot(TweetData['Text_words'],kde=False)\nplt.subplot(1,2,2)\nsns.distplot(TweetData['Select_text_words'],kde=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Majority of text have length 1-10 and it has been reduced to 1 to 3 or 4 length word as we can se above It might helpful while applying model in test data\n\nLets Clean the tweets using Regular Expressions"},{"metadata":{"trusted":true},"cell_type":"code","source":"stop_words = set(stopwords.words('english'))\ndef clean_tweets(x):\n    # removing the hyperlinks\n    clean1 = re.sub('https?://[A-Za-z0-9./]+','',x)\n    # removing the hashtags\n    clean2 = re.sub('#[A-Za-z0-9]+','',clean1)\n    # removing @\n    clean3 = re.sub('@[A-Za-z0-9]','',clean2)\n    # removing punctuations and lower case conversion\n    clean4 = re.sub(r'[^\\w\\s]','',clean3).lower()\n    words = word_tokenize(clean4)\n    # removing stopwords\n    words = [w for w in words if not w in stop_words]\n    sent = ' '.join(words)\n    return sent","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets first check it for few sentences:\nfor i in range(12,30):\n    Clean = clean_tweets(TweetData['text'][i])\n    print(\"Text: \",TweetData['text'][i])\n    print(\"Clean Text: \", Clean)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets apply our clean tweet funcion for all tests\nTweetData['Clean_tweet'] = TweetData['text'].apply(lambda x : clean_tweets(x))\nTweetData['Clean_tweet_words'] = TweetData['Clean_tweet'].apply(lambda x : wordcount(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TweetData.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(style='whitegrid', rc={\"grid.linewidth\": 0.2})\nsns.set_context(\"paper\", font_scale=1.2)\nplt.figure(figsize=(12,6),dpi=100)\nplt.subplot(1,3,1)\nsns.distplot(TweetData['Text_words'],kde=False)\nplt.subplot(1,3,2)\nsns.distplot(TweetData['Select_text_words'],kde=False)\nplt.subplot(1,3,3)\nsns.distplot(TweetData['Clean_tweet_words'],kde=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Lets see the words Frequency Distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"# All word Freqency curve with out any pos tagging filter\nAll_words = []\nfor sent in TweetData['Clean_tweet']:\n    for word in word_tokenize(sent):\n            All_words.append(word)\nAll_words_freq = nltk.FreqDist(All_words)\nFreq_word_DF = pd.DataFrame({\"Data\":All_words_freq.most_common(15)})\nFreq_word_DF['Words'] = Freq_word_DF['Data'].apply(lambda x : x[0])\nFreq_word_DF['freq'] = Freq_word_DF['Data'].apply(lambda x : x[1])\nsns.set()\nsns.set(style='whitegrid', rc={\"grid.linewidth\": 0.2})\nsns.set_context(\"paper\", font_scale=2)\nfig=plt.figure(figsize =(20,8),dpi=50)\nsns.barplot('Words','freq',data = Freq_word_DF)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets See the Positive , Neutral and Negative tweets words Frequency Distribution Seperatly "},{"metadata":{"trusted":true},"cell_type":"code","source":"# TweetData[TweetData['sentiment']=='positive']['Clean_tweet']\nAll_words = []\nfor sent in TweetData[TweetData['sentiment']=='positive']['Clean_tweet']:\n    for word in word_tokenize(sent):\n            All_words.append(word)\nAll_words_freq = nltk.FreqDist(All_words)\nFreq_word_DF = pd.DataFrame({\"Data\":All_words_freq.most_common(15)})\nFreq_word_DF['Words'] = Freq_word_DF['Data'].apply(lambda x : x[0])\nFreq_word_DF['freq'] = Freq_word_DF['Data'].apply(lambda x : x[1])\nsns.set()\nsns.set(style='whitegrid', rc={\"grid.linewidth\": 0.2})\nsns.set_context(\"paper\", font_scale=2)\nfig=plt.figure(figsize =(20,8),dpi=50)\nax = sns.barplot('Words','freq',data = Freq_word_DF)\nax.set_title(\"Positive Tweets Words Frequency Plot\")\n\n# Negative tweets\nAll_words = []\nfor sent in TweetData[TweetData['sentiment']=='negative']['Clean_tweet']:\n    for word in word_tokenize(sent):\n            All_words.append(word)\nAll_words_freq = nltk.FreqDist(All_words)\nFreq_word_DF = pd.DataFrame({\"Data\":All_words_freq.most_common(15)})\nFreq_word_DF['Words'] = Freq_word_DF['Data'].apply(lambda x : x[0])\nFreq_word_DF['freq'] = Freq_word_DF['Data'].apply(lambda x : x[1])\nsns.set()\nsns.set(style='whitegrid', rc={\"grid.linewidth\": 0.2})\nsns.set_context(\"paper\", font_scale=2)\nfig=plt.figure(figsize =(20,8),dpi=50)\nax = sns.barplot('Words','freq',data = Freq_word_DF)\nax.set_title(\"Negative Tweets Words Frequency Plot\")\n\n# Neutral Tweets\n\nAll_words = []\nfor sent in TweetData[TweetData['sentiment']=='neutral']['Clean_tweet']:\n    for word in word_tokenize(sent):\n            All_words.append(word)\nAll_words_freq = nltk.FreqDist(All_words)\nFreq_word_DF = pd.DataFrame({\"Data\":All_words_freq.most_common(15)})\nFreq_word_DF['Words'] = Freq_word_DF['Data'].apply(lambda x : x[0])\nFreq_word_DF['freq'] = Freq_word_DF['Data'].apply(lambda x : x[1])\nsns.set()\nsns.set(style='whitegrid', rc={\"grid.linewidth\": 0.2})\nsns.set_context(\"paper\", font_scale=2)\nfig=plt.figure(figsize =(20,8),dpi=50)\nax = sns.barplot('Words','freq',data = Freq_word_DF)\nax.set_title(\"Neutral Tweets Words Frequency Plot\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Observations:\n**We can see that positive tweets common word contain adjectives e.g Love Good Happy Thanks Hope\nand Negative tweets have common words such as dont cant sad sorry etc\nwhile neutral has words like get go ad nothing from positive or negative words like happy, sad good etc**\n\nThis seems significant deviaion and if we can make feature of each words of these 15*3 words we can train a model for the same. \nBut our job is not to make a model to predict the sentiment of the same. Our job is to predict which part of text is creating the sentiment.\n\nI will come to it but lets see how further we can explore the texts "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets do POS tagging of our clean tweets\ndef tagging(text):\n    tag = nltk.pos_tag(word_tokenize(text))\n    return tag","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TweetData['Pos_Clean'] = TweetData['Clean_tweet'].apply(lambda x : tagging(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TweetData[['Clean_tweet','Pos_Clean']].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets See Frequency Distribution of just Adjectives in clean tweet from positive negative and neutal tweets\nallowed_word_type = [\"JJ\",\"JJR\",\"JJS\"]\nall_words = []\nfor pos in TweetData['Pos_Clean']:\n    for w in pos:\n        if w[1] in allowed_word_type:\n            all_words.append(w[0])\nAll_words_freq = nltk.FreqDist(all_words)\nFreq_word_DF = pd.DataFrame({\"Data\":All_words_freq.most_common(15)})\nFreq_word_DF['Words'] = Freq_word_DF['Data'].apply(lambda x : x[0])\nFreq_word_DF['freq'] = Freq_word_DF['Data'].apply(lambda x : x[1])\nsns.set()\nsns.set(style='whitegrid', rc={\"grid.linewidth\": 0.2})\nsns.set_context(\"paper\", font_scale=2)\nfig=plt.figure(figsize =(20,8),dpi=50)\nax = sns.barplot('Words','freq',data = Freq_word_DF)\nax.set_title(\"Adjective words frequncy distribution\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The code worked above lets check for all types of setiment words:\nall_words = []\nallowed_word_type = [\"JJ\",\"JJR\",\"JJS\"]\nfor pos in TweetData[TweetData['sentiment']=='positive']['Pos_Clean']:\n    for w in pos:\n        if w[1] in allowed_word_type:\n            all_words.append(w[0])\nAll_words_freq = nltk.FreqDist(all_words)\nFreq_word_DF = pd.DataFrame({\"Data\":All_words_freq.most_common(15)})\nFreq_word_DF['Words'] = Freq_word_DF['Data'].apply(lambda x : x[0])\nFreq_word_DF['freq'] = Freq_word_DF['Data'].apply(lambda x : x[1])\nsns.set()\nsns.set(style='whitegrid', rc={\"grid.linewidth\": 0.2})\nsns.set_context(\"paper\", font_scale=2)\nfig=plt.figure(figsize =(20,8),dpi=50)\nax = sns.barplot('Words','freq',data = Freq_word_DF)\nax.set_title(\"Adjective words frequncy distribution for Positive Tweets\")\n\n# Negative words\nfor pos in TweetData[TweetData['sentiment']=='negative']['Pos_Clean']:\n    for w in pos:\n        if w[1] in allowed_word_type:\n            all_words.append(w[0])\nAll_words_freq = nltk.FreqDist(all_words)\nFreq_word_DF = pd.DataFrame({\"Data\":All_words_freq.most_common(15)})\nFreq_word_DF['Words'] = Freq_word_DF['Data'].apply(lambda x : x[0])\nFreq_word_DF['freq'] = Freq_word_DF['Data'].apply(lambda x : x[1])\nsns.set()\nsns.set(style='whitegrid', rc={\"grid.linewidth\": 0.2})\nsns.set_context(\"paper\", font_scale=2)\nfig=plt.figure(figsize =(20,8),dpi=50)\nax = sns.barplot('Words','freq',data = Freq_word_DF)\nax.set_title(\"Adjective words frequncy distribution for Negative Tweets\")\n\n# neutral words\nfor pos in TweetData[TweetData['sentiment']=='neutral']['Pos_Clean']:\n    for w in pos:\n        if w[1] in allowed_word_type:\n            all_words.append(w[0])\nAll_words_freq = nltk.FreqDist(all_words)\nFreq_word_DF = pd.DataFrame({\"Data\":All_words_freq.most_common(15)})\nFreq_word_DF['Words'] = Freq_word_DF['Data'].apply(lambda x : x[0])\nFreq_word_DF['freq'] = Freq_word_DF['Data'].apply(lambda x : x[1])\nsns.set()\nsns.set(style='whitegrid', rc={\"grid.linewidth\": 0.2})\nsns.set_context(\"paper\", font_scale=2)\nfig=plt.figure(figsize =(20,8),dpi=50)\nax = sns.barplot('Words','freq',data = Freq_word_DF)\nax.set_title(\"Adjective words frequncy distribution for Neutral Tweets\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TweetData[TweetData['sentiment']=='negative'].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Observations:\nThis adjective process does not provide great results \nwhat seems to me that for positive tweets adjective have great impact , for negative the verbs like dont,cant , and some adjactive like sad, sorry are differanciating \nTherefore in our analysis we will take this into our account when designing the model\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets make some word clouds:\nfrom wordcloud import WordCloud,STOPWORDS \nstopwords = set(STOPWORDS) \ncomment_words = ' '\nfor text in TweetData['Clean_tweet']:\n    for words in word_tokenize(text): \n        comment_words = comment_words + words + ' '\nwordcloud = WordCloud(width = 800, height = 800, \n                background_color ='white', \n                stopwords = stopwords, \n                min_font_size = 10).generate(comment_words)\nsns.set()\nplt.figure(figsize = (8, 8), facecolor = None,dpi=100) \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets make some word clouds for positive Negative and Neural text:\nfrom wordcloud import WordCloud,STOPWORDS \nstopwords = set(STOPWORDS) \ncomment_words = ' '\nfor text in TweetData[TweetData['sentiment']=='positive']['Clean_tweet']:\n    for words in word_tokenize(text): \n        comment_words = comment_words + words + ' '\nwordcloud = WordCloud(width = 800, height = 800, \n                background_color ='white', \n                stopwords = stopwords, \n                min_font_size = 10).generate(comment_words)\nsns.set()\nplt.figure(figsize = (8, 8), facecolor = None,dpi=100)\nplt.title(\"Positive Tweets word Cloud\")\nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0)\n\n\ncomment_words = ' '\nfor text in TweetData[TweetData['sentiment']=='negative']['Clean_tweet']:\n    for words in word_tokenize(text): \n        comment_words = comment_words + words + ' '\nwordcloud = WordCloud(width = 800, height = 800, \n                background_color ='white', \n                stopwords = stopwords, \n                min_font_size = 10).generate(comment_words)\nsns.set()\nplt.figure(figsize = (8, 8), facecolor = None,dpi=100)\nplt.title(\"Negative Tweets word Cloud\")\nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0)\n\n\ncomment_words = ' '\nfor text in TweetData[TweetData['sentiment']=='neutral']['Clean_tweet']:\n    for words in word_tokenize(text): \n        comment_words = comment_words + words + ' '\nwordcloud = WordCloud(width = 800, height = 800, \n                background_color ='white', \n                stopwords = stopwords, \n                min_font_size = 10).generate(comment_words)\nsns.set()\nplt.figure(figsize = (8, 8), facecolor = None,dpi=100)\nplt.title(\"Neutral Tweets word Cloud\")\nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# **Looking into the Selected Tweets**","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cleaning the Selected Tweets:\n# stop_words = set(stopwords.words('english'))\ndef clean_tweets_selected(x):\n    # removing the hyperlinks\n    clean1 = re.sub('https?://[A-Za-z0-9./]+','',x)\n    # removing the hashtags\n    clean2 = re.sub('#[A-Za-z0-9]+','',clean1)\n    # removing @\n    clean3 = re.sub('@[A-Za-z0-9]','',clean2)\n    # removing punctuations and lower case conversion\n    clean4 = re.sub(r'[^\\w\\s]','',clean3).lower()\n    words = word_tokenize(clean4)\n    # removing stopwords\n    words = [w for w in words if not w in stop_words]\n    sent = ' '.join(words)\n    return sent","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TweetData['Clean_Selected_Tweets'] = TweetData['selected_text'].apply(lambda x: clean_tweets_selected(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TweetData.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# All Selected word Freqency curve with out any pos tagging filter\nAll_words = []\nfor sent in TweetData['Clean_Selected_Tweets']:\n    for word in word_tokenize(sent):\n        All_words.append(word)\nAll_words_freq = nltk.FreqDist(All_words)\nFreq_word_DF = pd.DataFrame({\"Data\":All_words_freq.most_common(15)})\nFreq_word_DF['Words'] = Freq_word_DF['Data'].apply(lambda x : x[0])\nFreq_word_DF['freq'] = Freq_word_DF['Data'].apply(lambda x : x[1])\nsns.set()\nsns.set(style='whitegrid', rc={\"grid.linewidth\": 0.2})\nsns.set_context(\"paper\", font_scale=2)\nfig=plt.figure(figsize =(20,8),dpi=50)\nsns.barplot('Words','freq',data = Freq_word_DF)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets See only selected words with one and two words only\nAll_words = []\nfor sent in TweetData['selected_text']:\n    if len(word_tokenize(sent))<3:\n        for word in word_tokenize(sent):\n            All_words.append(word)\nAll_words_freq = nltk.FreqDist(All_words)\nFreq_word_DF = pd.DataFrame({\"Data\":All_words_freq.most_common(15)})\nFreq_word_DF['Words'] = Freq_word_DF['Data'].apply(lambda x : x[0])\nFreq_word_DF['freq'] = Freq_word_DF['Data'].apply(lambda x : x[1])\nsns.set()\nsns.set(style='whitegrid', rc={\"grid.linewidth\": 0.2})\nsns.set_context(\"paper\", font_scale=2)\nfig=plt.figure(figsize =(20,8),dpi=50)\nsns.barplot('Words','freq',data = Freq_word_DF)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observations:\nMost of the single and double letter words are adjectives lets do POS tagging these small selected text and check pos distributions"},{"metadata":{"trusted":true},"cell_type":"code","source":"TweetData['Pos_selected_clean'] = TweetData['Clean_Selected_Tweets'].apply(lambda x : tagging(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TweetData.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Pos = []\nfor pos in TweetData['Pos_selected_clean']:\n    for po in pos:\n        Pos.append(po[1])\n\nplt.figure(figsize=(12,6),dpi=100)\nsns.set_context(font_scale=.5)\nsns.countplot(Pos, facecolor=(0, 0, 0, 0),\n                   linewidth=2,\n                   edgecolor=sns.color_palette(\"muted\", 2))        \n\nplt.xticks(rotation=70)        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The count of selected text with less than 3 words and check the pos for these texts**\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"number of text with less than 3 selected text length: \",len(TweetData[TweetData['Select_text_words'] < 3]))\nPos = []\nfor pos in TweetData[TweetData['Select_text_words'] < 3]['Pos_selected_clean']:\n    for po in pos:\n        Pos.append(po[1])\n\nplt.figure(figsize=(12,6),dpi=100)\nsns.set_context(font_scale=.5)\nsns.countplot(Pos, facecolor=(0, 0, 0, 0),\n                   linewidth=2,\n                   edgecolor=sns.color_palette(\"muted\", 2))        \n\nplt.xticks(rotation=70)      ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Observations:\n**Most of the selected text are either adjective or common noun**\n**If we look into the selected text of two and less its dominated by singular common noun or adjective**\n\nNext I will do one check : I will select NN and NNS and JJ from clean text and same from the selected clean text, then I will check the differnece in number of these .. if its near to zero means all or most of the NN and NNS and JJ are in selected text.. which will give me guidance where to go furher "},{"metadata":{"trusted":true},"cell_type":"code","source":"TweetData.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"selected_pos =  [\"JJ\",\"NN\",\"NNS\"]\ndef pos_filter(text):\n    con = 0\n    for w in text:\n        if w[1] in selected_pos:\n            con+= 1\n    return con","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TweetData['count_text'] = TweetData['Pos_Clean'].apply(lambda x: pos_filter(x))\nTweetData['Count_selected_text'] = TweetData['Pos_selected_clean'].apply(lambda x: pos_filter(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TweetData.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Line plot for both counts above for different tweet sentiments , also for those which has only less that three words in selected text\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(TweetData)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,6),dpi=100)\nsns.set_context('paper',font_scale=1)\nsns.lineplot(x=TweetData.index[0:50],y = TweetData['count_text'][0:50])\nsns.lineplot(x=TweetData.index[0:50],y = TweetData['Count_selected_text'][0:50])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TweetDatasmall =  TweetData[TweetData['Select_text_words'] < 3]\nplt.figure(figsize=(12,6),dpi=100)\nsns.set_context('paper',font_scale=1)\nsns.lineplot(x=TweetDatasmall.index[0:50],y = TweetDatasmall['count_text'][0:50])\nsns.lineplot(x=TweetDatasmall.index[0:50],y = TweetDatasmall['Count_selected_text'][0:50])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Even for less than 3 words in selected text we can see significant gap.. perhaps we can apply double fitler one is finding adjective and nouns and filtering via most frequent words "},{"metadata":{"trusted":true},"cell_type":"code","source":"# live curve for less that 3 words seggregated for difference sentiments :\nPositive =  TweetDatasmall[TweetDatasmall['sentiment']=='positive']\nNegative =  TweetDatasmall[TweetDatasmall['sentiment']=='negative']\nNeutral =   TweetDatasmall[TweetDatasmall['sentiment']=='neutral']\n\nplt.figure(figsize=(12,12),dpi=100)\nplt.subplot(3,1,1)\nsns.set_context('paper',font_scale=1)\nsns.lineplot(x=Positive.index[0:50],y = Positive['count_text'][0:50])\nsns.lineplot(x=Positive.index[0:50],y = Positive['Count_selected_text'][0:50])\nplt.title(\"Positive tweets count Line Plots\")\nplt.subplot(3,1,2)\nsns.set_context('paper',font_scale=1)\nsns.lineplot(x=Negative.index[0:50],y = Negative['count_text'][0:50])\nsns.lineplot(x=Negative.index[0:50],y = Negative['Count_selected_text'][0:50])\nplt.title(\"Negative tweets count Line Plots\")\nplt.subplot(3,1,3)\nsns.set_context('paper',font_scale=1)\nsns.lineplot(x=Neutral.index[0:50],y = Neutral['count_text'][0:50])\nsns.lineplot(x=Neutral.index[0:50],y = Neutral['Count_selected_text'][0:50])\nplt.title(\"Neutral tweets count Line Plots\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Its a quite surprise for me but for neutral tweets are exactlly following  the JJ and NN rule in mjority of cases for selected tweet less than 3"},{"metadata":{},"cell_type":"markdown","source":"**Looking at N Grams **"},{"metadata":{"trusted":true},"cell_type":"code","source":"# For N grams first I would like to Clean the tweets with out removing stop words\ndef clean_tweets_N(x):\n    # removing the hyperlinks\n    clean1 = re.sub('https?://[A-Za-z0-9./]+','',x)\n    # removing the hashtags\n    clean2 = re.sub('#[A-Za-z0-9]+','',clean1)\n    # removing @\n    clean3 = re.sub('@[A-Za-z0-9]','',clean2)\n    # removing punctuations and lower case conversion\n    clean4 = re.sub(r'[^\\w\\s]','',clean3).lower()\n#     words = word_tokenize(clean4)\n#     # removing stopwords\n#     words = [w for w in words if not w in stop_words]\n#     sent = ' '.join(words)\n    return clean4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TweetData['Clean_for_N-gram'] = TweetData['text'].apply(lambda x : clean_tweets_N(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.util import ngrams\ndef extract_ngrams(data, num):\n    n_grams = ngrams(nltk.word_tokenize(data), num)\n    return [ ' '.join(grams) for grams in n_grams]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TweetData['Ngram-2'] = TweetData['Clean_for_N-gram'].apply(lambda x : extract_ngrams(x,2))\nTweetData['Ngram-3'] = TweetData['Clean_for_N-gram'].apply(lambda x : extract_ngrams(x,3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TweetData.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting Ngram-2\nAll_ngram = []\nfor sent in TweetData['Ngram-2']:\n    for gram in sent:\n        All_ngram.append(gram)\nAll_ngram_freq = nltk.FreqDist(All_ngram)\nFreq_word_DF = pd.DataFrame({\"Data\":All_ngram_freq.most_common(15)})\nFreq_word_DF['2-Grams'] = Freq_word_DF['Data'].apply(lambda x : x[0])\nFreq_word_DF['freq'] = Freq_word_DF['Data'].apply(lambda x : x[1])\nsns.set()\n# sns.set(style='whitegrid', rc={\"grid.linewidth\": 0.2})\nsns.set_context(\"paper\", font_scale=1)\nfig=plt.figure(figsize =(12,8),dpi=100)\nsns.barplot('2-Grams','freq',data = Freq_word_DF,dodge=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting Ngram-3\nAll_ngram = []\nfor sent in TweetData['Ngram-3']:\n    for gram in sent:\n        All_ngram.append(gram)\nAll_ngram_freq = nltk.FreqDist(All_ngram)\nFreq_word_DF = pd.DataFrame({\"Data\":All_ngram_freq.most_common(15)})\nFreq_word_DF['3-Grams'] = Freq_word_DF['Data'].apply(lambda x : x[0])\nFreq_word_DF['freq'] = Freq_word_DF['Data'].apply(lambda x : x[1])\nsns.set()\n# sns.set(style='whitegrid', rc={\"grid.linewidth\": 0.2})\nsns.set_context(\"paper\", font_scale=1)\nfig=plt.figure(figsize =(12,8),dpi=100)\nsns.barplot('3-Grams','freq',data = Freq_word_DF,dodge=False)\nplt.xticks(rotation=70)  ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}