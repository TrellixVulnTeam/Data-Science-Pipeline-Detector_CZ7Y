{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Glove + RN","metadata":{}},{"cell_type":"markdown","source":"En esta etapa del proceso comenzaremos a implementar una serie de modelos para analizar su comportamiento ante la misma base de datos. De tal manera, que podamos identificar las diferencias y capacidades de cada uno.","metadata":{}},{"cell_type":"code","source":"NB_START_EPOCHS = 10  # Número de iteraciones de entrenamiento\nBATCH_SIZE = 512  # Tamaño de grupos usados en entrenamiento\nGLOVE_DIM = 100  # Número de dimensiones los embeddings GloVe\nseq_length = 33","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Dense,Input,Activation,Flatten,Embedding,GlobalAveragePooling1D,Dropout,LSTM,Conv1D\nfrom tensorflow.keras.models import Model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport string\nfrom wordcloud import WordCloud, STOPWORDS\nimport tensorflow as tf\nimport missingno as msno\nfrom collections import defaultdict\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nimport json\n\n%matplotlib inline","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Reference https://www.kaggle.com/parulpandey/eda-and-preprocessing-for-bert\n\ndef clean(tweet):\n    tweet = str(tweet)\n\n    tweet=tweet.lower()\n\n    #Remove html tags\n    tweet=re.sub('<.*?>','',tweet)\n\n    #Remove text in square brackets\n    tweet=re.sub('\\[.*?\\]','',tweet)\n\n    #Remove hyperlinks\n    tweet=re.sub('https?://\\S+|www\\.\\S+','',tweet)\n\n\n    return tweet","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/tweet-sentiment-extraction/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/tweet-sentiment-extraction/test.csv\")\ntarget = train['sentiment']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_model = train.copy()\ntrain_model.dropna(inplace=True)\ntrain_model['clean_text']=train_model['text'].apply(clean)\ntrain_model['clean_selected_text']=train_model['selected_text'].apply(clean)\ntrain_model = train_model.reset_index()\ntrain_model.head()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train,X_val,Y_train,Y_val=train_test_split(train_model[['sentiment','clean_text','textID']],train_model['clean_selected_text'],\n                                               test_size=0.2,random_state=42,stratify=train_model['sentiment'])\n\nX_train,X_test,Y_train,Y_test=train_test_split(X_train,Y_train,\n                                               test_size=0.2,random_state=42,stratify=X_train['sentiment'])\n\nX_train.reset_index(inplace=True,drop=True)\nX_val.reset_index(inplace=True,drop=True)\nX_test.reset_index(inplace=True,drop=True)\n\nY_train=Y_train.reset_index(drop=True)\nY_val=Y_val.reset_index(drop=True)\nY_test=Y_test.reset_index(drop=True)\n\nprint('X_train Forma',X_train.shape,' Y_train Forma ',Y_train.shape)\nprint('X_val Forma',X_val.shape,' Y_val Forma ',Y_val.shape)\nprint('X_test Forma',X_test.shape,' Y_test Forma ',Y_test.shape)","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token='UNK',filters='')\ntokenizer.fit_on_texts(X_train['clean_text'].values+' '+X_train['sentiment'].values)\n\nX_train_clean_text = tokenizer.texts_to_sequences(X_train['sentiment'].values+' '+X_train['clean_text'].values)\nX_val_clean_text = tokenizer.texts_to_sequences(X_val['sentiment'].values+' '+X_val['clean_text'].values)\nX_test_clean_text = tokenizer.texts_to_sequences(X_test['sentiment'].values+' '+X_test['clean_text'].values)\n\nX_train_tokens = tf.keras.preprocessing.sequence.pad_sequences(X_train_clean_text,maxlen=seq_length,padding='post')\nX_val_tokens = tf.keras.preprocessing.sequence.pad_sequences(X_val_clean_text,maxlen=seq_length,padding='post')\nX_test_tokens = tf.keras.preprocessing.sequence.pad_sequences(X_test_clean_text,maxlen=seq_length,padding='post')\n\nprint('The shape of X_train_tokens ',X_train_tokens.shape)\nprint('The shape of X_val_tokens ',X_val_tokens.shape)\nprint('The shape of X_test_tokens ',X_test_tokens.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Una vez tokenizados los datos, es necesario identificar el entero asignado a cada sentimiento","metadata":{}},{"cell_type":"code","source":"print(X_train_tokens.T[0][0:3])\nprint(X_train['sentiment'].values[0:3])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Con los posibles sentimiento identificados, es necesario crear un diccionario para poder reperarlos o procesarlos en un futuro","metadata":{}},{"cell_type":"code","source":"# Creación de indices\n\ndef get_start_end_index(X_data,Y_data):\n    start_index=np.zeros((X_data.shape[0],32),dtype='int32')\n    end_index=np.zeros((X_data.shape[0],32),dtype='int32')\n\n    for k in range(X_data.shape[0]):\n        tx1=\" \".join(X_data['clean_text'][k].split())\n        tx2=\" \".join(Y_data[k].split())\n        # Se encuentra en indice de inicio y el de finalización\n        idx=tx1.find(tx2)\n\n        # Se insertan unos por cada caracter presente\n        chars=np.zeros(len(tx1))\n        chars[idx:idx+len(tx2)]=1\n\n        # Creación de offsets con (inicio, fin) para cada palabra \n        offsets=[]\n        j=0\n        for i in tx1.split():\n            offsets.append((j,j+len(i)+1))\n            j+=len(i)+1\n\n        vals=[]\n        for i,(o1,o2) in enumerate(offsets):\n            if(sum(chars[o1:o2])>0):\n                vals.append(i)\n\n        if(len(vals)>0 and len(vals)<=32):\n            start_index[k,vals[0]]=1\n            end_index[k,vals[-1]]=1\n        else:\n            start_index[k,0]=1\n            end_index[k,-1]=1\n    return start_index,end_index","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_tr_1,Y_tr_2=get_start_end_index(X_train,Y_train)\nY_val_1,Y_val_2=get_start_end_index(X_val,Y_val)\nY_te_1,Y_te_2=get_start_end_index(X_test,Y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train['clean_text'][129]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_train[129]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_tokens[129]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_tr_1[129]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_tr_2[129]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embeddings_index = {}\nf = open('/kaggle/input/glove-global-vectors-for-word-representation/glove.twitter.27B.100d.txt', encoding='utf-8')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\nprint('%s Vectores de palabras encontrados.' % len(embeddings_index))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab_size=len(tokenizer.word_index)+1\nprint(vocab_size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_matrix = np.zeros((vocab_size, GLOVE_DIM ))\nfor word, i in tokenizer.word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:    \n        embedding_matrix[i] = embedding_vector","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nif not os.path.exists('./model-glove'):\n    os.makedirs('./model-glove')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Red neuronal convolucional\n\ninput_layer=Input((seq_length,),name='input')\nembedding_layer=Embedding(vocab_size,100,weights=[embedding_matrix],input_length=seq_length,trainable=False)(input_layer)\n\nconv1d=Conv1D(6,2,kernel_initializer=tf.keras.initializers.glorot_uniform(seed=20),name='conv1d')(embedding_layer)\ndropout=Dropout(0.2,name=\"drop_out\")(conv1d)\nflatten=Flatten(name='flatten')(dropout)\n\noutput1=Dense(32,activation='softmax',kernel_initializer=tf.keras.initializers.glorot_uniform(seed=45),name='output1')(flatten)\n\noutput2=Dense(32,activation='softmax',kernel_initializer=tf.keras.initializers.glorot_uniform(seed=45),name='output2')(flatten)\n\nsimpleNeural=Model(inputs=[input_layer],outputs=[output1,output2])\n\nsimpleNeural.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import TensorBoard\n%load_ext tensorboard\n\nlog_dir='./model-glove/logs'\ntensorboard_callback = TensorBoard(log_dir=log_dir,histogram_freq=1, write_graph=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ModelCheckpoint\n\nfilepath=\"./model-glove/weights-{epoch:02d}-{val_loss:.4f}.hdf5\"\ncheckpoint = ModelCheckpoint(filepath=filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"simpleNeural.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n              loss=tf.keras.losses.CategoricalCrossentropy())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"callback=[tensorboard_callback,checkpoint]\nsimpleNeural.fit(X_train_tokens,[Y_tr_1,Y_tr_2],batch_size=32,epochs=20,callbacks=callback,\n           validation_data=(X_val_tokens,[Y_val_1,Y_val_2]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.utils.plot_model(simpleNeural, './model-glove/model.png',show_shapes=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Metrica definida\n\ndef jaccard(str1, str2):\n  a = set(str1.lower().split()) \n  b = set(str2.lower().split())\n  c = a.intersection(b)\n  return float(len(c)) / (len(a) + len(b) - len(c))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"st_idx,end_idx=simpleNeural.predict(X_test_tokens,batch_size=32,verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_jaccard(st_idx,end_idx,X,Y):\n    all_jaccard=[]\n    df=pd.DataFrame(columns=['clean_text','selected_text','predicted','jaccard','sentiment'])\n    for i in range(len(st_idx)):\n        initial=np.argmax(st_idx[i])\n        final=np.argmax(end_idx[i])\n        sent2=\" \".join(X['clean_text'][i].split()[initial:final+1])\n        sent1=Y[i] \n        df.loc[i,'clean_text']=X['clean_text'][i]   \n        df.loc[i,'selected_text']=sent1\n        df.loc[i,'predicted']=sent2\n        df.loc[i,'sentiment']=X['sentiment'][i]\n        jaccard_score=jaccard(sent1,sent2)\n        df.loc[i,'jaccard']=jaccard_score\n        all_jaccard.append(jaccard_score)\n    return np.mean(np.array(all_jaccard)),df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score,df=compute_jaccard(st_idx,end_idx,X_test,Y_test)\nprint(score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}