{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport math\n\n# CountVectorizer will help calculate word counts\nfrom sklearn.feature_extraction.text import CountVectorizer\n# Import the string dictionary that we'll use to remove punctuation\nimport string\n# Make training/test split\nfrom sklearn.model_selection import train_test_split\n\n\ndef build_vocab(cv, count_df, train, threshold):\n    neg_count_df, pos_count_df, neutral_count_df = count_df\n    neg_train, pos_train, neutral_train = train\n\n    # Create dictionaries of the words within each sentiment group, where the values are the proportions of tweets that\n    # contain those words\n\n    pos_words = {}\n    neutral_words = {}\n    neg_words = {}\n\n    for k in cv.get_feature_names():\n        pos = pos_count_df[k].sum()\n        neutral = neutral_count_df[k].sum()\n        neg = neg_count_df[k].sum()\n\n        pos_words[k] = pos/pos_train.shape[0]\n        neutral_words[k] = neutral/neutral_train.shape[0]\n        neg_words[k] = neg/neg_train.shape[0]\n\n    # We need to account for the fact that there will be a lot of words used in tweets of every sentiment.\n    # Therefore, we reassign the values in the dictionary by subtracting the proportion of tweets in the other\n    # sentiments that use that word.\n\n    neg_words_adj = {}\n    pos_words_adj = {}\n    neutral_words_adj = {}\n\n    for key, value in neg_words.items():\n        if(neutral_words[key] <= threshold and pos_words[key] <= threshold):\n            neg_words_adj[key] = math.sqrt(np.abs(neg_words[key]))\n        else:\n            neg_words_adj[key] = neg_words[key] - (neutral_words[key] + pos_words[key])\n\n    for key, value in pos_words.items():\n        if(neutral_words[key] <= threshold and neg_words[key] <= threshold):\n            pos_words_adj[key] = math.sqrt(np.abs(pos_words[key]))\n        else:\n            pos_words_adj[key] = pos_words[key] - (neutral_words[key] + neg_words[key])\n\n\n    for key, value in neutral_words.items():\n        if(pos_words[key] == threshold and neg_words[key] == threshold):\n            neutral_words_adj[key] = math.sqrt(np.abs(neutral_words[key]))\n        else:\n            neutral_words_adj[key] = neutral_words[key] - (neg_words[key] + pos_words[key])\n\n    return (neg_words_adj, pos_words_adj, neutral_words_adj)\n\ndef calculate_selected_text(pos_words_adj, neg_words_adj, df_row, tol = 0):\n        tweet = df_row['text']\n        sentiment = df_row['sentiment']\n\n        if(sentiment == 'neutral'):\n            return tweet\n        elif(sentiment == 'positive'):\n            dict_to_use = pos_words_adj # Calculate word weights using the pos_words dictionary\n        elif(sentiment == 'negative'):\n            dict_to_use = neg_words_adj # Calculate word weights using the neg_words dictionary\n\n        words = tweet.split()\n        words_len = len(words)\n        subsets = [words[i:j+1] for i in range(words_len) for j in range(i,words_len)]\n\n        score = 0\n        selection_str = '' # This will be our choice\n        lst = sorted(subsets, key = len) # Sort candidates by length\n\n        for i in range(len(subsets)):\n\n            new_sum = 0 # Sum for the current substring\n\n            # Calculate the sum of weights for each word in the substring\n            for p in range(len(lst[i])):\n                if(lst[i][p].translate(str.maketrans('','',string.punctuation)) in dict_to_use.keys()):\n                    new_sum += dict_to_use[lst[i][p].translate(str.maketrans('','',string.punctuation))]\n\n            # If the sum is greater than the score, update our current selection\n            if(new_sum > score + tol):\n                score = new_sum\n                selection_str = lst[i]\n                #tol = tol*5 # Increase the tolerance a bit each time we choose a selection\n\n        # If we didn't find good substrings, return the whole text\n        if(len(selection_str) == 0):\n            selection_str = words\n\n        return ' '.join(selection_str)\n\ndef jaccard(str1, str2):\n    a = set(str1.lower().split())\n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))\n\ndef which_longer(truth, prediction):\n    truth_set = set(truth.lower().split())\n    pred_set = set(prediction.lower().split())\n\n    return float(len(pred_set) - len(truth_set))\n\ndef load_data():\n    # Import datasets\n    train = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv')\n    test = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv')\n    sample = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/sample_submission.csv')\n    \n    # The row with index 13133 has NaN text, so remove it from the dataset\n    train[train['text'].isna()]\n\n    train.drop(314, inplace = True)\n\n    # Make all the text lowercase - casing doesn't matter when\n    # we choose our selected text.\n    train['text'] = train['text'].apply(lambda x: x.lower())\n    test['text'] = test['text'].apply(lambda x: x.lower())\n    return train, test, sample\n\n\ndef main():\n    # tol = 0.001\n    tol = 0.0009\n    train, test, sample = load_data()\n    # Modify code here to implent k-fold x validation\n    K = 10\n    jaccard_scores = []\n    vocab_weights = []\n    indexes = np.arange(train.shape[0])\n    np.random.shuffle(indexes)\n    print(\"Indexes shape: \" + str(indexes.shape))\n    print(\"Train shape: \" + str(train.shape))\n\n    for group in range(K):\n        group_start = int(group * (indexes.shape[0] / K ))\n        group_end = int((group + 1) * (indexes.shape[0] / K ))\n        #print(\"Group size: \" + str(group_end - group_start))\n        X_train_part_1 = train.iloc[indexes[:group_start]] # from 0 to start of group\n        X_train_part_2 = train.iloc[indexes[group_end:]] # from end of group to end of data\n        X_train = pd.concat([X_train_part_1, X_train_part_2])\n        #print(X_train_part_1.shape, X_train_part_2.shape, X_train.shape)\n        X_val = train.iloc[group_start:group_end]\n        \n        #X_train, X_val = train_test_split(train, train_size = 0.80, random_state = 0)\n\n        pos_train = X_train[X_train['sentiment'] == 'positive']\n        neutral_train = X_train[X_train['sentiment'] == 'neutral']\n        neg_train = X_train[X_train['sentiment'] == 'negative']\n        \n        # Use CountVectorizer to get the word counts within each dataset\n        cv = CountVectorizer(max_df=0.95, min_df=4, max_features=3000, stop_words='english')\n\n        # X_train_cv = cv.fit_transform(X_train['text'])\n        cv.fit_transform(X_train['text'])\n\n        X_pos = cv.transform(pos_train['text'])\n        X_neutral = cv.transform(neutral_train['text'])\n        X_neg = cv.transform(neg_train['text'])\n\n        pos_count_df = pd.DataFrame(X_pos.toarray(), columns=cv.get_feature_names())\n        neutral_count_df = pd.DataFrame(X_neutral.toarray(), columns=cv.get_feature_names())\n        neg_count_df = pd.DataFrame(X_neg.toarray(), columns=cv.get_feature_names())\n\n        count_df = (neg_count_df, pos_count_df, neutral_count_df)\n        train_df = (neg_train, pos_train, neutral_train)\n\n        threshold = 0\n        neg_words_adj, pos_words_adj, neutral_words_adj = build_vocab(cv, count_df, train_df, threshold)\n        vocab_weights.append({\n            'neg': neg_words_adj,\n            'pos': pos_words_adj,\n            'neu': neutral_words_adj\n        })\n        pd.options.mode.chained_assignment = None\n\n        X_val['predicted_selection'] = ''\n\n        for index, row in X_val.iterrows():\n            selected_text = calculate_selected_text(pos_words_adj, neg_words_adj, row, tol)\n            X_val.loc[X_val['textID'] == row['textID'], ['predicted_selection']] = selected_text\n\n        X_val['which_longer'] = X_val.apply(lambda x: which_longer(x['selected_text'], x['predicted_selection']), axis = 1)\n        X_val['jaccard'] = X_val.apply(lambda x: jaccard(x['selected_text'], x['predicted_selection']), axis = 1)\n        jaccard_scores.append(np.mean(X_val['jaccard']))\n        print('-------------------------------- K = ' + str(group + 1) + ' ---------------------------------------------')\n        print('The jaccard score for the validation set is:', np.mean(X_val['jaccard']))\n        print('The selected text for negative is on average {} words smaller'.format(str(np.mean((X_val[X_val['sentiment'] == 'negative'])['which_longer']))))\n        print('The selected text for positive is on average {} words smaller'.format(str(np.mean((X_val[X_val['sentiment'] == 'positive'])['which_longer']))))\n        print('The selected text for neutral is on average {} words smaller'.format(str(np.mean((X_val[X_val['sentiment'] == 'neutral'])['which_longer']))))\n    print(\"Average Jaccard score is: \" + str(sum(jaccard_scores)/len(jaccard_scores)))\n\n    max_vocab_weights = vocab_weights[jaccard_scores.index(max(jaccard_scores))]\n    neg_words_adj = max_vocab_weights['neg']\n    pos_words_adj = max_vocab_weights['pos']\n    neutral_words_adj = max_vocab_weights['neu']\n\n    # pos_tr = train[train['sentiment'] == 'positive']\n    # neutral_tr = train[train['sentiment'] == 'neutral']\n    # neg_tr = train[train['sentiment'] == 'negative']\n\n    # cv = CountVectorizer(max_df=0.95, min_df=2, max_features=10000, stop_words='english')\n\n    # # final_cv = cv.fit_transform(train['text'])\n    # cv.fit_transform(train['text'])\n\n    # X_pos = cv.transform(pos_tr['text'])\n    # X_neutral = cv.transform(neutral_tr['text'])\n    # X_neg = cv.transform(neg_tr['text'])\n\n    # pos_final_count_df = pd.DataFrame(X_pos.toarray(), columns=cv.get_feature_names())\n    # neutral_final_count_df = pd.DataFrame(X_neutral.toarray(), columns=cv.get_feature_names())\n    # neg_final_count_df = pd.DataFrame(X_neg.toarray(), columns=cv.get_feature_names())\n    \n    \n    # count_df_final = (neg_final_count_df, pos_final_count_df, neutral_final_count_df)\n    # tr = (neg_tr, pos_tr, neutral_tr)\n    # neg_words_adj, pos_words_adj, neutral_words_adj = build_vocab(cv, count_df_final, tr)\n\n\n    for index, row in test.iterrows():\n        selected_text = calculate_selected_text(pos_words_adj, neg_words_adj, row, tol)\n        sample.loc[sample['textID'] == row['textID'], ['selected_text']] = selected_text\n\n\n    sample.to_csv('submission.csv', index = False)\n\nif __name__ == \"__main__\":\n  main()\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}