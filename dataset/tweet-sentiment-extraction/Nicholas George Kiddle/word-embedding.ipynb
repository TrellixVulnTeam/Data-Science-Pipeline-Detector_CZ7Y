{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"code","source":"# Code based on notebook from Nick Koprowicz: https://www.kaggle.com/nkoprowicz/a-simple-solution-using-only-word-counts/notebook\n\nimport pandas as pd\nimport numpy as np\nimport pprint as pp\nfrom gensim.models import Word2Vec\n# CountVectorizer will help calculate word counts\nfrom sklearn.feature_extraction.text import CountVectorizer\n# Optionally, use the sklearn logistic regression model instead of the homebrewed one\nfrom sklearn.linear_model import LogisticRegression\n# Import the string dictionary that we'll use to remove punctuation\nimport string\n# Make training/test split\nfrom sklearn.model_selection import train_test_split\n\n\n\nclass LogisticRegressionCustom():\n\n    def __init__(self, H = 0.00001, batch_size = -1, iters = 200, verbose = False):\n        self.H = H\n        self.batch_size = batch_size\n        self.iters = iters\n        self.verbose = False\n\n    def score(self, X, Y):\n        # Calculate input values for sigmoid function\n        # (if negative, then predict 0)\n        #print(X, W)\n        predictors = np.matmul(X, self.W)\n        #print(\"X?\", np.where(X > 0))\n        #print(\"W?\", np.where(W > 0))\n\n        #print(predictors)\n\n        # Change predictors to 0 or 1 to predict events. (Iif W'x >= 0,\n        # then predict 1, that is the drawing is a 9 not a 4)\n        # Here we use standard decision rule\n        #print(np.where(pred\n        # ictors > 0))\n        predictors = np.where(predictors < 0, 0, 1)\n\n        # Calculate the number of falses (both false positives and false negatives)\n        falses = np.sum(np.abs(Y - predictors.T))\n\n        # Return the accuracy (the fraction of correct trials divided by total trials)\n        1 - (falses / (len(Y)))\n        return 1 - (falses / (len(Y)))\n\n    def predict(self, X):\n        return np.reciprocal((np.exp(-1 * np.matmul(X, self.W)) + 1))\n\n    def fit(self, X, Y):\n        X = np.nan_to_num(X)\n        Y = np.nan_to_num(Y)\n        #print(batch_size)\n\n        # Initialize W (weights) to be zero vector corresponding to columns in attribute mtx\n        W = np.zeros(X.shape[1])\n        batch_num = 0\n        # initialize gradient\n        grad = float('inf')\n\n        if self.batch_size == -1:\n            self.batch_size = X.shape[0]\n\n        # Also need condition to prevent exceeding number of training attributes\n        #while (grad > eps):\n        for j in range(int(self.iters * (X.shape[0] / self.batch_size))):\n\n            # Get the slice of attribute matrix corresponding to batch\n            batch = X[batch_num*self.batch_size:(batch_num+1)*self.batch_size,:]\n\n            # Get vector of etimated outputs using old weight\n            # Since batch has row vectors of attributes, multiply by column vector W\n            # (Versus standard practice of multiplying W transpose by a column vector or attributes)\n            estimated = np.reciprocal((np.exp(-1 * np.matmul(batch, W)) + 1))\n\n            # Get column vector of differences between estimated and actual\n            actual = Y[batch_num*self.batch_size:(batch_num+1)*self.batch_size]\n            #actual = Y\n            diffs = (estimated.T - actual)\n            #print(np.linalg.norm(diffs))\n\n            # Use diffs to construct diagonal matrix, multiply that by batch, then sum along the rows\n            grad = np.sum(np.multiply(diffs, batch.T), axis = 1)\n            #pp.pprint(diffs)\n            #pp.pprint(np.diag(diffs))\n\n            # Adjust weights\n            #print(np.linalg.norm(W), np.linalg.norm(H*grad))\n            W = W - (self.H * grad)\n            # replace grad with the magnitude (L2 norm) of the vector\n            grad = np.linalg.norm(grad)\n            #print(grad, np.linalg.norm(W))\n\n            # Increment batch number\n            batch_num += 1\n            # Check if the next batch will exceed the number of training trials\n            if ((batch_num+1)*self.batch_size - 1 > X.shape[0]):\n                batch_num = 0\n            \n            self.W = W\n            if self.verbose is True:\n                print(self.score(X, Y))\n\n        if self.verbose is True:\n            print(\"Found weights are:\")\n            pp.pprint(W)\n\n\ndef apply_model(model, x):\n    #print(x)\n    sum = np.zeros(50)\n    count = 0\n    for i in range(len(x)):\n        if x[i] in model.wv.vocab:\n            sum += model.wv[x[i]]\n            count += 1\n    \n    #print(sum.shape)\n    #print(count)\n    #if count == 0:\n    #    print(x)\n\n    return np.divide(sum, count)\n\ndef is_positive(x):\n    if x == \"positive\":\n        return 1\n    else:\n        return 0\n\ndef is_negative(x):\n    if x == \"negative\":\n        return 1\n    else:\n        return 0\n\ndef get_logistic(full_train):\n    # build word2vec model\n    #print(((pos_train['text'])[:20]).values.split(' '))\n    text = full_train.apply(lambda x: x['text'].split(), axis = 1)\n\n\n    #text = pos_train['text'].values\n    #split_text = np.where(text, text.split())\n    print(\"Training word embedding...\")\n    model = Word2Vec(text.values,\n                     min_count=2,\n                     window=2,\n                     size=50,\n                     sample=6e-5, \n                     alpha=0.03,\n                     min_alpha=0.0007,\n                     negative=20)\n\n    applied_model = text.apply(lambda x: apply_model(model, x))\n\n    X = np.array(list(applied_model.values))\n    print(\"Done with that!\\n\")\n\n    Ypos = full_train['sentiment'].apply(lambda x: is_positive(x))\n    Ypos = Ypos.values.ravel()\n\n    Yneg = full_train['sentiment'].apply(lambda x: is_negative(x))\n    Yneg = Yneg.values.ravel()\n\n    X = np.nan_to_num(X)\n    Ypos = np.nan_to_num(Ypos)\n    Yneg = np.nan_to_num(Yneg)\n\n    print(\"Fitting logistic regression models...\")\n    # all parameters not specified are set to their defaults\n    posLogisticRegr = LogisticRegression()\n    posLogisticRegr.fit(X, Ypos)\n\n    negLogisticRegr = LogisticRegression()\n    negLogisticRegr.fit(X, Yneg)\n    # Use score method to get accuracy of model\n    pos_score = posLogisticRegr.score(X, Ypos)\n    neg_score = negLogisticRegr.score(X, Yneg)\n    print(\"Done!\\n\")\n    print(\"Positive Score: \" + str(pos_score))\n    print(\"Negative Score: \" + str(neg_score))\n\n    \n    return (posLogisticRegr, negLogisticRegr, model)\n\ndef calculate_selected_text(df_row, positiveModel, negativeModel, word2vecModel, tol = 0):\n        tweet = df_row['text']\n        sentiment = df_row['sentiment']\n\n        if(sentiment == 'neutral'):\n            return tweet\n        elif(sentiment == 'positive'):\n            logisticToUse = positiveModel # Calculate word weights using the pos_words dictionary\n        elif(sentiment == 'negative'):\n            logisticToUse = negativeModel # Calculate word weights using the neg_words dictionary\n\n        words = tweet.split()\n        words_len = len(words)\n        subsets = [words[i:j+1] for i in range(words_len) for j in range(i,words_len)]\n\n        score = 0\n        selection_str = '' # This will be our choice\n        lst = sorted(subsets, key = len) # Sort candidates by length\n\n        failed_subsets = 0\n        for i in range(len(subsets)):\n\n            sum = np.zeros(50)\n            count = 0\n            for j in range(len(lst[i])):\n                if (lst[i][j]) in word2vecModel.wv.vocab:\n                    sum += word2vecModel.wv[lst[i][j]]\n                    count += 1\n            if count > 0:\n                new_score = logisticToUse.predict([np.divide(sum, count)])\n            else:\n                new_score = 0\n                failed_subsets += 1\n            \n            # If the sum is greater than the score, update our current selection\n            if(new_score > score + tol):\n                score = new_score\n                selection_str = lst[i]\n                #tol = tol*5 # Increase the tolerance a bit each time we choose a selection\n        # print(str(failed_subsets) + \" / \" + str(len(subsets)) + str(\" subsets failed\"))\n\n        # If we didn't find good substrings, return the whole text\n        if(len(selection_str) == 0):\n            selection_str = words\n\n        return ' '.join(selection_str)\n\ndef jaccard(str1, str2):\n    a = set(str1.lower().split())\n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))\n\ndef which_longer(truth, prediction):\n    truth_set = set(truth.lower().split())\n    pred_set = set(prediction.lower().split())\n    intersect = truth_set.intersection(pred_set)\n\n    return float(len(truth_set - intersect) + len(pred_set - intersect))\n\ndef load_data():\n    # Import datasets\n    train = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv')\n    test = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv')\n    sample = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/sample_submission.csv')\n    \n    # The row with index 13133 has NaN text, so remove it from the dataset\n    train[train['text'].isna()]\n\n    train.drop(314, inplace = True)\n\n    # Make all the text lowercase - casing doesn't matter when\n    # we choose our selected text.\n    train['text'] = train['text'].apply(lambda x: x.lower())\n    test['text'] = test['text'].apply(lambda x: x.lower())\n    return train, test, sample\n\n\ndef main():\n    tol = 0.001\n\n    train, test, sample = load_data()\n    # Use K-fold validation\n    K = 5\n    indexes = np.arange(train.shape[0])\n    np.random.shuffle(indexes)\n    bestScore = 0\n    bestModel = None\n\n    for group in range(K):\n        group_start = int(group * (indexes.shape[0] / K ))\n        group_end = int((group + 1) * (indexes.shape[0] / K ))\n        #print(\"Group size: \" + str(group_end - group_start))\n        X_train_part_1 = train.iloc[indexes[:group_start]] # from 0 to start of group\n        X_train_part_2 = train.iloc[indexes[group_end:]] # from end of group to end of data\n        X_train = pd.concat([X_train_part_1, X_train_part_2])\n        #print(X_train_part_1.shape, X_train_part_2.shape, X_train.shape)\n        X_val = train.iloc[group_start:group_end]\n        \n        posLogisticModel, negLogisticModel, word2vecModel = get_logistic(X_train)\n\n        pd.options.mode.chained_assignment = None\n \n        X_val['predicted_selection'] = ''\n\n        for index, row in X_val.iterrows():\n            selected_text = calculate_selected_text(row, posLogisticModel, negLogisticModel, word2vecModel)\n            X_val.loc[X_val['textID'] == row['textID'], ['predicted_selection']] = selected_text\n\n        X_val['which_longer'] = X_val.apply(lambda x: which_longer(x['selected_text'], x['predicted_selection']), axis = 1)\n        X_val['jaccard'] = X_val.apply(lambda x: jaccard(x['selected_text'], x['predicted_selection']), axis = 1)\n        print('The jaccard score for the validation set is:', np.mean(X_val['jaccard']))\n        print('The selected text for negative is on average {} words different'.format(str(np.mean((X_val[X_val['sentiment'] == 'negative'])['which_longer']))))\n        print('The selected text for positive is on average {} words different'.format(str(np.mean((X_val[X_val['sentiment'] == 'positive'])['which_longer']))))\n        print('The selected text for neutral is on average {} words different'.format(str(np.mean((X_val[X_val['sentiment'] == 'neutral'])['which_longer']))))\n\n        if np.mean(X_val['jaccard']) > bestScore:\n            bestModel = (posLogisticModel, negLogisticModel, word2vecModel)\n\n    (posLogisticModel, negLogisticModel, word2vecModel) = bestModel\n    \n    for index, row in test.iterrows():\n        selected_text = calculate_selected_text(row, posLogisticModel, negLogisticModel, word2vecModel)\n        sample.loc[sample['textID'] == row['textID'], ['selected_text']] = selected_text\n\n\n    sample.to_csv('submission.csv', index = False)\n\n\nif __name__ == \"__main__\":\n  main()\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}