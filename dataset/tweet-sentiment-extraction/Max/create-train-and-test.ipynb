{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport re \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport shutil\n        \nshutil.copyfile(src = \"../input/bad-words-for-tweets/bad_words.py\", dst = \"../working/bad_words.py\")\nfrom bad_words import whole_words","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"all_data = pd.read_csv(\"/kaggle/input/emotion/text_emotion.csv\")\ntrain = pd.read_csv(\"/kaggle/input/tweet-sentiment-extraction/train.csv\")\nval = pd.read_csv(\"/kaggle/input/tweet-sentiment-extraction/test.csv\")\n\n# we use train + test to test the mapping \ntrain = pd.concat([train, val])\ntrain.dropna(how=\"any\", subset=[\"text\"], inplace=True)\ntrain.reset_index(inplace=True, drop=True)\n\nall_data = all_data.rename(columns={\"tweet_id\" : \"textID\", \"content\" : \"text\"})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_html(text):\n\ttext = re.sub(\"&quot;\", \"'\", text)\n\ttext = re.sub(\"&gt;\", \">\",  text)\n\ttext = re.sub(\"&lt;\", \"<\", text)\n\ttext = re.sub(\"&le;\", \"≤\", text)\n\ttext = re.sub(\"&ge;\", \"≥\", text)\n\ttext = re.sub(\"&amp;\", \"&\", text)\n\treturn text\n\ndef find_all(input_str, search_str):\n    l1 = []\n    length = len(input_str)\n    index = 0\n    while index < length:\n        i = input_str.find(search_str, index)\n        if i == -1:\n            return l1\n        l1.append(i)\n        index = i + 1\n    return l1\n\ninchars = \"abcdefghijklmnopqrstuvwxyzåä*'ö0123456789\"\ndef clean_text(text):\n    text = re.sub( \"'\", \"`\", text)\n    text = remove_html(text)\n    text = re.sub( \"@[a-zA-Z0-9]+\", '', str(text))  # sloppy regex to remove @user \n    for word in whole_words:\n        old_txt = text\n        if word.lower() in text.lower():\n            starts = find_all(text.lower(), word.lower())\n            while len(starts) != 0:\n                start = starts[0]\n                end = start+len(word)\n                # skip the word if the preceding or end character is a number or in the alphabet\n                if len(text[:start]) != 0 and text[:start][-1].lower() in inchars:\n                    starts.remove(start)\n                    continue\n                elif len(text[end:]) != 0 and text[end:][0].lower() in inchars:\n                    starts.remove(start)\n                    continue\n                \n                text = text[:start] + \"****\" + text[end:]\n                starts = find_all(text.lower(), word.lower())\n   \n    # only edge case\n    text = re.sub(\" x x \", ' **** ',  text)    \n    return text\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Add old_text for reference and then clean the data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data[\"old_text\"] = all_data.text\nall_data.text = all_data.text.map(clean_text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Check how well the mapping worked ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"added = 0\nunmapped = 0\nall_texts = train.text.tolist()\nall_ids = train.textID.astype(str).tolist()\nfor idx in range(len(all_data)):\n    text = all_data.text[idx]\n    \n    if text in all_texts:\n        index = all_texts.index(text)\n        all_texts.pop(index)\n        text_id = all_ids.pop(index)\n        all_data.loc[idx, \"aux_id\"] = text_id\n        added += 1\n    else:\n        unmapped += 1\n\nprint(f\"Unmapped:{unmapped} Total:{len(all_data)} Prop: {(added)/len(all_data)}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Santiy check that we got all the text in train+test mapped correctly","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the unique Id's \nauxes = all_data.aux_id.unique().tolist()\n# remove \"nan\" which is the first index \nauxes.pop(0) \n# show the unmapped train+test texts\ntrain[~train.textID.isin(auxes)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data = all_data.replace(r'^\\s*$', np.nan, regex=True)\nall_data = all_data.where(pd.notnull(all_data), None)\n\nindex = 1000000000\nfor idx in range(len(all_data)):\n    if all_data.aux_id[idx] == None:\n        all_data.loc[idx, \"aux_id\"] = f\"p{index}\" \n        index += 1\n\nall_data.rename(columns={\"aux_id\" : \"textID\"}, inplace=True)\nall_data.to_csv(\"all_data.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}