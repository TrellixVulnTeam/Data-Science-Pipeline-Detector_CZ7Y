{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Import modules"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n!pip install twython\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os, sys, gc\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\nimport string\nimport time\nimport re\nimport nltk\nimport spacy\nfrom spacy import displacy\nfrom spacy.pipeline import Sentencizer\nfrom spacy.lang.en import English\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\nfrom textblob import TextBlob\n\n\nnlp = spacy.load(\"en_core_web_sm\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load train and test datasets"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv')\ntest = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Define some helper functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text) # remove any links\n    text = re.sub('&quot;', '\"', text)\n    text = re.sub('&amp;', '&', text)\n    # replace multiple non-latin and non-digit chars into one char. E.g.: '!!!!' into '!'\n    text = re.sub(r'([^a-zA-Z0-9\\s])\\1+', r'\\g<1>', text) \n    # e.g.: replace 'nothing.to.do' into 'nothing. to. do' \n    # in order to interpret them as different tokens for vader.sentimentAnalyzer\n    text = re.sub(r'([?!.])', '\\g<1> ', text) \n    return text.strip()\n\ndef text2postag(train, column, attributes):\n    attr_values = [[] for _ in attributes]\n    for text in train[column].values:\n        for index, attr in enumerate(attributes):\n            attr_values[index].append([])\n        for token in nlp(text):\n            if token.pos_ == 'SPACE':\n                continue\n            for index, attr in enumerate(attributes):\n                attr_values[index][-1].append(getattr(token, attr))\n    for index, attr in enumerate(attributes):\n        train[f'{column}_{attr}'] = attr_values[index]\n\ndef label_sequences(train):\n    target_sequences = []\n    weights = []\n    for index, sequence in enumerate(train.cleaned_text_text.values):\n        selected_sequence = train.loc[index, 'cleaned_selected_text_text']\n        labeled_sequence = np.zeros(len(sequence), dtype=np.int)\n        if not selected_sequence:\n            target_sequences.append(labeled_sequence)\n            weights.append(labeled_sequence)\n            continue\n        local_index = 0\n        while len(sequence) >= local_index + len(selected_sequence):\n            if sequence[local_index : local_index+len(selected_sequence)] == selected_sequence:\n                labeled_sequence[local_index : local_index+len(selected_sequence)] = 1\n                target_sequences.append(labeled_sequence)\n                weights.append(np.ones(len(sequence), dtype=np.float16))\n                break\n            local_index += 1\n        else:\n            local_index = 0\n            while len(sequence) >= local_index + len(selected_sequence):\n                if (sequence[local_index][-len(selected_sequence[0]):] == selected_sequence[0] and \n                    sequence[local_index + len(selected_sequence) - 1][:len(selected_sequence[-1])] == selected_sequence[-1] and \n                    sequence[local_index+1 : local_index+len(selected_sequence)-1] == selected_sequence[1:-1]):\n                    \n                    labeled_sequence[local_index : local_index+len(selected_sequence)] = 1\n                    target_sequences.append(labeled_sequence)\n                    weights.append(np.ones(len(sequence), dtype=np.float16))\n                    if sequence[local_index] == selected_sequence[0]:\n                        weights[-1][local_index] = 0.5\n                    if sequence[local_index + len(selected_sequence) - 1] == selected_sequence[-1]:\n                        weights[-1][local_index + len(selected_sequence) - 1] = 0.5\n                    break\n                local_index += 1\n            else:\n                target_sequences.append(labeled_sequence)\n                weights.append(labeled_sequence)\n    train['target_sequence'] = target_sequences\n    train['target_sequence_weights'] = weights\n    \ndef drop_unlabled_sequences(train):\n    cnt = 0\n    drop_indexes = []\n    for index, seq in enumerate(train.target_sequence.values):\n        if not seq.any():\n            cnt += 1\n            drop_indexes += [index]\n    if drop_indexes:\n        train.drop(drop_indexes, inplace=True, axis=0)\n        train.reset_index(drop=True, inplace=True)\n    print(f'dropped {cnt} samples')\n    \ndef onehot_encode(sequence, set_of_tags, tag2idx):\n    ohe = np.zeros((len(sequence), len(set_of_tags)))\n    for index, tag in enumerate(sequence):\n        if tag in set_of_tags:\n            ohe[index, tag2idx[tag]] = 1\n    return ohe\n\ndef label_encode(sequence, set_of_tags, tag2idx):\n    encoded_seq = []\n    for index, tag in enumerate(sequence):\n        if tag in set_of_tags:\n            encoded_seq.append(tag2idx[tag])\n    return np.array(encoded_seq, dtype=np.int)\n\ndef encode_all(tagged_text, set_of_tags, tag2idx, encode_func=onehot_encode):\n    sequences = []\n    for sequence in tagged_text:\n        sequences.append(encode_func(sequence, set_of_tags, tag2idx))\n    return sequences\n\n\nclass SentimentAnalyzer:\n    \n    def __init__(self, sentiment_analyzer):\n        self.sentiment_analyzer = sentiment_analyzer\n        if type(self.sentiment_analyzer) not in [SentimentIntensityAnalyzer, TextBlob]:\n            raise BaseException('Unknown sentiment analyzer', self.sentiment_analyzer)\n        \n    def __call__(self, text):\n        if isinstance(self.sentiment_analyzer, SentimentIntensityAnalyzer):\n            return self.sentiment_analyzer.polarity_scores(text)['compound']\n        elif isinstance(self.sentiment_analyzer, TextBlob):\n            return TextBlob(text).sentiment.polarity\n            \n            \n\ndef words_polarity_score(words, sentiment, n_gram_window_range=(0, 4), sentiment_analyzers=[SentimentAnalyzer(SentimentIntensityAnalyzer()), \n                                                                                            SentimentAnalyzer(TextBlob('123'))]):\n    \"\"\"\n    n_gram_window: word[i - n_gram_window_range[1]], ..., words[i], ..., word[i + n_gram_window_range[1]]\n    \"\"\"\n    sentiment_kef = 1 if sentiment == 'positive' else -1\n    polarity_scores = []\n    for _ in range(len(sentiment_analyzers)):\n        polarity_scores.append([])\n    for index, word in enumerate(words):\n        for _ in range(len(sentiment_analyzers)):\n            polarity_scores[_].append([])\n        for window in range(*n_gram_window_range):\n            for index_, sentiment_analyzer in enumerate(sentiment_analyzers):\n                polarity_scores[index_][-1].append(sentiment_analyzer(' '.join(words[max(0, index-window):min(len(words), index+window+1)])))\n    return polarity_scores\n\ndef calculate_texts_polarity_score(train, n_gram_window_range=(0, 4), sentiment_analyzers=[SentimentAnalyzer(SentimentIntensityAnalyzer()), \n                                                                                            SentimentAnalyzer(TextBlob('123'))]):\n    \n    column_basename = 'words_polarity_scores'\n    polarity_scores = []\n    for _ in range(len(sentiment_analyzers)):\n        polarity_scores.append([])\n        \n    for words, sentiment in train[['cleaned_text_text', 'sentiment']].values:\n        scores = words_polarity_score(words, sentiment, n_gram_window_range, sentiment_analyzers)\n        for index, scores_ in enumerate(scores):\n            polarity_scores[index].append(scores_)\n            \n    for index, sentiment_analyzer in enumerate(sentiment_analyzers):\n        if isinstance(sentiment_analyzer.sentiment_analyzer, SentimentIntensityAnalyzer):\n            train[f'vader_{column_basename}'] = polarity_scores[index]\n        elif isinstance(sentiment_analyzer.sentiment_analyzer, TextBlob):\n            train[f'textblob_{column_basename}'] = polarity_scores[index]\n#         elif # add your own sentiment analyzer\n    return\n\ndef count_sentences(text):\n    return len(re.split('[.!?]+', text))\n\ndef concat_encoded_features(train, columns):\n    concateneted_features = []\n    for row in train[columns].values:\n        concateneted_features.append(np.concatenate((*row[:-2], [[row[-2]]]*len(row[0]), [[row[-1]]]*len(row[0])), axis=1))\n    return concateneted_features\n\ndef pad_sequence(feature_sequence, target_sequence, target_weight_sequence, max_seq_len, mode='uniform'):\n    \"\"\"\n    mode: uniform - num of zero-padding for pre and post padding is aprxmtly the same\n          pre - ...\n          post - ...\n    \"\"\"\n    if mode == 'uniform':\n        pre_pad_length = (max_seq_len - len(feature_sequence)) // 2\n        pre_pad = [[0]*len(feature_sequence[0])]*pre_pad_length\n        post_pad_length = max_seq_len - len(feature_sequence) - pre_pad_length\n        post_pad = [[0]*len(feature_sequence[0])]*post_pad_length\n        feature_sequence = (feature_sequence,)\n        target_sequence = (target_sequence,)\n        target_weight_sequence = (target_weight_sequence,)\n        if pre_pad:\n            feature_sequence = (pre_pad,) + feature_sequence\n            target_sequence = ([0]*pre_pad_length,) + target_sequence\n            target_weight_sequence = ([1]*pre_pad_length,) + target_weight_sequence\n        if post_pad:\n            feature_sequence += (post_pad,)\n            target_sequence += ([0]*post_pad_length,)\n            target_weight_sequence += ([1]*post_pad_length,)\n        if len(feature_sequence) > 1:\n            feature_sequence = np.concatenate(feature_sequence, axis=0)\n            target_sequence = np.concatenate(target_sequence, axis=0)\n            target_weight_sequence = np.concatenate(target_weight_sequence, axis=0)\n        else:\n            feature_sequence, target_sequence, target_weight_sequence = feature_sequence[0], target_sequence[0], target_weight_sequence[0]\n    elif mode == 'pre':\n        pre_pad_length = (max_seq_len - len(feature_sequence)) // 2\n        pre_pad = [[0]*len(feature_sequence[0])]*pre_pad_length\n        feature_sequence = (feature_sequence,)\n        target_sequence = (target_sequence,)\n        target_weight_sequence = (target_weight_sequence,)\n        if pre_pad:\n            feature_sequence = (pre_pad,) + feature_sequence\n            target_sequence = ([0]*pre_pad_length,) + target_sequence\n            target_weight_sequence = ([1]*pre_pad_length,) + target_weight_sequence\n        if len(feature_sequence) > 1:\n            feature_sequence = np.concatenate(feature_sequence, axis=0)\n            target_sequence = np.concatenate(target_sequence, axis=0)\n            target_weight_sequence = np.concatenate(target_weight_sequence, axis=0)\n        else:\n            feature_sequence, target_sequence, target_weight_sequence = feature_sequence[0], target_sequence[0], target_weight_sequence[0]\n    elif mode == 'post':\n        post_pad_length = (max_seq_len - len(feature_sequence)) // 2\n        post_pad = [[0]*len(feature_sequence[0])]*post_pad_length\n        feature_sequence = (feature_sequence,)\n        target_sequence = (target_sequence,)\n        target_weight_sequence = (target_weight_sequence,)\n        if post_pad:\n            feature_sequence += (post_pad,)\n            target_sequence += ([0]*post_pad_length,)\n            target_weight_sequence += ([1]*post_pad_length,)\n        if len(feature_sequence) > 1:\n            feature_sequence = np.concatenate(feature_sequence, axis=0)\n            target_sequence = np.concatenate(target_sequence, axis=0)\n            target_weight_sequence = np.concatenate(target_weight_sequence, axis=0)\n        else:\n            feature_sequence, target_sequence, target_weight_sequence = feature_sequence[0], target_sequence[0], target_weight_sequence[0]\n    else:\n        raise BaseException(f'Unknown mode: {mode}. Choose one of the following: [\"uniform\",\"pre\", \"post\"]')\n    return feature_sequence, target_sequence, target_weight_sequence\n\ndef pad_sequences(features, target, target_weights, max_seq_len, mode='uniform'):\n    features, target, target_weights = list(zip(*[pad_sequence(features[index], target[index], target_weights[index], max_seq_len, mode) \\\n                                               for index in range(len(features))]))\n    return features, target, target_weights\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Preprocessor:\n\n    def __init__(self, cat_encoding_func, sentiment_analyzers=[SentimentAnalyzer(SentimentIntensityAnalyzer())]):\n        self.isTrain = True\n        self.cat_encoding_func = cat_encoding_func\n        self.sentiment_analyzers = sentiment_analyzers\n    \n    def fit(self, X, y=None):\n        self.isTrain = True\n        return self\n    \n    def transform(self, X):\n        X = X.copy()\n        if self.isTrain:\n            X.fillna('', inplace=True)\n        # drop neutral samples, because we would select the whole text for neutral sentiment text\n        X.drop(X[X.sentiment == 'neutral'].index, inplace=True)\n        X.reset_index(drop=True, inplace=True)\n        # clean texts\n        print('Clean text...')\n        X['cleaned_text'] = X.text.apply(clean_text)\n        print('text2postag...')\n        text2postag(X, 'cleaned_text', ['pos_', 'dep_', 'text'])\n        if self.isTrain:\n            X['cleaned_selected_text'] = X.selected_text.apply(clean_text)\n            text2postag(X, 'cleaned_selected_text', ['pos_', 'dep_', 'text'])\n            print('label_sequences...')\n            label_sequences(X)\n            print('drop_unlabled_sequences...')\n            drop_unlabled_sequences(X) # drop samples where (target_sequence == 0).all()\n            self.set_of_pos_tags = set(tag for tags in X.cleaned_text_pos_.values for tag in tags if tag != 'SPACE')\n            self.pos2idx = dict((tag, index) for index, tag in enumerate(self.set_of_pos_tags))\n            self.idx2pos = dict(enumerate(self.set_of_pos_tags))\n            self.set_of_dep_tags = set(tag for tags in X.cleaned_text_dep_.values for tag in tags if tag != 'SPACE')\n            self.dep2idx = dict((tag, index) for index, tag in enumerate(self.set_of_dep_tags))\n            self.idx2dep = dict(enumerate(self.set_of_dep_tags))\n        print('pos and dep encoding...')\n        X['pos_tag_encoded'] = encode_all(X.cleaned_text_pos_.values, self.set_of_pos_tags, self.pos2idx, self.cat_encoding_func)\n        X['dep_tag_encoded'] = encode_all(X.cleaned_text_dep_.values, self.set_of_dep_tags, self.dep2idx, self.cat_encoding_func)\n        print('calculate_texts_polarity_score...')\n        calculate_texts_polarity_score(X, sentiment_analyzers=self.sentiment_analyzers)\n    \n        # calculate number of sentences\n        X['cleaned_text_num_sents'] = X.cleaned_text.apply(count_sentences)\n        # calculate number of tokens (words + punctuation)\n        X['cleaned_text_num_tokens'] = X.cleaned_text_text.apply(len)\n\n        # min_max scaling\n        if self.isTrain:\n            self.max_num_sents = X.cleaned_text_num_sents.max()\n            self.max_num_tokens = X.cleaned_text_num_tokens.max()\n        X['cleaned_text_num_sents'] = X.cleaned_text_num_sents.values / self.max_num_sents\n        X['cleaned_text_num_tokens'] = X.cleaned_text_num_tokens.values / self.max_num_tokens\n        \n        # concatenate features\n        print('concatenating...')\n        if self.isTrain:\n            self.concatenate_feature_columns = X.columns[X.columns.tolist().index('pos_tag_encoded'):]\n        features = concat_encoded_features(X, self.concatenate_feature_columns)\n        if self.isTrain:\n            target = X.target_sequence.values\n            target_weights = X.target_sequence_weights.values\n            return X, features, target, target_weights\n        return X, features\n        \n        \n    def fit_transform(self, X, y=None):\n        self.fit(X, y)\n        return self.transform(X)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preprocessing train_set"},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocessor = Preprocessor(onehot_encode)\nnew_train, features, target, target_weights = preprocessor.fit_transform(train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Pad sequences"},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_SEQ_LENGTH = max(len(seq) for seq in features)\nprint('MAX_SEQ_LENGTH:', MAX_SEQ_LENGTH)\n\nfeatures, target, target_weights = pad_sequences(features, target, target_weights, MAX_SEQ_LENGTH, mode='uniform')\nfeatures = np.array(features)\ntarget = np.array(target)\ntarget_weights = np.array(target_weights)\ntarget = target.reshape(*target.shape[:2], 1)\ntarget_weights = target_weights.reshape(*target_weights.shape[:2], 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train-test split"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n\ntrain_index, val_index, _, _ = train_test_split(new_train.index, new_train.sentiment.values, test_size=.15, random_state=123, shuffle=True, stratify=new_train.sentiment.values)\ntrain_sequences, val_sequences = features[train_index], features[val_index]\ntrain_target, val_target = target[train_index], target[val_index]\ntrain_target_weights, val_target_weights = target_weights[train_index], target_weights[val_index]\ntrain_sequences.shape, val_sequences.shape, train_target.shape, train_target_weights.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model creation"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import InputLayer, Bidirectional, LSTM, Dense, Dropout, GRU, Activation\n# from keras_contrib.layers import CRF","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Bidirectional(LSTM(units=128, recurrent_dropout=0.2, return_sequences=True), input_shape=features.shape[-2:]))\nmodel.add(Bidirectional(LSTM(units=64, recurrent_dropout=0.2, return_sequences=True)))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(optimizer=tf.keras.optimizers.Adam(0.003), \n              loss=tf.keras.losses.binary_crossentropy, \n              sample_weight_mode='temporal', \n              metrics=[tf.keras.metrics.BinaryAccuracy(), tf.keras.metrics.AUC(name='auc', curve='PR'),\n                      tf.keras.metrics.Precision(name='precision'), tf.keras.metrics.Recall(name='recall')])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(train_sequences, train_target, \n          batch_size=32, epochs=10, \n          validation_data=(val_sequences, val_target, val_target_weights.reshape(*val_target_weights.shape[:2])), \n          sample_weight=train_target_weights.reshape(*train_target_weights.shape[:2]),\n         verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pseudo_features = np.array(concat_encoded_features(new_train, preprocessor.concatenate_feature_columns))\npseudo_features.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_words(model, features, train, index=None):\n    if index is None:\n        index = train.index.values\n    words = [0] * len(features)\n    num_tokens = train.loc[index, 'cleaned_text_num_tokens'].unique()\n    for num_tokens_ in num_tokens:\n        index_ = train[train.loc[index, 'cleaned_text_num_tokens'] == num_tokens_].index.values\n        features_ = np.concatenate(features[index_]).reshape(len(index_), -1, features[0].shape[-1])\n        pred = model(features_).numpy().reshape(len(index_), -1)\n        wrds = [[word for index2, word in enumerate(words_) if pred[index1][index2] >= 0.5] for index1, words_ in enumerate(train.loc[index_, 'cleaned_text_text'].values)]\n        for index1 in range(len(index_)):\n            words[index_[index1]] = wrds[index1]\n            \n    return np.array(words)[index]\n        \n        \n        \ntm = time.time()\npredicted_words = extract_words(model, pseudo_features, new_train)\nprint(time.time() - tm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def jaccard(str1, str2): \n    a = {word.lower() for word in str1}# if word not in string.punctuation}\n    b = {word.lower() for word in str2}# if word not in string.punctuation}\n    if not a and not b:\n        return -1\n#     print(a, b, f'\"{\" \".join(str1)}\"', f'\"{\" \".join(str2)}\"')\n    c = a.intersection(b)\n    return len(c) / (len(a) + len(b) - len(c))\n\ndef evaluate(y_true, y_pred):\n    score = []\n    for i in range(len(y_true)):\n        jac_score = jaccard(y_true[i], y_pred[i])\n        if jac_score == -1:\n            continue\n        score.append(jac_score)\n    return sum(score) / len(score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('all dataset jacard_score: {}, train: {}, val: {}'.format(evaluate(predicted_words, new_train['cleaned_selected_text_text'].values), \n                                                                evaluate(predicted_words[train_index], new_train.loc[train_index, 'cleaned_selected_text_text'].values), \n                                                                evaluate(predicted_words[val_index], new_train.loc[val_index, 'cleaned_selected_text_text'].values)\n                                                               )\n     )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Test prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocessor.isTrain = False\nnew_test, test_features = preprocessor.transform(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Transformers"},{"metadata":{"trusted":true},"cell_type":"code","source":"import transformers\nimport tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MODELNAME = 'bert-base-uncased' #distilbert-base-uncased\ntokenizer = transformers.AutoTokenizer.from_pretrained(MODELNAME)\nBERT = transformers.TFAutoModel.from_pretrained(MODELNAME)\n# tokenizer.tokenize(train.loc[0].text), tokenizer.tokenize('somethere?!?!')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MODELNAME = 'albert-base-v2' # '▁' E.g.: ['▁spent', '▁the', '▁entire',]\ntokenizer = transformers.AutoTokenizer.from_pretrained(MODELNAME)\nBERT = transformers.TFAutoModel.from_pretrained(MODELNAME)\n# tokenizer.tokenize(train.loc[0].text), tokenizer.tokenize('somethere?!?!')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MODELNAME = 'roberta-base' #distilroberta-base # 'Ġ' E.g.: ['Sp', 'ent', 'Ġthe','Ġentire',]\ntokenizer = transformers.AutoTokenizer.from_pretrained(MODELNAME)\nBERT = transformers.TFAutoModel.from_pretrained(MODELNAME)\n# tokenizer.tokenize(train.loc[0].text.lower()), tokenizer.tokenize('somethere')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res = BERT(np.array([tokenizer.convert_tokens_to_ids(tokenizer.tokenize(train.loc[0].text))]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res[0].shape, res[1].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer.tokenize('nothing...i think'), tokenizer.tokenize('pre?!?!else somethere predefined')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer.convert_tokens_to_string(tokenizer.tokenize('nothing?!?!else somethere'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MODELNAME","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentiment_map = {\n        'positive': 3893,\n        'negative': 4997,\n        'neutral': 8699,\n    }\n\ndef clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text) # remove any links\n    text = re.sub('\\s\\s+', ' ', text) # remove any links\n#     text = re.sub('&quot;', '\"', text)\n#     text = re.sub('&amp;', '&', text)\n    # replace multiple non-latin and non-digit chars into one char. E.g.: '!!!!' into '!'\n#     text = re.sub(r'([^a-zA-Z0-9\\s])\\1+', r'\\g<1>', text) \n    # e.g.: replace 'nothing.to.do' into 'nothing. to. do' \n    # in order to interpret them as different tokens for vader.sentimentAnalyzer\n#     text = re.sub(r'([?!.])', '\\g<1> ', text) \n    return text.lower().strip()\n\ndef bert_tokenize(train, column, tokenizer):\n    bert_tokens = []\n    for text, sentiment in train[[column, 'sentiment']].values:\n#         if 'selected' not in column:\n        bert_tokens.append([101, sentiment_map[sentiment], 102] + tokenizer.tokenize(text) + [102])\n#         else:\n#             bert_tokens.append(tokenizer.tokenize(text))\n    \n    if 'selected' in column:\n        train['selected_bert_tokens'] = bert_tokens\n    else:\n        train['bert_tokens'] = bert_tokens\n        \n\ndef bertTokens2text(train, column):\n    def bertTokens2words_local(tokens): # bert and distilbert\n        '''\n        Convert bert tokens kinda ['some', '##ther', '##e'] into ['somethere', ...] and [number of byte-pairs (3 in that case), ...] for pos_tagging\n        '''\n        words = []\n        words_bpe_length = []\n        for index, token in enumerate(tokens[3:-1]):\n            if token[0] == '#' and token[-1] != '#':\n                words[-1] += token.replace('#', '')\n                words_bpe_length[-1] += 1\n            else:\n                words.append(token)\n                words_bpe_length.append(1)\n        return ' '.join(words).strip(), words_bpe_length\n    def albertTokens2words_local(tokens): # albert\n        '''\n        Convert bert tokens kinda ['▁some', 'there'] into ['somethere', ...] and [number of byte-pairs (2 in that case), ...] for pos_tagging\n        '''\n        words = []\n        words_bpe_length = []\n        for index, token in enumerate(tokens[3:-1]):\n            if token[0] != '▁' and token not in string.punctuation:# and (len(words[-1])>1 or words[-1] not in string.punctuation): \n                words[-1] += token\n                words_bpe_length[-1] += 1\n            elif token == '▁':\n                words.append(token)\n                words_bpe_length.append(1)\n            else:\n                words.append(token.replace('▁', ''))\n                words_bpe_length.append(1)\n        return ' '.join(words).strip(), words_bpe_length\n    def robertTokens2words_local(tokens): # robert / distilrobert\n        '''\n        Convert bert tokens kinda ['s', 'ome', 'here'] into ['somethere', ...] and [number of byte-pairs (3 in that case), ...] for pos_tagging\n        '''\n        words = []\n        words_bpe_length = []\n        for index, token in enumerate(tokens[3:-1]):\n            if words and token[0] != 'Ġ' and len(token)>1 and token not in string.punctuation:\n                words[-1] += token\n                words_bpe_length[-1] += 1\n            elif token == 'Ġ':\n                words.append(token)\n                words_bpe_length.append(1)\n            else:\n                words.append(token.replace('Ġ', ''))\n                words_bpe_length.append(1)\n        return ' '.join(words).strip(), words_bpe_length\n    \n    bertTokens_texts = []\n    bertTokens_words_bpe_length = []\n    for bertTokens in train[column].values:\n        if 'albert' in MODELNAME: # albert\n            text, words_bpe_length = albertTokens2words_local(bertTokens)\n        elif 'robert' in MODELNAME: # robert or distilrobert\n            text, words_bpe_length = robertTokens2words_local(bertTokens)\n        else: # bert or distilbert\n            text, words_bpe_length = bertTokens2words_local(bertTokens)\n        bertTokens_texts.append(text)\n        bertTokens_words_bpe_length.append(words_bpe_length)\n    train[f'{column}_texts'] = bertTokens_texts\n    if 'selected' not in column:\n        train[f'{column}_words_bpe_length'] = bertTokens_words_bpe_length\n        \n# def bert_label_sequences(train):\n#     target_sequences = []\n#     for index, sequence in enumerate(train.bert_tokens_texts.values):\n#         sequence = sequence.split()\n#         bert_tokens_words_bpe_length = train.loc[index, 'bert_tokens_words_bpe_length']\n#         selected_sequence = train.loc[index, 'selected_bert_tokens_texts'].split()\n#         labeled_sequence = np.zeros(len(train.loc[index, 'bert_tokens']), dtype=np.int)\n#         if not selected_sequence:\n#             target_sequences.append(labeled_sequence)\n#             continue\n#         local_index = 0\n#         labeled_sequence_index = 0\n#         while len(sequence) >= local_index + len(selected_sequence):\n#             if sequence[local_index : local_index+len(selected_sequence)] == selected_sequence:\n#                 for jndex, val in enumerate(bert_tokens_words_bpe_length[local_index:]):\n#                     if local_index + jndex >= local_index + len(selected_sequence):\n#                         break\n#                     labeled_sequence[labeled_sequence_index : labeled_sequence_index + val] = 1\n#                     labeled_sequence_index += val\n                    \n#                 break\n#             labeled_sequence_index += bert_tokens_words_bpe_length[local_index]\n#             local_index += 1\n#         else:\n#             local_index = 0\n#             labeled_sequence_index = 0\n#             while len(sequence) >= local_index + len(selected_sequence):\n#                 if (sequence[local_index][-len(selected_sequence[0]):] == selected_sequence[0] and \n#                     sequence[local_index + len(selected_sequence) - 1][:len(selected_sequence[-1])] == selected_sequence[-1] and \n#                     sequence[local_index+1 : local_index+len(selected_sequence)-1] == selected_sequence[1:-1]):\n                        \n#                     reserved = labeled_sequence_index\n                    \n#                     for jndex, val in enumerate(bert_tokens_words_bpe_length[local_index:]):\n#                         if local_index + jndex >= local_index + len(selected_sequence):\n#                             break\n#                         labeled_sequence[labeled_sequence_index : labeled_sequence_index + val] = 1\n#                         labeled_sequence_index += val\n                        \n#                     if sequence[local_index] != selected_sequence[0]:\n#                         labeled_sequence[reserved : reserved + bert_tokens_words_bpe_length[local_index]] = 0\n\n#                     if sequence[local_index + len(selected_sequence) - 1] != selected_sequence[-1]:\n#                         labeled_sequence_index -= bert_tokens_words_bpe_length[local_index + len(selected_sequence) - 1]\n#                         labeled_sequence[labeled_sequence_index : labeled_sequence_index + bert_tokens_words_bpe_length[local_index + len(selected_sequence) - 1]] = 0\n#                     break\n#                 labeled_sequence_index += bert_tokens_words_bpe_length[local_index]\n#                 local_index += 1\n#         target_sequences.append(labeled_sequence)\n#     train['target_sequence'] = target_sequences\n\ndef bert_label_sequences(train):\n    target_sequences = []\n    for index, sequence in enumerate(train.bert_tokens_texts.values):\n        sequence = sequence.split()\n        bert_tokens_words_bpe_length = train.loc[index, 'bert_tokens_words_bpe_length']\n        selected_sequence = train.loc[index, 'selected_bert_tokens_texts'].split()\n        labeled_sequence = [0, 0]\n        if not selected_sequence:\n            target_sequences.append([0, 0])\n            continue\n        local_index = 0\n        labeled_sequence_index = 0\n        while len(sequence) >= local_index + len(selected_sequence):\n            if sequence[local_index : local_index+len(selected_sequence)] == selected_sequence:\n                local_index += 3\n                labeled_sequence = [local_index, local_index + len(selected_sequence)]\n                    \n                break\n            labeled_sequence_index += bert_tokens_words_bpe_length[local_index]\n            local_index += 1\n        else:\n            local_index = 0\n            labeled_sequence_index = 0\n            while len(sequence) >= local_index + len(selected_sequence):\n                if (sequence[local_index][-len(selected_sequence[0]):] == selected_sequence[0] and \n                    sequence[local_index + len(selected_sequence) - 1][:len(selected_sequence[-1])] == selected_sequence[-1] and \n                    sequence[local_index+1 : local_index+len(selected_sequence)-1] == selected_sequence[1:-1]):\n                        \n                    labeled_sequence = [local_index + 3, local_index + len(selected_sequence) + 3]\n                        \n                    if sequence[local_index] != selected_sequence[0]:\n                        labeled_sequence[0] += 1\n\n                    if sequence[local_index + len(selected_sequence) - 1] != selected_sequence[-1]:\n                        labeled_sequence[1] -= 1\n                    break\n                labeled_sequence_index += bert_tokens_words_bpe_length[local_index]\n                local_index += 1\n        if labeled_sequence[1] <= labeled_sequence[0]:\n            labeled_sequence = [0, 0]\n        target_sequences.append(labeled_sequence)\n    train['target_sequence'] = target_sequences   \n        \ndef drop_unlabled_sequences(train):\n    cnt = 0\n    drop_indexes = []\n    for index, seq in enumerate(train.target_sequence.values):\n        if not any(seq):\n            cnt += 1\n            drop_indexes += [index]\n    if drop_indexes:\n        train.drop(drop_indexes, inplace=True, axis=0)\n        train.reset_index(drop=True, inplace=True)\n    print(f'dropped {cnt} samples')\n    \ndef text2tag(train, column, attributes):\n    attr_values = [[] for _ in attributes]\n    for text in train[column].values:\n        for index, attr in enumerate(attributes):\n            attr_values[index].append([])\n        for token in nlp(text):\n            if token.pos_ == 'SPACE':\n                continue\n            for index, attr in enumerate(attributes):\n                attr_values[index][-1].append(getattr(token, attr))\n    for index, attr in enumerate(attributes):\n        train[f'{\"_\".join(column.split(\"_\")[:-1])}_{attr}'] = attr_values[index]\n\ndef tag_bert_tokens(train, columns):\n    for column in columns:\n        attr_values = []\n        for ind, (tags, bert_tokens_words_bpe_length) in enumerate(train[[column, 'bert_tokens_words_bpe_length']].values):\n            if type(tags) is list:\n                try:\n                    if 'polarity' in column:\n                        attr_values.append([[0]*len(tags[0])]*3 + [tags[index] for index, value in enumerate(bert_tokens_words_bpe_length) for _ in range(value)] + [[0]*len(tags[0])])\n                    else:\n                        attr_values.append([0]*3 + [tags[index] for index, value in enumerate(bert_tokens_words_bpe_length) for _ in range(value)] + [0])\n                except:\n                    print(train.iloc[ind].values)\n                    print(len(bert_tokens_words_bpe_length), len(tags), tags, bert_tokens_words_bpe_length)\n                    assert False\n            else:\n                attr_values.append([[0]]*3 + [[tags] for value in bert_tokens_words_bpe_length for _ in range(value)] + [[0]])\n        train[column] = attr_values\n    \ndef onehot_encode(sequence, set_of_tags, tag2idx):\n    ohe = np.zeros((len(sequence), len(set_of_tags)))\n    for index, tag in enumerate(sequence):\n        if tag in set_of_tags:\n            ohe[index, tag2idx[tag]] = 1\n    return ohe\n\ndef label_encode(sequence, set_of_tags, tag2idx):\n    encoded_seq = []\n    for index, tag in enumerate(sequence):\n        if tag in set_of_tags:\n            encoded_seq.append(tag2idx[tag])\n    return np.array(encoded_seq, dtype=np.int)\n\ndef encode_all(tagged_text, set_of_tags, tag2idx, encode_func=onehot_encode):\n    sequences = []\n    for sequence in tagged_text:\n        sequences.append(encode_func(sequence, set_of_tags, tag2idx))\n    return sequences\n\n\nclass SentimentAnalyzer:\n    \n    def __init__(self, sentiment_analyzer):\n        self.sentiment_analyzer = sentiment_analyzer\n        if type(self.sentiment_analyzer) not in [SentimentIntensityAnalyzer, TextBlob]:\n            raise BaseException('Unknown sentiment analyzer', self.sentiment_analyzer)\n        \n    def __call__(self, text):\n        if isinstance(self.sentiment_analyzer, SentimentIntensityAnalyzer):\n            return self.sentiment_analyzer.polarity_scores(text)['compound']\n        elif isinstance(self.sentiment_analyzer, TextBlob):\n            return TextBlob(text).sentiment.polarity\n            \n            \n\ndef words_polarity_score(words, sentiment, n_gram_window_range=(0, 4), sentiment_analyzers=[SentimentAnalyzer(SentimentIntensityAnalyzer()), \n                                                                                            SentimentAnalyzer(TextBlob('123'))]):\n    \"\"\"\n    n_gram_window: word[i - n_gram_window_range[1]], ..., words[i], ..., word[i + n_gram_window_range[1]]\n    \"\"\"\n    sentiment_kef = 1 if sentiment == 'positive' else -1\n    polarity_scores = []\n    for _ in range(len(sentiment_analyzers)):\n        polarity_scores.append([])\n    for index, word in enumerate(words):\n        for _ in range(len(sentiment_analyzers)):\n            polarity_scores[_].append([])\n        for window in range(*n_gram_window_range):\n            for index_, sentiment_analyzer in enumerate(sentiment_analyzers):\n                polarity_scores[index_][-1].append(sentiment_analyzer(' '.join(words[max(0, index-window):min(len(words), index+window+1)])))\n    return polarity_scores\n\ndef calculate_texts_polarity_score(train, n_gram_window_range=(0, 4), sentiment_analyzers=[SentimentAnalyzer(SentimentIntensityAnalyzer()), \n                                                                                            SentimentAnalyzer(TextBlob('123'))]):\n    \n    column_basename = 'words_polarity_scores'\n    polarity_scores = []\n    for _ in range(len(sentiment_analyzers)):\n        polarity_scores.append([])\n        \n    for words, sentiment in train[['bert_tokens_texts', 'sentiment']].values:\n        words = words.split()\n        scores = words_polarity_score(words, sentiment, n_gram_window_range, sentiment_analyzers)\n        for index, scores_ in enumerate(scores):\n            polarity_scores[index].append(scores_)\n            \n    for index, sentiment_analyzer in enumerate(sentiment_analyzers):\n        if isinstance(sentiment_analyzer.sentiment_analyzer, SentimentIntensityAnalyzer):\n            train[f'vader_{column_basename}'] = polarity_scores[index]\n        elif isinstance(sentiment_analyzer.sentiment_analyzer, TextBlob):\n            train[f'textblob_{column_basename}'] = polarity_scores[index]\n#         elif # add your own sentiment analyzer\n    return\n\ndef count_sentences(text):\n    return len(re.split('[.!?]+', text))\n\ndef concat_encoded_features(train, columns):\n    concateneted_features = []\n    for row in train[columns].values:\n        concateneted_features.append(np.concatenate(row, axis=1))\n    return concateneted_features\n\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Preprocessor():\n\n    def __init__(self, cat_encoding_func, sentiment_analyzers=[SentimentAnalyzer(SentimentIntensityAnalyzer())], tokenizer=tokenizer):\n        self.isTrain = True\n        self.cat_encoding_func = cat_encoding_func\n        self.sentiment_analyzers = sentiment_analyzers\n        self.tokenizer = tokenizer\n    \n    def fit(self, X, y=None):\n        self.isTrain = True\n        return self\n    \n    def transform(self, X):\n        X = X.copy()\n        if self.isTrain:\n            X.fillna('', inplace=True)\n        # drop neutral samples, because we would select the whole text for neutral sentiment text\n        X.drop(X[X.sentiment == 'neutral'].index, inplace=True)\n        X.reset_index(drop=True, inplace=True)\n        # clean texts\n        print('Clean text...')\n        X['cleaned_text'] = X.text.apply(clean_text)\n        if self.isTrain:\n            X['cleaned_selected_text'] = X.selected_text.apply(clean_text)\n        for column in ['cleaned_text', 'cleaned_selected_text']:\n            if 'selected' in column:\n                if not self.isTrain:\n                    continue\n                bert_tokenize(X, column, self.tokenizer)\n                bertTokens2text(X, 'selected_' + 'bert_tokens') # column = 'bert_tokens' either 'bert_selected_tokens'\n            else:\n                bert_tokenize(X, column, self.tokenizer)\n                bertTokens2text(X, 'bert_tokens') # column = 'bert_tokens' either 'bert_selected_tokens'\n        print('text2postag...')\n        text2tag(X, 'bert_tokens_texts', ['pos_', 'dep_'])\n        if self.isTrain:\n            print('label_sequences...')\n            bert_label_sequences(X)\n            print('drop_unlabled_sequences...')\n            drop_unlabled_sequences(X) # drop samples where (target_sequence == 0).all()\n            self.set_of_pos_tags = set(tag for tags in X.bert_tokens_pos_.values for tag in tags[3:-1] if tag != 'SPACE')\n            self.pos2idx = dict((tag, index) for index, tag in enumerate(self.set_of_pos_tags))\n            self.idx2pos = dict(enumerate(self.set_of_pos_tags))\n            self.set_of_dep_tags = set(tag for tags in X.bert_tokens_dep_.values for tag in tags[3:-1] if tag != 'SPACE')\n            self.dep2idx = dict((tag, index) for index, tag in enumerate(self.set_of_dep_tags))\n            self.idx2dep = dict(enumerate(self.set_of_dep_tags))\n            \n        print('calculate_texts_polarity_score...')\n        calculate_texts_polarity_score(X, sentiment_analyzers=self.sentiment_analyzers)\n    \n        # calculate number of sentences\n        X['cleaned_text_num_sents'] = X.cleaned_text.apply(count_sentences)\n        # calculate number of tokens (words + punctuation)\n        X['cleaned_text_num_tokens'] = X.bert_tokens_texts.apply(lambda x: len(x.split()))\n\n        # min_max scaling\n        if self.isTrain:\n            self.max_num_sents = X.cleaned_text_num_sents.max()\n            self.max_num_tokens = X.cleaned_text_num_tokens.max()\n        X['cleaned_text_num_sents'] = X.cleaned_text_num_sents.values / self.max_num_sents\n        X['cleaned_text_num_tokens'] = X.cleaned_text_num_tokens.values / self.max_num_tokens\n        \n        # expand features for bert tokens\n        tag_bert_tokens(X, ['bert_tokens_pos_', 'bert_tokens_dep_', 'cleaned_text_num_sents', 'cleaned_text_num_tokens', 'vader_words_polarity_scores'])\n        \n        print('pos and dep encoding...')\n        X['bert_pos_tag_encoded'] = encode_all(X.bert_tokens_pos_.values, self.set_of_pos_tags, self.pos2idx, self.cat_encoding_func)\n        X['bert_dep_tag_encoded'] = encode_all(X.bert_tokens_dep_.values, self.set_of_dep_tags, self.dep2idx, self.cat_encoding_func)\n#         print(X.head())\n        # concatenate features\n        print('concatenating...')\n        if self.isTrain:\n            self.concatenate_feature_columns = ['bert_pos_tag_encoded', 'bert_dep_tag_encoded', 'cleaned_text_num_sents', 'cleaned_text_num_tokens', 'vader_words_polarity_scores']\n#         for col in self.concatenate_feature_columns:\n#             print(np.array(X.loc[0, col]).shape)\n        features = concat_encoded_features(X, self.concatenate_feature_columns)\n        if self.isTrain:\n            target = X.target_sequence.values\n            return X, features, target\n        return X, features\n        \n        \n    def fit_transform(self, X, y=None):\n        self.fit(X, y)\n        return self.transform(X)\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocessor = Preprocessor(onehot_encode)\nnew_train, features, target = preprocessor.fit_transform(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocessor.isTrain = False\nnew_test, test_features = preprocessor.transform(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN = 128\n# for bert_tokens in new_train[new_train.sentiment != 'neutral'].bert_tokens.values:\n#     MAX_LEN = max(MAX_LEN, len(bert_tokens))\n# print(f'MAX_LEN: {MAX_LEN}')\nencoded_text = []\nattention_masks = []\nfor bert_tokens, sentiment in new_train[new_train.sentiment != 'neutral'][['bert_tokens', 'sentiment']].values:\n    encoded_text.append([101, sentiment_map[sentiment], 102] + tokenizer.encode(bert_tokens[3:-1], add_special_tokens=False) + [102] + [0]*(MAX_LEN - len(bert_tokens)))\n    attention_masks.append([1]*len(bert_tokens) + [0]*(MAX_LEN - len(bert_tokens)))\nencoded_text = np.array(encoded_text)\nattention_masks = np.array(attention_masks)\nencoded_text[:10].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def pad_sequence(feature_sequence, target_sequence, max_seq_len, mode='uniform'):\n#     \"\"\"\n#     mode: uniform - num of zero-padding for pre and post padding is aprxmtly the same\n#           pre - ...\n#           post - ...\n#     \"\"\"\n#     if mode == 'uniform':\n#         pre_pad_length = (max_seq_len - len(feature_sequence)) // 2\n#         pre_pad = [[0]*len(feature_sequence[0])]*pre_pad_length\n#         post_pad_length = max_seq_len - len(feature_sequence) - pre_pad_length\n#         post_pad = [[0]*len(feature_sequence[0])]*post_pad_length\n#         feature_sequence = (feature_sequence,)\n#         target_sequence = (target_sequence,)\n#         if pre_pad:\n#             feature_sequence = (pre_pad,) + feature_sequence\n#             target_sequence = ([0]*pre_pad_length,) + target_sequence\n#         if post_pad:\n#             feature_sequence += (post_pad,)\n#             target_sequence += ([0]*post_pad_length,)\n#         if len(feature_sequence) > 1:\n#             feature_sequence = np.concatenate(feature_sequence, axis=0)\n#             target_sequence = np.concatenate(target_sequence, axis=0)\n#         else:\n#             feature_sequence, target_sequence = feature_sequence[0], target_sequence[0]\n#     elif mode == 'pre':\n#         pre_pad_length = max_seq_len - len(feature_sequence)\n#         pre_pad = [[0]*len(feature_sequence[0])]*pre_pad_length\n#         feature_sequence = (feature_sequence,)\n#         target_sequence = (target_sequence,)\n#         if pre_pad:\n#             feature_sequence = (pre_pad,) + feature_sequence\n#             target_sequence = ([0]*pre_pad_length,) + target_sequence\n#         if len(feature_sequence) > 1:\n#             feature_sequence = np.concatenate(feature_sequence, axis=0)\n#             target_sequence = np.concatenate(target_sequence, axis=0)\n#         else:\n#             feature_sequence, target_sequence = feature_sequence[0], target_sequence[0]\n#     elif mode == 'post':\n#         post_pad_length = max_seq_len - len(feature_sequence)\n#         post_pad = [[0]*len(feature_sequence[0])]*post_pad_length\n#         feature_sequence = (feature_sequence,)\n#         target_sequence = (target_sequence,)\n#         if post_pad:\n#             feature_sequence += (post_pad,)\n#             target_sequence += ([0]*post_pad_length,)\n#         if len(feature_sequence) > 1:\n#             feature_sequence = np.concatenate(feature_sequence, axis=0)\n#             target_sequence = np.concatenate(target_sequence, axis=0)\n#         else:\n#             feature_sequence, target_sequence = feature_sequence[0], target_sequence[0]\n#     else:\n#         raise BaseException(f'Unknown mode: {mode}. Choose one of the following: [\"uniform\",\"pre\", \"post\"]')\n#     return feature_sequence, target_sequence\n\n# def pad_sequences(features, target, max_seq_len, mode='uniform'):\n#     features, target = list(zip(*[pad_sequence(features[index], target[index], max_seq_len, mode) \\\n#                                                for index in range(len(features))]))\n#     return features, target\n\n# padded_features, padded_target = pad_sequences(features, target, MAX_LEN, mode='post')\n# padded_features = np.array(padded_features)\n# padded_target = np.array(padded_target)\n# padded_target = padded_target.reshape(*padded_target.shape[:2], 1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def pad_sequence(feature_sequence, max_seq_len, mode='uniform'):\n    \"\"\"\n    mode: uniform - num of zero-padding for pre and post padding is aprxmtly the same\n          pre - ...\n          post - ...\n    \"\"\"\n    if mode == 'uniform':\n        pre_pad_length = (max_seq_len - len(feature_sequence)) // 2\n        pre_pad = [[0]*len(feature_sequence[0])]*pre_pad_length\n        post_pad_length = max_seq_len - len(feature_sequence) - pre_pad_length\n        post_pad = [[0]*len(feature_sequence[0])]*post_pad_length\n        feature_sequence = (feature_sequence,)\n        if pre_pad:\n            feature_sequence = (pre_pad,) + feature_sequence\n        if post_pad:\n            feature_sequence += (post_pad,)\n        if len(feature_sequence) > 1:\n            feature_sequence = np.concatenate(feature_sequence, axis=0)\n        else:\n            feature_sequence = feature_sequence[0]\n    elif mode == 'pre':\n        pre_pad_length = max_seq_len - len(feature_sequence)\n        pre_pad = [[0]*len(feature_sequence[0])]*pre_pad_length\n        feature_sequence = (feature_sequence,)\n        if pre_pad:\n            feature_sequence = (pre_pad,) + feature_sequence\n        if len(feature_sequence) > 1:\n            feature_sequence = np.concatenate(feature_sequence, axis=0)\n        else:\n            feature_sequence = feature_sequence[0]\n    elif mode == 'post':\n        post_pad_length = max_seq_len - len(feature_sequence)\n        post_pad = [[0]*len(feature_sequence[0])]*post_pad_length\n        feature_sequence = (feature_sequence,)\n        if post_pad:\n            feature_sequence += (post_pad,)\n        if len(feature_sequence) > 1:\n            feature_sequence = np.concatenate(feature_sequence, axis=0)\n        else:\n            feature_sequence = feature_sequence[0]\n    else:\n        raise BaseException(f'Unknown mode: {mode}. Choose one of the following: [\"uniform\",\"pre\", \"post\"]')\n    return feature_sequence\n\ndef pad_sequences(features, max_seq_len, mode='uniform'):\n    features = [pad_sequence(features[index], max_seq_len, mode) for index in range(len(features))]\n    return features\n\npadded_features = pad_sequences(features, MAX_LEN, mode='post')\npadded_features = np.array(padded_features)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Spliting train set on train and validation set (holdout)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n\ntest_size = .15\nrandom_state = 123\nshuffle = True\nstratify = new_train.sentiment.values\ntrain_index, val_index, _, _ = train_test_split(new_train.index, new_train.sentiment.values, \n                                                test_size=test_size, random_state=random_state, \n                                                shuffle=shuffle, stratify=stratify)\ntrain_sequences = [encoded_text[train_index], attention_masks[train_index], padded_features[train_index]]\nval_sequences = [encoded_text[val_index], attention_masks[val_index], padded_features[val_index]]\ntrain_target, val_target = target[train_index], target[val_index]\ntrain_target = np.concatenate(train_target).reshape(-1, 2)\nval_target = np.concatenate(val_target).reshape(-1, 2)\n# train_target, val_target = padded_target[train_index], padded_target[val_index]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Define model"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom tensorflow.keras import Sequential, Model, Input\nimport tensorflow.keras.layers as L\nfrom tensorflow.keras.layers import InputLayer, Bidirectional, LSTM, Dense, Dropout, GRU, Activation, Concatenate\nfrom tensorflow.keras.initializers import TruncatedNormal\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MyModel(tf.keras.Model):\n\n    def __init__(self):\n        super(MyModel, self).__init__()\n        self.bert = BERT\n        self.dropout = L.Dropout(0.1)\n        self.qa_outputs = L.Dense(2, \n#                                 kernel_initializer=TruncatedNormal(stddev=config.initializer_range),\n                                dtype='float32',\n                                name=\"qa_outputs\")\n        self.aux_input = InputLayer(input_shape=(None, 68), )\n        self.concat = Concatenate(axis=-1)\n#         self.birnn = Bidirectional(LSTM(units=128, recurrent_dropout=0.2, return_sequences=True))#, input_shape=features.shape[-2:])\n#         self.fc = Dense(64, activation='relu')\n#         self.dropout = Dropout(0.2)\n#         self.out = Dense(1, activation='sigmoid')\n\n    def call(self, inputs, **kwargs):\n        last_out = self.bert(inputs[0], attention_mask=inputs[1])[0]\n        \n#         hidden_states = self.concat([\n#             hidden_states[-i] for i in range(1, self.NUM_HIDDEN_STATES+1)\n#         ])\n#         hidden_states = self.birnn(hidden_states, training=kwargs.get(\"training\", False))\n#         hidden_states = self.dropout(hidden_states, training=kwargs.get(\"training\", False))\n#         aux_inp = self.aux_input()\n#         last_out = self.concat([last_out, inputs[2]])\n        hidden_states = self.dropout(last_out, training=kwargs.get(\"training\", False))\n        logits = self.qa_outputs(hidden_states)\n        start_logits, end_logits = tf.split(logits, 2, axis=-1)\n        start_logits = tf.squeeze(start_logits, axis=-1)\n        end_logits = tf.squeeze(end_logits, axis=-1)\n        \n        return start_logits, end_logits\n        \n#         bert_out = self.bert(inputs[0], attention_mask=inputs[1])[0]\n#         aux_inp = self.aux_input(inputs[2])\n#         concat = self.concat([aux_inp, bert_out])\n#         x = self.birnn(concat)\n#         x = self.fc(x)\n#         x = self.dropout(x)\n#         x = self.out(x)\n#         return x\n\n\n\nmodel = MyModel()\n# model()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res[0].shape, tf.argmax(tf.nn.softmax(res), axis=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_fn =  tf.keras.losses.sparse_categorical_crossentropy\ndef custom_loss(y_true, y_pred):\n#     print(y_true, y_true[:, 0], y_pred)\n#     print\n    loss  = loss_fn(y_true[:, 0], y_pred[0], from_logits=True)\n    loss += loss_fn(y_true[:, 1], y_pred[1], from_logits=True)\n    return loss\n\ncustom_loss(train_target[:n], res)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n# loss_fn =  tf.keras.losses.sparse_categorical_crossentropy\ndef custom_loss(y_true, y_pred):\n    print(y_true.shape, y_pred.shape)\n#     print\n    loss  = loss_fn(y_true[0], y_pred[0])#, from_logits=True)\n    loss += loss_fn(y_true[1], y_pred[1])#, from_logits=True)\n    return loss\n\nmodel.compile(optimizer=tf.keras.optimizers.Adam(3e-5), \n              loss=custom_loss,\n             )\n#               sample_weight_mode='temporal')#, \n#               metrics=[tf.keras.metrics.BinaryAccuracy(), tf.keras.metrics.AUC(name='auc', curve='PR'),\n#                       tf.keras.metrics.Precision(name='precision'), tf.keras.metrics.Recall(name='recall')])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MyCustomCallback(tf.keras.callbacks.Callback):\n    def __init__(self, batch_size, n_epochs, train_n_samples, val_n_samples):\n        super(MyCustomCallback, self).__init__()\n        self.batch_size = batch_size\n        self.n_epochs = n_epochs\n        self.train_n_samples = train_n_samples\n        self.val_n_samples = val_n_samples\n\n    def on_train_batch_begin(self, batch, logs=None):\n        self.local_time = time.time()\n\n    def on_train_batch_end(self, batch, logs=None):\n        self.batch_number += 1\n        batch_calc_time = time.time() - self.local_time\n        epoch_time = batch_calc_time * self.train_n_samples / self.batch_size\n        aprxt_time_to_end_epoch = epoch_time - batch_calc_time * self.batch_number\n#         print(epoch_time)\n#         print(epoch_time, batch_calc_time * self.batch_number, aprxt_time_to_end_epoch)\n        print(f'batch_number: {self.batch_number}/{int(self.train_n_samples / self.batch_size) + 1}, batch calculation time: {round(batch_calc_time, 2)} sec, ' +\n              f'aprxt_time to end epoch: {round(aprxt_time_to_end_epoch / 60, 2)} min, ' +\n              f'aprxmt_time to end training: {round((aprxt_time_to_end_epoch + epoch_time*(self.n_epochs - self.cur_epoch))/ 60, 2)} min', \n#               end=\"\\r\", \n              )\n        print('For batch {}, loss is {:7.2f}.'.format(batch, logs['loss']))\n    \n    def on_epoch_begin(self, epoch, logs=None):\n        self.batch_number = 0\n        self.epoch_start_time = time.time()\n        self.cur_epoch = epoch + 1\n        print('Current epoch:', self.cur_epoch)\n    \n    def on_epoch_end(self, epoch, logs=None):\n        print(f'\\nepoch №:{epoch + 1} has been ended for {round((time.time() - self.epoch_start_time) / 60, 2)} minutes', flush=True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## train"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_target","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 32\nn_epochs = 9\nmodel.fit(train_sequences, [train_target.T[0], train_target.T[1]], #tf.constant(train_target)\n          batch_size=batch_size, epochs=n_epochs, \n          validation_data=(val_sequences, [val_target.T[0], val_target.T[1]]), \n          callbacks=[MyCustomCallback(batch_size, n_epochs, len(train_sequences[0]), len(val_sequences[0]))],\n          verbose=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.Tensor(train_target, dtype=tf.int32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.stack(([1, 2, 3], [3, 4, 5]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n = 3\nres = model([train_sequences[0][:n], train_sequences[1][:n], train_sequences[2][:n]])\nres","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for ind in range(n):\n    print(res[ind][attention_masks[ind].astype(bool)].reshape(-1).shape, res[ind][attention_masks[ind].astype(bool)].reshape(-1))\n    predictions = np.round(res[ind][attention_masks[ind].astype(bool)].reshape(-1)).astype(bool)\n    print(new_train.loc[ind, 'cleaned_text'], new_train.loc[ind, 'bert_tokens'], predictions, np.array(new_train.loc[ind, 'bert_tokens'])[predictions])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res[attention_masks[:n].astype(bool)].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"local_string = 'spe nothing?!?!else somethere 321as'\ntokens = np.array(tokenizer.tokenize(local_string))\nprint(tokens)\ntokenizer.convert_tokens_to_string(tokens)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res[0].shape, tf.argmax(tf.nn.softmax(res), axis=-1)[:, 0].numpy(), train_target[:n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def bert_post_processing(source_text, bert_tokens, start_end, pred_start_end):\n    # source_text: str 'text' from train/test df\n    # predicted_text: str after tokenizer.convert_tokens_to_string method\n    source_text = source_text.split(' ')\n    \n    pred_start_end = (max(pred_start_end[0], 3), min(len(bert_tokens)-1, pred_start_end[1]))\n    if pred_start_end[1] < pred_start_end[0]:\n        return source_text.split(' ')\n    index0 = 0\n    predicted_text = []\n    local_word = ''\n    isInclude = False\n    for index, tkn in enumerate(bert_tokens[3:-1]):\n        index += 3\n        if index > pred_start_end[1]:\n            break\n        isInclude |= pred_start_end[0] <= index < pred_start_end[1]\n#         print(tkn)\n        local_word += tkn.replace('##', '')\n        if local_word == source_text[index0]:\n            if isInclude:\n                predicted_text.append(source_text[index0])\n            local_word = ''\n            index0 += 1\n            isInclude = False\n#     if not predicted_text and pred_start_end[0] == pred_start_end[1] and 3 <= pred_start_end[0] < len(bert_tokens) - 1:\n#         return bert_tokens[pred_start_end[0]]\n    return ' '.join(predicted_text)\n        \n\n# index = 0\n\n# local_string = new_train.loc[train_index[0], 'cleaned_text']\n# bert_tokens = new_train.loc[train_index[0], 'bert_tokens']\n# start_end = train_target[index]\n# pred_start_end = tf.argmax(tf.nn.softmax(res), axis=-1)[:, 0].numpy()\n# print(local_string, bert_tokens, len(bert_tokens), new_train.loc[train_index[0], 'selected_text'], start_end, pred_start_end)\n# bert_post_processing(local_string, bert_tokens, start_end, pred_start_end)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict_text(model, sequences, indexes, df, pred_start_end=np.array([])):\n    if not pred_start_end.shape[0]:\n        pred_start_end = tf.argmax(tf.nn.softmax(model.predict(sequences)), axis=-1).numpy().T\n#         proba_predictions = model.predict(sequences) # all tokens proba predictions\n    text_predictions = [] # selected text predictions\n#     print(pred_start_end)\n    for index, ind in enumerate(indexes):\n        pred_start_end_local = pred_start_end[index]\n        true_start_end_local = df.loc[ind, 'target_sequence']\n        if pred_start_end_local[0] >= pred_start_end_local[1] or len(df.loc[ind, 'cleaned_text'].split()) <= 3: # select all the text because all values equal to 0 or number of tokens <= 3\n            text_predictions.append(df.loc[ind, 'cleaned_text'])\n        else: \n            bert_tokens = np.array(df.loc[ind, 'bert_tokens'])\n            if 'roberta' in MODELNAME:\n                # todo\n                if bert_tokens[0][0] != 'Ġ':\n                    bert_tokens[0] = 'Ġ' + bert_tokens[0]\n                text_predictions.append(roberta_post_processing(bert_tokens, occurences, prediction_mask, mode='expand')) # truncate or expand\n#                 text_predictions.append(tokenizer.convert_tokens_to_string(bert_tokens[prediction_mask]).strip()) # truncate or expand\n            elif 'albert' in MODELNAME:\n                pass\n            else: # bert; expand post_processing\n#                 print(df.loc[ind, 'cleaned_text'], bert_tokens, len(bert_tokens), df.loc[ind, 'selected_text'], true_start_end_local, pred_start_end_local)\n                post_processed_text = bert_post_processing(df.loc[ind, 'cleaned_text'], bert_tokens, true_start_end_local, pred_start_end_local)\n                text_predictions.append(post_processed_text)\n                \n        \n    return pred_start_end, text_predictions\n    \n    \nprint('train eval...')\ntrain_pred_start_end, train_text_predictions = predict_text(model, train_sequences, train_index, new_train)#, train_pred_start_end)\nprint('validation eval...')\nval_pred_start_end, val_text_predictions = predict_text(model, val_sequences, val_index, new_train)#, val_pred_start_end)\n# n = 10\n# print('train eval...')\n# train_pred_start_end, train_text_predictions = predict_text(model, [train_sequences[0][:n], train_sequences[1][:n], train_sequences[2][:n]], train_index[:n], new_train)\n# print('validation eval...')\n# val_pred_start_end, val_text_predictions = predict_text(model, [val_sequences[0][:n], val_sequences[1][:n], val_sequences[2][:n]], val_index[:n], new_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_text_predictions[:10], new_train.loc[train_index[:10], 'selected_text'].values.tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def bert_post_processing(source_text, bert_tokens, occurancies, prediction_mask):\n    # source_text: str 'text' from train/test df\n    # predicted_text: str after tokenizer.convert_tokens_to_string method\n    source_text = source_text.split(' ')\n    \n    index0 = 0\n    predicted_text = []\n    local_word = ''\n    isInclude = False\n    for index, tkn in enumerate(bert_tokens):\n        isInclude |= prediction_mask[index]\n        local_word += tkn.replace('##', '')\n        if local_word == source_text[index0]:\n            if isInclude:\n                predicted_text.append(source_text[index0])\n            local_word = ''\n            index0 += 1\n            isInclude = False\n    return ' '.join(predicted_text)\n        \n\n# local_string = 'spe nothing?!?!else somethere 321as'\n# tokens = np.array(tokenizer.tokenize(local_string))\n# mask = np.array([False, False, False, True, True, True, True, False, False, False, False, False, False])\n# occ = np.where(mask == True)[0]\n# print(mask, tokens, occ, len(tokens), len(mask))\n# print(local_string)\n# print(bert_post_processing(local_string, tokens, occ, mask))\n# local_string = 'nothing?!?!else somethere'\n# new_str = local_string[3:]\n# tokenizer.convert_tokens_to_string(tokenizer.tokenize(new_str)), post_processing(local_string, tokenizer.convert_tokens_to_string(tokenizer.tokenize(new_str)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def roberta_post_processing(bert_tokens, occurancies, prediction_mask, mode='expand'):\n    if mode == 'expand':\n        index = occurancies[0] - 1\n        if bert_tokens[occurancies[0]][0] != 'Ġ':\n#             print(f'{mode} pre del')\n            while index >= 0:\n                prediction_mask[index] = True\n                if bert_tokens[index][0] == 'Ġ':\n                    break\n                index -= 1\n\n        index = occurancies[-1] + 1\n#         if index != len(bert_tokens) and bert_tokens[index][0] != 'Ġ':\n#             print(f'{mode} post del')\n        while index != len(bert_tokens) and bert_tokens[index][0] != 'Ġ':\n            prediction_mask[index] = True\n            index += 1\n    elif mode == 'truncate':\n        isThereAnyG = [True for _ in bert_tokens[prediction_mask] if _[0] == 'Ġ']\n        if isThereAnyG:\n            \n            if bert_tokens[occurancies[0]][0] != 'Ġ':\n                index = 0\n#                 print(f'{mode} pre del {len(isThereAnyG)}')\n                while index < len(occurancies) and bert_tokens[occurancies[index]][0] != 'Ġ':\n                    prediction_mask[occurancies[index]] = False\n                    index += 1\n                    \n            if len(isThereAnyG)>1:\n#                 print(f'{mode} post del {len(isThereAnyG)}')\n#                 print(bert_tokens[occurancies[-1]][0] != 'Ġ', len(bert_tokens)>occurancies[-1]+1, bert_tokens[occurancies[-1]+1][0] == 'Ġ')\n                if len(bert_tokens)>occurancies[-1]+1 and bert_tokens[occurancies[-1]+1][0] != 'Ġ':\n                    index = len(occurancies) - 1\n#                     print(f'{mode} post del 1 {len(isThereAnyG)}')\n                    while index > 0 and bert_tokens[occurancies[index]][0] != 'Ġ':\n                        prediction_mask[occurancies[index]] = False\n                        index -= 1\n                    prediction_mask[occurancies[index]] = False\n            else: # len(isThereAnyG) = 1\n#                 print(f'{mode} post del {len(isThereAnyG)}')\n                index = occurancies[-1] + 1\n#                 if index != len(bert_tokens) and bert_tokens[index][0] != 'Ġ':\n#                     print(f'{mode} post del')\n                while index != len(bert_tokens) and bert_tokens[index][0] != 'Ġ':\n                    prediction_mask[index] = True\n                    index += 1\n        else:\n#             print(f'{mode} post del')\n            return roberta_post_processing(bert_tokens, occurancies, prediction_mask, mode='expand')\n#     print(len(bert_tokens), len(prediction_mask))\n#     return prediction_mask, tokenizer.convert_tokens_to_string(bert_tokens[prediction_mask]).strip()\n    return tokenizer.convert_tokens_to_string(bert_tokens[prediction_mask]).strip()\n    \n        \n# local_string = ' spe nothing?!?!else somethere 321as'\n# tokens = np.array(tokenizer.tokenize(local_string))\n# mask = np.array([False, False, True, True, True, True, True, False, False])\n# occ = np.where(mask == True)[0]\n# print(mask, tokens, occ, len(tokens), len(mask))\n# print(roberta_post_processing(tokens, occ, mask, mode='expand'))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict_text(model, sequences, indexes, df, proba_predictions=np.array([])):\n    if not proba_predictions.shape[0]:\n        proba_predictions = model.predict(sequences) # all tokens proba predictions\n    text_predictions = [] # selected text predictions\n    selected_proba_predictions = []\n    for index, ind in enumerate(indexes):\n        selected_proba_predictions.append(proba_predictions[index][attention_masks[ind].astype(bool)].reshape(-1))\n        prediction_mask = np.round(selected_proba_predictions[-1]).astype(bool)\n        occurences = np.where(prediction_mask == True)[0]\n        if not occurences.shape[0] or len(df.loc[ind, 'cleaned_text'].split()) <= 3: # select all the text because all values equal to 0 or number of tokens <= 3\n            text_predictions.append(df.loc[ind, 'cleaned_text'])\n        else: \n            bert_tokens = np.array(df.loc[ind, 'bert_tokens'])\n            prediction_mask[occurences[0] : occurences[-1] + 1] = True\n            if 'roberta' in MODELNAME:\n                if bert_tokens[0][0] != 'Ġ':\n                    bert_tokens[0] = 'Ġ' + bert_tokens[0]\n                text_predictions.append(roberta_post_processing(bert_tokens, occurences, prediction_mask, mode='expand')) # truncate or expand\n#                 text_predictions.append(tokenizer.convert_tokens_to_string(bert_tokens[prediction_mask]).strip()) # truncate or expand\n            elif 'albert' in MODELNAME:\n                pass\n            else: # bert; expand post_processing\n                post_processed_text = bert_post_processing(df.loc[ind, 'cleaned_text'], bert_tokens, occurences, prediction_mask)\n                text_predictions.append(post_processed_text)\n                \n        \n    return proba_predictions, text_predictions, selected_proba_predictions\n    \n    \nprint('train eval...')\ntrain_proba_predictions, train_text_predictions, train_selected_proba_predictions = predict_text(model, train_sequences, train_index, new_train)#, train_proba_predictions)\nprint('validation eval...')\nval_proba_predictions, val_text_predictions, val_selected_proba_predictions = predict_text(model, val_sequences, val_index, new_train)#, val_proba_predictions)\n# n = 10\n# print('train eval...')\n# train_proba_predictions, train_text_predictions, train_selected_proba_predictions = predict_text(model, [train_sequences[0][:n], train_sequences[1][:n], train_sequences[2][:n]], train_index[:n], new_train)\n# print('validation eval...')\n# val_proba_predictions, val_text_predictions, val_selected_proba_predictions = predict_text(model, [val_sequences[0][:n], val_sequences[1][:n], val_sequences[2][:n]], val_index[:n], new_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_proba_predictions[:2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_selected_proba_predictions[:2], train_text_predictions[:2],train_proba_predictions[:2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tkns = tokenizer.tokenize('thinks tonight couldn\\'t have gone more perfect.')\nprint(tkns)\ntkns[0] = 'Ġ' + tkns[0]\ntokenizer.convert_tokens_to_string(tkns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_train.loc[train_index[0]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('train_proba_predictions.txt', 'w') as f:\n#     f.write(str(train_pred) + '\\n')\n    f.write(str(train_predictions) + '\\n')\n    f.write(str(train_proba_predictions) + '\\n')\n#     f.write(str(val_pred) + '\\n')\n    f.write(str(val_predictions) + '\\n')\n    f.write(str(val_proba_predictions) + '\\n')\nprint('saved train_val predictions')\nprint('saving weights...')\nmodel.save_weights('roberta_weights')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_text_predictions[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))\n\ndef score_all(indexes, text_predictions):\n    jaccard_scores = []\n    for ind, index in enumerate(indexes):\n        jaccard_scores.append(jaccard(new_train.loc[index, 'cleaned_selected_text'], text_predictions[ind]))\n    return jaccard_scores\n\ntrain_jaccard_scores = score_all(train_index, train_text_predictions)\nprint('TRAIN JACARD-SCORE:', np.mean(train_jaccard_scores))\n\nval_jaccard_scores = score_all(val_index, val_text_predictions)\nprint('VAL JACARD-SCORE:', np.mean(val_jaccard_scores))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))\n\ndef score_all(indexes, text_predictions):\n    jaccard_scores = []\n    for ind, index in enumerate(indexes):\n        jaccard_scores.append(jaccard(new_train.loc[index, 'cleaned_selected_text'], text_predictions[ind]))\n    return jaccard_scores\n\ntrain_jaccard_scores = score_all(train_index, train_text_predictions)\nprint('TRAIN JACARD-SCORE:', np.mean(train_jaccard_scores))\n\nval_jaccard_scores = score_all(val_index, val_text_predictions)\nprint('VAL JACARD-SCORE:', np.mean(val_jaccard_scores))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}