{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"DIR = \"/kaggle/input/\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Main Idea:\n - get a BERT/transformer model to classify on the emotions (this might add some knowledge to the model for the actual task)\n - get another BERT model to do something similar to entity extraction with the tweet and emotion as the input (cureently did QA BERT)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(DIR+\"tweet-sentiment-extraction/train.csv\")\ndf.dropna(inplace=True)\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text = df['text'].values\nselected_text = df['selected_text'].values\nsentiment = df['sentiment'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MODEL_NAME = DIR+\"qa-model3\"#\"twittersentimentqamodel1\" #\"bert-base-uncased\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import BertTokenizer\ntokenizer = BertTokenizer.from_pretrained(MODEL_NAME) #, do_lower_case=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import BertForQuestionAnswering, BertConfig, AdamW\n\nmodel = BertForQuestionAnswering.from_pretrained(MODEL_NAME, output_attentions=False, output_hidden_states=False)\n\nmodel.cuda()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = pd.read_csv(DIR+\"tweet-sentiment-extraction/test.csv\")#(\"../input/tweet-sentiment-extraction/test.csv\")\n\nprint(df_test['text'].isnull().sum())\nprint(df_test['sentiment'].isnull().sum())\n\ndf_test.dropna(axis = 0, how ='any',inplace=True) ;\n\ntext = df_test['text'].apply(str)\nsentiment = df_test['sentiment'].apply(str)\n\ntext = text.values\nprint(text[0])\nsentiment = sentiment.values\nprint(sentiment[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\n\ninput_ids = []\nattention_masks = []\ntoken_type_ids = []\n\nfor i in range(len(text)):\n    encoded = tokenizer.encode_plus(\n      sentiment[i],\n      text[i],\n      add_special_tokens=True,\n      max_length=150,\n      pad_to_max_length=True,\n      return_token_type_ids=True,\n      return_attention_mask=True,\n      return_tensors='pt'\n    )\n    \n    input_ids.append(encoded['input_ids'])\n    attention_masks.append(encoded['attention_mask'])\n    token_type_ids.append(encoded['token_type_ids'])\n\ninput_ids = torch.cat(input_ids, dim=0)\nattention_masks = torch.cat(attention_masks, dim=0)\ntoken_type_ids = torch.cat(token_type_ids, dim=0)\n\nprint(input_ids[0])\nprint(attention_masks[0])\nprint(token_type_ids[0])\n\nprint(len(input_ids[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n\nfrom torch.utils.data import TensorDataset, random_split\n\ntestset = TensorDataset(input_ids, attention_masks, token_type_ids)\nbatch_size = 64\n\ntestloader = DataLoader(testset,\n                      batch_size,\n                      sampler=SequentialSampler(testset))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test[\"selected_text\"] = df_test[\"text\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n\ndf_submit = pd.read_csv(DIR+\"tweet-sentiment-extraction/sample_submission.csv\")#(\"../input/tweet-sentiment-extraction/sample_submission.csv\")\ndef test():\n    key = 0\n    model.eval()\n\n    for step, data in enumerate(testloader):\n\n        input_id = data[0].to(device)\n        attention_mask = data[1].to(device)\n        token_type_id = data[2].to(device)\n\n        with torch.no_grad():\n            start_scores, end_scores = model(input_id, attention_mask, token_type_id)\n        \n        series_submit = pd.Series()    \n        for i in range(input_id.shape[0]):\n            all_tokens = tokenizer.convert_ids_to_tokens(input_id[i])\n            answer = ' '.join(all_tokens[torch.argmax(start_scores[i]) : torch.argmax(end_scores[i])+1])\n            print(answer)\n        \n#             answer = answer.detach()\n#             series_submit = pd.Series(answer)\n            df_test['selected_text'][key] = answer\n            key += 1\n\ntest()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = df_test\nsub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_all(a_str, sub, start = 0):\n  while start <= len(a_str):\n    start = a_str.lower().find(sub, start)\n    #print(start)\n    if start == -1: return\n    yield start\n    start += 1 #len(sub) # use start += 1 to find overlapping matches","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"selected_text_list = []\ncounter = 0\nfor index, row in sub.iterrows():\n  txt_lst = str(row[\"selected_text\"]).split(\" \")\n  for i, token in enumerate(txt_lst):\n    if token[:2] == \"##\":\n      txt_lst[i] = token[2:]\n    \n    \n  start = row[\"text\"].lower().find(txt_lst[0])\n  last_word = txt_lst[-1]\n  end = row[\"text\"].lower().rfind(last_word) + len(last_word)\n  final_text = row[\"text\"][start:end]\n  '''print(\"XXX \", final_text)'''\n  \n  '''while len(final_text.replace(\" \", \"\")) > len(''.join(txt_lst)):\n    #counter +=1\n    start = final_text.lower().find(txt_lst[0], start+1)\n    end = final_text.lower().rfind(last_word, start-1 + len(last_word)) + len(last_word)\n    final_text = final_text[start:end]\n    print(\"XXX \", final_text)'''\n  if len(final_text.replace(\" \", \"\")) != len(''.join(txt_lst)):\n    counter +=1\n    print(row[\"text\"])\n    print(txt_lst)\n    print(start)\n    print(end)\n    print(final_text)\n    print(\"-----\")\n\n  selected_text_list.append(final_text)\n\nprint(counter)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"selected_text_list = []\ncounter = 0\nfor index, row in sub.iterrows():\n  txt_lst = row[\"selected_text\"].split(\" \")\n  txt_lst_short = []\n  for i, token in enumerate(txt_lst):\n    if token[:2] == \"##\":\n      txt_lst[i] = token[2:]\n\n  \n  start_list = list(find_all(row[\"text\"], txt_lst[0]))\n  start = start_list[0]\n  \n  if len(start_list) > 1 and len(txt_lst) > 1:\n    second_list = list(find_all(row[\"text\"], txt_lst[1], start_list[0]+len(txt_lst[0])))\n    #print(second_list)\n    for start_item in start_list:\n      #print(start_item)\n      if second_list[0] == start_item + len(txt_lst[0]) or second_list[0] == start_item + len(txt_lst[0]) + 1:\n        #print(\"selected!\")\n        start = start_item\n        break\n\n\n  last_word = txt_lst[-1]\n  #end = row[\"text\"].lower().rfind(last_word) + len(last_word)\n  #end = row[\"text\"].rfind(last_word) + len(last_word)\n  \n  end_list = list(find_all(row[\"text\"], txt_lst[-1]))\n  end = end_list[-1] + len(last_word)\n  '''if len(end_list) > 1 and len(txt_lst) > 1:\n    secondlast_list = list(find_all(row[\"text\"], txt_lst[-2], start + len(txt_lst[0])))\n    for end_item in end_list:\n      if len(secondlast_list) > 0 and (end_item + len(last_word) > start):\n        if secondlast_list[-1] + len(txt_lst[-2]) == end_item or secondlast_list[-1] + len(txt_lst[-2]) + 1 == end_item:\n          end = end_item + len(last_word)\n          break'''\n\n\n  final_text = row[\"text\"][start:end]\n\n  #while len(final_text.lower().replace(\" \", \"\")) > len(''.join(txt_lst)):\n  #  end = end_list[end_list.index(end) - 1]\n    \n\n  #if final_text.lower().replace(\" \", \"\") != ''.join(txt_lst):\n  #  final_text = \" \".join(txt_lst)\n\n\n  if final_text.lower().replace(\" \", \"\") != ''.join(txt_lst):\n    counter +=1\n    print(\"start_list: \", start_list)\n    print(\"end_list: \", end_list)\n    print(row[\"text\"])\n    print(txt_lst)\n    print(start)\n    print(end)\n    print(final_text)\n    print(\"-----\")\n\n  selected_text_list.append(final_text)\n\n#print(counter)\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(counter)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub[\"selected_text\"] = selected_text_list\nsub.drop(columns=[\"text\", \"sentiment\"], inplace = True)\nsub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}