{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# we don't want Weights And Biases Logging, the Trainer class by ðŸ¤— Transformers seems to need login credentials which I don't have.\n# so bye-bye wandb\n!pip uninstall -y wandb","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nfrom pathlib import Path\nimport warnings\nimport random\nimport torch \nfrom torch import nn\nimport torch.optim as optim\nfrom sklearn.model_selection import train_test_split\nimport tokenizers\nfrom transformers import RobertaModel, RobertaConfig\n\nwarnings.filterwarnings('ignore')\n\nimport torch\ntorch.cuda.is_available()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def seed_everything(seed_value):\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    os.environ['PYTHONHASHSEED'] = str(seed_value)\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\nseed = 80085\nseed_everything(seed)\n\n\ntrain_split = 0.9\nmax_length = 128\nvocab_size = 8000  # we didn't choose 8k, 8k chose us!\n\n\n# create required directories\nlm_data_dir = \"/kaggle/working/lm_data\"\nmodel_dir = \"/kaggle/working/kidBERTa\"\n!mkdir {lm_data_dir}\n!mkdir {model_dir}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data prep","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/tweet-sentiment-extraction/train.csv')\ntest_df = pd.read_csv('../input/tweet-sentiment-extraction/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = train_df['text'].values.tolist() + test_df['text'].values.tolist()\nprint(len(data), 'total tweets (train + test)')\n\ntrain_data_size = int(len(data)*train_split)\ntrain_data = data[:train_data_size]\neval_data = data[train_data_size:]\n\ndef dump2file(d, fp):\n    with open(fp, 'w') as f:\n        for item in d:\n            f.write(\"%s\\n\" % item)\n\n# we need to train the tokernizer with everything we got\ndump2file(data, os.path.join(lm_data_dir,'everything.txt'))\n\n# the Language Model training data\ndump2file(train_data, os.path.join(lm_data_dir,'train.txt'))\n\n# the Language Model eval data\ndump2file(eval_data, os.path.join(lm_data_dir,'eval.txt'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train the tokenizer with everything we got","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from tokenizers import ByteLevelBPETokenizer\n\ntokenizer = ByteLevelBPETokenizer()\ntokenizer.train(files=[f'{lm_data_dir}/everything.txt'], vocab_size=vocab_size, min_frequency=2, special_tokens=[\n    \"<s>\",\n    \"<pad>\",\n    \"</s>\",\n    \"<unk>\",\n    \"<mask>\",\n])\n\n# tokenizer_config = {\n#     \"max_len\": 512\n# }\n# import json\n# with open(f\"{model_dir}/tokenizer_config.json\", 'w+') as fp:\n#     json.dump(tokenizer_config, fp)\n\ntokenizer.save(model_dir)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tokenizers.implementations import ByteLevelBPETokenizer\nfrom tokenizers.processors import BertProcessing\n\n\ntokenizer = ByteLevelBPETokenizer(\n    f\"{model_dir}/vocab.json\",\n    f\"{model_dir}/merges.txt\",\n)\ntokenizer._tokenizer.post_processor = BertProcessing(\n    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n)\ntokenizer.enable_truncation(max_length=max_length)\n\n\n\ntokenizer.encode(\"the kid shall not overfit!\").tokens","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's reload, else we'll get complains.\n\nfrom transformers import RobertaTokenizerFast\ntokenizer = RobertaTokenizerFast.from_pretrained(model_dir, max_len=max_length)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define kidBERTa\n\n* You can experiment with the sizes, make the kid fatter and name it fatKidBERTa!\n* I like my kids fit, neither overfit nor underfit :D","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import RobertaConfig\n\nconfig = RobertaConfig(\n    vocab_size=vocab_size,\n    intermediate_size=256,\n    max_position_embeddings=256+2,\n    num_attention_heads=1,\n    num_hidden_layers=2,\n    type_vocab_size=1,\n    hidden_size=128,\n)\n\n# save the config for later use\nconfig.to_json_file(f\"{model_dir}/config.json\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import RobertaForMaskedLM\nmodel = RobertaForMaskedLM(config=config)\nmodel","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import DataCollatorForLanguageModeling\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom transformers import LineByLineTextDataset\n\ntrain_dataset = LineByLineTextDataset(\n    tokenizer=tokenizer,\n    file_path=f'{lm_data_dir}/train.txt',\n    block_size=128,\n)\n\neval_dataset = LineByLineTextDataset(\n    tokenizer=tokenizer,\n    file_path=f'{lm_data_dir}/eval.txt',\n    block_size=128,\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import Trainer, TrainingArguments\n\nEPOCHS = 20\n\ntraining_args = TrainingArguments(\n    learning_rate=1e-3,\n    output_dir=model_dir,\n    overwrite_output_dir=True,\n    num_train_epochs=EPOCHS,\n    per_gpu_train_batch_size=128,\n    save_steps=0,\n    save_total_limit=1,\n    do_eval=True,\n    logging_steps=200,\n    evaluate_during_training=True,\n    seed=seed\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    prediction_loss_only=True,\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntrainer.train()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainer.evaluate(eval_dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainer.save_model(model_dir)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Done\n\nNow we need to finetune as Q&A on the comepetition dataset. \n\nTo keep things short and simple, we'll do that in another kernel\n\nBut let's check whether we can load a Q&A model from this LanguageModel or not..","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls {model_dir}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kidBERTa_config = RobertaConfig.from_pretrained(f'{model_dir}/config.json', output_hidden_states=True)    \nkidBERTa = RobertaModel.from_pretrained(f'{model_dir}/pytorch_model.bin', config=kidBERTa_config)\nkidBERTa","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}