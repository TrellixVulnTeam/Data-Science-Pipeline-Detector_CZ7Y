{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport warnings\nimport random\nimport torch\nimport torch.nn as nn\nfrom sklearn.model_selection import StratifiedKFold\nimport tokenizers\nfrom transformers import RobertaModel, RobertaConfig\n\nfrom tqdm import tqdm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/tweet-sentiment-extraction/train.csv')\ntrain_df['text'] = train_df['text'].astype(str)\ntrain_df['selected_text'] = train_df['selected_text'].astype(str)\nt = {}\nfor i in range(len(train_df)):\n    if train_df['sentiment'][i] == 'neutral': continue\n    w = train_df['selected_text'][i][0]\n    if w == '#':\n        print('---------')\n        print(train_df['text'][i])\n        print(train_df['selected_text'][i])\n        \n    if not w.isalpha():\n        if w not in t:\n            t[w] = 1\n        else:\n            t[w] += 1\nt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def check(a, b, l):\n    for i in range(len(b)):\n        if b[i] == ' ' and a[l] != ' ': \n            continue \n        if b[i] != ' ' and l <= len(a)- 1 and a[l] == ' ':\n            l += 1\n            continue\n        if l <= len(a) - 1 and b[i] == a[l]:\n            l += 1\n        else:\n            return False          \n    return True\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# base = pd.read_csv('../input/baseline-result/baseline_result.csv')\n# test = pd.read_csv('../input/tweet-sentiment-extraction/test.csv')\n# print(len(base))\n# num = 0\n# t1 = 0\n# t2 = 0\n# t3 = 0\n# t4 = 0\n# tot = 0\n# for i in range(len(base)):\n#     a = test['text'][i].lower()\n#     b = base['selected_text'][i].lower()\n#     while b[0] == ' ':\n#         b = b[1:]\n#     for j in range(len(a)):\n#         if check(a, b, j):\n#             if j - 1 >=0 and a[j-1] != ' ' and not a[j-1].isalpha():\n#                 num += 1\n#                 print('------------')\n#                 print(a)\n#                 print(b)\n#             if j - 2 >= 0 and a[j-1] == ' ' and  a[j-2] != ' ' and not a[j-2].isalpha():\n#                 num += 1\n#                 print(a)\n#                 print(b)\n#                 print('------------')\n#             if j - 3 >=0 and a[j-1] == ' ' and a[j-2] == ' ' and a[j-3] != ' ' and not a[j-3].isalpha():\n#                 print(a)\n#                 print(b)\n#                 print('---------')\n#                 num += 1\n#             if j - 4 >=0 and a[j-1] == ' ' and a[j-2] == ' '  and a[j-3] == ' ' and not a[j-4].isalpha():\n#                 print(a)\n#                 print(b)\n#                 print('---------')\n#                 num += 1\n#             break\n# print(num)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# base = pd.read_csv('../input/baseline-result/baseline_result.csv')\n# test = pd.read_csv('../input/tweet-sentiment-extraction/test.csv')\n# for i in range(len(base)):\n#     a = test['text'][i].lower()\n#     b = base['selected_text'][i].lower()\n#     while b[0] == ' ':\n#         b = b[1:]\n#     for j in range(len(a)):\n#         if check(a, b, j):\n#             if j - 1 >=0 and a[j-1] != ' ' and not a[j-1].isalpha():\n#                 b = a[j-1] + ' ' + b\n#             if j - 2 >= 0 and a[j-1] == ' ' and  a[j-2] != ' ' and not a[j-2].isalpha():\n#                 b = a[j-2] + ' ' + b\n#             if j - 3 >=0 and a[j-1] == ' ' and a[j-2] == ' ' and a[j-3] != ' ' and not a[j-3].isalpha():\n#                 b = a[j-3] + ' ' + b\n#             if j - 4 >=0 and a[j-1] == ' ' and a[j-2] == ' '  and a[j-3] == ' ' and not a[j-4].isalpha():\n#                 b = a[j-4] + ' ' + b\n#             break\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def pp(text, str2):\n\n#     text = text.lower()\n#     str2 = str2.lower()\n#     print('-------')\n#     print(str2)\n#     print(str2[0])\n#     while str2[0] == ' ':\n#         str2 = str2[1:]\n#     for j in range(len(text)):\n#         if check(text, str2, j):\n#             if j - 1 >=0 and text[j-1] != ' ' and not text[j-1].isalpha():\n#                 str2 = text[j-1] + ' ' + str2\n#             if j - 2 >= 0 and text[j-1] == ' ' and  text[j-2] != ' ' and not text[j-2].isalpha():\n#                 str2 = text[j-2] + ' ' + str2\n#             if j - 3 >=0 and text[j-1] == ' ' and text[j-2] == ' ' and text[j-3] != ' ' and not text[j-3].isalpha():\n#                 str2 = text[j-3] + ' ' + str2\n#             if j - 4 >=0 and text[j-1] == ' ' and text[j-2] == ' '  and text[j-3] == ' ' and text[j-4] != ' ' and not text[j-4].isalpha():\n#                 str2 = text[j-4] + ' ' + str2\n#             break\n#     return str2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pre = pd.read_csv('../input/pre-selected-text/pre.csv')\n# print(len(pre))\n# text = []\n# selected_text = []\n# p = []\n# sentiment = []\n# for i in range(len(pre)):\n#     if pre['sentiment'][i] == 'neutral': continue\n#     if pre['selected_text'][i].lower().split() != pre['pre_selected_text'][i].lower().split() and not pre['selected_text'][i][0].isalpha():\n        \n#         text.append(pre['text'][i])\n#         selected_text.append(pre['selected_text'][i])\n#         p.append(pre['pre_selected_text'][i])\n#         sentiment.append(pre['sentiment'][i])\n\n# d = {'text': pd.Series(text), \n#      'selected_text': pd.Series(selected_text),\n#      'pre_selected_text': pd.Series(p),\n#      'sentiment': pd.Series(sentiment)}\n# df = pd.DataFrame(d)\n# t1 = 0\n# t2 = 0\n# t3 = 0\n# for i in range(len(df)):\n#     if '!' in df['text'][i]:\n#         t1 += 1\n#     if '!' in df['selected_text'][i]:\n#         t2 += 1\n#     if '!' in df['pre_selected_text'][i]:\n#         t3 += 1\n# print(t1, t2, t3)\n# print(len(df))\n# df.to_csv('an.csv')\n# print(len(selected_text))\n# print(len(p))\n# print(len(sentiment))\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_df = pd.read_csv('../input/tweet-sentiment-extraction/train.csv')\n# train_df['text'] = train_df['text'].astype(str)\n# train_df['selected_text'] = train_df['selected_text'].astype(str)\n# train_neutral = train_df[train_df['sentiment'] == 'neutral']\n# train_neutral.to_csv('train_neutral.csv')\n# train_positive = train_df[train_df['sentiment'] == 'positive']\n# train_positive.to_csv('train_positive.csv')\n# train_negative = train_df[train_df['sentiment'] == 'negative']\n# train_negative.to_csv('train_negative.csv')\n# print(len(train_neutral), len(train_positive), len(train_negative))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  **Seed**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed_value):\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    os.environ['PYTHONHASHSEED'] = str(seed_value)\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = True\n\nseed = 42\nseed_everything(seed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Data Loader**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class TweetDataset(torch.utils.data.Dataset):\n    def __init__(self, df, max_len=96):\n        self.df = df\n        self.max_len = max_len\n        self.labeled = 'selected_text' in df\n        self.tokenizer = tokenizers.ByteLevelBPETokenizer(\n            vocab_file='../input/roberta/vocab.json', \n            merges_file='../input/roberta/merges.txt', \n            lowercase=True,\n            add_prefix_space=True)\n\n    def __getitem__(self, index):\n        data = {}\n        row = self.df.iloc[index]\n        \n        ids, masks, tweet, offsets = self.get_input_data(row)\n        data['ids'] = ids\n        data['masks'] = masks\n        data['tweet'] = tweet\n        data['offsets'] = offsets\n        data['sentiment'] = row.sentiment\n      \n        data['selected_text'] = row.text\n        if self.labeled:\n            start_idx, end_idx = self.get_target_idx(row, tweet, offsets)\n            data['start_idx'] = start_idx\n            data['end_idx'] = end_idx\n        \n        return data\n\n    def __len__(self):\n        return len(self.df)\n    \n    def get_input_data(self, row):\n        \n        tweet = \" \" + \" \".join(row.text.lower().split())\n        \n        encoding = self.tokenizer.encode(tweet)\n       \n        sentiment_id = self.tokenizer.encode(row.sentiment).ids\n        \n        ids = [0] + sentiment_id + [2, 2] + encoding.ids + [2]\n        \n        offsets = [(0, 0)] * 4 + encoding.offsets + [(0, 0)]\n        \n        pad_len = self.max_len - len(ids)\n        if pad_len > 0:\n            ids += [1] * pad_len\n            offsets += [(0, 0)] * pad_len\n        \n        ids = torch.tensor(ids)\n        masks = torch.where(ids != 1, torch.tensor(1), torch.tensor(0))\n        offsets = torch.tensor(offsets)\n        \n        return ids, masks, tweet, offsets\n        \n    def get_target_idx(self, row, tweet, offsets):\n        selected_text = \" \" +  \" \".join(row.selected_text.lower().split())\n\n        len_st = len(selected_text) - 1\n        idx0 = None\n        idx1 = None\n\n        for ind in (i for i, e in enumerate(tweet) if e == selected_text[1]):\n            if \" \" + tweet[ind: ind+len_st] == selected_text:\n                idx0 = ind\n                idx1 = ind + len_st - 1\n                break\n\n        char_targets = [0] * len(tweet)\n        if idx0 != None and idx1 != None:\n            for ct in range(idx0, idx1 + 1):\n                char_targets[ct] = 1\n\n        target_idx = []\n        for j, (offset1, offset2) in enumerate(offsets):\n            if sum(char_targets[offset1: offset2]) > 0:\n                target_idx.append(j)\n\n        start_idx = target_idx[0]\n        end_idx = target_idx[-1]\n        \n        return start_idx, end_idx\n        \ndef get_train_val_loaders(df, train_idx, val_idx, batch_size=8):\n    train_df = df.iloc[train_idx]\n    val_df = df.iloc[val_idx]\n\n    train_loader = torch.utils.data.DataLoader(\n        TweetDataset(train_df), \n        batch_size=batch_size, \n        shuffle=True, \n        num_workers=2,\n        drop_last=True)\n\n    val_loader = torch.utils.data.DataLoader(\n        TweetDataset(val_df), \n        batch_size=batch_size, \n        shuffle=False, \n        num_workers=2)\n\n    dataloaders_dict = {\"train\": train_loader, \"val\": val_loader}\n\n    return dataloaders_dict\n\ndef get_test_loader(df, batch_size=32):\n    loader = torch.utils.data.DataLoader(\n        TweetDataset(df), \n        batch_size=batch_size, \n        shuffle=False, \n        num_workers=2)    \n    return loader","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Model**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class Build_model(nn.Module):\n    def __init__(self):\n        super(Build_model, self).__init__()\n        config = RobertaConfig.from_pretrained('../input/roberta/config.json', output_hidden_states=True)\n        self.roberta = RobertaModel.from_pretrained('../input/roberta/pytorch_model.bin', config=config)\n        self.dropout = nn.Dropout(0.5)\n        self.final_projection = nn.Linear(config.hidden_size, 2)\n        nn.init.normal_(self.final_projection.weight, std=0.02)\n        nn.init.normal_(self.final_projection.bias, 0)\n        \n        self.w1 = torch.nn.Parameter(torch.FloatTensor(1), requires_grad=True)\n        self.w2 = torch.nn.Parameter(torch.FloatTensor(1), requires_grad=True)\n        self.w3 = torch.nn.Parameter(torch.FloatTensor(1), requires_grad=True)\n        self.w1.data.fill_(0.3)\n        self.w2.data.fill_(0.3)\n        self.w3.data.fill_(0.3)\n    \n    def forward(self, input_ids, attention_mask):\n#         self.w1 = nn.ReLU(self.w1)\n#         self.w2 = nn.ReLU(self.w2)\n#         self.w3 = nn.ReLU(self.w3)\n#         w = self.w1 + self.w2 + self.w3\n        _, _, hs = self.roberta(input_ids, attention_mask)\n        x = torch.stack([hs[-1], hs[-2], hs[-3]])\n        x = torch.mean(x, 0)\n#         x = (self.w1 / w) * hs[-1] + (self.w2 / w) * hs[-1] + (self.w3 / w) * hs[-3]\n        x = self.dropout(x)\n        x = self.final_projection(x)\n        start_prediction, end_prediction = x.split(1, dim=-1)\n        start_prediction = start_prediction.squeeze(-1)\n        end_prediction = end_prediction.squeeze(-1)\n        return start_prediction, end_prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Loss Function**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def loss_function(start_prediction, end_prediction, start_target, end_target):\n    nn_cross = nn.CrossEntropyLoss()\n    start_loss = nn_cross(start_prediction, start_target)\n    end_loss = nn_cross(end_prediction, end_target)\n    return start_loss + end_loss","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Evaluation Function**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def pp(text, str2):\n    if str2 == '': return str2\n    text = text.lower()\n    str2 = str2.lower()\n\n    while str2[0] == ' ':\n        str2 = str2[1:]\n    for j in range(len(text)):\n        if check(text, str2, j):\n            if j - 1 >=0 and text[j-1] != ' ' and not text[j-1].isalpha():\n                str2 = text[j-1] + ' ' + str2\n                return str2\n            if j - 2 >= 0 and text[j-1] == ' ' and  text[j-2] != ' ' and not text[j-2].isalpha():\n                str2 = text[j-2] + ' ' + str2\n                return str2\n#             if j - 3 >=0 and text[j-1] == ' ' and text[j-2] == ' ' and text[j-3] != ' ' and not text[j-3].isalpha():\n#                 str2 = text[j-3] + ' ' + str2\n#                 return str2\n#             if j - 4 >=0 and text[j-1] == ' ' and text[j-2] == ' '  and text[j-3] == ' ' and text[j-4] != ' ' and not text[j-4].isalpha():\n#                 str2 = text[j-4] + ' ' + str2\n#                 return str2\n            break\n    return str2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_selected_text(text, start_idx, end_idx, offsets):\n    selected_text = \"\"\n    for ix in range(start_idx, end_idx + 1):\n        selected_text += text[offsets[ix][0]: offsets[ix][1]]\n        if (ix + 1) < len(offsets) and offsets[ix][1] < offsets[ix + 1][0]:\n            selected_text += \" \"\n    return selected_text\n\ndef jaccard(str1, str2, text): \n    \n#     aa = str1\n#     bb = str2\n#     a = set(str1.lower().split()) \n#     b = set(str2.lower().split())\n#     c = a.intersection(b)\n#     s1 = float(len(c)) / (len(a) + len(b) - len(c))\n    \n    if len(str2.split()) == 1:\n        str2 = pp(text, str2)\n    str2 = pp(text, str2)\n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    s2 = float(len(c)) / (len(a) + len(b) - len(c))\n    return s2\n#     if len(str2) >= 1 and str2[0] == ',' and len(str2)<=5:\n#         if s1 > s2: \n#             print('bad')\n#             print(text)\n#             print(aa)\n#             print(bb)\n#             print(str2)\n#             print(s1, s2)\n#         if s1 < s2: \n#             print('good')\n#             print(s1, s2)\n# #         print('--------')\n# #         print(text)\n# #         print(aa)\n# #         print(bb)\n# #         print(str2)\n#         return s2\n#     return s1\n\n#     if s1 > s2:\n#         print('-------------')\n#         print(text)\n#         print(aa)\n#         print(bb)\n#         print(str2)\n#     if s1 > s2:\n        \n#         print('-------------')\n#         print(len(bb))\n#         print(text)\n#         print(aa)\n#         print(bb)\n#         print(str2)\n#    return max(s1, s2)\n\ndef compute_jaccard_score(text, start_idx, end_idx, start_logits, end_logits, offsets):\n    start_pred = np.argmax(start_logits)\n    end_pred = np.argmax(end_logits)\n    if start_pred > end_pred:\n        pred = text\n    else:\n        pred = get_selected_text(text, start_pred, end_pred, offsets)\n        \n    true = get_selected_text(text, start_idx, end_idx, offsets)\n    \n    return jaccard(true, pred, text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# jaccard(',       rather', ', rather tiredd')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Training & Prediction**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_or_eval_model(model, dataloaders_dict, criterion, optimizer, num_epochs, filename):\n    model.cuda()\n    for epoch in range(num_epochs):\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()\n            else:\n                model.eval()\n            epoch_loss = 0.0\n            epoch_jaccard = 0.0\n            #progress_bar = tqdm(dataloaders_dict[phase], desc='| Epoch {:03d}'.format(epoch), leave=False, disable=False)\n            #for i, data in enumerate(progress_bar):\n            for data in (dataloaders_dict[phase]):\n                ids = data['ids'].cuda()\n                masks = data['masks'].cuda()\n                tweet = data['tweet']\n                offsets = data['offsets'].numpy()\n                start_idx = data['start_idx'].cuda()\n                end_idx = data['end_idx'].cuda()\n                optimizer.zero_grad()\n\n                with torch.set_grad_enabled(phase == 'train'):\n\n                    start_logits, end_logits = model(ids, masks)\n                    loss = criterion(start_logits, end_logits, start_idx, end_idx)\n                \n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n\n                    epoch_loss += loss.item() * len(ids)\n                    \n                    start_idx = start_idx.cpu().detach().numpy()\n                    end_idx = end_idx.cpu().detach().numpy()\n                    start_logits = torch.softmax(start_logits, dim=1).cpu().detach().numpy()\n                    end_logits = torch.softmax(end_logits, dim=1).cpu().detach().numpy()\n                    \n                    for i in range(len(ids)):                        \n                        jaccard_score = compute_jaccard_score(\n                            tweet[i],\n                            start_idx[i],\n                            end_idx[i],\n                            start_logits[i], \n                            end_logits[i], \n                            offsets[i])\n                        epoch_jaccard += jaccard_score\n                    #progress_bar.set_postfix({'loss': epoch_loss / len(dataloaders_dict[phase].dataset), 'jac': epoch_jaccard/len(dataloaders_dict[phase].dataset)}, refresh=False)\n                    \n            epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n            epoch_jaccard = epoch_jaccard / len(dataloaders_dict[phase].dataset)\n            print('Epoch {}/{} | {:^5} | Loss: {:.4f} | Jaccard: {:.4f}'.format(\n                epoch + 1, num_epochs, phase, epoch_loss, epoch_jaccard))\n    \n    torch.save(model.state_dict(), filename)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CHECK****","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# k_fold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n# test = pd.read_csv('../input/tweet-sentiment-extraction/train.csv')\n# test['text'] = test['text'].astype(str)\n# test['selected_text'] = test['selected_text'].astype(str)\n# test_loader = get_test_loader(test)\n# predictions = []\n# models = []\n# for fold in range(k_fold.n_splits):\n#     model = Build_model()\n#     model.cuda()\n#     file_name = f'pseudo_fold{fold+1}.pth'\n#     file_name = '../input/parameter/' + file_name\n#     model.load_state_dict(torch.load(file_name))\n#     model.eval()\n#     models.append(model)\n# num = 0\n# for data in test_loader:\n#     ids = data['ids'].cuda()\n#     masks = data['masks'].cuda()\n#     tweet = data['tweet']\n#     offsets = data['offsets'].numpy()\n#     sentiment = data['sentiment']\n#     selected_text = data['selected_text']\n\n#     start_logits = []\n#     end_logits = []\n#     for model in models:\n#         with torch.no_grad():\n#             output = model(ids, masks)\n#             start_logits.append(torch.softmax(output[0], dim=1).cpu().detach().numpy())\n#             end_logits.append(torch.softmax(output[1], dim=1).cpu().detach().numpy())\n\n#     start_logits = np.mean(start_logits, axis=0)\n#     end_logits = np.mean(end_logits, axis=0)\n    \n#     for i in range(len(ids)): \n#         if sentiment[i] != 'neutral':\n#             start_pred = np.argmax(start_logits[i])\n#             end_pred = np.argmax(end_logits[i])\n#             if start_pred > end_pred:\n#                 pred = tweet[i]\n#             else:\n#                 pred = get_selected_text(tweet[i], start_pred, end_pred, offsets[i])\n#             predictions.append(pred)\n#         else:\n#             num += 1\n#             predictions.append(selected_text[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(len(predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test['pre_selected_text'] = predictions\n# test.to_csv('pre.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# text = []\n# selected_text = []\n# pre_selected_text = []\n# sentiment = []\n# for i in range(len(test)):\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Main**\n**I.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# epochs = 3\n# batch_size = 32\n# k_fold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n\n# train_df = pd.read_csv('../input/tweet-sentiment-extraction/train.csv')\n# train_df['text'] = train_df['text'].astype(str)\n# train_df['selected_text'] = train_df['selected_text'].astype(str)\n\n# #train_neutral = train_df[train_df['sentiment'] == 'neutral']\n# #train_neutral.to_csv('train_neutral.csv')\n# #train_positive = train_df[train_df['sentiment'] == 'positive']\n# #train_positive.to_csv('train_positive.csv')\n# #train_negative = train_df[ train_df['sentiment'] == 'negative']\n# #train_negative.to_csv('train_negative.csv')\n# train_2 = train_df[train_df['sentiment'] != 'neutral']\n# print(len(train_2))\n# for fold,(train_idx, val_idx) in enumerate(k_fold.split(train_2, train_2.sentiment), start=1):\n#     print(f'Fold: {fold}')\n#     model = Build_model()\n#     optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5, betas=(0.9, 0.999))\n#     criterion = loss_function\n#     dataloaders_dict = get_train_val_loaders(train_2, train_idx, val_idx,batch_size)\n    \n#     train_or_eval_model(model, dataloaders_dict, criterion, optimizer, epochs, f'roberta_fold{fold}.pth')\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"II.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"k_fold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\ntest_df = pd.read_csv('../input/tweet-sentiment-extraction/test.csv')\ntest_df['text'] = test_df['text'].astype(str)\ntest_loader = get_test_loader(test_df)\npredictions = []\nmodels = []\nfor fold in range(k_fold.n_splits):\n    print(fold)\n    model = Build_model()\n    model.cuda()\n    #model.load_state_dict(torch.load(f'roberta_fold{fold+1}.pth'))\n    file_name = f'pseudo_fold{fold+1}.pth'\n    file_name = '../input/parameter/' + file_name\n    model.load_state_dict(torch.load(file_name))\n    model.eval()\n    models.append(model)\nnum = 0\nfor data in test_loader:\n    ids = data['ids'].cuda()\n    masks = data['masks'].cuda()\n    tweet = data['tweet']\n    offsets = data['offsets'].numpy()\n    sentiment = data['sentiment']\n    selected_text = data['selected_text']\n\n    start_logits = []\n    end_logits = []\n    for model in models:\n        with torch.no_grad():\n            output = model(ids, masks)\n            start_logits.append(torch.softmax(output[0], dim=1).cpu().detach().numpy())\n            end_logits.append(torch.softmax(output[1], dim=1).cpu().detach().numpy())\n\n    start_logits = np.mean(start_logits, axis=0)\n    end_logits = np.mean(end_logits, axis=0)\n    \n    for i in range(len(ids)): \n        if sentiment[i] != 'neutral':\n            start_pred = np.argmax(start_logits[i])\n            end_pred = np.argmax(end_logits[i])\n            if start_pred > end_pred:\n                pred = tweet[i]\n            else:\n                pred = get_selected_text(tweet[i], start_pred, end_pred, offsets[i])\n                if len(pred.split()) <= 1:\n                    pred = pp(tweet[i], pred)\n            predictions.append(pred)\n        else:\n            num += 1\n            predictions.append(selected_text[i])\nprint(num)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Submission**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# sub_df = pd.read_csv('../input/tweet-sentiment-extraction/sample_submission.csv')\n# sub_df['selected_text'] = predictions\n# sub_df['selected_text'] = sub_df['selected_text'].apply(lambda x: x.replace('!!!!', '!') if len(x.split())==1 else x)\n# sub_df['selected_text'] = sub_df['selected_text'].apply(lambda x: x.replace('..', '.') if len(x.split())==1 else x)\n# sub_df['selected_text'] = sub_df['selected_text'].apply(lambda x: x.replace('...', '.') if len(x.split())==1 else x)\n# sub_df.to_csv('submission.csv', index=False)\n# sub_df.head()\n\ntest_df['selected_text'] = predictions\ntest_df[['textID','selected_text']].to_csv('submission.csv',index=False)\npd.set_option('max_colwidth', 60)\ntest_df.sample(25)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test_df = pd.read_csv(\"../input/tweet-sentiment-extraction/test.csv\").set_index(\"textID\")\n\n# sub_df = pd.read_csv(\"../input/submission-2/submission (4).csv\").set_index(\"textID\")\n\n# # Everything not presented in the public set \n# # will take a value of the original text\n# test_df[\"selected_text\"] = test_df.text\n\n# # Get the public ids and assign them\n# public_idxs = sub_df.index.values\n# test_df.loc[public_idxs, \"selected_text\"] = sub_df.selected_text.values\n# test_df[[\"selected_text\"]].to_csv(\"submission.csv\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}