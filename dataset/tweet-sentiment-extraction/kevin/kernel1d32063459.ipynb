{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport warnings\nimport random\nimport torch \nfrom torch import nn\nimport torch.optim as optim\nfrom sklearn.model_selection import StratifiedKFold\nimport tokenizers\nfrom transformers import RobertaModel, RobertaConfig\nfrom tqdm.notebook import tqdm\nimport sys\nimport matplotlib.pyplot as plt\nimport re\nimport string\n\n%matplotlib inline\nwarnings.filterwarnings('ignore')\n\ndef seed_everything(seed_value):\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    os.environ['PYTHONHASHSEED'] = str(seed_value)\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = True\n\nseed = 42\nseed_everything(seed)\n\nbatch_size = 32\nN = 10\n\nskf = StratifiedKFold(n_splits=N, shuffle=True, random_state=seed)\nNUM_WORKERS = 2\n\nROBERTA_PATH = '/kaggle/input/robertamodel0524/'\nMODEL_CONFIG_PATH = ROBERTA_PATH+'roberta-base-config.json'\nMODEL_PATH = ROBERTA_PATH+'roberta-base-pytorch_model.bin'\nMODEL_VOCAB_PATH = ROBERTA_PATH+'roberta-base-vocab.json'\nMODEL_VOCAB_MERGES_PATH = ROBERTA_PATH+'roberta-base-merges.txt'\noutdir = '/kaggle/input/roberta714kernel/'\n\ntest_file = '/kaggle/input/tweet-sentiment-extraction/test.csv'\nsubmission_template = '/kaggle/input/tweet-sentiment-extraction/sample_submission.csv'\n\nMAX_LEN = 96\nLINEAR_DROPOUT = 0.2\n\nCLS_TOK = 0\nPAD_TOK = 1\nSEP_TOK = 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TweetDataset(torch.utils.data.Dataset):\n  def __init__(self, df, max_len=MAX_LEN):\n    self.df = df\n    self.max_len = max_len\n    self.labeled = 'selected_text' in df\n    self.tokenizer = tokenizers.ByteLevelBPETokenizer(\n        vocab_file = MODEL_VOCAB_PATH, \n        merges_file = MODEL_VOCAB_MERGES_PATH, \n        lowercase=True,\n        add_prefix_space=True)\n\n  def __getitem__(self, index):\n    data = {}\n    row = self.df.iloc[index]\n    \n    ids, masks, tweet, offsets = self.get_input_data(row)\n    data['ids'] = ids\n    data['masks'] = masks\n    data['tweet'] = tweet\n    data['offsets'] = offsets\n    \n    if self.labeled:\n      start_idx, end_idx = self.get_target_idx(row, tweet, offsets)\n      data['start_idx'] = start_idx\n      data['end_idx'] = end_idx\n    \n    return data\n\n  def __len__(self):\n    return len(self.df)\n  \n  def get_input_data(self, row):\n    tweet = \" \" + \" \".join(row.text.lower().split())\n    encoding = self.tokenizer.encode(tweet)\n    sentiment_id = self.tokenizer.encode(row.sentiment).ids\n    ids = [CLS_TOK] + sentiment_id + [SEP_TOK, SEP_TOK] + encoding.ids + [SEP_TOK]\n    offsets = [(0, 0)] * 4 + encoding.offsets + [(0, 0)]\n            \n    pad_len = self.max_len - len(ids)\n    if pad_len > 0:\n      ids += [PAD_TOK] * pad_len\n      offsets += [(0, 0)] * pad_len\n    \n    ids = torch.tensor(ids)\n    masks = torch.where(ids != 1, torch.tensor(1), torch.tensor(0))\n    offsets = torch.tensor(offsets)\n    \n    return ids, masks, tweet, offsets\n      \n  def get_target_idx(self, row, tweet, offsets):\n    selected_text = \" \" +  \" \".join(row.selected_text.lower().split())\n\n    len_st = len(selected_text) - 1\n    idx0 = None\n    idx1 = None\n\n    for ind in (i for i, e in enumerate(tweet) if e == selected_text[1]):\n      if \" \" + tweet[ind: ind+len_st] == selected_text:\n        idx0 = ind\n        idx1 = ind + len_st - 1\n        break\n\n    char_targets = [0] * len(tweet)\n    if idx0 != None and idx1 != None:\n      for ct in range(idx0, idx1 + 1):\n        char_targets[ct] = 1\n\n    target_idx = []\n    for j, (offset1, offset2) in enumerate(offsets):\n      if sum(char_targets[offset1: offset2]) > 0:\n        target_idx.append(j)\n\n    start_idx = target_idx[0]\n    end_idx = target_idx[-1]\n    \n    return start_idx, end_idx","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_test_loader(df, batch_size=32):\n  loader = torch.utils.data.DataLoader(\n    TweetDataset(df), \n    batch_size=batch_size, \n    shuffle=False, \n    num_workers=NUM_WORKERS)    \n  return loader","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TweetModel(nn.Module):\n  def __init__(self):\n    super(TweetModel, self).__init__()\n    \n    config = RobertaConfig.from_pretrained(\n        MODEL_CONFIG_PATH, output_hidden_states=True)    \n    self.roberta = RobertaModel.from_pretrained(\n        MODEL_PATH, config=config)\n    self.dropout = nn.Dropout(LINEAR_DROPOUT)\n    self.fc = nn.Linear(config.hidden_size, 2)\n    nn.init.normal_(self.fc.weight, std=0.02)\n    nn.init.normal_(self.fc.bias, 0)\n\n  def forward(self, input_ids, attention_mask):\n    _, _, hs = self.roberta(input_ids, attention_mask)\n      \n    x = torch.stack([hs[-1], hs[-2], hs[-3]])\n    x = torch.mean(x, 0)\n    x = self.dropout(x)\n    x = self.fc(x)\n    start_logits, end_logits = x.split(1, dim=-1)\n    start_logits = start_logits.squeeze(-1)\n    end_logits = end_logits.squeeze(-1)\n            \n    return start_logits, end_logits","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_selected_text(text, start_idx, end_idx, offsets):\n  selected_text = \"\"\n  for ix in range(start_idx, end_idx + 1):\n    selected_text += text[offsets[ix][0]: offsets[ix][1]]\n    if (ix + 1) < len(offsets) and offsets[ix][1] < offsets[ix + 1][0]:\n      selected_text += \" \"\n  return selected_text\n\ndef jaccard(str1, str2): \n  a = set(str1.lower().split()) \n  b = set(str2.lower().split())\n  c = a.intersection(b)\n  return float(len(c)) / (len(a) + len(b) - len(c))\n\ndef compute_jaccard_score(text, start_idx, end_idx, start_logits, end_logits, offsets):\n  start_pred = np.argmax(start_logits)\n  end_pred = np.argmax(end_logits)\n  if start_pred > end_pred:\n    pred = text\n  else:\n    pred = get_selected_text(text, start_pred, end_pred, offsets)\n      \n  true = get_selected_text(text, start_idx, end_idx, offsets)\n  \n  return jaccard(true, pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ntest_df = pd.read_csv(test_file)\ntest_df['text'] = test_df['text'].astype(str)\ntest_loader = get_test_loader(test_df)\npredictions = []\nmax_votes = []\nmodels = []\n\nprint(\"loading models..\")\nfor fold in range(2, skf.n_splits):\n    model = TweetModel()\n    model.cuda()\n    model.load_state_dict(torch.load(f'{outdir}roberta_fold{fold+1}.pth'))\n    model.eval()\n    print(f\"load {outdir}roberta_fold{fold+1}.pth\")\n    models.append(model)\n    \nfor data in tqdm(test_loader):\n  ids = data['ids'].cuda()\n  masks = data['masks'].cuda()\n  tweet = data['tweet']\n  offsets = data['offsets'].numpy()\n\n  start_logits = []\n  end_logits = []\n  for model in models:\n    with torch.no_grad():\n        output = model(ids, masks)\n        start_logits.append(torch.softmax(output[0], dim=1).cpu().detach().numpy())\n        end_logits.append(torch.softmax(output[1], dim=1).cpu().detach().numpy())\n\n  mean_start_logits = np.mean(start_logits, axis=0)\n  mean_end_logits = np.mean(end_logits, axis=0)\n\n  for i in range(len(ids)):\n    prediction = []\n    mean_pred = None\n    # calculate mean logits\n    mean_start_pred = np.argmax(mean_start_logits[i])\n    mean_end_pred = np.argmax(mean_end_logits[i])\n    if mean_start_pred > mean_end_pred:\n        mean_pred = tweet[i]\n    else:\n        mean_pred = get_selected_text(tweet[i], mean_start_pred, mean_end_pred, offsets[i])\n    \n    for k in range(len(models)):\n        start = np.argmax(start_logits[k][i])\n        end = np.argmax(end_logits[k][i])\n        if start > end:\n            pred = tweet[i]\n        else:\n            pred = get_selected_text(tweet[i], start, end, offsets[i])\n        prediction.append(pred)\n    votes = {}\n    votes[mean_pred] = 1\n    for p in prediction:\n        if p in votes:\n            votes[p] += 1\n        else:\n            votes[p] = 1\n    \n    max_vote = max(votes.values())\n    for v in votes:\n        if votes[v] == max_vote:\n            predictions.append(v)\n            max_votes.append(votes[v])\n            break\n    \n\n#   for i in range(len(ids)):    \n#     start_pred = np.argmax(start_logits[i])\n#     end_pred = np.argmax(end_logits[i])\n#     if start_pred > end_pred:\n#         pred = tweet[i]\n#     else:\n#         pred = get_selected_text(tweet[i], start_pred, end_pred, offsets[i])\n#     predictions.append(pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df = pd.read_csv(submission_template)\n\nsub_df['selected_text'] = predictions\nsub_df['selected_text'] = sub_df['selected_text'].apply(lambda x: x.replace('!!!!', '!') if len(x.split())==1 else x)\nsub_df['selected_text'] = sub_df['selected_text'].apply(lambda x: x.replace('..', '.') if len(x.split())==1 else x)\nsub_df['selected_text'] = sub_df['selected_text'].apply(lambda x: x.replace('...', '.') if len(x.split())==1 else x)\nsub_df.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df.head(batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df_out = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv')\ntest_df_out['selected_text'] = predictions\ntest_df_out['selected_text'] = test_df_out['selected_text'].apply(lambda x: x.replace('!!!!', '!') if len(x.split())==1 else x)\ntest_df_out['selected_text'] = test_df_out['selected_text'].apply(lambda x: x.replace('..', '.') if len(x.split())==1 else x)\ntest_df_out['selected_text'] = test_df_out['selected_text'].apply(lambda x: x.replace('...', '.') if len(x.split())==1 else x)\ntest_df_out['max_votes'] = max_votes\ntest_df_out.to_csv('test_pred.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df_out.head(batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}