{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom transformers import *\nfrom sklearn.model_selection import train_test_split\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thanks to Chris Deotte for his amazing (https://www.kaggle.com/cdeotte/tensorflow-roberta-0-705), which inspired my distilbert attempt, and to Abhishek Thakur for the tokenization (https://www.kaggle.com/abhishek/roberta-inference-5-folds)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Scieżki i wczytanie","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_data_full = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv')\ntest_data = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv') \nsubmission_data = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/sample_submission.csv') \nPRETRAINED_DIR = '../input/roberta-transformers-pytorch/distilroberta-base/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_full = train_data_full.fillna('')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Wydzielenie zbioru walidacyjnego","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data, val_data = train_test_split(train_data_full, test_size=0.1)\ntrain_data = train_data.reset_index(drop=True)\nval_data = val_data.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Szybki test env do sprawdzania składni przed commitem","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"###########################\n###########################\n\n### TEST ENV\n\n# train_data = train_data.head()\n# val_data = val_data.head()\n# test_data = test_data.head()\n\n###########################\n###########################","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Meta parametry","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN = 96\nBATCH_SIZE = 32\nEPOCHS_NUM = 4\nDROPOUT_RATE = 0.2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Wczytanie tokenizera","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = RobertaTokenizer.from_pretrained(PRETRAINED_DIR, lowercase=True, add_prefix_space=True)\nsentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Inicjalizacja tablic","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# train set\ntrain_data_shape = train_data.shape[0]\ninput_ids = np.ones((train_data_shape,MAX_LEN),dtype='int32')\nattention_mask = np.zeros((train_data_shape,MAX_LEN),dtype='int32')\ntoken_type_ids = np.zeros((train_data_shape,MAX_LEN),dtype='int32')\nstart_tokens = np.zeros((train_data_shape,MAX_LEN),dtype='int32')\nend_tokens = np.zeros((train_data_shape,MAX_LEN),dtype='int32')\n\n# val set\nval_data_shape = val_data.shape[0]\ninput_ids_val = np.ones((val_data_shape,MAX_LEN),dtype='int32')\nattention_mask_val = np.zeros((val_data_shape,MAX_LEN),dtype='int32')\ntoken_type_ids_val = np.zeros((val_data_shape,MAX_LEN),dtype='int32')\nstart_tokens_val = np.zeros((val_data_shape,MAX_LEN),dtype='int32')\nend_tokens_val = np.zeros((val_data_shape,MAX_LEN),dtype='int32')\n\n# test set\ntest_data_shape = test_data.shape[0]\ninput_ids_test = np.ones((test_data_shape,MAX_LEN),dtype='int32')\nattention_mask_test = np.zeros((test_data_shape,MAX_LEN),dtype='int32')\ntoken_type_ids_test = np.zeros((test_data_shape,MAX_LEN),dtype='int32')\n\n# predykcja i walidacja\njac = [];\npreds_start = np.zeros((input_ids_test.shape[0],MAX_LEN))\npreds_end = np.zeros((input_ids_test.shape[0],MAX_LEN))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tokenizowanie danych treningowych","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for k in range(train_data_shape):\n    text1 = \" \"+\" \".join(train_data.loc[k,'text'].split())\n    text2 = \" \".join(train_data.loc[k,'selected_text'].split())\n    idx = text1.find(text2)\n    chars = np.zeros((len(text1)))\n    chars[idx:idx+len(text2)]=1\n    if text1[idx-1]==' ': chars[idx-1] = 1 \n    enc = tokenizer.encode(text1) \n        \n    offsets = []; idx=0\n    for t in enc:\n        w = tokenizer.decode([t])\n        offsets.append((idx,idx+len(w)))\n        idx += len(w)\n    \n    toks = []\n    for i,(a,b) in enumerate(offsets):\n        sm = np.sum(chars[a:b])\n        if sm>0: toks.append(i) \n        \n    s_tok = sentiment_id[train_data.loc[k,'sentiment']]\n    input_ids[k,:len(enc)+5] = [0] + enc + [2,2] + [s_tok] + [2]\n    attention_mask[k,:len(enc)+5] = 1\n    if len(toks)>0:\n        start_tokens[k,toks[0]+1] = 1\n        end_tokens[k,toks[-1]+1] = 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tokenizowanie danych walidacyjnych","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for k in range(val_data_shape):\n    text1 = \" \"+\" \".join(val_data.loc[k,'text'].split())\n    text2 = \" \".join(val_data.loc[k,'selected_text'].split())\n    idx = text1.find(text2)\n    chars = np.zeros((len(text1)))\n    chars[idx:idx+len(text2)]=1\n    if text1[idx-1]==' ': chars[idx-1] = 1 \n    enc = tokenizer.encode(text1) \n        \n    offsets = []; idx=0\n    for t in enc:\n        w = tokenizer.decode([t])\n        offsets.append((idx,idx+len(w)))\n        idx += len(w)\n    \n    toks = []\n    for i,(a,b) in enumerate(offsets):\n        sm = np.sum(chars[a:b])\n        if sm>0: toks.append(i) \n        \n    s_tok = sentiment_id[val_data.loc[k,'sentiment']]\n    input_ids_val[k,:len(enc)+5] = [0] + enc + [2,2] + [s_tok] + [2]\n    attention_mask_val[k,:len(enc)+5] = 1\n    if len(toks)>0:\n        start_tokens_val[k,toks[0]+1] = 1\n        end_tokens_val[k,toks[-1]+1] = 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tokenizowanie danych testowych (submission)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for k in range(test_data_shape):\n    text1 = \" \"+\" \".join(test_data.loc[k,'text'].split())\n    enc = tokenizer.encode(text1)                \n    s_tok = sentiment_id[test_data.loc[k,'sentiment']]\n    input_ids_test[k,:len(enc)+5] = [0] + enc + [2,2] + [s_tok] + [2]\n    attention_mask_test[k,:len(enc)+5] = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    if (len(a)==0) & (len(b)==0): return 0.5\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Wczytanie distilberta i dodanie do architekutury ostatnich warstwy","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"K.clear_session()\n\nids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\nmask = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\ntokens = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\nconfig = RobertaConfig.from_pretrained(PRETRAINED_DIR)\nbert_model = TFRobertaModel.from_pretrained(PRETRAINED_DIR,config=config, from_pt=True)\nx = bert_model(ids,attention_mask=mask,token_type_ids=tokens)\nx1 = tf.keras.layers.Dropout(DROPOUT_RATE)(x[0])\nx1 = tf.keras.layers.BatchNormalization()(x1)\nx1 = tf.keras.layers.Conv1D(1,1)(x1)\nx1 = tf.keras.layers.Flatten()(x1)\nx1 = tf.keras.layers.Activation('softmax')(x1)\nx2 = tf.keras.layers.Dropout(DROPOUT_RATE)(x[0]) \nx2 = tf.keras.layers.BatchNormalization()(x2)\nx2 = tf.keras.layers.Conv1D(1,1)(x2)\nx2 = tf.keras.layers.Flatten()(x2)\nx2 = tf.keras.layers.Activation('softmax')(x2)\nmodel = tf.keras.models.Model(inputs=[ids, mask, tokens], outputs=[x1,x2])\noptimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\nmodel.compile(loss='categorical_crossentropy', optimizer=optimizer)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Douczenie modelu","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"callbacks = tf.keras.callbacks.ModelCheckpoint('saved_model.h5', \n                                        monitor='val_loss', \n                                        verbose=1, \n                                        save_best_only=True, \n                                        save_weights_only=True, \n                                        save_freq='epoch')\n\nhistory = model.fit([input_ids, attention_mask, token_type_ids], [start_tokens, end_tokens], \n    epochs=EPOCHS_NUM, \n    batch_size=BATCH_SIZE, \n    verbose=1, \n    callbacks=[callbacks],\n    validation_data=([input_ids_val, attention_mask_val, token_type_ids_val], \n    [start_tokens_val, end_tokens_val]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Wczytanie najlepszego modelu","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model.load_weights('saved_model.h5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predykcja na danych testowych do spradzenia J-score","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_train = model.predict([input_ids,attention_mask,token_type_ids],verbose=1)\npreds_start_train = preds_train[0]\npreds_end_train = preds_train[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_train = []\nall_jac = []\nfor k in range(input_ids.shape[0]):\n    a = np.argmax(preds_start_train[k,])\n    b = np.argmax(preds_end_train[k,])\n    if a>b: \n        st = train_data.loc[k,'text']\n    else:\n        text1 = \" \"+\" \".join(train_data.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode(enc[a:b+1])\n    all_train.append(st)\n    all_jac.append(jaccard(st,train_data.loc[k,'selected_text']))\nprint('Jaccard = {}'.format(np.mean(all_jac)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predykcja dla danych testowych (submission)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = model.predict([input_ids_test,attention_mask_test,token_type_ids_test],verbose=1)\npreds_start = preds[0]\npreds_end = preds[1]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"all_test = []\nfor k in range(input_ids_test.shape[0]):\n    a = np.argmax(preds_start[k,])\n    b = np.argmax(preds_end[k,])\n    if a>b: \n        st = test_data.loc[k,'text']\n    else:\n        text1 = \" \"+\" \".join(test_data.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode(enc[a:b])\n    all_test.append(st)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_data['selected_text'] = all_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_data.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}