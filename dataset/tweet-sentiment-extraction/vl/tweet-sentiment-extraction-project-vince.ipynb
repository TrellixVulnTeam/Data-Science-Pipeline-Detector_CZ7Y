{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Tweet Sentiment Project","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Import relevant libraries","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np    # linear algebra\nimport pandas as pd   # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport math\n\n# plot visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\n# nlp libraries\nimport re\nimport string\nimport nltk\nimport spacy\n\nimport tensorflow as tf\nfrom transformers import *\nimport tokenizers\nimport tensorflow.keras.backend as K\nfrom sklearn.model_selection import StratifiedKFold\nprint('TF version',tf.__version__)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Read data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"datapath= \"/kaggle/input/tweet-sentiment-extraction\"\ntrain   = pd.read_csv(datapath+\"/train.csv\").fillna('')\ntest    = pd.read_csv(datapath+\"/test.csv\").fillna('')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.head())\nprint(train.shape)\nprint(\"Unique sentiment\", train.sentiment.unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(test.head())\nprint(test.shape)\nprint(\"Unique sentiment\", test.sentiment.unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EDA and Data-Preprocessing","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Missing values","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"There is no missing values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train[train.isna().any(axis=1)])\nprint(test[test.isna().any(axis=1)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.dropna(inplace=True)\nprint(train.shape)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Distribution of tweets","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"To check whether there are imbalance positive, neutral and negative tweets","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1,2, figsize=(13,8))\nfig.suptitle('Distribution of sentiment for train and test set')\n\nax1= sns.countplot(x='sentiment', data=train, ax=ax1)\nax2= sns.countplot(x='sentiment', data=test,  ax=ax2)\n\nfor p in ax1.patches:\n    ax1.annotate(format(p.get_height(), '.2f'), \n                 (p.get_x() + p.get_width() / 2., p.get_height()), \n                 ha = 'center', va = 'center', xytext = (0, 10), \n                 textcoords = 'offset points')\nfor p in ax2.patches:\n    ax2.annotate(format(p.get_height(), '.2f'), \n                 (p.get_x() + p.get_width() / 2., p.get_height()), \n                 ha = 'center', va = 'center', xytext = (0, 10), \n                 textcoords = 'offset points')\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There more neutral sentiment than others. Training set has more positive than negative and test set has more positive sentiments.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Check difference between number words in text and selected text","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"eda= train.copy()\neda['words_text']       = train.text.apply(lambda x:len(str(x).split()))\neda['words_selectedtxt']= train.selected_text.apply(lambda x:len(str(x).split()))\n\nfig, ax = plt.subplots(3,2, figsize=(13,15))\nfig.suptitle('Distribution of words by sentiment in training set')\n\nax1= sns.distplot(eda[eda['sentiment']=='positive']['words_text'],       ax=ax[0][0])\nax1.set_title('Positive: Text')\nax2= sns.distplot(eda[eda['sentiment']=='positive']['words_selectedtxt'],ax=ax[0][1])\nax2.set_title('Positive: Selected_Text')\nax3= sns.distplot(eda[eda['sentiment']=='negative']['words_text'],       ax=ax[1][0], color='r')\nax3.set_title('Negative: Text')\nax4= sns.distplot(eda[eda['sentiment']=='negative']['words_selectedtxt'],ax=ax[1][1], color='r')\nax4.set_title('Negative: Selected_Text')\nax5= sns.distplot(eda[eda['sentiment']=='neutral']['words_text'],       ax=ax[2][0], color='g')\nax5.set_title('Neutral: Text')\nax6= sns.distplot(eda[eda['sentiment']=='neutral']['words_selectedtxt'],ax=ax[2][1], color='g')\nax6.set_title('Neutral: Selected_Text')\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Selected text seems to have words that can determine whether the text is positive and negative sentiment. The frequency of selected text for positive and negative sentiment is mainly 1-2 words. Positive and neutral text has more towards less than 10 words than negative. This may imply that tweet users tend to write longer text when the sentiment is negative.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Pre-processing text","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# function to pre-process text\n\ndef pre_process(text):\n    '''Lowercase text, \n    remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = str(text).lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eda['text2']        = eda['text'].apply(lambda x: pre_process(x))\neda['selectedtxt2'] = eda['selected_text'].apply(lambda x: pre_process(x))\neda.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['text2'] = test['text'].apply(lambda x: pre_process(x))\ntest.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Wordcloud","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# function to plot wordcloud\n\ndef word_cloud(text, mask=None, max_words=200, max_font_size=100, \n                   figure_size=(24.0,16.0), color = 'white',\n                   title = None, title_size=40, image_color=False):\n    stopwords = set(STOPWORDS)\n    more_stopwords = {\"u\", \"im\", \"i'll\",\"we're\", \"i'm\", \"wat\", \"about\", \"oh\",'got','one'}\n    stopwords = stopwords.union(more_stopwords)\n\n    wordcloud = WordCloud(background_color=color,\n                    stopwords = stopwords,\n                    max_words = max_words,\n                    max_font_size = max_font_size, \n                    random_state = 42,\n                    width=400, \n                    height=200,\n                    mask = mask)\n    wordcloud.generate(str(text))\n    \n    plt.figure(figsize=figure_size)\n    if image_color:\n        image_colors = ImageColorGenerator(mask);\n        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n        plt.title(title, fontdict={'size': title_size,  \n                                  'verticalalignment': 'bottom'})\n    else:\n        plt.imshow(wordcloud);\n        plt.title(title, fontdict={'size': title_size, 'color': 'black', \n                                  'verticalalignment': 'bottom'})\n    plt.axis('off');\n    plt.tight_layout()  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_cloud(eda[eda['sentiment']=='neutral'].text2, color='white',\n               max_font_size=60,title_size=30,\n               title=\"Wordcloud_NeutralTweets\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Interestingly, some sentiment-linked words like 'best','love','hopeless','smiles', and 'shameless' are part of the neutral tweets. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"word_cloud(eda[eda['sentiment']=='positive'].text2, color='white',\n               max_font_size=60,title_size=30,\n               title=\"Wordcloud_PositiveTweets\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"'Enjoy' and 'fun' and 'laughter' and 'Wow' are part of the positive sentiment tweets.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"word_cloud(eda[eda['sentiment']=='negative'].text2, color='white',\n               max_font_size=60,title_size=30,\n               title=\"Wordcloud_NegativeTweets\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Examples of negative words are 'bullying', 'sad','pain', 'boss', 'couldn't'.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Analysis approach","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Pre-trained model: roBERTa","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model_path = '/kaggle/input/tf-roberta/'\nMAX_LEN    = 96\n\ntokenizer = tokenizers.ByteLevelBPETokenizer(\n    vocab_file=model_path+'vocab-roberta-base.json', \n    merges_file=model_path+'merges-roberta-base.txt', \n    lowercase=True,\n    add_prefix_space=True\n)\nsentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Transform train input","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"trainrows     = train.shape[0]\ninput_ids     = np.ones((trainrows, MAX_LEN),dtype='int32')\nattention_mask= np.zeros((trainrows,MAX_LEN),dtype='int32')\ntoken_type_ids= np.zeros((trainrows,MAX_LEN),dtype='int32')\nstart_tokens  = np.zeros((trainrows,MAX_LEN),dtype='int32')\nend_tokens    = np.zeros((trainrows,MAX_LEN),dtype='int32')\n\nfor k in range(trainrows):\n    \n    # FIND OVERLAP\n    text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n    text2 = \" \".join(train.loc[k,'selected_text'].split())\n    idx   = text1.find(text2)\n    chars = np.zeros((len(text1)))\n    chars[idx:idx+len(text2)]=1\n    if text1[idx-1]==' ': chars[idx-1] = 1 \n    enc = tokenizer.encode(text1) \n        \n    # ID_OFFSETS\n    offsets = []; idx=0\n    for t in enc.ids:\n        w = tokenizer.decode([t])\n        offsets.append((idx,idx+len(w)))\n        idx += len(w)\n    \n    # START END TOKENS\n    toks = []\n    for i,(a,b) in enumerate(offsets):\n        sm = np.sum(chars[a:b])\n        if sm>0: toks.append(i) \n        \n    s_tok = sentiment_id[train.loc[k,'sentiment']]\n    input_ids[k,:len(enc.ids)+3] = [0, s_tok] + enc.ids + [2]\n    attention_mask[k,:len(enc.ids)+3] = 1\n    if len(toks)>0:\n        start_tokens[k,toks[0]+2] = 1\n        end_tokens[k,toks[-1]+2] = 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Transform test input","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"testrows        = test.shape[0]\ninput_ids_t     = np.ones((testrows, MAX_LEN), dtype='int32')\nattention_mask_t= np.zeros((testrows, MAX_LEN), dtype='int32')\ntoken_type_ids_t= np.zeros((testrows, MAX_LEN), dtype='int32')\n\nfor k in range(test.shape[0]):\n        \n    # INPUT_IDS\n    text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n    enc = tokenizer.encode(text1)                \n    s_tok = sentiment_id[test.loc[k,'sentiment']]\n    input_ids_t[k,:len(enc.ids)+3] = [0, s_tok] + enc.ids + [2]\n    attention_mask_t[k,:len(enc.ids)+3] = 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Define model for roBERTa","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We use a pretrained roBERTa base model and add a custom question answer head. First tokens are input into bert_model and we use BERT's first output, i.e. x[0] below. These are embeddings of all input tokens and have shape (batch_size, MAX_LEN, 768). Next we apply tf.keras.layers.Conv1D(filters=1, kernel_size=1) and transform the embeddings into shape (batch_size, MAX_LEN, 1). We then flatten this and apply softmax, so our final output from x1 has shape (batch_size, MAX_LEN). These are one hot encodings of the start tokens indicies (for selected_text). And x2 are the end tokens indicies.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\n\ndef save_weights(model, dst_fn):\n    weights = model.get_weights()\n    with open(dst_fn, 'wb') as f:\n        pickle.dump(weights, f)\n\n\ndef load_weights(model, weight_fn):\n    with open(weight_fn, 'rb') as f:\n        weights = pickle.load(f)\n    model.set_weights(weights)\n    return model\n\ndef loss_fn(y_true, y_pred):\n    # adjust the targets for sequence bucketing\n    ll = tf.shape(y_pred)[1]\n    y_true = y_true[:, :ll]\n    loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred,\n        from_logits=False, label_smoothing=LABEL_SMOOTHING)\n    loss = tf.reduce_mean(loss)\n    return loss\n\ndef build_model():\n    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    padding = tf.cast(tf.equal(ids, PAD_ID), tf.int32)\n\n    lens = MAX_LEN - tf.reduce_sum(padding, -1)\n    max_len = tf.reduce_max(lens)\n    ids_ = ids[:, :max_len]\n    att_ = att[:, :max_len]\n    tok_ = tok[:, :max_len]\n\n    config = RobertaConfig.from_pretrained(model_path+'config-roberta-base.json')\n    bert_model = TFRobertaModel.from_pretrained(model_path+'pretrained-roberta-base.h5',config=config)\n    x = bert_model(ids_,attention_mask=att_,token_type_ids=tok_)\n    \n    x1 = tf.keras.layers.Dropout(0.1)(x[0])\n    x1 = tf.keras.layers.Conv1D(768, 2,padding='same')(x1)\n    x1 = tf.keras.layers.LeakyReLU()(x1)\n    x1 = tf.keras.layers.Dense(1)(x1)\n    x1 = tf.keras.layers.Flatten()(x1)\n    x1 = tf.keras.layers.Activation('softmax')(x1)\n    \n    x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n    x2 = tf.keras.layers.Conv1D(768, 2,padding='same')(x2)\n    x2 = tf.keras.layers.LeakyReLU()(x2)\n    x2 = tf.keras.layers.Dense(1)(x2)\n    x2 = tf.keras.layers.Flatten()(x2)\n    x2 = tf.keras.layers.Activation('softmax')(x2)\n\n    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5) \n    model.compile(loss=loss_fn, optimizer=optimizer)\n\n    # this is required as `model.predict` needs a fixed size!\n    x1_padded = tf.pad(x1, [[0, 0], [0, MAX_LEN - max_len]], constant_values=0.)\n    x2_padded = tf.pad(x2, [[0, 0], [0, MAX_LEN - max_len]], constant_values=0.)\n    \n    padded_model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1_padded,x2_padded])\n    return model, padded_model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model evaluation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    if (len(a)==0) & (len(b)==0): return 0.5\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Train roBERTa Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED   = 42\nEPOCHS = 3\nBATCH_SIZE = 32\nPAD_ID = 1\nLABEL_SMOOTHING = 0.1\nnp.random.seed(SEED)\njac = []\nVER='v0'\nDISPLAY=1 # USE display=1 FOR INTERACTIVE\noof_start   = np.zeros((input_ids.shape[0],MAX_LEN))\noof_end     = np.zeros((input_ids.shape[0],MAX_LEN))\npreds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\npreds_end   = np.zeros((input_ids_t.shape[0],MAX_LEN))\n\nskf = StratifiedKFold(n_splits=3,shuffle=True,random_state=SEED)\nfor fold,(idxT,idxV) in enumerate(skf.split(input_ids,train.sentiment.values)):\n\n    print('#'*25)\n    print('### FOLD %i'%(fold+1))\n    print('#'*25)\n    \n    K.clear_session()\n    model, padded_model = build_model()\n\n    inpT = [input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]]\n    targetT = [start_tokens[idxT,], end_tokens[idxT,]]\n    inpV = [input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]]\n    targetV = [start_tokens[idxV,], end_tokens[idxV,]]\n\n    # sort the validation data\n    shuffleV = np.int32(sorted(range(len(inpV[0])), key=lambda k: (inpV[0][k] == PAD_ID).sum(), reverse=True))\n    inpV = [arr[shuffleV] for arr in inpV]\n    targetV = [arr[shuffleV] for arr in targetV]\n    weight_fn = '%s-roberta-%i.h5'%(VER,fold)\n    \n    for epoch in range(1, EPOCHS + 1):\n        # sort and shuffle: We add random numbers to not have the same order in each epoch\n        shuffleT = np.int32(sorted(range(len(inpT[0])), \n                                   key=lambda k: (inpT[0][k] == PAD_ID).sum() + np.random.randint(-3, 3), \n                                   reverse=True))\n        # shuffle in batches, otherwise short batches will always come in the beginning of each epoch\n        num_batches = math.ceil(len(shuffleT) / BATCH_SIZE)\n        batch_inds = np.random.permutation(num_batches)\n        shuffleT_ = []\n        for batch_ind in batch_inds:\n            shuffleT_.append(shuffleT[batch_ind * BATCH_SIZE: (batch_ind + 1) * BATCH_SIZE])\n        shuffleT = np.concatenate(shuffleT_)\n        # reorder the input data\n        inpT = [arr[shuffleT] for arr in inpT]\n        targetT = [arr[shuffleT] for arr in targetT]\n        model.fit(inpT, targetT, \n            epochs=epoch, initial_epoch=epoch - 1, batch_size=BATCH_SIZE, verbose=DISPLAY, callbacks=[],\n            validation_data=(inpV, targetV), shuffle=False)  # don't shuffle in `fit`\n        save_weights(model, weight_fn)\n\n    print('Loading model...')\n    # model.load_weights('%s-roberta-%i.h5'%(VER,fold))\n    load_weights(model, weight_fn)\n\n    print('Predicting OOF...')\n    oof_start[idxV,],oof_end[idxV,] = padded_model.predict([input_ids[idxV,],attention_mask[idxV,],\n                                                            token_type_ids[idxV,]],verbose=DISPLAY)\n    \n    print('Predicting Test...')\n    preds = padded_model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n    preds_start += preds[0]/skf.n_splits\n    preds_end += preds[1]/skf.n_splits\n    \n    # DISPLAY FOLD JACCARD\n    all = []\n    for k in idxV:\n        a = np.argmax(oof_start[k,])\n        b = np.argmax(oof_end[k,])\n        if a>b: \n            st = train.loc[k,'text'] # IMPROVE CV/LB with better choice here\n        else:\n            text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n            enc = tokenizer.encode(text1)\n            st = tokenizer.decode(enc.ids[a-2:b-1])\n        all.append(jaccard(st,train.loc[k,'selected_text']))\n    jac.append(np.mean(all))\n    print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all))\n    print()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submission","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"selectedtext = []\nfor k in range(input_ids_t.shape[0]):\n    a = np.argmax(preds_start[k,])\n    b = np.argmax(preds_end[k,])\n    if a>b: \n        st = test.loc[k,'text']\n    else:\n        text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode(enc.ids[a-2:b-1])\n    selectedtext.append(st)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['selected_text'] = selectedtext\ntest[['textID','selected_text']].to_csv('submission.csv',index=False)\n#pd.set_option('max_colwidth', 60)\ntest.sample(10)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}