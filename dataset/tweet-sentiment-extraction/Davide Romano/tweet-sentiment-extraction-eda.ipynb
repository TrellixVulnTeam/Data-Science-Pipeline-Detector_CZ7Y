{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Tweet Sentiment Extraction - EDA"},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"0.1\"></a>\n## Table of Contents\n  \n1. [Import libraries](#1)\n1. [Download data](#2)\n1. [EDA](#3)\n1. [WordCloud](#4)"},{"metadata":{},"cell_type":"markdown","source":"## 1. Import libraries <a class=\"anchor\" id=\"1\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\n#nltk.download('stopwords')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport seaborn as sns\n\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nstop = set(stopwords.words('english'))\n\nfrom wordcloud import WordCloud\n\nfrom collections import defaultdict\nfrom collections import Counter\nplt.style.use('ggplot')\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport string","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Download data <a class=\"anchor\" id=\"2\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir('../input/tweet-sentiment-extraction')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/tweet-sentiment-extraction/train.csv')\ntest = pd.read_csv('../input/tweet-sentiment-extraction/test.csv')\nsubmission = pd.read_csv(\"../input/tweet-sentiment-extraction/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('There are {} rows and {} columns in train'.format(train.shape[0],train.shape[1]))\nprint('There are {} rows and {} columns in test'.format(test.shape[0],test.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems that `text` and `selected_text` got one missing data."},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train.text.isnull()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's remove it."},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train[train['text'].notna()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. EDA <a class=\"anchor\" id=\"3\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{},"cell_type":"markdown","source":"### Class distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.sentiment.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"positive_len = train[train['sentiment']=='positive'].shape[0]\nneutral_len = train[train['sentiment']=='neutral'].shape[0]\nnegative_len = train[train['sentiment']=='negative'].shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (7, 5)\nlabels = ['Positive', 'asd0', 'bas']\nplt.bar(10,positive_len,3, label=\"Positive\", color='green')\nplt.bar(15,neutral_len,3, label=\"Neutral\", color='gray')\nplt.bar(20,negative_len,3, label=\"Negative\", color='red')\nplt.legend()\nplt.ylabel('Number of examples')\nplt.title('Sentiment distribution')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Number of characters in tweets"},{"metadata":{"trusted":true},"cell_type":"code","source":"def length(text):    \n    '''a function which returns the length of text'''\n    return len(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['length'] = train['text'].apply(length)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (18.0, 6.0)\nbins = 150\nplt.hist(train[train['sentiment']=='positive']['length'], alpha=0.3, bins=bins, label='Positive')\nplt.hist(train[train['sentiment']=='neutral']['length'], alpha=0.5, bins=bins, label='Neutral')\nplt.hist(train[train['sentiment']=='negative']['length'], alpha=0.65, bins=bins, label='Negative')\nplt.xlabel('length')\nplt.ylabel('numbers')\nplt.legend(loc='upper right')\nplt.xlim(0,150)\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,(ax1,ax2,ax3)=plt.subplots(1,3,figsize=(15,5))\ntweet_len=train[train['sentiment']=='positive']['text'].str.len()\nax1.hist(tweet_len,color='green')\nax1.set_title('positive tweets')\n\ntweet_len=train[train['sentiment']=='neutral']['text'].str.len()\nax2.hist(tweet_len,color='gray')\nax2.set_title('neutral tweets')\n\ntweet_len=train[train['sentiment']=='negative']['text'].str.len()\nax3.hist(tweet_len,color='red')\nax3.set_title('negative tweets')\n\nfig.suptitle('Characters in tweets')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Number of words in a tweet"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,(ax1,ax2,ax3)=plt.subplots(1,3,figsize=(15,5))\ntweet_len=train[train['sentiment']=='positive']['text'].str.split().map(lambda x: len(x))\nax1.hist(tweet_len,color='green')\nax1.set_title('positive tweets')\n\ntweet_len=train[train['sentiment']=='neutral']['text'].str.split().map(lambda x: len(x))\nax2.hist(tweet_len,color='gray')\nax2.set_title('neutral tweets')\n\ntweet_len=train[train['sentiment']=='negative']['text'].str.split().map(lambda x: len(x))\nax3.hist(tweet_len,color='red')\nax3.set_title('negative tweets')\n\nfig.suptitle('Words in a tweet')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  Average word length in a tweet"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,(ax1,ax2,ax3) = plt.subplots(1,3,figsize=(15,5))\n\nword=train[train['sentiment']=='positive']['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax1,color='green')\nax1.set_title('positive tweets')\n\nword=train[train['sentiment']=='neutral']['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='gray')\nax2.set_title('neutral tweets')\n\nword=train[train['sentiment']=='negative']['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax3,color='red')\nax3.set_title('negative tweets')\n\nfig.suptitle('Average word length in each tweet')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_corpus(target):\n    corpus = []\n    \n    for x in train[train['sentiment']==target]['text'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Common stopwords in tweets"},{"metadata":{},"cell_type":"markdown","source":"##### `Positive` class"},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus = create_corpus('positive')\n\ndic = defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n        \ntop = sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (18.0, 6.0)\nx,y = zip(*top)\nplt.bar(x,y,color='green')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### `Neutral` class"},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus = create_corpus('neutral')\n\ndic = defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n\ntop = sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] \n    \n\nplt.rcParams['figure.figsize'] = (18.0, 6.0)\nx,y = zip(*top)\nplt.bar(x,y,color='gray')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### `Negative` class"},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus = create_corpus('negative')\n\ndic = defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n\ntop = sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] \n    \n\nplt.rcParams['figure.figsize'] = (18.0, 6.0)\nx,y = zip(*top)\nplt.bar(x,y,color='red')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In all the tweets belonging to the three sentiment classes `to` is the most common stopword, followed by `the` and `a` (except for the *Negative class* that has `my` before `a`)."},{"metadata":{},"cell_type":"markdown","source":"### Analyzing punctuations"},{"metadata":{},"cell_type":"markdown","source":"##### `Positive` class"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,5))\ncorpus = create_corpus('positive')\n\ndic = defaultdict(int)\nspecial = string.punctuation\nfor i in (corpus):\n    if i in special:\n        dic[i]+=1\n        \nx,y = zip(*dic.items())\nplt.bar(x,y,color='green')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### `Neutral` class"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,5))\ncorpus=create_corpus('neutral')\ndic = defaultdict(int)\nspecial = string.punctuation\nfor i in (corpus):\n    if i in special:\n        dic[i]+=1\n        \nx,y = zip(*dic.items())\nplt.bar(x,y,color='gray')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### `Negative` class"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,5))\ncorpus = create_corpus('negative')\ndic = defaultdict(int)\nspecial = string.punctuation\nfor i in (corpus):\n    if i in special:\n        dic[i]+=1\n        \nx,y = zip(*dic.items())\nplt.bar(x,y,color='red')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Common words"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,5))\ncounter = Counter(corpus)\nmost = counter.most_common()\nx = []\ny = []\nfor word,count in most[:40]:\n    if (word not in stop) :\n        x.append(word)\n        y.append(count)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x=y,y=x)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### N-gram analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_top_tweet_ngrams(corpus, ngram=2, n=None):\n    vec = CountVectorizer(ngram_range=(ngram, ngram)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Most common bigrams"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,5))\ntop_tweet_bigrams=get_top_tweet_ngrams(train['text'], 2)[:10]\nx,y=map(list,zip(*top_tweet_bigrams))\nsns.barplot(x=y,y=x)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Most common trigrams"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,5))\ntop_tweet_bigrams=get_top_tweet_ngrams(train['text'], 3)[:10]\nx,y=map(list,zip(*top_tweet_bigrams))\nsns.barplot(x=y,y=x)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. WordCloud <a class=\"anchor\" id=\"4\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_corpus_df(tweet, target):\n    corpus = []\n    \n    for x in tweet[tweet['sentiment']==target]['text'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### `Positive` class"},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus_new_positive = create_corpus_df(train,'positive')\nlen(corpus_new_positive)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus_new_positive[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nword_cloud = WordCloud(\n    background_color='black',\n    max_font_size=80\n).generate(\" \".join(corpus_new_positive[:50]))\nplt.imshow(word_cloud)\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### `Neutral` class"},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus_new_neutral = create_corpus_df(train,'neutral')\nlen(corpus_new_neutral)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus_new_neutral[:10]","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Generating the wordcloud with the values under the category dataframe\nplt.figure(figsize=(12,8))\nword_cloud = WordCloud(\n    background_color='black',\n    max_font_size=80\n).generate(\" \".join(corpus_new_neutral[:50]))\nplt.imshow(word_cloud)\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### `Negative` class"},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus_new_negative = create_corpus_df(train,'negative')\nlen(corpus_new_negative)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus_new_negative[:10]","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nword_cloud = WordCloud(\n    background_color='black',\n    max_font_size=80\n).generate(\" \".join(corpus_new_negative[:50]))\nplt.imshow(word_cloud)\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}