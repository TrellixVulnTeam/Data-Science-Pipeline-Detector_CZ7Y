{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install transformers","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport plotly\nimport plotly.figure_factory as ff\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport plotly.express as px\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport string\nfrom wordcloud import WordCloud, STOPWORDS \nimport spacy\nfrom tqdm import tqdm\nimport random\nfrom spacy.util import compounding\nfrom spacy.util import minibatch\n\n\nimport os\nimport string\nimport re\n\nimport math\nfrom collections import defaultdict\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom gensim.models import Word2Vec, KeyedVectors\nfrom gensim.models.phrases import Phraser, Phrases\n\nimport nltk\nfrom nltk.corpus import stopwords\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/tweet-sentiment-extraction/train.csv')\ntest_df = pd.read_csv('../input/tweet-sentiment-extraction/test.csv')\ntrain_df.head(8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_df.shape)\nprint(test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_df[train_df['text'].isnull()])\nprint(train_df[train_df['selected_text'].isnull()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.dropna(inplace = True)\n\ntrain_df = train_df.reset_index(drop = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['N_text_words'] = train_df['text'].apply(lambda tweet : len(tweet.split()))\n\ntrain_df['N_selected_text_words'] = train_df['selected_text'].apply(lambda tweet : len(tweet.split()))\n\ntrain_df['N_words_difference'] = train_df['N_text_words'] - train_df['N_selected_text_words']\n\ntrain_df.head(8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"There are {0} unique sentiments having values {1}\".format(train_df['sentiment'].nunique(), train_df['sentiment'].unique()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_neutral = train_df['sentiment'].loc[train_df['sentiment'] == 'neutral'].count()\n\nn_positive = train_df['sentiment'].loc[train_df['sentiment'] == 'positive'].count()\n\nn_negative = train_df['sentiment'].loc[train_df['sentiment'] == 'negative'].count()\n\nprint(f\"Neutral tweets : {n_neutral}\")\nprint(f\"Positive tweets : {n_positive}\")\nprint(f\"Negative tweets : {n_negative}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentiments = ['Neutral', 'Positive', 'Negative']\nfig = go.Figure(data = [go.Pie(labels = sentiments, values=[n_neutral, n_positive, n_negative])])\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))\n\njaccard_score = []\nfor i in range(train_df.shape[0]):\n    str1 = train_df['text'][i].strip()\n    str2 = train_df['selected_text'][i].strip()\n    jaccard_score.append(jaccard(str1,str2))\n\ntrain_df['Jaccard_score'] = jaccard_score\n\ntrain_df.head(8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = str(text).lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['text_cleaned'] = train_df['text'].apply(lambda x:clean_text(x))\ntrain_df['selected_text_cleaned'] = train_df['selected_text'].apply(lambda x:clean_text(x))\n\nSTOPWORDS = stopwords.words('english')\ndef remove_stopwords(text):\n    return [word for word in text.split() if word not in STOPWORDS]\n\ntrain_df['text_cleaned'] = train_df['text_cleaned'].apply(lambda x : remove_stopwords(x))\ntrain_df['selected_text_cleaned'] = train_df['selected_text_cleaned'].apply(lambda x : remove_stopwords(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head(8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_all_words(df_col):\n    all_words_text = []\n    for row in df_col:\n        for word in row:\n            all_words_text.append(word)\n    return all_words_text\n\nall_words_text = get_all_words(train_df['text_cleaned'])\nall_words_selected_text = get_all_words(train_df['selected_text_cleaned'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_words_neutral = get_all_words(train_df[train_df['sentiment'] == 'neutral']['text_cleaned'])\nall_words_positive = get_all_words(train_df[train_df['sentiment'] == 'positive']['text_cleaned'])\nall_words_negative = get_all_words(train_df[train_df['sentiment'] == 'negative']['text_cleaned'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_wordcloud(all_words):\n    stopwords = set(STOPWORDS)\n    more_stopwords = {'u', \"im\"}\n    stopwords = stopwords.union(more_stopwords)\n    all_words = \" \".join(all_words)\n    wordcloud = WordCloud(width = 400, height = 200, \n                background_color ='white',\n                max_words = 200,\n                stopwords = stopwords,\n                min_font_size = 10)\n    wordcloud = wordcloud.generate(all_words)\n    \n    # plot the WordCloud image                        \n    plt.figure(figsize = (8, 8), facecolor = None) \n    plt.imshow(wordcloud) \n    plt.axis(\"off\") \n    plt.tight_layout(pad = 0) \n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_wordcloud(all_words_positive)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_wordcloud(all_words_negative)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_wordcloud(all_words_neutral)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv')\ndf_test = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv')\ndf_submission = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TfidfEmbeddingVectorizer(object):\n\n    def __init__(self, word_model):\n        self.word_model = word_model\n        self.word_idf_weight = None\n        self.vector_size = word_model.wv.vector_size\n       \n\n    def fit(self, docs):  \n        text_docs = []\n        for doc in docs:\n            text_docs.append(\" \".join(doc))\n\n        tfidf = TfidfVectorizer(stop_words='english', max_features=300)\n        tfidf.fit(text_docs)  \n        max_idf = max(tfidf.idf_) \n        self.word_idf_weight = defaultdict(lambda: max_idf,\n                           [(word, tfidf.idf_[i]) for word, i in tfidf.vocabulary_.items()])\n        self.vocabulary_ = tfidf.vocabulary_\n        return self\n\n\n    def transform(self, docs):  \n        doc_word_vector = self.word_average_list(docs)\n        return doc_word_vector\n\n\n    def word_average(self, sent):\n        mean = []\n        for word in sent:\n            if word in self.word_model.wv.vocab:\n                mean.append(self.word_model.wv.get_vector(word) * self.word_idf_weight[word])  # idf weighted\n\n        if not mean: \n            return np.zeros(self.vector_size)\n        else:\n            mean = np.array(mean).mean(axis=0)\n            return mean\n\n\n    def word_average_list(self, docs):\n        return np.vstack([self.word_average(sent) for sent in docs])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nclass MultinomialNBClassifier():\n    def __init__(self, alpha=0):\n        self.prob_w_given_pos = None\n        self.prob_w_given_neut = None\n        self.prob_w_given_neg = None\n        self.prob_pos = None\n        self.prob_neut = None\n        self.prob_neg = None\n    \n\n    def fit(self, X_pos, X_neut, X_neg, alpha=0):\n        num_features = X_pos.shape[1]\n        prob_w_given_pos = np.zeros(num_features)\n        prob_w_given_neut = np.zeros(num_features)\n        prob_w_given_neg = np.zeros(num_features)\n\n        all_feature_sum_pos = X_pos.sum()\n        all_feature_sum_neut = X_neut.sum()\n        all_feature_sum_neg = X_neg.sum()\n\n        for feature in range(num_features):\n            feature_sum_pos = X_pos[:,feature].sum()\n            feature_sum_neut = X_neut[:,feature].sum()\n            feature_sum_neg = X_neg[:,feature].sum()\n\n            prob_w_given_pos[feature] = (feature_sum_pos+alpha)/(all_feature_sum_pos+num_features*alpha)\n            prob_w_given_neut[feature] =(feature_sum_neut+alpha)/(all_feature_sum_neut+num_features*alpha)\n            prob_w_given_neg[feature] =(feature_sum_neg+alpha)/(all_feature_sum_neg+num_features*alpha)\n\n        self.prob_w_given_pos = prob_w_given_pos - (prob_w_given_neut + prob_w_given_neg)\n        self.prob_w_given_neut = prob_w_given_neut - (prob_w_given_neg + prob_w_given_pos)\n        self.prob_w_given_neg = prob_w_given_neg - (prob_w_given_neut + prob_w_given_pos)\n\n        self.prob_pos = X_pos.shape[0]/(X_pos.shape[0] + X_neut.shape[0] + X_neg.shape[0])\n        self.prob_neut = X_neut.shape[0]/(X_pos.shape[0] + X_neut.shape[0] + X_neg.shape[0])\n        self.prob_neg = X_neg.shape[0]/(X_pos.shape[0] + X_neut.shape[0] + X_neg.shape[0])\n\n\n    def predict_selected_text(self, vocab_to_index, text, sentiments):\n        predictions = []\n        num_examples = len(text)\n        for i in range(num_examples):\n            weights_to_use = None\n            tweet = text[i]\n            sentiment = sentiments[i]\n\n            if sentiment == 'neutral':\n                predictions.append(tweet)\n                continue\n            elif sentiment == 'positive':\n                weights_to_use = self.prob_w_given_pos\n            elif sentiment == 'negative':\n                weights_to_use = self.prob_w_given_neg\n\n            words_in_tweet = tweet.split()\n            word_subsets = [words_in_tweet[i:j+1]\n                            for i in range(len(words_in_tweet)) for j in range(i, len(words_in_tweet))]\n\n            lst = sorted(word_subsets, key=len)\n\n            max_weight_sum = 0\n            selected_text = None\n\n            for word_subset in lst:\n                weight_sum = 0\n                for word in word_subset:\n                    translated_word = word.translate(str.maketrans('', '', string.punctuation))\n                    if translated_word in vocab_to_index.keys():\n                        weight_sum += weights_to_use[vocab_to_index[translated_word]]\n\n                if weight_sum > max_weight_sum:\n                    max_weight_sum = weight_sum\n                    selected_text = word_subset\n\n            if selected_text == None:\n                predictions.append(tweet)\n            else:\n                predictions.append(\" \".join(selected_text))\n        return predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_data(rootdir='./'):\n    print('load data \\n')\n    train = pd.read_csv(os.path.join(rootdir, 'train.csv'))\n    test = pd.read_csv(os.path.join(rootdir, 'test.csv'))\n    sample = pd.read_csv(os.path.join(rootdir, 'sample_submission.csv'))\n\n    return train, test, sample\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def jaccard(str1, str2): \n    if len(str1) == 0 and len(str2) == 0:\n        return 1\n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef clean_text(text):\n    text = str(text).lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    \n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert_data(input_data):\n    converted_data = [clean_text(tweet).split() for tweet in input_data['text']]\n    converted_data = [tweet for tweet in converted_data if tweet != []]\n\n    return converted_data\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef train_data(input_data):\n    print('Training data using Word2Vec model \\n')\n\n    common_terms = [\"of\", \"with\", \"without\", \"and\", \"or\", \"the\", \"a\"]\n    phrases = Phrases(input_data, common_terms=common_terms)\n    bigram = Phraser(phrases)\n    input_data = list(bigram[input_data])\n    model = Word2Vec(input_data, min_count=3, size=300, workers=5, window=5, iter=30, sg=1)\n    return model\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef examples(model):\n    print(len(model.wv.vocab))\n    print('Looking into similarities to the word happy:', model.wv.most_similar('happy'))\n    print('Looking into similarities to the word funny:', model.wv.most_similar('funny'))\n    print('Looking into similarities to the word danger:', model.wv.most_similar('danger'))  \n    print('Looking at the similarity distance between happy and weekend:', model.wv.similarity('happy', 'weekend'))\n    print('Looking at the similarity distance between alright and disappointed:', model.wv.similarity('alright', 'disappointed'))\n    print('Looking at the similarity distance between sniffle and sob:', model.wv.similarity('sniffle', 'sob'))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict_selected_text(df, vocab_to_index, pos_w, neut_w, neg_w):\n    predictions = []\n\n    for i, row in df.iterrows():\n        weights_to_use = None\n        tweet = row['text']\n        sentiment = row['sentiment']\n\n        if sentiment == 'neutral':\n            predictions.append(tweet)\n            continue\n        elif sentiment == 'positive':\n            weights_to_use = pos_w\n        elif sentiment == 'negative':\n            weights_to_use = neg_w\n\n        words_in_tweet = tweet.split()\n        word_subsets = [words_in_tweet[i:j+1] for i in range(len(words_in_tweet)) for j in range(i, len(words_in_tweet))]\n        lst = sorted(word_subsets, key = len)\n        \n        max_weight_sum = 0\n        selected_text = None\n\n        for word_subset in lst:\n            weight_sum = 0\n            for word in word_subset:\n                translated_word = word.translate(str.maketrans('', '', string.punctuation))\n                if translated_word in vocab_to_index.keys():\n                    print(translated_word, vocab_to_index[translated_word])\n                    weight_sum += weights_to_use[vocab_to_index[translated_word]]\n                \n            if weight_sum > max_weight_sum:\n                max_weight_sum = weight_sum\n                selected_text = word_subset\n        \n        if selected_text == None:\n            predictions.append(tweet)\n        else:\n            predictions.append(\" \".join(selected_text))\n    return predictions\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"    \nif __name__ == '__main__':\n  train=pd.read_csv('../input/tweet-sentiment-extraction/train.csv')\n  test=pd.read_csv('../input/tweet-sentiment-extraction/test.csv')\n  train.dropna(inplace=True)  \n  converted_data = convert_data(train)\n  w2v_model = train_data(converted_data)\n  examples(w2v_model)\n\n  train['text'] = train['text'].apply(lambda x: clean_text(x))\n  train['selected_text'] = train['selected_text'].apply(lambda x: clean_text(x))\n\n  X_train, X_val = train_test_split(train, train_size = 0.80, random_state = 0)\n    \n  positive_train = X_train[X_train['sentiment'] == 'positive']\n  neutral_train = X_train[X_train['sentiment'] == 'neutral']\n  negative_train = X_train[X_train['sentiment'] == 'negative']\n\n  tfidf_vec_tr = vectorizer = TfidfEmbeddingVectorizer(w2v_model)\n  tfidf_vec_tr.fit(converted_data)                            \n\n   \n  X_positive = tfidf_vec_tr.transform(positive_train['text'])\n  X_neutral = tfidf_vec_tr.transform(neutral_train['text'])\n  X_negative = tfidf_vec_tr.transform(negative_train['text'])\n\n  nb = MultinomialNBClassifier()\n  nb.fit(X_positive, X_neutral, X_negative, alpha=4)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_to_index = {k: v for k, v in vectorizer.vocabulary_.items()}\npredicted_text = nb.predict_selected_text(vocab_to_index, X_val['text'].to_numpy(), X_val['sentiment'].to_numpy())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"    X_val = X_val.assign(predicted_text=predicted_text)\n    X_val['jaccard'] = X_val.apply(lambda x: jaccard(x['selected_text'], x['predicted_text']), axis = 1)\n    print(X_val)\n    print(\"Word2Vec + Tfidf + MultiNB Jaccard Score: {}\".format(np.mean(X_val['jaccard'])))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"    submission_predicted_text = nb.predict_selected_text(vocab_to_index, test['text'].to_numpy(), test['sentiment'].to_numpy())\n    submission_df = pd.DataFrame({'textID': test['textID'], 'selected_text': submission_predicted_text})\n    submission_df.to_csv(os.path.join('./', 'submission.csv'), index=False)\n    print (submission_df)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}