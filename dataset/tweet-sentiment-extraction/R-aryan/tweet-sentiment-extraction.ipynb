{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-30T17:48:18.208202Z","iopub.execute_input":"2021-07-30T17:48:18.208583Z","iopub.status.idle":"2021-07-30T17:48:18.21927Z","shell.execute_reply.started":"2021-07-30T17:48:18.208554Z","shell.execute_reply":"2021-07-30T17:48:18.218128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv').dropna().reset_index(drop=True)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-30T17:48:18.228247Z","iopub.execute_input":"2021-07-30T17:48:18.228558Z","iopub.status.idle":"2021-07-30T17:48:18.322483Z","shell.execute_reply.started":"2021-07-30T17:48:18.228529Z","shell.execute_reply":"2021-07-30T17:48:18.321496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport re\nimport string\nimport random\nimport torch\nimport torch.nn as nn\nimport transformers\nfrom sklearn import model_selection\nfrom tqdm import tqdm\nfrom tokenizers.implementations import BertWordPieceTokenizer\nfrom transformers import AdamW, get_linear_schedule_with_warmup\nfrom torch.utils.data import DataLoader\n# import tokenizers","metadata":{"execution":{"iopub.status.busy":"2021-07-30T17:48:18.32427Z","iopub.execute_input":"2021-07-30T17:48:18.324714Z","iopub.status.idle":"2021-07-30T17:48:18.331323Z","shell.execute_reply.started":"2021-07-30T17:48:18.324675Z","shell.execute_reply":"2021-07-30T17:48:18.33007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Settings:\n#     PROJ_NAME = 'Text_Extraction_AKA_Question_Answering_BERT'\n#     root_path = os.getcwd().split(PROJ_NAME)[0] + PROJ_NAME + \"\\\\\"\n#     APPLICATION_PATH = root_path + \"backend\\\\services\\\\text_extraction\\\\application\\\\\"\n    MAX_LEN = 128\n    TRAIN_BATCH_SIZE = 32\n    VALID_BATCH_SIZE = 16\n    EPOCHS = 10\n    RANDOM_STATE = 42\n    # MODEL_PATH = 'entity_model.bin'\n    TRAIN_NUM_WORKERS = 4\n    VAL_NUM_WORKERS = 2\n\n    # training data directory\n    TRAIN_DATA = '/kaggle/input/tweet-sentiment-extraction/train.csv'\n\n    # test data directory\n    TEST_DATA = '/kaggle/input/tweet-sentiment-extraction/test.csv'\n\n    # weights path\n    WEIGHTS_PATH =\"text_extraction_model.bin\"\n\n    # BERT path\n    BERT_PATH = \"/kaggle/input/bert-base-uncased/\"\n\n    # vocab path\n    VOCAB_PATH = \"/kaggle/input/bert-base-uncased/vocab.txt\"\n\n    # setting up logs path\n#     LOGS_DIRECTORY = root_path + \"backend\\\\services\\\\text_extraction\\\\logs\\\\logs.txt\"\n\n    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    input_dim = 768\n    output_dim = 2\n    bert_model_name = 'bert-base-uncased'\n\n    TOKENIZER = BertWordPieceTokenizer(\n                   f\"{BERT_PATH}/vocab.txt\",\n                   lowercase=True\n      )\n\n    # TOKENIZER = transformers.BertTokenizer.from_pretrained(\n    #     bert_model_name,\n    #     do_lower_case=True\n    # )\n\n    DROPOUT = 0.3\n    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n\n    seed_value = 42\n    test_size = 0.1\n\n    sentiment_id = {\n        'positive': 3893,\n        'negative': 4997,\n        'neutral': 8699\n    }\n    \n    threshold = 0.3\n\n    SPECIAL_TOKENS = (\"[CLS]\", \"[SEP]\")\n","metadata":{"execution":{"iopub.status.busy":"2021-07-30T17:48:18.33361Z","iopub.execute_input":"2021-07-30T17:48:18.334002Z","iopub.status.idle":"2021-07-30T17:48:18.362918Z","shell.execute_reply.started":"2021-07-30T17:48:18.333962Z","shell.execute_reply":"2021-07-30T17:48:18.361536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BERTBaseUncased(nn.Module):\n    def __init__(self):\n        super(BERTBaseUncased, self).__init__()\n        self.settings = Settings\n        self.bert = transformers.BertModel.from_pretrained(self.settings.bert_model_name, return_dict=False)\n        self.l0 = nn.Linear(self.settings.input_dim, self.settings.output_dim)\n\n    def forward(self, input_ids, attention_mask, token_type_ids):\n        # not using sentiment at all\n        sequence_output, pooled_output = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids\n        )\n        # (batch_size, num_tokens, 768)\n        logits = self.l0(sequence_output)\n        # (batch_size, num_tokens, 2)\n        # (batch_size, num_tokens, 1), (batch_size, num_tokens, 1)\n        start_logits, end_logits = logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n        # (batch_size, num_tokens), (batch_size, num_tokens)\n\n        return start_logits, end_logits\n","metadata":{"execution":{"iopub.status.busy":"2021-07-30T17:48:18.3649Z","iopub.execute_input":"2021-07-30T17:48:18.365379Z","iopub.status.idle":"2021-07-30T17:48:18.376918Z","shell.execute_reply.started":"2021-07-30T17:48:18.365339Z","shell.execute_reply":"2021-07-30T17:48:18.37605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Preprocess:\n    def __init__(self):\n        self.settings = Settings\n\n    def clean_text(self, text):\n        text = text.lower()\n        text = re.sub('\\[.*?\\]', '', text)\n        text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n        text = re.sub('<.*?>+', '', text)\n        text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n        text = re.sub('\\n', '', text)\n        text = re.sub('\\w*\\d\\w*', '', text)\n        return text\n\n    def process_data(self, tweet, selected_text, sentiment, tokenizer, max_len):\n        tweet = \" \".join(str(tweet).split())\n        selected_text = \" \".join(str(selected_text).split())\n\n        len_st = len(selected_text)\n        idx0 = None\n        idx1 = None\n        for ind in (i for i, e in enumerate(tweet) if e == selected_text[0]):\n            if tweet[ind: ind + len_st] == selected_text:\n                idx0 = ind\n                idx1 = ind + len_st\n                break\n\n        char_targets = [0] * len(tweet)\n        if idx0 is not None and idx1 is not None:\n            for ct in range(idx0, idx1):\n                char_targets[ct] = 1\n\n        tok_tweet = tokenizer.encode(tweet)\n        input_ids_orig = tok_tweet.ids[1:-1]\n        tweet_offsets = tok_tweet.offsets[1:-1]\n\n        target_idx = []\n        for j, (offset1, offset2) in enumerate(tweet_offsets):\n            if sum(char_targets[offset1: offset2]) > 0:\n                target_idx.append(j)\n\n        targets_start = target_idx[0]\n        targets_end = target_idx[-1]\n\n        input_ids = [101] + [self.settings.sentiment_id[sentiment]] + [102] + input_ids_orig + [102]\n        token_type_ids = [0, 0, 0] + [1] * (len(input_ids_orig) + 1)\n        mask = [1] * len(token_type_ids)\n        tweet_offsets = [(0, 0)] * 3 + tweet_offsets + [(0, 0)]\n        targets_start += 3\n        targets_end += 3\n\n        padding_length = max_len - len(input_ids)\n        if padding_length > 0:\n            input_ids = input_ids + ([0] * padding_length)\n            mask = mask + ([0] * padding_length)\n            token_type_ids = token_type_ids + ([0] * padding_length)\n            tweet_offsets = tweet_offsets + ([(0, 0)] * padding_length)\n\n        return {\n            'ids': input_ids,\n            'mask': mask,\n            'token_type_ids': token_type_ids,\n            'targets_start': targets_start,\n            'targets_end': targets_end,\n            'orig_tweet': tweet,\n            'orig_selected': selected_text,\n            'sentiment': sentiment,\n            'offsets': tweet_offsets\n        }\n","metadata":{"execution":{"iopub.status.busy":"2021-07-30T17:48:18.378373Z","iopub.execute_input":"2021-07-30T17:48:18.378778Z","iopub.status.idle":"2021-07-30T17:48:18.399329Z","shell.execute_reply.started":"2021-07-30T17:48:18.378734Z","shell.execute_reply":"2021-07-30T17:48:18.398231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AverageMeter:\n    \"\"\"\n    Computes and stores the average and current value\n    \"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\nclass EarlyStopping:\n    def __init__(self, patience=7, mode=\"max\", delta=0.001):\n        self.patience = patience\n        self.counter = 0\n        self.mode = mode\n        self.best_score = None\n        self.early_stop = False\n        self.delta = delta\n        if self.mode == \"min\":\n            self.val_score = np.Inf\n        else:\n            self.val_score = -np.Inf\n\n    def __call__(self, epoch_score, model, model_path):\n\n        if self.mode == \"min\":\n            score = -1.0 * epoch_score\n        else:\n            score = np.copy(epoch_score)\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(epoch_score, model, model_path)\n        elif score < self.best_score + self.delta:\n            self.counter += 1\n            print('EarlyStopping counter: {} out of {}'.format(self.counter, self.patience))\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(epoch_score, model, model_path)\n            self.counter = 0\n\n    def save_checkpoint(self, epoch_score, model, model_path):\n        if epoch_score not in [-np.inf, np.inf, -np.nan, np.nan]:\n            print('Validation score improved ({} --> {}). Saving model!'.format(self.val_score, epoch_score))\n            torch.save(model.state_dict(), model_path)\n        self.val_score = epoch_score\n\n\ndef jaccard(str1, str2):\n    a = set(str1.lower().split())\n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))","metadata":{"execution":{"iopub.status.busy":"2021-07-30T17:48:18.40077Z","iopub.execute_input":"2021-07-30T17:48:18.401231Z","iopub.status.idle":"2021-07-30T17:48:18.417144Z","shell.execute_reply.started":"2021-07-30T17:48:18.401189Z","shell.execute_reply":"2021-07-30T17:48:18.416088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TextExtractionDataset:\n    def __init__(self, tweet, sentiment, selected_text):\n        self.settings = Settings\n        self.tweet = tweet\n        self.sentiment = sentiment\n        self.selected_text = selected_text\n        self.tokenizer = self.settings.TOKENIZER\n        self.max_len = self.settings.MAX_LEN\n    \n    def __len__(self):\n        return len(self.tweet)\n    \n    def __getitem__(self, item):\n        tweet = \" \".join(str(self.tweet[item]).split())\n        selected_text = \" \".join(str(self.selected_text[item]).split())\n        \n        len_st = len(selected_text)\n        idx0 = -1\n        idx1 = -1\n        for ind in (i for i, e in enumerate(tweet) if e == selected_text[0]):\n            if tweet[ind: ind+len_st] == selected_text:\n                idx0 = ind\n                idx1 = ind + len_st - 1\n                break\n\n        char_targets = [0] * len(tweet)\n        if idx0 != -1 and idx1 != -1:\n            for j in range(idx0, idx1 + 1):\n                if tweet[j] != \" \":\n                    char_targets[j] = 1\n        \n        tok_tweet = self.tokenizer.encode(sequence=self.sentiment[item], pair=tweet)\n        tok_tweet_tokens = tok_tweet.tokens\n        tok_tweet_ids = tok_tweet.ids\n        tok_tweet_offsets = tok_tweet.offsets[3:-1]\n        # print(tok_tweet_tokens)\n        # print(tok_tweet.offsets)\n        # ['[CLS]', 'spent', 'the', 'entire', 'morning', 'in', 'a', 'meeting', 'w', '/', \n        # 'a', 'vendor', ',', 'and', 'my', 'boss', 'was', 'not', 'happy', 'w', '/', 'them', \n        # '.', 'lots', 'of', 'fun', '.', 'i', 'had', 'other', 'plans', 'for', 'my', 'morning', '[SEP]']\n        targets = [0] * (len(tok_tweet_tokens) - 4)\n        if self.sentiment[item] == \"positive\" or self.sentiment[item] == \"negative\":\n            sub_minus = 8\n        else:\n            sub_minus = 7\n\n        for j, (offset1, offset2) in enumerate(tok_tweet_offsets):\n            if sum(char_targets[offset1 - sub_minus:offset2 - sub_minus]) > 0:\n                targets[j] = 1\n        \n        targets = [0] + [0] + [0] + targets + [0]\n\n        #print(tweet)\n        #print(selected_text)\n        #print([x for i, x in enumerate(tok_tweet_tokens) if targets[i] == 1])\n        targets_start = [0] * len(targets)\n        targets_end = [0] * len(targets)\n\n        non_zero = np.nonzero(targets)[0]\n        if len(non_zero) > 0:\n            targets_start[non_zero[0]] = 1\n            targets_end[non_zero[-1]] = 1\n        \n        #print(targets_start)\n        #print(targets_end)\n\n        mask = [1] * len(tok_tweet_ids)\n        token_type_ids = [0] * 3 + [1] * (len(tok_tweet_ids) - 3)\n\n        padding_length = self.max_len - len(tok_tweet_ids)\n        ids = tok_tweet_ids + ([0] * padding_length)\n        mask = mask + ([0] * padding_length)\n        token_type_ids = token_type_ids + ([0] * padding_length)\n        targets = targets + ([0] * padding_length)\n        targets_start = targets_start + ([0] * padding_length)\n        targets_end = targets_end + ([0] * padding_length)\n\n        sentiment = [1, 0, 0]\n        if self.sentiment[item] == \"positive\":\n            sentiment = [0, 0, 1]\n        if self.sentiment[item] == \"negative\":\n            sentiment = [0, 1, 0]\n\n        return {\n            'ids': torch.tensor(ids, dtype=torch.long),\n            'mask': torch.tensor(mask, dtype=torch.long),\n            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n            'tweet_tokens': \" \".join(tok_tweet_tokens),\n            'targets': torch.tensor(targets, dtype=torch.long),\n            'targets_start': torch.tensor(targets_start, dtype=torch.long),\n            'targets_end': torch.tensor(targets_end, dtype=torch.long),\n            'padding_len': torch.tensor(padding_length, dtype=torch.long),\n            'orig_tweet': self.tweet[item],\n            'orig_selected': self.selected_text[item],\n            'sentiment': torch.tensor(sentiment, dtype=torch.float),\n            'orig_sentiment': self.sentiment[item]\n        }","metadata":{"execution":{"iopub.status.busy":"2021-07-30T17:48:18.418516Z","iopub.execute_input":"2021-07-30T17:48:18.419095Z","iopub.status.idle":"2021-07-30T17:48:18.442223Z","shell.execute_reply.started":"2021-07-30T17:48:18.419057Z","shell.execute_reply":"2021-07-30T17:48:18.441313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Engine:\n    def __init__(self):\n        self.settings = Settings\n\n    def loss_fn(self, start_logits, end_logits, start_positions, end_positions):\n        l1 = nn.BCEWithLogitsLoss()(start_logits, start_positions)\n        l2 = nn.BCEWithLogitsLoss()(end_logits, end_positions)\n        total_loss = (l1 + l2)\n        return total_loss\n\n    def set_seed(self, seed_value=42):\n        random.seed(seed_value)\n        np.random.seed(seed_value)\n        torch.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value)\n\n    def calculate_jaccard_score(self,\n                                original_tweet,\n                                target_string,\n                                sentiment_val,\n                                idx_start,\n                                idx_end,\n                                offsets_start,\n                                offsets_end,\n                                verbose=False):\n\n        offsets = list(zip(offsets_start, offsets_end))\n\n        if idx_end < idx_start:\n            idx_end = idx_start\n\n        filtered_output = \"\"\n        original_tweet_sp = \" \".join(original_tweet.split())\n        for ix in range(idx_start, idx_end + 1):\n            if offsets[ix][0] == 0 and offsets[ix][1] == 0:\n                continue\n            filtered_output += original_tweet_sp[offsets[ix][0]: offsets[ix][1]]\n            if (ix + 1) < len(offsets) and offsets[ix][1] < offsets[ix + 1][0]:\n                filtered_output += \" \"\n\n        filtered_output = filtered_output.replace(\" .\", \".\")\n        filtered_output = filtered_output.replace(\" ?\", \"?\")\n        filtered_output = filtered_output.replace(\" !\", \"!\")\n        filtered_output = filtered_output.replace(\" ,\", \",\")\n        filtered_output = filtered_output.replace(\" ' \", \"'\")\n        filtered_output = filtered_output.replace(\" n't\", \"n't\")\n        filtered_output = filtered_output.replace(\" 'm\", \"'m\")\n        filtered_output = filtered_output.replace(\" do not\", \" don't\")\n        filtered_output = filtered_output.replace(\" 's\", \"'s\")\n        filtered_output = filtered_output.replace(\" 've\", \"'ve\")\n        filtered_output = filtered_output.replace(\" 're\", \"'re\")\n\n        if sentiment_val == \"neutral\":\n            filtered_output = original_tweet\n\n        if sentiment_val != \"neutral\" and verbose == True:\n            if filtered_output.strip().lower() != target_string.strip().lower():\n                print(\"********************************\")\n                print(f\"Output= {filtered_output.strip()}\")\n                print(f\"Target= {target_string.strip()}\")\n                print(f\"Tweet= {original_tweet.strip()}\")\n                print(\"********************************\")\n\n        jac = utils.jaccard(target_string.strip(), filtered_output.strip())\n        return jac\n\n    def train_fn(self, data_loader, model, optimizer, device, schedular):\n        print(\"Starting training...\\n\")\n        model.train()\n        losses = AverageMeter()\n        jaccards = AverageMeter()\n        tk0 = tqdm(data_loader, total=len(data_loader))\n        for bi, d in enumerate(tk0):\n            ids = d[\"ids\"]\n            token_type_ids = d[\"token_type_ids\"]\n            mask = d[\"mask\"]\n            sentiment = d[\"sentiment\"]\n            orig_selected = d[\"orig_selected\"]\n            orig_tweet = d[\"orig_tweet\"]\n            targets_start = d[\"targets_start\"]\n            targets_end = d[\"targets_end\"]\n            \n            # moving tensors to device\n\n            ids = ids.to(device, dtype=torch.long)\n            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n            mask = mask.to(device, dtype=torch.long)\n            targets_start = targets_start.to(device, dtype=torch.float)\n            targets_end = targets_end.to(device, dtype=torch.float)\n\n            # Always clear any previously calculated gradients before performing a\n            # backward pass. PyTorch doesn't do this automatically because\n            # accumulating the gradients is \"convenient while training RNNs\".\n            # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n            model.zero_grad()\n\n            outputs_start, outputs_end = model(\n                input_ids=ids,\n                attention_mask=mask,\n                token_type_ids=token_type_ids,\n            )\n\n            loss = self.loss_fn(outputs_start, outputs_end, targets_start, targets_end)\n\n            # Perform a backward pass to calculate the gradients.\n            loss.backward()\n\n            # Clip the norm of the gradients to 1.0.\n            # This is to help prevent the \"exploding gradients\" problem.\n            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            # Update parameters and take a step using the computed gradient.\n            # The optimizer dictates the \"update rule\"--how the parameters are\n            # modified based on their gradients, the learning rate, etc\n            optimizer.step()\n            # Update the learning rate\n            schedular.step()\n            losses.update(loss.item(), ids.size(0))\n            tk0.set_postfix(loss=losses.avg, jaccard=jaccards.avg)\n\n    def eval_fn(self, data_loader, model, device):\n        model.eval()\n        losses = AverageMeter()\n        jaccards = AverageMeter()\n        all_outputs = []\n        fin_outputs_start = []\n        fin_outputs_end = []\n        fin_tweet_tokens = []\n        fin_padding_lens = []\n        fin_orig_selected = []\n        fin_orig_sentiment = []\n        fin_orig_tweet = []\n        fin_tweet_token_ids = []\n        with torch.no_grad():\n            tk0 = tqdm(data_loader, total=len(data_loader))\n            for bi, d in enumerate(tk0):\n                ids = d[\"ids\"]\n                token_type_ids = d[\"token_type_ids\"]\n                mask = d[\"mask\"]\n                tweet_tokens = d[\"tweet_tokens\"]\n                padding_len = d[\"padding_len\"]\n                sentiment = d[\"sentiment\"]\n                orig_selected = d[\"orig_selected\"]\n                orig_sentiment = d[\"orig_sentiment\"]\n                orig_tweet = d[\"orig_tweet\"]\n                targets_start = d[\"targets_start\"]\n                targets_end = d[\"targets_end\"]\n\n                ids = ids.to(device, dtype=torch.long)\n                token_type_ids = token_type_ids.to(device, dtype=torch.long)\n                mask = mask.to(device, dtype=torch.long)\n                targets_start = targets_start.to(device, dtype=torch.float)\n                targets_end = targets_end.to(device, dtype=torch.float)\n                sentiment = sentiment.to(device, dtype=torch.float)\n\n                outputs_start, outputs_end = model(\n                    input_ids=ids,\n                    attention_mask=mask,\n                    token_type_ids=token_type_ids\n                )\n                loss = self.loss_fn(outputs_start, outputs_end, targets_start, targets_end)\n\n                fin_outputs_start.append(torch.sigmoid(outputs_start).cpu().detach().numpy())\n                fin_outputs_end.append(torch.sigmoid(outputs_end).cpu().detach().numpy())\n\n                fin_padding_lens.extend(padding_len.cpu().detach().numpy().tolist())\n                fin_tweet_token_ids.append(ids.cpu().detach().numpy().tolist())\n\n                fin_tweet_tokens.extend(tweet_tokens)\n                fin_orig_sentiment.extend(orig_sentiment)\n                fin_orig_selected.extend(orig_selected)\n                fin_orig_tweet.extend(orig_tweet)\n\n                losses.update(loss.item(), ids.size(0))\n                tk0.set_postfix(loss=losses.avg)\n\n            fin_outputs_start = np.vstack(fin_outputs_start)\n            fin_outputs_end = np.vstack(fin_outputs_end)\n            fin_tweet_token_ids = np.vstack(fin_tweet_token_ids)\n            threshold = self.settings.threshold\n\n            jaccard_scores = []\n\n            for j in range(len(fin_tweet_tokens)):\n                target_string = fin_orig_selected[j]\n                tweet_tokens = fin_tweet_tokens[j]\n                padding_len = fin_padding_lens[j]\n                original_tweet = fin_orig_tweet[j]\n                sentiment_val = fin_orig_sentiment[j]\n\n                if padding_len > 0:\n                    mask_start = fin_outputs_start[j, :][:-padding_len] >= threshold\n                    mask_end = fin_outputs_end[j, :][:-padding_len] >= threshold\n\n                else:\n                    mask_start = fin_outputs_start[j, 3:-1] >= threshold\n                    mask_end = fin_outputs_end[j, 3:-1] >= threshold\n\n                mask = [0] * len(mask_start)\n                idx_start = np.nonzero(mask_start)[0]\n                idx_end = np.nonzero(mask_end)[0]\n\n                if len(idx_start) > 0:\n                    idx_start = idx_start[0]\n                    if len(idx_end) > 0:\n                        idx_end = idx_end[0]\n                    else:\n                        idx_end = idx_start\n                else:\n                    idx_start = 0\n                    idx_end = 0\n\n                for mj in range(idx_start, idx_end + 1):\n                    mask[mj] = 1\n\n                output_tokens = [x for p, x in enumerate(tweet_tokens.split()) if mask[p] == 1]\n                output_tokens = [x for x in output_tokens if x not in self.settings.SPECIAL_TOKENS]\n\n                final_output = \"\"\n                for ot in output_tokens:\n                    if ot.startswith(\"##\"):\n                        final_output += ot[2:]\n                    elif len(ot) == 1 and ot in string.punctuation:\n                        final_output += ot\n                    else:\n                        final_output += \" \" + ot\n\n                final_output = final_output.strip()\n\n                if sentiment == \"neutral\" or len(original_tweet.split())<4:\n                    final_output = original_tweet\n\n                jac = jaccard(target_string.strip(),final_output.strip())\n\n                jaccard_scores.append(jac)\n\n        mean_jac = np.mean(jaccard_scores)\n        # print(f\"Jaccard score = {mean_jac}\")\n        return mean_jac\n","metadata":{"execution":{"iopub.status.busy":"2021-07-30T17:48:18.444482Z","iopub.execute_input":"2021-07-30T17:48:18.444841Z","iopub.status.idle":"2021-07-30T17:48:18.486762Z","shell.execute_reply.started":"2021-07-30T17:48:18.444804Z","shell.execute_reply":"2021-07-30T17:48:18.48585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Train:\n    def __init__(self):\n        # initialize required class\n        self.settings = Settings\n        self.engine = Engine()\n        self.preprocess = Preprocess()\n\n        # initialize required variables\n        self.bert_text_model = None\n        self.optimizer = None\n        self.scheduler = None\n        self.train_data_loader = None\n        self.val_data_loader = None\n        self.total_steps = None\n        self.best_jaccard = 0\n        self.param_optimizer = None\n        self.optimizer_parameters = None\n        self.total_steps = None\n        self.train_data_loader = None\n        self.validation_data_loader = None\n        self.model_config = None\n\n    def optimizer_params(self):\n        self.param_optimizer = list(self.bert_text_model.named_parameters())\n        self.optimizer_parameters = [\n            {\n                \"params\": [\n                    p for n, p in self.param_optimizer if not any(nd in n for nd in self.settings.no_decay)\n                ],\n                \"weight_decay\": 0.001,\n            },\n            {\n                \"params\": [\n                    p for n, p in self.param_optimizer if any(nd in n for nd in self.settings.no_decay)\n                ],\n                \"weight_decay\": 0.0,\n            },\n        ]\n\n    def __initialize(self):\n        # Instantiate Bert Classifier\n        # self.model_config = transformers.BertConfig.from_pretrained(self.settings.BERT_PATH)\n        # self.model_config.output_hidden_states = True\n        # self.bert_text_model = BERTBaseUncased(conf=self.model_config)\n        self.bert_text_model = BERTBaseUncased()\n        self.bert_text_model.to(self.settings.DEVICE)\n        self.optimizer_params()\n\n        # Create the optimizer\n        self.optimizer = AdamW(self.optimizer_parameters,\n                               lr=5e-5,  # Default learning rate\n                               eps=1e-8  # Default epsilon value\n                               )\n\n        # Set up the learning rate scheduler\n        self.scheduler = get_linear_schedule_with_warmup(self.optimizer,\n                                                         num_warmup_steps=0,  # Default value\n                                                         num_training_steps=self.total_steps)\n\n    def create_data_loaders(self, tweet, sentiment, selected_text, batch_size, num_workers):\n        dataset = TextExtractionDataset(tweet=tweet, sentiment=sentiment, selected_text=selected_text)\n        data_loader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers)\n\n        return data_loader\n\n    def load_data(self, csv_data_path):\n        df = pd.read_csv(csv_data_path).dropna().reset_index(drop=True)\n        #df['text'] = df['text'].apply(lambda x: self.preprocess.clean_text(x))\n        #df['selected_text'] = df['selected_text'].apply(lambda x: self.preprocess.clean_text(x))\n\n        df_train, df_valid = model_selection.train_test_split(\n            df,\n            random_state=self.settings.seed_value,\n            test_size=self.settings.test_size,\n            stratify=df.sentiment.values\n\n        )\n\n        df_train = df_train.reset_index(drop=True)\n        df_valid = df_valid.reset_index(drop=True)\n\n        # creating Data Loaders\n        # train data loader\n        self.train_data_loader = self.create_data_loaders(tweet=df_train.text.values,\n                                                          sentiment=df_train.sentiment.values,\n                                                          selected_text=df_train.selected_text.values,\n                                                          batch_size=self.settings.TRAIN_BATCH_SIZE,\n                                                          num_workers=self.settings.TRAIN_NUM_WORKERS)\n        # validation data loader\n        self.validation_data_loader = self.create_data_loaders(tweet=df_valid.text.values,\n                                                               sentiment=df_valid.sentiment.values,\n                                                               selected_text=df_valid.selected_text.values,\n                                                               batch_size=self.settings.TRAIN_BATCH_SIZE,\n                                                               num_workers=self.settings.TRAIN_NUM_WORKERS)\n        # validation data loader\n\n        self.total_steps = int(len(df_train) / self.settings.TRAIN_BATCH_SIZE * self.settings.EPOCHS)\n\n    def train(self):\n        early_stopping = EarlyStopping(patience=5, mode=\"max\")\n        for epochs in range(self.settings.EPOCHS):\n            self.engine.train_fn(data_loader=self.train_data_loader,\n                                 model=self.bert_text_model,\n                                 optimizer=self.optimizer,\n                                 device=self.settings.DEVICE,\n                                 schedular=self.scheduler)\n\n            self.best_jaccard = self.engine.eval_fn(data_loader=self.validation_data_loader,\n                                                    model=self.bert_text_model,\n                                                    device=self.settings.DEVICE)\n\n            print(f\"Jaccard Score = {self.best_jaccard}\")\n            early_stopping(epoch_score=self.best_jaccard,\n                           model=self.bert_text_model,\n                           model_path=self.settings.WEIGHTS_PATH)\n            if early_stopping.early_stop:\n                print(\"Early stopping\")\n                break\n\n    def run(self):\n        try:\n            print(\"Loading and Preparing the Dataset-----!! \")\n            self.load_data(csv_data_path=self.settings.TRAIN_DATA)\n            print(\"Dataset Successfully Loaded and Prepared-----!! \")\n            print()\n            print(\"-\" * 70)\n            print(\"Loading and Initializing the Bert Model -----!! \")\n            self.__initialize()\n            print(\"Model Successfully Loaded and Initialized-----!! \")\n            print()\n            print(\"-\" * 70)\n            print(\"------------------Starting Training-----------!!\")\n            self.engine.set_seed()\n            self.train()\n            print(\"Training complete-----!!!\")\n\n        except BaseException as ex:\n            print(\"Following Exception Occurred---!! \", str(ex))\n","metadata":{"execution":{"iopub.status.busy":"2021-07-30T17:48:18.488411Z","iopub.execute_input":"2021-07-30T17:48:18.489Z","iopub.status.idle":"2021-07-30T17:48:18.51148Z","shell.execute_reply.started":"2021-07-30T17:48:18.48894Z","shell.execute_reply":"2021-07-30T17:48:18.51055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    t1=Train()\n    t1.run()","metadata":{"execution":{"iopub.status.busy":"2021-07-30T17:48:18.512904Z","iopub.execute_input":"2021-07-30T17:48:18.513423Z","iopub.status.idle":"2021-07-30T18:21:25.451056Z","shell.execute_reply.started":"2021-07-30T17:48:18.513386Z","shell.execute_reply":"2021-07-30T18:21:25.44982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference","metadata":{}},{"cell_type":"code","source":"device = Settings.DEVICE","metadata":{"execution":{"iopub.status.busy":"2021-07-30T18:23:28.777896Z","iopub.execute_input":"2021-07-30T18:23:28.778339Z","iopub.status.idle":"2021-07-30T18:23:28.784161Z","shell.execute_reply.started":"2021-07-30T18:23:28.778302Z","shell.execute_reply":"2021-07-30T18:23:28.783243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = BERTBaseUncased()\nmodel.to(device)\nmodel.load_state_dict(torch.load(Settings.WEIGHTS_PATH))\nmodel.eval()","metadata":{"execution":{"iopub.status.busy":"2021-07-30T18:23:29.78108Z","iopub.execute_input":"2021-07-30T18:23:29.781426Z","iopub.status.idle":"2021-07-30T18:23:42.40676Z","shell.execute_reply.started":"2021-07-30T18:23:29.781397Z","shell.execute_reply":"2021-07-30T18:23:42.405996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = pd.read_csv(Settings.TEST_DATA).dropna().reset_index(drop=True)\ndf_test.loc[:, \"selected_text\"] = df_test.text.values","metadata":{"execution":{"iopub.status.busy":"2021-07-30T18:27:13.610478Z","iopub.execute_input":"2021-07-30T18:27:13.610808Z","iopub.status.idle":"2021-07-30T18:27:13.632272Z","shell.execute_reply.started":"2021-07-30T18:27:13.610776Z","shell.execute_reply":"2021-07-30T18:27:13.631496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-30T18:27:18.474991Z","iopub.execute_input":"2021-07-30T18:27:18.475369Z","iopub.status.idle":"2021-07-30T18:27:18.490167Z","shell.execute_reply.started":"2021-07-30T18:27:18.475335Z","shell.execute_reply":"2021-07-30T18:27:18.489259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = TextExtractionDataset(\n        tweet=df_test.text.values,\n        sentiment=df_test.sentiment.values,\n        selected_text=df_test.selected_text.values\n    )\n\ndata_loader = torch.utils.data.DataLoader(\n    test_dataset,\n    shuffle=False,\n    batch_size=Settings.VALID_BATCH_SIZE,\n    num_workers=Settings.VAL_NUM_WORKERS\n)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T18:22:19.92179Z","iopub.execute_input":"2021-07-30T18:22:19.922137Z","iopub.status.idle":"2021-07-30T18:22:19.928257Z","shell.execute_reply.started":"2021-07-30T18:22:19.922104Z","shell.execute_reply":"2021-07-30T18:22:19.927185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_outputs = []\nfin_outputs_start = []\nfin_outputs_end = []\nfin_outputs_start2 = []\nfin_outputs_end2 = []\nfin_tweet_tokens = []\nfin_padding_lens = []\nfin_orig_selected = []\nfin_orig_sentiment = []\nfin_orig_tweet = []\nfin_tweet_token_ids = []\n\nwith torch.no_grad():\n    for bi, d in tqdm(enumerate(data_loader), total=len(data_loader)):\n        ids = d[\"ids\"]\n        token_type_ids = d[\"token_type_ids\"]\n        mask = d[\"mask\"]\n        tweet_tokens = d[\"tweet_tokens\"]\n        padding_len = d[\"padding_len\"]\n        sentiment = d[\"sentiment\"]\n        orig_selected = d[\"orig_selected\"]\n        orig_sentiment = d[\"orig_sentiment\"]\n        orig_tweet = d[\"orig_tweet\"]\n\n        ids = ids.to(device, dtype=torch.long)\n        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n        mask = mask.to(device, dtype=torch.long)\n        sentiment = sentiment.to(device, dtype=torch.float)\n\n        outputs_start, outputs_end = model(\n            input_ids=ids,\n            attention_mask=mask,\n            token_type_ids=token_type_ids\n        )\n        \n#         outputs_start2, outputs_end2 = model2(\n#             ids=ids,\n#             mask=mask,\n#             token_type_ids=token_type_ids\n#         )\n\n        fin_outputs_start.append(torch.sigmoid(outputs_start).cpu().detach().numpy())\n        fin_outputs_end.append(torch.sigmoid(outputs_end).cpu().detach().numpy())\n#         fin_outputs_start2.append(torch.sigmoid(outputs_start2).cpu().detach().numpy())\n#         fin_outputs_end2.append(torch.sigmoid(outputs_end2).cpu().detach().numpy())\n        \n        fin_padding_lens.extend(padding_len.cpu().detach().numpy().tolist())\n        fin_tweet_token_ids.append(ids.cpu().detach().numpy().tolist())\n\n        fin_tweet_tokens.extend(tweet_tokens)\n        fin_orig_sentiment.extend(orig_sentiment)\n        fin_orig_selected.extend(orig_selected)\n        fin_orig_tweet.extend(orig_tweet)\n\nfin_outputs_start = np.vstack(fin_outputs_start)\nfin_outputs_end = np.vstack(fin_outputs_end)\n# fin_outputs_start2 = np.vstack(fin_outputs_start2)\n# fin_outputs_end2 = np.vstack(fin_outputs_end2)\n\n# fin_outputs_start = (fin_outputs_start + fin_outputs_start2) / 2\n# fin_outputs_end = (fin_outputs_end + fin_outputs_end2) / 2\n\nfin_tweet_token_ids = np.vstack(fin_tweet_token_ids)\njaccards = []\nthreshold = 0.3\nfor j in range(len(fin_tweet_tokens)):\n    target_string = fin_orig_selected[j]\n    tweet_tokens = fin_tweet_tokens[j]\n    padding_len = fin_padding_lens[j]\n    original_tweet = fin_orig_tweet[j]\n    sentiment_val = fin_orig_sentiment[j]\n\n    if padding_len > 0:\n        mask_start = fin_outputs_start[j, 3:-1][:-padding_len] >= threshold\n        mask_end = fin_outputs_end[j, 3:-1][:-padding_len] >= threshold\n        tweet_token_ids = fin_tweet_token_ids[j, 3:-1][:-padding_len]\n    else:\n        mask_start = fin_outputs_start[j, 3:-1] >= threshold\n        mask_end = fin_outputs_end[j, 3:-1] >= threshold\n        tweet_token_ids = fin_tweet_token_ids[j, 3:-1]\n\n    mask = [0] * len(mask_start)\n    idx_start = np.nonzero(mask_start)[0]\n    idx_end = np.nonzero(mask_end)[0]\n    if len(idx_start) > 0:\n        idx_start = idx_start[0]\n        if len(idx_end) > 0:\n            idx_end = idx_end[0]\n        else:\n            idx_end = idx_start\n    else:\n        idx_start = 0\n        idx_end = 0\n\n    for mj in range(idx_start, idx_end + 1):\n        mask[mj] = 1\n\n    output_tokens = [x for p, x in enumerate(tweet_token_ids) if mask[p] == 1]\n\n    filtered_output = Settings.TOKENIZER.decode(output_tokens)\n    filtered_output = filtered_output.strip().lower()\n\n    if sentiment_val == \"neutral\":\n        filtered_output = original_tweet\n\n    all_outputs.append(filtered_output.strip())","metadata":{"execution":{"iopub.status.busy":"2021-07-30T18:27:22.063241Z","iopub.execute_input":"2021-07-30T18:27:22.063577Z","iopub.status.idle":"2021-07-30T18:27:37.700397Z","shell.execute_reply.started":"2021-07-30T18:27:22.063546Z","shell.execute_reply":"2021-07-30T18:27:37.699473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample = pd.read_csv(\"../input/tweet-sentiment-extraction/sample_submission.csv\")\nsample.loc[:, 'selected_text'] = all_outputs\nsample.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T18:27:40.638511Z","iopub.execute_input":"2021-07-30T18:27:40.639312Z","iopub.status.idle":"2021-07-30T18:27:40.679112Z","shell.execute_reply.started":"2021-07-30T18:27:40.639259Z","shell.execute_reply":"2021-07-30T18:27:40.675769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-30T18:27:41.818691Z","iopub.execute_input":"2021-07-30T18:27:41.819008Z","iopub.status.idle":"2021-07-30T18:27:41.828459Z","shell.execute_reply.started":"2021-07-30T18:27:41.818976Z","shell.execute_reply":"2021-07-30T18:27:41.827404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}