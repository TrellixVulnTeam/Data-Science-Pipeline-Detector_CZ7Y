{"cells":[{"metadata":{"trusted":false},"cell_type":"code","source":"# !pip install torch-lr-finder\n# !pip install fairseq fastBPE\n# !kaggle datasets download -d christofhenkel/bertweet-base-transformers -p transformers_data/bertweet-base-transformers --unzip\n\n# !mkdir models\n# !mkdir data\n\n# model = transformers.RobertaModel.from_pretrained('roberta-base')\n# tokenizer = transformers.RobertaTokenizerFast.from_pretrained('roberta-base')\n# output_path = './transformers_data/roberta-base'\n\n# Path(output_path).mkdir(parents=True, exist_ok=True)\n\n# model.save_pretrained(output_path)\n# tokenizer.save_pretrained(output_path)\n\n\n# !ls -thora $output_path","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install /kaggle/input/fairseq-and-fastbpe/sacrebleu-1.4.9-py3-none-any.whl\n!pip install /kaggle/input/fairseq-and-fastbpe/fairseq-0.9.0-cp37-cp37m-linux_x86_64.whl\n!pip install /kaggle/input/fairseq-and-fastbpe/fastBPE-0.1.0-cp37-cp37m-linux_x86_64.whl","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfrom pathlib import Path\nimport torch\nimport pandas as pd\nimport torch.nn as nn\nimport numpy as np\nimport torch.nn.functional as F\nfrom torch.optim import lr_scheduler\nimport json\n\nfrom sklearn import model_selection\nfrom sklearn import metrics\nimport transformers\nimport tokenizers\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup\nfrom tqdm.autonotebook import tqdm\nimport subprocess\nimport random\n\nfrom types import SimpleNamespace\n\nfrom fairseq.data.encoders.fastbpe import fastBPE\nfrom fairseq.data import Dictionary\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def seed_everything(seed_value):\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    os.environ['PYTHONHASHSEED'] = str(seed_value)\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = True\n\nseed = 777\nseed_everything(seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"class BERTweetTokenizer():\n    def __init__(self, path):\n        self.bpe = fastBPE(SimpleNamespace(bpe_codes=os.path.join(path, 'bpe.codes')))\n        self.vocab = Dictionary()\n        self.vocab.add_from_file(os.path.join(path, 'dict.txt'))\n        \n    def encode(self, text):\n        tokens = self.tokenize(text)\n        input_ids = self.convert_tokens_to_ids(tokens)\n        offsets = []\n        \n        curr_pos = 0\n        for idx, token in enumerate(tokens):\n            # If previous token didn't end with '@@', it means there's a space we\n            # need to account for in offsets\n            need_space = idx > 0 and (not tokens[idx-1].endswith('@@'))\n            curr_pos += need_space\n            offsets.append((curr_pos, curr_pos + len(token.replace('@@', ''))))\n            curr_pos += len(token.replace('@@', ''))            \n        \n        return SimpleNamespace(ids=input_ids, tokens=tokens, offsets=offsets)\n    \n    def tokenize(self, text):\n        return self.bpe.encode(text).split()\n    \n    def convert_tokens_to_ids(self, tokens):\n        input_ids = self.vocab.encode_line(' '.join(tokens), append_eos=False, add_if_not_exist=False).long().tolist()\n        return input_ids\n    \n    def decode_tokens(self, tokens):\n        decoded = ' '.join(tokens).replace('@@ ', '').strip()\n        return decoded\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"class config:\n    VERSION = 'v0-0-5-bertweet'\n\n    INPUT_DATASET = 'tweet-sentiment-extraction'\n    MODEL_DATASET = f'model-{VERSION}'\n    ROBERTA_DATASET = 'bertweet-base-transformers'\n    \n    # If False, only inference on test data will be performed using TEST_FILE, MODELS_DIR, ROBERTA_DIR\n    TRAIN = False\n    IS_KAGGLE = True\n    \n    DATA_DIR = 'data' if not IS_KAGGLE else f'../input/{INPUT_DATASET}'\n    MODELS_DIR = os.path.join(\n        'models' if not IS_KAGGLE else '../input',\n        MODEL_DATASET\n    )\n    ROBERTA_DIR = os.path.join(\n        'transformers_data' if not IS_KAGGLE else '../input',\n        ROBERTA_DATASET\n    )\n\n    FOLD = None\n    N_FOLDS = 5\n#     LEARNING_RATE = 0.2 * 3e-5\n    LEARNING_RATE = 3e-5\n    N_LAST_LAYERS = 3\n    MAX_LEN = 120\n    TRAIN_BATCH_SIZE = 32\n    VALID_BATCH_SIZE = 32\n    EPOCHS = 5\n\n\n    TRAINING_FILE = os.path.join(DATA_DIR, 'train.csv')\n    TEST_FILE = os.path.join(DATA_DIR, 'test.csv')\n    SAMPLE_FILE = os.path.join(DATA_DIR, 'sample_submission.csv')\n    \n    TOKENIZER = BERTweetTokenizer(ROBERTA_DIR)\n\n    @classmethod\n    def get_model_paths(cls):\n        res = []\n        for filename in os.listdir(cls.MODELS_DIR):\n            if filename.endswith('.bin'):\n                res.append(os.path.join(cls.MODELS_DIR, filename))\n        return res\n        \n# !rm -rf $config.MODELS_DIR","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))\n\nclass AverageMeter:\n    \"\"\"\n    Computes and stores the average and current value\n    \"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n        \ndef upload_kaggle_dataset(dir_path, id_, user='drack3800', title=None):\n    title = title or id_\n    metadata = {\n      \"title\": title,\n      \"id\": f\"{user}/{id_}\",\n      \"licenses\": [\n        {\n          \"name\": \"CC0-1.0\"\n        }\n      ]\n    }\n    with open(os.path.join(dir_path, 'dataset-metadata.json'), 'w') as fout:\n        fout.write(json.dumps(metadata) + '\\n')\n    \n    proc = subprocess.run(['kaggle', 'datasets', 'create', '-p' , dir_path], capture_output=True)\n    print(proc.stdout.decode())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"class TweetDataset(torch.utils.data.Dataset):\n    def __init__(self, df, max_len=config.MAX_LEN):\n        self.df = df\n        self.max_len = max_len\n        self.labeled = 'selected_text' in df\n        self.tokenizer = config.TOKENIZER\n\n    def __getitem__(self, index):\n        data = {}\n        row = self.df.iloc[index]\n        \n        ids, masks, tweet, offsets, token_type_ids = self.get_input_data(row)\n        data['ids'] = ids\n        data['mask'] = masks\n        data['tweet'] = tweet\n        data['offsets'] = offsets\n        data['token_type_ids'] = token_type_ids\n        data['sentiment'] = row.sentiment\n        \n        if self.labeled:\n            start_idx, end_idx = self.get_target_idx(row, tweet, offsets)\n            data['start_idx'] = start_idx\n            data['end_idx'] = end_idx\n            data['selected_tweet'] = \" \".join(str(row.selected_text).lower().split())\n        \n        return data\n\n    def __len__(self):\n        return len(self.df)\n    \n    def get_input_data(self, row):\n        tweet = \" \".join(str(row.text).lower().split())\n        encoding = self.tokenizer.encode(tweet)\n        sentiment_id = self.tokenizer.encode(row.sentiment).ids\n        ids = [0] + sentiment_id + [2, 2] + encoding.ids + [2]\n        offsets = [(0, 0)] * 4 + encoding.offsets + [(0, 0)]\n        \n        pad_len = self.max_len - len(ids)\n        if pad_len > 0:\n            ids += [1] * pad_len\n            offsets += [(0, 0)] * pad_len\n        \n        ids = torch.tensor(ids)\n        masks = torch.where(ids != 1, torch.tensor(1), torch.tensor(0))\n        offsets = torch.tensor(offsets)\n        token_type_ids = torch.zeros(len(ids), dtype=torch.int)\n        \n        return ids, masks, tweet, offsets, token_type_ids\n        \n    def get_target_idx(self, row, tweet, offsets):\n        selected_text = \" \".join(str(row.selected_text).lower().split())\n\n        len_st = len(selected_text)\n        idx0 = None\n        idx1 = None\n\n        for ind in range(len(tweet)):\n            if tweet[ind: ind + len_st] == selected_text:\n                idx0 = ind\n                idx1 = ind + len_st - 1\n                break\n\n        char_targets = [0] * len(tweet)\n        if idx0 != None and idx1 != None:\n            for ct in range(idx0, idx1 + 1):\n                char_targets[ct] = 1\n\n        target_idx = []\n        for j, (offset1, offset2) in enumerate(offsets):\n            if sum(char_targets[offset1: offset2]) > 0:\n                target_idx.append(j)\n\n        start_idx = target_idx[0]\n        end_idx = target_idx[-1]\n        \n        return start_idx, end_idx","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"class TweetModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        conf = transformers.RobertaConfig.from_pretrained(\n            config.ROBERTA_DIR,\n            output_hidden_states=True,\n        )\n        self.roberta = transformers.RobertaModel.from_pretrained(\n            os.path.join(config.ROBERTA_DIR, 'model.bin'),\n            config=conf\n        )\n        self.drop_out = nn.Dropout(0.5)\n#         self.l0 = nn.Linear(768 * config.N_LAST_LAYERS, 2)\n        self.l0 = nn.Linear(768, 2)\n        nn.init.normal_(self.l0.weight, std=0.02)\n        nn.init.normal_(self.l0.bias, 0)\n        self.layer_weights = nn.Parameter(torch.ones(config.N_LAST_LAYERS) / config.N_LAST_LAYERS)\n    \n    def forward(self, ids, mask, token_type_ids=None):\n        _, _, out = self.roberta(\n            ids,\n            attention_mask=mask,\n            token_type_ids=token_type_ids\n        )\n        \n        last_layers_out = torch.stack([out[-i] for i in range(1, config.N_LAST_LAYERS + 1)], dim=0)\n        layer_weights = torch.softmax(self.layer_weights, dim=0)\n        out = torch.sum(layer_weights[:, None, None, None] * last_layers_out, dim=0)\n#         out = torch.mean(torch.stack(last_layers_out), dim=0)\n#         out = torch.cat(last_layers_out, dim=-1)\n        out = self.drop_out(out)\n        logits = self.l0(out)\n\n        start_logits, end_logits = logits.split(1, dim=-1)\n\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n\n        return start_logits, end_logits\n    \n\ndef build_model(state_dict_path=None, train=True, device=None):\n    model = TweetModel()\n    if device:\n        model.to(device)\n    if state_dict_path:\n        print(f\"Loaded model from {state_dict_path}\")\n        model.load_state_dict(torch.load(state_dict_path))\n    model.train(train)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def loss_fn(start_logits, end_logits, start_positions, end_positions):\n    loss_fct = nn.CrossEntropyLoss()\n    start_loss = loss_fct(start_logits, start_positions)\n    end_loss = loss_fct(end_logits, end_positions)\n    total_loss = (start_loss + end_loss)\n    return total_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def train_fn(data_loader, model, optimizer, device, num_batches, scheduler=None):\n    ewma_loss = None\n    model.train()\n    tk0 = tqdm(data_loader, total=num_batches, desc=\"Training\")\n    for bi, d in enumerate(tk0):\n        ids = d[\"ids\"]\n        token_type_ids = d[\"token_type_ids\"]\n        mask = d[\"mask\"]\n        targets_start = d['start_idx']\n        targets_end = d['end_idx']\n\n        ids = ids.to(device, dtype=torch.long)\n        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n        mask = mask.to(device, dtype=torch.long)\n        targets_start = targets_start.to(device, dtype=torch.long)\n        targets_end = targets_end.to(device, dtype=torch.long)\n\n        model.zero_grad()\n        outputs_start, outputs_end = model(\n            ids=ids,\n            mask=mask,\n            token_type_ids=token_type_ids,\n        )\n        loss = loss_fn(outputs_start, outputs_end, targets_start, targets_end)\n        loss.backward()\n        optimizer.step()\n        if scheduler is not None:\n            scheduler.step()\n            \n        loss_item = loss.item()\n        if ewma_loss is None:\n            ewma_loss = loss_item\n        else:\n            ewma_loss = 0.8 * ewma_loss + 0.2 * loss_item\n\n        tk0.set_postfix(loss=loss_item, ewma=ewma_loss)\n    return ewma_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def get_best_start_end_idxs(start_logits, end_logits):\n    max_len = len(start_logits)\n    a = np.tile(start_logits, (max_len, 1))\n    b = np.tile(end_logits, (max_len, 1))\n    c = np.tril(a + b.T, k=0).T\n    c[c == 0] = -1000\n    return np.unravel_index(c.argmax(), c.shape)\n\ndef get_selected_text(text, start_idx, end_idx, offsets):\n    selected_text = \"\"\n    for ix in range(start_idx, end_idx + 1):\n        selected_text += text[offsets[ix][0]: offsets[ix][1]]\n        if (ix + 1) < len(offsets) and offsets[ix][1] < offsets[ix + 1][0]:\n            selected_text += \" \"\n    return selected_text\n\ndef compute_jaccard_score(text, start_idx, end_idx, start_logits, end_logits, offsets):\n    start_pred = np.argmax(start_logits)\n    end_pred = np.argmax(end_logits)\n    if start_pred > end_pred:\n        pred = text\n    else:\n        pred = get_selected_text(text, start_pred, end_pred, offsets)\n        \n    true = get_selected_text(text, start_idx, end_idx, offsets)\n    \n    return jaccard(true, pred)\n\ndef eval_fn(data_loader, model, device):\n    model.eval()\n    losses = AverageMeter()\n    jaccards = AverageMeter()\n    \n    with torch.no_grad():\n        for bi, d in enumerate(data_loader):\n            ids = d[\"ids\"]\n            token_type_ids = d[\"token_type_ids\"]\n            mask = d[\"mask\"]\n            sentiment = d[\"sentiment\"]\n            tweet = d[\"tweet\"]\n            targets_start = d['start_idx']\n            targets_end = d['end_idx']\n            offsets = d[\"offsets\"].cpu().numpy()\n\n            ids = ids.to(device, dtype=torch.long)\n            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n            mask = mask.to(device, dtype=torch.long)\n            targets_start = targets_start.to(device, dtype=torch.long)\n            targets_end = targets_end.to(device, dtype=torch.long)\n\n            outputs_start, outputs_end = model(\n                ids=ids,\n                mask=mask,\n                token_type_ids=token_type_ids\n            )\n            loss = loss_fn(outputs_start, outputs_end, targets_start, targets_end)\n            outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n            outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n            jaccard_scores = []\n            for px in range(len(tweet)):\n                jaccard_score = compute_jaccard_score(\n                    tweet[px],\n                    targets_start[px],\n                    targets_end[px],\n                    outputs_start[px], \n                    outputs_end[px], \n                    offsets[px]\n                )\n    \n                jaccard_scores.append(jaccard_score)\n\n            jaccards.update(np.mean(jaccard_scores), ids.size(0))\n            losses.update(loss.item(), ids.size(0))\n\n    return jaccards.avg","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"def run(model, df_train, df_valid, model_name, device='cuda'):\n    model = model.to(device)\n\n    train_dataset = TweetDataset(df_train)\n    train_data_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=config.TRAIN_BATCH_SIZE,\n        shuffle=True,\n        drop_last=True,\n        num_workers=2\n    )\n\n    valid_dataset = TweetDataset(df_valid)\n    valid_data_loader = torch.utils.data.DataLoader(\n        valid_dataset,\n        batch_size=config.VALID_BATCH_SIZE,\n        shuffle=False,\n        drop_last=False,\n        num_workers=1\n    )\n\n    num_train_steps = int(len(df_train) / config.TRAIN_BATCH_SIZE * config.EPOCHS)\n    param_optimizer = list(model.named_parameters())\n#     no_decay = [\n#         \"bias\",\n#         \"LayerNorm.bias\",\n#         \"LayerNorm.weight\"\n#     ]\n#     optimizer_parameters = [\n#         {\n#             'params': [\n#                 p for n, p in param_optimizer if not any(\n#                     nd in n for nd in no_decay\n#                 )\n#             ], \n#          'weight_decay': 0.001\n#         },\n#         {\n#             'params': [\n#                 p for n, p in param_optimizer if any(\n#                     nd in n for nd in no_decay\n#                 )\n#             ], \n#             'weight_decay': 0.0\n#         },\n#     ]\n    optimizer = AdamW(\n#         optimizer_parameters, \n        model.parameters(),\n        lr=config.LEARNING_RATE\n    )\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=int(0.1 * num_train_steps),\n        num_training_steps=num_train_steps\n    )\n\n    best_jac = 0\n    num_batches = int(len(df_train) / config.TRAIN_BATCH_SIZE)\n    \n    print(\"Training is Starting....\")\n\n    for epoch in range(config.EPOCHS):\n        ewma_loss = train_fn(\n            train_data_loader, \n            model, \n            optimizer, \n            device,\n            num_batches,\n            scheduler\n        )\n\n        jac = eval_fn(\n            valid_data_loader, \n            model, \n            device\n        )\n        print(f'Epoch={epoch}, Jaccard={jac:.4f}, Train Loss={ewma_loss:.4f}')\n        if jac > best_jac:\n            model_path = os.path.join(\n                config.MODELS_DIR,\n                f'{model_name}.bin'\n            )\n            os.makedirs(config.MODELS_DIR, exist_ok=True)\n            print(f\"Model Improved! Saving model to {model_path}\")\n            torch.save(model.state_dict(), model_path)\n            best_jac = jac\n            \n    return best_jac\n\n\ndef generate_folds(dfx):\n    skf = model_selection.StratifiedKFold(n_splits=config.N_FOLDS, random_state=777)\n    dfx = dfx.copy()\n    dfx['kfold'] = 0\n    for n_fold, (_, val_idx) in enumerate(skf.split(dfx.values, dfx.sentiment.values)):\n        dfx.loc[val_idx, 'kfold'] = n_fold\n    return dfx","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# df_train = pd.read_csv(config.TRAINING_FILE)\n\n# train_dataset = TweetDataset(df_train)\n    \n# train_data_loader = torch.utils.data.DataLoader(\n#     train_dataset,\n#     batch_size=config.TRAIN_BATCH_SIZE,\n#     shuffle=True,\n#     drop_last=True,\n#     num_workers=2\n# )","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# d = next(iter(train_data_loader))\n# device = 'cuda'\n\n\n# ids = d[\"ids\"]\n# token_type_ids = d[\"token_type_ids\"]\n# mask = d[\"mask\"]\n# sentiment = d[\"sentiment\"]\n# orig_selected = d[\"orig_selected\"]\n# orig_tweet = d[\"orig_tweet\"]\n# targets_start = d[\"targets_start\"]\n# targets_end = d[\"targets_end\"]\n# offsets = d[\"offsets\"].cpu().numpy()\n\n# ids = ids.to(device, dtype=torch.long)\n# token_type_ids = token_type_ids.to(device, dtype=torch.long)\n# mask = mask.to(device, dtype=torch.long)\n# targets_start = targets_start.to(device, dtype=torch.long)\n# targets_end = targets_end.to(device, dtype=torch.long)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# outputs_start, outputs_end = MX(\n#     ids=ids,\n#     mask=mask,\n#     token_type_ids=token_type_ids\n# )","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"if config.TRAIN:\n#     from hyperdash import Experiment\n#     exp = Experiment(config.VERSION)\n    fold_metrics = []\n    dfx = pd.read_csv(config.TRAINING_FILE)\n    dfx = generate_folds(dfx)\n#     dfx = dfx.sample(1000).reset_index(drop=True)\n    \n    if config.FOLD is not None:\n        folds = [config.FOLD]\n        print(\"Going to train in single-fold mode\")\n    else:\n        folds = list(range(config.N_FOLDS))\n        print(\"Going to train in all-folds mode\")\n\n    for n_fold in folds:\n        model_name = f'model_{n_fold}'\n\n        MX = build_model()\n        df_train = dfx[dfx.kfold != n_fold].reset_index(drop=True)\n        df_valid = dfx[dfx.kfold == n_fold].reset_index(drop=True)\n\n        print(f'\\n\\n>>> Running training for fold={n_fold}, model_name={model_name}')\n        jac = run(MX, df_train, df_valid,  model_name, device='cuda')\n        \n        print(f'>>> Finished training for fold={n_fold}. Jaccard = {jac:.4f}')\n        fold_metrics.append(jac)\n#         exp.metric(f'fold-jaccard', jac)\n    \n    print('\\n\\n\\n*********** REPORT **********')\n    print(f'CV Jaccard = {np.mean(fold_metrics):.4f} +- {np.std(fold_metrics):.4f}')\n#     exp.end()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"if config.TRAIN:\n    upload_kaggle_dataset(config.MODELS_DIR, config.MODEL_DATASET)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Predict on Test","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"df_test = pd.read_csv(config.TEST_FILE)\ndevice = torch.device('cuda')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"final_output_per_model = []\n\ntest_dataset = TweetDataset(df_test)\ndata_loader = torch.utils.data.DataLoader(\n    test_dataset,\n    batch_size=config.VALID_BATCH_SIZE,\n    shuffle=False,\n    num_workers=1\n)\n\nfor model_path in config.get_model_paths():\n    model = build_model(model_path, train=False, device=device)\n    final_output = []\n    with torch.no_grad():\n        tk0 = tqdm(data_loader, total=len(data_loader))\n        for bi, d in enumerate(tk0):\n            ids = d[\"ids\"]\n            token_type_ids = d[\"token_type_ids\"]\n            mask = d[\"mask\"]\n\n            ids = ids.to(device, dtype=torch.long)\n            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n            mask = mask.to(device, dtype=torch.long)\n\n            output_start, output_end = model(ids=ids, mask=mask, token_type_ids=token_type_ids)\n\n            output_start = torch.softmax(output_start, dim=1).cpu().detach().numpy()\n            output_end = torch.softmax(output_end, dim=1).cpu().detach().numpy()\n\n            final_output.append((\n                output_start, \n                output_end,\n            ))\n\n    final_output_per_model.append(final_output)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"final_output = []\nwith torch.no_grad():\n    tk0 = tqdm(data_loader, total=len(data_loader))\n    for bi, d in enumerate(tk0):\n        ids = d[\"ids\"]\n        token_type_ids = d[\"token_type_ids\"]\n        mask = d[\"mask\"]\n        sentiment = d[\"sentiment\"]\n        tweet = d[\"tweet\"]\n        offsets = d[\"offsets\"].numpy()\n\n        ids = ids.to(device, dtype=torch.long)\n        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n        mask = mask.to(device, dtype=torch.long)\n        \n        outputs_start = [x[bi][0] for x in final_output_per_model]\n        outputs_end = [x[bi][1] for x in final_output_per_model]\n        \n        output_start = sum(outputs_start) / len(outputs_start)\n        output_end = sum(outputs_end) / len(outputs_end)\n        \n        for i in range(len(ids)):    \n            start_pred = np.argmax(output_start[i])\n            end_pred = np.argmax(output_end[i])\n            if start_pred > end_pred or sentiment[i] == 'neural' or len(tweet[i].split()) < 3:\n                pred = tweet[i]\n            else:\n                pred = get_selected_text(tweet[i], start_pred, end_pred, offsets[i])\n            final_output.append(pred)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sample = pd.read_csv(config.SAMPLE_FILE)\nsample.loc[:, 'selected_text'] = final_output","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sample","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"if config.TRAIN:\n    subm_file = f'{config.VERSION}_submission.csv'\nelse:\n    subm_file = 'submission.csv'\n\nsample.to_csv(subm_file, index=False)\nprint(\"saved to\", subm_file)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python [conda env:kg]","language":"python","name":"conda-env-kg-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false}},"nbformat":4,"nbformat_minor":4}