{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport tokenizers\nimport string\nimport torch\nimport transformers\nimport torch.nn as nn\nfrom tqdm import tqdm\nimport re","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"MAX_LEN = 128\nTRAIN_BATCH_SIZE = 32\nVALID_BATCH_SIZE = 16\nEPOCHS = 50\nBERT_PATH = \"../input/bert-base-uncased/\"\nTOKENIZER = tokenizers.BertWordPieceTokenizer(\n    f\"{BERT_PATH}/vocab.txt\",\n    lowercase=True\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class BERTBaseUncased(nn.Module):\n    def __init__(self):\n        super(BERTBaseUncased, self).__init__()\n        self.bert = transformers.BertModel.from_pretrained(BERT_PATH)\n        self.l0 = nn.Linear(768, 2)\n    \n    def forward(self, ids, mask, token_type_ids):\n        # not using sentiment at all\n        sequence_output, pooled_output = self.bert(\n            ids, \n            attention_mask=mask,\n            token_type_ids=token_type_ids\n        )\n        # (batch_size, num_tokens, 768)\n        logits = self.l0(sequence_output)\n        # (batch_size, num_tokens, 2)\n        # (batch_size, num_tokens, 1), (batch_size, num_tokens, 1)\n        start_logits, end_logits = logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n        # (batch_size, num_tokens), (batch_size, num_tokens)\n\n        return start_logits, end_logits","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda\")\nmodel = BERTBaseUncased()\nmodel.to(device)\nmodel = nn.DataParallel(model)\nmodel.load_state_dict(torch.load(\"../input/tmodel/model.bin\"))\nmodel.eval()\n\nmodel2 = BERTBaseUncased()\nmodel2.to(device)\nmodel2 = nn.DataParallel(model2)\nmodel2.load_state_dict(torch.load(\"../input/tweet-model/model.bin\"))\nmodel2.eval()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TweetDataset:\n    def __init__(self, tweet, sentiment, selected_text):\n        self.tweet = tweet\n        self.sentiment = sentiment\n        self.selected_text = selected_text\n        self.tokenizer = TOKENIZER\n        self.max_len = MAX_LEN\n    \n    def __len__(self):\n        return len(self.tweet)\n    \n    def __getitem__(self, item):\n        tweet = \" \".join(str(self.tweet[item]).split())\n        selected_text = \" \".join(str(self.selected_text[item]).split())\n        \n        len_st = len(selected_text)\n        idx0 = -1\n        idx1 = -1\n        for ind in (i for i, e in enumerate(tweet) if e == selected_text[0]):\n            if tweet[ind: ind+len_st] == selected_text:\n                idx0 = ind\n                idx1 = ind + len_st - 1\n                break\n\n        char_targets = [0] * len(tweet)\n        if idx0 != -1 and idx1 != -1:\n            for j in range(idx0, idx1 + 1):\n                if tweet[j] != \" \":\n                    char_targets[j] = 1\n        \n        tok_tweet = self.tokenizer.encode(sequence=self.sentiment[item], pair=tweet)\n        tok_tweet_tokens = tok_tweet.tokens\n        tok_tweet_ids = tok_tweet.ids\n        tok_tweet_offsets = tok_tweet.offsets[3:-1]\n        # print(tok_tweet_tokens)\n        # print(tok_tweet.offsets)\n        # ['[CLS]', 'spent', 'the', 'entire', 'morning', 'in', 'a', 'meeting', 'w', '/', \n        # 'a', 'vendor', ',', 'and', 'my', 'boss', 'was', 'not', 'happy', 'w', '/', 'them', \n        # '.', 'lots', 'of', 'fun', '.', 'i', 'had', 'other', 'plans', 'for', 'my', 'morning', '[SEP]']\n        targets = [0] * (len(tok_tweet_tokens) - 4)\n        if self.sentiment[item] == \"positive\" or self.sentiment[item] == \"negative\":\n            sub_minus = 8\n        else:\n            sub_minus = 7\n\n        for j, (offset1, offset2) in enumerate(tok_tweet_offsets):\n            if sum(char_targets[offset1 - sub_minus:offset2 - sub_minus]) > 0:\n                targets[j] = 1\n        \n        targets = [0] + [0] + [0] + targets + [0]\n\n        #print(tweet)\n        #print(selected_text)\n        #print([x for i, x in enumerate(tok_tweet_tokens) if targets[i] == 1])\n        targets_start = [0] * len(targets)\n        targets_end = [0] * len(targets)\n\n        non_zero = np.nonzero(targets)[0]\n        if len(non_zero) > 0:\n            targets_start[non_zero[0]] = 1\n            targets_end[non_zero[-1]] = 1\n        \n        #print(targets_start)\n        #print(targets_end)\n\n        mask = [1] * len(tok_tweet_ids)\n        token_type_ids = [0] * 3 + [1] * (len(tok_tweet_ids) - 3)\n\n        padding_length = self.max_len - len(tok_tweet_ids)\n        ids = tok_tweet_ids + ([0] * padding_length)\n        mask = mask + ([0] * padding_length)\n        token_type_ids = token_type_ids + ([0] * padding_length)\n        targets = targets + ([0] * padding_length)\n        targets_start = targets_start + ([0] * padding_length)\n        targets_end = targets_end + ([0] * padding_length)\n\n        sentiment = [1, 0, 0]\n        if self.sentiment[item] == \"positive\":\n            sentiment = [0, 0, 1]\n        if self.sentiment[item] == \"negative\":\n            sentiment = [0, 1, 0]\n\n        return {\n            'ids': torch.tensor(ids, dtype=torch.long),\n            'mask': torch.tensor(mask, dtype=torch.long),\n            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n            'tweet_tokens': \" \".join(tok_tweet_tokens),\n            'targets': torch.tensor(targets, dtype=torch.long),\n            'targets_start': torch.tensor(targets_start, dtype=torch.long),\n            'targets_end': torch.tensor(targets_end, dtype=torch.long),\n            'padding_len': torch.tensor(padding_length, dtype=torch.long),\n            'orig_tweet': self.tweet[item],\n            'orig_selected': self.selected_text[item],\n            'sentiment': torch.tensor(sentiment, dtype=torch.float),\n            'orig_sentiment': self.sentiment[item]\n        }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = pd.read_csv(\"../input/tweet-sentiment-extraction/test.csv\")\ndf_test.loc[:, \"selected_text\"] = df_test.text.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dataset = TweetDataset(\n        tweet=df_test.text.values,\n        sentiment=df_test.sentiment.values,\n        selected_text=df_test.selected_text.values\n    )\n\ndata_loader = torch.utils.data.DataLoader(\n    test_dataset,\n    shuffle=False,\n    batch_size=VALID_BATCH_SIZE,\n    num_workers=1\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_outputs = []\nfin_outputs_start = []\nfin_outputs_end = []\nfin_outputs_start2 = []\nfin_outputs_end2 = []\nfin_tweet_tokens = []\nfin_padding_lens = []\nfin_orig_selected = []\nfin_orig_sentiment = []\nfin_orig_tweet = []\nfin_tweet_token_ids = []\n\nwith torch.no_grad():\n    for bi, d in tqdm(enumerate(data_loader), total=len(data_loader)):\n        ids = d[\"ids\"]\n        token_type_ids = d[\"token_type_ids\"]\n        mask = d[\"mask\"]\n        tweet_tokens = d[\"tweet_tokens\"]\n        padding_len = d[\"padding_len\"]\n        sentiment = d[\"sentiment\"]\n        orig_selected = d[\"orig_selected\"]\n        orig_sentiment = d[\"orig_sentiment\"]\n        orig_tweet = d[\"orig_tweet\"]\n\n        ids = ids.to(device, dtype=torch.long)\n        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n        mask = mask.to(device, dtype=torch.long)\n        sentiment = sentiment.to(device, dtype=torch.float)\n\n        outputs_start, outputs_end = model(\n            ids=ids,\n            mask=mask,\n            token_type_ids=token_type_ids\n        )\n        \n        outputs_start2, outputs_end2 = model2(\n            ids=ids,\n            mask=mask,\n            token_type_ids=token_type_ids\n        )\n\n        fin_outputs_start.append(torch.sigmoid(outputs_start).cpu().detach().numpy())\n        fin_outputs_end.append(torch.sigmoid(outputs_end).cpu().detach().numpy())\n        fin_outputs_start2.append(torch.sigmoid(outputs_start2).cpu().detach().numpy())\n        fin_outputs_end2.append(torch.sigmoid(outputs_end2).cpu().detach().numpy())\n        \n        fin_padding_lens.extend(padding_len.cpu().detach().numpy().tolist())\n        fin_tweet_token_ids.append(ids.cpu().detach().numpy().tolist())\n\n        fin_tweet_tokens.extend(tweet_tokens)\n        fin_orig_sentiment.extend(orig_sentiment)\n        fin_orig_selected.extend(orig_selected)\n        fin_orig_tweet.extend(orig_tweet)\n\nfin_outputs_start = np.vstack(fin_outputs_start)\nfin_outputs_end = np.vstack(fin_outputs_end)\nfin_outputs_start2 = np.vstack(fin_outputs_start2)\nfin_outputs_end2 = np.vstack(fin_outputs_end2)\n\nfin_outputs_start = (fin_outputs_start + fin_outputs_start2) / 2\nfin_outputs_end = (fin_outputs_end + fin_outputs_end2) / 2\n\nfin_tweet_token_ids = np.vstack(fin_tweet_token_ids)\njaccards = []\nthreshold = 0.3\nfor j in range(len(fin_tweet_tokens)):\n    target_string = fin_orig_selected[j]\n    tweet_tokens = fin_tweet_tokens[j]\n    padding_len = fin_padding_lens[j]\n    original_tweet = fin_orig_tweet[j]\n    sentiment_val = fin_orig_sentiment[j]\n\n    if padding_len > 0:\n        mask_start = fin_outputs_start[j, 3:-1][:-padding_len] >= threshold\n        mask_end = fin_outputs_end[j, 3:-1][:-padding_len] >= threshold\n        tweet_token_ids = fin_tweet_token_ids[j, 3:-1][:-padding_len]\n    else:\n        mask_start = fin_outputs_start[j, 3:-1] >= threshold\n        mask_end = fin_outputs_end[j, 3:-1] >= threshold\n        tweet_token_ids = fin_tweet_token_ids[j, 3:-1]\n\n    mask = [0] * len(mask_start)\n    idx_start = np.nonzero(mask_start)[0]\n    idx_end = np.nonzero(mask_end)[0]\n    if len(idx_start) > 0:\n        idx_start = idx_start[0]\n        if len(idx_end) > 0:\n            idx_end = idx_end[0]\n        else:\n            idx_end = idx_start\n    else:\n        idx_start = 0\n        idx_end = 0\n\n    for mj in range(idx_start, idx_end + 1):\n        mask[mj] = 1\n\n    output_tokens = [x for p, x in enumerate(tweet_token_ids) if mask[p] == 1]\n\n    filtered_output = TOKENIZER.decode(output_tokens)\n    filtered_output = filtered_output.strip().lower()\n\n    if sentiment_val == \"neutral\":\n        filtered_output = original_tweet\n\n    all_outputs.append(filtered_output.strip())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample = pd.read_csv(\"../input/tweet-sentiment-extraction/sample_submission.csv\")\nsample.loc[:, 'selected_text'] = all_outputs\nsample.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}