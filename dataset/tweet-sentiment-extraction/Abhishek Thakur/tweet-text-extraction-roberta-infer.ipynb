{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport tokenizers\nimport string\nimport torch\nimport transformers\nimport torch.nn as nn\nfrom tqdm import tqdm\nimport re","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"MAX_LEN = 128\nTRAIN_BATCH_SIZE = 32\nVALID_BATCH_SIZE = 8\nEPOCHS = 10\nBERT_PATH = \"../input/roberta-base/\"\nMODEL_PATH = \"model.bin\"\nTRAINING_FILE = \"../input/train.csv\"\nTOKENIZER = tokenizers.ByteLevelBPETokenizer(\n    vocab_file=f\"{BERT_PATH}/vocab.json\", \n    merges_file=f\"{BERT_PATH}/merges.txt\", \n    lowercase=True,\n    add_prefix_space=True\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TweetModel(nn.Module):\n    def __init__(self):\n        super(TweetModel, self).__init__()\n        self.bert = transformers.RobertaModel.from_pretrained(BERT_PATH)\n        self.l0 = nn.Linear(768, 2)\n    \n    def forward(self, ids, mask, token_type_ids):\n        sequence_output, pooled_output = self.bert(\n            ids, \n            attention_mask=mask\n        )\n        logits = self.l0(sequence_output)\n        start_logits, end_logits = logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n\n        return start_logits, end_logits","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda\")\nmodel = TweetModel()\nmodel.to(device)\nmodel = nn.DataParallel(model)\nmodel.load_state_dict(torch.load(\"../input/roberta-tweet-model/model.bin\"))\nmodel.eval()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TweetDataset:\n    def __init__(self, tweet, sentiment, selected_text):\n        self.tweet = tweet\n        self.sentiment = sentiment\n        self.selected_text = selected_text\n        self.tokenizer = TOKENIZER\n        self.max_len = MAX_LEN\n    \n    def __len__(self):\n        return len(self.tweet)\n    \n    def __getitem__(self, item):\n        # For Roberta, CLS = <s> and SEP = </s>\n        # Multiple strings: '<s>hi, my name is abhishek!!!</s></s>whats ur name</s>'\n        # id for <s>: 0\n        # id for </s>: 2\n    \n        tweet = \" \" + \" \".join(str(self.tweet[item]).split())\n        selected_text = \" \" + \" \".join(str(self.selected_text[item]).split())\n    \n        len_st = len(selected_text)\n        idx0 = -1\n        idx1 = -1\n        for ind in (i for i, e in enumerate(tweet) if e == selected_text[0]):\n            if tweet[ind: ind+len_st] == selected_text:\n                idx0 = ind\n                idx1 = ind + len_st\n                break\n        #print(f\"idx0: {idx0}\")\n        #print(f\"idx1: {idx1}\")\n        #print(f\"len_st: {len_st}\")\n        #print(f\"idxed tweet: {tweet[idx0: idx1]}\")\n\n        char_targets = [0] * len(tweet)\n        if idx0 != -1 and idx1 != -1:\n            for ct in range(idx0, idx1):\n                # if tweet[ct] != \" \":\n                char_targets[ct] = 1\n\n        #print(f\"char_targets: {char_targets}\")\n\n        tok_tweet = self.tokenizer.encode(tweet)\n        tok_tweet_tokens = tok_tweet.tokens\n        tok_tweet_ids = tok_tweet.ids\n        tok_tweet_offsets = tok_tweet.offsets\n        #print(tweet)\n        #print(selected_text)\n        #print(tok_tweet_tokens)\n        #print(f\"tok_tweet.offsets= {tok_tweet.offsets}\")\n        \n        targets = [0] * len(tok_tweet_ids)\n        target_idx = []\n        for j, (offset1, offset2) in enumerate(tok_tweet_offsets):\n            #print(\"**************\")\n            #print(offset1, offset2)\n            #print(tweet[offset1: offset2])\n            #print(char_targets[offset1: offset2])\n            #print(\"\".join(tok_tweet_tokens)[offset1: offset2])\n            #print(\"**************\")\n            if sum(char_targets[offset1: offset2]) > 0:\n                targets[j] = 1\n                target_idx.append(j)\n\n        #print(f\"targets= {targets}\")\n        #print(f\"target_idx= {target_idx}\")\n\n        #print(tok_tweet_tokens[target_idx[0]])\n        #print(tok_tweet_tokens[target_idx[-1]])\n        \n        targets_start = [0] * len(targets)\n        targets_end = [0] * len(targets)\n\n        non_zero = np.nonzero(targets)[0]\n        if len(non_zero) > 0:\n            targets_start[non_zero[0]] = 1\n            targets_end[non_zero[-1]] = 1\n        \n        #print(targets_start)\n        #print(targets_end)\n        #print(tok_tweet_tokens)\n        #print([x for jj, x in enumerate(tok_tweet_tokens) if targets_start[jj] == 1])\n        #print([x for jj, x in enumerate(tok_tweet_tokens) if targets_end[jj] == 1])\n        \n\n        # check padding:\n        # <s> pos/neg/neu </s> </s> tweet </s>\n        if len(tok_tweet_tokens) > self.max_len - 5:\n            tok_tweet_tokens = tok_tweet_tokens[:self.max_len - 5]\n            tok_tweet_ids = tok_tweet_ids[:self.max_len - 5]\n            targets_start = targets_start[:self.max_len - 5]\n            targets_end = targets_end[:self.max_len - 5]\n        \n        # positive: 1313\n        # negative: 2430\n        # neutral: 7974\n\n        sentiment_id = {\n            'positive': 1313,\n            'negative': 2430,\n            'neutral': 7974\n        }\n\n        tok_tweet_ids = [0] + [sentiment_id[self.sentiment[item]]] + [2] + [2] + tok_tweet_ids + [2]\n        targets_start = [0] + [0] + [0] + [0] + targets_start + [0]\n        targets_end = [0] + [0] + [0] + [0] + targets_end + [0]\n        token_type_ids = [0, 0, 0, 0] + [0] * (len(tok_tweet_ids) - 5) + [0]\n        mask = [1] * len(token_type_ids)\n\n        #print(\"Before padding\")\n        #print(f\"len(tok_tweet_ids)= {len(tok_tweet_ids)}\")\n        #print(f\"len(targets_start)= {len(targets_start)}\")\n        #print(f\"len(targets_end)= {len(targets_end)}\")\n        #print(f\"len(token_type_ids)= {len(token_type_ids)}\")\n        #print(f\"len(mask)= {len(mask)}\")\n\n        padding_length = self.max_len - len(tok_tweet_ids)\n        \n        tok_tweet_ids = tok_tweet_ids + ([1] * padding_length)\n        mask = mask + ([0] * padding_length)\n        token_type_ids = token_type_ids + ([0] * padding_length)\n        targets_start = targets_start + ([0] * padding_length)\n        targets_end = targets_end + ([0] * padding_length)\n\n        #print(\"After padding\")\n        #print(f\"len(tok_tweet_ids)= {len(tok_tweet_ids)}\")\n        #print(f\"len(targets_start)= {len(targets_start)}\")\n        #print(f\"len(targets_end)= {len(targets_end)}\")\n        #print(f\"len(token_type_ids)= {len(token_type_ids)}\")\n        #print(f\"len(mask)= {len(mask)}\")\n\n        return {\n            'ids': torch.tensor(tok_tweet_ids, dtype=torch.long),\n            'mask': torch.tensor(mask, dtype=torch.long),\n            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n            'targets_start': torch.tensor(targets_start, dtype=torch.float),\n            'targets_end': torch.tensor(targets_end, dtype=torch.float),\n            'padding_len': torch.tensor(padding_length, dtype=torch.long),\n            'orig_tweet': self.tweet[item],\n            'orig_selected': self.selected_text[item],\n            'sentiment': self.sentiment[item]\n        }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = pd.read_csv(\"../input/tweet-sentiment-extraction/test.csv\")\ndf_test.loc[:, \"selected_text\"] = df_test.text.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dataset = TweetDataset(\n        tweet=df_test.text.values,\n        sentiment=df_test.sentiment.values,\n        selected_text=df_test.selected_text.values\n    )\n\ndata_loader = torch.utils.data.DataLoader(\n    test_dataset,\n    shuffle=False,\n    batch_size=VALID_BATCH_SIZE,\n    num_workers=1\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_outputs = []\nfin_outputs_start = []\nfin_outputs_end = []\nfin_padding_lens = []\nfin_orig_selected = []\nfin_orig_sentiment = []\nfin_orig_tweet = []\nfin_tweet_token_ids = []\n\nwith torch.no_grad():\n    tk0 = tqdm(data_loader, total=len(data_loader))\n    for bi, d in enumerate(tk0):\n        ids = d[\"ids\"]\n        token_type_ids = d[\"token_type_ids\"]\n        mask = d[\"mask\"]\n        padding_len = d[\"padding_len\"]\n        sentiment = d[\"sentiment\"]\n        orig_selected = d[\"orig_selected\"]\n        orig_tweet = d[\"orig_tweet\"]\n        targets_start = d[\"targets_start\"]\n        targets_end = d[\"targets_end\"]\n\n        ids = ids.to(device, dtype=torch.long)\n        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n        mask = mask.to(device, dtype=torch.long)\n        targets_start = targets_start.to(device, dtype=torch.float)\n        targets_end = targets_end.to(device, dtype=torch.float)\n\n        outputs_start, outputs_end = model(\n            ids=ids,\n            mask=mask,\n            token_type_ids=token_type_ids\n        )\n\n        fin_outputs_start.append(torch.sigmoid(outputs_start).cpu().detach().numpy())\n        fin_outputs_end.append(torch.sigmoid(outputs_end).cpu().detach().numpy())\n        fin_padding_lens.extend(padding_len.cpu().detach().numpy().tolist())\n        fin_tweet_token_ids.append(ids.cpu().detach().numpy().tolist())\n\n        fin_orig_sentiment.extend(sentiment)\n        fin_orig_selected.extend(orig_selected)\n        fin_orig_tweet.extend(orig_tweet)\n\nfin_outputs_start = np.vstack(fin_outputs_start)\nfin_outputs_end = np.vstack(fin_outputs_end)\nfin_tweet_token_ids = np.vstack(fin_tweet_token_ids)\njaccards = []\nthreshold = 0.2\nfor j in range(fin_outputs_start.shape[0]):\n    target_string = fin_orig_selected[j]\n    padding_len = fin_padding_lens[j]\n    sentiment_val = fin_orig_sentiment[j]\n    original_tweet = fin_orig_tweet[j]\n\n    if padding_len > 0:\n        mask_start = fin_outputs_start[j, 4:-1][:-padding_len] >= threshold\n        mask_end = fin_outputs_end[j, 4:-1][:-padding_len] >= threshold\n        tweet_token_ids = fin_tweet_token_ids[j, 4:-1][:-padding_len]\n    else:\n        mask_start = fin_outputs_start[j, 4:-1] >= threshold\n        mask_end = fin_outputs_end[j, 4:-1] >= threshold\n        tweet_token_ids = fin_tweet_token_ids[j, 4:-1][:-padding_len]\n\n    mask = [0] * len(mask_start)\n    idx_start = np.nonzero(mask_start)[0]\n    idx_end = np.nonzero(mask_end)[0]\n    if len(idx_start) > 0:\n        idx_start = idx_start[0]\n        if len(idx_end) > 0:\n            idx_end = idx_end[0]\n        else:\n            idx_end = idx_start\n    else:\n        idx_start = 0\n        idx_end = 0\n\n    for mj in range(idx_start, idx_end + 1):\n        mask[mj] = 1\n\n    output_tokens = [x for p, x in enumerate(tweet_token_ids) if mask[p] == 1]\n\n    filtered_output = TOKENIZER.decode(output_tokens)\n    filtered_output = filtered_output.strip().lower()\n\n    all_outputs.append(filtered_output.strip())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample = pd.read_csv(\"../input/tweet-sentiment-extraction/sample_submission.csv\")\nsample.loc[:, 'selected_text'] = all_outputs\nsample.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}