{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a class=\"anchor\" id=\"0\"></a>\n# [Tweet Sentiment Extraction](https://www.kaggle.com/c/tweet-sentiment-extraction)","metadata":{}},{"cell_type":"markdown","source":"## 1. Import libraries <a class=\"anchor\" id=\"1\"></a>","metadata":{}},{"cell_type":"code","source":"import re\nimport string\nimport numpy as np \nimport random\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom plotly import graph_objs as go\nimport plotly.express as px\nimport plotly.figure_factory as ff\nfrom collections import Counter\n\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\n\nimport nltk\nfrom nltk.corpus import stopwords\n\nfrom tqdm import tqdm\nimport os\nimport nltk\nimport spacy\nimport random\nfrom spacy.util import compounding\nfrom spacy.util import minibatch\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-07-10T12:54:46.443474Z","iopub.execute_input":"2021-07-10T12:54:46.444066Z","iopub.status.idle":"2021-07-10T12:54:51.144428Z","shell.execute_reply.started":"2021-07-10T12:54:46.444022Z","shell.execute_reply":"2021-07-10T12:54:51.143306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib\nimport matplotlib.patches as mpatches\n%matplotlib inline\nimport seaborn as sns; sns.set(style='white')\nfrom mpl_toolkits.mplot3d import Axes3D\n\nfrom sklearn.decomposition import PCA, TruncatedSVD\nimport math\nimport pickle\n\nfrom scipy.cluster import hierarchy\nfrom scipy.spatial.distance import pdist\nfrom sklearn.manifold import TSNE\nfrom sklearn.cluster import KMeans\n\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom transformers import *\nimport tokenizers\nfrom sklearn.model_selection import StratifiedKFold\n\npd.set_option('max_colwidth', 40)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T12:54:51.149574Z","iopub.execute_input":"2021-07-10T12:54:51.152004Z","iopub.status.idle":"2021-07-10T12:54:56.881415Z","shell.execute_reply.started":"2021-07-10T12:54:51.15196Z","shell.execute_reply":"2021-07-10T12:54:56.880353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Below is a helper Function which generates random colors which can be used to give different colors to your plots.Feel free to use it**","metadata":{}},{"cell_type":"code","source":"def random_colours(number_of_colors):\n    '''\n    Simple function for random colours generation.\n    Input:\n        number_of_colors - integer value indicating the number of colours which are going to be generated.\n    Output:\n        Color in the following format: ['#E86DA4'] .\n    '''\n    colors = []\n    for i in range(number_of_colors):\n        colors.append(\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)]))\n    return colors","metadata":{"execution":{"iopub.status.busy":"2021-07-10T12:54:56.883213Z","iopub.execute_input":"2021-07-10T12:54:56.883627Z","iopub.status.idle":"2021-07-10T12:54:56.891591Z","shell.execute_reply.started":"2021-07-10T12:54:56.883572Z","shell.execute_reply":"2021-07-10T12:54:56.890366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reading the Data","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv')\ntest = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv')\nss = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-07-10T12:54:56.893254Z","iopub.execute_input":"2021-07-10T12:54:56.893691Z","iopub.status.idle":"2021-07-10T12:54:57.04035Z","shell.execute_reply.started":"2021-07-10T12:54:56.893655Z","shell.execute_reply":"2021-07-10T12:54:57.039545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train.shape)\nprint(test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T12:54:57.046238Z","iopub.execute_input":"2021-07-10T12:54:57.046509Z","iopub.status.idle":"2021-07-10T12:54:57.05445Z","shell.execute_reply.started":"2021-07-10T12:54:57.046472Z","shell.execute_reply":"2021-07-10T12:54:57.053697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So We have 27486 tweets in the train set and 3535 tweets in the test set","metadata":{}},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2021-07-10T12:54:57.05893Z","iopub.execute_input":"2021-07-10T12:54:57.059177Z","iopub.status.idle":"2021-07-10T12:54:57.085571Z","shell.execute_reply.started":"2021-07-10T12:54:57.059153Z","shell.execute_reply":"2021-07-10T12:54:57.084743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have one null Value in the train , as the test field for value is NAN we will just remove it","metadata":{}},{"cell_type":"code","source":"train.dropna(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T12:54:57.088453Z","iopub.execute_input":"2021-07-10T12:54:57.088712Z","iopub.status.idle":"2021-07-10T12:54:57.108694Z","shell.execute_reply.started":"2021-07-10T12:54:57.088687Z","shell.execute_reply":"2021-07-10T12:54:57.107771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.info()","metadata":{"execution":{"iopub.status.busy":"2021-07-10T12:54:57.110063Z","iopub.execute_input":"2021-07-10T12:54:57.110673Z","iopub.status.idle":"2021-07-10T12:54:57.121279Z","shell.execute_reply.started":"2021-07-10T12:54:57.11063Z","shell.execute_reply":"2021-07-10T12:54:57.119989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are no null Values in the test set","metadata":{}},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-10T12:54:57.122808Z","iopub.execute_input":"2021-07-10T12:54:57.12325Z","iopub.status.idle":"2021-07-10T12:54:57.139443Z","shell.execute_reply.started":"2021-07-10T12:54:57.123213Z","shell.execute_reply":"2021-07-10T12:54:57.138584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Selected_text is a subset of text ","metadata":{}},{"cell_type":"code","source":"train.describe()","metadata":{"execution":{"iopub.status.busy":"2021-07-10T12:54:57.141444Z","iopub.execute_input":"2021-07-10T12:54:57.141826Z","iopub.status.idle":"2021-07-10T12:54:57.242343Z","shell.execute_reply.started":"2021-07-10T12:54:57.141789Z","shell.execute_reply":"2021-07-10T12:54:57.241469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets look at the distribution of tweets in the train set","metadata":{}},{"cell_type":"code","source":"temp = train.groupby('sentiment').count()['text'].reset_index().sort_values(by='text',ascending=False)\ntemp.style.background_gradient(cmap='Purples')","metadata":{"execution":{"iopub.status.busy":"2021-07-10T12:54:57.244107Z","iopub.execute_input":"2021-07-10T12:54:57.244486Z","iopub.status.idle":"2021-07-10T12:54:57.338195Z","shell.execute_reply.started":"2021-07-10T12:54:57.244447Z","shell.execute_reply":"2021-07-10T12:54:57.337454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,6))\nsns.countplot(x='sentiment',data=train)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T12:54:57.339814Z","iopub.execute_input":"2021-07-10T12:54:57.34024Z","iopub.status.idle":"2021-07-10T12:54:57.513676Z","shell.execute_reply.started":"2021-07-10T12:54:57.340194Z","shell.execute_reply":"2021-07-10T12:54:57.512911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## What do we currently Know About our Data:\n\nBefore starting let's look at some things that we already know about the data and will help us in gaining more new insights:\n* We Know that selected_text is a subset of text\n* We know that selected_text contains only one segment of text,i.e,It does not jump between two sentences.For Eg:- If text is 'Spent the entire morning in a meeting w/ a vendor, and my boss was not happy w/ them. Lots of fun.  I had other plans for my morning' The selected text can be 'my boss was not happy w/ them. Lots of fun' or 'Lots of fun' but cannot be 'Morning,vendor and my boss","metadata":{}},{"cell_type":"markdown","source":"## Generating Meta-Features","metadata":{}},{"cell_type":"markdown","source":"**In the previous versions of this notebook,I used Number of words in selected text and main text ,Length of words in text and selected as main meta features,but in the context of this competition where we have to predict selected_text which is a subset of text, more useful features to generate would be** :-\n* Difference In Number Of words of Selected_text and Text\n* Jaccard Similarity Scores between text and Selected_text\n\nThus it will not be useful for us to generate features we used before as they are of no importance here","metadata":{}},{"cell_type":"code","source":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))","metadata":{"execution":{"iopub.status.busy":"2021-07-10T12:54:57.514834Z","iopub.execute_input":"2021-07-10T12:54:57.515081Z","iopub.status.idle":"2021-07-10T12:54:57.521202Z","shell.execute_reply.started":"2021-07-10T12:54:57.515057Z","shell.execute_reply":"2021-07-10T12:54:57.520201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results_jaccard=[]\n\nfor ind,row in train.iterrows():\n    sentence1 = row.text\n    sentence2 = row.selected_text\n\n    jaccard_score = jaccard(sentence1,sentence2)\n    results_jaccard.append([sentence1,sentence2,jaccard_score])","metadata":{"execution":{"iopub.status.busy":"2021-07-10T12:54:57.522719Z","iopub.execute_input":"2021-07-10T12:54:57.523323Z","iopub.status.idle":"2021-07-10T12:55:00.792848Z","shell.execute_reply.started":"2021-07-10T12:54:57.523283Z","shell.execute_reply":"2021-07-10T12:55:00.791949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"jaccard = pd.DataFrame(results_jaccard,columns=[\"text\",\"selected_text\",\"jaccard_score\"])\ntrain = train.merge(jaccard,how='outer')","metadata":{"execution":{"iopub.status.busy":"2021-07-10T12:55:00.796521Z","iopub.execute_input":"2021-07-10T12:55:00.796777Z","iopub.status.idle":"2021-07-10T12:55:00.850807Z","shell.execute_reply.started":"2021-07-10T12:55:00.796752Z","shell.execute_reply":"2021-07-10T12:55:00.849686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['Num_words_ST'] = train['selected_text'].apply(lambda x:len(str(x).split())) #Number Of words in Selected Text\ntrain['Num_word_text'] = train['text'].apply(lambda x:len(str(x).split())) #Number Of words in main text\ntrain['difference_in_words'] = train['Num_word_text'] - train['Num_words_ST'] #Difference in Number of words text and Selected Text","metadata":{"execution":{"iopub.status.busy":"2021-07-10T12:55:00.85306Z","iopub.execute_input":"2021-07-10T12:55:00.853382Z","iopub.status.idle":"2021-07-10T12:55:00.969277Z","shell.execute_reply.started":"2021-07-10T12:55:00.853351Z","shell.execute_reply":"2021-07-10T12:55:00.96858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-10T12:55:00.970551Z","iopub.execute_input":"2021-07-10T12:55:00.970934Z","iopub.status.idle":"2021-07-10T12:55:00.990258Z","shell.execute_reply.started":"2021-07-10T12:55:00.970893Z","shell.execute_reply":"2021-07-10T12:55:00.989001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's look at the distribution of Meta-Features\n","metadata":{}},{"cell_type":"code","source":"hist_data = [train['Num_words_ST'],train['Num_word_text']]\n\ngroup_labels = ['Selected_Text', 'Text']\n\n# Create distplot with custom bin_size\nfig = ff.create_distplot(hist_data, group_labels,show_curve=False)\nfig.update_layout(title_text='Distribution of Number Of words')\nfig.update_layout(\n    autosize=False,\n    width=900,\n    height=700,\n    paper_bgcolor=\"LightSteelBlue\",\n)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-10T12:55:00.992145Z","iopub.execute_input":"2021-07-10T12:55:00.99266Z","iopub.status.idle":"2021-07-10T12:55:02.508814Z","shell.execute_reply.started":"2021-07-10T12:55:00.992619Z","shell.execute_reply":"2021-07-10T12:55:02.506768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* The number of words plot is really interesting ,the tweets having number of words greater than 25 are very less and thus the number of words distribution plot is right skewed","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12,6))\np1=sns.kdeplot(train['Num_words_ST'], shade=True, color=\"r\").set_title('Kernel Distribution of Number Of words')\np1=sns.kdeplot(train['Num_word_text'], shade=True, color=\"b\")","metadata":{"execution":{"iopub.status.busy":"2021-07-10T12:55:02.509974Z","iopub.execute_input":"2021-07-10T12:55:02.510327Z","iopub.status.idle":"2021-07-10T12:55:02.832248Z","shell.execute_reply.started":"2021-07-10T12:55:02.510292Z","shell.execute_reply":"2021-07-10T12:55:02.831293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Now It will be more interesting to see the differnce in number of words and jaccard_scores across different Sentiments**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12,6))\np1=sns.kdeplot(train[train['sentiment']=='positive']['difference_in_words'], shade=True, color=\"b\").set_title('Kernel Distribution of Difference in Number Of words')\np2=sns.kdeplot(train[train['sentiment']=='negative']['difference_in_words'], shade=True, color=\"r\")","metadata":{"execution":{"iopub.status.busy":"2021-07-10T12:55:02.833619Z","iopub.execute_input":"2021-07-10T12:55:02.833986Z","iopub.status.idle":"2021-07-10T12:55:03.141904Z","shell.execute_reply.started":"2021-07-10T12:55:02.83395Z","shell.execute_reply":"2021-07-10T12:55:03.140844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,6))\nsns.distplot(train[train['sentiment']=='neutral']['difference_in_words'],kde=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T12:55:03.143889Z","iopub.execute_input":"2021-07-10T12:55:03.144409Z","iopub.status.idle":"2021-07-10T12:55:03.479005Z","shell.execute_reply.started":"2021-07-10T12:55:03.144368Z","shell.execute_reply":"2021-07-10T12:55:03.478052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I was not able to plot kde plot for neutral tweets because most of the values for difference in number of words were zero. We can see it clearly now ,if we had used the feature in the starting we would have known that text and selected text are mostly the same for neutral tweets,thus its always important to keep the end goal in mind while performing EDA","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12,6))\np1=sns.kdeplot(train[train['sentiment']=='positive']['jaccard_score'], shade=True, color=\"b\").set_title('KDE of Jaccard Scores across different Sentiments')\np2=sns.kdeplot(train[train['sentiment']=='negative']['jaccard_score'], shade=True, color=\"r\")\nplt.legend(labels=['positive','negative'])","metadata":{"execution":{"iopub.status.busy":"2021-07-10T12:55:03.480805Z","iopub.execute_input":"2021-07-10T12:55:03.481249Z","iopub.status.idle":"2021-07-10T12:55:03.809766Z","shell.execute_reply.started":"2021-07-10T12:55:03.481202Z","shell.execute_reply":"2021-07-10T12:55:03.808901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I was not able to plot kde of jaccard_scores of neutral tweets for the same reason,thus I will plot a distribution plot","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12,6))\nsns.distplot(train[train['sentiment']=='neutral']['jaccard_score'],kde=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T12:55:03.811316Z","iopub.execute_input":"2021-07-10T12:55:03.811945Z","iopub.status.idle":"2021-07-10T12:55:04.142877Z","shell.execute_reply.started":"2021-07-10T12:55:03.811819Z","shell.execute_reply":"2021-07-10T12:55:04.142012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see some interesting trends here:\n* Positive and negative tweets have high kurtosis and thus values are concentrated in two regions narrow and high density \n* Neutral tweets have a low kurtosis value and their is bump in density near values of 1\n\nFor those who don't know :\n* Kurtosis is the measure of how peaked a distribution is and how much spread it is around that peak\n* Skewness measures how much a curve deviates from a normal distribution","metadata":{}},{"cell_type":"markdown","source":"## Conclusion Of EDA\n\n* We can see from the jaccard score plot that there is peak for negative and positive plot around score of 1 .That means there is a cluster of tweets where there is a high similarity between text and selected texts ,if we can find those clusters then we can predict text for selected texts for those tweets irrespective of segment\n\nLet's see if we can find those clusters,one interesting idea would be to check tweets which have number of words lesss than 3 in text, because there the text might be completely used as text","metadata":{}},{"cell_type":"code","source":"k = train[train['Num_word_text']<=2]","metadata":{"execution":{"iopub.status.busy":"2021-07-10T12:55:04.144851Z","iopub.execute_input":"2021-07-10T12:55:04.145212Z","iopub.status.idle":"2021-07-10T12:55:04.152357Z","shell.execute_reply.started":"2021-07-10T12:55:04.145174Z","shell.execute_reply":"2021-07-10T12:55:04.151464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"k.groupby('sentiment').mean()['jaccard_score']","metadata":{"execution":{"iopub.status.busy":"2021-07-10T12:55:04.153728Z","iopub.execute_input":"2021-07-10T12:55:04.154109Z","iopub.status.idle":"2021-07-10T12:55:04.168835Z","shell.execute_reply.started":"2021-07-10T12:55:04.154068Z","shell.execute_reply":"2021-07-10T12:55:04.167901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that there is similarity between text and selected text .Let's have closer look","metadata":{}},{"cell_type":"code","source":"k[k['sentiment']=='positive']","metadata":{"execution":{"iopub.status.busy":"2021-07-10T12:55:04.1701Z","iopub.execute_input":"2021-07-10T12:55:04.170576Z","iopub.status.idle":"2021-07-10T12:55:04.193063Z","shell.execute_reply.started":"2021-07-10T12:55:04.170529Z","shell.execute_reply":"2021-07-10T12:55:04.19223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Thus its clear that most of the times , text is used as selected text.We can improve this by preprocessing the text which have word length less than 3.We will remember this information and use it in model building","metadata":{}},{"cell_type":"markdown","source":"### Cleaning the Corpus\nNow Before We Dive into extracting information out of words in text and selected text,let's first clean the data","metadata":{}},{"cell_type":"code","source":"def clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = str(text).lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text","metadata":{"execution":{"iopub.status.busy":"2021-07-10T12:55:04.194342Z","iopub.execute_input":"2021-07-10T12:55:04.194741Z","iopub.status.idle":"2021-07-10T12:55:04.201914Z","shell.execute_reply.started":"2021-07-10T12:55:04.194703Z","shell.execute_reply":"2021-07-10T12:55:04.200913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['text'] = train['text'].apply(lambda x:clean_text(x))\ntrain['selected_text'] = train['selected_text'].apply(lambda x:clean_text(x))","metadata":{"execution":{"iopub.status.busy":"2021-07-10T12:55:04.203291Z","iopub.execute_input":"2021-07-10T12:55:04.203876Z","iopub.status.idle":"2021-07-10T12:55:05.665379Z","shell.execute_reply.started":"2021-07-10T12:55:04.203834Z","shell.execute_reply":"2021-07-10T12:55:05.664448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-10T12:55:05.666889Z","iopub.execute_input":"2021-07-10T12:55:05.667291Z","iopub.status.idle":"2021-07-10T12:55:05.682701Z","shell.execute_reply.started":"2021-07-10T12:55:05.667251Z","shell.execute_reply":"2021-07-10T12:55:05.681699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Most Common words in our Target-Selected Text","metadata":{}},{"cell_type":"code","source":"train['temp_list'] = train['selected_text'].apply(lambda x:str(x).split())\ntop = Counter([item for sublist in train['temp_list'] for item in sublist])\ntemp = pd.DataFrame(top.most_common(20))\ntemp.columns = ['Common_words','count']\ntemp.style.background_gradient(cmap='Blues')","metadata":{"execution":{"iopub.status.busy":"2021-07-10T12:55:05.685003Z","iopub.execute_input":"2021-07-10T12:55:05.685478Z","iopub.status.idle":"2021-07-10T12:55:05.784127Z","shell.execute_reply.started":"2021-07-10T12:55:05.685375Z","shell.execute_reply":"2021-07-10T12:55:05.783185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.bar(temp, x=\"count\", y=\"Common_words\", title='Commmon Words in Selected Text', orientation='h', \n             width=700, height=700,color='Common_words')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-10T12:55:05.785953Z","iopub.execute_input":"2021-07-10T12:55:05.786321Z","iopub.status.idle":"2021-07-10T12:55:06.446143Z","shell.execute_reply.started":"2021-07-10T12:55:05.786284Z","shell.execute_reply":"2021-07-10T12:55:06.445254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"While we cleaned our dataset we didnt remove the stop words and hence we can see the most coomon word is 'to' . Let's try again after removing the stopwords","metadata":{}},{"cell_type":"code","source":"def remove_stopword(x):\n    return [y for y in x if y not in stopwords.words('english')]\ntrain['temp_list'] = train['temp_list'].apply(lambda x:remove_stopword(x))","metadata":{"execution":{"iopub.status.busy":"2021-07-10T12:55:06.456938Z","iopub.execute_input":"2021-07-10T12:55:06.457194Z","iopub.status.idle":"2021-07-10T12:55:28.776488Z","shell.execute_reply.started":"2021-07-10T12:55:06.457168Z","shell.execute_reply":"2021-07-10T12:55:28.77571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top = Counter([item for sublist in train['temp_list'] for item in sublist])\ntemp = pd.DataFrame(top.most_common(20))\ntemp = temp.iloc[1:,:]\ntemp.columns = ['Common_words','count']\ntemp.style.background_gradient(cmap='Purples')","metadata":{"execution":{"iopub.status.busy":"2021-07-10T12:55:28.779474Z","iopub.execute_input":"2021-07-10T12:55:28.779904Z","iopub.status.idle":"2021-07-10T12:55:28.827851Z","shell.execute_reply.started":"2021-07-10T12:55:28.779865Z","shell.execute_reply":"2021-07-10T12:55:28.82676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.treemap(temp, path=['Common_words'], values='count',title='Tree of Most Common Words')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-10T12:55:28.829317Z","iopub.execute_input":"2021-07-10T12:55:28.829684Z","iopub.status.idle":"2021-07-10T12:55:28.911866Z","shell.execute_reply.started":"2021-07-10T12:55:28.829647Z","shell.execute_reply":"2021-07-10T12:55:28.910965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Most Common words in Text\n\nLet's also look at the most common words in Text","metadata":{}},{"cell_type":"code","source":"train['temp_list1'] = train['text'].apply(lambda x:str(x).split()) #List of words in every row for text\ntrain['temp_list1'] = train['temp_list1'].apply(lambda x:remove_stopword(x)) #Removing Stopwords","metadata":{"execution":{"iopub.status.busy":"2021-07-10T12:55:28.913907Z","iopub.execute_input":"2021-07-10T12:55:28.914293Z","iopub.status.idle":"2021-07-10T12:55:57.159243Z","shell.execute_reply.started":"2021-07-10T12:55:28.914253Z","shell.execute_reply":"2021-07-10T12:55:57.156687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top = Counter([item for sublist in train['temp_list1'] for item in sublist])\ntemp = pd.DataFrame(top.most_common(25))\ntemp = temp.iloc[1:,:]\ntemp.columns = ['Common_words','count']\ntemp.style.background_gradient(cmap='Blues')","metadata":{"execution":{"iopub.status.busy":"2021-07-10T12:55:57.160483Z","iopub.status.idle":"2021-07-10T12:55:57.161326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So the first two common word was I'm so I removed it and took data from second row","metadata":{}},{"cell_type":"code","source":"fig = px.bar(temp, x=\"count\", y=\"Common_words\", title='Commmon Words in Text', orientation='h', \n             width=700, height=700,color='Common_words')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-10T12:55:57.162635Z","iopub.status.idle":"2021-07-10T12:55:57.163429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"SO we can see the Most common words in Selected text and Text are almost the same,which was obvious","metadata":{}},{"cell_type":"markdown","source":"# Most common words Sentiments Wise\n\nLet's look at the most common words in different sentiments","metadata":{}},{"cell_type":"code","source":"Positive_sent = train[train['sentiment']=='positive']\nNegative_sent = train[train['sentiment']=='negative']\nNeutral_sent = train[train['sentiment']=='neutral']","metadata":{"execution":{"iopub.status.busy":"2021-07-10T12:55:57.16485Z","iopub.status.idle":"2021-07-10T12:55:57.165683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#MosT common positive words\ntop = Counter([item for sublist in Positive_sent['temp_list'] for item in sublist])\ntemp_positive = pd.DataFrame(top.most_common(20))\ntemp_positive.columns = ['Common_words','count']\ntemp_positive.style.background_gradient(cmap='Greens')","metadata":{"execution":{"iopub.status.busy":"2021-07-10T12:55:57.166864Z","iopub.status.idle":"2021-07-10T12:55:57.16782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.bar(temp_positive, x=\"count\", y=\"Common_words\", title='Most Commmon Positive Words', orientation='h', \n             width=700, height=700,color='Common_words')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-10T12:55:57.169133Z","iopub.status.idle":"2021-07-10T12:55:57.169948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#MosT common negative words\ntop = Counter([item for sublist in Negative_sent['temp_list'] for item in sublist])\ntemp_negative = pd.DataFrame(top.most_common(20))\ntemp_negative = temp_negative.iloc[1:,:]\ntemp_negative.columns = ['Common_words','count']\ntemp_negative.style.background_gradient(cmap='Reds')","metadata":{"execution":{"iopub.status.busy":"2021-07-10T12:55:57.171239Z","iopub.status.idle":"2021-07-10T12:55:57.172207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.treemap(temp_negative, path=['Common_words'], values='count',title='Tree Of Most Common Negative Words')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-10T12:55:57.173463Z","iopub.status.idle":"2021-07-10T12:55:57.174421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#MosT common Neutral words\ntop = Counter([item for sublist in Neutral_sent['temp_list'] for item in sublist])\ntemp_neutral = pd.DataFrame(top.most_common(20))\ntemp_neutral = temp_neutral.loc[1:,:]\ntemp_neutral.columns = ['Common_words','count']\ntemp_neutral.style.background_gradient(cmap='Reds')","metadata":{"execution":{"iopub.status.busy":"2021-07-10T12:55:57.1758Z","iopub.status.idle":"2021-07-10T12:55:57.176679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.bar(temp_neutral, x=\"count\", y=\"Common_words\", title='Most Commmon Neutral Words', orientation='h', \n             width=700, height=700,color='Common_words')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-10T12:55:57.177916Z","iopub.status.idle":"2021-07-10T12:55:57.178719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.treemap(temp_neutral, path=['Common_words'], values='count',title='Tree Of Most Common Neutral Words')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-10T12:55:57.179953Z","iopub.status.idle":"2021-07-10T12:55:57.180785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* We can see words like get,go,dont,got,u,cant,lol,like are common in all three segments . That's interesting because words like dont and cant are more of negative nature and words like lol are more of positive nature.Does this mean our data is incorrectly labelled , we will have more insights on this after N-gram analysis\n* It will be interesting to see the word unique to different sentiments","metadata":{}},{"cell_type":"markdown","source":"## Let's Look at Unique Words in each Segment\n\nWe will look at unique words in each segment in the Following Order:\n* Positive\n* Negative\n* Neutral","metadata":{}},{"cell_type":"code","source":"raw_text = [word for word_list in train['temp_list1'] for word in word_list]","metadata":{"execution":{"iopub.status.busy":"2021-07-10T12:55:57.181958Z","iopub.status.idle":"2021-07-10T12:55:57.183128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def words_unique(sentiment,numwords,raw_words):\n    '''\n    Input:\n        segment - Segment category (ex. 'Neutral');\n        numwords - how many specific words do you want to see in the final result; \n        raw_words - list  for item in train_data[train_data.segments == segments]['temp_list1']:\n    Output: \n        dataframe giving information about the name of the specific ingredient and how many times it occurs in the chosen cuisine (in descending order based on their counts)..\n\n    '''\n    allother = []\n    for item in train[train.sentiment != sentiment]['temp_list1']:\n        for word in item:\n            allother .append(word)\n    allother  = list(set(allother ))\n    \n    specificnonly = [x for x in raw_text if x not in allother]\n    \n    mycounter = Counter()\n    \n    for item in train[train.sentiment == sentiment]['temp_list1']:\n        for word in item:\n            mycounter[word] += 1\n    keep = list(specificnonly)\n    \n    for word in list(mycounter):\n        if word not in keep:\n            del mycounter[word]\n    \n    Unique_words = pd.DataFrame(mycounter.most_common(numwords), columns = ['words','count'])\n    \n    return Unique_words","metadata":{"execution":{"iopub.status.busy":"2021-07-10T12:55:57.185248Z","iopub.status.idle":"2021-07-10T12:55:57.186414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Positive Tweets","metadata":{}},{"cell_type":"code","source":"Unique_Positive= words_unique('positive', 20, raw_text)\nprint(\"The top 20 unique words in Positive Tweets are:\")\nUnique_Positive.style.background_gradient(cmap='Greens')","metadata":{"execution":{"iopub.status.busy":"2021-07-10T12:55:57.188121Z","iopub.status.idle":"2021-07-10T12:55:57.189034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.treemap(Unique_Positive, path=['words'], values='count',title='Tree Of Unique Positive Words')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-10T12:55:57.190337Z","iopub.status.idle":"2021-07-10T12:55:57.191291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from palettable.colorbrewer.qualitative import Pastel1_7\nplt.figure(figsize=(16,10))\nmy_circle=plt.Circle((0,0), 0.7, color='white')\nplt.pie(Unique_Positive['count'], labels=Unique_Positive.words, colors=Pastel1_7.hex_colors)\np=plt.gcf()\np.gca().add_artist(my_circle)\nplt.title('DoNut Plot Of Unique Positive Words')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-10T12:55:57.192475Z","iopub.status.idle":"2021-07-10T12:55:57.193427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Unique_Negative= words_unique('negative', 10, raw_text)\nprint(\"The top 10 unique words in Negative Tweets are:\")\nUnique_Negative.style.background_gradient(cmap='Reds')","metadata":{"execution":{"iopub.status.busy":"2021-07-10T12:55:57.194895Z","iopub.status.idle":"2021-07-10T12:55:57.195767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from palettable.colorbrewer.qualitative import Pastel1_7\nplt.figure(figsize=(16,10))\nmy_circle=plt.Circle((0,0), 0.7, color='white')\nplt.rcParams['text.color'] = 'black'\nplt.pie(Unique_Negative['count'], labels=Unique_Negative.words, colors=Pastel1_7.hex_colors)\np=plt.gcf()\np.gca().add_artist(my_circle)\nplt.title('DoNut Plot Of Unique Negative Words')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-10T12:55:57.197001Z","iopub.status.idle":"2021-07-10T12:55:57.1978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Unique_Neutral= words_unique('neutral', 10, raw_text)\nprint(\"The top 10 unique words in Neutral Tweets are:\")\nUnique_Neutral.style.background_gradient(cmap='Oranges')","metadata":{"execution":{"iopub.status.busy":"2021-07-10T12:55:57.198999Z","iopub.status.idle":"2021-07-10T12:55:57.199817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from palettable.colorbrewer.qualitative import Pastel1_7\nplt.figure(figsize=(16,10))\nmy_circle=plt.Circle((0,0), 0.7, color='white')\nplt.pie(Unique_Neutral['count'], labels=Unique_Neutral.words, colors=Pastel1_7.hex_colors)\np=plt.gcf()\np.gca().add_artist(my_circle)\nplt.title('DoNut Plot Of Unique Neutral Words')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-10T12:55:57.201023Z","iopub.status.idle":"2021-07-10T12:55:57.201901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Download data & FE <a class=\"anchor\" id=\"2\"></a>","metadata":{}},{"cell_type":"code","source":"MAX_LEN = 96\nPATH = '../input/tf-roberta/'\n# 分词器  字节级别的 BPE (Byte Pair Encoding) 训练和 tokenize，facebook 那几个预训练模型比如 Roberta 就用的这个，应对多语言比较方便\ntokenizer = tokenizers.ByteLevelBPETokenizer(\n    vocab_file=PATH+'vocab-roberta-base.json', \n    merges_file=PATH+'merges-roberta-base.txt', \n    lowercase=True,\n    add_prefix_space=True\n)\nEPOCHS = 3 # originally 3\nBATCH_SIZE = 32 # originally 32\nPAD_ID = 1\nSEED = 88888\nLABEL_SMOOTHING = 0.1\ntf.random.set_seed(SEED)\nnp.random.seed(SEED)\nsentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}\ntrain = pd.read_csv('../input/tweet-sentiment-extraction/train.csv').fillna('')\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-10T12:55:57.203209Z","iopub.status.idle":"2021-07-10T12:55:57.204242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv('../input/tweet-sentiment-extraction/test.csv').fillna('')\n\nct = test.shape[0]\ninput_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\nattention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\ntoken_type_ids_t = np.zeros((ct,MAX_LEN),dtype='int32')\n\nfor k in range(test.shape[0]):\n        \n    # INPUT_IDS\n    # 每句话开头加个空格\n    text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n    enc = tokenizer.encode(text1)                \n    s_tok = sentiment_id[test.loc[k,'sentiment']]\n    input_ids_t[k,:len(enc.ids)+3] = [0, s_tok] + enc.ids + [2]\n    attention_mask_t[k,:len(enc.ids)+3] = 1","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-10T12:55:57.20568Z","iopub.status.idle":"2021-07-10T12:55:57.206471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Model tuning <a class=\"anchor\" id=\"3\"></a>","metadata":{}},{"cell_type":"markdown","source":"## 3.1. My upgrade of parameters <a class=\"anchor\" id=\"3.1\"></a>","metadata":{}},{"cell_type":"code","source":"Dropout_new = 0.15     # originally 0.1\nn_split = 5            # originally 5\nlr = 3e-5              # originally 3e-5","metadata":{"execution":{"iopub.status.busy":"2021-07-10T12:55:57.207871Z","iopub.status.idle":"2021-07-10T12:55:57.208681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Previous successful commits","metadata":{}},{"cell_type":"markdown","source":"### Commit 3 (with original parameters)\n\n* Dropout_new = 0.1\n* n_split = 5\n* lr = 3e-5\n\nLB = 0.711","metadata":{}},{"cell_type":"markdown","source":"### Commit 5\n\n* Dropout_new = 0.15\n* n_split = 5\n* lr = 3e-5\n\nLB = 0.713","metadata":{}},{"cell_type":"markdown","source":"### Commit 6\n\n* Dropout_new = 0.15\n* n_split = 7\n* lr = 3e-5\n\nLB = 0.709","metadata":{}},{"cell_type":"markdown","source":"### Commit 7\n\n* Dropout_new = 0.15\n* n_split = 5\n* lr = 4e-5\n\nLB = 0.709","metadata":{}},{"cell_type":"markdown","source":"### Commit 8\n\n* Dropout_new = 0.15\n* n_split = 5\n* lr = 2e-5\n\nLB = 0.712","metadata":{}},{"cell_type":"markdown","source":"### Commit 9\n\n* Dropout_new = 0.15\n* n_split = 5\n* lr = 3e-5\n* LeakyReLU_alpha=0.05\n\nLB = 0.711","metadata":{}},{"cell_type":"markdown","source":"### Commit 10\n\n* Dropout_new = 0.15\n* n_split = 5\n* lr = 3e-5\n* LeakyReLU_alpha=0.3\n\nLB = 0.711","metadata":{}},{"cell_type":"markdown","source":"### Commit 12\n\n* Dropout_new = 0.15\n* n_split = 5\n* lr = 3e-5\n* SEED = 42\n\nLB = 0.711","metadata":{}},{"cell_type":"markdown","source":"### Commit 13\n\n* Dropout_new = 0.16\n* n_split = 5\n* lr = 3e-5\n\nLB = 0.711","metadata":{}},{"cell_type":"markdown","source":"### Commit 14\n\n* Dropout_new = 0.15\n* n_split = 5\n* lr = 3e-5\n\n**LB = 0.715 (the best)**","metadata":{}},{"cell_type":"markdown","source":"### Commit 15\n\n* Dropout_new = 0.16\n* n_split = 5\n* lr = 3e-5\n* SEED = 777\n\nLB = 0.710","metadata":{}},{"cell_type":"markdown","source":"### Commit 17\n\n* Dropout_new = 0.15\n* n_split = 5\n* lr = 1e-5\n\nLB = 0.709","metadata":{}},{"cell_type":"markdown","source":"### Commit 18\n\n* Dropout_new = 0.15\n* n_split = 5\n* lr = 3e-5\n* BATCH_SIZE = 24      # originally 32\n\nLB = 0.704","metadata":{}},{"cell_type":"markdown","source":"### Commit 19\n\n* Dropout_new = 0.125\n* n_split = 5\n* lr = 3e-5\n\nLB = 0.711","metadata":{}},{"cell_type":"markdown","source":"### Commit 20\n\n* Dropout_new = 0.15\n* n_split = 5\n* lr = 1e-4\n\nLB = 0.709","metadata":{}},{"cell_type":"markdown","source":"### Commit 21\n\n* Dropout_new = 0.15\n* n_split = 5\n* lr = 1e-4\n* num_cnn2 = 96          # originally 64\n\nLB = 0.712","metadata":{}},{"cell_type":"markdown","source":"## 3.2. Model training <a class=\"anchor\" id=\"3.2\"></a>","metadata":{}},{"cell_type":"code","source":"import pickle\n\ndef save_weights(model, dst_fn):\n    weights = model.get_weights()\n    with open(dst_fn, 'wb') as f:\n        pickle.dump(weights, f)\n\n\ndef load_weights(model, weight_fn):\n    with open(weight_fn, 'rb') as f:\n        weights = pickle.load(f)\n    model.set_weights(weights)\n    return model\n\ndef loss_fn(y_true, y_pred):\n    # adjust the targets for sequence bucketing\n    ll = tf.shape(y_pred)[1]\n    y_true = y_true[:, :ll]\n    loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred,\n        from_logits=False, label_smoothing=LABEL_SMOOTHING)\n    loss = tf.reduce_mean(loss)\n    return loss\n\n\ndef build_model():\n    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    padding = tf.cast(tf.equal(ids, PAD_ID), tf.int32)\n\n    lens = MAX_LEN - tf.reduce_sum(padding, -1)\n    max_len = tf.reduce_max(lens)\n    ids_ = ids[:, :max_len]\n    att_ = att[:, :max_len]\n    tok_ = tok[:, :max_len]\n\n    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n    x = bert_model(ids_,attention_mask=att_,token_type_ids=tok_)\n    \n    x1 = tf.keras.layers.Dropout(Dropout_new)(x[0])\n    x1 = tf.keras.layers.Conv1D(768, 2,padding='same')(x1)\n    x1 = tf.keras.layers.LeakyReLU()(x1)\n    x1 = tf.keras.layers.Conv1D(64, 2,padding='same')(x1)\n    x1 = tf.keras.layers.Dense(1)(x1)\n    x1 = tf.keras.layers.Flatten()(x1)\n    x1 = tf.keras.layers.Activation('softmax')(x1)\n    \n    x2 = tf.keras.layers.Dropout(Dropout_new)(x[0]) \n    x2 = tf.keras.layers.Conv1D(768, 2,padding='same')(x2)\n    x2 = tf.keras.layers.LeakyReLU()(x2)\n    x2 = tf.keras.layers.Conv1D(64, 2, padding='same')(x2)\n    x2 = tf.keras.layers.Dense(1)(x2)\n    x2 = tf.keras.layers.Flatten()(x2)\n    x2 = tf.keras.layers.Activation('softmax')(x2)\n\n    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n    optimizer = tf.keras.optimizers.Adam(learning_rate=lr) \n    model.compile(loss=loss_fn, optimizer=optimizer)\n    \n    # this is required as `model.predict` needs a fixed size!\n    x1_padded = tf.pad(x1, [[0, 0], [0, MAX_LEN - max_len]], constant_values=0.)\n    x2_padded = tf.pad(x2, [[0, 0], [0, MAX_LEN - max_len]], constant_values=0.)\n    \n    padded_model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1_padded,x2_padded])\n    return model, padded_model","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-10T12:55:57.209972Z","iopub.status.idle":"2021-07-10T12:55:57.210813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    if (len(a)==0) & (len(b)==0): return 0.5\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-10T12:55:57.212023Z","iopub.status.idle":"2021-07-10T12:55:57.212833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ct = train.shape[0]\ninput_ids = np.ones((ct,MAX_LEN),dtype='int32')\nattention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\ntoken_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\nstart_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\nend_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n\nfor k in range(train.shape[0]):\n    \n    # FIND OVERLAP\n    text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n    text2 = \" \".join(train.loc[k,'selected_text'].split())\n    # 返回开始的索引值idx\n    idx = text1.find(text2)\n    chars = np.zeros((len(text1)))\n    chars[idx:idx+len(text2)]=1\n    if text1[idx-1]==' ': \n        chars[idx-1] = 1 \n    enc = tokenizer.encode(text1) \n        \n    # ID_OFFSETS\n    offsets = []; idx=0\n    for t in enc.ids:\n        w = tokenizer.decode([t])\n        offsets.append((idx,idx+len(w)))\n        idx += len(w)\n    \n    # START END TOKENS\n    toks = []\n    for i,(a,b) in enumerate(offsets):\n        sm = np.sum(chars[a:b])\n        if sm>0: \n            toks.append(i) \n        \n    s_tok = sentiment_id[train.loc[k,'sentiment']]\n    input_ids[k,:len(enc.ids)+3] = [0, s_tok] + enc.ids + [2]\n    attention_mask[k,:len(enc.ids)+3] = 1\n    if len(toks)>0:\n        start_tokens[k,toks[0]+2] = 1\n        end_tokens[k,toks[-1]+2] = 1","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-10T12:55:57.214163Z","iopub.status.idle":"2021-07-10T12:55:57.215118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\njac = []; VER='v0'; DISPLAY=1 # USE display=1 FOR INTERACTIVE\noof_start = np.zeros((input_ids.shape[0],MAX_LEN))\noof_end = np.zeros((input_ids.shape[0],MAX_LEN))\npreds_start_train = np.zeros((input_ids.shape[0],MAX_LEN))\npreds_end_train = np.zeros((input_ids.shape[0],MAX_LEN))\npreds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\npreds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\n\nskf = StratifiedKFold(n_splits=n_split,shuffle=True,random_state=SEED)\nfor fold,(idxT,idxV) in enumerate(skf.split(input_ids,train.sentiment.values)):\n\n    print('#'*25)\n    print('### FOLD %i'%(fold+1))\n    print('#'*25)\n    \n    K.clear_session()\n    model, padded_model = build_model()\n        \n    #sv = tf.keras.callbacks.ModelCheckpoint(\n    #    '%s-roberta-%i.h5'%(VER,fold), monitor='val_loss', verbose=1, save_best_only=True,\n    #    save_weights_only=True, mode='auto', save_freq='epoch')\n    inpT = [input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]]\n    targetT = [start_tokens[idxT,], end_tokens[idxT,]]\n    inpV = [input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]]\n    targetV = [start_tokens[idxV,], end_tokens[idxV,]]\n    # sort the validation data\n    shuffleV = np.int32(sorted(range(len(inpV[0])), key=lambda k: (inpV[0][k] == PAD_ID).sum(), reverse=True))\n    inpV = [arr[shuffleV] for arr in inpV]\n    targetV = [arr[shuffleV] for arr in targetV]\n    #weight_fn = '%s-roberta-%i.h5'%(VER,fold)\n    for epoch in range(1, EPOCHS + 1):\n        # sort and shuffle: We add random numbers to not have the same order in each epoch\n        shuffleT = np.int32(sorted(range(len(inpT[0])), key=lambda k: (inpT[0][k] == PAD_ID).sum() + np.random.randint(-3, 3), reverse=True))\n        # shuffle in batches, otherwise short batches will always come in the beginning of each epoch\n        num_batches = math.ceil(len(shuffleT) / BATCH_SIZE)\n        batch_inds = np.random.permutation(num_batches)\n        shuffleT_ = []\n        for batch_ind in batch_inds:\n            shuffleT_.append(shuffleT[batch_ind * BATCH_SIZE: (batch_ind + 1) * BATCH_SIZE])\n        shuffleT = np.concatenate(shuffleT_)\n        # reorder the input data\n        inpT = [arr[shuffleT] for arr in inpT]\n        targetT = [arr[shuffleT] for arr in targetT]\n        model.fit(inpT, targetT, \n            epochs=epoch, initial_epoch=epoch - 1, batch_size=BATCH_SIZE, verbose=DISPLAY, callbacks=[],\n            validation_data=(inpV, targetV), shuffle=False)  # don't shuffle in `fit`\n        #save_weights(model, weight_fn)\n\n    #print('Loading model...')\n    # model.load_weights('%s-roberta-%i.h5'%(VER,fold))\n    #load_weights(model, weight_fn)\n\n    print('Predicting OOF...')\n    oof_start[idxV,],oof_end[idxV,] = padded_model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n    \n    print('Predicting all Train for Outlier analysis...')\n    preds_train = padded_model.predict([input_ids,attention_mask,token_type_ids],verbose=DISPLAY)\n    preds_start_train += preds_train[0]/skf.n_splits\n    preds_end_train += preds_train[1]/skf.n_splits\n\n    print('Predicting Test...')\n    preds = padded_model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n    preds_start += preds[0]/skf.n_splits\n    preds_end += preds[1]/skf.n_splits\n    \n    # DISPLAY FOLD JACCARD\n    all = []\n    for k in idxV:\n        a = np.argmax(oof_start[k,])\n        b = np.argmax(oof_end[k,])\n        if a>b: \n            st = train.loc[k,'text'] # IMPROVE CV/LB with better choice here\n        else:\n            text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n            enc = tokenizer.encode(text1)\n            st = tokenizer.decode(enc.ids[a-2:b-1])\n        all.append(jaccard(st,train.loc[k,'selected_text']))\n    jac.append(np.mean(all))\n    print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all))\n    print()","metadata":{"execution":{"iopub.status.busy":"2021-07-10T12:55:57.216567Z","iopub.status.idle":"2021-07-10T12:55:57.217351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('>>>> OVERALL 5Fold CV Jaccard =',np.mean(jac))\nprint(jac) # Jaccard CVs","metadata":{"execution":{"iopub.status.busy":"2021-07-10T12:55:57.218655Z","iopub.status.idle":"2021-07-10T12:55:57.219486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Submission <a class=\"anchor\" id=\"4\"></a>","metadata":{}},{"cell_type":"code","source":"all = []\nfor k in range(input_ids_t.shape[0]):\n    a = np.argmax(preds_start[k,])\n    b = np.argmax(preds_end[k,])\n    if a>b: \n        st = test.loc[k,'text']\n    else:\n        text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode(enc.ids[a-2:b-1])\n    all.append(st)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-10T12:55:57.221374Z","iopub.status.idle":"2021-07-10T12:55:57.222181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['selected_text'] = all\ntest[['textID','selected_text']].to_csv('submission.csv',index=False)\ntest.sample(10)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T12:55:57.223546Z","iopub.status.idle":"2021-07-10T12:55:57.22449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[Go to Top](#0)","metadata":{}}]}