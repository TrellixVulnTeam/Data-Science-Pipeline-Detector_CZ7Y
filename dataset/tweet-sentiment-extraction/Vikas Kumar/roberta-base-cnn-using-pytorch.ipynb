{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\nimport re\nimport numpy as np\nimport pandas as pd\nimport transformers\nimport tokenizers\nimport torch\nimport torch.nn as nn\nfrom tqdm.autonotebook import tqdm\nfrom transformers import get_linear_schedule_with_warmup\nfrom transformers import AdamW\nMAX_LEN=192\nEPOCHS=5\nTRAIN_BATCH_SIZE=64\nVALID_BATCH_SIZE=16\nBERT_PATH=\"../input/roberta-base\"\nMODEL_PATH = \"model.bin\"\nTOKENIZER=tokenizers.ByteLevelBPETokenizer(\n    vocab_file=f'{BERT_PATH}/vocab.json',\n    merges_file=f'{BERT_PATH}/merges.txt',\n    lowercase=True,\n    add_prefix_space=True\n)\ndef process_data(tweet, selected_text, sentiment, max_len, tokenizer):\n    selected_text=' '.join(str(selected_text).split())\n    tweet= \" \".join(str(tweet).split())\n    len_st = len(str(selected_text))\n    idx0 = None\n    idx1 = None\n    for ind in (i for i, e in enumerate(str(tweet)) if e == selected_text[0]):\n        if tweet[ind: ind+len_st] == selected_text:\n            idx0 = ind\n            idx1 = ind + len_st - 1\n            break\n\n    char_targets = [0] * len(tweet)\n    if idx0 != None and idx1 != None:\n        for ct in range(idx0, idx1 + 1):\n            char_targets[ct] = 1\n    \n    tok_tweet = tokenizer.encode(tweet)\n    ids_orig = tok_tweet.ids\n    tweet_offset = tok_tweet.offsets\n    \n    target_idx = []\n    for j, (offset1, offset2) in enumerate(tweet_offset):\n        if sum(char_targets[offset1: offset2]) > 0:\n            target_idx.append(j)\n    orig_text=tweet\n    target_start = target_idx[0]\n    target_end = target_idx[-1]\n\n    sentiment_id = {\n        'positive': 1313,\n        'negative': 2430,\n        'neutral': 7974\n    }\n    \n    ids = [0] + [sentiment_id[sentiment]] + [2] + [2] + ids_orig + [2]\n    token_type_ids = [0, 0, 0, 0] + [0] * (len(ids_orig) + 1)\n    mask = [1] * len(token_type_ids)\n    tweet_offset = [(0, 0)] * 4 + tweet_offset + [(0, 0)]\n    target_start += 4\n    target_end += 4\n\n    padding_length = max_len - len(ids)\n    if padding_length > 0:\n        ids = ids + ([0] * padding_length)\n        mask = mask + ([0] * padding_length)\n        token_type_ids = token_type_ids + ([0] * padding_length)\n        tweet_offset = tweet_offset + ([(0, 0)] * padding_length)\n    return {\n        'ids': ids,\n        'text_offset': tweet_offset,\n        'token_type_ids': token_type_ids,\n        'ids_orig': ids_orig,\n        'masks': mask,\n        'target_end': target_end,\n        'target_start': target_start,\n        'sentiment': sentiment,\n        'selected_text': str(selected_text),\n        'orig_text': orig_text \n    }\nclass BertData:\n    def __init__(self,text,selected_text,sentiment):\n        self.text = text\n        self.selected_text = selected_text\n        self.sentiment = sentiment\n        self.max_len=MAX_LEN\n        self.tokenizer=TOKENIZER\n    \n    def __len__(self):\n        return len(self.text)\n\n    def __getitem__(self,item):\n        data = process_data(self.text[item],\n                            self.selected_text[item],\n                            self.sentiment[item],\n                            self.max_len,\n                            self.tokenizer)\n        return {\n        'ids': torch.tensor (data['ids'], dtype=torch.long),\n        'text_offset': torch.tensor (data['text_offset'], dtype=torch.long),\n        'token_type_ids': torch.tensor (data['token_type_ids'], dtype=torch.long),\n        'masks': torch.tensor (data['masks'], dtype=torch.long),\n        'target_end': torch.tensor (data['target_end'], dtype=torch.long),\n        'target_start': torch.tensor (data['target_start'], dtype=torch.long),\n        'sentiment': data['sentiment'],\n        'selected_text': data['selected_text'],\n        'orig_text': data['orig_text'] \n    }\nclass bertmodel(transformers.BertPreTrainedModel):\n    def __init__(self, conf):\n        super(bertmodel, self).__init__(conf)\n        self.bert = transformers.RobertaModel.from_pretrained(BERT_PATH, config=conf)\n        self.dropout= nn.Dropout(0.5)\n        \n        self.bn1 = nn.BatchNorm1d(num_features=192)\n        self.bn2 = nn.BatchNorm1d(num_features=192)\n        \n        \n        self.c1=nn.Conv1d(768,768,2)\n        self.c11=nn.Conv1d(768,256,2)\n        self.c111=nn.Conv1d(256,64,2)\n        self.c2=nn.Conv1d(768,768,2)\n        self.c22=nn.Conv1d(768,256,2)\n        self.c222=nn.Conv1d(256,64,2)\n        self.Leaky= nn.ReLU(0.3)\n        self.i0=nn.Linear(64,1)\n        self.i1=nn.Linear(64,1)\n        nn.init.normal_(self.i0.bias, 0)\n        nn.init.normal_(self.i0.weight, std=0.02)\n        nn.init.normal_(self.i1.bias, 0)\n        nn.init.normal_(self.i1.weight, std=0.02)\n        \n\n    def forward(self, ids, masks, token_type_ids):\n        _,_,out=self.bert(\n            ids,\n            attention_mask=masks,\n            token_type_ids=token_type_ids\n        )\n        out = torch.stack([out[-1], out[-2], out[-3], out[-4]])\n        out = torch.mean(out, 0)\n\n        #out=torch.cat((out[-1],out[-2]), dim=-1)\n        out=self.dropout(out)\n        out = nn.functional.pad(out.transpose(1,2), (1, 0))\n\n        out1 = self.c1(out).transpose(1,2)\n        out1=self.Leaky(self.bn1 (out1))\n        out1 = self.c11(nn.functional.pad(out1.transpose(1,2), (1, 0))).transpose(1,2)\n        out1=self.Leaky(self.bn2 (out1))\n        out1 = self.c111(nn.functional.pad(out1.transpose(1,2), (1, 0))).transpose(1,2)\n        out1=self.Leaky(self.bn2 (out1))\n        \n        out2 = self.c2(out).transpose(1,2)\n        out2=self.Leaky(self.bn1 (out2))\n        out2 = self.c22(nn.functional.pad(out2.transpose(1,2), (1, 0))).transpose(1,2)\n        out2=self.Leaky(self.bn2 (out2))\n        out2 = self.c222(nn.functional.pad(out2.transpose(1,2), (1, 0))).transpose(1,2)\n        out2=self.Leaky(self.bn2 (out2))\n        start_logits = self.i0(self.dropout(out1)).squeeze(-1)\n        end_logits = self.i1(self.dropout(out2)).squeeze(-1)\n        return start_logits, end_logits\n\n\ndef loss_fn(start_logits, end_logits, start_pos, end_pos):\n    loss= nn.CrossEntropyLoss()\n    start_loss=loss(start_logits, start_pos)\n    end_loss=loss(end_logits, end_pos)\n    total_loss=(start_loss+end_loss)\n    return total_loss\ndef jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))\n\ndef calculate_jaccard(orig_text, sentiment, target_str, ind_start, ind_end, offsets, verbose=False):\n    filtered_out=''\n    if ind_start>ind_end:\n        filtered_out=orig_text\n    \n    for i in range(ind_start, ind_end+1):\n        filtered_out+=orig_text[offsets[i][0]:offsets[i][1]]\n        if (i+1)<len(offsets) and offsets[i][1]<offsets[i+1][0]:\n            filtered_out+=' '\n    if sentiment=='neutral' or len(str(orig_text).split())<2:\n        filtered_out=orig_text\n    \n    jac=jaccard(str(target_str).strip(),str(filtered_out).strip())\n    return jac, filtered_out\nclass AverageMeter:\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\ndef train_fn(data_loader, model, optimizer, device, scheduler=None):\n    model.train()\n    losses=AverageMeter()\n    jaccards=AverageMeter()\n\n    tk0 = tqdm(data_loader, total=len(data_loader))\n    for ind, d in enumerate(tk0,0):\n        \n        ids = d['ids']\n        text_offset = d['text_offset']\n        token_type_ids = d['token_type_ids']\n        target_end = d['target_end']\n        target_start = d['target_start']\n        sentiment = d['sentiment']\n        selected_text = d['selected_text']\n        masks = d['masks']\n        orig_text = d['orig_text']\n        \n        ids = ids.to(device, dtype=torch.long)\n        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n        target_start = target_start.to(device, dtype=torch.long)\n        target_end = target_end.to(device, dtype=torch.long)\n        masks = masks.to(device, dtype=torch.long)\n\n        model.zero_grad()\n        out_start, out_end = model(\n            ids,\n            token_type_ids = token_type_ids,\n            masks = masks\n        )\n\n        loss=loss_fn(out_start, out_end, target_start, target_end)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n\n        out_start = torch.softmax(out_start, dim=1).cpu().detach().numpy()\n        out_end = torch.softmax(out_end, dim=1).cpu().detach().numpy()  \n\n        jaccard_scores=[]\n        for ind, x in enumerate(orig_text):\n            selected=selected_text[ind]\n            sentiment1=sentiment[ind]\n            jaccard_score,_=calculate_jaccard(x,\n                        sentiment1,\n                        selected,\n                        np.argmax(out_start[ind,:]),\n                        np.argmax(out_end[ind,:]),\n                        text_offset[ind]\n            )\n            jaccard_scores.append(jaccard_score)\n        jaccards.update(np.mean(jaccard_scores),ids.size(0))\n        losses.update(loss.item(),ids.size(0))\n        tk0.set_postfix(loss=losses.avg, jaccard=jaccards.avg)\ndef eval_fn(data_loader,model, device):\n    model.eval()\n    losses=AverageMeter()\n    jaccards=AverageMeter()\n\n    with torch.no_grad():\n        tk0 = tqdm(data_loader, total=len(data_loader))\n        for ind, d in enumerate(tk0,0):\n        \n            ids = d['ids']\n            text_offset = d['text_offset'].numpy()\n            token_type_ids = d['token_type_ids']\n            target_end = d['target_end']\n            target_start = d['target_start']\n            sentiment = d['sentiment']\n            selected_text = d['selected_text']\n            masks = d['masks']\n            orig_text = d['orig_text']\n        \n            ids = ids.to(device, dtype=torch.long)\n            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n            target_start = target_start.to(device, dtype=torch.long)\n            target_end = target_end.to(device, dtype=torch.long)\n            masks = masks.to(device, dtype=torch.long)\n\n            out_start, out_end = model(\n            ids=ids,\n            token_type_ids = token_type_ids,\n            masks = masks\n            )\n\n            loss=loss_fn(out_start, out_end, target_start, target_end)\n            \n            out_start = torch.softmax(out_start, dim=1).cpu().detach().numpy()\n            out_end = torch.softmax(out_end, dim=1).cpu().detach().numpy()  \n\n            jaccard_scores=[]\n            for ind, x in enumerate(orig_text):\n                selected=selected_text[ind]\n                sentiment1=sentiment[ind]\n                jaccard_score,_=calculate_jaccard(x,\n                        sentiment1,\n                        selected,\n                        np.argmax(out_start[ind,:]),\n                        np.argmax(out_end[ind,:]),\n                        text_offset[ind]\n\n                )\n                jaccard_scores.append(jaccard_score)\n            jaccards.update(np.mean(jaccard_scores),ids.size(0))\n            losses.update(loss.item(),ids.size(0))\n            tk0.set_postfix(loss=losses.avg, jaccard=jaccards.avg)\n    print(f'jaccard score: {jaccards.avg}')\n    return jaccards.avg\nclass EarlyStopping:\n    def __init__(self, patience=7, mode=\"max\", delta=0.001):\n        self.patience = patience\n        self.counter = 0\n        self.mode = mode\n        self.best_score = None\n        self.early_stop = False\n        self.delta = delta\n        if self.mode == \"min\":\n            self.val_score = np.Inf\n        else:\n            self.val_score = -np.Inf\n\n    def __call__(self, epoch_score, model, model_path):\n\n        if self.mode == \"min\":\n            score = -1.0 * epoch_score\n        else:\n            score = np.copy(epoch_score)\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(epoch_score, model, model_path)\n        elif score < self.best_score + self.delta:\n            self.counter += 1\n            print('EarlyStopping counter: {} out of {}'.format(self.counter, self.patience))\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(epoch_score, model, model_path)\n            self.counter = 0\n\n    def save_checkpoint(self, epoch_score, model, model_path):\n        if epoch_score not in [-np.inf, np.inf, -np.nan, np.nan]:\n            print('Validation score improved ({} --> {}). Saving model!'.format(self.val_score, epoch_score))\n            torch.save(model.state_dict(), model_path)\n        self.val_score = epoch_score\ndef run(fold):\n    df=pd.read_csv('../input/tweet-sentiment-extraction/train.csv')\n    df_len_per_fold=int(len(df)/5)\n    df_fold=df.copy()\n    df_fold.loc[:,'kfold']=-1\n    df_fold.loc[:df_len_per_fold*1,'kfold']=0\n    df_fold.loc[df_len_per_fold*1:df_len_per_fold*2,'kfold']=1\n    df_fold.loc[df_len_per_fold*2:df_len_per_fold*3,'kfold']=2\n    df_fold.loc[df_len_per_fold*3:df_len_per_fold*4,'kfold']=3\n    df_fold.loc[df_len_per_fold*4:,'kfold']=4\n    \n    train_data_fold= df_fold[df_fold.loc[:,'kfold']!=fold].reset_index(drop=True)\n    valid_data_fold= df_fold[df_fold.loc[:,'kfold']==fold].reset_index(drop=True)\n\n    train_data=BertData(\n        text=train_data_fold.text.values,\n        selected_text=train_data_fold.selected_text.values,\n        sentiment=train_data_fold.sentiment.values\n    )\n\n    train_data_loader= torch.utils.data.DataLoader(\n        train_data,\n        batch_size=TRAIN_BATCH_SIZE,\n        num_workers=4\n    )\n\n    valid_data=BertData(\n        text=valid_data_fold.text.values,\n        selected_text=valid_data_fold.selected_text.values,\n        sentiment=valid_data_fold.sentiment.values\n    )\n\n    valid_data_loader= torch.utils.data.DataLoader(\n        valid_data,\n        batch_size=VALID_BATCH_SIZE,\n        num_workers=2\n    )\n\n    device = torch.device(\"cuda\")\n    model_config= transformers.BertConfig.from_pretrained(BERT_PATH)\n    model_config.output_hidden_states=True\n    model= bertmodel(conf=model_config)\n    model.to(device)\n\n    train_steps= len(train_data_fold)/TRAIN_BATCH_SIZE* EPOCHS\n    param_optimizer= list(model.named_parameters())\n    no_decay=['bias', 'LayerNorm.bias', 'LayerNorm.weights']\n    optimizer_paramters=[\n                         {'params': [para for name, para in param_optimizer if not any(nd in name for nd in no_decay)], 'weight_decay': 0.001},\n                         {'params': [para for name, para in param_optimizer if any(nd in name for nd in no_decay)], 'weight_decay': 0.0}\n    ]\n\n    optimizer= AdamW(optimizer_paramters, lr=3e-5)\n    scheduler= get_linear_schedule_with_warmup(\n        optimizer, \n        num_warmup_steps=0, \n        num_training_steps= train_steps\n    )\n\n    es= EarlyStopping(patience=6, mode='max')\n    print(f'Training strated for fold {fold}')\n\n    for epoch in range(5):\n        train_fn(train_data_loader, model,optimizer ,device, scheduler=scheduler)\n        jaccard=eval_fn(valid_data_loader, model, device)\n        print(f'Jaccard score: {jaccard}')\n        es(jaccard, model, model_path=f'model_{fold}.bin')\n        if es.early_stop:\n            print('Early stopping...')\n            break\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"run(fold=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"run(fold=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"run(fold=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"run(fold=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"run(fold=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test=pd.read_csv('../input/tweet-sentiment-extraction/test.csv')\ndf_test.loc[:,'selected_text']=df_test.text.values\ndevice='cuda'\nmodel_config=transformers.BertConfig.from_pretrained(BERT_PATH)\nmodel_config.output_hidden_states=True\nmodel1=bertmodel(conf=model_config)\nmodel1.to(device)\nmodel1.load_state_dict(torch.load('model_0.bin'))\nmodel1.eval()\n\nmodel2=bertmodel(conf=model_config)\nmodel2.to(device)\nmodel2.load_state_dict(torch.load('model_1.bin'))\nmodel2.eval()\n\nmodel3=bertmodel(conf=model_config)\nmodel3.to(device)\nmodel3.load_state_dict(torch.load('model_2.bin'))\nmodel3.eval()\n\nmodel4=bertmodel(conf=model_config)\nmodel4.to(device)\nmodel4.load_state_dict(torch.load('model_3.bin'))\nmodel4.eval()\n\nmodel5=bertmodel(conf=model_config)\nmodel5.to(device)\nmodel5.load_state_dict(torch.load('model_4.bin'))\nmodel5.eval()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_out=[]\n\ntest_data=BertData(\n    text=df_test.text.values,\n    selected_text=df_test.selected_text.values,\n    sentiment=df_test.sentiment.values\n)\n\ntest_data_loader=torch.utils.data.DataLoader(\n    test_data,\n    batch_size=VALID_BATCH_SIZE,\n    shuffle=False,\n    num_workers=1\n)\n\nwith torch.no_grad():\n    tk0=tqdm(test_data_loader, total=len(test_data_loader))\n    for ind, d in enumerate(tk0):\n        ids = d['ids']\n        text_offset = d['text_offset']\n        token_type_ids = d['token_type_ids']\n        target_end = d['target_end']\n        target_start = d['target_start']\n        sentiment = d['sentiment']\n        selected_text = d['selected_text']\n        masks = d['masks']\n        orig_text = d['orig_text']\n        \n        ids = ids.to(device, dtype=torch.long)\n        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n        target_start = target_start.to(device, dtype=torch.long)\n        target_end = target_end.to(device, dtype=torch.long)\n        masks = masks.to(device, dtype=torch.long)\n\n        out_start1, out_end1 = model1(\n            ids,\n            token_type_ids = token_type_ids,\n            masks = masks\n            )\n        out_start2, out_end2 = model2(\n            ids,\n            token_type_ids = token_type_ids,\n            masks = masks\n            )\n        out_start3, out_end3 = model3(\n            ids,\n            token_type_ids = token_type_ids,\n            masks = masks\n            )\n        out_start4, out_end4 = model4(\n            ids,\n            token_type_ids = token_type_ids,\n            masks = masks\n            )\n        out_start5, out_end5 = model5(\n            ids,\n            token_type_ids = token_type_ids,\n            masks = masks\n            )\n        out_start=(out_start1 + out_start2 + out_start3 + out_start4 + out_start5)/5\n        out_end=(out_end1 + out_end2 + out_end3 + out_end4 + out_end5)/5\n\n        out_start = torch.softmax(out_start, dim=1).cpu().detach().numpy()\n        out_end = torch.softmax(out_end, dim=1).cpu().detach().numpy()\n\n        for ind, x in enumerate(orig_text):\n            selected=selected_text[ind]\n            sentiment1=sentiment[ind]\n            _, out_sequences=calculate_jaccard(\n                       x,\n                       sentiment1,\n                        selected,\n                        np.argmax(out_start[ind,:]),\n                        np.argmax(out_end[ind,:]),\n                        text_offset[ind]\n            )\n            final_out.append(out_sequences)\n\ndef process(selected):\n    return ' '.join(set(selected.lower().split()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample=pd.read_csv('../input/tweet-sentiment-extraction/sample_submission.csv')\nsample.loc[:,'selected_text']=final_out\nsample.selected_text=sample.selected_text.map(process)\nsample.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}