{"cells":[{"metadata":{"id":"vJDrVncV3Tvx"},"cell_type":"markdown","source":"NLP M3 TEST/PROBLEM STATEMENT SOLUTION BY :\n\n\n1.   *J005* - Harsh Bhabhera\n2.   *J019* - Mit Inamdar\n3.   *J020* - Avyact Jain\n\n**Date of Submission - April 11, 2020**\n\nNote :- This is a one click run notebook. It approximately takes 5-7 minutes to run completely. The trained models took about 9 hours to train.\n\nThe final result would be saved in a file 'final_result.csv', flushed in the local directory of the program.\n\n"},{"metadata":{"id":"YlHv0TIG3WIF","outputId":"01ca2942-29a5-4541-8bf1-ae35922b1e9d","trusted":true},"cell_type":"code","source":"import re\nimport string\nimport numpy as np \nimport random\nimport pandas as pd \nimport nltk\nnltk.download('stopwords')\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom plotly import graph_objs as go\nimport plotly.express as px\nimport plotly.figure_factory as ff\nfrom collections import Counter\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport nltk\nfrom nltk.corpus import stopwords\nfrom tqdm import tqdm\nimport os\nimport nltk\nimport spacy\nimport random\nfrom spacy.util import compounding\nfrom spacy.util import minibatch\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"id":"8Q4b4w_SyxsB"},"cell_type":"markdown","source":"Clone the repository with all the trained models and files"},{"metadata":{"id":"5AjUELhbywcK","outputId":"1e0d5756-a72d-42cf-fca9-cd26f79fc6ff","trusted":true},"cell_type":"code","source":"! git clone https://github.com/avyactjain/nlpM3.git","execution_count":null,"outputs":[]},{"metadata":{"id":"HO1pu4hzrSs4"},"cell_type":"markdown","source":"## Exploratory Data Analysis"},{"metadata":{"id":"TBFPcqP8rWAz","trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv')\ntest = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"id":"E0Iq0MDarWHd","outputId":"8666f670-5f91-465a-dd46-6a104844a20c","trusted":true},"cell_type":"code","source":"print(train.shape)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{"id":"5XXJka34rWKf","outputId":"79031ad3-5bf7-45aa-a220-a04113a6737e","trusted":true},"cell_type":"code","source":"train.dropna(inplace=True)\ntrain.info()","execution_count":null,"outputs":[]},{"metadata":{"id":"uQMRwAH6rWOS","outputId":"6da666fa-bf1d-46bd-83fb-fb7d62c0025b","trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"xsENRu6frWRg","outputId":"af1fdb9b-6a4d-4680-dc73-1d9df6866dab","trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"id":"9XWH7meqrWW2","outputId":"0c240aad-e559-458e-8a29-468b03028993","trusted":true},"cell_type":"code","source":"temp = train.groupby('sentiment').count()['text'].reset_index().sort_values(by='text',ascending=False)\ntemp.style.background_gradient(cmap='Greens')","execution_count":null,"outputs":[]},{"metadata":{"id":"AL6qK260rWdQ","outputId":"fddfa4d0-42ce-44a9-d904-73aa47e5401b","trusted":true},"cell_type":"code","source":"fig = go.Figure(go.Funnelarea(\n    text =temp.sentiment,\n    values = temp.text,\n    title = {\"position\": \"top center\", \"text\": \"Funnel-Chart of Sentiment Distribution\"}\n    ))\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"NeQxCD26rWhk","trusted":true},"cell_type":"code","source":"def clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = str(text).lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text","execution_count":null,"outputs":[]},{"metadata":{"id":"K9W13R9xrWgQ","trusted":true},"cell_type":"code","source":"train['text'] = train['text'].apply(lambda x:clean_text(x))\ntrain['selected_text'] = train['selected_text'].apply(lambda x:clean_text(x))","execution_count":null,"outputs":[]},{"metadata":{"id":"R4CM1NBNzlv1","outputId":"0fd1c8aa-a700-458f-a868-d9b76c74a096","trusted":true},"cell_type":"code","source":"train['temp_list'] = train['selected_text'].apply(lambda x:str(x).split())\ntop = Counter([item for sublist in train['temp_list'] for item in sublist])\ntemp = pd.DataFrame(top.most_common(20))\ntemp.columns = ['Common_words','count']\ntemp.style.background_gradient(cmap='Blues')","execution_count":null,"outputs":[]},{"metadata":{"id":"RdceYSMpvyp9","trusted":true},"cell_type":"code","source":"def remove_stopword(x):\n    return [y for y in x if y not in stopwords.words('english')]\ntrain['temp_list'] = train['temp_list'].apply(lambda x:remove_stopword(x))","execution_count":null,"outputs":[]},{"metadata":{"id":"anz6IMp8v1ue","outputId":"120c8746-f113-4a14-9818-395e6212f75b","trusted":true},"cell_type":"code","source":"fig = px.bar(temp, x=\"count\", y=\"Common_words\", title='Commmon Words in Selected Text', orientation='h', \n             width=700, height=700,color='Common_words')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"-YqHrjup093L","trusted":true},"cell_type":"code","source":"def plot_wordcloud(text, mask=None, max_words=200, max_font_size=100, figure_size=(24.0,16.0), color = 'white',\n                   title = None, title_size=40, image_color=False):\n    stopwords = set(STOPWORDS)\n    more_stopwords = {'u', \"im\"}\n    stopwords = stopwords.union(more_stopwords)\n\n    wordcloud = WordCloud(background_color=color,\n                    stopwords = stopwords,\n                    max_words = max_words,\n                    max_font_size = max_font_size, \n                    random_state = 42,\n                    width=1980, \n                    height=1020,\n                    mask = mask)\n    wordcloud.generate(str(text))\n    \n    plt.figure(figsize=figure_size)\n    if image_color:\n        image_colors = ImageColorGenerator(mask);\n        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n        plt.title(title, fontdict={'size': title_size,  \n                                  'verticalalignment': 'bottom'})\n    else:\n        plt.imshow(wordcloud);\n        plt.title(title, fontdict={'size': title_size, 'color': 'black', \n                                  'verticalalignment': 'bottom'})\n    plt.axis('off');\n    plt.tight_layout()  ","execution_count":null,"outputs":[]},{"metadata":{"id":"i5Ht5Oyc1UL4","outputId":"8664af9a-54b6-41f3-adee-ca4240aae603","trusted":true},"cell_type":"code","source":"plot_wordcloud(train[train['sentiment'] == 'neutral']['text'],color='white',max_font_size=100,title_size=100,title=\"WordCloud of Neutral Tweets\")","execution_count":null,"outputs":[]},{"metadata":{"id":"qerxxypB144L","outputId":"e571b139-c246-4e38-c6d0-7b6991aa7630","trusted":true},"cell_type":"code","source":"plot_wordcloud(train[train['sentiment'] == 'positive']['text'],color='white',max_font_size=100,title_size=100,title=\"WordCloud of Positive Tweets\")","execution_count":null,"outputs":[]},{"metadata":{"id":"dTfpYCQ-19dN","outputId":"6c80bd1f-718d-4e6e-eb76-77e06c87329d","trusted":true},"cell_type":"code","source":"plot_wordcloud(train[train['sentiment'] == 'negative']['text'],color='white',max_font_size=100,title_size=100,title=\"WordCloud of Negative Tweets\")","execution_count":null,"outputs":[]},{"metadata":{"id":"A4pcjjaPwPSA"},"cell_type":"markdown","source":"# Model - Named Entity Recognition using Spacy. \nAs this is essentially a problem that deals with extracting the most important words that impact the sentiment of the tweet, we believe it could be solved by Named entity recognition. We used spacy for the same. We have provided trained models which have been trained for 1000 epochs. However, in this notebook we have set epochs to 3 just for demo purpose. We would be using 2 separate models (one for positive sentiment and another for negative sentiment) as for the neutral sentiment tweet we observed that the extracted keywords are same as the tweet itself."},{"metadata":{"id":"eBMSevF9jixv","trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv')\ndf_test = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"id":"mJixZaglkExp","trusted":true},"cell_type":"code","source":"df_train['Num_words_text'] = df_train['text'].apply(lambda x:len(str(x).split())) #Number Of words in main Text in train set","execution_count":null,"outputs":[]},{"metadata":{"id":"jzzUAKfqkLKD","trusted":true},"cell_type":"code","source":"df_train = df_train[df_train['Num_words_text']>=3]","execution_count":null,"outputs":[]},{"metadata":{"id":"qbqJBqFPxFTy"},"cell_type":"markdown","source":"# Helper functions"},{"metadata":{"id":"t_Tfa52RkMyw","trusted":true},"cell_type":"code","source":"def save_model(output_dir, nlp, new_model_name):\n\n    output_dir = '/kaggle/working/working/' + output_dir\n    if output_dir is not None:        \n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n        nlp.meta[\"name\"] = new_model_name\n        nlp.to_disk(output_dir)\n        print(\"Saved model to\", output_dir)\n\n\ndef train(train_data, output_dir, n_iter=20, model=None):\n\n    if model is not None:\n        nlp = spacy.load(output_dir)  # load existing spaCy model\n        print(\"Loaded model '%s'\" % model)\n    else:\n        nlp = spacy.blank(\"en\")  # create blank Language class\n        print(\"Created blank 'en' model\")\n    if \"ner\" not in nlp.pipe_names:\n        ner = nlp.create_pipe(\"ner\")\n        nlp.add_pipe(ner, last=True)\n    else:\n        ner = nlp.get_pipe(\"ner\")\n    \n    for _, annotations in train_data:\n        for ent in annotations.get(\"entities\"):\n            ner.add_label(ent[2])\n\n    # get names of other pipes to disable them during training\n    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n    with nlp.disable_pipes(*other_pipes):  # only train NER\n        if model is None:\n            nlp.begin_training()\n        else:\n            nlp.resume_training()\n\n\n        for itn in tqdm(range(n_iter)):\n            random.shuffle(train_data)\n            batches = minibatch(train_data, size=compounding(4.0, 500.0, 1.001))    \n            losses = {}\n            for batch in batches:\n                texts, annotations = zip(*batch)\n                nlp.update(texts,  # batch of texts\n                            annotations,  # batch of annotations\n                            drop=0.5,   # dropout - make it harder to memorise data\n                            losses=losses, \n                            )\n            print(\"Losses\", losses)\n    save_model(output_dir, nlp, 'st_ner')\n\n\ndef get_model_out_path(sentiment):\n\n    model_out_path = None\n    if sentiment == 'positive':\n        model_out_path = 'models/model_pos'\n    elif sentiment == 'negative':\n        model_out_path = 'models/model_neg'\n    return model_out_path\n\n\ndef get_training_data(sentiment):\n\n    train_data = []\n    for index, row in df_train.iterrows():\n        if row.sentiment == sentiment:\n            selected_text = row.selected_text\n            text = row.text\n            start = text.find(selected_text)\n            end = start + len(selected_text)\n            train_data.append((text, {\"entities\": [[start, end, 'selected_text']]}))\n    return train_data\n\ndef predict_entities(text, model):\n    doc = model(text)\n    ent_array = []\n    for ent in doc.ents:\n        start = text.find(ent.text)\n        end = start + len(ent.text)\n        new_int = [start, end, ent.label_]\n        if new_int not in ent_array:\n            ent_array.append([start, end, ent.label_])\n    selected_text = text[ent_array[0][0]: ent_array[0][1]] if len(ent_array) > 0 else text\n    return selected_text","execution_count":null,"outputs":[]},{"metadata":{"id":"AIgPXU6QxLB1"},"cell_type":"markdown","source":"Creating the model for Positive Tweets.\nThese models will be saved in a directory called 'workingmodels'"},{"metadata":{"id":"39_7r1Z8kVrW","outputId":"73ff2734-4122-4046-85c0-fcb30ece0417","trusted":true},"cell_type":"code","source":"sentiment = 'positive'\n\ntrain_data = get_training_data(sentiment)\nmodel_path = get_model_out_path(sentiment)\n\ntrain(train_data, model_path, n_iter=3, model=None)","execution_count":null,"outputs":[]},{"metadata":{"id":"b5-RVvo8xPOC"},"cell_type":"markdown","source":"Creating the model for Negative Tweets. These models will be saved in a directory called 'workingmodels'\n"},{"metadata":{"id":"F3GdkltAkXE8","outputId":"6b4cfe88-5063-4a1d-d3b8-bf2ba3dfa8e8","trusted":true},"cell_type":"code","source":"sentiment = 'negative'\n\ntrain_data = get_training_data(sentiment)\nmodel_path = get_model_out_path(sentiment)\n\ntrain(train_data, model_path, n_iter=3, model=None)","execution_count":null,"outputs":[]},{"metadata":{"id":"AQktEAytyDT6"},"cell_type":"markdown","source":"In the following cell, there is the prediction step of the notebook. **You could Change the MODELS_BASE_PATH** to either the saved models or the workingmodels directory. By deafult we have kept it to the trained models."},{"metadata":{"id":"S-7ewKvilfs3","outputId":"fd43438e-f933-4e13-9c6b-c7c174d6f6f2","trusted":true},"cell_type":"code","source":"selected_texts = []\nMODELS_BASE_PATH = '/kaggle/working/nlpM3/saved_models/'\n# MODELS_BASE_PATH = '/kaggle/working/working/models/'\n\nif MODELS_BASE_PATH is not None:\n    print(\"Loading Models  from \", MODELS_BASE_PATH)\n    model_pos = spacy.load(MODELS_BASE_PATH + 'model_pos')\n    model_neg = spacy.load(MODELS_BASE_PATH + 'model_neg')\n        \n    for index, row in df_test.iterrows():\n        text = row.text\n        output_str = \"\"\n        if row.sentiment == 'neutral' or len(text.split()) <= 2:\n            selected_texts.append(text)\n        elif row.sentiment == 'positive':\n            selected_texts.append(predict_entities(text, model_pos))\n        else:\n            selected_texts.append(predict_entities(text, model_neg))\n        \ndf_test['selected_text(output)'] = selected_texts","execution_count":null,"outputs":[]},{"metadata":{"id":"RPO3wV-rm9qT","outputId":"f6f6ccdc-5d9e-427e-bc29-c7bce4db50f7","trusted":true},"cell_type":"code","source":"df_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_submission = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/sample_submission.csv')\ndf_submission['selected_text'] = df_test['selected_text(output)']\ndisplay(df_submission.head(10))\ndf_submission.to_csv(\"submission.csv\", index=False)\n","execution_count":null,"outputs":[]}],"metadata":{"colab":{"name":"NLP M3.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":4}