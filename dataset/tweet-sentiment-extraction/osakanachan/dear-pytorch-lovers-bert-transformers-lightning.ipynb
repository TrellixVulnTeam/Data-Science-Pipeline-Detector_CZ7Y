{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Preface"},{"metadata":{},"cell_type":"markdown","source":"Some non-engineer NLP beginners, like me, may be literate for PyTorch but not for Keras/TensorFlow.  \n  \nThis notebook has two major objectives:\n\n- To share starter code for PyTorch.\n\n- To provide an example of easy implementation with PyTorch-Lightning. PyTorch does not provide wrappers for training neural networks, but PyTorch-Lightning will free us from explicit for-loops."},{"metadata":{},"cell_type":"markdown","source":"# 0. Dependencies"},{"metadata":{},"cell_type":"markdown","source":"### 0-0. Debug mode flag"},{"metadata":{"trusted":true},"cell_type":"code","source":"DEBUG_MODE = False","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 0-1. Install PyTorch-Lightning"},{"metadata":{},"cell_type":"markdown","source":"This notebook requires PyTorch-Lighning.  \n\nTo use PyTorch-Lighning on Internet-off notebook, the following dataset is helpful:  \n\n[pytorch-lightning 0.7.1](https://www.kaggle.com/higepon/pytorchlightning-071) by @[higepon](https://www.kaggle.com/higepon)"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!pip install ../input/pytorchlightning-071/pytorch-lightning-0.7.1/pytorch-lightning-0.7.1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 0-2. Import modules"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom pathlib import Path\n\nimport os\nimport random\n\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom collections import Counter\nfrom transformers import BertForQuestionAnswering\nfrom transformers import BertTokenizer\n\nimport pytorch_lightning as pl\n\nfrom tqdm import tqdm_notebook as tqdm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Preparation"},{"metadata":{},"cell_type":"markdown","source":"### 1-1. Ensure determinism"},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed=1234):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seed_everything()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1-2. Load dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"DIR = Path('../input/tweet-sentiment-extraction')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv(DIR / 'train.csv')\ndf_test = pd.read_csv(DIR / 'test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1-3. Clean dataset"},{"metadata":{},"cell_type":"markdown","source":"Convert a few float type samples in 'text' and 'selected_text' columns into strings."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['text'] = df_train['text'].apply(lambda x: str(x))\ndf_test['text'] = df_test['text'].apply(lambda x: str(x))\ndf_train['uncased_text'] = df_train['text'].apply(lambda x: x.lower())\ndf_test['uncased_text'] = df_test['text'].apply(lambda x: x.lower())\ndf_train['selected_text'] = df_train['selected_text'].apply(lambda x: str(x).lower())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1-4. Tokenize"},{"metadata":{},"cell_type":"markdown","source":"Hugging Face Transformers tokenizers must be prepared in your Kaggle dataset to use in off-line notebook.  \n  \nSee also [berttokenizer-base-uncased](https://www.kaggle.com/yutanakamura/berttokenizer-base-uncased)."},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('../input/berttokenizer-base-uncased')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tokenize\ndf_train['tokenized_text'] = df_train['uncased_text'].apply(tokenizer.tokenize)\ndf_test['tokenized_text'] = df_test['uncased_text'].apply(tokenizer.tokenize)\ndf_train['tokenized_selected_text'] = df_train['selected_text'].apply(tokenizer.tokenize)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1-5. Start & end positions"},{"metadata":{},"cell_type":"markdown","source":"Start positions and end positions of selected texts in tokenized source texts."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filter train data\nstart_position_candidates = []\nend_position_candidates = []\ndf_train['select_length'] = df_train['tokenized_selected_text'].map(len)\n\nfor i in tqdm(range(len(df_train))):\n    start_position_candidate = [j for j, tok in enumerate(df_train['tokenized_text'].iloc[i]) if tok == df_train['tokenized_selected_text'].iloc[i][0]]\n    end_position_candidate = [j for j, tok in enumerate(df_train['tokenized_text'].iloc[i]) if tok == df_train['tokenized_selected_text'].iloc[i][-1]]\n\n    start_position_candidate = [idx for idx in start_position_candidate if idx + df_train['select_length'].iloc[i] - 1 in end_position_candidate]\n    end_position_candidate = [idx for idx in end_position_candidate if idx - df_train['select_length'].iloc[i] + 1 in start_position_candidate]\n\n    start_position_candidates.append(start_position_candidate)\n    end_position_candidates.append(end_position_candidate)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If more than one candidates are available, we here take only the first candidate and discard the rest for plainness."},{"metadata":{"trusted":true},"cell_type":"code","source":"start_position_candidates = [l[0] if len(l) > 0 else -1 for l in start_position_candidates]\nend_position_candidates = [l[0] if len(l) > 0 else -1 for l in end_position_candidates]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For test data, set default start & end positions to dummy integer (-1)."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['start_position'] = start_position_candidates\ndf_train['end_position'] = end_position_candidates\ndf_test['start_position'] = -1\ndf_test['end_position'] = -1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Drop train data with no candidates of start & end positions due to poor segmentation of selected texts.  \n  \nSee also [this discussion](https://www.kaggle.com/c/tweet-sentiment-extraction/discussion/138272)."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = df_train.query('start_position!=-1')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1-6. Train/Val split"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train, df_val = train_test_split(df_train, train_size=0.8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1-7. Pos/Neg/Neu split"},{"metadata":{},"cell_type":"markdown","source":"This notebook will deal with positive, negative and neutral samples independently."},{"metadata":{"trusted":true},"cell_type":"code","source":"pos_train = df_train.query('sentiment==\"positive\"')\nneg_train = df_train.query('sentiment==\"negative\"')\nneu_train = df_train.query('sentiment==\"neutral\"')\n\npos_val = df_train.query('sentiment==\"positive\"')\nneg_val = df_train.query('sentiment==\"negative\"')\nneu_val = df_train.query('sentiment==\"neutral\"')\n\npos_test = df_test.query('sentiment==\"positive\"')\nneg_test = df_test.query('sentiment==\"negative\"')\nneu_test = df_test.query('sentiment==\"neutral\"')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. BERT fine-tuning"},{"metadata":{},"cell_type":"markdown","source":"Two fine-tuned models will be individual used for positive samples and negative.  \n  \nHugging Face Transformers BERT models must be prepared in your Kaggle dataset for use in the off-line notebook.  \n  \nSee also [bertforquestionanswering-base-uncased](https://www.kaggle.com/yutanakamura/bertforquestionanswering-base-uncased)."},{"metadata":{"trusted":true},"cell_type":"code","source":"pos_model = BertForQuestionAnswering.from_pretrained('../input/bertforquestionanswering-base-uncased')\nneg_model = BertForQuestionAnswering.from_pretrained('../input/bertforquestionanswering-base-uncased')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LENGTH = 128\nBATCH_SIZE = 32","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TrainDataset(Dataset):\n    def __init__(self, df):\n        super().__init__()\n        self.texts = df['uncased_text'].values\n        self.start_ids = df['start_position'].values\n        self.end_ids = df['end_position'].values\n        self.hash_index = df['textID'].values\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        returns = {\n            'text' : self.texts[idx],\n            'start' : self.start_ids[idx],\n            'end' : self.end_ids[idx],\n            'idx' : idx\n        }\n        return returns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TestDataset(Dataset):\n    def __init__(self, df):\n        super().__init__()\n        self.texts = df['uncased_text'].values\n        self.hash_index = df['textID'].values\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        returns = {\n            'text' : self.texts[idx],\n            'idx' : idx\n        }\n        return returns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ds_pos_train = TrainDataset(pos_train)\nds_neg_train = TrainDataset(neg_train)\n\nds_pos_val = TrainDataset(pos_val)\nds_neg_val = TrainDataset(neg_val)\n\nds_pos_test = TestDataset(pos_test)\nds_neg_test = TestDataset(neg_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dl_pos_train = DataLoader(ds_pos_train, batch_size=BATCH_SIZE, shuffle=True)\ndl_neg_train = DataLoader(ds_neg_train, batch_size=BATCH_SIZE, shuffle=True)\n\ndl_pos_val = DataLoader(ds_pos_val, batch_size=BATCH_SIZE, shuffle=False)\ndl_neg_val = DataLoader(ds_neg_val, batch_size=BATCH_SIZE, shuffle=False)\n\ndl_pos_test = DataLoader(ds_pos_test, batch_size=BATCH_SIZE, shuffle=False)\ndl_neg_test = DataLoader(ds_neg_test, batch_size=BATCH_SIZE, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class BaseSuperModule(pl.LightningModule):\n    def __init__(self, bertmodel, tokenizer, prediction_save_path):\n        super().__init__()\n        self.bertmodel = bertmodel\n        self.tokenizer = tokenizer\n        self.prediction_save_path = prediction_save_path\n\n    def get_device(self):\n        return self.bertmodel.state_dict()['bert.embeddings.word_embeddings.weight'].device\n\n    def save_predictions(self, start_positions, end_positions):\n        d = pd.DataFrame({'start_position':start_positions, 'end_position':end_positions})\n        d.to_csv(self.prediction_save_path, index=False)\n\n    def forward(self, batch):\n        \"\"\"\n        Input:\n            batch(dict), where\n                batch['text'] = uncased text: str\n                batch['idx'] = raw text: list(int)\n                batch['start'] = start position indices : list(int) (for train & val batch only)\n                batch['end'] = end position indices : list(int) (for train & val batch only)\n\n        Output:\n            For train batch, which has 'start' key and 'end' key:\n                Tuple of (loss(int), start_score(torch.tensor), end_score(torch.tensor))\n            For test batch, without 'start' key and 'end' key:\n                Tuple of (start_score(torch.tensor), end_score(torch.tensor))\n        \"\"\"\n        encoded_batch = tokenizer.batch_encode_plus(batch['text'], max_length=MAX_LENGTH, pad_to_max_length=True)\n        input_ids = torch.tensor(encoded_batch['input_ids']).to(self.get_device())\n        attention_mask = torch.tensor(encoded_batch['attention_mask']).to(self.get_device())\n        start_positions = batch['start'].to(self.get_device()) + 1  if 'start' in batch.keys() else None\n        end_positions = batch['end'].to(self.get_device()) + 1  if 'end' in batch.keys() else None\n\n        model_inputs = {\n            'input_ids' : input_ids,\n            'attention_mask' : attention_mask,\n            'start_positions' : start_positions,\n            'end_positions' : end_positions\n        }\n        \n        return self.bertmodel(**model_inputs)\n\n    def training_step(self, batch, batch_nb):\n        \"\"\"\n        (batch) -> (dict or OrderedDict)\n        # Caution: key for loss function must exactly be 'loss'.\n        \"\"\"\n        idx = batch['idx']\n        loss = self.forward(batch)[0]\n        return {'loss':loss, 'idx':idx}\n\n    def validation_step(self, batch, batch_nb):\n        \"\"\"\n        (batch) -> (dict or OrderedDict)\n        # Caution: key for loss function must exactly be 'loss'.\n        \"\"\"\n        idx = batch['idx']\n        loss = self.forward(batch)[0]\n        return {'loss':loss, 'idx':idx}\n\n    def test_step(self, batch, batch_nb):\n        \"\"\"\n        (batch) -> (dict or OrderedDict)\n        \"\"\"\n        idx = batch['idx']\n        start_scores = self.forward(batch)[0]\n        end_scores = self.forward(batch)[1]\n        return {'start_scores':start_scores, 'end_scores':end_scores, 'idx':idx}\n\n    def training_end(self, outputs):\n        \"\"\"\n        outputs(dict) -> loss(dict or OrderedDict)\n        # Caution: key must exactly be 'loss'.\n        \"\"\"\n        return {'loss':outputs['loss']}\n\n    def validation_end(self, outputs):\n        \"\"\"\n        For single dataloader:\n            outputs(list of dict) -> (dict or OrderedDict)\n        For multiple dataloaders:\n            outputs(list of (list of dict)) -> (dict or OrderedDict)\n        \"\"\"        \n        return {'loss':torch.mean(torch.tensor([output['loss'] for output in outputs])).detach()}\n\n    def test_end(self, outputs):\n        \"\"\"\n        For single dataloader:\n            outputs(list of dict) -> (dict or OrderedDict)\n        For multiple dataloaders:\n            outputs(list of (list of dict)) -> (dict or OrderedDict)\n        \"\"\"\n        start_scores = torch.cat([output['start_scores'] for output in outputs]).detach().cpu().numpy()\n        start_positions = np.argmax(start_scores, axis=1) - 1\n\n        end_scores = torch.cat([output['end_scores'] for output in outputs]).detach().cpu().numpy()\n        end_positions = np.argmax(end_scores, axis=1) - 1\n        self.save_predictions(start_positions, end_positions)\n        return {}\n\n    def configure_optimizers(self):\n        return optim.Adam(self.parameters(), lr=2e-5)\n\n    @pl.data_loader\n    def train_dataloader(self):\n        pass\n\n    @pl.data_loader\n    def val_dataloader(self):\n        pass\n\n    @pl.data_loader\n    def test_dataloader(self):\n        pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class PositiveModule(BaseSuperModule):\n    def __init__(self, bertmodel, tokenizer, prediction_save_path):\n        super().__init__(bertmodel, tokenizer, prediction_save_path)\n\n    @pl.data_loader\n    def train_dataloader(self):\n        return dl_pos_train\n\n    @pl.data_loader\n    def val_dataloader(self):\n        return dl_pos_val\n\n    @pl.data_loader\n    def test_dataloader(self):\n        return dl_pos_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class NegativeModule(BaseSuperModule):\n    def __init__(self, bertmodel, tokenizer, prediction_save_path):\n        super().__init__(bertmodel, tokenizer, prediction_save_path)\n\n    @pl.data_loader\n    def train_dataloader(self):\n        return dl_neg_train\n\n    @pl.data_loader\n    def val_dataloader(self):\n        return dl_neg_val\n\n    @pl.data_loader\n    def test_dataloader(self):\n        return dl_neg_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pos_module = PositiveModule(pos_model, tokenizer, 'pos_pred.csv')\nneg_module = NegativeModule(neg_model, tokenizer, 'neg_pred.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device('cuda:0') if torch.cuda.is_available() else 'cpu'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pos_module.to(device)\nneg_module.to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pos_trainer = pl.Trainer(max_nb_epochs=3, fast_dev_run=DEBUG_MODE)\nneg_trainer = pl.Trainer(max_nb_epochs=3, fast_dev_run=DEBUG_MODE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pos_trainer.fit(pos_module)\nneg_trainer.fit(neg_module)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pos_trainer.test()\nneg_trainer.test()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Postprocessing"},{"metadata":{},"cell_type":"markdown","source":"The previous cell will save prediction results as pos_pred.csv and neg_pred.csv.  \nLet us load them."},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pos_pred = pd.read_csv('pos_pred.csv')\nneg_pred = pd.read_csv('neg_pred.csv')\npos_pred","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Apply predicsion results onto df_test for positive & negative samples."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.index = df_test['textID']\ndf_test['selected_text'] = ''\n\ndf_test.loc[ds_pos_test.hash_index[:BATCH_SIZE if DEBUG_MODE else len(df_test)], 'start_position':'end_position'] = pos_pred.values\ndf_test.loc[ds_neg_test.hash_index[:BATCH_SIZE if DEBUG_MODE else len(df_test)], 'start_position':'end_position'] = neg_pred.values\ndf_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Convert start & end position into selected texts."},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in tqdm(range(len(df_test))):\n    if df_test['sentiment'].iloc[i] in ('positive', 'negative'):\n        tokenized_text = df_test['tokenized_text'].iloc[i]\n        start_position = max(df_test['start_position'].iloc[i], 0)\n        end_position = min(df_test['end_position'].iloc[i], len(tokenized_text) - 1)\n        \n        # restore original text\n        selected_text = tokenizer.convert_tokens_to_string(tokenized_text[start_position : end_position + 1])\n        for original_token in df_test['text'].iloc[i].split():\n            tokenized_form = tokenizer.convert_tokens_to_string(tokenizer.tokenize(original_token))\n            selected_text = selected_text.replace(tokenized_form, original_token, 1)\n        \n        df_test['selected_text'].iloc[i] = selected_text","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For neutral samples, use original texts as they are."},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in tqdm(range(len(df_test))):\n    if df_test['sentiment'].iloc[i] == 'neutral':\n        df_test['selected_text'].iloc[i] = df_test['text'].iloc[i]\n    else:\n        pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.loc[:, ['textID', 'selected_text']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.loc[:, ['textID', 'selected_text']].to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hope this notebook will help you all.  \n\nYour upvotes and comments will greatly be appreciated.\n\nLet us enjoy NLP!! ;)"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}