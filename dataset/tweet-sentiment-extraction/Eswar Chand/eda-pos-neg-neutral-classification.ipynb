{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":false,"_kg_hide-output":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"!pip install chart_studio\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom wordcloud import WordCloud, STOPWORDS\nimport spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom spacy.lang.en import English\nimport re\nimport string\n\nimport matplotlib.pyplot as plt\nfrom plotly import tools\nimport chart_studio.plotly as py\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nimport seaborn as sns\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reading the data"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv')\ntest = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv')\nsub = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['text'] = train['text'].str.replace('[{}]'.format(string.punctuation), '')\ntest['text'] = test['text'].str.replace('[{}]'.format(string.punctuation), '')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.size)\nprint(train.shape)\nprint(test.shape)\nprint(test.size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(train['sentiment'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Word clouds of Text:"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"# Thanks : https://www.kaggle.com/aashita/word-clouds-of-various-shapes ##\ndef plot_wordcloud(text, mask=None, max_words=200, max_font_size=100, figure_size=(10.0,10.0), color = 'white',\n                   title = None, title_size=40, image_color=False):\n    stopwords = set(STOPWORDS)\n    more_stopwords = {'one', 'br', 'Po', 'th', 'sayi', 'fo', 'Unknown'}\n    stopwords = stopwords.union(more_stopwords)\n\n    wordcloud = WordCloud(background_color=color,\n                    stopwords = stopwords,\n                    max_words = max_words,\n                    max_font_size = max_font_size, \n                    random_state = 42,\n                    width=600, \n                    height=300,\n                    mask = mask)\n    wordcloud.generate(str(text))\n    \n    plt.figure(figsize=figure_size)\n    if image_color:\n        image_colors = ImageColorGenerator(mask);\n        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n        plt.title(title, fontdict={'size': title_size,  \n                                  'verticalalignment': 'bottom'})\n    else:\n        plt.imshow(wordcloud);\n        plt.title(title, fontdict={'size': title_size, 'color': 'black', \n                                  'verticalalignment': 'bottom'})\n    plt.axis('off');\n    plt.tight_layout()  \n    \nplot_wordcloud(train.loc[train['sentiment'] == 'neutral', 'text'].append(test.loc[test['sentiment'] == 'neutral', 'text']), title=\"Word Cloud of Neutral tweets\",color = 'white')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plot_wordcloud(train.loc[train['sentiment'] == 'positive', 'text'].append(test.loc[test['sentiment'] == 'positive', 'text']), title=\"Word Cloud of Positive tweets\",color = 'green')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plot_wordcloud(train.loc[train['sentiment'] == 'negative', 'text'].append(test.loc[test['sentiment'] == 'negative', 'text']), title=\"Word Cloud of negative tweets\",color = 'red')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Text Data Preprocessing"},{"metadata":{},"cell_type":"markdown","source":"## 1.Ngram Analysis:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import defaultdict\ntrain0_df = train[train[\"sentiment\"]=='positive'].dropna().append(test[test[\"sentiment\"]=='positive'].dropna())\ntrain1_df = train[train[\"sentiment\"]=='neutral'].dropna().append(test[test[\"sentiment\"]=='neutral'].dropna())\ntrain2_df = train[train[\"sentiment\"]=='negative'].dropna().append(test[test[\"sentiment\"]=='neutral'].dropna())\n\n## custom function for ngram generation ##\ndef generate_ngrams(text, n_gram=1):\n    token = [token for token in text.lower().split(\" \") if token != \"\" if token not in STOPWORDS]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [\" \".join(ngram) for ngram in ngrams]\n\n## custom function for horizontal bar chart ##\ndef horizontal_bar_chart(df, color):\n    trace = go.Bar(\n        y=df[\"word\"].values[::-1],\n        x=df[\"wordcount\"].values[::-1],\n        showlegend=False,\n        orientation = 'h',\n        marker=dict(\n            color=color,\n        ),\n    )\n    return trace\n\n## Get the bar chart from sincere questions ##\nfreq_dict = defaultdict(int)\nfor sent in train0_df[\"text\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(25), 'red')\n\n## Get the bar chart from insincere questions ##\nfreq_dict = defaultdict(int)\nfor sent in train1_df[\"text\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(25), 'green')\n\n## Get the bar chart from sincere questions ##\nfreq_dict = defaultdict(int)\nfor sent in train2_df[\"text\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace2 = horizontal_bar_chart(fd_sorted.head(25), 'orange')\n\n# Creating two subplots\nfig = tools.make_subplots(rows=3, cols=1, vertical_spacing=0.04,\n                          subplot_titles=[\"Frequent words of positive tweets\", \"Frequent words of neutral tweets\",\n                                          \"Frequent words of negative tweets\"])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 2, 1)\nfig.append_trace(trace2, 3, 1)\nfig['layout'].update(height=1200, width=900, paper_bgcolor='rgb(233,233,233)', title=\"Word Count Plots\")\niplot(fig, filename='word-plots')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.Bi-gram Plots:"},{"metadata":{"trusted":true},"cell_type":"code","source":"freq_dict = defaultdict(int)\nfor sent in train0_df[\"text\"]:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(25), 'gray')\n\n\nfreq_dict = defaultdict(int)\nfor sent in train1_df[\"text\"]:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(25), 'orange')\n\nfreq_dict = defaultdict(int)\nfor sent in train2_df[\"text\"]:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace2 = horizontal_bar_chart(fd_sorted.head(25), 'brown')\n\n\n\n# Creating two subplots\nfig = tools.make_subplots(rows=3, cols=1, vertical_spacing=0.04,horizontal_spacing=0.25,\n                          subplot_titles=[\"Bigram plots of Positive tweets\", \n                                          \"Bigram plots of Neutral tweets\",\n                                          \"Bigram plots of Negative tweets\"\n                                          ])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 2, 1)\nfig.append_trace(trace2, 3, 1)\n\n\nfig['layout'].update(height=1000, width=800, paper_bgcolor='rgb(233,233,233)', title=\"Bigram Plots\")\niplot(fig, filename='word-plots')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.Tri-gram Plots:"},{"metadata":{"trusted":true},"cell_type":"code","source":"for sent in train0_df[\"text\"]:\n    for word in generate_ngrams(sent,3):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(25), 'blue')\n\n\nfreq_dict = defaultdict(int)\nfor sent in train1_df[\"text\"]:\n    for word in generate_ngrams(sent,3):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(25), 'green')\n\nfreq_dict = defaultdict(int)\nfor sent in train2_df[\"text\"]:\n    for word in generate_ngrams(sent,3):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace2 = horizontal_bar_chart(fd_sorted.head(25), 'violet')\n\n\n\n\n# Creating two subplots\nfig = tools.make_subplots(rows=3, cols=1, vertical_spacing=0.04, horizontal_spacing=0.05,\n                          subplot_titles=[\"Tri-gram plots of Positive tweets\", \n                                          \"Tri-gram plots of Neutral tweets\",\n                                          \"Tri-gram plots of Negative tweets\"])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 2, 1)\nfig.append_trace(trace2, 3, 1)\nfig['layout'].update(height=1200, width=1200, paper_bgcolor='rgb(233,233,233)', title=\"Trigram Count Plots\")\niplot(fig, filename='word-plots')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From above Ngaram analysis we can observe that neutral tweets and negative tweets had more amount of repeteted words than positive tweets."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"train[\"num_words\"] = train[\"text\"].apply(lambda x: len(str(x).split()))\ntest[\"num_words\"] = test[\"text\"].apply(lambda x: len(str(x).split()))\ntrain['select_num_words'] = train[\"selected_text\"].apply(lambda x: len(str(x).split()))\n\n## Number of unique words in the text ##\ntrain[\"num_unique_words\"] = train[\"text\"].apply(lambda x: len(set(str(x).split())))\ntest[\"num_unique_words\"] = test[\"text\"].apply(lambda x: len(set(str(x).split())))\ntrain['select_num_unique_words'] = train[\"selected_text\"].apply(lambda x: len(set(str(x).split())))\n\n## Number of characters in the text ##\ntrain[\"num_chars\"] = train[\"text\"].apply(lambda x: len(str(x)))\ntest[\"num_chars\"] = test[\"text\"].apply(lambda x: len(str(x)))\ntrain['select_num_chars'] = train[\"selected_text\"].apply(lambda x: len(str(x)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.Histogram plot of Number of words"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure()\nfig.add_trace(go.Histogram(x=train['num_words'],name = 'Number of words in text of train data'))\nfig.add_trace(go.Histogram(x=test['num_words'],name = 'Number of words in text of test data'))\nfig.add_trace(go.Histogram(x=train['select_num_words'],name = 'Number of words in selected text'))\n\n# Overlay both histograms\nfig.update_layout(barmode='stack')\n# Reduce opacity to see both histograms\nfig.update_traces(opacity=0.75)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can observe from above histogram plot that the number of words in train text and test text ranges from 1 to 30.Selected text words mostly fall in range of 1-10. "},{"metadata":{},"cell_type":"markdown","source":"# Histogram plots of Number of characters"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig_ = go.Figure()\nfig_.add_trace(go.Histogram(x=train['num_chars'],name = 'Number of characters in text of train data',marker = dict(color = 'rgba(222, 111, 33, 0.8)')))\nfig_.add_trace(go.Histogram(x=test['num_chars'],name = 'Number of characters in text of test data',marker = dict(color = 'rgba(33, 1, 222, 0.8)')))\nfig_.add_trace(go.Histogram(x=train['select_num_chars'],name = 'Number of characters in selected text',marker = dict(color = 'rgba(108, 25, 7, 0.8)')))\n\n# Overlay both histograms\nfig_.update_layout(barmode='stack')\n# Reduce opacity to see both histograms\nfig_.update_traces(opacity=0.75)\nfig_.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From above plot we can see that number of characters in test and train set was in same range.In selected text the range flows from 3 to 138 Characters."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig_ = go.Figure()\nfig_.add_trace(go.Histogram(x=train['num_unique_words'],name = 'Number of unique words in text of train data',marker = dict(color = 'rgba(222, 1, 3, 0.8)')))\nfig_.add_trace(go.Histogram(x=test['num_unique_words'],name = 'Number of unique words in text of test data',marker = dict(color = 'rgba(3, 221, 2, 0.8)')))\nfig_.add_trace(go.Histogram(x=train['select_num_unique_words'],name = 'Number of unique words in selected text',marker = dict(color = 'rgba(1, 2, 237, 0.8)')))\n\n# Overlay both histograms\nfig_.update_layout(barmode='stack')\n# Reduce opacity to see both histograms\nfig_.update_traces(opacity=0.75)\nfig_.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that number of unique words in train and test sets range from 1 to 26. In selected text most number  "},{"metadata":{},"cell_type":"markdown","source":"**NOW BASIC EDA, TEXT PREPROCESSING IS COMPLETED **"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Pos-Neg-Neutral Classification"},{"metadata":{"trusted":true},"cell_type":"code","source":"number_of_class_labels=len(train['sentiment'].unique())\nnumber_of_class_labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Count_Row=train.shape[0] \nCount_Col=train.shape[1] \nprint(Count_Col)\nprint(Count_Row)\nprint(train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_prob_df = pd.DataFrame(columns=['sentiment', 'probability'], index=range(number_of_class_labels))\nclass_prob_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"i=0\nfor val, cnt in train['sentiment'].value_counts().iteritems():\n    print ('value', val, 'was found', cnt, 'times')\n    class_prob_df.loc[i].sentiment = val\n    class_prob_df.loc[i].probability = cnt/Count_Row\n    i = i +1\n    \nclass_prob_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nnltk.download('punkt')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['selected_text'].dtype","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['selected_text']=train['selected_text'].apply(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nfrom nltk import sent_tokenize\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\n\nall_tokens = []\n\nfor idx, row in train.iterrows():\n    for word in word_tokenize(row.selected_text):\n        all_tokens.append(word)\n    \nprint(len(all_tokens), all_tokens)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_tokens_unique = set(all_tokens)\nprint(len(all_tokens_unique), all_tokens_unique)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stop_words = set(stopwords.words('english'))\n\ntokens = [w for w in all_tokens_unique if not w in stop_words]\nprint(len(tokens), tokens)\n\ntokens1=[]\ntokens = [word for word in tokens if word.isalpha()]\nprint(len(tokens), tokens)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word = ['@', 'rr', '!', '$', '@', 'jfjf', '&','(', ')', ',']\nfor word in word:\n    if word.isalpha():\n        print(\"yes it is alpha: \", word)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"merged_train_df = train.groupby('sentiment')['selected_text'].apply(' '.join).reset_index()\n\nmerged_train_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for idx, row in merged_train_df.iterrows():\n    \n    temp1_tokens = []\n    for word in word_tokenize(row.selected_text):\n        temp1_tokens.append(word)\n    \n    temp1_tokens = set(temp1_tokens)\n         \n    temp2_tokens = []\n    for word in temp1_tokens:\n        if not word in stop_words:\n            temp2_tokens.append(word)           \n    \n    temp3_tokens = []\n    for word in temp2_tokens:\n        if word.isalpha():\n            temp3_tokens.append(word)\n            \n    print(temp3_tokens)\n    temp4_tokens = \" \".join(temp3_tokens)\n    print(temp4_tokens)\n    \n    merged_train_df.at[idx, 'selected_text'] = temp4_tokens\n    merged_train_df.at[idx, 'no_of_words_in_category'] = len(temp3_tokens)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"merged_train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"merged_train_df = pd.merge(merged_train_df, class_prob_df[['sentiment', 'probability']], on='sentiment')\nmerged_train_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_df = pd.DataFrame()\n\nrow_counter = 0\n\nfor idx, row in merged_train_df.iterrows():\n    for token in tokens:\n        # find the number of occurances of the token in the current category of documents\n        no_of_occurances = row.selected_text.count(token)\n        no_of_words_in_category = row.no_of_words_in_category\n        no_unique_words_all = len(tokens)\n        \n        prob_of_token = (no_of_occurances+ 1)/ (no_of_words_in_category+ no_unique_words_all)\n        #print(row.class_label, token, no_of_occurances, prob_of_token)\n        final_df.at[row_counter, 'Result'] = row.sentiment\n        final_df.at[row_counter, 'token'] = token\n        final_df.at[row_counter, 'no_of_occurances'] = no_of_occurances\n        final_df.at[row_counter, 'no_of_words_in_category'] = no_of_words_in_category\n        final_df.at[row_counter, 'no_unique_words_all'] = no_unique_words_all\n        final_df.at[row_counter, 'prob_of_token_category'] = prob_of_token\n        \n        row_counter = row_counter + 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We got probability of each and every word. By using this probabiity we can determine whether the word is positive, negative or neutral"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate P(Category/Document) \n#      = P(Category) * P(Word1/Category) * P(Word2/Category) * P(Word3/Category)\n\n# P(Auto/D6) = P(Auto) * P(Engine/Auto) * P(Noises/Auto) * P(Car/Auto)\nfor idx, row in test.iterrows():\n    \n    # tokenize & unique words\n    temp1_tokens = []\n    for word in word_tokenize(row.sentiment):\n        temp1_tokens.append(word)\n        #temp1_tokens = set(temp1_tokens)\n        \n    # remove stop words\n    temp2_tokens = []\n    for word in temp1_tokens:\n        if not word in stop_words:\n            temp2_tokens.append(word)\n          \n    # remove punctuations\n    temp3_tokens = []\n    for word in temp2_tokens:\n        if word.isalpha():\n            temp3_tokens.append(word)\n            \n    #temp4_tokens = \" \".join(temp3_tokens)\n    #print(temp4_tokens)\n    \n    prob = 1 \n    \n    # process for each class_label\n    for idx1, row1 in merged_train_df.iterrows():\n        print(\"class: \"+ row1.sentiment)\n        for token in temp3_tokens:\n            # find the token in final_df for the given category, get the probability\n            # row1.class_label & token\n        \n            print(\"      : \"+ token)  \n        \n            temp_df = final_df[(final_df['Result'] == row1.sentiment) & (final_df['token'] == token)]\n\n            # process for exception\n            if (temp_df.shape[0] == 0):\n                token_prob = 1/(row1.no_of_words_in_category+ no_unique_words_all)\n                print(\"       no token found prob :\", token_prob)\n                prob = prob * token_prob\n            else:\n                token_prob = temp_df.get_value(temp_df.index[0],'prob_of_token_category')\n                print(\"       token prob          :\", token_prob)\n                prob = prob * token_prob\n\n            prob = prob * row1.probability\n\n        col_at = 'prob_'+row1.sentiment\n\n        test.at[idx, col_at] = prob\n\n\ntest","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" WE HAVE CALCULATED PROBABILITIES OF BECOMING POSITIVE , NEGATIVE and NEUTRAL FOR EACH AND EVERY SENTENCE"},{"metadata":{},"cell_type":"markdown","source":"**BASED ON THIS PROBABILITY WE CAN CLASSIFY WETHER THE SNTENCE IS NEGATIVE, NEUTRAL or POSITIVE**"},{"metadata":{},"cell_type":"markdown","source":"PLEASE **UPVOTE **IF YOU LIKED IT OR USEFUL AND KEEP ME MOTIVATED \n"},{"metadata":{},"cell_type":"markdown","source":"**THANK YOU IN ADVANCE AND ALL THE BEST**"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":4}