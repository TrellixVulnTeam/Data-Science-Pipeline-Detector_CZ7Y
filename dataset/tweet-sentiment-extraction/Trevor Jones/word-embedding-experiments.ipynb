{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd \nimport numpy as np\nimport re\nimport string\nfrom collections import defaultdict\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# open and read the GloVe twitter dataset into a word2vec dictionary.\n# if not on kaggle, the dataset can be acquired from \n# http://nlp.stanford.edu/data/glove.twitter.27B.zip\n#\n# we found we had the best results when using 50d vectors, as going \n# higher didn't increase accuracy but had a large performance cost\n\nwith open(\"/kaggle/input/glove-global-vectors-for-word-representation/glove.twitter.27B.50d.txt\", \"rb\") as lines:\n    w2v = {line.split()[0].decode(\"utf-8\"): np.array([float(value) for value in line.split()[1:]])\n           for line in lines}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# read in the csv data\ntrain = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv')\ntest = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv')\nsample = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop the null entries\ntrain[train['text'].isna()]\ntrain.drop(314, inplace = True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# this function replaces all sequential occurances of a symbol with a single word\ndef replace_symbol_word(text, symbol, word):\n    starIdx = text.find(symbol)\n    count = 0\n    while starIdx > -1 and count < 20:\n        firstIdx = starIdx\n        while(starIdx+1 < len(text) and text[starIdx+1] == symbol):\n            starIdx += 1\n        text = text[:firstIdx] + \" \" + word + \" \" + text[starIdx+1:]\n        starIdx = -1\n        starIdx = text.find(symbol)\n        count += 1\n    \n    return text\n\n# cleans the text by removing urls, numbers, punctuation, and changing any sequence of 3 or more of the same letter to just 2\ndef clean_text(text):    \n    text = re.sub(r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))', '', text)\n    \n    # remove the characters [\\], ['], [`], and [\"]\n    text = re.sub(r\"\\\\\", \"\", text)\n    text = re.sub(r\"\\'\", \"\", text)\n    text = re.sub(r\"\\`\", \"\", text)\n    text = re.sub(r\"\\\"\", \"\", text)\n    \n    # remove numbers\n    text = re.sub(r\"[0-9]+\", \"\", text)\n    \n    # convert text to lowercase\n    text = text.strip().lower()\n    \n    # we attempted to replace symbols with words, but it made performance worse.\n    # we tried a few different words, but couldn't find one that worked well with word embeddings\n#     text = replace_symbol_word(text, '*', 'abusive')\n#     text = replace_symbol_word(text, '!', 'exclaim')\n    \n    # replace 3 or more of the same letter with just 2\n    # no word in the english dictionary has 3 of the same letter in a row,\n    # they all use a hyphen in between.\n    # for example turns cooooool into cool and yummmmy into yummy\n    # doesn't work great for examples like looooool -> lool, but at least \n    # looool with any number of zeros always ends up as the same word lool\n    # which helps when doing anything with word counts\n    text = re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n    \n    # replace punctuation characters with spaces\n    filters = '!\"\\'#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n    translate_dict = dict((c, \" \") for c in filters)\n    translate_map = str.maketrans(translate_dict)\n    text = text.translate(translate_map)\n    text = ' '.join(text.split())\n    \n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# clean the text and selected_text and then write them to their own columns\ntrain['clean_selected_text'] = train['selected_text'].apply(clean_text)\ntrain['clean_text'] = train['text'].apply(clean_text)\n\n# X_train = train.copy()\n\n# split the dataset for validation purposes\n# make sure to train on the full dataset for submission\nX_train, X_val = train_test_split(\n    train, train_size = 0.80, random_state = 0)\nX_train = X_train.copy()\nX_val = X_val.copy()\n\n\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# splits up the training data based on sentiment\npos_train = X_train[X_train['sentiment'] == 'positive']\nneutral_train = X_train[X_train['sentiment'] == 'neutral']\nneg_train = X_train[X_train['sentiment'] == 'negative']\n\n# count vectorizer to get word counts\nn = 1\ncv = CountVectorizer(ngram_range=(n, n), max_df=0.8, min_df=2,\n                                         max_features=None,\n                                         stop_words='english')\n\n# vectorize the cleaned selected text for all the data, and for each sentiment\nX_train_cv = cv.fit_transform(X_train['clean_selected_text'])\n\nX_pos = cv.transform(pos_train['clean_selected_text'])\nX_neutral = cv.transform(neutral_train['clean_selected_text'])\nX_neg = cv.transform(neg_train['clean_selected_text'])\n\n# create a dataframe where the columns are all of the words minus stopwords,\n# and each row is a tweet in count vectorized form, \n# where each value is the number of times that columns words appears in the tweet\npos_count_df = pd.DataFrame(X_pos.toarray(), columns=cv.get_feature_names())\nneutral_count_df = pd.DataFrame(X_neutral.toarray(), columns=cv.get_feature_names())\nneg_count_df = pd.DataFrame(X_neg.toarray(), columns=cv.get_feature_names())\n\n# empty dictionaries that we will fill\n# these 3 contain the total counts of each word over each sentiment\npos_words = {}\nneut_words = {}\nneg_words = {}\n# these 3 contain the proportion of tweets in sentiment which contain the word\npos_words_proportion = {}\nneutral_words_proportion = {}\nneg_words_proportion = {}\n\nfor k in cv.get_feature_names():\n    # gets raw word count of each word for each sentiment\n    pos_words[k] = pos_count_df[k].sum()\n    neut_words[k] = neutral_count_df[k].sum()\n    neg_words[k] = neg_count_df[k].sum()\n    \n    # divide word counts by number of samples to get proportion\n    pos_words_proportion[k] = pos_words[k]/pos_train.shape[0]\n    neutral_words_proportion[k] = neut_words[k]/neutral_train.shape[0]\n    neg_words_proportion[k] = neg_words[k]/neg_train.shape[0]\n    \nneg_words_adj = {}\npos_words_adj = {}\nneutral_words_adj = {}\n\n# adjust the proportion value to take into account the fact that words will show up in tweets of other sentiments\nfor key, value in neg_words_proportion.items():\n    neg_words_adj[key] = neg_words_proportion[key] - (neutral_words_proportion[key] + pos_words_proportion[key])\n\nfor key, value in pos_words_proportion.items():\n    pos_words_adj[key] = pos_words_proportion[key] - (neutral_words_proportion[key] + neg_words_proportion[key])\n\nfor key, value in neutral_words_proportion.items():\n    neutral_words_adj[key] = neutral_words_proportion[key] - (neg_words_proportion[key] + pos_words_proportion[key])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MeanEmbeddingVectorizer(object):\n    def __init__(self, word2vec, pos_words, neg_words, neut_words):\n        self.pos_words = pos_words\n        self.neg_words = neg_words\n        self.neut_words = neut_words\n        self.word2vec = word2vec\n        # if a text is empty we should return a vector of zeros\n        # with the same dimensionality as all the other vectors\n        self.dim = len(next(iter(word2vec.items()))[1])\n\n    # Here X is the clean_selected_text ground truth, and y is the sentiment.\n    def fit(self, X, y):\n        ratio = 0.8\n        self.average_positive = self.get_average_vector(X[y == 'positive'], 'positive', ratio)\n        self.average_neutral = self.get_average_vector(X[y == 'neutral'], 'neutral', ratio)\n        self.average_negative = self.get_average_vector(X[y == 'negative'], 'negative', ratio)\n        \n        # print the similarity between the average negative and positive tweet\n        print(np.dot(self.average_negative, self.average_positive)/(np.linalg.norm(self.average_negative)*np.linalg.norm(self.average_positive)))\n        return self\n    \n    # takes data from one sentiment, that sentiment, and the ratio to use for determing which words are often enough to use\n    # and then calculates the average vector for that sentiment from that data\n    # note the data that gets passed in is from clean_selected_text\n    def get_average_vector(self, X, sentiment, ratio):\n        # used to get the ratio of how many of that words appears in the given sentiment\n        numerator_dict = (self.pos_words if sentiment == 'positive' else self.neg_words if sentiment == 'negative' else self.neut_words)\n        denominator_dict = {k: self.pos_words[k] + self.neut_words[k] + self.neg_words[k] for k in self.neut_words.keys()}\n        # default dict to handle words we haven't seen and stop words that won't show up in this dict, but do in the clean text\n        word_proportion_dict = defaultdict(float)\n        for k in numerator_dict.keys():\n            word_proportion_dict[k] = numerator_dict[k]/denominator_dict[k]\n                \n        sent_vec_list = []\n        for sent in X:\n            sent_word_vecs = []\n            for w in sent.split(\" \"):\n                if w in self.word2vec and word_proportion_dict[w] > ratio:\n                    # if we have a vector for the word and its ratio is high enough to use for the vector, then add it\n                    sent_word_vecs.append(self.word2vec[w])\n            if(len(sent_word_vecs) > 0):\n                # once we have added all words, if we have at least 1, then get \n                # the average of that tweet and append it to our tweet list\n                sent_vec_list.append(np.mean(sent_word_vecs, axis=0))\n        \n        # return the average of all the tweets over axis 0, so we get one 50d vector that is the average\n        # of all the words that appear often in that sentiment's selected_text\n        #\n        # this means that words that appear often are included multiple times, and thus have more effect\n        # which is why we don't weight this with word counts.\n        return np.mean(np.array(sent_vec_list), axis=0)\n\n    # transforms one sentence to a vector with the mean of the words\n    # sent is a list of words, where each item is one word, this means no need to split here\n    def transform(self, sent, sentiment):\n        sent_vec_list = []\n        scalars = pos_words_adj if sentiment == 'positive' else neg_words_adj\n        # checking if its in pos_words allows us to strip all of the stop words that were removed as part of CountVectorizer\n        # as pos_words only contains words from the CountVectorizer, and contains all the same words as neg_words and neut_words\n        sent_word_vecs = [[x * scalars[w] for x in self.word2vec[w]]  for w in sent if (w in self.word2vec and w in pos_words.keys())]\n        if(len(sent_word_vecs) > 0):\n            # as long as we have at least 1 word, then average all the words and add it to our list\n            sent_vec_list.append(np.mean(sent_word_vecs, axis=0))\n        \n        # make sure we actually got a vector output, and if we did then return it, otherwise return a vector of zeros\n        if(len(sent_vec_list)):\n            return np.array(sent_vec_list)\n        return np.zeros(self.dim)\n    \n    # get the cosine similarity between the 3 average vectors and a given sentence\n    def get_sent_dist(self, sent, sentiment):\n        sent_vect = self.transform(sent, sentiment)\n                     \n        # the sum will be zero if we return an array of zeros from transform becaue we couldn't find any valid words\n        if sent_vect.sum() != 0.0:\n            # cosine similarity = dot(vec1, vec2) / (norm(vec1) * norm(vec2))\n            sim_pos = np.dot(sent_vect, self.average_positive)/(np.linalg.norm(sent_vect)*np.linalg.norm(self.average_positive))\n            sim_neut = np.dot(sent_vect, self.average_neutral)/(np.linalg.norm(sent_vect)*np.linalg.norm(self.average_neutral))\n            sim_neg = np.dot(sent_vect, self.average_negative)/(np.linalg.norm(sent_vect)*np.linalg.norm(self.average_negative))\n            return sim_pos[0], sim_neut[0], sim_neg[0]\n        # if we were unable to extract any words from the given sentence, then we say the similarity is 0\n        return 0, 0, 0\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# creates and computs the average vectors for each sentiment from the data\nmev = MeanEmbeddingVectorizer(w2v, pos_words, neg_words, neut_words)\nmev = mev.fit(X_train['clean_selected_text'], X_train['sentiment'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def calc_selected_text(df_row):\n    \n    words_in_tweet = df_row['text'].split()\n    sentiment = df_row['sentiment']\n    \n    # we just return the entire tweet if the sentiment is neutral or if there are less than 3 words\n    # almost every neutral tweet has selected text that is the same as the tweet\n    # and most short tweets ended up using all words. \n    # This second part mostly saves computation time, but does increase accuracy a very small amount \n    # as the jaccard score is about 0.77 on average when just returning the tweet for short tweets\n    if sentiment == 'neutral' or len(words_in_tweet) < 3:\n        return df_row['text']\n    \n    # we get all of the possible subsets and sort them by length\n    word_subsets = [words_in_tweet[i:j+1]\n                    for i in range(len(words_in_tweet)) for j in range(i, len(words_in_tweet))]\n\n    sorted_subsets = sorted(word_subsets, key=len)\n\n    max_val = -10000000;\n    final_subset = []\n\n    # for each subset, we get the cosine similarity between that subset and the average vector for that sentiment\n    # whichever one has the most similarity is the one that we return\n    for subset in sorted_subsets:\n        # clean the text, then split it on spaces to get an array\n        cleaned_text = clean_text(' '.join(subset)).split(\" \")\n        \n        # we get the cosine similarity between the subset and each average vector\n        pos, neut, neg = mev.get_sent_dist(cleaned_text, sentiment)\n#         print(pos, neut, neg)\n        # then depending on which sentiment that tweet was, we figure which subset has the highest similarity\n        val_to_check = pos if sentiment == 'positive' else neg\n        if val_to_check > max_val:\n            max_val = val_to_check\n            final_subset = subset\n\n    # then we just return the final_subset as a string\n    # note we return the un-cleaned text as thats what the problem requires\n    return \" \".join(final_subset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def calc_jaccard_df(data):\n    # create/reset the columns we are going to be working with\n    data['predicted_selection'] = ''\n    data['jaccard'] = 0.0\n    \n    # for each sample in our data, we calculate the selected text and set the predicted_selection in our dataframe\n    for index, row in data.iterrows():\n        selected_text = calc_selected_text(row)\n        data.loc[data['textID'] == row['textID'], ['predicted_selection']] = selected_text\n\n    # calculate the jaccard score over the entire dataframe based off of the ground truth and our prediction\n    data['jaccard'] = data.apply(lambda x: jaccard(x['selected_text'], x['predicted_selection']), axis = 1)\n    # average all of the jaccard scores to get the total score for the validation set\n    print('The jaccard score for the validation set is:', np.mean(data['jaccard']))\n    \ncalc_jaccard_df(X_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# loop through the test dataset and calculate a prediction for each sample, then write it back into the dataframe\nfor index, row in test.iterrows():\n    selected_text = calc_selected_text(row)\n    sample.loc[sample['textID'] == row['textID'], ['selected_text']] = selected_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# write the sample dataframe to a submissions file\nsample.to_csv('submission.csv', index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}