{"cells":[{"metadata":{},"cell_type":"markdown","source":"A short notebook where I show how to fine-tune the RoBERTa base model on the competition dataset using [transformers](https://huggingface.co/transformers/examples.html) CLI script. \nThis would have been better on the [SquaD](https://rajpurkar.github.io/SQuAD-explorer/) dataset but due to lack of time, didn't try it. Also, notice that one of the versions is running using TPU. \nEnjoy!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Â Not necessary but you can get some logs.\n!wandb login 22f6b417e27a2c1fd23e5d45d687926b3f9e3b85","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, I start by loading the train and test, concatenating them and saving so that the file is available in a write folder. \nThis is necssary for the CLI to work since accessing from the input folder alone fails (it is read-only). As I said, running on the SquaD dataset (or using the squad roberta base https://huggingface.co/deepset/roberta-base-squad2) would have been better I guess. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n\n\ntrain_df = pd.read_csv(\"../input/tweet-sentiment-extraction/train.csv\")\ntest_df = pd.read_csv(\"../input/tweet-sentiment-extraction/test.csv\")\n\ndf = pd.concat([test_df, train_df])\ndf.to_csv(\"train_test.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# If you want TPU.\n\"\"\"\n!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev\n!export XLA_USE_BF16=1\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So you need mainly need five things:\n    \n- where to store the fine-tuned model via the `output_dir` flag\n- The model type via the `model_type` flag: here `roberta`\n- The model base via the `model_name_or_path` flag: here `roberta-base` \n- The nature of the task via `--mlm` since it is masked language model here.\n- The train input via `train_data_file` flag: here `train_test.csv`","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"%%time\n!python ../input/pytorchtransformers/examples/language-modeling/run_language_modeling.py \\\n--output_dir=fine_tuned_roberta_update                                                     \\\n--model_type=roberta                                                                 \\\n--model_name_or_path=roberta-base                                                     \\\n--do_train                                                                             \\\n--train_data_file=train_test.csv                                                             \\\n--mlm \\\n--num_train_epochs 5","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}