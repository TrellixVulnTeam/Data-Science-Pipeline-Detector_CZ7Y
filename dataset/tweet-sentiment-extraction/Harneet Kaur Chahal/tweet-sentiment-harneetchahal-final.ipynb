{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#importing the required libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Dense, Input, GlobalAveragePooling1D, Dropout, SpatialDropout1D\n#RNN\nfrom tensorflow.keras.layers import LSTM, Embedding\nfrom tensorflow.keras.models import Model\n#optimization\nfrom tensorflow.keras.optimizers import Adam\n#to calculate scores\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Some data analysis\ntrain=pd.read_csv(\"/kaggle/input/tweet-sentiment-extraction/train.csv\")\ntest=pd.read_csv(\"/kaggle/input/tweet-sentiment-extraction/test.csv\")\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objs as go\nfig=make_subplots(1,2,subplot_titles=('Train set','Test set'))\nx=train.sentiment.value_counts()\nfig.add_trace(go.Bar(x=x.index,y=x.values,marker_color=['red','blue','green'],name='train'),row=1,col=1)\nx=test.sentiment.value_counts()\nfig.add_trace(go.Bar(x=x.index,y=x.values,marker_color=['red','blue','green'],name='test'),row=1,col=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.concat([train,test])\ndf['text']=df['text'].astype(str)\nsent=df.sentiment.unique()\ncolors=['blue','green','red']\nfrom wordcloud import WordCloud, STOPWORDS\nstopwords = set(STOPWORDS)\n\ndef show_wordcloud(data, title = None,ax=None):\n    wordcloud = WordCloud(\n        background_color='white',\n        stopwords=stopwords,\n        max_words=100,\n        max_font_size=30, \n        scale=3,\n        random_state=1 \n        )\n    \n    wordcloud=wordcloud.generate(str(data))\n    ax.imshow(wordcloud,interpolation='nearest')\n    ax.axis('off')\n\nfig,ax=plt.subplots(1,3,figsize=(20,12))\nfor i in range(3):\n    new=df[df['sentiment']==sent[i]]['text']\n    show_wordcloud(new,ax=ax[i])\n    ax[i].set_title(sent[i],color=colors[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nimport string\nfrom nltk.corpus import stopwords\nimport nltk\nfrom nltk.corpus import wordnet\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize.treebank import TreebankWordDetokenizer\nfrom collections import Counter\nfrom wordcloud import WordCloud\nfrom nltk.corpus import stopwords\nimport nltk\nfrom gensim.utils import simple_preprocess\nfrom nltk.corpus import stopwords\nimport gensim\nfrom sklearn.model_selection import train_test_split\nimport spacy\nimport pickle\nimport warnings\nwarnings.filterwarnings('ignore')\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt \nimport tensorflow as tf\nimport keras","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#dataset length\nprint(len(train))\n#sentiments\ntrain['sentiment'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Only those columns are kept which are going to be useful\ntrain = train[['selected_text','sentiment']]\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Filling the null values if there are any\ntrain[\"selected_text\"].fillna(\"No content\", inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#DATA cleaning\n'''Remove URLs from the tweets\nTokenize text\nRemove emails\nRemove new lines characters\nRemove distracting single quotes\nRemove all punctuation signs\nLowercase all text\nDetokenize text\nConvert list of texts to Numpy array'''\ndef depure_data(data):\n\n    #URL/Link removal\n    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n    data = url_pattern.sub(r'', data)\n\n    # Emailremoval\n    data = re.sub('\\S*@\\S*\\s?', '', data)\n\n    # Remove new line characters\n    data = re.sub('\\s+', ' ', data)\n\n    # Remove distracting single quotes\n    data = re.sub(\"\\'\", \"\", data)\n        \n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = []\ndata_to_list = train['selected_text'].values.tolist()\nfor i in range(len(data_to_list)):\n    temp.append(depure_data(data_to_list[i]))\nlist(temp[:5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# deacc=True removes punctuations\ndef sent_to_words(sentences):\n    for sentence in sentences:\n        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  \n        \n\ndata_words = list(sent_to_words(temp))\n\nprint(data_words[:10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def detokenize(text):\n    return TreebankWordDetokenizer().detokenize(text)\n\ndata = []\nfor i in range(len(data_words)):\n    data.append(detokenize(data_words[i]))\nprint(data[:5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = np.array(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dataset is categorical\n#convert the sentiment labels to a float type\n#implementing to_categorical method from Keras.\nlabels = np.array(train['sentiment'])\ny = []\nfor i in range(len(labels)):\n    if labels[i] == 'neutral':\n        y.append(0)\n    if labels[i] == 'negative':\n        y.append(1)\n    if labels[i] == 'positive':\n        y.append(2)\ny = np.array(y)\nlabels = tf.keras.utils.to_categorical(y, 3, dtype=\"float32\")\ndel y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(labels))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#implementing the Keras tokenizer \n#transform text data into 3D float data\nfrom keras.models import Sequential\nfrom keras import layers\nfrom keras.optimizers import RMSprop,Adam\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras import regularizers\nfrom keras import backend as K\nfrom keras.callbacks import ModelCheckpoint\nmax_words = 5000\nmax_len = 200\n\ntokenizer = Tokenizer(num_words=max_words)\ntokenizer.fit_on_texts(data)\nsequences = tokenizer.texts_to_sequences(data)\ntweets = pad_sequences(sequences, maxlen=max_len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(tweets)\nprint(labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Splitting the data\nX_train, X_test, y_train, y_test = train_test_split(tweets,labels, random_state=0)\nprint (len(X_train),len(X_test),len(y_train),len(y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#LSTM model\nmodel1 = Sequential()\nmodel1.add(layers.Embedding(max_words, 20))\nmodel1.add(layers.LSTM(15,dropout=0.5))\nmodel1.add(layers.Dense(3,activation='softmax'))\n\n\nmodel1.compile(optimizer='rmsprop',loss='categorical_crossentropy',\n               metrics=['accuracy'])\n#Implementing model checkpoins to save the best metric \n#and do not lose it on training.\ncheckpoint1 = ModelCheckpoint(\"best_model1.hdf5\", \n                              monitor='val_accuracy', verbose=1,\n                              save_best_only=True, mode='auto', period=1,\n                              save_weights_only=False)\nhistory = model1.fit(X_train, y_train, epochs=70,\n                     validation_data=(X_test, y_test),\n                     callbacks=[checkpoint1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's load the best model obtained during training\nbest_model = keras.models.load_model(\"best_model1.hdf5\")\n\ntest_loss, test_acc = best_model.evaluate(X_test, y_test, verbose=2)\nprint('Model accuracy: ',test_acc)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}