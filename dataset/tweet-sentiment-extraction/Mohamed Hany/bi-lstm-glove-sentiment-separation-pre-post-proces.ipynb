{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import sys, os, re, csv, codecs, numpy as np, pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom tensorflow.keras import *\nfrom tensorflow.keras.layers import *\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\nfrom keras.layers import Bidirectional, GlobalMaxPool1D\nfrom keras.models import Model\nfrom keras import initializers, regularizers, constraints, optimizers, layers,losses\nfrom keras.utils import to_categorical\nimport keras.backend as kb\nfrom nltk.corpus import words\nimport random as rand\nimport spacy\nimport re\nfrom tqdm import tqdm\nimport scipy.stats as ss\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/tweet-sentiment-extraction/train.csv')\ntest = pd.read_csv('../input/tweet-sentiment-extraction/test.csv')\n\n#test data frame doens't have a selected text column, so it's created to be later filled\ntest[\"selected_text\"]=\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.sample(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# We will do some EDA to get further insight about our data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"ratio of each sentiment relative to total number of tweets:\")\ntrain_relative_count=train.sentiment.value_counts(normalize=True)\ntest_relative_count=test.sentiment.value_counts(normalize=True)\nprint(\"train data:\\n\",train_relative_count)\nprint(\"test data:\\n\",test_relative_count)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_jaccard(sentiment_data):\n    jaccard_list=[]\n    for i in sentiment_data.index:\n        text_set=set(sentiment_data.text[i].split())\n        selected_set=set(sentiment_data.selected_text[i].split())\n        intersection=text_set.intersection(selected_set)\n        jaccard= float(len(intersection)/(len(text_set)+len(selected_set)-len(intersection)))\n        jaccard_list.append(jaccard)\n    \n    average_jaccard=sum(jaccard_list)/len(jaccard_list)\n    return average_jaccard","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sentiment_split(data):\n    \n    data_copy=data.copy()\n    \n    neutral_tweets=data_copy[data_copy[\"sentiment\"]==\"neutral\"]\n    neutral_tweets=neutral_tweets.copy()\n    \n    positive_tweets=data_copy[data_copy.sentiment==\"positive\"]\n    positive_tweets=positive_tweets.copy()\n    \n    negative_tweets=data_copy[data_copy.sentiment==\"negative\"]\n    negative_tweets=negative_tweets.copy()\n\n    return neutral_tweets,positive_tweets,negative_tweets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_copy=train.copy()\ntrain_copy.dropna(axis=0, how=\"any\", inplace=True)\ntrain_copy.reset_index(inplace=True)\nprint(train_copy.sentiment.value_counts(normalize=True))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# We will check if there are rows in which selected text is not part of the text and try to fix it (we will correct misspelled words in selected text)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def split_word(word): \n    return [char for char in word] \n\ndef get_jaccard_word(word1,word2):\n    word1_char=split_word(word1)\n    word2_char=split_word(word2)\n    set1=set(word1_char)\n    set2=set(word2_char)\n    intersection=set1.intersection(set2)\n    jaccard_score=len(intersection)/(len(set1)+len(set2)-len(intersection))\n    return jaccard_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_index_word_in_string(word,text_split):\n    word_index=[]\n    for i in range(len(text_split)):\n        split_word=re.sub(\" \",\"\",text_split[i])\n        if word==split_word:\n            word_index.append(i)\n    if len(word_index)==0:\n        word_index=-1\n    return word_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"selected_text_mismatch=0\nmismatched_rows=[]\nfor i in tqdm(train_copy.index):\n    mismatch=0\n    text_split=train_copy.text[i].split()\n    selected_split=train_copy.selected_text[i].split()    \n    correct_selected_text=[]\n    for word in selected_split:  \n        \n        word_match=get_index_word_in_string(word,text_split)\n#        match=0\n#        for j in range(len(text_split)):\n#            if word==text_split[j]:\n#                match=1\n#                break\n                \n        if word_match!=-1:\n            correct_selected_text.append(word)\n            continue\n        if word_match==-1:\n            mismatch=1\n            jaccard_scores=np.zeros(len(text_split))\n            for k in range(len(text_split)):\n                jaccard_scores[k]=get_jaccard_word(word,text_split[k])\n                \n            correct_word_index=np.argmax(jaccard_scores)\n            correct_word=text_split[correct_word_index]\n            correct_selected_text.append(correct_word)\n            \n    train_copy.selected_text[i]=\" \".join(correct_selected_text)\n    if mismatch==1:\n        selected_text_mismatch=selected_text_mismatch+1\n        mismatched_rows.append(i)\n\nprint(\"number of rows in which mismatch between text and selected text was found: \",selected_text_mismatch)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"neutral_tweets,positive_tweets,negative_tweets=sentiment_split(train_copy)\n\nprint(\"jaccard score between text and selected text:\")\n#sentiment jaccard score\nneutral_jaccard=get_jaccard(neutral_tweets)\nprint(\"neutral sentiments jaccard score: \",neutral_jaccard)\npositive_jaccard=get_jaccard(positive_tweets)\nprint(\"positive sentiments jaccard score: \",positive_jaccard)\nnegative_jaccard=get_jaccard(negative_tweets)\nprint(\"negative sentiments jaccard score: \",negative_jaccard)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# neutral tweets have a jaccard score of approximately 1, so they don't really need a model to train. We could just use the text as the selected text","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#split test data\nneutral_tweets_test,positive_tweets_test,negative_tweets_test=sentiment_split(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# we will remove any special characters in our data to be able to better train it.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_special_characters(str1):\n    '''\n    Takes a string, removes(substituted by \"\") all special characters and URLS and returns it.\n    '''\n    url_pattern = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n    # Remove URLs with re.sub(pattern, replacement, string)\n    cache = re.sub(url_pattern, ' ', str(str1))  \n    # Removing spec. characters\n    \n    character_pattern = '\\||\\,|\\;|\\.|\\:|\\#|\\*|\\~|\\+|\\-|\\/|\\_|\\?|\\!|\\\"|\\'|\\`|\\(|\\)|\\=|\\&|\\%|\\$|\\ยง' \n    split_text=str(cache).split()\n    filtered_text=[]\n    for i in range(len(split_text)):\n        contains_alphabet=re.search('[a-zA-Z]', split_text[i])\n        # doing this step keeps separate symbols\n        if contains_alphabet is not None:\n            filtered_text.append(str(re.sub(character_pattern, ' ', split_text[i])))\n        else:\n            filtered_text.append(split_text[i])\n    \n    filtered_text=\" \".join(filtered_text)\n    \n    #doing this step removes spaces from words\n    output_text=filtered_text.split()\n    for i in range(len(output_text)):\n        output_text[i]=re.sub(\" \",\"\",output_text[i])\n        \n    output=\" \".join(output_text)\n#    return re.sub(character_pattern, ' ', str(cache))\n    return str(output)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def check_empty_string(data):\n    text_counter=0\n    selected_text_counter=0\n    for i in data.index:\n        if \"clean_text\" in data:\n            if len(data.clean_text[i].split())==0:\n                text_counter=text_counter+1\n        if \"clean_selected\" in data:\n            if len(data.clean_selected[i].split())==0:\n                selected_text_counter=selected_text_counter+1   \n                \n    print(\"null clean text: \", text_counter)\n    if \"clean_selected\" in data:\n        print(\"null selected clean text: \", selected_text_counter)\n    return","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"positive_tweets['clean_text'] = positive_tweets['text'].str.lower().apply(remove_special_characters)\npositive_tweets['clean_selected'] = positive_tweets['selected_text'].str.lower().apply(remove_special_characters)\nprint(\"positive tweets:\")\ncheck_empty_string(positive_tweets)\n\nnegative_tweets['clean_text'] = negative_tweets['text'].str.lower().apply(remove_special_characters)\nnegative_tweets['clean_selected'] = negative_tweets['selected_text'].str.lower().apply(remove_special_characters)\nprint(\"negative tweets:\")\ncheck_empty_string(negative_tweets)\n\npositive_tweets_test['clean_text'] = positive_tweets_test['text'].str.lower().apply(remove_special_characters)\nprint(\"test positive tweets:\")\ncheck_empty_string(positive_tweets_test)\n\nnegative_tweets_test['clean_text'] = negative_tweets_test['text'].str.lower().apply(remove_special_characters)\nprint(\"test negative tweets:\")\ncheck_empty_string(negative_tweets_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# In this next step, we plot histograms for the tweets in order to decide the value of the length of text input to the neural network","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"totalNumWords = [len(positive_tweets['clean_text'][index].split()) for index in positive_tweets.index]\nplt.hist(totalNumWords,bins = np.arange(0,35,1))\nplt.title('positive tweets')\nplt.xlabel('length of tweet')\nplt.ylabel('number of tweets')\nplt.show()\n\ntotalNumWords = [len(negative_tweets['clean_text'][index].split()) for index in negative_tweets.index]\nplt.hist(totalNumWords,bins = np.arange(0,35,1))\nplt.title('negative tweets')\nplt.xlabel('length of tweet')\nplt.ylabel('number of tweets')\nplt.show()\n\ntotalNumWords = [len(positive_tweets_test['clean_text'][index].split()) for index in positive_tweets_test.index]\nplt.hist(totalNumWords,bins = np.arange(0,35,1))\nplt.title('test positive tweets')\nplt.xlabel('length of tweet')\nplt.ylabel('number of tweets')\nplt.show()\n\ntotalNumWords = [len(negative_tweets_test['clean_text'][index].split()) for index in negative_tweets_test.index]\nplt.hist(totalNumWords,bins = np.arange(0,35,1))\nplt.title('test negative tweets')\nplt.xlabel('length of tweet')\nplt.ylabel('number of tweets')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# instead of using a dictionary for nltk or spacy, we used the text from the training set and test set to make the dictionary","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"t1=pd.DataFrame({'A': positive_tweets.clean_text})\nt2=pd.DataFrame({'A': positive_tweets.clean_selected})\n\nt3=pd.DataFrame({'A': negative_tweets.clean_text})\nt4=pd.DataFrame({'A': negative_tweets.clean_selected})\n\nt5=pd.DataFrame({'A': positive_tweets_test.clean_text})\nt6=pd.DataFrame({'A': negative_tweets_test.clean_text})\n\nall_text_tokens=pd.concat([t1,t2,t3,t4,t5,t6],axis=0)\nall_text_tokens.reset_index(inplace=True)\n\ntokens_set=set()\nfor i in tqdm(all_text_tokens.index):\n    tokens_set.update(all_text_tokens.A[i].split())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# converting text into numerical tokens\n\nmax_word_dict = len(tokens_set)\n\ntokenizer = Tokenizer(num_words=max_word_dict, lower=True, oov_token=\"<UKN>\",filters='')\ntokenizer.fit_on_texts(tokens_set)\n\nvocab_size=len(tokenizer.word_index)+1\n\npositive_tweets_train = tokenizer.texts_to_sequences(positive_tweets['clean_text'])\npositive_selected_tokenize=tokenizer.texts_to_sequences(positive_tweets['clean_selected'])\n\nnegative_tweets_train = tokenizer.texts_to_sequences(negative_tweets['clean_text'])\nnegative_selected_tokenize=tokenizer.texts_to_sequences(negative_tweets['clean_selected'])\n\npositive_test = tokenizer.texts_to_sequences(positive_tweets_test['clean_text'])\nnegative_test = tokenizer.texts_to_sequences(negative_tweets_test['clean_text'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The following function is used to create an array of 0's & 1's as the output of the NN. The 1's are placed in the indices of the words, which make up the selected text, in the input tweet ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_y_train(tokenized_selected_text_train,x_train):\n    y_train=[]\n    empty_selected=[]\n    for i in range(len(x_train)):\n        selected_text_array=np.array(tokenized_selected_text_train[i])\n        if len(selected_text_array)==0:\n            #print(\"empty selected text in index: \",i)\n            empty_selected.append(i)\n            selected_text=np.zeros(max_tweet_length)\n            y_train.append(selected_text)\n            continue\n        first_word_indices=np.where(x_train[i]==selected_text_array[0])\n        first_word_indices=np.array(first_word_indices)\n        for j in first_word_indices[0]:\n            start_index=j\n            end_index=j+np.max(np.shape(selected_text_array))-1\n            x_train_sub_array=x_train[i][start_index:end_index+1]\n            if np.array_equal(x_train_sub_array,selected_text_array):\n                break\n        selected_text=np.zeros(max_tweet_length)\n        selected_text[start_index:end_index+1]=1\n        y_train.append(selected_text)\n    y_train=np.array(y_train) \n    print(\"number of empty selected text: \",len(empty_selected))\n    return y_train,empty_selected","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The following are all the hyperparameters in the model:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# length of input to NN\nmax_tweet_length=34\n\n# embedding vector length of the embedding layer (should match the used vector length in the pretrained network)\nembedding_vector_length=300\n\n# number of units in LSTM layer \nHIDDEN_DIM=512\n\n# LSTM L2 regularization parameter\nlstm_l2=0.03\n\n# dense layers number of units\ndense_units=64\n\n# dropout value\ndropout_value=0.3\n\n# loss used in cost function   (choose either \"binary_crossentropy\" or losses.SparseCategoricalCrossentropy())\nmodel_loss=\"binary_crossentropy\"\n\n# batch size\nbatch_size=32\n\n# number of epochs\nepochs=34\n\n# validation split of training data\nvalidation_split=0.1\n\n#learning metric      (choose either \"accuracy\" of \"binary_accuracy\")\nlearning_metric=\"binary_accuracy\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"positive_x_train = pad_sequences(positive_tweets_train, maxlen=max_tweet_length)\nnegative_x_train = pad_sequences(negative_tweets_train, maxlen=max_tweet_length)\n\npositive_x_test = pad_sequences(positive_test, maxlen=max_tweet_length)\nnegative_x_test = pad_sequences(negative_test, maxlen=max_tweet_length)\n\npositive_y_train,positive_selected_empty=get_y_train(positive_selected_tokenize,positive_x_train)\nnegative_y_train,negative_selected_empty=get_y_train(negative_selected_tokenize,negative_x_train)\n\n#positive_y_train=pad_sequences(positive_selected_tokenize, maxlen=max_tweet_length)\n#negative_y_train=pad_sequences(negative_selected_tokenize, maxlen=max_tweet_length)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# We used a pre-trained network for the word embedding of our text. GloVe for twitter is used here","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_glove():\n    tqdm.pandas()\n    f = open('../input/glove840b300dtxt/glove.840B.300d.txt')\n    embedding_dict = {}\n    for line in tqdm(f):\n        word_emb=line.split(' ')\n        word=word_emb[0]\n        embedding_vector=np.array(word_emb[1:],dtype = 'float32')\n        embedding_dict[word]=embedding_vector\n    \n    f.close()\n    return embedding_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fit_glove(tokenizer,embedding_dict,embedding_vector_length):\n    \n    vocab_size=len(tokenizer.word_index)+1\n    \n    all_embs = np.stack(embedding_dict.values())\n    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n    embedding_matrix = np.random.normal(emb_mean, emb_std, (vocab_size, embedding_vector_length))\n    \n    out_of_dict=[]\n    \n    for word,value in tqdm(tokenizer.word_index.items()):\n        embedding_vector=embedding_dict.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[value]=embedding_vector\n        else:\n            out_of_dict.append(word)     \n                \n    print(\"number of words not in embedding dictionary: \", len(out_of_dict))        \n    return embedding_matrix,out_of_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_dict=load_glove()\nembedding_matrix,out_of_embedding_dict=fit_glove(tokenizer,embedding_dict,embedding_vector_length)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(out_of_embedding_dict[0:51])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Next is the model architecture","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\n    \ninputs_n = Input(shape=(max_tweet_length,), dtype='int32')\n\nembedding_layer_n = Embedding(vocab_size, embedding_vector_length, weights=[embedding_matrix],trainable=False)\nencoder_LSTM_n = Bidirectional(LSTM(HIDDEN_DIM,return_sequences=True,kernel_regularizer=regularizers.l2(lstm_l2)))\ndense_layer_relu_n1 = TimeDistributed(Dense(dense_units, activation='relu'))\ndense_layer_relu_n2 = TimeDistributed(Dense(dense_units, activation='relu'))\ndense_layer_sigmoid_n = TimeDistributed(Dense(64, activation='sigmoid'))\nDrop_n = TimeDistributed(Dropout(dropout_value))\ndense_layer_n = TimeDistributed(Dense(1, activation='sigmoid'))\n\nencoder_embedding_n = embedding_layer_n(inputs_n)\nEncoded_seq_n = encoder_LSTM_n(encoder_embedding_n)\noutputs_n = Drop_n(dense_layer_relu_n1(Encoded_seq_n))\noutputs_n = Drop_n(dense_layer_relu_n2(outputs_n))\n#outputs_n= Drop_n(dense_layer_sigmoid_n(outputs_n))\noutputs_n = dense_layer_n(outputs_n)\n\nnegative_model = Model(inputs=inputs_n, outputs=outputs_n)  \nnegative_model.compile(optimizer='Adam', loss=model_loss,metrics=[learning_metric])\nnegative_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inputs_p = Input(shape=(max_tweet_length,), dtype='int32')\n\nembedding_layer_p = Embedding(vocab_size, embedding_vector_length, weights=[embedding_matrix],trainable=False)\nencoder_LSTM_p = Bidirectional(LSTM(HIDDEN_DIM,return_sequences=True,kernel_regularizer=regularizers.l2(lstm_l2)))\ndense_layer_relu_p1 = TimeDistributed(Dense(dense_units, activation='relu'))\ndense_layer_relu_p2 = TimeDistributed(Dense(dense_units, activation='relu'))\ndense_layer_sigmoid_p = TimeDistributed(Dense(64, activation='sigmoid'))\nDrop_p = TimeDistributed(Dropout(dropout_value))\ndense_layer_p = TimeDistributed(Dense(1, activation='sigmoid'))\n\nencoder_embedding_p = embedding_layer_p(inputs_p)\nEncoded_seq_p = encoder_LSTM_p(encoder_embedding_p)\noutputs_p = Drop_p(dense_layer_relu_p1(Encoded_seq_p))\noutputs_p = Drop_p(dense_layer_relu_p2(outputs_p))\n#outputs_p= Drop_p(dense_layer_sigmoid_p(outputs_p))\noutputs_p = dense_layer_p(outputs_p)\n\npositive_model = Model(inputs=inputs_p, outputs=outputs_p)  \npositive_model.compile(optimizer='Adam', loss=model_loss,metrics=[learning_metric])\npositive_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if model_loss != \"binary_crossentropy\":\n    y_train=np.reshape(negative_y_train,np.product(np.shape(negative_y_train)))\n    print(np.shape(y_train))\n    negative_y_train=y_train\n    y_train=np.reshape(positive_y_train,np.product(np.shape(positive_y_train)))\n    positive_y_train=y_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## the inputs to the method fit() have to be arrays not lists\npositive_model.fit(positive_x_train,positive_y_train, batch_size=batch_size, epochs=epochs, validation_split=validation_split)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## the inputs to the method fit() have to be arrays not lists\nnegative_model.fit(negative_x_train,negative_y_train, batch_size=batch_size, epochs=epochs, validation_split=validation_split)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"negative_y_test=negative_model.predict(negative_x_train)\nnegative_y_test=np.around(negative_y_test)\n\npositive_y_test=positive_model.predict(positive_x_train)\npositive_y_test=np.around(positive_y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(np.transpose(negative_y_test[2]))\nprint(negative_y_train[2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert_pred_index_into_selected_text(x,y,tweets_data,tokenizer):\n    \n    reverse_tokenizer=dict(map(reversed,tokenizer.word_index.items()))\n    data_text_lower=[item.lower() for item in tweets_data.text]\n    \n    no_selected_text=0\n    for i in range(len(tweets_data)):\n        \n## debugging\n#        if i==7:\n#            print(\"index number\", i, \"has the error\")        \n#            return\n        \n        actual_selected_words=[]\n        \n        tweets_data_index=tweets_data.index[i]        \n        split_tweet=tweets_data.text[tweets_data_index].split()\n\n        #filtered_split_tweet=[]\n        \n        # doing it this way instead of doing it without split solves this example:\n        # (hello ... world), the \"...\" will be removed but it will still take an index 1 and world will take index 2\n        # if split was not used, world will now take index 1\n#        for j in range(len(split_tweet)):\n#            filtered_split_tweet.append(remove_special_characters(split_tweet[j].lower()))\n        \n        filtered_split_tweet=remove_special_characters(tweets_data.text[tweets_data_index].lower())\n        filtered_split_tweet=filtered_split_tweet.split()\n        \n        predicted_indices=np.where(y[i]==1)\n        if len(predicted_indices[0])==0:\n            tweets_data.selected_text[tweets_data_index]=\"\"\n            #print(\"no selected text for tweet in row: \",tweets_data_index, \"index number: \",i)\n            no_selected_text=no_selected_text+1\n            continue\n            \n  \n        for index in predicted_indices[0]:\n            if x[i][index] !=0:\n                selected_word=reverse_tokenizer.get(x[i][index])\n                selected_word_index=get_index_word_in_string(selected_word,filtered_split_tweet)\n                # np.where doesn't work with strings, even if split\n                #selected_word_index=np.where(filtered_split_tweet==selected_word)\n                \n# the commented code is used for debugging  \n        #        print(i)\n#                if i==7:\n#                    print(\"predicted words indices: \",predicted_indices[0])\n#                    print(\"selected word: \", selected_word)\n#                    print(\"filtered text: \",filtered_split_tweet)\n#                    print(\"selected word index:\", selected_word_index)\n#                    return\n###################################        \n                if selected_word_index==-1:\n                    print(\"there is an error in conversion for tweet row: \",tweets_data_index, \"index number: \",i)\n                elif len(selected_word_index)==1:                     \n                    tweet_text_lower_split=tweets_data.text[tweets_data_index].lower().split()\n                    update=0\n                    for j in range(len(tweet_text_lower_split)):\n                        found_word=tweet_text_lower_split[j].find(selected_word)\n                        if found_word !=-1:\n                            actual_selected_word=tweets_data.text[tweets_data_index].split()[j]\n                            duplicate=0\n                            for k in range(len(actual_selected_words)):\n                                duplicate_found=get_index_word_in_string(actual_selected_word,actual_selected_words)\n                                if duplicate_found !=-1:\n                                    #print(\"duplicate found in row: \",tweets_data_index, \"index number: \",i)\n                                    duplicate=1\n                                    update=1\n                                    break\n                            if duplicate !=1:\n                                actual_selected_words.append(actual_selected_word)\n                                update=1\n                            break\n                    if update ==0:\n                        print(\"error in find for row: \",tweets_data_index, \"index number: \",i)\n                    #actual_selected_words.append(tweets_data.text[tweets_data_index].split()[selected_word_index[0]])\n                elif len(selected_word_index)>1:\n                    ## for now, we chose the word to be of the first index, but should be later updated to be smarter ##\n                    #actual_selected_words.append(tweets_data.text[tweets_data_index].split()[selected_word_index[0]])\n                    tweet_text_lower_split=tweets_data.text[tweets_data_index].lower().split()\n                    update=0\n                    for j in range(len(tweet_text_lower_split)):\n                        found_word=tweet_text_lower_split[j].find(selected_word)\n                        if found_word !=-1:\n                            actual_selected_word=tweets_data.text[tweets_data_index].split()[j]\n                            duplicate=0\n                            for k in range(len(actual_selected_words)):\n                                duplicate_found=get_index_word_in_string(actual_selected_word,actual_selected_words)\n                                if duplicate_found !=-1:\n                                    #print(\"duplicate found in row: \",tweets_data_index, \"index number: \",i)\n                                    duplicate=1\n                                    update=1\n                                    break\n                            if duplicate !=1:\n                                actual_selected_words.append(actual_selected_word)\n                                update=1\n                            break\n                    if update ==0:\n                        print(\"error in find for row: \",tweets_data_index, \"index number: \",i)\n# the commented code is used for debugging  \n#        print(i)\n#            if i==6:\n#                print(\"predicted words indices: \",predicted_indices[0])\n#                print(\"selected word: \", selected_word)\n#                print(\"filtered text: \",filtered_split_tweet)\n#                print(\"selected word index:\", selected_word_index)\n#                print(\"actual text split: \",tweets_data.text[tweets_data_index].split())\n#                return\n           \n        actual_selected_text=\" \".join(actual_selected_words)\n        tweets_data.selected_text[tweets_data_index]=actual_selected_text      \n        #selected_text=\" \".join(selected_words)    \n    print(\"total empty selected text: \",no_selected_text)\n    return","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def populate_test_selected_text(positive_tweets,negative_tweets,test_data):\n    for i in tqdm(test_data.index):\n        if test_data.sentiment[i]==\"neutral\":\n            test_data.selected_text[i]=test_data.text[i]\n            \n        elif test_data.sentiment[i]==\"positive\":\n            if positive_tweets.textID[i] != test_data.textID[i]:\n                print(\"There is an error in indices for index: \",i)\n                for j in positive_tweets.index:\n                    if positive_tweets.textID[j] == test_data.textID[i]:\n                        test_data.selected_text[i]=positive_tweets.selected_text[j]\n                        break\n            else:\n                test_data.selected_text[i]=positive_tweets.selected_text[i]\n                \n        elif test_data.sentiment[i]==\"negative\":\n            if negative_tweets.textID[i] != test_data.textID[i]:\n                print(\"There is an error in indices for index: \",i)\n                for j in negative_tweets.index:\n                    if negative_tweets.textID[j] == test_data.textID[i]:\n                        test_data.selected_text[i]=negative_tweets.selected_text[j]\n                        break\n            else:\n                test_data.selected_text[i]=negative_tweets.selected_text[i]\n                \n    return\n                                ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"positive_train_copy1=positive_tweets.copy()\nconvert_pred_index_into_selected_text(positive_x_train,positive_y_test,positive_train_copy1,tokenizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"negative_train_copy1=negative_tweets.copy()\nconvert_pred_index_into_selected_text(negative_x_train,negative_y_test,negative_train_copy1,tokenizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_copy_copy=train_copy.copy()\npopulate_test_selected_text(positive_train_copy1,negative_train_copy1,train_copy_copy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"correct_train_copy=train.copy()\ncorrect_train_copy.dropna(axis=0, how=\"any\", inplace=True)\ncorrect_train_copy.reset_index(inplace=True)\n\njaccard_list=[]\nfor i in correct_train_copy.index:\n    correct_set=set(correct_train_copy.selected_text[i].split())\n    predicted_set=set(train_copy_copy.selected_text[i].split())\n    intersection=correct_set.intersection(predicted_set)\n    jaccard= float(len(intersection)/(len(correct_set)+len(predicted_set)-len(intersection)))\n    jaccard_list.append(jaccard)\n\naverage_jaccard=sum(jaccard_list)/len(jaccard_list)\nprint(\"average jaccard: \", average_jaccard)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"negative_y_test=negative_model.predict(negative_x_test)\nnegative_y_test=np.around(negative_y_test)\nnegative_test_data_copy=negative_tweets_test.copy()\nconvert_pred_index_into_selected_text(negative_x_test,negative_y_test,negative_test_data_copy,tokenizer)\n\npositive_y_test=positive_model.predict(positive_x_test)\npositive_y_test=np.around(positive_y_test)\npositive_test_data_copy=positive_tweets_test.copy()\nconvert_pred_index_into_selected_text(positive_x_test,positive_y_test,positive_test_data_copy,tokenizer)\ntest_data_copy=test.copy()\npopulate_test_selected_text(positive_test_data_copy,negative_test_data_copy,test_data_copy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"jaccard score between text and selected text for test set:\")\n#sentiment jaccard score\npositive_test_jaccard=get_jaccard(positive_test_data_copy)\nprint(\"positive sentiments jaccard score: \",positive_test_jaccard)\nnegative_test_jaccard=get_jaccard(negative_test_data_copy)\nprint(\"negative sentiments jaccard score: \",negative_test_jaccard)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def csv_submission(test_data):\n    submission_data={\"textID\":test_data.textID,\n               \"selected_text\":test_data.selected_text,\n    }\n    test_csv=pd.DataFrame(submission_data,columns=[\"textID\",\"selected_text\"])\n    test_csv.to_csv('submission.csv',index=False)\n    return","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"csv_submission(test_data_copy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}