{"cells":[{"metadata":{"_uuid":"3188564a-04a8-49fe-9aaa-60ee9b7b220b","_cell_guid":"4597dade-8ff4-4e14-b6d1-f003cf17a7f0","trusted":true},"cell_type":"markdown","source":"**1.1 Importing Libraries**"},{"metadata":{"_uuid":"85765198-9824-4853-aa64-ab29fc34fe31","_cell_guid":"5895c376-a09d-4d21-9c08-c1fde2bf160c","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport string\nfrom wordcloud import WordCloud, STOPWORDS\nimport tensorflow as tf\nimport missingno as msno\nfrom collections import defaultdict\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nimport json\nfrom sklearn.model_selection import StratifiedKFold\nfrom transformers import *\nimport tokenizers\nimport tensorflow.keras.backend as K\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9ba4c9a7-557d-4f77-ade8-f3a4af0cf282","_cell_guid":"e1867df7-59b9-438c-a3f5-50370bef149c","trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/tweet-sentiment-extraction/train.csv\").fillna(\"\")\ntest = pd.read_csv(\"/kaggle/input/tweet-sentiment-extraction/test.csv\")\nsample_submission = pd.read_csv(\"/kaggle/input/tweet-sentiment-extraction/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"text\"] = train[\"text\"].apply(lambda x : x.strip())\ntest[\"text\"] = test[\"text\"].apply(lambda x : x.strip())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.__version__","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN = 128\nPATH = \"../input/tf-roberta/\"\ntokenizer = tokenizers.ByteLevelBPETokenizer(\n    vocab_file = PATH + \"vocab-roberta-base.json\",\n    merges_file = PATH + \"merges-roberta-base.txt\",\n    lowercase = True,\n    add_prefix_space = True\n)\n\nsentiment_id = {\"positive\": 1313, \"negative\": 2430, \"neutral\": 7974}\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preparing train data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make tokens\n# Make input ids  0-start, 2-end, 1-pad\n# make attention masks\n# make start_token\n# make end token\nnum_rows = train.shape[0]\ntrain_input_ids = []\ntrain_attention_masks = []\ntrain_start_tokens = []\ntrain_end_tokens = []\n\nfor row in range(num_rows):\n    encoding = tokenizer.encode(train.iloc[row, 1])\n    text_tokens = encoding.tokens\n    senti = tokenizer.encode(train.iloc[row, 3])\n    padding = MAX_LEN - len(encoding.ids)\n    input_id = [0] + encoding.ids + [2, 2] + senti.ids + [2] + [1] * (padding - 5)\n    attention_mask = [1] * len([0] + encoding.ids + [2, 2] + senti.ids + [2]) + [0] * (padding - 5)\n    selected_tokens = tokenizer.encode(train.iloc[row, 2]).tokens\n    tok_start = 0\n    for tok_s in selected_tokens:\n        for i, tok_t in enumerate(text_tokens):\n            if tok_t == tok_s:\n                tok_start = i + 1\n                break  \n        break\n    tok_end = tok_start + len(selected_tokens) - 1\n    start_tok, end_tok = [0] * len(input_id), [0] * len(input_id)\n    start_tok[tok_start] = 1\n    end_tok[tok_end] = 1\n    \n    train_input_ids.append(input_id)\n    train_attention_masks.append(attention_mask)\n    train_start_tokens.append(start_tok)\n    train_end_tokens.append(end_tok)\n    \ntrain_input_ids = np.array(train_input_ids, dtype = \"int32\")\ntrain_attention_masks = np.array(train_attention_masks, dtype = \"int32\")\ntrain_start_tokens = np.array(train_start_tokens, dtype = \"int32\")\ntrain_end_tokens = np.array(train_end_tokens, dtype = \"int32\")\ntrain_token_type_ids = np.zeros((num_rows,MAX_LEN), dtype = \"int32\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nnum_rows = test.shape[0]\ntest_input_ids = []\ntest_attention_masks = []\n\nfor row in range(num_rows):\n    encoding = tokenizer.encode(test.iloc[row, 1])\n    text_tokens = encoding.tokens\n    senti = tokenizer.encode(test.iloc[row, -1])\n    padding = MAX_LEN - len(encoding.ids)\n    input_id = [0] + encoding.ids + [2, 2] + senti.ids + [2] + [1] * (padding - 5)\n    attention_mask = [1] * len([0] + encoding.ids + [2, 2] + senti.ids + [2]) + [0] * (padding - 5)\n    test_input_ids.append(input_id)\n    test_attention_masks.append(attention_mask)\n    \ntest_input_ids = np.array(test_input_ids, dtype = \"int32\")\ntest_attention_mask = np.array(test_attention_masks, dtype = \"int32\")    \ntest_token_type_ids = np.zeros((num_rows,MAX_LEN), dtype = \"int32\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ids = tf.keras.layers.Input((MAX_LEN, ), dtype = tf.int32)\natt = tf.keras.layers.Input((MAX_LEN, ), dtype = tf.int32)\ntoken = tf.keras.layers.Input((MAX_LEN, ), dtype = tf.int32)\n\nconfig = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\nbert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n\nx = bert_model(ids,attention_mask=att,token_type_ids=token)\n# training for starting indices\nx1 = tf.keras.layers.Dropout(0.1)(x[0]) \nx1 = tf.keras.layers.Conv1D(128, 1,padding='same')(x1)\nx1 = tf.keras.layers.Conv1D(1,1)(x1)\nx1 = tf.keras.layers.Flatten()(x1)\nx1 = tf.keras.layers.Activation('softmax')(x1)\n# training for end indices\nx2 = tf.keras.layers.Dropout(0.1)(x[0]) \nx2 = tf.keras.layers.Conv1D(128, 1,padding='same')(x2)\nx2 = tf.keras.layers.Conv1D(1,1)(x2)\nx2 = tf.keras.layers.Flatten()(x2)\nx2 = tf.keras.layers.Activation('softmax')(x2)\n\nmodel = tf.keras.models.Model(inputs=[ids, att, token], outputs=[x1,x2])\noptimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\nmodel.compile(loss='categorical_crossentropy', optimizer=optimizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit([train_input_ids.reshape(train.shape[0], 128), train_attention_masks.reshape(train.shape[0], 128), train_token_type_ids.reshape(train.shape[0], 128)], \n          [train_start_tokens.reshape(train.shape[0], 128), train_end_tokens.reshape(train.shape[0], 128)], epochs=3, batch_size=32) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = model.predict([test_input_ids.reshape(test.shape[0], 128), test_attention_mask.reshape(test.shape[0], 128), test_token_type_ids.reshape(test.shape[0], 128)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = []\nfor k in range(test_input_ids.shape[0]):\n    a = np.argmax(preds[0][k])\n    b = np.argmax(preds[1][k])\n    if a>b: \n        st = test.iloc[k,1]\n    else:\n        st = tokenizer.decode(test_input_ids[k][a:b + 1])\n    preds.append(st)","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":4}