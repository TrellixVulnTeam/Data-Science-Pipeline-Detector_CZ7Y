{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom transformers import BertModel, BertPreTrainedModel, BertConfig, BertTokenizer, AdamW, get_linear_schedule_with_warmup\nfrom sklearn.model_selection import KFold\n\nimport re\n\ntqdm.pandas()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Import Training Dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Cleaning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_leading_truncuated_word(row):\n    text = row['text']\n    selected_text = row['selected_text']\n\n    first_selected_text_word = selected_text.lower().split()[0]\n    if (first_selected_text_word not in text.lower().split()) and \\\n        (len(first_selected_text_word) == 1):\n        return \" \".join(selected_text.lower().split()[1:])\n        \n    else:\n        return selected_text\n\ndef modified_text_to_bert_input(text, tokenizer):\n    return tokenizer.convert_tokens_to_string(tokenizer.tokenize(text))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('/kaggle/input/bertbaseuncased', max_len=192)\n\ntrain_dataset['text'] = train_dataset['text'].apply(modified_text_to_bert_input, args=(tokenizer,))\ntrain_dataset['selected_text'] = train_dataset['selected_text'].apply(modified_text_to_bert_input, args=(tokenizer,))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset['selected_text'] = train_dataset.apply(remove_leading_truncuated_word, axis=1)\n\ntrain_dataset['selected_text'].replace('', np.nan, inplace=True)\ntrain_dataset.dropna(inplace=True)  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Check if the 'selected_text' is a substring of 'text'","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def is_substring(row):\n    return row.loc['selected_text'] in row.loc['text']\n\ntrain_dataset['is_substring'] = train_dataset.apply(is_substring, axis=1)\ntrain_dataset['is_substring'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Investigate dataset with neutral sentiment","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"neutral_dataset = train_dataset[train_dataset['sentiment'] == 'neutral']\nneutral_dataset['is_same'] = neutral_dataset['selected_text'] == neutral_dataset['text']\nneutral_dataset['is_same'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, most of the data with neutral sentiment will have 'text' == 'selected_text'.\n\nSo we will not input neutral sentiment dataset into the model, we will just copy the whole 'text' as 'selected_text'","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = train_dataset[~(train_dataset['sentiment'] == 'neutral')]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dataset\n## Helper Function","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_start_and_end_char_index(row):\n    text = row['text']\n    selected_text = row['selected_text']\n    \n    start_idx = text.find(selected_text)\n    end_idx = start_idx + len(selected_text)\n\n    return start_idx, end_idx\n\ndef target_answer(row, tokenizer):\n    def is_whitespace(c):\n        if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n            return True\n        return False\n\n    doc_tokens = []\n    char_to_word_offset = []\n    prev_is_whitespace = True\n\n    for c in row['text']:\n        if is_whitespace(c):\n            prev_is_whitespace = True\n        else:\n            if prev_is_whitespace:\n                doc_tokens.append(c)\n            else:\n                doc_tokens[-1] += c\n            prev_is_whitespace = False\n        char_to_word_offset.append(len(doc_tokens) - 1)\n\n    start_idx = row['start_char_idx']\n    end_idx = row['end_char_idx']\n\n    start_position = char_to_word_offset[start_idx]\n    end_position = char_to_word_offset[end_idx - 1]\n\n    actual_text = \" \".join(doc_tokens[start_position:end_position+1])\n\n    tok_to_orig_index = []\n    orig_to_tok_index = []\n    all_doc_tokens = []\n\n    for i, token in enumerate(doc_tokens):\n        orig_to_tok_index.append(len(all_doc_tokens))\n        sub_tokens = tokenizer.tokenize(token)\n        for sub_token in sub_tokens:\n            tok_to_orig_index.append(i)\n            all_doc_tokens.append(sub_token)\n\n    tok_start_position = orig_to_tok_index[start_position]\n    if end_position < len(doc_tokens) - 1:\n        tok_end_position = orig_to_tok_index[end_position + 1] - 1\n    else:\n        tok_end_position = len(all_doc_tokens) - 1\n\n    # Improve span answer\n    tok_answer_text = tokenizer.tokenize(row['selected_text'])\n\n    for new_start in range(tok_start_position, tok_end_position + 1):\n        for new_end in range(tok_end_position, tok_start_position - 1, -1):\n            text_span = all_doc_tokens[new_start : new_end + 1]\n            if text_span == tok_answer_text:\n                return new_start, new_end, tokenizer.convert_tokens_to_string(text_span)\n    \n    text_span = all_doc_tokens[tok_start_position : tok_end_position + 1]\n\n    return tok_start_position, tok_end_position, tokenizer.convert_tokens_to_string(text_span)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Get 'start_char_idx' & 'end_char_idx' for each data\n'start_char_idx' is the position of the first character of 'selected_text' in 'text'\n\n'end_char_idx' is the position of the last character of 'selected_text' in 'text'","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset[['start_char_idx', 'end_char_idx']] = train_dataset.apply(get_start_and_end_char_index, axis=1, result_type='expand')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Get 'target_start_idx' & 'target_end_idx' for each data\n'target_start_idx' is the position of the first token of 'selected_text' in 'text'\n\n'target_end_idx' is the position of the last token of 'selected_text' in 'text'","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset[['target_start_idx', 'target_end_idx', 'target_answer']] = train_dataset.progress_apply(target_answer, axis=1, result_type='expand', args=(tokenizer,))\ntrain_dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset['is_same'] = train_dataset['selected_text'] == train_dataset['target_answer']\ntrain_dataset['is_same'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the results above, we can see that some of the 'target_answer' is not the same as 'selected_text'.\n\nThis is might due to truncuated word in 'selected_text' that is not appear in the tokens of the 'text'","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Pytorch Dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class TweetDataset(Dataset):\n    def __init__(self, df, tokenizer):\n        # assert len(text) == len(selected_text), \"Length of list of 'text' should be same as list of 'selected_text'\"\n\n        self.text = df['text'].tolist()\n        self.selected_text = df['selected_text'].tolist()\n        self.target_start_idx = df['target_start_idx'].tolist()\n        self.target_end_idx = df['target_end_idx'].tolist()\n        self.sentiment = df['sentiment'].tolist()\n        self.tokenizer = tokenizer\n\n    def __len__(self):\n        return len(self.text)\n\n    def __getitem__(self, idx):\n        encoded = self._process_data(self.text[idx], self.sentiment[idx], self.target_start_idx[idx], self.target_end_idx[idx])\n        data = {\n            'input_ids': torch.tensor(encoded['input_ids'], dtype=torch.long),\n            'token_type_ids': torch.tensor(encoded['token_type_ids'], dtype=torch.long),\n            'attention_mask': torch.tensor(encoded['attention_mask'], dtype=torch.float),\n            'target_start_idx': torch.tensor(encoded['target_start_idx'], dtype=torch.long),\n            'target_end_idx': torch.tensor(encoded['target_end_idx'], dtype=torch.long)       \n        }\n\n        return data \n\n    def _process_data(self, text, sentiment, target_start_idx, target_end_idx):\n        data = self.tokenizer.encode_plus(sentiment, text, pad_to_max_length=True)\n        \n        data['target_start_idx'] = target_start_idx + 3\n        data['target_end_idx'] = target_end_idx + 3\n\n        return data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class SentimentExtractionBert(nn.Module):\n    def __init__(self):\n        super(SentimentExtractionBert, self).__init__()\n        self.num_labels = 2\n\n        self.bert = BertModel.from_pretrained('/kaggle/input/bertbaseuncased/')\n        self.dropout = nn.Dropout(0.2)\n        self.outputs = nn.Linear(768 * 2, self.num_labels)\n\n        # Initialise weight for the outputs layer\n        nn.init.zeros_(self.outputs.bias)\n        nn.init.normal_(self.outputs.weight, mean=0.0, std=0.02)\n\n    def forward(self,\n        input_ids,\n        attention_mask,\n        token_type_ids,\n    ):\n\n        outputs = self.bert(\n            input_ids=input_ids, \n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids\n            )\n        \n        sequence_output = outputs[0]\n        sentiment_rep = sequence_output[:,1].unsqueeze(1).repeat(1, 192, 1)\n        sent_seq_output = torch.cat([sequence_output, sentiment_rep], dim=-1)\n\n        logits = self.outputs(self.dropout(sent_seq_output))\n        start_logits, end_logits = logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n\n        return start_logits, end_logits","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training\n## Hyperparameters","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"FOLD = 5\n\nEPOCHS = 3\nBATCH_SIZE = 32\nLEARNING_RATE = 3e-5\n\n# Optimizer Hyperparams\nWEIGHT_DECAY = 0.01\nWARMUP_STEPS = 0\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kf = KFold(n_splits=FOLD)\nkfold_list = list(kf.split(train_dataset))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_one_epoch(model, train_dl, optimizer, scheduler):\n    criterion = nn.CrossEntropyLoss()\n\n    training_loss = 0\n    model.train()\n    for data in tqdm(train_dl):\n        input_ids = data['input_ids'].to(device)\n        token_type_ids = data['token_type_ids'].to(device)\n        attention_mask = data['attention_mask'].to(device)\n        target_start_idx = data['target_start_idx'].to(device)\n        target_end_idx = data['target_end_idx'].to(device)\n\n        optimizer.zero_grad()\n        start_logits, end_logits = model(input_ids, attention_mask, token_type_ids)\n        loss = criterion(start_logits, target_start_idx) + criterion(end_logits, target_end_idx)\n\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n\n        training_loss += loss.item()\n    training_loss /= len(train_dl)\n \n    return training_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(fold):\n    model = SentimentExtractionBert()\n    \n    model = model.to(device)\n    train_ds = TweetDataset(train_dataset.iloc[kfold_list[fold][0]], tokenizer)\n    train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    optimizer_grouped_parameters = [\n        {\n            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n            \"weight_decay\": WEIGHT_DECAY,\n        },\n        {   \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \n            \"weight_decay\": 0.0\n        },\n    ]\n    optimizer = AdamW(optimizer_grouped_parameters, lr=LEARNING_RATE, betas=(0.9, 0.98), eps=1e-8)\n\n    t_total = len(train_dl) * EPOCHS\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=WARMUP_STEPS, num_training_steps=t_total)\n\n    for epoch in range(EPOCHS):\n        training_loss = train_one_epoch(model, train_dl, optimizer, scheduler)\n        with open(\"./log.txt\", 'a') as f:\n            f.write(\"Fold {}, Epoch {}, Learning Rate: {},Loss: {}\\n\".format(fold, epoch, optimizer.param_groups[0]['lr'], training_loss))\n\n    torch.save(model.state_dict(), 'ft_ckpt_{}.pth'.format(fold))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for fold in range(FOLD):\n    train(fold)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Testing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model0 = SentimentExtractionBert()\nmodel0.load_state_dict(torch.load(\"ft_ckpt_0.pth\"))\nmodel0.to(device)\nmodel0.eval()\n\nmodel1 = SentimentExtractionBert()\nmodel1.load_state_dict(torch.load(\"ft_ckpt_1.pth\"))\nmodel1.to(device)\nmodel1.eval()\n\nmodel2 = SentimentExtractionBert()\nmodel2.load_state_dict(torch.load(\"ft_ckpt_2.pth\"))\nmodel2.to(device)\nmodel2.eval()\n\nmodel3 = SentimentExtractionBert()\nmodel3.load_state_dict(torch.load(\"ft_ckpt_3.pth\"))\nmodel3.to(device)\nmodel3.eval()\n\nmodel4 = SentimentExtractionBert()\nmodel4.load_state_dict(torch.load(\"ft_ckpt_4.pth\"))\nmodel4.to(device)\nmodel4.eval()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_test_data(row):\n        data = tokenizer.encode_plus(row['sentiment'], row['text'], pad_to_max_length=True)\n        \n        tensor_data = {\n            'input_ids': torch.tensor(data['input_ids'], dtype=torch.long).unsqueeze(0),\n            'token_type_ids': torch.tensor(data['token_type_ids'], dtype=torch.long).unsqueeze(0),\n            'attention_mask': torch.tensor(data['attention_mask'], dtype=torch.float).unsqueeze((0)),          \n        }\n\n        return tensor_data\n\ndef postprocessing(text):\n    out_text = (\n        text.replace(\" ` \", \"`\")\n        .replace(\"* * * *\", \"****\")\n    )\n    \n    return out_text\n    \ndef test(row):\n    if row['sentiment'] == 'neutral':\n        predicted_text = row['text']\n\n    else:\n        data = process_test_data(row)\n\n        input_ids = data['input_ids'].to(device)\n        token_type_ids = data['token_type_ids'].to(device)\n        attention_mask = data['attention_mask'].to(device)\n\n        with torch.no_grad():\n            start_logits_0, end_logits_0 = model0(input_ids, attention_mask, token_type_ids)\n            start_logits_1, end_logits_1 = model1(input_ids, attention_mask, token_type_ids)\n            start_logits_2, end_logits_2 = model2(input_ids, attention_mask, token_type_ids)\n            start_logits_3, end_logits_3 = model3(input_ids, attention_mask, token_type_ids)\n            start_logits_4, end_logits_4 = model4(input_ids, attention_mask, token_type_ids)\n\n        start_logits = (\n                    start_logits_0\n                    + start_logits_1\n                    + start_logits_2\n                    + start_logits_3 \n                    + start_logits_4\n        ) / 5\n        end_logits = (\n                end_logits_0\n                + end_logits_1\n                + end_logits_2\n                + end_logits_3\n                + end_logits_4\n        ) / 5\n        \n        predicted_start_idx = torch.argmax(torch.softmax(start_logits, dim=1), dim=1).item()\n        predicted_end_idx = torch.argmax(torch.softmax(end_logits, dim=1), dim=1).item()\n\n        input_ids = input_ids.squeeze()\n        \n        predicted_text = tokenizer.decode(input_ids[predicted_start_idx : predicted_end_idx + 1], skip_special_tokens=True)\n        predicted_text = postprocessing(predicted_text)\n        \n    return predicted_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dataset = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv')\n\ntest_dataset['predicted_text'] = test_dataset.progress_apply(test, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/sample_submission.csv')\nsubmission['selected_text'] = test_dataset['predicted_text']\n\nsubmission.to_csv('./submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}