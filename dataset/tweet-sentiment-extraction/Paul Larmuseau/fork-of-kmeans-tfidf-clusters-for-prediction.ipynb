{"cells":[{"metadata":{},"cell_type":"markdown","source":"# rake\nscripts test\nwill be used later"},{"metadata":{"_uuid":"bb857762-571a-41e0-bc79-92afb548cb61","_cell_guid":"7babe24c-aea8-4e27-b5c7-ccd07571397f","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n        \n#https://www.kaggle.com/jbencina/clustering-documents-with-tfidf-and-kmeans\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport numpy as np\n\nprint_every = 50\ninit_size = 2000\nbatch_size = 4000\n\ndef drop_empty_rows(df):\n    nan_value = float(\"NaN\")\n    df.replace(\"\", nan_value, inplace=True)\n    \n\ndef levenshtein_distance(s, t):\n    ''' From Wikipedia article; Iterative with two matrix rows. '''\n    if s == t: return 0\n    elif len(s) == 0: return len(t)\n    elif len(t) == 0: return len(s)\n    v0 = [None] * (len(t) + 1)\n    v1 = [None] * (len(t) + 1)\n    for i in range(len(v0)):\n        v0[i] = i\n    for i in range(len(s)):\n        v1[0] = i + 1\n        for j in range(len(t)):\n            cost = 0 if s[i] == t[j] else 1\n            v1[j + 1] = min(v1[j] + 1, v0[j + 1] + 1, v0[j] + cost)\n        for j in range(len(v0)):\n            v0[j] = v1[j]\n            \n    return v1[len(t)]    \n        \ndef find_optimal_clusters(data, max_k,column,init_size,batch_size):\n    iters = range(2, max_k+1, 2)\n    \n    sse = []\n    for k in iters:\n        sse.append(MiniBatchKMeans(n_clusters=k, init_size=init_size, batch_size=batch_size, random_state=20).fit(data).inertia_)\n        if (k % print_every == 0):\n            print('Fit {} clusters for  column: {}'.format(k,column))\n    f, ax = plt.subplots(1, 1)\n    ax.plot(iters, sse, marker='o')\n    ax.set_xlabel('Cluster Centers')\n    ax.set_xticks(iters)\n    ax.set_xticklabels(iters)\n    ax.set_ylabel('SSE')\n    ax.set_title('SSE by Cluster Center Plot')\n    plt.show()\n    \ndef plot_tsne_pca(data, labels,column):\n    max_label = max(labels)\n    max_items = np.random.choice(range(data.shape[0]), size=min(3000,data.shape[0]), replace=False)\n    \n    pca = PCA(n_components=2).fit_transform(data[max_items,:].todense())\n    tsne = TSNE().fit_transform(PCA(n_components=20).fit_transform(data[max_items,:].todense()))\n    \n    \n    idx = np.random.choice(range(pca.shape[0]), size=500, replace=False)\n    label_subset = labels[max_items]\n    label_subset = [cm.hsv(i/max_label) for i in label_subset[idx]]\n    \n    f, ax = plt.subplots(1, 2, figsize=(14, 6))\n    \n    scatt=ax[0].scatter(pca[idx, 0], pca[idx, 1], c=label_subset)\n    #ax[0].legend(*scatt.legend_elements())\n    ax[0].set_title('PCA Cluster Plot ' + column)\n    \n    ax[1].scatter(tsne[idx, 0], tsne[idx, 1], c=label_subset)\n    ax[1].set_title('TSNE Cluster Plot ' + column)\n    plt.show()\n    from scipy.sparse.linalg import svds, eigs\n    from scipy.sparse import csc_matrix\n\n    um,sm,vm=svds(data*1.0,k=5)\n    um=pd.DataFrame(um)\n    um['label']=labels\n    return um\n    \ndef get_top_keywords(data, clusters, labels, n_terms,column):\n    df = pd.DataFrame(data.todense()).groupby(clusters).mean()\n    \n    for i,r in df.iterrows():\n        print('\\nCluster {} column: {}'.format(i,column))\n        print(','.join(set([labels[t] for t in np.argsort(r)[-n_terms:]])))\n\n            \ntfidf = TfidfVectorizer(\n    min_df = 1,\n    max_df = 0.95,\n    stop_words = 'english',    \n    max_features = 450\n)\n\nimport pandas as pd\n\n\n#RAKE/rake.py /\n#@polymeris polymeris FIX stop_words_path in Rake class initialization\n#@idf@mikeiannacone@aneesha@GMadorell@polymeris\n  \n# Implementation of RAKE - Rapid Automtic Keyword Exraction algorithm\n# as described in:\n# Rose, S., D. Engel, N. Cramer, and W. Cowley (2010). \n# Automatic keyword extraction from indi-vidual documents. \n# In M. W. Berry and J. Kogan (Eds.), Text Mining: Applications and Theory.unknown: John Wiley and Sons, Ltd.\n\nimport re\nimport operator\n\ndebug = False\n\n\ndef is_number(s):\n    try:\n        float(s) if '.' in s else int(s)\n        return True\n    except ValueError:\n        return False\n\n\ndef load_stop_words(stop_word_file):\n    \"\"\"\n    Utility function to load stop words from a file and return as a list of words\n    @param stop_word_file Path and file name of a file containing stop words.\n    @return list A list of stop words.\n    \"\"\"\n    stop_words = []\n    for line in open(stop_word_file):\n        if line.strip()[0:1] != \"#\":\n            for word in line.split():  # in case more than one per line\n                stop_words.append(word)\n    return stop_words\n\n\ndef separate_words(text, min_word_return_size):\n    \"\"\"\n    Utility function to return a list of all words that are have a length greater than a specified number of characters.\n    @param text The text that must be split in to words.\n    @param min_word_return_size The minimum no of characters a word must have to be included.\n    \"\"\"\n    splitter = re.compile('[^a-zA-Z0-9_\\\\+\\\\-/]')\n    words = []\n    for single_word in splitter.split(text):\n        current_word = single_word.strip().lower()\n        #leave numbers in phrase, but don't count as words, since they tend to invalidate scores of their phrases\n        if len(current_word) > min_word_return_size and current_word != '' and not is_number(current_word):\n            words.append(current_word)\n    return words\n\n\ndef split_sentences(text):\n    \"\"\"\n    Utility function to return a list of sentences.\n    @param text The text that must be split in to sentences.\n    \"\"\"\n    sentence_delimiters = re.compile(u'[.!?,;:\\t\\\\\\\\\"\\\\(\\\\)\\\\\\'\\u2019\\u2013]|\\\\s\\\\-\\\\s')\n    sentences = sentence_delimiters.split(text)\n    return sentences\n\n\ndef build_stop_word_regex(stop_word_file_path):\n    stop_word_list = load_stop_words(stop_word_file_path)\n    stop_word_regex_list = []\n    for word in stop_word_list:\n        word_regex = r'\\b' + word + r'(?![\\w-])'  # added look ahead for hyphen\n        stop_word_regex_list.append(word_regex)\n    stop_word_pattern = re.compile('|'.join(stop_word_regex_list), re.IGNORECASE)\n    return stop_word_pattern\n\n\ndef generate_candidate_keywords(sentence_list, stopword_pattern):\n    phrase_list = []\n    for s in sentence_list:\n        tmp = re.sub(stopword_pattern, '|', s.strip())\n        phrases = tmp.split(\"|\")\n        for phrase in phrases:\n            phrase = phrase.strip().lower()\n            if phrase != \"\":\n                phrase_list.append(phrase)\n    return phrase_list\n\n\ndef calculate_word_scores(phraseList):\n    word_frequency = {}\n    word_degree = {}\n    for phrase in phraseList:\n        word_list = separate_words(phrase, 0)\n        word_list_length = len(word_list)\n        word_list_degree = word_list_length - 1\n        #if word_list_degree > 3: word_list_degree = 3 #exp.\n        for word in word_list:\n            word_frequency.setdefault(word, 0)\n            word_frequency[word] += 1\n            word_degree.setdefault(word, 0)\n            word_degree[word] += word_list_degree  #orig.\n            #word_degree[word] += 1/(word_list_length*1.0) #exp.\n    for item in word_frequency:\n        word_degree[item] = word_degree[item] + word_frequency[item]\n\n    # Calculate Word scores = deg(w)/frew(w)\n    word_score = {}\n    for item in word_frequency:\n        word_score.setdefault(item, 0)\n        word_score[item] = word_degree[item] / (word_frequency[item] * 1.0)  #orig.\n    #word_score[item] = word_frequency[item]/(word_degree[item] * 1.0) #exp.\n    return word_score\n\n\ndef generate_candidate_keyword_scores(phrase_list, word_score):\n    keyword_candidates = {}\n    for phrase in phrase_list:\n        keyword_candidates.setdefault(phrase, 0)\n        word_list = separate_words(phrase, 0)\n        candidate_score = 0\n        for word in word_list:\n            candidate_score += word_score[word]\n        keyword_candidates[phrase] = candidate_score\n    return keyword_candidates\n\n\ndef RAKE(text,__stop_words_pattern):\n\n    sentence_list = split_sentences(text)\n    phrase_list = generate_candidate_keywords(sentence_list, __stop_words_pattern)\n    word_scores = calculate_word_scores(phrase_list)\n    keyword_candidates = generate_candidate_keyword_scores(phrase_list, word_scores)\n    sorted_keywords = sorted(keyword_candidates.items(), key=operator.itemgetter(1), reverse=True)\n    return sorted_keywords\n\ndebug=True\n\nif True:\n    text = \"Compatibility of systems of linear constraints over the set of natural numbers. Criteria of compatibility of a system of linear Diophantine equations, strict inequations, and nonstrict inequations are considered. Upper bounds for components of a minimal set of solutions and algorithms of construction of minimal generating sets of solutions for all types of systems are given. These criteria and the corresponding algorithms for constructing a minimal supporting set of solutions can be used in solving all the considered types of systems and systems of mixed types.\"\n\n    # Split text into sentences\n    sentenceList = split_sentences(text)\n    #stoppath = \"FoxStoplist.txt\" #Fox stoplist contains \"numbers\", so it will not find \"natural numbers\" like in Table 1.1\n    stoppath = \"/kaggle/input/smartstoplist/SmartStoplist.txt\"  #SMART stoplist misses some of the lower-scoring keywords in Figure 1.5, which means that the top 1/3 cuts off one of the 4.0 score words in Table 1.1\n    stopwordpattern = build_stop_word_regex(stoppath)\n\n    # generate candidate keywords\n    phraseList = generate_candidate_keywords(sentenceList, stopwordpattern)\n\n    # calculate individual word scores\n    wordscores = calculate_word_scores(phraseList)\n\n    # generate candidate keyword scores\n    keywordcandidates = generate_candidate_keyword_scores(phraseList, wordscores)\n    if debug: print (keywordcandidates)\n\n    sortedKeywords = sorted(keywordcandidates.items(), key=operator.itemgetter(1), reverse=True)\n    if debug: print (sortedKeywords)\n\n    totalKeywords = len(sortedKeywords)\n    if debug: print (totalKeywords)\n    print (sortedKeywords[0:(int(totalKeywords / 3) )])\n\n    #rake = Rake(\"SmartStoplist.txt\")\n    keywords = RAKE(text,stopwordpattern)\n    print (keywords)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# text + sentiment tfidf \ntest if sentiment clusters are seperatated with tfidf date combined with sentiment\n> evidently if you add the sentiment word to the text, that the separation becomes a fact"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv',encoding='utf-8')\ntfidf = TfidfVectorizer( ngram_range=(1, 1) )\nfrom sklearn import preprocessing\nle = preprocessing.LabelEncoder()\nfrom scipy.sparse import csc_matrix\nX=tfidf.fit_transform(train.text.fillna(' ')+' '+train.sentiment)   \nX=X.T.multiply(np.exp( np.array( le.fit_transform(train.sentiment) ) ) )\nX=csc_matrix(X.T)\nplot_tsne_pca(X, le.fit_transform(train.sentiment)+1,'tt')  \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# text+sentiment ngram1-2 Countvectorizer\nis better"},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv',encoding='utf-8')\nfrom sklearn import preprocessing\nle = preprocessing.LabelEncoder()\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(ngram_range=(1, 2))\nXc = vectorizer.fit_transform(train.text.fillna(' ') +' '+train.sentiment)\n#XcS=Xc.T.multiply(np.exp( np.array( le.fit_transform(train.sentiment) ) ) )\n#XcS=csc_matrix(XcS.T)\nplot_tsne_pca(Xc, le.fit_transform(train.sentiment)+1,'tt')  \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"# usvd separation word/text vectors\nmakes it evenbetter"},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.sparse.linalg import svds, eigs\nfrom scipy.sparse import csc_matrix\n\nu,s,v=svds(Xc*1.0,k=30)\nplot_tsne_pca(csc_matrix(u*s), le.fit_transform(train.sentiment)+1,'tn')  \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"find_optimal_clusters(u,20,2,20,1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(vectorizer.vocabulary_['positive'],vectorizer.vocabulary_['negative'],vectorizer.vocabulary_['neutral'])\n\nlabels= np.array([0 for x in range(len(v.T)) ] )\nlabels[vectorizer.vocabulary_['positive']]=2\nlabels[vectorizer.vocabulary_['negative']]=3\nlabels[vectorizer.vocabulary_['neutral']]=1                 ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# FIND POSITIVE NEGATIVE AND NEUTRAL WORDS"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics.pairwise import cosine_similarity\n#cosine_similarity(v.T,\nv123=np.concatenate((v.T[vectorizer.vocabulary_['neutral']],v.T[vectorizer.vocabulary_['positive']],v.T[vectorizer.vocabulary_['negative']])).reshape(3,-1)\nv123=pd.DataFrame(cosine_similarity(v.T,v123))\n#labels=\nv123['label']=0\nfor xi in range (len(v123)):\n    maxrow=v123.iloc[xi,:3].max()\n    for yi in range(3):\n        if v123.iloc[xi,yi]>0.335:\n            #print(xi,yi,maxrow)\n            v123.iat[xi,3]=yi+1\n        \nlabels=v123.label.values\n        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_tsne_pca(csc_matrix(v.T),labels,'tn')  \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EMBEDDING"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv',encoding='utf-8')\ntest = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv',encoding='utf-8')\ntest['sentiment']='unknown'\nfrom sklearn import preprocessing\nle = preprocessing.LabelEncoder()\nfrom sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(ngram_range=(1, 2))\n\nXc = vectorizer.fit_transform( (train.text.fillna(' ') +' '+train.sentiment).append(test.text.fillna(' ')) )\n#XcS=Xc.T.multiply(np.exp( np.array( le.fit_transform(train.sentiment) ) ) )\n#XcS=csc_matrix(XcS.T)\nplot_tsne_pca(Xc, le.fit_transform(train.sentiment.append(test.sentiment) ) +1,'tt')  \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"le.inverse_transform([ 0, 1, 2,3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.sparse.linalg import svds, eigs\nfrom scipy.sparse import csc_matrix\n\nu,s,v=svds(Xc*1.0,k=30)\nplot_tsne_pca(csc_matrix(u*s), le.fit_transform(train.sentiment.append(test.sentiment) )+1,'tn')  \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics.pairwise import cosine_similarity\n#cosine_similarity(v.T,\nv123=np.concatenate((v.T[vectorizer.vocabulary_['neutral']],v.T[vectorizer.vocabulary_['positive']],v.T[vectorizer.vocabulary_['negative']])).reshape(3,-1)\nv123=pd.DataFrame(cosine_similarity(v.T,v123))\n#labels=\nv123['label']=0\nfor xi in range (len(v123)):\n    maxrow=v123.iloc[xi,:3].max()\n    for yi in range(3):\n        if v123.iloc[xi,yi]>0.335:\n            #print(xi,yi,maxrow)\n            v123.iat[xi,3]=yi+1\n        \nlabels=v123.label.values\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Xn = vectorizer.transform(train.selected_text[train.sentiment=='negative'].fillna(' ') )\nXwn=Xn.sum(axis=0)\nXp = vectorizer.transform(train.selected_text[train.sentiment=='positive'].fillna(' ') )\nXwp=Xp.sum(axis=0)\n#[xi for xi in range(26000) if Xwn[:,xi]>Xwp[:,xi] and Xwp[:,xi]>0]\nXo = vectorizer.transform(train.selected_text[train.sentiment=='neutral'].fillna(' ') )\nXwo=Xo.sum(axis=0)\nXwo","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.sparse import csc_matrix\n\nus=np.dot(Xc,csc_matrix(v.T))\nplot_tsne_pca(us, le.fit_transform(train.sentiment.append(test.sentiment) )+1,'tn')  \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# find cosine similarity of text vectors"},{"metadata":{"trusted":true},"cell_type":"code","source":"usim=pd.DataFrame(u)\nusim['label']=le.fit_transform(train.sentiment.append(test.sentiment) )\nugroup=usim.groupby('label').mean()\nfrom sklearn.metrics.pairwise import cosine_similarity\nprint(u.shape,ugroup.shape)\nucosim=pd.DataFrame( cosine_similarity(u,ugroup) )\nucosim['label']=le.fit_transform(train.sentiment.append(test.sentiment) )\nucosim['max']=ucosim.iloc[:,:3].idxmax(axis=1)\n\nucosim\nprint(le.inverse_transform([ 0, 1, 2,3]))\nucosim.groupby(['label','max']).count()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#totalu=pd.DataFrame(u)\nkmea=MiniBatchKMeans(n_clusters=3, init_size=10, batch_size=100, random_state=20).fit_transform(u)\nkmea=pd.DataFrame(kmea)\nkmea['label']=le.fit_transform(train.sentiment.append(test.sentiment) )\nkmea['max']=kmea.iloc[:,:3].idxmax(axis=1)\nkmea.groupby(['label','max']).count()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# find relevant texts"},{"metadata":{"trusted":true},"cell_type":"code","source":"subm = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/sample_submission.csv',encoding='utf-8')\nsubm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# searching kmeans keywords"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntrain = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv',encoding='utf-8')\ntrain[\"text\"] = train[\"text\"].astype(str)\ntrain[\"text\"] = train[\"text\"].str.lower()\ntrain[\"selected_text\"] = train[\"selected_text\"].astype(str)\ntrain[\"selected_text\"] = train[\"selected_text\"].str.lower()\ntrain[\"sentiment\"] = train[\"sentiment\"].astype(str)\ntrain[\"sentiment\"] = train[\"sentiment\"].str.lower()\ndrop_empty_rows(train)\n\ntest = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv',encoding='utf-8')\ntest[\"text\"] = test[\"text\"].astype(str)\ntest[\"text\"] = test[\"text\"].str.lower()\ntest[\"sentiment\"] = test[\"sentiment\"].astype(str)\ntest[\"sentiment\"] = test[\"sentiment\"].str.lower()\ndrop_empty_rows(test)\n\ncommon_cols = list(set.intersection(*(set(df.columns) for df in [train,test])))\ncombined = pd.concat([df[common_cols] for df in [train,test]], ignore_index=True)\n# applying groupby() function to \n# group the data on team value. \ngp = combined.groupby('sentiment') \n  \n# Let's print the first entries \n# in all the groups formed. \nfor name, group in gp: \n    print(name) \n    print(group) \n    print(len(group)) \n\noptimal_clusters = 150\nfor name, group in gp: \n    tfidf.fit(group.text)\n    text = tfidf.transform(group.text)   \n    find_optimal_clusters(text, optimal_clusters,name,init_size,batch_size)\n\nkmeans_collection = {}\nn_clusters = 150           \nfor name, group in gp: \n    tfidf.fit(group.text)\n    text = tfidf.transform(group.text) \n    kmeans = MiniBatchKMeans(n_clusters=n_clusters, init_size=init_size, batch_size=batch_size, random_state=20)\n    kmeans.fit(text)\n    clusters = kmeans.predict(text) \n    plot_tsne_pca(text, clusters,name)  \n    get_top_keywords(text, clusters, tfidf.get_feature_names(), 5,name)\n    kmeans_collection[name.lower()] = kmeans\n\n\ndef get_keywords(line,data, clusters, labels, n_terms,column):\n    df = pd.DataFrame(data.todense()).groupby(clusters).mean()\n    selected_text = []\n    for i,r in df.iterrows():\n        #print('\\nCluster {} column: {}'.format(i,column))\n        #key_words = ','.join(set([labels[t] for t in np.argsort(r)[-n_terms:]]))\n        #print(key_words)\n        n_terms = len(labels)\n        key_words = ','.join(set([labels[t] for t in np.argsort(r)[-n_terms:]]))\n        for word in line.strip().split():\n            for kw in key_words:\n                word = word.strip()\n                kw = kw.strip()\n                ld = 1.0-levenshtein_distance(word,kw)/max(len(word),len(kw))\n                if ld > 0.1:\n                    selected_text.append(word) \n                    break\n            #if word in key_words:\n            #    selected_text.append(word)\n    return \" \".join(selected_text)\n\n            \ndef jaccard(str1, str2): \n    if str1 and str2:\n        a = set(str1.strip().split()) \n        b = set(str2.strip().split())\n        c = a.intersection(b)\n        return float(len(c)) / (len(a) + len(b) - len(c))\n    else:\n        return 0.0\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"@ the RAKE method"},{"metadata":{"trusted":true},"cell_type":"code","source":"# RAKE( ' '.join(list( train.text.fillna(' ').values) ) ,stopwordpattern)\n\n#RAKE(train.text[1],stopwordpattern)\n    # Split text into sentences\n    sentenceList = split_sentences(train.text[1])\n\n    # generate candidate keywords\n    phraseList = generate_candidate_keywords(sentenceList, stopwordpattern)\n    print('wordgroups',phraseList)\n\n    # calculate individual word scores\n    wordscores = calculate_word_scores(phraseList)\n    print('wordscore',wordscores)\n\n    # generate candidate keyword scores\n    keywordcandidates = generate_candidate_keyword_scores(phraseList, wordscores)\n    print ('keyword',keywordcandidates)\n\n    sortedKeywords = sorted(keywordcandidates.items(), key=operator.itemgetter(1), reverse=True)\n    print('sort keyword',sortedKeywords)\n\n    totalKeywords = len(sortedKeywords)\n    print('nr keywords',totalKeywords)\n    print ('top 30%',sortedKeywords[0:(int(totalKeywords / 3) )])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"cosine_similarity(v.T[vectorizer.vocabulary_['sooo sad']].reshape(1,-1),v123)"},{"metadata":{"trusted":true},"cell_type":"code","source":"v123=np.concatenate((v.T[vectorizer.vocabulary_['neutral']],v.T[vectorizer.vocabulary_['positive']],v.T[vectorizer.vocabulary_['negative']])).reshape(3,-1)\n\nfor ti in range(1,10):\n    #keywordkandidates\n    sentenceList = split_sentences(train.text[ti])\n    phraseList = generate_candidate_keywords(sentenceList, stopwordpattern)\n    wordscores = calculate_word_scores(phraseList)\n    keywordcandidates = generate_candidate_keyword_scores(phraseList, wordscores)\n    print(train.selected_text[ti:ti+1].values)\n    costemp=[]\n    wordtemp=[]\n    #keywordkandidates: sekect highest positive - negative or neutral, the one with the highest cosinsimilarity\n    for xi in keywordcandidates:\n        try:\n            wordtemp.append(xi)\n            costemp.append( np.max( cosine_similarity(v.T[vectorizer.vocabulary_[xi]].reshape(1,-1),v123)  )  )\n        except:\n            pass\n    try:\n        print(wordtemp[np.argmax(np.array(costemp))] )\n    except:\n        print(train.text[ti])\n#tfidfsp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"v123=np.concatenate((v.T[vectorizer.vocabulary_['neutral']],v.T[vectorizer.vocabulary_['positive']],v.T[vectorizer.vocabulary_['negative']])).reshape(3,-1)\nsubm['selected_text']=' '\nfor ti in range(2,len(test)):\n    #keywordkandidates\n    sentenceList = split_sentences(test.text[ti])\n    phraseList = generate_candidate_keywords(sentenceList, stopwordpattern)\n    wordscores = calculate_word_scores(phraseList)\n    keywordcandidates = generate_candidate_keyword_scores(phraseList, wordscores)\n    costemp=[]\n    wordtemp=[]\n    #print(test.text[ti])\n    #keywordkandidates: sekect highest positive - negative or neutral, the one with the highest cosinsimilarity\n    for xi in keywordcandidates:\n        try:\n            costemp.append( np.max( cosine_similarity(v.T[vectorizer.vocabulary_[xi]].reshape(1,-1),v123)  )  )\n            wordtemp.append(xi)\n\n        except:\n            pass\n    try:\n        #print(ti,xi,costemp,wordtemp[np.argmax(np.array(costemp))],wordtemp,subm.iloc[ti,0])\n        subm.iat[ti,1]=wordtemp[np.argmax(np.array(costemp))] \n    except:\n        subm.iat[ti,1]=test.text[ti]\n#tfidfsp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subm.to_csv(\"submission.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"from sklearn.metrics.pairwise import cosine_similarity\n#cosine_similarity(v.T,\nv123=np.concatenate((v.T[vectorizer.vocabulary_['neutral']],v.T[vectorizer.vocabulary_['positive']],v.T[vectorizer.vocabulary_['negative']])).reshape(3,-1)\nv123=pd.DataFrame(cosine_similarity(v.T,v123))\n#labels=\nv123['label']=0\nfor xi in range (len(v123)):\n    maxrow=v123.iloc[xi,:3].max()\n    for yi in range(3):\n        if v123.iloc[xi,yi]>0.335:\n            #print(xi,yi,maxrow)\n            v123.iat[xi,3]=yi+1\n        \nlabels=v123.label.values\n"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"scores = pd.DataFrame(columns = [\"sentiment\",\"text\",\"selected_text\",\"result\",\"jaccard_score\"])\ncount = 1\nmax_count = len(train)\nprint_every = 1000\ngp = train.groupby('sentiment') \nfor name, group in gp:\n    for query,selected_text in zip(group.text,group.selected_text):\n        text = tfidf.transform([query])      \n        cluster = kmeans_collection[name.lower()].predict(text)\n        result = get_keywords(selected_text,text,cluster,tfidf.get_feature_names(), 10,name)\n        js = jaccard(selected_text,result)\n        new_row = {'sentiment':name,'text':query, 'selected_text':selected_text, 'result':result, 'jaccard_score':js}\n        scores = scores.append(new_row, ignore_index=True)\n        if (count % print_every == 0):\n            print(\"Train Processed:\",count)\n        count = count + 1\n        if max_count < count:\n            break\nplt.figure()\nscores.sort_values(by=['jaccard_score'],inplace=True,ascending=True)    \nscores[\"jaccard_score\"].plot.kde()\nplt.hist(scores[\"jaccard_score\"], color = 'blue', edgecolor = 'black')\nplt.show()\nprint(scores[\"jaccard_score\"].mean())\n\nsample_submission = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/sample_submission.csv',encoding='utf-8')\nsample_submission[\"selected_text\"] = sample_submission[\"selected_text\"].astype(str)\nfor index in range(len(test)):\n    text = tfidf.transform([test.iloc[index]['text']])      \n    cluster = kmeans_collection[test.iloc[index]['sentiment'].lower()].predict(text)\n    result = get_keywords(test.iloc[index]['text'],text,cluster,tfidf.get_feature_names(), 10,name)\n    sample_submission.at[index,'selected_text'] = result\n    if (index % print_every == 0):\n        print(\"Result:\" ,result)\n        print(\"Test Processed:\",index)\n        \nsample_submission.to_csv(\"submission.csv\",index=False)"}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":4}