{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport re\nimport nltk\nimport string\nimport warnings\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize\nfrom nltk import pos_tag\nfrom nltk.corpus import conll2000\nfrom nltk.corpus import brown\nfrom nltk.stem.wordnet import WordNetLemmatizer\nimport string \nimport plotly.graph_objs as go\nimport plotly.express as px\nimport matplotlib.pyplot as pt\nimport seaborn as sns\n\n%matplotlib inline\n\nfrom collections import defaultdict\nfrom collections import Counter\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\nimport transformers\nfrom tqdm import tqdm\nimport spacy\nimport random\nfrom spacy.util import compounding\nfrom spacy.util import minibatch\n\nimport torch \nfrom torch import nn\nfrom torch.nn import functional as F\nimport torch.optim as optim\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom plotly import tools\nfrom plotly.subplots import make_subplots\n\n\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom sklearn.model_selection import StratifiedKFold \nfrom transformers import *\nimport tokenizers\n\n\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv')\ntest_data = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv')\nsubmission_data = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <span style='color:purple'>Exploration </span>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_data.shape, test_data.shape, submission_data.shape)\ndisplay(train_data.head())\ndisplay(test_data.head())\ndisplay(submission_data.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <span style = 'color:purple'>Objective of this competition is to predict the 'selected_text' for given text and corresponding sentiment for test dataset. </span>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.isna().any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.isna().any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.loc[train_data['selected_text'].isnull()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.loc[train_data['text'].isnull()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Same row has missing text as well as selected_text so let's remove this row.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing the row from train data with missing value\ntrain_data_new = train_data.drop([314])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_new.isna().any()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <span style= 'color: purple'>Distribution of Sentiments </span>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['sentiment'].unique() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def count_senti(df):\n    sum = df['sentiment'].value_counts()\n    percent = df['sentiment'].value_counts(normalize = True)\n    return pd.concat([sum, percent], axis = 1, keys = ['Sum', 'Percent'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Sentiments for train data\")\nsenti_train = count_senti(train_data)\ndisplay(senti_train)\nprint(\"Sentiments for test data\")\nsenti_test = count_senti(test_data)\ndisplay(senti_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"colors = ['purple', 'green', 'red']\nfig = make_subplots(rows = 1, cols = 2, specs = [[{\"type\":\"pie\"}, {\"type\":\"pie\"}]])\nfig.add_trace(go.Pie(labels = list(senti_train.index),\n                     values = list(senti_train.Sum.values), hoverinfo = 'label+percent', \n                     textinfo = 'value+percent',\n                     marker = dict(colors = colors)), row = 1, col = 1)\nfig.add_trace(go.Pie(labels = list(senti_test.index), \n                     values = list(senti_test.Sum.values), hoverinfo = 'label+percent', \n                     textinfo = 'value+percent',\n                     marker = dict(colors =colors)), row = 1, col = 2)\nfig.update_layout(title_text = \"Train and Test Sentiment Percentages\", title_x = 0.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Distribution of netural, positive and negative sentiments are about similar distribution on both train and test data.<br>Neutral sentiments are in majority in both train and test data.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## <span style='color:purple'>Majority words in text </span>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### <span style='color:purple'>Cleaning text before counting words in text</span>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = train_data.copy()\ndf_test = test_data.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[df_train['sentiment'] == 'positive']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[df_train['sentiment'] == 'neutral']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[df_train['sentiment'] == 'negative']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def text_cleaning(txt):\n    \"\"\"\n    Convert given text to lower case, remove all non-word characters, digits, links.\n    \"\"\"\n    txt = str(txt).lower()\n    txt= re.sub('https?://\\S+|www\\.\\S+', '', txt)\n    txt = re.sub('\\[.*?\\]', '', txt)\n    txt = re.sub('<.*?>+', '', txt)\n    txt = re.sub('[%s]' % re.escape(string.punctuation), ' ', txt)\n    txt = re.sub('\\n', '', txt)\n    txt = re.sub('\\w*\\d\\w*', '', txt)\n    return txt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def rm_stopword(text):\n    text_tokens = word_tokenize(text)\n    stop_list = stopwords.words('english')\n    new_text = [word for word in text_tokens if word not in stop_list]\n    final_text = ' '.join(new_text)\n    return final_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Clean text and selected_text\ndf_train['clean_text'] = df_train['text'].apply(lambda x: text_cleaning(x))\ndf_train['clean_selected_text'] = df_train['selected_text'].apply(lambda x: text_cleaning(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head(50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove stopwords from both text and selected text\ndf_train['clean_text'] = df_train['clean_text'].apply(lambda x: rm_stopword(x))\ndf_train['clean_selected_text'] = df_train['clean_selected_text'].apply(lambda x: rm_stopword(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def count_words(df,feature, senti):\n    word_list = []\n    for x in df[df['sentiment'] == senti][feature].str.split():\n        for i in x:\n            word_list.append(i)\n    cnt = Counter()\n    for word in word_list:\n        cnt[word] +=1\n    df_cnt = pd.DataFrame(cnt.most_common(10))\n    df_cnt.columns = ['Freq_words', 'Freq']\n    df_cnt.style.background_gradient(cmap = 'purple')\n    return df_cnt ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##  <span style='color:green'>Most frequent positive words</span>\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"positive_top10 = count_words(df_train,'clean_text','positive')\ndisplay(positive_top10)\n\nfig = px.bar(positive_top10, x = 'Freq', y = 'Freq_words', title = 'Top 10 frequent postive words',\n            orientation = 'h', width = 600, height = 600, color = 'Freq_words')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <span style='color:purple'> Most frequent neutral words</span>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"neutral_top10 = count_words(df_train,'clean_text','neutral')\ndisplay(neutral_top10)\n\nfig = px.bar(neutral_top10, x = 'Freq', y = 'Freq_words', title = 'Top 10 frequent neutral words',\n            orientation = 'h', width = 600, height = 600, color = 'Freq_words')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## <span style='color:red'>Most frequent negative words</span>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"negative_top10 = count_words(df_train,'clean_text','negative')\ndisplay(negative_top10)\n\nfig = px.bar(negative_top10, x = 'Freq', y = 'Freq_words', title = 'Top 10 frequent negative words',\n            orientation = 'h', width = 600, height = 600, color = 'Freq_words')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### There are words which are highly frequent in all three sentiments.<br> Examples: go, get, work etc. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def pos_freq(df,feature, senti):\n    total_pos_count = []\n    for x in df[df['sentiment'] == senti][feature].str.split():\n        pos_count = nltk.pos_tag(x)\n        total_pos_count.extend(pos_count) \n    tag_freq = nltk. FreqDist(tag for (word, tag) in total_pos_count)\n    ans = tag_freq.most_common()[0:10]\n    return ans ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pos_freq(df_train,'text', 'negative')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pos_freq(df_train,'text', 'positive')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# roBERTa Model \n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN = 198\nTRAIN_BATCH_SIZE = 32\nVALID_BATCH_SIZE = 8\nEPOCHS = 10\nROBERTA_PATH = \"../input/roberta-base/\"\nMODEL_PATH = \"model.bin\"\nTRAINING_FILE = \"../input/train.csv\"\n\nTOKENIZER = tokenizers.ByteLevelBPETokenizer(\n    vocab_file=f\"{ROBERTA_PATH}/vocab.json\", \n    merges_file=f\"{ROBERTA_PATH}/merges.txt\", \n    lowercase=True,\n    add_prefix_space=True\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TweetModel(nn.Module):\n    def __init__(self):\n        super(TweetModel, self).__init__()\n        self.bert = transformers.RobertaModel.from_pretrained(ROBERTA_PATH)\n        self.l0 = nn.Linear(768, 2)\n    \n    def forward(self, ids, mask, token_type_ids):\n        sequence_output, pooled_output = self.bert(\n            ids, \n            attention_mask=mask\n        )\n        logits = self.l0(sequence_output)\n        start_logits, end_logits = logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n\n        return start_logits, end_logits","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda\")\nmodel = TweetModel()\nmodel.to(device)\nmodel = nn.DataParallel(model)\nmodel.load_state_dict(torch.load(\"../input/roberta-tweet-model/model.bin\"))\nmodel.eval()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TweetDataset:\n    def __init__(self, tweet, sentiment, selected_text):\n        self.tweet = tweet\n        self.sentiment = sentiment\n        self.selected_text = selected_text\n        self.tokenizer = TOKENIZER\n        self.max_len = MAX_LEN\n    \n    def __len__(self):\n        return len(self.tweet)\n    \n    def __getitem__(self, item):\n        # For Roberta, CLS = <s> and SEP = </s>\n        # Multiple strings: '<s>hi, my name is abhishek!!!</s></s>whats ur name</s>'\n        # id for <s>: 0\n        # id for </s>: 2\n    \n        tweet = \" \" + \" \".join(str(self.tweet[item]).split())\n        selected_text = \" \" + \" \".join(str(self.selected_text[item]).split())\n        \n        len_st = len(selected_text)\n        idx0 = -1\n        idx1 = -1\n        for ind in (i for i, e in enumerate(tweet) if e == selected_text[0]):\n            if tweet[ind: ind+len_st] == selected_text:\n                idx0 = ind\n                idx1 = ind + len_st\n                break\n        #print(f\"idx0: {idx0}\")\n        #print(f\"idx1: {idx1}\")\n        #print(f\"len_st: {len_st}\")\n        #print(f\"idxed tweet: {tweet[idx0: idx1]}\")\n\n        char_targets = [0] * len(tweet)\n        if idx0 != -1 and idx1 != -1:\n            for ct in range(idx0, idx1):\n                # if tweet[ct] != \" \":\n                char_targets[ct] = 1\n\n        #print(f\"char_targets: {char_targets}\")\n\n        tok_tweet = self.tokenizer.encode(tweet)\n        tok_tweet_tokens = tok_tweet.tokens\n        tok_tweet_ids = tok_tweet.ids\n        tok_tweet_offsets = tok_tweet.offsets\n        \n         #print(tweet)\n        #print(selected_text)\n        #print(tok_tweet_tokens)\n        #print(f\"tok_tweet.offsets= {tok_tweet.offsets}\")\n        \n        targets = [0] * len(tok_tweet_ids)\n        target_idx = []\n        for j, (offset1, offset2) in enumerate(tok_tweet_offsets):\n            #print(\"**************\")\n            #print(offset1, offset2)\n            #print(tweet[offset1: offset2])\n            #print(char_targets[offset1: offset2])\n            #print(\"\".join(tok_tweet_tokens)[offset1: offset2])\n            #print(\"**************\")\n            if sum(char_targets[offset1: offset2]) > 0:\n                targets[j] = 1\n                target_idx.append(j)\n\n        #print(f\"targets= {targets}\")\n        #print(f\"target_idx= {target_idx}\")\n\n        #print(tok_tweet_tokens[target_idx[0]])\n        #print(tok_tweet_tokens[target_idx[-1]])\n        \n        targets_start = [0] * len(targets)\n        targets_end = [0] * len(targets)\n\n        non_zero = np.nonzero(targets)[0]\n        if len(non_zero) > 0:\n            targets_start[non_zero[0]] = 1\n            targets_end[non_zero[-1]] = 1\n        \n        #print(targets_start)\n        #print(targets_end)\n        #print(tok_tweet_tokens)\n        #print([x for jj, x in enumerate(tok_tweet_tokens) if targets_start[jj] == 1])\n        #print([x for jj, x in enumerate(tok_tweet_tokens) if targets_end[jj] == 1])\n        \n\n        # check padding:\n        # <s> pos/neg/neu </s> </s> tweet </s>\n        if len(tok_tweet_tokens) > self.max_len - 5:\n            tok_tweet_tokens = tok_tweet_tokens[:self.max_len - 5]\n            tok_tweet_ids = tok_tweet_ids[:self.max_len - 5]\n            targets_start = targets_start[:self.max_len - 5]\n            targets_end = targets_end[:self.max_len - 5]\n        \n        # positive: 1313\n        # negative: 2430\n        # neutral: 7974\n        \n        sentiment_id = {\n            'positive': 1313,\n            'negative': 2430,\n            'neutral': 7974\n        }\n\n        tok_tweet_ids = [0] + [sentiment_id[self.sentiment[item]]] + [2] + [2] + tok_tweet_ids + [2]\n        targets_start = [0] + [0] + [0] + [0] + targets_start + [0]\n        targets_end = [0] + [0] + [0] + [0] + targets_end + [0]\n        token_type_ids = [0, 0, 0, 0] + [0] * (len(tok_tweet_ids) - 5) + [0]\n        mask = [1] * len(token_type_ids)\n\n        #print(\"Before padding\")\n        #print(f\"len(tok_tweet_ids)= {len(tok_tweet_ids)}\")\n        #print(f\"len(targets_start)= {len(targets_start)}\")\n        #print(f\"len(targets_end)= {len(targets_end)}\")\n        #print(f\"len(token_type_ids)= {len(token_type_ids)}\")\n        #print(f\"len(mask)= {len(mask)}\")\n\n        padding_length = self.max_len - len(tok_tweet_ids)\n        \n        tok_tweet_ids = tok_tweet_ids + ([1] * padding_length)\n        mask = mask + ([0] * padding_length)\n        token_type_ids = token_type_ids + ([0] * padding_length)\n        targets_start = targets_start + ([0] * padding_length)\n        targets_end = targets_end + ([0] * padding_length)\n        \n        #print(\"After padding\")\n        #print(f\"len(tok_tweet_ids)= {len(tok_tweet_ids)}\")\n        #print(f\"len(targets_start)= {len(targets_start)}\")\n        #print(f\"len(targets_end)= {len(targets_end)}\")\n        #print(f\"len(token_type_ids)= {len(token_type_ids)}\")\n        #print(f\"len(mask)= {len(mask)}\")\n\n        return {\n            'ids': torch.tensor(tok_tweet_ids, dtype=torch.long),\n            'mask': torch.tensor(mask, dtype=torch.long),\n            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n            'targets_start': torch.tensor(targets_start, dtype=torch.float),\n            'targets_end': torch.tensor(targets_end, dtype=torch.float),\n            'padding_len': torch.tensor(padding_length, dtype=torch.long),\n            'orig_tweet': self.tweet[item],\n            'orig_selected': self.selected_text[item],\n            'sentiment': self.sentiment[item]\n        }\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1_test = pd.read_csv(\"../input/tweet-sentiment-extraction/test.csv\")\ndf1_test.loc[:, \"selected_text\"] = df1_test.text.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dataset = TweetDataset(\n        tweet=df1_test.text.values,\n        sentiment=df1_test.sentiment.values,\n        selected_text=df1_test.selected_text.values\n    )\n\ndata_loader = torch.utils.data.DataLoader(\n    test_dataset,\n    shuffle=False,\n    batch_size=VALID_BATCH_SIZE,\n    num_workers=1\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_outputs = []\nfin_outputs_start = []\nfin_outputs_end = []\nfin_padding_lens = []\nfin_orig_selected = []\nfin_orig_sentiment = []\nfin_orig_tweet = []\nfin_tweet_token_ids = []\n\nwith torch.no_grad():\n    tk0 = tqdm(data_loader, total=len(data_loader))\n    for bi, d in enumerate(tk0):\n        ids = d[\"ids\"]\n        token_type_ids = d[\"token_type_ids\"]\n        mask = d[\"mask\"]\n        padding_len = d[\"padding_len\"]\n        sentiment = d[\"sentiment\"]\n        orig_selected = d[\"orig_selected\"]\n        orig_tweet = d[\"orig_tweet\"]\n        targets_start = d[\"targets_start\"]\n        targets_end = d[\"targets_end\"]\n\n        ids = ids.to(device, dtype=torch.long)\n        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n        mask = mask.to(device, dtype=torch.long)\n        targets_start = targets_start.to(device, dtype=torch.float)\n        targets_end = targets_end.to(device, dtype=torch.float)\n\n        outputs_start, outputs_end = model(\n            ids=ids,\n            mask=mask,\n            token_type_ids=token_type_ids\n        )\n        \n        fin_outputs_start.append(torch.sigmoid(outputs_start).cpu().detach().numpy())\n        fin_outputs_end.append(torch.sigmoid(outputs_end).cpu().detach().numpy())\n        fin_padding_lens.extend(padding_len.cpu().detach().numpy().tolist())\n        fin_tweet_token_ids.append(ids.cpu().detach().numpy().tolist())\n\n        fin_orig_sentiment.extend(sentiment)\n        fin_orig_selected.extend(orig_selected)\n        fin_orig_tweet.extend(orig_tweet)\n        \n        \n\nfin_outputs_start = np.vstack(fin_outputs_start)\nfin_outputs_end = np.vstack(fin_outputs_end)\nfin_tweet_token_ids = np.vstack(fin_tweet_token_ids)\njaccards = []\nthreshold = 0.2\nfor j in range(fin_outputs_start.shape[0]):\n    target_string = fin_orig_selected[j]\n    padding_len = fin_padding_lens[j]\n    sentiment_val = fin_orig_sentiment[j]\n    original_tweet = fin_orig_tweet[j]\n\n    if padding_len > 0:\n        mask_start = fin_outputs_start[j, 4:-1][:-padding_len] >= threshold\n        mask_end = fin_outputs_end[j, 4:-1][:-padding_len] >= threshold\n        tweet_token_ids = fin_tweet_token_ids[j, 4:-1][:-padding_len]\n    else:\n        mask_start = fin_outputs_start[j, 4:-1] >= threshold\n        mask_end = fin_outputs_end[j, 4:-1] >= threshold\n        tweet_token_ids = fin_tweet_token_ids[j, 4:-1][:-padding_len]\n        \n        \n        \n    mask = [0] * len(mask_start)\n    idx_start = np.nonzero(mask_start)[0]\n    idx_end = np.nonzero(mask_end)[0]\n    if len(idx_start) > 0:\n        idx_start = idx_start[0]\n        if len(idx_end) > 0:\n            idx_end = idx_end[0]\n        else:\n            idx_end = idx_start\n    else:\n        idx_start = 0\n        idx_end = 0\n\n    for mj in range(idx_start, idx_end + 1):\n        mask[mj] = 1\n\n    output_tokens = [x for p, x in enumerate(tweet_token_ids) if mask[p] == 1]\n\n    filtered_output = TOKENIZER.decode(output_tokens)\n    filtered_output = filtered_output.strip().lower()\n\n    all_outputs.append(filtered_output.strip())\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample = pd.read_csv(\"../input/tweet-sentiment-extraction/sample_submission.csv\")\nsample.loc[:, 'selected_text'] = all_outputs\nsample.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}