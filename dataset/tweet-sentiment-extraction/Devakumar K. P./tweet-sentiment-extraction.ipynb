{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Objective\n> ### Objetive is to construct a model that can look at the labeled sentiment for a given tweet and figure out what word or phrase best supports it."},{"metadata":{},"cell_type":"markdown","source":"# Libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# basic\n# =====\nimport numpy as np \nimport pandas as pd\n\n# Visualizations\n# ==============\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# import plotly_express as px\n\n\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom transformers import *\nimport tokenizers\n\nimport tensorflow as tf\nimport tensorflow.keras.backend as K","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# machine learning\n# ================\n# from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder\n# from sklearn.model_selection import cross_val_score, train_test_split\n# from sklearn.linear_model import RidgeClassifier, LogisticRegression\n# from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n# from sklearn.neighbors import KNeighborsClassifier\n# from sklearn.svm import SVC\n# from sklearn.tree import DecisionTreeClassifier\n# from sklearn.ensemble import RandomForestClassifier\n# from sklearn.metrics import accuracy_score, confusion_matrix\n\n# nlp\n# ===\n# from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n# from nltk.corpus import stopwords\n# from wordcloud import WordCloud, STOPWORDS \n# import nltk\n# import spacy\n\n# deep learning\n# =============\n# import tensorflow as tf\n# from tensorflow.keras.preprocessing.text import Tokenizer\n# from tensorflow.keras.preprocessing.sequence import pad_sequences","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# palette\n# =======\ntw_pal = ['#1DCAFF', '#292F33', '#E0245E', '#E1E8ED', '#CCD6DD', '#E1E8ED']\ntw_pal_1 = ['#1DCAFF', '#E0245E']\n\nsns.set_style(\"whitegrid\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# list files\n# ! ls ../input/tweet-sentiment-extraction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load data\ntrain = pd.read_csv(\"../input/tweet-sentiment-extraction/train.csv\")\ntest = pd.read_csv(\"../input/tweet-sentiment-extraction/test.csv\")\nsample = pd.read_csv(\"../input/tweet-sentiment-extraction/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# random rows from training dataset\ntrain.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# random rows from test dataset\ntest.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# random rows from sample submission files\ntest.sample(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# shape of the dataset\nprint(train.shape)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# info of the dataset\nprint(train.info())\nprint(\"\")\nprint(test.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# describe the dataset\nprint(train.describe())\nprint(\"\")\nprint(test.describe())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Missing and unique values"},{"metadata":{"trusted":true},"cell_type":"code","source":"# number of missing values\nprint(train.isna().sum())\nprint(\"\")\nprint(test.isna().sum())\nprint(\"\")\nprint(sample.isna().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# row with missing values\ntrain[train.isna().any(axis=1)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# droping rows with missing value\ntrain = train.dropna()\n\n# total missing values after dropping rows with missing values\nprint(train.isna().sum().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# no. of unique values()\nprint(train.nunique())\nprint('')\nprint(test.nunique())\nprint('')\nprint(sample.nunique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# EDA"},{"metadata":{},"cell_type":"markdown","source":"### Sentiment Distribution in train and test dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# figure properties\nfig, axes = plt.subplots(1, 2, figsize=(10, 4))\nfig.subplots_adjust(wspace=0.4)\n\n# train dataset distribution\nsns.countplot(x='sentiment', data=train, order=['positive', 'neutral', 'negative'], palette=tw_pal, ax=axes[0])\naxes[0].set_title('Train dataset')\naxes[0].set_xlabel('')\naxes[0].set_ylabel('')\n\n# test dataset distribution\nsns.countplot(x='sentiment', data=test, order=['positive', 'neutral', 'negative'], palette=tw_pal, ax=axes[1])\naxes[1].set_title('Test dataset')\naxes[1].set_xlabel('')\naxes[1].set_ylabel('')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Top selected words"},{"metadata":{"trusted":true},"cell_type":"code","source":"# table\ntemp = train.groupby(['selected_text', 'sentiment']).count()\ntemp = temp.reset_index().sort_values('textID', ascending=False)\n# temp.head()\n\n# plot \nplt.figure(figsize=(10, 6))\nax = sns.barplot(data=temp.head(20), x='textID', y='selected_text', hue='sentiment', dodge=False, palette=tw_pal_1)\nax.set_ylabel('')\nax.set_xlabel('')\nax.set_title('Top 20 selected text')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Top selected words from each sentiment"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(1, 3, figsize=(18, 4))\nfig.subplots_adjust(wspace=0.4)\n\nsns.barplot(data=temp[temp['sentiment']=='positive'].head(6), x='textID', y='selected_text', \n            dodge=False, color=tw_pal[0], ax=axes[0])\naxes[0].set_title('Top positive selected text')\naxes[0].set_ylabel('')\naxes[0].set_xlabel('')\n\nsns.barplot(data=temp[temp['sentiment']=='neutral'].head(6), x='textID', y='selected_text', \n            dodge=False, color=tw_pal[1], ax=axes[1])\naxes[1].set_title('Top neutral selected text')\naxes[1].set_ylabel('')\naxes[1].set_xlabel('')\n\nsns.barplot(data=temp[temp['sentiment']=='negative'].head(6), x='textID', y='selected_text', \n            dodge=False, color=tw_pal[2], ax=axes[2])\naxes[2].set_title('Top negative selected text')\naxes[2].set_ylabel('')\naxes[2].set_xlabel('')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Mean no. of characters, words, sentences"},{"metadata":{"trusted":true},"cell_type":"code","source":"# utility function\n# =================\n\ndef plot_bar(col, col_names):\n    fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n    fig.subplots_adjust(wspace=0.4)\n    for ind, val in enumerate(col):\n        sns.barplot(x='sentiment', y=val, palette=tw_pal, data=train, ax=ax[ind], order=['positive', 'neutral', 'negative'])\n        ax[ind].set_title(col_names[ind])\n        ax[ind].set_ylabel('')\n        ax[ind].set_xlabel('')\n    plt.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# no. of characters\n# =================\n\ntrain['no_chars_text'] = train['text'].apply(len)\ntrain['no_chars_sel_text'] = train['selected_text'].apply(len)\n\ncol = ['no_chars_text', 'no_chars_sel_text']\ncol_names = ['Mean no. of chars in text', 'Mean no. of chars in selected text']\n\nplot_bar(col, col_names)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# no. of words\n# ============\n\ntrain['no_words'] = train['text'].str.split().apply(len)\ntrain['no_words_sel_text'] = train['selected_text'].str.split().apply(len)\n\ncol = ['no_words', 'no_words_sel_text']\ncol_names = ['Mean no. of words in text', 'Mean no. of words in selected text']\n\nplot_bar(col, col_names)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# no. of sentences\n# ================\n\ntrain['no_sent'] = train['text'].str.split('.').apply(len)\ntrain['no_sent_sel_text'] = train['selected_text'].str.split('.').apply(len)\n\ncol = ['no_sent', 'no_sent_sel_text']\ncol_names = ['Mean no. of sentances in text', 'Mean no. of sentances in selected text']\n\nplot_bar(col, col_names)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Mean no. of hashtags, mentions, web addresses"},{"metadata":{"trusted":true},"cell_type":"code","source":"# utility function\n# =================\n\ndef hash_count(tweet):\n    w = tweet.split()\n    return len([word for word in w if word.startswith('#')])\n\ndef mention_count(tweet):\n    w = tweet.split()\n    return len([word for word in w if word.startswith('@')])\n\ndef web_add(tweet):\n    w = tweet.split()\n    return len([word for word in w if word.startswith('http')])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# no. of hashtags\n# ===============\n\ntrain['no_hashtags'] = train['text'].apply(hash_count)\ntrain['no_hashtags_sel_text'] = train['selected_text'].apply(hash_count)\n\ncol = ['no_hashtags', 'no_hashtags_sel_text']\ncol_names = ['Mean no. of hashtags in text', 'Mean no. of hashtags in selected text']\n\nplot_bar(col, col_names)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# no. of mentions\n# ===============\n\ntrain['no_mentions'] = train['text'].apply(mention_count)\ntrain['no_mentions_sel_text'] = train['selected_text'].apply(mention_count)\n\ncol = ['no_mentions', 'no_mentions_sel_text']\ncol_names = ['Mean no. of mentions in text', 'Mean no. of mentions in selected text']\n\nplot_bar(col, col_names)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# no. of web address\n# ==================\n\ntrain['no_web_add'] = train['text'].apply(hash_count)\ntrain['no_web_add_sel_text'] = train['selected_text'].apply(hash_count)\n\ncol = ['no_web_add', 'no_web_add_sel_text']\ncol_names = ['Mean no. of web address in text', 'Mean no. of web address in selected text']\n\nplot_bar(col, col_names)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model"},{"metadata":{},"cell_type":"markdown","source":"https://www.kaggle.com/cdeotte/tensorflow-roberta-0-705"},{"metadata":{},"cell_type":"markdown","source":"https://www.kaggle.com/al0kharba/tensorflow-roberta-0-712/output"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"https://www.kaggle.com/mohannksr/tensorflow-roberta-cnn-head-lb-v2"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd, numpy as np\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom sklearn.model_selection import StratifiedKFold\nfrom transformers import *\nimport tokenizers\nprint('TF version',tf.__version__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_train():\n    train=pd.read_csv('../input/tweet-sentiment-extraction/train.csv')\n    train['text']=train['text'].astype(str)\n    train['selected_text']=train['selected_text'].astype(str)\n    return train\n\ndef read_test():\n    test=pd.read_csv('../input/tweet-sentiment-extraction/test.csv')\n    test['text']=test['text'].astype(str)\n    return test\n\ndef read_submission():\n    test=pd.read_csv('../input/tweet-sentiment-extraction/sample_submission.csv')\n    return test\n    \ntrain_df = read_train()\ntest_df = read_test()\nsubmission_df = read_submission()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def jaccard_improve(str1, str2): \n#     str1 = str1.lower()\n#     str2 = str2.lower()    \n    \n#     index = str1.find(str2) \n#     text1 = str1[:index]\n#     #print(text1)\n    \n#     text2 = str1[index:].replace(str2, '')\n#     words1 = text1.split()\n#     words2 = text2.split()\n#     #print(words1[-3:])\n\n#     if len(words1) > len(words2):\n#         words1 = words1[-3:]\n#         mod_text = \" \".join(words1) + \" \" + str2\n#     else:\n#         words2 = words2[0:2]\n#         mod_text = str2 + \" \" + \" \".join(words2)\n    \n#     return mod_text ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def jaccard(str1, str2): \n    a = set(str(str1).lower().split()) \n    b = set(str(str2).lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(len(train_df))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_df['selected_text_mod'] = train_df['selected_text']\n# train_df['mod'] = 0\n\n# train_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN = 96\nPATH = '../input/tf-roberta/'\ntokenizer = tokenizers.ByteLevelBPETokenizer(\n    vocab_file=PATH+'vocab-roberta-base.json', \n    merges_file=PATH+'merges-roberta-base.txt', \n    lowercase=True,\n    add_prefix_space=True\n)\nsentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ct = train_df.shape[0]\ninput_ids = np.ones((ct,MAX_LEN),dtype='int32')\nattention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\ntoken_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\nstart_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\nend_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n\nfor k in range(train_df.shape[0]):\n    \n    # FIND OVERLAP\n    text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n    text2 = \" \".join(train_df.loc[k,'selected_text'].split())\n    idx = text1.find(text2)\n    chars = np.zeros((len(text1)))\n    chars[idx:idx+len(text2)]=1\n    if text1[idx-1]==' ': chars[idx-1] = 1 \n    enc = tokenizer.encode(text1) \n        \n    # ID_OFFSETS\n    offsets = []; idx=0\n    for t in enc.ids:\n        w = tokenizer.decode([t])\n        offsets.append((idx,idx+len(w)))\n        idx += len(w)\n    \n    # START END TOKENS\n    toks = []\n    for i,(a,b) in enumerate(offsets):\n        sm = np.sum(chars[a:b])\n        if sm>0: toks.append(i) \n        \n    s_tok = sentiment_id[train_df.loc[k,'sentiment']]\n    input_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n    attention_mask[k,:len(enc.ids)+5] = 1\n    if len(toks)>0:\n        start_tokens[k,toks[0]+1] = 1\n        end_tokens[k,toks[-1]+1] = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for k in range(train_df.shape[0]):\n    \n#     # FIND OVERLAP\n#     text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n#     text2 = \" \".join(train_df.loc[k,'selected_text'].split())\n#     idx = text1.find(text2)\n#     chars = np.zeros((len(text1)))\n#     chars[idx:idx+len(text2)]=1\n#     if text1[idx-1]==' ': chars[idx-1] = 1 \n#     enc = tokenizer.encode(text1) \n        \n#     # ID_OFFSETS\n#     offsets = []; idx=0\n#     for t in enc.ids:\n#         w = tokenizer.decode([t])\n#         offsets.append((idx,idx+len(w)))\n#         idx += len(w)\n    \n#     # START END TOKENS\n#     toks = []\n#     for i,(a,b) in enumerate(offsets):\n#         sm = np.sum(chars[a:b])\n#         if sm>0: toks.append(i) \n        \n#     s_tok = sentiment_id[train_df.loc[k,'sentiment']]\n#     input_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n#     attention_mask[k,:len(enc.ids)+5] = 1\n#     if len(toks)>0:\n#         start_tokens[k,toks[0]+1] = 1\n#         end_tokens[k,toks[-1]+1] = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ct = test_df.shape[0]\ninput_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\nattention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\ntoken_type_ids_t = np.zeros((ct,MAX_LEN),dtype='int32')\n\nfor k in range(test_df.shape[0]):\n        \n    # INPUT_IDS\n    text1 = \" \"+\" \".join(test_df.loc[k,'text'].split())\n    enc = tokenizer.encode(text1)                \n    s_tok = sentiment_id[test_df.loc[k,'sentiment']]\n    input_ids_t[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n    attention_mask_t[k,:len(enc.ids)+5] = 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"def scheduler(epoch):\n    return 3e-5 * 0.2**epoch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model():\n    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n\n    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n    x = bert_model(ids,attention_mask=att,token_type_ids=tok)\n    \n    \n    x1 = tf.keras.layers.Dropout(0.1)(x[0]) \n    x1 = tf.keras.layers.Conv1D(128, 2,padding='same')(x1)\n    x1 = tf.keras.layers.LeakyReLU()(x1)\n    x1 = tf.keras.layers.Conv1D(64, 2,padding='same')(x1)\n    x1 = tf.keras.layers.Dense(1)(x1)\n    x1 = tf.keras.layers.Flatten()(x1)\n    x1 = tf.keras.layers.Activation('softmax')(x1)\n    \n    x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n    x2 = tf.keras.layers.Conv1D(128, 2, padding='same')(x2)\n    x2 = tf.keras.layers.LeakyReLU()(x2)\n    x2 = tf.keras.layers.Conv1D(64, 2, padding='same')(x2)\n    x2 = tf.keras.layers.Dense(1)(x2)\n    x2 = tf.keras.layers.Flatten()(x2)\n    x2 = tf.keras.layers.Activation('softmax')(x2)\n\n    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n    model.compile(loss='binary_crossentropy', optimizer=optimizer)\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train"},{"metadata":{"trusted":true},"cell_type":"code","source":"n_splits = 5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\npreds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\nDISPLAY=1\nfor i in range(10):\n    print('#'*25)\n    print('### MODEL %i'%(i+1))\n    print('#'*25)\n    \n    K.clear_session()\n    model = build_model()\n    model.load_weights('../input/model4/v4-roberta-%i.h5'%i)\n\n    print('Predicting Test...')\n    preds = model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n    preds_start += preds[0]/n_splits\n    preds_end += preds[1]/n_splits","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all = []\nfor k in range(input_ids_t.shape[0]):\n    a = np.argmax(preds_start[k,])\n    b = np.argmax(preds_end[k,])\n    if a>b: \n        st = test_df.loc[k,'text']\n    else:\n        text1 = \" \"+\" \".join(test_df.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode(enc.ids[a-1:b])\n    all.append(st)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df['selected_text'] = all\ntest_df[['textID','selected_text']].to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}