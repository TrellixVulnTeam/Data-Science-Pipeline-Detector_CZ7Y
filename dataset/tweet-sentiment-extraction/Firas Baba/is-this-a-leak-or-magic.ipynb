{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Did any one wonder about the reason: why the selected_text has a lot of noise? \n \n# Well, I was one of the Kagglers who thought about this question and I was 100% sure that explaining the reason of these noisy targets is the secret for jumping in the top LB. After all, it is hosted by Kaggle and I don't think that Kaggle wanted to use noisy labels to challenge us(this is not helpful in real life).\n\n# I was asking myself a lot of questions and I made a lot of hypothisis to explain the errors. Until the day, I found the original data in a shared kernel ([here](https://www.kaggle.com/jonathanbesomi/private-test-not-that-private-afterall))\n\n# When comparing the competition data with the original data I realized that all the mentions in the tweets were removed (@Mohamed @love123 @azeikhff ect...) from the text and I remarked that the majority of the tweets which were filtred from these mentions have noisy labels. Starting from this point I found a way to reverse engineer the selected_text from the correct one to the noisy style lol.\n\n# Let's try to print some examples and try to find some points in common.\n","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_all(word, text):\n    import re\n    word = word.replace(\".\",\"\\.\")\n    word = word.replace(\")\",\"\\)\")\n    word = word.replace(\"(\",\"\\(\")\n    word = word.replace(\"?\",\"\\?\")\n    word = word.replace(\"!\",\"\\!\")\n    word = word.replace(\"*\",\"\\*\")\n    word = word.replace(\"$\",\"\\$\")\n    word = word.replace(\"[\",\"\\[\")\n    word = word.replace(\"]\",\"\\]\")\n    word = word.replace(\"+\",\"\\+\")\n    return [m.start() for m in re.finditer(word, text)]\n               \ndef extract_end_index(text, selected_text):\n    i=0\n    last_word = selected_text.split()[-1]\n    index_last_word = find_all(last_word, text)\n    n_occ = len(index_last_word)\n    \n    selected_text_split = selected_text.split()\n    text_split = text.split()\n    n_end = 0\n    if len(selected_text_split)==len(text_split):\n        return len(text)\n    for j, elm in enumerate(text_split[len(selected_text_split):]):\n        i = j + len(selected_text_split)\n        if elm == last_word :\n            n_end +=1\n            if text_split[j+1:i+1] == selected_text_split:\n                break\n   \n    return index_last_word[n_end-1] + len(selected_text_split[-1])\n\ndef extract_start_index(text, selected_text):\n    first_word = selected_text.split()[0]\n    index_first_word = find_all(first_word, text)\n    n_occ = len(index_first_word)\n    \n    selected_text_split = selected_text.split()\n    text_split = text.split()\n    n_start = 0\n    for i, elm in enumerate(text_split):\n        if (first_word !=elm) and (first_word in elm):\n            n_start += elm.count(first_word)\n        if elm == first_word :\n            n_start +=1\n            if text_split[i:i+len(selected_text_split)] == selected_text_split:\n                break\n    return index_first_word[n_start-1]\n\ndef jaccard(str1, str2): \n    a = set(str(str1).lower().split()) \n    b = set(str(str2).lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))\n\ndef pp_v2(text, predicted, spaces):\n    text = text.lower()\n    predicted = predicted.strip()\n    try : \n        index_start = extract_start_index(text,predicted)\n        index_end = extract_end_index(text,predicted)\n        if text[index_start:index_end]==\"\":\n            return predicted\n    except:\n        return predicted\n  \n    if spaces == 1:\n        return text[max(0,index_start-1):index_end]\n    elif spaces == 2:\n        return text[max(0,index_start-2):index_end]\n    elif spaces == 3:\n        return text[max(0,index_start-3):index_end-1]\n    elif spaces == 4:\n        return text[max(0,index_start-4):index_end-2]\n    else:\n        return predicted","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# We will need these functions later","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"original_data = pd.read_csv(\"../input/emotion/text_emotion.csv\")\ncompetition_data = pd.read_csv(\"../input/tweet-sentiment-extraction/train.csv\")\nmy_predictions = pd.read_csv(\"../input/tweets-predictions/prediction_examples.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# This function will help us to extract some samples that are impossible to predicted using a word level tokenization","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def impossible_to_predict(text, selected_text):\n    text = str(text)\n    selected_text = str(selected_text)\n    \n    text = set(text.split())\n    selected_text = set(selected_text.split())\n    \n    return not selected_text.issubset(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"competition_data[\"is_impossible\"] = competition_data.apply(lambda x:impossible_to_predict(x.text, x.selected_text), axis=1)\ncompetition_data[\"is_impossible\"].sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# As you can see we have 2905 samples at least that are impossible to predict. \n# Let's print them and print their original tweet","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"competition_data[competition_data[\"is_impossible\"]==True]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The rows number 27470, 27476, 27477 have the same problem which is an extra letter at the beginning","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# index = 27470","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(competition_data.loc[27470].text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t = \"did you fall asleep??\"\nt= t.lower()\nprint(original_data[original_data.content.str.lower().str.contains(t.lower())].content.values[0])\nprint(competition_data.loc[27470].text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# index = 27476","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(competition_data.loc[27476].text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t = \"wish we could come see u\"\nt= t.lower()\nprint(original_data[original_data.content.str.lower().str.contains(t.lower())].content.values[0])\nprint(competition_data.loc[27476].text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# index = 27477","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(competition_data.loc[27477].text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t = \"wondered about rake to.\"\nt= t.lower()\nprint(original_data[original_data.content.str.lower().str.contains(t.lower())].content.values[0])\nprint(competition_data.loc[27477].text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Can you see a common point? Yes!! I do.\n# All of these sentences have extra spaces. Look at the beginning of each sentence... It has an extra space... Not only that but for the index = 27477 we have a second extra space \".  The client\" There are 2 spaces between the point and \"the client\" and it should be one space.\n# Lets calculate the number of extra spaces for all sentences","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def calculate_spaces(text, selected_text):\n    text = str(text)\n    selected_text = str(selected_text)\n    index = text.index(selected_text)\n    x = text[:index]\n    try:\n        if x[-1]==\" \":\n            x= x[:-1]\n    except:\n        pass\n    l1 = len(x)\n    l2 = len(\" \".join(x.split()))\n    return l1-l2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"competition_data[\"extra_spaces\"] =  competition_data.apply(lambda x:calculate_spaces(x.text, x.selected_text), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"competition_data[competition_data.extra_spaces==2].head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Can you see the trick? look at the noise in the selected_text above...\n# Congrats you find the magic. Let's do a further step","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"competition_data[competition_data.extra_spaces>3].head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# still noise but this time the noise is more intense (3 extra letters instead of one in the left and missing letter in the right)\n\n# Can you now elaborate the magic?\n\n# Here is the reverse of the noise:\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1. Calculate the number of extra spaces in the text. We will call this n_extra_spaces\n# 2. Shift your predicted_selected_text n_extra_spaces in the beginning to the left\n# 3. Shift your predicted_selected_text max((n_extra_spaces-2),0) in the end to the left","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let's apply it to some predictions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"my_predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Score without reversing the trick is {my_predictions.score.mean()}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let's apply the postprocessing to build the noise","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def calculate_spaces(text, selected_text):\n    text = str(text)\n    selected_text = str(selected_text)\n    text = text.lower()\n    selected_text = selected_text.lower().strip()\n    index = extract_start_index(text, selected_text)\n    x = text[:index]\n    try:\n        if x[-1]==\" \":\n            x= x[:-1]\n    except:\n        pass\n    l1 = len(x)\n    l2 = len(\" \".join(x.split()))\n    return l1-l2\nmy_predictions[\"extra_spaces\"] =  my_predictions.apply(lambda x:calculate_spaces(x.text, x.predicted), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_predictions[\"new_selected\"] = my_predictions.apply(lambda x: pp_v2(x.text, x.predicted,x.extra_spaces), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_predictions[\"new_score\"] = my_predictions.apply(lambda x: jaccard(x.selected_text, x.new_selected), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Score after rebuilding the noise is : {my_predictions.new_score.mean()}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# I hope a part of the trick is well explained here.\n# The question now: is this magic or leak or what?\n# Thank you","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}