{"cells":[{"metadata":{"papermill":{"duration":0.029012,"end_time":"2020-10-27T19:07:04.6607","exception":false,"start_time":"2020-10-27T19:07:04.631688","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## Libraries"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-27T19:07:04.726221Z","iopub.status.busy":"2020-10-27T19:07:04.725485Z","iopub.status.idle":"2020-10-27T19:07:11.351346Z","shell.execute_reply":"2020-10-27T19:07:11.350535Z"},"papermill":{"duration":6.662437,"end_time":"2020-10-27T19:07:11.351484","exception":false,"start_time":"2020-10-27T19:07:04.689047","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \n\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom keras.utils import plot_model\nfrom keras.callbacks import ModelCheckpoint,EarlyStopping\n\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tokenizers import BertWordPieceTokenizer\n\nfrom keras.layers import BatchNormalization\nimport tensorflow as tf\nimport keras\nfrom keras.constraints import unit_norm\nfrom keras import regularizers\nfrom keras import backend as K\nfrom keras.layers import Input, Embedding,Flatten,concatenate, Conv1D, Bidirectional,Dropout\nfrom keras.models import load_model\nfrom numpy.testing import assert_allclose","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.execute_input":"2020-10-27T19:07:11.414672Z","iopub.status.busy":"2020-10-27T19:07:11.413995Z","iopub.status.idle":"2020-10-27T19:07:11.513886Z","shell.execute_reply":"2020-10-27T19:07:11.513287Z"},"papermill":{"duration":0.134336,"end_time":"2020-10-27T19:07:11.514002","exception":false,"start_time":"2020-10-27T19:07:11.379666","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# reading data set\ndata = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-27T19:07:11.581817Z","iopub.status.busy":"2020-10-27T19:07:11.581116Z","iopub.status.idle":"2020-10-27T19:07:11.603998Z","shell.execute_reply":"2020-10-27T19:07:11.603361Z"},"papermill":{"duration":0.062177,"end_time":"2020-10-27T19:07:11.604113","exception":false,"start_time":"2020-10-27T19:07:11.541936","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# removing a noisy data point\ndata = data[data.textID != '12f21c8f19']\ndata","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-27T19:07:11.676219Z","iopub.status.busy":"2020-10-27T19:07:11.675366Z","iopub.status.idle":"2020-10-27T19:07:11.701532Z","shell.execute_reply":"2020-10-27T19:07:11.700804Z"},"papermill":{"duration":0.067859,"end_time":"2020-10-27T19:07:11.701653","exception":false,"start_time":"2020-10-27T19:07:11.633794","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# removing empty rows\ndata['text'].replace('', np.nan, inplace=True)\ndata.dropna(subset=['text'], inplace=True)\ndata.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-27T19:07:11.795575Z","iopub.status.busy":"2020-10-27T19:07:11.780026Z","iopub.status.idle":"2020-10-27T19:07:11.834195Z","shell.execute_reply":"2020-10-27T19:07:11.8335Z"},"papermill":{"duration":0.104087,"end_time":"2020-10-27T19:07:11.834309","exception":false,"start_time":"2020-10-27T19:07:11.730222","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"data['text'] = data['text'].apply(lambda x: \" \".join(x.split()))\ndata['selected_text'] = data['selected_text'].apply(lambda x: \" \".join(x.split()))\n","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-27T19:07:11.909816Z","iopub.status.busy":"2020-10-27T19:07:11.909124Z","iopub.status.idle":"2020-10-27T19:07:11.924115Z","shell.execute_reply":"2020-10-27T19:07:11.923477Z"},"papermill":{"duration":0.061631,"end_time":"2020-10-27T19:07:11.92423","exception":false,"start_time":"2020-10-27T19:07:11.862599","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"data = data.astype({\"text\": str, \"selected_text\": str, 'sentiment': str})\ndata","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.028613,"end_time":"2020-10-27T19:07:11.982922","exception":false,"start_time":"2020-10-27T19:07:11.954309","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## Splitting data into train cv and test"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-27T19:07:12.047818Z","iopub.status.busy":"2020-10-27T19:07:12.047151Z","iopub.status.idle":"2020-10-27T19:07:12.064255Z","shell.execute_reply":"2020-10-27T19:07:12.063682Z"},"papermill":{"duration":0.052574,"end_time":"2020-10-27T19:07:12.064376","exception":false,"start_time":"2020-10-27T19:07:12.011802","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"\nx_train,x_test = train_test_split(data, test_size = 0.05, random_state=42)\nx_train,x_cv = train_test_split(x_train, test_size = 0.1, random_state = 42)\n\nprint(\"x_train shape is\", x_train.shape)\nprint(\"x_cv shape is\", x_cv.shape)\nprint(\"x_test shape is\", x_test.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-27T19:07:12.12885Z","iopub.status.busy":"2020-10-27T19:07:12.12794Z","iopub.status.idle":"2020-10-27T19:07:12.131108Z","shell.execute_reply":"2020-10-27T19:07:12.130473Z"},"papermill":{"duration":0.037561,"end_time":"2020-10-27T19:07:12.131226","exception":false,"start_time":"2020-10-27T19:07:12.093665","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# index reset.\nx_train.reset_index(inplace = True, drop = True)\nx_cv.reset_index(inplace = True, drop = True)\nx_test.reset_index(inplace = True, drop = True)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-27T19:07:12.206585Z","iopub.status.busy":"2020-10-27T19:07:12.205814Z","iopub.status.idle":"2020-10-27T19:07:12.563551Z","shell.execute_reply":"2020-10-27T19:07:12.562905Z"},"papermill":{"duration":0.399509,"end_time":"2020-10-27T19:07:12.563667","exception":false,"start_time":"2020-10-27T19:07:12.164158","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# https://stackoverflow.com/questions/31749448/how-to-add-percentages-on-top-of-bars-in-seaborn\nfig,ax = plt.subplots(figsize = (15,5), nrows =1, ncols = 3)\nax = ax.flatten()\nsns.countplot(x_train.sentiment, ax = ax[0], order = ['neutral', 'positive', 'negative'])\ntotal = x_train.shape[0]\nfor p in ax[0].patches:\n    height = p.get_height()\n    ax[0].text(p.get_x()+p.get_width()/2.,\n            height + 4,\n            '{:1.2f}%'.format(height*100/total),\n            ha=\"center\") \nsns.countplot(x_cv.sentiment, ax = ax[1], order = ['neutral', 'positive', 'negative'])\ntotal = x_cv.shape[0]\nfor p in ax[1].patches:\n    height = p.get_height()\n    ax[1].text(p.get_x()+p.get_width()/2.,\n            height + 4,\n            '{:1.2f}%'.format(height*100/total),\n            ha=\"center\") \nsns.countplot(x_test.sentiment, ax = ax[2], order = ['neutral', 'positive', 'negative'])\ntotal = x_test.shape[0]\nfor p in ax[2].patches:\n    height = p.get_height()\n    ax[2].text(p.get_x()+p.get_width()/2.,\n            height + 4,\n            '{:1.2f}%'.format(height*100/total),\n            ha=\"center\") ","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-27T19:07:12.637875Z","iopub.status.busy":"2020-10-27T19:07:12.637096Z","iopub.status.idle":"2020-10-27T19:07:12.641559Z","shell.execute_reply":"2020-10-27T19:07:12.640959Z"},"papermill":{"duration":0.047256,"end_time":"2020-10-27T19:07:12.641669","exception":false,"start_time":"2020-10-27T19:07:12.594413","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"x_train","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-27T19:07:12.708844Z","iopub.status.busy":"2020-10-27T19:07:12.708163Z","iopub.status.idle":"2020-10-27T19:07:12.71196Z","shell.execute_reply":"2020-10-27T19:07:12.711329Z"},"papermill":{"duration":0.039761,"end_time":"2020-10-27T19:07:12.712076","exception":false,"start_time":"2020-10-27T19:07:12.672315","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"print(x_train.shape)\nprint(x_cv.shape)\nprint(x_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load the fast tokenizer from saved file\nfrom nltk.tokenize import WordPunctTokenizer \n\ntokenizer = WordPunctTokenizer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# spliting text into words using tokenizer.\ntrain_text = []\n\nfor i in range(x_train.shape[0]):\n    enc = tokenizer.tokenize(x_train.loc[i,'text'])\n    try:\n        \n        train_text.append(enc)\n    except:\n        print(temp)\n    \ncv_text = []\n\nfor i in range(x_cv.shape[0]):\n    enc = tokenizer.tokenize(x_cv.loc[i,'text'])\n    \n    cv_text.append(enc) \n\ntest_text = []\n\nfor i in range(x_test.shape[0]):\n    enc = tokenizer.tokenize(x_test.loc[i,'text'])\n    \n    test_text.append(enc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# splitting selected_text into words using tokenizer.\ntrain_stext = []\n\nfor i in range(x_train.shape[0]):\n    enc = tokenizer.tokenize(x_train.loc[i,'selected_text'])\n    \n    \n    train_stext.append(enc)\n    \ncv_stext = []\n\nfor i in range(x_cv.shape[0]):\n    enc = tokenizer.tokenize(x_cv.loc[i,'selected_text'])\n    \n\n    cv_stext.append(enc) \n\ntest_stext = []\n\nfor i in range(x_test.shape[0]):\n    enc = tokenizer.tokenize(x_test.loc[i,'selected_text'])\n    \n    \n    test_stext.append(enc)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.031266,"end_time":"2020-10-27T19:07:12.774874","exception":false,"start_time":"2020-10-27T19:07:12.743608","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## Vectorization"},{"metadata":{"trusted":true},"cell_type":"code","source":"#https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n# tokenizing text to sequences and padding.\n\n\ntext_tokenizer = Tokenizer()\ntext_tokenizer.fit_on_texts(train_text)\nvocab_size_1 = len(text_tokenizer.word_index) + 1\n# integer encode the documents\nprint(\"vocab size is:\",vocab_size_1)\n\ntrain_text = text_tokenizer.texts_to_sequences((train_text))\ncv_text = text_tokenizer.texts_to_sequences((cv_text))\ntest_text = text_tokenizer.texts_to_sequences((test_text))\n\ntrain_select_text = text_tokenizer.texts_to_sequences(list(train_stext))\ncv_select_text = text_tokenizer.texts_to_sequences(list(cv_stext))\ntest_select_text = text_tokenizer.texts_to_sequences(list(test_stext))\n\n\nmax_length = 64 # max length of a tweet\n\ntrain_text = pad_sequences(train_text, maxlen=max_length, padding='post')\ncv_text =  pad_sequences(cv_text, maxlen=max_length, padding='post')\ntest_text = pad_sequences(test_text, maxlen = max_length, padding = 'post')\n\n\n\n\nprint(\"no. of rows sequences in train:\",len(train_text))\nprint(\"no. of rows of sequences in validataion:\", len(cv_text))\nprint(\"max length of sequences\",max_length)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-27T19:07:14.539009Z","iopub.status.busy":"2020-10-27T19:07:14.538252Z","iopub.status.idle":"2020-10-27T19:07:14.545118Z","shell.execute_reply":"2020-10-27T19:07:14.5444Z"},"papermill":{"duration":0.045182,"end_time":"2020-10-27T19:07:14.545274","exception":false,"start_time":"2020-10-27T19:07:14.500092","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# sample datapoint\ni = 1\nprint('text:')\nprint(x_train.loc[i,'text'])\nprint('sequence of text:')\nprint(text_tokenizer.texts_to_sequences([x_train.loc[i,'text']]))\nprint('sequence of text after padding:')\nprint(train_text[i])\nprint('select text:')\nprint(x_train.loc[i,'selected_text'])\nprint('sequence of select text:')\n\nprint(train_select_text[i])","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-27T19:07:14.635072Z","iopub.status.busy":"2020-10-27T19:07:14.630098Z","iopub.status.idle":"2020-10-27T19:07:14.988038Z","shell.execute_reply":"2020-10-27T19:07:14.988538Z"},"papermill":{"duration":0.410259,"end_time":"2020-10-27T19:07:14.988675","exception":false,"start_time":"2020-10-27T19:07:14.578416","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# tokenizing sentiment.\nsentiment_tokenizer = Tokenizer()\nsentiment_tokenizer.fit_on_texts(x_train['sentiment'])\nvocab_size_2 = len(sentiment_tokenizer.word_index) +1\n\ntrain_sentiment = sentiment_tokenizer.texts_to_sequences(x_train['sentiment'])\ncv_sentiment = sentiment_tokenizer.texts_to_sequences(x_cv['sentiment'])\ntest_sentiment = sentiment_tokenizer.texts_to_sequences(x_test['sentiment'])\n\n\nprint(sentiment_tokenizer.word_index)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train target creation"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_start = np.zeros((x_train.shape[0],64), dtype = 'int32')\ntrain_end = np.zeros((x_train.shape[0],64), dtype = 'int32')\n\n\nfor k in range(x_train.shape[0]):\n        tweet = x_train.loc[k,'text']\n        selected_text = x_train.loc[k,'selected_text']\n        tweet1 = x_train.loc[k,'text'].replace(\" \", \"\")\n        selected_text1 = x_train.loc[k,'selected_text'].replace(\" \", \"\")\n        idx0 = None\n        idx1 = None\n        \n        st_len = len(selected_text1)\n        for i in range(len(tweet1)):\n            if(tweet1[i:i+st_len]==selected_text1):\n                idx0 = i\n                idx1 = i + st_len -1\n                break\n\n        char_targets = [0]*len(tweet1)\n\n        for i in range(len(tweet1)):\n            if idx0 != None and idx1!=None:\n                if i>=idx0 and i<=idx1:\n                    char_targets[i] = 1\n\n        # ID_OFFSETS\n        offsets = []; idx=0\n        for t in tokenizer.tokenize(tweet):\n            offsets.append((idx,idx+len(t)))\n            idx += len(t)\n            \n        targets_index = []\n        for i, (off1,off2) in enumerate(offsets):\n            if sum(char_targets[off1:off2])>0:\n                targets_index.append(i)\n\n        train_start[k,targets_index[0]] = 1\n        train_end[k,targets_index[-1]] = 1\n        \n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# To join selected words back to sentence.\n# https://stackoverflow.com/questions/21948019/python-untokenize-a-sentence\nimport re\ndef untokenize(words):\n    \"\"\"\n    Untokenizing a text undoes the tokenizing operation, restoring\n    punctuation and spaces to the places that people expect them to be.\n    Ideally, `untokenize(tokenize(text))` should be identical to `text`,\n    except for line breaks.\n    \"\"\"\n    text = ' '.join(words)\n    step1 = text.replace(\"`` \", '\"').replace(\" ''\", '\"').replace('. . .',  '...')\n    step2 = step1.replace(\" ( \", \" (\").replace(\" ) \", \") \")\n    step3 = re.sub(r' ([.,:;?!%]+)([ \\'\"`])', r\"\\1\\2\", step2)\n    step4 = re.sub(r' ([.,:;?!%]+)$', r\"\\1\", step3)\n    step5 = step4.replace(\" '\", \"'\").replace(\" n't\", \"n't\").replace(\n         \"can not\", \"cannot\")\n    step6 = step5.replace(\" ` \", \"`\")\n    step6 = step6.replace(\" '\",\"'\")\n    step6 = step6.replace(\"# \", '#')\n    step6 =step6.replace(\" - \",\"-\")\n    step6 = step6.replace(\" ?\",\"?\" )\n    step6 = step6.replace(' ,',',')\n    step6 = step6.replace(' !','!')\n    step6 = step6.replace('( ','(')\n    step6 = step6.replace('< ','<')\n    step6 = step6.replace(' && ','&&')\n    step6 = step6.replace(' / ', '/')\n    step6 = step6.replace('. com','.com')\n    step6 = step6.replace(' :// ', '//')\n    #step6 = step6.replace(' = ', '=')\n    step6 = step6.replace('$ ','$')\n    return step6.strip()\n\ntokenized = ['I', \"'ve\", 'found', 'a', 'medicine', 'for', 'my','disease', '.']\nsent = untokenize(tokenized)\nsent","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-27T19:07:15.198017Z","iopub.status.busy":"2020-10-27T19:07:15.173851Z","iopub.status.idle":"2020-10-27T19:07:15.201339Z","shell.execute_reply":"2020-10-27T19:07:15.200829Z"},"papermill":{"duration":0.104562,"end_time":"2020-10-27T19:07:15.201455","exception":false,"start_time":"2020-10-27T19:07:15.096893","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# coverting lists to array\ntrain_select_text = np.array(train_select_text)\ncv_select_text = np.array(cv_select_text)\ntest_select_text = np.array(test_select_text)\n\ntrain_sentiment = np.array(train_sentiment)\ncv_sentiment =np.array(cv_sentiment)\ntest_sentiment =np.array(test_sentiment)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# metric\ndef jaccard(str1, str2):\n    a = set(str1.lower().split())\n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-27T19:07:15.275414Z","iopub.status.busy":"2020-10-27T19:07:15.274678Z","iopub.status.idle":"2020-10-27T19:07:17.76233Z","shell.execute_reply":"2020-10-27T19:07:17.761328Z"},"papermill":{"duration":2.52889,"end_time":"2020-10-27T19:07:17.762495","exception":false,"start_time":"2020-10-27T19:07:15.233605","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# checking whether all created targets are correct.\ncount = 0\nscore=0\nsample = 0\nfor i in range(x_train.shape[0]):\n    \n    temp = tokenizer.tokenize(x_train.loc[i,'text'])\n    \n    offsets = []; idx=0\n    for t in tokenizer.tokenize(tweet):\n        offsets.append((idx,idx+len(t)))\n        idx += len(t)\n    if (x_train.loc[i,'selected_text']== untokenize(temp[np.argmax(train_start[i]):np.argmax(train_end[i])+1])):\n        count+=1\n    elif(sample<10):\n        print(i)\n        print('actual:',x_train.loc[i,'selected_text'])\n        print('label:',untokenize(temp[np.argmax(train_start[i]):np.argmax(train_end[i])+1]))\n        print('jaccard:',jaccard(x_train.loc[i,'selected_text'],untokenize(temp[np.argmax(train_start[i]):np.argmax(train_end[i])+1])))\n        sample+=1\n\n    score+=jaccard(x_train.loc[i,'selected_text'],untokenize(temp[np.argmax(train_start[i]):np.argmax(train_end[i])+1]))\n\nprint('##############################################################################')    \nif count ==x_train.shape[0]:\n    print('all targets are correct')\n    print('jaccard score is:',score/x_train.shape[0])\nelse:\n    print(count,'targets are correct')\n    print('jaccard score is:',score/x_train.shape[0])    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Note: There will be some errors in target creation due to noisy labels in data."},{"metadata":{},"cell_type":"markdown","source":"## CV target creation"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-27T19:07:17.837563Z","iopub.status.busy":"2020-10-27T19:07:17.836874Z","iopub.status.idle":"2020-10-27T19:07:18.423593Z","shell.execute_reply":"2020-10-27T19:07:18.424532Z"},"papermill":{"duration":0.629447,"end_time":"2020-10-27T19:07:18.424756","exception":false,"start_time":"2020-10-27T19:07:17.795309","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"cv_start = np.zeros((x_cv.shape[0],64), dtype = 'int32')\ncv_end = np.zeros((x_cv.shape[0],64), dtype = 'int32')\n\n\nfor k in range(x_cv.shape[0]):\n        tweet = x_cv.loc[k,'text']\n        selected_text = x_cv.loc[k,'selected_text']\n        tweet1 = x_cv.loc[k,'text'].replace(\" \", \"\")\n        selected_text1 = x_cv.loc[k,'selected_text'].replace(\" \", \"\")\n        idx0 = None\n        idx1 = None\n        \n        st_len = len(selected_text1)\n        for i in range(len(tweet1)):\n            if(tweet1[i:i+st_len]==selected_text1):\n                idx0 = i\n                idx1 = i + st_len -1\n                break\n\n        char_targets = [0]*len(tweet1)\n\n        for i in range(len(tweet1)):\n            if idx0 != None and idx1!=None:\n                if i>=idx0 and i<=idx1:\n                    char_targets[i] = 1\n\n        # ID_OFFSETS\n        offsets = []; idx=0\n        for t in tokenizer.tokenize(tweet):\n            offsets.append((idx,idx+len(t)))\n            idx += len(t)\n            \n        targets_index = []\n        for i, (off1,off2) in enumerate(offsets):\n            if sum(char_targets[off1:off2])>0:\n                targets_index.append(i)\n\n        cv_start[k,targets_index[0]] = 1\n        cv_end[k,targets_index[-1]] = 1\n        \n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking whether all created targets are correct.\ncount = 0\nscore=0\nsample = 0\nfor i in range(x_cv.shape[0]):\n    \n    temp = tokenizer.tokenize(x_cv.loc[i,'text'])\n    \n    offsets = []; idx=0\n    for t in tokenizer.tokenize(tweet):\n        offsets.append((idx,idx+len(t)))\n        idx += len(t)\n    if (x_cv.loc[i,'selected_text']== untokenize(temp[np.argmax(cv_start[i]):np.argmax(cv_end[i])+1])):\n        count+=1\n    elif(sample<10):\n        print(i)\n        print('actual:',x_cv.loc[i,'selected_text'])\n        print('label:',untokenize(temp[np.argmax(cv_start[i]):np.argmax(cv_end[i])+1]))\n        print('jaccard:',jaccard(x_cv.loc[i,'selected_text'],untokenize(temp[np.argmax(cv_start[i]):np.argmax(cv_end[i])+1])))\n        sample+=1\n\n    score+=jaccard(x_cv.loc[i,'selected_text'],untokenize(temp[np.argmax(cv_start[i]):np.argmax(cv_end[i])+1]))\n\nprint('##############################################################################')    \nif count ==x_cv.shape[0]:\n    print('all targets are correct')\n    print('jaccard score is:',score/x_cv.shape[0])\nelse:\n    print(count,'targets are correct')\n    print('jaccard score is:',score/x_cv.shape[0])    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Test targets creation"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-27T19:07:18.498878Z","iopub.status.busy":"2020-10-27T19:07:18.498181Z","iopub.status.idle":"2020-10-27T19:07:18.666939Z","shell.execute_reply":"2020-10-27T19:07:18.66745Z"},"papermill":{"duration":0.210378,"end_time":"2020-10-27T19:07:18.667593","exception":false,"start_time":"2020-10-27T19:07:18.457215","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"test_start = np.zeros((x_test.shape[0],64), dtype = 'int32')\ntest_end = np.zeros((x_test.shape[0],64), dtype = 'int32')\n\n\nfor k in range(x_test.shape[0]):\n        tweet = x_test.loc[k,'text']\n        selected_text = x_test.loc[k,'selected_text']\n        tweet1 = x_test.loc[k,'text'].replace(\" \", \"\")\n        selected_text1 = x_test.loc[k,'selected_text'].replace(\" \", \"\")\n        idx0 = None\n        idx1 = None\n        \n        st_len = len(selected_text1)\n        for i in range(len(tweet1)):\n            if(tweet1[i:i+st_len]==selected_text1):\n                idx0 = i\n                idx1 = i + st_len -1\n                break\n\n        char_targets = [0]*len(tweet1)\n\n        for i in range(len(tweet1)):\n            if idx0 != None and idx1!=None:\n                if i>=idx0 and i<=idx1:\n                    char_targets[i] = 1\n\n        # ID_OFFSETS\n        offsets = []; idx=0\n        for t in tokenizer.tokenize(tweet):\n            offsets.append((idx,idx+len(t)))\n            idx += len(t)\n            \n        targets_index = []\n        for i, (off1,off2) in enumerate(offsets):\n            if sum(char_targets[off1:off2])>0:\n                targets_index.append(i)\n\n        test_start[k,targets_index[0]] = 1\n        test_end[k,targets_index[-1]] = 1\n        \n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking whether all created targets are correct.\ncount = 0\nscore=0\nsample = 0\nfor i in range(x_test.shape[0]):\n    \n    temp = tokenizer.tokenize(x_test.loc[i,'text'])\n    \n    offsets = []; idx=0\n    for t in tokenizer.tokenize(tweet):\n        offsets.append((idx,idx+len(t)))\n        idx += len(t)\n    if (x_test.loc[i,'selected_text']== untokenize(temp[np.argmax(test_start[i]):np.argmax(test_end[i])+1])):\n        count+=1\n    elif(sample<10):\n        print(i)\n        print('actual:',x_test.loc[i,'selected_text'])\n        print('label:',untokenize(temp[np.argmax(test_start[i]):np.argmax(test_end[i])+1]))\n        print('jaccard:',jaccard(x_test.loc[i,'selected_text'],untokenize(temp[np.argmax(test_start[i]):np.argmax(test_end[i])+1])))\n        sample+=1\n\n    score+=jaccard(x_test.loc[i,'selected_text'],untokenize(temp[np.argmax(test_start[i]):np.argmax(test_end[i])+1]))\n\nprint('##############################################################################')    \nif count ==x_test.shape[0]:\n    print('all targets are correct')\n    print('jaccard score is:',score/x_test.shape[0])\nelse:\n    print(count,'targets are correct')\n    print('jaccard score is:',score/x_test.shape[0])    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## word embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"EMBEDDING_FILE = '../input/glove-twitter/glove.twitter.27B.200d.txt'\ndef get_coefs(word, *arr): \n    return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\ncount=0\nword_index = text_tokenizer.word_index\nnb_words = len(word_index) + 1\n#change below line if computing normal stats is too slow\nembedding_matrix = embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= nb_words: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: \n        embedding_matrix[i] = embedding_vector\n        count+=1\ncount","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-27T19:07:18.976715Z","iopub.status.busy":"2020-10-27T19:07:18.975645Z","iopub.status.idle":"2020-10-27T19:07:19.163623Z","shell.execute_reply":"2020-10-27T19:07:19.162948Z"},"papermill":{"duration":0.23112,"end_time":"2020-10-27T19:07:19.163764","exception":false,"start_time":"2020-10-27T19:07:18.932644","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# padding select text sequences\ntrain_select_text = pad_sequences(train_select_text, maxlen=max_length, padding='post')\ncv_select_text =  pad_sequences(cv_select_text, maxlen=max_length, padding='post')\ntest_select_text = pad_sequences(test_select_text, maxlen = max_length, padding = 'post')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\ncount=0\nword_index = sentiment_tokenizer.word_index\nnb_words = len(word_index) + 1\n#change below line if computing normal stats is too slow\nembedding_matrix2  = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= nb_words: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: \n        embedding_matrix2[i] = embedding_vector\n        count+=1\ncount","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.032658,"end_time":"2020-10-27T19:07:19.230509","exception":false,"start_time":"2020-10-27T19:07:19.197851","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## Modelling"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-27T19:07:19.318058Z","iopub.status.busy":"2020-10-27T19:07:19.317085Z","iopub.status.idle":"2020-10-27T19:07:19.320391Z","shell.execute_reply":"2020-10-27T19:07:19.319723Z"},"papermill":{"duration":0.056925,"end_time":"2020-10-27T19:07:19.320505","exception":false,"start_time":"2020-10-27T19:07:19.26358","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n\n\ndef build_model(n1,n2,n3,n4,drop,mode,bidir=False):\n\n    \"\"\"\n    inputs:\n    \n    n1: no. of units in first layer if mode is 'lstm' else no. of filters in conv layer\n    n2: no. of units in second layer if mode is 'lstm' else kernel size in conv layer\n    n3: no. of neurons in first dense layer\n    n4: no. of neurons in second dense layer\n    mode: lstm/conv\n    bidir: normal lstm or bidirectional lstm\n    drop: dropout rate\n    \n    action:\n    \n    creates a neural network based on given inputs\n    \n    output:\n    \n    returns the model\n    \n    \"\"\"\n    \n    keras.backend.clear_session()\n    \n    i1 = Input(shape=(64,), dtype='int32')\n    e = Embedding(vocab_size_1, 200, weights=[embedding_matrix],  trainable=False )(i1)\n    if(mode=='lstm'):\n        if bidir:\n            x1 = Bidirectional(keras.layers.GRU(n1, return_sequences=True, kernel_constraint=unit_norm(),kernel_regularizer=regularizers.l2(0.0001) ))(e)\n            x1 = Bidirectional(keras.layers.GRU(n2, return_sequences=True, kernel_constraint=unit_norm(),kernel_regularizer=regularizers.l2(0.0001) ))(x1)\n        \n            \n            i2 = Input(shape=(1,), dtype='int32')\n            e = Embedding(vocab_size_2, 200,weights=[embedding_matrix2] ,trainable= 'False')(i2)\n            x2 = e\n\n        else:\n            x1 = keras.layers.GRU(n1, return_sequences=True, kernel_constraint=unit_norm(),kernel_regularizer=regularizers.l2(0.0001) )(e)\n            x1 = keras.layers.GRU(n2, return_sequences=True, kernel_constraint=unit_norm(),kernel_regularizer=regularizers.l2(0.0001) )(x1)\n            i2 = Input(shape=(1,), dtype='int32')\n            e = Embedding(vocab_size_2, 200,weights=[embedding_matrix2] ,trainable= 'False')(i2)\n            x2= keras.layers.GRU(n2, return_sequences=True, kernel_constraint=unit_norm(),kernel_regularizer=regularizers.l2(0.0001) )(e)\n    elif(mode=='conv'):\n        x1=Conv1D(n1,n2,activation = 'relu',)(e)\n        x1=Conv1D(n1/2,n2,activation = 'relu',)(x1)\n    \n    \n        i2 = Input(shape=(1,), dtype='int32')\n        e = Embedding(vocab_size_2, 200,weights=[embedding_matrix2] ,trainable= 'False')(i2)\n        x2=Conv1D(n1/2,n2,activation = 'relu',)(e)\n    \n\n    \n    x2 = tf.keras.layers.Flatten()(x2)\n    x1 = tf.keras.layers.Flatten()(x1)\n            \n    con = tf.keras.layers.Concatenate()([x1,x2])\n    \n    x1 = keras.layers.Dense(n3, activation = 'relu', kernel_initializer='he_uniform',kernel_constraint=unit_norm(),kernel_regularizer=regularizers.l2(0.0001))(con)\n \n    x1 = Dropout(drop)(x1)\n    \n\n    x2 = keras.layers.Dense(n3, activation = 'relu', kernel_initializer='he_uniform',kernel_constraint=unit_norm(),kernel_regularizer=regularizers.l2(0.0001))(con)\n    \n    x2 = Dropout(drop)(x2)\n\n    \n    \n    output1 = keras.layers.Dense(64, activation = 'softmax')(x1)\n    output2 = keras.layers.Dense(64, activation = 'softmax')(x2)\n\n    model = keras.models.Model(inputs =[i1,i2], outputs = [output1,output2] )\n\n    opt = keras.optimizers.Adam(lr=3e-5, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0,clipnorm=1)\n\n    model.compile(optimizer = opt, loss = 'categorical_crossentropy' )\n\n\n    return model\n","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-27T19:07:20.900567Z","iopub.status.busy":"2020-10-27T19:07:20.899929Z","iopub.status.idle":"2020-10-27T19:07:20.902886Z","shell.execute_reply":"2020-10-27T19:07:20.902221Z"},"papermill":{"duration":0.043714,"end_time":"2020-10-27T19:07:20.903002","exception":false,"start_time":"2020-10-27T19:07:20.859288","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# model checkpoint to save best model.\nfilepath = \"/kaggle/working/best_model.h5\" \ncheckpoint = ModelCheckpoint(filepath=filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n\n","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-27T19:07:21.759017Z","iopub.status.busy":"2020-10-27T19:07:21.757374Z","iopub.status.idle":"2020-10-27T19:07:22.733885Z","shell.execute_reply":"2020-10-27T19:07:22.733176Z"},"papermill":{"duration":1.017894,"end_time":"2020-10-27T19:07:22.734012","exception":false,"start_time":"2020-10-27T19:07:21.716118","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"model = build_model(128,64,64,16,0.2,'lstm',True )\n","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-27T19:07:22.811354Z","iopub.status.busy":"2020-10-27T19:07:22.810623Z","iopub.status.idle":"2020-10-27T19:07:22.981233Z","shell.execute_reply":"2020-10-27T19:07:22.980577Z"},"papermill":{"duration":0.211394,"end_time":"2020-10-27T19:07:22.981345","exception":false,"start_time":"2020-10-27T19:07:22.769951","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"plot_model(model, show_shapes = True)\n","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-27T19:07:23.060227Z","iopub.status.busy":"2020-10-27T19:07:23.059493Z","iopub.status.idle":"2020-10-27T19:07:23.062449Z","shell.execute_reply":"2020-10-27T19:07:23.061932Z"},"papermill":{"duration":0.044363,"end_time":"2020-10-27T19:07:23.062567","exception":false,"start_time":"2020-10-27T19:07:23.018204","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"#model.fit([train_text,train_sentiment],[train_start,train_end], validation_data = ([cv_text,cv_sentiment], [cv_start, cv_end]),epochs =20, batch_size = 32, callbacks= [checkpoint])","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-27T19:07:23.143206Z","iopub.status.busy":"2020-10-27T19:07:23.142452Z","iopub.status.idle":"2020-10-27T19:07:23.730399Z","shell.execute_reply":"2020-10-27T19:07:23.729688Z"},"papermill":{"duration":0.630725,"end_time":"2020-10-27T19:07:23.730521","exception":false,"start_time":"2020-10-27T19:07:23.099796","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"#https://stackoverflow.com/questions/51700351/valueerror-unknown-metric-function-when-using-custom-metric-in-keras\n# loading the bestmodel out of three.\nbest_model = load_model(\"../input/nn-word-attention/best_model (10).h5\",)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-27T19:07:23.811286Z","iopub.status.busy":"2020-10-27T19:07:23.810608Z","iopub.status.idle":"2020-10-27T19:07:25.466205Z","shell.execute_reply":"2020-10-27T19:07:25.465489Z"},"papermill":{"duration":1.698621,"end_time":"2020-10-27T19:07:25.466325","exception":false,"start_time":"2020-10-27T19:07:23.767704","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"test_start,test_end= best_model.predict([test_text,test_sentiment])","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-27T19:07:25.547668Z","iopub.status.busy":"2020-10-27T19:07:25.547016Z","iopub.status.idle":"2020-10-27T19:07:25.550325Z","shell.execute_reply":"2020-10-27T19:07:25.549778Z"},"papermill":{"duration":0.046458,"end_time":"2020-10-27T19:07:25.550434","exception":false,"start_time":"2020-10-27T19:07:25.503976","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# calculating the jaccard score.\n\nscore = 0\nfor i in range(x_test.shape[0]):\n    temp = tokenizer.tokenize(x_test.loc[i,'text'])\n    \n    score = score + jaccard(untokenize(temp[np.argmax(test_start[i]):np.argmax(test_end[i])+1]),x_test.selected_text[i])\n    \n\n    \nprint(score/x_test.shape[0])        ","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-27T19:07:25.63619Z","iopub.status.busy":"2020-10-27T19:07:25.635491Z","iopub.status.idle":"2020-10-27T19:07:25.688809Z","shell.execute_reply":"2020-10-27T19:07:25.687954Z"},"papermill":{"duration":0.099842,"end_time":"2020-10-27T19:07:25.688959","exception":false,"start_time":"2020-10-27T19:07:25.589117","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# calculating the jaccard score.\n\nscore = 0\nx_test['jaccard_score'] = 0\nfor i in range(x_test.shape[0]):\n    \n    if x_test.loc[i,'sentiment']!='neutral':\n        temp = tokenizer.tokenize(x_test.loc[i,'text'])\n        \n        if np.argmax(test_start[i])<=np.argmax(test_end[i]):\n            x_test.loc[i,'jaccard_score'] = jaccard(untokenize(temp[np.argmax(test_start[i]):np.argmax(test_end[i])+1]),x_test.selected_text[i])\n            score = score + jaccard(untokenize(temp[np.argmax(test_start[i]):np.argmax(test_end[i])+1]),x_test.selected_text[i])\n        else:\n            x_test.loc[i,'jaccard_score'] = jaccard(x_test.text[i],x_test.selected_text[i])\n\n            score = score + jaccard(x_test.text[i],x_test.selected_text[i])\n\n    else:\n        x_test.loc[i,'jaccard_score'] = jaccard(x_test.text[i],x_test.selected_text[i])\n\n        score = score + jaccard(x_test.text[i],x_test.selected_text[i])\n\n\n    \nprint(score/x_test.shape[0])        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('avg positive jaccard score: ',x_test[x_test['sentiment']=='positive'].jaccard_score.mean())\n\nprint('avg negative jaccard score: ',x_test[x_test['sentiment']=='negative'].jaccard_score.mean())\nprint('avg neutral jaccard score: ',x_test[x_test['sentiment']=='neutral'].jaccard_score.mean())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sample predictions\nfor i in range(0,x_test.shape[0],100):\n    \n    temp = tokenizer.tokenize(x_test.loc[i,'text'])\n    \n    print('text:',x_test.loc[i,'text'])\n    print('sentiment',x_test.loc[i,'sentiment'])\n    print('actual:',x_test.loc[i,'selected_text'])\n    print('predicted:',untokenize(temp[np.argmax(test_start[i]):np.argmax(test_end[i])+1]))\n    print('jaccard:',jaccard(x_test.loc[i,'selected_text'],untokenize(temp[np.argmax(test_start[i]):np.argmax(test_end[i])+1])))\n\n\n\n    print('##############################################################################\\n')    \n","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.037316,"end_time":"2020-10-27T19:07:25.862653","exception":false,"start_time":"2020-10-27T19:07:25.825337","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# Test Data Predictions"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-27T19:07:25.947664Z","iopub.status.busy":"2020-10-27T19:07:25.946972Z","iopub.status.idle":"2020-10-27T19:07:28.397709Z","shell.execute_reply":"2020-10-27T19:07:28.397025Z"},"papermill":{"duration":2.496932,"end_time":"2020-10-27T19:07:28.397853","exception":false,"start_time":"2020-10-27T19:07:25.900921","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"test = pd.read_csv('../input/tweet-sentiment-extraction/test.csv')\n\n\ntest_text = []\n\nfor i in range(test.shape[0]):\n    enc = tokenizer.tokenize(test.loc[i,'text'])\n    \n    test_text.append(enc)\n# tokenizing test set.\ntest_text = text_tokenizer.texts_to_sequences(test_text)\ntest_text = pad_sequences(test_text, maxlen=max_length, padding='post')\ntest_sentiment = sentiment_tokenizer.texts_to_sequences(test['sentiment'])\n\n# coverting lists to array.\ntest_text = np.array(test_text)\ntest_sentiment = np.array(test_sentiment)\n\n# predicting using best model.\n\npreds = []\ntest_start,test_end= best_model.predict([test_text,test_sentiment])\n\nfor i in range(test.shape[0]):\n    temp = tokenizer.tokenize(test.loc[i,'text'])\n    if test.loc[i,'sentiment']!='neutral' and np.argmax(test_start[i])<=np.argmax(test_end[i]):\n        preds.append(untokenize(temp[np.argmax(test_start[i]):np.argmax(test_end[i])+1]))\n    else:\n        preds.append(test.text[i])","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-27T19:07:28.482547Z","iopub.status.busy":"2020-10-27T19:07:28.481705Z","iopub.status.idle":"2020-10-27T19:07:28.714715Z","shell.execute_reply":"2020-10-27T19:07:28.714034Z"},"papermill":{"duration":0.277136,"end_time":"2020-10-27T19:07:28.714851","exception":false,"start_time":"2020-10-27T19:07:28.437715","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# creating submission file.\nsubmission = pd.read_csv('../input/tweet-sentiment-extraction/sample_submission.csv')\nsubmission['selected_text'] = preds\n\nsubmission.to_csv('submission.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}