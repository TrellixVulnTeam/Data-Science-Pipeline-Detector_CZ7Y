{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \n\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom keras.utils import plot_model\nfrom keras.callbacks import ModelCheckpoint,EarlyStopping\n\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nfrom keras.layers import BatchNormalization\nimport tensorflow as tf\nimport keras\nfrom keras.constraints import unit_norm\nfrom keras import regularizers\nfrom keras import backend as K\nfrom keras.layers import Input, Embedding,Flatten,concatenate, Conv1D, Bidirectional,Dropout\nfrom keras.models import load_model\nfrom numpy.testing import assert_allclose\nfrom keras.layers import Attention","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# reading data set\ndata = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# removing a noisy data point\ndata = data[data.textID != '12f21c8f19']\ndata","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# removing empty rows\ndata['text'].replace('', np.nan, inplace=True)\ndata.dropna(subset=['text'], inplace=True)\ndata.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['text'] = data['text'].apply(lambda x: \" \".join(x.split()))\ndata['selected_text'] = data['selected_text'].apply(lambda x: \" \".join(x.split()))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.astype({\"text\": str, \"selected_text\": str, 'sentiment': str})\ndata","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Splitting data into train cv and test"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nx_train,x_test = train_test_split(data, test_size = 0.05, random_state=42)\nx_train,x_cv = train_test_split(x_train, test_size = 0.1, random_state = 42)\n\nprint(\"x_train shape is\", x_train.shape)\nprint(\"x_cv shape is\", x_cv.shape)\nprint(\"x_test shape is\", x_test.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# index reset.\nx_train.reset_index(inplace = True, drop = True)\nx_cv.reset_index(inplace = True, drop = True)\nx_test.reset_index(inplace = True, drop = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://stackoverflow.com/questions/31749448/how-to-add-percentages-on-top-of-bars-in-seaborn\nfig,ax = plt.subplots(figsize = (15,5), nrows =1, ncols = 3)\nax = ax.flatten()\nsns.countplot(x_train.sentiment, ax = ax[0], order = ['neutral', 'positive', 'negative'])\ntotal = x_train.shape[0]\nfor p in ax[0].patches:\n    height = p.get_height()\n    ax[0].text(p.get_x()+p.get_width()/2.,\n            height + 4,\n            '{:1.2f}%'.format(height*100/total),\n            ha=\"center\") \nsns.countplot(x_cv.sentiment, ax = ax[1], order = ['neutral', 'positive', 'negative'])\ntotal = x_cv.shape[0]\nfor p in ax[1].patches:\n    height = p.get_height()\n    ax[1].text(p.get_x()+p.get_width()/2.,\n            height + 4,\n            '{:1.2f}%'.format(height*100/total),\n            ha=\"center\") \nsns.countplot(x_test.sentiment, ax = ax[2], order = ['neutral', 'positive', 'negative'])\ntotal = x_test.shape[0]\nfor p in ax[2].patches:\n    height = p.get_height()\n    ax[2].text(p.get_x()+p.get_width()/2.,\n            height + 4,\n            '{:1.2f}%'.format(height*100/total),\n            ha=\"center\") ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(x_train.shape)\nprint(x_cv.shape)\nprint(x_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Vectorization"},{"metadata":{"trusted":true},"cell_type":"code","source":"#https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n# tokenizing text to sequences and padding.\n\n\ntext_tokenizer = Tokenizer(char_level =True)\ntext_tokenizer.fit_on_texts(list(x_train['text']))\nvocab_size_1 = len(text_tokenizer.word_index) + 1\n# integer encode the documents\nprint(\"vocab size is:\",vocab_size_1)\n\ntrain_text = text_tokenizer.texts_to_sequences(list(x_train['text']))\ncv_text = text_tokenizer.texts_to_sequences(list(x_cv['text']))\ntest_text = text_tokenizer.texts_to_sequences(list(x_test['text']))\n\ntrain_select_text = text_tokenizer.texts_to_sequences(list(x_train['selected_text']))\ncv_select_text = text_tokenizer.texts_to_sequences(list(x_cv['selected_text']))\ntest_select_text = text_tokenizer.texts_to_sequences(list(x_test['selected_text']))\n\n\nmax_length = 141 # max length of a tweet\n\ntrain_text = pad_sequences(train_text, maxlen=max_length, padding='post')\ncv_text =  pad_sequences(cv_text, maxlen=max_length, padding='post')\ntest_text = pad_sequences(test_text, maxlen = max_length, padding = 'post')\n\n\n\n\nprint(\"no. of rows sequences in train:\",len(train_text))\nprint(\"no. of rows of sequences in validataion:\", len(cv_text))\nprint(\"max length of sequences\",max_length)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sample datapoint\ni = 1\nprint('text:')\nprint(x_train.loc[i,'text'])\nprint('sequence of text:')\nprint(text_tokenizer.texts_to_sequences([x_train.loc[i,'text']]))\nprint('sequence of text after padding:')\nprint(train_text[i])\nprint('select text:')\nprint(x_train.loc[i,'selected_text'])\nprint('sequence of select text:')\n\nprint(train_select_text[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tokenizing sentiment.\nsentiment_tokenizer = Tokenizer(char_level = True)\nsentiment_tokenizer.fit_on_texts(x_train['sentiment'])\nvocab_size_2 = len(sentiment_tokenizer.word_index) +1\n\ntrain_sentiment = sentiment_tokenizer.texts_to_sequences(x_train['sentiment'])\ncv_sentiment = sentiment_tokenizer.texts_to_sequences(x_cv['sentiment'])\ntest_sentiment = sentiment_tokenizer.texts_to_sequences(x_test['sentiment'])\n\n\nprint(sentiment_tokenizer.word_index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sentiment = pad_sequences(train_sentiment, maxlen=8, padding='post')\ncv_sentiment =  pad_sequences(cv_sentiment, maxlen=8, padding='post')\ntest_sentiment = pad_sequences(test_sentiment, maxlen = 8, padding = 'post')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://stackoverflow.com/questions/7100242/python-numpy-first-occurrence-of-subarray\n# creating new target variables.\ndef target_creation(tweets, sub_tweets):\n    \"\"\"\n    inputs:\n    tokenized tweet and tokenized selected_text.\n    \n    action:\n    calculates start and end index of subtweet within tweet.\n    \n    output:\n    returns start and end indices.\n    \n    \n    \"\"\"\n    \n    start = np.zeros(tweets.shape, dtype = 'int32')\n    end = np.zeros(tweets.shape, dtype = 'int32')\n    \n    for i in range(tweets.shape[0]):\n        \n            \n        a=tweets[i]\n        b = sub_tweets[i]\n        for j in range(len(a)):\n            if (a[j:j+len(b)]==b).all():\n                break\n\n        start[i,j] = 1\n        end[i,j+len(sub_tweets[i])] = 1\n       \n    \n    return start,end","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# coverting lists to array\ntrain_select_text = np.array(train_select_text)\ncv_select_text = np.array(cv_select_text)\ntest_select_text = np.array(test_select_text)\n\ntrain_sentiment = np.array(train_sentiment)\ncv_sentiment =np.array(cv_sentiment)\ntest_sentiment =np.array(test_sentiment)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking whether all created targets are correct.\n\ntrain_start,train_end = target_creation(train_text,train_select_text)\ncount = 0\nfor i in range(x_train.shape[0]):\n\n    \n    if (train_text[i][np.argmax(train_start[i]):np.argmax(train_end[i])]==train_select_text[i]).all():\n\n        count+=1\n    else:\n        print(len(train_text[i][np.argmax(train_start[i]):np.argmax(train_end[i])]))\n        print(len(train_text[i]))\n        print(len(train_select_text[i]))\n        print(train_text[i][np.argmax(train_start[i]):np.argmax(train_end[i])])\n        print(train_text[i])\n        print(train_select_text[i])\n\n        print(x_train.loc[i])\n        print(i)\n\nif count ==x_train.shape[0]:\n    print('all targets are correct')\nelse:\n    print(count,'targets are correct')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking whether all created targets are correct.\n\ncv_start,cv_end = target_creation(cv_text,cv_select_text)\ncount = 0\nfor i in range(x_cv.shape[0]):\n\n    \n    if (cv_text[i][np.argmax(cv_start[i]):np.argmax(cv_end[i])]==cv_select_text[i]).all():\n\n        count+=1\n    else:\n        print(len(cv_text[i][np.argmax(cv_start[i]):np.argmax(cv_end[i])]))\n        print(len(cv_text[i]))\n        print(len(cv_select_text[i]))\n        print(cv_text[i][np.argmax(cv_start[i]):np.argmax(cv_end[i])])\n        print(cv_text[i])\n        print(cv_select_text[i])\n        print(x_cv.loc[i])\n        print(i)\n        \n    \n    \nif count ==x_cv.shape[0]:\n    print('all targets are correct')\nelse:\n    print(count,'targets are correct')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking whether all targets are correct.\ntest_start,test_end = target_creation(test_text,test_select_text)\ncount = 0\nfor i in range(x_test.shape[0]):\n\n    \n    if (test_text[i][np.argmax(test_start[i]):np.argmax(test_end[i])]==test_select_text[i]).all():\n\n        count+=1\n    else:\n        print(len(test_text[i][np.argmax(test_start[i]):np.argmax(test_end[i])]))\n        print(len(test_text[i]))\n        print(len(test_select_text[i]))\n        print(test_text[i][np.argmax(test_start[i]):np.argmax(test_end[i])])\n        print(test_text[i])\n        print(test_select_text[i])\n        print(x_test.loc[i])\n        print(i)\n\n        \nif count ==x_test.shape[0]:\n    print('all targets are correct')\nelse:\n    print(count,'targets are correct')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Char Embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"#loading char glove vectors\nchar2vec = {}\nwith open('../input/glove840b300dchar/glove.840B.300d-char.txt') as f:\n    for line in f:\n        values = line.split()\n        char = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        char2vec[char] = coefs\n        \nprint('no. of char vectors',len(char2vec))\nprint('chars covered in the model', list(char2vec.keys()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating embedding matrix\n\nvocab =text_tokenizer.word_index\nembedding_matrix = np.zeros((len(vocab) + 1, 300))\nfor word, i in vocab.items():\n    vector = char2vec.get(word)\n\n    if vector is not None:\n        embedding_matrix[i] = vector\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab =sentiment_tokenizer.word_index\nembedding_matrix2 = np.zeros((len(vocab) + 1, 300))\nfor word, i in vocab.items():\n    vector = char2vec.get(word)\n\n    if vector is not None:\n        embedding_matrix2[i] = vector\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix2.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# padding select text sequences\ntrain_select_text = pad_sequences(train_select_text, maxlen=max_length, padding='post')\ncv_select_text =  pad_sequences(cv_select_text, maxlen=max_length, padding='post')\ntest_select_text = pad_sequences(test_select_text, maxlen = max_length, padding = 'post')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modelling"},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n\n\ndef build_model(n1,n2,n3,n4,drop,mode,bidir=False):\n\n    \"\"\"\n    inputs:\n    \n    n1: no. of units in first layer if mode is 'lstm' else no. of filters in conv layer\n    n2: no. of units in second layer if mode is 'lstm' else kernel size in conv layer\n    n3: no. of neurons in first dense layer\n    n4: no. of neurons in second dense layer\n    mode: lstm/conv\n    bidir: normal lstm or bidirectional lstm\n    drop: dropout rate\n    \n    action:\n    \n    creates a neural network based on given inputs\n    \n    output:\n    \n    returns the model\n    \n    \"\"\"\n    \n    keras.backend.clear_session()\n    \n    i1 = Input(shape=(141,), dtype='int32')\n    e = Embedding(vocab_size_1, 300, weights=[embedding_matrix],  trainable=False, mask_zero=True  )(i1)#mask_zero=True\n    if(mode=='lstm'):\n        if bidir:\n            x1 = Bidirectional(keras.layers.LSTM(n1, return_sequences=True, kernel_constraint=unit_norm(),kernel_regularizer=regularizers.l2(0.0001) ))(e)\n            x1 = Dropout(drop)(x1)\n\n            x1 = Bidirectional(keras.layers.LSTM(n2, return_sequences=True, kernel_constraint=unit_norm(),kernel_regularizer=regularizers.l2(0.0001) ))(x1)\n            \n            i2 = Input(shape=(8,), dtype='int32')\n            x2 = Embedding(vocab_size_2, 300,mask_zero=True,weights=[embedding_matrix2],  trainable=False)(i2)\n            x2 = Bidirectional(keras.layers.LSTM(n2, return_sequences=True, kernel_constraint=unit_norm(),kernel_regularizer=regularizers.l2(0.0001) ))(x2)\n            x1x2_att = Attention()([x1,x2])\n\n            \n            x2 = tf.keras.layers.Flatten()(x2)\n            x1x2_att = tf.keras.layers.Flatten()(x1x2_att)\n            \n            con = tf.keras.layers.Concatenate()([x2,x1x2_att])\n\n        else:\n            x1 = keras.layers.LSTM(n1, return_sequences=True, kernel_constraint=unit_norm(),kernel_regularizer=regularizers.l2(0.0001) )(e)\n            x1 = Dropout(drop)(x1)\n\n            x1 = keras.layers.LSTM(n2, return_sequences=True, kernel_constraint=unit_norm(),kernel_regularizer=regularizers.l2(0.0001) )(x1)\n            i2 = Input(shape=(8,), dtype='int32')\n            x2 = Embedding(vocab_size_2, 300,mask_zero=True,weights=[embedding_matrix2],  trainable=False)(i2)\n            x2 = keras.layers.LSTM(n2, return_sequences=True, kernel_constraint=unit_norm(),kernel_regularizer=regularizers.l2(0.0001) )(x2)\n            \n\n            x1x2_att = Attention()([x1,x2])\n\n            x2 = tf.keras.layers.Flatten()(x2)\n            x1x2_att = tf.keras.layers.Flatten()(x1x2_att)\n            \n            con = tf.keras.layers.Concatenate()([x2,x1x2_att])\n    elif(mode=='conv'):\n        x1=Conv1D(n1,n2,activation = 'relu',)(e)\n        x1 = Dropout(drop)(x1)\n\n        x1=Conv1D(n1/4,n2,activation = 'relu',)(x1)\n        \n        i2 = Input(shape=(8,), dtype='int32')\n        x2 = Embedding(vocab_size_2, 300,mask_zero=True,weights=[embedding_matrix2],  trainable=False)(i2)\n        x2 = Conv1D(n1/4,n2,activation = 'relu' )(x2)\n        \n\n        x1x2_att = Attention()([x1,x2]) # concatenate()([x1,x2])#\n        x1x2_att = tf.keras.layers.Flatten()(x1x2_att)\n        x2 = tf.keras.layers.Flatten()(x2)\n        \n        con = tf.keras.layers.Concatenate()([x2,x1x2_att])\n\n    \n    con = Dropout(drop)(con)\n\n    x1 = keras.layers.Dense(n3, activation = 'relu', kernel_initializer='he_uniform',kernel_constraint=unit_norm(),kernel_regularizer=regularizers.l2(0.0001))(con)\n    \n\n    x2 = keras.layers.Dense(n3, activation = 'relu', kernel_initializer='he_uniform',kernel_constraint=unit_norm(),kernel_regularizer=regularizers.l2(0.0001))(con)\n    \n\n    \n    output1 = keras.layers.Dense(141, activation = 'softmax')(x1)\n    output2 = keras.layers.Dense(141, activation = 'softmax')(x2)\n\n    model = keras.models.Model(inputs =[i1,i2], outputs = [output1,output2] )\n\n    opt = keras.optimizers.Adam(lr=3e-3, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0,clipnorm=1)\n\n    model.compile(optimizer = opt, loss = 'categorical_crossentropy' )\n\n\n    return model\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = build_model(128,64,64,16,0.2,'lstm',False )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualizing the model\nplot_model(model, show_shapes = True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model checkpoint to save best model.\nfilepath = \"/kaggle/working/best_model.h5\" \ncheckpoint = ModelCheckpoint(filepath=filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#model.fit([train_text,train_sentiment],[train_start,train_end], validation_data = ([cv_text,cv_sentiment], [cv_start, cv_end]),epochs =20, batch_size = 32, callbacks= [checkpoint])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = build_model(512,4,128,64,0.2,'conv' )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_model(model, show_shapes = True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#model.fit([train_text,train_sentiment],[train_start,train_end], validation_data = ([cv_text,cv_sentiment], [cv_start, cv_end]),epochs =30, batch_size = 32, callbacks= [checkpoint])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = build_model(128,64,64,16,0.2,'lstm',True )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_model(model, show_shapes = True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#model.fit([train_text,train_sentiment],[train_start,train_end], validation_data = ([cv_text,cv_sentiment], [cv_start, cv_end]),epochs =50, batch_size = 32, callbacks= [checkpoint])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def step_decay(epoch):\n    \n    if epoch%2==0:\n        return 3e-4\n    \n    return 3e-5\n\nfrom keras.callbacks import LearningRateScheduler\n\nlrate = LearningRateScheduler(step_decay)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#model.fit([train_text,train_sentiment],[train_start,train_end], validation_data = ([cv_text,cv_sentiment], [cv_start, cv_end]),epochs =100, batch_size = 32, callbacks= [checkpoint,lrate])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#https://stackoverflow.com/questions/51700351/valueerror-unknown-metric-function-when-using-custom-metric-in-keras\n# loading the bestmodel out of three.\nbest_model = load_model(\"../input/nn-attention-055/best_model (7).h5\", )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_start,test_end= best_model.predict([test_text,test_sentiment])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# metric\ndef jaccard(str1, str2):\n    a = set(str1.lower().split())\n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = 0\nx_test['jaccard_score'] = 0\nfor i in range(x_test.shape[0]):\n    \n        \n    x_test.loc[i,'jaccard_score'] = jaccard(x_test.text[i][np.argmax(test_start[i]):np.argmax(test_end[i])+1],x_test.selected_text[i])\n    score = score + jaccard(x_test.text[i][np.argmax(test_start[i]):np.argmax(test_end[i])+1],x_test.selected_text[i])\n\nprint(score/x_test.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('average jaccard score for positive tweets:',x_test[x_test['sentiment']=='positive']['jaccard_score'].mean())\nprint('average jaccard score for negative tweets:',x_test[x_test['sentiment']=='negative']['jaccard_score'].mean())\nprint('average jaccard score for neutral tweets:',x_test[x_test['sentiment']=='neutral']['jaccard_score'].mean())\nprint('Total jaccard score:', x_test.jaccard_score.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# prediction examples.\n\nfor i in range(0,x_test.shape[0],100):\n    print('text:', x_test.text[i])\n    print('selected text:', x_test.selected_text[i])\n    print('sentiment:',x_test.sentiment[i] )\n    print('predicted:',x_test.text[i][np.argmax(test_start[i]):np.argmax(test_end[i])+1])\n    print('jaccard score:', jaccard(x_test.selected_text[i], x_test.text[i][np.argmax(test_start[i]):np.argmax(test_end[i])+1]))\n    print('#################################\\n')\n    #score = score + jaccard(x_test.text[i][np.argmax(test_start[i]):np.argmax(test_end[i])+1],x_test.selected_text[i])\n\n\n    \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Test Data Predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('../input/tweet-sentiment-extraction/test.csv')\n\n# tokenizing test set.\ntest_text = text_tokenizer.texts_to_sequences(test['text'])\ntest_text = pad_sequences(test_text, maxlen=max_length, padding='post')\ntest_sentiment = sentiment_tokenizer.texts_to_sequences(test['sentiment'])\ntest_sentiment = pad_sequences(test_sentiment, maxlen=8, padding='post')\n\n# coverting lists to array.\ntest_text = np.array(test_text)\ntest_sentiment = np.array(test_sentiment)\n\n# predicting using best model.\n\npreds = []\ntest_start,test_end= best_model.predict([test_text,test_sentiment])\n\nfor i in range(test.shape[0]):\n    preds.append(test.text[i][np.argmax(test_start[i]):np.argmax(test_end[i])])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating submission file.\nsubmission = pd.read_csv('../input/tweet-sentiment-extraction/sample_submission.csv')\nsubmission['selected_text'] = preds\n\nsubmission.to_csv('submission.csv', index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}