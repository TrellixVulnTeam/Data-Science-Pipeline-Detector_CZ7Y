{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install /kaggle/input/fairseq-and-fastbpe/sacrebleu-1.4.9-py3-none-any.whl\n!pip install /kaggle/input/fairseq-and-fastbpe/fairseq-0.9.0-cp37-cp37m-linux_x86_64.whl\n!pip install /kaggle/input/fairseq-and-fastbpe/fastBPE-0.1.0-cp37-cp37m-linux_x86_64.whl","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom tqdm.autonotebook import tqdm\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import lr_scheduler\n\nfrom sklearn import model_selection\nfrom sklearn import metrics\nimport transformers\nimport tokenizers\n\nimport argparse\nfrom fairseq.data.encoders.fastbpe import fastBPE\nfrom fairseq.data import Dictionary\n\nfrom joblib import Parallel, delayed\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LEARNING_RATE = 6e-5\nMAX_LEN = 126\nTRAIN_BATCH_SIZE = 35\nVALID_BATCH_SIZE = 32\nEPOCHS = 3\nINPUT_PATH = \"/kaggle/input/\"\nMODEL_INPUT_PATH = f\"{INPUT_PATH}bertweet-model/\"\n\nkey = argparse.Namespace(bpe_codes= f\"{MODEL_INPUT_PATH}BERTweet_base_transformers/bpe.codes\")\nbpe = fastBPE(key)\n\n# Load the dictionary  \nvocab = Dictionary()\nvocab.add_from_file(f\"{MODEL_INPUT_PATH}BERTweet_base_transformers/dict.txt\")\n\n# TOKENIZER = transformers.RobertaTokenizer(\n#     vocab_file =  f'{ROBERTA_PATH}vocab.json',\n#     merges_file = f'{ROBERTA_PATH}merges.txt',\n#     lowercase = True,\n#     add_prefix_space = True\n# )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TweetDataset:\n    def __init__(self, tweets, sentiments, selected_texts):\n        self.tweets = [' '+' '.join(str(tweet).split()) for tweet in tweets]\n        self.sentiments = [' '+' '.join(str(sentiment).split()) for sentiment in sentiments]\n        self.selected_texts = [' '+' '.join(str(selected_text).split()) for selected_text in selected_texts]\n        self.max_len = MAX_LEN\n        \n    \n    def __len__(self):\n        return len(self.tweets)\n\n    def __getitem__(self, item):\n        e_tweet = '<s> ' + bpe.encode(self.tweets[item]) + ' </s>'\n        enc_tweet = vocab.encode_line(e_tweet, append_eos=False, add_if_not_exist=False).long().tolist()\n\n        if self.sentiments[item] != 'neutral':\n            e_sentiment = '</s> ' + bpe.encode(self.sentiments[item]) + \" \" + bpe.encode(self.sentiments[item]) + ' </s>'\n        else:\n            e_sentiment = '</s> ' + bpe.encode(self.sentiments[item]) + ' </s>'\n        enc_sentiment = vocab.encode_line(e_sentiment, append_eos=False, add_if_not_exist=False).long().tolist()\n\n        enc_tweet_sentiment = enc_tweet + enc_sentiment\n\n        padding_len = self.max_len - len(enc_tweet_sentiment)\n        input_ids = enc_tweet_sentiment + ([0] * padding_len)\n        attention_mask = ([1] * len(enc_tweet_sentiment)) + ([0] * padding_len)\n\n        start_index, end_index = 0, 0\n        token_type_ids = [0] * self.max_len\n\n        e_selected_text_ids = bpe.encode(self.selected_texts[item])\n        enc_selected_text_ids = vocab.encode_line(e_selected_text_ids, append_eos=False, add_if_not_exist=False).long().tolist()\n\n        for j in (i for i,e in enumerate(enc_tweet_sentiment) if e == enc_selected_text_ids[0]):\n            if enc_tweet_sentiment[j:j+len(enc_selected_text_ids)] == enc_selected_text_ids:\n                start_index = j\n                end_index = j+(len(enc_selected_text_ids))\n\n        return {\n            'ids': torch.tensor(input_ids, dtype=torch.long),\n            'mask': torch.tensor(attention_mask, dtype=torch.long),\n            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n            'targets_start': torch.tensor(start_index, dtype=torch.long),\n            'targets_end': torch.tensor(end_index, dtype=torch.long),\n            'orig_tweet': self.tweets[item],\n            'orig_selected': self.selected_texts[item],\n            'sentiment': self.sentiments[item]\n        }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TweetModel(transformers.BertPreTrainedModel):\n    def __init__(self, conf):\n        super(TweetModel, self).__init__(conf)\n        self.roberta = transformers.RobertaModel.from_pretrained(\n            \"/kaggle/input/bertweet-model/BERTweet_base_transformers/model.bin\",\n            config=conf\n        )\n        self.drop_out = nn.Dropout(0.1)\n        self.activation = nn.LeakyReLU()\n        self.l0 = nn.Linear(768 * 2, 2)\n\n        torch.nn.init.normal_(self.l0.weight, std=0.02)\n    \n    def forward(self, ids, mask, token_type_ids):\n        _, _, out = self.roberta(\n            ids,\n            attention_mask=mask,\n            token_type_ids=token_type_ids\n        )\n\n        out = torch.cat((out[-1], out[-2]), dim=-1)\n        out = self.drop_out(out)\n        logits = self.l0(out)\n\n        start_logits, end_logits = logits.split(1, dim=-1)\n\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n\n        return start_logits, end_logits","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# INFERENCE\ndf_test = pd.read_csv(\"/kaggle/input/tweet-sentiment-extraction/test.csv\")\ndf_test.loc[:, \"selected_text\"] = df_test.text.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_config = transformers.RobertaConfig.from_pretrained(\n                f\"{MODEL_INPUT_PATH}BERTweet_base_transformers/config.json\"\n            )\nmodel_config.output_hidden_states = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dataset = TweetDataset(\n        tweets=df_test.text.values,\n        sentiments=df_test.sentiment.values,\n        selected_texts=df_test.selected_text.values\n    )\n\ndata_loader = torch.utils.data.DataLoader(\n    test_dataset,\n    shuffle=False,\n    batch_size=VALID_BATCH_SIZE,\n    num_workers=1\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel1 = TweetModel(conf=model_config)\nmodel1.to(device)\nmodel1.load_state_dict(torch.load(f\"{INPUT_PATH}twitroberta/model_0.bin\"))\nmodel1.eval()\n\nmodel2 = TweetModel(conf=model_config)\nmodel2.to(device)\nmodel2.load_state_dict(torch.load(f\"{INPUT_PATH}twitroberta/model_1.bin\"))\nmodel2.eval()\n\nmodel3 = TweetModel(conf=model_config)\nmodel3.to(device)\nmodel3.load_state_dict(torch.load(f\"{INPUT_PATH}twitroberta/model_2.bin\"))\nmodel3.eval()\n\nmodel4 = TweetModel(conf=model_config)\nmodel4.to(device)\nmodel4.load_state_dict(torch.load(f\"{INPUT_PATH}twitroberta/model_3.bin\"))\nmodel4.eval()\n\nmodel5 = TweetModel(conf=model_config)\nmodel5.to(device)\nmodel5.load_state_dict(torch.load(f\"{INPUT_PATH}twitroberta/model_4.bin\"))\nmodel5.eval()\n\nmodel6 = TweetModel(conf=model_config)\nmodel6.to(device)\nmodel6.load_state_dict(torch.load(f\"{INPUT_PATH}twitroberta/model_5.bin\"))\nmodel6.eval()\n\nmodel7 = TweetModel(conf=model_config)\nmodel7.to(device)\nmodel7.load_state_dict(torch.load(f\"{INPUT_PATH}twitroberta/model_6.bin\"))\nmodel7.eval()\n\nmodel8 = TweetModel(conf=model_config)\nmodel8.to(device)\nmodel8.load_state_dict(torch.load(f\"{INPUT_PATH}twitroberta/model_7.bin\"))\nmodel8.eval()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_output = []\nimport string\nfrom nltk.corpus import stopwords\nstop_words = set(stopwords.words('english'))\nwith torch.no_grad():\n    tk0 = tqdm(data_loader, total=len(data_loader))\n    for bi, d in enumerate(tk0):\n        ids = d[\"ids\"]\n        token_type_ids = d[\"token_type_ids\"]\n        mask = d[\"mask\"]\n        sentiment = d[\"sentiment\"]\n        orig_selected = d[\"orig_selected\"]\n        orig_tweet = d[\"orig_tweet\"]\n        targets_start = d[\"targets_start\"]\n        targets_end = d[\"targets_end\"]\n\n        ids = ids.to(device, dtype=torch.long)\n        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n        mask = mask.to(device, dtype=torch.long)\n        targets_start = targets_start.to(device, dtype=torch.long)\n        targets_end = targets_end.to(device, dtype=torch.long)\n\n        outputs_start1, outputs_end1 = model1(\n            ids=ids,\n            mask=mask,\n            token_type_ids=token_type_ids\n        )\n        \n        outputs_start2, outputs_end2 = model2(\n            ids=ids,\n            mask=mask,\n            token_type_ids=token_type_ids\n        )\n        \n        outputs_start3, outputs_end3 = model3(\n            ids=ids,\n            mask=mask,\n            token_type_ids=token_type_ids\n        )\n        \n        outputs_start4, outputs_end4 = model4(\n            ids=ids,\n            mask=mask,\n            token_type_ids=token_type_ids\n        )\n        \n        outputs_start5, outputs_end5 = model5(\n            ids=ids,\n            mask=mask,\n            token_type_ids=token_type_ids\n        )\n        \n        outputs_start6, outputs_end6 = model6(\n            ids=ids,\n            mask=mask,\n            token_type_ids=token_type_ids\n        )\n        \n        outputs_start7, outputs_end7 = model7(\n            ids=ids,\n            mask=mask,\n            token_type_ids=token_type_ids\n        )\n        \n        outputs_start8, outputs_end8 = model8(\n            ids=ids,\n            mask=mask,\n            token_type_ids=token_type_ids\n        )\n        \n        outputs_start = (outputs_start1 + outputs_start2 + \n                         outputs_start3 + outputs_start4 + \n                         outputs_start5 + outputs_start7 + \n                         outputs_start6 + outputs_start8) / 8\n\n        outputs_end = (outputs_end1 + outputs_end2 + outputs_end3 + outputs_end4 +\n                       outputs_end5 + outputs_end6 + \n                       outputs_end7 + outputs_end8) / 8\n        \n        outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n        outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n        \n        for id, tweet in enumerate(orig_tweet):\n            a = np.argmax(outputs_start[id])\n            b = np.argmax(outputs_end[id])\n            if a > b:\n                selected_text = tweet\n            else:\n                n_tweet = '<s> ' + bpe.encode(tweet) + ' </s>'\n                nn_tweet = vocab.encode_line(n_tweet, append_eos=False, add_if_not_exist=False).long().tolist()\n                select = nn_tweet[a:b+1]\n                selected_text = bpe.decode(\" \".join([vocab[i] for i in select]))\n                selected_text = selected_text.replace(\"<s>\",\"\").replace(\"</s>\",\"\").replace(\"<unk>\", \"\")\n                if selected_text.split()[-1] in set(string.punctuation) or selected_text.split()[-1] in stop_words:\n                    selected_text = \" \".join(selected_text.split()[:-1])\n                if selected_text.strip() == \"\":\n                    selected_text = tweet\n            final_output.append(selected_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a = pd.read_csv(\"/kaggle/input/tweet-sentiment-extraction/sample_submission.csv\")\na.loc[:, 'selected_text'] = final_output\na.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a.sample(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}