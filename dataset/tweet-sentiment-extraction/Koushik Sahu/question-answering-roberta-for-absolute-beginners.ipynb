{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"This kernel is to show how roberta tokenizes and how to output of roberta looks like for absolute beginners. Once you see the output of the model you can pass the output through linear layers of desired dimension according to dataset and use case.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"I am using [Abhishek Thakur's](https://www.kaggle.com/abhishek) pretrained roberta base [model](https://www.kaggle.com/abhishek/roberta-base) and data from the contest [Tweet Sentiment Extraction](https://www.kaggle.com/c/tweet-sentiment-extraction) in this kernel.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport transformers\nimport tokenizers\nimport torch\nimport torch.nn as nn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/tweet-sentiment-extraction/train.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ROBERTA_PATH = '../input/roberta-base'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# How roberta tokenizes the text?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN = 192\nTOKENIZER = tokenizers.ByteLevelBPETokenizer(vocab_file=f\"{ROBERTA_PATH}/vocab.json\", \n                                             merges_file=f\"{ROBERTA_PATH}/merges.txt\", \n                                             add_prefix_space=True, \n                                             lowercase=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note here we have an argument add_prefix_space which is set to True so the tokenizer will add a space to start of the text passed into it.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tokens = TOKENIZER.encode(data.text.values[0])\ntokens","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokens.tokens","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems like to every word that is splitted by the tokenizer it is adding a special character 'Ä ' to the start of the first part of the splitted text.\n\nAlso it is to be noted that special tokens have not been added by the tokenizer.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tokens.ids","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokens.type_ids","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Roberta doesn't use the type_ids so we will be passing all zero vector of size of length of token ids everytime","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tokens.offsets","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here the first token is 'i' which is the first part when we split \"i'd\" in the input sentence. 'i' is the first character of the sentence so the offset should be (0, 1) but we have to take into account that a space was added by the tokenizer to the text.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tokens.attention_mask","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All ones as there is no padding\n\nThe above are the attributes that would be needed to train roBERTa","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## What happens if set add_prefix_space to false?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"expt_tokenizer = tokenizers.ByteLevelBPETokenizer(vocab_file=f\"{ROBERTA_PATH}/vocab.json\", \n                                                merges_file=f\"{ROBERTA_PATH}/merges.txt\", \n                                                add_prefix_space=False, \n                                                lowercase=True)\n\ntemp = expt_tokenizer.encode(data.text.values[0])\ntemp.tokens, temp.ids, temp.type_ids, temp.offsets, temp.attention_mask","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The results are exactly the same. The reason is that we require add_prefix_space as True because tokenizer need a space to start the input string. If we set it to False then tokenizer encode and decode method will not conserve the absence of a space at the beginning of a string. Look at the example below","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"expt_tokenizer.decode(expt_tokenizer.encode(\"Hello\").ids)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TOKENIZER.decode(TOKENIZER.encode(\"Hello\").ids)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Notice the difference in results","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Pretrained RoBERTa output","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"conf = transformers.ReformerConfig.from_pretrained(ROBERTA_PATH)\nmodel = transformers.RobertaModel.from_pretrained(ROBERTA_PATH, config=conf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ids = torch.tensor([[0] + tokens.ids + [2]])\nattention_mask = torch.tensor([[1, 1] + tokens.attention_mask])\ntoken_type_ids = torch.tensor([tokens.type_ids + [0, 0]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output = model(ids, attention_mask=attention_mask, token_type_ids=token_type_ids)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(output)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output[0].shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The first output is the sequenced output. One 768 sized tensor for each of the 12 tokens.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"output[1].shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The second output is the result of pooling together all of the 768 length layers of first output.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Using the question answering model from huggingface with pretrained model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = transformers.RobertaForQuestionAnswering.from_pretrained('roberta-base')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will be using the tokenizer from above examples","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ques = \"What is the name of prime minister of India?\"\ntext = \"India is one of the largest country in the world and its current prime minister is Narendra Modi.\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tok_ques = TOKENIZER.encode(ques)\ntok_text = TOKENIZER.encode(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(tok_ques.ids), len(tok_text.ids)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ids = torch.tensor([[0] + tok_ques.ids + [2, 2] + tok_text.ids + [2]])\nattention_mask = torch.tensor([[1] + tok_ques.attention_mask + [1, 1] + tok_text.attention_mask + [1]])\n# roberta doesn't make use of token_type_ids so we can have a all zero tensor of correct dimension\ntoken_type_ids = torch.tensor([[0] + tok_ques.type_ids + [0, 0] + tok_text.type_ids + [0]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start, end = model(ids, attention_mask=attention_mask, token_type_ids=token_type_ids)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = nn.Softmax()(start)\nend = nn.Softmax()(end)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start.shape, end.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n = start.shape[1]\nmax_ij = 0\n\nstart_idx = None\nend_idx = None\n\nfor i in range(14, n-2):\n    for j in range(i+1, n-1):\n        if start[0][i] + end[0][j] > max_ij:\n            max_ij = start[0][i] + end[0][j]\n            start_idx = i\n            end_idx = j","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start_idx, end_idx, max_ij","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = list(ids[0][start_idx: end_idx+1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TOKENIZER.decode(ids=result)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Without any fine tuning it is just way off the answer. The point of this kernel wasn't have accuracy but to show how we can use huggingface's implementation of RoBERTa.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"With fine tuning and better function to choose start and end index roberta can give very accurate results","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# fin","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}