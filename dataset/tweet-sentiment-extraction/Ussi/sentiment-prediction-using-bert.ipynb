{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport transformers as ppb\n\nfrom tqdm import tqdm_notebook as tqdm\nimport random\nimport matplotlib.pyplot as plt\nimport warnings\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input/tweet-sentiment-extraction/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nwarnings.simplefilter('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1.Make a Dataset and DataLoaders","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def label_encoding(sentiment):\n    #label encoding\n    if sentiment == 'positive':\n        return 0\n    elif sentiment == 'negative':\n        return 1\n    else:\n        return 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tokenize_fn(text):\n    #BERT model宣言\n    tokenizer = ppb.BertTokenizerFast.from_pretrained('/kaggle/input/bert-base-uncased/')\n    if str(text) == 'nan':\n        text = ' '\n    text = text.lower()\n    #tokenize using BERTtokenizer\n    text = tokenizer.encode(text,do_lower_case=True)\n    \n    text = np.pad(text,[0,110-len(text)],'constant')\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Mydatasets(torch.utils.data.Dataset):\n    def __init__(self, path):\n        self.data = pd.read_csv(path)\n        self.tokenized = torch.empty(self.data.shape[0], 110)\n        self.labels = torch.empty(self.data.shape[0])\n        \n        for i in tqdm(range(self.data.shape[0])):\n            self.tokenized[i] = torch.from_numpy(tokenize_fn(self.data['text'][i]))\n            self.labels[i] = label_encoding(self.data['sentiment'][i])\n        self.len = self.tokenized.shape[0]\n        \n    def __len__(self):\n        return self.len\n\n    def __getitem__(self, idx):\n        out_data = self.tokenized[idx]\n        out_label = self.labels[idx]\n\n        return out_data, out_label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainset = Mydatasets('/kaggle/input/tweet-sentiment-extraction/train.csv')\ntestset = Mydatasets('/kaggle/input/tweet-sentiment-extraction/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_size = int(0.8 * len(trainset))\ntest_size = len(trainset) - train_size\ntrain_dataset, val_dataset = torch.utils.data.random_split(trainset, [train_size, test_size])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 16\n\ntrainloader = torch.utils.data.DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle = True, num_workers = 4)\ntestloader = torch.utils.data.DataLoader(testset, batch_size = BATCH_SIZE, shuffle = False, num_workers = 4)\nvalloader = torch.utils.data.DataLoader(val_dataset, batch_size = BATCH_SIZE, shuffle = False, num_workers = 4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Create sentiment prediction model using BERT","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class BERTmodel(nn.Module):\n    def __init__(self,bert):\n        super(BERTmodel,self).__init__()\n        self.bert = bert\n        #in_feature=768 depend on output size of BERT model\n        self.cls = nn.Linear(in_features=768,out_features=3)\n    \n    def forward(self,x,token_type_ids=None,attention_mask=None):\n        encoded_layers,_ = self.bert(x,token_type_ids,attention_mask)\n        word_vec = encoded_layers[:,0,:].view(-1,768)\n        out = self.cls(word_vec)\n        return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(net,optimizer,criterion,epochs,batch_size,trainset,valset):\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n    print('device:',device)\n    print('---start---')\n    net.to(device)\n    torch.backends.cudnn.benchmark = True\n    \n    train_acc_list,train_loss_list = [],[]\n    val_acc_list,val_loss_list = [],[]\n    \n    for epoch in range(epochs):\n        for phase in ['train','val']:\n            print('---%s_%d---' % (phase,epoch))\n            if phase == 'train':\n                dataset = trainset\n                net.train()\n            else:\n                dataset = valset\n                net.eval()\n            epoch_loss = 0.0\n            epoch_corrects = 0\n            total = 0\n            \n            for inputs,labels in tqdm(dataset):\n                inputs = inputs.long().to(device)\n                labels = labels.long().to(device)\n                optimizer.zero_grad()\n                with torch.set_grad_enabled(phase=='train'):\n                    outputs = net(inputs)\n                    loss = criterion(outputs,labels)\n                    \n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n                        \n                    pred = torch.argmax(outputs,dim=1)\n                    total += labels.size(0)\n                epoch_corrects += (pred == labels).sum().item()\n                epoch_loss += loss.item()\n                \n            if phase == 'train':\n                train_loss_list.append(epoch_loss/len(dataset))\n                train_acc_list.append(100 * epoch_corrects / total)\n            else:\n                val_loss_list.append(epoch_loss/len(dataset))\n                val_acc_list.append(100 * epoch_corrects / total)\n            print('Loss: %f' % (epoch_loss/len(dataset)))\n            print('Accuracy: %f' % (epoch_corrects/total))\n            \n    return (train_acc_list,train_loss_list),(val_acc_list,val_loss_list)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## BERT hyper-parameters","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"EPOCHS=20\n\nbert = ppb.BertModel.from_pretrained('/kaggle/input/bert-base-uncased/')\nnet = BERTmodel(bert)\n\n# No gradient calculation for the 1st to 11th transformer. \n# It should be all calculated, but to reduce the cost of calculation.\nfor name, param in net.named_parameters():\n    param.requires_grad = False\n    \n# 12th transformer with gradient calculation.\nfor name, param in net.bert.encoder.layer[-1].named_parameters():\n    param.requires_grad = True\n\n#dense layer with gradient calculation.(fine-tuning)\nfor name, param in net.cls.named_parameters():\n    param.requires_grad = True\n\n#param update\noptimizer = torch.optim.Adam([{'params': net.bert.encoder.layer[-1].parameters(),'lr':5e-5},\n                        {'params': net.cls.parameters(),'lr': 5e-5}], betas=(0.9,0.999))\n\n#loss function\ncriterion = nn.CrossEntropyLoss() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_result,val_result = train(net,optimizer,criterion,EPOCHS,BATCH_SIZE,trainloader,valloader)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Results","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_history(index,train_data,val_data,title):\n    # Plot the loss in the history\n    plt.plot(index, train_data, label='train')\n    plt.plot(index, val_data, label='val')\n    plt.title(title)\n    plt.xlabel('epoch')\n    plt.ylabel(title)\n    plt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"index = np.arange(20)\nplot_history(index,train_result[0],val_result[0],'acc')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_history(index,train_result[1],val_result[1],'loss')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\ntorch.backends.cudnn.benchmark = True\nnet.eval()\nepoch_corrects = 0\ntotal = 0\n\nfor inputs,labels in tqdm(testloader):\n    inputs = inputs.long().to(device)\n    labels = labels.long().to(device)\n    optimizer.zero_grad()\n    with torch.set_grad_enabled(False):\n        outputs = net(inputs)\n        loss = criterion(outputs,labels)\n        pred = torch.argmax(outputs,dim=1)\n        total += labels.size(0)\n    epoch_corrects += (pred == labels).sum().item()\n\nprint('Accuracy: %f' % (epoch_corrects/total))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}