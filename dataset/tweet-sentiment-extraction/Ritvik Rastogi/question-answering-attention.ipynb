{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-09T09:36:29.774119Z","iopub.execute_input":"2022-01-09T09:36:29.774491Z","iopub.status.idle":"2022-01-09T09:36:31.010671Z","shell.execute_reply.started":"2022-01-09T09:36:29.774377Z","shell.execute_reply":"2022-01-09T09:36:31.009562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers as L\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","metadata":{"execution":{"iopub.status.busy":"2022-01-09T09:36:31.012525Z","iopub.execute_input":"2022-01-09T09:36:31.012777Z","iopub.status.idle":"2022-01-09T09:36:37.781215Z","shell.execute_reply.started":"2022-01-09T09:36:31.012746Z","shell.execute_reply":"2022-01-09T09:36:37.780203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Data Preparation\n\nOur model would take in 2 inputs, ie context and question, and based on that it should be able to find a substring of the context as the answer to the question.\n\nSince answer is a substring of the context, we will use the same tokenizer for context and answer.\n\nWe will prepare the context, question and answer tensors:\n* tokenize and pad the context using context tokenizer to form context tensor\n* tokenize and pad the answer using context tokenizer\n* calculate the beginning and ending index of answer sub array in the context array as one hot encoded arrays and pad them\n* tokenize and pad the question using question tokenizer to form question tensor\n* concatenate beginning and ending indicies tensors to form the answer tensor","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/tweet-sentiment-extraction/train.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-09T09:36:37.782673Z","iopub.execute_input":"2022-01-09T09:36:37.782973Z","iopub.status.idle":"2022-01-09T09:36:37.954814Z","shell.execute_reply.started":"2022-01-09T09:36:37.782937Z","shell.execute_reply":"2022-01-09T09:36:37.953624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"context_tokenizer = Tokenizer()\ncontext_tokenizer.fit_on_texts(df.text.fillna(''))\ncontext = context_tokenizer.texts_to_sequences(df.text.fillna(''))\n\nanswers = context_tokenizer.texts_to_sequences(df.selected_text.fillna(''))\nbeg_pos = [[1 if a[x:x+len(b)] == b else 0 for x in range(len(a))] for a, b in zip(context, answers)]\nend_pos = [[1 if a[x:x+len(b)] == b else 0 for x in range(len(a))] for a, b in zip(context, answers)]","metadata":{"execution":{"iopub.status.busy":"2022-01-09T09:36:37.957062Z","iopub.execute_input":"2022-01-09T09:36:37.957606Z","iopub.status.idle":"2022-01-09T09:36:40.010462Z","shell.execute_reply.started":"2022-01-09T09:36:37.957566Z","shell.execute_reply":"2022-01-09T09:36:40.009599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"context = np.array(pad_sequences(context, maxlen=36, padding='post', truncating='post'))\nbeg_pos = np.array(pad_sequences(beg_pos, maxlen=36, padding='post', truncating='post'))\nend_pos = np.array(pad_sequences(end_pos, maxlen=36, padding='post', truncating='post'))\n\nall_zero = np.all((beg_pos == 0), axis=1)\n\ncontext = context[~all_zero]\nbeg_pos = beg_pos[~all_zero]\nend_pos = end_pos[~all_zero]\n\nbeg_pos = np.expand_dims(beg_pos, axis=2)\nend_pos = np.expand_dims(end_pos, axis=2)\nans_vec = np.concatenate((beg_pos, end_pos), axis=2)\n\ncontext.shape, beg_pos.shape, end_pos.shape, ans_vec.shape","metadata":{"execution":{"iopub.status.busy":"2022-01-09T09:36:40.011961Z","iopub.execute_input":"2022-01-09T09:36:40.01305Z","iopub.status.idle":"2022-01-09T09:36:40.763905Z","shell.execute_reply.started":"2022-01-09T09:36:40.012988Z","shell.execute_reply":"2022-01-09T09:36:40.762963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"question_tokenizer = Tokenizer()\nquestion_tokenizer.fit_on_texts(df.sentiment.fillna(''))\nquestion = question_tokenizer.texts_to_sequences(df.sentiment.fillna(''))\nquestion = np.array(pad_sequences(question, maxlen=36, padding='post', truncating='post'))\nquestion = question[~all_zero]\nquestion.shape","metadata":{"execution":{"iopub.status.busy":"2022-01-09T09:36:40.76516Z","iopub.execute_input":"2022-01-09T09:36:40.765398Z","iopub.status.idle":"2022-01-09T09:36:41.372049Z","shell.execute_reply.started":"2022-01-09T09:36:40.765368Z","shell.execute_reply":"2022-01-09T09:36:41.370952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"context_train, context_valid, question_train, question_valid, ans_vec_train, ans_vec_valid = train_test_split(\n    context, question, ans_vec, test_size=0.1, random_state=0\n)\n(\n    context_train.shape, context_valid.shape, question_train.shape, \n    question_valid.shape, ans_vec_train.shape, ans_vec_valid.shape\n)","metadata":{"execution":{"iopub.status.busy":"2022-01-09T09:36:41.373318Z","iopub.execute_input":"2022-01-09T09:36:41.373617Z","iopub.status.idle":"2022-01-09T09:36:41.398452Z","shell.execute_reply.started":"2022-01-09T09:36:41.37355Z","shell.execute_reply":"2022-01-09T09:36:41.397615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model\n\nAn Extractive Question Answering Model is pretty simple and similar to that of token classification, ie there are only a couple of changes, since we need two text inputs, we embed both of them and then pass through the encoder blocks and then we concatenate them, and the final output layer does a multilabel token classification with two classes, where 1 class represent beginning of answer and other class means end of the asnwer therefore the architechture looks something like:\n\n* Input Layers\n* Embeddingss\n* Transformer Encoder Blocks\n* Dropout (optional)\n* Concatenation\n* Classification Layer","metadata":{}},{"cell_type":"code","source":"class PositionalEmbedding(L.Layer):\n    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n        super().__init__(**kwargs)\n        self.token_embeddings = L.Embedding(input_dim, output_dim)\n        self.position_embeddings = L.Embedding(sequence_length, output_dim)\n        self.sequence_length = sequence_length\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        \n    def call(self, inputs):\n        length = tf.shape(inputs)[-1]\n        positions = tf.range(start=0, limit=length, delta=1)\n        embedded_tokens = self.token_embeddings(inputs)\n        embedded_positions = self.position_embeddings(positions)\n        return embedded_tokens + embedded_positions\n        \n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \"output_dim\": self.output_dim,\n            \"sequence_length\": self.sequence_length,\n            \"input_dim\": self.input_dim,\n        })\n        return config\n    \nclass TransformerEncoder(L.Layer):\n    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim\n        self.dense_dim = dense_dim\n        self.num_heads = num_heads\n        self.attention = L.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.dense_proj = keras.Sequential([L.Dense(dense_dim, activation='relu'), L.Dense(embed_dim)])\n        self.layernorm1 = L.LayerNormalization()\n        self.layernorm2 = L.LayerNormalization()\n    \n    def call(self, inputs, mask=None):\n        if mask is not None:\n            mask = mask[: tf.newaxis, :]\n        attention_output = self.attention(inputs, inputs, attention_mask=mask)\n        proj_input = self.layernorm1(inputs + attention_output)\n        proj_output = self.dense_proj(proj_input)\n        return self.layernorm2(proj_input + proj_output)\n    \n    def get_config(self):\n        config = super().get_confog()\n        config.update({\n            \"embed_dim\": self.embed_dim,\n            \"num_heads\": self.num_heads,\n            \"dense_dim\": self.dense_dim\n        })\n        return config    ","metadata":{"execution":{"iopub.status.busy":"2022-01-09T09:36:41.400074Z","iopub.execute_input":"2022-01-09T09:36:41.400572Z","iopub.status.idle":"2022-01-09T09:36:41.416795Z","shell.execute_reply.started":"2022-01-09T09:36:41.40053Z","shell.execute_reply":"2022-01-09T09:36:41.415952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_LEN = 36\nEMBED_DIM = 64\nDENSE_DIM = 64\nNUM_HEADS = 2\n\ncontext_inp = L.Input(shape=(MAX_LEN, ), name='context')\nquestion_inp = L.Input(shape=(MAX_LEN, ), name='question')\n\ncontext_emb = PositionalEmbedding(\n    MAX_LEN, len(context_tokenizer.word_index)+1, EMBED_DIM, name='context_embeddings'\n)(context_inp)\nquestion_emb = PositionalEmbedding(\n    MAX_LEN, len(question_tokenizer.word_index)+1, EMBED_DIM, name='question_embeddings'\n)(question_inp)\n\ncontext_emb = TransformerEncoder(EMBED_DIM, DENSE_DIM, NUM_HEADS, name='context_encoder')(context_emb)\nquestion_emb = TransformerEncoder(EMBED_DIM, DENSE_DIM, NUM_HEADS, name='question_encoder')(question_emb)\n\nconcat_emb = L.Concatenate(axis=-1, name='concatenate')([context_emb, question_emb])\n\noutputs = L.Dense(2, activation='sigmoid', name='outputs')(concat_emb)\n\nmodel = keras.Model(inputs=[context_inp, question_inp], outputs=outputs)\nmodel.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(1e-4))\nmodel.summary()\nkeras.utils.plot_model(model, show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-09T09:36:41.418093Z","iopub.execute_input":"2022-01-09T09:36:41.418732Z","iopub.status.idle":"2022-01-09T09:36:43.00876Z","shell.execute_reply.started":"2022-01-09T09:36:41.418698Z","shell.execute_reply":"2022-01-09T09:36:43.00349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"es = keras.callbacks.EarlyStopping(min_delta=1e-4, patience=5, verbose=1, restore_best_weights=True)\nrlp = keras.callbacks.ReduceLROnPlateau(patience=2, verbose=1)\n\nhistory = model.fit(\n    [context_train, question_train], ans_vec_train, validation_data=([context_valid, question_valid], ans_vec_valid),\n    epochs=25, callbacks=[es, rlp]\n)","metadata":{"execution":{"iopub.status.busy":"2022-01-09T09:36:43.01313Z","iopub.execute_input":"2022-01-09T09:36:43.013819Z","iopub.status.idle":"2022-01-09T09:41:03.385669Z","shell.execute_reply.started":"2022-01-09T09:36:43.01377Z","shell.execute_reply":"2022-01-09T09:41:03.384368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(history.history)[['loss', 'val_loss']].plot();","metadata":{"execution":{"iopub.status.busy":"2022-01-09T09:41:03.387353Z","iopub.execute_input":"2022-01-09T09:41:03.387691Z","iopub.status.idle":"2022-01-09T09:41:08.749966Z","shell.execute_reply.started":"2022-01-09T09:41:03.387657Z","shell.execute_reply":"2022-01-09T09:41:08.748936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"idx = 28\nquery_context = context_valid[idx:idx+1]\nquery_question = question_valid[idx:idx+1]\nquery_ans_vec = ans_vec_valid[idx:idx+1]\nquery_ans_beg, query_ans_end  = np.ravel(ans_vec_valid[idx:idx+1].argmax(axis=1))\nprint('Context:', context_tokenizer.sequences_to_texts(query_context))\nprint('Question:', question_tokenizer.sequences_to_texts(query_question))\nprint('Answer:', context_tokenizer.sequences_to_texts([query_context[0][query_ans_beg: query_ans_end+1]]))\npred_ans_beg, pred_ans_end = np.ravel(model([query_context, query_question]).numpy().argmax(axis=1))\nprint('Predicted Answer:', context_tokenizer.sequences_to_texts([query_context[0][pred_ans_beg: pred_ans_end+1]]))","metadata":{"execution":{"iopub.status.busy":"2022-01-09T09:41:08.751398Z","iopub.execute_input":"2022-01-09T09:41:08.751727Z","iopub.status.idle":"2022-01-09T09:41:08.822755Z","shell.execute_reply.started":"2022-01-09T09:41:08.751644Z","shell.execute_reply":"2022-01-09T09:41:08.821799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}