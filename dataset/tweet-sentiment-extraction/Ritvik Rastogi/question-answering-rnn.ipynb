{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-09T08:42:21.620711Z","iopub.execute_input":"2022-01-09T08:42:21.62106Z","iopub.status.idle":"2022-01-09T08:42:22.714654Z","shell.execute_reply.started":"2022-01-09T08:42:21.620959Z","shell.execute_reply":"2022-01-09T08:42:22.713713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers as L\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","metadata":{"execution":{"iopub.status.busy":"2022-01-09T08:42:22.716578Z","iopub.execute_input":"2022-01-09T08:42:22.716896Z","iopub.status.idle":"2022-01-09T08:42:28.651778Z","shell.execute_reply.started":"2022-01-09T08:42:22.716855Z","shell.execute_reply":"2022-01-09T08:42:28.650993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Data Preparation","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/tweet-sentiment-extraction/train.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-09T08:42:28.652991Z","iopub.execute_input":"2022-01-09T08:42:28.653278Z","iopub.status.idle":"2022-01-09T08:42:28.804126Z","shell.execute_reply.started":"2022-01-09T08:42:28.653239Z","shell.execute_reply":"2022-01-09T08:42:28.803286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"context_tokenizer = Tokenizer()\ncontext_tokenizer.fit_on_texts(df.text.fillna(''))\ncontext = context_tokenizer.texts_to_sequences(df.text.fillna(''))\n\nanswers = context_tokenizer.texts_to_sequences(df.selected_text.fillna(''))\nbeg_pos = [[1 if a[x:x+len(b)] == b else 0 for x in range(len(a))] for a, b in zip(context, answers)]\nend_pos = [[1 if a[x:x+len(b)] == b else 0 for x in range(len(a))] for a, b in zip(context, answers)]","metadata":{"execution":{"iopub.status.busy":"2022-01-09T08:42:28.806371Z","iopub.execute_input":"2022-01-09T08:42:28.806772Z","iopub.status.idle":"2022-01-09T08:42:30.720297Z","shell.execute_reply.started":"2022-01-09T08:42:28.806728Z","shell.execute_reply":"2022-01-09T08:42:30.719649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"context = np.array(pad_sequences(context, maxlen=36, padding='post', truncating='post'))\nbeg_pos = np.array(pad_sequences(beg_pos, maxlen=36, padding='post', truncating='post'))\nend_pos = np.array(pad_sequences(end_pos, maxlen=36, padding='post', truncating='post'))\n\nall_zero = np.all((beg_pos == 0), axis=1)\n\ncontext = context[~all_zero]\nbeg_pos = beg_pos[~all_zero]\nend_pos = end_pos[~all_zero]\n\nbeg_pos = np.expand_dims(beg_pos, axis=2)\nend_pos = np.expand_dims(end_pos, axis=2)\nans_vec = np.concatenate((beg_pos, end_pos), axis=2)\n\ncontext.shape, beg_pos.shape, end_pos.shape, ans_vec.shape","metadata":{"execution":{"iopub.status.busy":"2022-01-09T08:42:30.721803Z","iopub.execute_input":"2022-01-09T08:42:30.722368Z","iopub.status.idle":"2022-01-09T08:42:31.478817Z","shell.execute_reply.started":"2022-01-09T08:42:30.72231Z","shell.execute_reply":"2022-01-09T08:42:31.478063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"question_tokenizer = Tokenizer()\nquestion_tokenizer.fit_on_texts(df.sentiment.fillna(''))\nquestion = question_tokenizer.texts_to_sequences(df.sentiment.fillna(''))\nquestion = np.array(pad_sequences(question, maxlen=36, padding='post', truncating='post'))\nquestion = question[~all_zero]\nquestion.shape","metadata":{"execution":{"iopub.status.busy":"2022-01-09T08:42:31.479886Z","iopub.execute_input":"2022-01-09T08:42:31.480115Z","iopub.status.idle":"2022-01-09T08:42:32.080022Z","shell.execute_reply.started":"2022-01-09T08:42:31.480087Z","shell.execute_reply":"2022-01-09T08:42:32.079195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"context_train, context_valid, question_train, question_valid, ans_vec_train, ans_vec_valid = train_test_split(\n    context, question, ans_vec, test_size=0.1, random_state=0\n)\n(\n    context_train.shape, context_valid.shape, question_train.shape, \n    question_valid.shape, ans_vec_train.shape, ans_vec_valid.shape\n)","metadata":{"execution":{"iopub.status.busy":"2022-01-09T08:42:32.081286Z","iopub.execute_input":"2022-01-09T08:42:32.081674Z","iopub.status.idle":"2022-01-09T08:42:32.104001Z","shell.execute_reply.started":"2022-01-09T08:42:32.08163Z","shell.execute_reply":"2022-01-09T08:42:32.103481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"EMBED_DIM = 64\nN_REC = 64\n\ncontext_inp = L.Input(shape=(36, ), name='context')\nquestion_inp = L.Input(shape=(36, ), name='question')\n\ncontext_emb = L.Embedding(len(context_tokenizer.word_index)+1, EMBED_DIM, name='context_embeddings')(context_inp)\nquestion_emb = L.Embedding(len(question_tokenizer.word_index)+1, EMBED_DIM, name='question_embeddings')(question_inp)\n\ncontext_emb = L.GRU(N_REC, return_sequences=True, name='context_gru')(context_emb)\nquestion_emb = L.GRU(N_REC, return_sequences=True, name='question_gru')(question_emb)\n\nconcat_emb = L.Concatenate(axis=-1, name='concatenate')([context_emb, question_emb])\n\noutputs = L.Dense(2, activation='sigmoid', name='outputs')(concat_emb)\n\nmodel = keras.Model(inputs=[context_inp, question_inp], outputs=outputs)\nmodel.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(1e-4))\nmodel.summary()\nkeras.utils.plot_model(model, show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-09T08:42:32.105357Z","iopub.execute_input":"2022-01-09T08:42:32.105681Z","iopub.status.idle":"2022-01-09T08:42:33.599515Z","shell.execute_reply.started":"2022-01-09T08:42:32.105649Z","shell.execute_reply":"2022-01-09T08:42:33.598828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"es = keras.callbacks.EarlyStopping(min_delta=1e-4, patience=5, verbose=1, restore_best_weights=True)\nrlp = keras.callbacks.ReduceLROnPlateau(patience=2, verbose=1)\n\nhistory = model.fit(\n    [context_train, question_train], ans_vec_train, validation_data=([context_valid, question_valid], ans_vec_valid),\n    epochs=25, callbacks=[es, rlp]\n)","metadata":{"execution":{"iopub.status.busy":"2022-01-09T08:42:33.600824Z","iopub.execute_input":"2022-01-09T08:42:33.601584Z","iopub.status.idle":"2022-01-09T08:50:50.370954Z","shell.execute_reply.started":"2022-01-09T08:42:33.601548Z","shell.execute_reply":"2022-01-09T08:50:50.370129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(history.history)[['loss', 'val_loss']].plot();","metadata":{"execution":{"iopub.status.busy":"2022-01-09T08:50:50.37411Z","iopub.execute_input":"2022-01-09T08:50:50.374432Z","iopub.status.idle":"2022-01-09T08:50:50.663817Z","shell.execute_reply.started":"2022-01-09T08:50:50.374376Z","shell.execute_reply":"2022-01-09T08:50:50.663165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"idx = 28\nquery_context = context_valid[idx:idx+1]\nquery_question = question_valid[idx:idx+1]\nquery_ans_vec = ans_vec_valid[idx:idx+1]\nquery_ans_beg, query_ans_end  = np.ravel(ans_vec_valid[idx:idx+1].argmax(axis=1))\nprint('Context:', context_tokenizer.sequences_to_texts(query_context))\nprint('Question:', question_tokenizer.sequences_to_texts(query_question))\nprint('Answer:', context_tokenizer.sequences_to_texts([query_context[0][query_ans_beg: query_ans_end+1]]))\npred_ans_beg, pred_ans_end = np.ravel(model([query_context, query_question]).numpy().argmax(axis=1))\nprint('Predicted Answer:', context_tokenizer.sequences_to_texts([query_context[0][pred_ans_beg: pred_ans_end+1]]))","metadata":{"execution":{"iopub.status.busy":"2022-01-09T08:50:50.665392Z","iopub.execute_input":"2022-01-09T08:50:50.66595Z","iopub.status.idle":"2022-01-09T08:50:50.756561Z","shell.execute_reply.started":"2022-01-09T08:50:50.665906Z","shell.execute_reply":"2022-01-09T08:50:50.755603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}