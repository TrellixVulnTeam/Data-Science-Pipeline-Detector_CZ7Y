{"cells":[{"metadata":{},"cell_type":"markdown","source":"# TensorFlow BERTweet","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"This notebook uses BERTweet from:\n\nhttps://github.com/VinAIResearch/BERTweet\n\nhttps://arxiv.org/abs/2005.10200\n\n(see discussion here: https://www.kaggle.com/c/tweet-sentiment-extraction/discussion/152861)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"This is mostly adjusting and combining code from the following:\n\nhttps://www.kaggle.com/cdeotte/tensorflow-roberta-0-705 (for majority of script)\n\nhttps://www.kaggle.com/al0kharba/tensorflow-roberta-0-712 (for inference and CNN head, switched from dropout to batch normalization)\n\nhttps://www.kaggle.com/christofhenkel/setup-tokenizer (for tokenizer)\n\nhttps://www.kaggle.com/nandhuelan/bertweet-first-look (for offsets and decoding)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Changes from V2: Changed CNN head and added LR schedule\n\nChanges from V3: Adjusted epochs and batch size to get fold 4 training to learn (last iteration was stuck and produced a 0.65 jaccard)\n\nChanges from V6: Adjusted post-processing for unk tokens. Added jaccard scores for sentiments. Set neutral equal to text instead of relying on model (jaccard is higher in CV when we do this).","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Load  data and libraries","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"See tips on how to install offline: https://www.kaggle.com/c/severstal-steel-defect-detection/discussion/113195","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"internet_on = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if internet_on==True:\n    !pip install fairseq fastBPE ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if internet_on==True:\n    !ls /root/.cache/pip/wheels/df/60/ff/1764bce64cccd9d2c06ba19e5f6f4108ad29e2d48e1068c684","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if internet_on==True:\n    !ls /root/.cache/pip/wheels/fb/85/9b/286072121774d5b8b0253ab66271b558069189cbe795bc6084","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if internet_on==True:\n    !mv /root/.cache/pip/wheels/df/60/ff/1764bce64cccd9d2c06ba19e5f6f4108ad29e2d48e1068c684/* /kaggle/working\n    !mv /root/.cache/pip/wheels/fb/85/9b/286072121774d5b8b0253ab66271b558069189cbe795bc6084/* /kaggle/working","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note: I could not figure out how to get sacrebleu, but found it loaded to Kaggle already","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"if internet_on==False:\n    !pip install ../input/fairseq-and-fastbpe/sacrebleu-1.4.9-py3-none-any.whl ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# These files were saved in version 1 of this notebook when internet_on was True\nif internet_on==False:\n    !pip install ../input/v1-fairseq-fastbpe/fastBPE-0.1.0-cp36-cp36m-linux_x86_64.whl\n    !pip install ../input/v1-fairseq-fastbpe/fairseq-0.9.0-cp36-cp36m-linux_x86_64.whl","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd, numpy as np\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom sklearn.model_selection import StratifiedKFold\nfrom transformers import *\nimport tokenizers\nprint('TF version',tf.__version__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#From: https://www.kaggle.com/christofhenkel/setup-tokenizer\nfrom types import SimpleNamespace\nfrom fairseq.data.encoders.fastbpe import fastBPE\nfrom fairseq.data import Dictionary\n\nclass BERTweetTokenizer():\n    \n    def __init__(self,pretrained_path = 'pretrained_models/BERTweet_base_transformers/'):\n        \n        self.bpe = fastBPE(SimpleNamespace(bpe_codes= pretrained_path + \"bpe.codes\"))\n        self.vocab = Dictionary()\n        self.vocab.add_from_file(pretrained_path + \"dict.txt\")\n        self.cls_token_id = 0\n        self.pad_token_id = 1\n        self.sep_token_id = 2\n        self.pad_token = '<pad>'\n        self.cls_token = '<s>'\n        self.sep_token = '</s>'\n        \n    def bpe_encode(self,text):\n        return self.bpe.encode(text)\n    \n    def encode(self,text,add_special_tokens=False):\n        subwords = self.bpe.encode(text)\n        input_ids = self.vocab.encode_line(subwords, append_eos=False, add_if_not_exist=False).long().tolist()\n        return input_ids\n    \n    def tokenize(self,text):\n        return self.bpe_encode(text).split()\n    \n    def convert_tokens_to_ids(self,tokens):\n        input_ids = self.vocab.encode_line(' '.join(tokens), append_eos=False, add_if_not_exist=False).long().tolist()\n        return input_ids\n    \n    #from: https://www.kaggle.com/nandhuelan/bertweet-first-look\n    def decode_id(self,id):\n        return self.vocab.string(id, bpe_symbol = '@@')\n    \n    def decode_id_nospace(self,id):\n        return self.vocab.string(id, bpe_symbol = '@@ ')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = BERTweetTokenizer('/kaggle/input/bertweet-base-transformers/')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer.encode('positive')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer.encode('negative')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer.encode('neutral')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer.decode_id([14058])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_train():\n    train=pd.read_csv('../input/tweet-sentiment-extraction/train.csv')\n    train['text']=train['text'].astype(str)\n    train['selected_text']=train['selected_text'].astype(str)\n    return train\n\ndef read_test():\n    test=pd.read_csv('../input/tweet-sentiment-extraction/test.csv')\n    test['text']=test['text'].astype(str)\n    return test\n\ndef read_submission():\n    test=pd.read_csv('../input/tweet-sentiment-extraction/sample_submission.csv')\n    return test\n    \ntrain_df = read_train()\ntest_df = read_test()\nsubmission_df = read_submission()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.sentiment.value_counts(dropna=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def jaccard(str1, str2): \n    a = set(str(str1).lower().split()) \n    b = set(str(str2).lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data preproccesing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN = 96\nPATH = '../input/bertweet-base-transformers/'\nsentiment_id = {'positive': 1809, 'negative': 3392, 'neutral': 14058}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ct = train_df.shape[0]\ninput_ids = np.ones((ct,MAX_LEN),dtype='int32')\nattention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\ntoken_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\nstart_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\nend_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n\nfor k in range(train_df.shape[0]):\n    \n    # FIND OVERLAP\n    text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n    text2 = \" \".join(train_df.loc[k,'selected_text'].split())\n    idx = text1.find(text2)\n    chars = np.zeros((len(text1)))\n    chars[idx:idx+len(text2)]=1\n    if text1[idx-1]==' ': chars[idx-1] = 1 \n    enc = tokenizer.encode(text1) \n    \n    # ID_OFFSETS\n    # From: https://www.kaggle.com/nandhuelan/bertweet-first-look (comments)\n    offsets = []; idx=0\n    for t in enc:\n        w = tokenizer.decode_id([t])\n        if text1[text1.find(w,idx)-1] == \" \":\n            idx+=1\n            offsets.append((idx,idx+len(w)))\n            idx += len(w)\n        else:\n            offsets.append((idx,idx+len(w)))\n            idx += len(w)\n\n    # START END TOKENS\n    toks = []\n    for i,(a,b) in enumerate(offsets):\n        sm = np.sum(chars[a:b])\n        if sm>0: toks.append(i) \n        \n    s_tok = sentiment_id[train_df.loc[k,'sentiment']]\n    if len(enc)<92:\n        input_ids[k,:len(enc)+5] = [0] + enc + [2,2] + [s_tok] + [2]\n        attention_mask[k,:len(enc)+5] = 1\n        if len(toks)>0:\n            start_tokens[k,toks[0]+1] = 1\n            end_tokens[k,toks[-1]+1] = 1        \n    if len(enc)>91:\n        input_ids[k,:96] = [0] + enc[:91] + [2,2] + [s_tok] + [2]\n        attention_mask[k,:96] = 1        \n        if len(toks)>0:\n            start_tokens[k,toks[0]+1] = 1\n            end_tokens[k,96-1] = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ct = test_df.shape[0]\ninput_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\nattention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\ntoken_type_ids_t = np.zeros((ct,MAX_LEN),dtype='int32')\n\nfor k in range(test_df.shape[0]):        \n    # INPUT_IDS\n    text1 = \" \"+\" \".join(test_df.loc[k,'text'].split())\n    enc = tokenizer.encode(text1)                \n    s_tok = sentiment_id[test_df.loc[k,'sentiment']]    \n    if len(enc)<92:\n        input_ids_t[k,:len(enc)+5] = [0] + enc + [2,2] + [s_tok] + [2]\n        attention_mask_t[k,:len(enc)+5] = 1\n    if len(enc)>91:\n        input_ids_t[k,:96] = [0] + enc[:91] + [2,2] + [s_tok] + [2]\n        attention_mask_t[k,:96] = 1  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"How good does our process work?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"all=[]\ncount=0\nfor k in range(train_df.shape[0]):    \n    a = np.argmax(start_tokens[k,])\n    b = np.argmax(end_tokens[k,])\n    text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n    enc = tokenizer.encode(text1)\n    st = tokenizer.decode_id_nospace(enc[a-1:b])\n    st = st.replace('<unk>','')\n    all.append(jaccard(st,train_df.loc[k,'selected_text']))\nprint('>>>> Jaccard =',np.mean(all))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"improve_jacc_review = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if improve_jacc_review == True:\n    all=[]\n    count=0\n    for k in range(train_df.shape[0]):    \n        a = np.argmax(start_tokens[k,])\n        b = np.argmax(end_tokens[k,])\n        text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode_id_nospace(enc[a-1:b])\n        st = st.replace('<unk>','')\n        if jaccard(st,train_df.loc[k,'selected_text'])<.3:\n            print(k)\n            print(st)\n            print(train_df.loc[k,'selected_text'])\n            print()\n        all.append(jaccard(st,train_df.loc[k,'selected_text']))\n    print('>>>> Jaccard =',np.mean(all))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def scheduler(epoch):\n    return 5e-5 * 0.2**epoch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model():\n    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n\n    config = RobertaConfig.from_pretrained(PATH+'config.json')\n    bert_model = TFRobertaModel.from_pretrained(PATH+'model.bin',config=config,from_pt=True)\n    x = bert_model(ids,attention_mask=att,token_type_ids=tok)\n\n    x1 = tf.keras.layers.Conv1D(128, 2,padding='same')(x[0])\n    x1 = tf.keras.layers.BatchNormalization()(x1)\n    x1 = tf.keras.layers.LeakyReLU()(x1)\n    x1 = tf.keras.layers.Conv1D(64, 2,padding='same')(x1)\n    x1 = tf.keras.layers.Dense(1)(x1)\n    x1 = tf.keras.layers.Flatten()(x1)\n    x1 = tf.keras.layers.Activation('softmax')(x1)\n    \n    x2 = tf.keras.layers.Conv1D(128, 2, padding='same')(x[0])\n    x2 = tf.keras.layers.BatchNormalization()(x2)\n    x2 = tf.keras.layers.LeakyReLU()(x2)\n    x2 = tf.keras.layers.Conv1D(64, 2, padding='same')(x2)\n    x2 = tf.keras.layers.Dense(1)(x2)\n    x2 = tf.keras.layers.Flatten()(x2)\n    x2 = tf.keras.layers.Activation('softmax')(x2)\n    \n    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n    optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)    \n    model.compile(loss='binary_crossentropy', optimizer=optimizer)\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#5-fold CV\nn_splits = 5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#This will loop over more than one seed and average all folds from all seeds together\nn_seeds = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Set equal to False if you already have model trained and just want to generate predictions. You'll need to save the model weights to input>pre-trained-model.\ntrainModel=False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if trainModel==True:\n\n    for x in range(n_seeds): \n        \n        jac = []; VER='v0'; DISPLAY=1 # USE display=1 FOR INTERACTIVE\n        oof_start = np.zeros((input_ids.shape[0],MAX_LEN))\n        oof_end = np.zeros((input_ids.shape[0],MAX_LEN))\n        preds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\n        preds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\n\n        skf = StratifiedKFold(n_splits=n_splits,shuffle=True,random_state=777+x)\n        for fold,(idxT,idxV) in enumerate(skf.split(input_ids,train_df.sentiment.values)):\n            \n            print('#'*25)\n            print('### FOLD %i'%(fold+1))\n            print('#'*25)\n\n            K.clear_session()\n            model = build_model()\n\n            reduce_lr = tf.keras.callbacks.LearningRateScheduler(scheduler)\n            \n            sv = tf.keras.callbacks.ModelCheckpoint(\n                '%s-roberta-%i-%x.h5'%(VER,fold,x), monitor='val_loss', verbose=1, save_best_only=True,\n                save_weights_only=True, mode='auto', save_freq='epoch')\n\n            hist = model.fit([input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]], [start_tokens[idxT,], end_tokens[idxT,]], \n                epochs=3, batch_size=8, verbose=DISPLAY, callbacks=[sv, reduce_lr],\n                validation_data=([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]], \n                [start_tokens[idxV,], end_tokens[idxV,]]))\n\n            print('Loading model...')\n            model.load_weights('%s-roberta-%i-%x.h5'%(VER,fold,x))\n\n            print('Predicting OOF...')\n            oof_start[idxV,],oof_end[idxV,] = model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n\n            print('Predicting Test...')\n            preds = model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n            preds_start += preds[0]/(n_splits*n_seeds)\n            preds_end += preds[1]/(n_splits*n_seeds)\n\n            # DISPLAY FOLD JACCARD\n            all = []\n            for k in idxV:\n                a = np.argmax(oof_start[k,])\n                b = np.argmax(oof_end[k,])\n                if a>b: \n                    text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n                    enc = tokenizer.encode(text1)                   \n                    st = tokenizer.decode_id_nospace(enc[a-1:a+3])\n                else:\n                    text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n                    enc = tokenizer.encode(text1)\n                    st = tokenizer.decode_id_nospace(enc[a-1:b])\n                st = st.replace('<unk>','')\n                all.append(jaccard(st,train_df.loc[k,'selected_text']))\n            jac.append(np.mean(all))\n            print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all))\n            print()\n        \n        print('>>>> OVERALL 5Fold CV Jaccard =',np.mean(jac))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Inference","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"if trainModel==False:\n        \n    DISPLAY=1\n    \n    for x in range(n_seeds): \n        \n        jac = []\n    \n        oof_start = np.zeros((input_ids.shape[0],MAX_LEN))\n        oof_end = np.zeros((input_ids.shape[0],MAX_LEN))\n    \n        preds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\n        preds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\n        \n        print('#'*70)\n        print('### SEED %x'%(x+1))\n        print('#'*70)\n\n        skf = StratifiedKFold(n_splits=n_splits,shuffle=True,random_state=777+x)\n        for fold,(idxT,idxV) in enumerate(skf.split(input_ids,train_df.sentiment.values)):  \n            \n            print('#'*25)\n            print('### MODEL %i'%(fold+1))\n            print('#'*25)\n\n            K.clear_session()\n            model = build_model()\n            model.load_weights('../input/bertweet-files/v0-roberta-%i-%x.h5'%(fold,x))\n\n            print('Predicting Test...')\n            preds = model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n            preds_start += preds[0]/(n_splits*n_seeds)\n            preds_end += preds[1]/(n_splits*n_seeds)\n            \n            print('Predicting OOF...')\n            oof_start[idxV,],oof_end[idxV,] = model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n            \n            # DISPLAY FOLD JACCARD\n            all = []; pos = []; neg = []; nue = []\n            for k in idxV:\n                a = np.argmax(oof_start[k,])\n                b = np.argmax(oof_end[k,])\n                if a>b: \n                    text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n                    enc = tokenizer.encode(text1)\n                    st = tokenizer.decode_id_nospace(enc[a-1:a+3])\n                else:\n                    text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n                    enc = tokenizer.encode(text1)\n                    st = tokenizer.decode_id_nospace(enc[a-1:b])\n                st = st.replace('<unk>','')                                              \n                if train_df.loc[k,'sentiment']=='positive':\n                    pos.append(jaccard(st,train_df.loc[k,'selected_text']))\n                if train_df.loc[k,'sentiment']=='negative':\n                    neg.append(jaccard(st,train_df.loc[k,'selected_text']))\n                if train_df.loc[k,'sentiment']=='neutral':\n                    st = text1\n                    nue.append(jaccard(st,train_df.loc[k,'selected_text']))\n                all.append(jaccard(st,train_df.loc[k,'selected_text']))  \n            jac.append(np.mean(all))\n            print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all))\n            print('>>>> FOLD %i Neutral Jaccard ='%(fold+1),np.mean(nue))\n            print('>>>> FOLD %i Positive Jaccard ='%(fold+1),np.mean(pos))\n            print('>>>> FOLD %i Negative Jaccard ='%(fold+1),np.mean(neg))                 \n            print()\n            \n        print('>>>> OVERALL 5Fold CV Jaccard =',np.mean(jac))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Test predictions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"all = []\nfor k in range(input_ids_t.shape[0]):\n    a = np.argmax(preds_start[k,])\n    b = np.argmax(preds_end[k,])\n    if a>b: \n        text1 = \" \"+\" \".join(test_df.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode_id_nospace(enc[a-1:a+3])\n    else:\n        text1 = \" \"+\" \".join(test_df.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode_id_nospace(enc[a-1:b])\n    st = st.replace('<unk>','')\n    if test_df.loc[k,'sentiment']=='neutral':\n        st = text1\n    all.append(st)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df['selected_text'] = all\ntest_df[['textID','selected_text']].to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}