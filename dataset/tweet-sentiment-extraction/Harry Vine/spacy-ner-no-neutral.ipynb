{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import string\nimport numpy as np \nimport pandas as pd \nfrom tqdm import tqdm\nimport os\nimport spacy\nimport random\nfrom spacy.util import compounding\nfrom spacy.util import minibatch\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BASE_PATH = '../input/tweet-sentiment-extraction/'\ntrain_df = pd.read_csv(BASE_PATH+ 'train.csv')\n# train_df = train_df[:1000]\ntest_df = pd.read_csv( BASE_PATH+ 'test.csv')\n# test_df = test_df[:1000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove any rows containing nan values\ntrain_df= train_df.dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#jaccard method \ndef jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Begin training data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# def train(train_data, output_dir, n_iter=20, model=None):\ndef train(train_data, nlp, n_iter=20 ):\n    \"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\n    \"\"\n    \n    # create the built-in pipeline components and add them to the pipeline\n    # nlp.create_pipe works for built-ins that are registered with spaCy\n    if \"ner\" not in nlp.pipe_names:\n        ner = nlp.create_pipe(\"ner\")\n        nlp.add_pipe(ner, last=True)\n    # otherwise, get it so we can add labels\n    else:\n        ner = nlp.get_pipe(\"ner\")\n    \n    # add label to the model, which is always 'selected_text'\n    for _, annotations in train_data:\n        for ent in annotations.get(\"entities\"):\n            ner.add_label(ent[2])\n\n    # get names of other pipes to disable them during training\n    # we are interested only in 'ner' pipeline\n    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n#     nlp.begin_training()\n    with nlp.disable_pipes(*other_pipes):  # only train NER\n        optimizer = nlp.begin_training()\n\n        for itn in tqdm(range(n_iter)):\n            #shuffle the data\n            random.shuffle(train_data)\n            # making batches of train_data \n            # size of the batch is determined by compounding func, \n            # which yield an infinite series of compounding values. \n            # Each time the generator is called, a value is produced by\n            # multiplying the previous value by the compound rate.\n            # in this case min batch size is 5, max is 500, compound rate of 1.001 per iteration\n            batches = minibatch(train_data, size=compounding(start=4.0, stop=500.0, compound=1.001))    \n            batch_len = 0\n            # dict to store losses info during training\n            losses = {}\n            for batch in batches:\n                batch_len +=1\n                texts, annotations = zip(*batch)\n                # update the model\n                nlp.update(\n                    texts,  # batch of texts\n                    annotations,  # batch of annotations\n                    drop=0.5,   # dropout rate, preventing overfitting, use default 0.5 rate\n                    losses=losses, \n                    sgd=optimizer #optimizer\n                )\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating data in spacy data input format, like this:\n# {\n#     text_string,\n#     entities: [{start_index, end_index, ENTITY}]\n# }\n\ndef preprocess_data(train_df, sentiment):\n    train_data = []\n    for row in train_df.itertuples():\n        if row.sentiment == sentiment:\n            selected_text = row.selected_text\n            text = row.text\n            start = text.find(selected_text)\n            end = start + len(selected_text)\n            train_data.append((text, {\"entities\": [[start, end, 'selected_text']]}))\n    return train_data\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Traing for positive sentiment\n\nsentiment = 'positive'\n\nprocessed_train_data = preprocess_data(train_df, sentiment)\n# model_path = get_model_out_path(sentiment)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a blank english language model\nmodel_pos = spacy.blank(\"en\")\n# train the data for 4 iterations, more tends to overfit\ntrain(processed_train_data, model_pos, n_iter=4 )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Traing for negative sentiment\n\nsentiment = 'negative'\n\nprocessed_train_data = preprocess_data(train_df, sentiment)\n# model_path = get_model_out_path(sentiment)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a blank english language model\nmodel_neg = spacy.blank(\"en\")\n# train the data for 4 iterations\ntrain(processed_train_data, model_neg, n_iter=4 )\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Ignore neutral sentiment","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pass text into model and return selected_text\ndef predict_entities(text, model):\n    doc = model(text)\n    ent_array = []\n    for ent in doc.ents:\n        start = text.find(ent.text)\n        end = start + len(ent.text)\n        new_int = [start, end, ent.label_]\n        if new_int not in ent_array:\n            ent_array.append([start, end, ent.label_])\n    selected_text = text[ent_array[0][0]: ent_array[0][1]] if len(ent_array) > 0 else text\n    return selected_text\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"selected_texts = []\n# loop through the test data and generate predictions\nfor row in test_df.itertuples():\n    text = row.text\n    output_str = \"\"\n    if row.sentiment == 'neutral':\n        selected_texts.append(text)\n    elif row.sentiment == 'positive':\n        selected_texts.append(predict_entities(text, model_pos))\n    else:\n        selected_texts.append(predict_entities(text, model_neg))\n\ndf_submission = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/sample_submission.csv')\ndf_submission['selected_text'] = selected_texts\ndf_submission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}