{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n# import seaborn as sns\n\nimport re\nimport string \nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nimport spacy\n\nfrom tqdm import trange\nimport random\nfrom spacy.util import compounding,minibatch\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import svm\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n# Credit to https://www.kaggle.com/nkoprowicz/a-simple-solution-using-only-word-counts\ndef jaccard(str1, str2): \n    # If both strings are empty\n    if len(str1) == 0 and len(str2) == 0:\n        return 1\n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\ntrain = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv')\ntest = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv')\nsubmission = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/sample_submission.csv')\n\n# %% [code]\nprint('train shape:',train.shape)\nprint('test shape:',test.shape)\ntrain.head()\ntrain = train.to_numpy()\ntest.head()\ntest = test.to_numpy()\n\nnlp = spacy.load(\"en_core_web_lg\")\n\nprint(\"\\nLoading Data!\")\nVALIDATION_SPLIT = int(0.70 * 27481)\nvalidation = train[1500:2000]\ntrain = train[:1500]\nprint(\"T: \", train.shape)\nprint(\"V: \", validation.shape)\ntraining_sentences = []\ntraining_vectors = []\nfor sentence in train:\n#     if the tweet was empty ignore it\n    if (type(sentence[1]) is float and str(sentence[1]) == 'nan' ):\n        continue\n        \n    tokens = list(nlp(sentence[1]))\n    st_span_s = sentence[1].find(sentence[2])\n    st_span_e = 0\n    if (st_span_s < 0):\n        print(\"Error, selected text not found in sentence!\")\n    else:\n        st_span_e = st_span_s + len(sentence[2])\n    for stoken in tokens:\n        assert sentence[1][stoken.idx:stoken.idx + len(stoken.text)] == stoken.text\n        is_in_selected = (stoken.idx >= st_span_s) and (stoken.idx + len(stoken.text) <= st_span_e)\n            \n        training_sentences.append([str(stoken),sentence[0], sentence[3], is_in_selected])\n        training_vectors.append(stoken.vector)\n        \nvalidation_sentences = []\nvalidation_vectors = []\nfor sentence in validation:\n#     if the tweet was empty ignore it\n    if (type(sentence[1]) is float and str(sentence[1]) == 'nan' ):\n        continue\n        \n    tokens = list(nlp(sentence[1]))\n    st_span_s = sentence[1].find(sentence[2])\n    st_span_e = 0\n    if (st_span_s < 0):\n        print(\"Error, selected text not found in sentence!\")\n    else:\n        st_span_e = st_span_s + len(sentence[2])\n    for stoken in tokens:\n        assert sentence[1][stoken.idx:stoken.idx + len(stoken.text)] == stoken.text\n        is_in_selected = (stoken.idx >= st_span_s) and (stoken.idx + len(stoken.text) <= st_span_e)\n            \n        validation_sentences.append([str(stoken),sentence[0], sentence[3], is_in_selected])\n        validation_vectors.append(stoken.vector)\n        \ntest_sentences = []\ntest_vectors = []\nfor sentence in test:\n#     if the tweet was empty ignore it\n    if (type(sentence[1]) is float and str(sentence[1]) == 'nan' ):\n        continue\n        \n    tokens = list(nlp(sentence[1]))\n         \n    for stoken in tokens:   \n        test_sentences.append([str(stoken),sentence[0], sentence[2]])\n        test_vectors.append(stoken.vector)\n        \ntraining_sentences = np.asarray(training_sentences)\ntraining_vectors = np.asarray(training_vectors)\nvalidation_sentences = np.asarray(validation_sentences)\nvalidation_vectors = np.asarray(validation_vectors)\ntest_sentences = np.asarray(test_sentences)\ntest_vectors = np.asarray(test_vectors)\n        \nprint(\"TS: \", training_sentences.shape)\nprint(\"TV: \", training_vectors.shape)\nprint(\"VS: \", validation_sentences.shape)\nprint(\"VV: \", validation_vectors.shape)\nprint(\"TTS: \", test_sentences.shape)\nprint(\"TTV: \", test_vectors.shape)\n\n# Turn classification strings into a categorical integer (0='Neutral' or whatever it is)\ntclassifications = np.asarray(pd.factorize(training_sentences[:, 2], sort=True)[0].tolist())\n\n# Add classification integers to training vectors\ntraining_vectors = np.append(training_vectors, np.array([tclassifications]).T, 1)\n\n# Turn classification strings into a categorical integer (0='Neutral' or whatever it is)\nvclassifications = np.asarray(pd.factorize(validation_sentences[:, 2], sort=True)[0].tolist())\n\n# Add classification integers to validation vectors\nvalidation_vectors = np.append(validation_vectors, np.array([vclassifications]).T, 1)\n\n# Turn classification strings into a categorical integer (0='Neutral' or whatever it is)\nttclassifications = np.asarray(pd.factorize(test_sentences[:, 2], sort=True)[0].tolist())\n\n# Add classification integers to test vectors\ntest_vectors = np.append(test_vectors, np.array([ttclassifications]).T, 1)\n\nprint(\"\\nStarting Training!\")\n\nclf = svm.SVC()\nclf.fit(training_vectors, training_sentences[:, 3])\n\nprint(\"\\nDone Training!\")\n\n\n# validation_sentences = np.append(validation_sentences, np.array([clf.predict(validation_vectors)]).T, 1)\n# # print(\"Out: \", validation_sentences[:,4])\n\n# print(\"\\nPredicting Sentences!\")\n\n# pred_sentences = []\n# for sentence in validation:\n# #     Get words predictions in the sentence\n#     if sentence[3] == 'neutral':\n#         pred_sentences.append(sentence[1])\n#     else:\n#         words = validation_sentences[validation_sentences[:,1] == sentence[0]]\n\n#         words = words[words[:,4]=='True']\n\n#         s = \"\"\n#         first = True\n#         for wi in words:\n#             s += wi[0]\n#             if not first:\n#                 s += \" \"\n#             else:\n#                 first = False\n            \n#         pred_sentences.append(s)\n\n# print(\"Sentences: \", [validation[:,1]])\n# print(\"Expected: \", [validation[:,2]])\n# print(\"Predictions: \", np.asarray(pred_sentences))    \n# print(\"\\nDone: \", len(pred_sentences) == len(validation))\n    \n\n# print(\"\\nResults:\")\n# jaccard_data = np.append(np.array([validation[:,2]]).T, np.array([pred_sentences]).T, 1)\n# jaccard_results = np.apply_along_axis(lambda x: jaccard(x[0], x[1]), 1, jaccard_data)\n    \n# print(\"JACCARD SCORE (VALIDATION): \", np.mean(jaccard_results))\n\nprint(\"\\nTesting!\")\n\n# print(\"ts: \", test_sentences)\n# print(\"tv: \", test_vectors)\ntest_sentences = np.append(test_sentences, np.array([clf.predict(test_vectors)]).T, 1)\nprint(test_sentences[:,3])\n\nprint(\"\\nWriting Submission!\")\n\nwith open('submission.csv', 'w+') as f:\n    f.write(\"textID,selected_text\\n\")\n    for sentence in test:\n    #     Get words predictions in the sentence\n        s = \"\"\n        if sentence[2] == 'neutral':\n            s = sentence[1]\n        else:\n            words = test_sentences[test_sentences[:,1] == sentence[0]]\n#             print('w: ', words)\n\n            words = words[words[:,3]=='True']\n\n            first = True\n            for wi in words:\n                s += wi[0]\n                if not first:\n                    s += \" \"\n                else:\n                    first = False \n                    \n        f.write(sentence[0])\n        f.write(',')\n        f.write(s)\n        f.write('\\n')\n        \nprint(\"\\nDone!\")\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}