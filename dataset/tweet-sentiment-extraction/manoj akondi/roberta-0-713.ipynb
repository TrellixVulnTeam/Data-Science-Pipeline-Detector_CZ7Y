{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install transformers","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Importing dependencies","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport keras\nimport transformers \nfrom transformers import * \nimport pandas as pd\nimport numpy as np\nimport tokenizers","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Printing tf version\n","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"print(tf.__version__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_file = '../input/tf-roberta/vocab-roberta-base.json'\nmerge_file = '../input/tf-roberta/merges-roberta-base.txt'\ntokenizer = tokenizers.ByteLevelBPETokenizer(vocab_file, merge_file,lowercase = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN = 100\nsentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}\ntrain = pd.read_csv('../input/tweet-sentiment-extraction/train.csv').fillna('')\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tokenization <br/>\nThe tokenization logic was inpired from Abhishek Takur","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ct = train.shape[0]\ninput_ids = np.ones((ct,MAX_LEN),dtype='int32')\nattention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\ntoken_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\nstart_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\nend_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n\nfor k in range(train.shape[0]):\n    \n    # FIND OVERLAP\n    text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n    text2 = \" \".join(train.loc[k,'selected_text'].split())\n    idx = text1.find(text2)\n    chars = np.zeros((len(text1)))\n    chars[idx:idx+len(text2)]=1\n    if text1[idx-1]==' ': chars[idx-1] = 1 \n    enc = tokenizer.encode(text1) \n        \n    # ID_OFFSETS\n    offsets = []; idx=0\n    for t in enc.ids:\n        w = tokenizer.decode([t])\n        offsets.append((idx,idx+len(w)))\n        idx += len(w)\n    \n    # START END TOKENS\n    toks = []\n    for i,(a,b) in enumerate(offsets):\n        sm = np.sum(chars[a:b])\n        \n        if sm>0: toks.append(i) \n        \n    s_tok = sentiment_id[train.loc[k,'sentiment']]\n    input_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n    attention_mask[k,:len(enc.ids)+5] = 1\n    if len(toks)>0:\n        start_tokens[k,toks[0]+1] = 1\n        end_tokens[k,toks[-1]+1] = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('../input/tweet-sentiment-extraction/test.csv').fillna('')\n\nct = test.shape[0]\ninput_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\nattention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\ntoken_type_ids_t = np.zeros((ct,MAX_LEN),dtype='int32')\n\nfor k in range(test.shape[0]):\n        \n    # INPUT_IDS\n    text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n    enc = tokenizer.encode(text1)                \n    s_tok = sentiment_id[test.loc[k,'sentiment']]\n    input_ids_t[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n    attention_mask_t[k,:len(enc.ids)+5] = 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Building the model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\natt = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\ntok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\nconfig = RobertaConfig.from_pretrained('../input/tf-roberta/config-roberta-base.json')\nbert_model = TFRobertaModel.from_pretrained('../input/tf-roberta/pretrained-roberta-base.h5',config=config)\nx = bert_model(ids,attention_mask=att,token_type_ids=tok)\ndrop1 = tf.keras.layers.Dropout(0.2)(x[0])\nlayer2 = tf.keras.layers.Conv1D(50 ,kernel_size = 1)(drop1)\ndrop2 = tf.keras.layers.Dropout(0.2)(layer2)\nlayer3 = tf.keras.layers.Conv1D(1 ,kernel_size = 1)(drop2)\nlayer4 = tf.keras.layers.Flatten()(layer3)\noutput_1 = tf.keras.layers.Activation('softmax')(layer4)\n\ndrop1_ = tf.keras.layers.Dropout(0.2)(x[0])\nlayer1_ = tf.keras.layers.Conv1D(50 ,kernel_size = 1)(drop1_)\ndrop2_ = tf.keras.layers.Dropout(0.2)(layer1_)\nlayer2_ = tf.keras.layers.Conv1D(1 ,kernel_size = 1)(drop2_)\nlayer3_ = tf.keras.layers.Flatten()(layer2_)\noutput_2 = tf.keras.layers.Activation('softmax')(layer3_)\nmodel = tf.keras.Model(inputs = [ids ,att ,tok] ,outputs = [output_1 ,output_2])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def my_loss(alpha ,gamma):\n    '''defining focal loss with gamma and alpha parameters'''\n    def focal_loss(y_true ,y_pred):\n        y_true = tf.cast(y_true ,dtype = tf.float32)\n        y_pred = tf.cast(y_pred ,dtype = tf.float32)\n        log_lik = y_true*tf.keras.backend.log(y_pred)\n        log_lik = alpha*((1-y_pred)**gamma)*log_lik\n        return -tf.keras.backend.sum(log_lik)\n    return focal_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = tf.keras.optimizers.Adam(learning_rate=0.000001)\nmodel.compile(loss = my_loss(1.0 ,2.0) ,optimizer = optimizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit([input_ids[800:], attention_mask[800:],token_type_ids[800:]], [start_tokens[800:], end_tokens[800:]] ,epochs = 30 ,\n         validation_split = 0.1 ,batch_size = 32)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Defining metric\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    if (len(a)==0) & (len(b)==0): return 0.5\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oof_start = np.zeros((input_ids.shape[0],MAX_LEN))\noof_end = np.zeros((input_ids.shape[0],MAX_LEN))\noof_start,oof_end = model.predict([input_ids[:800],attention_mask[:800],token_type_ids[:800]],verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all = []\njac = []\nfor k in range(800):\n    a = np.argmax(oof_start[k,])\n    b = np.argmax(oof_end[k,])\n    if a>b: \n        st = train.loc[k,'text'] # IMPROVE CV/LB with better choice here\n    else:\n        text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode(enc.ids[a-1:b])\n    all.append(jaccard(st,train.loc[k,'selected_text']))\njac.append(np.mean(all))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(np.mean(jac))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Kaggle submission\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = model.predict([input_ids_t ,attention_mask_t ,token_type_ids_t] ,verbose = 1)\npreds_start = preds[0]\npreds_end = preds[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all = []\nfor k in range(input_ids_t.shape[0]):\n    a = np.argmax([preds_start[k ,]])\n    b = np.argmax(preds_end[k ,])\n    if a>b:\n        st = test.loc[k ,'text']\n    else:\n        text1 = \" \" + \" \".join(test.loc[k ,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode(enc.ids[a-1:b])\n    all.append(st)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['selected_text'] = all\ntest[['textID' ,'selected_text']].to_csv('submission.csv' ,index = False)\npd.set_option('max_colwidth' ,60)\ntest.sample(25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}