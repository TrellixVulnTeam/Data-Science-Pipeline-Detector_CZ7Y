{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\n\n# Get the GPU device name.\ndevice_name = tf.test.gpu_device_name()\n\n# The device name should look like the following:\nif device_name == '/device:GPU:0':\n    print('Found GPU at: {}'.format(device_name))\nelse:\n    raise SystemError('GPU device not found')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\n\n# If there's a GPU available...\nif torch.cuda.is_available():    \n\n    # Tell PyTorch to use the GPU.    \n    device = torch.device(\"cuda\")\n\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\n\n# If not...\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install transformers","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from transformers import BertForQuestionAnswering, AdamW, BertConfig, RobertaForQuestionAnswering, RobertaTokenizer, BertTokenizer\n\n\noutput_dir = '/kaggle/input/roberta-2'\n\n\n\n\n\n# # Load a trained model and vocabulary that you have fine-tuned\nmodel = RobertaForQuestionAnswering.from_pretrained(output_dir)\ntokenizer = RobertaTokenizer.from_pretrained(output_dir)\n\n# # Copy the model to the GPU.\nmodel.to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom torch.utils.data import TensorDataset, random_split\nfrom torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n\nmax_len = 192\n\n\n# Load the dataset into a pandas dataframe.\ndf_test = pd.read_csv(\"../input/tweet-sentiment-extraction/test.csv\")\n\n# Report the number of sentences.\nprint('Number of test sentences: {:,}\\n'.format(df_test.shape[0]))\ndf_test['id_num'] = np.arange(len(df_test))\n\n# Tokenize all of the sentences and map the tokens to thier word IDs.\ninput_ids = []\nattention_masks = []\ntoken_type_ids = []\ntextID = []\n\n# For every sentence...\nfor i in range(len(df_test['text'])):\n    question = 'what portion of texts best reflect ' + df_test['sentiment'][i] + 'sentiment'\n    # `encode_plus` will:\n    #   (1) Tokenize the sentence.\n    #   (2) Prepend the `[CLS]` token to the start.\n    #   (3) Append the `[SEP]` token to the end.\n    #   (4) Map tokens to their IDs.\n    #   (5) Pad or truncate the sentence to `max_length`\n    #   (6) Create attention masks for [PAD] tokens.\n    text = df_test['text'][i]\n    encoded_dict = tokenizer.encode_plus(\n                        question,\n                        text,                      # Sentence to encode.\n                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n                        max_length = max_len,           # Pad & truncate all sentences.\n                        return_token_type_ids = True,\n                        pad_to_max_length = True,\n                        return_attention_mask = True,   # Construct attn. masks.\n                        return_tensors = 'pt',     # Return pytorch tensors.\n                   )\n    \n    # Add the encoded sentence to the list.    \n    input_ids.append(encoded_dict['input_ids'])\n    \n    # And its attention mask (simply differentiates padding from non-padding).\n    attention_masks.append(encoded_dict['attention_mask'])\n\n    # Add its token type map\n    token_type_ids.append(encoded_dict['token_type_ids'])\n\n    textID.append(df_test['id_num'][i])\n\n\n# Convert the lists into tensors.\ninput_ids = torch.cat(input_ids, dim=0)\nattention_masks = torch.cat(attention_masks, dim=0)\ntoken_type_ids = torch.cat(token_type_ids, dim=0)\ntextID = torch.tensor([int(x) for x in textID])\n\n\n# Set the batch size.  \nbatch_size = 32  \n\n# Create the DataLoader.\n\nprediction_data = TensorDataset(input_ids, attention_masks,  token_type_ids, textID)\nprediction_sampler = SequentialSampler(prediction_data)\nprediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def id_to_word(answer_start, answer_end, input_ids):\n    idx = input_ids[int(answer_start)+1:int(answer_end)+1]\n    answer = tokenizer.decode(idx)\n    return str(answer)\n\n\ndef get_start_end(start_score, end_score):\n    starts = np.zeros(len(start_score))\n    ends = np.zeros(len(start_score))\n    for i in range(len(start_score)):\n        total_score = []\n        arg = []\n        for a in range(len(start_score[i])):\n            for b in range(a, len(end_score[i])):\n                total_score.append( start_score[i][a] + end_score[i][b])\n                arg.append((a,b))\n\n        total_score = torch.tensor(total_score)\n\n        max_idx = torch.argmax(total_score)\n        start, end = arg[max_idx]\n        starts[i] = start\n        ends[i] = end\n    return starts, ends","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions  = []\ntextID = []\n# Predict \nfor batch in prediction_dataloader:\n  # Add batch to GPU\n    batch = tuple(t.to(device) for t in batch)\n  \n  # Unpack the inputs from our dataloader\n    b_input_ids, b_input_mask, b_input_type_ids, b_textID = batch\n  \n  # Telling the model not to compute or store gradients, saving memory and \n  # speeding up prediction\n    with torch.no_grad():\n      # Forward pass, calculate logit predictions\n        start_score, end_score, hidden_state = model(b_input_ids, \n                                   token_type_ids = b_input_type_ids,\n                                   attention_mask=b_input_mask\n                                   )\n\n        # Move logits and labels to CPU\n        start_score_pred = start_score.detach().cpu().numpy()\n        end_score_pred = end_score.detach().cpu().numpy()\n\n        b_input_ids = b_input_ids.to('cpu').numpy()\n        b_textID = b_textID.detach().cpu().numpy()\n        # Calculate the accuracy for this batch of test sentences, and\n        # accumulate it over all batches.\n        start_pred, end_pred = get_start_end(start_score_pred, end_score_pred)\n        for i in range(len(b_input_ids)):\n            predicted_text = tokenizer.decode(b_input_ids[i][int(start_pred[i]):int(end_pred[i])]) \n            predictions.append(predicted_text)\n            textID.append(b_textID[i])\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/tweet-sentiment-extraction/sample_submission.csv\")\ndf['selected_text'] = predictions\n# df['selected_text'] = df['selected_text'].apply(lambda x: x.replace('!!!!', '!') if len(x.split())==1 else x)\n# df['selected_text'] = df['selected_text'].apply(lambda x: x.replace('..', '.') if len(x.split())==1 else x)\n# df['selected_text'] = df['selected_text'].apply(lambda x: x.replace('...', '.') if len(x.split())==1 else x)\n\ndf.to_csv('/kaggle/working/submission.csv', index=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}