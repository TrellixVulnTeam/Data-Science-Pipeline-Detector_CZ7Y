{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pickle\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import StratifiedKFold\nimport tokenizers\nimport math\nimport re\nfrom numba import jit\nfrom tqdm import tqdm\n\n\nMAX_LEN = 120\nPATH = '../input/tf-roberta/'\ntokenizer = tokenizers.ByteLevelBPETokenizer(\n    vocab_file=PATH+'vocab-roberta-base.json', \n    merges_file=PATH+'merges-roberta-base.txt', \n    lowercase=True,\n    add_prefix_space=True\n)\nsentiment_id = {'positive': 1809, 'negative': 3392, 'neutral': 14058}\n\ndef jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    if (len(a)==0) & (len(b)==0): return 0.5\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# preprocess","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/tweet-sentiment-extraction/train.csv').fillna('')\n\ntext = train['text'].values\nselected_text = train['selected_text'].values.copy()\nsentiments = train['sentiment'].values\n\nct = train.shape[0]\ninput_ids = np.ones((ct,MAX_LEN),dtype='int32')\nattention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\ntoken_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\nstart_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\nend_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n\n\"\"\"\nfor k in tqdm(range(train.shape[0])):\n    ss = text[k].find(selected_text[k])\n    if text[k][max(ss - 2, 0):ss] == '  ':\n        ss -= 2\n    if ss > 0  and text[k][ss - 1] == ' ':\n        ss -= 1\n\n    ee = ss + len(selected_text[k])\n\n    #if re.match(r' [^ ]', text[k]) is not None:\n    if len(text[k]) > 0 and text[k][0] == ' ':  # 変更箇所 guchio\n        #ee -= 1\n        front_spaces = re.findall(\"^ +[^ ]\", text[k]) # 変更箇所 guchio\n        ee -= front_spaces[0].count(' ') # 変更箇所 guchio\n        for cnt_base in re.findall(\"[^ ]  +[^ ]\", text[k][:ee].strip()): # 変更箇所 guchio\n            ee -= cnt_base[2:].count(' ') # 変更箇所 guchio\n    ss = max(0, ss)\n    if '  ' in text[k][:ss] and sentiments[k] != 'neutral':\n        text1 = \" \".join(text[k].split())\n        sel = text1[ss:ee].strip()\n        if len(sel) > 1 and sel[-2] == ' ':\n            sel = sel[:-2]\n\n        selected_text[k] = sel\n        \n    text1 = \" \"+\" \".join(text[k].split())\n    text2 = \" \".join(selected_text[k].split()).lstrip(\".,;:\")\n\n    idx = text1.find(text2)\n    if idx != -1:\n        chars = np.zeros((len(text1)))\n        chars[idx:idx+len(text2)]=1\n        if text1[idx-1]==' ': chars[idx-1] = 1 \n    else:\n        import pdb;pdb.set_trace()\n        chars = np.ones((len(text1)))\n    enc = tokenizer.encode(text1) \n\n    # ID_OFFSETS\n    offsets = enc.offsets\n\n    # START END TOKENS\n    _toks = []\n\n    for i,(a,b) in enumerate(offsets):\n        sm = np.mean(chars[a:b])\n        # if (sm > 0.6 and chars[a] != 0):  # こうすると若干伸びるけど...\n        if (sm > 0.5 and chars[a] != 0): \n            _toks.append(i) \n\n    toks = _toks\n    s_tok = sentiment_id[sentiments[k]]\n    input_ids[k,:len(enc.ids)+3] = [0, s_tok] + enc.ids + [2]\n    attention_mask[k,:len(enc.ids)+3] = 1\n    if len(toks)>0:\n        start_tokens[k,toks[0]+2] = 1\n        end_tokens[k,toks[-1]+2] = 1\n\"\"\"\n\nfor k in tqdm(range(train.shape[0])):\n    ss = text[k].find(selected_text[k])\n    if text[k][max(ss - 2, 0):ss] == '  ':\n        ss -= 2\n    if ss > 0  and text[k][ss - 1] == ' ':\n        ss -= 1\n\n    ee = ss + len(selected_text[k])\n\n    if re.match(r' [^ ]', text[k]) is not None:\n        ee -= 1\n\n    ss = max(0, ss)\n    if '  ' in text[k][:ss] and sentiments[k] != 'neutral':\n        text1 = \" \".join(text[k].split())\n        sel = text1[ss:ee].strip()\n        if len(sel) > 1 and sel[-2] == ' ':\n            sel = sel[:-2]\n\n        selected_text[k] = sel\n\n    text1 = \" \"+\" \".join(text[k].split())\n    text2 = \" \".join(selected_text[k].split()).lstrip(\".,;:\")\n\n    idx = text1.find(text2)\n    if idx != -1:\n        chars = np.zeros((len(text1)))\n        chars[idx:idx+len(text2)]=1\n        if text1[idx-1]==' ': chars[idx-1] = 1 \n    else:\n        import pdb;pdb.set_trace()\n        chars = np.ones((len(text1)))\n    enc = tokenizer.encode(text1) \n\n    # ID_OFFSETS\n    offsets = enc.offsets\n\n    # START END TOKENS\n    _toks = []\n\n    for i,(a,b) in enumerate(offsets):\n        sm = np.mean(chars[a:b])\n        #if (sm > 0.6 and chars[a] != 0):  # こうすると若干伸びるけど...\n        if (sm > 0.5 and chars[a] != 0): \n            _toks.append(i)\n\n    toks = _toks\n    s_tok = sentiment_id[sentiments[k]]\n    input_ids[k, :len(enc.ids)+3] = [0, s_tok] + enc.ids + [2]\n    attention_mask[k,:len(enc.ids)+3] = 1\n    if len(toks)>0:\n        start_tokens[k,toks[0]+2] = 1\n        end_tokens[k,toks[-1]+2] = 1  \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Postprocess functions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport re\ndef modify_punc_length(text, selected_text):\n    m = re.search(r'[!\\.\\?]+$', selected_text)        \n    if m is None:\n        return selected_text\n    \n    conti_punc = len(m.group())\n\n    if conti_punc >= 4:\n        selected_text = selected_text[:-(conti_punc-2)]\n    elif conti_punc == 1:# 元のtextを探しに行く\n        tmp = re.sub(r\"([\\\\\\*\\+\\.\\?\\{\\}\\(\\)\\[\\]\\^\\$\\|])\", r\"\\\\\\g<0>\", selected_text)\n        pat = re.sub(r\" \", \" +\", tmp)\n        m = re.search(pat, text)\n        f_idx0 = m.start()\n        f_idx1 = m.end()\n\n        if f_idx1 != len(text) and text[f_idx1] in (\"!\", \".\", \"?\"):\n            f_idx1 += 1\n            selected_text = text[f_idx0:f_idx1]\n    return selected_text\n\n\nimport math\ndef postprocess(row):\n    if row.original_text == '':\n        return row.normalized_text.strip()\n    original_text = row.original_text.replace('\\t', '')\n    y_start_char = row.y_start_char\n    y_end_char = row.y_end_char\n    y_selected_text = row.normalized_text[y_start_char:y_end_char].strip()\n    if (y_end_char < len(row.normalized_text) and row.sentiment != 'neutral' and\n        y_selected_text[-1] == '.' and\n        (row.normalized_text[y_end_char] == '.' or \n         y_selected_text[-2] == '.')):\n        y_selected_text = re.sub('\\.+$', '..', y_selected_text)\n\n    tmp = re.sub(r\"([\\\\\\*\\+\\.\\?\\{\\}\\(\\)\\[\\]\\^\\$\\|])\", r\"\\\\\\g<0>\", y_selected_text)\n    pat = re.sub(r\" \", \" +\", tmp)\n    m = re.search(pat, original_text)\n    if m is None:\n        print(row.normalized_text[y_start_char:y_end_char].strip())\n        print(row.normalized_text)\n        print(y_selected_text)\n    ss2 = m.start()\n    ee2 = m.end()\n    \n    # 'neutral' およびほぼ文書全体が抜き出されるもの\n    if row.sentiment == 'neutral' or ((ee2 - ss2) / len(original_text) > 0.75 and  (ee2 - ss2) > 9):\n        if len(original_text) > 0 and original_text[0] != '_' and ss2 < 5:\n            ss2 = 0 \n        if (ee2 < len(original_text)-1 and original_text[ee2:ee2+2] in ('..', '!!', '??', '((', '))')):\n            ee2 += 1\n        st =  original_text[ss2:ee2].lstrip(' ½¿')\n        y_selected_text = st #re.sub(r' .$', '', st)#.strip('`') ###  この一行追加\n                \n    else:\n        if original_text[:int((ss2+ee2) * 0.5) + 1].count('  ') > 0:\n            ss = y_start_char\n            ee = y_end_char + 1\n            if ss > 1 and original_text[ss-1:ss+1] == '..' and  original_text[ss+1] != '.':\n                ss -= 1\n            st = original_text[ss:ee]#.lstrip(' ½¿')\n            y_selected_text = re.sub(r' .$', '', st)#.strip('`') ###  この一行追加\n        else:\n            if (ee2 < len(original_text)-1 and original_text[ee2:ee2+2] in ('..', '!!', '??', '((', '))')):\n                ee2 += 1\n            # 先頭の空白分後退\n            if  original_text[0] == ' ':\n                ss2 -= 1\n\n            y_selected_text = original_text[ss2:ee2].strip(' ½')\n\n            if row.normalized_text[:y_end_char + 5] == \" \" + row.original_text[:ee2 + 4]: # 簡単のため、長さが同じ場合に限定している\n                y_selected_text = modify_punc_length(original_text, y_selected_text)\n            \n            \n    return y_selected_text","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Inference","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/tweet-sentiment-extraction/train.csv').fillna('')\n\ntext = train['text'].values\nselected_text = train['selected_text'].values.copy()\nsentiments = train['sentiment'].values\nids = train['textID'].values\n\ndef jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    if (len(a)==0) & (len(b)==0): return 0.5\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))\n\n\nall = []\nlist_st = []\nall_nn = []\nall_p = []\nall_n = []\n\nfrom collections import namedtuple\nRow = namedtuple('Row', ['original_text', 'normalized_text', 'sentiment', 'y_start_char', 'y_end_char'])\nfor k in tqdm(list(range(train.shape[0]))):\n    text0 = text[k]\n    text1 = \" \" + \" \".join(text[k].split())\n    enc = tokenizer.encode(text1)\n\n    aa = np.argmax(start_tokens[k])\n    bb = np.argmax(end_tokens[k])\n\n    ss = enc.offsets[aa - 2][0]\n    ee = enc.offsets[bb - 2][1] \n    st = text1[ss:ee].strip()\n\n    row = Row(\n        original_text=text0,\n        normalized_text=text1,\n        sentiment=sentiments[k],\n        y_start_char=ss,\n        y_end_char=ee,\n    )\n    try:\n        st = postprocess(row)\n    except Exception as e:\n        raise e\n        print(k)\n\n    list_st.append(st)\n    sc = jaccard(st,selected_text[k])\n    if sentiments[k] == 'neutral':\n        all_nn.append(sc)\n    elif sentiments[k] == 'positive':\n        all_p.append(sc)\n    else:\n        all_n.append(sc)\n\n    all.append(sc)\nprint(a, b, '>>>> FOLD Jaccard all =',np.mean(all))#, np.mean(all_nn), np.mean(all_p), np.mean(all_n))\nprint('>>>> FOLD Jaccard neutral =',np.mean(all_nn))\nprint('>>>> FOLD Jaccard positive =',np.mean(all_p))\nprint('>>>> FOLD Jaccard negative =',np.mean(all_n))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* >>>> FOLD Jaccard all = 0.9798332533206786\n* >>>> FOLD Jaccard neutral = 0.9961648726550028\n* >>>> FOLD Jaccard positive = 0.9695626620283051\n* >>>> FOLD Jaccard negative = 0.9678254485028062","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* >>>> FOLD Jaccard all = 0.9796338924844558\n* >>>> FOLD Jaccard neutral = 0.996154667866922\n* >>>> FOLD Jaccard positive = 0.969079424824062\n* >>>> FOLD Jaccard negative = 0.967668908646805","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# CV inference","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/tweet-sentiment-extraction/train.csv').fillna('')\ntext = train['text'].values\nselected_text = train['selected_text'].values.copy()\nsentiments = train['sentiment'].values\n\ndef jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    if (len(a)==0) & (len(b)==0): return 0.5\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))\n\n\nall = []\nlist_st = []\nall_nn = []\nall_p = []\nall_n = []\n\nDIR = '../input/tweet-cv-ens-0609/'\n\noof_chr_start = np.load(DIR + 'oof_chr_start.npy')\noof_chr_end = np.load(DIR + 'oof_chr_end.npy')\n\n\nfrom collections import namedtuple\nRow = namedtuple('Row', ['original_text', 'normalized_text', 'sentiment', 'y_start_char', 'y_end_char'])\nfor k in tqdm(list(range(train.shape[0]))):\n    text0 = text[k]\n    text1 = \" \" + \" \".join(text[k].split())\n    enc = tokenizer.encode(text1)\n\n    start_prob = oof_chr_start[k]\n    end_prob = oof_chr_end[k] + np.arange(141) * 1.0e-15\n\n    y_start_char = start_prob.argmax()\n    end_prob[:y_start_char] = 0\n    y_end_char = end_prob.argmax() + 1\n            \n\n    ss = y_start_char\n    ee = y_end_char\n    st = text1[ss:ee].strip()\n\n        \n    row = Row(\n        original_text=text0,\n        normalized_text=text1,\n        sentiment=sentiments[k],\n        y_start_char=ss,\n        y_end_char=ee,\n    )\n    try:\n        st = postprocess(row)\n    except:\n        print(k)\n\n    list_st.append(st)\n    sc = jaccard(st,selected_text[k])\n    if sentiments[k] == 'neutral':\n        all_nn.append(sc)\n    elif sentiments[k] == 'positive':\n        all_p.append(sc)\n    else:\n        all_n.append(sc)\n\n    all.append(sc)\nprint('>>>> FOLD Jaccard all =',np.mean(all))#, np.mean(all_nn), np.mean(all_p), np.mean(all_n))\nprint('>>>> FOLD Jaccard neutral =',np.mean(all_nn))\nprint('>>>> FOLD Jaccard positive =',np.mean(all_p))\nprint('>>>> FOLD Jaccard negative =',np.mean(all_n))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}