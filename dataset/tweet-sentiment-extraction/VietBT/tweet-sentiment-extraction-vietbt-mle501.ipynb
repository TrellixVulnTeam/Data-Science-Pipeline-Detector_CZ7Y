{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"!cd /kaggle/input/apexpytorch/ && pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.read_csv('/kaggle/input/tweet-sentiment-extraction/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import unicodedata\nfrom tqdm import tqdm\n\ndef normalize(s):\n    s = str(s)\n    #s = unicodedata.normalize('NFKC', str(s))\n    #s = s.replace('`', \"'\")\n    return s.strip()\n\ndef read_dataset(file_name):\n    data = pd.read_csv(file_name)\n    dataset = []\n    for i in tqdm(range(len(data))):\n        if \"selected_text\" in data:\n            items = [data['text'][i], data['selected_text'][i], data['sentiment'][i]]\n        else:\n            items = [data['text'][i], \"NO_DATA\", data['sentiment'][i]]\n        textID = data['textID'][i]\n        items = [normalize(item) for item in items]\n        text = items[0]\n        selected_text = items[1]\n        label = items[2]\n        words = text.split()\n        selected_text_len = len(selected_text.split())\n        sentence = []\n        i = 0\n        while i < len(words):\n            word = words[i]\n            next_sentence = \" \".join(words[i:i+selected_text_len])\n            if next_sentence == selected_text:\n                sentence.append((words[i], \"B-\"+label))\n                for _word in words[i+1:i+selected_text_len]:\n                    sentence.append((_word, \"I-\"+label))\n                i += selected_text_len\n            else:\n                sentence.append((word, \"O\"))\n                i += 1\n        \n        dataset.append({\"sentence\": sentence, \"label\": label, \"textID\": textID})\n    return dataset\n\ntrainset = read_dataset('/kaggle/input/tweet-sentiment-extraction/train.csv')\ntestset = read_dataset('/kaggle/input/tweet-sentiment-extraction/test.csv')\nprint(\"trainset:\", len(trainset))\nprint(\"testset:\", len(testset))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import BertTokenizer, BertModel\nimport torch\ntokenizer = BertTokenizer.from_pretrained('/kaggle/input/bert-base-uncased/')\nprint(tokenizer.encode(\"hello my world\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = [\"O\", \"B-neutral\", \"I-neutral\", \"B-negative\", \"I-negative\", \"B-positive\", \"I-positive\"]\ndef encode_dataset(dataset, ignore_index=-100, max_seq_len=512):\n    intent_labels = [\"neutral\", \"negative\", \"positive\"]\n    data_inputs = []\n    for data in tqdm(dataset):\n        input_ids = []\n        label_ids = []\n        token_type_ids = []\n        for word, label in data[\"sentence\"]:\n            ids = tokenizer.encode(word, add_special_tokens=False)\n            input_ids += ids\n            label_ids += [labels.index(label)] + [ignore_index]*(len(ids) - 1)\n            \n        token_type_ids = [0]* (len(input_ids)+1)\n        ids = tokenizer.encode(data[\"label\"], add_special_tokens=False)\n        input_ids += [tokenizer.sep_token_id] + ids\n        label_ids += [ignore_index]*(len(ids)+1)\n        \n        input_ids = input_ids[:max_seq_len-2]\n        label_ids = label_ids[:max_seq_len-2]\n        token_type_ids = token_type_ids[:max_seq_len]\n        \n        input_ids = [tokenizer.cls_token_id] + input_ids + [tokenizer.sep_token_id]\n        label_ids = [ignore_index] + label_ids + [ignore_index]\n        \n        mask_ids = [1] * len(input_ids) + [0] * (max_seq_len - len(input_ids))\n        input_ids += [tokenizer.pad_token_id] * (max_seq_len - len(input_ids))\n        token_type_ids += [1] * (max_seq_len - len(token_type_ids))\n        label_ids += [ignore_index] * (max_seq_len - len(label_ids))\n        \n        data_inputs.append((input_ids, mask_ids, token_type_ids, label_ids, data))\n    return data_inputs\n\ntrain_ids = encode_dataset(trainset)\ntest_ids = encode_dataset(testset)\nprint(\"train_ids:\", len(train_ids))\nprint(\"test_ids:\", len(test_ids))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import DataLoader\n\nclass TextDataLoader(DataLoader):\n    def __init__(self, data_set, shuffle=False, device=\"cuda\", batch_size=16):\n        super(TextDataLoader, self).__init__(dataset=data_set, collate_fn=self.collate_fn, shuffle=shuffle, batch_size=batch_size)\n        self.device = device\n\n    def collate_fn(self, data):\n        examples = []\n        data_infor = []\n        max_length = max(map(lambda x: sum(x[1]), data))\n        for sample in data:\n            example = []\n            example.append(sample[0][:max_length])\n            example.append(sample[1][:max_length])\n            example.append(sample[2][:max_length])\n            example.append(sample[3][:max_length])\n            data_infor.append(sample[4])\n            examples.append(example)\n        result = []\n        for sample in zip(*examples):\n            result.append(torch.LongTensor(sample).to(self.device))\n        result.append(data_infor)\n        return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model(torch.nn.Module):\n    def __init__(self, decoder_output_dim, dropout_rate=0.5, device=\"cuda\"):\n        super(Model, self).__init__()\n        self.encoder = BertModel.from_pretrained('/kaggle/input/bert-base-uncased/')\n        encoder_output_dim = self.encoder.config.hidden_size\n        self.dropout = torch.nn.Dropout(dropout_rate)\n        self.decoder = torch.nn.Linear(encoder_output_dim, decoder_output_dim)\n        self.output_dim = decoder_output_dim\n        self.loss_fct = torch.nn.CrossEntropyLoss()\n        self.to(device)\n\n    def forward(self, input_ids, mask_ids, token_type_ids, label_ids=None):\n        bert_output = self.encoder(input_ids, mask_ids, token_type_ids)[0]\n        decoder_input = self.dropout(bert_output)\n        logits = self.decoder(decoder_input)\n        if label_ids is not None:\n            return self.loss_fct(logits.view(-1, self.output_dim), label_ids.view(-1))\n        else:\n            return logits","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Model(len(labels))\nprint(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import AdamW\n\ndef get_optimizer(model, bert_lr, lr, bert_weight_decay=0.05, adam_epsilon=1e-8):\n    optimizer_grouped_parameters = []\n    for n, p in model.named_parameters():\n        optimizer_params = {\"params\": p}\n        if \"encoder\" in n:\n            optimizer_params[\"lr\"] = bert_lr\n            if any(x in n for x in ['bias', 'LayerNorm.weight']):\n                optimizer_params[\"weight_decay\"] = 0\n            else:\n                optimizer_params[\"weight_decay\"] = bert_weight_decay\n        else:\n            optimizer_params[\"lr\"] = lr\n        optimizer_grouped_parameters.append(optimizer_params)\n    return AdamW(optimizer_grouped_parameters, eps=adam_epsilon)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = get_optimizer(model, 2e-5, 0.001)\nprint(optimizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from apex import amp\nmodel, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import get_linear_schedule_with_warmup\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=100, num_training_steps=10000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(model, optimizer, scheduler, data_loader, epochs=10):\n    model.train()\n    for epoch in range(epochs):\n        for step, batch in enumerate(data_loader):\n            input_ids = batch[0]\n            mask_ids = batch[1]\n            token_type_ids = batch[2]\n            label_ids = batch[3]\n            loss = model(input_ids, mask_ids, token_type_ids, label_ids)\n            print(f\"Epoch: {epoch} - step: {step} - loss: {loss.item()}\", end=\"\\n\" if step%100==0 else \"\\r\")\n            with amp.scale_loss(loss, optimizer) as scaled_loss:\n                scaled_loss.backward()\n            optimizer.step()\n            scheduler.step()\n            model.zero_grad()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def eval(model, data_loader, ignore_index=-100):\n    results = []\n    model.eval()\n    for step, batch in enumerate(data_loader):\n        input_ids = batch[0]\n        mask_ids = batch[1]\n        token_type_ids = batch[2]\n        label_ids = batch[3].cpu().data.numpy()\n        data_infor = batch[4]\n        logits = model(input_ids, mask_ids, token_type_ids)\n        logits = torch.argmax(logits, -1).cpu().data.numpy()\n        for infor, predicted_label_ids, target_label_ids in zip(data_infor, logits, label_ids):\n            predicted_labels = []\n            for predicted_label_id, target_label_id in zip(predicted_label_ids, target_label_ids):\n                if target_label_id != ignore_index:\n                    predicted_labels.append(labels[predicted_label_id])\n            \n            result = []\n            for word, predicted_label in zip(infor['sentence'], predicted_labels):\n                if predicted_label.endswith(infor['label']):\n                    result.append(word[0])\n                else:\n                    if len(result) > 0:\n                        break\n            if len(result) == 0:\n                for word, predicted_label in zip(infor['sentence'], predicted_labels):\n                    if predicted_label != \"O\":\n                        result.append(word[0])\n                    else:\n                        if len(result) > 0:\n                            break\n            if len(result) == 0:\n                for word, predicted_label in zip(infor['sentence'], predicted_labels):\n                    result.append(word[0])\n            results.append((infor[\"textID\"], \" \".join(result)))\n    return results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_loader = TextDataLoader(train_ids, shuffle=True, batch_size=32)\ntrain(model, optimizer, scheduler, train_loader, epochs=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_loader = TextDataLoader(test_ids, shuffle=False, batch_size=32)\nresults = eval(model, test_loader)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(results))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame(results, columns=[\"textID\", \"selected_text\"])\nsubmission.to_csv(\"submission.csv\", index=False)\nsubmission","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}