{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_BASE_PATH = '../input/tweet-sentiment-extraction/' \n\n# Read training & test data as pandas-dataframe (ending _df)\ntrain_df = pd.read_csv(DATA_BASE_PATH + 'train.csv') \ntest_df = pd.read_csv(DATA_BASE_PATH + 'test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Train-Data shape is: {train_df.shape}\") \nprint(f\"Test-Data shape is: {test_df.shape}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.dropna(inplace = True, how = 'any')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.sample(15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.loc[[9710]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.loc[[1305]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\ntrain_absolute_count = train_df[\"sentiment\"].value_counts()\ntrain_relative_count = train_df[\"sentiment\"].value_counts(normalize = True)\ntest_relative_count = test_df[\"sentiment\"].value_counts(normalize = True)\n\n# Create figure and add traces\nfig = make_subplots(1,3, subplot_titles = ('TRAIN data absolute amount',\n                                           'TRAIN data relative amount', \n                                           'TEST data relative amount'))\nfor i in fig['layout']['annotations']:\n            i['font'] = dict(size = 13)\n  \nfig.add_trace(go.Bar(x = train_absolute_count.index, y = train_absolute_count.values, \n                     marker_color = ['blue','green','red'], name= ''), row = 1, col = 1)\n\nfig.add_trace(go.Bar(x = train_relative_count.index, y = train_relative_count.values,                     \n                     marker_color = ['blue','green','red'], name= ''), row = 1, col = 2)\n\nfig.add_trace(go.Bar(x = test_relative_count.index, y = test_relative_count.values,\n                     marker_color = ['blue','green','red'], name= ''), row = 1, col = 3)\n\n\ntitle_text = \"Absolut and relative distribution of sentiments in train and test data\"\nfig.update_layout(title={'text': title_text})\n\n# Define default go-layout for future use\ndefault_layout =  go.Layout(  \n    title = {                    \n        'y': 0.95,\n        'x': 0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'\n    },\n    titlefont = {\n     'size' : 15, \n     'color': 'black'\n    },\n    font = {\n      'size' : 10 \n    })\n\nfig.update_layout(default_layout)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train=pd.read_csv(\"/kaggle/input/tweet-sentiment-extraction/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_count = train['sentiment'].value_counts()\nclass_count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cache = \"\"\"\ntrain_max_len = max(train_df[\"TEXT_number_of_words\"])\nbins = np.linspace(0, train_max_len, train_max_len) # np.linspace takes following arguments: startpoint, endpoint, number of steps\n\n# Cache tbd removed?\ncache = plt.hist(train_df[\"TEXT_number_of_words\"], bins, alpha=0.5, label = 'Number of words in \"TEXT\"')\ncache = plt.hist(train_df[\"SELECTED_TEXT_number_of_words\"], bins, alpha=0.5, label = 'Number of words in \"SELECTED_TEXT\"')\ncache = plt.legend(loc = 'upper right')\ncache = plt.style.use('seaborn-deep')\nplt.gcf().set_size_inches(15, 7)  # Wow, there's really no set_size_cm ...but yea, a conversion via a pre-defined tuple could help here.\ncache = plt.show()\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\n\ntrain_df[\"TEXT_number_of_words\"] = train_df[\"text\"].apply(lambda x: len(str(x).split()))  \ntrain_df[\"SELECTED_TEXT_number_of_words\"] = train_df[\"selected_text\"].apply(lambda x: len(str(x).split()))\ntrain_max_len = max(train_df[\"TEXT_number_of_words\"])\n\nfig = plt.figure(figsize=(18,8))\n# Number of bins shall equal the max-length in train_df['text']\nbins = np.linspace(0, train_max_len, train_max_len) \n# sns.distplot is a nice combination of sns.hist and sns.kdplot\nplot1 = sns.distplot(train_df['TEXT_number_of_words'], \n                     bins = bins, \n                     label = 'TEXT_number_of_words')\nplot1 = sns.distplot(train_df['SELECTED_TEXT_number_of_words'], \n                     bins =  bins,  \n                     label = 'SELECTED_TEXT_number_of_words')  \ncache = plt.legend() \n\nfig.suptitle('Distribution of number of words', fontsize = 20)\n\n# Defining default parameter for plt.rc for later re-use.\nparams = {\n    'figure.titlesize': 22, # fontsize of plot title / fig.suptitle\n    'axes.titlesize': 14,   # fontsize of the axes title\n    'axes.labelsize': 11,   # fontsize of the x and y labels    \n    'xtick.labelsize': 11,  # fontsize of the tick labels\n    'ytick.labelsize': 11,  # fontsize of the tick labels\n    'legend.fontsize': 12,  # fontsize for legends (plt.legend(), fig.legend())\n}\n\nplt.rcParams.update(params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['Diff_len_text_selected_text'] = train_df['TEXT_number_of_words'] - train_df['SELECTED_TEXT_number_of_words']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\n\ngrid = sns.FacetGrid(train_df, col = 'sentiment', height = 8)\n \ngrid.map(sns.lineplot, 'TEXT_number_of_words', 'Diff_len_text_selected_text', ci = None)\ngrid.add_legend()\nplt.subplots_adjust(top = 0.8)\ncache = grid.fig.suptitle('Difference of len(TEXT) and len(SELECTED_TEXT) over number of words in \"TEXT\"', fontsize = 24)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nimport spacy\nfrom tqdm import trange\nimport random\nfrom spacy.util import compounding,minibatch\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\nfig_box = go.Figure()\n\nfor _, sentiment in enumerate(sentiments):\n    # CHANGE HERE! You can change \"Diff_len_text_selected_text\" in below line to \n    # \"TEXT_number_of_words\" or \"SELECTED_TEXT_number_of_words\" to get the respective boxplots.  \n    fig_box.add_trace(go.Box(y = train_df[train_df['sentiment'] == sentiment]['Diff_len_text_selected_text'], name = sentiment)) \n\ntitle_text = 'Boxplot diagram difference in len(text) and len(selected_text)'\nfig_box.update_layout(title = {'text': title_text})\nfig_box.update_layout(default_layout)\n          \nfig_box.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def startsOrEndsWithSpecialCharacter(rowString): \n    '''\n    Returns a boolean for whether a given string starts OR ends with an unexpected characters\n    '''\n    # Check beginning of the string\n    pattern = '^[\\/_,.|;:#*~+-?!].*'\n    value =  0 if (re.match(pattern, rowString) == None) else 1\n    # Check ending: expected interpunctuation at the end of the selected text (! ? .) is allowed.  \n    pattern = '.*[\\/_,|;:#*~+-]$' \n    value = value | (0 if (re.match(pattern, rowString) == None) else 1)\n    # Check ending for white spaces before ending on a spec. character. E.g. \"hi .\"\n    value = value | endsWithAppendedSpecialCharactersAndWhitespace(rowString) \n    # print(re.match(pattern, rowString)) # for analysis, if needed\n    return value","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def startsWithPrependedSpecialCharactersAndWhitespace(rowString):   \n    pattern = '(^[\\/_|,;.:#*~+-!?]\\s+.*)'\n    # print(re.match(pattern, rowString)) \n    return 0 if (re.match(pattern, rowString) == None) else 1\n\n# looking for stuff like ',so happy'\ndef startsWithPrependedSpecialCharactersNoWhitespace(rowString): \n    pattern = '(^[\\/_|,;.:#*~+-](?!\\s+).*)'\n    # print(re.match(pattern, rowString))\n    return 0 if (re.match(pattern, rowString) == None) else 1\n\n# looking for stuff like 'so happy,') \n# expected interpunctuation at the end of the selected text (! ? .) is allowed\ndef endsWithAppendedSpecialCharactersNoWhitespace(rowString):  \n    pattern = '.*(<\\s)[\\/_|,;:#*~+-]$'  \n    # print(re.match(pattern, rowString))\n    return 0 if (re.match(pattern, rowString) == None) else 1\n\n# looking for stuff like 'so happy ,') \ndef endsWithAppendedSpecialCharactersAndWhitespace(rowString):  \n    pattern = '(.*\\s+)?[\\/_,.|;:#*~+-?!]$'\n    # print(re.match(pattern, rowString))\n    return 0 if (re.match(pattern, rowString) == None) else 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n\nstartsOrEndsWithSpecialCharacter_series = []\nstartsOrEndsWithSpecialCharacter_series = train_df[\"selected_text\"].apply(lambda x: startsOrEndsWithSpecialCharacter(x))\nstartsOrEndsWithSpecialCharacter_series = startsOrEndsWithSpecialCharacter_series[startsOrEndsWithSpecialCharacter_series > 0]\nstartsOrEndsWithSpecialCharacter_len = len((startsOrEndsWithSpecialCharacter_series[startsOrEndsWithSpecialCharacter_series == 1]))\nprint(f\"There are {startsOrEndsWithSpecialCharacter_len} tweets\"\n      f\"(= {startsOrEndsWithSpecialCharacter_len/train_df.shape[0] * 100:.2}%) that start or end with a special character pattern. \\n\"\n      \"Some examples look like this:\")\ntrain_df.loc[startsOrEndsWithSpecialCharacter_series.index].sample(10)[[\"textID\", \"text\", \"selected_text\", \"sentiment\"]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import spacy\n\nspaCy_model = 'en_core_web_lg' \nnlp = spacy.load(spaCy_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spaCy_vocab_list = (list(nlp.vocab.strings))\n# set ensures that all values are unique. All words converted to lower case.\nspaCy_vocab_set = set([word.lower() for word in spaCy_vocab_list])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"The loaded spaCy vocab {spaCy_model} contains unique lower case words: {len(spaCy_vocab_set)}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['clean_text'] = train_df['text'].str.lower()\ntrain_df['clean_selected_text'] = train_df['selected_text'].str.lower()\ntest_df['clean_text'] = test_df['text'].str.lower()\n\n# Create sets with unique words and update them \ntrain_text_vocab_set = set()\ntrain_selected_text_vocab_set  = set()\ntest_text_vocab_set  = set()\n\n# Apply set.update to fill the sets\ntrain_df['clean_text'].str.lower().str.split().apply(train_text_vocab_set.update)\ncache = train_df['clean_selected_text'].str.lower().str.split().apply(train_selected_text_vocab_set.update)\ncache = test_df['clean_text'].str.lower().str.split().apply(test_text_vocab_set.update)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\n    f\"The used spaCy model contains unique words: {len(spaCy_vocab_set)}  \\n\"\n    f\"TRAIN-datas 'text' column contains unique words: {(len(train_text_vocab_set))}  \\n\"\n    f\"TRAIN-datas 'selected_text' column contains unique words: {len(train_selected_text_vocab_set)} \\n\"\n    f\"TEST-datas 'selected_text' column contains unique words: {len(test_text_vocab_set)} \\n\"\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fraction_shared_words_train_text_to_spaCy = len(train_text_vocab_set.intersection(spaCy_vocab_set)) / (len(train_text_vocab_set))\nfraction_shared_words_train_selected_text_spaCy = len(train_selected_text_vocab_set.intersection(spaCy_vocab_set)) / (len(train_selected_text_vocab_set))\nfraction_shared_words_test_text_spaCy = len(test_text_vocab_set.intersection(spaCy_vocab_set)) / (len(test_text_vocab_set))\nfraction_shared_words_test_text_train_text = len(test_text_vocab_set.intersection(train_text_vocab_set)) / (len(test_text_vocab_set))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure([go.Bar(x = [\"train['text'] & spaCy\", \"train['selected_text'] & spaCy\", \"test['text'] & spaCy\", \"test['text'] & train['text']\"], \n                        y = [fraction_shared_words_train_text_to_spaCy, fraction_shared_words_train_selected_text_spaCy, fraction_shared_words_test_text_spaCy, fraction_shared_words_test_text_train_text], \n                        marker_color = ['blue','green','red'])])\n\ntitle_text = 'Fraction of words contained in both sets for relevant pairs'\nfig.update_layout(title={'text': title_text})\nfig.update_layout(default_layout)\n \nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Only {len(train_selected_text_vocab_set.intersection(train_text_vocab_set)) / (len(train_selected_text_vocab_set)):.4f}%\"\n      f\" of 'selected_text' is fully available in 'text',.. giving us {len(train_selected_text_vocab_set - train_text_vocab_set)} words which are cut off: \\n\")\nprint(train_selected_text_vocab_set - train_text_vocab_set)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_clean_text_word_list = ' '.join([i for i in train_df['clean_text']]).split()  \ntrain_clean_selected_text_word_list = ' '.join([i for i in train_df['clean_selected_text']]).split()  \ntest_clean_text_word_list = ' '.join([i for i in test_df['clean_text']]).split()  \n\n# Calculate differences \nnot_shared_train_text_to_spaCy = [word for word in train_clean_text_word_list if ((word in train_text_vocab_set) and (word not in spaCy_vocab_set))]\nnot_shared_train_selected_text_to_spaCy = [word for word in train_clean_selected_text_word_list if ((word in train_selected_text_vocab_set) and (word not in spaCy_vocab_set))]\nnot_shared_test_text_to_spaCy = [word for word in test_clean_text_word_list if ((word in test_text_vocab_set) and (word not in spaCy_vocab_set))]\nnot_shared_test_text_to_train_text = [word for word in test_clean_text_word_list if ((word in test_text_vocab_set) and (word not in train_text_vocab_set))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=[30, 15])\nwordcloud1 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(not_shared_train_text_to_spaCy ))\ntitleFontsize = 20\n\nax1.imshow(wordcloud1)\nax1.axis('off')\nax1.set_title('Not shared words: train_text and spaCy vocab',fontsize = titleFontsize);\n\nwordcloud2 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(not_shared_train_selected_text_to_spaCy))\nax2.imshow(wordcloud2)\nax2.axis('off')\nax2.set_title('Not shared words: train_selected_text and spaCy vocab',fontsize = titleFontsize)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=[30, 15])\nwordcloud1 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(not_shared_train_text_to_spaCy ))\ntitleFontsize = 20\n\nwordcloud3 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(not_shared_test_text_to_spaCy))\nax1.imshow(wordcloud3)\nax1.axis('off')\nax1.set_title('Not shared words: test_text and spaCy vocab',fontsize=titleFontsize);\n\nwordcloud4 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(not_shared_test_text_to_train_text))\nax2.imshow(wordcloud4)\nax2.axis('off')\nax2.set_title('Not shared words: Test_text to train_text',fontsize=titleFontsize)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def jaccard_score (str1, str2):\n    '''\n    Returns the Jaccard Score (intersection over union) of two strings.\n    '''\n    a = set(str1.lower().split())\n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    \n    return float(len(c) / (len(a) + len(b) - len(c)))  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"Jaccard_Score_text_selected_text\"] = train_df.apply(lambda x: jaccard_score(str(x[\"text\"]), str(x[\"selected_text\"])), axis = 1) \n[avg_Jaccard_neu_train, avg_Jaccard_neg_train, avg_Jaccard_pos_train] = [train_df[train_df[\"sentiment\"] == sentiment][\"Jaccard_Score_text_selected_text\"].mean() for sentiment in [\"neutral\", \"negative\", \"positive\"]]\navg_Jaccard_train = pd.Series(data = {'Avg Jaccard neutral': avg_Jaccard_neu_train, 'Avg Jaccard negative': avg_Jaccard_neg_train, \"Avg Jaccard positive\": avg_Jaccard_pos_train}, name = 'Jaccard Score per sentiment' )\n\nprint(f\"Overall avg. Jaccard Score: {(avg_Jaccard_neu_train + avg_Jaccard_neg_train + avg_Jaccard_pos_train) / 3}\")\nprint(avg_Jaccard_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_val_split = 0.9\n\ntrain_df_copy = train_df.copy() # Create a copy of the train_df to work with and not change stuff in-place\n# Setting a random state for reproducable splits\ntrain_set_df = train_df_copy.sample(frac = train_val_split, random_state = 42) \nval_set_df = train_df_copy.drop(train_set_df.index)\nval_set_df.drop(['clean_text', 'clean_selected_text'], axis='columns', inplace = True)\n\n# Get val_set for each sentiment\nval_set_pos_df = val_set_df[val_set_df['sentiment'] == 'positive'].copy()\nval_set_neg_df = val_set_df[val_set_df['sentiment'] == 'negative'].copy()\nval_set_neu_df = val_set_df[val_set_df['sentiment'] == 'neutral'].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_training_data(sentiment, splitAtLength = 0):\n    '''\n    Returns the training data in spaCy-required format for the given sentiment.\n    If 'splitAtLength' is > 0, two none-empty arrays are returned: \n    first array containing the train_data with a 'text' containing more words than 'splitAtLength' \n    and the second arraycontaining train_data with 'text' containing number of words up to 'splitAtLength'.\n    If splitAtLength is 0 or None, only one none-empty array \n    containing all train_data is returned, the second array is empty.\n    \n            Parameters:\n                    sentiment (str): sentiment for which train_data needs to be returned\n                    splitAtLength (int, optional): determins if and where the train_data is split\n\n            Returns:\n                    train_data (array): Returns the train data in an array, or array of arrays, if splitAtLength > 0.\n    '''\n    train_data_long = []\n    train_data_short = []    \n    \n    for idx in train_set_df.index:\n        if train_set_df.at[idx, 'sentiment'] == sentiment:\n            text = train_set_df.at[idx,'text']\n            len_text = len(text.split())            \n            selected_text = train_set_df.at[idx,'selected_text']\n            start = text.find(selected_text)\n            end = start + len(selected_text)\n            # create the train data in spaCy-required format. We can choose any \"dummy_label\" here\n            # as we are anyway training just ONE model per sentiment:\n            # all labels would be identical (e.g. positive) anyway.\n            if (splitAtLength == None) or (splitAtLength == 0):\n                   train_data_long.append((text, {\"entities\": [[start, end, \"dummy_label\"]]}))\n            elif len(len_text) >= splitAtLength:\n                   train_data_long.append((text, {\"entities\": [[start, end, \"dummy_label\"]]}))\n            elif len(len_text) < splitAtLength:\n                   train_data_short.append((text, {\"entities\": [[start, end, \"dummy_label\"]]}))\n            else: print(\"something is wrong in getting training data\")  \n            \n    return [train_data_long, train_data_short]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_validation_data(sentiment, model_type, splitAtLength = None):\n    '''\n    Returns the validation data used in training function\n           \n           Parameters:\n                    sentiment (str): sentiment for which validation data needs to be returned\n                    model_type (str): determins if data for short or long model is needed\n                    splitAtLength (int, optional): determins if and where the data is split\n\n           Returns:\n                    train_data (array): Returns the train data in an array, or array of arrays, if splitAtLength > 0.\n    '''\n    if ((splitAtLength is None) or (splitAtLength == 0)):\n        val_set_new_df = val_set_df[val_set_df['sentiment'] == sentiment].copy()  \n        return val_set_new_df\n    \n    if (splitAtLength > 0) and (model_type == \"short\"): # tweets with length up to splitAtLength\n        val_set_new_df = val_set_df[val_set_df[\"text\"].str.split().str.len() < splitAtLength].copy()\n        val_set_new_df = val_set_new_df[val_set_new_df['sentiment'] == sentiment].copy()\n        return val_set_new_df\n    \n    if (splitAtLength > 0) and (model_type == \"long\"): # tweets with length > splitAtLength\n        val_set_new_df = val_set_df[val_set_df[\"text\"].str.split().str.len() >= splitAtLength].copy()\n        val_set_new_df = val_set_new_df[val_set_new_df['sentiment'] == sentiment].copy()\n        return val_set_new_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_model_output_path(sentiment, splitAtLength = None, train_val_split = 0.90):\n    '''\n    Creates an easy to understand path for saving the model based on  the models parameters\n    '''\n    model_out_paths = []\n    if splitAtLength == None or splitAtLength == 0:\n        model_out_paths.append('models/model_'\n                               + str(sentiment)\n                               + '_splitAtLength_Longer_than'\n                               + str(splitAtLength)\n                               + '_train_val_split_'\n                               + str(train_val_split))\n        model_out_paths.append(None)\n    elif splitAtLength != None:\n        model_out_paths.append('models/model_' \n                               + str(sentiment)\n                               + '_splitAtLength_Longer_than'\n                               + str(splitAtLength)\n                               + '_train_val_split_'\n                               + str(train_val_split))\n        model_out_paths.append('models/model_'\n                               + str(sentiment)\n                               + '_splitAtLength_Up_To'\n                               + str(splitAtLength) + '_train_val_split_' \n                               + str(train_val_split))\n    return model_out_paths","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def save_model(output_dir, model , new_model_name, rank):\n    '''\n    Saves the model to an easy to understand path\n    \n        Parameters:\n            output_dir (str): easy to understand path derived from models parameters\n            model (model): the trained NLP model from spaCy            \n            new_model_name (str): name for the model, NOT used for loading later\n            rank (str): determins if the model is the best, 2nd best or 3rd best and adds it to save-path.\n    '''\n    output_dir = f'../working/{output_dir}_{rank}'\n    if output_dir is not None:        \n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n        model.meta[\"name\"] = new_model_name\n        model.to_disk(output_dir)\n        print(\"Saved model to\", output_dir)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # algebra\nimport pandas as pd # data frames & processing, CSV file I/O \n\n# Visualization, graphs & plots\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go # plots & graphics \nfrom plotly.subplots import make_subplots \nimport seaborn as sns # plots & graphics\n\n# SpaCy\nimport spacy\nfrom spacy.util import compounding\nfrom spacy.util import minibatch\nfrom thinc.neural.optimizers import Adam # for hyperparameter tuning\nfrom thinc.neural import Model # for hyperparameter tuning\n\n# Utilities & helpers\nfrom tqdm import tqdm_notebook as tqdm # progress bar\nimport time\nimport os\nimport re # Regex library\nimport random\nimport timeit # measuring execution time\nfrom wordcloud import WordCloud\nimport warnings\nwarnings.filterwarnings('ignore') #Ignore \"future\" warnings and Data-Frame-Slicing warnings.\n\n\ndef train(sentiment, output_dir, epochs = 1, model = None, optimizer = None, dropout = 0.5, splitAtLength = 0, fill_no_predictions_with_text = False):\n    '''\n    Load the model, set up the pipeline and train the entity recognizer\n    '''\n    # define aborting conditions\n    if epochs == 0 or epochs == None:\n        return\n    if output_dir == [] or output_dir == None:\n        return\n\n    # extract info if this model shall be for short or for long tweets from \"output_dir\"\n    if \"Longer_than\" in output_dir:\n        model_type = \"long\"\n        # for long model, get 1st entry in array from get_training_data (used below)\n        short_data = 0 \n    elif \"Up_To\" in output_dir:\n        model_type = \"short\" \n         # for short model, get 2nd entry in array from get_training_data (used below)\n        short_data = 1\n        \n    # get train data relevant for this training session\n    train_data = get_training_data(sentiment, splitAtLength = 0)[short_data]\n    \n    # get validation data relevant for this training session\n    val_data = get_validation_data(sentiment, model_type, splitAtLength = splitAtLength)\n   \n    if output_dir == None:\n        return \n   \n    if model is not None:\n        nlp = spacy.load(output_dir)  # load existing spaCy model to continue training\n        print(f'Loaded model {model}')\n    else:\n        nlp = spacy.blank(\"en\")  # create blank Language class\n        print(\"Created blank 'en' model\")\n    \n    # create the built-in pipeline components and add them to the pipeline\n    # nlp.create_pipe works for built-ins that are registered with spaCy\n    if \"ner\" not in nlp.pipe_names:\n        ner = nlp.create_pipe(\"ner\")\n        nlp.add_pipe(ner, last = True)\n    # otherwise, get it so we can add labels\n    else:\n        ner = nlp.get_pipe(\"ner\")\n        print(\"Getting NER-pipe in spaCy model.\")\n    \n    # add all labels available in train_data\n    # we could adjust train_data to get more/different labels\n    # this will be explained in detail later.\n    for _, annotations in train_data:\n        for ent in annotations.get(\"entities\"):\n            ner.add_label(ent[2])\n\n    # get names of other pipes to disable them during training\n    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n    with nlp.disable_pipes(*other_pipes):  # only train NER       \n        if model is None:\n            nlp.begin_training()\n            optimizer = nlp.begin_training() if optimizer is None else optimizer\n        else:\n            nlp.resume_training() \n            optimizer = nlp.resume_training() if optimizer is None else optimizer\n\n        # initialize values to avoid \"referenced before assignment\" issues.\n        best_Jaccard_score, second_best_Jaccard_score, third_best_Jaccard_score = [0,0,0]\n        last_update_best_model, last_update_2nd_best_model, last_update_3rd_best_model = [0,0,0]\n        improvement_best, improvement_2ndbest, improvement_3rdbest = [0,0,0]\n        \n       \n        # actual train-step\n        for itn in tqdm(range(epochs)):\n            random.shuffle(train_data)\n            # batch up the examples using spaCy's minibatch\n            # compounding(start batch size, end batch size,  compounding factor).\n            batches = minibatch(train_data, size = compounding(1.0, 100.0, 1.15) )    \n            losses = {}\n            for batch in batches:\n                texts, annotations = zip(*batch)\n                nlp.update(texts,  # batch of texts\n                                    annotations,  # batch of annotations\n                                    drop = dropout,   # makes it harder to memorize data\n                                    sgd = optimizer,\n                                    losses=losses, \n                                    )\n            \n       # test model on validattion_df and measure time\n            start = time.time()\n            [avg_pred_jaccard, number_no_predictions] = val_predictions_and_calc_Jaccard(\n                sentiment = sentiment,\n                model = nlp,\n                val_df = val_data,               \n                fill_no_predictions_with_text = fill_no_predictions_with_text)\n            ende = time.time()\n            print(\"Losses\", losses)\n            print(f'Avg. Jaccard Score for sentiment \"{sentiment}\" is: {avg_pred_jaccard:.4f},'                  \n                  f' Number of empty predictions: {number_no_predictions}')\n            \n            # keep track of top 3 models & save them                      \n            if avg_pred_jaccard > best_Jaccard_score:\n                        improvement_best = avg_pred_jaccard - best_Jaccard_score\n                        best_Jaccard_score = avg_pred_jaccard\n                        save_model(output_dir,\n                                   model = nlp, \n                                   new_model_name = output_dir.split('/')[-1],\n                                   rank = 'best')\n                        last_update_best_model = itn + 1  \n            elif avg_pred_jaccard > second_best_Jaccard_score:\n                        improvement_2ndbest = avg_pred_jaccard - second_best_Jaccard_score\n                        second_best_Jaccard_score = avg_pred_jaccard\n                        save_model(output_dir,\n                                   model = nlp,\n                                   new_model_name = output_dir.split('/')[-1],\n                                   rank = 'second_best')\n                        last_update_2nd_best_model = itn + 1     \n            elif avg_pred_jaccard > third_best_Jaccard_score:\n                        improvement_3rdbest = avg_pred_jaccard - third_best_Jaccard_score\n                        third_best_Jaccard_score = avg_pred_jaccard\n                        save_model(output_dir,\n                                   model = nlp,\n                                   new_model_name = output_dir.split('/')[-1],\n                                   rank = 'third_best')  \n                        last_update_3rd_best_model = itn + 1   \n            else: print(\"Model didn't perform better, therefore not saved.\")    \n                    \n    \n    if best_Jaccard_score > 0: # only if some progress was made:\n        print(f'\\n \\n \\nBest model reached {best_Jaccard_score:.4f} and was updated' \n              f' (+{improvement_best:.4f}) last in {last_update_best_model}th epoch \\n'\n              f'2nd best model reached {second_best_Jaccard_score:.4f} and was updated'\n              f' (+{improvement_2ndbest:.4f}) last in {last_update_2nd_best_model}th epoch\\n'\n              f'3rd best model reached {third_best_Jaccard_score:.4f} and was updated' \n              f' (+{improvement_3rdbest:.4f})last in {last_update_3rd_best_model}th epoch')    \n\n    log = str(splitAtLength) + '_' + str(model_type)\n    results[log] = {'best_Jaccard_score': best_Jaccard_score,\n                                  'sentiment': sentiment,\n                                  'splitAtLength': splitAtLength,                              \n                                  'model_type': model_type}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_predictions(text, model, fill_no_predictions_with_text):\n    '''\n    Predicts entities based on the given model.\n    Set fill_no_predictions_with_text to TRUE for creating a valid (high scoring) submission.\n    If fill_no_predictions_with_text is false; all no-predictions will be marked with\n    \"NO-PREDICTION\" for fruther evaluation.\n    '''\n    doc = model(text)\n    ent_array = []\n    for ent in doc.ents:\n        start = text.find(ent.text)\n        end = start + len(ent.text)\n        new_int = [start, end, ent.label_]\n        ent_array.append([start, end, ent.label_])\n    if fill_no_predictions_with_text:\n        selected_text = text[ent_array[0][0]: ent_array[0][1]] if len(ent_array) > 0 else text\n    else:\n        selected_text = text[ent_array[0][0]: ent_array[0][1]] if len(ent_array) > 0 else \"NO-PREDICTION\"\n    return selected_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def val_predictions_and_calc_Jaccard (sentiment, model, val_df, fill_no_predictions_with_text):    \n    val_df['prediction']  = val_df[\"text\"].apply(str).apply(lambda x: make_predictions(x, model, fill_no_predictions_with_text))\n    val_df['pred_jaccard'] = val_df.apply(lambda x: jaccard_score (str(x['selected_text']), str(x['prediction'])), axis = 1)\n    return [val_df['pred_jaccard'].mean(), np.sum(val_df[\"prediction\"] == \"NO-PREDICTION\")]           ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nsplitAtLength: Allows to create a model per sentiment for tweets longer than splitAtLength words\nand another model for shorter tweets --> so two models per sentiment.\nsplitAtLength = 0 (default): means NO split at all --> 1 model per sentiment.\nsplitAtLength_list: Can contain one or multiple values which will be iterated over.\nUse this for grid search for optimal length!\nMedian for positive and negative tweets is 12, so half of the tweets contain less than \n(or equal to) 12 words and the other half contains more.\n'''\nimport numpy as np # algebra\nimport pandas as pd # data frames & processing, CSV file I/O \n\n# Visualization, graphs & plots\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go # plots & graphics \nfrom plotly.subplots import make_subplots \nimport seaborn as sns # plots & graphics\n\n# SpaCy\nimport spacy\nfrom spacy.util import compounding\nfrom spacy.util import minibatch\nfrom thinc.neural.optimizers import Adam # for hyperparameter tuning\nfrom thinc.neural import Model # for hyperparameter tuning\n\n# Utilities & helpers\nfrom tqdm import tqdm_notebook as tqdm # progress bar\nimport time\nimport os\nimport re # Regex library\nimport random\nimport timeit # measuring execution time\nfrom wordcloud import WordCloud\nimport warnings\nwarnings.filterwarnings('ignore') #Ignore \"future\" warnings and Data-Frame-Slicing warnings.\nsplitAtLength_list = [0] \n\n# Epochs for long and short model\nepochs_long = 30\nepochs_short = 30\n\ndropout_rate = 0.2\n\n'''\nIf fill_no_predictions_with_text is FALSE: all rows without a prediction will be marked with\n\"NO-PREDICTION\" for further evaluation.\n'''\nfill_no_predictions_with_text = True \n\n# Optimizer options\nops = Model.ops\nlearn_rate = 0.0015 # default: 0.001\nL2 = 1e-5 # L2 regularisation penatly. Default: 1e-6\nmax_grad_norm = 1.0 # avoiding exploding gradients. Default: 1\noptimizer = Adam(ops, learn_rate, L2 = L2) \noptimizer.max_grad_norm = max_grad_norm\n\n# Determin for which sentiments to train a model for.\n# Neutral sentiment is skipped here, as we can't get a better score\n# for neutral sentiment than just using text als selected_text.\n\ntrain_for_sentiments = ['negative', 'positive']\n\n## Start the training\nfor splitAtLength in splitAtLength_list:\n    for sentiment in train_for_sentiments:\n        # Gget model path and determin whether to train two or one model per sentiment based on \n        # the value of  'splitAtLength'. returns [model_path_long, []] if splitAtLength = 0. \n        # Returns two paths if splitAtLength > 0\n        [model_path_long, model_paths_short]  = get_model_output_path(\n            sentiment,\n            splitAtLength = splitAtLength,\n            train_val_split = train_val_split) \n\n        train(sentiment, \n              output_dir = model_path_long, \n              epochs = epochs_long, \n              model = None,\n              optimizer = optimizer,\n              dropout = dropout_rate, \n              splitAtLength = splitAtLength, \n              fill_no_predictions_with_text = fill_no_predictions_with_text)\n        \n        # Train for short model will only be executed if model_paths_short != [] \n        train(sentiment, \n              output_dir = model_paths_short, \n              epochs = epochs_short, \n              model = None,\n              optimizer = optimizer, \n              dropout = dropout_rate,\n              splitAtLength = splitAtLength, \n              fill_no_predictions_with_text = fill_no_predictions_with_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_best_models(splitAtLength):\n    '''\n    Loads the best models, predicting for each row in \"target_df\" and putting result in \"target_column\"\n    '''\n    BASE_PATH = f'../working/'\n    BASE_PATH_BEST_LONG_POS = BASE_PATH + get_model_output_path(\"positive\", splitAtLength, train_val_split)[0] + \"_best\"\n    BASE_PATH_BEST_SHORT_POS = BASE_PATH + get_model_output_path(\"positive\", splitAtLength, train_val_split)[1] + \"_best\" if get_model_output_path(\"positive\", splitAtLength, train_val_split)[1] else \"no-model\"\n    BASE_PATH_BEST_LONG_NEG = BASE_PATH + get_model_output_path(\"negative\", splitAtLength, train_val_split)[0] + \"_best\"\n    BASE_PATH_BEST_SHORT_NEG = BASE_PATH + get_model_output_path(\"negative\", splitAtLength, train_val_split)[1] + \"_best\" if get_model_output_path(\"positive\", splitAtLength, train_val_split)[1] else \"no-model\"\n        \n    model_long_best_pos = spacy.load(BASE_PATH_BEST_LONG_POS) if os.path.isdir(BASE_PATH_BEST_LONG_POS) else None\n    model_short_best_pos = spacy.load(BASE_PATH_BEST_SHORT_POS) if os.path.isdir(BASE_PATH_BEST_SHORT_POS) else None\n    model_long_best_neg = spacy.load(BASE_PATH_BEST_LONG_NEG) if os.path.isdir(BASE_PATH_BEST_LONG_NEG) else None\n    model_short_best_neg = spacy.load(BASE_PATH_BEST_SHORT_NEG) if os.path.isdir(BASE_PATH_BEST_SHORT_NEG) else None\n    print(f'Models loaded:\\nModel_pos_long_best: {model_long_best_pos != None} \\n'\n          f'Model_pos_short_best: {model_short_best_pos != None} \\n'\n          f'Model_neg_long_best: {model_long_best_neg != None}\\n'\n          f'Model_neg_short_best: {model_short_best_neg != None}')\n    return [model_long_best_pos, model_short_best_pos, model_long_best_neg, model_short_best_neg]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fill_dataframe_with_predictions(splitAtLength, target_df, target_column, fill_no_predictions_with_text):\n    '''\n    Loads the best models, predicting for each row in \"target_df\" and putting result in \"target_column\"\n    '''\n    \n    [model_long_best_pos, model_short_best_pos, model_long_best_neg, model_short_best_neg] = load_best_models(splitAtLength)\n                 \n    # making it easier to deal with splitAtLength = None\n    if splitAtLength is None:\n        splitAtLength = 0\n            \n    target_df[target_column] = \"EMPTY\"\n        \n    for idx in target_df.index:\n        text = target_df.at[idx,'text']\n        sentiment = target_df.at[idx,'sentiment']\n        if sentiment == 'neutral':  \n            target_df.at[idx, target_column] = target_df.at[idx, 'text']\n            # positive sentiment    \n        elif sentiment == 'positive' and len(text.split()) > splitAtLength:\n            target_df.at[idx, target_column] = make_predictions(\n                                                                text,\n                                                                model_long_best_pos,\n                                                                fill_no_predictions_with_text) if model_long_best_pos != None else text  \n        elif sentiment == 'positive' and len(text.split()) <= splitAtLength:\n            target_df.at[idx, target_column] = make_predictions(text,\n                                                                model_short_best_pos,\n                                                                fill_no_predictions_with_text) if model_short_best_pos != None else text\n            # negative sentiment      \n        elif sentiment == 'negative' and len(text.split()) > splitAtLength:\n            target_df.at[idx, target_column] = make_predictions(text,\n                                                                model_long_best_neg,\n                                                                fill_no_predictions_with_text) if model_long_best_neg != None else text\n        elif sentiment == 'negative' and len(text.split()) <= splitAtLength:\n            target_df.at[idx, target_column] = make_predictions(text,\n                                                                model_short_best_neg,\n                                                                fill_no_predictions_with_text) if model_short_best_neg != None else text\n        else:\n            print('something is wrong with fillSubmissionDf()')\n    return target_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_set_with_preds_df = fill_dataframe_with_predictions(splitAtLength,\n                                                        val_set_df.copy(),\n                                                        target_column = 'pred_selected_text',\n                                                        fill_no_predictions_with_text = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}