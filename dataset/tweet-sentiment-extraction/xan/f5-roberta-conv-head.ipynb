{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install --upgrade pip\n!pip install tokenizers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport gc\nimport os\nimport random\nimport time\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\n# torch modules\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n\n# transformer modules\nfrom transformers import RobertaConfig\nfrom transformers import RobertaTokenizer\nfrom transformers import RobertaModel\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup\n\n# tokenizer modules\nimport tokenizers\n\n# sklearn modules\nfrom sklearn import metrics\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed_value):\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    os.environ['PYTHONHASHSEED'] = str(seed_value)\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = True\n\nseed = 42\nseed_everything(seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = torch.randn(32, 192, 768)\nx = torch.nn.functional.pad(input=x.transpose(1, 2), pad=(1, 0), mode='constant', value=0)\nconv = nn.Conv1d(768, 128, 2, stride=1)\nfc = nn.Linear(128, 1)\ncout = conv(x)\nout = fc(cout.transpose(1, 2))\nprint(cout.size())\nprint(cout.transpose(1, 2).size())\nprint(out.size())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# map text, selected_text and offsets\n\n# offset - start and end position of any tokens\n# pseudocode\n\n# get indicies of the selected_text in text\n\ndef process_data(text, selected_text, sentiment, tokenizer, max_length):\n    \"\"\"\n    Process text, selected_text, sentiment etc. by encoding them using tokenizer\n    \n    return: \n            padded input_ids, token_type_ids, attention_mask, offsets, \n            target_start_idx, target_end_idx, orig_text, orig_selected_text, sentiment\n    \"\"\"\n    \n    text = \" \" + \" \".join(str(text).strip().lower().split())\n    selected_text = \" \" + \" \".join(str(selected_text).strip().lower().split())\n    sentiment = sentiment.lower()\n    \n    char_targets = [0] * len(text)\n\n    for idx in (i for i, e in enumerate(text) if e == selected_text[1]):\n        if \" \" + text[idx:idx + len(selected_text) - 1] == selected_text:\n            idx0 = idx\n            idx1 = idx + len(selected_text) - 1\n            break\n\n    for idx in range(idx0, idx1):\n        char_targets[idx] = 1\n\n    assert len(char_targets) == len(text), \"Length of char_targets not equal to len(text)\"\n\n    # encoding by tokenizer\n    output = tokenizer.encode(text)\n    \n    input_ids_orig = output.ids\n    offsets_orig = output.offsets\n    \n    target_idx = [j for j, (offset1, offset2) in enumerate(offsets_orig) \n                  if sum(char_targets[offset1: offset2]) > 0]\n\n    target_start_idx = target_idx[0]\n    target_end_idx = target_idx[-1]\n    \n    # token ids for sentiments\n    SENTIMENT = {'positive': 1313, 'negative': 2430, 'neutral': 7974}\n    \n    # create other output variables\n    input_ids = [0] + [SENTIMENT[sentiment]] + [2] + [2] + input_ids_orig + [2]\n    token_type_ids = [0] * len(input_ids)\n    attention_mask = [1] * len(token_type_ids)\n    offsets = [(0, 0)] * 4 + offsets_orig + [(0, 0)]\n    target_start_idx += 4\n    target_end_idx += 4\n    \n    # add padding\n    padding_length = max_length - len(input_ids)\n    \n    if padding_length > 0:\n        input_ids += [0] * padding_length\n        token_type_ids += [1] * padding_length\n        attention_mask += [0] * padding_length\n        offsets += [(0, 0)] * padding_length\n    \n    assert len(input_ids) == len(token_type_ids) == len(attention_mask) == len(offsets), \\\n    f\"Lengths mismatch: {len(input_ids)}, {len(token_type_ids)}, {len(attention_mask)}, {len(offsets)} \"\n\n    return {\n        'input_ids': input_ids,\n        'token_type_ids': token_type_ids,\n        'attention_mask': attention_mask,\n        'offsets': offsets,\n        'target_start_idx': target_start_idx,\n        'target_end_idx': target_end_idx,\n        'orig_text': text,\n        'orig_selected_text': selected_text,\n        'sentiment': sentiment}\n\n\nclass TweetDataset(Dataset):\n    def __init__(\n        self, \n        text=None, \n        selected_text=None, \n        sentiment=None, \n        tokenizer=None, \n        max_length=None):\n        \n        self.text = text\n        self.selected_text = selected_text\n        self.sentiment = sentiment\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        \n    def __len__(self):\n        return self.text.shape[0]\n    \n    def __getitem__(self, item):\n        output = process_data(\n            text=self.text[item],\n            selected_text=self.selected_text[item],\n            sentiment=self.sentiment[item],\n            tokenizer=self.tokenizer,\n            max_length=self.max_length)\n        \n        return {\n            'input_ids': torch.tensor(output['input_ids'], dtype=torch.long),\n            'token_type_ids': torch.tensor(output['token_type_ids'], dtype=torch.long),\n            'attention_mask': torch.tensor(output['attention_mask'], dtype=torch.long),\n            'offsets': torch.tensor(output['offsets'], dtype=torch.long),\n            'target_start_idx': torch.tensor(output['target_start_idx'], dtype=torch.long),\n            'target_end_idx': torch.tensor(output['target_end_idx'], dtype=torch.long),\n            'orig_text': output['orig_text'],\n            'orig_selected_text': output['orig_selected_text'],\n            'sentiment': output['sentiment']}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# add conv layers to the roberta model\n\nclass RobertaBaseModel(nn.Module):\n    def __init__(self):\n        super(RobertaBaseModel, self).__init__()\n        config = RobertaConfig.from_pretrained('roberta-base', output_hidden_states=True)\n        self.roberta_model = RobertaModel.from_pretrained('roberta-base', config=config)\n        self.fc = nn.Linear(config.hidden_size, 2)\n        self.dropout = nn.Dropout(0.2)\n        \n        nn.init.normal_(self.fc.weight, std=0.02)\n        nn.init.normal_(self.fc.bias, 0)\n        \n        \n    def forward(self, input_ids, token_type_ids=None, attention_mask=None):\n        _, _, hs = self.roberta_model(\n            input_ids=input_ids, \n            token_type_ids=token_type_ids,\n            attention_mask=attention_mask)\n         \n        x = torch.stack([hs[-1], hs[-2], hs[-3], hs[-4], hs[6]])\n        x = torch.mean(x, 0)\n        x = self.dropout(x)\n        x = self.fc(x)\n        start_logits, end_logits = x.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n\n        return start_logits, end_logits\n    \n\nclass RobertaConvHead(nn.Module):\n    def __init__(self):\n        super(RobertaConvHead, self).__init__()\n        config = RobertaConfig.from_pretrained('roberta-base')\n        config.output_hidden_states=True\n        \n        self.roberta = RobertaModel.from_pretrained('roberta-base', config=config)\n        \n        _size = 128\n        \n        self.conv1a = nn.Conv1d(config.hidden_size, _size, 2)\n        self.conv1b = nn.Conv1d(config.hidden_size, _size, 2)\n        \n        self.fc1a = nn.Linear(_size, 1)\n        self.fc1b = nn.Linear(_size, 1) \n        \n        self.relu = nn.ReLU(True)\n        self.leaky_relu = nn.LeakyReLU(0.1)\n        \n        self.dropout = nn.Dropout(0.2)\n        \n        # initialize weights and biases\n        nn.init.normal_(self.conv1a.weight, std=0.02)\n        nn.init.normal_(self.conv1b.weight, std=0.02)\n        \n        nn.init.normal_(self.conv1a.bias, 0)\n        nn.init.normal_(self.conv1b.bias, 0)\n        \n        nn.init.normal_(self.fc1a.weight, std=0.02)\n        nn.init.normal_(self.fc1a.bias, 0)\n        \n        nn.init.normal_(self.fc1b.weight, std=0.02)\n        nn.init.normal_(self.fc1b.bias, 0)\n        \n        \n    def forward(\n        self, \n        input_ids, \n        relu=False, \n        token_type_ids=None, \n        attention_mask=None):\n\n        _, _, hs = self.roberta(\n            input_ids=input_ids, \n            token_type_ids=token_type_ids, \n            attention_mask=attention_mask)\n        \n        \n        # stack the last four layer hidden states\n        hidden_states = self.dropout(torch.stack([hs[-1], hs[-2], hs[-3], hs[-4]]))\n        \n        # mean pool\n        _mp = torch.mean(hidden_states, axis=0)\n        \n        # pad: to get same size after convolution\n        _out = nn.functional.pad(\n            _mp.transpose(1, 2), pad=(1, 0), mode='constant', value=0)\n        \n        if relu is True:\n            cout_a = self.dropout(self.relu(self.conv1a(_out)))\n            cout_b = self.dropout(self.relu(self.conv1b(_out)))\n            \n        else:\n            cout_a = self.dropout(self.leaky_relu(self.conv1a(_out)))\n            cout_b = self.dropout(self.leaky_relu(self.conv1b(_out)))\n        \n        out_a = self.fc1a(cout_a.transpose(1, 2))\n        out_b = self.fc1b(cout_b.transpose(1, 2))\n        \n        start_logits = out_a.squeeze(-1)\n        end_logits = out_b.squeeze(-1)\n        \n        return start_logits, end_logits","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def loss_fn(start_logits, end_logits, target_start_idx, target_end_idx):\n    ce = nn.CrossEntropyLoss()\n    start_loss = ce(start_logits, target_start_idx)\n    end_loss = ce(end_logits, target_end_idx)\n    return start_loss + end_loss\n\n\ndef jaccard(str1, str2):\n    a = set(str1.lower().split())\n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))\n\n# To compute jaccard score we need two strings - text and selected_text\n# we have the text we only need to find the selected_text\n\ndef compute_jaccard_score(text, start_idx, end_idx, start_logits, end_logits, offsets):\n    start_pred = np.argmax(start_logits)\n    end_pred = np.argmax(end_logits)\n    if start_pred > end_pred:\n        pred = text\n    else:\n        pred = get_selected_text(text, start_pred, end_pred, offsets)\n        \n    true = get_selected_text(text, start_idx, end_idx, offsets)\n    \n    return jaccard(true, pred)\n\n\ndef get_selected_text(text, start_idx, end_idx, offsets):\n    selected_text = \"\"\n    for ix in range(start_idx, end_idx + 1):\n        selected_text += text[offsets[ix][0]: offsets[ix][1]]\n        if (ix + 1) < len(offsets) and offsets[ix][1] < offsets[ix + 1][0]:\n            selected_text += \" \"\n    return selected_text\n\n\ndef train_fn(model, dataloader, optimizer=None, scheduler=None, device=None):\n    model.train()\n    train_loss = []\n    \n    for i, data in enumerate(dataloader):\n        input_ids = data['input_ids']\n        token_type_ids = data['token_type_ids']\n        attention_mask = data['attention_mask']\n        target_start_idx = data['target_start_idx']\n        target_end_idx = data['target_end_idx']\n\n        input_ids = input_ids.to(device)\n        token_type_ids = token_type_ids.to(device)\n        attention_mask = attention_mask.to(device)\n        target_start_idx = target_start_idx.to(device)\n        target_end_idx = target_end_idx.to(device)\n        \n        optimizer.zero_grad()\n        \n        start_logits, end_logits = model(\n            input_ids=input_ids,\n            token_type_ids=token_type_ids,\n            attention_mask=attention_mask)\n            \n        loss = loss_fn(start_logits, end_logits, target_start_idx, target_end_idx)\n        train_loss.append(loss.item())\n        \n        \n        if i % 100 == 0:\n            #print(start_logits.size(), end_logits.size(), len(input_ids))\n            it_loss = loss.item() #* len(input_ids)\n            print(f\"iter: {i} | loss: {it_loss:.5f}\")\n        \n        loss.backward()\n        optimizer.step()\n        \n        if scheduler is not None:\n            scheduler.step()\n        \n    return train_loss\n      \n\ndef valid_fn(model, dataloader, optimizer=None, scheduler=None, device=None):\n    jaccard_score = 0.0\n    \n    with torch.no_grad():\n        \n        for i, data in enumerate(dataloader):\n            input_ids = data['input_ids']\n            token_type_ids = data['token_type_ids']\n            attention_mask = data['attention_mask']\n            target_start_idx = data['target_start_idx']\n            target_end_idx = data['target_end_idx']\n            offsets = data['offsets'].numpy()\n            text_original = data['orig_text']\n            \n            input_ids = input_ids.to(device)\n            token_type_ids = token_type_ids.to(device)\n            attention_mask = attention_mask.to(device)\n            target_start_idx = target_start_idx.to(device)\n            target_end_idx = target_end_idx.to(device)\n            \n            start_logits, end_logits = model(\n                input_ids=input_ids, \n                token_type_ids=token_type_ids, \n                attention_mask=attention_mask)\n            \n            start_logits = torch.softmax(start_logits, dim=1).cpu().detach().numpy()\n            end_logits = torch.softmax(end_logits, dim=1).cpu().detach().numpy()\n            \n            start_idx = target_start_idx.cpu().detach().numpy()\n            end_idx = target_end_idx.cpu().detach().numpy()\n            \n            # compute jaccard scores\n            for j in range(len(input_ids)):\n                jscore = compute_jaccard_score(\n                    text_original[j],\n                    target_start_idx[j],\n                    target_end_idx[j],\n                    start_logits[j],\n                    end_logits[j],\n                    offsets[j])\n                \n                jaccard_score += jscore\n    \n    return jaccard_score\n\n\ndef run(\n    fold, \n    df, \n    train_idx, \n    valid_idx, \n    num_epochs, \n    tokenizer, \n    max_length, \n    batch_size, \n    lr, \n    model, \n    device):\n    \n    # datasets and dataloaders\n    train_df = df.iloc[train_idx]\n    valid_df = df.iloc[valid_idx]\n    \n    train_dataset = TweetDataset(\n        train_df.text.values, \n        train_df.selected_text.values, \n        train_df.sentiment.values, \n        tokenizer=tokenizer, \n        max_length=max_length)\n    \n    valid_dataset = TweetDataset(\n        valid_df.text.values,\n        valid_df.selected_text.values,\n        valid_df.sentiment.values, \n        tokenizer=tokenizer,\n        max_length=max_length)\n    \n    train_loader = DataLoader(\n        train_dataset, \n        batch_size=batch_size, \n        shuffle=True)\n    \n    valid_loader = DataLoader(\n        valid_dataset, \n        batch_size=batch_size, \n        shuffle=False)\n    \n    #print(f\"Finished loading data!\")\n\n    optimizer = AdamW(model.parameters(), lr=lr)\n    \n    num_training_steps = len(train_loader) * num_epochs\n    warmup_steps = int(0.05 * num_training_steps)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, \n        num_training_steps=num_training_steps, \n        num_warmup_steps=0)\n    \n    # train and eval function\n    for epoch in range(num_epochs):\n        start = time.time()\n\n        train_loss = train_fn(\n            model, \n            train_loader, \n            optimizer=optimizer, \n            scheduler=scheduler, \n            device=device)\n        \n        end = time.time()\n        t = end - start\n\n        jaccard_score = valid_fn(model, valid_loader, device=device)\n\n        train_loss = np.mean(train_loss)\n        jscore = jaccard_score / len(valid_dataset)\n        print(f\"time: {(t/60):.2f} mins\")\n        print(f\"Epoch: {epoch+1}/{num_epochs} | train loss: {train_loss:.5f} | jaccard score: {jscore:.5f}\")\n        \n    torch.save(model.state_dict(), \n               f'fold{fold+1}_roberta_e{epoch+1}_bs{batch_size}.pth')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EPOCHS = 4\n\nROBERTA_PATH = \"/kaggle/input/tokenizers/model_dumps/roberta-base\"\n\nTOKENIZER = tokenizers.ByteLevelBPETokenizer(\n    vocab_file=f\"{ROBERTA_PATH}/vocab.json\",\n    merges_file=f\"{ROBERTA_PATH}/merges.txt\",\n    lowercase=True,\n    add_prefix_space=True)\n\nMAX_LENGTH = 192\nBATCH_SIZE = 32\nLR = 1e-5 * 3\n\nTRAIN_PATH = \"/kaggle/input/tweet-sentiment-extraction/train.csv\"\n\ndf = pd.read_csv(TRAIN_PATH, usecols=['text', 'selected_text', 'sentiment']).fillna('none')\ndf = df.sample(frac=1)\n#df = df[:1000]\nprint(df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nskf = StratifiedKFold(n_splits=6, shuffle=True, random_state=seed)\n\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(df.text, df.sentiment)):\n    # model\n    MODEL = RobertaConvHead()\n    MODEL = MODEL.to(DEVICE)\n    \n    #print(f\"Finished loading model!\")\n    \n    print(f\"Fold: {fold+1}\")\n    \n    run(\n        fold=fold,\n        df=df,\n        train_idx=train_idx,\n        valid_idx=valid_idx,\n        num_epochs=EPOCHS, \n        tokenizer=TOKENIZER, \n        max_length=MAX_LENGTH, \n        batch_size=BATCH_SIZE, \n        lr=LR,\n        model=MODEL,\n        device=DEVICE)\n    \n    print(\"\\n\")\n\nprint(f\"Finished running script.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}