{"cells":[{"metadata":{},"cell_type":"markdown","source":"To attain features from the input data we must first create a function to extract a column from the csv file:"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import time as t\n#27486\nsample_limit = 100\nstart = t.time()\n\n#Import pandas library for data handling.\nimport pandas as pd\n\ndef get_data_from_column(column_title,file):\n    #Variable to hold the location of input data.\n    data_dir = \"/kaggle/input/tweet-sentiment-extraction/\"\n    #Create the path to the file needed.\n    file_path = data_dir + file\n    \n    #Open the csv file using pandas.\n    data_file = pd.read_csv(file_path)\n    #Extract the selected column from the csv file.\n    data_column = data_file[column_title]\n    \n    #Return the column data.\n    return data_column\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can extract data from any of the csv files, we now need to create a function to produce the N-grams for each of the tweets:"},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nimport numpy as np\n\ndef generate_ngrams(tweet):\n    #Generates ngrams in a tuple list\n    tuple_grams = list(nltk.everygrams(str(tweet).split(' '), 1, len(str(tweet))))\n    #Converts tuple list to a vertical numpy array\n    ngrams = np.array([' '.join(i) for i in tuple_grams])[:, None]  \n    return np.concatenate(ngrams)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that the ngrams can be produced, we can now write the feature extraction functions:"},{"metadata":{},"cell_type":"markdown","source":"A function to encode sentiment infomation to numerical:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode_sentiment(sentiment):\n    if sentiment == \"positive\":\n        return 1\n    if sentiment == \"neutral\":\n        return 0\n    if sentiment == \"negative\":\n        return -1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A function to extract the sentiment scores from a string of characters:"},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\nsys.path.append(\"/kaggle/input/vader-sentiment/\")\n\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n\n#Create an instance of the SentimentIntensityAnalyzer.\nanalyser = SentimentIntensityAnalyzer()\n\ndef extract_sentiments_from_text(text):\n    sentiment_list = []\n    \n    #Calculate the sentiment scores for the ngram.\n    sentiment_scores = analyser.polarity_scores(text)\n    \n    #Append each of the sentiment scores to the feature list\n    for label in list(sentiment_scores):\n        sentiment_list.append(sentiment_scores[label])\n        \n    return sentiment_list","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A function to extract the POS infomation about an ngram:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_tag_list():\n    tagdict = nltk.load('help/tagsets/upenn_tagset.pickle')\n    taglist = tagdict.keys()\n    return list(taglist)\n\ntag_list = get_tag_list()\n\ndef get_POS_info(ngram):\n    tag_count = [0]* len(tag_list)\n    \n    tagged = nltk.pos_tag(nltk.word_tokenize(ngram))\n    \n    for tag in tagged:\n        if tag[1] in tag_list:\n            tag_count[tag_list.index(tag[1])] += 1\n     \n    return tag_count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_all_caps_count(text):\n    return sum(map(str.isupper, text.split()))\n\ndef find_punctuation_count(text):\n    mark_count = []\n    mark_lookup = ['!','?','#']\n    \n    for mark in mark_lookup:\n        mark_count.append(text.count(mark))\n        \n    return mark_count\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"BERT Implementation for feature extraction:"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport transformers as ppb # pytorch transformers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, '/kaggle/input/distilbertbaseuncased')\n\n# Load pretrained model/tokenizer\ntokenizer = tokenizer_class.from_pretrained(pretrained_weights)\nmodel = model_class.from_pretrained(pretrained_weights)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can write a function to collate and compile all the features for one ngram:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def default_features(text):\n    def_features = []\n    def_features.append(len(text))\n    def_features += extract_sentiments_from_text(text)\n    def_features.append(find_all_caps_count(text))\n    def_features += find_punctuation_count(text)\n    return def_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_features_from_ngram(tweet, tweet_sentiment, ngram):\n    #List to hold features.\n    ngram_features = []\n    \n    ngram_features += default_features(tweet)\n    ngram_features.append(encode_sentiment(tweet_sentiment))\n\n    ngram_features += default_features(ngram)\n    ngram_features.append(len(ngram)/len(tweet))    \n\n    return ngram_features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Feature order is as follows:\n\n* Tweet Length\n* Tweet Overall Sentiment\n* Tweet Negative Sentiment Score\n* Tweet Neutral Sentiment Score\n* Tweet Positive Sentiment Score\n* Tweet Compound Sentiment Score\n* Tweet Punctuation Count\n* Tweet All Caps Count \n\n* N-gram Length\n* N-gram to Tweet Length Ratio\n* N-gram Negative Sentiment Score\n* N-gram Neutral Sentiment Score\n* N-gram Positive Sentiment Score\n* N-gram Compound Sentiment Score\n* Tweet Punctuation Count\n* Tweet All Caps Count \n\nThis set of features is extracted for each of the N-grams from one tweet:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_features_from_tweet(tweet,tweet_sentiment):\n    #Array to store all tweet features.\n    tweet_features = []\n    \n    #Generate all N-grams for the tweet.\n    tweet_ngrams = generate_ngrams(str(tweet))\n    \n    #Loop through all the N-grams.\n    for ngram in tweet_ngrams:\n        #Extract the features from each N-gram.\n        ngram_features = extract_features_from_ngram(str(tweet),tweet_sentiment,str(ngram))\n        #Append the N-gram features to the tweet features.\n        tweet_features.append(ngram_features)\n            \n    return np.array(tweet_features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we can extract all the features we need from a tweet, we must now calculate the label for each N-gram; the jaccard similarity:"},{"metadata":{},"cell_type":"markdown","source":"Code for jaccard similarity calculation:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def jaccard(str1, str2):\n    a = set(str(str1).lower().split())\n    b = set(str(str2).lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can write a function to calculate the jaccard similarity for each of the N-grams in a tweet:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_ngram_jaccard_labels(tweet, tweet_selected_text):\n    #List to hold ngram jaccard scores.\n    labels = []\n    \n    #Generate all the ngrams of this tweet.\n    ngrams = generate_ngrams(tweet)\n    #Loop through the ngrams generated\n    for ngram in ngrams:\n        #Calculate the jaccard score for each ngram.\n        jaccard_score = jaccard(ngram,tweet_selected_text)\n        #Append it to the labels list.\n        labels.append(jaccard_score)\n    \n    return np.array(labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we can extract both the features and labels from a tweet we can write a function to collate all the features and labels from all of the tweets:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_all_features(tweets,sentiments): \n    #List to hold all the features of the tweets\n    features = []\n    \n    #Iterate the variable i to the length of the tweets.\n    for i in range(0,len(tweets)):\n        #Extract the features from a tweet given its string and sentiment.\n        tweet_features = extract_features_from_tweet(tweets[i],sentiments[i])\n        #Append features to total features.\n        features.append(tweet_features)\n\n    return np.array(features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_all_labels(tweets,selected_texts):\n    #List to hold labels for all the tweets' ngrams\n    labels = []\n    \n    #Iterate the variable i to the length of the tweets.\n    for i in range(0,len(tweets)):\n        #Get the label for all ngrams for a tweet given its string and target string.\n        tweet_labels = get_ngram_jaccard_labels(tweets[i],selected_texts[i])\n        #Append labels to total labels.\n        labels.append(tweet_labels)\n        \n    return np.asanyarray(labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then we can bring these two functions together when fitting the regression model:"},{"metadata":{},"cell_type":"markdown","source":"Load CSV data for training:"},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets = get_data_from_column(\"text\",\"train.csv\")\nsentiments = get_data_from_column(\"sentiment\",\"train.csv\")\nselected_texts = get_data_from_column(\"selected_text\",\"train.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Extract training features:"},{"metadata":{"trusted":true},"cell_type":"code","source":"total_features = np.concatenate(get_all_features(tweets, sentiments))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Get training labels:"},{"metadata":{"trusted":true},"cell_type":"code","source":"total_labels = np.concatenate(get_all_labels(tweets,selected_texts))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train LightGBM model:"},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\n\n#Compile a training dataset\ntrain_data = lgb.Dataset(total_features, label=total_labels)\n\n#Set the model's parameters\nnum_round = 10\nparam = {'num_leaves': 500, 'objective': 'huber','boosting': 'dart'}\n\n#Train the model\nbst = lgb.train(param, train_data, num_round)\nbst.save_model('model.txt')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that our model is fitting with the training data we can apply it to the test data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create a list of all ngrams for all tweets.\ndef get_test_ngrams(tweets):\n    #List to hold all ngrams.\n    test_ngrams = []\n    #Loop through all tweets.\n    for tweet in tweets:\n        #Generate the ngrams for that tweet.\n        ngram = generate_ngrams(tweet)\n        #Append to that to the total list.\n        test_ngrams.append([ngram])\n        \n    return np.array(test_ngrams)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Load testing data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_tweets = get_data_from_column(\"text\",\"test.csv\")\ntest_sentiments = get_data_from_column(\"sentiment\",\"test.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Extract features:"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_features = get_all_features(test_tweets,test_sentiments)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bst = lgb.Booster(model_file='model.txt')  # init model\n\n\n#Generate test ngrams.\ntest_ngrams = get_test_ngrams(test_tweets)\n#List to hold all resulting predicted texts.\nselected_texts = []\n\n#Iterate variable i to the length of the test tweets list\nfor i in range(0,len(test_tweets)):\n    \n    #Produce a list of jaccard score predictions for each ngram feature list for each tweet.\n    prediction = bst.predict(test_features[i], num_iteration=bst.best_iteration)\n    \n    #Get the index in that list of the maximum score.\n    best_prediction_index = list(prediction).index(max(list(prediction)))\n    \n    #Get the ngram with the maximum score.\n    selected_text = test_ngrams[i][0][best_prediction_index]\n    #Append it to the total list.\n    selected_texts.append(selected_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import csv\n#Converting the selected texts list to numpy array.\nselected_texts = np.asanyarray(selected_texts)\n\ntextIDs = get_data_from_column(\"textID\",\"sample_submission.csv\")\ntextIDs = textIDs[:len(selected_texts)]\n#Create a dataframe from the numpy array using IDs as the index.\nselected_texts = pd.DataFrame(selected_texts,index=textIDs,columns=[\"selected_text\"])\n#Output to CSV\nselected_texts.to_csv('submission.csv')\n\nprint(selected_texts)\nend = t.time()\nprint(str(end-start) + \" Seconds\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}