{"cells":[{"metadata":{},"cell_type":"markdown","source":"# ******* Grupo Laura-Borja-Nehomar *******","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"1. Preparación del Entorno","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#Básicos\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport re\nimport itertools\nimport datetime\n\nfrom textblob import TextBlob\n\n# NLTK\nimport nltk\nnltk.download('wordnet')\nfrom nltk.stem import WordNetLemmatizer \nfrom nltk.tokenize import word_tokenize\n\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nimport unidecode\nimport string\n\nfrom nltk.probability import FreqDist\n\nimport json\nuse_cuda = False","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2. Cargamos los datos","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv')\ntrain_df = train_df[train_df['text'].notna()]\ntrain_df = train_df.reset_index()\ntrain_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3. StopWords","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"stop_words = set(stopwords.words('english'))\n\nappos = {\n\"aren't\" : \"are not\",\n\"can't\" : \"cannot\",\n\"couldn't\" : \"could not\",\n\"didn't\" : \"did not\",\n\"doesn't\" : \"does not\",\n\"don't\" : \"do not\",\n\"hadn't\" : \"had not\",\n\"hasn't\" : \"has not\",\n\"haven't\" : \"have not\",\n\"he'd\" : \"he would\",\n\"he'll\" : \"he will\",\n\"he's\" : \"he is\",\n\"i'd\" : \"i would\",\n\"i'd\" : \"i had\",\n\"i'll\" : \"i will\",\n\"i'm\" : \"i am\",\n\"isn't\" : \"is not\",\n\"it's\" : \"it is\",\n\"it'll\":\"it will\",\n\"i've\" : \"i have\",\n\"let's\" : \"let us\",\n\"mightn't\" : \"might not\",\n\"mustn't\" : \"must not\",\n\"shan't\" : \"shall not\",\n\"she'd\" : \"she would\",\n\"she'll\" : \"she will\",\n\"she's\" : \"she is\",\n\"shouldn't\" : \"should not\",\n\"that's\" : \"that is\",\n\"there's\" : \"there is\",\n\"they'd\" : \"they would\",\n\"they'll\" : \"they will\",\n\"they're\" : \"they are\",\n\"they've\" : \"they have\",\n\"we'd\" : \"we would\",\n\"we're\" : \"we are\",\n\"weren't\" : \"were not\",\n\"we've\" : \"we have\",\n\"what'll\" : \"what will\",\n\"what're\" : \"what are\",\n\"what's\" : \"what is\",\n\"what've\" : \"what have\",\n\"where's\" : \"where is\",\n\"who'd\" : \"who would\",\n\"who'll\" : \"who will\",\n\"who're\" : \"who are\",\n\"who's\" : \"who is\",\n\"who've\" : \"who have\",\n\"won't\" : \"will not\",\n\"wouldn't\" : \"would not\",\n\"you'd\" : \"you would\",\n\"you'll\" : \"you will\",\n\"you're\" : \"you are\",\n\"you've\" : \"you have\",\n\"'re\": \" are\",\n\"wasn't\": \"was not\",\n\"we'll\":\" will\",\n\"didn't\": \"did not\"\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"4. Función principal de pre-procesado de datos","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def text_preprocess(text):\n    lemma = nltk.wordnet.WordNetLemmatizer()\n    \n    text = str(text)\n    \n    #removing mentions and hashtags\n\n    text = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|(#[A-Za-z0-9]+)\", \" \", text).split())\n    \n    #remove http links from tweets\n    \n    \n    link_regex    = re.compile('((https?):((//)|(\\\\\\\\))+([\\w\\d:#@%/;$()~_?\\+-=\\\\\\.&](#!)?)*)', re.DOTALL)\n    links         = re.findall(link_regex, text)\n    for link in links:\n        text = text.replace(link[0], '')  \n    \n    text_pattern = re.sub(\"`\", \"'\", text)\n    \n    #fix misspelled words\n\n    '''Here we are not actually building any complex function to correct the misspelled words but just checking that each character \n    should occur not more than 2 times in every word. It’s a very basic misspelling check.'''\n\n    text = ''.join(''.join(s)[:2] for _, s in itertools.groupby(text))\n    \n    \n   # print(text_pattern)\n    \n    #Convert to lower and negation handling\n    \n    text_lr = text_pattern.lower()\n    \n   # print(text_lr)\n    \n    words = text_lr.split()\n    text_neg = [appos[word] if word in appos else word for word in words]\n    text_neg = \" \".join(text_neg) \n   # print(text_neg)\n    \n    #remove stopwords\n    \n    tokens = word_tokenize(text_neg)\n    text_nsw = [i for i in tokens if i not in stop_words]\n    text_nsw = \" \".join(text_nsw) \n   # print(text_nsw)\n    \n    \n    #remove tags\n    \n    text_tags=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text_nsw)\n\n    # remove special characters and digits\n    text_alpha=re.sub(\"(\\\\d|\\\\W)+\",\" \",text_tags)\n    \n    #Remove accented characters\n    text = unidecode.unidecode(text_alpha)\n    \n    '''#Remove punctuation\n    table = str.maketrans('', '', string.punctuation)\n    text = [w.translate(table) for w in text.split()]'''\n    \n    sent = TextBlob(text)\n    tag_dict = {\"J\": 'a', \n                \"N\": 'n', \n                \"V\": 'v', \n                \"R\": 'r'}\n    words_and_tags = [(w, tag_dict.get(pos[0], 'n')) for w, pos in sent.tags]    \n    lemmatized_list = [wd.lemmatize(tag) for wd, tag in words_and_tags]\n   \n    return \" \".join(lemmatized_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['processed_text'] = None\n\nfor i in range(len(train_df)):\n    train_df.processed_text[i] = text_preprocess(train_df.text[i])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"5. Vemos si el DataSet esta debalanceado","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nax = train_df['sentiment'].value_counts(sort=False).plot(kind='barh')\nax.set_xlabel('Número de muestras')\nax.set_ylabel('Etiqueta')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"6. Wordcloud","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\n# Polarity ==  negative\ntrain_s0 = train_df[train_df.sentiment == 'negative']\nall_text = ' '.join(word for word in train_s0.processed_text)\nwordcloud_neg = WordCloud(colormap='Reds', width=1000, height=1000, background_color='white').generate(all_text) #mode='RGBA'\nplt.figure(figsize=(20,10))\nplt.title('Negative sentiment - Wordcloud')\nplt.imshow(wordcloud_neg, interpolation='bilinear')\nplt.axis(\"off\")\nplt.margins(x=0, y=0)\nplt.show()\n\nwordcloud_neg.to_file('negative_senti_wordcloud.jpg')\n\n# Polarity ==  neutral\ntrain_s1 = train_df[train_df.sentiment == 'neutral']\nall_text = ' '.join(word for word in train_s1.processed_text)\nwordcloud_neu = WordCloud(width=1000, height=1000, colormap='Blues', background_color='white').generate(all_text)\nplt.figure( figsize=(20,10))\nplt.title('Neutral sentiment - Wordcloud')\nplt.imshow(wordcloud_neu, interpolation='bilinear')\nplt.axis(\"off\")\nplt.margins(x=0, y=0)\nplt.show()\n\nwordcloud_neu.to_file('neutral_senti_wordcloud.jpg')\n\n# Polarity ==  positive\ntrain_s2 = train_df[train_df.sentiment  == 'positive']\nall_text = ' '.join(word for word in train_s2.processed_text)\nwordcloud_pos = WordCloud(width=1000, height=1000, colormap='Wistia',background_color='white').generate(all_text)\nplt.figure(figsize=(20,10))\nplt.title('Positive sentiment - Wordcloud')\nplt.imshow(wordcloud_pos, interpolation='bilinear')\nplt.axis(\"off\")\nplt.margins(x=0, y=0)\nplt.show()\n\nwordcloud_pos.to_file('positive_senti_wordcloud.jpg')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cargamos el resto de los datos","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv')\nsub_df = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = np.array(train_df)\ntest = np.array(test_df)\n\n!mkdir -p data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Prepare data in QA format\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = [\n    {\n        'context': \"This tweet sentiment extraction challenge is great\",\n        'qas': [\n            {\n                'id': \"00001\",\n                'question': \"positive\",\n                'answers': [\n                    {\n                        'text': \"is great\",\n                        'answer_start': 43\n                    }\n                ]\n            }\n        ]\n    }\n    ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n\"\"\"\nPrepare training data in QA-compatible format\n\"\"\"\n\n# Adpated from https://www.kaggle.com/cheongwoongkang/roberta-baseline-starter-simple-postprocessing\ndef find_all(input_str, search_str):\n    l1 = []\n    length = len(input_str)\n    index = 0\n    while index < length:\n        i = input_str.find(search_str, index)\n        if i == -1:\n            return l1\n        l1.append(i)\n        index = i + 1\n    return l1\n\ndef do_qa_train(train):\n\n    output = []\n    for line in train:\n        context = line[1]\n        qas = []\n        question = line[-1]\n        qid = line[0]\n        answers = []\n        answer = line[2]\n        if type(answer) != str or type(context) != str or type(question) != str:\n            print(context, type(context))\n            print(answer, type(answer))\n            print(question, type(question))\n            continue\n        answer_starts = find_all(context, answer)\n        for answer_start in answer_starts:\n            answers.append({'answer_start': answer_start, 'text': answer.lower()})\n            break\n        qas.append({'question': question, 'id': qid, 'is_impossible': False, 'answers': answers})\n\n        output.append({'context': context.lower(), 'qas': qas})\n        \n    return output\n\nqa_train = do_qa_train(train)\n\nwith open('data/train.json', 'w') as outfile:\n    json.dump(qa_train, outfile)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n\"\"\"\nPrepare testing data in QA-compatible format\n\"\"\"\n\ndef do_qa_test(test):\n    output = []\n    for line in test:\n        context = line[1]\n        qas = []\n        question = line[-1]\n        qid = line[0]\n        if type(context) != str or type(question) != str:\n            print(context, type(context))\n            print(answer, type(answer))\n            print(question, type(question))\n            continue\n        answers = []\n        answers.append({'answer_start': 1000000, 'text': '__None__'})\n        qas.append({'question': question, 'id': qid, 'is_impossible': False, 'answers': answers})\n        output.append({'context': context.lower(), 'qas': qas})\n    return output\n\nqa_test = do_qa_test(test)\n\nwith open('data/test.json', 'w') as outfile:\n    json.dump(qa_test, outfile)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install '/kaggle/input/simple-transformers-pypi/seqeval-0.0.12-py3-none-any.whl' -q\n!pip install '/kaggle/input/simple-transformers-pypi/simpletransformers-0.22.1-py3-none-any.whl' -q","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n\nfrom simpletransformers.question_answering import QuestionAnsweringModel\n\nMODEL_PATH = '/kaggle/input/transformers-pretrained-distilbert/distilbert-base-uncased-distilled-squad/'\n\n# Create the QuestionAnsweringModel\nmodel = QuestionAnsweringModel('distilbert', \n                               MODEL_PATH, \n                               args={'reprocess_input_data': True,\n                                     'overwrite_output_dir': True,\n                                     'learning_rate': 5e-5,\n                                     'num_train_epochs': 3,\n                                     'max_seq_length': 192,\n                                     'doc_stride': 64,\n                                     'fp16': False,\n                                    },\n                              use_cuda=use_cuda)\n\nmodel.train_model('data/train.json')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Submission","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\npredictions = model.predict(qa_test)\npredictions_df = pd.DataFrame.from_dict(predictions)\n\nsub_df['selected_text'] = predictions_df['answer']\n\nsub_df.to_csv('submission.csv', index=False)\n\nprint(\"File submitted successfully.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}