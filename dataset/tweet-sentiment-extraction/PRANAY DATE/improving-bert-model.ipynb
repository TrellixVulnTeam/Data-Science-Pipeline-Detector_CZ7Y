{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport os\nimport re\nimport json\nimport string\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tokenizers import BertWordPieceTokenizer\nfrom transformers import BertTokenizer, TFBertModel, BertConfig, TFRobertaModel, RobertaTokenizerFast, RobertaTokenizer, BertTokenizerFast \nprint(tf.__version__) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/tweet-sentiment-extraction/train.csv\")\ntest = pd.read_csv(\"../input/tweet-sentiment-extraction/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data[data.sentiment != \"neutral\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_len = 384\nconfiguration = BertConfig()  # default paramters and configuration for BERT\n\n# model_to_use = 'bert-base-uncased'\n\n# # # Save the slow pretrained tokenizer\n# #slow_tokenizer = BertTokenizer.from_pretrained(\"../input/bert-base-uncased/\")\n# tokenizer = BertTokenizer.from_pretrained(model_to_use)\n# save_path = \"bert_base_uncased/\"\n# if not os.path.exists(save_path):\n#     os.makedirs(save_path)\n# tokenizer.save_pretrained(save_path)\n\n# # Load the fast tokenizer from saved file\n# #tokenizer = BertWordPieceTokenizer(\"../input/bert-base-uncased/vocab.txt\", lowercase=True)\n# tokenizer = BertWordPieceTokenizer(\"bert_base_uncased/vocab.txt\", lowercase=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rtokenizer = BertTokenizerFast.from_pretrained(\"../input/spanbert-pt/spanbert-base-cased/\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# s = tokenizer.encode(data.text.values[1])\n# print(data.text.values[1])\n# print(s)\n# print(s.ids)\n# print(s.tokens)\n# print(s.offsets)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#rtokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s = rtokenizer.encode_plus(data.text.values[1], return_offsets_mapping=True)\nprint(data.text.values[1])\nprint(s)\nprint(s.offset_mapping)\nprint(s.input_ids)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TweetExample:\n    def __init__(self, sentiment, text, start_char_idx, selected_text, all_answers):\n        self.sentiment = sentiment\n        self.text = text\n        self.start_char_idx = start_char_idx\n        self.selected_text = selected_text\n        self.all_answers = all_answers\n        self.skip = False\n\n    def preprocess(self):\n        text = self.text\n        sentiment = self.sentiment\n        selected_text = self.selected_text\n        start_char_idx = self.start_char_idx\n\n        # Clean context, answer and question\n        sentiment = \" \".join(str(sentiment).split())\n        text = \" \".join(str(text).split())\n        answer = \" \".join(str(selected_text).split())\n\n        # Find end character index of answer in context\n        end_char_idx = start_char_idx + len(answer)\n        if end_char_idx >= len(text):\n            self.skip = True\n            return\n\n        # Mark the character indexes in context that are in answer\n        is_char_in_ans = [0] * len(text)\n        for idx in range(start_char_idx, end_char_idx):\n            is_char_in_ans[idx] = 1\n\n        # Tokenize context\n        tokenized_context = rtokenizer.encode_plus(text, return_offsets_mapping=True, max_length = max_len)\n\n        # Find tokens that were created from answer characters\n        ans_token_idx = []\n        for idx, (start, end) in enumerate(tokenized_context.offset_mapping):\n            if sum(is_char_in_ans[start:end]) > 0:\n                ans_token_idx.append(idx)\n\n        if len(ans_token_idx) == 0:\n            self.skip = True\n            return\n\n        # Find start and end token index for tokens from answer\n        start_token_idx = ans_token_idx[0]\n        end_token_idx = ans_token_idx[-1]\n\n        # Tokenize question\n        #tokenized_question = tokenizer.encode(question)\n        tokenized_question = rtokenizer.encode_plus(sentiment, return_offsets_mapping=True, max_length = max_len)\n\n        # Create inputs\n        input_ids = tokenized_context.input_ids + tokenized_question.input_ids[1:]\n        token_type_ids = [0] * len(tokenized_context.input_ids) + [1] * len(\n            tokenized_question.input_ids[1:]\n        )\n        attention_mask = [1] * len(input_ids)\n\n        # Pad and create attention masks.\n        # Skip if truncation is needed\n        padding_length = max_len - len(input_ids)\n        if padding_length > 0:  # pad\n            input_ids = input_ids + ([0] * padding_length)\n            attention_mask = attention_mask + ([0] * padding_length)\n            token_type_ids = token_type_ids + ([0] * padding_length)\n        elif padding_length < 0:  # skip\n            self.skip = True\n            return\n\n        self.input_ids = input_ids\n        self.token_type_ids = token_type_ids\n        self.attention_mask = attention_mask\n        self.start_token_idx = start_token_idx\n        self.end_token_idx = end_token_idx\n        self.context_token_to_char = tokenized_context.offset_mapping\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for idx, value in enumerate(data.text): ## remove hyperlinks\n#     words = str(value).split()\n#     words = [x for x in words if not x.startswith(\"http\")]\n#     data[\"text\"][idx] = \" \".join(words)\n#     #print(data[\"text\"][idx])\n    \n# for idx, value in enumerate(test.text):\n#     words = str(value).split()\n#     words = [x for x in words if not x.startswith(\"http\")]\n#     test[\"text\"][idx] = \" \".join(words)\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data[pd.notnull(data.selected_text)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(data.text), len(data.textID))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rtokenizer.encode(test.sentiment[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_tweet_examples(data):\n    squad_examples = []\n    all_answers = data.selected_text.values\n    count = 0\n    for index, row in data.iterrows():\n        sentiment = row['sentiment']\n        text = row['text']\n        selected_text = row['selected_text']\n        #print(sentiment, text, selected_text)\n        try:\n            start_char_idx = row.text.index(row.selected_text.split()[0])\n            squad_eg = TweetExample(\n                sentiment, text, start_char_idx, selected_text, all_answers\n            )\n            squad_eg.preprocess()\n            squad_examples.append(squad_eg)\n        except:\n            count += 1\n    print(\"Couldn't find \",count,\"tokens\")\n    return squad_examples","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class InputTweetExample:\n    def __init__(self, sentiment, text):\n        self.sentiment = sentiment\n        self.text = text\n        self.skip = False\n\n    def preprocess(self):\n        text = self.text\n        sentiment = self.sentiment\n\n        # Clean context, answer and question\n        sentiment = \" \".join(str(sentiment).split())\n        text = \" \".join(str(text).split())\n\n        # Tokenize context\n        tokenized_context = rtokenizer.encode_plus(text, return_offsets_mapping=True, max_length = max_len)\n\n        # Tokenize question\n        #tokenized_question = tokenizer.encode(question)\n        tokenized_question = rtokenizer.encode_plus(sentiment, return_offsets_mapping=True, max_length = max_len)\n\n        # Create inputs\n        input_ids = tokenized_context.input_ids + tokenized_question.input_ids[1:]\n        token_type_ids = [0] * len(tokenized_context.input_ids) + [1] * len(\n            tokenized_question.input_ids[1:]\n        )\n        attention_mask = [1] * len(input_ids)\n\n        # Pad and create attention masks.\n        # Skip if truncation is needed\n        padding_length = max_len - len(input_ids)\n        if padding_length > 0:  # pad\n            input_ids = input_ids + ([0] * padding_length)\n            attention_mask = attention_mask + ([0] * padding_length)\n            token_type_ids = token_type_ids + ([0] * padding_length)\n        elif padding_length < 0:  # skip\n            self.skip = True\n            return\n\n        self.input_ids = input_ids\n        self.token_type_ids = token_type_ids\n        self.attention_mask = attention_mask\n        self.context_token_to_char = tokenized_context.offset_mapping\n        self.start_token_idx = 0\n        self.end_token_idx = 0\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_tweet_examples_test(data):\n    squad_examples = []\n    all_answers = None\n    count = 0\n    for index, row in data.iterrows():\n        sentiment = row['sentiment']\n        text = row['text']\n        selected_text = None\n        #print(sentiment, text, selected_text)\n        try:\n            start_char_idx = 0\n            squad_eg = InputTweetExample(\n                sentiment, text\n            )\n            squad_eg.preprocess()\n            squad_examples.append(squad_eg)\n        except:\n            count += 1\n    print(\"Couldn't find \",count,\"tokens\")\n    return squad_examples","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_inputs_targets(squad_examples):\n    dataset_dict = {\n        \"input_ids\": [],\n        \"token_type_ids\": [],\n        \"attention_mask\": [],\n        \"start_token_idx\": [],\n        \"end_token_idx\": [],\n    }\n    for item in squad_examples:\n        if item.skip == False:\n            for key in dataset_dict:\n                dataset_dict[key].append(getattr(item, key))\n    for key in dataset_dict:\n        dataset_dict[key] = np.array(dataset_dict[key])\n\n    x = [\n        dataset_dict[\"input_ids\"],\n        dataset_dict[\"token_type_ids\"],\n        dataset_dict[\"attention_mask\"],\n    ]\n    y = [dataset_dict[\"start_token_idx\"], dataset_dict[\"end_token_idx\"]]\n    return x, y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain, validation = train_test_split(data, test_size = 0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_squad_examples = create_tweet_examples(train)\nx_train, y_train = create_inputs_targets(train_squad_examples)\nprint(f\"{len(train_squad_examples)} training points created.\")\n\neval_squad_examples = create_tweet_examples(validation)\nx_eval, y_eval = create_inputs_targets(eval_squad_examples)\nprint(f\"{len(eval_squad_examples)} evaluation points created.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_squad_examples = create_tweet_examples_test(test)\nx_test, y_test = create_inputs_targets(test_squad_examples)\nprint(f\"{len(test_squad_examples)} test points created.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\"y_test\" is nothing, but now we've changed the test data to the format that the model takes in.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model():\n    ## BERT encoder\n    #encoder = TFBertModel.from_pretrained(\"../input/bert-base-uncased/\", from_pt = True)\n#     encoder = TFBertModel.from_pretrained(model_to_use)\n\n    PATH = '../input/spanbert-pt/spanbert-base-cased/'\n    #encoder = TFRobertaModel.from_pretrained('../input/roberta-base', from_pt = True)\n\n    #config = RobertaConfig.from_pretrained(PATH+'config.json')\n    encoder = TFBertModel.from_pretrained(PATH, from_pt = True)\n    \n## QA Model\n    input_ids = layers.Input(shape=(max_len ,), dtype=tf.int32)\n    token_type_ids = layers.Input(shape=(max_len,), dtype=tf.int32)\n    attention_mask = layers.Input(shape=(max_len,), dtype=tf.int32)\n    embedding = encoder(\n         input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask\n    )[0]\n    \n\n    start_logits = layers.Dropout(0.3)(embedding)\n    start_logits = layers.Conv1D(128,2,padding='same')(start_logits)\n    start_logits = layers.LeakyReLU()(start_logits)\n    start_logits = layers.Conv1D(64,2,padding='same')(start_logits)\n    start_logits = layers.Dense(1, name=\"start_logit\")(start_logits)\n    start_logits = layers.Flatten()(start_logits)\n\n    end_logits = layers.Dropout(0.3)(embedding)\n    end_logits = layers.Conv1D(128,2,padding='same')(end_logits)\n    end_logits = layers.LeakyReLU()(end_logits)\n    end_logits = layers.Conv1D(64,2,padding='same')(end_logits)\n    end_logits = layers.Dense(1, name=\"end_logit\")(end_logits)\n    end_logits = layers.Flatten()(end_logits)\n\n    start_probs = layers.Activation(keras.activations.softmax)(start_logits)\n    end_probs = layers.Activation(keras.activations.softmax)(end_logits)\n\n    model = keras.Model(\n        inputs=[input_ids, token_type_ids, attention_mask],\n        outputs=[start_probs, end_probs],\n    )\n    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n    optimizer = keras.optimizers.Adam(lr=5e-5)\n    model.compile(optimizer=optimizer, loss=[loss, loss])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"use_tpu = False\nif use_tpu:\n    # Create distribution strategy\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\n    # Create model\n    with strategy.scope():\n        model = create_model()\nelse:\n#     strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n#     with strategy.scope():\n    model = create_model()\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def normalize_text(text):\n    text = text.lower()\n\n    # Remove punctuations\n    exclude = set(string.punctuation)\n    text = \"\".join(ch for ch in text if ch not in exclude)\n\n    # Remove articles\n    regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n    text = re.sub(regex, \" \", text)\n\n    # Remove extra white space\n    text = \" \".join(text.split())\n    return text\n\n\nclass ExactMatch(keras.callbacks.Callback):\n    \"\"\"\n    Each `SquadExample` object contains the character level offsets for each token\n    in its input paragraph. We use them to get back the span of text corresponding\n    to the tokens between our predicted start and end tokens.\n    All the ground-truth answers are also present in each `SquadExample` object.\n    We calculate the percentage of data points where the span of text obtained\n    from model predictions matches one of the ground-truth answers.\n    \"\"\"\n\n    def __init__(self, x_eval, y_eval):\n        self.x_eval = x_eval\n        self.y_eval = y_eval\n\n    def on_epoch_end(self, epoch, logs=None):\n        pred_start, pred_end = self.model.predict(self.x_eval)\n        count = 0\n        eval_examples_no_skip = [_ for _ in eval_squad_examples if _.skip == False]\n        for idx, (start, end) in enumerate(zip(pred_start, pred_end)):\n            squad_eg = eval_examples_no_skip[idx]\n            offsets = squad_eg.context_token_to_char\n            start = np.argmax(start)\n            end = np.argmax(end)\n            if start >= len(offsets):\n                continue\n            pred_char_start = offsets[start][0]\n            if end < len(offsets):\n                pred_char_end = offsets[end][1]\n                pred_ans = squad_eg.text[pred_char_start:pred_char_end]\n            else:\n                pred_ans = squad_eg.text[pred_char_start:]\n\n            normalized_pred_ans = normalize_text(pred_ans)\n            normalized_true_ans = [normalize_text(_) for _ in squad_eg.all_answers]\n            if normalized_pred_ans in normalized_true_ans:\n                count += 1\n        acc = count / len(self.y_eval[0])\n        print(f\"\\nepoch={epoch+1}, exact match score={acc:.2f}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 8\ndiv = len(x_train[0]) - (len(x_train[0]) % batch_size)\nx = list(np.array(x_train)[:,:div]) ## inputs must be divisible by batch size \ny = list(np.array(y_train)[:,:div])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"exact_match_callback = ExactMatch(x_eval, y_eval)\nmodel.fit(\n    list(np.array(x)),\n    list(np.array(y)),\n    epochs=1,  # For demonstration, 3 epochs are recommended\n    verbose=2,\n    batch_size=8,\n    callbacks=[exact_match_callback],\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.array(x_test).shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = model.predict(x_test)\nnp.array(pred).shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['prediction'] = np.zeros(len(test.text.values))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_start, pred_end = pred\ncount = 0\n#test_examples_no_skip = [_ for _ in test_squad_examples if _.skip == False]\npred_text = []\nfor idx, (start, end) in enumerate(zip(pred_start, pred_end)):\n    squad_eg = test_squad_examples[idx]\n    if (squad_eg.skip == True):\n        print(idx)\n        pred_text.append(\"\")\n    else: \n        offsets = squad_eg.context_token_to_char\n        start = np.argmax(start)\n        end = np.argmax(end)\n        if start >= len(offsets):\n            continue\n        pred_char_start = offsets[start][0]\n        if end < len(offsets):\n            pred_char_end = offsets[end][1]\n            pred_ans = squad_eg.text[pred_char_start:pred_char_end + 1]\n        else:\n            pred_ans = squad_eg.text[pred_char_start:]\n\n        normalized_pred_ans = normalize_text(pred_ans)\n        #print(normalized_pred_ans)\n        pred_text.append(normalized_pred_ans)\n        test['prediction'][idx] = normalized_pred_ans","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test[-50:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.prediction[test.prediction == 0] = test.text[test.prediction == 0]\ntest.prediction[test.sentiment == \"neutral\"] = test.text[test.sentiment==\"neutral\"]\ntest.prediction[test.prediction ==''] = test.text[test.prediction =='']\n\nimport string\ndef clean_text(text):\n    words = str(text).split()\n    words = [x for x in words if not x.startswith(\"http\")]\n    words = \" \".join(words)\n    return words\n\ntest['prediction'] = test['prediction'].apply(clean_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test[:50]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluation = test.textID.copy().to_frame()\nevaluation['selected_text'] = test['prediction']\nevaluation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluation.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}