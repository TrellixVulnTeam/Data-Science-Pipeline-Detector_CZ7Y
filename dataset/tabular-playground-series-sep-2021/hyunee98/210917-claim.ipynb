{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-17T01:12:49.772036Z","iopub.execute_input":"2021-09-17T01:12:49.772686Z","iopub.status.idle":"2021-09-17T01:12:49.795074Z","shell.execute_reply.started":"2021-09-17T01:12:49.772618Z","shell.execute_reply":"2021-09-17T01:12:49.794361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\n\n\n#생략 없이 출력\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\npd.set_option('display.max_seq_items', None)\n\n# seed 값 설정\nseed = 3\nnp.random.seed(seed)\ntf.random.set_seed(seed)\n\n# 데이터 입력\ntrain = pd.read_csv('/kaggle/input/tabular-playground-series-sep-2021/train.csv',sep=',')\ntest = pd.read_csv('/kaggle/input/tabular-playground-series-sep-2021/test.csv',sep=',')\nlabel = pd.read_csv('/kaggle/input/tabular-playground-series-sep-2021/sample_solution.csv',sep=',')\n\n#데이터 정보 확인\n#print(train.head(5))\n#print(train.info())\n#print(train.describe())\n\n#공백 바로 위의 데이터를 입력\ntrain=train.fillna(method = 'ffill')\ntest=test.fillna(method = 'ffill')\n\n#수정된 데이터 확인\n#print(train.describe())\n   \n#데이터 삭제\ntrain=train.drop(['id'],axis=1)\ntest=test.drop(['id'],axis=1)\n\n#수정된 데이터 확인\n#print(train.info())\n\n#문자열을 숫자로 변환 후 원핫인코딩\ntrian_dataset = train.values\ntest_dataset = test.values\n\n#훈련_데이터분류\nX_train = trian_dataset[:,0:118]\nY_train = trian_dataset[:,118]\n#샘플_데이터\nX_sample = test_dataset[:,0:118]\n\nX_train, X_test, Y_train, Y_test = train_test_split(X_train, Y_train, test_size=0.3, random_state=seed)\n\n#모델생성\nmodel = Sequential()\nmodel.add(Dense(108, input_dim=118, activation='relu'))\nmodel.add(Dense(87, activation='relu'))\nmodel.add(Dense(69, activation='relu'))\nmodel.add(Dense(42, activation='relu'))\nmodel.add(Dense(24, activation='relu'))\nmodel.add(Dense(11, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy',\n          optimizer='adam',\n          metrics=['accuracy'])\n\n\n# 모델 저장 폴더 설정\nMODEL_DIR = '../model/'\nif not os.path.exists(MODEL_DIR):\n    os.mkdir(MODEL_DIR)\n\n# 모델 저장 조건 설정\nmodelpath=\"../model/claim-{epoch:02d}-accuracy:{accuracy:.4f}.hdf5\"\n\n# 모델 업데이트 및 저장\ncheckpointer = ModelCheckpoint(filepath=modelpath, monitor='accuracy', verbose=1, save_best_only=True)\n\n# 학습 자동 중단 설정\nearly_stopping_callback = EarlyStopping(monitor='accuracy', patience=100)\n\n# 모델 실행\nhistory=model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=1000, batch_size=200, verbose=0, \n                   callbacks=[early_stopping_callback,checkpointer])\n\n\"\"\"\n# 테스트셋 실험 결과의 값을 저장\ny_vloss=history.history['val_loss']\ny_vacc=history.history['val_accuracy']\n\n# 학습 셋 측정한 값을 저장\ny_acc=history.history['accuracy']\ny_loss=history.history['loss']\n\n# x값을 지정하고 나머지 표시\nx_len = np.arange(len(y_loss))\n\n#loss그래프\nplt.plot(x_len, y_vloss, \"o\", c=\"red\",  markersize=5, label='Testset')\nplt.plot(x_len, y_loss, \"o\", c=\"blue\", markersize=5, label='Trainset')\nplt.grid()\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.show()\n\n#acc그래프\nplt.plot(x_len, y_vacc, \"o\", c=\"pink\", markersize=5, label='Testset')\nplt.plot(x_len, y_acc, \"o\", c=\"skyblue\", markersize=5, label='Trainset')\nplt.grid()\nplt.xlabel('epoch')\nplt.ylabel('acc')\nplt.show()\n\"\"\"\n\n# 예측 값과 실제 값의 csv저장\nY_prediction = model.predict(X_sample).flatten()\nY_prediction=np.round(Y_prediction,1)\n#왜.. index 0이 nan인거지...\nY_prediction[0]=0\n      \ndf=pd.DataFrame({\n\"id\":label[\"id\"],\n\"claim\":Y_prediction\n})\nprint(df.head(10))\ndf.to_csv('./submission.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]}]}