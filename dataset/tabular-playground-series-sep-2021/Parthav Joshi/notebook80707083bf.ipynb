{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-30T12:30:05.905395Z","iopub.execute_input":"2021-09-30T12:30:05.906258Z","iopub.status.idle":"2021-09-30T12:30:06.003048Z","shell.execute_reply.started":"2021-09-30T12:30:05.906172Z","shell.execute_reply":"2021-09-30T12:30:06.002329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n\nfrom lightgbm import LGBMRegressor\nimport lightgbm as lgb","metadata":{"execution":{"iopub.status.busy":"2021-09-30T12:31:00.456343Z","iopub.execute_input":"2021-09-30T12:31:00.456746Z","iopub.status.idle":"2021-09-30T12:31:03.282233Z","shell.execute_reply.started":"2021-09-30T12:31:00.456711Z","shell.execute_reply":"2021-09-30T12:31:03.281557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/tabular-playground-series-sep-2021/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/tabular-playground-series-sep-2021/test.csv\")\nsubmission = pd.read_csv(\"/kaggle/input/tabular-playground-series-sep-2021/sample_solution.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-09-30T12:30:12.231088Z","iopub.execute_input":"2021-09-30T12:30:12.231903Z","iopub.status.idle":"2021-09-30T12:30:53.913064Z","shell.execute_reply.started":"2021-09-30T12:30:12.231858Z","shell.execute_reply":"2021-09-30T12:30:53.912227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def check(df):\n    col_list = train.columns.values\n    rows = []\n    for col in col_list:\n        tmp = (col,\n              train[col].dtype,\n              train[col].isnull().sum(),\n              train[col].count(),\n              train[col].nunique(),\n              train[col].unique())\n        rows.append(tmp)\n    df = pd.DataFrame(rows) \n    df.columns = ['feature','dtype','nan','count','nunique','unique']\n    return df\n\ncheck(train)","metadata":{"execution":{"iopub.status.busy":"2021-09-30T12:30:53.91449Z","iopub.execute_input":"2021-09-30T12:30:53.914761Z","iopub.status.idle":"2021-09-30T12:31:00.254637Z","shell.execute_reply.started":"2021-09-30T12:30:53.914728Z","shell.execute_reply":"2021-09-30T12:31:00.253023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_cell = np.product(train.shape)\nmissing_values_count = train.isnull().sum()\ntotal_missing = missing_values_count.sum()\npercent_missing = (total_missing / total_cell)* 100\n\nprint(percent_missing, \" % missing\")","metadata":{"execution":{"iopub.status.busy":"2021-09-30T12:31:00.255848Z","iopub.execute_input":"2021-09-30T12:31:00.256212Z","iopub.status.idle":"2021-09-30T12:31:00.454607Z","shell.execute_reply.started":"2021-09-30T12:31:00.256174Z","shell.execute_reply":"2021-09-30T12:31:00.453865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import KFold\n\ntrain[\"kfold\"] = -1\nkf = KFold(n_splits = 10, shuffle=True, random_state = 0)\nfor fold,(train_index, valid_index) in enumerate(kf.split(X = train)):\n    print(fold,train_index, valid_index)\n    train.loc[valid_index, \"kfold\"] = fold\ntrain.kfold.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-09-30T12:31:03.283604Z","iopub.execute_input":"2021-09-30T12:31:03.283855Z","iopub.status.idle":"2021-09-30T12:31:03.471892Z","shell.execute_reply.started":"2021-09-30T12:31:03.283822Z","shell.execute_reply":"2021-09-30T12:31:03.471123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"useful = [col for col in train.columns if col not in ('id','claim','kfold')]\ntest = test[useful]","metadata":{"execution":{"iopub.status.busy":"2021-09-30T12:31:04.012201Z","iopub.execute_input":"2021-09-30T12:31:04.013061Z","iopub.status.idle":"2021-09-30T12:31:04.15269Z","shell.execute_reply.started":"2021-09-30T12:31:04.013024Z","shell.execute_reply":"2021-09-30T12:31:04.15185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param = {      \n        \"objective\": \"binary\",\n        \"metric\": \"auc\",\n        \"verbosity\": -1,\n        \"boosting_type\": \"gbdt\",\n        \"device\": \"gpu\",\n        \"gpu_platform_id\": 0,\n        \"gpu_device_id\": 0,\n        \"n_estimators\" : 1000,\n        \"early_stopping_rounds\" : 10,\n    \n        \"feature_fraction\" : 1.0,\n        \"num_leaves\" : 94,\n        \"bagging_fraction\": 0.6737009142690187,\n        \"bagging_freq\": 1,\n        \"lambda_l1\": 5.559056252126386, \n        \"lambda_l2\": 9.77312118560801,\n        \"min_child_samples\" : 50\n     }\n\nscore = []\ntest_pred = []\nvalid_pred = {}\nfor fold in range(10):\n    \n    X_train = train[train.kfold != fold].reset_index(drop = True)\n    X_valid = train[train.kfold == fold].reset_index(drop = True)\n    \n    X_test = test.copy()\n    valid_ids = X_valid.id.values.tolist()\n    \n    y_train = X_train.claim\n    y_valid = X_valid.claim\n    \n    X_train = X_train[useful]\n    X_valid = X_valid[useful]\n    \n    feature = list(X_train.columns[1:])\n    \n    X_train['n_missing'] = X_train[feature].isna().sum(axis = 1)\n    X_train['std'] = X_train[feature].std(axis = 1)\n    X_train['mean'] = X_train[feature].mean(axis = 1)\n    X_train['median'] = X_train[feature].mean(axis = 1)\n    X_train['kurt'] = X_train[feature].kurtosis(axis = 1)\n\n    X_valid['n_missing'] = X_valid[feature].isna().sum(axis = 1)\n    X_valid['std'] = X_valid[feature].std(axis = 1)\n    X_valid['mean'] = X_valid[feature].mean(axis = 1)\n    X_valid['median'] = X_valid[feature].mean(axis = 1)\n    X_valid['kurt'] = X_valid[feature].kurtosis(axis = 1)\n\n    X_test['n_missing'] = X_test[feature].isna().sum(axis = 1)\n    X_test['std'] = X_test[feature].std(axis = 1)\n    X_test['mean'] = X_test[feature].mean(axis = 1)\n    X_test['median'] = X_train[feature].mean(axis = 1)\n    X_test['kurt'] = X_test[feature].kurtosis(axis = 1)\n    \n    feature += ['n_missing','std','mean','median','kurt']\n    X_train[feature] = X_train[feature].fillna(X_train[feature].mean())\n    X_valid[feature] = X_valid[feature].fillna(X_valid[feature].mean())\n    X_test[feature] = X_test[feature].fillna(X_test[feature].mean())\n    \n    scaler= StandardScaler()\n    X_train = pd.DataFrame(scaler.fit_transform(X_train))\n    X_valid = pd.DataFrame(scaler.transform(X_valid))\n    X_test = pd.DataFrame(scaler.transform(X_test))\n                                \n    lgb_train = lgb.Dataset(X_train, y_train)\n    lgb_valid = lgb.Dataset(X_valid, y_valid,reference = lgb.train)\n    \n    model = LGBMRegressor()\n    model = lgb.train(param,   lgb_train, valid_sets = [lgb_valid],verbose_eval = 100)\n                   \n    pred_valid = model.predict(X_valid)\n    valid_pred.update(dict(zip(valid_ids, pred_valid)))\n    test_preds = model.predict(X_test)\n    test_pred.append(test_preds)\n    \n    auc = roc_auc_score(y_valid, pred_valid)\n    print(fold,auc)\n    score.append(auc)\n    \nprint(score)","metadata":{"execution":{"iopub.status.busy":"2021-09-30T12:31:17.953255Z","iopub.execute_input":"2021-09-30T12:31:17.954101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid_prediction = pd.DataFrame.from_dict(valid_pred, orient = \"index\").reset_index()\nvalid_prediction.columns = ['id', 'pred_23']\nvalid_prediction.to_csv(\"valid_pred_23.csv\", index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission['claim'] = np.mean(np.column_stack(test_pred), axis = 1)\nsubmission.to_csv(\"submission.csv\", index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}