{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <center>Tabular Playground Series - Sep 2021</center>\n\nThis notebook is work in progress...\n<hr>\n\n## 1. Problem Definition\n\nAlthough the dataset for this competition is synthetic, it is based on a real dataset and it has been generated using [CTGAN](https://github.com/sdv-dev/CTGAN). This dataset involves predicting whether a claim will be made on an insurance policy. Features have been anonymized and they have properties relating to real-world features.\n\n- It is a <span style=\"color:skyblue;\">binary (2-class) classification</span> problem. \n- The number of observations for each class is <span style=\"color:skyblue;\">balanced</span>. \n- There are 957,919 observations in the training data with 118 input variables (including 'id') and 1 output variable ('claim'). \n- There are 493,474 observations in the test data with 118 input variables (including 'id')\n- There are 493,474 rows in the sample solution with 2 columns ('id','claim')\n- The dataset has a <span style=\"color:skyblue;\">lot of missing values</span> which have been encoded with NaN values. \n- The variable names are as follows: ('id', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60', f61', 'f62', 'f63', 'f64', 'f65', 'f66', 'f67', 'f68', 'f69', 'f70', 'f71', 'f72', 'f73', 'f74', 'f75', f76', 'f77', 'f78', 'f79', 'f80', 'f81', 'f82', 'f83', 'f84', 'f85', 'f86', 'f87', 'f88', 'f89', 'f90', f91', 'f92', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98', 'f99', 'f100', 'f101', 'f102', 'f103', 'f104', f105', 'f106', 'f107', 'f108', 'f109', 'f110', 'f111', 'f112', 'f113', 'f114', 'f115', 'f116', 'f117', f118', 'claim')\n\n<u>Goal</u>: Predict whether a claim will be made on an insurance policy.\n\n<hr>\n\n## 2. Load data\n\nLet's start off by loading the libraries required for this project.\n\n#### Install libraries\n\nLetâ€™s begin by installing the latest stable version of datatable.","metadata":{}},{"cell_type":"code","source":"!pip install datatable","metadata":{"execution":{"iopub.status.busy":"2021-09-17T18:19:50.735559Z","iopub.execute_input":"2021-09-17T18:19:50.735907Z","iopub.status.idle":"2021-09-17T18:20:01.442413Z","shell.execute_reply.started":"2021-09-17T18:19:50.735864Z","shell.execute_reply":"2021-09-17T18:20:01.44127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Load libraries","metadata":{}},{"cell_type":"code","source":"# Load libraries\nimport datatable as dt\nfrom datatable.models import Ftrl\nprint(dt.__version__)\n\nimport time\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\n# from bokeh.plotting import *\n# output_notebook()\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\nsns.set_context(rc={'figure.figsize':(12, 6)})\n\n# to print all outputs of a cell\nfrom IPython.core.interactiveshell import InteractiveShell  \nInteractiveShell.ast_node_interactivity = \"all\"\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2021-09-17T18:20:01.444591Z","iopub.execute_input":"2021-09-17T18:20:01.44488Z","iopub.status.idle":"2021-09-17T18:20:01.478595Z","shell.execute_reply.started":"2021-09-17T18:20:01.444845Z","shell.execute_reply":"2021-09-17T18:20:01.477473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Load data\n\nInitially I tried to read data using pandas, but it was very slow, hence I have used datatable instead.","metadata":{}},{"cell_type":"code","source":"## Data Table Reading\nstart = time.time()\ndata_dir = Path('../input/tabular-playground-series-sep-2021/')\ndt_train = dt.fread(data_dir / \"train.csv\")\ndt_test = dt.fread(data_dir / \"test.csv\")\ndt_submission = dt.fread(data_dir / \"sample_solution.csv\")\nend = time.time()\nprint(end - start)","metadata":{"execution":{"iopub.status.busy":"2021-09-17T18:20:01.480786Z","iopub.execute_input":"2021-09-17T18:20:01.481175Z","iopub.status.idle":"2021-09-17T18:20:10.862519Z","shell.execute_reply.started":"2021-09-17T18:20:01.481125Z","shell.execute_reply":"2021-09-17T18:20:10.861748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Exploratory Data Analysis\n\n- Automatic EDA using sweetviz can be found in [this notebook](https://www.kaggle.com/sugamkhetrapal/tabular-playground-sep-21-eda-dataprep).\n\n- Automatic EDA using dataprep can be found in [this notebook](https://www.kaggle.com/sugamkhetrapal/tabular-playground-sep-21-eda-sweetviz/notebook). Click on the output tab to download the report.\n\nWe are also going to cover the following steps:\n1. Take a peek at our raw data.\n2. Review the dimensions of our dataset.\n3. Review the data types of attributes in our data.\n4. Summarize the distribution of instances across classes in our dataset.\n5. Summarize our data using descriptive statistics.\n6. Understand the relationships in our data using correlations.\n7. Review the skew of the distributions of each attribute.\n\n##### Peek at our data\n\nLet's review the first five rows of the data.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-11T02:03:55.635454Z","iopub.execute_input":"2021-09-11T02:03:55.635804Z","iopub.status.idle":"2021-09-11T02:04:04.570073Z","shell.execute_reply.started":"2021-09-11T02:03:55.635759Z","shell.execute_reply":"2021-09-11T02:04:04.569307Z"}}},{"cell_type":"code","source":"dt_train.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-09-17T18:20:10.867458Z","iopub.execute_input":"2021-09-17T18:20:10.870078Z","iopub.status.idle":"2021-09-17T18:20:10.881157Z","shell.execute_reply.started":"2021-09-17T18:20:10.870018Z","shell.execute_reply":"2021-09-17T18:20:10.880356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Dimensions of our data","metadata":{"execution":{"iopub.status.busy":"2021-09-11T02:27:40.820696Z","iopub.execute_input":"2021-09-11T02:27:40.821457Z","iopub.status.idle":"2021-09-11T02:27:41.483072Z","shell.execute_reply.started":"2021-09-11T02:27:40.82141Z","shell.execute_reply":"2021-09-11T02:27:41.482177Z"}}},{"cell_type":"code","source":"# number of rows and columns in training dataset\ndt_train.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-17T18:20:10.885598Z","iopub.execute_input":"2021-09-17T18:20:10.888491Z","iopub.status.idle":"2021-09-17T18:20:10.898984Z","shell.execute_reply.started":"2021-09-17T18:20:10.88843Z","shell.execute_reply":"2021-09-17T18:20:10.898007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Training dataset has 957,919 rows and 120 columns","metadata":{}},{"cell_type":"code","source":"# number of rows and columns in test dataset\ndt_test.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-17T18:20:10.90467Z","iopub.execute_input":"2021-09-17T18:20:10.905301Z","iopub.status.idle":"2021-09-17T18:20:10.918221Z","shell.execute_reply.started":"2021-09-17T18:20:10.905257Z","shell.execute_reply":"2021-09-17T18:20:10.917029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Test dataset has 493,474 rows and 119 columns\n\n##### Column names","metadata":{}},{"cell_type":"code","source":"# To get the column names\ndt_train.names","metadata":{"execution":{"iopub.status.busy":"2021-09-17T18:20:10.920563Z","iopub.execute_input":"2021-09-17T18:20:10.921317Z","iopub.status.idle":"2021-09-17T18:20:10.930169Z","shell.execute_reply.started":"2021-09-17T18:20:10.921264Z","shell.execute_reply":"2021-09-17T18:20:10.929194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- We have the 'id' variable\n- We have variables from f1, f2, ..., f118.\n- We have target variable titled 'claim'.\n\nNow, let's look at the data types of each of these variables","metadata":{}},{"cell_type":"code","source":"for i in range(len(dt_train.names)):\n    print(dt_train.names[i], \":\", dt_train.stypes[i])","metadata":{"execution":{"iopub.status.busy":"2021-09-17T18:20:10.931592Z","iopub.execute_input":"2021-09-17T18:20:10.931864Z","iopub.status.idle":"2021-09-17T18:20:10.988423Z","shell.execute_reply.started":"2021-09-17T18:20:10.931834Z","shell.execute_reply":"2021-09-17T18:20:10.98753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The 'id' variable is of type 'Integer'\n- Variables named f1, f2, ..., f118 are of type 'float64'.\n- The output variable 'claim' is of type 'boolean'.\n\nLet's check what the submission file looks like.","metadata":{}},{"cell_type":"code","source":"dt_submission.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-09-17T18:20:10.989887Z","iopub.execute_input":"2021-09-17T18:20:10.990246Z","iopub.status.idle":"2021-09-17T18:20:10.998516Z","shell.execute_reply.started":"2021-09-17T18:20:10.9902Z","shell.execute_reply":"2021-09-17T18:20:10.996773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The submission file has probabilites (0.5) instead of 0 and 1.\n- It is mentioned under the evaluation section that for each id in the test set, we must predict a probability for the claim variable","metadata":{}},{"cell_type":"code","source":"dt_submission.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-17T18:20:11.002761Z","iopub.execute_input":"2021-09-17T18:20:11.003164Z","iopub.status.idle":"2021-09-17T18:20:11.012987Z","shell.execute_reply.started":"2021-09-17T18:20:11.003123Z","shell.execute_reply":"2021-09-17T18:20:11.011825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The submission file has 493,474 rows and 2 colums (id, claim).","metadata":{}},{"cell_type":"markdown","source":"##### Summary Statistics\n\nLet us get the mean, minimum, maximum and standard deviation of the columns using datatable","metadata":{}},{"cell_type":"code","source":"# mean\ndt_train.mean()","metadata":{"execution":{"iopub.status.busy":"2021-09-17T18:20:11.01513Z","iopub.execute_input":"2021-09-17T18:20:11.015437Z","iopub.status.idle":"2021-09-17T18:20:11.548007Z","shell.execute_reply.started":"2021-09-17T18:20:11.015407Z","shell.execute_reply":"2021-09-17T18:20:11.547105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# max\ndt_train.max()","metadata":{"execution":{"iopub.status.busy":"2021-09-17T18:20:11.549652Z","iopub.execute_input":"2021-09-17T18:20:11.553499Z","iopub.status.idle":"2021-09-17T18:20:11.561419Z","shell.execute_reply.started":"2021-09-17T18:20:11.55344Z","shell.execute_reply":"2021-09-17T18:20:11.56068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# min\ndt_train.min()","metadata":{"execution":{"iopub.status.busy":"2021-09-17T18:20:11.562887Z","iopub.execute_input":"2021-09-17T18:20:11.563649Z","iopub.status.idle":"2021-09-17T18:20:11.577283Z","shell.execute_reply.started":"2021-09-17T18:20:11.563591Z","shell.execute_reply":"2021-09-17T18:20:11.576135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# standard deviation\ndt_train.sd()","metadata":{"execution":{"iopub.status.busy":"2021-09-17T18:20:11.579198Z","iopub.execute_input":"2021-09-17T18:20:11.579768Z","iopub.status.idle":"2021-09-17T18:20:11.588406Z","shell.execute_reply.started":"2021-09-17T18:20:11.579727Z","shell.execute_reply":"2021-09-17T18:20:11.587611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# missing values\n# https://www.machinelearningplus.com/data-manipulation/101-python-datatable-exercises-pydatatable/\n# How to count NA values in every column of a datatable Frame?\ndt_train.countna()","metadata":{"execution":{"iopub.status.busy":"2021-09-17T18:20:11.590124Z","iopub.execute_input":"2021-09-17T18:20:11.591388Z","iopub.status.idle":"2021-09-17T18:20:11.60584Z","shell.execute_reply.started":"2021-09-17T18:20:11.591338Z","shell.execute_reply":"2021-09-17T18:20:11.604809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- This dataset has lot of missing values which need to be dropped or treated.\n\n##### Class Distribution (Classification Only)\n\nOn classification problems we need to know how balanced the class values are. Highly imbalanced problems (a lot more observations for one class than another) are common and may need special handling in the data preparation stage of our project.","metadata":{}},{"cell_type":"code","source":"# Class Distribution\n# start = time.time()\n# for i in range(10000):\n#     dt_train[:, dt.sum(dt.f.claim), dt.by(dt.f.claim)]\n# end = time.time()\n# print(end - start)\n\n# Class Distribution in pandas\nclass_counts = dt_train.to_pandas().groupby('claim').size()\nprint(class_counts)\n# find out how to do this in datatable","metadata":{"execution":{"iopub.status.busy":"2021-09-17T18:20:11.607349Z","iopub.execute_input":"2021-09-17T18:20:11.608324Z","iopub.status.idle":"2021-09-17T18:20:12.227685Z","shell.execute_reply.started":"2021-09-17T18:20:11.608276Z","shell.execute_reply":"2021-09-17T18:20:12.226469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The training dataset appears to be balanced as we have approx. 480K cases in which claims were not made and approx. 477K cases in which claims were made.\n- Since data imbalance is not there, it is not required to be treated in this competition.\n\n##### Calculate the mean, minimum, maximum and standard deviation of each column in which claims were made (i.e. claim = 1)","metadata":{}},{"cell_type":"code","source":"dt_train[dt.f.claim == 1, :].mean()","metadata":{"execution":{"iopub.status.busy":"2021-09-17T18:20:12.229434Z","iopub.execute_input":"2021-09-17T18:20:12.229783Z","iopub.status.idle":"2021-09-17T18:20:12.593688Z","shell.execute_reply.started":"2021-09-17T18:20:12.229737Z","shell.execute_reply":"2021-09-17T18:20:12.592823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dt_train[dt.f.claim == 1, :].min()","metadata":{"execution":{"iopub.status.busy":"2021-09-17T18:20:12.595077Z","iopub.execute_input":"2021-09-17T18:20:12.595731Z","iopub.status.idle":"2021-09-17T18:20:12.842289Z","shell.execute_reply.started":"2021-09-17T18:20:12.595687Z","shell.execute_reply":"2021-09-17T18:20:12.841532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dt_train[dt.f.claim == 1, :].max()","metadata":{"execution":{"iopub.status.busy":"2021-09-17T18:20:12.843982Z","iopub.execute_input":"2021-09-17T18:20:12.844542Z","iopub.status.idle":"2021-09-17T18:20:13.101293Z","shell.execute_reply.started":"2021-09-17T18:20:12.844499Z","shell.execute_reply":"2021-09-17T18:20:13.100366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Calculate the mean, minimum, maximum and standard deviation of each column in which claims were not made (i.e. claim = 0)","metadata":{"execution":{"iopub.status.busy":"2021-09-11T03:23:47.503702Z","iopub.execute_input":"2021-09-11T03:23:47.504416Z","iopub.status.idle":"2021-09-11T03:23:47.859812Z","shell.execute_reply.started":"2021-09-11T03:23:47.504376Z","shell.execute_reply":"2021-09-11T03:23:47.859081Z"}}},{"cell_type":"code","source":"dt_train[dt.f.claim == 0, :].mean()","metadata":{"execution":{"iopub.status.busy":"2021-09-17T18:20:13.106029Z","iopub.execute_input":"2021-09-17T18:20:13.107133Z","iopub.status.idle":"2021-09-17T18:20:13.471463Z","shell.execute_reply.started":"2021-09-17T18:20:13.107087Z","shell.execute_reply":"2021-09-17T18:20:13.470773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dt_train[dt.f.claim == 0, :].min()","metadata":{"execution":{"iopub.status.busy":"2021-09-17T18:20:13.472713Z","iopub.execute_input":"2021-09-17T18:20:13.473296Z","iopub.status.idle":"2021-09-17T18:20:13.718179Z","shell.execute_reply.started":"2021-09-17T18:20:13.473257Z","shell.execute_reply":"2021-09-17T18:20:13.717258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dt_train[dt.f.claim == 0, :].max()","metadata":{"execution":{"iopub.status.busy":"2021-09-17T18:20:13.723122Z","iopub.execute_input":"2021-09-17T18:20:13.725756Z","iopub.status.idle":"2021-09-17T18:20:13.978658Z","shell.execute_reply.started":"2021-09-17T18:20:13.725692Z","shell.execute_reply":"2021-09-17T18:20:13.977885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\n#### Correlations Between Attributes\nCorrelation refers to the relationship between two variables and how they may or may not change together. The most common method for calculating correlation is Pearson's Correlation Coefficient, that assumes a normal distribution of the attributes involved. A correlation of -1 or 1 shows a full negative or positive correlation respectively. Whereas a value of 0 shows no correlation at all. Some machine learning algorithms like linear and logistic regression can suffer poor performance if there are highly correlated attributes in our dataset. As such, it is a good idea to review all of the pairwise correlations of the attributes in our dataset.","metadata":{}},{"cell_type":"code","source":"start = time.time()\n\n# Pairwise Pearson correlations\ncorrelations = dt_train.to_pandas().corr(method='pearson')\nprint(correlations)\n\nend = time.time()\nprint(end - start)","metadata":{"execution":{"iopub.status.busy":"2021-09-17T18:20:13.984109Z","iopub.execute_input":"2021-09-17T18:20:13.986314Z","iopub.status.idle":"2021-09-17T18:20:54.30049Z","shell.execute_reply.started":"2021-09-17T18:20:13.986251Z","shell.execute_reply":"2021-09-17T18:20:54.299495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numeric_data = dt_train[:, [int, float]]\nnumeric_ncols = numeric_data.ncols\nnumeric_names = list(numeric_data.names)\ncorr_matrix = dt.Frame([[None] * numeric_ncols] * (numeric_ncols + 1), names=['Columns'] + numeric_names)\ncorr_matrix[:, 0] = dt.Frame(numeric_names)\n\nfor i in range(numeric_data.ncols):\n    for j in range(i, numeric_data.ncols):\n        corr_matrix[i, j+1] = numeric_data[:, dt.corr(dt.f[i], dt.f[j])]\n        corr_matrix[j, i+1] = corr_matrix[i, j+1]\n\ncorr_matrix","metadata":{"execution":{"iopub.status.busy":"2021-09-17T18:20:54.301903Z","iopub.execute_input":"2021-09-17T18:20:54.302202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- List down the attribute combinations which are positively correlated (i.e. > 0.5)\n- List down the attribute combinations which are negatively correlated\n\n### Skew of Univariate Distributions\n\nSkew refers to a distribution that is assumed Gaussian (normal or bell curve) that is shifted or squashed in one direction or another. Many machine learning algorithms assume a Gaussian distribution. Knowing that an attribute has a skew may allow us to perform data preparation to correct the skew and later improve the accuracy of our models.","metadata":{}},{"cell_type":"code","source":"dt_train.skew()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dt_train.kurt()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dt_train.nunique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numeric_data = dt_train[:, [int, float]]\nnumeric_ncols = numeric_data.ncols\nnumeric_names = list(numeric_data.names)\ncov_matrix = dt.Frame([[None] * numeric_ncols] * (numeric_ncols + 1), names=['Columns'] + numeric_names)\ncov_matrix[:, 0] = dt.Frame(numeric_names)\n\nfor i in range(numeric_data.ncols):\n    for j in range(i, numeric_data.ncols):\n        cov_matrix[i, j+1] = numeric_data[:, dt.corr(dt.f[i], dt.f[j])]\n        cov_matrix[j, i+1] = cov_matrix[i, j+1]\n\ncov_matrix","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Understand our data with visualization","metadata":{}},{"cell_type":"code","source":"# Univariate Histograms\n# dt_train.to_pandas().hist()\n# pyplot.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Univariate Density Plots\n# dt_train.to_pandas().plot(kind='density', subplots=True, layout=(3,3), sharex=False)\n# pyplot.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Box and Whisker Plots\n# dt_train.to_pandas().plot(kind='box', subplots=True, layout=(3,3), sharex=False, sharey=False)\n# pyplot.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# split into train and test sets\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n# https://stackoverflow.com/questions/63022043/how-to-split-datatable-dataframe-into-train-and-test-dataset-in-python\nfrom sklearn.model_selection import train_test_split\n\nX = dt_train[:, [col for col in dt_train.names if col != 'claim']]\ny = dt_train[:, -1]\n\nX = X.to_numpy()\ny = y.to_numpy()\n\n# dt_df = dt_train.to_numpy()\nX_train, X_validation, y_train, y_validation = train_test_split(X, y, test_size=0.3)\n\nX_train = dt.Frame(X_train)\nX_validation = dt.Frame(X_validation)\ny_train = dt.Frame(y_train)\ny_validation = dt.Frame(y_validation)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X_train.shape\n# X_validation.shape\n# y_train.shape\n# y_validation.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train a FTRL model model_ftrl_1 using train_data and train_target and assign the predictions of valid_data to preds_valid_1 and of test to preds_test_1\nfrom datatable.models import Ftrl\n\nmodel_ftrl_1 = Ftrl()\nmodel_ftrl_1.fit(X_train, y_train)\nmodel_ftrl_1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction_validation_1 = model_ftrl_1.predict(X_validation)\nprediction_validation_1.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test = dt_test[:,:]\nX_test = X_test.to_numpy()\nX_test = dt.Frame(X_test)\n\nprediction_test_1 = model_ftrl_1.predict(X_test)\nprediction_test_1.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display the feature importances of model_ftrl_1 in descending order and calculate the logloss of y_validation and prediction_validation_1\nmodel_ftrl_1.feature_importances[:, :, dt.sort(-dt.f.feature_importance)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print all the column names and column types of data in column-name : column-type format\nfor i in range(dt_train.ncols):\n    print(f'{dt_train.names[i]} : {dt_train.types[i].name}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = dt.cbind(y_validation, prediction_validation_1)\n# print(preds) very important to print pred because we will come to know that claim has been renamed to C0\npreds[:, -dt.mean(dt.f.C0 * dt.math.log(dt.f['True']) + (1-dt.f.C0) * dt.math.log(dt.f['False']))][0, 0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_ids = dt_submission['id']\nprint(submission_ids)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create submission_1 in the submission format of the competition, write it as submission_1.csv and submit it on Kaggle\nsubmission_1 = dt.Frame(id=submission_ids, claim=prediction_test_1['True'])\nsubmission_1.to_csv('submission_1.csv')\nsubmission_1.head()\n# submission scored 0.79455","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train another FTRL model model_ftrl_2 with nepochs=3, `nbins=10 8, display it's feature importances, score & evaluate it's logloss onvalid_dataand submit the predictionspreds_test_2oftestassubmission_2`**\nmodel_ftrl_2 = Ftrl(nepochs=3, nbins=10**8)\nmodel_ftrl_2.fit(X_train, y_train)\nmodel_ftrl_2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_ftrl_2.feature_importances[:, :, dt.sort(-dt.f.feature_importance)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction_validation_2 = model_ftrl_2.predict(X_validation)\nprediction_validation_2.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction_test_2 = model_ftrl_2.predict(X_test)\nprediction_test_2.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = dt.cbind(y_validation, prediction_validation_2)\npreds[:, -dt.mean(dt.f.C0 * dt.math.log(dt.f['True']) + (1-dt.f.C0) * dt.math.log(dt.f['False']))][0, 0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_2 = dt.Frame(id=submission_ids, claim=prediction_test_2['True'])\nsubmission_2.to_csv('submission_2.csv')\nsubmission_2.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Submit a ensemble of model_ftrl_1 and model_ftrl_2 by averaging the predictions as submission_ensemble\nsubmission_ensemble = dt.cbind(submission_1, submission_2)\nsubmission_ensemble[:, dt.update(claim = 0.5 * dt.f.claim + 0.5 * dt.f.claim)]\ndel submission_ensemble[:, ['id.0', 'claim.0']]\nsubmission_ensemble.to_csv('submission_ensemble.csv')\nsubmission_ensemble.head()\n# submission scored 0.79485","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dt_train.countna()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# imputing with a constant\n\nfrom sklearn.impute import SimpleImputer\ntrain_constant = dt_train.copy()\n#setting strategy to 'constant' \nmean_imputer = SimpleImputer(strategy='constant') # imputing using constant value\ntrain_constant[:,:] = mean_imputer.fit_transform(train_constant)\ntrain_constant.countna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer\ntrain_most_frequent = dt_train.copy()\n#setting strategy to 'mean' to impute by the mean\nmean_imputer = SimpleImputer(strategy='most_frequent')# strategy can also be mean or median \ntrain_most_frequent[:,:] = mean_imputer.fit_transform(train_most_frequent)\ntrain_most_frequent.countna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://www.kaggle.com/general/76911\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nMiceImputed = dt_train.copy(deep=True)\nmice_imputer = IterativeImputer()\nMiceImputed[:, :] = mice_imputer.fit_transform(dt_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Notes and Learning Opportunities\n\n- Replace missing values with mean and evaluate it's impact on model evaluation and performance. (pending)\n- Drop missing values and evaluate it's impact on model evaluation and performance (pending)\n- Can we use MICE to impute missing values? What will be it's impact on model evaluation and performance? (pending)\n- Could we use R to impute missing values using data.table and then use the imputed dataset in python? (pending) https://www.datacamp.com/community/tutorials/using-both-python-r Can we embed R in python using rpy2, do missing value imputation using R and pass on the imputed dataset to python? (find out)\n- Could we use number of missing values in each column as a feature? (pending)\n- Did we try Hyperparameter optimization using Optuna? (pending)\n- How to do visualization on large datasets? (find out)\n- Do we have any missing values in test data? Do we need to check it? If not, why not? (find out)","metadata":{}},{"cell_type":"markdown","source":"# References and Credits\n\n- https://www.kaggle.com/bextuychiev/7-coolest-packages-top-kagglers-are-using#2.-Datatable\n- https://www.kaggle.com/sudalairajkumar/getting-started-with-python-datatable\n- cannot use datatable because it does have functionality to handle missing values yet.\n- https://github.com/vopani/datatableton#set-04--frame-operations--beginner--exercises-31-40\n- http://webcache.googleusercontent.com/search?q=cache:okPGVK9Fxd0J:https://towardsdatascience.com/introducing-datatableton-python-datatable-tutorials-exercises-a0887f4323b0&hl=en&gl=in&strip=1&vwsrc=0\n- https://datatable.readthedocs.io/en/latest/manual/comparison_with_pandas.html#missing-functionality\n- https://www.kaggle.com/chayan8/missing-value-imputation-using-mice-knn-ckd-data\n- https://www.kaggle.com/general/187601\n- https://www.kaggle.com/melanie7744/tps9-how-to-transform-your-data try each and test the impact on the model\n- http://webcache.googleusercontent.com/search?q=cache:okPGVK9Fxd0J:https://towardsdatascience.com/introducing-datatableton-python-datatable-tutorials-exercises-a0887f4323b0&hl=en&gl=in&strip=1&vwsrc=0\n- http://webcache.googleusercontent.com/search?q=cache:NtZTpcPxjRUJ:https://towardsdatascience.com/speed-up-your-data-analysis-with-pythons-datatable-package-56e071a909e9&hl=en&gl=in&strip=1&vwsrc=0\n- https://github.com/vopani/datatableton#set-05--column-aggregations--beginner--exercises-41-50\n- http://webcache.googleusercontent.com/search?q=cache:uPQQGanfFDUJ:https://towardsdatascience.com/beyond-pandas-spark-dask-vaex-and-other-big-data-technologies-battling-head-to-head-a453a1f8cc13&hl=en&gl=in&strip=1&vwsrc=0\n","metadata":{}}]}