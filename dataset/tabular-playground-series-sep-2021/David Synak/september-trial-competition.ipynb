{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Thsi notebook will first serve as a baseline LightGBM run, which I will type by hand - based on \"LightGBM Starter\" by firefliesqn. Once I've established this sort of baseline, I'll branch out on my own and experiment with some additional FE and maybe some other models. This notebook will probably NOT include any NN; I might come back to this later. ","metadata":{}},{"cell_type":"markdown","source":"I would ideally fork the notebook, but I would like to practice by typing out all the code myself.","metadata":{}},{"cell_type":"code","source":"#The initial dependencies; will be updated if needed\n\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datatable as dt\nimport optuna\n\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom sklearn.preprocessing import RobustScaler #I'll look into other scaling methods in the future versions\nfrom sklearn.metrics import roc_auc_score\n\nimport lightgbm as lgb\n\nimport warnings\nwarnings.simplefilter('ignore')\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-30T18:23:35.288797Z","iopub.execute_input":"2021-09-30T18:23:35.289583Z","iopub.status.idle":"2021-09-30T18:23:35.304065Z","shell.execute_reply.started":"2021-09-30T18:23:35.289546Z","shell.execute_reply":"2021-09-30T18:23:35.303218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = dt.fread('/kaggle/input/tabular-playground-series-sep-2021/train.csv').to_pandas()\ntest_df = dt.fread('/kaggle/input/tabular-playground-series-sep-2021/test.csv').to_pandas()\nsample_df = dt.fread('/kaggle/input/tabular-playground-series-sep-2021/sample_solution.csv').to_pandas()","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:13:41.105741Z","iopub.execute_input":"2021-09-30T18:13:41.106074Z","iopub.status.idle":"2021-09-30T18:13:57.604505Z","shell.execute_reply.started":"2021-09-30T18:13:41.106033Z","shell.execute_reply":"2021-09-30T18:13:57.603844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Datasets imported, let's now check the shape of those datasets:","metadata":{}},{"cell_type":"code","source":"print(f'Shape of train_df: {train_df.shape}')\nprint(f'Shape of test_df: {test_df.shape}')\nprint(f'Shape of sample_df: {sample_df.shape}')","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:13:57.605666Z","iopub.execute_input":"2021-09-30T18:13:57.60613Z","iopub.status.idle":"2021-09-30T18:13:57.613422Z","shell.execute_reply.started":"2021-09-30T18:13:57.60609Z","shell.execute_reply":"2021-09-30T18:13:57.612533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:13:57.615121Z","iopub.execute_input":"2021-09-30T18:13:57.616098Z","iopub.status.idle":"2021-09-30T18:13:57.670863Z","shell.execute_reply.started":"2021-09-30T18:13:57.616057Z","shell.execute_reply":"2021-09-30T18:13:57.670169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we can split the datasets into the appropriate Xs and ys:","metadata":{"execution":{"iopub.status.busy":"2021-09-15T18:19:12.897534Z","iopub.execute_input":"2021-09-15T18:19:12.898942Z","iopub.status.idle":"2021-09-15T18:19:12.906029Z","shell.execute_reply.started":"2021-09-15T18:19:12.898881Z","shell.execute_reply":"2021-09-15T18:19:12.905076Z"}}},{"cell_type":"code","source":"X_train = train_df.drop(['id', 'claim'], axis=1)\ny_train = train_df['claim'].copy()\n\nX_test = test_df.drop('id', axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:13:57.673806Z","iopub.execute_input":"2021-09-30T18:13:57.674164Z","iopub.status.idle":"2021-09-30T18:13:58.056511Z","shell.execute_reply.started":"2021-09-30T18:13:57.674122Z","shell.execute_reply":"2021-09-30T18:13:58.055412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Soo... Browsing through the discussions, I've learned that a big lesson to be learned from this dataset is that the missing values are not put there at random. Or, put another way, it is a feature itself which may help establish if there was a claim or not. Well, let's add this feature as well as the standard deviation to our dataset to help us make a better prediction:","metadata":{}},{"cell_type":"code","source":"for column in X_train.columns:\n    print(f'{column}: {X_train[column].isna().sum()}')","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:13:58.058269Z","iopub.execute_input":"2021-09-30T18:13:58.058831Z","iopub.status.idle":"2021-09-30T18:13:58.341178Z","shell.execute_reply.started":"2021-09-30T18:13:58.058786Z","shell.execute_reply":"2021-09-30T18:13:58.340387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Approximately 15k out of nearly 1M rows; I think it's safe to impute these values with the median value and not affect the data too severely, but at least get rid of the NaNs","metadata":{}},{"cell_type":"code","source":"X_train['n_miss'] = X_train.isna().sum(axis=1)\nX_test['n_miss'] = X_test.isna().sum(axis=1)\n\n#Now I realize why there was a list of columns, if we compute the standard deviation as is, 'n_miss' will be also taken into account\n#I'll stick to using a slice of the dataframe:\nX_train['std'] = X_train[:-1].std(axis=1)\nX_test['std'] = X_test[:-1].std(axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:13:58.34316Z","iopub.execute_input":"2021-09-30T18:13:58.343377Z","iopub.status.idle":"2021-09-30T18:14:00.21284Z","shell.execute_reply.started":"2021-09-30T18:13:58.343352Z","shell.execute_reply":"2021-09-30T18:14:00.212038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:14:00.214213Z","iopub.execute_input":"2021-09-30T18:14:00.215122Z","iopub.status.idle":"2021-09-30T18:14:00.256253Z","shell.execute_reply.started":"2021-09-30T18:14:00.215081Z","shell.execute_reply":"2021-09-30T18:14:00.255412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Now in the original author's notebook, the NaN's were imputed with mean values, but I would like to stick to median values:\nX_train = X_train.fillna(X_train.median())\nX_test = X_test.fillna(X_test.median())\n\nprint(f'NaNs in X_train: {X_train.isna().sum().sum()}')\nprint(f'NaNs in X_test: {X_test.isna().sum().sum()}')","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:14:00.257645Z","iopub.execute_input":"2021-09-30T18:14:00.25807Z","iopub.status.idle":"2021-09-30T18:14:04.801979Z","shell.execute_reply.started":"2021-09-30T18:14:00.258026Z","shell.execute_reply":"2021-09-30T18:14:04.801115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Great! Now that the missing values have been taken care of, we can handle some scaling efforts:","metadata":{}},{"cell_type":"code","source":"scaler = RobustScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.fit_transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:14:04.803455Z","iopub.execute_input":"2021-09-30T18:14:04.803911Z","iopub.status.idle":"2021-09-30T18:14:12.890397Z","shell.execute_reply.started":"2021-09-30T18:14:04.803872Z","shell.execute_reply":"2021-09-30T18:14:12.889543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''A function to reduce the amount of memory taken up by each feature by compressing it to the appropriate datatype\nverbose parameter is used to output a message regarding the exact memory usage reduction'''\ndef reduce_memory_usage(df, verbose=True):\n    numerics = ['int8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024 ** 2 #initial memory usage to compare to\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            #extract the min and max values\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                else:\n                #elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() / 1024 ** 2 #new memory_usage\n    if verbose:\n        print(\n            \"Memory usage decreased to: {:.2f} Mb - {:.1f}% reduction\".format(\n                end_mem, 100 * (start_mem - end_mem) / start_mem\n                \n                )\n            )\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:14:12.891797Z","iopub.execute_input":"2021-09-30T18:14:12.892318Z","iopub.status.idle":"2021-09-30T18:14:12.90802Z","shell.execute_reply.started":"2021-09-30T18:14:12.892278Z","shell.execute_reply":"2021-09-30T18:14:12.907135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = pd.DataFrame(X_train)\nX_test = pd.DataFrame(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:14:12.911056Z","iopub.execute_input":"2021-09-30T18:14:12.911278Z","iopub.status.idle":"2021-09-30T18:14:12.922406Z","shell.execute_reply.started":"2021-09-30T18:14:12.911253Z","shell.execute_reply":"2021-09-30T18:14:12.921499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"X_train redution:\")\nX_train = reduce_memory_usage(X_train)\nprint(\"X_test reduction:\")\nX_test = reduce_memory_usage(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:14:12.923905Z","iopub.execute_input":"2021-09-30T18:14:12.924262Z","iopub.status.idle":"2021-09-30T18:14:38.962723Z","shell.execute_reply.started":"2021-09-30T18:14:12.924222Z","shell.execute_reply":"2021-09-30T18:14:38.961899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#X_train_df = pd.DataFrame(X_train)\n#X_train_df.hist(bins=50, figsize=(20,15))\n#plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:14:38.965568Z","iopub.execute_input":"2021-09-30T18:14:38.966396Z","iopub.status.idle":"2021-09-30T18:14:38.97041Z","shell.execute_reply.started":"2021-09-30T18:14:38.966355Z","shell.execute_reply":"2021-09-30T18:14:38.969624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lots of different distributions, some normal, some bi- or multimodal; tough luck... Probably the best solution would be to apply a transformation across the board.\n","metadata":{}},{"cell_type":"markdown","source":"Below are some initial params of the LightGBM algorhithm; I'll write them down for now. But probably I'll extend them to be lists of hyperparameters to tune:","metadata":{}},{"cell_type":"markdown","source":"The initial n_estimators did not result in early stopping, so it's probably wise to continue with the estimators - Early stopping will help us achieve this goal","metadata":{}},{"cell_type":"code","source":"x_tra, x_val, y_tra, y_val = train_test_split(X_train, y_train, test_size=0.2, shuffle=True, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:14:38.971527Z","iopub.execute_input":"2021-09-30T18:14:38.97188Z","iopub.status.idle":"2021-09-30T18:14:39.9744Z","shell.execute_reply.started":"2021-09-30T18:14:38.971839Z","shell.execute_reply":"2021-09-30T18:14:39.97338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def objective(trial):\n    num_leaves = trial.suggest_int(\"num_leaves\", 20, 40)\n    n_estimators = trial.suggest_int(\"n_estimators\", 500, 2000)\n    max_depth = trial.suggest_int('max_depth', 3, 8)\n    min_child_samples = trial.suggest_int('min_child_samples', 200, 750)\n    learning_rate = trial.suggest_uniform('learning_rate', 0.10, 0.30)\n    bagging_fraction = trial.suggest_uniform('bagging_fraction', 0.50, 1.0)\n    colsample_bytree = trial.suggest_uniform('colsample_bytree', 0.50, 1.0)\n    \n    model = lgb.LGBMClassifier(\n        objective='binary',\n        metric='auc',\n        num_leaves=num_leaves,\n        n_estimators=n_estimators, \n        max_depth=max_depth, \n        min_child_samples=min_child_samples, \n        learning_rate=learning_rate,\n        colsample_bytree=colsample_bytree,\n        random_state=42,\n    )\n    \n    model.fit(x_tra, y_tra)\n    #see link in markdown above for this next line\n    score = roc_auc_score(y_val, model.predict_proba(x_val)[:,1])\n    return score","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:50:28.477912Z","iopub.execute_input":"2021-09-30T18:50:28.478562Z","iopub.status.idle":"2021-09-30T18:50:28.486304Z","shell.execute_reply.started":"2021-09-30T18:50:28.478524Z","shell.execute_reply":"2021-09-30T18:50:28.485632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=10)\nparams = study.best_params #getting best params from study","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:50:31.911396Z","iopub.execute_input":"2021-09-30T18:50:31.911651Z","iopub.status.idle":"2021-09-30T19:33:43.662708Z","shell.execute_reply.started":"2021-09-30T18:50:31.911624Z","shell.execute_reply":"2021-09-30T19:33:43.662002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgb_params = {\n    'objective': 'binary',\n    'n_estimators': 20000, #worth tuning\n    'random_state': 42,\n    'learning_rate': 4e-3, #worth tuning\n    'subsample': 0.6,\n    'subsample_freq': 1,\n    'colsample_bytree': 0.4,\n    'reg_alpha': 12.0,\n    'reg_lambda': 1e-1,\n    'min_child_weight': 256,\n    'min_child_samples': 20,\n}","metadata":{"execution":{"iopub.status.busy":"2021-09-19T08:55:14.149799Z","iopub.execute_input":"2021-09-19T08:55:14.150474Z","iopub.status.idle":"2021-09-19T08:55:14.155012Z","shell.execute_reply.started":"2021-09-19T08:55:14.15044Z","shell.execute_reply":"2021-09-19T08:55:14.153997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lowercase and shortened to distinguish from the 'original' train sets\n#x_tra, x_val, y_tra, y_val = train_test_split(X_train, y_train, test_size=0.2, shuffle=True, random_state=42)\n\nlgb_classifier = lgb.LGBMClassifier(**params)\n\nlgb_classifier.fit(x_tra, y_tra, eval_set=[(x_val, y_val)],\n                  eval_metric='auc', early_stopping_rounds=200,\n                  verbose=500,\n                  )\ny_pred = lgb_classifier.predict_proba(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-09-30T19:35:06.506495Z","iopub.execute_input":"2021-09-30T19:35:06.506764Z","iopub.status.idle":"2021-09-30T19:38:56.974734Z","shell.execute_reply.started":"2021-09-30T19:35:06.506736Z","shell.execute_reply":"2021-09-30T19:38:56.97414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_df['claim'] = y_pred[:,1]","metadata":{"execution":{"iopub.status.busy":"2021-09-30T19:41:07.393935Z","iopub.execute_input":"2021-09-30T19:41:07.394712Z","iopub.status.idle":"2021-09-30T19:41:07.400671Z","shell.execute_reply.started":"2021-09-30T19:41:07.394667Z","shell.execute_reply":"2021-09-30T19:41:07.399974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_df.to_csv('submission_8.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-30T19:41:23.9593Z","iopub.execute_input":"2021-09-30T19:41:23.959551Z","iopub.status.idle":"2021-09-30T19:41:25.511704Z","shell.execute_reply.started":"2021-09-30T19:41:23.959525Z","shell.execute_reply":"2021-09-30T19:41:25.510985Z"},"trusted":true},"execution_count":null,"outputs":[]}]}