{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Welcome to the September 2021 Tabular Playground Competition! #\n\nIn this competition, we predict whether a customer will make an insurance claim.\n\n# Data #\n\nThe full dataset has almost one million rows. We'll use just a sample so we can explore the data more quickly.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom pathlib import Path\n\n\nimport matplotlib\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\n\ndata_dir = Path('../input/tabular-playground-series-sep-2021/')\n\ndf_train = pd.read_csv(\n    data_dir / \"train.csv\",\n    index_col='id',\n    #nrows=25000,  # comment this row to use the full dataset\n)\n\ndf_train.head(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Heatmap to View Missing Values by Variable\nplt.figure(figsize = (14,6))\np = sns.heatmap(df_train.isnull(), yticklabels = False, cbar = False, cmap = 'viridis')\np.axes.set_title(\"Valores Ausentes\", fontsize = 20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the nan values\n{df_train[col].isna().sum():col for col in df_train.columns if df_train[col].isna().sum() > 0}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the ZERO values\n{(df_train[col] == 0).sum():col for col in df_train.columns if (df_train[col] == 0).sum() > 0}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating a feature with a count of null columns per row\ndf_train[\"null_count\"] = df_train.isnull().sum(axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df_train_slice = df_train[(df_train.null_count > 3) &  (df_train.null_count < 7)] \n#df_train_slice[\"variance\"] = df_train_slice.var(axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df_train_slice.groupby(['null_count','claim'])['null_count'].count()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Features and target\nFEATURES = df_train.drop('claim', axis = 1)\nTARGET = df_train['claim'].astype(int).astype(str)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set(style=\"whitegrid\")\n\n# Using a bar chart to show the distribution of classes\nbp = sns.countplot(x=df_train['claim'])\nplt.title(\"Distribuição de classe do conjunto de dados\")\nbp.set_xticklabels([\"0\",\"1\"])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from numpy import mean\nfrom numpy import std\nimport numpy as np\nfrom scipy import stats\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer, KNNImputer\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom xgboost import XGBRegressor, XGBClassifier\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.metrics import mean_absolute_error, classification_report\nfrom sklearn.model_selection import cross_val_score, KFold\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer,QuantileTransformer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sliced impucation of missing values\nimp = SimpleImputer(missing_values=np.nan, strategy='median') # feel free to use others strategy\nFEATURES[(FEATURES.null_count < 4)] = imp.fit_transform(FEATURES[(FEATURES.null_count < 4)])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sliced impucation of missing values\n#imp = KNNImputer(n_neighbors=5) # feel free to use others strategy\nimp = SimpleImputer(missing_values=np.nan, strategy='mean') # feel free to use others strategy\nFEATURES[((FEATURES.null_count > 3) &  (FEATURES.null_count < 7))] = imp.fit_transform(FEATURES[((FEATURES.null_count > 3) &  (FEATURES.null_count < 7))])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sliced impucation of missing values\n#imp_estimator = ExtraTreesRegressor(n_estimators=5, n_jobs=-1, criterion=\"mse\", verbose=1, random_state=42)\n#imp = IterativeImputer(random_state=42, estimator=imp_estimator)\nimp = SimpleImputer(missing_values=np.nan, strategy='most_frequent') # feel free to use others strategy\nFEATURES[((FEATURES.null_count > 6) &  (FEATURES.null_count < 13))] = imp.fit_transform(FEATURES[((FEATURES.null_count > 6) &  (FEATURES.null_count < 13))])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sliced impucation of missing values\nimp = SimpleImputer(strategy='mean') # feel free to use others strategy\nFEATURES[(FEATURES.null_count > 12)] = imp.fit_transform(FEATURES[(FEATURES.null_count > 12)])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sliced impucation of missing values\n#imp = KNNImputer(n_neighbors=5) # feel free to use others strategy\n#FEATURES[(FEATURES.null_count > 12)] = imp.fit_transform(FEATURES[(FEATURES.null_count > 12)])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"z = np.abs(stats.zscore(FEATURES))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"z.max()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FEATURES.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FEATURES[(z < 3).all(axis=1)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# impute media to outliear them\nimp = SimpleImputer(strategy='median') # feel free to use others strategy\nFEATURES[(z < 3).all(axis=1)] = imp.fit_transform(FEATURES[(z < 3).all(axis=1)])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Break off validation set from training data\nX_train, X_valid, y_train, y_valid = train_test_split(FEATURES, TARGET, \n                                                      train_size=0.8, test_size=0.2, random_state=42, shuffle=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model #\n\nLet's try out a simple XGBoost model. This algorithm can handle missing values, but you could try imputing them instead.  We use `XGBClassifier` (instead of `XGBRegressor`, for instance), since this is a classification problem.","metadata":{}},{"cell_type":"code","source":"# machine learning model configuration\nXGB = XGBClassifier(\n        learning_rate= 0.00312345,\n        reg_alpha = 0.1,\n        max_depth=5,\n        subsample=0.8,\n        colsample_bytree=0.4,\n        objective='multi:softprob',\n        n_estimators=27000,\n        eval_metric='auc',\n        num_class=2,\n        n_jobs=-1,\n        tree_method='gpu_hist',\n        # Uncomment if you want to use GPU. Recommended for whole training set.\n        #tree_method='gpu_hist',\n        random_state=42,\n        )\n\n#steps = [('imputer', SimpleImputer(strategy='most_frequent')),\nsteps = [('scle', MinMaxScaler()),\n         ('m', XGB)]\nmodel = Pipeline(steps=steps)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = X_train\ny = y_train.values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit the model\nmodel.fit(X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get predictions\ny_pred = model.predict_proba(X_valid)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n# Evaluation #\n\nThe evaluation metric is AUC, which stands for \"area under curve\".  Run the next code cell to evaluate the model.","metadata":{}},{"cell_type":"markdown","source":"A \"neutral\" AUC is 0.5, so anything better than that means our model learned something useful.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import *","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# retrieve just the probabilities for the positive class\npos_probs = y_pred[:, 1]\n# plot no skill roc curve\nplt.plot([0, 1], [0, 1], linestyle='--', label='No Skill')\n# calculate roc curve for model\nfpr, tpr, _ = roc_curve(y_valid.astype(str).astype(int), pos_probs)\n# plot model roc curve\nplt.plot(fpr, tpr, marker='.', label='Logistic')\n# axis labels\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\n# show the legend\nplt.legend()\n# show the plot\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"precisions, recalls, thresholds = precision_recall_curve(y_valid.astype(str).astype(int), y_pred[:,1])\n\ndef plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n    plt.xlabel(\"Threshold\")\n    plt.legend(loc=\"upper left\")\n    plt.ylim([0, 1])\n    \nplot_precision_recall_vs_threshold(precisions, recalls, thresholds)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import scikitplot as skplt\nskplt.metrics.plot_roc(y_valid.astype(str).astype(int), y_pred, figsize=(10, 8))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Make Submission #\n\nOur predictions are binary 0 and 1, but you're allowed to submit probabilities instead. In scikit-learn, you would use the `predict_proba` method instead of `predict`.","metadata":{}},{"cell_type":"code","source":"# reading test data\nX_test = pd.read_csv(data_dir / \"test.csv\", index_col='id')\nX_test[\"null_count\"] = X_test.isnull().sum(axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sliced impucation of missing values\nimp = SimpleImputer(missing_values=np.nan, strategy='median') # feel free to use others strategy\nX_test[(X_test.null_count < 4)] = imp.fit_transform(X_test[(X_test.null_count < 4)])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sliced impucation of missing values\nimp = SimpleImputer(missing_values=np.nan, strategy='mean') # feel free to use others strategy\nX_test[((X_test.null_count > 3) &  (X_test.null_count < 7))] = imp.fit_transform(X_test[((X_test.null_count > 3) &  (X_test.null_count < 7))])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sliced impucation of missing values\nimp = SimpleImputer(missing_values=np.nan, strategy='most_frequent') # feel free to use others strategy\nX_test[((X_test.null_count > 6) &  (X_test.null_count < 13))] = imp.fit_transform(X_test[((X_test.null_count > 6) &  (X_test.null_count < 13))])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sliced impucation of missing values\nimp = SimpleImputer(strategy='mean') # feel free to use others strategy\nX_test[(X_test.null_count > 12)] = imp.fit_transform(X_test[(X_test.null_count > 12)])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"z = np.abs(stats.zscore(X_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# impute media to outliear them\nimp = SimpleImputer(strategy='median') # feel free to use others strategy\nX_test[(z < 3).all(axis=1)] = imp.fit_transform(X_test[(z < 3).all(axis=1)])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get predictions\ny_pred = model.predict_proba(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test.groupby(['null_count'])['null_count'].count()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred[:, 1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_test = pd.Series(\n    y_pred[:, 1],\n    index=X_test.index,\n    name='claim',\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create submission file\ny_pred_test.to_csv(\"submission.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}