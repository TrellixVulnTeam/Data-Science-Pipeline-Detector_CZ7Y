{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Stacking with Two Levels (Base: 5 models) for TPS Sep 2021\nIn this notebook, I will use a 2-level stacking model with a meta-learner to predict probabilities for claims based on insurance policies (on Tabular Playground Sep 2021).\n\nThe levels are:\n1. RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, HistGradientBoostingClassifier, and GaussianNB\n2. XGBClassifier","metadata":{}},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# organization\nfrom sklearn.pipeline import Pipeline\n\n# data preprocessing\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\n# models\n# Level 1\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier, \\\n                              AdaBoostClassifier, HistGradientBoostingClassifier)\nfrom sklearn.naive_bayes import GaussianNB\n\n# Level 2\nfrom xgboost import XGBClassifier\n\n# Cross-validation and out-of-folds prediction\nfrom sklearn.model_selection import KFold\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-09-25T20:45:33.919421Z","iopub.execute_input":"2021-09-25T20:45:33.919746Z","iopub.status.idle":"2021-09-25T20:45:35.168296Z","shell.execute_reply.started":"2021-09-25T20:45:33.919645Z","shell.execute_reply":"2021-09-25T20:45:35.167409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Imports and Preprocessing","metadata":{}},{"cell_type":"code","source":"train_data = pd.read_csv(\"../input/tabular-playground-series-sep-2021/train.csv\")\ntest_data = pd.read_csv(\"../input/tabular-playground-series-sep-2021/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-09-25T20:45:35.170075Z","iopub.execute_input":"2021-09-25T20:45:35.170315Z","iopub.status.idle":"2021-09-25T20:46:12.181112Z","shell.execute_reply.started":"2021-09-25T20:45:35.170289Z","shell.execute_reply":"2021-09-25T20:46:12.180256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = [col for col in train_data.columns if col != \"claim\" and col != \"id\"] # keeping track of the features\nX = train_data[features]\nX_test = test_data[features]\ny = train_data[\"claim\"]\nX.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-25T20:46:12.182407Z","iopub.execute_input":"2021-09-25T20:46:12.182601Z","iopub.status.idle":"2021-09-25T20:46:12.760417Z","shell.execute_reply.started":"2021-09-25T20:46:12.182579Z","shell.execute_reply":"2021-09-25T20:46:12.759733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preprocessing of data\npreprocessor = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy = \"mean\")),\n    (\"scaler\", StandardScaler())\n])\n\nimputed_X = pd.DataFrame(preprocessor.fit_transform(X))\nimputed_X_test = X = pd.DataFrame(preprocessor.transform(X_test))\n\nimputed_X.columns = X.columns\nimputed_X_test.columns = X_test.columns\n\nX = imputed_X\nX_test = imputed_X_test","metadata":{"execution":{"iopub.status.busy":"2021-09-25T20:46:12.761655Z","iopub.execute_input":"2021-09-25T20:46:12.761845Z","iopub.status.idle":"2021-09-25T20:46:18.776467Z","shell.execute_reply.started":"2021-09-25T20:46:12.761824Z","shell.execute_reply":"2021-09-25T20:46:18.775783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check to see that all of the values were imputed\nseries = X.isna().count(False) != 957919\nprint(series.sum())\nseries = X_test.isna().count(False) == X_test.iloc[0].count()\nprint(series.sum())","metadata":{"execution":{"iopub.status.busy":"2021-09-25T20:46:18.778398Z","iopub.execute_input":"2021-09-25T20:46:18.778605Z","iopub.status.idle":"2021-09-25T20:46:19.214187Z","shell.execute_reply.started":"2021-09-25T20:46:18.778582Z","shell.execute_reply":"2021-09-25T20:46:19.213328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training and Levels\nI will train a 2-level model in this section. The levels will be:\n1. Level 1:\n* RandomForestClassifier\n* ExtraTreesClassifier\n* AdaBoostClassifier\n* HistGradientBoostingClassifier\n* GaussianNB\n2. Level 2:\n* XGBClassifier","metadata":{}},{"cell_type":"code","source":"# see references for these functions\n\n# Parameters to be used later\nntrain = X.shape[0]\nntest = X_test.shape[0]\nSEED = 0 # for reproducibility\nNFOLDS = 5 # folds for out-of-fold prediction\nkf = KFold(n_splits= NFOLDS)\n\n# Class to extend the sklearn classifier\nclass SklearnHelper(object):\n    def __init__(self, clf, seed=0, params=None):\n        if (clf != GaussianNB):\n            params['random_state'] = seed\n        self.clf = clf(**params)\n\n    def train(self, x_train, y_train):\n        self.clf.fit(x_train, y_train)\n\n    def predict(self, x):\n        return self.clf.predict(x)\n    \n    def fit(self,x,y):\n        return self.clf.fit(x,y)\n    \n    def feature_importances(self,x,y):\n        print(self.clf.fit(x,y).feature_importances_)\n\n# function prevents train-test contamination (with edits to make it up-to-date)\ndef get_oof(clf, x_train, y_train, x_test):\n    oof_train = np.zeros((ntrain,))\n    oof_test = np.zeros((ntest,))\n    oof_test_skf = np.empty((NFOLDS, ntest))\n\n    for i, (train_index, test_index) in enumerate(kf.split(x_train, y_train)):\n        x_tr = x_train.loc[train_index]\n        y_tr = y_train.loc[train_index]\n        x_te = x_train.loc[test_index]\n\n        clf.train(x_tr, y_tr)\n\n        oof_train[test_index] = clf.predict(x_te)\n        oof_test_skf[i, :] = clf.predict(x_test)\n\n    oof_test[:] = oof_test_skf.mean(axis=0)\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)","metadata":{"execution":{"iopub.status.busy":"2021-09-25T20:46:19.215515Z","iopub.execute_input":"2021-09-25T20:46:19.215732Z","iopub.status.idle":"2021-09-25T20:46:19.226112Z","shell.execute_reply.started":"2021-09-25T20:46:19.215708Z","shell.execute_reply":"2021-09-25T20:46:19.225118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Level 1","metadata":{}},{"cell_type":"code","source":"# Parameters for level 1 models\n\n# Random Forest Parameters\nrf_params = {\n    'n_jobs': -1,\n    'n_estimators': 250,\n    'max_depth': 6,\n    'min_samples_leaf': 20,\n    'max_features' : 'sqrt',\n    'verbose': 0\n}\n\n# Extra Trees Parameters\net_params = {\n    'n_jobs': -1,\n    'n_estimators': 250,\n    'max_depth': 8,\n    'min_samples_leaf': 20,\n    'verbose': 0\n}\n\n\n# AdaBoost parameters\nada_params = {\n    'n_estimators': 150,\n    'learning_rate' : 0.75\n}\n\n# Histogram Gradient Boosting parameters\nhgb_params = {\n    'max_iter': 250,\n    'max_depth': 5,\n    'min_samples_leaf': 2,\n    'verbose': 0\n}\n\n# Gaussian Naive-Bayes Classifier parameters (none needed)\ngnb_params = {}","metadata":{"execution":{"iopub.status.busy":"2021-09-25T20:46:19.227336Z","iopub.execute_input":"2021-09-25T20:46:19.228068Z","iopub.status.idle":"2021-09-25T20:46:19.238734Z","shell.execute_reply.started":"2021-09-25T20:46:19.228037Z","shell.execute_reply":"2021-09-25T20:46:19.237884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Level 1 Model Creation\nrf = SklearnHelper(clf=RandomForestClassifier, seed=SEED, params=rf_params)\nprint(\"RandomForest model created\")\net = SklearnHelper(clf=ExtraTreesClassifier, seed=SEED, params=et_params)\nprint(\"ExtraTrees model created\")\nada = SklearnHelper(clf=AdaBoostClassifier, seed=SEED, params=ada_params)\nprint(\"AdaBoost model created\")\nhgb = SklearnHelper(clf=HistGradientBoostingClassifier, seed=SEED, params=hgb_params)\nprint(\"HistGradientBoosting model created\")\ngnb = SklearnHelper(clf=GaussianNB, params=gnb_params)\nprint(\"GaussianNB model created\")","metadata":{"execution":{"iopub.status.busy":"2021-09-25T20:46:19.239663Z","iopub.execute_input":"2021-09-25T20:46:19.240192Z","iopub.status.idle":"2021-09-25T20:46:19.254823Z","shell.execute_reply.started":"2021-09-25T20:46:19.240131Z","shell.execute_reply":"2021-09-25T20:46:19.254026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Level 1 Model Training\nrf_oof_train, rf_oof_test = get_oof(rf, X, y, X_test) # Random Forest\nprint(\"Random Forest training done\")\net_oof_train, et_oof_test = get_oof(et, X, y, X_test) # Extra Trees\nprint(\"Extra Trees training done\")\nada_oof_train, ada_oof_test = get_oof(ada, X, y, X_test) # AdaBoost\nprint(\"AdaBoost training done\")\nhgb_oof_train, hgb_oof_test = get_oof(hgb, X, y, X_test) # Histogram Gradient Boost\nprint(\"HistGradientBoosting training done\")\ngnb_oof_train, gnb_oof_test = get_oof(gnb, X, y, X_test) # Gaussian Naive Bayes\nprint(\"GaussianNB training done\")","metadata":{"execution":{"iopub.status.busy":"2021-09-25T20:46:19.25592Z","iopub.execute_input":"2021-09-25T20:46:19.256245Z","iopub.status.idle":"2021-09-25T21:50:25.131411Z","shell.execute_reply.started":"2021-09-25T20:46:19.256207Z","shell.execute_reply":"2021-09-25T21:50:25.130295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Set-up for Level 2\nFrom here, we need to combine the dataframes into one larger dataframe (for each of train and test).","metadata":{}},{"cell_type":"code","source":"X_final = np.concatenate(( rf_oof_train, et_oof_train, ada_oof_train, hgb_oof_train, gnb_oof_train), axis=1)\nX_test_final = np.concatenate(( rf_oof_test, et_oof_test, ada_oof_test, hgb_oof_test, gnb_oof_test), axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-09-25T22:03:22.643349Z","iopub.execute_input":"2021-09-25T22:03:22.644109Z","iopub.status.idle":"2021-09-25T22:03:22.762042Z","shell.execute_reply.started":"2021-09-25T22:03:22.644077Z","shell.execute_reply":"2021-09-25T22:03:22.761436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Level 2\n### Final Model with XGBoost","metadata":{}},{"cell_type":"code","source":"stacked_model = XGBClassifier(\n    n_estimators= 2000,\n    objective= 'binary:logistic',\n    n_jobs = -1,\n    learning_rate = 0.01)","metadata":{"execution":{"iopub.status.busy":"2021-09-25T22:15:14.962145Z","iopub.execute_input":"2021-09-25T22:15:14.962974Z","iopub.status.idle":"2021-09-25T22:15:14.967281Z","shell.execute_reply.started":"2021-09-25T22:15:14.962937Z","shell.execute_reply":"2021-09-25T22:15:14.966445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stacked_model.fit(X_final, y)","metadata":{"execution":{"iopub.status.busy":"2021-09-25T22:15:17.41599Z","iopub.execute_input":"2021-09-25T22:15:17.416845Z","iopub.status.idle":"2021-09-25T22:18:25.955303Z","shell.execute_reply.started":"2021-09-25T22:15:17.416806Z","shell.execute_reply":"2021-09-25T22:18:25.954366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Final Training and Submission\nHere, I'll do the final model training and the submission.","metadata":{}},{"cell_type":"code","source":"predictions = stacked_model.predict(X_test_final)","metadata":{"execution":{"iopub.status.busy":"2021-09-25T22:18:25.956545Z","iopub.execute_input":"2021-09-25T22:18:25.956957Z","iopub.status.idle":"2021-09-25T22:18:29.364493Z","shell.execute_reply.started":"2021-09-25T22:18:25.956928Z","shell.execute_reply":"2021-09-25T22:18:29.363757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.read_csv(\"../input/tabular-playground-series-sep-2021/sample_solution.csv\")\nsubmission.claim = predictions\nsubmission.to_csv(\"submission.csv\", index=False, header=True)\nprint(\"Final submission created!\")","metadata":{"execution":{"iopub.status.busy":"2021-09-25T22:18:29.365598Z","iopub.execute_input":"2021-09-25T22:18:29.365807Z","iopub.status.idle":"2021-09-25T22:18:29.994208Z","shell.execute_reply.started":"2021-09-25T22:18:29.365784Z","shell.execute_reply":"2021-09-25T22:18:29.993565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## References\nI referred to the [Introduction to Ensembling/Stacking in Python](https://www.kaggle.com/arthurtok/introduction-to-ensembling-stacking-in-python) notebook by Anisotropic for the SklearnHelper class and the get_oof function (for out-of-fold predictions).","metadata":{}}]}