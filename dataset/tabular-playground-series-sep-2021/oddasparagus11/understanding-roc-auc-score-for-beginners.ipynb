{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-02T10:06:22.905205Z","iopub.execute_input":"2021-09-02T10:06:22.905641Z","iopub.status.idle":"2021-09-02T10:06:22.919599Z","shell.execute_reply.started":"2021-09-02T10:06:22.905556Z","shell.execute_reply":"2021-09-02T10:06:22.918597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Contents\n\n1. Confusion Matrix\n2. ROC curves\n3. Intepretations from ROC curve\n\nROC and Precision Recall curves are tools that helps in the interpretation of how our classfier model is working. ROC stands for `Reciever Operating Characterestic` curves. Generally ROC is used for datasets with balanced data and precision recall curve when we have imbalanced dataset.\n\nFor understanding how ROC and Precision Recall Curves work, first you need to understand about `Confusion Matrix`.\n\n## Confusion Matrix\n\nConfusion matrix also refrerred to as error matrix, allows visualization of the performance of algorithm. It shows the way in which our model is actually confused. It segregates the error into 4 classifications - True Positive, True Negative, False Positive, False Negative.\n\nThis is how a confusion matrix generally looks like:\n\n![](https://miro.medium.com/max/1000/1*fxiTNIgOyvAombPJx5KGeA.png)\n\nThe left side part shows the predicted class and the top side part shows the actual class of the datapoint.\n\n* TP means the values that you predicted to be `Positive ` and it is ` Positive`.\n\n* TN means the values that you predicted to be ` Negative` and it's `Negative`.\n\n* FP means the values that you predicted `Positive ` but it's `Negative `. This is also referred to as `Type 1 Error`.\n\n* FN means the values that you predicted `Negative ` but it's `Positive` . This is also referred to as ` Type 2 Error`.\n\nROC and Precision Recall curves are used to get more insights from this classification matrix. Now first let's discuss about ROC curves.\n\n\n## ROC curves\n\nA receiver operating characteristic curve, or ROC curve, is a graphical plot that illustrates the diagnostic ability of a binary classifier across different threshold values.\n\nIt helps in understanding the trade off between `true positive rate ` and `false positive rate ` across different threshold values. \n\nThe formulas for `true positive rate ` and `true negative rate ` is given by:","metadata":{"execution":{"iopub.status.busy":"2021-09-02T10:06:38.449259Z","iopub.execute_input":"2021-09-02T10:06:38.449792Z","iopub.status.idle":"2021-09-02T10:06:38.459184Z","shell.execute_reply.started":"2021-09-02T10:06:38.449758Z","shell.execute_reply":"2021-09-02T10:06:38.45782Z"}}},{"cell_type":"markdown","source":"![](https://www.researchgate.net/profile/Md-Ashraful-Amin/publication/220176738/figure/fig4/AS:669969142534168@1536744499664/The-confusion-matrix-left-and-the-calculation-of-true-positive-rate-false-positive.png)\n\nFor understanding the concept behind this, you can see like this:\n\n`True Positive Rate ` gives us information about how good the model is predicting when the actual value is positive. `TPR ` is also referred to as sensitivity in some cases.\n\n`False Positive Rate ` gives us information about how often a class is predicted as positive when it is actually negative. `FPR ` is referred to as ` 1 - specificity ` .\n\nSo basically what ROC curve will contain is \n\n1. `False Positive Rate ` on x-axis\n2. `True Positive Rate ` on y-axis\n3. Variation of Probability Threshold(0-1)\n\n## Intepretations from ROC curve\n\n1. ROC curve shows us the tradeoff between sensitivity and specificity. \n\n2. Classifiers that give curves closer to the top-left corner indicate a better performance.\n\n3. Smaller x-axis on the ROC plot means we have low false positive values and high true negative values.\n\n4. Higher values on y-axis means we have high true positive values and lower false negative values.\n\n5. AUC(Area under ROC curve) is used to summarize the performance of the classification model. It is equivalent to the probability that a randomly chosen positive instance is ranked higher than a randomly chosen negative instance.\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}