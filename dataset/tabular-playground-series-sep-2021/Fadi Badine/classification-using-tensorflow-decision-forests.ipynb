{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-01T13:26:20.495222Z","iopub.execute_input":"2021-10-01T13:26:20.49553Z","iopub.status.idle":"2021-10-01T13:26:20.521986Z","shell.execute_reply.started":"2021-10-01T13:26:20.495447Z","shell.execute_reply":"2021-10-01T13:26:20.521197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FILE_PATH = \"/kaggle/input/tabular-playground-series-sep-2021/\"\n\ntrain_file_path = os.path.join(FILE_PATH, \"train.csv\")\ntest_file_path  = os.path.join(FILE_PATH, \"test.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-10-01T13:26:58.45087Z","iopub.execute_input":"2021-10-01T13:26:58.451448Z","iopub.status.idle":"2021-10-01T13:26:58.455047Z","shell.execute_reply.started":"2021-10-01T13:26:58.451417Z","shell.execute_reply":"2021-10-01T13:26:58.454504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this notebook, we will use TensorFlow Decision Forests `GradientBoostedTreesModel` in order to create a classification model that can achieve good results.\n\nWe will use 2 preprocessing methods:\n* On the training and validation dataframes directly\n* Creating a Keras model that we will pass to the GradientBoostedTreesModel preprocessing parameter\n\nBoth methods achieved almost the same result on the validation set\n\nSo, let's start ... first let's install `tensorflow_decision_forests`","metadata":{}},{"cell_type":"code","source":"!pip install tensorflow_decision_forests","metadata":{"execution":{"iopub.status.busy":"2021-10-01T13:27:02.381148Z","iopub.execute_input":"2021-10-01T13:27:02.381964Z","iopub.status.idle":"2021-10-01T13:28:25.139842Z","shell.execute_reply.started":"2021-10-01T13:27:02.381932Z","shell.execute_reply":"2021-10-01T13:28:25.138723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Imports & Configuration","metadata":{}},{"cell_type":"code","source":"import os\nimport sys\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow_decision_forests as tfdf\n\nfrom tensorflow import keras\n\nimport matplotlib.pyplot as plt\n\nprint(\"TensorFlow Version: {}\".format(tf.__version__))\nprint(\"TensorFlow Decision Forests: {}\".format(tfdf.__version__))","metadata":{"execution":{"iopub.status.busy":"2021-10-01T13:36:32.505198Z","iopub.execute_input":"2021-10-01T13:36:32.507149Z","iopub.status.idle":"2021-10-01T13:36:35.354763Z","shell.execute_reply.started":"2021-10-01T13:36:32.50709Z","shell.execute_reply":"2021-10-01T13:36:35.353925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.random.seed(1337)\ntf.random.set_seed(1337)\n\nVALID_RATIO = 0.1","metadata":{"execution":{"iopub.status.busy":"2021-10-01T13:36:39.746694Z","iopub.execute_input":"2021-10-01T13:36:39.746962Z","iopub.status.idle":"2021-10-01T13:36:39.752909Z","shell.execute_reply.started":"2021-10-01T13:36:39.746931Z","shell.execute_reply":"2021-10-01T13:36:39.751926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Loading","metadata":{}},{"cell_type":"code","source":"train_full_data = pd.read_csv(train_file_path)\nprint(\"Full train dataset shape is {}\".format(train_full_data.shape))","metadata":{"execution":{"iopub.status.busy":"2021-10-01T13:38:27.865639Z","iopub.execute_input":"2021-10-01T13:38:27.865949Z","iopub.status.idle":"2021-10-01T13:38:55.198816Z","shell.execute_reply.started":"2021-10-01T13:38:27.865923Z","shell.execute_reply":"2021-10-01T13:38:55.198129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_full_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-01T13:39:59.64995Z","iopub.execute_input":"2021-10-01T13:39:59.650787Z","iopub.status.idle":"2021-10-01T13:39:59.692743Z","shell.execute_reply.started":"2021-10-01T13:39:59.650744Z","shell.execute_reply":"2021-10-01T13:39:59.692209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The data is composed of 120 columns all of which are numerical:\n* 118 feature columns named `f1, f2, ... f118`\n* label column named `claim`\n* An `id` column that we will drop","metadata":{}},{"cell_type":"code","source":"train_full_data = train_full_data.drop('id', axis=1)\nfeatures = [f'f{i}' for i in range(1, 119)]\nlabel = 'claim'","metadata":{"execution":{"iopub.status.busy":"2021-10-01T13:40:10.887049Z","iopub.execute_input":"2021-10-01T13:40:10.887869Z","iopub.status.idle":"2021-10-01T13:40:11.193671Z","shell.execute_reply.started":"2021-10-01T13:40:10.887823Z","shell.execute_reply":"2021-10-01T13:40:11.192466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check if we have missing data","metadata":{}},{"cell_type":"code","source":"train_full_data[features].isna().sum()","metadata":{"execution":{"iopub.status.busy":"2021-10-01T13:40:15.035142Z","iopub.execute_input":"2021-10-01T13:40:15.035444Z","iopub.status.idle":"2021-10-01T13:40:15.666247Z","shell.execute_reply.started":"2021-10-01T13:40:15.035412Z","shell.execute_reply":"2021-10-01T13:40:15.665412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that the data contains a lot of missing values. Approximately 15000 for each feature. That's around 1.5%\n\nIn the approach that we will use in this notebook, we will keep the missing values but will add 3 additional features:\n* `Number of missing values` in each sample. So for each sample out of the 957919, we will see how many values are missing across all features\n* `Standard deviation` over axis=1 which gives us the standard deviation for each sample\n* `Unbiased Variance` over axis=1 which gives us the variance for each sample\n\nWe will be implementing this preprocessing using 2 methods as I said before.\nLet's start with the first one","metadata":{}},{"cell_type":"code","source":"def split_dataset(dataset, test_ratio=0.1):\n    test_indices = np.random.rand(len(dataset)) < test_ratio\n    return dataset[~test_indices], dataset[test_indices]","metadata":{"execution":{"iopub.status.busy":"2021-10-01T13:41:42.112798Z","iopub.execute_input":"2021-10-01T13:41:42.113124Z","iopub.status.idle":"2021-10-01T13:41:42.118492Z","shell.execute_reply.started":"2021-10-01T13:41:42.113092Z","shell.execute_reply":"2021-10-01T13:41:42.117452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# First Approach: Preprocessing using pandas","metadata":{}},{"cell_type":"markdown","source":"## Preprocessing\n\nLet's add the 3 additional features using pandas","metadata":{}},{"cell_type":"code","source":"train_full_data['nan'] = train_full_data[features].isnull().sum(axis=1)\ntrain_full_data['std'] = train_full_data[features].std(axis=1)\ntrain_full_data['var'] = train_full_data[features].var(axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-10-01T13:41:45.217477Z","iopub.execute_input":"2021-10-01T13:41:45.217727Z","iopub.status.idle":"2021-10-01T13:41:48.50402Z","shell.execute_reply.started":"2021-10-01T13:41:45.217702Z","shell.execute_reply":"2021-10-01T13:41:48.503204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Datasets\n\nSplit the dataframe into training and validation sets","metadata":{}},{"cell_type":"code","source":"train_ds_pd, valid_ds_pd = split_dataset(train_full_data, test_ratio=VALID_RATIO)\nprint(\"{} samples in training and {} in validation\".format(train_ds_pd.shape[0], valid_ds_pd.shape[0]))","metadata":{"execution":{"iopub.status.busy":"2021-10-01T13:41:52.281595Z","iopub.execute_input":"2021-10-01T13:41:52.281912Z","iopub.status.idle":"2021-10-01T13:41:53.382275Z","shell.execute_reply.started":"2021-10-01T13:41:52.281879Z","shell.execute_reply":"2021-10-01T13:41:53.381426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create the training and validation datasets using TensorFlow Decision Forests `pd_dataframe_to_tf_dataset`","metadata":{}},{"cell_type":"code","source":"train_ds = tfdf.keras.pd_dataframe_to_tf_dataset(train_ds_pd, label=label)\nvalid_ds = tfdf.keras.pd_dataframe_to_tf_dataset(valid_ds_pd, label=label)","metadata":{"execution":{"iopub.status.busy":"2021-10-01T13:41:55.885035Z","iopub.execute_input":"2021-10-01T13:41:55.885817Z","iopub.status.idle":"2021-10-01T13:41:57.407627Z","shell.execute_reply.started":"2021-10-01T13:41:55.885785Z","shell.execute_reply":"2021-10-01T13:41:57.406757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## GradientBoostedTreesModel Training","metadata":{}},{"cell_type":"markdown","source":"For hyperparameter tuning, we did the following:\n* Tried the predefined hyperparameters which did not give good results especially `benchmark_rank1` which gave very bad results. `better_default` on the other hand gave acceptable results\n* Used Keras tuner in order to search for hyperparameters that maximise AUC. If you want to check how to do this, check out the following [notebook](https://www.kaggle.com/ekaterinadranitsyna/kerastuner-tf-decision-forest?linkId=133421702) by Ekaterina Dranitsyna\n\n**Note:** The idea of adding 3 new columns is taken also from the above mentioned notebook by Ekaterina Dranitsyna\n\nFinally the hyperparameters that gave me the best results where the below which are `better_default` with `l1_regularization`","metadata":{}},{"cell_type":"code","source":"model_1 = tfdf.keras.GradientBoostedTreesModel(\n    growing_strategy = 'BEST_FIRST_GLOBAL',\n    l1_regularization = 0.8\n)\n\nmodel_1.compile(metrics=[keras.metrics.AUC()])","metadata":{"execution":{"iopub.status.busy":"2021-10-01T13:44:16.285271Z","iopub.execute_input":"2021-10-01T13:44:16.285983Z","iopub.status.idle":"2021-10-01T13:44:16.320865Z","shell.execute_reply.started":"2021-10-01T13:44:16.285928Z","shell.execute_reply":"2021-10-01T13:44:16.319969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You can now run the below cell and go prepare a cup of ☕ ... it will take around 30 minutes to finish.","metadata":{}},{"cell_type":"code","source":"%%time\nmodel_1.fit(train_ds, verbose=0)","metadata":{"execution":{"iopub.status.busy":"2021-10-01T13:44:20.442268Z","iopub.execute_input":"2021-10-01T13:44:20.443053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Post Training Analysis","metadata":{}},{"cell_type":"markdown","source":"`model.summary()` shows us the overall structure of the model","metadata":{}},{"cell_type":"code","source":"model_1.summary()","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:57:16.164292Z","iopub.execute_input":"2021-09-30T18:57:16.164688Z","iopub.status.idle":"2021-09-30T18:57:16.173211Z","shell.execute_reply.started":"2021-09-30T18:57:16.164652Z","shell.execute_reply":"2021-09-30T18:57:16.172265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can access all this information using the model inspector","metadata":{}},{"cell_type":"code","source":"inspector = model_1.make_inspector()","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:57:47.231741Z","iopub.execute_input":"2021-09-30T18:57:47.232071Z","iopub.status.idle":"2021-09-30T18:57:47.333276Z","shell.execute_reply.started":"2021-09-30T18:57:47.232039Z","shell.execute_reply":"2021-09-30T18:57:47.332583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Model contains {} trees\".format(inspector.num_trees()))","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:59:00.755783Z","iopub.execute_input":"2021-09-30T18:59:00.756586Z","iopub.status.idle":"2021-09-30T18:59:00.761704Z","shell.execute_reply.started":"2021-09-30T18:59:00.756543Z","shell.execute_reply":"2021-09-30T18:59:00.760783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inspector.features()","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:59:09.189109Z","iopub.execute_input":"2021-09-30T18:59:09.18977Z","iopub.status.idle":"2021-09-30T18:59:09.200459Z","shell.execute_reply.started":"2021-09-30T18:59:09.18973Z","shell.execute_reply":"2021-09-30T18:59:09.199505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inspector.variable_importances()","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:59:14.939714Z","iopub.execute_input":"2021-09-30T18:59:14.940006Z","iopub.status.idle":"2021-09-30T18:59:14.973799Z","shell.execute_reply.started":"2021-09-30T18:59:14.939976Z","shell.execute_reply":"2021-09-30T18:59:14.972883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training Logs","metadata":{}},{"cell_type":"code","source":"logs = inspector.training_logs()\n\nplt.figure(figsize=(12, 4))\nplt.subplot(1, 2, 1)\nplt.plot([log.num_trees for log in logs], [log.evaluation.accuracy for log in logs])\nplt.xlabel('Number of trees')\nplt.ylabel('Accuracy (out-of-bag)')\nplt.subplot(1, 2, 2)\nplt.plot([log.num_trees for log in logs], [log.evaluation.loss for log in logs])\nplt.xlabel('Number of trees')\nplt.ylabel('Logloss (out-of-bag)')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:59:26.886959Z","iopub.execute_input":"2021-09-30T18:59:26.887811Z","iopub.status.idle":"2021-09-30T18:59:27.51457Z","shell.execute_reply.started":"2021-09-30T18:59:26.887761Z","shell.execute_reply":"2021-09-30T18:59:27.513678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation\n\nLet's evaluate the model using the validation dataset","metadata":{}},{"cell_type":"code","source":"evaluation = model_1.evaluate(valid_ds, return_dict = True)\nfor name, value in evaluation.items():\n    print(\"{}: {}\".format(name, value))","metadata":{"execution":{"iopub.status.busy":"2021-09-30T19:00:24.858221Z","iopub.execute_input":"2021-09-30T19:00:24.858565Z","iopub.status.idle":"2021-09-30T19:00:46.273417Z","shell.execute_reply.started":"2021-09-30T19:00:24.858532Z","shell.execute_reply":"2021-09-30T19:00:46.272402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model achieved `AUC = 0.8160` on the validation dataset\n\nLet's now look at our second approach","metadata":{}},{"cell_type":"markdown","source":"# Second Approach: TensorFlow based preprocessing\n\nIn this second approach, we will add the 3 columns that we added using pandas but rather through TensorFlow / Keras preprocessing that we will pass to the model via the preprocessing parameter.\n\nYou will notice that this approach is more difficult, more complicated, however it has the advantage of having the preprocessing within the model itself.","metadata":{}},{"cell_type":"code","source":"train_full_data = pd.read_csv(train_file_path)","metadata":{"execution":{"iopub.status.busy":"2021-09-30T20:28:47.903942Z","iopub.execute_input":"2021-09-30T20:28:47.904768Z","iopub.status.idle":"2021-09-30T20:29:09.751208Z","shell.execute_reply.started":"2021-09-30T20:28:47.904714Z","shell.execute_reply":"2021-09-30T20:29:09.750482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_full_data = train_full_data.drop('id', axis=1)\nfeatures = [f'f{i}' for i in range(1, 119)]\nlabel = 'claim'","metadata":{"execution":{"iopub.status.busy":"2021-09-30T20:29:09.752676Z","iopub.execute_input":"2021-09-30T20:29:09.75346Z","iopub.status.idle":"2021-09-30T20:29:10.106326Z","shell.execute_reply.started":"2021-09-30T20:29:09.75342Z","shell.execute_reply":"2021-09-30T20:29:10.104977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Datasets","metadata":{}},{"cell_type":"code","source":"train_ds_pd, valid_ds_pd = split_dataset(train_full_data, test_ratio=VALID_RATIO)\nprint(\"{} samples in training and {} in validation\".format(train_ds_pd.shape[0], valid_ds_pd.shape[0]))","metadata":{"execution":{"iopub.status.busy":"2021-09-30T20:29:43.315367Z","iopub.execute_input":"2021-09-30T20:29:43.315669Z","iopub.status.idle":"2021-09-30T20:29:43.793969Z","shell.execute_reply.started":"2021-09-30T20:29:43.31564Z","shell.execute_reply":"2021-09-30T20:29:43.793034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds = tfdf.keras.pd_dataframe_to_tf_dataset(train_ds_pd, label=label)\nvalid_ds = tfdf.keras.pd_dataframe_to_tf_dataset(valid_ds_pd, label=label)","metadata":{"execution":{"iopub.status.busy":"2021-09-30T20:29:47.766845Z","iopub.execute_input":"2021-09-30T20:29:47.767507Z","iopub.status.idle":"2021-09-30T20:29:49.58518Z","shell.execute_reply.started":"2021-09-30T20:29:47.767465Z","shell.execute_reply":"2021-09-30T20:29:49.584234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing","metadata":{}},{"cell_type":"code","source":"def reduce_mean_without_nan(input_tensor):\n    return tf.experimental.numpy.nanmean(input_tensor, axis=1, keepdims=True)\n\ndef get_nan_std_var(input_tensor):\n    # nan tensor\n    is_nan = tf.math.is_nan(input_tensor)\n    \n    # Get the number of nans available in each sample\n    nan_number_per_sample = tf.cast(\n        tf.math.reduce_sum(\n            tf.where(is_nan, [1], [0]),\n            axis=1,\n            keepdims=True\n        ),\n        tf.float32\n    )\n    \n    # Calculate mean excluding nan\n    mean_excluding_nan = keras.layers.Lambda(reduce_mean_without_nan)(input_tensor)\n    \n    # input tensor replacing nan with the mean for each row (i.e. for each sample)\n    input_tensor_with_nan_replaced_by_mean = tf.where(\n        is_nan,\n        mean_excluding_nan,\n        input_tensor\n    )\n    \n    squared_distance_from_mean = tf.math.reduce_sum(\n        tf.math.square(\n            input_tensor_with_nan_replaced_by_mean - mean_excluding_nan,\n        ),\n        axis=1,\n        keepdims=True\n    )\n    \n    # Calculate std\n    std = tf.math.sqrt(\n        tf.math.divide(\n            squared_distance_from_mean,\n            input_tensor.shape[1] - nan_number_per_sample,\n        )\n    )\n    \n    # Calculate var\n    var = tf.math.divide(\n        squared_distance_from_mean,\n        input_tensor.shape[1] - nan_number_per_sample - 1,\n    )\n    \n    stack =tf.stack([nan_number_per_sample, std, var], axis=1)\n    \n    return tf.squeeze(stack, axis=-1)","metadata":{"execution":{"iopub.status.busy":"2021-09-30T20:29:52.298519Z","iopub.execute_input":"2021-09-30T20:29:52.298834Z","iopub.status.idle":"2021-09-30T20:29:52.30979Z","shell.execute_reply.started":"2021-09-30T20:29:52.2988Z","shell.execute_reply":"2021-09-30T20:29:52.308752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_preprocessing_model(features):\n    # Create inputs\n    input_layers = []\n\n    # Each feature will be one input\n    for feature in features:\n        input_layers.append(keras.layers.Input(shape=(1,), name=feature))\n    \n    # Concatenate all inputs\n    inputs = keras.layers.concatenate(input_layers, name=\"inputs\")\n        \n    \n    # Add 3 additional features:\n    # - How many nan are they in each sample\n    # - std accross the features of each sample\n    # - var accross the features of each sample\n    additional_features = get_nan_std_var(inputs)\n    \n    outputs = keras.layers.concatenate([inputs, additional_features])\n    \n    return keras.Model(input_layers, outputs)\n\npreprocessing_model = build_preprocessing_model(features)\npreprocessing_model.summary()","metadata":{"execution":{"iopub.status.busy":"2021-09-30T20:29:54.041012Z","iopub.execute_input":"2021-09-30T20:29:54.041308Z","iopub.status.idle":"2021-09-30T20:29:54.367014Z","shell.execute_reply.started":"2021-09-30T20:29:54.041278Z","shell.execute_reply":"2021-09-30T20:29:54.365912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## GradientBoostedTreesModel Training","metadata":{}},{"cell_type":"markdown","source":"Let's first define the features that we will be using ","metadata":{}},{"cell_type":"code","source":"tfdf_features = []\n\nfor feature in features:\n    print(\"Creating FeatureUsage for {}\".format(feature))\n    tfdf_features.append(tfdf.keras.FeatureUsage(name=feature))","metadata":{"execution":{"iopub.status.busy":"2021-09-30T19:57:38.792479Z","iopub.execute_input":"2021-09-30T19:57:38.79359Z","iopub.status.idle":"2021-09-30T19:57:38.814727Z","shell.execute_reply.started":"2021-09-30T19:57:38.79353Z","shell.execute_reply":"2021-09-30T19:57:38.813604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_2 = tfdf.keras.GradientBoostedTreesModel(\n    growing_strategy = 'BEST_FIRST_GLOBAL',\n    l1_regularization = 0.6,\n    preprocessing = preprocessing_model\n)\n\nmodel_2.compile(metrics=[keras.metrics.AUC()])","metadata":{"execution":{"iopub.status.busy":"2021-09-30T20:30:34.488716Z","iopub.execute_input":"2021-09-30T20:30:34.489057Z","iopub.status.idle":"2021-09-30T20:30:34.515604Z","shell.execute_reply.started":"2021-09-30T20:30:34.489025Z","shell.execute_reply":"2021-09-30T20:30:34.51462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nmodel_2.fit(train_ds, verbose=0)","metadata":{"execution":{"iopub.status.busy":"2021-09-30T20:30:43.810057Z","iopub.execute_input":"2021-09-30T20:30:43.810929Z","iopub.status.idle":"2021-09-30T20:53:09.732735Z","shell.execute_reply.started":"2021-09-30T20:30:43.810888Z","shell.execute_reply":"2021-09-30T20:53:09.731808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Post Training Analysis","metadata":{}},{"cell_type":"code","source":"model_2.summary()","metadata":{"execution":{"iopub.status.busy":"2021-09-30T20:54:15.282859Z","iopub.execute_input":"2021-09-30T20:54:15.283236Z","iopub.status.idle":"2021-09-30T20:54:15.295322Z","shell.execute_reply.started":"2021-09-30T20:54:15.283198Z","shell.execute_reply":"2021-09-30T20:54:15.294433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inspector = model_2.make_inspector()","metadata":{"execution":{"iopub.status.busy":"2021-09-30T20:54:29.761399Z","iopub.execute_input":"2021-09-30T20:54:29.761695Z","iopub.status.idle":"2021-09-30T20:54:29.781579Z","shell.execute_reply.started":"2021-09-30T20:54:29.761667Z","shell.execute_reply":"2021-09-30T20:54:29.78053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Model contains {} trees\".format(inspector.num_trees()))","metadata":{"execution":{"iopub.status.busy":"2021-09-30T20:54:31.422175Z","iopub.execute_input":"2021-09-30T20:54:31.422513Z","iopub.status.idle":"2021-09-30T20:54:31.428173Z","shell.execute_reply.started":"2021-09-30T20:54:31.422482Z","shell.execute_reply":"2021-09-30T20:54:31.427062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inspector.features()","metadata":{"execution":{"iopub.status.busy":"2021-09-30T20:54:35.680216Z","iopub.execute_input":"2021-09-30T20:54:35.680537Z","iopub.status.idle":"2021-09-30T20:54:35.689724Z","shell.execute_reply.started":"2021-09-30T20:54:35.680508Z","shell.execute_reply":"2021-09-30T20:54:35.688751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inspector.variable_importances()","metadata":{"execution":{"iopub.status.busy":"2021-09-30T20:54:43.374798Z","iopub.execute_input":"2021-09-30T20:54:43.375096Z","iopub.status.idle":"2021-09-30T20:54:43.408073Z","shell.execute_reply.started":"2021-09-30T20:54:43.375069Z","shell.execute_reply":"2021-09-30T20:54:43.407482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training Logs","metadata":{}},{"cell_type":"code","source":"logs = inspector.training_logs()\n\nplt.figure(figsize=(12, 4))\nplt.subplot(1, 2, 1)\nplt.plot([log.num_trees for log in logs], [log.evaluation.accuracy for log in logs])\nplt.xlabel('Number of trees')\nplt.ylabel('Accuracy (out-of-bag)')\nplt.subplot(1, 2, 2)\nplt.plot([log.num_trees for log in logs], [log.evaluation.loss for log in logs])\nplt.xlabel('Number of trees')\nplt.ylabel('Logloss (out-of-bag)')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-30T20:54:53.03067Z","iopub.execute_input":"2021-09-30T20:54:53.030967Z","iopub.status.idle":"2021-09-30T20:54:53.409072Z","shell.execute_reply.started":"2021-09-30T20:54:53.030938Z","shell.execute_reply":"2021-09-30T20:54:53.407974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tensorboard","metadata":{}},{"cell_type":"code","source":"inspector.export_to_tensorboard(\"./model_2_logs\")","metadata":{"execution":{"iopub.status.busy":"2021-09-30T20:26:07.068667Z","iopub.execute_input":"2021-09-30T20:26:07.069395Z","iopub.status.idle":"2021-09-30T20:26:07.33641Z","shell.execute_reply.started":"2021-09-30T20:26:07.069359Z","shell.execute_reply":"2021-09-30T20:26:07.335695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%tensorboard --logdir \"./model_2_logs\"","metadata":{"execution":{"iopub.status.busy":"2021-09-30T20:26:19.757328Z","iopub.execute_input":"2021-09-30T20:26:19.758216Z","iopub.status.idle":"2021-09-30T20:26:26.943446Z","shell.execute_reply.started":"2021-09-30T20:26:19.758169Z","shell.execute_reply":"2021-09-30T20:26:26.94236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation","metadata":{}},{"cell_type":"code","source":"evaluation = model_2.evaluate(valid_ds, return_dict = True)\nfor name, value in evaluation.items():\n    print(\"{}: {}\".format(name, value))","metadata":{"execution":{"iopub.status.busy":"2021-09-30T20:55:01.604457Z","iopub.execute_input":"2021-09-30T20:55:01.604753Z","iopub.status.idle":"2021-09-30T20:55:20.792002Z","shell.execute_reply.started":"2021-09-30T20:55:01.604723Z","shell.execute_reply":"2021-09-30T20:55:20.791125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This model achieved a lower `AUC = 0.8111` on the validation dataset than the previous one which achieved `0.8160`\n\nThis model as you noticed created the additional features differently. While the pandas based method counts the nan values and calculates std and var on the whole training dataset, this model calculates and counts them for each batch.","metadata":{}},{"cell_type":"markdown","source":"# Test Set Prediction\nWe will use the 2nd approach for our prediction","metadata":{}},{"cell_type":"code","source":"model = model_2","metadata":{"execution":{"iopub.status.busy":"2021-09-30T20:55:56.483645Z","iopub.execute_input":"2021-09-30T20:55:56.484536Z","iopub.status.idle":"2021-09-30T20:55:56.487935Z","shell.execute_reply.started":"2021-09-30T20:55:56.484497Z","shell.execute_reply":"2021-09-30T20:55:56.487264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data = pd.read_csv(test_file_path)\nids = test_data.pop('id')","metadata":{"execution":{"iopub.status.busy":"2021-09-30T21:02:35.462176Z","iopub.execute_input":"2021-09-30T21:02:35.46256Z","iopub.status.idle":"2021-09-30T21:02:46.248863Z","shell.execute_reply.started":"2021-09-30T21:02:35.462522Z","shell.execute_reply":"2021-09-30T21:02:46.247883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if model == model_1:\n    test_data['nan'] = test_data[features].isnull().sum(axis=1)\n    test_data['std'] = test_data[features].std(axis=1)\n    test_data['var'] = test_data[features].var(axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_ds = tfdf.keras.pd_dataframe_to_tf_dataset(test_data)","metadata":{"execution":{"iopub.status.busy":"2021-09-30T21:02:51.289601Z","iopub.execute_input":"2021-09-30T21:02:51.289913Z","iopub.status.idle":"2021-09-30T21:02:52.28668Z","shell.execute_reply.started":"2021-09-30T21:02:51.28988Z","shell.execute_reply":"2021-09-30T21:02:52.285659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = model.predict(test_ds)","metadata":{"execution":{"iopub.status.busy":"2021-09-30T21:02:54.324484Z","iopub.execute_input":"2021-09-30T21:02:54.324782Z","iopub.status.idle":"2021-09-30T21:04:19.179929Z","shell.execute_reply.started":"2021-09-30T21:02:54.324754Z","shell.execute_reply":"2021-09-30T21:04:19.179052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output = pd.DataFrame({'id': ids,\n                       'claim': preds.squeeze()})\n\noutput.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-30T21:04:30.540015Z","iopub.execute_input":"2021-09-30T21:04:30.540959Z","iopub.status.idle":"2021-09-30T21:04:30.555102Z","shell.execute_reply.started":"2021-09-30T21:04:30.5409Z","shell.execute_reply":"2021-09-30T21:04:30.554196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_filename = \"test_prediction_output.csv\"\noutput.to_csv(output_filename, index=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-30T20:58:06.187737Z","iopub.execute_input":"2021-09-30T20:58:06.18804Z","iopub.status.idle":"2021-09-30T20:58:07.703469Z","shell.execute_reply.started":"2021-09-30T20:58:06.188011Z","shell.execute_reply":"2021-09-30T20:58:07.702584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# References\n\n\n*   [KerasTuner + TF Decision Forest](https://www.kaggle.com/ekaterinadranitsyna/kerastuner-tf-decision-forest?linkId=133421702) by [Ekaterina Dranitsyna](https://www.kaggle.com/ekaterinadranitsyna)\n*   [TensorFlow Decision Forests tutorials](https://www.tensorflow.org/decision_forests/tutorials) which are a set of 3 very interesting (beginner, intermediate and advanced levels) tutorials.\n*   The [TensorFlow Forum](https://discuss.tensorflow.org/) where one can get in touch with the TensorFlow community. Check it out if you haven't yet.\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}