{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Description <a name=\"introduction\"></a>\n\nBelow is my solution. I used LightGBM, initially I experimented with 3 different models.  Random Forest, the default GBDT , and Gradient based one sided sampling. GBDT, generated the best AUC followed by GOSS then Random Forest. Despite, GBDT yielding the best AUC I decided stack the 3 diverse and different models to archive a much high AUC. This second model used was a Logistic regression used to generate the submission predictions.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport sklearn.model_selection as ms\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nfrom lightgbm import LGBMClassifier\nfrom collections import namedtuple\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import SGDClassifier\n","metadata":{"execution":{"iopub.status.busy":"2021-09-27T20:39:12.691598Z","iopub.execute_input":"2021-09-27T20:39:12.692286Z","iopub.status.idle":"2021-09-27T20:39:15.168581Z","shell.execute_reply.started":"2021-09-27T20:39:12.692134Z","shell.execute_reply":"2021-09-27T20:39:15.167699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\nfolds = 10\ntrain = pd.read_csv(\"/kaggle/input/tabular-playground-series-sep-2021/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/tabular-playground-series-sep-2021/test.csv\")\nx = pd.DataFrame(train.drop(columns=[\"claim\", \"id\"]))\ny = train[\"claim\"]\n","metadata":{"execution":{"iopub.status.busy":"2021-09-27T20:41:24.492312Z","iopub.execute_input":"2021-09-27T20:41:24.493119Z","iopub.status.idle":"2021-09-27T20:41:55.834561Z","shell.execute_reply.started":"2021-09-27T20:41:24.493073Z","shell.execute_reply":"2021-09-27T20:41:55.833653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# x, weight_x, y, weight_y = ms.train_test_split(X, Y, test_size=.05, shuffle=True, random_state=0)\n\ntest = test\nxval = pd.DataFrame(test.drop(columns=[\"id\"]))\ndf_split = ms.StratifiedKFold(n_splits=folds, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T20:41:55.985823Z","iopub.execute_input":"2021-09-27T20:41:55.986073Z","iopub.status.idle":"2021-09-27T20:41:56.143693Z","shell.execute_reply.started":"2021-09-27T20:41:55.986038Z","shell.execute_reply":"2021-09-27T20:41:56.142637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Hyperparameters\nParameters were determined using randomized search and grid search. Below are the parameters for the 3 models. ","metadata":{}},{"cell_type":"code","source":"dt = namedtuple(\"dt\", \"model_ best_para\")\npara = []\n\npara.append(dt(model_=\"goss\", best_para={\"boosting_type\":\"goss\", \"objective\": \"cross_entropy\", \"n_estimators\": 878,\n                                         \"lambda_l1\": 0.02119367084330647, \n                                         \"lambda_l2\": 9.259284311814404e-05, \n                                         \"num_leaves\": 85, \"min_child_samples\": 42, \"verbose\" :-1}))\npara.append(dt(model_=\"rf\", best_para={\"boosting_type\":\"rf\",\"n_estimators\": 327,\n                                       \"lambda_l1\": 0.00012043760866269098, \n                                       \"lambda_l2\": 6.649019338833096e-06, \n                                       \"num_leaves\": 246, \"min_child_samples\": 99,\n                                       \"feature_fraction\": 0.7734184326473208,\n                                       \"bagging_fraction\": 0.999835036473764, \"bagging_freq\": 3, \"verbose\" :-1}))\npara.append(dt(model_=\"gbdt\", best_para={\"boosting_type\":\"gbdt\",\"n_estimators\": 499, \n                                         \"lambda_l1\": 1.0450194511913434e-06,\n                                         \"lambda_l2\": 2.2690854683431152e-07, \n                                         \"num_leaves\": 110, \"min_child_samples\": 14, \n                                         \"feature_fraction\": 0.7468626653258925,\n                                         \"bagging_fraction\": 0.9944777742119832,\n                                         \"bagging_freq\": 4, \"verbose\" :-1}))\n\npara = pd.DataFrame(para)\n\n# arrays to hold meta data, and weights\nmeta_val = np.zeros((len(xval.index) * len(para.index), folds))\nmeta_val_ave = np.zeros((len(xval.index), len(para.index)))\nval_len = len(xval.index)\n\n\ntrain_meta = np.zeros((len(x.index), len(para.index) + 1))\n","metadata":{"execution":{"iopub.status.busy":"2021-09-27T20:41:56.145474Z","iopub.execute_input":"2021-09-27T20:41:56.145908Z","iopub.status.idle":"2021-09-27T20:41:56.165391Z","shell.execute_reply.started":"2021-09-27T20:41:56.145873Z","shell.execute_reply":"2021-09-27T20:41:56.164457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Stage 1 Base models\nStage 1, using 9 folds to fit base models with the remaining fold to create meta data. Also, this last fold is used to generate meta predictions using each base model. Finally, the meta data is used to fit the meta model in stage two and the predictions are generated using the meta prediction from the base models.","metadata":{}},{"cell_type":"code","source":"\nstart = 0\nend = 0\nfor counter, (trn, val) in enumerate(df_split.split(x, y)):\n    end += len(val)\n    train_meta[start:end, 0] = y.iloc[val].values\n\n    for p in para.itertuples():\n        model = LGBMClassifier(n_jobs=-1, **p.best_para)\n        model.fit(x.iloc[trn, :], y.iloc[trn])\n        train_meta[start:end, p.Index + 1] = model.predict_proba(x.iloc[val, :])[:, 1]\n        meta_val[val_len * p.Index:val_len * (p.Index + 1), counter] = model.predict_proba(xval)[:, 1]\n    start +=len(val)\n\n    if counter == folds - 1:\n\n        for r in range(0,len(para.index)):\n            mv = meta_val[val_len * r:val_len * (r + 1),]\n            meta_val_ave[:, r] = np.mean(mv, axis=1)\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Stage 2 Meta Model\nMeta data is used to fit Logistics type model to meta data. Then prediction are made using the prediction data from the base models. ","metadata":{}},{"cell_type":"code","source":"\nmeta_model = SGDClassifier(max_iter=10000, loss='log')\nmeta_model.fit(train_meta[:, 1:], train_meta[:, 0])\npred = meta_model.predict_proba(meta_val_ave)[:, 1]\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfinal = pd.DataFrame(test[\"id\"])\nfinal = final.merge(pd.DataFrame(pred), right_index=True, left_index=True)\nfinal.columns = [\"id\", \"claim\"]\nfinal.to_csv(\"final.csv\", index=False)\n\n","metadata":{},"execution_count":null,"outputs":[]}]}