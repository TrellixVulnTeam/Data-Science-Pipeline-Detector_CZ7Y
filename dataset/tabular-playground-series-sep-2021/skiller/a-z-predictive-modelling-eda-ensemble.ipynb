{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-26T16:20:04.007405Z","iopub.execute_input":"2021-09-26T16:20:04.007669Z","iopub.status.idle":"2021-09-26T16:20:04.020529Z","shell.execute_reply.started":"2021-09-26T16:20:04.007643Z","shell.execute_reply":"2021-09-26T16:20:04.01975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Hi,\\\n**Welcome to my Notebook !!**.\\\nThe following notebook deals with basic EDA and model training.\n\n**Please leave an upvote or feedback, if you like or use at part of it. \\\nWould encourage me more to share.**\n\nI have obtained the optimised params with optuna using from the following notebook:-\\\nhttps://www.kaggle.com/skiller/best-params-detection-optuna \n\n\\\nBonus Notebooks\\\nIf you have interest in **NLP** do checkout my notebook on NER and text classification :-\n* https://www.kaggle.com/skiller/yes-torch-bert-finetuning-top-5\n* https://www.kaggle.com/skiller/ner-pytorch-masked-accuracy-and-loss\n\n\nWarm Regards","metadata":{}},{"cell_type":"code","source":"import os\nimport joblib\nimport numpy as np\nimport pandas as pd\nimport warnings\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib import ticker\nimport seaborn as sns\n\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2021-09-26T16:16:36.504836Z","iopub.execute_input":"2021-09-26T16:16:36.505028Z","iopub.status.idle":"2021-09-26T16:16:37.233819Z","shell.execute_reply.started":"2021-09-26T16:16:36.505006Z","shell.execute_reply":"2021-09-26T16:16:37.233117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load the Data","metadata":{}},{"cell_type":"code","source":"# import datasets\ntrain_df = pd.read_csv('/kaggle/input/tabular-playground-series-sep-2021/train.csv')\ntest_df = pd.read_csv('/kaggle/input/tabular-playground-series-sep-2021/test.csv')\nsubmission = pd.read_csv('/kaggle/input/tabular-playground-series-sep-2021/sample_solution.csv')","metadata":{"execution":{"iopub.status.busy":"2021-09-26T16:16:37.23505Z","iopub.execute_input":"2021-09-26T16:16:37.235332Z","iopub.status.idle":"2021-09-26T16:17:16.325352Z","shell.execute_reply.started":"2021-09-26T16:16:37.235298Z","shell.execute_reply":"2021-09-26T16:17:16.324503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Lets see data","metadata":{}},{"cell_type":"code","source":"train_df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing_train_df = pd.DataFrame(train_df.isna().sum(axis=0))\nmissing_train_df = missing_train_df.drop(['id', 'claim']).reset_index()\nmissing_train_df.columns = ['feature', 'count']\nmissing_train_df['count_percent'] = missing_train_df['count']/train_df.shape[0]\n\n\nmissing_test_df = pd.DataFrame(test_df.isna().sum())\nmissing_test_df = missing_test_df.drop(['id']).reset_index()\nmissing_test_df.columns = ['feature', 'count']\nmissing_test_df['count_percent'] = missing_test_df['count']/test_df.shape[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing_train_row = train_df.drop(['id', 'claim'], axis=1).isna().sum(axis=1)\nmissing_train_feature_numbers = pd.DataFrame(missing_train_row.value_counts()/train_df.shape[0]).reset_index()\nmissing_train_feature_numbers.columns = ['no_of_feature', 'count_percent']\n\nmissing_test_row = test_df.drop(['id'], axis=1).isna().sum(axis=1)\nmissing_test_feature_numbers = pd.DataFrame(missing_test_row.value_counts()/test_df.shape[0]).reset_index()\nmissing_test_feature_numbers.columns = ['no_of_feature', 'count_percent']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Lets see the visual tables quickly ","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(16, 16))\nax0_sns = sns.barplot(y=missing_train_df['feature'], x=missing_train_df['count_percent'], \n                      zorder=2, linewidth=0, orient='h', saturation=1, alpha=1)\nax0_sns.set_xlabel(\"missing values\", weight='bold')\nax0_sns.set_ylabel(\"features\", weight='bold')\nax0_sns.grid(which='major', axis='x', zorder=0, color='#EEEEEE')\nax0_sns.grid(which='major', axis='y', zorder=0, color='#EEEEEE')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"See all less than 2%","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(16, 16))\nax0_sns = sns.barplot(y=missing_train_feature_numbers['no_of_feature'], x=missing_train_feature_numbers['count_percent'], \n                      zorder=2, linewidth=0, orient='h', saturation=1, alpha=1)\nax0_sns.set_xlabel(\"missing values\", weight='bold')\nax0_sns.set_ylabel(\"features\", weight='bold')\nax0_sns.grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\nax0_sns.grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Well can't throw the null data more than 37% ","metadata":{}},{"cell_type":"markdown","source":"Lets check same for test","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(16, 16))\nax0_sns = sns.barplot(y=missing_test_df['feature'], x=missing_train_df['count_percent'], \n                      zorder=2, linewidth=0, orient='h', saturation=1, alpha=1)\nax0_sns.set_xlabel(\"missing values\", weight='bold')\nax0_sns.set_ylabel(\"features\", weight='bold')\nax0_sns.grid(which='major', axis='x', zorder=0, color='#EEEEEE')\nax0_sns.grid(which='major', axis='y', zorder=0, color='#EEEEEE')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(16, 16))\nax0_sns = sns.barplot(y=missing_test_feature_numbers['no_of_feature'], x=missing_test_feature_numbers['count_percent'], \n                      zorder=2, linewidth=0, orient='h', saturation=1, alpha=1)\nax0_sns.set_xlabel(\"missing values\", weight='bold')\nax0_sns.set_ylabel(\"features\", weight='bold')\nax0_sns.grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\nax0_sns.grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# A lot of missing values. Lets see the correlation","metadata":{}},{"cell_type":"markdown","source":"Lets see why people are obsessed with the null counts","metadata":{}},{"cell_type":"code","source":"train_df['num_nulls'] = train_df.drop(['id', 'claim'], axis = 1).isna().sum(axis = 1)\ntest_df['num_nulls'] = test_df.drop(['id'], axis = 1).isna().sum(axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['num_nulls'].corr(train_df['claim'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Damn!! thats a large correlation . Need to keep this factor. ","metadata":{}},{"cell_type":"markdown","source":"# Lets also look if we have imbalance case","metadata":{}},{"cell_type":"code","source":"train_df.claim.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Good to go. No Imbalance Class","metadata":{}},{"cell_type":"markdown","source":"# So now the work remaining is the removal of null values. (Also a bit of Preprocessing) \nBut we can't drop the rows owing to the large amount single null rows","metadata":{}},{"cell_type":"code","source":"# import packages\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom xgboost import XGBClassifier \nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\nfrom tqdm import tqdm\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import QuantileTransformer,  KBinsDiscretizer, RobustScaler\nfrom sklearn.impute import SimpleImputer","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nfeatures = [col for col in train_df.columns if col not in ['claim', 'id']]\npipe = Pipeline([\n        ('imputer', SimpleImputer(strategy='median',missing_values=np.nan)),\n        (\"scaler\", QuantileTransformer(n_quantiles=64,output_distribution='uniform')),\n        ('bin', KBinsDiscretizer(n_bins=64, encode='ordinal',strategy='uniform'))\n        ])\ntrain_df[features] = pipe.fit_transform(train_df[features])\ntest_df[features] = pipe.transform(test_df[features])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Lets Begin the training","metadata":{}},{"cell_type":"code","source":"N_SPLITS = 5\nEARLY_STOPPING_ROUNDS = 200\nSEED = 42\nTRAINING_METHODS = {\n    'XGB' : True,\n    'LGDM' : True,\n    'CAT' : True\n}","metadata":{"execution":{"iopub.status.busy":"2021-09-26T16:21:50.358655Z","iopub.execute_input":"2021-09-26T16:21:50.358917Z","iopub.status.idle":"2021-09-26T16:21:50.36368Z","shell.execute_reply.started":"2021-09-26T16:21:50.35889Z","shell.execute_reply":"2021-09-26T16:21:50.362796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Lets Define the training code","metadata":{}},{"cell_type":"code","source":"def cross_validate(\n    model,\n    train_df,\n    test_df,\n    early_stopping='True'\n):\n    train_oof = np.zeros(len(train_df))\n    predictions = np.zeros(len(test_df))\n\n    skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n\n    for fold, (train_idx, valid_idx) in tqdm(enumerate(skf.split(train_df.drop(['claim', 'id'],axis=1), train_df['claim']))):\n        X_train, X_valid = train_df.iloc[train_idx], train_df.iloc[valid_idx]\n        y_train = X_train['claim']\n        y_valid = X_valid['claim']\n        X_train = X_train.drop(['claim', 'id'], axis=1)\n        X_valid = X_valid.drop(['claim', 'id'], axis=1)\n        \n        if early_stopping:\n            model.fit(\n                X_train, \n                y_train,\n                eval_set=[(X_valid, y_valid)],\n                early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n                verbose=0\n            )\n        else:\n            model.fit(\n                X_train, \n                y_train\n            )\n         \n        temp_oof = model.predict_proba(X_valid)[:, 1]\n        train_oof[valid_idx] = temp_oof\n        print(f'Fold {fold} AUC: ', roc_auc_score(y_valid, temp_oof))\n        predictions += model.predict_proba(test_df)[:, -1] / N_SPLITS\n\n    print(f'OOF AUC: ', roc_auc_score(train_df['claim'], train_oof))\n    \n    return train_oof, predictions, model    ","metadata":{"execution":{"iopub.status.busy":"2021-09-26T16:21:03.030085Z","iopub.execute_input":"2021-09-26T16:21:03.030654Z","iopub.status.idle":"2021-09-26T16:21:03.04255Z","shell.execute_reply.started":"2021-09-26T16:21:03.03062Z","shell.execute_reply":"2021-09-26T16:21:03.041696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = test_df.drop('id', axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# XGBOOST","metadata":{}},{"cell_type":"code","source":"xgb_params = {\n        'n_estimators': 16939, \n        'learning_rate': 0.1876042995729744, \n        'subsample': 0.9947704250490819, \n        'colsample_bytree': 0.714913373260802, \n        'max_depth': 1, \n        'min_child_weight': 300, \n        'reg_lambda': 2.520228860596293e-05, \n        'reg_alpha': 0.00045044167069949973,\n        'tree_method': 'gpu_hist'\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nif TRAINING_METHODS['XGB']:\n    xg_train_oof, xg_predictions, model = cross_validate(\n                                            XGBClassifier(**xgb_params),\n                                        train_df,\n                                        test_df,\n                                    )\n    model.save_model('xgb_model')\n    np.save('xg_train_oof', xg_train_oof)\n    np.save('xg_predictions', xg_predictions)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LGDM CLASSIFIER","metadata":{}},{"cell_type":"code","source":"lgb_params = {\n            'n_estimators': 12000, \n            'learning_rate': 0.027934730713420564, \n            'reg_alpha': 1.1799328678792862e-05, \n            'reg_lambda': 0.38585046073832296, \n            'num_leaves': 23, \n            'feature_fraction': 0.5301717514985537, \n            'bagging_fraction': 0.7745063435612487, \n            'bagging_freq': 6, \n            'min_child_samples': 19, \n            'min_child_weight': 193, \n            'colsample_bytree': 0.5145963018815463,\n            \"objective\": \"binary\",\n            \"metric\": \"binary_logloss\",\n            \"boosting_type\": \"gbdt\",\n            \"device_type\" : \"gpu\"\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nif TRAINING_METHODS['LGDM']:\n    lgd_train_oof, lgd_predictions, model = cross_validate(\n                                            LGBMClassifier(**lgb_params),\n                                        train_df,\n                                        test_df,\n                                    )\n    np.save('lgd_train_oof', lgd_train_oof)\n    np.save('lgd_predictions', lgd_predictions)\n    model.booster_.save_model('lgdm_model', num_iteration=model.best_iteration_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CATBOOST","metadata":{}},{"cell_type":"code","source":"cat_params = {\n        'iterations': 12000, \n        'objective': 'Logloss', \n        'bootstrap_type': 'Bayesian', \n        'od_wait': 1491, \n        'learning_rate': 0.07733510576652604, \n        'reg_lambda': 6.067283648607877, \n        'random_strength': 19.03761597798964, \n        'depth': 4, \n        'min_data_in_leaf': 17, \n        'leaf_estimation_iterations': 8, \n        'bagging_temperature': 0.7761781866167776,\n        'task_type' : 'GPU'\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nif TRAINING_METHODS['LGDM']:\n    cat_train_oof, cat_predictions, model = cross_validate(\n                                            CatBoostClassifier(**cat_params),\n                                        train_df,\n                                        test_df,\n                                    )\n    np.save('cat_train_oof', cat_train_oof)\n    np.save('cat_predictions', cat_predictions)\n    model.save_model('cat_boost')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Lets stack them for Ensemble modelling","metadata":{}},{"cell_type":"markdown","source":"Create a dataframe for final ensemble input","metadata":{}},{"cell_type":"code","source":"cols = [\"lgb\", \"xgb\", \"cat\"]\ndf_oof = pd.DataFrame(\n    dict(\n        zip(cols, [lgd_train_oof, xg_train_oof, cat_train_oof])\n    )\n)\ndf_pred = pd.DataFrame(\n    dict(\n        zip(cols, [lgd_predictions, xg_predictions, cat_predictions])\n    )\n)","metadata":{"execution":{"iopub.status.busy":"2021-09-26T16:20:58.891762Z","iopub.execute_input":"2021-09-26T16:20:58.892014Z","iopub.status.idle":"2021-09-26T16:20:58.906939Z","shell.execute_reply.started":"2021-09-26T16:20:58.891989Z","shell.execute_reply":"2021-09-26T16:20:58.906225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_oof['claim'] = train_df['claim']\ndf_oof['id'] = df_oof.index","metadata":{"execution":{"iopub.status.busy":"2021-09-26T16:21:24.734878Z","iopub.execute_input":"2021-09-26T16:21:24.735171Z","iopub.status.idle":"2021-09-26T16:21:24.748561Z","shell.execute_reply.started":"2021-09-26T16:21:24.735135Z","shell.execute_reply":"2021-09-26T16:21:24.7479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train the Multiple ensemble model","metadata":{}},{"cell_type":"code","source":"# params = {\"objective\": \"binary\", \"metric\": \"binary_logloss\", \"random_state\": SEED, \"device_type\" : \"gpu\", 'verbose':0, \"n_estimators\" : 2000} \n# oof_lgb2, pred_lgb2, _ = cross_validate(\n#     LGBMClassifier(**params),\n#     df_oof,\n#     df_pred\n# )\n\n# params = {\"objective\": \"binary:logistic\", \"random_state\": SEED, 'tree_method': 'gpu_hist', 'verbose':0, 'n_estimators': 2000}\n# oof_xgb2, pred_xgb2, _ = cross_validate(\n#     XGBClassifier(**params),\n#     df_oof,\n#     df_pred\n# )\n\n# params = {\"random_state\": SEED, 'task_type': 'GPU', 'verbose':0, 'iterations': 2000}\n# oof_cat2, pred_cat2, _ = cross_validate(\n#     CatBoostClassifier(**params),\n#     df_oof, \n#     df_pred\n# )\n\nparams = {\"random_state\": SEED, 'n_jobs': -1 , 'C':1000, 'max_iter':1000}\noof_log2, pred_log2, _ = cross_validate(\n    LogisticRegression(**params), \n    df_oof,\n    df_pred,\n    early_stopping=False\n)\n","metadata":{"execution":{"iopub.status.busy":"2021-09-26T16:23:17.171704Z","iopub.execute_input":"2021-09-26T16:23:17.172295Z","iopub.status.idle":"2021-09-26T16:26:17.075486Z","shell.execute_reply.started":"2021-09-26T16:23:17.172261Z","shell.execute_reply":"2021-09-26T16:26:17.074481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ensemble_predictions = np.array([pred_lgb2, pred_xgb2, pred_cat2, pred_log2]).mean(axis=0)","metadata":{"execution":{"iopub.status.busy":"2021-09-26T16:26:34.66646Z","iopub.execute_input":"2021-09-26T16:26:34.667201Z","iopub.status.idle":"2021-09-26T16:26:34.678473Z","shell.execute_reply.started":"2021-09-26T16:26:34.667155Z","shell.execute_reply":"2021-09-26T16:26:34.677706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ensemble_predictions","metadata":{"execution":{"iopub.status.busy":"2021-09-26T16:26:36.701765Z","iopub.execute_input":"2021-09-26T16:26:36.702019Z","iopub.status.idle":"2021-09-26T16:26:36.711056Z","shell.execute_reply.started":"2021-09-26T16:26:36.701992Z","shell.execute_reply":"2021-09-26T16:26:36.708424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission['claim'] = pred_log2.tolist()\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-26T16:26:38.885989Z","iopub.execute_input":"2021-09-26T16:26:38.886529Z","iopub.status.idle":"2021-09-26T16:26:40.543315Z","shell.execute_reply.started":"2021-09-26T16:26:38.886492Z","shell.execute_reply":"2021-09-26T16:26:40.54257Z"},"trusted":true},"execution_count":null,"outputs":[]}]}