{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Hi,\\\n**Welcome to my Notebook !!**.\\\nThe following notebook deals with finding the best params with optuna study.\n\n**Please leave an upvote or feedback, if you like or use at part of it. \\\nWould encourage me more to share.**\n\n\nI have finally used the obtained params in this notebook:- \\\nhttps://www.kaggle.com/skiller/a-z-predictive-modelling-welcome \n\nI have **not doing any EDA in the current notebook** as i have already taken care of it in the above notebook. \n\nWarm Regards","metadata":{}},{"cell_type":"code","source":"import os\nimport joblib\nimport numpy as np\nimport pandas as pd\nimport warnings\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib import ticker\nimport seaborn as sns\n\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import datasets\ntrain_df = pd.read_csv('/kaggle/input/tabular-playground-series-sep-2021/train.csv')\ntest_df = pd.read_csv('/kaggle/input/tabular-playground-series-sep-2021/test.csv')\nsubmission = pd.read_csv('/kaggle/input/tabular-playground-series-sep-2021/sample_solution.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['num_nulls'] = train_df.drop(['id', 'claim'], axis = 1).isna().sum(axis = 1)\ntest_df['num_nulls'] = test_df.drop(['id'], axis = 1).isna().sum(axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Removal of null values. (Also a bit of Preprocessing) \nBut we can't drop the rows owing to the large amount single null rows","metadata":{}},{"cell_type":"code","source":"%%time\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import QuantileTransformer,  KBinsDiscretizer\nfrom sklearn.impute import SimpleImputer\n\nfeatures = [col for col in train_df.columns if col not in ['claim', 'id']]\npipe = Pipeline([\n        ('imputer', SimpleImputer(strategy='median',missing_values=np.nan)),\n        (\"scaler\", QuantileTransformer(n_quantiles=64,output_distribution='uniform')),\n        ('bin', KBinsDiscretizer(n_bins=64, encode='ordinal',strategy='uniform'))\n        ])\ntrain_df[features] = pipe.fit_transform(train_df[features])\ntest_df[features] = pipe.transform(test_df[features])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ok now Lets find the params for our models \n3 models to be looked here are -> **XGBoost, CatBoost, LGDM**","metadata":{}},{"cell_type":"code","source":"# import packages\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom xgboost import XGBClassifier \nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\nfrom tqdm import tqdm\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve, auc\nimport optuna","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CAT Boost Best Params","metadata":{}},{"cell_type":"code","source":"OPTUNA_OPTIMIZATION = True\n\ndef objective(trial):    \n    params = {\n        'iterations':trial.suggest_int(\"iterations\", 1000, 20000),\n        'objective': trial.suggest_categorical('objective', ['Logloss', 'CrossEntropy']),\n        'bootstrap_type': trial.suggest_categorical('bootstrap_type', ['Bayesian', 'Bernoulli', 'MVS']),\n        'od_wait':trial.suggest_int('od_wait', 500, 2000),\n        'learning_rate' : trial.suggest_uniform('learning_rate',0.001,1),\n        'reg_lambda': trial.suggest_uniform('reg_lambda',1e-5,100),\n        'random_strength': trial.suggest_uniform('random_strength', 10, 50),\n        'depth': trial.suggest_int('depth',1 , 15),\n        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 30),\n        'leaf_estimation_iterations': trial.suggest_int('leaf_estimation_iterations', 1, 15),\n#         'task_type' : 'GPU',\n        'devices' : '0'\n    }\n    \n    if params['bootstrap_type'] == 'Bayesian':\n        params['bagging_temperature'] = trial.suggest_float('bagging_temperature', 0, 10)\n    elif params['bootstrap_type'] == 'Bernoulli':\n        params['subsample'] = trial.suggest_float('subsample', 0.1, 1)\n\n    training_df, validation_df =  train_test_split(train_df, test_size=0.1, shuffle=True, random_state=1)\n\n    model = CatBoostClassifier(**params)\n    \n    x_train = training_df.drop(['claim', 'id'],axis=1) \n    y_train = training_df['claim']\n    x_valid = validation_df.drop(['claim', 'id'],axis=1) \n    y_valid = validation_df['claim']\n    model.fit(\n        x_train , y_train,\n        eval_set=[(x_valid, y_valid)],\n        early_stopping_rounds=100,\n        use_best_model=True,\n        verbose=0\n    )\n    \n    return roc_auc_score(y_valid,  model.predict_proba(x_valid)[:,1])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nstudy = optuna.create_study(\n    direction='maximize',\n    study_name='CatbClf'\n)\n\nstudy.optimize(\n    objective,\n    n_trials=100\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Best Trial: {study.best_trial.value}\")\nprint(f\"Best Params: {study.best_trial.params}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets look at params dependencies with accuracy","metadata":{}},{"cell_type":"code","source":"optuna.visualization.plot_optimization_history(study)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optuna.visualization.plot_parallel_coordinate(study)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optuna.visualization.plot_param_importances(study)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Delete Study from RAM","metadata":{}},{"cell_type":"code","source":"del study","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# XGBOOST Best Params","metadata":{}},{"cell_type":"code","source":"OPTUNA_OPTIMIZATION = True\n\ndef objective(trial):    \n    params = {\n            'n_estimators':trial.suggest_int(\"n_estimators\", 1000, 20000),\n            'learning_rate' : trial.suggest_uniform('learning_rate', 0.001, 1),\n            'subsample': trial.suggest_uniform('subsample', 0.1, 1),\n            'colsample_bytree':trial.suggest_uniform('colsample_bytree', 0.1, 1),\n            'max_depth': trial.suggest_categorical('max_depth', [1,3,5,7,9,11,13,15,17,20]),\n            'min_child_weight': trial.suggest_int('min_child_weight', 1, 300),\n            'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-5, 10.0),\n            'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-5, 10.0),\n#             'tree_method': 'gpu_hist'\n        }\n    \n    model = XGBClassifier(**params)\n    \n    training_df, validation_df =  train_test_split(train_df, test_size=0.1, shuffle=True, random_state=1)\n\n    x_train = training_df.drop(['claim', 'id'],axis=1) \n    y_train = training_df['claim']\n    x_valid = validation_df.drop(['claim', 'id'],axis=1) \n    y_valid = validation_df['claim']\n\n    model.fit(\n        x_train , y_train,\n        eval_set=[(x_valid, y_valid)],\n        early_stopping_rounds=100,\n        verbose=0\n    )\n    \n    return roc_auc_score(y_valid,  model.predict_proba(x_valid)[:,1])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nstudy = optuna.create_study(\n    direction='maximize',\n    study_name='XG_boost'\n)\n\nstudy.optimize(\n    objective,\n    n_trials=100\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Best Trial: {study.best_trial.value}\")\nprint(f\"Best Params: {study.best_trial.params}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets look at params dependencies with accuracy","metadata":{}},{"cell_type":"code","source":"optuna.visualization.plot_optimization_history(study)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optuna.visualization.plot_parallel_coordinate(study)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optuna.visualization.plot_param_importances(study)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Delete study from RAM","metadata":{}},{"cell_type":"code","source":"del study","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LGDM CLASSIFIER Best Params","metadata":{}},{"cell_type":"code","source":"OPTUNA_OPTIMIZATION = True\n\ndef objective(trial):    \n    params = {\n        \"objective\": \"binary\",\n        \"metric\": \"binary_logloss\",\n        \"verbose\": -1,\n        \"boosting_type\": \"gbdt\",\n        'n_estimators':trial.suggest_int(\"n_estimators\", 1000, 20000),\n        'learning_rate' : trial.suggest_uniform('learning_rate', 0.001, 1),\n        \"reg_alpha\": trial.suggest_loguniform(\"reg_alpha\", 1e-5, 10.0),\n        \"reg_lambda\": trial.suggest_loguniform(\"reg_lambda\", 1e-5, 10.0),\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n        \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.1, 1.0),\n        \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.1, 1.0),\n        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 1, 300),\n        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 300),\n        'colsample_bytree':trial.suggest_uniform('colsample_bytree', 0.1, 1),\n#         \"device_type\" : \"gpu\"\n    }\n\n    training_df, validation_df =  train_test_split(train_df, test_size=0.1, shuffle=True, random_state=1)\n\n    model = LGBMClassifier(**params)\n    \n    x_train = training_df.drop(['claim', 'id'],axis=1) \n    y_train = training_df['claim']\n    x_valid = validation_df.drop(['claim', 'id'],axis=1) \n    y_valid = validation_df['claim']\n    model.fit(\n        x_train , y_train,\n        eval_set=[(x_valid, y_valid)],\n        early_stopping_rounds=100,\n        verbose=-1,\n    )\n    \n    return roc_auc_score(y_valid,  model.predict_proba(x_valid)[:,1])","metadata":{"execution":{"iopub.status.busy":"2021-09-26T18:54:34.805881Z","iopub.execute_input":"2021-09-26T18:54:34.806713Z","iopub.status.idle":"2021-09-26T18:54:34.817617Z","shell.execute_reply.started":"2021-09-26T18:54:34.806669Z","shell.execute_reply":"2021-09-26T18:54:34.816587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nstudy = optuna.create_study(\n    direction='maximize',\n    study_name='LGDM'\n)\n\nstudy.optimize(\n    objective,\n    n_trials=100\n)","metadata":{"execution":{"iopub.status.busy":"2021-09-26T18:54:36.774453Z","iopub.execute_input":"2021-09-26T18:54:36.774929Z","iopub.status.idle":"2021-09-26T19:56:30.706106Z","shell.execute_reply.started":"2021-09-26T18:54:36.774893Z","shell.execute_reply":"2021-09-26T19:56:30.705438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Best Trial: {study.best_trial.value}\")\nprint(f\"Best Params: {study.best_trial.params}\")","metadata":{"execution":{"iopub.status.busy":"2021-09-26T19:57:27.641578Z","iopub.execute_input":"2021-09-26T19:57:27.64239Z","iopub.status.idle":"2021-09-26T19:57:27.648704Z","shell.execute_reply.started":"2021-09-26T19:57:27.642336Z","shell.execute_reply":"2021-09-26T19:57:27.647341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets look at params dependencies with accuracy","metadata":{}},{"cell_type":"code","source":"optuna.visualization.plot_optimization_history(study)","metadata":{"execution":{"iopub.status.busy":"2021-09-26T19:57:30.761322Z","iopub.execute_input":"2021-09-26T19:57:30.762316Z","iopub.status.idle":"2021-09-26T19:57:30.795959Z","shell.execute_reply.started":"2021-09-26T19:57:30.762267Z","shell.execute_reply":"2021-09-26T19:57:30.795178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optuna.visualization.plot_parallel_coordinate(study)","metadata":{"execution":{"iopub.status.busy":"2021-09-26T19:57:34.694463Z","iopub.execute_input":"2021-09-26T19:57:34.694839Z","iopub.status.idle":"2021-09-26T19:57:34.734985Z","shell.execute_reply.started":"2021-09-26T19:57:34.694807Z","shell.execute_reply":"2021-09-26T19:57:34.734312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optuna.visualization.plot_param_importances(study)","metadata":{"execution":{"iopub.status.busy":"2021-09-26T19:57:38.92796Z","iopub.execute_input":"2021-09-26T19:57:38.928227Z","iopub.status.idle":"2021-09-26T19:57:40.358153Z","shell.execute_reply.started":"2021-09-26T19:57:38.928199Z","shell.execute_reply":"2021-09-26T19:57:40.357511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Delete Study","metadata":{}},{"cell_type":"code","source":"del study","metadata":{"execution":{"iopub.status.busy":"2021-09-26T18:54:26.485504Z","iopub.execute_input":"2021-09-26T18:54:26.485755Z","iopub.status.idle":"2021-09-26T18:54:26.489607Z","shell.execute_reply.started":"2021-09-26T18:54:26.485729Z","shell.execute_reply":"2021-09-26T18:54:26.48872Z"},"trusted":true},"execution_count":null,"outputs":[]}]}