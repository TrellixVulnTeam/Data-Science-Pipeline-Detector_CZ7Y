{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Load Competition Dataset","metadata":{}},{"cell_type":"markdown","source":"Competition dataset located in \"/kaggle/input\"; This path defined by Kaggle to access the competition file. We will list two files from this path as input files.","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        path=os.path.join(dirname, filename)\n        if 'train' in path:\n            __training_path=path\n        elif 'test' in path:\n            __test_path=path","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Check training and test path","metadata":{}},{"cell_type":"code","source":"#loaded files\nprint(f'Training path:{__training_path}\\nTest path:{__test_path}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Kaggle Environment Prepration\n#update kaggle env\nimport sys\n#you may update the environment that allow you to run the whole code\n#!{sys.executable} -m pip install --upgrade scikit-learn==\"0.24.2\" \n#record this information if you need to run the Kernel internally\nimport sklearn; sklearn.show_versions() ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Input Dataset","metadata":{}},{"cell_type":"code","source":"def __load__data(__training_path, __test_path, concat=False):\n\t\"\"\"load data as input dataset\n\tparams: __training_path: the training path of input dataset\n\tparams: __test_path: the path of test dataset\n\tparams: if it is True, then it will concatinate the training and test dataset as output\n\treturns: generate final loaded dataset as dataset, input and test\n\t\"\"\"\n\t# LOAD DATA\n\timport pandas as pd\n\t__train_dataset = pd.read_csv(__training_path, delimiter=',')\n\t__test_dataset = pd.read_csv(__test_path, delimiter=',')\n\tif not concat:\n\t    __dataset = __train_dataset.copy()\n\telse:\n\t    __dataset = pd.concat([__train_dataset, __test_dataset], axis=0\n\t        ).reset_index(drop=True)\n\treturn __dataset, __train_dataset, __test_dataset\n__dataset, __train_dataset, __test_dataset = __load__data(__training_path, __test_path, concat=True)\n__dataset","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Generate Submission file\nWe have to maintain the following columns in submission.csv.\nthen, we can drop that column(s) from original dataset because it is unique and not useful for training a model.\nIn some cases, an ID may carry useful information such as student ID where it may consist of admission year and other related info.","metadata":{}},{"cell_type":"code","source":"submission_columns = ['id']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission=pd.DataFrame(__test_dataset[submission_columns].copy())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# DISCARD IRRELEVANT COLUMNS\n__dataset.drop(['id'], axis=1, inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Remove Missing Data in Numerical Columns","metadata":{}},{"cell_type":"markdown","source":"In the given input dataset there are <b>118 columns </b> with  missing data as follows:","metadata":{}},{"cell_type":"markdown","source":"f1, f2, f3, f4, f5, f6, f7, f8, f9, f10, f11, f12, f13, f14, f15, f16, f17, f18, f19, f20, f21, f22, f23, f24, f25, f26, f27, f28, f29, f30, f31, f32, f33, f34, f35, f36, f37, f38, f39, f40, f41, f42, f43, f44, f45, f46, f47, f48, f49, f50, f51, f52, f53, f54, f55, f56, f57, f58, f59, f60, f61, f62, f63, f64, f65, f66, f67, f68, f69, f70, f71, f72, f73, f74, f75, f76, f77, f78, f79, f80, f81, f82, f83, f84, f85, f86, f87, f88, f89, f90, f91, f92, f93, f94, f95, f96, f97, f98, f99, f100, f101, f102, f103, f104, f105, f106, f107, f108, f109, f110, f111, f112, f113, f114, f115, f116, f117, f118","metadata":{}},{"cell_type":"markdown","source":"The following code removes the missing values from those columns. SML uses average value (median) of each column to replace the null values.","metadata":{}},{"cell_type":"code","source":"def __fillna__(__dataset):\n\t\"\"\"Fill null values with median of each col\"\"\"\n\t# PREPROCESSING-1\n\t_NUM_COLM_HAS_MISSING = ['f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60', 'f61', 'f62', 'f63', 'f64', 'f65', 'f66', 'f67', 'f68', 'f69', 'f70', 'f71', 'f72', 'f73', 'f74', 'f75', 'f76', 'f77', 'f78', 'f79', 'f80', 'f81', 'f82', 'f83', 'f84', 'f85', 'f86', 'f87', 'f88', 'f89', 'f90', 'f91', 'f92', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98', 'f99', 'f100', 'f101', 'f102', 'f103', 'f104', 'f105', 'f106', 'f107', 'f108', 'f109', 'f110', 'f111', 'f112', 'f113', 'f114', 'f115', 'f116', 'f117', 'f118']\n\tfor _col in _NUM_COLM_HAS_MISSING:\n\t\t__dataset[_col] = __dataset[_col].fillna(__dataset[_col].median(),axis=0)\n\treturn __dataset\n__dataset = __fillna__(__dataset)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Drop Target Column\nWe need to drop target column from the training dataset.","metadata":{}},{"cell_type":"markdown","source":"Now let's drops target columns of <b>claim</b> from dataset.","metadata":{}},{"cell_type":"markdown","source":"### Set Target Column\nThe target column in the value which we need to predict.\nTherefore, we need to detach the target columns in prediction.\nPlease note that if we don't drop this fields, it will generate a model with high accuracy on training and worst accuracy on test (because the value in test dataset is Null).","metadata":{}},{"cell_type":"code","source":"target_column_pred = \"claim\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# DETATCH TARGET\n__feature = __dataset.drop(['claim'], axis=1)\n__target =__dataset['claim']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Split Train/Test\nWe have to separate train and test before start training a model","metadata":{}},{"cell_type":"code","source":"# TRAIN TEST SPLIT\n__num_of_training_instances = __train_dataset.shape[0]\n__feature_train = __feature.iloc[:__num_of_training_instances,:]\n__feature_test = __feature.iloc[__num_of_training_instances:,:]\n__target_train = __target.iloc[:__num_of_training_instances]\n__target_test = __target.iloc[__num_of_training_instances:]\n__feature_train","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training Model and Prediction\nFirst, we will train a model, then predict test values based on the trained model.","metadata":{}},{"cell_type":"code","source":"# MODEL\nfrom lightgbm import LGBMRegressor\n__lgbmregressor=LGBMRegressor()\n__lgbmregressor.fit(__feature_train, __target_train)\n__y_pred = __lgbmregressor.predict(__feature_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load target columns for prediction and generate submission file for the competition.\nsubmission[target_column_pred]=__y_pred\nsubmission","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate a submission CSV file; we have to save the file as a CSV and avoid adding dataframe index into it.\nsubmission.to_csv('submission.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now, let's review submission file.\nsubmission","metadata":{},"execution_count":null,"outputs":[]}]}