{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Tab Play September\n- This notebook covers my code for the **Tabular Playground Series - September challenge**, which can be found [here](https://www.kaggle.com/c/tabular-playground-series-sep-2021).\n- In this notebook, I have used an **Auto-Visualization Library** for visualizing the data, which can be found [here](https://github.com/AutoViML/AutoViz).\n- I have used **Mean Imputation** for all the features having NULL(s)\n- After that, I have determined the **PCC (Pearson Correlation Coefficient)** of all the features with the 'claim' variable, and eliminated all those features having |PCC| <= 0.0025, based on the fact that they don't explain the target variable to any considerable extent.\n- Also, I standardized all the features with the help of **StandardScaler**\n- I also tried using **PCA (Principal Component Analysis)**, but it only deteriorated the score, hence, I didn't use it in the final submission.\n- For training purposes, I used multiple models such as **Gaussian Naive Bayes**, **Logistic Regression**, **Gradient Boosting Classifier**, and **Light Gradient Boosted Machine (LGBM)**, out of which LGBM gave the best score.\n\n**I would love to improve my existing score and am open to any suggestions. Please do leave them in the comments section, and if you liked my work, an upvote would be awesome :)**","metadata":{}},{"cell_type":"code","source":"!pip install xlrd\n!pip install autoviz","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-09-05T17:14:53.667477Z","iopub.execute_input":"2021-09-05T17:14:53.667879Z","iopub.status.idle":"2021-09-05T17:15:10.025395Z","shell.execute_reply.started":"2021-09-05T17:14:53.667801Z","shell.execute_reply":"2021-09-05T17:15:10.024284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom tqdm import tqdm\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.decomposition import PCA\nfrom lightgbm import LGBMClassifier\nfrom autoviz.AutoViz_Class import AutoViz_Class\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-05T17:17:42.106687Z","iopub.execute_input":"2021-09-05T17:17:42.10704Z","iopub.status.idle":"2021-09-05T17:17:42.117552Z","shell.execute_reply.started":"2021-09-05T17:17:42.107009Z","shell.execute_reply":"2021-09-05T17:17:42.116402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importing the Dataset","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv(\"../input/tabular-playground-series-sep-2021/train.csv\")\ndf_test = pd.read_csv(\"../input/tabular-playground-series-sep-2021/test.csv\")\ndf_sub = pd.read_csv(\"../input/tabular-playground-series-sep-2021/sample_solution.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-09-05T17:15:13.536456Z","iopub.execute_input":"2021-09-05T17:15:13.536968Z","iopub.status.idle":"2021-09-05T17:15:55.212801Z","shell.execute_reply.started":"2021-09-05T17:15:13.536925Z","shell.execute_reply":"2021-09-05T17:15:55.211948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df_train.shape)\ndf_train.info(verbose=True, show_counts=True)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-09-05T17:15:57.45108Z","iopub.execute_input":"2021-09-05T17:15:57.451436Z","iopub.status.idle":"2021-09-05T17:15:57.695565Z","shell.execute_reply.started":"2021-09-05T17:15:57.451406Z","shell.execute_reply":"2021-09-05T17:15:57.694745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df_test.shape)\ndf_test.info(verbose=True, show_counts=True)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-09-05T17:15:58.851896Z","iopub.execute_input":"2021-09-05T17:15:58.852231Z","iopub.status.idle":"2021-09-05T17:15:58.98883Z","shell.execute_reply.started":"2021-09-05T17:15:58.852203Z","shell.execute_reply":"2021-09-05T17:15:58.98786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Keeping a separator variable and the target variable\nsep = df_train.shape[0]\nY = df_train[\"claim\"]\n\n# Dropping the IDs and the target variable\ndf_train.drop([\"id\", \"claim\"], axis=1, inplace=True)\ndf_test.drop([\"id\"], axis=1, inplace=True)\n\n# Concatenating the datasets for pre-processing\ndf = pd.concat([df_train, df_test], axis=0)\n\nprint(df_train.shape, Y.shape, sep)","metadata":{"execution":{"iopub.status.busy":"2021-09-05T17:16:05.729389Z","iopub.execute_input":"2021-09-05T17:16:05.729755Z","iopub.status.idle":"2021-09-05T17:16:06.552221Z","shell.execute_reply.started":"2021-09-05T17:16:05.729721Z","shell.execute_reply":"2021-09-05T17:16:06.551395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualizing & Pre-processing the Dataset\n- From the above code cells, we can see that all the features are numerical.\n- However, for some of the features, there exists some data-points which have NULL as a value, so, we will perform mean-imputation for those features","metadata":{}},{"cell_type":"code","source":"AV = AutoViz_Class()\ndata = AV.AutoViz('../input/tabular-playground-series-sep-2021/train.csv')","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\ndf = imp_mean.fit_transform(df)\ndf = pd.DataFrame(df)","metadata":{"execution":{"iopub.status.busy":"2021-09-05T17:16:19.487166Z","iopub.execute_input":"2021-09-05T17:16:19.487511Z","iopub.status.idle":"2021-09-05T17:16:22.916957Z","shell.execute_reply.started":"2021-09-05T17:16:19.48748Z","shell.execute_reply":"2021-09-05T17:16:22.916054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info(verbose=True, show_counts=True)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-09-05T17:16:24.426377Z","iopub.execute_input":"2021-09-05T17:16:24.426724Z","iopub.status.idle":"2021-09-05T17:16:24.800578Z","shell.execute_reply.started":"2021-09-05T17:16:24.426693Z","shell.execute_reply":"2021-09-05T17:16:24.799562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We are trying to find PCC (Pearson Correlation Coefficient) between features.\n# So that, we can eliminate some of the redundant features. But for plotting the\n# correlation matrix, we will use the training set only.\n\n# Getting the train set\ndf_train = df.iloc[ : sep, : ]\ndf_train = df_train.assign(claim = pd.Series(Y))\nprint(df_train.shape)\n\n# Calculating the PCC\ncor_mat = df_train.corr(method='pearson', min_periods=50)\nprint(cor_mat.shape)\n\n# Number of variables having abs(PCC) with 'claim', less than or equal to 0.005\n# We will simply eliminate those features, as they are related with the 'claim', to the minimum extent\nred_fea = []\nfor i, pcc in enumerate(cor_mat['claim']):\n    if(-0.0025 <= pcc and pcc <= 0.0025):\n        red_fea.append(cor_mat.index[i])\nprint(red_fea)","metadata":{"execution":{"iopub.status.busy":"2021-09-05T17:16:27.835511Z","iopub.execute_input":"2021-09-05T17:16:27.835885Z","iopub.status.idle":"2021-09-05T17:17:00.84953Z","shell.execute_reply.started":"2021-09-05T17:16:27.835845Z","shell.execute_reply":"2021-09-05T17:17:00.848426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dropping all the Redundant features\ndf.drop(red_fea, axis=1, inplace=True)\nprint(df.shape)","metadata":{"execution":{"iopub.status.busy":"2021-09-05T17:17:05.028519Z","iopub.execute_input":"2021-09-05T17:17:05.028905Z","iopub.status.idle":"2021-09-05T17:17:05.308622Z","shell.execute_reply.started":"2021-09-05T17:17:05.028872Z","shell.execute_reply":"2021-09-05T17:17:05.307754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Splitting the df back into df_train and df_test\ndf_train = df.iloc[ :sep, : ]\ndf_test = df.iloc[sep: , : ]\nprint(df_train.shape, df_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-09-05T17:17:09.219922Z","iopub.execute_input":"2021-09-05T17:17:09.220258Z","iopub.status.idle":"2021-09-05T17:17:09.229841Z","shell.execute_reply.started":"2021-09-05T17:17:09.220229Z","shell.execute_reply":"2021-09-05T17:17:09.228835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = StandardScaler()\ndf_train = scaler.fit_transform(df_train)\ndf_test = scaler.transform(df_test)\nprint(df_train.shape, df_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-09-05T17:17:10.690118Z","iopub.execute_input":"2021-09-05T17:17:10.690434Z","iopub.status.idle":"2021-09-05T17:17:12.464266Z","shell.execute_reply.started":"2021-09-05T17:17:10.690406Z","shell.execute_reply":"2021-09-05T17:17:12.463397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PCA is reducing the accuracy in the case of any model, hence, not using it.\n# pca = PCA(n_components = 70, random_state = 42)\n# df_train = pca.fit_transform(df_train)\n# df_test = pca.transform(df_test)\n# print(\"Explained Variance Ratio: \", np.sum(pca.explained_variance_ratio_))","metadata":{"execution":{"iopub.status.busy":"2021-09-05T17:22:09.238772Z","iopub.execute_input":"2021-09-05T17:22:09.23916Z","iopub.status.idle":"2021-09-05T17:22:16.498757Z","shell.execute_reply.started":"2021-09-05T17:22:09.239127Z","shell.execute_reply":"2021-09-05T17:22:16.497758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training the Model","metadata":{}},{"cell_type":"code","source":"# Splitting the df_train into train & val sets\nX_train, X_val, y_train, y_val = train_test_split(df_train, Y, test_size=0.1, random_state=42)\nprint(X_train.shape, X_val.shape, y_train.shape, y_val.shape)","metadata":{"execution":{"iopub.status.busy":"2021-09-05T17:22:22.147473Z","iopub.execute_input":"2021-09-05T17:22:22.147811Z","iopub.status.idle":"2021-09-05T17:22:23.014869Z","shell.execute_reply.started":"2021-09-05T17:22:22.147781Z","shell.execute_reply":"2021-09-05T17:22:23.013913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Gaussian Naive Bayes\n# gnb = GaussianNB()\n# gnb.fit(X_train, y_train)\n# y_pred = gnb.predict_proba(X_val)[ : , 1]\n# print(roc_auc_score(y_val, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Logistic Regression\n# lr = LogisticRegression(C = 0.001)\n# lr.fit(X_train, y_train)\n# y_pred = lr.predict_proba(X_val)[ : , 1]\n# print(roc_auc_score(y_val, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Gradient Boosting Classifier\n# rfc = RandomForestClassifier(n_estimators = 10, verbose = 1)\n# rfc.fit(X_train, y_train)\n# y_pred = rfc.predict_proba(X_val)[ : , 1]\n# print(roc_auc_score(y_val, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Light Gradient Boosted Machine (LightGBM)\nlgbm = LGBMClassifier(\n    max_depth = 3, \n    num_leaves = 7, \n    n_estimators = 10000, \n    colsample_bytree = 0.3, \n    subsample = 0.5, \n    random_state = 41, \n    reg_alpha=18, \n    reg_lambda=17, \n    learning_rate = 0.095, \n    device = 'gpu', \n    objective= 'binary'\n)\nlgbm.fit(X_train, y_train)\ny_pred = lgbm.predict_proba(X_val)[ : , 1]\nprint(roc_auc_score(y_val, y_pred))","metadata":{"execution":{"iopub.status.busy":"2021-09-05T17:22:28.490408Z","iopub.execute_input":"2021-09-05T17:22:28.490738Z","iopub.status.idle":"2021-09-05T17:28:47.036991Z","shell.execute_reply.started":"2021-09-05T17:22:28.490708Z","shell.execute_reply":"2021-09-05T17:28:47.035473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submitting the Predictions","metadata":{}},{"cell_type":"code","source":"# Training the model on the entire df_train\nmodel = LGBMClassifier(\n    max_depth = 3, \n    num_leaves = 7, \n    n_estimators = 10000, \n    colsample_bytree = 0.3, \n    subsample = 0.5, \n    random_state = 41, \n    reg_alpha=18, \n    reg_lambda=17, \n    learning_rate = 0.095, \n    device = 'gpu', \n    objective= 'binary'\n)\nmodel.fit(df_train, Y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test = model.predict_proba(df_test)[ : , 1]\ndf_sub['claim'] = y_test\nprint(df_sub.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sub.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sub.to_csv(\"submission.csv\", index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}