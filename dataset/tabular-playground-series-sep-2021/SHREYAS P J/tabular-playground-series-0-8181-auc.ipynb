{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Importing necessary libraries","metadata":{}},{"cell_type":"code","source":"import tensorflow.compat.v1 as tf\nfrom sklearn.metrics import confusion_matrix\nimport numpy as np\nfrom scipy.io import loadmat\nimport os\nfrom pywt import wavedec\nfrom functools import reduce\nfrom scipy import signal\nfrom scipy.stats import entropy\nfrom scipy.fft import fft, ifft\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom tensorflow import keras as K\nimport matplotlib.pyplot as plt\nimport scipy\nimport tqdm\nfrom sklearn import metrics\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import KFold,cross_validate\nfrom tensorflow.keras.layers import Dense, Activation, Flatten,Embedding, concatenate, Input, Dropout, LSTM, Bidirectional,BatchNormalization,PReLU,ReLU,Reshape\nfrom tensorflow_addons.layers import WeightNormalization\nfrom tensorflow.keras.optimizers import RMSprop\nfrom sklearn.metrics import classification_report\nfrom tensorflow.keras.models import Sequential, Model, load_model\nimport matplotlib.pyplot as plt;\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom sklearn.decomposition import PCA\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Conv1D,Conv2D,Add\nfrom tensorflow.keras.layers import MaxPool1D, MaxPooling2D\nimport seaborn as sns\nimport sklearn","metadata":{"execution":{"iopub.status.busy":"2021-09-22T05:09:42.11387Z","iopub.execute_input":"2021-09-22T05:09:42.114409Z","iopub.status.idle":"2021-09-22T05:09:42.124208Z","shell.execute_reply.started":"2021-09-22T05:09:42.114371Z","shell.execute_reply":"2021-09-22T05:09:42.123433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Reading the train and test datasets.","metadata":{}},{"cell_type":"code","source":"train_data = pd.read_csv('../input/tabular-playground-series-sep-2021/train.csv')\ntest_data = pd.read_csv('../input/tabular-playground-series-sep-2021/test.csv')","metadata":{"execution":{"iopub.status.busy":"2021-09-22T05:09:42.531425Z","iopub.execute_input":"2021-09-22T05:09:42.532153Z","iopub.status.idle":"2021-09-22T05:10:10.814223Z","shell.execute_reply.started":"2021-09-22T05:09:42.532112Z","shell.execute_reply":"2021-09-22T05:10:10.813454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data","metadata":{"execution":{"iopub.status.busy":"2021-09-22T05:10:10.815932Z","iopub.execute_input":"2021-09-22T05:10:10.816204Z","iopub.status.idle":"2021-09-22T05:10:10.968733Z","shell.execute_reply.started":"2021-09-22T05:10:10.81616Z","shell.execute_reply":"2021-09-22T05:10:10.967747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data","metadata":{"execution":{"iopub.status.busy":"2021-09-22T05:10:10.9703Z","iopub.execute_input":"2021-09-22T05:10:10.9707Z","iopub.status.idle":"2021-09-22T05:10:11.070698Z","shell.execute_reply.started":"2021-09-22T05:10:10.970652Z","shell.execute_reply":"2021-09-22T05:10:11.069821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.describe()","metadata":{"execution":{"iopub.status.busy":"2021-09-22T05:10:11.072956Z","iopub.execute_input":"2021-09-22T05:10:11.073325Z","iopub.status.idle":"2021-09-22T05:10:15.383758Z","shell.execute_reply.started":"2021-09-22T05:10:11.073288Z","shell.execute_reply":"2021-09-22T05:10:15.38303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.describe()","metadata":{"execution":{"iopub.status.busy":"2021-09-22T05:10:15.385081Z","iopub.execute_input":"2021-09-22T05:10:15.385367Z","iopub.status.idle":"2021-09-22T05:10:17.386125Z","shell.execute_reply.started":"2021-09-22T05:10:15.385331Z","shell.execute_reply":"2021-09-22T05:10:17.385313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Deleting unnecessary columns.","metadata":{}},{"cell_type":"markdown","source":"From the count rows in the tables above, we see that columns like \"Id\" are unique, hence wont contribute for our predictions. Hence we remove them.","metadata":{}},{"cell_type":"code","source":"train_data.pop('id')\ntest_data.pop('id')\ny = train_data.pop('claim')","metadata":{"execution":{"iopub.status.busy":"2021-09-22T05:10:17.387702Z","iopub.execute_input":"2021-09-22T05:10:17.387978Z","iopub.status.idle":"2021-09-22T05:10:17.396947Z","shell.execute_reply.started":"2021-09-22T05:10:17.387941Z","shell.execute_reply":"2021-09-22T05:10:17.396129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dealing with missing values.","metadata":{}},{"cell_type":"code","source":"train_data.isna().sum() , test_data.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2021-09-22T05:10:17.398312Z","iopub.execute_input":"2021-09-22T05:10:17.399164Z","iopub.status.idle":"2021-09-22T05:10:17.706558Z","shell.execute_reply.started":"2021-09-22T05:10:17.399118Z","shell.execute_reply":"2021-09-22T05:10:17.705712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that each column has multiple missing values, hence we use Imputation to fill the null values instead of dropping those rows which would lead to loss of information.\n","metadata":{}},{"cell_type":"markdown","source":"## Feature engineering, Median imputation and Z-score normalization.","metadata":{}},{"cell_type":"markdown","source":"1. Feature Engineering: We construct new features with summary statistics like Mean, Variance, Standard Deviation (SD) and additional features like n_missing denoting the number of missing values along each row.\n\n2. Imputation: We use median imputation for filling in the null values. We create a dictionary to decide how to impute each column.\n\n    Mean: normal distribution\n\n    Median: unimodal and skewed \n\n    Mode: all other cases\n\n3. Normalization/ Standardization: We observe that the values in each column have different scales, hence we perform RobustScaling to avoid effect of outliers. ","metadata":{}},{"cell_type":"code","source":"features = [x for x in train_data.columns.values if x[0]==\"f\"]\n\ntrain_data['n_missing'] = train_data[features].isna().sum(axis = 1)\ntest_data['n_missing'] = test_data[features].isna().sum(axis = 1)\n\ntrain_data['std'] = train_data[features].std(axis = 1)\ntest_data['std'] = test_data[features].std(axis = 1)\n\ntrain_data['var'] = train_data[features].var(axis = 1)\ntest_data['var'] = test_data[features].var(axis = 1)\n\ntrain_data['mean'] = train_data[features].mean(axis = 1)\ntest_data['mean'] = test_data[features].mean(axis = 1)\n\ntrain_data['median'] = train_data[features].median(axis = 1)\ntest_data['median'] = test_data[features].median(axis = 1)\n\ntrain_data['rms'] = (train_data.iloc[:,1:]**2).sum(1).pow(1/2)\ntest_data['rms'] = (test_data.iloc[:,1:]**2).sum(1).pow(1/2)\n\ntrain_data['abs_sum'] = train_data[features].abs().sum(axis = 1)\ntest_data['abs_sum'] = test_data[features].abs().sum(axis = 1)\n\ntrain_data['max'] = train_data[features].max(axis = 1)\ntest_data['max'] = test_data[features].max(axis = 1)\n\ntrain_data['min'] = train_data[features].min(axis = 1)\ntest_data['min'] = test_data[features].min(axis = 1)\n\nfeatures += ['n_missing', 'std', 'var','mean','median','rms','abs_sum','max','min']\n\n","metadata":{"execution":{"iopub.status.busy":"2021-09-22T05:12:17.20877Z","iopub.execute_input":"2021-09-22T05:12:17.209314Z","iopub.status.idle":"2021-09-22T05:12:40.262951Z","shell.execute_reply.started":"2021-09-22T05:12:17.209276Z","shell.execute_reply":"2021-09-22T05:12:40.262206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sc = RobustScaler()\ntrain_data[features] = sc.fit_transform(train_data[features])\ntest_data[features] = sc.transform(test_data[features])","metadata":{"execution":{"iopub.status.busy":"2021-09-22T05:12:47.618035Z","iopub.execute_input":"2021-09-22T05:12:47.618554Z","iopub.status.idle":"2021-09-22T05:13:40.161736Z","shell.execute_reply.started":"2021-09-22T05:12:47.618514Z","shell.execute_reply":"2021-09-22T05:13:40.160973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fill_value_dict = {\n    'f1': 'Mean', \n    'f2': 'Median', \n    'f3': 'Median', \n    'f4': 'Median', \n    'f5': 'Mode', \n    'f6': 'Mean', \n    'f7': 'Median', \n    'f8': 'Median', \n    'f9': 'Median', \n    'f10': 'Median', \n    'f11': 'Mean', \n    'f12': 'Median', \n    'f13': 'Mean', \n    'f14': 'Median', \n    'f15': 'Mean', \n    'f16': 'Median', \n    'f17': 'Median', \n    'f18': 'Median', \n    'f19': 'Median', \n    'f20': 'Median', \n    'f21': 'Median', \n    'f22': 'Mean', \n    'f23': 'Mode', \n    'f24': 'Median', \n    'f25': 'Median', \n    'f26': 'Median', \n    'f27': 'Median', \n    'f28': 'Median', \n    'f29': 'Mode', \n    'f30': 'Median', \n    'f31': 'Median', \n    'f32': 'Median', \n    'f33': 'Median', \n    'f34': 'Mean', \n    'f35': 'Median', \n    'f36': 'Mean', \n    'f37': 'Median', \n    'f38': 'Median', \n    'f39': 'Median', \n    'f40': 'Mode', \n    'f41': 'Median', \n    'f42': 'Mode', \n    'f43': 'Mean', \n    'f44': 'Median', \n    'f45': 'Median', \n    'f46': 'Mean', \n    'f47': 'Mode', \n    'f48': 'Mean', \n    'f49': 'Mode', \n    'f50': 'Mode', \n    'f51': 'Median', \n    'f52': 'Median', \n    'f53': 'Median', \n    'f54': 'Mean', \n    'f55': 'Mean', \n    'f56': 'Mode', \n    'f57': 'Mean', \n    'f58': 'Median', \n    'f59': 'Median', \n    'f60': 'Median', \n    'f61': 'Median', \n    'f62': 'Median', \n    'f63': 'Median', \n    'f64': 'Median', \n    'f65': 'Mode', \n    'f66': 'Median', \n    'f67': 'Median', \n    'f68': 'Median', \n    'f69': 'Mean', \n    'f70': 'Mode', \n    'f71': 'Median', \n    'f72': 'Median', \n    'f73': 'Median', \n    'f74': 'Mode', \n    'f75': 'Mode', \n    'f76': 'Mean', \n    'f77': 'Mode', \n    'f78': 'Median', \n    'f79': 'Mean', \n    'f80': 'Median', \n    'f81': 'Mode', \n    'f82': 'Median', \n    'f83': 'Mode', \n    'f84': 'Median', \n    'f85': 'Median', \n    'f86': 'Median', \n    'f87': 'Median', \n    'f88': 'Median', \n    'f89': 'Median', \n    'f90': 'Mean', \n    'f91': 'Mode', \n    'f92': 'Median', \n    'f93': 'Median', \n    'f94': 'Median', \n    'f95': 'Median', \n    'f96': 'Median', \n    'f97': 'Mean', \n    'f98': 'Median', \n    'f99': 'Median', \n    'f100': 'Mode', \n    'f101': 'Median', \n    'f102': 'Median', \n    'f103': 'Median', \n    'f104': 'Median', \n    'f105': 'Median', \n    'f106': 'Median', \n    'f107': 'Median', \n    'f108': 'Median', \n    'f109': 'Mode', \n    'f110': 'Median', \n    'f111': 'Median', \n    'f112': 'Median', \n    'f113': 'Mean', \n    'f114': 'Median', \n    'f115': 'Median', \n    'f116': 'Mode', \n    'f117': 'Median', \n    'f118': 'Mean'\n}\n\n\nfor col in (features):\n    if fill_value_dict.get(col)=='Mean':\n        fill_value = train_data[col].mean()\n    elif fill_value_dict.get(col)=='Median':\n        fill_value = train_data[col].median()\n    elif fill_value_dict.get(col)=='Mode':\n        fill_value = train_data[col].mode().iloc[0]\n    \n    train_data[col].fillna(fill_value, inplace=True)\n    test_data[col].fillna(fill_value, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-22T05:13:57.358521Z","iopub.execute_input":"2021-09-22T05:13:57.358835Z","iopub.status.idle":"2021-09-22T05:13:59.217463Z","shell.execute_reply.started":"2021-09-22T05:13:57.358802Z","shell.execute_reply":"2021-09-22T05:13:59.216697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Removing outliers using Z-score for data points.","metadata":{}},{"cell_type":"code","source":"#from scipy import stats\n#train_data = train_data[(np.abs(stats.zscore(train_data)) < 3).all(axis=1)]","metadata":{"execution":{"iopub.status.busy":"2021-09-22T05:10:17.745766Z","iopub.status.idle":"2021-09-22T05:10:17.746251Z","shell.execute_reply.started":"2021-09-22T05:10:17.745969Z","shell.execute_reply":"2021-09-22T05:10:17.745991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data","metadata":{"execution":{"iopub.status.busy":"2021-09-22T05:14:56.136114Z","iopub.execute_input":"2021-09-22T05:14:56.136378Z","iopub.status.idle":"2021-09-22T05:14:56.344397Z","shell.execute_reply.started":"2021-09-22T05:14:56.136348Z","shell.execute_reply":"2021-09-22T05:14:56.343591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dividing the dependent and independent variables.","metadata":{}},{"cell_type":"code","source":"x = pd.DataFrame(train_data)\ny = pd.Series(y)","metadata":{"execution":{"iopub.status.busy":"2021-09-22T05:15:02.378892Z","iopub.execute_input":"2021-09-22T05:15:02.379148Z","iopub.status.idle":"2021-09-22T05:15:02.383003Z","shell.execute_reply.started":"2021-09-22T05:15:02.379122Z","shell.execute_reply":"2021-09-22T05:15:02.382089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Hyperparameter tuning of LGBM Classifier.","metadata":{}},{"cell_type":"code","source":"\"\"\"\nimport optuna\nfrom lightgbm import LGBMClassifier\ndef objective(trial, data = x, target = y):\n\n    \n    params = {\n        'n_estimators': trial.suggest_int('n_estimators', 1000, 40000),\n        'max_depth': trial.suggest_int('max_depth', 2, 3),\n        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.2),\n        'reg_alpha': trial.suggest_float('reg_alpha', 0.001, 10.0),\n        'reg_lambda': trial.suggest_float('reg_lambda', 0.001, 10.0),\n        'num_leaves': trial.suggest_int('num_leaves', 50, 500),\n        'min_data_per_group': trial.suggest_int('min_data_per_group', 50, 200),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 200),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 0.8),\n        'boosting_type': 'gbdt',\n        'objective': 'binary',\n        'random_state': 228,\n        'metric': 'auc',\n        'device_type': 'gpu',\n        'gpu_platform_id': 0,\n        'gpu_device_id': 0\n    }\n    \n    model = LGBMClassifier(**params)\n    scores = []\n    k = StratifiedKFold(n_splits = 2, random_state = 228, shuffle = True)\n    for i, (trn_idx, val_idx) in enumerate(k.split(x, y)):\n        \n        X_train, X_val = x.iloc[trn_idx], x.iloc[val_idx]\n        y_train, y_val = y.iloc[trn_idx], y.iloc[val_idx]\n\n        model.fit(X_train, y_train, eval_set = [(X_val, y_val)], early_stopping_rounds = 300, verbose = False)\n        \n        tr_preds = model.predict_proba(X_train)[:,1]\n        tr_score = sklearn.metrics.roc_auc_score(y_train, tr_preds)\n        \n        val_preds = model.predict_proba(X_val)[:,1]\n        val_score = sklearn.metrics.roc_auc_score(y_val, val_preds)\n\n        scores.append((tr_score, val_score))\n        \n        print(f\"Fold {i+1} | AUC: {val_score}\")\n        \n    scores = pd.DataFrame(scores, columns = ['train score', 'validation score'])\n    \n    return scores['validation score'].mean()\n\nstudy = optuna.create_study(direction = 'maximize')\nstudy.optimize(objective, n_trials = 10)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)\nprint('Best value:', study.best_value)\n\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2021-09-22T05:15:12.152522Z","iopub.execute_input":"2021-09-22T05:15:12.1532Z","iopub.status.idle":"2021-09-22T05:33:02.674601Z","shell.execute_reply.started":"2021-09-22T05:15:12.153163Z","shell.execute_reply":"2021-09-22T05:33:02.672596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Performing 10 fold cross validation using LGBMClassifier.","metadata":{}},{"cell_type":"code","source":"from lightgbm import LGBMClassifier\n\nSEED = 228\nparamsLGBM = {'objective': 'binary',\n               'boosting_type': 'gbdt',\n               'num_leaves': 6,\n               'max_depth': 2,\n               'n_estimators': 40000,\n               'reg_alpha': 25.0,\n               'reg_lambda': 76.7,\n               'random_state': SEED,\n               'bagging_seed': SEED, \n               'feature_fraction_seed': SEED,\n               'n_jobs': -1,\n               'subsample': 0.98,\n               'subsample_freq': 1,\n               'colsample_bytree': 0.69,\n               'min_child_samples': 54,\n               'min_child_weight': 256,\n               'learning_rate': 0.2,\n               'metric': 'AUC',\n               'verbosity': -1,\n              }\n\nfolds = StratifiedKFold(n_splits = 5, random_state = SEED, shuffle = True)\ny_pred = np.zeros(len(test_data))\nfor fold, (trn_idx, val_idx) in enumerate(folds.split(x, y)):\n    \n    X_train, X_val = x.iloc[trn_idx], x.iloc[val_idx]\n    y_train, y_val = y.iloc[trn_idx], y.iloc[val_idx]\n\n    model = LGBMClassifier(**paramsLGBM)\n   \n    model.fit(X_train, y_train, eval_set = [(X_val, y_val)], verbose = False, early_stopping_rounds = 300)\n    \n    y_pred += model.predict_proba(test_data)[:,1] / folds.n_splits ","metadata":{"execution":{"iopub.status.busy":"2021-09-22T05:36:00.433751Z","iopub.execute_input":"2021-09-22T05:36:00.43432Z","iopub.status.idle":"2021-09-22T06:19:34.075736Z","shell.execute_reply.started":"2021-09-22T05:36:00.434285Z","shell.execute_reply":"2021-09-22T06:19:34.075117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submitting predictions.","metadata":{}},{"cell_type":"code","source":"submission = pd.read_csv('../input/tabular-playground-series-sep-2021/sample_solution.csv')\nsubmission['claim'] = y_pred\nsubmission.to_csv('submission.csv',index = False)","metadata":{"execution":{"iopub.status.busy":"2021-09-22T06:27:39.422064Z","iopub.execute_input":"2021-09-22T06:27:39.422328Z","iopub.status.idle":"2021-09-22T06:27:41.326801Z","shell.execute_reply.started":"2021-09-22T06:27:39.422299Z","shell.execute_reply":"2021-09-22T06:27:41.325949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### With this, we end up with a score of 0.81793 on the leaderboard. There is definitely room for improvement.","metadata":{}},{"cell_type":"markdown","source":"#### Please upvote if you liked it :)","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}