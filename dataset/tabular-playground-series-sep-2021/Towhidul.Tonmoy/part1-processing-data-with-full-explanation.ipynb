{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-16T17:05:03.00675Z","iopub.execute_input":"2021-09-16T17:05:03.007781Z","iopub.status.idle":"2021-09-16T17:05:03.024104Z","shell.execute_reply.started":"2021-09-16T17:05:03.007713Z","shell.execute_reply":"2021-09-16T17:05:03.022977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os \n#The functions that the OS module provides allows you to interface with the operating system that Python is running on\nfrom sklearn.pipeline import Pipeline \n#Pipeline is used to assemble several steps that can be cross-validated together while setting different parameters. \n\n\nfrom sklearn.impute import SimpleImputer\n#Imputation transformer for completing missing values.\nfrom sklearn.preprocessing import QuantileTransformer,  KBinsDiscretizer\n#QuantileTransformer method transforms the features to follow a uniform or a normal distribution\n#KBinsDiscretizer bins continuous data into intervals.","metadata":{"execution":{"iopub.status.busy":"2021-09-16T17:08:45.330529Z","iopub.execute_input":"2021-09-16T17:08:45.330884Z","iopub.status.idle":"2021-09-16T17:08:45.336145Z","shell.execute_reply.started":"2021-09-16T17:08:45.330842Z","shell.execute_reply":"2021-09-16T17:08:45.33533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# %%time prints the wall time for the entire cell whereas %time gives you the time for first line only\n\ntrain = pd.read_csv('../input/tabular-playground-series-sep-2021/train.csv')\ntest  = pd.read_csv('../input/tabular-playground-series-sep-2021/test.csv')\nsub   = pd.read_csv('../input/tabular-playground-series-sep-2021/sample_solution.csv')","metadata":{"execution":{"iopub.status.busy":"2021-09-16T17:05:28.901006Z","iopub.execute_input":"2021-09-16T17:05:28.902204Z","iopub.status.idle":"2021-09-16T17:06:15.411711Z","shell.execute_reply.started":"2021-09-16T17:05:28.902143Z","shell.execute_reply":"2021-09-16T17:06:15.410397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-16T17:07:12.881964Z","iopub.execute_input":"2021-09-16T17:07:12.882283Z","iopub.status.idle":"2021-09-16T17:07:12.934945Z","shell.execute_reply.started":"2021-09-16T17:07:12.882252Z","shell.execute_reply":"2021-09-16T17:07:12.933964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-09-16T17:08:28.930768Z","iopub.execute_input":"2021-09-16T17:08:28.932049Z","iopub.status.idle":"2021-09-16T17:08:29.32996Z","shell.execute_reply.started":"2021-09-16T17:08:28.931968Z","shell.execute_reply":"2021-09-16T17:08:29.329292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrain['n_missing'] = train.isna().sum(axis=1) \n#Checking the total null value row wise and storing it to a new column\n\ntest['n_missing'] = test.isna().sum(axis=1)\n#Checking the total null value row wise and storing it to a new column\ntrain['claim'] = train['claim'].astype(str)\n#Converting the int datatypes to string datatype\n\nfeatures = [col for col in train.columns if col not in ['claim', 'id']]\n#Here we are taking only the features . We are not invluding the output(claim) and id.\npipe = Pipeline([\n        ('imputer', SimpleImputer(strategy='median',missing_values=np.nan)),\n        (\"scaler\", QuantileTransformer(n_quantiles=256,output_distribution='uniform')),\n        ('bin', KBinsDiscretizer(n_bins=256, encode='ordinal',strategy='uniform'))\n        ])\n#Now we are ready to create a pipeline object by providing with the list of steps. \n#Our steps are â€” SimpleImputer,QuantileTransformer and KBinsDiscretizer\n#These steps are list of tuples consisting of name and an instance of the transformer or estimator. \n#For imputing the missing value, we have used median.\n\ntrain[features] = pipe.fit_transform(train[features])\ntest[features] = pipe.transform(test[features])\n#transforming the features with pipeline","metadata":{"execution":{"iopub.status.busy":"2021-09-16T17:08:50.741674Z","iopub.execute_input":"2021-09-16T17:08:50.742181Z","iopub.status.idle":"2021-09-16T17:10:09.586892Z","shell.execute_reply.started":"2021-09-16T17:08:50.742149Z","shell.execute_reply":"2021-09-16T17:10:09.585817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.to_csv('Tabular Playground_Series_Sep_2021_train_processed_data.csv')\ntest.to_csv('Tabular Playground_Series_Sep_2021_test_processed_data.csv')\n#saving the preprocessed dataset ,so that it can be later used in another kernal.\n#It is for saving time, so that we can focus separately in data processing or model building","metadata":{"execution":{"iopub.status.busy":"2021-09-16T17:10:14.546905Z","iopub.execute_input":"2021-09-16T17:10:14.548157Z","iopub.status.idle":"2021-09-16T17:13:03.141524Z","shell.execute_reply.started":"2021-09-16T17:10:14.548085Z","shell.execute_reply":"2021-09-16T17:13:03.140446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}