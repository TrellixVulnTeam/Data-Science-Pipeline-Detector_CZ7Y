{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1 div class='alert alert-success'><center> TPS-Set: Feature Engineering</center></h1>\n\n![](https://storage.googleapis.com/kaggle-competitions/kaggle/26480/logos/header.png?t=2021-04-09-00-57-05)","metadata":{"ExecuteTime":{"end_time":"2021-09-03T02:54:45.753398Z","start_time":"2021-09-03T02:54:45.745397Z"},"papermill":{"duration":0.04007,"end_time":"2021-09-07T03:25:29.071604","exception":false,"start_time":"2021-09-07T03:25:29.031534","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# <div class=\"alert alert-success\">  0. IMPORTAÇÕES </div>","metadata":{"papermill":{"duration":0.039251,"end_time":"2021-09-07T03:25:29.149414","exception":false,"start_time":"2021-09-07T03:25:29.110163","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import warnings\nimport random\nimport os\nimport gc\nimport shap","metadata":{"ExecuteTime":{"end_time":"2021-09-03T02:55:34.274579Z","start_time":"2021-09-03T02:55:34.267579Z"},"papermill":{"duration":7.344564,"end_time":"2021-09-07T03:25:36.532198","exception":false,"start_time":"2021-09-07T03:25:29.187634","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-09-12T04:14:35.334641Z","iopub.execute_input":"2021-09-12T04:14:35.334958Z","iopub.status.idle":"2021-09-12T04:14:42.111964Z","shell.execute_reply.started":"2021-09-12T04:14:35.334929Z","shell.execute_reply":"2021-09-12T04:14:42.111153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas            as pd\nimport numpy             as np\nimport matplotlib.pyplot as plt \nimport seaborn           as sns\nimport joblib            as jb\nimport matplotlib.pyplot as plt","metadata":{"ExecuteTime":{"end_time":"2021-09-03T02:56:31.813996Z","start_time":"2021-09-03T02:56:31.802997Z"},"papermill":{"duration":0.092081,"end_time":"2021-09-07T03:25:36.663359","exception":false,"start_time":"2021-09-07T03:25:36.571278","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-09-12T04:14:42.113576Z","iopub.execute_input":"2021-09-12T04:14:42.113944Z","iopub.status.idle":"2021-09-12T04:14:42.160041Z","shell.execute_reply.started":"2021-09-12T04:14:42.113906Z","shell.execute_reply":"2021-09-12T04:14:42.159184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection   import train_test_split\nfrom sklearn.preprocessing     import QuantileTransformer\nfrom sklearn.cluster           import KMeans\nfrom yellowbrick.cluster       import KElbowVisualizer, SilhouetteVisualizer\nfrom scipy.stats.mstats        import winsorize\nfrom sklearn.decomposition     import PCA\nfrom sklearn.impute            import SimpleImputer\nfrom sklearn.metrics           import silhouette_samples, silhouette_score\nfrom sklearn.model_selection   import train_test_split, KFold, RepeatedStratifiedKFold, StratifiedKFold  \nfrom sklearn                   import metrics\nfrom prettytable               import PrettyTable","metadata":{"ExecuteTime":{"end_time":"2021-09-03T03:17:16.668053Z","start_time":"2021-09-03T03:17:16.339329Z"},"papermill":{"duration":0.167592,"end_time":"2021-09-07T03:25:36.868742","exception":false,"start_time":"2021-09-07T03:25:36.70115","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-09-12T04:14:42.161813Z","iopub.execute_input":"2021-09-12T04:14:42.162195Z","iopub.status.idle":"2021-09-12T04:14:42.272456Z","shell.execute_reply.started":"2021-09-12T04:14:42.162158Z","shell.execute_reply":"2021-09-12T04:14:42.271664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import xgboost                 as xgb","metadata":{"papermill":{"duration":0.107468,"end_time":"2021-09-07T03:25:37.015085","exception":false,"start_time":"2021-09-07T03:25:36.907617","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-09-12T04:14:42.273969Z","iopub.execute_input":"2021-09-12T04:14:42.27433Z","iopub.status.idle":"2021-09-12T04:14:42.33902Z","shell.execute_reply.started":"2021-09-12T04:14:42.274295Z","shell.execute_reply":"2021-09-12T04:14:42.338198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 0.2. Funções","metadata":{"ExecuteTime":{"end_time":"2021-09-03T02:58:45.163478Z","start_time":"2021-09-03T02:58:45.15543Z"},"papermill":{"duration":0.038897,"end_time":"2021-09-07T03:25:37.092489","exception":false,"start_time":"2021-09-07T03:25:37.053592","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def jupyter_setting():\n    \n    %matplotlib inline\n      \n    #os.environ[\"WANDB_SILENT\"] = \"true\" \n    #plt.style.use('bmh') \n    #plt.rcParams['figure.figsize'] = [20,15]\n    #plt.rcParams['font.size']      = 13\n     \n    pd.options.display.max_columns = None\n    #pd.set_option('display.expand_frame_repr', False)\n\n    warnings.filterwarnings(action='ignore')\n    warnings.simplefilter('ignore')\n    warnings.filterwarnings('ignore')\n    #warnings.filterwarnings(category=UserWarning)\n    \n    #pd.set_option('display.max_rows', 5)\n    #pd.set_option('display.max_columns', 500)\n    #pd.set_option('display.max_colwidth', None)\n\n    icecream = [\"#00008b\", \"#960018\",\"#008b00\", \"#00468b\", \"#8b4500\", \"#582c00\"]\n    #sns.palplot(sns.color_palette(icecream))\n    \n    return icecream\n\nicecream = jupyter_setting()\n\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\nwarnings.filterwarnings('ignore', category=FutureWarning)\nwarnings.filterwarnings('ignore', category=RuntimeWarning)\nwarnings.filterwarnings('ignore', category=UserWarning)\n#warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)","metadata":{"ExecuteTime":{"end_time":"2021-09-03T03:17:18.184927Z","start_time":"2021-09-03T03:17:18.147928Z"},"code_folding":[0],"papermill":{"duration":0.050507,"end_time":"2021-09-07T03:25:37.181354","exception":false,"start_time":"2021-09-07T03:25:37.130847","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-09-12T04:14:42.3405Z","iopub.execute_input":"2021-09-12T04:14:42.34085Z","iopub.status.idle":"2021-09-12T04:14:42.350557Z","shell.execute_reply.started":"2021-09-12T04:14:42.340815Z","shell.execute_reply":"2021-09-12T04:14:42.349469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def missing_zero_values_table(df):\n        mis_val         = df.isnull().sum()\n        mis_val_percent = round(df.isnull().mean().mul(100), 2)\n        mz_table        = pd.concat([mis_val, mis_val_percent], axis=1)\n        mz_table        = mz_table.rename(columns = {df.index.name:'col_name', \n                                                     0 : 'Valores ausentes', \n                                                     1 : '% de valores totais'})\n        \n        mz_table['Tipo de dados'] = df.dtypes\n        mz_table                  = mz_table[mz_table.iloc[:,1] != 0 ]. \\\n                                     sort_values('% de valores totais', ascending=False)\n        \n        msg = \"Seu dataframe selecionado tem {} colunas e {} \" + \\\n              \"linhas. \\nExistem {} colunas com valores ausentes.\"\n            \n        print (msg.format(df.shape[1], df.shape[0], mz_table.shape[0]))\n        \n        return mz_table.reset_index()","metadata":{"ExecuteTime":{"end_time":"2021-09-03T03:17:20.903258Z","start_time":"2021-09-03T03:17:20.883256Z"},"code_folding":[0],"papermill":{"duration":0.048044,"end_time":"2021-09-07T03:25:37.267553","exception":false,"start_time":"2021-09-07T03:25:37.219509","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-09-12T04:14:42.352211Z","iopub.execute_input":"2021-09-12T04:14:42.352564Z","iopub.status.idle":"2021-09-12T04:14:42.362275Z","shell.execute_reply.started":"2021-09-12T04:14:42.352528Z","shell.execute_reply":"2021-09-12T04:14:42.361443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reduce_memory_usage(df, verbose=True):\n    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n    start_mem = df.memory_usage().sum() / 1024 ** 2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if (\n                    c_min > np.finfo(np.float16).min\n                    and c_max < np.finfo(np.float16).max\n                ):\n                    df[col] = df[col].astype(np.float16)\n                elif (\n                    c_min > np.finfo(np.float32).min\n                    and c_max < np.finfo(np.float32).max\n                ):\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() / 1024 ** 2\n    if verbose:\n        print(\n            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n                end_mem, 100 * (start_mem - end_mem) / start_mem\n            )\n        )\n        \n    return df","metadata":{"execution":{"iopub.status.busy":"2021-09-12T04:14:42.363469Z","iopub.execute_input":"2021-09-12T04:14:42.36385Z","iopub.status.idle":"2021-09-12T04:14:42.377918Z","shell.execute_reply.started":"2021-09-12T04:14:42.363813Z","shell.execute_reply":"2021-09-12T04:14:42.377077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n    \n    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n    plt.rcParams['font.size'] = 12\n    plt.title('Precision Recall vs threshold')\n    plt.xlabel('Threshold')\n    plt.legend(loc=\"lower left\")\n    \n    plt.grid(True)","metadata":{"execution":{"iopub.status.busy":"2021-09-12T04:14:42.38104Z","iopub.execute_input":"2021-09-12T04:14:42.381536Z","iopub.status.idle":"2021-09-12T04:14:42.388826Z","shell.execute_reply.started":"2021-09-12T04:14:42.381402Z","shell.execute_reply":"2021-09-12T04:14:42.388084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_precision_vs_recall(precisions, recalls):\n    plt.plot(recalls[:-1], precisions[:-1], \"b-\", label=\"Precision\")\n    \n    plt.rcParams['font.size'] = 12\n    plt.title('Precision vs recall')\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    # plt.legend(loc=\"lower left\")\n    \n    plt.grid(True)","metadata":{"execution":{"iopub.status.busy":"2021-09-12T04:14:42.390931Z","iopub.execute_input":"2021-09-12T04:14:42.391232Z","iopub.status.idle":"2021-09-12T04:14:42.399197Z","shell.execute_reply.started":"2021-09-12T04:14:42.391202Z","shell.execute_reply":"2021-09-12T04:14:42.398392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_roc_curve(fpr, tpr, label=None):\n    fig, ax = plt.subplots()\n    ax.plot(fpr, tpr, \"r-\", label=label)\n    ax.plot([0, 1], [0, 1], transform=ax.transAxes, ls=\"--\", c=\".3\")\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.0])\n    plt.rcParams['font.size'] = 12\n    plt.title('XGBR ROC curve for TPS 09')\n    plt.xlabel('False Positive Rate (1 - Specificity)')\n    plt.ylabel('True Positive Rate (Sensitivity)')\n    plt.legend(loc=\"lower right\")\n    plt.grid(True)","metadata":{"execution":{"iopub.status.busy":"2021-09-12T04:14:42.40037Z","iopub.execute_input":"2021-09-12T04:14:42.400875Z","iopub.status.idle":"2021-09-12T04:14:42.408492Z","shell.execute_reply.started":"2021-09-12T04:14:42.40084Z","shell.execute_reply":"2021-09-12T04:14:42.407673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 0.3. Carregar Dados","metadata":{"papermill":{"duration":0.037872,"end_time":"2021-09-07T03:25:37.343153","exception":false,"start_time":"2021-09-07T03:25:37.305281","status":"completed"},"tags":[]}},{"cell_type":"code","source":"path = '../input/tabular-playground-series-sep-2021/'\n#path = '../input/tabular-playground-series-sep-2021/'\n#path = 'Data/'","metadata":{"ExecuteTime":{"end_time":"2021-09-03T03:17:22.020763Z","start_time":"2021-09-03T03:17:21.642689Z"},"papermill":{"duration":0.044207,"end_time":"2021-09-07T03:25:37.425666","exception":false,"start_time":"2021-09-07T03:25:37.381459","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-09-12T04:14:42.409773Z","iopub.execute_input":"2021-09-12T04:14:42.410185Z","iopub.status.idle":"2021-09-12T04:14:42.415935Z","shell.execute_reply.started":"2021-09-12T04:14:42.41015Z","shell.execute_reply":"2021-09-12T04:14:42.415027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1_train     = pd.read_csv(path + 'train.csv', index_col='id')\ndf1_test      = pd.read_csv(path + 'test.csv',index_col='id')\ndf_submission = pd.read_csv(path + 'sample_solution.csv')\n\ndf1_train.shape, df1_test.shape, df_submission.shape","metadata":{"ExecuteTime":{"end_time":"2021-09-03T03:10:15.136039Z","start_time":"2021-09-03T03:09:53.107041Z"},"papermill":{"duration":41.004582,"end_time":"2021-09-07T03:26:19.201867","exception":false,"start_time":"2021-09-07T03:25:38.197285","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-09-12T04:14:42.417395Z","iopub.execute_input":"2021-09-12T04:14:42.417763Z","iopub.status.idle":"2021-09-12T04:15:23.358971Z","shell.execute_reply.started":"2021-09-12T04:14:42.417712Z","shell.execute_reply":"2021-09-12T04:15:23.358198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1_test.head()","metadata":{"papermill":{"duration":0.159441,"end_time":"2021-09-07T03:26:19.401579","exception":false,"start_time":"2021-09-07T03:26:19.242138","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-09-12T04:15:23.360275Z","iopub.execute_input":"2021-09-12T04:15:23.360619Z","iopub.status.idle":"2021-09-12T04:15:23.470526Z","shell.execute_reply.started":"2021-09-12T04:15:23.360585Z","shell.execute_reply":"2021-09-12T04:15:23.469559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1_train = reduce_memory_usage(df1_train)\ndf1_test  = reduce_memory_usage(df1_test)\n\ndf1_train.shape, df1_test.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-12T04:15:23.472103Z","iopub.execute_input":"2021-09-12T04:15:23.47244Z","iopub.status.idle":"2021-09-12T04:15:47.391286Z","shell.execute_reply.started":"2021-09-12T04:15:23.472405Z","shell.execute_reply":"2021-09-12T04:15:47.390435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <div class=\"alert alert-success\"> 1. Feature Engineering </div>","metadata":{"papermill":{"duration":0.040419,"end_time":"2021-09-07T03:26:19.484639","exception":false,"start_time":"2021-09-07T03:26:19.44422","status":"completed"},"tags":[]}},{"cell_type":"code","source":"features_df = df1_train.columns.to_list()\nfeatures_df.remove('claim')","metadata":{"papermill":{"duration":0.047395,"end_time":"2021-09-07T03:26:19.572148","exception":false,"start_time":"2021-09-07T03:26:19.524753","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-09-12T04:15:47.392648Z","iopub.execute_input":"2021-09-12T04:15:47.393224Z","iopub.status.idle":"2021-09-12T04:15:47.39812Z","shell.execute_reply.started":"2021-09-12T04:15:47.393187Z","shell.execute_reply":"2021-09-12T04:15:47.39723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.1. Step 01","metadata":{"papermill":{"duration":0.040512,"end_time":"2021-09-07T03:26:19.652851","exception":false,"start_time":"2021-09-07T03:26:19.612339","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"A ideia de adicionar o recurso \"n_missing\" abaixo foi tirada deste [bloco notas](https://www.kaggle.com/maximkazantsev/tps-09-21-eda-lightgbm-with-folds) de por [BIZEN](https://www.kaggle.com/hiro5299834) .","metadata":{"papermill":{"duration":0.0402,"end_time":"2021-09-07T03:26:19.733482","exception":false,"start_time":"2021-09-07T03:26:19.693282","status":"completed"},"tags":[]}},{"cell_type":"code","source":"df1_train['fe_n_missing'] = df1_train[features_df].isna().sum(axis=1)\ndf1_test['fe_n_missing']  = df1_test[features_df].isna().sum(axis=1)\n\ndf1_train.shape, df1_test.shape","metadata":{"papermill":{"duration":0.714804,"end_time":"2021-09-07T03:26:20.488546","exception":false,"start_time":"2021-09-07T03:26:19.773742","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-09-12T04:15:47.399448Z","iopub.execute_input":"2021-09-12T04:15:47.399829Z","iopub.status.idle":"2021-09-12T04:15:48.540898Z","shell.execute_reply.started":"2021-09-12T04:15:47.399774Z","shell.execute_reply":"2021-09-12T04:15:48.540127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_df.append('fe_n_missing')","metadata":{"papermill":{"duration":0.047498,"end_time":"2021-09-07T03:26:20.57734","exception":false,"start_time":"2021-09-07T03:26:20.529842","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-09-12T04:15:48.54221Z","iopub.execute_input":"2021-09-12T04:15:48.54253Z","iopub.status.idle":"2021-09-12T04:15:48.546896Z","shell.execute_reply.started":"2021-09-12T04:15:48.542495Z","shell.execute_reply":"2021-09-12T04:15:48.546022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.1. Step 02 \nTratamento de valores faltantes, para os valores faltantes vamos imputar a mediana. ","metadata":{"papermill":{"duration":0.04052,"end_time":"2021-09-07T03:26:20.658398","exception":false,"start_time":"2021-09-07T03:26:20.617878","status":"completed"},"tags":[]}},{"cell_type":"code","source":"imputer   = SimpleImputer(missing_values=np.nan, strategy='median')\ndf1_train = pd.DataFrame(imputer.fit_transform(df1_train), columns=df1_train.columns)\ndf1_test  = pd.DataFrame(imputer.fit_transform(df1_test),  columns=df1_test.columns)\ngc.collect()","metadata":{"ExecuteTime":{"end_time":"2021-09-03T03:18:25.993971Z","start_time":"2021-09-03T03:18:22.611432Z"},"papermill":{"duration":38.726024,"end_time":"2021-09-07T03:26:59.425156","exception":false,"start_time":"2021-09-07T03:26:20.699132","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing_zero_values_table(df1_train)","metadata":{"papermill":{"duration":0.424719,"end_time":"2021-09-07T03:26:59.891617","exception":false,"start_time":"2021-09-07T03:26:59.466898","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing_zero_values_table(df1_test)","metadata":{"papermill":{"duration":0.238348,"end_time":"2021-09-07T03:27:00.171594","exception":false,"start_time":"2021-09-07T03:26:59.933246","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.3. Step 03\nAqui vamos criar duas variáveis que indica os valores estão próximo de zero ou não. ","metadata":{"papermill":{"duration":0.042712,"end_time":"2021-09-07T03:27:00.261964","exception":false,"start_time":"2021-09-07T03:27:00.219252","status":"completed"},"tags":[]}},{"cell_type":"code","source":"df1_train['fe_f5_zero']  = (df1_train['f5'] < 0.02).astype(int)\ndf1_test['fe_f5_zero']   = (df1_test['f5'] < 0.02).astype(int)\ndf1_train['fe_f50_zero'] = (df1_train['f50'] < 0.02).astype(int)\ndf1_test['fe_f50_zero']  = (df1_test['f50'] < 0.02).astype(int)","metadata":{"papermill":{"duration":0.063728,"end_time":"2021-09-07T03:27:00.369016","exception":false,"start_time":"2021-09-07T03:27:00.305288","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Agora vamos criar algumas variáveis de estatística descritiva. ","metadata":{"papermill":{"duration":0.041203,"end_time":"2021-09-07T03:27:00.451976","exception":false,"start_time":"2021-09-07T03:27:00.410773","status":"completed"},"tags":[]}},{"cell_type":"code","source":"df1_train['fe_mean']    = df1_train.mean(axis=1)\ndf1_test['fe_mean']     = df1_test.mean(axis=1)\n\ndf1_train['fe_median']  = df1_train.median(axis=1)\ndf1_test['fe_median']   = df1_test.median(axis=1)\n\ndf1_train['fe_std']     = df1_train.std(axis=1)\ndf1_test['fe_std']      = df1_test.std(axis=1)\n\ndf1_train['fe_min']     = df1_train.min(axis=1)\ndf1_test['fe_min']      = df1_test.min(axis=1)\n\ndf1_train['fe_max']     = df1_train.max(axis=1)\ndf1_test['fe_max']      = df1_test.max(axis=1)\n\ndf1_train['fe_skew']    = df1_train.skew(axis=1)\ndf1_test['fe_skew']     = df1_test.skew(axis=1)\n\ndf1_train.shape, df1_test.shape","metadata":{"papermill":{"duration":8.139572,"end_time":"2021-09-07T03:27:08.634129","exception":false,"start_time":"2021-09-07T03:27:00.494557","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.4. Step 04 \nNeste step vamos criar uma nova variável com o algoritmo `kmeans`, para quem conhence o algoritmo vamos fazer uma clusterização, mas em primeiro lugar vamos criar algumas variáveis com uma PCD com as 40 principais componentes para a clusterização.","metadata":{"papermill":{"duration":0.041688,"end_time":"2021-09-07T03:27:08.717902","exception":false,"start_time":"2021-09-07T03:27:08.676214","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### 2.4.1. Scaler ","metadata":{"papermill":{"duration":0.041318,"end_time":"2021-09-07T03:27:08.801088","exception":false,"start_time":"2021-09-07T03:27:08.75977","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"Vamos fazer o scaler dos datasets com o `QuatileTransforme` que teve melhor resultado na validação cruzada e no kaggle com AUC de 0.74479 com XGB, que se encontra no [notebook anterior](https://www.kaggle.com/rogeriodelfim/01-tps-set-ponto-de-partida-eda-e-linha-de-base?scriptVersionId=73995146).  ","metadata":{"papermill":{"duration":0.041576,"end_time":"2021-09-07T03:27:08.884273","exception":false,"start_time":"2021-09-07T03:27:08.842697","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%%time\nscaler        = QuantileTransformer(output_distribution='normal', random_state=0)\n\ndf1_train_qt  = df1_train.copy().drop('claim', axis=1) \ncols_quantile = df1_train_qt.columns\n\ndf1_train_qt  = pd.DataFrame(scaler.fit_transform(df1_train_qt), columns=cols_quantile)\ndf1_test_qt   = pd.DataFrame(scaler.fit_transform(df1_test), columns=cols_quantile )","metadata":{"papermill":{"duration":53.168046,"end_time":"2021-09-07T03:28:02.09447","exception":false,"start_time":"2021-09-07T03:27:08.926424","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.4.2. Gerar PCA\nNesta etapa vamos criar 20 componente principal que serão serão utilizadas na clusterização, normalmente as primeiras componentes de uma PCA resumem a maior parte da variância dos dados, no caso desse conjunto de dados vamos precisamos de 20 à 40 compontes para termos uma melhor explicabilidade da variância dos dados.  ","metadata":{"papermill":{"duration":0.041815,"end_time":"2021-09-07T03:28:02.179557","exception":false,"start_time":"2021-09-07T03:28:02.137742","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%%time\npca           = PCA(random_state=123)\ndf1_train_pca = pd.DataFrame(pca.fit_transform(df1_train_qt))","metadata":{"papermill":{"duration":9.950958,"end_time":"2021-09-07T03:28:12.172354","exception":false,"start_time":"2021-09-07T03:28:02.221396","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(12,8))\nfig.subplots_adjust(wspace=.4, hspace=.4)\n\nax = fig.add_subplot(2, 1, 1)\n\nax.bar(range(1, 1+pca.n_components_),\n       pca.explained_variance_ratio_,\n       color='#FFB13F')\n\nax.set(xticks=[1, 2, 3, 4])\nplt.yticks(np.arange(0, 1.1, 0.1))\nplt.title('Variância explicada', fontsize=15)\nplt.xlabel('Componentes principais', fontsize=13)\nplt.ylabel('% de variância explicada', fontsize=13)\n\nax = fig.add_subplot(2, 1, 2)\nax.bar(range(1, 1+pca.n_components_),\n       np.cumsum(pca.explained_variance_ratio_),\n       color='#FFB13F')\n\nax.set(xticks=[1, 2, 3, 4])\nplt.yticks(np.arange(0, 1.1, 0.1))\nplt.title('Variância explicada cumulativa', fontsize=15)\nplt.xlabel('Componentes principais', fontsize=13)\nplt.ylabel('% de variância explicada', fontsize=13)\nplt.show()\n\nt = PrettyTable(['Componente',\n                 'Variância explicada',\n                 'Variância explicada cumulativa'])\n\nprincipal_component = 1\ncum_explained_var   = 0\n\nfor explained_var in pca.explained_variance_ratio_:\n    cum_explained_var += explained_var\n    t.add_row([principal_component, explained_var, cum_explained_var])\n    principal_component += 1\n\nprint(t)","metadata":{"papermill":{"duration":0.731974,"end_time":"2021-09-07T03:28:12.946987","exception":false,"start_time":"2021-09-07T03:28:12.215013","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**`NOTA:`**\n\nNo primeiro gráfico observamos que os quatros primeiro compontes são pouco relevantes, com pouca explicabilidade, como podemos observar no segundo gráfico e na tabela de variância acumulada, sendo assim, vou selecionar 20 componeste para a clusterização, pois a explicabilidade chega à 35.45%.","metadata":{"papermill":{"duration":0.043394,"end_time":"2021-09-07T03:28:13.034328","exception":false,"start_time":"2021-09-07T03:28:12.990934","status":"completed"},"tags":[]}},{"cell_type":"code","source":"n_components  = 20\npca           = PCA(n_components=n_components, random_state=123)\npca_feats     = [f'feature_pca_{i}' for i in range(n_components)]\ndf1_train_pca = pd.DataFrame(pca.fit_transform(df1_train_qt), columns=pca_feats)\ndf1_test_pca  = pd.DataFrame(pca.fit_transform(df1_test_qt) , columns=pca_feats)\n\ndf1_train_pca.head()","metadata":{"papermill":{"duration":19.50547,"end_time":"2021-09-07T03:28:32.586093","exception":false,"start_time":"2021-09-07T03:28:13.080623","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Pronto agora vamos para a próxima etapa que é gerar a variável de clusterização. ","metadata":{"papermill":{"duration":0.044374,"end_time":"2021-09-07T03:28:32.676103","exception":false,"start_time":"2021-09-07T03:28:32.631729","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### 2.1.2. Clusterização\n\nNesta etapa vamos cria uma nova variável com utilização do algoritmo `kmeams`, que tem a finalidade de criar clusters, vamos utilizar as variáveis criadas no processo anterior `2.4.2. Gerar PCA`.","metadata":{"papermill":{"duration":0.044466,"end_time":"2021-09-07T03:28:32.76463","exception":false,"start_time":"2021-09-07T03:28:32.720164","status":"completed"},"tags":[]}},{"cell_type":"code","source":"df1_train  = df1_train.copy()\ndf1_test   = df1_test.copy()\ndf1_train.shape, df1_test.shape","metadata":{"papermill":{"duration":0.57501,"end_time":"2021-09-07T03:28:33.384187","exception":false,"start_time":"2021-09-07T03:28:32.809177","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time \ngc.collect()\n\nplt.figure(figsize=(12, 7))\nvisualizer_1 = KElbowVisualizer(KMeans(random_state=59), k=(2,12))\nvisualizer_1.fit(df1_train_pca);\nvisualizer_1.poof();","metadata":{"papermill":{"duration":187.869218,"end_time":"2021-09-07T03:31:41.298105","exception":false,"start_time":"2021-09-07T03:28:33.428887","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Como podemos observar no gráfico acima, precisamos criar um modelo para clusterização de 4 clusters, então vamos fazer isso agora nos dados de treino e replicar para os dados de teste. ","metadata":{"papermill":{"duration":0.047028,"end_time":"2021-09-07T03:31:41.391938","exception":false,"start_time":"2021-09-07T03:31:41.34491","status":"completed"},"tags":[]}},{"cell_type":"code","source":"gc.collect()\n\nmodel_kmeans = KMeans(n_clusters=4, random_state=59)\nmodel_kmeans.fit(df1_train_pca);\n\nclusters_train = model_kmeans.predict(df1_train_pca)\nclusters_test  = model_kmeans.predict(df1_test_pca)\n\ndf1_train['cluster'] = clusters_train\ndf1_test['cluster']  = clusters_test\n\ndf1_train.shape, df1_test.shape","metadata":{"papermill":{"duration":7.969221,"end_time":"2021-09-07T03:31:49.407732","exception":false,"start_time":"2021-09-07T03:31:41.438511","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Vamos salvar os dataset, caso seja necessário refazer o processo podemos partir desse ponto. ","metadata":{"papermill":{"duration":0.047022,"end_time":"2021-09-07T03:31:49.502604","exception":false,"start_time":"2021-09-07T03:31:49.455582","status":"completed"},"tags":[]}},{"cell_type":"code","source":"!mkdir Data\n!mkdir Data/pkl\n!mkdir Data/sumbmission\n!mkdir model\n!mkdir model/preds\n!mkdir model/optuna","metadata":{"papermill":{"duration":2.053184,"end_time":"2021-09-07T03:31:51.603618","exception":false,"start_time":"2021-09-07T03:31:49.550434","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"jb.dump(df1_train,  \"Data/pkl/df2_train.pkl.z\")\njb.dump(df1_test,  \"Data/pkl/df2_test.pkl.z\")","metadata":{"papermill":{"duration":98.40597,"end_time":"2021-09-07T03:33:30.057336","exception":false,"start_time":"2021-09-07T03:31:51.651366","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"papermill":{"duration":0.275331,"end_time":"2021-09-07T03:33:30.380242","exception":false,"start_time":"2021-09-07T03:33:30.104911","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <div class=\"alert alert-success\"> 2. Split Train/Test </div>","metadata":{"papermill":{"duration":0.04712,"end_time":"2021-09-07T03:33:30.511949","exception":false,"start_time":"2021-09-07T03:33:30.464829","status":"completed"},"tags":[]}},{"cell_type":"code","source":"df2_train = df1_train.copy()\ndf2_test  = df1_test.copy() \n\n# df2_train     = jb.load('./Data/pkl/df2_train.pkl.z')\n# df2_test      = jb.load('./Data/pkl/df2_test.pkl.z')\n# df_submission = pd.read_csv('../input/tabular-playground-series-sep-2021/sample_solution.csv')\n\ndf2_train.shape, df2_test.shape","metadata":{"papermill":{"duration":0.580141,"end_time":"2021-09-07T03:33:31.139468","exception":false,"start_time":"2021-09-07T03:33:30.559327","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X      = df2_train.drop(['claim'], axis=1)\ny      = df2_train['claim']\ncols   = X.columns\nX_test = df2_test\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, \n                                                      test_size    = 0.2,\n                                                      shuffle      = True, \n                                                      stratify     = y,\n                                                      random_state = 0)\n\nX_train.shape, y_train.shape, X_valid.shape, y_valid.shape ","metadata":{"papermill":{"duration":2.70451,"end_time":"2021-09-07T03:33:33.891867","exception":false,"start_time":"2021-09-07T03:33:31.187357","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <div class=\"alert alert-success\"> 3. Modelagem </div>","metadata":{"papermill":{"duration":0.048072,"end_time":"2021-09-07T03:33:33.988163","exception":false,"start_time":"2021-09-07T03:33:33.940091","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"Parametros do algoritmo","metadata":{"papermill":{"duration":0.047823,"end_time":"2021-09-07T03:33:34.083284","exception":false,"start_time":"2021-09-07T03:33:34.035461","status":"completed"},"tags":[]}},{"cell_type":"code","source":"params = {'random_state': 0,          \n          'predictor'   : 'gpu_predictor',\n          'tree_method' : 'gpu_hist',\n          'eval_metric' : 'auc'}","metadata":{"papermill":{"duration":0.0553,"end_time":"2021-09-07T03:33:34.186423","exception":false,"start_time":"2021-09-07T03:33:34.131123","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_xgb = xgb.XGBClassifier(**params)\nmodel_xgb.fit(X_train, y_train)\n\ny_hat_prob = model_xgb.predict_proba(X_valid)[:, 1]\ny_hat      = (y_hat_prob >.5).astype(int)\n\nfpr, tpr, thresholds = metrics.roc_curve(y_valid, y_hat_prob)\nlog_loss_     = metrics.log_loss(y_valid, y_hat_prob)                \nf1_score_     = metrics.f1_score(y_valid, y_hat)        \nauc_          = metrics.auc(fpr, tpr)    \n\nprint('AUC: {:.5f} - F1: {:.5f} - L. LOSS: {:.5f}'.format(auc_, f1_score_, log_loss_))\nprint('')","metadata":{"papermill":{"duration":9.508661,"end_time":"2021-09-07T03:33:43.743387","exception":false,"start_time":"2021-09-07T03:33:34.234726","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Obtivemos uma `AUC` de `0.81061`, isso que dizer que o acrescimo de novas variáveis nos dataset ajudou, porém precisams fazer uma validação mais robusta (validação cruzada) em vez de uma separação simples (Holdout), na validação cruzada vamos ter uma estimativa mais consistente.  ","metadata":{"papermill":{"duration":0.048049,"end_time":"2021-09-07T03:33:43.842441","exception":false,"start_time":"2021-09-07T03:33:43.794392","status":"completed"},"tags":[]}},{"cell_type":"code","source":"print(metrics.classification_report(y_valid, (y_hat_prob >.35).astype(int)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics.plot_confusion_matrix(model_xgb, X_valid, y_valid, cmap='inferno')\nplt.title('Confusion matrix')\nplt.grid(False)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"precisions, recalls, thresholds = metrics.precision_recall_curve(y_valid, (y_hat_prob >.45).astype(int))\n\nplot_precision_recall_vs_threshold(precisions, recalls, thresholds)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_precision_vs_recall(precisions, recalls)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"threshold = .37\ny_hat_threshold = (y_hat_prob>threshold).astype(int)\nfpr, tpr, thresholds = metrics. roc_curve(y_valid, y_hat_threshold)\n\nplot_roc_curve(fpr, tpr, label=\"XGB\")\nplt.show()\n\nprint('F1-score: {:2.5f}'.format(metrics.f1_score(y_valid, y_hat)))\nprint('F1-score: {:2.5f} threshold({:2.2f})'.format(metrics.f1_score(y_valid, y_hat_threshold), threshold))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n## 4.1. Validação Cruzada","metadata":{"papermill":{"duration":0.048252,"end_time":"2021-09-07T03:33:43.939526","exception":false,"start_time":"2021-09-07T03:33:43.891274","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%%time\nX_train     = df2_train.drop(['claim'], axis=1)\ny_train     = df2_train['claim']\n\nparams = {'random_state': 0,          \n          'predictor'   : 'gpu_predictor',\n          'tree_method' : 'gpu_hist',\n          'eval_metric' : 'auc'}\n\ngc.collect()\n\nX_train_oof_shap = np.zeros((X_train.shape[0],X_train.shape[1]+1))\nfeature          = X_test.columns.to_list()\nscalers          = [#QuantileTransformer(output_distribution='uniform', random_state=0),\n                    QuantileTransformer(output_distribution='normal' , random_state=0)]\n\nfeature.append('claim')\n\nX_ts = pd.DataFrame(scaler.fit_transform(X_test.copy()), columns=X_test.columns)\n\nSEED = 59 \n\ngc.collect()\n\nX_train_oof_shap = np.zeros((X_train.shape[0],X_train.shape[1]+2))\nfeature          = X_test.columns.to_list()\nscalers          = [#QuantileTransformer(output_distribution='uniform', random_state=0),\n                    QuantileTransformer(output_distribution='normal' , random_state=0)]\n\nfeature.append('claim')","metadata":{"papermill":{"duration":0.356176,"end_time":"2021-09-07T03:33:44.344969","exception":false,"start_time":"2021-09-07T03:33:43.988793","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nfor scaler in scalers: \n\n    FOLDS               = 5\n    df_submission.claim = 0\n    auc                 = []\n    lloss               = []\n    f1                  = []\n    model_feature_imp   = 0 \n    scale_distribution  = str.upper(scaler.get_params(True)['output_distribution']) \n    \n    kfold               = KFold(n_splits = FOLDS, random_state = 0, shuffle = True)\n \n    print('='*80)\n    print('Scaler: {} => {}'.format(scaler, scale_distribution))\n    print('='*80)\n\n    cols_model_base = ['fe_f50_zero', 'fe_f5_zero', 'fe_n_missing', 'cluster', \n                       'fe_std', 'f105', 'f102', 'f22', 'f79', 'f106', 'f71', \n                       'f77', 'f69', #'f38','f57','f40', 'f1', 'f26', 'f82'\n                      ]\n    \n    for i, (train_idx, test_idx) in enumerate(kfold.split(X_train)):\n\n        i+=1\n        \n        X_tr, y_tr = X_train.iloc[train_idx], y_train.iloc[train_idx]\n        X_vl, y_vl = X_train.iloc[test_idx], y_train.iloc[test_idx]\n        \n        X_tr = pd.DataFrame(scaler.fit_transform(X_tr), columns=cols) \n        X_vl = pd.DataFrame(scaler.fit_transform(X_vl), columns=cols)                \n        \n        # # Treinar o modelo baseline\n        model_base = xgb.XGBClassifier(**params)\n        model_base.fit(X_tr[cols_model_base], y_tr)\n        \n        threshold = .5\n        \n        y_hat_prob_tr_mb = (model_base.predict_proba(X_tr[cols_model_base])[:, 1]>threshold).astype(int) \n        y_hat_prob_vl_mb = (model_base.predict_proba(X_vl[cols_model_base])[:, 1]>threshold).astype(int) \n        y_hat_prob_ts_mb = (model_base.predict_proba(X_ts[cols_model_base])[:, 1]>threshold).astype(int) \n        \n        X_tr['fe_md_baseline'] = y_hat_prob_tr_mb \n        X_vl['fe_md_baseline'] = y_hat_prob_vl_mb\n        X_ts['fe_md_baseline'] = y_hat_prob_ts_mb\n        \n        params_xgb = {#'n_estimators'    : 500,\n                      'max_depth'        : 7,\n                      'min_child_weight' : 40,\n                      'subsample'        : 0.662776837991846,\n                      'colsample_bynode' : 0.7392362752914446,\n                      'learning_rate'    : 0.18833941612449953,\n                      'colsample_bytree' : 0.6094875461582554,\n                      'reg_lambda'       : 10,\n                      'reg_alpha'        : 42,\n                      'eta'              : 0.07162137592923906,\n                      'alpha'            : 0.5197749697495789}\n\n        # Treinar XGB com a variável fe_md_baseline \n        model = xgb.XGBClassifier(**params_xgb, \n                                  objective         = 'binary:logistic',                  \n                                  predictor         = 'gpu_predictor',\n                                  tree_method       = 'gpu_hist',\n                                  eval_metric       = 'auc', \n                                  random_state      = 59 \n                                 ) \n\n        model.fit(X_tr, y_tr)\n\n        y_hat_prob = model.predict_proba(X_vl)[:, 1]\n        y_hat      = (y_hat_prob >.5).astype(int) \n        \n        fpr, tpr, thresholds = metrics.roc_curve(y_vl, y_hat_prob)\n                 \n        log_loss_     = metrics.log_loss(y_vl, y_hat_prob)                \n        f1_score_     = metrics.f1_score(y_vl, y_hat)        \n        auc_          = metrics.auc(fpr, tpr)    \n                \n        msg = '[Fold {}] AUC: {:.5f} - F1: {:.5f} - L. LOSS: {:.5f}'\n        print(msg.format(i, auc_, f1_score_, log_loss_))\n\n        df_submission.claim += model.predict_proba(X_ts)[:, 1]/FOLDS\n        model_feature_imp   += model.feature_importances_ / FOLDS\n        \n        X_train_oof_shap[test_idx, :] = np.hstack([X_vl , y_hat.reshape(-1, 1)])\n                \n        f1.append(f1_score_)\n        lloss.append(log_loss_)\n        auc.append(auc_)\n        \n    auc_mean   = np.mean(auc)\n    auc_std    = np.std(auc)\n    lloss_mean = np.mean(lloss)\n    f1_mean    = np.mean(f1)\n    \n    print('-'*80)\n    msg = '[Mean Fold] AUC: {:.5f}(Std:{:.5f}) - F1: {:.5f} - L. LOSS: {:.5f}'\n    print(msg.format(auc_mean, auc_std, f1_mean, lloss_mean))\n    print('='*80)\n    print('')\n    \n    # Gerar o arquivo de submissão \n    name_file_subm = str(scaler).lower()[:4] + '_' + scale_distribution \n    name_file_subm = 'Data/sumbmission/001_xgb_feature_engineering_' + name_file_subm + '.csv'\n    \n    df_submission.to_csv(name_file_subm, index=False)\n    \n    gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ================================================================================\n# Scaler: QuantileTransformer(output_distribution='normal', random_state=0) => NORMAL\n# ================================================================================\n# [Fold 1] AUC: 0.81068 - F1: 0.79229 - L. LOSS: 0.51150\n# [Fold 2] AUC: 0.76741 - F1: 0.73193 - L. LOSS: 0.59884\n# [Fold 3] AUC: 0.76947 - F1: 0.74404 - L. LOSS: 0.58815\n# [Fold 4] AUC: 0.76998 - F1: 0.73370 - L. LOSS: 0.59284\n# [Fold 5] AUC: 0.81071 - F1: 0.79160 - L. LOSS: 0.51071\n# --------------------------------------------------------------------------------\n# [Mean Fold] AUC: 0.78565(Std:0.02047) - F1: 0.75871 - L. LOSS: 0.56041\n# ================================================================================\n# \n# CPU times: user 3min 55s, sys: 10.8 s, total: 4min 5s\n# Wall time: 3min 59s","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-info\" role=\"alert\">\n    \n**`NOTA:`**\n\nComo podemos obsevar acima, a média de AUC na validação cruzada foi de 0.78552 e na submissão do kaggle foi uma AUC de 0.80771, sem ajustes nos parametros do XGB.  ","metadata":{"papermill":{"duration":0.069147,"end_time":"2021-09-07T03:37:37.805905","exception":false,"start_time":"2021-09-07T03:37:37.736758","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## 4.2. Feature importances","metadata":{"papermill":{"duration":0.077846,"end_time":"2021-09-07T03:37:37.965495","exception":false,"start_time":"2021-09-07T03:37:37.887649","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"Nesta etapa vamos utilizar duas formas de verificar a importância das variáveis para o modelo: \n- As feature importance (feature_importances_) do próprio modelo \n- Shap","metadata":{"papermill":{"duration":0.07929,"end_time":"2021-09-07T03:37:38.12355","exception":false,"start_time":"2021-09-07T03:37:38.04426","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### 4.2.1. Feature_importances_ ","metadata":{"papermill":{"duration":0.081279,"end_time":"2021-09-07T03:37:38.301412","exception":false,"start_time":"2021-09-07T03:37:38.220133","status":"completed"},"tags":[]}},{"cell_type":"code","source":"cols_feature    = X.columns.to_list()\ncols_feature.append('fe_md_baseline')\n\ndf               = pd.DataFrame()\ndf[\"Feature\"]    = cols_feature\ndf[\"Importance\"] = model_feature_imp / model_feature_imp.sum()\n\ndf.sort_values(\"Importance\", axis=0, ascending=False, inplace=True)\n\nfig, ax = plt.subplots(figsize=(10, 30))\nbars = ax.barh(df[\"Feature\"], df[\"Importance\"], \n               height    = 0.8,\n               color     = \"blue\",  # mediumorchid\n               edgecolor = \"black\")\n\nax.set_title(\"Feature importances \\n\", fontsize=30, pad=15)\nax.set_ylabel(\"Feature name\", fontsize=20, labelpad=15)\nax.set_xlabel(\"Importance\", fontsize=15, labelpad=15)\nax.set_yticks(df[\"Feature\"])\nax.set_yticklabels(df[\"Feature\"], fontsize=15)\nax.tick_params(axis=\"x\", labelsize=15)\nax.grid(axis=\"x\")\n\n# Adding labels on top\nax2 = ax.secondary_xaxis('top')\n#ax2.set_xlabel(\"Feature importance\", fontsize=20, labelpad=15)\nax2.tick_params(axis=\"x\", labelsize=15)\n\n# Inverting y axis direction so the values are decreasing\nplt.gca().invert_yaxis()","metadata":{"papermill":{"duration":2.555072,"end_time":"2021-09-07T03:37:40.938722","exception":false,"start_time":"2021-09-07T03:37:38.38365","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Acima podemos observar que algumas das variáveis que criamos estão entre as 25 variáveis mais importantes para o modelo, porém não gosto muito de jeito de analisar a importância das variáveis, vamos utilizar o `Shap` que utilza vários métodos diferentes para encontrar as variáveis importantes. ","metadata":{"papermill":{"duration":0.052472,"end_time":"2021-09-07T03:37:41.04507","exception":false,"start_time":"2021-09-07T03:37:40.992598","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## Shap\n\nSHAP (SHapley Additive exPlanations) é uma abordagem teórica de jogos para explicar a saída de qualquer modelo de aprendizado de máquina, ver o  [artigos](https://github.com/slundberg/shap#citations) para detalhes e citações.","metadata":{"papermill":{"duration":0.052339,"end_time":"2021-09-07T03:37:41.149896","exception":false,"start_time":"2021-09-07T03:37:41.097557","status":"completed"},"tags":[]}},{"cell_type":"code","source":"shap.summary_plot(X_train_oof_shap[:,:-1], cols_feature,)","metadata":{"papermill":{"duration":152.503489,"end_time":"2021-09-07T03:40:13.713128","exception":false,"start_time":"2021-09-07T03:37:41.209639","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Como podemo observar a ordem das variáveis importantes para o modelo mudarão, agora temos as 4 primeira variáveis que foram criadas como principais, no notebook de feature select vou utilizar o `Shap` para analise da seleção das variáveis. ","metadata":{"papermill":{"duration":0.054827,"end_time":"2021-09-07T03:40:13.843192","exception":false,"start_time":"2021-09-07T03:40:13.788365","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"Vamos salvar o dataset para o próximo notebook, que será a tunagem de parametros. ","metadata":{"papermill":{"duration":0.054978,"end_time":"2021-09-07T03:40:13.953192","exception":false,"start_time":"2021-09-07T03:40:13.898214","status":"completed"},"tags":[]}},{"cell_type":"code","source":"jb.dump(df2_train,  \"Data/pkl/df3_train.pkl.z\")\njb.dump(df2_test,  \"Data/pkl/df3_test.pkl.z\")\n\ngc.collect()","metadata":{"papermill":{"duration":100.755506,"end_time":"2021-09-07T03:41:54.763746","exception":false,"start_time":"2021-09-07T03:40:14.00824","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- ","metadata":{"papermill":{"duration":0.058744,"end_time":"2021-09-07T03:41:54.881541","exception":false,"start_time":"2021-09-07T03:41:54.822797","status":"completed"},"tags":[]}}]}