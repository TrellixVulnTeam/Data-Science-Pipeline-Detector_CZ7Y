{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Extreme Gradient Boosting Model testing\nAim of this notebook is to review the extreme gradient boosting model which can be used during a binary classification challenge.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-22T14:53:22.163691Z","iopub.execute_input":"2021-09-22T14:53:22.164066Z","iopub.status.idle":"2021-09-22T14:53:22.181038Z","shell.execute_reply.started":"2021-09-22T14:53:22.163988Z","shell.execute_reply":"2021-09-22T14:53:22.179961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Extreme Gradient Boosting","metadata":{}},{"cell_type":"code","source":"# Import modules for model analysis\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\n\n# Import xgb modules\nimport xgboost as xgb","metadata":{"execution":{"iopub.status.busy":"2021-09-22T14:53:22.182767Z","iopub.execute_input":"2021-09-22T14:53:22.183122Z","iopub.status.idle":"2021-09-22T14:53:23.16588Z","shell.execute_reply.started":"2021-09-22T14:53:22.183086Z","shell.execute_reply":"2021-09-22T14:53:23.165087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read in the data\ntrain = pd.read_csv('../input/tabular-playground-series-sep-2021/train.csv',index_col=0)\ntest  = pd.read_csv('../input/tabular-playground-series-sep-2021/test.csv', index_col=0)\n\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-22T14:53:23.168014Z","iopub.execute_input":"2021-09-22T14:53:23.168335Z","iopub.status.idle":"2021-09-22T14:54:04.822553Z","shell.execute_reply.started":"2021-09-22T14:53:23.168302Z","shell.execute_reply":"2021-09-22T14:54:04.821715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the memory consumed by the DataFrame\ntrain.info(memory_usage='deep')","metadata":{"execution":{"iopub.status.busy":"2021-09-22T14:54:04.824083Z","iopub.execute_input":"2021-09-22T14:54:04.824455Z","iopub.status.idle":"2021-09-22T14:54:04.843541Z","shell.execute_reply.started":"2021-09-22T14:54:04.82442Z","shell.execute_reply":"2021-09-22T14:54:04.842445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Memory usage by variable in MB\ntrain.memory_usage(deep=True) * 1e-6","metadata":{"execution":{"iopub.status.busy":"2021-09-22T14:54:04.844845Z","iopub.execute_input":"2021-09-22T14:54:04.845224Z","iopub.status.idle":"2021-09-22T14:54:04.872577Z","shell.execute_reply.started":"2021-09-22T14:54:04.845187Z","shell.execute_reply":"2021-09-22T14:54:04.871734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets reduce the memory usage of the features\n# First - check the integer values and downcast\ndef int_downcast(df):\n    int_cols = df.select_dtypes(include=['int64'])\n\n    for col in int_cols.columns:\n        print(col, 'min:',df[col].min(),'; max:',df[col].max())\n        df[col] = pd.to_numeric(df[col], downcast ='integer')\n    return df\n\nint_downcast(train)\ntrain.memory_usage(deep=True) * 1e-6","metadata":{"execution":{"iopub.status.busy":"2021-09-22T14:54:04.873885Z","iopub.execute_input":"2021-09-22T14:54:04.874264Z","iopub.status.idle":"2021-09-22T14:54:04.910898Z","shell.execute_reply.started":"2021-09-22T14:54:04.874227Z","shell.execute_reply":"2021-09-22T14:54:04.909729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Second - check the float values and downcast. Method will have to be applied to the train and test DataFrames\ndef float_downcast(df):\n    float_cols = df.select_dtypes(include=['float64'])\n\n    for col in float_cols.columns:\n#         print(col, 'min:',df[col].min(),'; max:',df[col].max())\n        df[col] = pd.to_numeric(df[col], downcast ='float')\n    return df\n\nfloat_downcast(train)\nfloat_downcast(test)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-09-22T14:54:04.912448Z","iopub.execute_input":"2021-09-22T14:54:04.912802Z","iopub.status.idle":"2021-09-22T14:54:28.394246Z","shell.execute_reply.started":"2021-09-22T14:54:04.912765Z","shell.execute_reply":"2021-09-22T14:54:28.39342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the memory usage by feature\ntrain.memory_usage(deep=True) * 1e-6\ntest.memory_usage(deep=True) * 1e-6","metadata":{"execution":{"iopub.status.busy":"2021-09-22T14:54:28.396975Z","iopub.execute_input":"2021-09-22T14:54:28.397498Z","iopub.status.idle":"2021-09-22T14:54:28.417882Z","shell.execute_reply.started":"2021-09-22T14:54:28.397457Z","shell.execute_reply":"2021-09-22T14:54:28.416832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Review the memory usage by DataFrame\ntrain.info(memory_usage='deep')\ntest.info(memory_usage='deep')","metadata":{"execution":{"iopub.status.busy":"2021-09-22T14:54:28.419805Z","iopub.execute_input":"2021-09-22T14:54:28.420252Z","iopub.status.idle":"2021-09-22T14:54:28.441529Z","shell.execute_reply.started":"2021-09-22T14:54:28.420214Z","shell.execute_reply":"2021-09-22T14:54:28.440656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Missing value treatment","metadata":{}},{"cell_type":"code","source":"# Check for missing values\ntrain.isnull().sum()\ntest.isnull().sum()\n\n# Add a dummy missing value for a row with missing data\nfeatures = [x for x in train.columns.values if x[0]==\"f\"]\ntrain['n_missing'] = train[features].isna().sum(axis=1)\ntest['n_missing'] = test[features].isna().sum(axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-09-22T14:54:28.442728Z","iopub.execute_input":"2021-09-22T14:54:28.443111Z","iopub.status.idle":"2021-09-22T14:54:29.527527Z","shell.execute_reply.started":"2021-09-22T14:54:28.443073Z","shell.execute_reply":"2021-09-22T14:54:29.52649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Analysis","metadata":{}},{"cell_type":"code","source":"X = train.drop('claim', axis=1)\ny = train['claim']","metadata":{"execution":{"iopub.status.busy":"2021-09-22T14:54:29.529137Z","iopub.execute_input":"2021-09-22T14:54:29.529592Z","iopub.status.idle":"2021-09-22T14:54:29.669325Z","shell.execute_reply.started":"2021-09-22T14:54:29.529547Z","shell.execute_reply":"2021-09-22T14:54:29.668063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Impute the missing value as the median value\n\n# Create function for the missing value review\ndef impute_miss_values(df, strategy='median'):\n    \n    # List of column names for review\n    column_names = [col for col in df.columns]\n    \n    # create the imputer, the strategy can be mean and median.\n    imputer = SimpleImputer(missing_values=np.nan, strategy=strategy)\n\n    # fit the imputer to the train data\n    imputer.fit(df)\n\n    # apply the transformation to the train and test\n    df_out = pd.DataFrame(imputer.transform(df), columns=column_names)\n    return df_out","metadata":{"execution":{"iopub.status.busy":"2021-09-22T14:54:29.670817Z","iopub.execute_input":"2021-09-22T14:54:29.671342Z","iopub.status.idle":"2021-09-22T14:54:29.679102Z","shell.execute_reply.started":"2021-09-22T14:54:29.6713Z","shell.execute_reply":"2021-09-22T14:54:29.67797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Impute missing value for the X and test DataFrames\nX = impute_miss_values(X)\ntest = impute_miss_values(test)","metadata":{"execution":{"iopub.status.busy":"2021-09-22T14:54:29.680669Z","iopub.execute_input":"2021-09-22T14:54:29.681159Z","iopub.status.idle":"2021-09-22T14:55:07.791103Z","shell.execute_reply.started":"2021-09-22T14:54:29.681119Z","shell.execute_reply":"2021-09-22T14:55:07.790177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prepare the data to be used within the model. Make use of the lgb.Dataset() method to optimise the memory usage\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=6, stratify=y)","metadata":{"execution":{"iopub.status.busy":"2021-09-22T14:55:07.792404Z","iopub.execute_input":"2021-09-22T14:55:07.79275Z","iopub.status.idle":"2021-09-22T14:55:09.401986Z","shell.execute_reply.started":"2021-09-22T14:55:07.792715Z","shell.execute_reply":"2021-09-22T14:55:09.401085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Baseline model","metadata":{}},{"cell_type":"code","source":"# Instantiate the XGBClassifier: xg_cl\nxg_cl = xgb.XGBClassifier(objective='binary:logistic', \n                          n_estimators=10, \n                          seed=123, \n                          use_label_encoder=False, \n                          eval_metric='auc', \n                          tree_method='gpu_hist')\n\n# Fit the classifier to the training set\nxg_cl.fit(X_train, y_train)\n\n# Predict the labels of the test set: preds\npreds = xg_cl.predict(X_test)\n\n# Compute the accuracy: accuracy\naccuracy = float(np.sum(preds==y_test))/y_test.shape[0]\nprint(\"accuracy: %f\" % (accuracy))","metadata":{"execution":{"iopub.status.busy":"2021-09-22T14:55:09.403267Z","iopub.execute_input":"2021-09-22T14:55:09.403817Z","iopub.status.idle":"2021-09-22T14:55:15.641441Z","shell.execute_reply.started":"2021-09-22T14:55:09.403775Z","shell.execute_reply":"2021-09-22T14:55:15.640524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate models\ndef eval_model(model):\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    return roc_auc_score(y_test, y_pred)","metadata":{"execution":{"iopub.status.busy":"2021-09-22T14:55:15.644416Z","iopub.execute_input":"2021-09-22T14:55:15.6447Z","iopub.status.idle":"2021-09-22T14:55:15.649736Z","shell.execute_reply.started":"2021-09-22T14:55:15.644673Z","shell.execute_reply":"2021-09-22T14:55:15.648713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eval_model(xg_cl)","metadata":{"execution":{"iopub.status.busy":"2021-09-22T14:55:15.651097Z","iopub.execute_input":"2021-09-22T14:55:15.651567Z","iopub.status.idle":"2021-09-22T14:55:18.15757Z","shell.execute_reply.started":"2021-09-22T14:55:15.65153Z","shell.execute_reply":"2021-09-22T14:55:18.156706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets use the boosting and inbuild CV methods\n\n# Create the DMatrix from X and y: churn_dmatrix\nd_train = xgb.DMatrix(data=X_train, label=y_train)\nd_test = xgb.DMatrix(data=X_test, label=y_test)\nxgd_test = xgb.DMatrix(data=test)\n\n# Create the parameter dictionary: params. NOTE: have to explicitly provide the objective param\nparams = {\"objective\":\"binary:logistic\", \n          \"max_depth\":3,\n#           \"use_label_encoder\":False, \n          \"eval_metric\":'auc', \n          \"tree_method\":'gpu_hist'\n         }\n\n# Reviewing the AUC metric\n# Perform cross_validation: cv_results\ncv_results = xgb.cv(dtrain=d_train, params=params,\n                  nfold=3, num_boost_round=10, \n                  metrics=\"auc\", as_pandas=True, seed=123)\n\n# Print cv_results\nprint(cv_results)\n\n# Print the AUC\nprint((cv_results[\"test-auc-mean\"]).iloc[-1])","metadata":{"execution":{"iopub.status.busy":"2021-09-22T14:55:18.158918Z","iopub.execute_input":"2021-09-22T14:55:18.159266Z","iopub.status.idle":"2021-09-22T14:55:25.917852Z","shell.execute_reply.started":"2021-09-22T14:55:18.159228Z","shell.execute_reply":"2021-09-22T14:55:25.916047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Review the train method\nparams = {\n    \"objective\": \"binary:logistic\", \n    \"max_depth\": 3,\n    \"eval_metric\": 'auc', \n    \"tree_method\": 'gpu_hist'\n}\n\n# train - verbose_eval option switches off the log outputs\nxgb_clf = xgb.train(\n    params,\n    d_train,\n    num_boost_round=5000,\n    evals=[(d_train, 'train'), (d_test, 'test')],\n    early_stopping_rounds=100,\n    verbose_eval=0\n)\n\n# predict\ny_pred = xgb_clf.predict(d_test)\n# Compute and print metrics\nprint(f\"AUC : {roc_auc_score(y_test, y_pred)}\")","metadata":{"execution":{"iopub.status.busy":"2021-09-22T14:57:18.002707Z","iopub.execute_input":"2021-09-22T14:57:18.003021Z","iopub.status.idle":"2021-09-22T14:57:23.72589Z","shell.execute_reply.started":"2021-09-22T14:57:18.002994Z","shell.execute_reply":"2021-09-22T14:57:23.724505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Make submission","metadata":{}},{"cell_type":"code","source":"def submission_sample(model, df_test, model_name):\n    sample = pd.read_csv('../input/tabular-playground-series-sep-2021/sample_solution.csv')\n    sample['claim'] = model.predict(df_test)\n    return sample.to_csv(f'submission_{model_name}.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-22T14:55:32.578414Z","iopub.execute_input":"2021-09-22T14:55:32.578746Z","iopub.status.idle":"2021-09-22T14:55:32.583689Z","shell.execute_reply.started":"2021-09-22T14:55:32.578711Z","shell.execute_reply":"2021-09-22T14:55:32.582863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Baseline submission - original code versions\n# submission_sample(xgb_clf, xgd_test, 'xgb_base')","metadata":{"execution":{"iopub.status.busy":"2021-09-22T14:28:11.442946Z","iopub.execute_input":"2021-09-22T14:28:11.443307Z","iopub.status.idle":"2021-09-22T14:28:14.577537Z","shell.execute_reply.started":"2021-09-22T14:28:11.443276Z","shell.execute_reply":"2021-09-22T14:28:14.576669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Perform Hyperparameter tuning","metadata":{}},{"cell_type":"code","source":"# Max_depth - maximum number of nodes from root to leaves. Larger the more complex the model will be.\n# Min_child_weight - minimum weight required to create a new node\n\n# params_grid = {\n#     (max_depth, min_child_weight)\n#     for max_depth in np.arange(3, 11, 1)\n#     for min_child_weight in np.arange(5, 9, 1)\n# }\n\n# # Create the parameter dictionary: params.\n# params = {\"objective\":\"binary:logistic\", \n#           \"eval_metric\":'auc', \n#           \"tree_method\":'gpu_hist'\n#          }\n\n# # Define initial best params and MAE\n# auc_mean = float(\"Inf\")\n# best_params = None\n\n# for max_depth, min_child_weight in params_grid:\n#     print(f'max_depth: {max_depth} & min_child_weight {min_child_weight}')\n#     params['max_depth'] = max_depth\n#     params['min_child_weight'] = min_child_weight\n#     # Reviewing the AUC metric\n#     # Perform cross_validation: cv_results\n#     cv_results = xgb.cv(dtrain=d_train, params=params,\n#                       nfold=3, num_boost_round=10, \n#                       metrics=\"auc\", as_pandas=True, seed=123)\n\n#     # Print the AUC\n#     print((cv_results[\"test-auc-mean\"]).iloc[-1])\n#     # Update best AUC\n#     mean_auc = cv_results[\"test-auc-mean\"].iloc[-1]\n#     if mean_auc > auc_mean:\n#         auc_mean = mean_auc\n#         best_params = (max_depth, min_child_weight)\n# print(\"Best params: {}, {}, MAE: {}\".format(best_params[0], best_params[1], auc_mean))","metadata":{"execution":{"iopub.status.busy":"2021-09-22T15:35:30.981247Z","iopub.execute_input":"2021-09-22T15:35:30.981633Z","iopub.status.idle":"2021-09-22T15:38:21.047883Z","shell.execute_reply.started":"2021-09-22T15:35:30.981602Z","shell.execute_reply":"2021-09-22T15:38:21.046735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Review the train method\nparams = {\n    \"objective\": \"binary:logistic\", \n    \"eval_metric\": 'auc', \n    \"tree_method\": 'gpu_hist',\n    \"max_depth\": 3,\n    \"min_child_weight\": 4,\n#     \"subsample\": .8\n    \"eta\": 0.05\n}\n\n# train - verbose_eval option switches off the log outputs\nxgb_clf = xgb.train(\n    params,\n    d_train,\n    num_boost_round=5000,\n    evals=[(d_train, 'train'), (d_test, 'test')],\n    early_stopping_rounds=100,\n    verbose_eval=0\n)\n\n# predict\ny_pred = xgb_clf.predict(d_test)\n# Compute and print metrics\nprint(f\"AUC : {roc_auc_score(y_test, y_pred)}\")","metadata":{"execution":{"iopub.status.busy":"2021-09-22T15:45:43.054236Z","iopub.execute_input":"2021-09-22T15:45:43.054584Z","iopub.status.idle":"2021-09-22T15:46:22.635214Z","shell.execute_reply.started":"2021-09-22T15:45:43.054552Z","shell.execute_reply":"2021-09-22T15:46:22.634318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Adjust ETA submission\nsubmission_sample(xgb_clf, xgd_test, 'xgb_eta')","metadata":{"execution":{"iopub.status.busy":"2021-09-22T15:47:22.600685Z","iopub.execute_input":"2021-09-22T15:47:22.601014Z","iopub.status.idle":"2021-09-22T15:47:36.682965Z","shell.execute_reply.started":"2021-09-22T15:47:22.600985Z","shell.execute_reply":"2021-09-22T15:47:36.682084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}