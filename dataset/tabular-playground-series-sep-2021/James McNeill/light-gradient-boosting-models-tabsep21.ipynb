{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Light Gradient Boosting Model testing\nAim of this notebook is to review the light gradient boosting model which can be used during a binary classification challenge.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-22T10:54:52.907815Z","iopub.execute_input":"2021-09-22T10:54:52.908197Z","iopub.status.idle":"2021-09-22T10:54:53.651627Z","shell.execute_reply.started":"2021-09-22T10:54:52.908103Z","shell.execute_reply":"2021-09-22T10:54:53.650608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Light Gradient Boosting","metadata":{}},{"cell_type":"code","source":"# Import modules for model analysis\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# Import lightgbm modules\nimport lightgbm as lgb","metadata":{"execution":{"iopub.status.busy":"2021-09-22T10:54:53.653174Z","iopub.execute_input":"2021-09-22T10:54:53.653535Z","iopub.status.idle":"2021-09-22T10:54:55.877627Z","shell.execute_reply.started":"2021-09-22T10:54:53.653496Z","shell.execute_reply":"2021-09-22T10:54:55.876802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read in the data\ntrain = pd.read_csv('../input/tabular-playground-series-sep-2021/train.csv',index_col=0)\ntest  = pd.read_csv('../input/tabular-playground-series-sep-2021/test.csv', index_col=0)\n\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-22T10:54:55.881622Z","iopub.execute_input":"2021-09-22T10:54:55.881884Z","iopub.status.idle":"2021-09-22T10:55:35.833537Z","shell.execute_reply.started":"2021-09-22T10:54:55.881858Z","shell.execute_reply":"2021-09-22T10:55:35.832741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the memory consumed by the DataFrame\ntrain.info(memory_usage='deep')","metadata":{"execution":{"iopub.status.busy":"2021-09-22T10:55:35.835088Z","iopub.execute_input":"2021-09-22T10:55:35.835455Z","iopub.status.idle":"2021-09-22T10:55:35.851713Z","shell.execute_reply.started":"2021-09-22T10:55:35.835418Z","shell.execute_reply":"2021-09-22T10:55:35.850786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Memory usage by variable in MB\ntrain.memory_usage(deep=True) * 1e-6","metadata":{"execution":{"iopub.status.busy":"2021-09-22T10:55:35.853268Z","iopub.execute_input":"2021-09-22T10:55:35.853838Z","iopub.status.idle":"2021-09-22T10:55:35.865459Z","shell.execute_reply.started":"2021-09-22T10:55:35.853791Z","shell.execute_reply":"2021-09-22T10:55:35.864411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets reduce the memory usage of the features\n# First - check the integer values and downcast\ndef int_downcast(df):\n    int_cols = df.select_dtypes(include=['int64'])\n\n    for col in int_cols.columns:\n        print(col, 'min:',df[col].min(),'; max:',df[col].max())\n        df[col] = pd.to_numeric(df[col], downcast ='integer')\n    return df\n\nint_downcast(train)\ntrain.memory_usage(deep=True) * 1e-6","metadata":{"execution":{"iopub.status.busy":"2021-09-22T10:55:35.866929Z","iopub.execute_input":"2021-09-22T10:55:35.867358Z","iopub.status.idle":"2021-09-22T10:55:35.902519Z","shell.execute_reply.started":"2021-09-22T10:55:35.867323Z","shell.execute_reply":"2021-09-22T10:55:35.901608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Second - check the float values and downcast. Method will have to be applied to the train and test DataFrames\ndef float_downcast(df):\n    float_cols = df.select_dtypes(include=['float64'])\n\n    for col in float_cols.columns:\n#         print(col, 'min:',df[col].min(),'; max:',df[col].max())\n        df[col] = pd.to_numeric(df[col], downcast ='float')\n    return df\n\nfloat_downcast(train)\nfloat_downcast(test)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-09-22T10:55:35.903937Z","iopub.execute_input":"2021-09-22T10:55:35.90428Z","iopub.status.idle":"2021-09-22T10:55:59.548018Z","shell.execute_reply.started":"2021-09-22T10:55:35.904231Z","shell.execute_reply":"2021-09-22T10:55:59.547215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the memory usage by feature\ntrain.memory_usage(deep=True) * 1e-6\ntest.memory_usage(deep=True) * 1e-6","metadata":{"execution":{"iopub.status.busy":"2021-09-22T10:55:59.550724Z","iopub.execute_input":"2021-09-22T10:55:59.551051Z","iopub.status.idle":"2021-09-22T10:55:59.570965Z","shell.execute_reply.started":"2021-09-22T10:55:59.551019Z","shell.execute_reply":"2021-09-22T10:55:59.570069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Review the memory usage by DataFrame\ntrain.info(memory_usage='deep')\ntest.info(memory_usage='deep')","metadata":{"execution":{"iopub.status.busy":"2021-09-22T10:55:59.57304Z","iopub.execute_input":"2021-09-22T10:55:59.573629Z","iopub.status.idle":"2021-09-22T10:55:59.593169Z","shell.execute_reply.started":"2021-09-22T10:55:59.573591Z","shell.execute_reply":"2021-09-22T10:55:59.592373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Missing value treatment","metadata":{}},{"cell_type":"code","source":"# Check for missing values\ntrain.isnull().sum()\ntest.isnull().sum()\n\n# Add a dummy missing value for a row with missing data\nfeatures = [x for x in train.columns.values if x[0]==\"f\"]\ntrain['n_missing'] = train[features].isna().sum(axis=1)\ntest['n_missing'] = test[features].isna().sum(axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-09-22T10:55:59.594525Z","iopub.execute_input":"2021-09-22T10:55:59.5949Z","iopub.status.idle":"2021-09-22T10:56:00.670406Z","shell.execute_reply.started":"2021-09-22T10:55:59.59485Z","shell.execute_reply":"2021-09-22T10:56:00.669529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Analysis","metadata":{}},{"cell_type":"code","source":"X = train.drop('claim', axis=1)\ny = train['claim']","metadata":{"execution":{"iopub.status.busy":"2021-09-22T10:56:00.671751Z","iopub.execute_input":"2021-09-22T10:56:00.6721Z","iopub.status.idle":"2021-09-22T10:56:00.80933Z","shell.execute_reply.started":"2021-09-22T10:56:00.672062Z","shell.execute_reply":"2021-09-22T10:56:00.808268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prepare the data to be used within the model. Make use of the lgb.Dataset() method to optimise the memory usage\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=6, stratify=y)","metadata":{"execution":{"iopub.status.busy":"2021-09-22T10:56:00.810842Z","iopub.execute_input":"2021-09-22T10:56:00.811222Z","iopub.status.idle":"2021-09-22T10:56:01.989014Z","shell.execute_reply.started":"2021-09-22T10:56:00.811185Z","shell.execute_reply":"2021-09-22T10:56:01.988072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Using LGB dataset method and train","metadata":{}},{"cell_type":"code","source":"# Review using the LGB dataset and model build methods\nlgb_train = lgb.Dataset(X_train, label=y_train)\nlgb_eval = lgb.Dataset(X_test, label=y_test, reference=lgb_train)\n\nprint(f'{type(lgb_train)}')\nprint(f'{lgb_train.data.info()}')","metadata":{"execution":{"iopub.status.busy":"2021-09-22T10:56:01.990391Z","iopub.execute_input":"2021-09-22T10:56:01.990745Z","iopub.status.idle":"2021-09-22T10:56:02.00894Z","shell.execute_reply.started":"2021-09-22T10:56:01.990706Z","shell.execute_reply":"2021-09-22T10:56:02.008056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(type(lgb_train))\nlgb_train.data.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-22T10:56:02.010106Z","iopub.execute_input":"2021-09-22T10:56:02.010472Z","iopub.status.idle":"2021-09-22T10:56:02.042943Z","shell.execute_reply.started":"2021-09-22T10:56:02.010436Z","shell.execute_reply":"2021-09-22T10:56:02.042236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Specify the configurations as a dict\nparams = {\n    'objective': 'binary',\n    'metric': 'auc',\n    'is_unbalance': 'true',\n    'boosting': 'gbdt',\n    'num_leaves': 31,\n    'feature_fraction': 0.5,\n    'bagging_fraction': 0.5,\n    'bagging_freq': 20,\n    'learning_rate': 0.05,\n    'verbose': 0,\n    'device': 'gpu'\n}\n\n# train - verbose_eval option switches off the log outputs\ngbm = lgb.train(\n    params,\n    lgb_train,\n    num_boost_round=5000,\n    valid_sets=lgb_eval,\n    early_stopping_rounds=100,\n    verbose_eval=-1,\n)\n\n# predict\ny_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n# Compute and print metrics\nprint(f\"AUC : {roc_auc_score(y_test, y_pred)}\")","metadata":{"execution":{"iopub.status.busy":"2021-09-22T10:56:02.044002Z","iopub.execute_input":"2021-09-22T10:56:02.044323Z","iopub.status.idle":"2021-09-22T10:57:30.192808Z","shell.execute_reply.started":"2021-09-22T10:56:02.044298Z","shell.execute_reply":"2021-09-22T10:57:30.191932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Feature importance\nlgb.plot_importance(gbm, max_num_features=15);\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-22T10:57:30.194034Z","iopub.execute_input":"2021-09-22T10:57:30.194545Z","iopub.status.idle":"2021-09-22T10:57:30.617673Z","shell.execute_reply.started":"2021-09-22T10:57:30.194505Z","shell.execute_reply":"2021-09-22T10:57:30.616716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's create a function to allow for future quick reviews of the same baseline model. Will allow for easy review of feature engineering and selection processing steps\ndef base_model(train, dep):\n    \n    # Create feature variables\n    X = train\n    y = dep\n    \n    # Prepare the data to be used within the model. Make use of the lgb.Dataset() method to optimise the memory usage\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=6, stratify=y)\n    \n    # Review using the LGB dataset and model build methods\n    lgb_train = lgb.Dataset(X_train, label=y_train)\n    lgb_eval = lgb.Dataset(X_test, label=y_test, reference=lgb_train)\n    \n    # Run the model\n    params = {\n        'objective': 'binary',\n        'metric': 'auc',\n        'is_unbalance': 'true',\n        'boosting': 'gbdt',\n        'num_leaves': 31,\n        'feature_fraction': 0.5,\n        'bagging_fraction': 0.5,\n        'bagging_freq': 20,\n        'learning_rate': 0.05,\n        'verbose': 0,\n        'device': 'gpu'\n    }\n\n    # train - verbose_eval option switches off the log outputs\n    model = lgb.train(\n        params,\n        lgb_train,\n        num_boost_round=5000,\n        valid_sets=lgb_eval,\n        early_stopping_rounds=100,\n        verbose_eval=-1,\n    )\n\n    # predict\n    y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n    # Compute and print metrics\n    print(f\"AUC : {roc_auc_score(y_test, y_pred)}\")\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-09-22T10:57:30.6214Z","iopub.execute_input":"2021-09-22T10:57:30.62174Z","iopub.status.idle":"2021-09-22T10:57:30.634097Z","shell.execute_reply.started":"2021-09-22T10:57:30.621707Z","shell.execute_reply":"2021-09-22T10:57:30.63332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Make submission","metadata":{}},{"cell_type":"code","source":"def submission_sample(model, df_test, model_name):\n    sample = pd.read_csv('../input/tabular-playground-series-sep-2021/sample_solution.csv')\n    sample['claim'] = model.predict(df_test)\n    return sample.to_csv(f'submission_{model_name}.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-22T10:57:30.637943Z","iopub.execute_input":"2021-09-22T10:57:30.64034Z","iopub.status.idle":"2021-09-22T10:57:30.646593Z","shell.execute_reply.started":"2021-09-22T10:57:30.640302Z","shell.execute_reply":"2021-09-22T10:57:30.645859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Engineering\n***\nAfter creating the initial baseline model we can start to perform some feature engineering steps. With feature engineering we are aiming to see if additional variables can be created that will help to improve the model.\n***\n1. Binning\n    * Create binned values (quantiles, deciles)\n2. Feature scaling\n    * MinMax scaling\n    * Standardization\n    * Winsorizing \n3. Statistical transformations\n    * Log\n    * Polynomials\n4. Feature Interactions\n    * Use PolynomialFeatures\n***\nPrior to this feature engineering we can review teh missing value replacement assessment.\n* Replace with mean / median / mode\n* End of tail imputation - works best with normally distributed features","metadata":{}},{"cell_type":"markdown","source":"Lets go back to reviewing the Train and Test DataFrames\n","metadata":{}},{"cell_type":"code","source":"# Lets confirm the feature data types\nprint(f'Train : \\n{train.dtypes.value_counts()}')\nprint(f'Test : \\n{test.dtypes.value_counts()}')","metadata":{"execution":{"iopub.status.busy":"2021-09-22T10:57:30.650447Z","iopub.execute_input":"2021-09-22T10:57:30.652497Z","iopub.status.idle":"2021-09-22T10:57:30.666591Z","shell.execute_reply.started":"2021-09-22T10:57:30.652461Z","shell.execute_reply":"2021-09-22T10:57:30.665587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Review missing value replacement","metadata":{}},{"cell_type":"code","source":"# List of column names for review\n# column_names = [col for col in train_miss.columns]\ncolumn_names = [col for col in X.columns]","metadata":{"execution":{"iopub.status.busy":"2021-09-22T10:57:30.670115Z","iopub.execute_input":"2021-09-22T10:57:30.671023Z","iopub.status.idle":"2021-09-22T10:57:30.678314Z","shell.execute_reply.started":"2021-09-22T10:57:30.670983Z","shell.execute_reply":"2021-09-22T10:57:30.677463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create function for the missing value review\ndef impute_miss_values(df_train, df_test, strategy='mean'):\n    # create the imputer, the strategy can be mean and median.\n    imputer = SimpleImputer(missing_values=np.nan, strategy=strategy)\n\n    # fit the imputer to the train data\n    imputer.fit(df_train)\n\n    # apply the transformation to the train and test\n    train_imp = pd.DataFrame(imputer.transform(df_train), columns=column_names)\n    test_imp = pd.DataFrame(imputer.transform(df_test), columns=column_names)\n    return train_imp, test_imp","metadata":{"execution":{"iopub.status.busy":"2021-09-22T10:57:30.682183Z","iopub.execute_input":"2021-09-22T10:57:30.683381Z","iopub.status.idle":"2021-09-22T10:57:30.839183Z","shell.execute_reply.started":"2021-09-22T10:57:30.683345Z","shell.execute_reply":"2021-09-22T10:57:30.8383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Median value replacement has helped to benefit the score accuracy the most. Lets review end of tail imputation as a comparison","metadata":{}},{"cell_type":"code","source":"# Update the train and test set to have the missing values as median\nX, test = impute_miss_values(X, test, strategy='median')\n# Confirm the model output still aligns to previous versions\nlgb_median = base_model(X, dep=train['claim'])","metadata":{"execution":{"iopub.status.busy":"2021-09-22T10:57:30.840574Z","iopub.execute_input":"2021-09-22T10:57:30.841023Z","iopub.status.idle":"2021-09-22T10:59:14.179927Z","shell.execute_reply.started":"2021-09-22T10:57:30.84091Z","shell.execute_reply":"2021-09-22T10:59:14.179001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Binning","metadata":{"_kg_hide-input":true}},{"cell_type":"markdown","source":"* Making use of the quartile binning, none of the variables appear to have added to the most important features\n* Appears to be a slight improvement by adding decile values for the features\n***\nThey could be a feature to add in future iterations of the model but they are not adding a lot to the final AUC improvements","metadata":{"_kg_hide-input":true}},{"cell_type":"markdown","source":"### Feature scaling","metadata":{}},{"cell_type":"code","source":"# Aiming to review the impact of using the scaling features\n# MinMaxScaler()\n# Create function for the scaling review\n# def impute_scaler(df_train, df_test, scaler=MinMaxScaler()):\n    \n#     # apply the transformation to the train and test\n#     train_imp = pd.DataFrame(scaler.fit_transform(df_train), columns=column_names)\n#     test_imp = pd.DataFrame(scaler.fit_transform(df_test), columns=column_names)\n#     return train_imp, test_imp","metadata":{"execution":{"iopub.status.busy":"2021-09-22T10:59:14.183872Z","iopub.execute_input":"2021-09-22T10:59:14.18414Z","iopub.status.idle":"2021-09-22T10:59:14.188217Z","shell.execute_reply.started":"2021-09-22T10:59:14.184112Z","shell.execute_reply":"2021-09-22T10:59:14.187127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Update the train and test set to have the missing values as median\n# X, test = impute_scaler(X, test, scaler=StandardScaler())\n# # Confirm the model output still aligns to previous versions\n# lgb_standard = base_model(X, dep=train['claim'])","metadata":{"execution":{"iopub.status.busy":"2021-09-22T10:59:14.190288Z","iopub.execute_input":"2021-09-22T10:59:14.190677Z","iopub.status.idle":"2021-09-22T10:59:14.198205Z","shell.execute_reply.started":"2021-09-22T10:59:14.190639Z","shell.execute_reply":"2021-09-22T10:59:14.197313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# submission_sample(lgb_standard, test, 'lgb_standard')","metadata":{"execution":{"iopub.status.busy":"2021-09-22T10:59:14.199632Z","iopub.execute_input":"2021-09-22T10:59:14.200044Z","iopub.status.idle":"2021-09-22T10:59:14.207729Z","shell.execute_reply.started":"2021-09-22T10:59:14.200007Z","shell.execute_reply":"2021-09-22T10:59:14.206825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Winsorizing","metadata":{}},{"cell_type":"code","source":"# def impute_winsor(df, prob=0.01):\n    \n#     # Review each of the columns and apply the clipping\n#     for col in df.columns:\n#         quant = np.quantile(df[col], [prob, (1-prob)])\n#         df.loc[(df[col] <= quant[0]), col] = quant[0]\n#         df.loc[(df[col] >= quant[1]), col] = quant[1]\n    \n#     return df","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-22T10:59:14.208926Z","iopub.execute_input":"2021-09-22T10:59:14.20932Z","iopub.status.idle":"2021-09-22T10:59:14.217627Z","shell.execute_reply.started":"2021-09-22T10:59:14.209266Z","shell.execute_reply":"2021-09-22T10:59:14.216731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X_new = impute_winsor(X)\n# test_new = impute_winsor(test)\n# # Review the model output\n# lgb_winsor = base_model(X_new, dep=train['claim'])\n# submission_sample(lgb_winsor, test_new, 'lgb_winsor')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-22T10:59:14.21886Z","iopub.execute_input":"2021-09-22T10:59:14.219245Z","iopub.status.idle":"2021-09-22T10:59:14.226709Z","shell.execute_reply.started":"2021-09-22T10:59:14.219208Z","shell.execute_reply":"2021-09-22T10:59:14.225816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Making use of the clipping didn't really benefit the model","metadata":{}},{"cell_type":"markdown","source":"### Polynomial Features","metadata":{}},{"cell_type":"code","source":"# from sklearn.preprocessing import PolynomialFeatures\n\n# # Interactions between features\n# def poly_interactions(df_train, df_test):\n    \n#     # Set-up the interactions feature\n#     interactions = PolynomialFeatures(interaction_only=True)\n    \n#     # apply the transformation to the train and test\n#     train_imp = pd.DataFrame(interactions.fit_transform(df_train), columns=column_names)\n#     test_imp = pd.DataFrame(interactions.fit_transform(df_test), columns=column_names)\n#     return train_imp, test_imp\n\n# Polynomial Features\n# def poly_features(df_train, df_test, num_features=2):\n    \n#     # Set-up the interactions feature\n#     poly = PolynomialFeatures(num_features)\n    \n#     # apply the transformation to the train and test\n#     train_imp = pd.DataFrame(poly.fit_transform(df_train), columns=column_names)\n#     test_imp = pd.DataFrame(poly.fit_transform(df_test), columns=column_names)\n#     return train_imp, test_imp","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-22T10:59:14.22977Z","iopub.execute_input":"2021-09-22T10:59:14.230065Z","iopub.status.idle":"2021-09-22T10:59:14.23796Z","shell.execute_reply.started":"2021-09-22T10:59:14.23004Z","shell.execute_reply":"2021-09-22T10:59:14.237011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Update the train and test set\n# X_pi, test_pi = poly_interactions(X, test)\n# # Run the model\n# lgb_poly_i = base_model(X_pi, dep=train['claim'])\n# submission_sample(lgb_poly_i, test_pi, 'lgb_poly_i')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-22T10:59:14.240619Z","iopub.execute_input":"2021-09-22T10:59:14.240966Z","iopub.status.idle":"2021-09-22T10:59:14.246828Z","shell.execute_reply.started":"2021-09-22T10:59:14.240936Z","shell.execute_reply":"2021-09-22T10:59:14.245929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Update the train and test set\n# X_pf, test_pf = poly_features(X, test)\n# # Run the model\n# lgb_poly_f = base_model(X_pf, dep=train['claim'])\n# submission_sample(lgb_poly_f, test_pf, 'lgb_poly_f')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-22T10:59:14.248216Z","iopub.execute_input":"2021-09-22T10:59:14.24865Z","iopub.status.idle":"2021-09-22T10:59:14.255391Z","shell.execute_reply.started":"2021-09-22T10:59:14.248612Z","shell.execute_reply":"2021-09-22T10:59:14.254441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Doesn't appear to like creating the polynomial features. May have to try in the future with less input features.","metadata":{}},{"cell_type":"markdown","source":"# Feature Selection\n***\nAims to reduce the dimensionality of the dataset\n***\n1. Remove co-linear features\n2. Remove features with large number of missing values\n3. Keep importance features","metadata":{}},{"cell_type":"code","source":"# Threshold for removing correlated variables\nthreshold = 0.9\n\n# Absolute value correlation matrix\ncorr_matrix = X.corr().abs()\n\n# Upper triangle of correlations\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n\n# Select columns with correlations above threshold\nto_drop = [column for column in upper.columns if any(upper[column] > threshold)]\nto_drop\n\n# Remove the columns from the train and test set\n# X = X.drop(columns = to_drop)","metadata":{"execution":{"iopub.status.busy":"2021-09-22T10:59:14.258627Z","iopub.execute_input":"2021-09-22T10:59:14.258875Z","iopub.status.idle":"2021-09-22T10:59:46.308407Z","shell.execute_reply.started":"2021-09-22T10:59:14.258851Z","shell.execute_reply":"2021-09-22T10:59:46.307436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets review thresholds for removing co-linear features\nthreshold_range = np.arange(0, 0.91, 0.025)\n\nto_drop_dict = dict()\nfor thres in threshold_range:\n    to_drop_dict[thres] = [column for column in upper.columns if any(upper[column] > thres)]\n\nto_drop_dict","metadata":{"execution":{"iopub.status.busy":"2021-09-22T10:59:46.309983Z","iopub.execute_input":"2021-09-22T10:59:46.310389Z","iopub.status.idle":"2021-09-22T10:59:46.958728Z","shell.execute_reply.started":"2021-09-22T10:59:46.310345Z","shell.execute_reply":"2021-09-22T10:59:46.95777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets review a heatmap of the correlations\nsns.heatmap(upper);","metadata":{"execution":{"iopub.status.busy":"2021-09-22T10:59:46.960117Z","iopub.execute_input":"2021-09-22T10:59:46.960481Z","iopub.status.idle":"2021-09-22T10:59:47.44627Z","shell.execute_reply.started":"2021-09-22T10:59:46.960444Z","shell.execute_reply":"2021-09-22T10:59:47.445515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It appears that the majority of the features are randomly correlated","metadata":{}},{"cell_type":"code","source":"# Remove features with zero importance - doesn't appear to be working. Review in future work.\n# feature_importances = pd.DataFrame({'feature': list(X.columns), \n#                             'importance': lgb_median.feature_importance})\n# # Find the features with zero importance\n# zero_features = list(feature_importances[feature_importances['importance'] == 0.0]['feature'])\n# print('\\nThere are %d features with 0.0 importance' % len(zero_features))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-22T10:59:47.447571Z","iopub.execute_input":"2021-09-22T10:59:47.447915Z","iopub.status.idle":"2021-09-22T10:59:47.451324Z","shell.execute_reply.started":"2021-09-22T10:59:47.44788Z","shell.execute_reply":"2021-09-22T10:59:47.450531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Hyperparameter tuning","metadata":{}},{"cell_type":"code","source":"# Let's create a function to allow for future quick reviews of the same baseline model. Will allow for easy review of feature engineering and selection processing steps\ndef model_tuning(train, dep):\n    \n    # Create feature variables\n    X = train\n    y = dep\n    \n    # Prepare the data to be used within the model. Make use of the lgb.Dataset() method to optimise the memory usage\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=6, stratify=y)\n    \n    # Review using the LGB dataset and model build methods\n    lgb_train = lgb.Dataset(X_train, label=y_train)\n    lgb_eval = lgb.Dataset(X_test, label=y_test, reference=lgb_train)\n    \n    # Run the model\n    params = {\n        'objective': 'binary',\n        'metric': 'auc',\n        'is_unbalance': 'true',\n        'boosting': 'gbdt',\n#         'num_leaves': 31,\n#         'feature_fraction': 0.5,\n#         'bagging_fraction': 0.5,\n        'bagging_freq': 20,\n#         'learning_rate': 0.05,\n        'verbose': 0,\n        'device': 'gpu'\n    }\n\n    # train - verbose_eval option switches off the log outputs\n    model = lgb.train(\n        params,\n        lgb_train,\n        num_boost_round=5000,\n        valid_sets=lgb_eval,\n        early_stopping_rounds=100,\n        verbose_eval=-1,\n    )\n\n    # Hyperparameter tuning\n    parameters = {'num_leaves':[20,40,60,80,100], \n                  'min_child_samples':[5,10,15],\n                  'max_depth':[-1,5,10,20],\n                  'learning_rate':[0.05,0.1,0.2],\n                  'reg_alpha':[0,0.01,0.03],\n                  'feature_fraction': [0.5, 0.6, 0.7],\n                  'bagging_fraction': [0.5, 0.6, 0.7]\n                 }\n    \n    # Setup the random grid search\n    gs = RandomizedSearchCV(\n        estimator=model, \n        param_distributions=parameters, \n        n_iter=100,\n        scoring='roc_auc',\n        cv=3,\n        refit=True,\n        random_state=6,\n        verbose=True)\n\n    # predict\n    y_pred = model.predict(X_test, num_iteration=gs.best_params_)\n    # Compute and print metrics\n    print(f\"AUC : {roc_auc_score(y_test, y_pred)}\")\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-09-22T10:59:47.452617Z","iopub.execute_input":"2021-09-22T10:59:47.453158Z","iopub.status.idle":"2021-09-22T10:59:47.464562Z","shell.execute_reply.started":"2021-09-22T10:59:47.453119Z","shell.execute_reply":"2021-09-22T10:59:47.463791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Confirm the model output - doesn't seem to be working\n# lgb_median_hyper = model_tuning(X, dep=train['claim'])","metadata":{"execution":{"iopub.status.busy":"2021-09-22T10:59:47.465709Z","iopub.execute_input":"2021-09-22T10:59:47.466067Z","iopub.status.idle":"2021-09-22T11:00:25.508264Z","shell.execute_reply.started":"2021-09-22T10:59:47.46603Z","shell.execute_reply":"2021-09-22T11:00:25.506473Z"},"trusted":true},"execution_count":null,"outputs":[]}]}