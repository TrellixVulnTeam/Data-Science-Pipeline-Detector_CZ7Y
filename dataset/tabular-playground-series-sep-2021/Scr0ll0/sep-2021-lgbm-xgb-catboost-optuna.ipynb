{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport optuna\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\nimport lightgbm as lgbm\nfrom lightgbm import LGBMClassifier\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nimport catboost\nfrom catboost import CatBoostClassifier\nfrom optuna.integration import LightGBMPruningCallback, XGBoostPruningCallback\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2021-09-18T20:13:55.096825Z","iopub.execute_input":"2021-09-18T20:13:55.097395Z","iopub.status.idle":"2021-09-18T20:13:55.10321Z","shell.execute_reply.started":"2021-09-18T20:13:55.097358Z","shell.execute_reply":"2021-09-18T20:13:55.102297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Load Data\ntrain = pd.read_csv('/kaggle/input/tabular-playground-series-sep-2021/train.csv')\ntest = pd.read_csv('/kaggle/input/tabular-playground-series-sep-2021/test.csv').drop('id', axis=1)\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-18T20:08:28.513303Z","iopub.execute_input":"2021-09-18T20:08:28.513563Z","iopub.status.idle":"2021-09-18T20:09:09.478048Z","shell.execute_reply.started":"2021-09-18T20:08:28.51353Z","shell.execute_reply":"2021-09-18T20:09:09.477263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Missing Values and Feature Engineering","metadata":{}},{"cell_type":"code","source":"#Imputation Pipeline\npipeline = Pipeline([('impute', SimpleImputer(strategy='mean')), ('scale', StandardScaler())])","metadata":{"execution":{"iopub.status.busy":"2021-09-18T20:09:09.479662Z","iopub.execute_input":"2021-09-18T20:09:09.479909Z","iopub.status.idle":"2021-09-18T20:09:09.484268Z","shell.execute_reply.started":"2021-09-18T20:09:09.479879Z","shell.execute_reply":"2021-09-18T20:09:09.483369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Drop ID and claim columns, latter temporarily\ntemp = train['claim']\ntrain.drop(columns = ['claim', 'id'], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-18T20:09:09.485786Z","iopub.execute_input":"2021-09-18T20:09:09.486055Z","iopub.status.idle":"2021-09-18T20:09:09.750169Z","shell.execute_reply.started":"2021-09-18T20:09:09.486023Z","shell.execute_reply":"2021-09-18T20:09:09.749319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Feature Engineering\ntrain['min'] = train.min(axis=1)\ntrain['max'] = train.max(axis=1)\ntrain['sum'] = train.isna().sum(axis=1)\ntrain['mean'] = train.mean(axis=1)\ntrain['std'] = train.std(axis=1)\ntest['min'] = test.min(axis=1)\ntest['max'] = test.max(axis=1)\ntest['sum'] = test.isna().sum(axis=1)\ntest['mean'] = test.mean(axis=1)\ntest['std'] = test.std(axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-09-18T20:09:09.753366Z","iopub.execute_input":"2021-09-18T20:09:09.753957Z","iopub.status.idle":"2021-09-18T20:09:17.059158Z","shell.execute_reply.started":"2021-09-18T20:09:09.753916Z","shell.execute_reply":"2021-09-18T20:09:17.05845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Restore datasets with missing data + new features\ntrain = pd.DataFrame(columns = train.columns, data=pipeline.fit_transform(train))\ntest = pd.DataFrame(columns = test.columns, data=pipeline.fit_transform(test))\ntrain['claim'] = temp\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-18T20:09:17.061185Z","iopub.execute_input":"2021-09-18T20:09:17.061383Z","iopub.status.idle":"2021-09-18T20:09:25.627771Z","shell.execute_reply.started":"2021-09-18T20:09:17.061361Z","shell.execute_reply":"2021-09-18T20:09:25.627106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Split into X and Y\ntrain_x = train\ntrain_y = train['claim']\ntrain_x.drop(columns = ['claim'], inplace=True)\ntrain_x.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-18T20:09:25.629085Z","iopub.execute_input":"2021-09-18T20:09:25.629331Z","iopub.status.idle":"2021-09-18T20:09:25.912657Z","shell.execute_reply.started":"2021-09-18T20:09:25.629299Z","shell.execute_reply":"2021-09-18T20:09:25.911826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Optuna","metadata":{}},{"cell_type":"code","source":"def Optuna(argument):\n    N_TRIALS = 20\n    N_SPLITS = 5\n    #Credit here for Objective Function: https://www.kaggle.com/bextuychiev/lgbm-optuna-hyperparameter-tuning-w-understanding\n    def LGBMObjective(trial, train_x = train_x, train_y = train_y):\n        param = {\n            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n            \"num_leaves\": trial.suggest_int(\"num_leaves\", 8, 4096),\n            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n            \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 200, 10000),\n            \"lambda_l1\": trial.suggest_int(\"lambda_l1\", 0, 100),\n            \"lambda_l2\": trial.suggest_int(\"lambda_l2\", 0, 100),\n            \"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n            \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.2, 0.95),\n            \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.2, 0.95),\n            \"device_type\": 'gpu',\n            \"n_estimators\": 10000,\n            \"bagging_freq\": 1,\n            \"metric\": 'auc',\n            \"objective\": 'binary'\n        }\n        cv = StratifiedKFold(n_splits=5, shuffle=True)\n        cv_scores = np.empty(5)\n        for idx, (train_idx, test_idx) in enumerate(cv.split(train_x, train_y)):\n            X_train, X_valid = train_x.iloc[train_idx], train_x.iloc[test_idx]\n            y_train, y_valid = train_y[train_idx], train_y[test_idx]\n            model = LGBMClassifier(**param)\n            model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], eval_metric=\"roc_auc_score\", early_stopping_rounds=100,\n                callbacks=[LightGBMPruningCallback(trial, \"auc\")], verbose = False\n            )\n            preds = model.predict_proba(X_valid)\n            cv_scores[idx] = roc_auc_score(y_valid, preds[:,1])\n        return np.mean(cv_scores)\n    #Credit here for objective function: https://www.kaggle.com/mohammadkashifunique/xgboost-hyperparametertuning-optuna\n    def XGBObjective(trial, x_train = train_x, y_train = train_y): \n        param = {\n            'max_depth': trial.suggest_int('max_depth', 6, 10), \n            'n_estimators': trial.suggest_int('n_estimators', 400, 4000, 400), \n            'eta': trial.suggest_float('eta', 0.007, 0.013), \n            'subsample': trial.suggest_discrete_uniform('subsample', 0.2, 0.9, 0.1),\n            'colsample_bytree': trial.suggest_discrete_uniform('colsample_bytree', 0.2, 0.9, 0.1),\n            'colsample_bylevel': trial.suggest_discrete_uniform('colsample_bylevel', 0.2, 0.9, 0.1),\n            'min_child_weight': trial.suggest_loguniform('min_child_weight', 1e-4, 1e4), \n            'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-4, 1e4),\n            'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-4, 1e4), \n            'gamma': trial.suggest_loguniform('gamma', 1e-4, 1e4),\n            'predictor': \"gpu_predictor\",\n            'eval_metric' : 'auc',\n            'objective' : 'binary:logistic',\n            'tree_method': 'gpu_hist',\n        }\n        cv = StratifiedKFold(n_splits=5, shuffle=True)\n        cv_scores = np.empty(5)\n        for idx, (train_idx, test_idx) in enumerate(cv.split(train_x, train_y)):\n            X_train, X_valid = train_x.iloc[train_idx], train_x.iloc[test_idx]\n            y_train, y_valid = train_y[train_idx], train_y[test_idx]\n            model = XGBClassifier(**param)\n            model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], early_stopping_rounds=100, \n                      callbacks=[XGBoostPruningCallback(trial, 'validation_0-auc')], verbose = 0)\n            preds = model.predict_proba(X_valid)\n            cv_scores[idx] = roc_auc_score(y_valid, preds[:,1])\n        return np.mean(cv_scores)\n    #Credit here for objective function: https://www.kaggle.com/mlanhenke/tps-09-optuna-study-catboostclassifier\n    def CatBoostObjective(trial, x_train = train_x, y_train = train_y):\n        param = {\n            'iterations':trial.suggest_int(\"iterations\", 1000, 20000),\n            'objective': trial.suggest_categorical('objective', ['Logloss', 'CrossEntropy']),\n            'bootstrap_type': trial.suggest_categorical('bootstrap_type', ['Bayesian', 'Bernoulli', 'MVS']),\n            'od_wait':trial.suggest_int('od_wait', 500, 2000),\n            'learning_rate' : trial.suggest_uniform('learning_rate',0.02,1),\n            'reg_lambda': trial.suggest_uniform('reg_lambda',1e-5,100),\n            'random_strength': trial.suggest_uniform('random_strength',10,50),\n            'depth': trial.suggest_int('depth',1,15),\n            'min_data_in_leaf': trial.suggest_int('min_data_in_leaf',1,30),\n            'leaf_estimation_iterations': trial.suggest_int('leaf_estimation_iterations',1,15),\n            'verbose': False,\n            'task_type' : 'GPU',\n            'devices' : '0',\n            'eval_metric':'AUC',\n            'od_type': 'IncToDec',\n            'od_pval': 1e-7, \n            'od_wait' : 100,\n        }\n        if param['bootstrap_type'] == 'Bayesian':\n            param['bagging_temperature'] = trial.suggest_float('bagging_temperature', 0, 10)\n        elif param['bootstrap_type'] == 'Bernoulli':\n            param['subsample'] = trial.suggest_float('subsample', 0.1, 1)\n        cv = StratifiedKFold(n_splits=5, shuffle=True)\n        cv_scores = np.empty(5)\n        for idx, (train_idx, test_idx) in enumerate(cv.split(train_x, train_y)):\n            X_train, X_valid = train_x.iloc[train_idx], train_x.iloc[test_idx]\n            y_train, y_valid = train_y[train_idx], train_y[test_idx]\n            model = CatBoostClassifier(**param)\n            #Notice: Optuna Callback not supported on CatBoost\n            model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)])\n            preds = model.predict_proba(X_valid)\n            cv_scores[idx] = roc_auc_score(y_valid, preds[:,1])\n        return np.mean(cv_scores)\n    def ObjectiveSelector(argument):\n        objective = {\n            'LGBM': LGBMObjective,\n            'XGB': XGBObjective,\n            'CatBoost': CatBoostObjective            \n        }\n        return objective.get(argument, \"Invalid Selection\")\n    def ModelSelector(argument, trial): #Switch case not usable here without crashing. \n        if(argument == 'LGBM'):\n            return LGBMClassifier(**trial.params)\n        elif(argument == 'XGB'):\n            return XGBClassifier(**trial.params)\n        elif(argument == 'CatBoost'):\n            return CatBoostClassifier(**trial.params)\n        return \"Invalid Model\"\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(ObjectiveSelector(argument), n_trials=N_TRIALS)\n    print(\"Number of finished trials: {}\".format(len(study.trials)))\n    print(\"Best trial:\")\n    trial = study.best_trial\n    print(\"  Value: {}\".format(trial.value))\n    print(\"  Params: \")\n    for key, value in trial.params.items():\n        print(\"    {}: {}\".format(key, value))\n    model = ModelSelector(argument, trial)\n    model.fit(train_x, train_y)\n    predictions_optuna = model.predict_proba(test)\n    return predictions_optuna","metadata":{"execution":{"iopub.status.busy":"2021-09-18T20:49:22.906581Z","iopub.execute_input":"2021-09-18T20:49:22.906915Z","iopub.status.idle":"2021-09-18T20:49:22.957554Z","shell.execute_reply.started":"2021-09-18T20:49:22.906873Z","shell.execute_reply":"2021-09-18T20:49:22.956825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"argument = 'XGB'\npredictions_optuna = Optuna(argument)","metadata":{"execution":{"iopub.status.busy":"2021-09-18T20:49:23.546615Z","iopub.execute_input":"2021-09-18T20:49:23.547299Z","iopub.status.idle":"2021-09-18T21:01:01.842559Z","shell.execute_reply.started":"2021-09-18T20:49:23.547264Z","shell.execute_reply":"2021-09-18T21:01:01.840754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"sample_solution = pd.read_csv('/kaggle/input/tabular-playground-series-sep-2021/sample_solution.csv')\nsample_solution['claim'] = predictions_optuna[:, 1]\nsample_solution.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-17T21:04:58.628209Z","iopub.execute_input":"2021-09-17T21:04:58.629166Z","iopub.status.idle":"2021-09-17T21:04:58.790326Z","shell.execute_reply.started":"2021-09-17T21:04:58.629115Z","shell.execute_reply":"2021-09-17T21:04:58.789537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_solution.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-17T21:04:59.157158Z","iopub.execute_input":"2021-09-17T21:04:59.157613Z","iopub.status.idle":"2021-09-17T21:05:00.903479Z","shell.execute_reply.started":"2021-09-17T21:04:59.157579Z","shell.execute_reply":"2021-09-17T21:05:00.902644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}