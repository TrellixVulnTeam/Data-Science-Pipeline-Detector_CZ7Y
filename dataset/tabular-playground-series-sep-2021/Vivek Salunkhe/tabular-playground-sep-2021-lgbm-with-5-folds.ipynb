{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **1. DATASET PREPARATION**\n\n## Importing Necessary Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport time\nimport optuna\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.model_selection import KFold, StratifiedKFold, train_test_split\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.impute import SimpleImputer\n\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom xgboost import XGBClassifier\n\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\npd.set_option('float_format', '{:f}'.format)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-06T07:16:42.051782Z","iopub.execute_input":"2021-09-06T07:16:42.052063Z","iopub.status.idle":"2021-09-06T07:16:45.441985Z","shell.execute_reply.started":"2021-09-06T07:16:42.052036Z","shell.execute_reply":"2021-09-06T07:16:45.441154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Reading the dataset files","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/tabular-playground-series-sep-2021/train.csv')\ntest = pd.read_csv('../input/tabular-playground-series-sep-2021/test.csv')\nsubmission = pd.read_csv('../input/tabular-playground-series-sep-2021/sample_solution.csv')","metadata":{"execution":{"iopub.status.busy":"2021-09-06T07:16:45.443389Z","iopub.execute_input":"2021-09-06T07:16:45.443724Z","iopub.status.idle":"2021-09-06T07:17:26.068337Z","shell.execute_reply.started":"2021-09-06T07:16:45.443688Z","shell.execute_reply":"2021-09-06T07:17:26.067417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **2. DATASET OVERVIEW**\n\n## Train Dataset","metadata":{}},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-06T07:17:26.070163Z","iopub.execute_input":"2021-09-06T07:17:26.070529Z","iopub.status.idle":"2021-09-06T07:17:26.140541Z","shell.execute_reply.started":"2021-09-06T07:17:26.070492Z","shell.execute_reply":"2021-09-06T07:17:26.139409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Printing the information about actual and missing values in Training Data","metadata":{}},{"cell_type":"code","source":"print(f'Number of Rows in Training Dataset: {train.shape[0]}  \\nNumber of Columns in Training Dataset: {train.shape[1]} \\nTotal Number of missing values in Training Dataset: {sum(train.isna().sum())}')","metadata":{"execution":{"iopub.status.busy":"2021-09-06T07:17:26.142306Z","iopub.execute_input":"2021-09-06T07:17:26.142687Z","iopub.status.idle":"2021-09-06T07:17:26.344964Z","shell.execute_reply.started":"2021-09-06T07:17:26.142648Z","shell.execute_reply":"2021-09-06T07:17:26.344108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Printing the basic statistics for each variable in Training Dataset which contain information on count, mean, standard deviation, minimum, 1st quartile, median, 3rd quartile and maximum.","metadata":{}},{"cell_type":"code","source":"train.describe()","metadata":{"execution":{"iopub.status.busy":"2021-09-06T07:17:26.34613Z","iopub.execute_input":"2021-09-06T07:17:26.346485Z","iopub.status.idle":"2021-09-06T07:17:30.682249Z","shell.execute_reply.started":"2021-09-06T07:17:26.346448Z","shell.execute_reply":"2021-09-06T07:17:30.681457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Test Dataset","metadata":{}},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-06T07:17:30.683479Z","iopub.execute_input":"2021-09-06T07:17:30.683947Z","iopub.status.idle":"2021-09-06T07:17:30.744997Z","shell.execute_reply.started":"2021-09-06T07:17:30.68391Z","shell.execute_reply":"2021-09-06T07:17:30.744107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Printing the information about actual and missing values in Testing Data","metadata":{}},{"cell_type":"code","source":"print(f'Number of Rows in Testing Dataset: {test.shape[0]}  \\nNumber of Columns in Testing Dataset: {test.shape[1]} \\nTotal Number of missing values in Testing Dataset: {sum(train.isna().sum())}')","metadata":{"execution":{"iopub.status.busy":"2021-09-06T07:17:30.746212Z","iopub.execute_input":"2021-09-06T07:17:30.746732Z","iopub.status.idle":"2021-09-06T07:17:30.949415Z","shell.execute_reply.started":"2021-09-06T07:17:30.746692Z","shell.execute_reply":"2021-09-06T07:17:30.948464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Printing the basic statistics for each variable in Testing Dataset which contain information on count, mean, standard deviation, minimum, 1st quartile, median, 3rd quartile and maximum.","metadata":{}},{"cell_type":"code","source":"test.describe()","metadata":{"execution":{"iopub.status.busy":"2021-09-06T07:17:30.951481Z","iopub.execute_input":"2021-09-06T07:17:30.951986Z","iopub.status.idle":"2021-09-06T07:17:33.564572Z","shell.execute_reply.started":"2021-09-06T07:17:30.951946Z","shell.execute_reply":"2021-09-06T07:17:33.563716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **3. EXPLORATORY DATA ANALYSIS**\n\n### Correlation Plot of the Dataset\n\n### We observed that majority of the correlation values are near to 0 which states that there are no highly dependent features.","metadata":{}},{"cell_type":"code","source":"corr = train.corr()\ncorr.style.background_gradient(cmap='coolwarm').set_precision(2)","metadata":{"execution":{"iopub.status.busy":"2021-09-06T07:17:33.568538Z","iopub.execute_input":"2021-09-06T07:17:33.570676Z","iopub.status.idle":"2021-09-06T07:18:08.455632Z","shell.execute_reply.started":"2021-09-06T07:17:33.570634Z","shell.execute_reply":"2021-09-06T07:18:08.454664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualizing the distribution of data into Training and Testing Set","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(5, 5))\npie = ax.pie([len(train), len(test)],\n             labels=[\"Train dataset\", \"Test dataset\"],\n             colors=[\"orange\", \"lightblue\"],\n             textprops={\"fontsize\": 15},\n             autopct='%1.1f%%')\nax.axis(\"equal\")\nax.set_title(\"Dataset length comparison\\n\", fontsize=18)\nfig.set_facecolor('white')\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-09-06T07:20:01.156033Z","iopub.execute_input":"2021-09-06T07:20:01.156357Z","iopub.status.idle":"2021-09-06T07:20:01.276845Z","shell.execute_reply.started":"2021-09-06T07:20:01.156325Z","shell.execute_reply":"2021-09-06T07:20:01.275867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualizing the distribution of Claim (Target) in the Training Set\n\n### Here we can observe that the target class is well balanced. This helps us to proceed with applying suitable techniques on the data during the data modeling phase. ","metadata":{}},{"cell_type":"code","source":"sns.set(font_scale=1.4)\ntrain['claim'].value_counts().plot(kind='bar',figsize=(7, 6), rot=0)\nplt.xlabel(\"Claim (target) value\", labelpad=14)\nplt.ylabel(\"Values\", labelpad=14)\nplt.title(\"Claim Value Distribution\", y=1.02);","metadata":{"execution":{"iopub.status.busy":"2021-09-06T07:20:02.796881Z","iopub.execute_input":"2021-09-06T07:20:02.797211Z","iopub.status.idle":"2021-09-06T07:20:02.953458Z","shell.execute_reply.started":"2021-09-06T07:20:02.797181Z","shell.execute_reply":"2021-09-06T07:20:02.952624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Analysing the features and their corresponding missing values in the Training Dataset","metadata":{}},{"cell_type":"code","source":"missing_train_df = pd.DataFrame(train.isna().sum())\nmissing_train_df = missing_train_df.drop(['id', 'claim']).reset_index()\nmissing_train_df.columns = ['feature', 'count']\nmissing_train_df['percentage'] = (missing_train_df['count']/train.shape[0])*100 \nmissing_train_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-06T07:20:05.055635Z","iopub.execute_input":"2021-09-06T07:20:05.055965Z","iopub.status.idle":"2021-09-06T07:20:05.26642Z","shell.execute_reply.started":"2021-09-06T07:20:05.055935Z","shell.execute_reply":"2021-09-06T07:20:05.265431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Missing feature values distribution in the Train dataset","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(20, 10))\n\nbars = ax.bar(missing_train_df['feature'],\n              missing_train_df['count'],\n              color=\"lightskyblue\",\n              edgecolor=\"black\",\n              width=0.5\n             )\nax.set_title(\"Missing feature values distribution in the train dataset\", fontsize=20, pad=15)\nax.set_ylabel(\"Missing values\", fontsize=14, labelpad=15)\nax.set_xlabel(\"Feature\", fontsize=14, labelpad=15)\nax.tick_params(axis=\"x\", rotation=90, labelsize=8)\nax.margins(0.005, 0.12)\nax.grid(axis=\"y\")\n\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-09-06T07:20:06.230181Z","iopub.execute_input":"2021-09-06T07:20:06.230611Z","iopub.status.idle":"2021-09-06T07:20:09.142021Z","shell.execute_reply.started":"2021-09-06T07:20:06.230576Z","shell.execute_reply":"2021-09-06T07:20:09.141141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Analysing the features and their corresponding missing values in the Testing Dataset","metadata":{}},{"cell_type":"code","source":"missing_test_df = pd.DataFrame(test.isna().sum())\nmissing_test_df = missing_test_df.drop(['id']).reset_index()\nmissing_test_df.columns = ['feature', 'count']\nmissing_test_df['percentage'] = (missing_test_df['count']/train.shape[0])*100 \nmissing_test_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-06T07:20:09.14375Z","iopub.execute_input":"2021-09-06T07:20:09.14411Z","iopub.status.idle":"2021-09-06T07:20:09.262827Z","shell.execute_reply.started":"2021-09-06T07:20:09.14407Z","shell.execute_reply":"2021-09-06T07:20:09.261853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Missing feature values distribution in the Test dataset","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(20, 10))\n\nbars = ax.bar(missing_test_df['feature'],\n              missing_test_df['count'],\n              color=\"lightskyblue\",\n              edgecolor=\"black\",\n              width=0.5\n             )\nax.set_title(\"Missing feature values distribution in the test dataset\", fontsize=20, pad=15)\nax.set_ylabel(\"Missing values\", fontsize=14, labelpad=15)\nax.set_xlabel(\"Feature\", fontsize=14, labelpad=15)\nax.tick_params(axis=\"x\", rotation=90, labelsize=8)\nax.margins(0.005, 0.12)\nax.grid(axis=\"y\")\n\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-09-06T07:20:09.264599Z","iopub.execute_input":"2021-09-06T07:20:09.264967Z","iopub.status.idle":"2021-09-06T07:20:12.041612Z","shell.execute_reply.started":"2021-09-06T07:20:09.26492Z","shell.execute_reply":"2021-09-06T07:20:12.040606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **4. DATASET PREPROCESSING**\n\n#### Creating a set of required features by dropping the id and claim column","metadata":{}},{"cell_type":"code","source":"features = [feature for feature in train.columns if feature not in ('id', 'claim')]","metadata":{"execution":{"iopub.status.busy":"2021-09-06T07:20:20.859005Z","iopub.execute_input":"2021-09-06T07:20:20.859324Z","iopub.status.idle":"2021-09-06T07:20:20.864321Z","shell.execute_reply.started":"2021-09-06T07:20:20.859294Z","shell.execute_reply":"2021-09-06T07:20:20.863495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Calculating the sum of missing values in each features and standard deviation of each feature. Adding those new calculated columns in our original dataset to use them in the modeling phase","metadata":{}},{"cell_type":"code","source":"train['num_missing'] = train[features].isna().sum(axis=1)\ntrain['std_dev'] = train[features].isna().std(axis=1)\n\ntest['num_missing'] = test[features].isna().sum(axis=1)\ntest['std_dev'] = test[features].isna().std(axis=1)\n\nfeatures += ['num_missing', 'std_dev']","metadata":{"execution":{"iopub.status.busy":"2021-09-06T07:20:22.056183Z","iopub.execute_input":"2021-09-06T07:20:22.056543Z","iopub.status.idle":"2021-09-06T07:20:24.708229Z","shell.execute_reply.started":"2021-09-06T07:20:22.05651Z","shell.execute_reply":"2021-09-06T07:20:24.707276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Imputing the null values with the mean of the respective column","metadata":{}},{"cell_type":"code","source":"train[features] = train[features].fillna(train[features].mean())\ntest[features] = test[features].fillna(test[features].mean())","metadata":{"execution":{"iopub.status.busy":"2021-09-06T07:20:24.710503Z","iopub.execute_input":"2021-09-06T07:20:24.711085Z","iopub.status.idle":"2021-09-06T07:20:27.857675Z","shell.execute_reply.started":"2021-09-06T07:20:24.711046Z","shell.execute_reply":"2021-09-06T07:20:27.85674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Scaling the training and testing features","metadata":{}},{"cell_type":"code","source":"scaler = StandardScaler()\ntrain[features] = scaler.fit_transform(train[features])\ntest[features] = scaler.transform(test[features])","metadata":{"execution":{"iopub.status.busy":"2021-09-06T07:20:27.8595Z","iopub.execute_input":"2021-09-06T07:20:27.860155Z","iopub.status.idle":"2021-09-06T07:21:13.63256Z","shell.execute_reply.started":"2021-09-06T07:20:27.860103Z","shell.execute_reply":"2021-09-06T07:21:13.631686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape, test.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-06T07:21:23.130882Z","iopub.execute_input":"2021-09-06T07:21:23.131327Z","iopub.status.idle":"2021-09-06T07:21:23.144014Z","shell.execute_reply.started":"2021-09-06T07:21:23.131284Z","shell.execute_reply":"2021-09-06T07:21:23.143006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = train.drop([\"id\", \"claim\"], axis=1)\nX_test = test.drop(\"id\", axis=1)\ny = train[\"claim\"]","metadata":{"execution":{"iopub.status.busy":"2021-09-06T07:21:23.506889Z","iopub.execute_input":"2021-09-06T07:21:23.507213Z","iopub.status.idle":"2021-09-06T07:21:24.813605Z","shell.execute_reply.started":"2021-09-06T07:21:23.507183Z","shell.execute_reply":"2021-09-06T07:21:24.812722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **5. HYPERPARAMETER OPTIMIZATION USING OPTUNA**\n\n#### Partial code for Hyperparameter Optimization using Optuna. This can further be extended to find the optimal hyperparameter value for training the Light GBM model.","metadata":{}},{"cell_type":"code","source":"def train_model_optuna(trial, X_train, X_valid, y_train, y_valid):\n    preds=0\n    lgbm_params = {\n        \"objective\": trial.suggest_categorical(\"objective\", ['binary']),\n        \"learning_rate\": trial.suggest_categorical(\"learning_rate\", [0.001, 0.005]),\n        \"n_estimators\": trial.suggest_categorical(\"n_estimators\", [20000])\n    }\n    \n    model = LGBMClassifier(**lgbm_params, device='gpu')\n    model.fit(X_train, y_train,\n              eval_set = [(X_valid, y_valid)],\n              eval_metric='auc',\n              early_stopping_rounds=100,\n              verbose=False\n             )\n    \n    print(f\"Number of boosting rounds: {model.best_iteration_}\")\n    oof = model.predict_proba(X_valid)[:, 1]\n    return roc_auc_score(y_valid, oof)","metadata":{"execution":{"iopub.status.busy":"2021-09-06T07:25:24.831636Z","iopub.execute_input":"2021-09-06T07:25:24.831979Z","iopub.status.idle":"2021-09-06T07:25:24.838705Z","shell.execute_reply.started":"2021-09-06T07:25:24.831948Z","shell.execute_reply":"2021-09-06T07:25:24.837711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=2021, stratify=y)\ntime_limit = 3600 * 4\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(lambda trial: train_model_optuna(trial, X_train, X_valid, y_train, y_valid),\n               n_trials=1,\n               timeout=time_limit\n              )\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial parameters:', study.best_trial.params)\nprint('Best score:', study.best_value)","metadata":{"execution":{"iopub.status.busy":"2021-09-06T07:25:25.366205Z","iopub.execute_input":"2021-09-06T07:25:25.366542Z","iopub.status.idle":"2021-09-06T07:40:20.187394Z","shell.execute_reply.started":"2021-09-06T07:25:25.366512Z","shell.execute_reply":"2021-09-06T07:40:20.186312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **6. DATA MODELING**\n\n## **LIGHTGBM CLASSIFIER WITH 5 FOLDS CV**","metadata":{}},{"cell_type":"code","source":"lgb_params = {\n    'objective': 'binary',\n    'n_estimators': 20000,\n    'random_state': 2021,\n    'learning_rate': 5e-3,\n    'subsample': 0.6,\n    'subsample_freq': 1,\n    'colsample_bytree': 0.4,\n    'reg_alpha': 10.0,\n    'reg_lambda': 1e-1,\n    'min_child_weight': 256,\n    'min_child_samples': 20,\n    'importance_type': 'gain',\n    'device': 'gpu'\n}","metadata":{"execution":{"iopub.status.busy":"2021-09-06T07:46:55.343008Z","iopub.execute_input":"2021-09-06T07:46:55.343331Z","iopub.status.idle":"2021-09-06T07:46:55.348017Z","shell.execute_reply.started":"2021-09-06T07:46:55.343301Z","shell.execute_reply":"2021-09-06T07:46:55.347163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgb_oof = np.zeros(train.shape[0])\nlgb_pred = np.zeros(test.shape[0])\nlgb_importances = pd.DataFrame()\n\nkf = KFold(n_splits=5, shuffle=True, random_state=2021)\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(train)):\n    print(f\"****** Fold {fold} ******\")\n    X_train = train[features].iloc[train_idx]\n    y_train = y.iloc[train_idx]\n    X_valid = train[features].iloc[val_idx]\n    y_valid = y.iloc[val_idx]\n    X_test = test[features]\n    \n    start = time.time()\n    model = LGBMClassifier(**lgb_params)\n    model.fit(X_train, y_train,\n              eval_set=[(X_valid, y_valid)],\n              eval_metric='auc',\n              early_stopping_rounds=200,\n              verbose=1000\n    )\n    \n    fi_temp = pd.DataFrame()\n    fi_temp['feature'] = model.feature_name_\n    fi_temp['importance'] = model.feature_importances_\n    fi_temp['fold'] = fold\n    fi_temp['seed'] = 2021\n    lgb_importances = lgb_importances.append(fi_temp)\n    \n    lgb_oof[val_idx] = model.predict_proba(X_valid)[:, -1]\n    lgb_pred += model.predict_proba(X_test)[:, -1] / 5\n    \n    elapsed = time.time() - start\n    auc = roc_auc_score(y_valid, lgb_oof[val_idx])\n    print(f\"fold {fold} - lgb auc: {auc:.6f}, elapsed time: {elapsed:.2f}sec\\n\")","metadata":{"execution":{"iopub.status.busy":"2021-09-06T07:46:59.988686Z","iopub.execute_input":"2021-09-06T07:46:59.989029Z","iopub.status.idle":"2021-09-06T10:25:43.101654Z","shell.execute_reply.started":"2021-09-06T07:46:59.988999Z","shell.execute_reply":"2021-09-06T10:25:43.099985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"oof lgb roc = {roc_auc_score(y, lgb_oof)}\")","metadata":{"execution":{"iopub.status.busy":"2021-09-06T10:26:22.538471Z","iopub.execute_input":"2021-09-06T10:26:22.538815Z","iopub.status.idle":"2021-09-06T10:26:22.896969Z","shell.execute_reply.started":"2021-09-06T10:26:22.538783Z","shell.execute_reply":"2021-09-06T10:26:22.896105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output = pd.DataFrame({'id': test.id,\n                       'claim': lgb_pred})\noutput.to_csv('submission_lgbm_hyper.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-06T10:26:43.098778Z","iopub.execute_input":"2021-09-06T10:26:43.099104Z","iopub.status.idle":"2021-09-06T10:26:44.868188Z","shell.execute_reply.started":"2021-09-06T10:26:43.099073Z","shell.execute_reply":"2021-09-06T10:26:44.867259Z"},"trusted":true},"execution_count":null,"outputs":[]}]}