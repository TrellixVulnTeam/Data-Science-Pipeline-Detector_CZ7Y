{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Working is still ongoing","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2021-09-19T12:50:36.971228Z","iopub.execute_input":"2021-09-19T12:50:36.971681Z","iopub.status.idle":"2021-09-19T12:50:36.996744Z","shell.execute_reply.started":"2021-09-19T12:50:36.971561Z","shell.execute_reply":"2021-09-19T12:50:36.994418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2021-09-19T12:50:36.998531Z","iopub.execute_input":"2021-09-19T12:50:36.998888Z","iopub.status.idle":"2021-09-19T12:50:37.908009Z","shell.execute_reply.started":"2021-09-19T12:50:36.998854Z","shell.execute_reply":"2021-09-19T12:50:37.907065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.graph_objects as go\nimport plotly.express as ex","metadata":{"execution":{"iopub.status.busy":"2021-09-19T12:50:37.909282Z","iopub.execute_input":"2021-09-19T12:50:37.909534Z","iopub.status.idle":"2021-09-19T12:50:39.563946Z","shell.execute_reply.started":"2021-09-19T12:50:37.909508Z","shell.execute_reply":"2021-09-19T12:50:39.562865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set_theme(\"paper\")\nsns.set_style(\"white\")","metadata":{"execution":{"iopub.status.busy":"2021-09-19T12:50:39.565319Z","iopub.execute_input":"2021-09-19T12:50:39.565588Z","iopub.status.idle":"2021-09-19T12:50:39.571906Z","shell.execute_reply.started":"2021-09-19T12:50:39.565558Z","shell.execute_reply":"2021-09-19T12:50:39.570826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option( \"display.max_rows\",120)\npd.set_option( \"display.max_columns\", 120)","metadata":{"execution":{"iopub.status.busy":"2021-09-19T12:50:39.575844Z","iopub.execute_input":"2021-09-19T12:50:39.576376Z","iopub.status.idle":"2021-09-19T12:50:39.582166Z","shell.execute_reply.started":"2021-09-19T12:50:39.576327Z","shell.execute_reply":"2021-09-19T12:50:39.581245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_file = \"../input/tabular-playground-series-sep-2021/train.csv\"\ntrain_master_df = pd.read_csv( train_file ,sep =\",\")\ntrain_master_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-19T12:50:39.583484Z","iopub.execute_input":"2021-09-19T12:50:39.583851Z","iopub.status.idle":"2021-09-19T12:51:10.562575Z","shell.execute_reply.started":"2021-09-19T12:50:39.58378Z","shell.execute_reply":"2021-09-19T12:51:10.561744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print ( \" Data frame shape = {}\".format(train_master_df.shape ))","metadata":{"execution":{"iopub.status.busy":"2021-09-19T12:51:10.563891Z","iopub.execute_input":"2021-09-19T12:51:10.564133Z","iopub.status.idle":"2021-09-19T12:51:10.569949Z","shell.execute_reply.started":"2021-09-19T12:51:10.564104Z","shell.execute_reply":"2021-09-19T12:51:10.568903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Check data  each column value distribution ","metadata":{}},{"cell_type":"code","source":"train_master_df.describe().T","metadata":{"execution":{"iopub.status.busy":"2021-09-19T12:51:10.571366Z","iopub.execute_input":"2021-09-19T12:51:10.571709Z","iopub.status.idle":"2021-09-19T12:51:15.797221Z","shell.execute_reply.started":"2021-09-19T12:51:10.571669Z","shell.execute_reply":"2021-09-19T12:51:15.796319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Have 118 features, and none of feaure are constant,\n* Values are linearly increasing","metadata":{}},{"cell_type":"markdown","source":"# Check number of claim and non claim count, to see data is imbalanced or not ","metadata":{}},{"cell_type":"code","source":"fig = plt.figure( figsize =(8,8), dpi = 90 )\nsns.countplot( train_master_df[\"claim\"])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-19T12:51:15.798742Z","iopub.execute_input":"2021-09-19T12:51:15.798997Z","iopub.status.idle":"2021-09-19T12:51:16.035962Z","shell.execute_reply.started":"2021-09-19T12:51:15.798968Z","shell.execute_reply":"2021-09-19T12:51:16.03508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Check for any missing values in dataframe ","metadata":{}},{"cell_type":"code","source":"train_master_df.isnull().sum(axis = 0)","metadata":{"execution":{"iopub.status.busy":"2021-09-19T12:51:16.037539Z","iopub.execute_input":"2021-09-19T12:51:16.037965Z","iopub.status.idle":"2021-09-19T12:51:16.29995Z","shell.execute_reply.started":"2021-09-19T12:51:16.037917Z","shell.execute_reply":"2021-09-19T12:51:16.299057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print( \"Max percentage of missing values in dataframe ={}\".format( train_master_df.isnull().sum().max()/ train_master_df.shape[0]))","metadata":{"execution":{"iopub.status.busy":"2021-09-19T12:51:16.301378Z","iopub.execute_input":"2021-09-19T12:51:16.301606Z","iopub.status.idle":"2021-09-19T12:51:16.55769Z","shell.execute_reply.started":"2021-09-19T12:51:16.30158Z","shell.execute_reply":"2021-09-19T12:51:16.556849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Above mising percentage might be differenet, taken only max number\n## Below Code to check missing values, we can make decission based on final percentage of data ","metadata":{}},{"cell_type":"code","source":"print( \"by droping all missing values remaining % data ={} %\".format(100* train_master_df.dropna(axis =0 ).shape[0]/train_master_df.shape[0] ) )","metadata":{"execution":{"iopub.status.busy":"2021-09-19T12:51:16.558961Z","iopub.execute_input":"2021-09-19T12:51:16.55921Z","iopub.status.idle":"2021-09-19T12:51:17.128115Z","shell.execute_reply.started":"2021-09-19T12:51:16.55918Z","shell.execute_reply":"2021-09-19T12:51:17.126905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* ** 37.52% data after removing all missing values, is not a good one we should do simthing better**\n* ** Let us do missing values imputation, I would select adding median value of respective column to mising places **\n* ** With this we can retain 100% of data **\n* ** There may be a question why median, why not mean or min or max, median will take middle value of distribution, were as mean will have high influence by outliers **\n","metadata":{}},{"cell_type":"code","source":"for each_column in train_master_df.columns:\n    if each_column not in [\"claim\",\"id\"] : train_master_df[each_column].fillna( train_master_df[each_column].median(), inplace = True )\ntrain_master_df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-09-19T12:51:17.129969Z","iopub.execute_input":"2021-09-19T12:51:17.130322Z","iopub.status.idle":"2021-09-19T12:51:19.244103Z","shell.execute_reply.started":"2021-09-19T12:51:17.130281Z","shell.execute_reply":"2021-09-19T12:51:19.243139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# With this, now we have 0 missing values, we can use 10% of data","metadata":{}},{"cell_type":"markdown","source":"# Visualize claim distribution with respect to other features\n# Using pair plot will create 117*117 plots which not suitable for visualization, might very hard to view as well\n# Instead of that we can use one feature and see how it is trending with respect to other. toget feel of data distribution of claim and non claim","metadata":{}},{"cell_type":"code","source":"'''\n# hacountplottal 118 columns, For ploting we can have 10 column and 12 rows\nfig = plt.figure( figsize = ( 10, 10 ), dpi = 90  )\ncounter =1\nfor each_column in train_master_df.columns:\n    if each_column not in [\"claim\", \"id\", \"f11\"]:\n        ax = plt.subplot( 12,10, counter )\n        sns.scatterplot( x = train_master_df[\"f11\"], y = train_master_df[each_column], ax = ax ,hue = train_master_df[\"claim\"] )\n        counter +=1\nplt.tight_layout()\nplt.show()\n'''","metadata":{"execution":{"iopub.status.busy":"2021-09-19T12:51:19.247017Z","iopub.execute_input":"2021-09-19T12:51:19.247243Z","iopub.status.idle":"2021-09-19T12:51:19.253181Z","shell.execute_reply.started":"2021-09-19T12:51:19.247216Z","shell.execute_reply":"2021-09-19T12:51:19.25264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* We have 117 features, that is plenty. We can reduce features wich are highly correlating and thos features which doesn't show different grouping\n* To remove features we can look at Correlation matrix or use RFE ( Recursive Feature Elimination ) to remove features. This we can set as baseline\n* Other way is to do PCA, by doing that we can remove all correlating features & dimensanality reduction as well.","metadata":{}},{"cell_type":"code","source":"feature_columns = train_master_df.columns.to_list()\nfeature_columns.remove(\"id\")\nfeature_columns.remove(\"claim\")","metadata":{"execution":{"iopub.status.busy":"2021-09-19T12:51:19.254332Z","iopub.execute_input":"2021-09-19T12:51:19.25514Z","iopub.status.idle":"2021-09-19T12:51:19.263958Z","shell.execute_reply.started":"2021-09-19T12:51:19.255107Z","shell.execute_reply":"2021-09-19T12:51:19.263192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr_matrix = train_master_df[feature_columns].corr()\ncorr_matrix.round( 2)","metadata":{"execution":{"iopub.status.busy":"2021-09-19T12:51:19.265066Z","iopub.execute_input":"2021-09-19T12:51:19.265433Z","iopub.status.idle":"2021-09-19T12:51:57.492985Z","shell.execute_reply.started":"2021-09-19T12:51:19.265374Z","shell.execute_reply":"2021-09-19T12:51:57.492277Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure( figsize = (25,25), dpi = 90 )\nsns.heatmap (corr_matrix.round(2),annot = True,annot_kws={\"size\":  5 } )\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-19T12:51:57.494207Z","iopub.execute_input":"2021-09-19T12:51:57.494576Z","iopub.status.idle":"2021-09-19T12:52:45.47959Z","shell.execute_reply.started":"2021-09-19T12:51:57.494545Z","shell.execute_reply":"2021-09-19T12:52:45.47889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Good news is, we don't have multi colinearity problem. we can use all features for model \n# Next step is to check is the data points trend, i.e min to max varaition, this give clues for us type of data scalar to use ","metadata":{}},{"cell_type":"code","source":"if False :\n\n    fig = plt.figure( figsize = ( 120,120 ), dpi = 100  )\n    counter =1\n    for each_column in train_master_df.columns:\n        if each_column not in [\"claim\", \"id\"]:\n            plt.subplot( 24,5, counter )\n            sns.histplot( x= train_master_df[each_column],hue = train_master_df[\"claim\"] )\n            counter +=1\n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-19T12:52:45.480744Z","iopub.execute_input":"2021-09-19T12:52:45.481027Z","iopub.status.idle":"2021-09-19T12:52:45.488322Z","shell.execute_reply.started":"2021-09-19T12:52:45.480989Z","shell.execute_reply":"2021-09-19T12:52:45.487268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# From plot we can clearly see, have good number of outliers\n# We should remove those outliersor clamp those values with certain threshold\n# First methos we do is removing outliers, and see how well the model behaves then followed by we can use clamping method","metadata":{}},{"cell_type":"code","source":"feature_columns = train_master_df.columns.to_list()\nfeature_columns.remove(\"id\")\nfeature_columns.remove(\"claim\")","metadata":{"execution":{"iopub.status.busy":"2021-09-19T12:52:45.489886Z","iopub.execute_input":"2021-09-19T12:52:45.490212Z","iopub.status.idle":"2021-09-19T12:52:45.502195Z","shell.execute_reply.started":"2021-09-19T12:52:45.49017Z","shell.execute_reply":"2021-09-19T12:52:45.501182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def df_plot( df ):\n    fig = plt.figure( figsize = ( 120,120 ), dpi = 100  )\n    counter =1\n    for each_column in train_master_df.columns:\n        if each_column not in [\"claim\", \"id\"]:\n            plt.subplot( 24,5, counter )\n            sns.histplot( x= df[each_column],hue = df[\"claim\"] )\n            counter +=1\n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-19T12:52:45.504155Z","iopub.execute_input":"2021-09-19T12:52:45.504547Z","iopub.status.idle":"2021-09-19T12:52:45.51566Z","shell.execute_reply.started":"2021-09-19T12:52:45.50449Z","shell.execute_reply":"2021-09-19T12:52:45.514851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.neighbors import LocalOutlierFactor","metadata":{"execution":{"iopub.status.busy":"2021-09-19T12:52:45.517105Z","iopub.execute_input":"2021-09-19T12:52:45.517801Z","iopub.status.idle":"2021-09-19T12:52:45.792177Z","shell.execute_reply.started":"2021-09-19T12:52:45.517729Z","shell.execute_reply":"2021-09-19T12:52:45.791231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"outlier_detector = LocalOutlierFactor( n_neighbors = 1, algorithm = \"auto\",metric =\"l1\" ,n_jobs = -1)\ndata = outlier_detector.fit_predict( train_master_df[ feature_columns[0:-1] ] )","metadata":{"execution":{"iopub.status.busy":"2021-09-19T13:13:35.29554Z","iopub.execute_input":"2021-09-19T13:13:35.296475Z","iopub.status.idle":"2021-09-19T13:13:46.322651Z","shell.execute_reply.started":"2021-09-19T13:13:35.29643Z","shell.execute_reply":"2021-09-19T13:13:46.321312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len( data[data ==-1 ])","metadata":{"execution":{"iopub.status.busy":"2021-09-19T13:13:47.7802Z","iopub.execute_input":"2021-09-19T13:13:47.780574Z","iopub.status.idle":"2021-09-19T13:13:47.790246Z","shell.execute_reply.started":"2021-09-19T13:13:47.780528Z","shell.execute_reply":"2021-09-19T13:13:47.789145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_master_df_2 = train_master_df[data ==1 ]\ntrain_master_df_2.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-19T13:13:54.839101Z","iopub.execute_input":"2021-09-19T13:13:54.839598Z","iopub.status.idle":"2021-09-19T13:13:55.356172Z","shell.execute_reply.started":"2021-09-19T13:13:54.839566Z","shell.execute_reply":"2021-09-19T13:13:55.355154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure( figsize = ( 120,120 ), dpi = 100  )\ncounter =1\nfor each_column in train_master_df_2.columns:\n    if each_column not in [\"claim\", \"id\"]:\n        plt.subplot( 24,5, counter )\n        sns.boxplot( y= train_master_df_2[each_column],x = train_master_df_2[\"claim\"] )\n        counter +=1\n\nplt.tight_layout()\nplt.show()\n#sns.boxplot( y = train_master_df_2[feature_columns[0]], x = train_master_df_2[\"claim\"])\n#sns.swarmplot( y = train_master_df_2[feature_columns[0]], x = train_master_df_2[\"claim\"], color = \"black\")","metadata":{"execution":{"iopub.status.busy":"2021-09-19T13:13:57.483311Z","iopub.execute_input":"2021-09-19T13:13:57.483668Z","iopub.status.idle":"2021-09-19T13:14:53.223024Z","shell.execute_reply.started":"2021-09-19T13:13:57.483618Z","shell.execute_reply":"2021-09-19T13:14:53.222079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2021-09-19T13:11:49.243624Z","iopub.execute_input":"2021-09-19T13:11:49.244443Z","iopub.status.idle":"2021-09-19T13:11:49.25854Z","shell.execute_reply.started":"2021-09-19T13:11:49.244391Z","shell.execute_reply":"2021-09-19T13:11:49.257442Z"},"trusted":true},"execution_count":null,"outputs":[]}]}