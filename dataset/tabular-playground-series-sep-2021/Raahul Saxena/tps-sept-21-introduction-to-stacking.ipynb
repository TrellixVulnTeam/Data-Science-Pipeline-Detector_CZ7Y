{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"background-color:rgba(215, 79, 21, 0.5);\">\n    <h1><center>Importing Libraries</center></h1>\n</div>","metadata":{}},{"cell_type":"code","source":"import random\nrandom.seed(123)\n\nimport pandas as pd\nimport numpy as np\nimport datatable as dt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# importing feature selection and processing packages\n\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom sklearn.feature_selection import SelectKBest,mutual_info_classif,SelectPercentile,VarianceThreshold\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS, ExhaustiveFeatureSelector as EFS\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler,PowerTransformer\nfrom sklearn.decomposition import PCA\nfrom sklearn.impute import SimpleImputer\n\n# importing modelling packages\n\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom xgboost import XGBClassifier\n\n# Optimisation Packages\n\nimport optuna\nfrom optuna import trial\nfrom optuna.samplers import TPESampler\nimport pprint\nimport joblib\nfrom skopt import BayesSearchCV\nfrom skopt.callbacks import DeadlineStopper, VerboseCallback, DeltaXStopper\nfrom skopt.space import Real, Categorical, Integer\nfrom time import time","metadata":{"execution":{"iopub.status.busy":"2021-09-24T12:26:33.173894Z","iopub.execute_input":"2021-09-24T12:26:33.174217Z","iopub.status.idle":"2021-09-24T12:26:36.459152Z","shell.execute_reply.started":"2021-09-24T12:26:33.174139Z","shell.execute_reply":"2021-09-24T12:26:36.4584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:rgba(215, 79, 21, 0.5);\">\n    <h1><center>Inputting Data</center></h1>\n</div>","metadata":{}},{"cell_type":"code","source":"# using datatable for faster loading\n\ntrain = dt.fread(r'../input/tabular-playground-series-sep-2021/train.csv').to_pandas()\ntest = dt.fread(r'../input/tabular-playground-series-sep-2021/test.csv').to_pandas()\nsub = dt.fread(r'../input/tabular-playground-series-sep-2021/sample_solution.csv').to_pandas()","metadata":{"execution":{"iopub.status.busy":"2021-09-24T12:26:42.130231Z","iopub.execute_input":"2021-09-24T12:26:42.130501Z","iopub.status.idle":"2021-09-24T12:26:57.791251Z","shell.execute_reply.started":"2021-09-24T12:26:42.130473Z","shell.execute_reply":"2021-09-24T12:26:57.790651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:rgba(215, 79, 21, 0.5);\">\n    <h1><center>Data Processing and Feature Engineering</center></h1>\n</div>","metadata":{}},{"cell_type":"code","source":"train_data = train.copy()\ntest_data = test.copy()\n\ntrain_data = train_data.drop('id',axis=1)\ntest_data = test_data.drop('id',axis=1)\n\nfeatures = train_data.columns[:-1]","metadata":{"execution":{"iopub.status.busy":"2021-09-24T12:27:41.014288Z","iopub.execute_input":"2021-09-24T12:27:41.014955Z","iopub.status.idle":"2021-09-24T12:27:41.866873Z","shell.execute_reply.started":"2021-09-24T12:27:41.014919Z","shell.execute_reply":"2021-09-24T12:27:41.866169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# adding the magic features - using missing values and trends of the data\n\ntrain_data['n_missing'] = train_data[features].isna().sum(axis=1)\ntest_data['n_missing'] = test_data[features].isna().sum(axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-09-24T12:27:47.322529Z","iopub.execute_input":"2021-09-24T12:27:47.32279Z","iopub.status.idle":"2021-09-24T12:27:47.949799Z","shell.execute_reply.started":"2021-09-24T12:27:47.322761Z","shell.execute_reply":"2021-09-24T12:27:47.948912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# splitting data and imputing missing values\n\nX = train_data.drop('claim',axis=1)\ny = train_data['claim'] # the target variable\n\nX = X.apply(lambda x:x.fillna(np.mean(x)))\ntest_data = test_data.apply(lambda x:x.fillna(np.mean(x)))","metadata":{"execution":{"iopub.status.busy":"2021-09-24T12:27:52.453861Z","iopub.execute_input":"2021-09-24T12:27:52.454145Z","iopub.status.idle":"2021-09-24T12:27:54.057157Z","shell.execute_reply.started":"2021-09-24T12:27:52.454096Z","shell.execute_reply":"2021-09-24T12:27:54.0563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# using minmax scaler to scale - we will use boosting models and need to converge to optimum value quickly\n\nscaler = MinMaxScaler()\nX = scaler.fit_transform(X)\ntest_for_model = scaler.transform(test_data)","metadata":{"execution":{"iopub.status.busy":"2021-09-24T12:27:57.177747Z","iopub.execute_input":"2021-09-24T12:27:57.178012Z","iopub.status.idle":"2021-09-24T12:27:59.208057Z","shell.execute_reply.started":"2021-09-24T12:27:57.177984Z","shell.execute_reply":"2021-09-24T12:27:59.207351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:rgba(215, 79, 21, 0.5);\">\n    <h1><center>Optuna on LightGBM</center></h1>\n</div>","metadata":{}},{"cell_type":"markdown","source":"The inspiration for choosing the hyper-parameter ranges is - \nhttps://www.kaggle.com/bextuychiev/lgbm-optuna-hyperparameter-tuning-w-understanding. \n\nPlease do upvote his work and check out his other illuminating notebooks. They are great if you have just started out on Kaggle.\nThis is the first time I set the lgbm ranges after understanding the parameters. Thanks a lot Bex!","metadata":{}},{"cell_type":"code","source":"def train_model_optuna(trial, X_train, X_valid, y_train, y_valid):\n\n    preds = 0\n           \n     #A set of hyperparameters to optimize by optuna\n    lgbm_params = {\n                     \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 3000, step=20),\n                     \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n                     \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n                      \"n_estimators\": trial.suggest_categorical(\"n_estimators\", [20000]),        \n                      \"lambda_l1\": trial.suggest_int(\"lambda_l1\", 0, 100, step=1),\n                     \"lambda_l2\": trial.suggest_int(\"lambda_l2\", 0, 100, step=1), \n                     \"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n                     \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.2, 0.95, step=0.1),\n                     \"bagging_freq\": trial.suggest_categorical(\"bagging_freq\", [1]),\n                     \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.2, 0.95, step=0.1),\n                      'device':'gpu'\n                     }\n\n    model = LGBMClassifier(**lgbm_params)\n    model.fit(X_train, y_train,eval_set=[(X_valid, y_valid)],eval_metric=\"auc\",\n               early_stopping_rounds=100,verbose=False)\n    \n    print(f\"Number of boosting rounds: {model.best_iteration_}\")\n    oof = model.predict_proba(X_valid)[:,1]\n    \n    return roc_auc_score(y_valid, oof)","metadata":{"execution":{"iopub.status.busy":"2021-09-20T10:22:16.030872Z","iopub.execute_input":"2021-09-20T10:22:16.031587Z","iopub.status.idle":"2021-09-20T10:22:16.042097Z","shell.execute_reply.started":"2021-09-20T10:22:16.031551Z","shell.execute_reply":"2021-09-20T10:22:16.041318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# setting up study on 0.67 training size\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.33, random_state=42, stratify=y)\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(lambda trial: train_model_optuna(trial, X_train, X_valid,y_train, y_valid),\n                n_trials = 10)\n\n# Showing optimization results\n\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial parameters:', study.best_trial.params)\nprint('Best score:', study.best_value)","metadata":{"execution":{"iopub.status.busy":"2021-09-20T10:22:28.428735Z","iopub.execute_input":"2021-09-20T10:22:28.42899Z","iopub.status.idle":"2021-09-20T10:42:37.912073Z","shell.execute_reply.started":"2021-09-20T10:22:28.428964Z","shell.execute_reply":"2021-09-20T10:42:37.911241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:rgba(215, 79, 21, 0.5);\">\n    <h1><center>Optuna on CatBoost</center></h1>\n</div>","metadata":{}},{"cell_type":"markdown","source":"References for parameter ranges -\n1. https://www.kaggle.com/mlanhenke/tps-09-optuna-study-catboostclassifier\n2. https://www.kaggle.com/ranjeetshrivastav/catboost-lightgbm\n\nPlease do upvote their work. I have used a combination from both these notebooks.","metadata":{}},{"cell_type":"code","source":"def train_model_optuna_2(trial, X_train, X_valid, y_train, y_valid):\n    \n    params = {'iterations':trial.suggest_int(\"iterations\", 1000, 20000),\n              'od_wait':trial.suggest_int('od_wait', 500, 2000),\n              'task_type':\"GPU\",\n              'learning_rate' : trial.suggest_uniform('learning_rate', 0.02 , 1),\n              'reg_lambda': trial.suggest_uniform('reg_lambda',1e-5,100),\n              'subsample': trial.suggest_uniform('subsample',0.9,1.0),\n              'random_strength': trial.suggest_uniform('random_strength',10,50),\n              'depth': trial.suggest_int('depth',1,15),\n              'min_data_in_leaf': trial.suggest_int('min_data_in_leaf',1,30),\n              'leaf_estimation_iterations': trial.suggest_int('leaf_estimation_iterations',1,15),\n              'bootstrap_type':'Poisson',\n               }\n    \n    model = CatBoostClassifier(**params)\n    model.fit(X_train, y_train,eval_set=[(X_valid,y_valid)], early_stopping_rounds=150, verbose=False)\n    \n    oof = model.predict_proba(X_valid)[:,1]\n    \n    return roc_auc_score(y_valid, oof)","metadata":{"execution":{"iopub.status.busy":"2021-09-20T11:07:08.263253Z","iopub.execute_input":"2021-09-20T11:07:08.263797Z","iopub.status.idle":"2021-09-20T11:07:08.274265Z","shell.execute_reply.started":"2021-09-20T11:07:08.263761Z","shell.execute_reply":"2021-09-20T11:07:08.273471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# setting up study on 0.67 training size\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.33, random_state=42, stratify=y)\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(lambda trial: train_model_optuna_2(trial,X_train, X_valid,y_train, y_valid),\n                n_trials = 10)\n\n# Showing optimization results\n\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial parameters:', study.best_trial.params)\nprint('Best score:', study.best_value)","metadata":{"execution":{"iopub.status.busy":"2021-09-20T11:07:13.483117Z","iopub.execute_input":"2021-09-20T11:07:13.483846Z","iopub.status.idle":"2021-09-20T11:24:56.799941Z","shell.execute_reply.started":"2021-09-20T11:07:13.483791Z","shell.execute_reply":"2021-09-20T11:24:56.798862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:rgba(215, 79, 21, 0.5);\">\n    <h1><center>Tuned Hyperparameters</center></h1>\n</div>","metadata":{}},{"cell_type":"code","source":"# LightGBM tuned parameters - I used 2000 instead of 20000 to get faster training\n# I am seeing what gets me good results for now\n\nlgbm_params = {'num_leaves': 2680, 'max_depth': 3, 'learning_rate': 0.1909226205418589,\n               'n_estimators': 20000, 'lambda_l1': 65, 'lambda_l2': 76,\n               'min_gain_to_split': 6.480697089399107, 'bagging_fraction': 0.7, \n               'bagging_freq': 1, 'feature_fraction': 0.6000000000000001}\n\n# CatBoost tuned parameters\n\ncat_params = {'iterations': 13969, 'od_wait': 1958, 'learning_rate': 0.04291773425770468,\n              'reg_lambda': 15.189348850727315, 'subsample': 0.9136087381151102, \n              'random_strength': 28.743778165335534, 'depth': 5, 'min_data_in_leaf': 25, \n              'leaf_estimation_iterations': 9,'bootstrap_type':'Poisson'}","metadata":{"execution":{"iopub.status.busy":"2021-09-24T12:28:06.52998Z","iopub.execute_input":"2021-09-24T12:28:06.530511Z","iopub.status.idle":"2021-09-24T12:28:06.536455Z","shell.execute_reply.started":"2021-09-24T12:28:06.530475Z","shell.execute_reply":"2021-09-24T12:28:06.535297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:rgba(215, 79, 21, 0.5);\">\n    <h1><center>Ensembling</center></h1>\n</div>","metadata":{}},{"cell_type":"code","source":"folds = StratifiedKFold(n_splits = 10, random_state = 228, shuffle = True)\n\npredictions_lgb = np.zeros(len(test_for_model))\npredictions_cb = np.zeros(len(test_for_model))\n\nlgb_oof = np.zeros(X.shape[0])\ncat_oof = np.zeros(X.shape[0])\n\nfor fold, (trn_idx, val_idx) in enumerate(folds.split(X,y)):\n    print(f\"Fold: {fold+1}\")\n    X_train, X_val = X[trn_idx], X[val_idx]\n    y_train, y_val = y[trn_idx], y[val_idx]\n\n    model_lgb =  LGBMClassifier(device='gpu',**lgbm_params)\n    model_cb =  CatBoostClassifier(task_type='GPU',**cat_params,verbose=0)\n    \n    model_lgb.fit(X_train, y_train)\n    pred_lgb = model_lgb.predict_proba(X_val)[:,1]\n    lgb_oof[val_idx] = pred_lgb\n    print('ROC of LGB: ',roc_auc_score(y_val,pred_lgb))\n    \n    model_cb.fit(X_train, y_train)\n    pred_cb = model_cb.predict_proba(X_val)[:,1]\n    cat_oof[val_idx] = pred_cb\n    print('ROC of CB: ',roc_auc_score(y_val,pred_cb))\n    \n    print(\"-\"*50)\n    \n    predictions_lgb += model_lgb.predict_proba(test_for_model)[:,1] / folds.n_splits\n    predictions_cb += model_cb.predict_proba(test_for_model)[:,1] / folds.n_splits","metadata":{"execution":{"iopub.status.busy":"2021-09-24T12:28:17.890869Z","iopub.execute_input":"2021-09-24T12:28:17.891686Z","iopub.status.idle":"2021-09-24T12:30:46.789971Z","shell.execute_reply.started":"2021-09-24T12:28:17.89164Z","shell.execute_reply":"2021-09-24T12:30:46.788155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# calculating appropriate weights for ensemble\n\nimport scipy\ndef class_optimizer(X, a0, a1):\n    oof = X[0]*a0 + (1-X[0])*a1\n    return (1-roc_auc_score(y, oof))\n\nres = scipy.optimize.minimize(\n    fun=class_optimizer,\n    x0=[0.5],\n    args=tuple([lgb_oof, cat_oof]),\n    method='BFGS',\n    options={'maxiter': 1000})\n\nprint(res)\nprint(f\"coef0 {res.x[0]}, coef1 {1-res.x[0]}\")","metadata":{"execution":{"iopub.status.busy":"2021-09-23T09:29:36.838591Z","iopub.execute_input":"2021-09-23T09:29:36.838873Z","iopub.status.idle":"2021-09-23T09:30:30.599066Z","shell.execute_reply.started":"2021-09-23T09:29:36.838844Z","shell.execute_reply":"2021-09-23T09:30:30.597987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Making ensemble using calculated weights\n\nensemble_oof = res.x[0] * lgb_oof + (1-res.x[0]) * cat_oof\nensemble_pred = res.x[0] * predictions_lgb  + (1-res.x[0]) * predictions_cb\n\n# tuned lgbm gave 0.81658 score and tuned catboost gave a score 0.81782\n\nprint(roc_auc_score(y, ensemble_oof))","metadata":{"execution":{"iopub.status.busy":"2021-09-23T09:30:38.189694Z","iopub.execute_input":"2021-09-23T09:30:38.189978Z","iopub.status.idle":"2021-09-23T09:30:38.530333Z","shell.execute_reply.started":"2021-09-23T09:30:38.189943Z","shell.execute_reply":"2021-09-23T09:30:38.52931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub['claim'] = ensemble_pred\nsub.to_csv('submission_cb_lgb_tuned_ensemble.csv',index = False) # gave a score of 0.81804","metadata":{"execution":{"iopub.status.busy":"2021-09-23T09:30:55.009092Z","iopub.execute_input":"2021-09-23T09:30:55.009418Z","iopub.status.idle":"2021-09-23T09:30:56.760461Z","shell.execute_reply.started":"2021-09-23T09:30:55.009388Z","shell.execute_reply":"2021-09-23T09:30:56.759316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:rgba(215, 79, 21, 0.5);\">\n    <h1><center>Stacking</center></h1>\n</div>","metadata":{}},{"cell_type":"code","source":"def Stacking(model, model_name, x_train, y_train, x_test, fold):\n\n    stk = StratifiedKFold(n_splits = fold, random_state = 42, shuffle = True)\n    \n    # Declaration Pred Datasets\n    train_fold_pred = np.zeros((x_train.shape[0], 1))\n    test_pred = np.zeros((x_test.shape[0], fold))\n    \n    for counter, (train_index, valid_index) in enumerate(stk.split(x_train, y_train)):\n        x_train, y_train = X[train_index], y[train_index]\n        x_valid, y_valid = X[valid_index], y[valid_index]\n        \n        print('------------ Fold', counter+1, 'Start! ------------')\n        if model_name == 'cat':\n            model.fit(x_train, y_train, eval_set=[(x_valid, y_valid)])\n        elif model_name == 'lgbm':\n            model.fit(x_train, y_train, eval_set=[(x_valid, y_valid)], eval_metric = 'auc')            \n        print('------------ Fold', counter+1, 'Done! ------------')\n        \n        train_fold_pred[valid_index, :] = model.predict_proba(x_valid)[:, 1].reshape(-1, 1)\n        test_pred[:, counter] = model.predict_proba(x_test)[:, 1]\n    \n    test_pred_mean = np.mean(test_pred, axis = 1).reshape(-1, 1)\n\n    print('Done!')\n    \n    return train_fold_pred, test_pred_mean","metadata":{"execution":{"iopub.status.busy":"2021-09-20T12:17:11.627359Z","iopub.execute_input":"2021-09-20T12:17:11.627631Z","iopub.status.idle":"2021-09-20T12:17:11.639849Z","shell.execute_reply.started":"2021-09-20T12:17:11.627604Z","shell.execute_reply":"2021-09-20T12:17:11.637468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_lgb =  LGBMClassifier(device = 'gpu',**lgbm_params,verbose=0)\nmodel_cb =  CatBoostClassifier(task_type='GPU',**cat_params,verbose=0)\n\ncat_train, cat_test = Stacking(model_cb, 'cat', X, y, test_for_model, 10)\nlgbm_train, lgbm_test = Stacking(model_lgb, 'lgbm', X, y, test_for_model, 10)","metadata":{"execution":{"iopub.status.busy":"2021-09-20T12:25:09.472888Z","iopub.execute_input":"2021-09-20T12:25:09.473205Z","iopub.status.idle":"2021-09-20T12:25:09.477536Z","shell.execute_reply.started":"2021-09-20T12:25:09.473173Z","shell.execute_reply":"2021-09-20T12:25:09.476878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating stack datasets for our meta-classifier - using created feature\n\nnew_features = train_data.columns[train_data.columns!='claim']\nimp_features= ['n_missing']\nX_new = pd.DataFrame(X,columns=new_features)\ntest_for_model_new = pd.DataFrame(test_for_model,columns=new_features)\ntrain_new = X_new[imp_features]\ntest_new = test_for_model_new[imp_features]\n\nlgbm_train_1 = pd.DataFrame(lgb_oof,columns=['LGBM_train'])\ncat_train_1 = pd.DataFrame(cat_oof,columns=['CAT_train'])\nlgbm_test_1 = pd.DataFrame(predictions_lgb,columns=['LGBM_train'])\ncat_test_1 = pd.DataFrame(predictions_cb,columns=['CAT_train'])\n\nstack_x_train = pd.concat((train_new,lgbm_train_1, cat_train_1), axis = 1)\nstack_x_test = pd.concat((test_new,lgbm_test_1, cat_test_1), axis = 1)","metadata":{"execution":{"iopub.status.busy":"2021-09-23T09:32:26.013159Z","iopub.execute_input":"2021-09-23T09:32:26.01353Z","iopub.status.idle":"2021-09-23T09:32:26.039681Z","shell.execute_reply.started":"2021-09-23T09:32:26.013501Z","shell.execute_reply":"2021-09-23T09:32:26.038412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I have used an untuned Logistic Regression model, based on what I learnt from the following notebook-\nhttps://www.kaggle.com/junhyeok99/stacking-ensemble-for-beginner\n\nPlease do upvote it. It helped me implement and learn stacking, being the beginner that I am, for the very first time!","metadata":{}},{"cell_type":"code","source":"stk = StratifiedKFold(n_splits = 5, random_state = 42)\n\ntest_pred = 0\nfold = 1\ntotal_auc = 0\n\nfor train_index, valid_index in stk.split(stack_x_train, y):\n    x_train, y_train = stack_x_train.iloc[train_index], y[train_index]\n    x_valid, y_valid = stack_x_train.iloc[valid_index], y[valid_index]\n    \n    #lr = LogisticRegression(n_jobs = -1, random_state = 42, C = 1000, max_iter = 1000)\n    lr = RidgeClassifier()\n    lr.fit(x_train, y_train)\n    \n    valid_pred = lr.predict_proba(x_valid)[:, 1]\n    test_pred += lr.predict_proba(stack_x_test)[:, 1]\n    auc = roc_auc_score(y_valid, valid_pred)\n    total_auc += auc / 10\n    print('Fold', fold, 'AUC :', auc)\n    fold += 1\n    \nprint('Total AUC score :', total_auc)","metadata":{"execution":{"iopub.status.busy":"2021-09-23T09:32:31.805257Z","iopub.execute_input":"2021-09-23T09:32:31.80556Z","iopub.status.idle":"2021-09-23T09:32:53.823143Z","shell.execute_reply.started":"2021-09-23T09:32:31.805532Z","shell.execute_reply":"2021-09-23T09:32:53.822118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub['claim'] = test_pred/10\nsub.to_csv('submission_lgb_cb_tuned_log_FE_stacking.csv', index = 0) # gave a score of 0.81805","metadata":{"execution":{"iopub.status.busy":"2021-09-23T09:32:58.144922Z","iopub.execute_input":"2021-09-23T09:32:58.145289Z","iopub.status.idle":"2021-09-23T09:32:59.739164Z","shell.execute_reply.started":"2021-09-23T09:32:58.145236Z","shell.execute_reply":"2021-09-23T09:32:59.738269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:rgba(215, 79, 21, 0.5);\">\n    <h1><center>Blending</center></h1>\n</div>","metadata":{}},{"cell_type":"code","source":"# Blending is a subtype of stacking - we will use only one fold, instead of 10 folds, here.\n\nX_train,X_val,y_train,y_val = train_test_split(X,y,test_size=0.33, random_state=2021,stratify=y)","metadata":{"execution":{"iopub.status.busy":"2021-09-19T08:14:00.614684Z","iopub.execute_input":"2021-09-19T08:14:00.614935Z","iopub.status.idle":"2021-09-19T08:14:02.508894Z","shell.execute_reply.started":"2021-09-19T08:14:00.614909Z","shell.execute_reply":"2021-09-19T08:14:02.508147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_lgb =  LGBMClassifier(device = 'gpu',verbose=0)\nmodel_cb =  CatBoostClassifier(task_type='GPU',verbose=0)\nmodel_xgb =  XGBClassifier(tree_method='gpu_hist',verbose=0)\n\npredictions_lgb = np.zeros(len(test_for_model))\npredictions_cb = np.zeros(len(test_for_model))\npredictions_xgb = np.zeros(len(test_for_model))\n\nlgb_oof = np.zeros(len(X_val))\ncat_oof = np.zeros(len(X_val))\nxgb_oof = np.zeros(len(X_val))","metadata":{"execution":{"iopub.status.busy":"2021-09-19T08:14:17.855276Z","iopub.execute_input":"2021-09-19T08:14:17.855843Z","iopub.status.idle":"2021-09-19T08:14:17.867234Z","shell.execute_reply.started":"2021-09-19T08:14:17.85581Z","shell.execute_reply":"2021-09-19T08:14:17.866541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# first model - LightGBM\n\nmodel_lgb.fit(X_train,y_train)\nlgb_oof = model_lgb.predict_proba(X_val)[:,1]\npredictions_lgb = model_lgb.predict_proba(test_for_model)[:,1]\n\n# second model - Catboost\n\nmodel_cb.fit(X_train,y_train)\ncat_oof = model_cb.predict_proba(X_val)[:,1]\npredictions_cb = model_cb.predict_proba(test_for_model)[:,1]\n\n# third model - XGBoost\n\nmodel_xgb.fit(X_train,y_train)\nxgb_oof = model_xgb.predict_proba(X_val)[:,1]\npredictions_xgb = model_xgb.predict_proba(test_for_model)[:,1]","metadata":{"execution":{"iopub.status.busy":"2021-09-19T08:14:25.882037Z","iopub.execute_input":"2021-09-19T08:14:25.882327Z","iopub.status.idle":"2021-09-19T08:17:10.067442Z","shell.execute_reply.started":"2021-09-19T08:14:25.882296Z","shell.execute_reply":"2021-09-19T08:17:10.066653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating datasets for our meta-classifier - using all features\n\nblend_x_val = pd.concat([pd.DataFrame(X_val),pd.DataFrame(lgb_oof,columns=['lgbm']),\n                         pd.DataFrame(cat_oof,columns=['cat']),\n                         pd.DataFrame(xgb_oof,columns=['xgb'])], axis = 1)\nblend_x_test = pd.concat([pd.DataFrame(test_for_model),pd.DataFrame(predictions_lgb,columns=['lgbm']),\n                          pd.DataFrame(predictions_cb,columns=['cat'])\n                          ,pd.DataFrame(predictions_xgb,columns=['xgb'])], axis = 1)","metadata":{"execution":{"iopub.status.busy":"2021-09-19T08:26:20.286876Z","iopub.execute_input":"2021-09-19T08:26:20.28745Z","iopub.status.idle":"2021-09-19T08:26:20.887864Z","shell.execute_reply.started":"2021-09-19T08:26:20.287411Z","shell.execute_reply":"2021-09-19T08:26:20.886994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = CatBoostClassifier(task_type='GPU',verbose=0)\n\nmodel.fit(blend_x_val,y_val)\npredictions = model.predict_proba(blend_x_test)[:,1]","metadata":{"execution":{"iopub.status.busy":"2021-09-19T08:26:23.65031Z","iopub.execute_input":"2021-09-19T08:26:23.651002Z","iopub.status.idle":"2021-09-19T08:26:38.947156Z","shell.execute_reply.started":"2021-09-19T08:26:23.650966Z","shell.execute_reply":"2021-09-19T08:26:38.946349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub['claim'] = predictions\nsub.to_csv('submission_lgb_cb_cb_FE_blending.csv', index = 0) # Only for illustration","metadata":{"execution":{"iopub.status.busy":"2021-09-19T08:27:14.341009Z","iopub.execute_input":"2021-09-19T08:27:14.341367Z","iopub.status.idle":"2021-09-19T08:27:16.098839Z","shell.execute_reply.started":"2021-09-19T08:27:14.34133Z","shell.execute_reply":"2021-09-19T08:27:16.097789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:rgba(215, 79, 21, 0.5);\">\n    <h1><center>Please upvote if you liked my notebook! Thanks :)</center></h1>\n</div>","metadata":{}}]}