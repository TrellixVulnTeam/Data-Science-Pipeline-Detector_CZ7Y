{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Dependencies","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport altair as alt\nalt.data_transformers.disable_max_rows()\nimport matplotlib.pyplot as plt\nfrom xgboost import XGBClassifier","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-04T06:13:49.024208Z","iopub.execute_input":"2021-09-04T06:13:49.026324Z","iopub.status.idle":"2021-09-04T06:13:50.186889Z","shell.execute_reply.started":"2021-09-04T06:13:49.026229Z","shell.execute_reply":"2021-09-04T06:13:50.185977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/tabular-playground-series-sep-2021/train.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-09-03T02:42:54.667571Z","iopub.execute_input":"2021-09-03T02:42:54.668049Z","iopub.status.idle":"2021-09-03T02:43:18.425823Z","shell.execute_reply.started":"2021-09-03T02:42:54.668005Z","shell.execute_reply":"2021-09-03T02:43:18.424681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2021-09-03T02:43:18.427172Z","iopub.execute_input":"2021-09-03T02:43:18.427505Z","iopub.status.idle":"2021-09-03T02:43:18.600215Z","shell.execute_reply.started":"2021-09-03T02:43:18.427465Z","shell.execute_reply":"2021-09-03T02:43:18.599179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"columns f1 to f118 are features that affect the the probability that a person will claim the insurance policy our task is to find out the relation between the features and the claim column","metadata":{}},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2021-09-03T02:43:18.601519Z","iopub.execute_input":"2021-09-03T02:43:18.601854Z","iopub.status.idle":"2021-09-03T02:43:23.546267Z","shell.execute_reply.started":"2021-09-03T02:43:18.60181Z","shell.execute_reply":"2021-09-03T02:43:23.54541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"f1\"].isnull().sum()\n5* 957919 / 100","metadata":{"execution":{"iopub.status.busy":"2021-09-03T02:43:23.547568Z","iopub.execute_input":"2021-09-03T02:43:23.547881Z","iopub.status.idle":"2021-09-03T02:43:23.556954Z","shell.execute_reply.started":"2021-09-03T02:43:23.54785Z","shell.execute_reply":"2021-09-03T02:43:23.555842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First thing we'll do is remove the NAN values. Since all the features are numerical I will fill the Nan rows with the mean of the specific column. If a column contains more NaN values than 50% of the number of rows it will be removed. ( 5% of 957919 = 47895.95)","metadata":{}},{"cell_type":"code","source":"column_names = []\nnumber_of_Nan_values = []\nfor x in df.columns:\n    column_names.append(x)\n    number_of_Nan_values.append(df[x].isnull().sum())\n\nnull_info = pd.DataFrame({\n    \"column_names\" : column_names,\n    \"number_of_NAN_values\" : number_of_Nan_values\n})\ndisplay(null_info)\ndisplay(null_info[null_info[\"number_of_NAN_values\"] < 47895.95])","metadata":{"execution":{"iopub.status.busy":"2021-09-03T02:43:23.559229Z","iopub.execute_input":"2021-09-03T02:43:23.559584Z","iopub.status.idle":"2021-09-03T02:43:23.855995Z","shell.execute_reply.started":"2021-09-03T02:43:23.559551Z","shell.execute_reply":"2021-09-03T02:43:23.854996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since all the collumns have less than 47895.95 null values we will not need to drop any of them.","metadata":{}},{"cell_type":"code","source":"for x in df.columns:\n    df[x].fillna(df[x].mean() , inplace = True)\n\n\ncolumn_names = []\nnumber_of_Nan_values = []\nfor x in df.columns:\n    column_names.append(x)\n    number_of_Nan_values.append(df[x].isnull().sum())\n\nnull_info = pd.DataFrame({\n    \"column_names\" : column_names,\n    \"number_of_NAN_values\" : number_of_Nan_values\n})\ndisplay(null_info)\ndisplay(null_info[null_info[\"number_of_NAN_values\"] < 47895.95])","metadata":{"execution":{"iopub.status.busy":"2021-09-03T02:43:23.857376Z","iopub.execute_input":"2021-09-03T02:43:23.857973Z","iopub.status.idle":"2021-09-03T02:43:24.576535Z","shell.execute_reply.started":"2021-09-03T02:43:23.857929Z","shell.execute_reply":"2021-09-03T02:43:24.575489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that we've removed ","metadata":{}},{"cell_type":"code","source":"data = {\n    \"claimed\" : len(df[df[\"claim\"] == 1]),\n    \"unclaimed\" : len(df[df[\"claim\"] == 0])\n}\nstatus = list(data.keys())\ncount = list(data.values())\nfig = plt.figure(figsize = (10, 5))\nplt.bar(status, count, color ='maroon',\n        width = 0.1)\n \nplt.xlabel(\"status\")\nplt.ylabel(\"No.\")\nplt.title(\"Class diffference\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-03T02:43:24.577743Z","iopub.execute_input":"2021-09-03T02:43:24.57804Z","iopub.status.idle":"2021-09-03T02:43:25.322764Z","shell.execute_reply.started":"2021-09-03T02:43:24.578013Z","shell.execute_reply":"2021-09-03T02:43:25.32168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since the two classes are the more or less the same we won't have to worry about class Imbalance","metadata":{}},{"cell_type":"markdown","source":"Next we'll attempt to train a model on the dataset the models we'll be using will be Logistic Regression , Random Forests and XGBoost we'll measure the models accuracy and auc score","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score , roc_auc_score\nfrom sklearn.model_selection import train_test_split\ncols = list(df.columns)\ncols.pop(-1)\ncols.pop(0)\nX = df[cols]\ny = df[\"claim\"]\n\ntrain_x , test_x , train_y , test_y = train_test_split(X,y,test_size = 0.33 , stratify = df[\"claim\"] , random_state = 42)","metadata":{"execution":{"iopub.status.busy":"2021-09-03T02:44:12.018474Z","iopub.execute_input":"2021-09-03T02:44:12.018829Z","iopub.status.idle":"2021-09-03T02:44:14.535915Z","shell.execute_reply.started":"2021-09-03T02:44:12.018801Z","shell.execute_reply":"2021-09-03T02:44:14.53453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def score(prediction , probability , true_value):\n    roc_score = roc_auc_score(true_value , probability)\n    accuracy = accuracy_score(true_value , prediction)\n    print(\"roc_auc_score :\" , roc_score)\n    print(\"accuracy :\", accuracy)","metadata":{"execution":{"iopub.status.busy":"2021-09-02T16:15:10.962249Z","iopub.execute_input":"2021-09-02T16:15:10.962557Z","iopub.status.idle":"2021-09-02T16:15:10.967707Z","shell.execute_reply.started":"2021-09-02T16:15:10.962528Z","shell.execute_reply":"2021-09-02T16:15:10.966563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Logistic Regression Model","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nLR = LogisticRegression()\nLR.fit(train_x,train_y)\nprediction = LR.predict(test_x)\nprobability = LR.predict_proba(test_x)\nscore(prediction , probability[: , -1] , test_y)","metadata":{"execution":{"iopub.status.busy":"2021-09-02T16:32:46.826421Z","iopub.execute_input":"2021-09-02T16:32:46.826857Z","iopub.status.idle":"2021-09-02T16:32:54.166354Z","shell.execute_reply.started":"2021-09-02T16:32:46.826819Z","shell.execute_reply":"2021-09-02T16:32:54.165522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Random Forests","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nRF = RandomForestClassifier(max_depth= 10)\nRF.fit(train_x,train_y)\nprediction = RF.predict(test_x)\nprobability = RF.predict_proba(test_x)\nscore(prediction , probability[: , 0] , test_y)","metadata":{"execution":{"iopub.status.busy":"2021-09-02T16:32:54.167798Z","iopub.execute_input":"2021-09-02T16:32:54.168097Z","iopub.status.idle":"2021-09-02T16:33:47.352203Z","shell.execute_reply.started":"2021-09-02T16:32:54.168065Z","shell.execute_reply":"2021-09-02T16:33:47.35009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### XGBoost Classifier","metadata":{}},{"cell_type":"code","source":"model = XGBClassifier(label_encoder = False ,  eval_metric = roc_auc_score , learning_rate = 0.1)\nmodel.fit(train_x,train_y)\nprediction = model.predict(test_x)\nprobability = model.predict_proba(test_x)\nscore(prediction , probability[: , 0] , test_y)","metadata":{"execution":{"iopub.status.busy":"2021-09-03T02:46:11.863413Z","iopub.execute_input":"2021-09-03T02:46:11.863824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since xgboost has the best score we'll train it on the entire train dataset and predict the test dataset","metadata":{}},{"cell_type":"code","source":"test_df = pd.read_csv(\"../input/tabular-playground-series-sep-2021/test.csv\")\nfor x in test_df.columns:\n    test_df[x].fillna(test_df[x].mean() , inplace = True)","metadata":{"execution":{"iopub.status.busy":"2021-09-04T06:18:59.656346Z","iopub.execute_input":"2021-09-04T06:18:59.656767Z","iopub.status.idle":"2021-09-04T06:19:14.828283Z","shell.execute_reply.started":"2021-09-04T06:18:59.656723Z","shell.execute_reply":"2021-09-04T06:19:14.827501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best = XGBClassifier(label_encoder = False ,  eval_metric = roc_auc_score , learning_rate = 0.1)\nbest.fit(X,y)\npredictions = best.predict_proba(test_df[cols])[:,0]\ndata = {\n    \"id\" : list(test_df[\"id\"]),\n    \"claim\" : list(predictions)\n}\nsubmission = pd.DataFrame(data)\n\nsubmission.to_csv(\"submission.csv\", index = False)","metadata":{"execution":{"iopub.status.busy":"2021-09-02T16:48:48.206276Z","iopub.execute_input":"2021-09-02T16:48:48.206986Z","iopub.status.idle":"2021-09-02T16:50:26.637293Z","shell.execute_reply.started":"2021-09-02T16:48:48.206947Z","shell.execute_reply":"2021-09-02T16:50:26.636363Z"},"trusted":true},"execution_count":null,"outputs":[]}]}