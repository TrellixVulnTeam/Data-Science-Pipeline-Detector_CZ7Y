{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Table of Contents\n<a id=\"table-of-contents\"></a>\n- [1 - Introduction](#1)\n- [2 - Libraries and Data import](#2)\n    - [2.1 - Memory Reduction Funtionality](#2.1)\n    - [2.2 - Min Max Scaler FUntion](#2.2)\n    - [2.3 - XGB Trainer Function](#2.3)\n    - [2.4 - Data Import](#2.4)\n- [3 - NA values in train and test](#3)\n- [4 - Feature Engeneer with NA Values](#4)\n- [5 - Filling NA value strategies](#5)\n    - [5.1 - No NA handling](#5.1)\n    - [5.2 - NA values to Zeros](#5.2)\n    - [5.3 - NA Values Imputed to mean](#5.3)\n    - [5.4 - NA Values Imputed to median](#5.4)\n    - [5.5 - MICE with a dropNa training set](#5.5)\n    - [5.6 - MICE with a sampled training set](#5.6)\n    - [5.7 - MICE with the whole training set](#5.7)\n    - [5.8 - Mean, Median, Mode](#5.8)\n- [6 - Results](#6)\n- [7 - Comments](#6)\n   ","metadata":{}},{"cell_type":"markdown","source":"[back to top](#table-of-contents)\n<a id=\"1\"></a>\n# 1 - Introduction\nHello everybody.\nIn this notebook I tried to calculate the differences in performance between several strategies of NA handling.\nI applied a simple, fast, barely tuned Stratified XGB Classifier in a 6 branches CrossValidation to understand which strategy could aim to better result.\nHope you like this notebook.\nFeel free to comment and please upvote if you like.\n","metadata":{}},{"cell_type":"markdown","source":"[back to top](#table-of-contents)\n<a id=\"1\"></a>\n# 2 - Libraries and Data import","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport random\n\nfrom xgboost import XGBClassifier\n\nfrom sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import roc_auc_score\n\n# Pandas setting to display more dataset rows and columns\npd.set_option('display.max_rows', 100)\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_colwidth', None)\npd.set_option('display.float_format', lambda x: '%.5f' % x)\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=UserWarning)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-26T14:28:30.867028Z","iopub.execute_input":"2021-09-26T14:28:30.86747Z","iopub.status.idle":"2021-09-26T14:28:31.927669Z","shell.execute_reply.started":"2021-09-26T14:28:30.86733Z","shell.execute_reply":"2021-09-26T14:28:31.926528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[back to top](#table-of-contents)\n<a id=\"2.1\"></a>\n## 2.1 - Memory Reduction Funtionality\n\nDue to the size of the DataSet I had to reduce memory usage to avoid NB crash.\n","metadata":{}},{"cell_type":"code","source":"def reduce_memory_usage(df, verbose=True):\n    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n    start_mem = df.memory_usage().sum() / 1024 ** 2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if (\n                    c_min > np.finfo(np.float16).min\n                    and c_max < np.finfo(np.float16).max\n                ):\n                    df[col] = df[col].astype(np.float16)\n                elif (\n                    c_min > np.finfo(np.float32).min\n                    and c_max < np.finfo(np.float32).max\n                ):\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() / 1024 ** 2\n    if verbose:\n        print(\n            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n                end_mem, 100 * (start_mem - end_mem) / start_mem\n            )\n        )\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-09-26T14:28:35.995054Z","iopub.execute_input":"2021-09-26T14:28:35.995466Z","iopub.status.idle":"2021-09-26T14:28:36.01229Z","shell.execute_reply.started":"2021-09-26T14:28:35.995434Z","shell.execute_reply":"2021-09-26T14:28:36.010517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[back to top](#table-of-contents)\n<a id=\"2.2\"></a>\n## 2.2 - Min Max Scaler function","metadata":{}},{"cell_type":"code","source":"def min_max_scaler(train, test):\n    x_Mm_scaler = MinMaxScaler()\n    X = pd.DataFrame(x_Mm_scaler.fit_transform(train.drop(\"claim\", axis=1)),\n                     columns=train.drop(\"claim\", axis=1).columns, index = train.index)\n    X_test = pd.DataFrame(x_Mm_scaler.transform(test), columns=test.columns, index = test.index)\n    return X, train.claim, X_test","metadata":{"execution":{"iopub.status.busy":"2021-09-26T14:28:38.392759Z","iopub.execute_input":"2021-09-26T14:28:38.393201Z","iopub.status.idle":"2021-09-26T14:28:38.401088Z","shell.execute_reply.started":"2021-09-26T14:28:38.393167Z","shell.execute_reply":"2021-09-26T14:28:38.399794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[back to top](#table-of-contents)\n<a id=\"2.3\"></a>\n## 2.3- XGB Trainer function\n","metadata":{}},{"cell_type":"code","source":"# xgb parameters obtained by optuna studies: \nxgb_params = {'n_estimators': 10000, \n              'learning_rate': 0.05378242228966539, \n              'subsample': 0.7502075656719964, \n              'colsample_bytree': 0.8665945134281674, \n              'max_depth': 6, 'booster': 'gbtree', \n              'tree_method': 'gpu_hist', \n              'reg_lambda': 99.32136303076183,\n              'reg_alpha': 32.78057640555784,\n              'random_state': 42,\n              'n_jobs': 4}\n\ndef xgb_train(name, X, y ,X_test,splits = 6, xgb_params = xgb_params):\n  \n    \n    predictions = pd.DataFrame()\n    predictions[\"id\"] = X_test.index\n    \n    skf = StratifiedKFold(n_splits=splits, shuffle=True, random_state=42)\n    oof_preds = np.zeros((X.shape[0],))\n    preds = 0\n    model_fi = 0\n    total_mean_rmse = 0\n    fold_roc_auc_score = 0\n    total_mean_roc_auc_score = 0\n\n    for fold, (train_indicies, valid_indicies) in enumerate(skf.split(X,y)):\n\n        X_train, X_valid = X.loc[train_indicies], X.loc[valid_indicies]\n        y_train, y_valid = y.loc[train_indicies], y.loc[valid_indicies]\n        print(f\"Training fold num. {fold+1} of {splits}\")\n        \n        model = XGBClassifier(**xgb_params)\n        model.fit(X_train, y_train,\n                  eval_set=[(X_train, y_train), (X_valid, y_valid)],\n                  eval_metric=\"rmse\",\n                  early_stopping_rounds=100,\n                  verbose=False)\n#         print(\"fitted\")\n        preds += (model.predict_proba(X_test))[:,1] / splits\n#         print(preds.shape)\n#         print(\"preds ok\")\n#         model_fi += model.feature_importances_\n#         print(\"model_fi ok\")\n        oof_preds[valid_indicies] =  model.predict_proba(X_valid)[:,1]\n        # print(oof_preds)\n        oof_preds[oof_preds < 0] = 0\n#         fold_rmse = np.sqrt(mean_squared_error(y_scaler.inverse_transform(np.array(y_valid).reshape(-1,1)), y_scaler.inverse_transform(np.array(oof_preds[valid_idx]).reshape(-1,1))))\n#         fold_rmse = np.sqrt(mean_squared_error(y_valid, oof_preds[valid_indicies]))\n        fold_roc_auc_score = roc_auc_score(y_valid, oof_preds[valid_indicies])\n\n        print(f\"\\nScorinf fold num. {fold+1} of {splits}: ROC AUC Score = {fold_roc_auc_score}\")\n\n#         print(f\"Fold {fold} RMSE: {fold_rmse}\")\n#         total_mean_rmse += fold_rmse / splits\n        total_mean_roc_auc_score += fold_roc_auc_score / splits\n    print(f\"\\nOverall mean Roc AUC Score: {total_mean_roc_auc_score}\")\n     \n    predictions[\"claim\"] = preds\n    csv_name = \"subm_\"+name+\".csv\"\n    predictions.to_csv(csv_name, index=False, header=predictions.columns)\n    \n    \n    return total_mean_roc_auc_score","metadata":{"execution":{"iopub.status.busy":"2021-09-26T14:28:41.45795Z","iopub.execute_input":"2021-09-26T14:28:41.458363Z","iopub.status.idle":"2021-09-26T14:28:41.474337Z","shell.execute_reply.started":"2021-09-26T14:28:41.45833Z","shell.execute_reply":"2021-09-26T14:28:41.470704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[back to top](#table-of-contents)\n<a id=\"2.4\"></a>\n## 2.4 - Data Import\n\nDue to the size of the DataSet I choose DataTable to create the DataFrame\n","metadata":{}},{"cell_type":"code","source":"import datatable as dt  # pip install datatable\n# Read the data\ntrain = dt.fread(\"../input/tabular-playground-series-sep-2021/train.csv\").to_pandas().set_index(\"id\")\ntest = dt.fread(\"../input/tabular-playground-series-sep-2021/test.csv\").to_pandas().set_index(\"id\")\ntrain = reduce_memory_usage(train, verbose=True)\ntest = reduce_memory_usage(test, verbose=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-26T14:28:44.937281Z","iopub.execute_input":"2021-09-26T14:28:44.937749Z","iopub.status.idle":"2021-09-26T14:29:32.804421Z","shell.execute_reply.started":"2021-09-26T14:28:44.937715Z","shell.execute_reply":"2021-09-26T14:29:32.802292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[back to top](#table-of-contents)\n<a id=\"3\"></a>\n# 3 - NA values in train and test\nLet's check how many NA values there are in train and test dataframe","metadata":{"execution":{"iopub.status.busy":"2021-09-01T12:43:35.04385Z","iopub.execute_input":"2021-09-01T12:43:35.044253Z","iopub.status.idle":"2021-09-01T12:43:35.048912Z","shell.execute_reply.started":"2021-09-01T12:43:35.044217Z","shell.execute_reply":"2021-09-01T12:43:35.047441Z"}}},{"cell_type":"code","source":"print(\"(train, test) na --> \",(train.isna().sum().sum(), test.isna().sum().sum()))","metadata":{"execution":{"iopub.status.busy":"2021-09-26T05:52:34.681338Z","iopub.execute_input":"2021-09-26T05:52:34.681702Z","iopub.status.idle":"2021-09-26T05:52:35.366923Z","shell.execute_reply.started":"2021-09-26T05:52:34.681665Z","shell.execute_reply":"2021-09-26T05:52:35.366032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Quite a lot Na Values to handle. We need to handle all the NA. \nWe have studied them in the EDA session: https://www.kaggle.com/sgiuri/sep21tp-eda-na-handle-xgbc\n","metadata":{}},{"cell_type":"markdown","source":"[back to top](#table-of-contents)\n<a id=\"4\"></a>\n\n## 4 - Feature Engeneer with NA Values\nAfter we have imputed the NA we will miss some information: We no longer know where the NA-values were and how many for each row. The following code will avoid at least the count.","metadata":{}},{"cell_type":"code","source":"print(train.shape, test.shape)\ntrain[\"nNA\"] = train.isna().sum(axis = 1)\ntest[\"nNA\"] = test.isna().sum(axis = 1)\nprint(train.shape, test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-09-26T14:43:44.017958Z","iopub.execute_input":"2021-09-26T14:43:44.018427Z","iopub.status.idle":"2021-09-26T14:43:44.803436Z","shell.execute_reply.started":"2021-09-26T14:43:44.01836Z","shell.execute_reply":"2021-09-26T14:43:44.802113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[back to top](#table-of-contents)\n<a id=\"5\"></a>\n\n# 5 - Filling NA value strategies\nThere are some ML alghorithms that doesn't support the presence of NA values in dataframe. \nI'd like to verify the efficiency of several methods applying a barely tuned XGBC:\n\n* [No NA handling (XGBC can handle a DF with NA)](#5.1)\n* [Filling all NA with zeros](#5.2)\n* [Filling all NA wth the mean value ](#5.3)\n* [Filling all NA with the median value for each features](#5.4)\n* [CINI ML training algorithm to search an appropriate feature value on a sample of datas without NA](#5.5)\n* [CINI ML training algorithm to search an appropriate feature value on a sample of datas ](#5.6)\n* [CINI ML training algorithm to search an appropriate feature value on the whole dataset](#5.7)\n* [Mean, Median, Mode](#5.8)","metadata":{}},{"cell_type":"code","source":"# I create a dictionary to store training results\nresults = {}","metadata":{"execution":{"iopub.status.busy":"2021-09-26T14:43:52.292301Z","iopub.execute_input":"2021-09-26T14:43:52.292721Z","iopub.status.idle":"2021-09-26T14:43:52.297252Z","shell.execute_reply.started":"2021-09-26T14:43:52.29269Z","shell.execute_reply":"2021-09-26T14:43:52.295993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[back to top](#table-of-contents)\n<a id=\"5.1\"></a>\n## 5.1 - No NA handling","metadata":{}},{"cell_type":"code","source":"%%time\n\nX, y, X_test = min_max_scaler(train, test)\nprint(\"(X, X_test) na --> \",(X.isna().sum().sum(), X_test.isna().sum().sum()))\n\nresults[\"no_NA_handling\"] = xgb_train(\"no_NA_handling\", X, y, X_test)","metadata":{"execution":{"iopub.status.busy":"2021-09-25T12:01:29.787197Z","iopub.execute_input":"2021-09-25T12:01:29.787523Z","iopub.status.idle":"2021-09-25T12:07:21.079658Z","shell.execute_reply.started":"2021-09-25T12:01:29.787488Z","shell.execute_reply":"2021-09-25T12:07:21.078771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[back to top](#table-of-contents)\n<a id=\"5.2\"></a>\n## 5.2 - NA Values Imputed to zeros","metadata":{}},{"cell_type":"markdown","source":"We will use sklearn \"Simple Imputer\".\nWe will fit the SimpleImputer only in the Train Set. We will aplly it to both Train and Test set to avoid \nhttps://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html","metadata":{}},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer","metadata":{"execution":{"iopub.status.busy":"2021-09-26T14:55:59.737928Z","iopub.execute_input":"2021-09-26T14:55:59.738301Z","iopub.status.idle":"2021-09-26T14:55:59.743617Z","shell.execute_reply.started":"2021-09-26T14:55:59.738259Z","shell.execute_reply":"2021-09-26T14:55:59.742092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-25T12:22:25.262625Z","iopub.execute_input":"2021-09-25T12:22:25.262976Z","iopub.status.idle":"2021-09-25T12:22:25.324125Z","shell.execute_reply.started":"2021-09-25T12:22:25.262942Z","shell.execute_reply":"2021-09-25T12:22:25.323173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nX, y, X_test = min_max_scaler(train, test)\nprint(\"(train, test) na --> \",(X.isna().sum().sum(), X_test.isna().sum().sum()))\n\nimputer = SimpleImputer(strategy=\"constant\", fill_value = 0)\nX = pd.DataFrame(imputer.fit_transform(X),\n                 columns=X.columns,\n                index = X.index)\nX_test = pd.DataFrame(imputer.transform(test), \n                      columns=X_test.columns, \n                      index = X_test.index)\n\nprint(\"(train, test) na --> \",(X.isna().sum().sum(), X_test.isna().sum().sum()))\nresults[\"NA_to_0\"] = xgb_train(\"NA_to_0\", X, y, X_test)","metadata":{"execution":{"iopub.status.busy":"2021-09-26T14:56:05.145525Z","iopub.execute_input":"2021-09-26T14:56:05.145902Z","iopub.status.idle":"2021-09-26T15:01:16.046121Z","shell.execute_reply.started":"2021-09-26T14:56:05.145872Z","shell.execute_reply":"2021-09-26T15:01:16.044963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[back to top](#table-of-contents)\n<a id=\"5.3\"></a>\n## 5.3 - NA Values Imputed to mean","metadata":{}},{"cell_type":"code","source":"%%time\nX, y, X_test = min_max_scaler(train, test)\nprint(\"(train, test) na --> \",(X.isna().sum().sum(), X_test.isna().sum().sum()))\n\nimputer = SimpleImputer(strategy=\"mean\")\nX = pd.DataFrame(imputer.fit_transform(X),\n                 columns=X.columns,\n                index = X.index)\nX_test = pd.DataFrame(imputer.transform(test), \n                      columns=X_test.columns, \n                      index = X_test.index)\n\nprint(\"(train, test) na --> \",(X.isna().sum().sum(), X_test.isna().sum().sum()))\nresults[\"NA_to_mean\"] = xgb_train(\"NA_to_mean\", X, y, X_test)\n","metadata":{"execution":{"iopub.status.busy":"2021-09-24T20:24:27.80476Z","iopub.execute_input":"2021-09-24T20:24:27.805168Z","iopub.status.idle":"2021-09-24T20:29:20.350168Z","shell.execute_reply.started":"2021-09-24T20:24:27.805131Z","shell.execute_reply":"2021-09-24T20:29:20.34862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[back to top](#table-of-contents)\n<a id=\"5.4\"></a>\n## 5.4 - NA Values Imputed to median","metadata":{}},{"cell_type":"code","source":"%%time\nX, y, X_test = min_max_scaler(train, test)\nprint(\"(train, test) na --> \",(X.isna().sum().sum(), X_test.isna().sum().sum()))\n\nimputer = SimpleImputer(strategy=\"median\")\nX = pd.DataFrame(imputer.fit_transform(X),\n                 columns=X.columns, \n                 index = X.index)\n\nX_test = pd.DataFrame(imputer.transform(test), \n                      columns=X_test.columns, \n                      index = X_test.index)\n\nprint(\"(train, test) na --> \",(X.isna().sum().sum(), X_test.isna().sum().sum()))\nresults[\"NA_to_median\"] = xgb_train(\"NA_to_median\", X, y, X_test)\n","metadata":{"execution":{"iopub.status.busy":"2021-09-24T20:29:20.35174Z","iopub.execute_input":"2021-09-24T20:29:20.352103Z","iopub.status.idle":"2021-09-24T20:34:22.476084Z","shell.execute_reply.started":"2021-09-24T20:29:20.352067Z","shell.execute_reply":"2021-09-24T20:34:22.475088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[back to top](#table-of-contents)\n<a id=\"5.5\"></a>\n## 5.5 - MICE with a dropNa training set\n\nApplying the multivariate feature imputation by chained equations (MICE) with a drop_NA training set","metadata":{}},{"cell_type":"code","source":"from sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n#train_mice = train.copy(deep=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-26T14:29:32.806187Z","iopub.execute_input":"2021-09-26T14:29:32.806606Z","iopub.status.idle":"2021-09-26T14:29:32.950815Z","shell.execute_reply.started":"2021-09-26T14:29:32.806564Z","shell.execute_reply":"2021-09-26T14:29:32.949797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nX, y, X_test = min_max_scaler(train, test)\nmice_imputer = IterativeImputer()\nmice_train_set = X.dropna()\n\nprint(\"The mice_train_set shape is: \", mice_train_set.shape)\nprint(\"mice_train_set na values =\", mice_train_set.isna().sum().sum())\n\nmice_imputer.fit(mice_train_set)\nX = pd.DataFrame(mice_imputer.transform(X),\n                 columns=X.columns,\n                 index = X.index)\nX_test = pd.DataFrame(mice_imputer.transform(X_test),\n                 columns=X_test.columns,\n                     index = X_test.index)\n\nprint(\"(train, test) na --> \",(X.isna().sum().sum(), X_test.isna().sum().sum()))\n\nresults[\"NA_to_MICE_naDrop\"] = xgb_train(\"NA_to_MICE_naDrop\", X, y, X_test)","metadata":{"execution":{"iopub.status.busy":"2021-09-24T20:20:39.381114Z","iopub.execute_input":"2021-09-24T20:20:39.381443Z","iopub.status.idle":"2021-09-24T20:20:43.515251Z","shell.execute_reply.started":"2021-09-24T20:20:39.381407Z","shell.execute_reply":"2021-09-24T20:20:43.514306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[back to top](#table-of-contents)\n<a id=\"5.6\"></a>\n## 5.6 - MICE with a sampled training set\nIn this test I use a sample training set o same shape of previous one.","metadata":{}},{"cell_type":"code","source":"%%time\n\nX, y, X_test = min_max_scaler(train, test)\n\nmice_imputer = IterativeImputer()\n\nmice_train_set = X.sample(X.dropna().shape[0])\nprint(\"The mice_train_set shape is: \", mice_train_set.shape)\nprint(\"mice_train_set na values =\", mice_train_set.isna().sum().sum())\n\nmice_imputer.fit(mice_train_set)\nX = pd.DataFrame(mice_imputer.transform(X),\n                 columns=X.columns,\n                index = X.index)\nX_test = pd.DataFrame(mice_imputer.transform(X_test),\n                 columns=X_test.columns,\n                     index = X_test.index)\n\n\n\nprint(\"(train, test) na --> \",(X.isna().sum().sum(), X_test.isna().sum().sum()))\nresults[\"NA_to_MICE_sampled\"] = xgb_train(\"NA_to_MICE_sampled\", X, y, X_test)","metadata":{"execution":{"iopub.status.busy":"2021-09-26T05:53:46.254286Z","iopub.execute_input":"2021-09-26T05:53:46.254652Z","iopub.status.idle":"2021-09-26T06:24:18.642006Z","shell.execute_reply.started":"2021-09-26T05:53:46.254621Z","shell.execute_reply":"2021-09-26T06:24:18.641244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"(train, test) na --> \",(X.isna().sum().sum(), X_test.isna().sum().sum()))","metadata":{"execution":{"iopub.status.busy":"2021-09-26T06:29:25.320346Z","iopub.execute_input":"2021-09-26T06:29:25.320712Z","iopub.status.idle":"2021-09-26T06:29:25.635839Z","shell.execute_reply.started":"2021-09-26T06:29:25.320681Z","shell.execute_reply":"2021-09-26T06:29:25.634935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[back to top](#table-of-contents)\n<a id=\"5.7\"></a>\n## 5.7 - MICE with the whole training set\nIn this test I'll use the whole training set to calculate the MICE Na Value","metadata":{}},{"cell_type":"code","source":"%%time\n\nX, y, X_test = min_max_scaler(train, test)\n\nmice_imputer = IterativeImputer()\n\nmice_train_set = X.copy(deep = True)\nprint(\"The mice_train_set shape is: \", mice_train_set.shape)\nprint(\"mice_train_set na values =\", mice_train_set.isna().sum().sum())\n\nmice_imputer.fit(mice_train_set)\nX = pd.DataFrame(mice_imputer.transform(X),\n                 columns=X.columns,\n                index = X.index)\nX_test = pd.DataFrame(mice_imputer.transform(X_test),\n                 columns=X_test.columns\n                     ,index = X_test.index)\n\nprint(\"(train, test) na --> \",(X.isna().sum().sum(), X_test.isna().sum().sum()))\nresults[\"NA_to_MICE\"] = xgb_train(\"NA_to_MICE\", X, y, X_test)","metadata":{"execution":{"iopub.status.busy":"2021-09-25T08:05:07.524032Z","iopub.execute_input":"2021-09-25T08:05:07.524385Z","iopub.status.idle":"2021-09-25T08:07:19.074204Z","shell.execute_reply.started":"2021-09-25T08:05:07.524333Z","shell.execute_reply":"2021-09-25T08:07:19.07341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"5.8 - ","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"[back to top](#table-of-contents)\n<a id=\"5.8\"></a>\n## 5.8 - Mean, Median, Mode\n\nIdea taken from www.kaggle.com/dlaststark/tps-sep-single-xgboost-model, copied from: https://www.kaggle.com/realtimshady/single-simple-lightgbm\n\nMean: normal distribution\nMedian: unimodal and skewed\nMode: all other cases","metadata":{}},{"cell_type":"code","source":"%%time\n\nX, y, X_test = min_max_scaler(train, test)\nfrom tqdm import tqdm\nfeatures = [x for x in X.columns.values if x[0]==\"f\"]\n\nfill_value_dict = {\n    'f1': 'Mean', \n    'f2': 'Median', \n    'f3': 'Median', \n    'f4': 'Median', \n    'f5': 'Mode', \n    'f6': 'Mean', \n    'f7': 'Median', \n    'f8': 'Median', \n    'f9': 'Median', \n    'f10': 'Median', \n    'f11': 'Mean', \n    'f12': 'Median', \n    'f13': 'Mean', \n    'f14': 'Median', \n    'f15': 'Mean', \n    'f16': 'Median', \n    'f17': 'Median', \n    'f18': 'Median', \n    'f19': 'Median', \n    'f20': 'Median', \n    'f21': 'Median', \n    'f22': 'Mean', \n    'f23': 'Mode', \n    'f24': 'Median', \n    'f25': 'Median', \n    'f26': 'Median', \n    'f27': 'Median', \n    'f28': 'Median', \n    'f29': 'Mode', \n    'f30': 'Median', \n    'f31': 'Median', \n    'f32': 'Median', \n    'f33': 'Median', \n    'f34': 'Mean', \n    'f35': 'Median', \n    'f36': 'Mean', \n    'f37': 'Median', \n    'f38': 'Median', \n    'f39': 'Median', \n    'f40': 'Mode', \n    'f41': 'Median', \n    'f42': 'Mode', \n    'f43': 'Mean', \n    'f44': 'Median', \n    'f45': 'Median', \n    'f46': 'Mean', \n    'f47': 'Mode', \n    'f48': 'Mean', \n    'f49': 'Mode', \n    'f50': 'Mode', \n    'f51': 'Median', \n    'f52': 'Median', \n    'f53': 'Median', \n    'f54': 'Mean', \n    'f55': 'Mean', \n    'f56': 'Mode', \n    'f57': 'Mean', \n    'f58': 'Median', \n    'f59': 'Median', \n    'f60': 'Median', \n    'f61': 'Median', \n    'f62': 'Median', \n    'f63': 'Median', \n    'f64': 'Median', \n    'f65': 'Mode', \n    'f66': 'Median', \n    'f67': 'Median', \n    'f68': 'Median', \n    'f69': 'Mean', \n    'f70': 'Mode', \n    'f71': 'Median', \n    'f72': 'Median', \n    'f73': 'Median', \n    'f74': 'Mode', \n    'f75': 'Mode', \n    'f76': 'Mean', \n    'f77': 'Mode', \n    'f78': 'Median', \n    'f79': 'Mean', \n    'f80': 'Median', \n    'f81': 'Mode', \n    'f82': 'Median', \n    'f83': 'Mode', \n    'f84': 'Median', \n    'f85': 'Median', \n    'f86': 'Median', \n    'f87': 'Median', \n    'f88': 'Median', \n    'f89': 'Median', \n    'f90': 'Mean', \n    'f91': 'Mode', \n    'f92': 'Median', \n    'f93': 'Median', \n    'f94': 'Median', \n    'f95': 'Median', \n    'f96': 'Median', \n    'f97': 'Mean', \n    'f98': 'Median', \n    'f99': 'Median', \n    'f100': 'Mode', \n    'f101': 'Median', \n    'f102': 'Median', \n    'f103': 'Median', \n    'f104': 'Median', \n    'f105': 'Median', \n    'f106': 'Median', \n    'f107': 'Median', \n    'f108': 'Median', \n    'f109': 'Mode', \n    'f110': 'Median', \n    'f111': 'Median', \n    'f112': 'Median', \n    'f113': 'Mean', \n    'f114': 'Median', \n    'f115': 'Median', \n    'f116': 'Mode', \n    'f117': 'Median', \n    'f118': 'Mean'\n}\n\n\nprint(\"(train, test) na --> \",(X.isna().sum().sum(), X_test.isna().sum().sum()))\n\nfor col in tqdm(features):\n    if fill_value_dict.get(col)=='Mean':\n        fill_value = X[col].mean()\n    elif fill_value_dict.get(col)=='Median':\n        fill_value = X[col].median()\n    elif fill_value_dict.get(col)=='Mode':\n        fill_value = X[col].mode().iloc[0]\n    \n    X[col].fillna(fill_value, inplace=True)\n    X_test[col].fillna(fill_value, inplace=True)\n\nprint(\"(train, test) na --> \",(X.isna().sum().sum(), X_test.isna().sum().sum()))\nresults[\"NA_to_MMM\"] = xgb_train(\"NA_to_MMM\", X, y, X_test)","metadata":{"execution":{"iopub.status.busy":"2021-09-26T14:45:14.940043Z","iopub.execute_input":"2021-09-26T14:45:14.940403Z","iopub.status.idle":"2021-09-26T14:49:50.961462Z","shell.execute_reply.started":"2021-09-26T14:45:14.940343Z","shell.execute_reply":"2021-09-26T14:49:50.960407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()\n","metadata":{"execution":{"iopub.status.busy":"2021-09-26T14:44:54.445368Z","iopub.execute_input":"2021-09-26T14:44:54.445797Z","iopub.status.idle":"2021-09-26T14:44:54.536324Z","shell.execute_reply.started":"2021-09-26T14:44:54.445767Z","shell.execute_reply":"2021-09-26T14:44:54.534858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"[back to top](#table-of-contents)\n<a id=\"6\"></a>\n# 6 - Results and Considerations:\nTime and pubblic results are based on version 10 of this notebook: \nThis are the results of the simulation :<br>","metadata":{}},{"cell_type":"code","source":"results_df = pd.DataFrame(list(results.items()),columns = ['Strategy','ROC AUC'], index = [(51+x)/10 for x in (range(len(results))) ] )\nresults_df[\"Wall Time\"] = [\"5min 5s\", \"4min 35s\", \"4min 43s\", \"4min 51s\", \"14min 46s\", \"35min 20s\", \"1h 24min 28s\", \"4m 10s\" ]\nresults_df[\"Pubblic Score\"] = [0.81722, 0.78181, 0.78671, 0.78640, 0.81743, 0.81747, 0.81727, 0.81730]\nprint(results_df.sort_values(by = ['ROC AUC'], ascending=False))","metadata":{"execution":{"iopub.status.busy":"2021-09-25T12:21:05.036378Z","iopub.execute_input":"2021-09-25T12:21:05.036702Z","iopub.status.idle":"2021-09-25T12:21:05.072681Z","shell.execute_reply.started":"2021-09-25T12:21:05.03667Z","shell.execute_reply":"2021-09-25T12:21:05.0707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[back to top](#table-of-contents)\n<a id=\"7\"></a>\n# 7 - Comments\nTHe best strategy looks like to be the CINI - MICE trained on a sample dataset. \nThis Imputer is really slower, but aim to better results.<br>\nI can't understand the very slow results of Pubblic Score of Simple Imputer strategies.. More studies soon.\n\n\n","metadata":{}}]}