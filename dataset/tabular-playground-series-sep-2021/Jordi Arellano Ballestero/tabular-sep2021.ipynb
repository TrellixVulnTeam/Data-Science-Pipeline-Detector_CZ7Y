{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os, gc, warnings\nimport random\nimport datetime\nfrom tqdm.notebook import tqdm\n\nfrom scipy import stats\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# from pandas.plotting import register_matplotlib_converters\n# register_matplotlib_converters()\n\nimport sklearn\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n\nimport optuna\nimport lightgbm as lgb\nfrom optuna.integration import LightGBMPruningCallback","metadata":{"execution":{"iopub.status.busy":"2021-09-13T14:24:22.791181Z","iopub.execute_input":"2021-09-13T14:24:22.791735Z","iopub.status.idle":"2021-09-13T14:24:25.959432Z","shell.execute_reply.started":"2021-09-13T14:24:22.791621Z","shell.execute_reply":"2021-09-13T14:24:25.958689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = '../input/tabular-playground-series-sep-2021/'\n# Input data files are available in the \"../input/\" directory.\nfor dirname, _, filenames in os.walk(path):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2021-09-13T14:23:46.451274Z","iopub.execute_input":"2021-09-13T14:23:46.451641Z","iopub.status.idle":"2021-09-13T14:23:46.462044Z","shell.execute_reply.started":"2021-09-13T14:23:46.451604Z","shell.execute_reply":"2021-09-13T14:23:46.461108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_data(source, dtypes, path=path):\n    ''' load tables '''\n    assert source in ['train', 'test']\n    df = pd.read_csv(f'{path}/{source}.csv', index_col=\"id\", dtype= dtypes)\n    return df","metadata":{"tags":[],"execution":{"iopub.status.busy":"2021-09-13T14:23:50.022065Z","iopub.execute_input":"2021-09-13T14:23:50.022588Z","iopub.status.idle":"2021-09-13T14:23:50.027236Z","shell.execute_reply.started":"2021-09-13T14:23:50.02254Z","shell.execute_reply":"2021-09-13T14:23:50.026629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrain = load_data('train', None)\nprint(f\"Data shape: {train.shape}\")\ntrain.sample(4)","metadata":{"execution":{"iopub.status.busy":"2021-09-13T14:23:50.160399Z","iopub.execute_input":"2021-09-13T14:23:50.161027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntest = load_data('test', None)\nprint(f\"Data shape: {test.shape}\")\ntest.sample(2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_name = \"claim\"\nfeatures = [col for col in train.columns if col not in [target_name]]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Some initial analize","metadata":{}},{"cell_type":"markdown","source":"## Check if in the data set there is missing data.","metadata":{}},{"cell_type":"code","source":"def missing_statistics(df):    \n    statitics = pd.DataFrame(df.isnull().sum()).reset_index()\n    statitics.columns=['COLUMN NAME',\"MISSING VALUES\"]\n    statitics['TOTAL ROWS'] = df.shape[0]\n    statitics['% MISSING'] = round((statitics['MISSING VALUES']/statitics['TOTAL ROWS'])*100,2)\n    return statitics","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"miss = missing_statistics(train)\nmiss","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"miss[\"% MISSING\"].describe()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del miss","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Discrete features?\n* **All the features are decimal, no categoraical input parameters.**","metadata":{}},{"cell_type":"markdown","source":"## Distribution Check.","metadata":{}},{"cell_type":"code","source":"train.describe().T","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Important step\n* To create in cross validation a proper missing data distribution check \"n_missing\" dataset implementation.","metadata":{}},{"cell_type":"code","source":"train[\"std\"] = train[features].std(axis=1)\ntest[\"std\"]  = test[features].std(axis=1)\ntrain[\"n_missing\"] = train[features].isna().sum(axis=1)\ntest[\"n_missing\"]  = test[features].isna().sum(axis=1)\nfeatures += ['std', 'n_missing']\nn_missing = train[\"n_missing\"].copy()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Filling missign data with mean\n* An mean of 1.597059% data is missing in each input column of this data set\n* Due to I have now context about each row and there is less than 2& missing data, I decided to use the mean value of the column to fill each NaN gap.","metadata":{}},{"cell_type":"code","source":"train[features] = train[features].fillna(train[features].mean())\ntest[features]  = test[features].fillna(test[features].mean())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature scalin","metadata":{}},{"cell_type":"code","source":"scaler = RobustScaler()\n\ntrain[features] = scaler.fit_transform(train[features])\ntest[features]  = scaler.transform(test[features])","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fisher asymmetry\n* If the value is close to 0, it means: normal distribution\n* If it is more positive: left skeewed distribution\n* If it is more negative: rigth skeewed distribution","metadata":{}},{"cell_type":"code","source":"stats.skew([1,2,3,4,5])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Kurtosis\n* If near 0: the distribuiton is cole to a normal one.\n* If it is positive, the values are really proxim to the central value, the data has no big tails.\n* If it is negative, less values centread in mean and big tails","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(11,11,figsize=(16, 16))\naxes = axes.flatten()\n\nfor idx, ax in tqdm(enumerate(axes)):\n    try:\n        idx += 1\n        values = train[f\"f{idx}\"].values\n        sns.kdeplot(data=train, x=f'f{idx}', \n                    fill=True, \n                    ax=ax)\n        sns.kdeplot(data=test, x=f'f{idx}', \n                    fill=True, \n                    ax=ax)\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.set_xlabel(f'skew:{round(stats.skew(values), 2)}, kurt:{round(stats.kurtosis(values),2)}')\n        ax.set_ylabel('')\n        ax.spines['left'].set_visible(False)\n        ax.set_title(f'f{idx}', loc='right', weight='bold', fontsize=10)\n    except Exception as e:\n        print(e)\n\nfig.supxlabel('Average by class (by feature)', ha='center', fontweight='bold')\n\nfig.tight_layout()\nplt.show()","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Determine wich columns are skewed\n* If stats.skew is higher than 1 is skewed data with right tail.\n* If stats.skew is less than -1 is skewed data with left tail.","metadata":{}},{"cell_type":"code","source":"def determine_skewed_columns(df, skew_top_threshold, skew_low_threshold):\n    col_names = df.columns[:-1]\n    skew = stats.skew(df.values)[:-1]\n    mask = (skew >= skew_top_threshold) | (skew <= skew_low_threshold)\n    \n    return col_names[mask]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"skew_columns_train = determine_skewed_columns(train, 1, -1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"skew_columns_test = determine_skewed_columns(test, 1, -1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list(set(skew_columns_train) - set(skew_columns_test))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"skew_columns = skew_columns_test","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Confusion matrix","metadata":{"tags":[]}},{"cell_type":"code","source":"%%time\nfig, ax = plt.subplots(1, 1, figsize=(12 , 12))\n\ncorr = train.corr()\n\nmask = np.zeros_like(corr, dtype=np.bool_)\nmask[np.triu_indices_from(mask)] = True\n\nsns.heatmap(corr, ax=ax,\n        square=True, center=0, linewidth=1,\n        cmap=sns.diverging_palette(240, 10, as_cmap=True),\n        cbar_kws={\"shrink\": .82},    \n        mask=mask\n       ) \n\nax.set_title(f'Correlation', loc='left', fontweight='bold')     \n\nplt.show()","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N_SPLITS = 5\nN_ESTIMATORS = 10000\nEARLY_STOPING_ROUND = 200\nVERBOSE = 1000\nSEED = 2021\n\nN_BINS = 20","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_params = {\n    'objective': 'binary',\n    'n_estimators' : N_ESTIMATORS,\n    'random_state' : SEED,\n    'learning_rate': 0.030305148136078583,\n    'subsample'    : 0.5150617351169511,\n    'reg_alpha'    : 0.2491671010019858,\n    'reg_labmda'   : 0.03618390402626644,\n    'subsample_freq': 1,\n    'colsample_bytree': 0.3917166178297055,\n    'min_child_weight': 2,\n    'min_child_sample': 48,\n    'max_depth': 7,\n}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LGBMRegressor model prepare","metadata":{"tags":[]}},{"cell_type":"code","source":"def fit_regressor(df, tr_idx, val_idx, features_arr, target_str, params):\n    # train\n    tr_x, tr_y = df[features_arr].iloc[tr_idx], df[target_str][tr_idx]\n    # evaluating (\"test\")\n    vl_x, vl_y = df[features_arr].iloc[val_idx], df[target_str][val_idx]\n    print({'df size':len(tr_x), 'eval size':len(vl_x)})\n\n    clf = lgb.LGBMClassifier(**params)\n    # Metric: Root Mean Square Error (RMSE), it tells you how concentrated the data is around the line of best fit.\n    clf.fit(tr_x, tr_y,\n            eval_set=[(vl_x, vl_y)],\n            early_stopping_rounds=EARLY_STOPING_ROUND,\n            eval_metric=\"auc\",\n            verbose=VERBOSE)\n    #\"l2\"\n    return clf","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Note:\n**StratifiedKFold**: We are forcing the model to train with the missing data properly distributed in each train/test sample.","metadata":{}},{"cell_type":"code","source":"kf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED) # Provides train/test indices to split data in train/test sets.\n# kf = KFold(n_splits=folds, shuffle=True, random_state=seed) #n_splits=folds\n    \n## generating 5 train/test pair of index_arrays, and analizing wich give the better results.\nmodels = []\nfor tr_idx, val_idx in tqdm(kf.split(X=train, y=n_missing), total=N_SPLITS): # train/test indices\n    clf = fit_regressor(train, tr_idx, val_idx, features, target_name, best_params)\n    models.append(clf)\n    \ngc.collect() # trigger a manual garbage collection process, cleans up a huge amount of objects.","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation method in train data","metadata":{}},{"cell_type":"code","source":"def evaluate(valid_targets, probs, name):\n    from sklearn.metrics import classification_report, accuracy_score, log_loss, roc_auc_score\n    \n    y_pred = np.array(probs > 0.5, dtype=int)\n    acc = accuracy_score(valid_targets, y_pred)\n    loss = log_loss(valid_targets, y_pred)\n    auc = roc_auc_score(valid_targets, probs)\n    print(\"Accuracy score: %.2f\"%(acc))\n    print(\"Log loss: %.2f\"%(loss))\n    print(\"AUC score:\", auc)\n    print(\"Classification report:\")\n    print(classification_report(valid_targets, y_pred))\n    return {\n        \"name\": name, \n        \"accuracy_score\": acc, \n        \"log_loss\": loss, \n        \"auc\": auc\n    }","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"probs = [model.predict_proba(train[features]) for model in models]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.shape(probs)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"probs = np.mean(probs, axis=0)\nprobs = probs.T[1]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate(train[target_name], probs, \"LGBMClassifier\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Algorithm ID3 regresion\n1. Calculate the initial system entropy based on the **objective** variable to predict.\n    * Entropy: Determine wich parameters are more important than others to have a better sort in the tree.","metadata":{}},{"cell_type":"code","source":"_ = lgb.plot_importance(models[0], importance_type='split', figsize=(20,20)) , #\"gain\"","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Check in prediction sample","metadata":{}},{"cell_type":"code","source":"# preds = [model.predict(test_update) for model in models]\ntest_probs = [model.predict_proba(test[features]) for model in models]\nout_loss = np.mean(test_probs, axis=0) # Using all the models and making the mean between each other.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Save submision","metadata":{"tags":[]}},{"cell_type":"code","source":"submission = pd.read_csv(f'{path}/sample_solution.csv')\nsubmission['claim'] = out_loss.T[1]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv(f'./submission.csv', index=False)\nsubmission.head(9)","metadata":{"tags":[]},"execution_count":null,"outputs":[]}]}