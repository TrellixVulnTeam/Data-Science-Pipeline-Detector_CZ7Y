{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-27T02:45:40.30989Z","iopub.execute_input":"2021-09-27T02:45:40.310277Z","iopub.status.idle":"2021-09-27T02:45:40.405764Z","shell.execute_reply.started":"2021-09-27T02:45:40.310163Z","shell.execute_reply":"2021-09-27T02:45:40.404954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/tabular-playground-series-sep-2021/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/tabular-playground-series-sep-2021/test.csv\")\nsubmission = pd.read_csv(\"/kaggle/input/tabular-playground-series-sep-2021/sample_solution.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-09-27T02:45:40.407391Z","iopub.execute_input":"2021-09-27T02:45:40.407706Z","iopub.status.idle":"2021-09-27T02:46:20.72907Z","shell.execute_reply.started":"2021-09-27T02:45:40.407672Z","shell.execute_reply":"2021-09-27T02:46:20.728331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-27T02:46:20.730495Z","iopub.execute_input":"2021-09-27T02:46:20.730744Z","iopub.status.idle":"2021-09-27T02:46:20.77572Z","shell.execute_reply.started":"2021-09-27T02:46:20.730714Z","shell.execute_reply":"2021-09-27T02:46:20.775026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-27T02:46:20.776804Z","iopub.execute_input":"2021-09-27T02:46:20.777089Z","iopub.status.idle":"2021-09-27T02:46:20.808775Z","shell.execute_reply.started":"2021-09-27T02:46:20.777051Z","shell.execute_reply":"2021-09-27T02:46:20.807932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def check(df):\n    col_list = train.columns.values\n    rows = []\n    for col in col_list:\n        tmp = (col,\n              train[col].dtype,\n              train[col].isnull().sum(),\n              train[col].count(),\n              train[col].nunique(),\n              train[col].unique())\n        rows.append(tmp)\n    df = pd.DataFrame(rows) \n    df.columns = ['feature','dtype','nan','count','nunique','unique']\n    return df\n\ncheck(train)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-27T02:46:20.811428Z","iopub.execute_input":"2021-09-27T02:46:20.811714Z","iopub.status.idle":"2021-09-27T02:46:26.986731Z","shell.execute_reply.started":"2021-09-27T02:46:20.811681Z","shell.execute_reply":"2021-09-27T02:46:26.986059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def check(df):\n    col_list = test.columns.values\n    rows = []\n    for col in col_list:\n        tmp = (col,\n              test[col].dtype,\n              test[col].isnull().sum(),\n              test[col].count(),\n              test[col].nunique(),\n              test[col].unique())\n        rows.append(tmp)\n    df = pd.DataFrame(rows) \n    df.columns = ['feature','dtype','nan','count','nunique','unique']\n    return df\n\ncheck(test)","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.claim.hist()","metadata":{"execution":{"iopub.status.busy":"2021-09-27T02:46:33.363094Z","iopub.execute_input":"2021-09-27T02:46:33.363367Z","iopub.status.idle":"2021-09-27T02:46:33.631125Z","shell.execute_reply.started":"2021-09-27T02:46:33.363335Z","shell.execute_reply":"2021-09-27T02:46:33.630398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_cell = np.product(train.shape)\nmissing_values_count = train.isnull().sum()\ntotal_missing = missing_values_count.sum()\npercent_missing = (total_missing / total_cell)* 100\n\nprint(percent_missing, \" % missing\")","metadata":{"execution":{"iopub.status.busy":"2021-09-27T02:46:33.63246Z","iopub.execute_input":"2021-09-27T02:46:33.632716Z","iopub.status.idle":"2021-09-27T02:46:33.834851Z","shell.execute_reply.started":"2021-09-27T02:46:33.632684Z","shell.execute_reply":"2021-09-27T02:46:33.834164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"--------------------","metadata":{}},{"cell_type":"markdown","source":"* I tried SimpleImputing, StandardScalaer, RobustScaler, QuantileTransform, and pipeline.\n* StandardScaler is better than other preprocessing.\n* add the train's colums \"Std\", \"mean\", \"median\", \"kurt\" is much better.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n\nfrom lightgbm import LGBMRegressor\nimport lightgbm as lgb","metadata":{"execution":{"iopub.status.busy":"2021-09-27T02:46:33.836037Z","iopub.execute_input":"2021-09-27T02:46:33.836583Z","iopub.status.idle":"2021-09-27T02:46:36.77642Z","shell.execute_reply.started":"2021-09-27T02:46:33.836544Z","shell.execute_reply":"2021-09-27T02:46:36.775699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import KFold\n\ntrain[\"kfold\"] = -1\nkf = KFold(n_splits = 10, shuffle=True, random_state = 0)\nfor fold,(train_index, valid_index) in enumerate(kf.split(X = train)):\n    print(fold,train_index, valid_index)\n    train.loc[valid_index, \"kfold\"] = fold\ntrain.kfold.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-09-27T02:46:36.77746Z","iopub.execute_input":"2021-09-27T02:46:36.777697Z","iopub.status.idle":"2021-09-27T02:46:36.960108Z","shell.execute_reply.started":"2021-09-27T02:46:36.777666Z","shell.execute_reply":"2021-09-27T02:46:36.959441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"10 splits is better than 5","metadata":{}},{"cell_type":"code","source":"useful = [col for col in train.columns if col not in ('id','claim','kfold')]\ntest = test[useful]","metadata":{"execution":{"iopub.status.busy":"2021-09-27T02:46:36.961243Z","iopub.execute_input":"2021-09-27T02:46:36.961641Z","iopub.status.idle":"2021-09-27T02:46:37.099515Z","shell.execute_reply.started":"2021-09-27T02:46:36.961606Z","shell.execute_reply":"2021-09-27T02:46:37.098724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Use oputna.integration.lightgbm\n* import optuna.integration.lightgbm as lgb\n* Choose the best params.(It took a lot of time to find the best params this competition)","metadata":{}},{"cell_type":"code","source":"param = {      \n        \"objective\": \"binary\",\n        \"metric\": \"auc\",\n        \"verbosity\": -1,\n        \"boosting_type\": \"gbdt\",\n        \"device\": \"gpu\",\n        \"gpu_platform_id\": 0,\n        \"gpu_device_id\": 0,\n        \"n_estimators\" : 1000,\n        \"early_stopping_rounds\" : 10,\n    \n        \"feature_fraction\" : 1.0,\n        \"num_leaves\" : 94,\n        \"bagging_fraction\": 0.6737009142690187,\n        \"bagging_freq\": 1,\n        \"lambda_l1\": 5.559056252126386, \n        \"lambda_l2\": 9.77312118560801,\n        \"min_child_samples\" : 50\n     }","metadata":{"execution":{"iopub.status.busy":"2021-09-27T02:46:37.101499Z","iopub.execute_input":"2021-09-27T02:46:37.101826Z","iopub.status.idle":"2021-09-27T02:46:37.108262Z","shell.execute_reply.started":"2021-09-27T02:46:37.101766Z","shell.execute_reply":"2021-09-27T02:46:37.107433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score = []\ntest_pred = []\nvalid_pred = {}\nfor fold in range(10):\n    \n    X_train = train[train.kfold != fold].reset_index(drop = True)\n    X_valid = train[train.kfold == fold].reset_index(drop = True)\n    \n    X_test = test.copy()\n    valid_ids = X_valid.id.values.tolist()\n    \n    y_train = X_train.claim\n    y_valid = X_valid.claim\n    \n    X_train = X_train[useful]\n    X_valid = X_valid[useful]\n    \n    feature = list(X_train.columns[1:])\n    \n    X_train['n_missing'] = X_train[feature].isna().sum(axis = 1)\n    X_train['std'] = X_train[feature].std(axis = 1)\n    X_train['mean'] = X_train[feature].mean(axis = 1)\n    X_train['median'] = X_train[feature].mean(axis = 1)\n    X_train['kurt'] = X_train[feature].kurtosis(axis = 1)\n\n    X_valid['n_missing'] = X_valid[feature].isna().sum(axis = 1)\n    X_valid['std'] = X_valid[feature].std(axis = 1)\n    X_valid['mean'] = X_valid[feature].mean(axis = 1)\n    X_valid['median'] = X_valid[feature].mean(axis = 1)\n    X_valid['kurt'] = X_valid[feature].kurtosis(axis = 1)\n\n    X_test['n_missing'] = X_test[feature].isna().sum(axis = 1)\n    X_test['std'] = X_test[feature].std(axis = 1)\n    X_test['mean'] = X_test[feature].mean(axis = 1)\n    X_test['median'] = X_train[feature].mean(axis = 1)\n    X_test['kurt'] = X_test[feature].kurtosis(axis = 1)\n    \n    feature += ['n_missing','std','mean','median','kurt']\n    X_train[feature] = X_train[feature].fillna(X_train[feature].mean())\n    X_valid[feature] = X_valid[feature].fillna(X_valid[feature].mean())\n    X_test[feature] = X_test[feature].fillna(X_test[feature].mean())\n    \n    scaler= StandardScaler()\n    X_train = pd.DataFrame(scaler.fit_transform(X_train))\n    X_valid = pd.DataFrame(scaler.transform(X_valid))\n    X_test = pd.DataFrame(scaler.transform(X_test))\n                                \n    lgb_train = lgb.Dataset(X_train, y_train)\n    lgb_valid = lgb.Dataset(X_valid, y_valid,reference = lgb.train)\n    \n    model = LGBMRegressor()\n    model = lgb.train(param,   lgb_train, valid_sets = [lgb_valid],verbose_eval = 100)\n                   \n    pred_valid = model.predict(X_valid)\n    valid_pred.update(dict(zip(valid_ids, pred_valid)))\n    test_preds = model.predict(X_test)\n    test_pred.append(test_preds)\n    \n    auc = roc_auc_score(y_valid, pred_valid)\n    print(fold,auc)\n    score.append(auc)\n    \nprint(score)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T02:47:30.686164Z","iopub.execute_input":"2021-09-27T02:47:30.686428Z","iopub.status.idle":"2021-09-27T02:58:48.450776Z","shell.execute_reply.started":"2021-09-27T02:47:30.686399Z","shell.execute_reply":"2021-09-27T02:58:48.450052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I hope this will help.","metadata":{}},{"cell_type":"code","source":"valid_prediction = pd.DataFrame.from_dict(valid_pred, orient = \"index\").reset_index()\nvalid_prediction.columns = ['id', 'pred_23']\nvalid_prediction.to_csv(\"valid_pred_23.csv\", index = False)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T03:02:00.064348Z","iopub.execute_input":"2021-09-27T03:02:00.064606Z","iopub.status.idle":"2021-09-27T03:02:03.875782Z","shell.execute_reply.started":"2021-09-27T03:02:00.06458Z","shell.execute_reply":"2021-09-27T03:02:03.874971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission['claim'] = np.mean(np.column_stack(test_pred), axis = 1)\nsubmission.to_csv(\"submission.csv\", index = False)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T03:02:03.877454Z","iopub.execute_input":"2021-09-27T03:02:03.87771Z","iopub.status.idle":"2021-09-27T03:02:05.446164Z","shell.execute_reply.started":"2021-09-27T03:02:03.877678Z","shell.execute_reply":"2021-09-27T03:02:05.445424Z"},"trusted":true},"execution_count":null,"outputs":[]}]}