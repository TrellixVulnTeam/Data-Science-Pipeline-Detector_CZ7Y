{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-06T17:50:03.435383Z","iopub.execute_input":"2021-09-06T17:50:03.43582Z","iopub.status.idle":"2021-09-06T17:50:03.445319Z","shell.execute_reply.started":"2021-09-06T17:50:03.435789Z","shell.execute_reply":"2021-09-06T17:50:03.444367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Following steps from the popular book hands-on machine learning aimed on how we use machine learning in practice.","metadata":{}},{"cell_type":"markdown","source":"**Run cell below and ignore the rest to produce preprocessed data and proceed to modelling. Do not ignore what is below the code below if you want to see why I made the choices in the code.**","metadata":{}},{"cell_type":"code","source":"# To preprocess data, run this code\n# X_train, y_train will be training input data and target labels\n# X_test will be test input data\n# y_test: store predictions here\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler  # use min max scaler (x- min)/(max - min)\n\n\npath_to_data = \"/kaggle/input/tabular-playground-series-sep-2021/\"\n\nsample_solution = pd.read_csv(path_to_data + \"sample_solution.csv\")\ntest_data = pd.read_csv(path_to_data + \"test.csv\")\ntrain_data = pd.read_csv(path_to_data + \"train.csv\")\n\n# unpack tdataset to retrieve last column of labels\nX, y = train_data.iloc[: , :train_data.shape[1] - 1], train_data.iloc[:, train_data.shape[1]-1]\nX_test = test_data\n\n#fill in missing values with median\nX = X.apply(lambda x: x.fillna(x.median()), axis=0)\n\n# scale training data\nscaler = MinMaxScaler()\nX_scaled = scaler.fit_transform(X)\nX_scaled = pd.DataFrame(X_scaled)\n\n#scale test data, independently of training data\nscaler2 = MinMaxScaler()\nX_scaled2 = scaler2.fit_transform(X_test)\nX_scaled2 = pd.DataFrame(X_scaled2)\n\nX_train = X_scaled\nX_test = X_scaled2\n\nX_train.drop(columns=[0], inplace=True)\nX_test.drop(columns=[0], inplace=True)\n#X_train.insert(0, 'id', X[X.columns[0]])\ny_train = y\n\n# do this if you want to drop the index column\n#X.drop(['id'], axis=1)\n\n# debug\n#X_train.hist(bins=50, figsize=(25,20))\n#plt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The reason my code above looks like that is all of the below.","metadata":{"execution":{"iopub.status.busy":"2021-09-06T18:31:21.798735Z","iopub.execute_input":"2021-09-06T18:31:21.799138Z","iopub.status.idle":"2021-09-06T18:31:21.805999Z","shell.execute_reply.started":"2021-09-06T18:31:21.799095Z","shell.execute_reply":"2021-09-06T18:31:21.804727Z"}}},{"cell_type":"code","source":"%matplotlib inline\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2021-09-06T17:50:05.428722Z","iopub.execute_input":"2021-09-06T17:50:05.429086Z","iopub.status.idle":"2021-09-06T17:50:05.435275Z","shell.execute_reply.started":"2021-09-06T17:50:05.429051Z","shell.execute_reply":"2021-09-06T17:50:05.434228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path_to_data = \"/kaggle/input/tabular-playground-series-sep-2021/\"\n\nsample_solution = pd.read_csv(path_to_data + \"sample_solution.csv\")\ntest_data = pd.read_csv(path_to_data + \"sample_solution.csv\")\ntrain_data = pd.read_csv(path_to_data + \"train.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-09-06T17:50:06.961188Z","iopub.execute_input":"2021-09-06T17:50:06.961562Z","iopub.status.idle":"2021-09-06T17:50:28.32952Z","shell.execute_reply.started":"2021-09-06T17:50:06.961528Z","shell.execute_reply":"2021-09-06T17:50:28.328736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-06T17:50:31.106033Z","iopub.execute_input":"2021-09-06T17:50:31.106703Z","iopub.status.idle":"2021-09-06T17:50:31.1371Z","shell.execute_reply.started":"2021-09-06T17:50:31.106654Z","shell.execute_reply":"2021-09-06T17:50:31.136105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-06T17:41:25.367126Z","iopub.execute_input":"2021-09-06T17:41:25.367408Z","iopub.status.idle":"2021-09-06T17:41:25.379114Z","shell.execute_reply.started":"2021-09-06T17:41:25.36737Z","shell.execute_reply":"2021-09-06T17:41:25.377912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# unpack tdataset to retrieve last column of labels\nX, y = train_data.iloc[: , :train_data.shape[1]-1], train_data.iloc[:, train_data.shape[1]-1]\nX_test = test_data[test_data.columns[0]]","metadata":{"execution":{"iopub.status.busy":"2021-09-06T17:52:11.332616Z","iopub.execute_input":"2021-09-06T17:52:11.333261Z","iopub.status.idle":"2021-09-06T17:52:11.612338Z","shell.execute_reply.started":"2021-09-06T17:52:11.333207Z","shell.execute_reply":"2021-09-06T17:52:11.61147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.info()","metadata":{"execution":{"iopub.status.busy":"2021-09-06T17:41:25.68381Z","iopub.execute_input":"2021-09-06T17:41:25.684216Z","iopub.status.idle":"2021-09-06T17:41:25.708744Z","shell.execute_reply.started":"2021-09-06T17:41:25.684179Z","shell.execute_reply":"2021-09-06T17:41:25.707371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All columns are numerical, there are no categorical columns. Had there been categorical columns, we would have used the .value_counts() method, which is applied to a column and outputs what categories exist and how many entries belong to each category.","metadata":{}},{"cell_type":"code","source":"X.hist(bins=50, figsize=(25,20))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-06T17:41:25.710437Z","iopub.execute_input":"2021-09-06T17:41:25.710754Z","iopub.status.idle":"2021-09-06T17:41:51.125954Z","shell.execute_reply.started":"2021-09-06T17:41:25.710724Z","shell.execute_reply":"2021-09-06T17:41:51.125127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# display summary statistics of the dataset\nX.describe()  # excludes nan values","metadata":{"execution":{"iopub.status.busy":"2021-09-06T17:41:51.127234Z","iopub.execute_input":"2021-09-06T17:41:51.127728Z","iopub.status.idle":"2021-09-06T17:41:55.979719Z","shell.execute_reply.started":"2021-09-06T17:41:51.127694Z","shell.execute_reply":"2021-09-06T17:41:55.978554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# count number of nan values per column\nprint(\"Count of nans per column\")\nprint(X.isna().sum(axis=0))\nprint(\"Count of nans per row\")\nprint(X.isna().sum(axis=1))\nprint(y.isna().sum(axis=0))  # label column has no nan values\nprint(\"Percentage of nan values per column is: \")\nprint(round((X.isna().sum(axis=0) / len(X)) * 100, 2))\nprint(\"The proportion of rows that contain nan values is \")\nnan_row_counts = X.isna().sum(axis=1)\nprint(str(round(len(nan_row_counts[nan_row_counts > 0]) / len(X) * 100, 2)) + \"%\" )","metadata":{"execution":{"iopub.status.busy":"2021-09-06T17:41:55.982429Z","iopub.execute_input":"2021-09-06T17:41:55.98287Z","iopub.status.idle":"2021-09-06T17:41:56.947453Z","shell.execute_reply.started":"2021-09-06T17:41:55.982825Z","shell.execute_reply":"2021-09-06T17:41:56.94621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are quite a few missing values for a feature. To deal with that, three options:\n\n**1.** Get rid of corresponding rows of that feature \n\n**2.** Get rid of entire feature.\n\n**3.** Set the values to some value (zero, mean, median, etc.).\n\nI wouldn't remove features for now, and I would also not remove rows that contain nan values, because that's 62% of our training data. Therefore, for me, the most sensible approach is to replace the values to either zero, mean, or median, or something else.\n\nIt doesn't make sense to me to replace the nans by the mean, because one can take a look at the features 51, 62 say, which have most of their values concentrated around 0 or around 1, so the mean will be in-between, possibly an outlier for that feature. \n\nIn conclusion, in the following I will fill in any missing value in any feature column with the median of the column column it belongs to. Recall we have no categorical columns, all columns are numerical.","metadata":{"execution":{"iopub.status.busy":"2021-09-06T14:49:47.972367Z","iopub.execute_input":"2021-09-06T14:49:47.972839Z","iopub.status.idle":"2021-09-06T14:49:47.979091Z","shell.execute_reply.started":"2021-09-06T14:49:47.972789Z","shell.execute_reply":"2021-09-06T14:49:47.97833Z"}}},{"cell_type":"code","source":"# fill in nan-s with mean\nX = X.apply(lambda x: x.fillna(x.median()), axis=0)\n\n# this prints 0 because there are no more nan values in the dataframe\nprint(X.isna().sum().sum())","metadata":{"execution":{"iopub.status.busy":"2021-09-06T17:52:23.901965Z","iopub.execute_input":"2021-09-06T17:52:23.902603Z","iopub.status.idle":"2021-09-06T17:52:26.334222Z","shell.execute_reply.started":"2021-09-06T17:52:23.902541Z","shell.execute_reply":"2021-09-06T17:52:26.333204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**We are now ready to begin analysing the data. It has no missing values now.**","metadata":{}},{"cell_type":"markdown","source":"Feature scaling:\n\nWe can see from the histogram plots that the features have quite different ranges. ","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler  # use min max scaler (x- min)/(max - min)\n\nscaler = MinMaxScaler()\nX_scaled = scaler.fit_transform(X)\nX_scaled = pd.DataFrame(X_scaled)","metadata":{"execution":{"iopub.status.busy":"2021-09-06T17:52:33.954503Z","iopub.execute_input":"2021-09-06T17:52:33.955032Z","iopub.status.idle":"2021-09-06T17:52:35.676861Z","shell.execute_reply.started":"2021-09-06T17:52:33.95499Z","shell.execute_reply":"2021-09-06T17:52:35.676117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.hist(bins=50, figsize=(25,20))\nX_scaled.hist(bins=50, figsize=(25,20))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-06T17:52:37.871219Z","iopub.execute_input":"2021-09-06T17:52:37.871793Z","iopub.status.idle":"2021-09-06T17:53:27.608667Z","shell.execute_reply.started":"2021-09-06T17:52:37.871744Z","shell.execute_reply":"2021-09-06T17:53:27.607575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = X_scaled","metadata":{"execution":{"iopub.status.busy":"2021-09-06T17:53:27.610494Z","iopub.execute_input":"2021-09-06T17:53:27.610925Z","iopub.status.idle":"2021-09-06T17:53:27.615351Z","shell.execute_reply.started":"2021-09-06T17:53:27.610883Z","shell.execute_reply":"2021-09-06T17:53:27.614381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**We are now ready to fit a model.**","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}