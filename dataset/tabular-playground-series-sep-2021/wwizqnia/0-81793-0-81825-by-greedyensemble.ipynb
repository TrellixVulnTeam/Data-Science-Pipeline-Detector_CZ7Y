{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# introduce\nIn this experiment, i try to get a nice score through some basic models\n\nI'm not good at FE, so,i only add some new features, and for Imputer and Scaler, i just let them be done by autoML tool, and I think the different preprocessing methods are good for Ensemble/stacking.\n\nIn short,my method is like:\n1. get original data\n2. send to Hypergbm\n3. get TopN best models\n4. run GreedyEnsemble\n\n\nAbout  PLB:\n\n\n    a. best xgb model, **PLB:0.81793**\n    \n    b. best lgb model, **PLB:0.81767**\n    \n    c. best cat model, **PLB:0.81744**\n    \n    e. GreedyEnsemble, **PLB:0.81825**\n    \nThere may also be performance improvements in basic models. Since we know trainning cost lot's of time in this competition.\n\nHope it can be helpful for u.\n","metadata":{}},{"cell_type":"code","source":"# #install HyperGBM\n!pip3 install -U hypergbm\n!pip3 install -U scikit-learn==0.23.2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import StratifiedKFold\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-30T09:39:46.503384Z","iopub.execute_input":"2021-09-30T09:39:46.50434Z","iopub.status.idle":"2021-09-30T09:39:47.348311Z","shell.execute_reply.started":"2021-09-30T09:39:46.504205Z","shell.execute_reply":"2021-09-30T09:39:47.347564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##load data\ntrain = pd.read_csv(\"/kaggle/input/tabular-playground-series-sep-2021/train.csv\", index_col=0)\ntest = pd.read_csv(\"/kaggle/input/tabular-playground-series-sep-2021/test.csv\", index_col=0)","metadata":{"execution":{"iopub.status.busy":"2021-09-30T09:39:49.344425Z","iopub.execute_input":"2021-09-30T09:39:49.345287Z","iopub.status.idle":"2021-09-30T09:40:31.050517Z","shell.execute_reply.started":"2021-09-30T09:39:49.345237Z","shell.execute_reply":"2021-09-30T09:40:31.049663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"About new features, i referred  https://www.kaggle.com/realtimshady/single-simple-lightgbm, thanks here.","metadata":{}},{"cell_type":"code","source":"#create new features\nfeatures = [x for x in train.columns.values if x[0]==\"f\"]\n\ntrain['n_missing'] = train[features].isna().sum(axis=1)\ntrain['abs_sum'] = train[features].abs().sum(axis=1)\ntrain['sem'] = train[features].sem(axis=1)\ntrain['std'] = train[features].std(axis=1)\ntrain['avg'] = train[features].mean(axis=1)\ntrain['max'] = train[features].max(axis=1)\ntrain['min'] = train[features].min(axis=1)\n\ntest['n_missing'] = test[features].isna().sum(axis=1)\ntest['abs_sum'] = test[features].abs().sum(axis=1)\ntest['sem'] = test[features].sem(axis=1)\ntest['std'] = test[features].std(axis=1)\ntest['avg'] = test[features].mean(axis=1)\ntest['max'] = test[features].max(axis=1)\ntest['min'] = test[features].min(axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-09-30T09:40:33.91552Z","iopub.execute_input":"2021-09-30T09:40:33.915884Z","iopub.status.idle":"2021-09-30T09:40:49.358111Z","shell.execute_reply.started":"2021-09-30T09:40:33.915834Z","shell.execute_reply":"2021-09-30T09:40:49.357341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For this experiment, I used HyperGBM, one kind of autoML tool.\nSimple introduction, HyperGBM is a library that supports full-pipeline AutoML, which completely covers the end-to-end stages of data cleaning, preprocessing, feature generation and selection, model selection and hyperparameter optimization.It is a real-AutoML tool for tabular data. You can visit [HyperGBM](https://github.com/DataCanvasIO/HyperGBM) for more details.","metadata":{}},{"cell_type":"markdown","source":"##### For first step, there are some things i did:\n1. custom search space.\n2. search best models.\n3. choice xgb models Top4, lgb models Top3, cat models Top2 (total 9 models) for next step.\n\n*(run lightgbm model for once--cv5, i cost 150mins,and only 2 cpu is running, i donot know why.\nso for this code, i only run xgboost ,it's so fast by using kaggle gpu)*","metadata":{}},{"cell_type":"code","source":"## custom search space\nROOT = '/kaggle/input/d/wumingyang/resource/'\nimport sys\nsys.path.append(ROOT)\nfrom MySearchSpace import MyGeneralSearchSpaceGenerator","metadata":{"execution":{"iopub.status.busy":"2021-09-30T09:41:00.245962Z","iopub.execute_input":"2021-09-30T09:41:00.246234Z","iopub.status.idle":"2021-09-30T09:41:12.594781Z","shell.execute_reply.started":"2021-09-30T09:41:00.246207Z","shell.execute_reply":"2021-09-30T09:41:12.593957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### I searched the base models locally, so just need reload history file.","metadata":{}},{"cell_type":"code","source":"from hypergbm import make_experiment\nfrom hypernets.core.trial import TrialHistory\nfrom hypernets.searchers import PlaybackSearcher\n\nhistory_file = f\"{ROOT}history_stepxgb.txt\" ## get from my datasets.\ntarget = 'claim'\nreward = 'auc' \n\nsearch_space_ = MyGeneralSearchSpaceGenerator(class_balancing=None)\nhistory = TrialHistory.load_history(search_space_, history_file)\n## for search algorithm, hypergbm support evolution,mcts, random, but here, i just need reload history with PlaybackSearcher.\nplayback = PlaybackSearcher(history, top_n=9, optimize_direction='max')","metadata":{"execution":{"iopub.status.busy":"2021-09-30T09:41:15.387226Z","iopub.execute_input":"2021-09-30T09:41:15.387493Z","iopub.status.idle":"2021-09-30T09:41:15.645574Z","shell.execute_reply.started":"2021-09-30T09:41:15.387465Z","shell.execute_reply":"2021-09-30T09:41:15.644679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### About GreedyEnsemble:\n\n    \"\"\"\n    References\n    ----------\n        Caruana, Rich, et al. \"Ensemble selection from libraries of models.\" Proceedings of the twenty-first international conference on Machine learning. 2004.\n    \"\"\"","metadata":{}},{"cell_type":"code","source":"##run make_experiment\nexp = make_experiment(\n                      train.copy(),\n                      target=target,\n                      reward_metric=reward,\n                      cv = True,\n                      num_folds= 5, ## 5 folds for Cross-validate\n                      max_trials=4,\n                      ensembel_size = 4, ## ensembel models with GreedyEnsembel\n                      early_stopping_time_limit=3600*3, ##early stop for 3h\n                      log_level = 'info',  ##some info will output while trainning\n                      searcher = playback,\n                      random_state=0)\nestimator = exp.run()","metadata":{"execution":{"iopub.status.busy":"2021-09-30T09:41:19.735794Z","iopub.execute_input":"2021-09-30T09:41:19.736442Z","iopub.status.idle":"2021-09-30T10:29:46.114482Z","shell.execute_reply.started":"2021-09-30T09:41:19.736403Z","shell.execute_reply":"2021-09-30T10:29:46.113617Z"},"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = estimator.predict_proba(test)[:,1]","metadata":{"execution":{"iopub.status.busy":"2021-09-30T10:30:20.438073Z","iopub.execute_input":"2021-09-30T10:30:20.438724Z","iopub.status.idle":"2021-09-30T10:49:08.720205Z","shell.execute_reply.started":"2021-09-30T10:30:20.438683Z","shell.execute_reply":"2021-09-30T10:49:08.719479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit_df = pd.read_csv(\"../input/tabular-playground-series-sep-2021/sample_solution.csv\")\nsubmit_df['claim'] = preds\nsubmit_df.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-30T10:59:24.182759Z","iopub.execute_input":"2021-09-30T10:59:24.183253Z","iopub.status.idle":"2021-09-30T10:59:26.689359Z","shell.execute_reply.started":"2021-09-30T10:59:24.183219Z","shell.execute_reply":"2021-09-30T10:59:26.688372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit_df.head(3)","metadata":{"execution":{"iopub.status.busy":"2021-09-30T11:03:49.213565Z","iopub.execute_input":"2021-09-30T11:03:49.214583Z","iopub.status.idle":"2021-09-30T11:03:49.227941Z","shell.execute_reply.started":"2021-09-30T11:03:49.214525Z","shell.execute_reply":"2021-09-30T11:03:49.226912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Try to do simple stacking.\n1. get new_train,new_test from model's predict result.\n2. train new_data","metadata":{}},{"cell_type":"code","source":"# ## get new_train, new_test\n# from hypergbm import HyperGBMEstimator\n\n# new_train = np.zeros((train.shape[0],10)) ## 9 models's predict result + claim\n# new_test = np.zeros((test.shape[0],9))\n\n# for i,trial in enumerate(exp.hyper_model_.history.trials):  ## get oof result from exp\n#     new_train[:,i] = trial.memo['oof'][:,1]  ##oof result saved in trial.memo for train data\n    \n#     model = HyperGBMEstimator.load(trial.model_file)  ##get trained model from model_file.\n#     new_test[:,i] = model.predict_proba(test)[:,1]  \n    \n# new_train[:,-1] = train[target].values\n\n","metadata":{"execution":{"iopub.status.busy":"2021-09-30T10:49:29.391808Z","iopub.execute_input":"2021-09-30T10:49:29.392066Z","iopub.status.idle":"2021-09-30T10:49:29.395925Z","shell.execute_reply.started":"2021-09-30T10:49:29.392039Z","shell.execute_reply":"2021-09-30T10:49:29.395058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # transform numpy to Dataframe\n# _columns = [f'feature{i}' for i in range(9)]\n# new_train = pd.DataFrame(new_train,columns=_columns+[target])\n# new_test = pd.DataFrame(new_test,columns=_columns)\n# new_train.head(3)","metadata":{"execution":{"iopub.status.busy":"2021-09-30T10:49:32.67378Z","iopub.execute_input":"2021-09-30T10:49:32.67404Z","iopub.status.idle":"2021-09-30T10:49:32.68024Z","shell.execute_reply.started":"2021-09-30T10:49:32.674014Z","shell.execute_reply":"2021-09-30T10:49:32.679444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import catboost\n# from sklearn.linear_model import LogisticRegression\n\n# meta_model1 = LogisticRegression(max_iter=1000,multi_class='multinomial',solver='lbfgs')\n# meta_model2 = catboost.CatBoostClassifier(depth=1,verbose=False)\n","metadata":{"execution":{"iopub.status.busy":"2021-09-30T10:49:34.928268Z","iopub.execute_input":"2021-09-30T10:49:34.928527Z","iopub.status.idle":"2021-09-30T10:49:34.932757Z","shell.execute_reply.started":"2021-09-30T10:49:34.928499Z","shell.execute_reply":"2021-09-30T10:49:34.931665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# iterators = StratifiedKFold(n_splits=5, shuffle=True,random_state=0)\n# y = new_train.pop(target)\n# X = new_train\n# preds = np.zeros((test.shape[0],2)) ## for 2 models\n# for n_fold, (train_idx, valid_idx) in enumerate(iterators.split(X, y)):\n#     x_train_fold, y_train_fold = X.iloc[train_idx], y.iloc[train_idx]\n#     x_val_fold, y_val_fold = X.iloc[valid_idx], y.iloc[valid_idx]\n#     for i,meta_model in enumerate([meta_model1,meta_model2]):\n#         meta_model.fit(x_train_fold,y_train_fold)\n#         pred = meta_model.predict_proba(x_val_fold)[:,1]\n#         preds[valid_idx,i] = pred\n    \n# ## just average the result\n# preds = np.mean(preds,axis=1)\n\n# # submission = pd.read_csv('../input/tabular-playground-series-sep-2021/sample_solution.csv', index_col='id')\n# # submission['claim'] = preds\n# # submission.to_csv('submission_simpleStacking.csv')\n\n# ## PLB: 0.81825","metadata":{"execution":{"iopub.status.busy":"2021-09-30T10:49:37.046965Z","iopub.execute_input":"2021-09-30T10:49:37.047243Z","iopub.status.idle":"2021-09-30T10:49:37.051538Z","shell.execute_reply.started":"2021-09-30T10:49:37.047216Z","shell.execute_reply":"2021-09-30T10:49:37.050559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Now, i have a new_train data which has 9 features,before move to the next step,add some new features.","metadata":{}},{"cell_type":"code","source":"\n# from itertools import combinations\n# def generate_new_features(df):\n#         combs = list(combinations(df.columns, 2))\n#         for i, j in combs:\n#             column_name = '%s-%s' % (i, j)\n#             df[column_name] = df[i] - df[j]\n#             # column_name = '%s+%s' % (i, j)\n#             # df[column_name] = df[i] + df[j]\n#         return df\n\n# new_train = generate_new_features(X)\n# new_train[target] = y\n# new_test = generate_new_features(new_test)","metadata":{"execution":{"iopub.status.busy":"2021-09-30T10:49:39.212509Z","iopub.execute_input":"2021-09-30T10:49:39.213125Z","iopub.status.idle":"2021-09-30T10:49:39.218319Z","shell.execute_reply.started":"2021-09-30T10:49:39.21309Z","shell.execute_reply":"2021-09-30T10:49:39.217272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}