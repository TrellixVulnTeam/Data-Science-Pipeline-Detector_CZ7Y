{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Welcome to the September 2021 Tabular Playground Competition! #\n\nIn this competition, we predict whether a customer will make an insurance claim.\n\n# Step1: Import Helpful Libraries #","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import OrdinalEncoder, StandardScaler\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score","metadata":{"execution":{"iopub.status.busy":"2021-09-03T07:53:42.943035Z","iopub.execute_input":"2021-09-03T07:53:42.943429Z","iopub.status.idle":"2021-09-03T07:53:43.770732Z","shell.execute_reply.started":"2021-09-03T07:53:42.943344Z","shell.execute_reply":"2021-09-03T07:53:43.769887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# Step2: Load Data","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv(\"../input/tabular-playground-series-sep-2021/train.csv\", index_col='id')\ntest = pd.read_csv(\"../input/tabular-playground-series-sep-2021/test.csv\", index_col='id')\n\nFEATURES = list(df_train.columns[:-1])\nTARGET = df_train.columns[-1]\n\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-03T07:53:51.04568Z","iopub.execute_input":"2021-09-03T07:53:51.046013Z","iopub.status.idle":"2021-09-03T07:54:32.200841Z","shell.execute_reply.started":"2021-09-03T07:53:51.045984Z","shell.execute_reply":"2021-09-03T07:54:32.200062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The target `'claim'` has binary outcomes: `0` for no claim and `1` for claim.","metadata":{}},{"cell_type":"code","source":"df_train.info()\ndf_train.describe()","metadata":{"execution":{"iopub.status.busy":"2021-09-03T07:54:32.202355Z","iopub.execute_input":"2021-09-03T07:54:32.2027Z","iopub.status.idle":"2021-09-03T07:54:36.204238Z","shell.execute_reply.started":"2021-09-03T07:54:32.202663Z","shell.execute_reply":"2021-09-03T07:54:36.203429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Missing Values\nRefer to [TPS Sep 2021 single LGBM](https://www.kaggle.com/hiro5299834/tps-sep-2021-single-lgbm/notebook) by [@hiro5299834](https://www.kaggle.com/hiro5299834)","metadata":{}},{"cell_type":"code","source":"df_train['n_missing'] = df_train[FEATURES].isna().sum(axis=1)\ntest['n_missing'] = test[FEATURES].isna().sum(axis=1)\n\ndf_train['std'] = df_train[FEATURES].std(axis=1)\ntest['std'] = test[FEATURES].std(axis=1)\n\ndf_train['mean'] = df_train[FEATURES].mean(axis=1)\ntest['mean'] = test[FEATURES].mean(axis=1)\n\nFEATURES += ['n_missing', 'std', 'mean']","metadata":{"execution":{"iopub.status.busy":"2021-09-03T07:56:55.354216Z","iopub.execute_input":"2021-09-03T07:56:55.354592Z","iopub.status.idle":"2021-09-03T07:56:57.489413Z","shell.execute_reply.started":"2021-09-03T07:56:55.35456Z","shell.execute_reply":"2021-09-03T07:56:57.487659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step3: Train Model #\n\nLet's try out a simple XGBoost model. This algorithm can handle missing values, but you could try imputing them instead.  We use `XGBClassifier` (instead of `XGBRegressor`, for instance), since this is a classification problem.","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBClassifier\n\nX = df_train.loc[:, FEATURES]\ny = df_train.loc[:, TARGET]\n\nfinal_predictions = []\nkf = KFold(n_splits=10, shuffle=True, random_state=42)\nfor fold, (train_indicies, valid_indicies) in enumerate(kf.split(X=X)):\n    X_train = X.loc[train_indicies]\n    X_valid = X.loc[valid_indicies]\n    X_test = test.copy()\n    \n    y_train = y.loc[train_indicies]\n    y_valid = y.loc[valid_indicies]\n    \n    \n    scaler = StandardScaler()\n    X_train[FEATURES] = scaler.fit_transform(X_train[FEATURES])\n    X_valid[FEATURES] = scaler.transform(X_valid[FEATURES])\n    X_test[FEATURES] = scaler.transform(X_test[FEATURES])\n    \n    model = XGBClassifier(\n        max_depth=3,\n        subsample=0.5,\n        colsample_bytree=0.5,\n        learning_rate= 0.01187431306013263,\n        n_estimators= 10000,\n        n_jobs=-1,\n        use_label_encoder=False,\n        objective='binary:logistic',\n        tree_method='gpu_hist',  # Use GPU \n        gpu_id=0,\n        predictor='gpu_predictor',\n    )\n    model.fit(X_train, y_train,\n             verbose = False,\n             eval_set = [(X_train, y_train), (X_valid, y_valid)],\n             eval_metric = \"auc\",\n             early_stopping_rounds = 200)\n    \n    preds_valid = model.predict_proba(X_valid)[:,1]\n    preds_test = model.predict_proba(X_test)[:,1]\n    final_predictions.append(preds_test)\n    print(fold, roc_auc_score(y_valid, preds_valid))","metadata":{"execution":{"iopub.status.busy":"2021-09-03T07:56:59.326278Z","iopub.execute_input":"2021-09-03T07:56:59.326613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Make Submission #\n\nOur predictions are binary 0 and 1, but you're allowed to submit probabilities instead. In scikit-learn, you would use the `predict_proba` method instead of `predict`.","metadata":{}},{"cell_type":"code","source":"preds = np.mean(np.column_stack(final_predictions), axis=1)\n\n# Make predictions\ny_pred = pd.Series(\n    preds,\n    index=X_test.index,\n    name=TARGET,\n)\n\n# Create submission file\ny_pred.to_csv(\"submission.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}