{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\nTabular Playground Series are a month-long competions that are released on 1st of every month. These are designed to be beginner friendly and help bridge the gap between inclass competition and featured competition.\n\nThe aim of TPS September 2021  is to predict if the customer will claim a insurance policy or not. The ground truth claim is binary valued, but a prediction may be any number from 0.0 to 1.0, representing the probability of a claim. The features in this dataset have been anonymized and may contain missing values.\n\nThis table of contents gives an overview about different sections in the notebook.\n\n1. [Load Required Libraries](#1)\n2. [Import the Dataset](#2)\n3. [Exploratory Data Analysis](#3)\n    * [Train Dataset](#3)\n    * [Test Dataset](#4)\n    * [Missing Values](#5)\n    * [Distributions](#6)\n    * [Correlations](#7)\n4. [Modeling](#8)\n5. [Submission](#9)","metadata":{}},{"cell_type":"markdown","source":"<a id = \"1\" ></a>\n## Loading Required Libraries","metadata":{}},{"cell_type":"code","source":"#importing libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#modeling\nfrom sklearn.model_selection import StratifiedKFold\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import roc_auc_score","metadata":{"execution":{"iopub.status.busy":"2021-10-11T20:13:42.121821Z","iopub.execute_input":"2021-10-11T20:13:42.122472Z","iopub.status.idle":"2021-10-11T20:13:43.374661Z","shell.execute_reply.started":"2021-10-11T20:13:42.122274Z","shell.execute_reply":"2021-10-11T20:13:43.373641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#set color palette\nsns.set_palette(\"Spectral_r\")","metadata":{"execution":{"iopub.status.busy":"2021-10-11T20:13:43.37638Z","iopub.execute_input":"2021-10-11T20:13:43.376806Z","iopub.status.idle":"2021-10-11T20:13:43.390831Z","shell.execute_reply.started":"2021-10-11T20:13:43.376758Z","shell.execute_reply":"2021-10-11T20:13:43.389569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"2\" ></a>\n## Importing the dataset\nWe are using three different files in this notebook and we will import all three files before starting our analysis.\n\n* `train.csv` - the training data with the target claim column\n* `test.csv` - the test set; you will be predicting the claim for each row in this file\n* `sample_submission.csv` - a sample submission file in the correct format","metadata":{}},{"cell_type":"code","source":"#import dataset \ntrain = pd.read_csv(\"../input/tabular-playground-series-sep-2021/train.csv\")\ntest = pd.read_csv(\"../input/tabular-playground-series-sep-2021/test.csv\")\n\n#output file \nsubmission = pd.read_csv(\"../input/tabular-playground-series-sep-2021/sample_solution.csv\")\n\nFEATURES = train.columns[:-1]\nTARGET = train.columns[-1]","metadata":{"execution":{"iopub.status.busy":"2021-10-11T20:13:43.395589Z","iopub.execute_input":"2021-10-11T20:13:43.395952Z","iopub.status.idle":"2021-10-11T20:14:28.38033Z","shell.execute_reply.started":"2021-10-11T20:13:43.395918Z","shell.execute_reply":"2021-10-11T20:14:28.379308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"3\" ></a>\n## Exploratory Data Analysis\nThe aim of this step is to explore the dataset a bit to get insights about the shape of the data, datatypes of the feature columns, missing values and so on.\n\n### Train Dataset","metadata":{}},{"cell_type":"code","source":"#Overview of train dataset\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-11T20:14:28.382111Z","iopub.execute_input":"2021-10-11T20:14:28.382483Z","iopub.status.idle":"2021-10-11T20:14:28.434812Z","shell.execute_reply.started":"2021-10-11T20:14:28.382441Z","shell.execute_reply":"2021-10-11T20:14:28.433775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dataframe dimensions\n* The `train` dataset contains 957919 rows of data and 120 features","metadata":{}},{"cell_type":"code","source":"#dimensions of the dataset\nprint(f'The shape of the train dataset {train.shape}')","metadata":{"execution":{"iopub.status.busy":"2021-10-11T20:14:28.436291Z","iopub.execute_input":"2021-10-11T20:14:28.436696Z","iopub.status.idle":"2021-10-11T20:14:28.442944Z","shell.execute_reply.started":"2021-10-11T20:14:28.436657Z","shell.execute_reply":"2021-10-11T20:14:28.441665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Quick summary statistics of the data\nThe summary statistics shows the min, max, mean, standard deviation and quartile infomation for each feature column","metadata":{}},{"cell_type":"code","source":"train.describe()","metadata":{"execution":{"iopub.status.busy":"2021-10-11T20:14:28.44487Z","iopub.execute_input":"2021-10-11T20:14:28.445304Z","iopub.status.idle":"2021-10-11T20:14:33.475768Z","shell.execute_reply.started":"2021-10-11T20:14:28.445262Z","shell.execute_reply":"2021-10-11T20:14:33.474519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"4\" ></a>\n### Test Data","metadata":{}},{"cell_type":"code","source":"#overview of test data\ntest.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-11T20:14:33.477733Z","iopub.execute_input":"2021-10-11T20:14:33.478182Z","iopub.status.idle":"2021-10-11T20:14:33.517018Z","shell.execute_reply.started":"2021-10-11T20:14:33.478135Z","shell.execute_reply":"2021-10-11T20:14:33.515758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dataset Dimensions\n* The `test` dataset contains 493474 rows of data and 119 features","metadata":{}},{"cell_type":"code","source":"print(f'The shape of the test data is {test.shape}')","metadata":{"execution":{"iopub.status.busy":"2021-10-11T20:14:33.520496Z","iopub.execute_input":"2021-10-11T20:14:33.521Z","iopub.status.idle":"2021-10-11T20:14:33.527202Z","shell.execute_reply.started":"2021-10-11T20:14:33.520949Z","shell.execute_reply":"2021-10-11T20:14:33.525703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Submission File\nThe format of the output submission file is shown below: \n* It contains only two columns namely, the `id` column and the `claim` column","metadata":{}},{"cell_type":"code","source":"submission.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-11T20:14:33.529811Z","iopub.execute_input":"2021-10-11T20:14:33.53029Z","iopub.status.idle":"2021-10-11T20:14:33.546898Z","shell.execute_reply.started":"2021-10-11T20:14:33.530242Z","shell.execute_reply":"2021-10-11T20:14:33.545407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"5\" ></a>\n### Missing values \nWe will check if there are missing values in our dataset","metadata":{}},{"cell_type":"code","source":"#missing values\nmissing = train.isnull().sum()\nmissing","metadata":{"execution":{"iopub.status.busy":"2021-10-11T20:14:33.548492Z","iopub.execute_input":"2021-10-11T20:14:33.548972Z","iopub.status.idle":"2021-10-11T20:14:33.781337Z","shell.execute_reply.started":"2021-10-11T20:14:33.548922Z","shell.execute_reply":"2021-10-11T20:14:33.780242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As shown above, our dataset contains missing values. Now, I will check the proportion of missing values in each column.\n\nIt can be noted that, on an average the proportion of missing values ranges between **(1.60 - 1.65)%**","metadata":{}},{"cell_type":"code","source":"#missing values plot\nmissing/len(train)","metadata":{"execution":{"iopub.status.busy":"2021-10-11T20:14:33.783014Z","iopub.execute_input":"2021-10-11T20:14:33.783425Z","iopub.status.idle":"2021-10-11T20:14:33.807198Z","shell.execute_reply.started":"2021-10-11T20:14:33.783383Z","shell.execute_reply":"2021-10-11T20:14:33.806102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Imbalance in the distribution of Target variable\nFrom the plot below, we see that the target variable `claim` is fairly balanced","metadata":{}},{"cell_type":"code","source":"#checking for imbalance in the dataset\ncount = train['claim'].value_counts().values\nsns.barplot(x = [0,1], y = count)\nplt.title('Target variable count')","metadata":{"execution":{"iopub.status.busy":"2021-10-11T20:14:33.808771Z","iopub.execute_input":"2021-10-11T20:14:33.809412Z","iopub.status.idle":"2021-10-11T20:14:33.989875Z","shell.execute_reply.started":"2021-10-11T20:14:33.809362Z","shell.execute_reply":"2021-10-11T20:14:33.988772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"6\" ></a>\n### Feature Distributions\nShowing distribution on each feature that are available in train and test dataset. We observe that all features distribution on train and test dataset are almost similar.","metadata":{}},{"cell_type":"code","source":"#distribution of features in train dataset\nfig = plt.figure(figsize = (20, 140))\nfor idx, i in enumerate(train.columns):\n    fig.add_subplot(np.ceil(len(train.columns)/4), 4, idx+1)\n    train.iloc[:, idx].hist(bins = 20)\n    plt.title(i)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-11T20:14:33.991549Z","iopub.execute_input":"2021-10-11T20:14:33.992003Z","iopub.status.idle":"2021-10-11T20:15:01.227024Z","shell.execute_reply.started":"2021-10-11T20:14:33.991961Z","shell.execute_reply":"2021-10-11T20:15:01.225845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#ditribution of features in test data\nfig = plt.figure(figsize = (20, 140))\nfor idx, i in enumerate(test.columns):\n    fig.add_subplot(np.ceil(len(test.columns)/4), 4, idx+1)\n    test.iloc[:, idx].hist(bins = 20)\n    plt.title(i)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-11T20:15:01.228346Z","iopub.execute_input":"2021-10-11T20:15:01.22875Z","iopub.status.idle":"2021-10-11T20:15:27.551107Z","shell.execute_reply.started":"2021-10-11T20:15:01.228694Z","shell.execute_reply":"2021-10-11T20:15:27.5498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"7\" ></a>\n### Correlations\nThere seem to be a very little or no correlation between features as well as feature-to-target correlation.","metadata":{}},{"cell_type":"code","source":"#correlation between features\ncorr = train.corr()\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\nwith sns.axes_style(\"white\"):\n    f, ax = plt.subplots(figsize=(16, 16))\n    ax = sns.heatmap(corr, mask=mask, cmap = 'Spectral_r', vmax=.3, square=True)","metadata":{"execution":{"iopub.status.busy":"2021-10-11T20:15:27.55292Z","iopub.execute_input":"2021-10-11T20:15:27.553328Z","iopub.status.idle":"2021-10-11T20:16:09.580423Z","shell.execute_reply.started":"2021-10-11T20:15:27.553288Z","shell.execute_reply":"2021-10-11T20:16:09.579457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"8\" ></a>\n## Modeling\nIn this notebook, I will be using XGBoost Classifier. I base my model on this notebook given by the [kaggle competition team](https://www.kaggle.com/hsuchialun/tps-xgboost-kfold-with-gpu#Step1:-Import-Helpful-Libraries). I changed few parameters in my model. ","metadata":{}},{"cell_type":"code","source":"#modeling\nX = train.loc[:, FEATURES]\ny = train.loc[:, TARGET]\n\nfinal_predictions = []\nkf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\nfor fold, (train_indicies, valid_indicies) in enumerate(kf.split(X, y)):\n    X_train = X.loc[train_indicies]\n    X_valid = X.loc[valid_indicies]\n    X_test = test.copy()\n    \n    y_train = y.loc[train_indicies]\n    y_valid = y.loc[valid_indicies]\n    \n    model = XGBClassifier(random_state=42, verbosity=0, tree_method='gpu_hist')\n    \n    model.fit(X_train, y_train,\n             verbose = False,\n             eval_set = [(X_train, y_train), (X_valid, y_valid)],\n             eval_metric = \"auc\",\n             early_stopping_rounds = 200)\n    preds_valid = model.predict_proba(X_valid)[:,1]\n    preds_test = model.predict_proba(X_test)[:,1]\n    final_predictions.append(preds_test)\n    print(fold, roc_auc_score(y_valid, preds_valid))","metadata":{"execution":{"iopub.status.busy":"2021-10-11T20:16:09.582026Z","iopub.execute_input":"2021-10-11T20:16:09.58263Z","iopub.status.idle":"2021-10-11T20:17:46.802214Z","shell.execute_reply.started":"2021-10-11T20:16:09.582587Z","shell.execute_reply":"2021-10-11T20:17:46.800528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"9\" ></a>\n## Submission\nThis is my final submission file. ","metadata":{}},{"cell_type":"code","source":"preds = np.mean(np.column_stack(final_predictions), axis=1)\n\n# Make predictions\ny_pred = pd.DataFrame({'id': submission['id'], 'claim': preds})\n\n# Create submission file\ny_pred.to_csv(\"submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-10-11T20:17:46.803672Z","iopub.execute_input":"2021-10-11T20:17:46.804077Z","iopub.status.idle":"2021-10-11T20:17:48.640472Z","shell.execute_reply.started":"2021-10-11T20:17:46.804035Z","shell.execute_reply":"2021-10-11T20:17:48.639551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Thanks for reading! Upvote if you find this notebook useful ","metadata":{}}]}