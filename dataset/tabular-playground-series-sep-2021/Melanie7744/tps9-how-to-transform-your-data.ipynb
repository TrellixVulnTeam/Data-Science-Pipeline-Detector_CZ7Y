{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Tutorial: the different scalers from scikit-learn applied to TPS9\nI write this tutorial while trying to find out which transformations should be applied to the data from [Tabular Playground Series September 2021](https://www.kaggle.com/c/tabular-playground-series-sep-2021). I will examine the impact of the different scalers on feature values and distributions. Many machine learning algorithm require scaling of the features to archeive optimal performance.\n\nFor a more comprehensive overview you should visit [this page](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html?highlight=standard%20scaler) from the scikit-learn homepage which compares the different scalers on data from the California Housing Dataset.\n\n----\n**Looking forward to your ideas and comments!**","metadata":{}},{"cell_type":"code","source":"# import packages\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nprint('Matplotlib: {}'.format(matplotlib.__version__))\n\nimport seaborn as sns\nprint('Seaborn %s' % sns.__version__)\n#-------\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-09-16T11:42:20.230042Z","iopub.execute_input":"2021-09-16T11:42:20.231674Z","iopub.status.idle":"2021-09-16T11:42:20.24563Z","shell.execute_reply.started":"2021-09-16T11:42:20.231618Z","shell.execute_reply":"2021-09-16T11:42:20.244095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# read input files\ndf_train = pd.read_csv(\"../input/tabular-playground-series-sep-2021/train.csv\")\ndf_test = pd.read_csv(\"../input/tabular-playground-series-sep-2021/test.csv\")\nsample_submission = pd.read_csv(\"../input/tabular-playground-series-sep-2021/sample_solution.csv\")\n\nfeature_cols = [col for col in df_train.columns if col.startswith(\"f\")]","metadata":{"execution":{"iopub.status.busy":"2021-09-16T11:42:20.247587Z","iopub.execute_input":"2021-09-16T11:42:20.247867Z","iopub.status.idle":"2021-09-16T11:42:58.1435Z","shell.execute_reply.started":"2021-09-16T11:42:20.247838Z","shell.execute_reply":"2021-09-16T11:42:58.142634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_histograms(df,title):\n    '''\n    takes a data frame and displays histograms of the columns in a x by 4 grid\n    title is added on top\n    a table with mean and std is added below\n    if you use it for more than 8 features the table becomes messy, better use plot_histograms2 then.\n    '''\n\n    cols=df.columns.values\n    stats = df.describe().loc[[\"mean\",\"std\"]]\n    subplot_rows = int( len(cols)// 4 + (len(cols) % 4 > 0)) + 1 # get number of rows needed for histogram grid (=divide the columns by 4 and round up), add 1 for table\n\n    fig=plt.figure(figsize=(20,subplot_rows*4))\n    fig.suptitle(title, fontsize=20)\n\n    for i in range(0,len(cols)):\n        ax = fig.add_subplot(subplot_rows,4,i+1)\n        p1 = sns.histplot(df.iloc[:,i], ax=ax)\n        p1.set(ylabel=None)\n    \n    # add table\n    ax_t =  fig.add_subplot(subplot_rows,1,subplot_rows)    \n    plt.table(cellText=np.round(stats.values,6), colLabels=stats.columns, rowLabels=[\"mean\",\"std\"], colWidths=tuple([0.1] * len(cols)), cellLoc='center', loc='center')\n\n    ax_t.spines['top'].set_visible(False)\n    ax_t.spines['right'].set_visible(False)\n    ax_t.spines['left'].set_visible(False)\n    ax_t.spines['bottom'].set_visible(False)\n    ax_t.get_yaxis().set_ticks([])\n    ax_t.get_xaxis().set_ticks([])# set no ticks\n\n    return","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-09-16T11:42:58.144885Z","iopub.execute_input":"2021-09-16T11:42:58.145186Z","iopub.status.idle":"2021-09-16T11:42:58.157241Z","shell.execute_reply.started":"2021-09-16T11:42:58.145147Z","shell.execute_reply":"2021-09-16T11:42:58.15663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# make a function for plotting\ndef plot_histograms_2(df,title):\n    '''\n    old function: kept for reference, uses different subplot method\n    takes a data frame and displays histograms of the columns in a x by 4 grid\n    title is added on top\n    '''\n    cols=df.columns.values\n    subplot_rows = int( len(cols)// 4 + (len(cols) % 4 > 0)) # divide len(cols) by four and round up if no whole number\n    fig, ax = plt.subplots(subplot_rows, 4, figsize=(20,subplot_rows*4))\n    fig.suptitle(title, fontsize=20)\n    cnt = 0\n    for column in cols:\n        if subplot_rows == 1:\n            p1 = sns.histplot(df[column], ax=ax[cnt%4])\n        else:\n            p1 = sns.histplot(df[column], ax=ax[cnt//4,cnt%4])\n        p1.set(ylabel=None) # no header on y axis\n        cnt += 1    \n    \n    return\n\n# get the statistics with\n#df.describe().loc[[\"mean\",\"std\"]]","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-09-16T11:42:58.158857Z","iopub.execute_input":"2021-09-16T11:42:58.159292Z","iopub.status.idle":"2021-09-16T11:42:58.178517Z","shell.execute_reply.started":"2021-09-16T11:42:58.15926Z","shell.execute_reply":"2021-09-16T11:42:58.177623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# choose the features you want to examine\n# the more features you choose the longer it will take to make the plot\nfeature_subset=[\"f1\",\"f9\",\"f29\",\"f35\",\"f50\",\"f73\",\"f96\",\"f100\",\"f116\",\"f118\"]\nfeature_subset_small_a=[\"f40\",\"f42\", \"f47\",\"f50\"]\nfeature_subset_small=[\"f2\",\"f10\", \"f30\",\"f34\"]\ndf_to_transform = df_train[feature_subset_small]","metadata":{"execution":{"iopub.status.busy":"2021-09-16T11:42:58.179847Z","iopub.execute_input":"2021-09-16T11:42:58.180837Z","iopub.status.idle":"2021-09-16T11:42:58.204707Z","shell.execute_reply.started":"2021-09-16T11:42:58.180762Z","shell.execute_reply":"2021-09-16T11:42:58.203876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_histograms(df_to_transform, \"Original data\")","metadata":{"execution":{"iopub.status.busy":"2021-09-16T11:42:58.206031Z","iopub.execute_input":"2021-09-16T11:42:58.206269Z","iopub.status.idle":"2021-09-16T11:43:05.969051Z","shell.execute_reply.started":"2021-09-16T11:42:58.206241Z","shell.execute_reply":"2021-09-16T11:43:05.968236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ndf = pd.DataFrame(scaler.fit_transform(df_to_transform), columns=df_to_transform.columns)\n\nplot_histograms(df, \"With Standard Scaler\")","metadata":{"execution":{"iopub.status.busy":"2021-09-16T11:43:05.9703Z","iopub.execute_input":"2021-09-16T11:43:05.970622Z","iopub.status.idle":"2021-09-16T11:43:13.648406Z","shell.execute_reply.started":"2021-09-16T11:43:05.970595Z","shell.execute_reply":"2021-09-16T11:43:13.647612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note how the x-axis changes. The mean of each feature is now (roughly) 0 and the features have a standard deviation of 1.\n\nIf you don't want the Scaler to scale the features to a standard deviation of 1, you can set *with_std=False*.\n\nIf you dont' want to center the data before scaling, i.e. just calculate x/std, you can set *with_mean=False*.\n\nJust don't set both to False, because then the scaler does nothing ;)\n\nThis scaler is very sensitive to the presence of outliers.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\ndf = pd.DataFrame(scaler.fit_transform(df_to_transform), columns=df_to_transform.columns)\n\nplot_histograms(df, \"With MinMax Scaler\")","metadata":{"execution":{"iopub.status.busy":"2021-09-16T11:43:13.649488Z","iopub.execute_input":"2021-09-16T11:43:13.64973Z","iopub.status.idle":"2021-09-16T11:43:21.631443Z","shell.execute_reply.started":"2021-09-16T11:43:13.649701Z","shell.execute_reply":"2021-09-16T11:43:21.630525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note how the x-axis changes. The minimum value is now 0, the maximum value is 1. This is the default setting of the MinMaxScaler. You can change this and scale each feature to any given range.\n\nThis scaler is very sensitive to the presence of outliers.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import RobustScaler\nscaler = RobustScaler()\ndf = pd.DataFrame(scaler.fit_transform(df_to_transform), columns=df_to_transform.columns)\n\nplot_histograms(df, \"With Robust Scaler\")","metadata":{"execution":{"iopub.status.busy":"2021-09-16T11:43:21.633793Z","iopub.execute_input":"2021-09-16T11:43:21.63406Z","iopub.status.idle":"2021-09-16T11:43:29.578988Z","shell.execute_reply.started":"2021-09-16T11:43:21.634029Z","shell.execute_reply":"2021-09-16T11:43:29.578111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note how the x-axis changes again. RobustScaler uses the median and the interquarile range instead of mean and std, like StandardScaler. This reduces the negative effect of outlies on RobustScaler. \n\nThis scaler is robust to outliers.","metadata":{}},{"cell_type":"code","source":"# transform f10 \"by hand\" instead of using RobustScaler for demo purposes\nq25 = df_train.f10.describe().loc[\"25%\"]\nq75 = df_train.f10.describe().loc[\"75%\"]\niqr = q75 -q25\nmedian = df_train.f10.describe().loc[\"50%\"]\n\nf10_scaled = (df_train.f10 - median) / iqr","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-09-16T11:43:29.58016Z","iopub.execute_input":"2021-09-16T11:43:29.580388Z","iopub.status.idle":"2021-09-16T11:43:29.706585Z","shell.execute_reply.started":"2021-09-16T11:43:29.580362Z","shell.execute_reply":"2021-09-16T11:43:29.705744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import PowerTransformer\ntransformer = PowerTransformer()\ndf = pd.DataFrame(transformer.fit_transform(df_to_transform), columns=df_to_transform.columns)\n\nplot_histograms(df, \"With Power Transformer\")","metadata":{"execution":{"iopub.status.busy":"2021-09-16T11:43:29.70794Z","iopub.execute_input":"2021-09-16T11:43:29.708156Z","iopub.status.idle":"2021-09-16T11:43:47.643853Z","shell.execute_reply.started":"2021-09-16T11:43:29.708131Z","shell.execute_reply":"2021-09-16T11:43:47.642911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note how the x-axis AND the y-axis change. PowerTransformer makes the shapes more Gaussian-like and tries to reduce skewness. As it changes the shape of the distribution it is called \"Transformer\" instead of \"Scaler\". Like StandardScaler, PowerTransformer normalizes the data to mean = 0 and std = 1. You can see that for feature f34, which already has a Gaussian-like distribution, PowerTransformer behaves similar to StandardScaler. For other features the changing shape of the distribution is clearly visible.\n\nBy default a transformation method called \"yeo-johnson\" is applied. This is a newer method, compared to the other supported method called \"box-cox\". The latter dates back to the 1960s and supports only positive values.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import QuantileTransformer\ntransformer = QuantileTransformer(output_distribution = 'normal')\ndf_qt = pd.DataFrame(transformer.fit_transform(df_to_transform), columns=df_to_transform.columns)\n\nplot_histograms(df_qt, \"With Quantile Transformer\")","metadata":{"execution":{"iopub.status.busy":"2021-09-16T11:43:47.64534Z","iopub.execute_input":"2021-09-16T11:43:47.645562Z","iopub.status.idle":"2021-09-16T11:43:56.571538Z","shell.execute_reply.started":"2021-09-16T11:43:47.645536Z","shell.execute_reply":"2021-09-16T11:43:56.570618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Omg, all features look the same now! Well, quite the same. QuantileTransformer does a non-linear transformation which changes all features to fit a normal distribution. If you don't set *output_distribution = 'normal'* it will fit all features to a uniform distribution.\n\nOuantile transformer is robust to outliers. It distorts correlations between the features.","metadata":{}}]}