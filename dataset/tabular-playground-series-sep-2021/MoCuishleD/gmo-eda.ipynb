{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 0. Setting","metadata":{}},{"cell_type":"markdown","source":"## 0.1 Calling Basic Libraries","metadata":{"execution":{"iopub.status.busy":"2021-09-13T10:37:41.941388Z","iopub.execute_input":"2021-09-13T10:37:41.941958Z","iopub.status.idle":"2021-09-13T10:37:41.955412Z","shell.execute_reply.started":"2021-09-13T10:37:41.941861Z","shell.execute_reply":"2021-09-13T10:37:41.954523Z"}}},{"cell_type":"code","source":"# import basic library\nfrom sklearn.impute import SimpleImputer\nfrom IPython.display import display\nimport plotly.figure_factory as ff\nimport plotly.graph_objects as go\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport sklearn\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\n\nfrom xgboost import XGBRegressor\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMRegressor\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostRegressor\nfrom catboost import CatBoostClassifier","metadata":{"execution":{"iopub.status.busy":"2021-09-13T10:45:59.40474Z","iopub.execute_input":"2021-09-13T10:45:59.405592Z","iopub.status.idle":"2021-09-13T10:46:04.429059Z","shell.execute_reply.started":"2021-09-13T10:45:59.405527Z","shell.execute_reply":"2021-09-13T10:46:04.428267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 0.2 Data Setting","metadata":{}},{"cell_type":"code","source":"# import data\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-13T10:46:23.548028Z","iopub.execute_input":"2021-09-13T10:46:23.54831Z","iopub.status.idle":"2021-09-13T10:46:23.556074Z","shell.execute_reply.started":"2021-09-13T10:46:23.548281Z","shell.execute_reply":"2021-09-13T10:46:23.555265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import train & test data\ntrain = pd.read_csv(\"../input/tabular-playground-series-sep-2021/train.csv\")\ntest = pd.read_csv(\"../input/tabular-playground-series-sep-2021/test.csv\")\nsample = pd.read_csv(\"../input/tabular-playground-series-sep-2021/sample_solution.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-09-13T10:46:28.206381Z","iopub.execute_input":"2021-09-13T10:46:28.207095Z","iopub.status.idle":"2021-09-13T10:47:10.606552Z","shell.execute_reply.started":"2021-09-13T10:46:28.207059Z","shell.execute_reply":"2021-09-13T10:47:10.605785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# information about test and train data\ndisplay(train.info())\ndisplay(test.info())","metadata":{"execution":{"iopub.status.busy":"2021-09-13T10:47:14.076448Z","iopub.execute_input":"2021-09-13T10:47:14.077192Z","iopub.status.idle":"2021-09-13T10:47:14.123626Z","shell.execute_reply.started":"2021-09-13T10:47:14.077143Z","shell.execute_reply":"2021-09-13T10:47:14.122654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. EDA","metadata":{}},{"cell_type":"markdown","source":"## 1.1 Skimming the Data sets","metadata":{}},{"cell_type":"code","source":"# basic structure of train data\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-13T10:47:17.407418Z","iopub.execute_input":"2021-09-13T10:47:17.408208Z","iopub.status.idle":"2021-09-13T10:47:17.445227Z","shell.execute_reply.started":"2021-09-13T10:47:17.408169Z","shell.execute_reply":"2021-09-13T10:47:17.444191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- 120 columns in train data\n\n > The dataset includes 118 features and one target variable, 'claim'.","metadata":{}},{"cell_type":"code","source":"# basic structure of train data 2\ntrain.describe().T","metadata":{"execution":{"iopub.status.busy":"2021-09-13T10:47:20.645054Z","iopub.execute_input":"2021-09-13T10:47:20.645307Z","iopub.status.idle":"2021-09-13T10:47:24.955902Z","shell.execute_reply.started":"2021-09-13T10:47:20.645278Z","shell.execute_reply":"2021-09-13T10:47:24.955027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- It is vague to understand what features are through the describe.","metadata":{}},{"cell_type":"markdown","source":"## 1.2 Cheacking the Missing Values","metadata":{}},{"cell_type":"code","source":"print(\" train data\")\nprint(f' Number of rows: {train.shape[0]}\\n Number of columns: {train.shape[1]}\\n No. of missing values: {sum(train.isna().sum())}')","metadata":{"execution":{"iopub.status.busy":"2021-09-13T11:04:01.203907Z","iopub.execute_input":"2021-09-13T11:04:01.204441Z","iopub.status.idle":"2021-09-13T11:04:01.406088Z","shell.execute_reply.started":"2021-09-13T11:04:01.204403Z","shell.execute_reply":"2021-09-13T11:04:01.40529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\" test data\")\nprint(f' Number of rows: {test.shape[0]}\\n Number of columns: {test.shape[1]}\\n No. of missing values: {sum(test.isna().sum())}')","metadata":{"execution":{"iopub.status.busy":"2021-09-13T11:04:10.167708Z","iopub.execute_input":"2021-09-13T11:04:10.168245Z","iopub.status.idle":"2021-09-13T11:04:10.274705Z","shell.execute_reply.started":"2021-09-13T11:04:10.168208Z","shell.execute_reply":"2021-09-13T11:04:10.273918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The training set has 1,820,782 missing values.\n- The testing set has 936,218 missing values.","metadata":{}},{"cell_type":"code","source":"# number of misssing values by feature\nprint(\"number of misssing values by feature\")\ntrain.isnull().sum().sort_values(ascending = False)","metadata":{"execution":{"iopub.status.busy":"2021-09-13T10:47:33.629505Z","iopub.execute_input":"2021-09-13T10:47:33.630141Z","iopub.status.idle":"2021-09-13T10:47:33.841658Z","shell.execute_reply.started":"2021-09-13T10:47:33.630105Z","shell.execute_reply":"2021-09-13T10:47:33.840807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_data missing values\nnull_values_train = []\nfor col in train.columns:\n    c = train[col].isna().sum()\n    pc = np.round((100 * (c)/len(train)), 2)            \n    dict1 ={\n        'Features' : col,\n        'null_train (count)': c,\n        'null_trian (%)': '{}%'.format(pc)\n    }\n    null_values_train.append(dict1)\nDF1 = pd.DataFrame(null_values_train, index=None).sort_values(by='null_train (count)',ascending=False)\n\n\n# test_data missing values\nnull_values_test = []\nfor col in test.columns:\n    c = test[col].isna().sum()\n    pc = np.round((100 * (c)/len(test)), 2)            \n    dict2 ={\n        'Features' : col,\n        'null_test (count)': c,\n        'null_test (%)': '{}%'.format(pc)\n    }\n    null_values_test.append(dict2)\nDF2 = pd.DataFrame(null_values_test, index=None).sort_values(by='null_test (count)',ascending=False)\n\n\ndf = pd.concat([DF1, DF2], axis=1)\ndf","metadata":{"execution":{"iopub.status.busy":"2021-09-13T10:47:39.41338Z","iopub.execute_input":"2021-09-13T10:47:39.413641Z","iopub.status.idle":"2021-09-13T10:47:39.786599Z","shell.execute_reply.started":"2021-09-13T10:47:39.413612Z","shell.execute_reply":"2021-09-13T10:47:39.785703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- It seems like every feature has approximatley same number of missing values.","metadata":{}},{"cell_type":"code","source":"df = pd.DataFrame()\ndf[\"n_missing\"] = train.drop([\"id\", \"claim\"], axis=1).isna().sum(axis=1)\ndf[\"claim\"] = train[\"claim\"].copy()\n\nfig, ax = plt.subplots(figsize=(12,5))\nax.hist(df[df[\"claim\"]==0][\"n_missing\"],\n        bins=10, edgecolor=\"black\",\n        color=\"darkseagreen\", alpha=0.7, label=\"claim is 0\")\nax.hist(df[df[\"claim\"]==1][\"n_missing\"],\n        bins=10, edgecolor=\"black\",\n        color=\"darkorange\", alpha=0.7, label=\"claim is 1\")\nax.set_title(\"Missing Values Distributionin in Each Target Class\", fontsize=20, pad=15)\nax.set_xlabel(\"Missing Values Per Row\", fontsize=14, labelpad=10)\nax.set_ylabel(\"Number of Rows\", fontsize=14, labelpad=10)\nax.legend(fontsize=14)\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-09-13T10:51:05.243019Z","iopub.execute_input":"2021-09-13T10:51:05.243634Z","iopub.status.idle":"2021-09-13T10:51:06.18344Z","shell.execute_reply.started":"2021-09-13T10:51:05.243593Z","shell.execute_reply":"2021-09-13T10:51:06.18274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The plot shows that the rows have missing values and claim = 0 is skewed to the first few rows.\n- The rows have missing values and claim = 1 are more likely distributed then claim = 0.","metadata":{}},{"cell_type":"code","source":"# looking at Claim column\nfig, ax = plt.subplots(figsize=(6, 6))\n\nbars = ax.bar(train[\"claim\"].value_counts().index,\n              train[\"claim\"].value_counts().values,              \n              edgecolor=\"black\",\n              width=0.4)\nax.set_title(\"Claim (target) values distribution\", fontsize=20, pad=15)\nax.set_ylabel(\"Amount of values\", fontsize=14, labelpad=15)\nax.set_xlabel(\"Claim (target) value\", fontsize=14, labelpad=10)\nax.set_xticks(train[\"claim\"].value_counts().index)\nax.tick_params(axis=\"both\", labelsize=14)\nax.bar_label(bars, [f\"{x:2.2f}%\" for x in train[\"claim\"].value_counts().values/(len(train)/100)],\n                 padding=5, fontsize=15)\nax.bar_label(bars, [f\"{x:2d}\" for x in train[\"claim\"].value_counts().values],\n                 padding=-30, fontsize=15)\nax.margins(0.2, 0.12)\nax.grid(axis=\"y\")\n\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-09-13T11:04:47.391814Z","iopub.execute_input":"2021-09-13T11:04:47.392377Z","iopub.status.idle":"2021-09-13T11:04:47.618881Z","shell.execute_reply.started":"2021-09-13T11:04:47.392339Z","shell.execute_reply":"2021-09-13T11:04:47.618176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Before the Nan-values are dropped, 'claim' = 0 and 1 have approximately have same number of rows.","metadata":{}},{"cell_type":"code","source":"# proportion of no null in each row\ntrain1 = train[train.isna().sum(axis=1)==0]\nprint(\"proportion of no null data : %.2f\" %(len(train1)/len(train)*100))\nprint(\"number of claim 1 in no null data : %d\" %(len(train1[train1['claim']==0])))\nprint(\"number of claim 0 in no null data : %d\" %(len(train1[train1['claim']==1])))","metadata":{"execution":{"iopub.status.busy":"2021-09-13T11:07:37.154716Z","iopub.execute_input":"2021-09-13T11:07:37.155275Z","iopub.status.idle":"2021-09-13T11:07:37.721863Z","shell.execute_reply.started":"2021-09-13T11:07:37.155236Z","shell.execute_reply":"2021-09-13T11:07:37.721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(6, 6))\n\nbars = ax.bar(train1[\"claim\"].value_counts().index,\n              train1[\"claim\"].value_counts().values,              \n              edgecolor=\"black\",\n              width=0.4)\nax.set_title(\"Claim (target) values distribution\", fontsize=20, pad=15)\nax.set_ylabel(\"Amount of values\", fontsize=14, labelpad=15)\nax.set_xlabel(\"Claim (target) value\", fontsize=14, labelpad=10)\nax.set_xticks(train1[\"claim\"].value_counts().index)\nax.tick_params(axis=\"both\", labelsize=14)\nax.bar_label(bars, [f\"{x:2.2f}%\" for x in train1[\"claim\"].value_counts().values/(len(train1)/100)],\n                 padding=5, fontsize=15)\nax.bar_label(bars, [f\"{x:2d}\" for x in train1[\"claim\"].value_counts().values],\n                 padding=-30, fontsize=15)\nax.margins(0.2, 0.12)\nax.grid(axis=\"y\")\n\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-09-13T11:07:44.839169Z","iopub.execute_input":"2021-09-13T11:07:44.839822Z","iopub.status.idle":"2021-09-13T11:07:45.058105Z","shell.execute_reply.started":"2021-09-13T11:07:44.839776Z","shell.execute_reply":"2021-09-13T11:07:45.057427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- However, if Nan-values are dropped, then proportion of 'claim' = 0 and 1 are vary different.\n- The plot tells most of missing values are located in rows where 'claim' = 1.\n- Thus, it will be inbalanced if the Nan-values are simply dropped.\n","metadata":{}},{"cell_type":"markdown","source":"## 1.3 Cheacking the Distribution of Features.","metadata":{}},{"cell_type":"code","source":"target = train.pop('claim')","metadata":{"execution":{"iopub.status.busy":"2021-09-13T11:09:41.02727Z","iopub.execute_input":"2021-09-13T11:09:41.027573Z","iopub.status.idle":"2021-09-13T11:09:41.035431Z","shell.execute_reply.started":"2021-09-13T11:09:41.027533Z","shell.execute_reply":"2021-09-13T11:09:41.034752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ = train[0:9579]\ntest_ = test[0:4934]","metadata":{"execution":{"iopub.status.busy":"2021-09-13T11:09:45.377898Z","iopub.execute_input":"2021-09-13T11:09:45.378852Z","iopub.status.idle":"2021-09-13T11:09:45.383926Z","shell.execute_reply.started":"2021-09-13T11:09:45.378802Z","shell.execute_reply":"2021-09-13T11:09:45.383152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# distribution of Features f1 to f60\nL = len(train.columns[0:60])\nnrow= int(np.ceil(L/6))\nncol= 6\n\nremove_last= (nrow * ncol) - L\n\nfig, ax = plt.subplots(nrow, ncol,figsize=(24, 30))\n#ax.flat[-remove_last].set_visible(False)\nfig.subplots_adjust(top=0.95)\ni = 1\nfor feature in train.columns[0:60]:\n    plt.subplot(nrow, ncol, i)\n    ax = sns.kdeplot(train_[feature], shade=True, color='cyan',  alpha=0.5, label='train')\n    ax = sns.kdeplot(test_[feature], shade=True, color='darkblue',  alpha=0.5, label='test')\n    plt.xlabel(feature, fontsize=9)\n    plt.legend()\n    i += 1\nplt.suptitle('DistPlot: train & test data', fontsize=20)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-13T11:14:11.962652Z","iopub.execute_input":"2021-09-13T11:14:11.963535Z","iopub.status.idle":"2021-09-13T11:14:25.156063Z","shell.execute_reply.started":"2021-09-13T11:14:11.963487Z","shell.execute_reply":"2021-09-13T11:14:25.155388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# distribution of Features f61 to f118\nL = len(train.columns[60:])\nnrow= int(np.ceil(L/6))\nncol= 6\n\nremove_last= (nrow * ncol) - L\n\nfig, ax = plt.subplots(nrow, ncol,figsize=(24, 30))\n#ax.flat[-remove_last].set_visible(False)\nfig.subplots_adjust(top=0.95)\ni = 1\nfor feature in train.columns[60:]:\n    plt.subplot(nrow, ncol, i)\n    ax = sns.kdeplot(train_[feature], shade=True, color='cyan',  alpha=0.5, label='train')\n    ax = sns.kdeplot(test_[feature], shade=True, color='darkblue',  alpha=0.5, label='test')\n    plt.xlabel(feature, fontsize=9)\n    plt.legend()\n    i += 1\nplt.suptitle('DistPlot: train & test data', fontsize=20)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-13T11:14:44.997224Z","iopub.execute_input":"2021-09-13T11:14:44.997485Z","iopub.status.idle":"2021-09-13T11:14:58.158287Z","shell.execute_reply.started":"2021-09-13T11:14:44.997457Z","shell.execute_reply":"2021-09-13T11:14:58.157447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Features in both traing and testing sets have similar distribution.\n- Thus, it is expected that the same imputation is going to be worked for both training snd testing sets.","metadata":{}},{"cell_type":"markdown","source":"## 1.4 Cheacking the Box-plots","metadata":{}},{"cell_type":"code","source":"# outlier of train data\ndf_plot = ((train - train.min())/(train.max() - train.min()))\nfig, ax = plt.subplots(4, 1, figsize = (25,25))\nsns.boxplot(data = df_plot.iloc[:, 1:30], ax = ax[0])\nsns.boxplot(data = df_plot.iloc[:, 30:60], ax = ax[1])\nsns.boxplot(data = df_plot.iloc[:, 60:90], ax = ax[2])\nsns.boxplot(data = df_plot.iloc[:, 90:120], ax = ax[3])","metadata":{"execution":{"iopub.status.busy":"2021-09-13T11:17:06.852106Z","iopub.execute_input":"2021-09-13T11:17:06.852394Z","iopub.status.idle":"2021-09-13T11:17:32.034253Z","shell.execute_reply.started":"2021-09-13T11:17:06.852364Z","shell.execute_reply":"2021-09-13T11:17:32.033517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# outlier of test data\ndf_plot = ((test - test.min())/(test.max() - test.min()))\nfig, ax = plt.subplots(4, 1, figsize = (25,25))\nsns.boxplot(data = df_plot.iloc[:, 1:30], ax = ax[0])\nsns.boxplot(data = df_plot.iloc[:, 30:60], ax = ax[1])\nsns.boxplot(data = df_plot.iloc[:, 60:90], ax = ax[2])\nsns.boxplot(data = df_plot.iloc[:, 90:119], ax = ax[3])","metadata":{"execution":{"iopub.status.busy":"2021-09-13T11:17:37.341063Z","iopub.execute_input":"2021-09-13T11:17:37.341337Z","iopub.status.idle":"2021-09-13T11:17:51.491924Z","shell.execute_reply.started":"2021-09-13T11:17:37.341308Z","shell.execute_reply":"2021-09-13T11:17:51.491119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Boxplots show that both training and testing sets are similarly distributed.","metadata":{}},{"cell_type":"code","source":"# correlation of train\ncorr = train.corr()\nmask = np.triu(np.ones_like(corr, dtype = bool))\n\nplt.figure(figsize = (15, 15))\nplt.title('Corelation matrix')\nsns.heatmap(corr, mask = mask, cmap = 'Spectral_r', linewidths = .5)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-13T11:21:04.896823Z","iopub.execute_input":"2021-09-13T11:21:04.897384Z","iopub.status.idle":"2021-09-13T11:21:39.23096Z","shell.execute_reply.started":"2021-09-13T11:21:04.897344Z","shell.execute_reply":"2021-09-13T11:21:39.230259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# correlation of train\ncorr = test.corr()\nmask = np.triu(np.ones_like(corr, dtype = bool))\n\nplt.figure(figsize = (15, 15))\nplt.title('Corelation matrix')\nsns.heatmap(corr, mask = mask, cmap = 'Spectral_r', linewidths = .5)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-13T11:21:39.232547Z","iopub.execute_input":"2021-09-13T11:21:39.23288Z","iopub.status.idle":"2021-09-13T11:21:57.947703Z","shell.execute_reply.started":"2021-09-13T11:21:39.232843Z","shell.execute_reply":"2021-09-13T11:21:57.946295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The correlation between the two data are also similar.\n- Overall, every feature in both training and testing sets are vary similar.","metadata":{}}]}