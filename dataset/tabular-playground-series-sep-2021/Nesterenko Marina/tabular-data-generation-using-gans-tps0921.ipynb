{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.metrics import precision_score, recall_score, f1_score,\\\n                            accuracy_score, classification_report, confusion_matrix\nfrom sklearn.model_selection import KFold, GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n#from imblearn.over_sampling import SMOTE\n#from imblearn.pipeline import Pipeline\n\nimport os\nfrom collections import Counter\n\nnp.random.seed(34)\npath = '../input/tabular-playground-series-sep-2021/'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-07T15:26:53.782047Z","iopub.execute_input":"2021-09-07T15:26:53.782406Z","iopub.status.idle":"2021-09-07T15:26:53.795269Z","shell.execute_reply.started":"2021-09-07T15:26:53.782349Z","shell.execute_reply":"2021-09-07T15:26:53.792883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Exploration and Cleaning","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\n\ndataset_train1 = pd.read_csv(f'{path}train.csv', index_col='id')\n#dataset_test1 = pd.read_csv(f'{path}test.csv', index_col='id')\n\ny = dataset_train1.claim\ny = pd.DataFrame(y)\n#dataset_train = dataset_train1.drop(['claim'], axis=1)\ndataset_train = dataset_train1.copy()\n\ndataset_train['nan'] = dataset_train.isnull().sum(axis=1)\ndataset_train['nan'] = dataset_train['nan']/dataset_train['nan'].max()\n\n#dataset_test1['nan'] = dataset_test1.isnull().sum(axis=1)\n#dataset_test1['nan'] = dataset_test1['nan']/dataset_test1['nan'].max()","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2021-09-07T15:26:53.802259Z","iopub.execute_input":"2021-09-07T15:26:53.802959Z","iopub.status.idle":"2021-09-07T15:27:13.984228Z","shell.execute_reply.started":"2021-09-07T15:26:53.80289Z","shell.execute_reply":"2021-09-07T15:27:13.983378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_train3 = dataset_train1.drop(['claim'], axis=1)\ndataset_train3['nan'] = dataset_train3.isnull().sum(axis=1)\ndataset_train3['nan'] = dataset_train3['nan']/dataset_train3['nan'].max()","metadata":{"execution":{"iopub.status.busy":"2021-09-07T15:27:13.986911Z","iopub.execute_input":"2021-09-07T15:27:13.987497Z","iopub.status.idle":"2021-09-07T15:27:14.593038Z","shell.execute_reply.started":"2021-09-07T15:27:13.987439Z","shell.execute_reply":"2021-09-07T15:27:14.592247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import QuantileTransformer, KBinsDiscretizer\n\nfrom sklearn.impute import SimpleImputer\nimputer = SimpleImputer(strategy='mean')\ndataset_train = imputer.fit_transform(dataset_train)\n#dataset_test = imputer.transform(dataset_test1)","metadata":{"execution":{"iopub.status.busy":"2021-09-07T15:27:14.595937Z","iopub.execute_input":"2021-09-07T15:27:14.596522Z","iopub.status.idle":"2021-09-07T15:27:19.007928Z","shell.execute_reply.started":"2021-09-07T15:27:14.596455Z","shell.execute_reply":"2021-09-07T15:27:19.006678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nx_scaler = MinMaxScaler()\ndataset_train_sc = x_scaler.fit_transform(dataset_train)\n#dataset_test_sc = x_scaler.transform(dataset_test1)","metadata":{"execution":{"iopub.status.busy":"2021-09-07T15:27:19.012204Z","iopub.execute_input":"2021-09-07T15:27:19.012707Z","iopub.status.idle":"2021-09-07T15:27:20.507611Z","shell.execute_reply.started":"2021-09-07T15:27:19.012485Z","shell.execute_reply":"2021-09-07T15:27:20.506758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N_split = int(0.2 * len(dataset_train_sc))\nX_train = dataset_train_sc[:-N_split, :]\nX_test = dataset_train_sc[-N_split:, :]\ny_train = y[:-N_split]\ny_test = y[-N_split:]","metadata":{"execution":{"iopub.status.busy":"2021-09-07T15:27:20.511605Z","iopub.execute_input":"2021-09-07T15:27:20.512249Z","iopub.status.idle":"2021-09-07T15:27:20.52174Z","shell.execute_reply.started":"2021-09-07T15:27:20.511911Z","shell.execute_reply":"2021-09-07T15:27:20.52032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Using GANs to generate new data","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply, Concatenate\nfrom tensorflow.keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D, LeakyReLU\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.optimizers import Adam, RMSprop\nfrom tensorflow.keras.initializers import RandomNormal\nimport tensorflow.keras.backend as K\nfrom sklearn.utils import shuffle","metadata":{"execution":{"iopub.status.busy":"2021-09-07T15:27:20.524071Z","iopub.execute_input":"2021-09-07T15:27:20.524803Z","iopub.status.idle":"2021-09-07T15:27:26.848203Z","shell.execute_reply.started":"2021-09-07T15:27:20.524632Z","shell.execute_reply":"2021-09-07T15:27:26.847266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class cGAN():\n    def __init__(self):\n        self.latent_dim = 120\n        self.out_shape = 120\n        self.num_classes = 2\n        self.clip_value = 0.01\n        #optimizer = Adam(0.00001)\n        optimizer = Adam(0.00001, 0.5)\n        #optimizer = RMSprop(lr=0.00005)\n\n        # build discriminator\n        self.discriminator = self.build_discriminator()\n        self.discriminator.compile(loss=['binary_crossentropy'],\n                                   optimizer=optimizer,\n                                   metrics=['accuracy'])\n\n        # build generator\n        self.generator = self.build_generator()\n\n        # generating new data samples\n        noise = Input(shape=(self.latent_dim,))\n        label = Input(shape=(1,))\n        gen_samples = self.generator([noise, label])\n\n        self.discriminator.trainable = False\n\n        # passing gen samples through disc. \n        valid = self.discriminator([gen_samples, label])\n\n        # combining both models\n        self.combined = Model([noise, label], valid)\n        self.combined.compile(loss=['binary_crossentropy'],\n                              optimizer=optimizer,\n                             metrics=['accuracy'])\n        self.combined.summary()\n\n    def wasserstein_loss(self, y_true, y_pred):\n        return K.mean(y_true * y_pred)\n\n    def build_generator(self):\n        init = RandomNormal(mean=0.0, stddev=0.02)\n        model = Sequential()\n\n        model.add(Dense(64, input_dim=self.latent_dim))\n        #model.add(Dropout(0.2))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n\n        model.add(Dense(128))\n        #model.add(Dropout(0.2))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n\n        model.add(Dense(256))\n        #model.add(Dropout(0.2))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n\n        model.add(Dense(self.out_shape, activation='tanh'))\n        model.summary()\n\n        noise = Input(shape=(self.latent_dim,))\n        label = Input(shape=(1,), dtype='int32')\n        label_embedding = Flatten()(Embedding(self.num_classes, self.latent_dim)(label))\n        \n        model_input = multiply([noise, label_embedding])\n        gen_sample = model(model_input)\n\n        return Model([noise, label], gen_sample, name=\"Generator\")\n\n    \n    def build_discriminator(self):\n        init = RandomNormal(mean=0.0, stddev=0.02)\n        model = Sequential()\n\n        model.add(Dense(256, input_dim=self.out_shape, kernel_initializer=init))\n        model.add(LeakyReLU(alpha=0.2))\n        \n        model.add(Dense(128, kernel_initializer=init))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.3))\n        \n        model.add(Dense(64, kernel_initializer=init))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.3))\n        \n        model.add(Dense(1, activation='sigmoid'))\n        model.summary()\n        \n        gen_sample = Input(shape=(self.out_shape,))\n        label = Input(shape=(1,), dtype='int32')\n        label_embedding = Flatten()(Embedding(self.num_classes, self.out_shape)(label))\n\n        model_input = multiply([gen_sample, label_embedding])\n        validity = model(model_input)\n\n        return Model(inputs=[gen_sample, label], outputs=validity, name=\"Discriminator\")\n\n\n    def train(self, X_train, y_train, pos_index, neg_index, epochs, batch_size=32, sample_interval=50):\n\n        # Adversarial ground truths\n        valid = np.ones((batch_size, 1))\n        fake = np.zeros((batch_size, 1))\n\n        for epoch in range(epochs):\n            \n            #  Train Discriminator with 8 sample from postivite class and rest with negative class\n            idx1 = np.random.choice(pos_index, 8)\n            idx0 = np.random.choice(neg_index, batch_size-8)\n            idx = np.concatenate((idx1, idx0))\n            samples, labels = X_train[idx], y_train[idx]\n            samples, labels = shuffle(samples, labels)\n            # Sample noise as generator input\n            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n\n            # Generate a half batch of new images\n            gen_samples = self.generator.predict([noise, labels])\n\n            # label smoothing\n            if epoch < epochs//1.5:\n                valid_smooth = (valid+0.1)-(np.random.random(valid.shape)*0.1)\n                fake_smooth = (fake-0.1)+(np.random.random(fake.shape)*0.1)\n            else:\n                valid_smooth = valid \n                fake_smooth = fake\n                \n            # Train the discriminator\n            self.discriminator.trainable = True\n            d_loss_real = self.discriminator.train_on_batch([samples, labels], valid_smooth)\n            d_loss_fake = self.discriminator.train_on_batch([gen_samples, labels], fake_smooth)\n            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n\n            # Train Generator\n            # Condition on labels\n            self.discriminator.trainable = False\n            sampled_labels = np.random.randint(0, 2, batch_size).reshape(-1, 1)\n            # Train the generator\n            g_loss = self.combined.train_on_batch([noise, sampled_labels], valid)\n\n            # Plot the progress\n            if (epoch+1)%sample_interval==0:\n                print (f\"{epoch} [D loss: {d_loss[0]}, acc.: {100*d_loss[1]}] [G loss: {g_loss}]\")","metadata":{"execution":{"iopub.status.busy":"2021-09-07T15:33:46.816249Z","iopub.execute_input":"2021-09-07T15:33:46.816649Z","iopub.status.idle":"2021-09-07T15:33:46.858198Z","shell.execute_reply.started":"2021-09-07T15:33:46.816575Z","shell.execute_reply":"2021-09-07T15:33:46.856597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train = np.array(y_train)\ny_train","metadata":{"execution":{"iopub.status.busy":"2021-09-07T15:27:26.903878Z","iopub.execute_input":"2021-09-07T15:27:26.904478Z","iopub.status.idle":"2021-09-07T15:27:26.926709Z","shell.execute_reply.started":"2021-09-07T15:27:26.90425Z","shell.execute_reply":"2021-09-07T15:27:26.925594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train = y_train.reshape(-1,1)\npos_index = np.where(y_train==1)[0]\nneg_index = np.where(y_train==0)[0]","metadata":{"execution":{"iopub.status.busy":"2021-09-07T15:27:26.928451Z","iopub.execute_input":"2021-09-07T15:27:26.928856Z","iopub.status.idle":"2021-09-07T15:27:26.959603Z","shell.execute_reply.started":"2021-09-07T15:27:26.928797Z","shell.execute_reply":"2021-09-07T15:27:26.958673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cgan = cGAN()","metadata":{"execution":{"iopub.status.busy":"2021-09-07T15:33:50.220431Z","iopub.execute_input":"2021-09-07T15:33:50.220799Z","iopub.status.idle":"2021-09-07T15:33:51.004961Z","shell.execute_reply.started":"2021-09-07T15:33:50.220742Z","shell.execute_reply":"2021-09-07T15:33:51.004112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cgan.train(X_train, y_train, pos_index, neg_index, epochs=10000)","metadata":{"execution":{"iopub.status.busy":"2021-09-07T15:33:53.533135Z","iopub.execute_input":"2021-09-07T15:33:53.533563Z","iopub.status.idle":"2021-09-07T15:36:59.903288Z","shell.execute_reply.started":"2021-09-07T15:33:53.533477Z","shell.execute_reply":"2021-09-07T15:36:59.901854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# generating new samples\nnoise = np.random.normal(0, 1, (200000, 120))\nsampled_labels = np.ones(200000).reshape(-1, 1)\n\ngen_samples = cgan.generator.predict([noise, sampled_labels])\ngen_samples = x_scaler.inverse_transform(gen_samples)\nprint(gen_samples.shape)","metadata":{"execution":{"iopub.status.busy":"2021-09-07T15:32:26.483091Z","iopub.status.idle":"2021-09-07T15:32:26.484116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(dataset_train3.columns)","metadata":{"execution":{"iopub.status.busy":"2021-09-07T15:32:26.486197Z","iopub.status.idle":"2021-09-07T15:32:26.487122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(len(gen_samples)):\n    gen_samples[i,-1] = 1 if gen_samples[i,-1] >= 0.5 else 0","metadata":{"execution":{"iopub.status.busy":"2021-09-07T15:32:26.488797Z","iopub.status.idle":"2021-09-07T15:32:26.48968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gen_samples[210:350,-1]\n","metadata":{"execution":{"iopub.status.busy":"2021-09-07T15:32:26.491332Z","iopub.status.idle":"2021-09-07T15:32:26.492203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gen_samples[-1,:]","metadata":{"execution":{"iopub.status.busy":"2021-09-07T15:32:26.49386Z","iopub.status.idle":"2021-09-07T15:32:26.494903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_train1 = pd.read_csv(f'{path}train.csv', index_col='id')","metadata":{"execution":{"iopub.status.busy":"2021-09-07T15:32:26.496793Z","iopub.status.idle":"2021-09-07T15:32:26.497708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols = [f'{i}' for i in range(118)]\ncols.append('nan')\ncols.append('claim')","metadata":{"execution":{"iopub.status.busy":"2021-09-07T15:32:26.499367Z","iopub.status.idle":"2021-09-07T15:32:26.500234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gen_df = pd.DataFrame(data = gen_samples, columns=cols)\ngen_df","metadata":{"execution":{"iopub.status.busy":"2021-09-07T15:32:26.501834Z","iopub.status.idle":"2021-09-07T15:32:26.50263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(gen_df)","metadata":{"execution":{"iopub.status.busy":"2021-09-07T15:32:26.504608Z","iopub.status.idle":"2021-09-07T15:32:26.505536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = 'gen_dataset.csv'\ngen_df.to_csv(path, index=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-07T15:32:26.507131Z","iopub.status.idle":"2021-09-07T15:32:26.507949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"noise = np.random.normal(0, 1, (200000, 120))\nnoise\n","metadata":{"execution":{"iopub.status.busy":"2021-09-07T15:32:26.50965Z","iopub.status.idle":"2021-09-07T15:32:26.510535Z"},"trusted":true},"execution_count":null,"outputs":[]}]}