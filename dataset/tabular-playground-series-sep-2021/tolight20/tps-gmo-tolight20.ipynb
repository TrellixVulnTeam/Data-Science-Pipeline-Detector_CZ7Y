{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-09T12:41:40.98771Z","iopub.execute_input":"2021-09-09T12:41:40.988032Z","iopub.status.idle":"2021-09-09T12:41:41.001747Z","shell.execute_reply.started":"2021-09-09T12:41:40.988002Z","shell.execute_reply":"2021-09-09T12:41:41.000429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sklearn\nsklearn.__version__","metadata":{"execution":{"iopub.status.busy":"2021-09-08T04:55:24.632734Z","iopub.execute_input":"2021-09-08T04:55:24.633153Z","iopub.status.idle":"2021-09-08T04:55:24.639309Z","shell.execute_reply.started":"2021-09-08T04:55:24.633117Z","shell.execute_reply":"2021-09-08T04:55:24.638109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.experimental import enable_iterative_imputer\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\n\nfrom xgboost import XGBRegressor\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMRegressor\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostRegressor\nfrom catboost import CatBoostClassifier","metadata":{"execution":{"iopub.status.busy":"2021-09-09T12:41:43.772597Z","iopub.execute_input":"2021-09-09T12:41:43.772902Z","iopub.status.idle":"2021-09-09T12:41:43.778825Z","shell.execute_reply.started":"2021-09-09T12:41:43.772872Z","shell.execute_reply":"2021-09-09T12:41:43.776971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"../input/tabular-playground-series-sep-2021/train.csv\")\ntest = pd.read_csv(\"../input/tabular-playground-series-sep-2021/test.csv\")\nsample_solution = pd.read_csv(\"../input/tabular-playground-series-sep-2021/sample_solution.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-09-09T11:13:09.733804Z","iopub.execute_input":"2021-09-09T11:13:09.734063Z","iopub.status.idle":"2021-09-09T11:13:50.569713Z","shell.execute_reply.started":"2021-09-09T11:13:09.734037Z","shell.execute_reply":"2021-09-09T11:13:50.56882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_params = {\n    'n_estimators' : 3600,\n    'reg_lambda' : 3,\n    'reg_alpha' : 26,\n    'subsample' : 0.6000000000000001,\n    'colsample_bytree' : 0.6000000000000001,\n    'max_depth' : 9,\n    'min_child_weight' : 5,\n    'gamma' : 13.054739572819486,\n    'learning_rate': 0.01,\n    'tree_method': 'gpu_hist',\n    'booster': 'gbtree'\n}\n\nlgbm_params = {\n    \"objective\": \"binary\",\n    \"learning_rate\": 0.008,\n    'device': 'gpu',\n    'n_estimators': 3205,\n    'num_leaves': 184,\n    'min_child_samples': 63,\n    'feature_fraction': 0.6864594334728974,\n    'bagging_fraction': 0.9497327922401265,\n    'bagging_freq': 1,\n    'reg_alpha': 19,\n    'reg_lambda': 19,\n    'gpu_platform_id': 0,\n    'gpu_device_id': 0,\n    'verbose' : -1\n}\n\ncatb_params = {\n    'iterations': 15585, \n    'objective': 'CrossEntropy', \n    'bootstrap_type': 'Bernoulli', \n    'od_wait': 1144, \n    'learning_rate': 0.023575206684596582, \n    'reg_lambda': 36.30433203563295, \n    'random_strength': 43.75597655616195, \n    'depth': 7, \n    'min_data_in_leaf': 11, \n    'leaf_estimation_iterations': 1, \n    'subsample': 0.8227911142845009,\n    'task_type' : 'GPU',\n    'devices' : '0',\n    'verbose' : 0\n}","metadata":{"execution":{"iopub.status.busy":"2021-09-09T12:42:53.326073Z","iopub.execute_input":"2021-09-09T12:42:53.326437Z","iopub.status.idle":"2021-09-09T12:42:53.335206Z","shell.execute_reply.started":"2021-09-09T12:42:53.326407Z","shell.execute_reply":"2021-09-09T12:42:53.334056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# quantile_________________________________________________________________________","metadata":{}},{"cell_type":"code","source":"# quantile + median dataset\nquantile_train = train.copy()\n\nmedian_set = ['f9', 'f12', 'f26', 'f27', 'f28', 'f32', 'f33', 'f35', 'f62', 'f74', 'f82', 'f86', 'f98', 'f108', 'f116']\n\nfor f in range(1,119):\n    col_name = f'f{f}'\n    \n    if col_name in median_set:\n        quantile_train[col_name].fillna(train[col_name].median(), inplace=True)\n    else:\n        quantile_train[col_name].fillna(train[col_name].quantile(0.75), inplace=True)\n\ndrop_set = ['id', 'claim']\nX = quantile_train.drop(drop_set, axis = 1)\nY = quantile_train['claim']\n# ---------------------------------------------------------------------- data setting\nquantile_train = None","metadata":{"execution":{"iopub.status.busy":"2021-09-09T12:41:49.183635Z","iopub.execute_input":"2021-09-09T12:41:49.183945Z","iopub.status.idle":"2021-09-09T12:41:51.623189Z","shell.execute_reply.started":"2021-09-09T12:41:49.183916Z","shell.execute_reply":"2021-09-09T12:41:51.6223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# xgboost\nmodel = XGBClassifier(**xgb_params)\ntrain_oof = np.zeros((957919,))\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, Y)):\n    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_train, y_val = Y.iloc[train_idx], Y.iloc[val_idx]\n    \n    model = model.fit(X_train, y_train, verbose=0)\n    temp_oof = model.predict_proba(X_val)[:, 1]\n    train_oof[val_idx] = temp_oof\n    print(f'Fold {fold} AUC : ', roc_auc_score(y_val, temp_oof))\n    \nprint(f'XgBoost OOF AUC : ', roc_auc_score(Y, train_oof))\n\n# lgbm\nmodel = LGBMClassifier(**lgbm_params)\ntrain_oof = np.zeros((957919,))\nskf = StratifiedKFold(shuffle=True, random_state=42)\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, Y)):\n    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_train, y_val = Y.iloc[train_idx], Y.iloc[val_idx]\n    \n    model = model.fit(X_train, y_train, verbose=0)\n    temp_oof = model.predict_proba(X_val)[:, 1]\n    train_oof[val_idx] = temp_oof\n    print(f'Fold {fold} AUC : ', roc_auc_score(y_val, temp_oof))\n    \nprint(f'LGBM OOF AUC : ', roc_auc_score(Y, train_oof))\n\n# catboost\nmodel = CatBoostClassifier(**catb_params)\ntrain_oof = np.zeros((957919,))\nskf = StratifiedKFold(shuffle=True, random_state=42)\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, Y)):\n    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_train, y_val = Y.iloc[train_idx], Y.iloc[val_idx]\n    \n    model = model.fit(X_train, y_train, verbose=0)\n    temp_oof = model.predict_proba(X_val)[:, 1]\n    train_oof[val_idx] = temp_oof\n    print(f'Fold {fold} AUC : ', roc_auc_score(y_val, temp_oof))\n    \nprint(f'Catboost OOF AUC : ', roc_auc_score(Y, train_oof))","metadata":{"execution":{"iopub.status.busy":"2021-09-09T12:44:15.01505Z","iopub.execute_input":"2021-09-09T12:44:15.015395Z","iopub.status.idle":"2021-09-09T12:45:22.258648Z","shell.execute_reply.started":"2021-09-09T12:44:15.015364Z","shell.execute_reply":"2021-09-09T12:45:22.251479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Quantile + missing setting--------------------------","metadata":{}},{"cell_type":"code","source":"# new_feature : null has\n# \nquantile_train = train.copy()\nmedian_set = ['f9', 'f12', 'f26', 'f27', 'f28', 'f32', 'f33', 'f35', 'f62', 'f74', 'f82', 'f86', 'f98', 'f108', 'f116']\n\nquantile_train['n_missing'] = train.isnull().sum(axis = 1)  \n\n    \n# Fill Null by using quantile + median\nfor f in range(1,119):\n    col_name = f'f{f}'\n    \n    if col_name in median_set:\n        quantile_train[col_name].fillna(train[col_name].median(), inplace=True)\n    else:\n        quantile_train[col_name].fillna(train[col_name].quantile(0.75), inplace=True)\n\n\n    \ndrop_set = ['id', 'claim']\nX = quantile_train.drop(drop_set, axis = 1)\nY = quantile_train['claim']\n# ---------------------------------------------------------------------- data setting\nquantile_train = None","metadata":{"execution":{"iopub.status.busy":"2021-09-09T09:35:44.309697Z","iopub.execute_input":"2021-09-09T09:35:44.310125Z","iopub.status.idle":"2021-09-09T09:35:44.392034Z","shell.execute_reply.started":"2021-09-09T09:35:44.310041Z","shell.execute_reply":"2021-09-09T09:35:44.389987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# xgboost\nmodel = XGBClassifier(**xgb_params)\ntrain_oof = np.zeros((957919,))\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, Y)):\n    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_train, y_val = Y.iloc[train_idx], Y.iloc[val_idx]\n    \n    model = model.fit(X_train, y_train, verbose=0)\n    temp_oof = model.predict_proba(X_val)[:, 1]\n    train_oof[val_idx] = temp_oof\n    print(f'Fold {fold} AUC : ', roc_auc_score(y_val, temp_oof))\n    \nprint(f'XgBoost OOF AUC : ', roc_auc_score(Y, train_oof))\n\n# lgbm\nmodel = LGBMClassifier(**lgbm_params)\ntrain_oof = np.zeros((957919,))\nskf = StratifiedKFold(shuffle=True, random_state=42)\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, Y)):\n    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_train, y_val = Y.iloc[train_idx], Y.iloc[val_idx]\n    \n    model = model.fit(X_train, y_train, verbose=0)\n    temp_oof = model.predict_proba(X_val)[:, 1]\n    train_oof[val_idx] = temp_oof\n    print(f'Fold {fold} AUC : ', roc_auc_score(y_val, temp_oof))\n    \nprint(f'LGBM OOF AUC : ', roc_auc_score(Y, train_oof))\n\n# catboost\nmodel = CatBoostClassifier(**catb_params)\ntrain_oof = np.zeros((957919,))\nskf = StratifiedKFold(shuffle=True, random_state=42)\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, Y)):\n    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_train, y_val = Y.iloc[train_idx], Y.iloc[val_idx]\n    \n    model = model.fit(X_train, y_train, verbose=0)\n    temp_oof = model.predict_proba(X_val)[:, 1]\n    train_oof[val_idx] = temp_oof\n    print(f'Fold {fold} AUC : ', roc_auc_score(y_val, temp_oof))\n    \nprint(f'Catboost OOF AUC : ', roc_auc_score(Y, train_oof))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# quantile + multiply setting -----------------------------------------","metadata":{}},{"cell_type":"code","source":"# new_feature : multiply all features\nfeatures = list(train.columns[1:119])\n\nquantile_train = train.copy()\nquantile_train['multiply'] = 1\n\nfor feature in features:\n    quantile_train['multiply'] = quantile_train[feature] * quantile_train['multiply']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# xgboost\nmodel = XGBClassifier(**xgb_params)\ntrain_oof = np.zeros((957919,))\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, Y)):\n    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_train, y_val = Y.iloc[train_idx], Y.iloc[val_idx]\n    \n    model = model.fit(X_train, y_train, verbose=0)\n    temp_oof = model.predict_proba(X_val)[:, 1]\n    train_oof[val_idx] = temp_oof\n    print(f'Fold {fold} AUC : ', roc_auc_score(y_val, temp_oof))\n    \nprint(f'XgBoost OOF AUC : ', roc_auc_score(Y, train_oof))\n\n# lgbm\nmodel = LGBMClassifier(**lgbm_params)\ntrain_oof = np.zeros((957919,))\nskf = StratifiedKFold(shuffle=True, random_state=42)\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, Y)):\n    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_train, y_val = Y.iloc[train_idx], Y.iloc[val_idx]\n    \n    model = model.fit(X_train, y_train, verbose=0)\n    temp_oof = model.predict_proba(X_val)[:, 1]\n    train_oof[val_idx] = temp_oof\n    print(f'Fold {fold} AUC : ', roc_auc_score(y_val, temp_oof))\n    \nprint(f'LGBM OOF AUC : ', roc_auc_score(Y, train_oof))\n\n# catboost\nmodel = CatBoostClassifier(**catb_params)\ntrain_oof = np.zeros((957919,))\nskf = StratifiedKFold(shuffle=True, random_state=42)\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, Y)):\n    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_train, y_val = Y.iloc[train_idx], Y.iloc[val_idx]\n    \n    model = model.fit(X_train, y_train, verbose=0)\n    temp_oof = model.predict_proba(X_val)[:, 1]\n    train_oof[val_idx] = temp_oof\n    print(f'Fold {fold} AUC : ', roc_auc_score(y_val, temp_oof))\n    \nprint(f'Catboost OOF AUC : ', roc_auc_score(Y, train_oof))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# mean + missing ------------------------------------------------------------","metadata":{}},{"cell_type":"code","source":"# mean filled data\nfeatures = list(train.columns[1:119])\nmean_data = train.copy()\n\nmean_data['n_missing'] = mean_data[features].isna().sum(axis=1)\n\nmean_data = train.fillna(train.mean())\n\n# mean_data['min'] = mean_data[features].min(axis=1)\n# mean_data['max'] = mean_data[features].max(axis=1)\n# mean_data['mean'] = mean_data[features].mean(axis=1)\n# mean_data['std'] = mean_data[features].std(axis=1)\n\n\ndrop_set = ['id', 'claim']\nX = mean_data.drop(drop_set, axis = 1)\nY = mean_data['claim']\n# ----------------------------------------------------------------------------------------data seting\n\nmean_data = None","metadata":{"execution":{"iopub.status.busy":"2021-09-09T12:20:30.152577Z","iopub.execute_input":"2021-09-09T12:20:30.15301Z","iopub.status.idle":"2021-09-09T12:20:31.847693Z","shell.execute_reply.started":"2021-09-09T12:20:30.15297Z","shell.execute_reply":"2021-09-09T12:20:31.846842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# xgboost\nmodel = XGBClassifier(**xgb_params)\ntrain_oof = np.zeros((957919,))\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, Y)):\n    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_train, y_val = Y.iloc[train_idx], Y.iloc[val_idx]\n    \n    model = model.fit(X_train, y_train, verbose=0)\n    temp_oof = model.predict_proba(X_val)[:, 1]\n    train_oof[val_idx] = temp_oof\n    print(f'Fold {fold} AUC : ', roc_auc_score(y_val, temp_oof))\n    \nprint(f'XgBoost OOF AUC : ', roc_auc_score(Y, train_oof))\n\n# lgbm\nmodel = LGBMClassifier(**lgbm_params)\ntrain_oof = np.zeros((957919,))\nskf = StratifiedKFold(shuffle=True, random_state=42)\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, Y)):\n    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_train, y_val = Y.iloc[train_idx], Y.iloc[val_idx]\n    \n    model = model.fit(X_train, y_train, verbose=0)\n    temp_oof = model.predict_proba(X_val)[:, 1]\n    train_oof[val_idx] = temp_oof\n    print(f'Fold {fold} AUC : ', roc_auc_score(y_val, temp_oof))\n    \nprint(f'LGBM OOF AUC : ', roc_auc_score(Y, train_oof))\n\n# catboost\nmodel = CatBoostClassifier(**catb_params)\ntrain_oof = np.zeros((957919,))\nskf = StratifiedKFold(shuffle=True, random_state=42)\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, Y)):\n    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_train, y_val = Y.iloc[train_idx], Y.iloc[val_idx]\n    \n    model = model.fit(X_train, y_train, verbose=0)\n    temp_oof = model.predict_proba(X_val)[:, 1]\n    train_oof[val_idx] = temp_oof\n    print(f'Fold {fold} AUC : ', roc_auc_score(y_val, temp_oof))\n    \nprint(f'Catboost OOF AUC : ', roc_auc_score(Y, train_oof))","metadata":{"execution":{"iopub.status.busy":"2021-09-09T12:20:34.398842Z","iopub.execute_input":"2021-09-09T12:20:34.399175Z","iopub.status.idle":"2021-09-09T12:41:30.712424Z","shell.execute_reply.started":"2021-09-09T12:20:34.399144Z","shell.execute_reply":"2021-09-09T12:41:30.71108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# mean + all feature setting ------------------------------","metadata":{}},{"cell_type":"code","source":"# mean filled data\nfeatures = list(train.columns[1:119])\nmean_data = train.copy()\n\nmean_data['n_missing'] = mean_data[features].isna().sum(axis=1)\n\nmean_data = train.fillna(train.mean())\n\nmean_data['min'] = mean_data[features].min(axis=1)\nmean_data['max'] = mean_data[features].max(axis=1)\nmean_data['mean'] = mean_data[features].mean(axis=1)\nmean_data['std'] = mean_data[features].std(axis=1)\n\n\ndrop_set = ['id', 'claim']\nX = mean_data.drop(drop_set, axis = 1)\nY = mean_data['claim']\n# ----------------------------------------------------------------------------------------data seting\n\nmean_data = None","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# xgboost\nmodel = XGBClassifier(**xgb_params)\ntrain_oof = np.zeros((957919,))\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, Y)):\n    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_train, y_val = Y.iloc[train_idx], Y.iloc[val_idx]\n    \n    model = model.fit(X_train, y_train, verbose=0)\n    temp_oof = model.predict_proba(X_val)[:, 1]\n    train_oof[val_idx] = temp_oof\n    print(f'Fold {fold} AUC : ', roc_auc_score(y_val, temp_oof))\n    \nprint(f'XgBoost OOF AUC : ', roc_auc_score(Y, train_oof))\n\n# lgbm\nmodel = LGBMClassifier(**lgbm_params)\ntrain_oof = np.zeros((957919,))\nskf = StratifiedKFold(shuffle=True, random_state=42)\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, Y)):\n    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_train, y_val = Y.iloc[train_idx], Y.iloc[val_idx]\n    \n    model = model.fit(X_train, y_train, verbose=0)\n    temp_oof = model.predict_proba(X_val)[:, 1]\n    train_oof[val_idx] = temp_oof\n    print(f'Fold {fold} AUC : ', roc_auc_score(y_val, temp_oof))\n    \nprint(f'LGBM OOF AUC : ', roc_auc_score(Y, train_oof))\n\n# catboost\nmodel = CatBoostClassifier(**catb_params)\ntrain_oof = np.zeros((957919,))\nskf = StratifiedKFold(shuffle=True, random_state=42)\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, Y)):\n    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_train, y_val = Y.iloc[train_idx], Y.iloc[val_idx]\n    \n    model = model.fit(X_train, y_train, verbose=0)\n    temp_oof = model.predict_proba(X_val)[:, 1]\n    train_oof[val_idx] = temp_oof\n    print(f'Fold {fold} AUC : ', roc_auc_score(y_val, temp_oof))\n    \nprint(f'Catboost OOF AUC : ', roc_auc_score(Y, train_oof))","metadata":{},"execution_count":null,"outputs":[]}]}