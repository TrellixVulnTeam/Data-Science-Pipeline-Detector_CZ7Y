{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<br>\n<br>\n\n# Table of Contents\n\n* [Introduction](#intro)\n* [Load Libraries](#1)\n* [Find Data Files In Input Folder](#2)\n* [Read In The Data Files](#3)\n* [Exploratory Data Analysis (EDA)](#4)\n    * [Dataset Dimensions](#5)\n    * [Dataset Head/Tail](#6)\n    * [Target Column (\"claim\")](#target)\n        * [Head](#target1)\n        * [Unique Values In The Target Column](#target2)\n        * [Target Column Data Type](#target3)\n        * [Value Counts Of The Target Column](#target4)\n        * [How Balanced Is The Target Column?](#target5)\n    * [Column Names And Check For Differences Between Train and Test](#7)\n    * [Describe](#8)\n    * [Info](#9)\n    * [Missing Values](#10)\n        * [Nullity Matrices](#11)\n        * [Nullity Bar Charts](#12)\n        * [Percent Missing In Each Column Bar Charts](#13)\n        * [How Many Missing Data Points Are There Total?  What Percent of Total Is Missing?](#14)\n        * [What If We Just Drop All Rows With Missing Values From The Datasets?](#15)\n    * [Feature Column Distributions](#16)\n        * [Train](#17)\n        * [Test](#18)\n    * [Feature Column Skewness and Kurtosis](#19)\n        * [Skewness](#20)\n        * [Kurtosis](#21)\n    * [Correlations](#22)\n        * [Correlations of Subsets](#23)\n* [TLDR: EDA Findings/Conclusions](#24)\n* [Preprocess](#25)\n    * [Train/Test Split](#28)\n    * [Min-Max Scaling](#26)\n    * [Impute Using Mean](#27)\n* [Baseline Model](#29)\n    * [Baseline Predictions](#30)\n    * [Define A Scoring Function](#31)\n    * [Choosing N Estimators](#32)\n    * [Tuning Results](#33)\n    * [Model Selection](#34)\n    * [Final Model](#35)\n    * [Final Baseline Prediction](#36)\n    ","metadata":{}},{"cell_type":"markdown","source":"<br>\n<br>\n\n<a id=intro></a>\n\n## Introduction","metadata":{"execution":{"iopub.status.busy":"2021-09-26T14:33:26.145027Z","iopub.execute_input":"2021-09-26T14:33:26.145378Z","iopub.status.idle":"2021-09-26T14:33:26.150935Z","shell.execute_reply.started":"2021-09-26T14:33:26.145341Z","shell.execute_reply":"2021-09-26T14:33:26.14986Z"}}},{"cell_type":"markdown","source":"The goal of this notebook is to explore the Tabular Playground Series - Sep 2021 dataset, make some decisions about pre-processing steps based on my exploration, look for promising feature engineering opportunties and then make a benchmark submission to the competition.\n\nThis is only my second public notebook so I'm sure mistakes will be made.  I'm open to suggestions for improvement.  Thanks!\n\n***From the TPS September 2021 Competition Description Page:***\n\n*The dataset used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting whether a claim will be made on an insurance policy. Although the features are anonymized, they have properties relating to real-world features.*\n\n*Submissions are evaluated on area under the ROC curve between the predicted probability and the observed target.*\n\n*For each id in the test set, you must predict a probability for the claim variable. The file should contain a header and have the following format:  id, claim*","metadata":{}},{"cell_type":"markdown","source":"<br>\n<br>\n<a id=1><a/>\n    \n## Load Libraries  ","metadata":{"execution":{"iopub.status.busy":"2021-09-26T13:54:45.405852Z","iopub.execute_input":"2021-09-26T13:54:45.406452Z","iopub.status.idle":"2021-09-26T13:54:45.429675Z","shell.execute_reply.started":"2021-09-26T13:54:45.406367Z","shell.execute_reply":"2021-09-26T13:54:45.428684Z"}}},{"cell_type":"code","source":"import numpy as np #working with matrices, arrays, data science-friendly arrays\nimport pandas as pd #data processing, CSV file I/O, preprocessing\nimport matplotlib.pyplot as plt  #data viz library\n#jupyter notebook magic function to make plots show in a notebook cell\n%matplotlib inline  \nplt.style.use('seaborn-whitegrid') #set my default matplotlib style to 'seaborn-whitegrid'\n\nimport seaborn as sns  #additional data viz helper library\nimport scipy.stats as st  #used to fit non-normal distributions with seaborn\nimport missingno as msno  #visualize missing values in the dataset\n\nimport os  #working with the operating system, filepaths, folders,etc.\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.impute import SimpleImputer #replace missing values with the mean\nfrom sklearn.feature_selection import mutual_info_regression #used to create a ranking with a feature utility metric \nfrom xgboost import XGBClassifier #first classifier model\nfrom sklearn.metrics import roc_auc_score","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:03:48.312622Z","iopub.execute_input":"2021-09-27T04:03:48.313102Z","iopub.status.idle":"2021-09-27T04:03:48.325043Z","shell.execute_reply.started":"2021-09-27T04:03:48.313065Z","shell.execute_reply":"2021-09-27T04:03:48.32388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n<br>\n\n<a id=2><a/>\n\n## Find Data Files In Input Folder","metadata":{}},{"cell_type":"code","source":"#This is default from Kaggle.  Basically uses os.walk to recursively \n#print the full filepath and filename for all files stored in the kaggle/input folder.\n#Some people modify this to not just print the full filepath but to also read them into a dataframe.  \n#I'm just going to use the print statement to inform my pd.read_csv function call later.\n\n####### DEFAULT COMMENTS AND CODE FROM KAGGLE ###############\n\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n####### DEFAULT COMMENTS AND CODE FROM KAGGLE ###############","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-27T04:03:48.326708Z","iopub.execute_input":"2021-09-27T04:03:48.327482Z","iopub.status.idle":"2021-09-27T04:03:48.344213Z","shell.execute_reply.started":"2021-09-27T04:03:48.327444Z","shell.execute_reply":"2021-09-27T04:03:48.343369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n<br>\n\n<a id =\"3\"></a>\n\n## Read In The Data Files","metadata":{}},{"cell_type":"code","source":"#using the above print statements, set the filepath of the csv files I want to read.\ntrain_filepath = \"../input/tabular-playground-series-sep-2021/train.csv\"\ntest_filepath = \"../input/tabular-playground-series-sep-2021/test.csv\"\nsample_solution_filepath = \"../input/tabular-playground-series-sep-2021/sample_solution.csv\"\n\n#read train.csv, test.csv, sample_solution.csv into a pandas dataframe\n#Need to use index_col = 0 because the id column is located at column index 0.  I found this below\n#after reading in the dataset and looking at the head of the dataframe.\ntrain_df = pd.read_csv(train_filepath, index_col = 0)  \ntest_df = pd.read_csv(test_filepath, index_col =0)\nss_df = pd.read_csv(sample_solution_filepath)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:03:48.347342Z","iopub.execute_input":"2021-09-27T04:03:48.34754Z","iopub.status.idle":"2021-09-27T04:04:27.336976Z","shell.execute_reply.started":"2021-09-27T04:03:48.347516Z","shell.execute_reply":"2021-09-27T04:04:27.336236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n<br>\n\n<a id=\"4\"></a>\n\n## Exploratory Data Analysis (EDA)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"5\"></a>\n\n### Dataset Dimensions","metadata":{}},{"cell_type":"code","source":"# training dataset dimensions using shape method\ntrain_df.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:04:27.338864Z","iopub.execute_input":"2021-09-27T04:04:27.339113Z","iopub.status.idle":"2021-09-27T04:04:27.34587Z","shell.execute_reply.started":"2021-09-27T04:04:27.339077Z","shell.execute_reply":"2021-09-27T04:04:27.345142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test dataset dimensions using shape method\ntest_df.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:04:27.347298Z","iopub.execute_input":"2021-09-27T04:04:27.347767Z","iopub.status.idle":"2021-09-27T04:04:27.359994Z","shell.execute_reply.started":"2021-09-27T04:04:27.347731Z","shell.execute_reply":"2021-09-27T04:04:27.359075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sample submission dataset dimensions using shape method\nss_df.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:04:27.361057Z","iopub.execute_input":"2021-09-27T04:04:27.361896Z","iopub.status.idle":"2021-09-27T04:04:27.369233Z","shell.execute_reply.started":"2021-09-27T04:04:27.361858Z","shell.execute_reply":"2021-09-27T04:04:27.368472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### OK, huge dataset.  Time to use the GPU.  Sklearn's RandomForest classifier is out because it does not allow GPU usage.  So maybe, XGBoost Classifier, Catboost,  with gpu enabled or RAPIDS package version of RandomForest classifier instead of sklearn's.  Using Sklearn will take too long to train models on a dataset this size.  ","metadata":{}},{"cell_type":"markdown","source":"<a id='6'></a>\n\n### Dataset Head/Tail","metadata":{}},{"cell_type":"markdown","source":"#### Training dataset","metadata":{}},{"cell_type":"code","source":"#training dataset head and tail to get a feel for what the data looks like\n\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:04:27.370489Z","iopub.execute_input":"2021-09-27T04:04:27.371029Z","iopub.status.idle":"2021-09-27T04:04:27.409115Z","shell.execute_reply.started":"2021-09-27T04:04:27.370993Z","shell.execute_reply":"2021-09-27T04:04:27.408204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.tail()","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:04:27.410387Z","iopub.execute_input":"2021-09-27T04:04:27.410763Z","iopub.status.idle":"2021-09-27T04:04:27.44263Z","shell.execute_reply.started":"2021-09-27T04:04:27.410727Z","shell.execute_reply":"2021-09-27T04:04:27.441995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Test Dataset","metadata":{}},{"cell_type":"code","source":"#training dataset head and tail to get a feel for what the data looks like\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:04:27.443712Z","iopub.execute_input":"2021-09-27T04:04:27.443936Z","iopub.status.idle":"2021-09-27T04:04:27.473016Z","shell.execute_reply.started":"2021-09-27T04:04:27.443906Z","shell.execute_reply":"2021-09-27T04:04:27.472231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.tail()","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:04:27.476102Z","iopub.execute_input":"2021-09-27T04:04:27.476308Z","iopub.status.idle":"2021-09-27T04:04:27.504788Z","shell.execute_reply.started":"2021-09-27T04:04:27.476284Z","shell.execute_reply":"2021-09-27T04:04:27.504113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Sample Submission ","metadata":{}},{"cell_type":"code","source":"ss_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:04:27.505891Z","iopub.execute_input":"2021-09-27T04:04:27.506161Z","iopub.status.idle":"2021-09-27T04:04:27.514453Z","shell.execute_reply.started":"2021-09-27T04:04:27.506127Z","shell.execute_reply":"2021-09-27T04:04:27.513753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ss_df.tail()","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:04:27.515862Z","iopub.execute_input":"2021-09-27T04:04:27.516385Z","iopub.status.idle":"2021-09-27T04:04:27.528111Z","shell.execute_reply.started":"2021-09-27T04:04:27.516304Z","shell.execute_reply":"2021-09-27T04:04:27.527215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## the sample solution dataframe is no longer need. delete the variable from memory to conserve a little RAM\ndel ss_df","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:04:27.52975Z","iopub.execute_input":"2021-09-27T04:04:27.530138Z","iopub.status.idle":"2021-09-27T04:04:27.535966Z","shell.execute_reply.started":"2021-09-27T04:04:27.530091Z","shell.execute_reply":"2021-09-27T04:04:27.535299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n<br>\n\n<a id=\"target\"></a>\n\n### Target Column (\"claim\")","metadata":{}},{"cell_type":"markdown","source":"<a id=\"target1\"></a>\n\n### Head","metadata":{}},{"cell_type":"code","source":"train_df['claim'].head()","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:04:27.537338Z","iopub.execute_input":"2021-09-27T04:04:27.537658Z","iopub.status.idle":"2021-09-27T04:04:27.547956Z","shell.execute_reply.started":"2021-09-27T04:04:27.537626Z","shell.execute_reply":"2021-09-27T04:04:27.547295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"target2\"></a>\n\n### Unique Values In The Target Column","metadata":{}},{"cell_type":"code","source":"train_df['claim'].unique()","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:04:27.549287Z","iopub.execute_input":"2021-09-27T04:04:27.549595Z","iopub.status.idle":"2021-09-27T04:04:27.56376Z","shell.execute_reply.started":"2021-09-27T04:04:27.549558Z","shell.execute_reply":"2021-09-27T04:04:27.563101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"target3\"></a>\n\n### Target Column Data Type","metadata":{}},{"cell_type":"code","source":"train_df['claim'].dtype","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:04:27.564869Z","iopub.execute_input":"2021-09-27T04:04:27.56533Z","iopub.status.idle":"2021-09-27T04:04:27.571261Z","shell.execute_reply.started":"2021-09-27T04:04:27.56528Z","shell.execute_reply":"2021-09-27T04:04:27.57041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"target4\"></a>\n\n### Value Counts Of The Target Column","metadata":{}},{"cell_type":"code","source":"#value counts of 'claim' column\ntrain_df['claim'].astype('str').value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:04:27.573154Z","iopub.execute_input":"2021-09-27T04:04:27.573496Z","iopub.status.idle":"2021-09-27T04:04:28.81754Z","shell.execute_reply.started":"2021-09-27T04:04:27.57346Z","shell.execute_reply":"2021-09-27T04:04:28.816686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"target5\"></a>\n\n### How Balanced Is The Target Column?","metadata":{}},{"cell_type":"code","source":"print('There is a {}% difference between \"0\" counts and \"1\" counts in the target \"claim\" column.'.format(((train_df['claim'].astype('str').value_counts()[0] - train_df['claim'].astype('str').value_counts()[1])/train_df['claim'].astype('str').value_counts()[0])*100))","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:04:28.819086Z","iopub.execute_input":"2021-09-27T04:04:28.81938Z","iopub.status.idle":"2021-09-27T04:04:32.67842Z","shell.execute_reply.started":"2021-09-27T04:04:28.819345Z","shell.execute_reply":"2021-09-27T04:04:32.676971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.subplots(1,1, figsize=(5,5))\nplt.subplot(1,1,1)\ntarget_data_obj = train_df['claim'].astype('str').value_counts()\nplt.bar(x=target_data_obj.index, height=target_data_obj.values, alpha=0.75, color='#7571B0')\nplt.title(\"Target ('claim') Column Value Counts\", fontsize=12,fontweight='bold')","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:04:32.679585Z","iopub.execute_input":"2021-09-27T04:04:32.679856Z","iopub.status.idle":"2021-09-27T04:04:34.107506Z","shell.execute_reply.started":"2021-09-27T04:04:32.679826Z","shell.execute_reply":"2021-09-27T04:04:34.106792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### The target \"claim\" column is a binary 1 or 0 integer column.  It looks like the claim column in the  submission file should be a probability between 0 and 1.\n#### The target \"claim\" column is very balanced with only a 0.6% difference in counts between the two classes (0 or 1).","metadata":{}},{"cell_type":"code","source":"del target_data_obj","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:04:34.108638Z","iopub.execute_input":"2021-09-27T04:04:34.110377Z","iopub.status.idle":"2021-09-27T04:04:34.114036Z","shell.execute_reply.started":"2021-09-27T04:04:34.110308Z","shell.execute_reply":"2021-09-27T04:04:34.113198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n<a id=\"7\"></a>\n\n### Column Names And Check For Differences Between Train and Test","metadata":{}},{"cell_type":"code","source":"print(\"There are {} columns in the training dataset.\".format(len(train_df.columns)))\ntrain_df.columns","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:04:34.115115Z","iopub.execute_input":"2021-09-27T04:04:34.115791Z","iopub.status.idle":"2021-09-27T04:04:34.129851Z","shell.execute_reply.started":"2021-09-27T04:04:34.115755Z","shell.execute_reply":"2021-09-27T04:04:34.129155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"There are {} columns in the test dataset.\".format(len(test_df.columns)))\ntest_df.columns","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:04:34.132447Z","iopub.execute_input":"2021-09-27T04:04:34.133163Z","iopub.status.idle":"2021-09-27T04:04:34.140878Z","shell.execute_reply.started":"2021-09-27T04:04:34.133125Z","shell.execute_reply":"2021-09-27T04:04:34.140089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#find any columns in train that do not appear in test\ncol_diff_list = [x for x in train_df.columns if x not in test_df.columns]\ncol_diff_list2 = [x for x in test_df.columns if x not in train_df.columns]\nprint('The column(s) that are in the training dataset but not in the test dataset are: {}'.format(col_diff_list))\nprint('The column(s) that are in the test dataset but not in the train dataset are: {}'.format(col_diff_list2))","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:04:34.142383Z","iopub.execute_input":"2021-09-27T04:04:34.142895Z","iopub.status.idle":"2021-09-27T04:04:34.150605Z","shell.execute_reply.started":"2021-09-27T04:04:34.142856Z","shell.execute_reply":"2021-09-27T04:04:34.149611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Both the training and test datasets appear to have same columns except the training dataset also includes the target \"claim\" column.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"8\"></a>\n\n### Describe","metadata":{}},{"cell_type":"code","source":"train_df.describe()","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:04:34.151938Z","iopub.execute_input":"2021-09-27T04:04:34.152717Z","iopub.status.idle":"2021-09-27T04:04:38.014117Z","shell.execute_reply.started":"2021-09-27T04:04:34.152681Z","shell.execute_reply":"2021-09-27T04:04:38.013414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.describe()","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:04:38.015547Z","iopub.execute_input":"2021-09-27T04:04:38.015818Z","iopub.status.idle":"2021-09-27T04:04:40.008378Z","shell.execute_reply.started":"2021-09-27T04:04:38.015783Z","shell.execute_reply":"2021-09-27T04:04:40.007677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"9\"></a>\n\n### Info\n\nThere are lots of columns so breaking the dataset into 3 subsets","metadata":{}},{"cell_type":"code","source":"train_df.iloc[:,:round(len(train_df.columns)/3)].info()","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:04:40.009704Z","iopub.execute_input":"2021-09-27T04:04:40.009969Z","iopub.status.idle":"2021-09-27T04:04:40.187201Z","shell.execute_reply.started":"2021-09-27T04:04:40.009934Z","shell.execute_reply":"2021-09-27T04:04:40.186431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.iloc[:,round(len(train_df.columns)/3):round(len(train_df.columns) * (2/3))].info()","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:04:40.188384Z","iopub.execute_input":"2021-09-27T04:04:40.189091Z","iopub.status.idle":"2021-09-27T04:04:40.361903Z","shell.execute_reply.started":"2021-09-27T04:04:40.189051Z","shell.execute_reply":"2021-09-27T04:04:40.361133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.iloc[:,round(len(train_df.columns) * (2/3)):round(len(train_df.columns) * (3/3))].info()","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:04:40.367096Z","iopub.execute_input":"2021-09-27T04:04:40.367704Z","iopub.status.idle":"2021-09-27T04:04:40.594986Z","shell.execute_reply.started":"2021-09-27T04:04:40.367671Z","shell.execute_reply":"2021-09-27T04:04:40.593849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['claim'].dtype","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:04:40.599401Z","iopub.execute_input":"2021-09-27T04:04:40.601516Z","iopub.status.idle":"2021-09-27T04:04:40.611606Z","shell.execute_reply.started":"2021-09-27T04:04:40.601477Z","shell.execute_reply":"2021-09-27T04:04:40.610637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.iloc[:,:round(len(test_df.columns)/3)].info()","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:04:40.616728Z","iopub.execute_input":"2021-09-27T04:04:40.618935Z","iopub.status.idle":"2021-09-27T04:04:40.685376Z","shell.execute_reply.started":"2021-09-27T04:04:40.618882Z","shell.execute_reply":"2021-09-27T04:04:40.684662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.iloc[:,round(len(test_df.columns)/3):round(len(test_df.columns) * (2/3))].info()","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:04:40.689085Z","iopub.execute_input":"2021-09-27T04:04:40.690999Z","iopub.status.idle":"2021-09-27T04:04:40.758361Z","shell.execute_reply.started":"2021-09-27T04:04:40.690965Z","shell.execute_reply":"2021-09-27T04:04:40.757711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.iloc[:,round(len(test_df.columns) * (2/3)):round(len(test_df.columns) * (3/3))].info()","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:04:40.762067Z","iopub.execute_input":"2021-09-27T04:04:40.763953Z","iopub.status.idle":"2021-09-27T04:04:40.82946Z","shell.execute_reply.started":"2021-09-27T04:04:40.763915Z","shell.execute_reply":"2021-09-27T04:04:40.828732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### All features columns are float65 type with the target \"claim\" column being an int64 type column.","metadata":{"execution":{"iopub.status.busy":"2021-09-26T16:20:54.542522Z","iopub.execute_input":"2021-09-26T16:20:54.543473Z","iopub.status.idle":"2021-09-26T16:20:54.551817Z","shell.execute_reply.started":"2021-09-26T16:20:54.543422Z","shell.execute_reply":"2021-09-26T16:20:54.551007Z"}}},{"cell_type":"markdown","source":"<a id=\"10\"></a>\n\n### Missing Values","metadata":{}},{"cell_type":"markdown","source":"<br>\n\n#### Which columns have missing values?","metadata":{}},{"cell_type":"code","source":"print('There are {} column(s) in {} with NULL values.'.format(len([col for col in train_df.columns if train_df[col].isnull().any()]),'train_df'))","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:04:40.833367Z","iopub.execute_input":"2021-09-27T04:04:40.835309Z","iopub.status.idle":"2021-09-27T04:04:40.982311Z","shell.execute_reply.started":"2021-09-27T04:04:40.835269Z","shell.execute_reply":"2021-09-27T04:04:40.980463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('There are {} column(s) in {} with NULL values.'.format(len([col for col in test_df.columns if train_df[col].isnull().any()]),'test_df'))","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:04:40.983853Z","iopub.execute_input":"2021-09-27T04:04:40.984311Z","iopub.status.idle":"2021-09-27T04:04:41.096954Z","shell.execute_reply.started":"2021-09-27T04:04:40.984271Z","shell.execute_reply":"2021-09-27T04:04:41.096249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n<a id=\"11\"></a>\n\n### Nullity Matrices","metadata":{}},{"cell_type":"code","source":"#nullity matrix on training dataset to understand which columns have NULL values and where NULL values are dispersed throughtout the dataset.  Is there a pattern?\nmsno.matrix(train_df,color=(0.27, 0.52, 1.0))","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:04:41.098329Z","iopub.execute_input":"2021-09-27T04:04:41.098749Z","iopub.status.idle":"2021-09-27T04:05:09.466694Z","shell.execute_reply.started":"2021-09-27T04:04:41.098711Z","shell.execute_reply":"2021-09-27T04:05:09.465922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#nullity matrix test dataset\nmsno.matrix(test_df,color=(0.27, 0.52, 1.0))","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:05:09.467985Z","iopub.execute_input":"2021-09-27T04:05:09.468588Z","iopub.status.idle":"2021-09-27T04:05:23.834657Z","shell.execute_reply.started":"2021-09-27T04:05:09.468546Z","shell.execute_reply":"2021-09-27T04:05:23.833921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### At first glance, it appears the NULL values are relatively randomly dispersed throughout all feature columns.\n\n#### Row 105 and 119 appear to have the maximum nullity in the training dataset.\n\n#### Visually, it does not appear a large percentage of the dataset is NULL, but all feature columns appear to have at least have some missing values.","metadata":{}},{"cell_type":"markdown","source":"<br>\n\n<a id=\"12\"></a>\n\n### Nullity Bar Charts","metadata":{}},{"cell_type":"markdown","source":"#### Due to the large number of columns, the Missingno library nullity bar charts cannot handle so many columns by default.  I'm going to break up the dataset into thirds to get a better look.  Using this plot to understand, approximately, how many NULLS are in each column.","metadata":{}},{"cell_type":"markdown","source":"#### Train","metadata":{}},{"cell_type":"code","source":"msno.bar(train_df.iloc[:,:round(len(train_df.columns)/3)],color=(0.27, 0.52, 1.0))","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:05:23.836067Z","iopub.execute_input":"2021-09-27T04:05:23.836513Z","iopub.status.idle":"2021-09-27T04:05:27.922864Z","shell.execute_reply.started":"2021-09-27T04:05:23.836474Z","shell.execute_reply":"2021-09-27T04:05:27.921923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"msno.bar(train_df.iloc[:,round(len(train_df.columns)/3):round(len(train_df.columns) * (2/3))],color=(0.27, 0.52, 1.0))","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:05:27.927201Z","iopub.execute_input":"2021-09-27T04:05:27.927474Z","iopub.status.idle":"2021-09-27T04:05:31.66748Z","shell.execute_reply.started":"2021-09-27T04:05:27.927438Z","shell.execute_reply":"2021-09-27T04:05:31.666777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"msno.bar(train_df.iloc[:,round(len(train_df.columns) * (2/3)):],color=(0.27, 0.52, 1.0))","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:05:31.668696Z","iopub.execute_input":"2021-09-27T04:05:31.669036Z","iopub.status.idle":"2021-09-27T04:05:35.388338Z","shell.execute_reply.started":"2021-09-27T04:05:31.668997Z","shell.execute_reply":"2021-09-27T04:05:35.387598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Test","metadata":{}},{"cell_type":"code","source":"msno.bar(test_df.iloc[:,:round(len(test_df.columns)/3)],color=(0.27, 0.52, 1.0))","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:05:35.389595Z","iopub.execute_input":"2021-09-27T04:05:35.389979Z","iopub.status.idle":"2021-09-27T04:05:38.974711Z","shell.execute_reply.started":"2021-09-27T04:05:35.389941Z","shell.execute_reply":"2021-09-27T04:05:38.973956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"msno.bar(test_df.iloc[:,round(len(test_df.columns)/3):round(len(train_df.columns) * (2/3))],color=(0.27, 0.52, 1.0))","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:05:38.976003Z","iopub.execute_input":"2021-09-27T04:05:38.976374Z","iopub.status.idle":"2021-09-27T04:05:42.491694Z","shell.execute_reply.started":"2021-09-27T04:05:38.976333Z","shell.execute_reply":"2021-09-27T04:05:42.490985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"msno.bar(test_df.iloc[:,round(len(test_df.columns) * (2/3)):],color=(0.27, 0.52, 1.0))","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:05:42.493083Z","iopub.execute_input":"2021-09-27T04:05:42.493364Z","iopub.status.idle":"2021-09-27T04:05:45.913829Z","shell.execute_reply.started":"2021-09-27T04:05:42.493328Z","shell.execute_reply":"2021-09-27T04:05:45.913134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Based on the above, there appears to be, at least, some NULL values in each feature column and no NULL values in the target 'claim' column.  The NULL values appears to be randomly spread out throughout the datset in each column and appear to represent a very small percentage of each column's data.","metadata":{}},{"cell_type":"markdown","source":"<br>\n\n<a id=\"13\"></a>\n\n### Percent Missing In Each Column Bar Charts","metadata":{}},{"cell_type":"code","source":"percent_missing_train_df = train_df.isnull().sum() * 100 / len(train_df)\nmissing_value_train_df = pd.DataFrame({'column_name': train_df.columns,\n                                 'percent_missing': percent_missing_train_df})\n\npercent_missing_test_df = test_df.isnull().sum() * 100 / len(test_df)\nmissing_value_test_df = pd.DataFrame({'column_name': test_df.columns,\n                                 'percent_missing': percent_missing_test_df})","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:05:45.915273Z","iopub.execute_input":"2021-09-27T04:05:45.915533Z","iopub.status.idle":"2021-09-27T04:05:46.221686Z","shell.execute_reply.started":"2021-09-27T04:05:45.9155Z","shell.execute_reply":"2021-09-27T04:05:46.220896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing_value_train_df.sort_values('percent_missing', inplace=True, ascending=True)\nmissing_value_test_df.sort_values('percent_missing', inplace=True, ascending=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:05:46.223142Z","iopub.execute_input":"2021-09-27T04:05:46.223425Z","iopub.status.idle":"2021-09-27T04:05:46.229807Z","shell.execute_reply.started":"2021-09-27T04:05:46.22339Z","shell.execute_reply":"2021-09-27T04:05:46.228884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing_value_train_df.plot.barh(x='column_name', y='percent_missing', rot=5,figsize=(10, 40),alpha=0.85,legend=False,color='#4F66AF')\nplt.title('Training Dataframe Percent Missing Values By Column Descending Order')","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:05:46.231589Z","iopub.execute_input":"2021-09-27T04:05:46.23188Z","iopub.status.idle":"2021-09-27T04:05:48.084504Z","shell.execute_reply.started":"2021-09-27T04:05:46.231847Z","shell.execute_reply":"2021-09-27T04:05:48.083784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing_value_test_df.plot.barh(x='column_name', y='percent_missing', rot=5,figsize=(10, 40),alpha=0.85,legend=False,color='#4F66AF')\nplt.title('Test Dataframe Percent Missing Values By Column Descending Order')","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:05:48.085773Z","iopub.execute_input":"2021-09-27T04:05:48.086116Z","iopub.status.idle":"2021-09-27T04:05:49.845241Z","shell.execute_reply.started":"2021-09-27T04:05:48.086077Z","shell.execute_reply":"2021-09-27T04:05:49.844535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Every feature column is a float64 type column.  All columns have, at least, some missing values.  No column has more than 2% missing values in either the training or test datasets.\n\n#### There are no missing values in the target \"claim\" column\n\n#### f31 and f46 have the most NaN values but still not more than 2%","metadata":{}},{"cell_type":"code","source":"#delete the above dataframes from memory to help conserve RAM\ndel percent_missing_train_df\ndel missing_value_train_df\ndel percent_missing_test_df\ndel missing_value_test_df","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:05:49.846482Z","iopub.execute_input":"2021-09-27T04:05:49.846991Z","iopub.status.idle":"2021-09-27T04:05:49.851559Z","shell.execute_reply.started":"2021-09-27T04:05:49.84695Z","shell.execute_reply":"2021-09-27T04:05:49.850708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n<br>\n\n<a id=\"14\"></a>\n\n### How Many Missing Data Points Are There Total?  What Percent of Total Is Missing?","metadata":{}},{"cell_type":"code","source":"train_df_missing_values_count = train_df.isnull().sum()\ntrain_df_total_cells = np.product(train_df.shape)\ntrain_df_total_missing = train_df_missing_values_count.sum()\ntrain_df_percent_missing = (train_df_total_missing/train_df_total_cells) * 100\nprint(\"There are {} missing data points in the training dataset out of {} total possible cells.\".format(train_df_total_missing,train_df_total_cells))\nprint(\"{}% of the training dataset is missing.\".format(round(train_df_percent_missing,3)))","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:05:49.852944Z","iopub.execute_input":"2021-09-27T04:05:49.853285Z","iopub.status.idle":"2021-09-27T04:05:50.058424Z","shell.execute_reply.started":"2021-09-27T04:05:49.853249Z","shell.execute_reply":"2021-09-27T04:05:50.057594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df_missing_values_count = test_df.isnull().sum()\ntest_df_total_cells = np.product(test_df.shape)\ntest_df_total_missing = test_df_missing_values_count.sum()\ntest_df_percent_missing = (test_df_total_missing/test_df_total_cells) * 100\nprint(\"There are {} missing data points in the training dataset out of {} total possible cells.\".format(test_df_total_missing,test_df_total_cells))\nprint(\"{}% of the training dataset is missing.\".format(round(test_df_percent_missing,3)))","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:05:50.059688Z","iopub.execute_input":"2021-09-27T04:05:50.060421Z","iopub.status.idle":"2021-09-27T04:05:50.169442Z","shell.execute_reply.started":"2021-09-27T04:05:50.060382Z","shell.execute_reply":"2021-09-27T04:05:50.168672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#delete the above variables to help conserve RAM\ndel train_df_missing_values_count\ndel train_df_total_cells\ndel train_df_total_missing\ndel train_df_percent_missing\n\ndel test_df_missing_values_count\ndel test_df_total_cells\ndel test_df_total_missing\ndel test_df_percent_missing","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:05:50.170843Z","iopub.execute_input":"2021-09-27T04:05:50.171289Z","iopub.status.idle":"2021-09-27T04:05:50.175822Z","shell.execute_reply.started":"2021-09-27T04:05:50.171251Z","shell.execute_reply":"2021-09-27T04:05:50.174967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### There isn't really documentation or column name clues to tell me why there are missing values in this dataset.  Overall, there is only about 1.6% missing values in botht the training and test datasets which is pretty balanced.  It may be reasonable to just drop these NULL values and move forward.","metadata":{}},{"cell_type":"markdown","source":"<br>\n<br>\n\n<a id=\"15\"></a>\n\n### What If We Just Drop All Rows With Missing Values From The Datasets?","metadata":{}},{"cell_type":"code","source":"print(\"{}% of the rows would remain in the training dataset if we simply dropped all rows with any missing value!\".format(round((train_df.dropna().shape[0]/train_df.shape[0])*100),2))","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:05:50.177298Z","iopub.execute_input":"2021-09-27T04:05:50.177575Z","iopub.status.idle":"2021-09-27T04:05:50.616737Z","shell.execute_reply.started":"2021-09-27T04:05:50.177531Z","shell.execute_reply":"2021-09-27T04:05:50.615942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"{}% of the rows would remain in the test dataset if we simply dropped all rows with any missing value!\".format(round((test_df.dropna().shape[0]/test_df.shape[0])*100),2))","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:05:50.617968Z","iopub.execute_input":"2021-09-27T04:05:50.618724Z","iopub.status.idle":"2021-09-27T04:05:50.79104Z","shell.execute_reply.started":"2021-09-27T04:05:50.618683Z","shell.execute_reply":"2021-09-27T04:05:50.790112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Wow, so 62% of the rows have at least 1 missing value.  We also know from above missing value analysis that all (100%) columns also have at least one missing value.  Therefore, simply dropping all rows or all columns with a missing value would remove too much of the original dataset.  In this case, it's better to impute, in my opinion.","metadata":{}},{"cell_type":"markdown","source":"<br>\n<br>\n\n<a id=\"16\"></a>\n\n### Feature Column Distributions\n\n##### We know all feature columns are numeric so we don't have any categorical columns in the intitial dataset to look at.  Let's look at each feature's distribution to see if we could possibly transform any.","metadata":{}},{"cell_type":"code","source":"feature_cols = [col for col in train_df.columns if col != 'claim']","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:05:50.792511Z","iopub.execute_input":"2021-09-27T04:05:50.792875Z","iopub.status.idle":"2021-09-27T04:05:50.79772Z","shell.execute_reply.started":"2021-09-27T04:05:50.792833Z","shell.execute_reply":"2021-09-27T04:05:50.796879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#define a function to loop over numeric feature columns and plot their distribution\ndef plot_feature_distributions(figrows,figcols,colstart,colend,collist,df_to_plot):\n    plt.figure(1)\n    plt.subplots(figrows,figcols, figsize=(20,20))\n    for i, item in enumerate(collist[colstart:colend]):\n        plt.subplot(figrows,figcols,i+1)\n        plt.hist(x=df_to_plot[item],color='#7571B0',alpha=0.75)\n        plt.title(item)\n        plt.grid(True)\n    plt.subplots_adjust(top=1.5, bottom=0.2, left=0.10, right=0.95, hspace=0.3,\n        wspace=0.35)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:05:50.799159Z","iopub.execute_input":"2021-09-27T04:05:50.799797Z","iopub.status.idle":"2021-09-27T04:05:50.808498Z","shell.execute_reply.started":"2021-09-27T04:05:50.799761Z","shell.execute_reply":"2021-09-27T04:05:50.807715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n<a id=\"17\"></a>\n\n#### Train","metadata":{}},{"cell_type":"code","source":"#plot first 39 columns of train_df\nplot_feature_distributions(13,3,0,39,feature_cols,train_df)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:05:50.809691Z","iopub.execute_input":"2021-09-27T04:05:50.810041Z","iopub.status.idle":"2021-09-27T04:05:56.972834Z","shell.execute_reply.started":"2021-09-27T04:05:50.810005Z","shell.execute_reply":"2021-09-27T04:05:56.972078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plot columns 40-78 of train_df\nplot_feature_distributions(13,3,39,78,feature_cols,train_df)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:05:56.974495Z","iopub.execute_input":"2021-09-27T04:05:56.974805Z","iopub.status.idle":"2021-09-27T04:06:03.866005Z","shell.execute_reply.started":"2021-09-27T04:05:56.974768Z","shell.execute_reply":"2021-09-27T04:06:03.86493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plot columns 79-119 of train_df\nplot_feature_distributions(14,3,78,119,feature_cols,train_df)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:06:03.868381Z","iopub.execute_input":"2021-09-27T04:06:03.868669Z","iopub.status.idle":"2021-09-27T04:06:10.544459Z","shell.execute_reply.started":"2021-09-27T04:06:03.868605Z","shell.execute_reply":"2021-09-27T04:06:10.543738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"18\"></a>\n\n#### Test","metadata":{}},{"cell_type":"code","source":"#plot first 39 columns of test_df\nplot_feature_distributions(13,3,0,39,feature_cols,test_df)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:06:10.545875Z","iopub.execute_input":"2021-09-27T04:06:10.54616Z","iopub.status.idle":"2021-09-27T04:06:16.450713Z","shell.execute_reply.started":"2021-09-27T04:06:10.546123Z","shell.execute_reply":"2021-09-27T04:06:16.449814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plot columns 40-78 of test_df\nplot_feature_distributions(13,3,39,78,feature_cols,test_df)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:06:16.452305Z","iopub.execute_input":"2021-09-27T04:06:16.452574Z","iopub.status.idle":"2021-09-27T04:06:22.5394Z","shell.execute_reply.started":"2021-09-27T04:06:16.452534Z","shell.execute_reply":"2021-09-27T04:06:22.53868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plot columns 79-119 of test_df\nplot_feature_distributions(14,3,78,119,feature_cols,test_df)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:06:22.540842Z","iopub.execute_input":"2021-09-27T04:06:22.541125Z","iopub.status.idle":"2021-09-27T04:06:28.804999Z","shell.execute_reply.started":"2021-09-27T04:06:22.541087Z","shell.execute_reply":"2021-09-27T04:06:28.804333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### With this many columns, it is sort of hard to wrap my mind around all the different distributions in the dataset.  In short, the 118 numeric feature columns have a variety of different distributions.  Some that look pretty normal and some that are pretty skewed and some even look sort of binary or like some kind of categorical column with most values falling into a narrow bin.\n\n####  All the different columns appear to be on different scales so some sort of scaling such as min-max scaling could be useful with this dataset. Some features are on a 0-1 scale while some are in the tens of thousands.  Some are have negative numbers while some have only positive numbers.\n\n#### Spot-checking a few features in both the train and test seem to show similar distributions in both train and test which makes me feel the train/test split is reasonably balanced.  For example, the distribution for f1 and f57 look pretty similar in both the training dataset and the test dataset.\n\n#### With so many skewed-looking columns, we could try some log, exponential, boxcox transformations during feature engineering to see if that may improve the model.","metadata":{}},{"cell_type":"markdown","source":"<br>\n<br>\n\n<a id=\"19\"></a>\n\n### Feature Column Skewness and Kurtosis","metadata":{}},{"cell_type":"markdown","source":"<a id=\"20\"></a>\n\n### Skewness","metadata":{}},{"cell_type":"code","source":"skewness_df = pd.DataFrame(train_df.skew(),columns=['skewness'])\nskewness_test_df = pd.DataFrame(test_df.skew(),columns=['skewness'])","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:06:28.806427Z","iopub.execute_input":"2021-09-27T04:06:28.806722Z","iopub.status.idle":"2021-09-27T04:06:31.712162Z","shell.execute_reply.started":"2021-09-27T04:06:28.806683Z","shell.execute_reply":"2021-09-27T04:06:31.710574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#which features are extremely skewed either positively or negatively?\nskewness_df[(skewness_df['skewness'] < -1) | (skewness_df['skewness'] > 1)]","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:06:31.714254Z","iopub.execute_input":"2021-09-27T04:06:31.71454Z","iopub.status.idle":"2021-09-27T04:06:31.732332Z","shell.execute_reply.started":"2021-09-27T04:06:31.714504Z","shell.execute_reply":"2021-09-27T04:06:31.731453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#which features are extremely skewed either positively or negatively?\nskewness_test_df[(skewness_test_df['skewness'] < -1) | (skewness_test_df['skewness'] > 1)]","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:06:31.736517Z","iopub.execute_input":"2021-09-27T04:06:31.736796Z","iopub.status.idle":"2021-09-27T04:06:31.77447Z","shell.execute_reply.started":"2021-09-27T04:06:31.736761Z","shell.execute_reply":"2021-09-27T04:06:31.77078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### There are 67 columns in both the training and testing datasets that are extremely skewed!","metadata":{}},{"cell_type":"markdown","source":"<a id=\"21\"></a>\n\n### Kurtosis\n\n##### This is the degree of presence of outliers in the distribution","metadata":{}},{"cell_type":"code","source":"kurt_df = pd.DataFrame(train_df.kurt(),columns=['kurtosis'])\nkurt_test_df = pd.DataFrame(test_df.kurt(),columns=['kurtosis'])","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:06:31.778848Z","iopub.execute_input":"2021-09-27T04:06:31.779116Z","iopub.status.idle":"2021-09-27T04:06:34.564375Z","shell.execute_reply.started":"2021-09-27T04:06:31.779083Z","shell.execute_reply":"2021-09-27T04:06:34.563593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kurt_df[kurt_df['kurtosis'] < 0].shape","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:06:34.565605Z","iopub.execute_input":"2021-09-27T04:06:34.566432Z","iopub.status.idle":"2021-09-27T04:06:34.57403Z","shell.execute_reply.started":"2021-09-27T04:06:34.566392Z","shell.execute_reply":"2021-09-27T04:06:34.573155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#how many features have a kurtosis less than 0?\nkurt_df[kurt_df['kurtosis'] < 0]","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:06:34.575587Z","iopub.execute_input":"2021-09-27T04:06:34.576015Z","iopub.status.idle":"2021-09-27T04:06:34.590254Z","shell.execute_reply.started":"2021-09-27T04:06:34.575824Z","shell.execute_reply":"2021-09-27T04:06:34.589302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#how many features have a kurtosis less than 0?\nkurt_test_df[kurt_test_df['kurtosis'] < 0]","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:06:34.591851Z","iopub.execute_input":"2021-09-27T04:06:34.59218Z","iopub.status.idle":"2021-09-27T04:06:34.607731Z","shell.execute_reply.started":"2021-09-27T04:06:34.592132Z","shell.execute_reply":"2021-09-27T04:06:34.60697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 38 columns have kurtosis less than 0 and some have a kurtosis less than -1 which may indicate outliers.","metadata":{}},{"cell_type":"markdown","source":"<br>\n<br>\n\n<a id=\"22\"></a>\n\n### Correlations","metadata":{}},{"cell_type":"code","source":"def highlight_abs_max(s):\n    '''\n    highlight the absolute maximum in a Series yellow.\n    '''\n    is_max = s == s.abs().max()\n    return ['background-color: yellow' if v else '' for v in is_max]","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:06:34.609404Z","iopub.execute_input":"2021-09-27T04:06:34.609646Z","iopub.status.idle":"2021-09-27T04:06:34.617816Z","shell.execute_reply.started":"2021-09-27T04:06:34.60962Z","shell.execute_reply":"2021-09-27T04:06:34.61694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"correlations =train_df.corr()\ncorrs_sorted = correlations['claim'].sort_values(ascending=False, key=abs).to_frame(name='Correlations With Target')\ncorrs_sorted[~corrs_sorted.index.isin(['claim','id'])].style.apply(highlight_abs_max)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:06:34.619431Z","iopub.execute_input":"2021-09-27T04:06:34.619882Z","iopub.status.idle":"2021-09-27T04:07:06.991388Z","shell.execute_reply.started":"2021-09-27T04:06:34.61983Z","shell.execute_reply":"2021-09-27T04:07:06.990505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Performing a correlation heatmap on the entire dataset is very difficult to see so I was not able to make one big correlation matrix between all 118 features plus the 'claim' column.  However, the largest correlation with the target 'claim' column is only -0.021.  None of the feature columns are very correlated with the 'claim' column","metadata":{}},{"cell_type":"markdown","source":"<br>\n\n<a id=\"23\"></a>\n\n### Correlations of Subsets","metadata":{}},{"cell_type":"code","source":"#correlations of subsections\ncorrelations = train_df.iloc[:,:round(len(train_df.columns)/4)].corr()\nf , ax = plt.subplots(figsize = (14,14))\nplt.title('Correlation of Numeric Variables f1 - f30',y=1,size=16)\nsns.heatmap(correlations,square = True,  vmax=0.8, cmap='viridis',linewidths=0.01,annot=True,annot_kws = {'size':7})","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:07:06.99284Z","iopub.execute_input":"2021-09-27T04:07:06.993107Z","iopub.status.idle":"2021-09-27T04:07:13.83996Z","shell.execute_reply.started":"2021-09-27T04:07:06.993075Z","shell.execute_reply":"2021-09-27T04:07:13.839194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#correlations of subsections\ncorrelations = train_df.iloc[:,round(len(train_df.columns)/4):round(len(train_df.columns) * (2/4))].corr()\nf , ax = plt.subplots(figsize = (14,14))\nplt.title('Correlation of Numeric Variables f31 - f60',y=1,size=16)\nsns.heatmap(correlations,square = True,  vmax=0.8, cmap='viridis',linewidths=0.01,annot=True,annot_kws = {'size':7})","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:07:13.841316Z","iopub.execute_input":"2021-09-27T04:07:13.841689Z","iopub.status.idle":"2021-09-27T04:07:20.906426Z","shell.execute_reply.started":"2021-09-27T04:07:13.841647Z","shell.execute_reply":"2021-09-27T04:07:20.905715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#correlations of subsections\ncorrelations = train_df.iloc[:,round(len(train_df.columns) * (2/4)):round(len(train_df.columns) * (3/4))].corr()\nf , ax = plt.subplots(figsize = (14,14))\nplt.title('Correlation of Numeric Variables f61 - f89',y=1,size=16)\nsns.heatmap(correlations,square = True,  vmax=0.8, cmap='viridis',linewidths=0.01,annot=True,annot_kws = {'size':8})","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:07:20.907729Z","iopub.execute_input":"2021-09-27T04:07:20.908204Z","iopub.status.idle":"2021-09-27T04:07:27.323559Z","shell.execute_reply.started":"2021-09-27T04:07:20.90815Z","shell.execute_reply":"2021-09-27T04:07:27.322872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#correlations of subsections\ncorrelations = train_df.iloc[:,round(len(train_df.columns) * (3/4)):round(len(train_df.columns) * (4/4))].corr()\nf , ax = plt.subplots(figsize = (14,14))\nplt.title('Correlation of Numeric Variables f90 - claim',y=1,size=16)\nsns.heatmap(correlations,square = True,  vmax=0.8, cmap='viridis',linewidths=0.01,annot=True,annot_kws = {'size':8})","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:07:27.324963Z","iopub.execute_input":"2021-09-27T04:07:27.325262Z","iopub.status.idle":"2021-09-27T04:07:34.242428Z","shell.execute_reply.started":"2021-09-27T04:07:27.325225Z","shell.execute_reply":"2021-09-27T04:07:34.237263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Lots of purple meaning very low correlation less than 0.1 on the color scale.  Across all subsections I could not find any columns even mildly correlated with another.","metadata":{}},{"cell_type":"markdown","source":"<br>\n<br>\n\n<a id=\"24\"></a>\n\n### TLDR: EDA Findings/Conclusions","metadata":{"execution":{"iopub.status.busy":"2021-09-27T01:32:20.585647Z","iopub.execute_input":"2021-09-27T01:32:20.58591Z","iopub.status.idle":"2021-09-27T01:32:20.592138Z","shell.execute_reply.started":"2021-09-27T01:32:20.585881Z","shell.execute_reply":"2021-09-27T01:32:20.591071Z"}}},{"cell_type":"markdown","source":"1. Training Dataset Shape:  ***(957919, 119)***\n2. Test Dataset Shape: ***(493474, 118)***\n3. This is a ***huge*** dataset that will likely test the default Kaggle CPU and RAM allocation.  GPU will likely be needed for faster iteration.  \n4. Due to necessity of GPU, models such as sklearn's RandomForest Classifier may not be appropriate due to its inability to work with GPU. Better choices might be XGBoost, Catboost, RAPIDS RandomForest, and other GPU-friendly classifier models/packages.  Check out this discussion for more [tips on using GPU in Kaggle](https://www.kaggle.com/c/tabular-playground-series-sep-2021/discussion/271900#1511854)\n5. The target column \"claim\" is a binary integer column with no missing values.  This is a probabilistic classification problem.  Models such as logistic regression, tree based models such as DecisionTrees, RandomForest, XGBoostClassifier, etc.\n6. All feature columns are float64 datatype\n7. All feature columns have at least one missing value with the most sparse column only missing 1.6% of its data.\n8. 62% of the rows have at least one missing value.\n9. The target \"claim\" column has balanced classes.  Only a 0.6% difference between class value counts.\n10. The training and test datasets have the same columns, column types and similar distributions of all feature columns.\n11. All 118 feature columns have at least one missing value.  Missing values appear randomly dispersed throughout the dataset.\n12. There are 67 feature columns with extremely skewed distributions.  Some distributions even look like categorical due to appeared binning of values with short ranges.\n13. Several feature columns have very negative kurtosis indicating possible outliers.\n14. None of the columns are correlated with each other (correlation is very small).\n15. None of the feature columns are correlated with the target column (correlation is very small).\n16. The feature columns (all numeric) appear to be on different scales.  For example, some features are on a 0-1 scale, some are in the 10,000s, some features have negative values.\n17. Ideas for feature engineering:  Lots of skewed numeric columns.  I'd like to try some transformations to see if those would improve the model.  Binning numeric columns.  Clustering the feature columns to created a new cluster feature.","metadata":{}},{"cell_type":"markdown","source":"<br>\n<br>\n\n<a id=\"25\"></a>\n\n### Preprocess","metadata":{}},{"cell_type":"markdown","source":"<br>\n\n<a id=\"28\"></a>\n\n### Train/Test Split","metadata":{"execution":{"iopub.status.busy":"2021-09-27T01:47:20.30748Z","iopub.execute_input":"2021-09-27T01:47:20.307791Z","iopub.status.idle":"2021-09-27T01:47:20.31489Z","shell.execute_reply.started":"2021-09-27T01:47:20.307761Z","shell.execute_reply":"2021-09-27T01:47:20.313827Z"}}},{"cell_type":"code","source":"#copy the training dataset\nX = train_df.copy()\ny = X.pop('claim')","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:07:34.243878Z","iopub.execute_input":"2021-09-27T04:07:34.244157Z","iopub.status.idle":"2021-09-27T04:07:34.56552Z","shell.execute_reply.started":"2021-09-27T04:07:34.24412Z","shell.execute_reply":"2021-09-27T04:07:34.56471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#split the dataset into a training/validation set \nX_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=0,train_size=0.8, test_size=0.2)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:07:34.566926Z","iopub.execute_input":"2021-09-27T04:07:34.567211Z","iopub.status.idle":"2021-09-27T04:07:35.565943Z","shell.execute_reply.started":"2021-09-27T04:07:34.567162Z","shell.execute_reply":"2021-09-27T04:07:35.565142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n<a id=\"26\"></a>\n\n### Min-Max Scaling\n\nWe saw above all feature columns are float64 type with the target column (\"claim\") being a binary integer column.  The numeric feature columns vary greatly in scale so I would like to put all the feature columns on the same scale","metadata":{}},{"cell_type":"code","source":"float_cols = [col for col in train_df if col != 'claim']","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:07:35.56747Z","iopub.execute_input":"2021-09-27T04:07:35.567841Z","iopub.status.idle":"2021-09-27T04:07:35.572622Z","shell.execute_reply.started":"2021-09-27T04:07:35.567801Z","shell.execute_reply":"2021-09-27T04:07:35.57192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#save the minmax scaler function as a variable\nmm_scaler = MinMaxScaler()","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:07:35.574194Z","iopub.execute_input":"2021-09-27T04:07:35.574675Z","iopub.status.idle":"2021-09-27T04:07:35.582919Z","shell.execute_reply.started":"2021-09-27T04:07:35.574639Z","shell.execute_reply":"2021-09-27T04:07:35.582133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#min-max scale the numeric columns only.  In this case, that is every column.\n\n#fit and transform the training df\nscaled_cols_train = pd.DataFrame(mm_scaler.fit_transform(X_train[float_cols]),index = X_train.index, columns = X_train.columns)\n\n#just transform the validation and test df.  \nscaled_cols_valid = pd.DataFrame(mm_scaler.transform(X_valid[float_cols]),index = X_valid.index, columns = X_valid.columns)\nscaled_cols_test = pd.DataFrame(mm_scaler.transform(test_df),index= test_df.index, columns = test_df.columns)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:07:35.584637Z","iopub.execute_input":"2021-09-27T04:07:35.584835Z","iopub.status.idle":"2021-09-27T04:07:37.816643Z","shell.execute_reply.started":"2021-09-27T04:07:35.584809Z","shell.execute_reply":"2021-09-27T04:07:37.815822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n<a id=\"27\"></a>\n\n\n### Impute Using Mean\n","metadata":{}},{"cell_type":"code","source":"#1.6% of the dataset is missing, however, 62% of the rows and 100% of the columns have at least 1 missing value.  This means\n#I will impute rather than drop.\n\n# Imputing AFTER min-max scaling so the mean imputation is on the same scale.\n\n#set simple imputer variable.  By default, this imputs using the mean to replace missing values\nmy_imputer = SimpleImputer()\n\n#fit and transform the training df\nimputed_X_train = pd.DataFrame(my_imputer.fit_transform(scaled_cols_train), index=X_train.index)\n\nimputed_X_valid = pd.DataFrame(my_imputer.transform(scaled_cols_valid), index=X_valid.index)\nimputed_X_test = pd.DataFrame(my_imputer.transform(scaled_cols_test), index=test_df.index)\n\n\n# Imputation removed column names; put them back\n\nimputed_X_train.columns = X_train.columns\nimputed_X_valid.columns = X_valid.columns\nimputed_X_test.columns = test_df.columns","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:07:37.818103Z","iopub.execute_input":"2021-09-27T04:07:37.818389Z","iopub.status.idle":"2021-09-27T04:07:40.304828Z","shell.execute_reply.started":"2021-09-27T04:07:37.818351Z","shell.execute_reply":"2021-09-27T04:07:40.304036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# deleting some variables to save memory\ndel train_df\ndel test_df\ndel X_train\ndel X_valid\ndel scaled_cols_valid\ndel scaled_cols_train\ndel scaled_cols_test\n","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:07:40.306297Z","iopub.execute_input":"2021-09-27T04:07:40.306592Z","iopub.status.idle":"2021-09-27T04:07:40.317291Z","shell.execute_reply.started":"2021-09-27T04:07:40.306556Z","shell.execute_reply":"2021-09-27T04:07:40.316455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n<br>\n\n<a id=\"29\"></a>\n\n### Baseline Model","metadata":{}},{"cell_type":"code","source":"baseline_model = XGBClassifier(random_state=0, verbosity=0, tree_method='gpu_hist',use_label_encoder=False,n_estimators=500,learning_rate=0.05,n_jobs=4)\nbaseline_model.fit(imputed_X_train, y_train,\n             verbose = False,\n             eval_set = [(imputed_X_valid, y_valid)],\n             eval_metric = \"auc\",\n             early_stopping_rounds = 200)\npreds_valid = baseline_model.predict_proba(imputed_X_valid)[:,1]\nprint(roc_auc_score(y_valid, preds_valid))","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:07:40.318806Z","iopub.execute_input":"2021-09-27T04:07:40.31916Z","iopub.status.idle":"2021-09-27T04:08:04.26765Z","shell.execute_reply.started":"2021-09-27T04:07:40.319124Z","shell.execute_reply":"2021-09-27T04:08:04.266868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n<a id=\"30\"></a>\n\n### Baseline Predictions","metadata":{}},{"cell_type":"code","source":"#commented out:  Score:  0.78903  Rank:  1117\n\n\npredictions = baseline_model.predict_proba(imputed_X_test)[:,1]\n\n# Save the predictions to a CSV file\noutput = pd.DataFrame({'Id': imputed_X_test.index,\n                       'claim': predictions})\noutput.to_csv('submission.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:08:04.274314Z","iopub.execute_input":"2021-09-27T04:08:04.274525Z","iopub.status.idle":"2021-09-27T04:08:11.841041Z","shell.execute_reply.started":"2021-09-27T04:08:04.274499Z","shell.execute_reply":"2021-09-27T04:08:11.839833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n<a id=\"31\"></a>\n\n### Define A Scoring Function","metadata":{}},{"cell_type":"code","source":"def get_score(x_t,y_t,x_v,y_v,n_estimator_var):\n    \"\"\"Return the area under the curve for each model\n\n    \"\"\"\n    \n    baseline_model = XGBClassifier(random_state=0, verbosity=0, tree_method='gpu_hist',use_label_encoder=False,n_estimators=n_estimator_var,learning_rate=0.05,n_jobs=4)\n    baseline_model.fit(x_t,y_t,\n             verbose = False,\n             eval_set = [(x_v, y_v)],\n             eval_metric = \"auc\",\n             early_stopping_rounds = 200)\n    preds_valid = baseline_model.predict_proba(x_v)[:,1]\n    print(roc_auc_score(y_v, preds_valid))\n    return(roc_auc_score(y_v, preds_valid))","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:08:11.842628Z","iopub.execute_input":"2021-09-27T04:08:11.843221Z","iopub.status.idle":"2021-09-27T04:08:11.852473Z","shell.execute_reply.started":"2021-09-27T04:08:11.843164Z","shell.execute_reply":"2021-09-27T04:08:11.851598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"32\"></a>\n\n### Choosing N Estimators","metadata":{}},{"cell_type":"code","source":"#function commented out to save on runtime when saving.  results saved below in results variable.\n#results = dict((key, get_score(imputed_X_train, y_train, imputed_X_valid, y_valid, key)) for key in range(50,5000,500))\n\nresults = {50: 0.6903504543569663, 550: 0.7900936067312764, 1050: 0.7938331780560485, 1550: 0.7941154722257339, 2050: 0.7941154722257339, 2550: 0.7941154722257339, 3050: 0.7941154722257339, 3550: 0.7941154722257339, 4050: 0.7941154722257339, 4550: 0.7941154722257339}","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:08:11.854177Z","iopub.execute_input":"2021-09-27T04:08:11.854545Z","iopub.status.idle":"2021-09-27T04:08:11.867064Z","shell.execute_reply.started":"2021-09-27T04:08:11.854506Z","shell.execute_reply":"2021-09-27T04:08:11.866161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"33\"></a>\n\n### Tuning Results","metadata":{}},{"cell_type":"code","source":"#plotting the results of all get_score() results found above.  Plotting number of trees vs. auc\nplt.plot(list(results.keys()), list(results.values()))\nplt.title(\"XG Boost Classifier Model N Trees Vs. Area under the ROC Curve\")\nplt.xlabel(\"N Trees\")\nplt.ylabel(\"AUC\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:10:34.884619Z","iopub.execute_input":"2021-09-27T04:10:34.88525Z","iopub.status.idle":"2021-09-27T04:10:35.074018Z","shell.execute_reply.started":"2021-09-27T04:10:34.885209Z","shell.execute_reply":"2021-09-27T04:10:35.073303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n<a id=\"34\"></a>\n\n### Model Selection\n\nBased on the above chart, performance stopped improving at 1550 estimators.\n\n{50: 0.6903504543569663,\n 550: 0.7900936067312764,\n 1050: 0.7938331780560485,\n 1550: 0.7941154722257339,\n 2050: 0.7941154722257339,\n 2550: 0.7941154722257339,\n 3050: 0.7941154722257339,\n 3550: 0.7941154722257339,\n 4050: 0.7941154722257339,\n 4550: 0.7941154722257339}","metadata":{"execution":{"iopub.status.busy":"2021-09-27T03:45:40.750337Z","iopub.execute_input":"2021-09-27T03:45:40.751034Z","iopub.status.idle":"2021-09-27T03:45:40.75873Z","shell.execute_reply.started":"2021-09-27T03:45:40.750998Z","shell.execute_reply":"2021-09-27T03:45:40.757989Z"}}},{"cell_type":"code","source":"results","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:08:12.067837Z","iopub.execute_input":"2021-09-27T04:08:12.068112Z","iopub.status.idle":"2021-09-27T04:08:12.073887Z","shell.execute_reply.started":"2021-09-27T04:08:12.068076Z","shell.execute_reply":"2021-09-27T04:08:12.073125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n<a id=\"35\"></a>\n\n### Final Model","metadata":{}},{"cell_type":"code","source":"final_model = XGBClassifier(random_state=0, verbosity=0, tree_method='gpu_hist',use_label_encoder=False,n_estimators=1550,learning_rate=0.05,n_jobs=4)\nfinal_model.fit(imputed_X_train, y_train,\n             verbose = False,\n             eval_set = [(imputed_X_valid, y_valid)],\n             eval_metric = \"auc\",\n             early_stopping_rounds = 200)\npreds_valid = final_model.predict_proba(imputed_X_valid)[:,1]\nprint(roc_auc_score(y_valid, preds_valid))","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:08:12.075539Z","iopub.execute_input":"2021-09-27T04:08:12.076106Z","iopub.status.idle":"2021-09-27T04:09:08.057841Z","shell.execute_reply.started":"2021-09-27T04:08:12.076064Z","shell.execute_reply":"2021-09-27T04:09:08.05693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n<a id=\"36\"></a>\n\n### Final Baseline Prediction","metadata":{}},{"cell_type":"code","source":"predictions = final_model.predict_proba(imputed_X_test)[:,1]\n\n# Save the predictions to a CSV file\noutput = pd.DataFrame({'Id': imputed_X_test.index,\n                       'claim': predictions})\noutput.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T04:09:08.059295Z","iopub.execute_input":"2021-09-27T04:09:08.059565Z","iopub.status.idle":"2021-09-27T04:09:23.567448Z","shell.execute_reply.started":"2021-09-27T04:09:08.059528Z","shell.execute_reply":"2021-09-27T04:09:23.566499Z"},"trusted":true},"execution_count":null,"outputs":[]}]}