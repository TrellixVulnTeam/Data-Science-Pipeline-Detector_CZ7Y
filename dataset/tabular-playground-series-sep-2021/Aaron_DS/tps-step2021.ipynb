{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-30T14:27:49.475976Z","iopub.execute_input":"2021-10-30T14:27:49.47649Z","iopub.status.idle":"2021-10-30T14:27:49.509728Z","shell.execute_reply.started":"2021-10-30T14:27:49.47636Z","shell.execute_reply":"2021-10-30T14:27:49.508847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# September 2021 Tabular Playground\nThis notebook aims to show an entire workflow of Data Science by using the dataset from September 2021 Tabular Playground. A competitive performance is ensured.\n* [**Author**](https://www.linkedin.com/in/chi-wang-22a337207/)\n* [**Dataset**](https://www.kaggle.com/c/tabular-playground-series-sep-2021/data)","metadata":{}},{"cell_type":"markdown","source":"# Key Findings\n* Generate a feature of **missing value number of each record** can help to significantly improve the performance.(Because of the scenario of the data: whether a customer made a claim upon an insurance policy, missing information seems doubtful)\n* Data with **appropriate group imputation** are better than do noting with the missing value. The group key in this dataset is whether there is missing value of a record (**missing_sign**).\n* It can be seen that if I use the label(\"claim\") as the group imputation key, the performance will reach to about 0.9 AUC(I didn't show this in the notebook), although it is not allowed because of the data leakage. However, it indicates that find or generate highly related features to label can help to improve the performance a lot. ---**The importance of feature**\n* Calculate **mutual information** or simplely visualize each feature can find several insights before modelling. Compare these results with the results from feature importance.\n* Use the **aggregate results** from the chosen model can help to slightly improve the final results.(If the performance of the chosen models are quite competitive). Eg. CatBoost and LightGBM in this notebook.","metadata":{}},{"cell_type":"markdown","source":"# Issues\n* Broader range of parameter tuning sets are required. (The limitation of Kaggle's resource)\n* Could try to generate more important feature to boost the performance.\n* Efficient way to find the optimal parameter","metadata":{}},{"cell_type":"markdown","source":"# Reference(Thanks for Inspiration)\n* [TPS September 2021 EDA](https://www.kaggle.com/dwin183287/tps-september-2021-eda)\n* [Takeaways from TPS Sep 2021](https://www.kaggle.com/c/tabular-playground-series-sep-2021/discussion/274966)","metadata":{}},{"cell_type":"markdown","source":"# Table of Content\n1. [Data Overview](#1)\n    * [1. Load Data](#1.1)\n    * [2. Data Type](#1.2)\n    * [3. Statistical View](#1.3)\n2. [Data Preprocessing](#2)\n    * [1. Drop Irrelevant Columns](#2.1)\n    * [2. Missing Value Detection](#2.2)\n    * [3. New Feature Generation](#2.3)\n    * [4. Data Imputation](#2.4)\n        * [1. Drop all NaN/NA/null](#2.4.1)\n        * [2. Median Imputation](#2.4.2)\n        * [3. Mean Imputation](#2.4.3)\n3. [Data Analysis](#3)\n    * [1. What is the distribution of claim? ](#3.1)\n    * [2. What is the distribution of features on claim? ](#3.2)\n4. [Feature Engineering](#4)\n    * [1. Correlation Analysis ](#4.1)\n    * [2. Mutual Information ](#4.2)\n5. [Modelling](#5)\n    * [1. Train Test Split ](#5.1)\n    * [2. Train Models ](#5.2)\n        * [0. Manually Prediction ](#5.2.0)\n        * [1. XGboost ](#5.2.1)\n        * [2. CatBoost ](#5.2.2)\n        * [3. LightGBM ](#5.2.3)\n    * [3. Model Comparison ](#5.3)\n    * [4. Best Model Explaination ](#5.4)\n    * [5. Parameter/Feature Tuning ](#5.5)\n6. [Prediction](#6)\n    * [1. Load Data](#6.1)\n    * [2. Drop Irrelevant Columns](#6.2)\n    * [3. New Feature Generation](#6.3)\n    * [4. Data Imputation](#6.4)\n    * [5. Make Prediction](#6.5)\n    * [6. Save the Prediction to CSV file](#6.6)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"1\"></a>\n# 1. Data Overview","metadata":{}},{"cell_type":"code","source":"# Import packages\nimport time\nimport gc\n\n## Basic data processing\nimport numpy as np\nimport pandas as pd\n\n## Data Visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\n\n## Modelling\nfrom sklearn.feature_selection import mutual_info_classif\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.metrics import roc_curve, roc_auc_score\nfrom sklearn.pipeline import make_pipeline\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier, plot_importance\n\n## Model Explanatory\nimport shap  # package used to calculate Shap values\nimport eli5\n\n## Settings\npd.set_option('display.max_columns', 500) # Able to display more columns.\npd.set_option('display.max_info_columns', 150) # Able to display more columns in info().","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"1.1\"></a>\n## 1.1. Load Data","metadata":{}},{"cell_type":"code","source":"# Load the dataset\ndata_df = pd.read_csv(\"../input/tabular-playground-series-sep-2021/train.csv\")\ndata_df.info() # show entries, dtypes, memory useage.","metadata":{"execution":{"iopub.status.busy":"2021-10-30T14:28:00.261261Z","iopub.execute_input":"2021-10-30T14:28:00.262087Z","iopub.status.idle":"2021-10-30T14:28:30.35813Z","shell.execute_reply.started":"2021-10-30T14:28:00.262039Z","shell.execute_reply":"2021-10-30T14:28:30.356921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Have a look\ndata_df.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-10-30T14:28:30.360744Z","iopub.execute_input":"2021-10-30T14:28:30.361092Z","iopub.status.idle":"2021-10-30T14:28:30.483483Z","shell.execute_reply.started":"2021-10-30T14:28:30.36105Z","shell.execute_reply":"2021-10-30T14:28:30.482632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* int64: id and claim\n* float64: all other features","metadata":{}},{"cell_type":"markdown","source":"<a id=\"1.2\"></a>\n## 1.2. Data Type\n\n> [NOIR](https://www.questionpro.com/blog/nominal-ordinal-interval-ratio/): Nominal, Ordinal, Interval, Ratio.  \n\nAs the features in this dataset have been anonymized, we just assume that the data type of each feature is what it looks like. --**Ratio**","metadata":{}},{"cell_type":"markdown","source":"<a id=\"1.3\"></a>\n## 1.3. Statistical View ","metadata":{}},{"cell_type":"code","source":"# Basic statistic on labels\ndata_df[\"claim\"].astype(\"object\").describe() # All the Nominal data can be treated as \"object\" type for simplicity.","metadata":{"execution":{"iopub.status.busy":"2021-10-30T14:28:30.484632Z","iopub.execute_input":"2021-10-30T14:28:30.484883Z","iopub.status.idle":"2021-10-30T14:28:30.73035Z","shell.execute_reply.started":"2021-10-30T14:28:30.484854Z","shell.execute_reply":"2021-10-30T14:28:30.729579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are two classes in label(claim), quite balanced.","metadata":{}},{"cell_type":"code","source":"# Basic statistic on features\ndata_df.loc[:, ~data_df.columns.isin([\"claim\"])].describe()","metadata":{"execution":{"iopub.status.busy":"2021-10-30T14:28:30.731493Z","iopub.execute_input":"2021-10-30T14:28:30.731747Z","iopub.status.idle":"2021-10-30T14:28:37.638883Z","shell.execute_reply.started":"2021-10-30T14:28:30.73172Z","shell.execute_reply":"2021-10-30T14:28:37.637968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is not very intuitive to see any insights from the pure statistic results. However, the range of several feature are quite huge that need to be noticed.  \nf9, f12, f26, f27, f35, f62, f73, f74, f82, f86, f98, f108, f116","metadata":{}},{"cell_type":"markdown","source":"<a id=\"2\"></a>\n# 2. Data Preprocessing","metadata":{}},{"cell_type":"markdown","source":"<a id=\"2.1\"></a>\n## 2.1. Drop Irrelevant Columns","metadata":{}},{"cell_type":"code","source":"# Irrelevant columns\n'''\nid: id is useless for analysis and modeling.\n'''\nirrelevant_columns = ['id']\ndata_preprocessed_df = data_df.drop(irrelevant_columns, axis=1)\ndata_preprocessed_df","metadata":{"execution":{"iopub.status.busy":"2021-10-30T14:28:37.640233Z","iopub.execute_input":"2021-10-30T14:28:37.640465Z","iopub.status.idle":"2021-10-30T14:28:38.111783Z","shell.execute_reply.started":"2021-10-30T14:28:37.640441Z","shell.execute_reply":"2021-10-30T14:28:38.110931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2.2\"></a>\n## 2.2. Missing Value Detection","metadata":{}},{"cell_type":"code","source":"# Replace the empty data with NaN\ndata_preprocessed_df.replace(\"\", float(\"NaN\"), inplace=True)\ndata_preprocessed_df.replace(\" \", float(\"NaN\"), inplace=True)\n\n# Count missing value(NaN, na, null, None) of each columns, Then transform the result to a pandas dataframe. \ncount_missing_value = data_preprocessed_df.isna().sum() / data_preprocessed_df.shape[0] * 100\ncount_missing_value_df = pd.DataFrame(count_missing_value.sort_values(ascending=False), columns=['Missing%'])","metadata":{"execution":{"iopub.status.busy":"2021-10-30T14:28:38.112959Z","iopub.execute_input":"2021-10-30T14:28:38.113192Z","iopub.status.idle":"2021-10-30T14:28:38.374854Z","shell.execute_reply.started":"2021-10-30T14:28:38.113166Z","shell.execute_reply":"2021-10-30T14:28:38.373934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualize the percentage(>0) of Missing value in each column.\nmissing_value_df = count_missing_value_df[count_missing_value_df['Missing%'] > 0]\n\nplt.figure(figsize=(10, 20)) # Set the figure size\nmissing_value_graph = sns.barplot(y = missing_value_df.index, x = \"Missing%\", data=missing_value_df, orient=\"h\")\nmissing_value_graph.set_title(\"Percentage Missing Value\", fontsize = 20)\nmissing_value_graph.set_xlabel(\"Features\")","metadata":{"execution":{"iopub.status.busy":"2021-10-30T14:28:38.37755Z","iopub.execute_input":"2021-10-30T14:28:38.377823Z","iopub.status.idle":"2021-10-30T14:28:40.455991Z","shell.execute_reply.started":"2021-10-30T14:28:38.377794Z","shell.execute_reply":"2021-10-30T14:28:40.455081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems there aren't many missing value in each feature. However, almost all the features have missing value **~=1.6%**.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"2.3\"></a>\n## 2.3. New Feature Generation","metadata":{}},{"cell_type":"code","source":"# Generate a new feature from the missing value of each records, it is inspired by the scenario of the dataset.(whether a customer made a claim upon an insurance policy)\n# The less the information of the user, the higher risk of the user.\ndata_preprocessed_df['missing_num'] = data_preprocessed_df.isnull().sum(axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-10-30T14:28:40.457659Z","iopub.execute_input":"2021-10-30T14:28:40.458Z","iopub.status.idle":"2021-10-30T14:28:40.809857Z","shell.execute_reply.started":"2021-10-30T14:28:40.457968Z","shell.execute_reply":"2021-10-30T14:28:40.809061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate a new feature by counting the negative value of each records.\ndata_preprocessed_df['neg_num'] = (data_preprocessed_df < 0).sum(axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-10-30T14:28:40.810951Z","iopub.execute_input":"2021-10-30T14:28:40.811179Z","iopub.status.idle":"2021-10-30T14:28:41.176541Z","shell.execute_reply.started":"2021-10-30T14:28:40.811154Z","shell.execute_reply":"2021-10-30T14:28:41.175645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate a new feature from the missing_num.(Binary value to represent whether there is missing value in this record)\ndata_preprocessed_df['missing_sign'] = data_preprocessed_df['missing_num'] != 0\ndata_preprocessed_df['missing_sign'] = data_preprocessed_df['missing_sign'] + 0","metadata":{"execution":{"iopub.status.busy":"2021-10-30T14:28:41.178093Z","iopub.execute_input":"2021-10-30T14:28:41.178419Z","iopub.status.idle":"2021-10-30T14:28:41.191453Z","shell.execute_reply.started":"2021-10-30T14:28:41.178381Z","shell.execute_reply":"2021-10-30T14:28:41.19044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2.4\"></a>\n## 2.4. Data Imputation\n> Choose the suitable imputation tech which can highly represent the central tendency of the data.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"2.4.1\"></a>\n### 2.4.1. Drop all NaN/NA/null ","metadata":{}},{"cell_type":"code","source":"# Drop all the instance with NaN/NA/null\ndata_preprocessed_dropNaN_df = data_preprocessed_df.dropna().copy()\ndata_preprocessed_dropNaN_df.reset_index(drop=True, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-10-30T14:28:41.19291Z","iopub.execute_input":"2021-10-30T14:28:41.193212Z","iopub.status.idle":"2021-10-30T14:28:42.060898Z","shell.execute_reply.started":"2021-10-30T14:28:41.193174Z","shell.execute_reply":"2021-10-30T14:28:42.059961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The percentage of rows with missing value\n(data_preprocessed_df.shape[0] - data_preprocessed_dropNaN_df.shape[0]) / data_preprocessed_df.shape[0] * 100","metadata":{"execution":{"iopub.status.busy":"2021-10-30T14:28:42.062183Z","iopub.execute_input":"2021-10-30T14:28:42.062823Z","iopub.status.idle":"2021-10-30T14:28:42.069057Z","shell.execute_reply.started":"2021-10-30T14:28:42.062786Z","shell.execute_reply":"2021-10-30T14:28:42.068133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems that we can't drop missing value directly. Although the missing value in each feature is quite few, it randomly appeared in each row. Therefore, there are almost 40% rows have missing value.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"2.4.2\"></a>\n### 2.4.2. Median Imputation ","metadata":{}},{"cell_type":"code","source":"# Get the feature names with missing values.\nmissing_features = list(missing_value_df.index)","metadata":{"execution":{"iopub.status.busy":"2021-10-30T14:28:42.070091Z","iopub.execute_input":"2021-10-30T14:28:42.070327Z","iopub.status.idle":"2021-10-30T14:28:42.080551Z","shell.execute_reply.started":"2021-10-30T14:28:42.070301Z","shell.execute_reply":"2021-10-30T14:28:42.079727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Grouped Median imputation by a feature which is highly related to label(claim): missing_sign\ndata_preprocessed_median_df = data_preprocessed_df.copy()\nfor feature in missing_features: \n    data_preprocessed_median_df[feature] = data_preprocessed_df.groupby('missing_sign', sort=False)[feature].apply(lambda x: x.fillna(x.median()))","metadata":{"execution":{"iopub.status.busy":"2021-10-30T14:28:42.082027Z","iopub.execute_input":"2021-10-30T14:28:42.082517Z","iopub.status.idle":"2021-10-30T14:29:07.505843Z","shell.execute_reply.started":"2021-10-30T14:28:42.082485Z","shell.execute_reply":"2021-10-30T14:29:07.504875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2.4.3\"></a>\n### 2.4.3. Mean Imputation ","metadata":{}},{"cell_type":"code","source":"# Grouped Mean imputation by a feature which is highly related to label(claim): missing_sign\ndata_preprocessed_mean_df = data_preprocessed_df.copy()\nfor feature in missing_features:\n    data_preprocessed_mean_df[feature] = data_preprocessed_df.groupby('missing_sign', sort=False)[feature].apply(lambda x: x.fillna(x.mean()))","metadata":{"execution":{"iopub.status.busy":"2021-10-30T14:29:07.50722Z","iopub.execute_input":"2021-10-30T14:29:07.508242Z","iopub.status.idle":"2021-10-30T14:29:34.093796Z","shell.execute_reply.started":"2021-10-30T14:29:07.508208Z","shell.execute_reply":"2021-10-30T14:29:34.092919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then, we get 4 datasets:\n* **data_preprocessed_df**: The original data without imputation.\n* **data_preprocessed_dropNaN_df**: Simply drop all the ***NaN*** value.\n* **data_preprocessed_median_df**: Grouped Median imputation on ***claim***.\n* **data_preprocessed_mean_df**: Grouped Mean imputation on ***claim***.","metadata":{}},{"cell_type":"code","source":"# Set one of the dataset for analysis and modeling; Choose the one with the best performance at last.\n#data_best_df = data_preprocessed_df.copy()\n#data_best_df = data_preprocessed_dropNaN_df.copy()\n#data_best_df = data_preprocessed_median_df.copy()\ndata_best_df = data_preprocessed_mean_df.copy()","metadata":{"execution":{"iopub.status.busy":"2021-10-30T14:29:34.095334Z","iopub.execute_input":"2021-10-30T14:29:34.096798Z","iopub.status.idle":"2021-10-30T14:29:34.841649Z","shell.execute_reply.started":"2021-10-30T14:29:34.096752Z","shell.execute_reply":"2021-10-30T14:29:34.840787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_best_df","metadata":{"execution":{"iopub.status.busy":"2021-10-30T14:29:34.843174Z","iopub.execute_input":"2021-10-30T14:29:34.843821Z","iopub.status.idle":"2021-10-30T14:29:35.002729Z","shell.execute_reply.started":"2021-10-30T14:29:34.84378Z","shell.execute_reply":"2021-10-30T14:29:35.001734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3\"></a>\n# 3. Data Analysis","metadata":{}},{"cell_type":"markdown","source":"<a id=\"3.1\"></a>\n## 3.1. What is the distribution of claim?","metadata":{}},{"cell_type":"code","source":"# Count the number of claim(0/1), transform the result to pandas dataframe\nclaim_counts = data_best_df[\"claim\"].value_counts()\nclaim_counts_df = pd.DataFrame(claim_counts)","metadata":{"execution":{"iopub.status.busy":"2021-10-30T14:29:35.004245Z","iopub.execute_input":"2021-10-30T14:29:35.004984Z","iopub.status.idle":"2021-10-30T14:29:35.015621Z","shell.execute_reply.started":"2021-10-30T14:29:35.00494Z","shell.execute_reply":"2021-10-30T14:29:35.014969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualize the distribution of the claim(label)\nclaim_fig = make_subplots(\n    rows=1, cols=2, \n    specs=[[{\"type\": \"xy\"}, {\"type\": \"domain\"}]])\n\nclaim_fig.add_trace(go.Bar(x=claim_counts_df.index, \n                           y=claim_counts_df[\"claim\"],\n                           text=claim_counts_df[\"claim\"],\n                           textposition='outside',\n                           showlegend=False),\n                           1, 1)\n\nclaim_fig.add_trace(go.Pie(labels=claim_counts_df.index, \n                           values=claim_counts_df[\"claim\"],\n                           showlegend=True),\n                           1, 2)\n\nclaim_fig.update_layout(\n                  height=600, \n                  width=1000,\n                  title={\n                  'text': \"The distribution of claim\",\n                  'font': {'size': 24},\n                  'y':0.95,\n                  'x':0.5,\n                  'xanchor': 'center',\n                  'yanchor': 'top'},\n                  xaxis1_title = 'claim', \n                  yaxis1_title = 'Counts',\n                  legend_title_text=\"claim\"\n                 )\nclaim_fig.update_xaxes(type='category')\nclaim_fig.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-30T14:29:35.016624Z","iopub.execute_input":"2021-10-30T14:29:35.017568Z","iopub.status.idle":"2021-10-30T14:29:35.306309Z","shell.execute_reply.started":"2021-10-30T14:29:35.01753Z","shell.execute_reply":"2021-10-30T14:29:35.305389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The label(claim) is quite balanced.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"3.2\"></a>\n## 3.2. What is the distribution of features on claim?","metadata":{}},{"cell_type":"code","source":"# Set up the matplotlib figure\n#f, axes = plt.subplots(2, 6, figsize=(30, 10)) #suitable for two line with 6 graph.\n\nf, axes = plt.subplots(20, 6, figsize=(30, 100))\nfor feature,number in zip(data_best_df.columns, range(118)):\n    yaxix_name = 'f'+str(number+1)\n    r_pos = number // 6\n    c_pos = number % 6\n    sns.boxplot(x='claim', y=yaxix_name, data=data_best_df, ax=axes[r_pos, c_pos]).set_title(yaxix_name)","metadata":{"execution":{"iopub.status.busy":"2021-10-30T14:29:35.30764Z","iopub.execute_input":"2021-10-30T14:29:35.307909Z","iopub.status.idle":"2021-10-30T14:30:25.820562Z","shell.execute_reply.started":"2021-10-30T14:29:35.307882Z","shell.execute_reply":"2021-10-30T14:30:25.819573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are not any feature have an significant difference on claim. If we have to say the slight different on claim, the features are: **f3, f5, f8, f9, f21, f31, f34, f38, f45, f57**.","metadata":{}},{"cell_type":"code","source":"# Plot features we created\nnew_features=['missing_num','neg_num', 'missing_sign']\nf, axes = plt.subplots(2, 2, figsize=(15, 10)) #suitable for two line with 6 graph.\nfor feature,number in zip(new_features, range(4)):\n    yaxix_name = feature\n    r_pos = number // 2\n    c_pos = number % 2\n    sns.boxplot(x='claim', y=yaxix_name, data=data_best_df, ax=axes[r_pos, c_pos]).set_title(yaxix_name)","metadata":{"execution":{"iopub.status.busy":"2021-10-30T14:30:25.821968Z","iopub.execute_input":"2021-10-30T14:30:25.822599Z","iopub.status.idle":"2021-10-30T14:30:27.638682Z","shell.execute_reply.started":"2021-10-30T14:30:25.822554Z","shell.execute_reply":"2021-10-30T14:30:27.637728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It proves that feature \"**missing_num**\" is important to label(\"claim\"), so as \"**missing_sign**\".","metadata":{}},{"cell_type":"markdown","source":"<a id=\"4\"></a>\n# 4. Feature Engineering\nAs all the features of the dataset are not categorical, there is no need to do with encoding. However, there are more than 100 features in this dataset, it is better to do **dimension reduction**.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"4.1\"></a>\n## 4.1. Correlation Analysis","metadata":{}},{"cell_type":"code","source":"# Show the heatmap\nplt.figure(figsize=(160, 120))\nsns.heatmap(data_best_df.drop('claim', axis=1).corr(), cmap=\"coolwarm\", annot = True, fmt='.3f').set_title('Pearson Correlation for continuous features', fontsize=22)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-30T14:30:27.642305Z","iopub.execute_input":"2021-10-30T14:30:27.642688Z","iopub.status.idle":"2021-10-30T14:32:09.734535Z","shell.execute_reply.started":"2021-10-30T14:30:27.642653Z","shell.execute_reply":"2021-10-30T14:32:09.733489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can not see any significant correlation between each feature.  \nNote: The correlation between \"missing_num\" and \"missing_sign\" is quite high. Because \"missing_sign\" comes from  \"missing_num\", which is used as the group imputation key.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"4.2\"></a>\n## 4.2. Mutual Information","metadata":{}},{"cell_type":"code","source":"# Calculate the mutual information of the dataset(Require No NaN)\ndef make_mi_scores(X, y, discrete_features):\n    mi_scores = mutual_info_classif(X, y, discrete_features=discrete_features)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores","metadata":{"execution":{"iopub.status.busy":"2021-10-30T14:32:09.736017Z","iopub.execute_input":"2021-10-30T14:32:09.736282Z","iopub.status.idle":"2021-10-30T14:32:09.742369Z","shell.execute_reply.started":"2021-10-30T14:32:09.736251Z","shell.execute_reply":"2021-10-30T14:32:09.741207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mi_scores = make_mi_scores(data_best_df, data_best_df['claim'], False)\nmi_scores[:10]  # show a few features with their MI scores","metadata":{"execution":{"iopub.status.busy":"2021-10-30T14:32:09.743736Z","iopub.execute_input":"2021-10-30T14:32:09.743998Z","iopub.status.idle":"2021-10-30T15:14:01.747678Z","shell.execute_reply.started":"2021-10-30T14:32:09.74397Z","shell.execute_reply":"2021-10-30T15:14:01.746606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Important feature discovery:\n1. Visualization: **f3, f5, f8, f9, f21, f31, f34, f38, f45, f57, missing_num**\n2. Mutual information: **missing_sign, missing_num, neg_num, f70, f11, f78, f52, f75, f118, f40**","metadata":{}},{"cell_type":"markdown","source":"<a id=\"5\"></a>\n# 5. Modelling\n\n* **data_preprocessed_df**: The original data without imputation.\n* **data_preprocessed_dropNaN_df**: Simply drop all the ***NaN*** value.\n* **data_preprocessed_median_df**: Grouped Median imputation on ***claim***.\n* **data_preprocessed_mean_df**: Grouped Mean imputation on ***claim***.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"5.1\"></a>\n## 5.1. Train Test Split","metadata":{}},{"cell_type":"code","source":"# Choose a dataset for modelling\ndata_modelling_df = data_preprocessed_mean_df.copy()","metadata":{"execution":{"iopub.status.busy":"2021-10-30T15:14:01.74911Z","iopub.execute_input":"2021-10-30T15:14:01.749348Z","iopub.status.idle":"2021-10-30T15:14:02.158799Z","shell.execute_reply.started":"2021-10-30T15:14:01.749323Z","shell.execute_reply":"2021-10-30T15:14:02.157975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train/Test Split\nX = data_modelling_df.drop(\"claim\", axis=1)\nY = data_modelling_df.claim\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.25, random_state=0)","metadata":{"execution":{"iopub.status.busy":"2021-10-30T15:14:02.160087Z","iopub.execute_input":"2021-10-30T15:14:02.160339Z","iopub.status.idle":"2021-10-30T15:14:04.024365Z","shell.execute_reply.started":"2021-10-30T15:14:02.160313Z","shell.execute_reply":"2021-10-30T15:14:04.023508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the evaluation dataset\neval_sets=[(x_train, y_train), (x_test, y_test)]","metadata":{"execution":{"iopub.status.busy":"2021-10-30T15:14:04.02595Z","iopub.execute_input":"2021-10-30T15:14:04.026614Z","iopub.status.idle":"2021-10-30T15:14:04.031526Z","shell.execute_reply.started":"2021-10-30T15:14:04.026548Z","shell.execute_reply":"2021-10-30T15:14:04.030735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"5.2\"></a>\n## 5.2. Train Models\n> Let's use three state-of-art ensembled models to make prediction\n\n* [XGBoost](https://xgboost.readthedocs.io/en/latest/)\n* [CatBoost](https://catboost.ai/)\n* [LightGBM](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.Booster.html)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"5.2.0\"></a>\n### 5.2.0. Manually Prediction ","metadata":{}},{"cell_type":"code","source":"# Use the missing_sign to simplely represent \"claim\"\nprint(f'Train AUC: {roc_auc_score(y_train, x_train[\"missing_sign\"])}')\nprint(f'Test AUC: {roc_auc_score(y_test, x_test[\"missing_sign\"])}')","metadata":{"execution":{"iopub.status.busy":"2021-10-30T15:14:04.033087Z","iopub.execute_input":"2021-10-30T15:14:04.033403Z","iopub.status.idle":"2021-10-30T15:14:04.268183Z","shell.execute_reply.started":"2021-10-30T15:14:04.033364Z","shell.execute_reply":"2021-10-30T15:14:04.267297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"5.2.1\"></a>\n### 5.2.1 XGboost","metadata":{}},{"cell_type":"code","source":"# Start time\nstart_time = time.time()\nxgbc = XGBClassifier(random_state=0, use_label_encoder=False)\nxgbc.fit(x_train, y_train, eval_set=eval_sets, eval_metric='auc', verbose=False)\n\n# Calculate the training time\nxgbc_time = time.time() - start_time\n\n# xgbc.evals_result() #Return the evaluation results of eval_sets\npredictions = xgbc.predict_proba(x_test)[:,1]\nauc_xgbc = roc_auc_score(y_test, predictions)\nprint(f'AUC: {auc_xgbc}')","metadata":{"execution":{"iopub.status.busy":"2021-10-30T15:14:04.269489Z","iopub.execute_input":"2021-10-30T15:14:04.269754Z","iopub.status.idle":"2021-10-30T15:25:17.436295Z","shell.execute_reply.started":"2021-10-30T15:14:04.269726Z","shell.execute_reply":"2021-10-30T15:25:17.435319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"5.2.1\"></a>\n### 5.2.1 CatBoost","metadata":{}},{"cell_type":"code","source":"# Start time\nstart_time = time.time()\ncatbc = CatBoostClassifier(random_state=0, eval_metric='AUC')\ncatbc.fit(x_train, y_train, eval_set=eval_sets, verbose=False)\n\n# Calculate the training time\ncatbc_time = time.time() - start_time\npredictions = catbc.predict_proba(x_test)[:,1]\nauc_catbc = roc_auc_score(y_test, predictions)\nprint(f'AUC: {auc_catbc}')","metadata":{"execution":{"iopub.status.busy":"2021-10-30T15:25:17.437744Z","iopub.execute_input":"2021-10-30T15:25:17.43798Z","iopub.status.idle":"2021-10-30T15:30:54.724529Z","shell.execute_reply.started":"2021-10-30T15:25:17.437952Z","shell.execute_reply":"2021-10-30T15:30:54.723581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"5.2.3\"></a>\n### 5.2.3 LGBM","metadata":{}},{"cell_type":"code","source":"# Start time\nstart_time = time.time()\n\nlgbc = LGBMClassifier(random_state=0)\nlgbc.fit(x_train, y_train, eval_set=eval_sets, eval_metric='auc', verbose=-1)\n\n# Calculate the training time\nlgbc_time = time.time() - start_time\npredictions = lgbc.predict_proba(x_test)[:,1]\nauc_lgbc = roc_auc_score(y_test, predictions)\nprint(f'AUC: {auc_lgbc}')","metadata":{"execution":{"iopub.status.busy":"2021-10-30T15:30:54.725781Z","iopub.execute_input":"2021-10-30T15:30:54.726004Z","iopub.status.idle":"2021-10-30T15:31:37.697201Z","shell.execute_reply.started":"2021-10-30T15:30:54.725977Z","shell.execute_reply":"2021-10-30T15:31:37.696041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"5.3\"></a>\n## 5.3. Model Comparison","metadata":{}},{"cell_type":"code","source":"# Collect all the model performance\nmodel_comparison = pd.DataFrame(data = [(auc_xgbc, xgbc_time), (auc_catbc, catbc_time), (auc_lgbc, lgbc_time)], \n                                index = [\"XGboost\", \"CatBoost\", \"LGBM\"],\n                                columns=['AUC', 'Time'])\\\n                     .sort_values(by = \"AUC\", ascending=False)\nmodel_comparison","metadata":{"execution":{"iopub.status.busy":"2021-10-30T15:31:37.698791Z","iopub.execute_input":"2021-10-30T15:31:37.69904Z","iopub.status.idle":"2021-10-30T15:31:37.715487Z","shell.execute_reply.started":"2021-10-30T15:31:37.699013Z","shell.execute_reply":"2021-10-30T15:31:37.714283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems that **CatBoost** model outperform all the chosen models. **LGBM** performs a little bit worse, but the training time is quite short.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"5.4\"></a>\n## 5.4. Best Model Explaination","metadata":{}},{"cell_type":"code","source":"# Save the feature importance as a dataframe  \ncatbc_importances_df = pd.DataFrame(pd.Series(catbc.feature_importances_, index=x_train.columns), columns=['Importance']).sort_values('Importance', ascending=False)[:10]\n# Visualize the feature importance of the trained tree\nplt.figure(figsize=(10, 10))\nfeature_importance_graph = sns.barplot(y = catbc_importances_df.index, x = \"Importance\", data=catbc_importances_df, orient=\"h\")\nfeature_importance_graph.set_title(\"Top 10 Feature importance by CatBoost Classification\", fontsize = 20)\nfeature_importance_graph.set_ylabel(\"Features\")\n# Use eli5 to show the value of feature importance with colors\neli5.show_weights(catbc, feature_names = list(x_train.columns))","metadata":{"execution":{"iopub.status.busy":"2021-10-30T15:31:37.717247Z","iopub.execute_input":"2021-10-30T15:31:37.717569Z","iopub.status.idle":"2021-10-30T15:31:38.063632Z","shell.execute_reply.started":"2021-10-30T15:31:37.717528Z","shell.execute_reply":"2021-10-30T15:31:38.062727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The top 10 important feature are: **missing_num, missing_sign, f40, f70, f47, f57, f35, f1, f106, f5**.","metadata":{}},{"cell_type":"code","source":"# Shap\nexplainer = shap.TreeExplainer(catbc)\nshap_values = explainer.shap_values(x_train)\nshap.summary_plot(shap_values, x_train)","metadata":{"execution":{"iopub.status.busy":"2021-10-30T15:31:38.065143Z","iopub.execute_input":"2021-10-30T15:31:38.065387Z","iopub.status.idle":"2021-10-30T15:35:39.499778Z","shell.execute_reply.started":"2021-10-30T15:31:38.065358Z","shell.execute_reply":"2021-10-30T15:35:39.49878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"5.5\"></a>\n## 5.5. Parameter/Feature Tuning  ","metadata":{}},{"cell_type":"code","source":"data_lst = [data_preprocessed_df, data_preprocessed_dropNaN_df, data_preprocessed_median_df, data_preprocessed_mean_df]","metadata":{"execution":{"iopub.status.busy":"2021-10-30T15:35:39.501492Z","iopub.execute_input":"2021-10-30T15:35:39.501887Z","iopub.status.idle":"2021-10-30T15:35:39.507642Z","shell.execute_reply.started":"2021-10-30T15:35:39.501843Z","shell.execute_reply":"2021-10-30T15:35:39.506738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tuning_res = []\n\nfor dataset_df in data_lst:\n    # Train/Test Split\n    X = dataset_df.drop(\"claim\", axis=1)\n    Y = dataset_df.claim\n    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.25, random_state=0)\n    \n    # Start time\n    start_time = time.time()\n    catbc = CatBoostClassifier(random_state=0, eval_metric='AUC')\n    catbc.fit(x_train, y_train, verbose=False)\n    \n    # Calculate the training time\n    catbc_time = time.time() - start_time\n    predictions = catbc.predict_proba(x_test)[:,1]\n    auc_catbc = roc_auc_score(y_test, predictions)\n    tuning_res.append((auc_catbc, catbc_time))","metadata":{"execution":{"iopub.status.busy":"2021-10-30T15:35:39.509058Z","iopub.execute_input":"2021-10-30T15:35:39.509343Z","iopub.status.idle":"2021-10-30T15:49:31.200847Z","shell.execute_reply.started":"2021-10-30T15:35:39.509316Z","shell.execute_reply":"2021-10-30T15:49:31.19998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compare performance on different dataset\ndataset_comparison = pd.DataFrame(data = tuning_res, \n                                index = [\"data_preprocessed_df\", \"data_preprocessed_dropNaN_df\", \"data_preprocessed_median_df\", \"data_preprocessed_mean_df\"],\n                                columns=['AUC', 'Time'])\\\n                     .sort_values(by = \"AUC\", ascending=False)\ndataset_comparison","metadata":{"execution":{"iopub.status.busy":"2021-10-30T15:49:31.202568Z","iopub.execute_input":"2021-10-30T15:49:31.203264Z","iopub.status.idle":"2021-10-30T15:49:31.218965Z","shell.execute_reply.started":"2021-10-30T15:49:31.203214Z","shell.execute_reply":"2021-10-30T15:49:31.217909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems that data with mean value imputation outperform from all the chosen dataset.","metadata":{}},{"cell_type":"code","source":"# Release big variables that are not used in the following.\ndel data_preprocessed_df\ndel data_preprocessed_dropNaN_df\ndel data_preprocessed_median_df\ndel data_best_df\ndel data_df\ndel data_modelling_df\ndel x_train\ndel x_test\n\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use the dataset with the best performance\ndataset_df = data_preprocessed_mean_df.copy()\nX = dataset_df.drop(\"claim\", axis=1)\nY = dataset_df.claim","metadata":{"execution":{"iopub.status.busy":"2021-10-30T16:10:41.147986Z","iopub.execute_input":"2021-10-30T16:10:41.148339Z","iopub.status.idle":"2021-10-30T16:10:42.102712Z","shell.execute_reply.started":"2021-10-30T16:10:41.148305Z","shell.execute_reply":"2021-10-30T16:10:42.101751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del data_preprocessed_mean_df\ndel dataset_df\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-10-30T16:10:49.345473Z","iopub.execute_input":"2021-10-30T16:10:49.345948Z","iopub.status.idle":"2021-10-30T16:10:49.674609Z","shell.execute_reply.started":"2021-10-30T16:10:49.345916Z","shell.execute_reply":"2021-10-30T16:10:49.674016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We can not really do the parameter tuning well on Kaggle because of the limitation of resource\n\n# catbc_best = CatBoostClassifier(random_state=0, eval_metric='AUC')\n# catbc_grid = {'max_depth': [4, 6, 8, 10],\n#               'n_estimators':[100, 300, 500, 1000, 2000],\n#               'min_data_in_leaf': [1, 3, 5, 10]\n#              }\n\n# catbc_search = GridSearchCV(catbc_best, \n#                             catbc_grid,\n#                             scoring=\"roc_auc\",\n#                             n_jobs=-1,\n#                             cv = 5)\n# catbc_search.fit(X,Y, verbose=False)\n\n# # Returns the estimator with the best performance\n# print(catbc_search.best_estimator_)\n\n# # Returns the best score\n# print(catbc_search.best_score_)\n\n# # Returns the best parameters\n# print(catbc_search.best_params_)","metadata":{"execution":{"iopub.status.busy":"2021-10-30T15:49:32.420255Z","iopub.execute_input":"2021-10-30T15:49:32.421096Z","iopub.status.idle":"2021-10-30T15:49:32.426724Z","shell.execute_reply.started":"2021-10-30T15:49:32.421052Z","shell.execute_reply":"2021-10-30T15:49:32.42542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the competitive models on all the known dataset\ncatbc_best = CatBoostClassifier(random_state=0, eval_metric='AUC')\ncatbc_best.fit(X, Y, verbose=False)\n\nlgbc_best = LGBMClassifier(random_state=0)\nlgbc_best.fit(X, Y, eval_metric='auc', verbose=-1)","metadata":{"execution":{"iopub.status.busy":"2021-10-30T16:11:05.215523Z","iopub.execute_input":"2021-10-30T16:11:05.216229Z","iopub.status.idle":"2021-10-30T16:16:59.280869Z","shell.execute_reply.started":"2021-10-30T16:11:05.216193Z","shell.execute_reply":"2021-10-30T16:16:59.280071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del X,Y\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-10-30T16:16:59.284831Z","iopub.execute_input":"2021-10-30T16:16:59.2854Z","iopub.status.idle":"2021-10-30T16:16:59.606001Z","shell.execute_reply.started":"2021-10-30T16:16:59.285363Z","shell.execute_reply":"2021-10-30T16:16:59.605015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Two best model Generated(on all training dataset):\n1. catbc_best\n2. lgbc_best","metadata":{}},{"cell_type":"markdown","source":"<a id=\"6\"></a>\n# 6. Prediction","metadata":{}},{"cell_type":"markdown","source":"<a id=\"6.1\"></a>\n## 6.1. Load Data","metadata":{}},{"cell_type":"code","source":"# Load the dataset\ntest_df = pd.read_csv(\"../input/tabular-playground-series-sep-2021/test.csv\")\ntest_df.info() # show entries, dtypes, memory useage.","metadata":{"execution":{"iopub.status.busy":"2021-10-30T16:19:34.388995Z","iopub.execute_input":"2021-10-30T16:19:34.389349Z","iopub.status.idle":"2021-10-30T16:19:48.535191Z","shell.execute_reply.started":"2021-10-30T16:19:34.389316Z","shell.execute_reply":"2021-10-30T16:19:48.534171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Have a look\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-30T16:19:48.536984Z","iopub.execute_input":"2021-10-30T16:19:48.538831Z","iopub.status.idle":"2021-10-30T16:19:48.656131Z","shell.execute_reply.started":"2021-10-30T16:19:48.538789Z","shell.execute_reply":"2021-10-30T16:19:48.655374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"6.2\"></a>\n## 6.2. Drop Irrelevant Columns","metadata":{}},{"cell_type":"code","source":"# Drop Irrelevant columns\ntest_preprocessed_df = test_df.drop(irrelevant_columns, axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-10-30T16:20:01.848581Z","iopub.execute_input":"2021-10-30T16:20:01.84905Z","iopub.status.idle":"2021-10-30T16:20:02.000852Z","shell.execute_reply.started":"2021-10-30T16:20:01.84902Z","shell.execute_reply":"2021-10-30T16:20:01.999628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"6.3\"></a>\n## 6.3. New Feature Generation","metadata":{}},{"cell_type":"code","source":"# Add a new feature represent the missing value number of each row\ntest_preprocessed_df['missing_num'] = test_preprocessed_df.isnull().sum(axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-10-30T16:20:03.03419Z","iopub.execute_input":"2021-10-30T16:20:03.034515Z","iopub.status.idle":"2021-10-30T16:20:03.13703Z","shell.execute_reply.started":"2021-10-30T16:20:03.034478Z","shell.execute_reply":"2021-10-30T16:20:03.136104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate a new feature by counting the negative value of each records.\ntest_preprocessed_df['neg_num'] = (test_preprocessed_df < 0).sum(axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-10-30T16:20:03.470823Z","iopub.execute_input":"2021-10-30T16:20:03.471603Z","iopub.status.idle":"2021-10-30T16:20:03.606735Z","shell.execute_reply.started":"2021-10-30T16:20:03.471547Z","shell.execute_reply":"2021-10-30T16:20:03.605684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate a new feature from the missing_num.(Binary value to represent whether there is missing value in this record)\ntest_preprocessed_df['missing_sign'] = test_preprocessed_df['missing_num'] != 0\ntest_preprocessed_df['missing_sign'] = test_preprocessed_df['missing_sign'] + 0","metadata":{"execution":{"iopub.status.busy":"2021-10-30T16:20:04.176571Z","iopub.execute_input":"2021-10-30T16:20:04.177475Z","iopub.status.idle":"2021-10-30T16:20:04.187189Z","shell.execute_reply.started":"2021-10-30T16:20:04.17742Z","shell.execute_reply":"2021-10-30T16:20:04.186345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"6.4\"></a>\n## 6.4. Data Imputation","metadata":{}},{"cell_type":"code","source":"# Replace the empty data with NaN\ntest_preprocessed_df.replace(\"\", float(\"NaN\"), inplace=True)\ntest_preprocessed_df.replace(\" \", float(\"NaN\"), inplace=True)\n\n# Count missing value(NaN, na, null, None) of each columns, Then transform the result to a pandas dataframe. \ncount_missing_value = test_preprocessed_df.isna().sum() / test_preprocessed_df.shape[0] * 100\ncount_missing_value_df = pd.DataFrame(count_missing_value.sort_values(ascending=False), columns=['Missing%'])\nmissing_value_df = count_missing_value_df[count_missing_value_df['Missing%'] > 0]\n\n# Get the feature names with missing values.\nmissing_features = list(missing_value_df.index)","metadata":{"execution":{"iopub.status.busy":"2021-10-30T16:20:05.00204Z","iopub.execute_input":"2021-10-30T16:20:05.002533Z","iopub.status.idle":"2021-10-30T16:20:05.131197Z","shell.execute_reply.started":"2021-10-30T16:20:05.00249Z","shell.execute_reply":"2021-10-30T16:20:05.130163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Group imputation\nfor feature in missing_features: \n    test_preprocessed_df[feature] = test_preprocessed_df.groupby('missing_sign', sort=False)[feature].apply(lambda x: x.fillna(x.mean()))","metadata":{"execution":{"iopub.status.busy":"2021-10-30T16:20:05.797444Z","iopub.execute_input":"2021-10-30T16:20:05.797977Z","iopub.status.idle":"2021-10-30T16:20:16.570834Z","shell.execute_reply.started":"2021-10-30T16:20:05.797928Z","shell.execute_reply":"2021-10-30T16:20:16.569779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"6.5\"></a>\n## 6.5. Make Prediction","metadata":{}},{"cell_type":"code","source":"# Use trained model(best) to make predictions\nresults1 = catbc_best.predict_proba(test_preprocessed_df)[:,1]\nresults_df1 = pd.DataFrame(results1, columns=['claim'])\n#predictions_df1 = pd.concat([test_df['id'], results_df1], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-10-30T16:20:16.572673Z","iopub.execute_input":"2021-10-30T16:20:16.572943Z","iopub.status.idle":"2021-10-30T16:20:17.099024Z","shell.execute_reply.started":"2021-10-30T16:20:16.572912Z","shell.execute_reply":"2021-10-30T16:20:17.098248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use trained model(best) to make predictions\nresults2 = lgbc_best.predict_proba(test_preprocessed_df)[:,1]\nresults_df2 = pd.DataFrame(results2, columns=['claim'])\n#predictions_df2 = pd.concat([test_df['id'], results_df2], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-10-30T16:20:17.10023Z","iopub.execute_input":"2021-10-30T16:20:17.100461Z","iopub.status.idle":"2021-10-30T16:20:19.589984Z","shell.execute_reply.started":"2021-10-30T16:20:17.100436Z","shell.execute_reply":"2021-10-30T16:20:19.589168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Aggregate the results from two best models\naggregate_df = pd.concat([test_df['id'], results_df1, results_df2], axis=1)\naggregate_df['mean'] = aggregate_df['claim'].mean(axis=1)\n\nfinal_df = aggregate_df[['id', 'mean']].copy()\nfinal_df.rename(columns={'mean': 'claim'}, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-10-30T16:20:19.591933Z","iopub.execute_input":"2021-10-30T16:20:19.595368Z","iopub.status.idle":"2021-10-30T16:20:19.634276Z","shell.execute_reply.started":"2021-10-30T16:20:19.59532Z","shell.execute_reply":"2021-10-30T16:20:19.63362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"6.6\"></a>\n## 6.6. Save the Prediction to CSV file","metadata":{}},{"cell_type":"code","source":"# Save predictions to .csv for project submission\nfinal_df.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-10-30T16:20:19.635428Z","iopub.execute_input":"2021-10-30T16:20:19.636247Z","iopub.status.idle":"2021-10-30T16:20:21.496969Z","shell.execute_reply.started":"2021-10-30T16:20:19.636211Z","shell.execute_reply":"2021-10-30T16:20:21.495977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Thanks for reading, have a good day ~","metadata":{}}]}