{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Representation Learning Background\n\nRepresentation learning has been used in many competitions on kaggle.  [The Porto Seguro first place finish](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/44629) is a wonderful example of the power to be found in representation learning.  Not only in Porto Seguro, but representation learning has played an outsized role in the tabular monthly comps.  [January's first place finish](https://www.kaggle.com/springmanndaniel/1st-place-turn-your-data-into-daeta), [February's first place finish](https://www.kaggle.com/c/tabular-playground-series-feb-2021/discussion/222745), [March's first place finish](https://www.kaggle.com/c/tabular-playground-series-mar-2021/discussion/229833), and [April's first place finish](https://www.kaggle.com/c/tabular-playground-series-apr-2021/discussion/235739) all leveraged representational learning to win.\n\nAll the above notebooks used a model known as the denoising autoencoder (DAE) in order to get representations of the data.  The DAE is given corrupted input and is forced to reconstruct it in the final layer.  The final layer of reconstructions is useless, but the hidden layers' activations are extracted to create a new dataset.  Subsequent feed forward neural networks are then trained using the new dataset created by the hidden layers' activations.  If want to know more, I strongly recommend you read [January's first place solution](https://www.kaggle.com/springmanndaniel/1st-place-turn-your-data-into-daeta).  This terrific notebook is filled with insight into the workings of the DAE.","metadata":{}},{"cell_type":"markdown","source":"# What's New\n\nWhy do we need another DAE?  I have 2 things to offer.  \n\n**1) This model is developed in tensorflow.**   Not that this is any better than pytorch (not trying to start any fights here!), but that some of us are more familiar with it and can more easily modify this code.  This implementation is model centric, so easy to read in my opinion.  Of course, I wrote it so maybe I'm biased...\n\n**2) This model has some additional noise.**  I recently read [this paper](https://arxiv.org/pdf/2106.01342.pdf) and found an interesting way to add noise.  In the paper, the authors created noise as a two stage process.  First, they used swap noise (also known as [cutmix](https://arxiv.org/abs/1905.04899) )on the raw data.  After an embedding layer, they used [mixup](https://arxiv.org/pdf/1710.09412.pdf) to blend samples together.  My DAE has recreated their approach to noise creation.  \n\nThe model first trains on 80% of the data with early stopping.  Once it finds the optimal stopping point, it then retrains from epoch 0 on all data for 10% more epochs than the early stopping.  ","metadata":{}},{"cell_type":"code","source":"###############################\n###############################\n#Hyperparams\n###############################\n###############################\n#Change this as desired to create your own representations!\n#The main sources of noise are DROP_RATE, MIXUP_RATE, and CUTMIX_RATE.  For tuning, I would make sure that the sum of \n# MIXUP_RATE and CUTMIX_RATE < .5.  But that is just me.  You do whatever you want!\n\nLAYER_SIZE=500 #Number of embeddings to be extracted from train and test data.  Size of the hidden dense layers.\nBATCH_SIZE = 10000 \nEPOCHS = 150 #The network typically early stops far below this number. \nDROP_RATE = .05 #dropout rate for hidden dense layers\nPATIENCE = 15 #Number of epochs with no improvement before early stopping kicks in\nRESTARTS = 3 #Number of times to fit a model to the data.  I models representations are averaged.  I was inspired by Chris Deotte in this https://www.kaggle.com/c/lish-moa/discussion/200680\nNUM_BLOCKS = 3#Number of hidden dense layers in the DAE.  This is effectively the size of the network.\nMIXUP_RATE = .3 #percent of input to be randomly swapped out\nCUTMIX_RATE = .2 #weight to be given to random sample mixed with sample.  Called alpha in \nEMBEDDED_DIMS = 5 #Number of embeddings each input feature is initially given. i.e. tensor shape (None, num_features) becomes (None, num_features, EMBEDDED_DIMS).\n                  #This is the very first layer of the DAE after loading the data and using swapnoise (cutmix).","metadata":{"execution":{"iopub.status.busy":"2021-09-09T17:34:13.379165Z","iopub.execute_input":"2021-09-09T17:34:13.379513Z","iopub.status.idle":"2021-09-09T17:34:13.386553Z","shell.execute_reply.started":"2021-09-09T17:34:13.37948Z","shell.execute_reply":"2021-09-09T17:34:13.383885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport sys\nimport os\nfrom lightgbm import LGBMRegressor, LGBMClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom tqdm.notebook import tqdm\nfrom time import time\nimport wandb\nimport tensorflow_addons as tfa\nfrom catboost import CatBoostClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import mean_squared_error\nfrom tqdm.notebook import tqdm\n\nimport psutil\nimport tensorflow as tf\nimport gc\nfrom sklearn.preprocessing import QuantileTransformer","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-09T17:34:15.759084Z","iopub.execute_input":"2021-09-09T17:34:15.759422Z","iopub.status.idle":"2021-09-09T17:34:23.650556Z","shell.execute_reply.started":"2021-09-09T17:34:15.759392Z","shell.execute_reply":"2021-09-09T17:34:23.649606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"###################################################\n#Layers I took from my github.com/Ottpocket/Neural\n####################################################\nclass CutMix(tf.keras.layers.Layer):\n    '''\n    Implementation of CutMix for Tabular data\n\n    Args\n    _____\n    noise: (R in [0,1)) probability that a value is not sampled from distribution\n\n    Application\n    ____________\n    CM = CutMix(.2)\n    x = tf.reshape(tf.range(0,10, dtype=tf.float32), (5,2))\n    print(x.numpy())\n\n    y = CM(x,True)\n    print(y.numpy())\n    '''\n    def __init__(self, noise, **kwargs):\n        super(CutMix, self).__init__(**kwargs)\n        self.noise = noise\n\n    def get_config(self):\n        config = super(CutMix, self).get_config()\n        config.update({\"noise\": self.noise})\n        return config\n\n    def call(self, inputs, training=None):\n        if training:\n            shuffled = tf.stop_gradient(tf.random.shuffle(inputs))\n            #print(shuffled.numpy())\n\n            msk = tf.keras.backend.random_bernoulli(tf.shape(inputs), p=1 - self.noise, dtype=tf.float32)\n            #print(msk)\n            return msk * inputs + (tf.ones_like(msk) - msk) * shuffled\n        return inputs\n\n\nclass EmbeddingLayer(tf.keras.layers.Layer):\n    '''\n    Implementation of the Embedding layer.  Embeds all features into `num_dims`\n    dimensions.  Takes in (None, FEATURES) tensor and outputs (None, FEATURES * num_dims) size tensor.\n\n    ARGUMENTS\n    _____\n    num_dims: (int) the number of embedded dimensions.  If None, skips embedding\n\n    Application\n    ______________\n    EL = EmbeddingLayer(3)\n    x = tf.reshape(tf.range(0,10, dtype=tf.float32), (5,2))\n    print(x.numpy())\n\n    y = EL(x)\n    print(y.numpy())\n    '''\n    def __init__(self, num_dims=None, **kwargs):\n        super(EmbeddingLayer, self).__init__(**kwargs)\n        self.num_dims = num_dims\n        if self.num_dims is not None:\n            self.emb = tf.keras.layers.Conv1D(filters=self.num_dims, kernel_size=1, activation='relu')\n            self.Flatten = tf.keras.layers.Flatten()\n\n    def get_config(self):\n        config = super(EmbeddingLayer, self).get_config()\n        config.update({\"num_dims\": self.num_dims})\n        return config\n\n    def call(self, inputs):\n        if self.num_dims is None:\n            return inputs\n\n        return self.Flatten(self.emb(tf.expand_dims(inputs, -1)))\n\n\nclass MixUp(tf.keras.layers.Layer):\n    '''\n    Implementation of MixUp \n\n    Args\n    _____\n    alpha: (R in [0,1)) percentage of random sample to input  used\n\n    Application\n    ____________\n    MU = MixUp(.1)\n    x = tf.reshape(tf.range(0,10, dtype=tf.float32), (5,2))\n    y = MU(x)\n    print(x.numpy())\n    print(y.numpy())\n    '''\n    def __init__(self, alpha, **kwargs):\n        super(MixUp, self).__init__(**kwargs)\n        self.alpha = alpha\n        self.alpha_constant = tf.constant(self.alpha)\n        self.one_minus_alpha = tf.constant(1.) - self.alpha\n\n    def get_config(self):\n        config = super(MixUp, self).get_config()\n        config.update({\"alpha\": self.alpha})\n        return config\n\n    def call(self, inputs, training=None):\n        if training:\n            shuffled = tf.stop_gradient(tf.random.shuffle(inputs))\n            #print(shuffled.numpy())\n            return self.alpha_constant * inputs + self.one_minus_alpha * shuffled\n        return inputs\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-09T17:34:23.653709Z","iopub.execute_input":"2021-09-09T17:34:23.653975Z","iopub.status.idle":"2021-09-09T17:34:23.668396Z","shell.execute_reply.started":"2021-09-09T17:34:23.653949Z","shell.execute_reply":"2021-09-09T17:34:23.667646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_feather('/kaggle/input/september-feather/train_rg_min')\ntest = pd.read_feather('/kaggle/input/september-feather/test_rg_min')\nss = pd.read_feather('/kaggle/input/september-feather/sample_solution')\nFEATURES = [feat for feat in train.columns if 'f' in feat] + ['nan_count']\nTARGET = 'loss'\n\n#Getting data ready for pretraining\ncombined = pd.concat([train[FEATURES], test[FEATURES]]).values\nmsk = np.random.choice([True, False], size = (combined.shape[0],), p=[.8,.2])\nX = combined[msk]\nval = combined[~msk]\n#Ending with 3.4\n\n\n#Getting dae-columns ready for predictions\nDAE_LAYERS = []\nfor layer in tqdm(range(LAYER_SIZE)):\n    name = f'dae_{layer}'\n    train[name] = 0\n    test[name] = 0\n    train[name] = train[name].astype(np.float16)\n    test[name] = test[name].astype(np.float16)\n    DAE_LAYERS.append(name)","metadata":{"execution":{"iopub.status.busy":"2021-09-09T17:34:23.670217Z","iopub.execute_input":"2021-09-09T17:34:23.670747Z","iopub.status.idle":"2021-09-09T17:34:42.974409Z","shell.execute_reply.started":"2021-09-09T17:34:23.67071Z","shell.execute_reply":"2021-09-09T17:34:42.973559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Batch_Drop_Dense(x, name, drop_rate, layer_size, activation = 'relu'):\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dropout(drop_rate)(x)\n    x = tfa.layers.WeightNormalization(tf.keras.layers.Dense(layer_size, activation=activation), name= f'Dense_{name}')(x)\n    #x = tf.keras.layers.Dense(layer_size, activation=activation, name= f'Dense_{name}')(x)\n    return x\n\n#Custom model.fit for DAE structure\n#stolen from https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit#a_first_simple_example\nclass DaeModel(tf.keras.Model):\n    def train_step(self, data):\n        with tf.GradientTape() as tape:\n            pred = self(data, training=True)  # Forward pass\n            loss = self.compiled_loss(data, pred, regularization_losses=self.losses)\n\n        # Compute gradients\n        trainable_vars = self.trainable_variables\n        gradients = tape.gradient(loss, trainable_vars)\n        # Update weights\n        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n        # Update metrics (includes the metric that tracks the loss)\n        self.compiled_metrics.update_state(data, pred)\n        # Return a dict mapping metric names to current value\n        return {m.name: m.result() for m in self.metrics}\n    \n    def test_step(self, data):\n        #data = data_adapter.expand_1d(data)\n        #x, y, sample_weight = data_adapter.unpack_x_y_sample_weight(data)\n\n        y_pred = self(data, training=False)\n        # Updates stateful loss metrics.\n        self.compiled_loss(\n            data, y_pred, regularization_losses=self.losses)\n        self.compiled_metrics.update_state(data, y_pred)\n        # Collect metrics to return\n        return_metrics = {}\n        for metric in self.metrics:\n            result = metric.result()\n            if isinstance(result, dict):\n                return_metrics.update(result)\n            else:\n                return_metrics[metric.name] = result\n        return return_metrics\n\ndef dae(num_input_columns, layer_size = 1000, BLOCKS = 3, drop_rate=.05, cutmix_rate=.2,\n                mixup_rate=.1, num_embedded_dims=None):\n    '''\n    Creates a DAE based on architecture and noise from `SAINT: Improved Neural Networks for Tabular Data\n    via Row Attention and Contrastive Pre-Training`\n\n    ARGUMENTS:\n    -----------\n    num_input_columns: (int) number of input (and output) columns in model\n    layer_size: (int) # of neurons in hidden layers\n    BLOCKS: (int) # BN, dropout, Dense layer blocks\n    drop_rate: (float [0,1)) dropout rate\n    cutmix_rate: (float [0,1)) percent of input randomly cutmixed\n    mixup_rate: (float [0,1)]) percent to blend embeddings.\n    num_embedded_dims: (int) how many embedded dimensions do you want the embedder to have.\n          If None, just skips the embedder\n    '''\n\n    inp = tf.keras.layers.Input(num_input_columns)\n    x = CutMix(cutmix_rate, name='CutMix')(inp)\n    x = EmbeddingLayer(num_dims=num_embedded_dims, name= 'EmbeddingLayer')(x)\n    x = Batch_Drop_Dense(x, name='zero', drop_rate= drop_rate, layer_size=layer_size)\n    x = MixUp(alpha = mixup_rate, name='MixUp')(x)\n    for name, i in enumerate(range(BLOCKS-1)):\n        x = Batch_Drop_Dense(x, name+1, drop_rate= drop_rate, layer_size=layer_size)\n    x = Batch_Drop_Dense(x, layer_size=num_input_columns, activation = None, name='Final_layer', drop_rate= drop_rate, )\n    model = DaeModel(inputs=inp, outputs=x)\n    model.compile(optimizer=tfa.optimizers.Lookahead(tf.optimizers.Adam(), sync_period=10),\n                  loss='MeanSquaredError',\n                  )\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-09-09T17:34:42.976087Z","iopub.execute_input":"2021-09-09T17:34:42.976597Z","iopub.status.idle":"2021-09-09T17:34:42.992739Z","shell.execute_reply.started":"2021-09-09T17:34:42.976545Z","shell.execute_reply":"2021-09-09T17:34:42.991976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dae_args = dict(num_input_columns= len(FEATURES), layer_size = LAYER_SIZE, BLOCKS = NUM_BLOCKS, drop_rate=DROP_RATE, cutmix_rate=CUTMIX_RATE,\n                mixup_rate=MIXUP_RATE, num_embedded_dims=EMBEDDED_DIMS)","metadata":{"execution":{"iopub.status.busy":"2021-09-09T17:34:42.993967Z","iopub.execute_input":"2021-09-09T17:34:42.994354Z","iopub.status.idle":"2021-09-09T17:34:43.007251Z","shell.execute_reply.started":"2021-09-09T17:34:42.99432Z","shell.execute_reply":"2021-09-09T17:34:43.00628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Callbacks\n#Memory was a concern to say the least.  A lots of the callbacks were just to call me down as the network trained.  \n# I wanted to be sure that I was not running out of memory as training progressed\nclass MemoryUsage(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs=None):\n        keys = list(logs.keys())\n        print(f\"Epoch {epoch+1}: {psutil.virtual_memory()[3] / 1024**3 :.4f} gb used.\")\n\nclass CleanMem(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs=None):\n        gc.collect()\n        \nES = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=PATIENCE, verbose=0,\n                                                                restore_best_weights=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-09T17:34:43.008436Z","iopub.execute_input":"2021-09-09T17:34:43.008955Z","iopub.status.idle":"2021-09-09T17:34:43.017878Z","shell.execute_reply.started":"2021-09-09T17:34:43.008918Z","shell.execute_reply":"2021-09-09T17:34:43.017087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Eccentricities\n\nThe below code is the training loop.  Why didn't I just `model.predict(train)` instead of the bizarre `for` loop?  Memory was kinda tight towards the end of this.  Having a (~1million , LAYER_SIZE) np.float32 ndarray typically gave me memory overload.  As a workaround, I did the predict in batches where each batch I turned the predictions to np.float16.  ","metadata":{}},{"cell_type":"code","source":"#######################################################################################################\n#Training loop\n#######################################################################################################\n\nfor i in range(RESTARTS):\n    repeat_start = time()\n    print(f'Starting Repeat {i+1} of {RESTARTS}')\n    model = dae(**dae_args)\n    model.save_weights('no_train.h5')\n    embedder = tf.keras.Model(inputs=model.inputs,\n                              outputs=model.get_layer(name=f'Dense_{dae_args[\"BLOCKS\"]-1}').output)\n\n    ##########################################################################   \n    #Finding number of iterations before early stopping \n    ##########################################################################\n    print(f'Mem before training: {psutil.virtual_memory()[3] / 1024**3 :.4f}')        \n    H = model.fit(x=X, validation_data = (val,), callbacks = [ES, MemoryUsage(), CleanMem()],\n                  epochs=EPOCHS, batch_size=BATCH_SIZE)\n    print(f'Mem after training: {psutil.virtual_memory()[3] / 1024**3 :.4f}')\n\n    ##########################################################################   \n    #Trainig model 10% longer than early stop on all data\n    ##########################################################################    \n    num_epochs = len(H.history['loss']) - PATIENCE\n    num_epochs = int(num_epochs * 1.1)\n    model.load_weights('no_train.h5')\n\n    print(f'Mem before training: {psutil.virtual_memory()[3] / 1024**3 :.4f}')\n    model.fit(x=combined, epochs= num_epochs, batch_size=BATCH_SIZE, \n              callbacks = [ES, MemoryUsage(), CleanMem()])\n\n    #DONT USE!!!!!! Memory overflow\n    #train[DAE_LAYERS] = embedder.predict(train[FEATURES].values, batch_size=10000).astype(np.float16)\n    #test[DAE_LAYERS] = embedder.predict(test[FEATURES].values, batch_size= 10000).astype(np.float16)\n    \n    ####################################################\n    #Getting embeddings for train\n    ####################################################\n    increment = 100000\n    columns = [position for position, name in enumerate(train.columns) if name in DAE_LAYERS]\n    print(f'Mem usage: {psutil.virtual_memory()[3] / 1024**3 :.4f}')\n    for i in tqdm(range(train.shape[0]//increment + 1)):\n        start = i*increment\n        finish = (i+1)*increment\n        train.iloc[start:finish, columns] += embedder.predict(train[FEATURES].iloc[start:finish].values, batch_size=increment).astype(np.float16) / RESTARTS\n\n    print(f'Mem usage: {psutil.virtual_memory()[3] / 1024**3 :.4f}')\n\n    ####################################################\n    #Getting embeddings for test\n    ####################################################\n    columns = [position for position, name in enumerate(test.columns) if name in DAE_LAYERS]\n    for i in tqdm(range(test.shape[0]//increment + 1)):\n        start = i*increment\n        finish = (i+1)*increment\n        test.iloc[start:finish, columns] += embedder.predict(test[FEATURES].iloc[start:finish].values, batch_size=increment).astype(np.float16) / RESTARTS\n\n    print(f'Mem usage: {psutil.virtual_memory()[3] / 1024**3 :.4f}')\n    \n    tf.keras.backend.clear_session() \n    print(f'Repeat {i+1}: {time() - repeat_start :.2f} seconds')","metadata":{"execution":{"iopub.status.busy":"2021-09-09T17:34:43.019593Z","iopub.execute_input":"2021-09-09T17:34:43.019951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del X, val, combined; gc.collect()\nprint(f'Mem usage: {psutil.virtual_memory()[3] / 1024**3 :.4f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[DAE_LAYERS].isnull().sum().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for feat in FEATURES:\n    del train[feat], test[feat]; gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.to_feather(f'train_daeta_{LAYER_SIZE}_{RESTARTS}_{MIXUP_RATE}_{CUTMIX_RATE}_{EMBEDDED_DIMS}')\ntest.to_feather(f'test_daeta_{LAYER_SIZE}_{RESTARTS}_{MIXUP_RATE}_{CUTMIX_RATE}_{EMBEDDED_DIMS}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}