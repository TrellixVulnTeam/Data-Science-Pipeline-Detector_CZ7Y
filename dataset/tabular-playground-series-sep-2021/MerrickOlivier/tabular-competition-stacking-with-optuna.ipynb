{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\n\nimport optuna\nimport xgboost as xgb\nfrom sklearn.metrics import roc_auc_score as ros\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import cross_val_score\nfrom catboost import CatBoostClassifier, Pool\nimport lightgbm as lgbm\nfrom sklearn.ensemble import StackingClassifier\n       \n%load_ext skip_kernel_extension_py","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-28T11:59:25.672027Z","iopub.execute_input":"2021-09-28T11:59:25.672514Z","iopub.status.idle":"2021-09-28T11:59:29.681638Z","shell.execute_reply.started":"2021-09-28T11:59:25.672415Z","shell.execute_reply":"2021-09-28T11:59:29.680518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Import and Preprocess Data\n\nIn a previous notebook, we found that most columns and rows contained NaNs. In order to fill these, we first normalized all non-NaN values in both the testing and training datasets. We then used Sklearn's IterativeImputer class with a BayesianRidge estimator, which was fit on the normalized training set, to impute the NaNs in both the training and testing datasets. Given that this was a computationally heavy task that took over an hour, we saved the resulting datasets, which we will use in this notebook to develop a suitable classification model. ","metadata":{}},{"cell_type":"code","source":"#Import previously normalized/imputed data.\ntest = pd.read_csv(\"../input/imputed-data-blended/imputed_test_blending.csv\")\nX = pd.read_csv(\"../input/imputed-data-blended/imputed_train_blending.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-09-28T11:59:29.683429Z","iopub.execute_input":"2021-09-28T11:59:29.683849Z","iopub.status.idle":"2021-09-28T12:00:10.427709Z","shell.execute_reply.started":"2021-09-28T11:59:29.683805Z","shell.execute_reply":"2021-09-28T12:00:10.426485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Import original data for target values.\ny = pd.read_csv(\"../input/tabular-playground-series-sep-2021/train.csv\", usecols=['claim'])","metadata":{"execution":{"iopub.status.busy":"2021-09-28T12:00:10.430275Z","iopub.execute_input":"2021-09-28T12:00:10.430804Z","iopub.status.idle":"2021-09-28T12:00:24.344909Z","shell.execute_reply.started":"2021-09-28T12:00:10.43076Z","shell.execute_reply":"2021-09-28T12:00:24.34367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Display samples of DataFrames.\nfor i in [X, test]:\n    display(i.head())","metadata":{"execution":{"iopub.status.busy":"2021-09-28T12:00:24.346929Z","iopub.execute_input":"2021-09-28T12:00:24.347438Z","iopub.status.idle":"2021-09-28T12:00:24.427021Z","shell.execute_reply.started":"2021-09-28T12:00:24.34738Z","shell.execute_reply":"2021-09-28T12:00:24.425722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Dislpay Nulls.\nprint(f'\\nRows with NaNs in training set: {X.isnull().any(axis=1).sum()}')\nprint(f'Columns with NaNs in training set: {X.isnull().any(axis=0).sum()}')\nprint(f'Rows in training set: {len(X)}')\nprint(f'\\nRows with NaNs in testing set: {test.isnull().any(axis=1).sum()}')\nprint(f'Columns with NaNs in testing set: {test.isnull().any(axis=0).sum()}')\nprint(f'Rows in testing set: {len(test)}')","metadata":{"execution":{"iopub.status.busy":"2021-09-28T12:00:24.428912Z","iopub.execute_input":"2021-09-28T12:00:24.429427Z","iopub.status.idle":"2021-09-28T12:00:24.821509Z","shell.execute_reply.started":"2021-09-28T12:00:24.429382Z","shell.execute_reply":"2021-09-28T12:00:24.820182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Examine distribution of target values.\ndisplay(y.value_counts())","metadata":{"execution":{"iopub.status.busy":"2021-09-28T12:00:24.8235Z","iopub.execute_input":"2021-09-28T12:00:24.824001Z","iopub.status.idle":"2021-09-28T12:00:24.854585Z","shell.execute_reply.started":"2021-09-28T12:00:24.823932Z","shell.execute_reply":"2021-09-28T12:00:24.853175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From what we can see here, there is nearly an equal distribution of classes in the training dataset. As such, we don't have to perform upsample/downsampling or SMOTEing. ","metadata":{}},{"cell_type":"code","source":"#Create correlation map.\ncorrmap = X.corr()\ncorr_mask = np.triu(corrmap)\nf, ax = plt.subplots(figsize=(15, 10))\nsns.heatmap(corrmap, square=True, mask=corr_mask, cmap=\"Blues\")","metadata":{"execution":{"iopub.status.busy":"2021-09-28T12:00:24.856643Z","iopub.execute_input":"2021-09-28T12:00:24.857161Z","iopub.status.idle":"2021-09-28T12:00:59.969141Z","shell.execute_reply.started":"2021-09-28T12:00:24.857117Z","shell.execute_reply":"2021-09-28T12:00:59.96807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that there is no real correlation between any of the features, nor between any feature and the target. Since there is no real inter-feature correlation, we don't have any redundant information, which means we can use all features. This will likely prove more useful than specifically selecting features with the highest correlations with the target, since none of the features show any significant correlation in this regard. ","metadata":{}},{"cell_type":"code","source":"#Compare distributions for all features.\nplt.figure(figsize=(24, 6*(118/4)))\nfor i in tqdm(range(len(X.columns.tolist()[:-1]))):\n    plt.subplot(30, 4, i+1)\n    sns.histplot(X[f'f{i+1}'], kde=True)\n    sns.histplot(test[f'f{i+1}'], kde=True, color='green')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-28T12:00:59.972705Z","iopub.execute_input":"2021-09-28T12:00:59.973187Z","iopub.status.idle":"2021-09-28T12:21:21.352502Z","shell.execute_reply.started":"2021-09-28T12:00:59.973143Z","shell.execute_reply":"2021-09-28T12:21:21.351182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The distributions of values for all features appears to be quite similar for both training and testing datasets. As such, there does not appear to be any dataset shift. Additionally, we expect there to be a similar distribution of target values between the training and testing datasets.","metadata":{}},{"cell_type":"code","source":"#Split data into training and validation sets.\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=5)","metadata":{"execution":{"iopub.status.busy":"2021-09-28T12:21:21.354778Z","iopub.execute_input":"2021-09-28T12:21:21.355314Z","iopub.status.idle":"2021-09-28T12:21:22.426124Z","shell.execute_reply.started":"2021-09-28T12:21:21.355255Z","shell.execute_reply":"2021-09-28T12:21:22.425046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Find Best Hyperparameters for Base Models","metadata":{"execution":{"iopub.status.busy":"2021-09-04T10:21:55.95372Z","iopub.execute_input":"2021-09-04T10:21:55.954281Z","iopub.status.idle":"2021-09-04T10:21:55.960788Z","shell.execute_reply.started":"2021-09-04T10:21:55.954246Z","shell.execute_reply":"2021-09-04T10:21:55.959966Z"}}},{"cell_type":"markdown","source":"Note, in previous versions of this notebook we ran the following cells to determine the optimal hyperparameters for these base models. As can be seen, we ran 75 trials for each model using optuna, which took quite a while. Since we're previously identified the optimal hyperparameters, we will skip these cells using a customized cell-magic command. \n\nCredit for this custom cell-magic script is due to RobbeL https://stackoverflow.com/questions/26494747/simple-way-to-choose-which-cells-to-run-in-ipython-notebook-during-run-all/43584169#43584169\n\nAs we are skipping the following cells, they won't output the AUC-ROC score identified with the optimal hyperparameters for each model. Nonethtless, we previously saved the scores, which are approximately as follows:\n\n* Catboost: 0.8155\n* LightGBM: 0.8154\n* XGBoost: 0.8158","metadata":{}},{"cell_type":"code","source":"%%skip True\n\n#Create helper function.\ndef tune_model(objective):\n    study = optuna.create_study(direction='maximize')\n    study.optimize(objective, n_trials=75)\n    best_params = study.best_params\n    best_score = study.best_value\n    print(f'Best roc-auc score: {best_score}')\n    print(f'\\nBest parameters: {best_params}')\n    return best_params","metadata":{"execution":{"iopub.status.busy":"2021-09-28T12:21:22.427935Z","iopub.execute_input":"2021-09-28T12:21:22.428575Z","iopub.status.idle":"2021-09-28T12:21:22.434753Z","shell.execute_reply.started":"2021-09-28T12:21:22.428527Z","shell.execute_reply":"2021-09-28T12:21:22.433112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%skip True\n\n%%time\n#Define an objective function to be maximized for XGBClassifier.\ndef objective_xgb(trial):\n    \n    #Set up parameters and dtrain matrix.\n    splits = 5\n    kf = StratifiedKFold(n_splits=splits, shuffle=True, random_state=5)\n    \n    #Create array of zeros for storing out-of-fold predictions.\n    oof_preds = np.zeros(X.shape[0])\n    preds = 0\n    model_feature_importance = 0\n    mean_auc = 0\n    \n    #Define parameter space.\n    param_space = {\n        'max_depth': trial.suggest_int(\"max_depth\", 1, 10, 1),\n        'gamma': trial.suggest_float('gamma', 0,1),\n        'reg_alpha' : trial.suggest_float('reg_alpha', 0,50),\n        'reg_lambda' : trial.suggest_float('reg_lambda', 10,100),\n        'colsample_bytree' : trial.suggest_float('colsample_bytree', 0,1),\n        'min_child_weight' : trial.suggest_float('min_child_weight', 0, 5),\n        'learning_rate': trial.suggest_float('learning_rate', 0, .15),\n        'tree_method':'gpu_hist', \n        'gpu_id': 0,\n        'random_state': 5,\n        'n_estimators' : 10000,\n        'max_bin' : trial.suggest_int('max_bin', 200, 550, 1),\n        'objective': 'binary:logistic',\n        'use_label_encoder':False\n    }\n    \n    #Generate folds, train model, and make predictions for each fold.\n    for num, (train_indx, valid_indx) in tqdm(enumerate(kf.split(X, y))):\n        X_train, X_valid = X.loc[train_indx], X.loc[valid_indx]\n        y_train, y_valid = y.loc[train_indx], y.loc[valid_indx]\n        \n        model = xgb.XGBClassifier(**param_space)\n        model.fit(X_train, y_train.values.ravel(),\n                 verbose=False,\n                 eval_set=[(X_valid, y_valid.values.ravel())],\n                 eval_metric='auc',\n                 early_stopping_rounds=300)\n        \n        model_feature_importance += model.feature_importances_ / splits\n        \n        #Obtain out of fold predictions from validation set.\n        #This is used for hyperparameter optimization.\n        oof_preds[valid_indx] = model.predict_proba(X_valid)[:, 1]\n        \n        #Obtain score for model fold.\n        fold_auc = ros(y_valid, oof_preds[valid_indx])\n        print(f'Fold {num} auc score: {fold_auc}')\n        \n        #Mean score for all folds.\n        mean_auc += fold_auc / splits\n        \n    print(f\"\\nOverall ROC AUC: {mean_auc}\")\n    \n    return mean_auc\n\n#Run trials\nxgb_optuna = tune_model(objective_xgb)","metadata":{"execution":{"iopub.status.busy":"2021-09-28T12:21:22.436512Z","iopub.execute_input":"2021-09-28T12:21:22.437275Z","iopub.status.idle":"2021-09-28T12:21:22.447734Z","shell.execute_reply.started":"2021-09-28T12:21:22.437221Z","shell.execute_reply":"2021-09-28T12:21:22.446092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%skip True\n\n#Define an objective function to be maximized for CatBoostClassifier.\ndef objective_cat(trial):\n    \n    #Set up parameters and dtrain matrix.\n    splits = 3\n    kf = StratifiedKFold(n_splits=splits, shuffle=True, random_state=5)\n    \n    #Create array of zeros for storing out-of-fold predictions.\n    oof_preds = np.zeros(X.shape[0])\n    preds = 0\n    model_feature_importance = 0\n    mean_auc = 0\n    \n    #Define parameter space.\n    param_space = {\n        'max_depth': trial.suggest_int(\"max_depth\", 1, 10, 1),\n        'reg_lambda' : trial.suggest_float('reg_lambda', 10,100),\n        'learning_rate': trial.suggest_float('learning_rate', 0, .15),\n        'n_estimators': 5000,\n        'task_type':'GPU', \n        'devices': '0:1',\n        'random_state': 5,\n        'max_bin' : trial.suggest_int('max_bin', 200, 550, 1),\n        'subsample' : trial.suggest_float('subsample', 0, 1),\n        'eval_metric': 'AUC',\n        'bootstrap_type':'Poisson'\n    }\n    \n    #Generate folds, train model, and make predictions for each fold.\n    for num, (train_indx, valid_indx) in tqdm(enumerate(kf.split(X, y))):\n        X_train, X_valid = X.loc[train_indx], X.loc[valid_indx]\n        y_train, y_valid = y.loc[train_indx], y.loc[valid_indx]\n        \n        eval_dataset = Pool(X_valid, y_valid)\n        \n        model = CatBoostClassifier(**param_space)\n        model.fit(X_train, y_train,\n                 verbose=False,\n                 eval_set=eval_dataset,\n                 early_stopping_rounds=300)\n        \n        model_feature_importance += model.feature_importances_ / splits\n        \n        #Obtain out of fold predictions from validation set.\n        #This is used for hyperparameter optimization.\n        oof_preds[valid_indx] = model.predict_proba(X_valid)[:, 1]\n        \n        #Obtain score for model fold.\n        fold_auc = ros(y_valid, oof_preds[valid_indx])\n        print(f'Fold {num} auc score: {fold_auc}')\n        \n        #Mean score for all folds.\n        mean_auc += fold_auc / splits\n        \n    print(f\"\\nOverall ROC AUC: {mean_auc}\")\n    \n    return mean_auc\n\n#Run trials\ncat_optuna = tune_model(objective_cat)","metadata":{"execution":{"iopub.status.busy":"2021-09-28T12:21:22.449562Z","iopub.execute_input":"2021-09-28T12:21:22.45006Z","iopub.status.idle":"2021-09-28T12:21:22.461557Z","shell.execute_reply.started":"2021-09-28T12:21:22.450015Z","shell.execute_reply":"2021-09-28T12:21:22.460137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**LGBM Parameters**","metadata":{}},{"cell_type":"code","source":"%%skip True\n\n#Define an objective function to be maximized for LBGMClassifier.\ndef objective_lbgm(trial):\n    \n    #Set up parameters and dtrain matrix.\n    splits = 5\n    kf = StratifiedKFold(n_splits=splits, shuffle=True, random_state=5)\n    \n    #Create array of zeros for storing out-of-fold predictions.\n    oof_preds = np.zeros(X.shape[0])\n    preds = 0\n    model_feature_importance = 0\n    mean_auc = 0\n    \n    #Define parameter space.\n    lgbm_params = {\n                     \"objective\": trial.suggest_categorical(\"objective\", ['binary']),\n                     \"boosting_type\": trial.suggest_categorical(\"boosting_type\", ['gbdt']),\n                     \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n                     \"max_depth\": trial.suggest_int(\"max_depth\", 1, 16),\n                     \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.1, 1, step=0.01),\n                     \"n_estimators\": 15000,        \n                     \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.1, 100.0, step=0.1),\n                     \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.1, 100.0, step=0.1),\n                     \"random_state\": 5,\n                     \"bagging_seed\": trial.suggest_categorical(\"bagging_seed\", [42]),\n                     \"feature_fraction_seed\": trial.suggest_categorical(\"feature_fraction_seed\", [42]),\n                     \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1, step=0.01),\n                     \"subsample_freq\": trial.suggest_int(\"subsample_freq\", 1, 7),\n                    \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.05, 1, step=0.01),\n                    'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n                    'min_child_weight': trial.suggest_categorical('min_child_weight', [256]),\n                     'max_bin' : trial.suggest_int('max_bin', 50, 250)\n}\n    \n    #Generate folds, train model, and make predictions for each fold.\n    for num, (train_indx, valid_indx) in tqdm(enumerate(kf.split(X, y))):\n        X_train, X_valid = X.loc[train_indx], X.loc[valid_indx]\n        y_train, y_valid = y.loc[train_indx], y.loc[valid_indx]\n        \n        model = lgbm.LGBMClassifier(**lgbm_params)\n        model.fit(X_train, y_train.values.ravel(),\n                 verbose=False,\n                 eval_set=[(X_valid, y_valid.values.ravel())],\n                 eval_metric='auc',\n                 early_stopping_rounds=150)\n        \n        model_feature_importance += model.feature_importances_ / splits\n        \n        #Obtain out of fold predictions from validation set.\n        #This is used for hyperparameter optimization.\n        oof_preds[valid_indx] = model.predict_proba(X_valid)[:, 1]\n        \n        #Obtain score for model fold.\n        fold_auc = ros(y_valid, oof_preds[valid_indx])\n        print(f'Fold {num} auc score: {fold_auc}')\n        \n        #Mean score for all folds.\n        mean_auc += fold_auc / splits\n        \n    print(f\"\\nOverall ROC AUC: {mean_auc}\")\n    \n    return mean_auc\n\n#Run trials\nlgbm_optuna = tune_model(objective_lbgm)","metadata":{"execution":{"iopub.status.busy":"2021-09-28T12:21:22.463261Z","iopub.execute_input":"2021-09-28T12:21:22.46386Z","iopub.status.idle":"2021-09-28T12:21:22.475058Z","shell.execute_reply.started":"2021-09-28T12:21:22.463816Z","shell.execute_reply":"2021-09-28T12:21:22.474047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Find Optimal Hyperparameters for Meta-Model","metadata":{}},{"cell_type":"markdown","source":"As with the base models, we used optuna to find the optimal hyperparameters for our meta-model. This was a very time-consuming process, as it involved cross-validation and obtaining predictions from multiple gradient-boosting models; as such, we ran the program using GPU and saved the optimal parameters. In order to save time, we will skip the optimization cell and simply use the previously-identified optimal hyperparameters for the meta-model in the next section.\n\nThe AUC-ROC score for the best-performing stacked model identified is: 0.8167. As can be seen, the stacked model performs better than each of the base models.","metadata":{}},{"cell_type":"code","source":"models = [('xgb1', xgb.XGBClassifier(max_depth=2,\n                                     gamma=0.42128623263686943,\n                                     reg_alpha=13.002596237041757,\n                                     reg_lambda=21.341562971454067,\n                                     colsample_bytree=0.2527228225382093,\n                                     min_child_weight=2.2987723459855838,\n                                     learning_rate=0.08687739423816797,\n                                     max_bin=495,\n                                     gpu_id=0,\n                                     tree_method='gpu_hist',\n                                     random_state=5,\n                                     n_estimators=8000,\n                                     use_label_encoder=False,\n                                     objective='binary:logistic')),\n          \n          ('cat', CatBoostClassifier(max_depth=4,\n                                     reg_lambda=24.520455674851036,\n                                     learning_rate=0.02867314631306076,\n                                     max_bin=505,\n                                     subsample=0.8068183652307186,\n                                     n_estimators=8000,\n                                     task_type='GPU',\n                                     devices='0:1',\n                                     random_state=5,\n                                     eval_metric='AUC',\n                                     bootstrap_type='Poisson',\n                                     verbose=False)),\n          \n          ('lbg', lgbm.LGBMClassifier(objective='binary',\n                                      boosting_type='gbdt',\n                                      num_leaves=6,\n                                      max_depth=2,\n                                      learning_rate=.1,\n                                      n_estimators=7000,\n                                      reg_alpha=25.0,\n                                      reg_lambda=76.7,\n                                      random_state=42,\n                                      bagging_seed=42,\n                                      feature_fraction_seed=42,\n                                      n_jobs=-1,\n                                      subsample=0.98,\n                                      subsample_freq=1,\n                                      colsample_bytree=0.69,\n                                      min_child_samples=54,\n                                      min_child_weight=256))]","metadata":{"execution":{"iopub.status.busy":"2021-09-28T12:21:22.476823Z","iopub.execute_input":"2021-09-28T12:21:22.477374Z","iopub.status.idle":"2021-09-28T12:21:22.493866Z","shell.execute_reply.started":"2021-09-28T12:21:22.47733Z","shell.execute_reply":"2021-09-28T12:21:22.492768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%skip True\n\n#Define an objective function to be maximized for StackingClassifier.\ndef objective_meta(trial):\n    \n    meta_param_space = {\n        'max_depth': trial.suggest_int(\"max_depth\", 1, 10, 1),\n        'gamma': trial.suggest_float('gamma', 0,1),\n        'reg_alpha' : trial.suggest_float('reg_alpha', 0,50),\n        'reg_lambda' : trial.suggest_float('reg_lambda', 10,100),\n        'colsample_bytree' : trial.suggest_float('colsample_bytree', 0,1),\n        'min_child_weight' : trial.suggest_float('min_child_weight', 0, 5),\n        'learning_rate': trial.suggest_float('learning_rate', 0, .15),\n        'tree_method':'gpu_hist', \n        'gpu_id': 0,\n        'random_state': 5,\n        'n_estimators' : 8000,\n        'max_bin' : trial.suggest_int('max_bin', 200, 550, 1),\n        'objective': 'binary:logistic',\n        'use_label_encoder':False}\n    \n    clf = StackingClassifier(estimators=models, \n                             final_estimator=xgb.XGBClassifier(**meta_param_space), \n                             cv=3)\n    \n    #Obtain predictions on X_valid.\n    clf.fit(X_train, y_train.values.ravel())\n    clf_pred = clf.predict_proba(X_valid)[:, 1]\n    \n    meta_auc = ros(y_valid, clf_pred)    \n        \n    print(f'Auc for this round: {meta_auc}')\n    \n    return meta_auc\n\nmeta_optuna = tune_model(objective_met2a)","metadata":{"execution":{"iopub.status.busy":"2021-09-28T12:21:22.495739Z","iopub.execute_input":"2021-09-28T12:21:22.496292Z","iopub.status.idle":"2021-09-28T12:21:22.50841Z","shell.execute_reply.started":"2021-09-28T12:21:22.496245Z","shell.execute_reply":"2021-09-28T12:21:22.507181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Stack Models and Obtain Prediction","metadata":{}},{"cell_type":"markdown","source":"Having obtained the optimal hyperparameters for all models, including our meta-model, we are now prepared to obtain predictions from our stacked model for the test dataset. In the following cell, we create an instance of StackingClassifier with a 5-fold cross validation. This classifier has two levels; level-0 is comprised of the base models, and level-1 is comprised of our meta-model. The meta-param-space variable contains the optimal hyperparameters identified for the meta-model. ","metadata":{}},{"cell_type":"code","source":"meta_param_space =  {'max_depth': 3, \n                     'gamma': 0.12970787138617168, \n                     'reg_alpha': 15.056592603960167, \n                     'reg_lambda': 76.8501171470906, \n                     'colsample_bytree': 0.6554985217430114, \n                     'min_child_weight': 3.654980551295688, \n                     'learning_rate': 0.06853956803108749, \n                     'max_bin': 206,\n                     'tree_method':'gpu_hist', \n                     'gpu_id': 0,\n                     'random_state': 5,\n                     'n_estimators' : 8000,\n                     'objective': 'binary:logistic',\n                     'use_label_encoder':False}\n\n#Create stacked model.\nmeta_model = StackingClassifier(estimators=models, \n                             final_estimator=xgb.XGBClassifier(**meta_param_space), \n                             cv=5)\n#Fit stacked model.\nmeta_model.fit(X_train, y_train.values.ravel())\n\n#Obtain predictions.\nmeta_pred_valid = meta_model.predict_proba(X_valid)[:, 1]\nmeta_pred_roc = ros(y_valid, meta_pred_valid)\nprint(f'AUC-ROC Score for validation set: {meta_pred_roc}')\n\nmeta_pred_test = meta_model.predict_proba(test)[:,1]","metadata":{"execution":{"iopub.status.busy":"2021-09-28T12:21:22.511951Z","iopub.execute_input":"2021-09-28T12:21:22.512396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"meta_pred_test = pd.Series(meta_pred_test)\ntest = pd.read_csv(\"../input/tabular-playground-series-sep-2021/test.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the predictions to a CSV file\noutput = pd.DataFrame({'id': test['id'],\n                       'claim': meta_pred_test})\noutput.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}