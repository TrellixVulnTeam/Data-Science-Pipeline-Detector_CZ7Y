{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Libraries and Data","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-06T08:37:41.883883Z","iopub.execute_input":"2021-09-06T08:37:41.884301Z","iopub.status.idle":"2021-09-06T08:37:41.897391Z","shell.execute_reply.started":"2021-09-06T08:37:41.884206Z","shell.execute_reply":"2021-09-06T08:37:41.896437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\npd.pandas.set_option('display.max_columns',None)\npd.pandas.set_option('display.max_rows',None)\n\n# Train Data\ndata = pd.read_csv('../input/tabular-playground-series-sep-2021/train.csv')\nprint(\"Train Shape:\", data.shape)\n\ndata.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-09-06T08:37:47.935732Z","iopub.execute_input":"2021-09-06T08:37:47.936051Z","iopub.status.idle":"2021-09-06T08:38:13.999497Z","shell.execute_reply.started":"2021-09-06T08:37:47.936021Z","shell.execute_reply":"2021-09-06T08:38:13.99852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Checking data Types:\")\nprint(data.dtypes)","metadata":{"execution":{"iopub.status.busy":"2021-09-06T08:38:18.526865Z","iopub.execute_input":"2021-09-06T08:38:18.527225Z","iopub.status.idle":"2021-09-06T08:38:18.53545Z","shell.execute_reply.started":"2021-09-06T08:38:18.52719Z","shell.execute_reply":"2021-09-06T08:38:18.534361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reducing some memory","metadata":{}},{"cell_type":"code","source":"# For Checking memory taken of the dataset \n\ndef memory_check(df):\n    if isinstance(df,pd.DataFrame):\n        usage_b = df.memory_usage(deep=True).sum()\n    else: \n        usage_b = df.memory_usage(deep=True)\n    usage_mb = usage_b / 1024 ** 2 # convert bytes to megabytes\n    return \"{:03.2f} MB\".format(usage_mb)","metadata":{"execution":{"iopub.status.busy":"2021-09-06T08:39:17.352934Z","iopub.execute_input":"2021-09-06T08:39:17.353307Z","iopub.status.idle":"2021-09-06T08:39:17.358192Z","shell.execute_reply.started":"2021-09-06T08:39:17.35327Z","shell.execute_reply":"2021-09-06T08:39:17.357015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets convert the columns with float64 dtype to float; so that we save some memory\n\nget_float = data.select_dtypes(include=['float'])\nconverted_float = get_float.apply(pd.to_numeric,downcast='float')","metadata":{"execution":{"iopub.status.busy":"2021-09-06T08:39:21.282432Z","iopub.execute_input":"2021-09-06T08:39:21.282756Z","iopub.status.idle":"2021-09-06T08:39:21.979367Z","shell.execute_reply.started":"2021-09-06T08:39:21.282727Z","shell.execute_reply":"2021-09-06T08:39:21.978528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_data = data.copy()\nnew_data[converted_float.columns] = converted_float\nprint(\"Memory Allocation Comparison:\")\nprint(\"Original data:\", memory_check(data))\nprint(\"Updated data:\", memory_check(new_data))","metadata":{"execution":{"iopub.status.busy":"2021-09-06T08:39:24.087552Z","iopub.execute_input":"2021-09-06T08:39:24.087929Z","iopub.status.idle":"2021-09-06T08:39:46.469656Z","shell.execute_reply.started":"2021-09-06T08:39:24.087884Z","shell.execute_reply":"2021-09-06T08:39:46.468688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The updated data has much less memory than original data**","metadata":{}},{"cell_type":"code","source":"# Lets clear the original data\n\nimport gc\ndel data\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-09-06T08:39:59.683693Z","iopub.execute_input":"2021-09-06T08:39:59.684021Z","iopub.status.idle":"2021-09-06T08:39:59.773431Z","shell.execute_reply.started":"2021-09-06T08:39:59.683994Z","shell.execute_reply":"2021-09-06T08:39:59.772772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"markdown","source":"## Data Description","metadata":{}},{"cell_type":"code","source":"new_data.describe()","metadata":{"execution":{"iopub.status.busy":"2021-09-06T08:40:05.386342Z","iopub.execute_input":"2021-09-06T08:40:05.386927Z","iopub.status.idle":"2021-09-06T08:40:10.099822Z","shell.execute_reply.started":"2021-09-06T08:40:05.386892Z","shell.execute_reply":"2021-09-06T08:40:10.098869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Missing Data","metadata":{}},{"cell_type":"code","source":"# Features having missing values with their missing percent\n\nmissing_features = [cols for cols in new_data.columns if new_data[cols].isnull().sum()>1]\n\nfor feature in missing_features:\n    print(feature, np.round(new_data[feature].isnull().mean(), 4),  ' % missing values')","metadata":{"execution":{"iopub.status.busy":"2021-09-06T08:40:53.089714Z","iopub.execute_input":"2021-09-06T08:40:53.090086Z","iopub.status.idle":"2021-09-06T08:40:53.53075Z","shell.execute_reply.started":"2021-09-06T08:40:53.090049Z","shell.execute_reply":"2021-09-06T08:40:53.529681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Almost every feature has some missing values**","metadata":{}},{"cell_type":"markdown","source":"## Lets check if missing values has any impact with the Target feature","metadata":{}},{"cell_type":"code","source":"for feature in missing_features:\n    data = new_data.copy()\n    \n    # Let's make a variable that indicates 1 if the observation was missing or zero otherwise\n    data[feature] = np.where(data[feature].isnull(), 1, 0)\n    \n    # Grouping by value count of claim and that feature\n    data.groupby(feature)['claim'].value_counts().unstack().plot.bar()\n    plt.title(feature)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-06T08:42:13.116746Z","iopub.execute_input":"2021-09-06T08:42:13.117225Z","iopub.status.idle":"2021-09-06T08:43:13.917874Z","shell.execute_reply.started":"2021-09-06T08:42:13.117187Z","shell.execute_reply":"2021-09-06T08:43:13.916997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**So it tells that almost all features with no NA rows have almost equal claim values(0 in feature) and for NA rows it has some what claim=1 values more**","metadata":{}},{"cell_type":"markdown","source":"## Lets check Discrete and Continuous numerical variables","metadata":{}},{"cell_type":"markdown","source":"### Discrete Features","metadata":{}},{"cell_type":"code","source":"# Get all numerical Features\n\nnumerical_features = [feature for feature in new_data.columns if new_data[feature].dtypes != 'O' and feature not in ['claim']]\nprint('Number of numerical variables: ', len(numerical_features))","metadata":{"execution":{"iopub.status.busy":"2021-09-06T08:44:07.046631Z","iopub.execute_input":"2021-09-06T08:44:07.047012Z","iopub.status.idle":"2021-09-06T08:44:07.057666Z","shell.execute_reply.started":"2021-09-06T08:44:07.046977Z","shell.execute_reply":"2021-09-06T08:44:07.05656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Discrete features: Numerical features with their unique values less than 30\n\ndiscrete_features = [feature for feature in numerical_features if len(new_data[feature].unique())<30 and feature not in ['Id']]\nprint(\"Discrete Variables Count: {}\".format(len(discrete_features)))","metadata":{"execution":{"iopub.status.busy":"2021-09-06T08:44:38.014397Z","iopub.execute_input":"2021-09-06T08:44:38.014814Z","iopub.status.idle":"2021-09-06T08:44:41.830817Z","shell.execute_reply.started":"2021-09-06T08:44:38.014781Z","shell.execute_reply":"2021-09-06T08:44:41.829747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**So, all out independent features are Continuous**","metadata":{}},{"cell_type":"markdown","source":"### Continuous Features","metadata":{}},{"cell_type":"code","source":"# Get all continuous features\n\ncontinuous_features = [feature for feature in numerical_features if feature not in discrete_features and feature not in ['id']]\nprint(\"Continuous feature Count {}\".format(len(continuous_features)))","metadata":{"execution":{"iopub.status.busy":"2021-09-06T08:45:06.147858Z","iopub.execute_input":"2021-09-06T08:45:06.148238Z","iopub.status.idle":"2021-09-06T08:45:06.153912Z","shell.execute_reply.started":"2021-09-06T08:45:06.148201Z","shell.execute_reply":"2021-09-06T08:45:06.152985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets check the distribution of each continuous variables\n\nfor feature in continuous_features:\n    data = new_data.copy()\n    data[feature].hist(bins=25)\n    plt.xlabel(feature)\n    plt.ylabel(\"Count\")\n    plt.title(feature)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-06T08:45:11.185929Z","iopub.execute_input":"2021-09-06T08:45:11.186319Z","iopub.status.idle":"2021-09-06T08:45:56.876216Z","shell.execute_reply.started":"2021-09-06T08:45:11.186282Z","shell.execute_reply":"2021-09-06T08:45:56.875085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Most of the features are skewly distributed; we can apply log transformation or Box-Cox transformation for handling it, keeping in mind the range of its values**","metadata":{}},{"cell_type":"markdown","source":"## Outliers","metadata":{}},{"cell_type":"code","source":"for feature in continuous_features:\n    data = new_data.copy()\n    if 0 in data[feature].unique():\n        pass\n    else:\n        data.boxplot(column=feature)\n        plt.ylabel(feature)\n        plt.title(feature)\n        plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-06T08:47:07.06801Z","iopub.execute_input":"2021-09-06T08:47:07.068419Z","iopub.status.idle":"2021-09-06T08:50:24.844536Z","shell.execute_reply.started":"2021-09-06T08:47:07.068381Z","shell.execute_reply":"2021-09-06T08:50:24.843589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Features have a lot of outliers to handle**","metadata":{}},{"cell_type":"markdown","source":"## Correlation Plot","metadata":{}},{"cell_type":"code","source":"#Correlation check\nimport seaborn as sns\n\ncorr = new_data.iloc[:,1:].corr()\nplt.subplots(figsize=(22,20))\nsns.heatmap(corr,xticklabels=corr.columns,yticklabels=corr.columns)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-06T08:51:01.42861Z","iopub.execute_input":"2021-09-06T08:51:01.428942Z","iopub.status.idle":"2021-09-06T08:51:42.838441Z","shell.execute_reply.started":"2021-09-06T08:51:01.428912Z","shell.execute_reply":"2021-09-06T08:51:42.837444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**No correlation can be seen among features**","metadata":{}},{"cell_type":"markdown","source":"**All the above analysis are done only on training dataset; same shall be done in testing to understand the features. The features have to be clean before starting. Also there are so many features, so we can apply PCA transformation to reduce some of these features before modelling.**","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}