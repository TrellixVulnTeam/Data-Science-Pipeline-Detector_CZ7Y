{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## TPS September 2021","metadata":{}},{"cell_type":"markdown","source":"*\"This is how you win ML competitions: you take other peoplesâ€™ work and ensemble them together.\"* - Vitaly Kuznetsov\n\nThis notebook is based on the article: https://mlwave.com/kaggle-ensembling-guide/","metadata":{}},{"cell_type":"markdown","source":"## What is Bagging Submissions?","metadata":{}},{"cell_type":"markdown","source":"Bagging submissions involves using others' submission files and averaging them in some way that essentially combines these predictions to be more powerful.","metadata":{}},{"cell_type":"markdown","source":"It also prevents overfitting. Multiple submissions have different predictions; these predictions combined will bring us closer to the best answer.\n\nConsider the picture below. Three linear regression lines averaged makes up a better separation.","metadata":{}},{"cell_type":"markdown","source":"![yeet](https://mlwave.com/beheer/wp-content/uploads/2015/06/perceptron-bagging.png)","metadata":{}},{"cell_type":"markdown","source":"## Read in submission files","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-23T04:25:54.325291Z","iopub.execute_input":"2021-09-23T04:25:54.325568Z","iopub.status.idle":"2021-09-23T04:25:54.356407Z","shell.execute_reply.started":"2021-09-23T04:25:54.325503Z","shell.execute_reply":"2021-09-23T04:25:54.355037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cb_sub = pd.read_csv('../input/bagging-submissions-dataset/CB_Submission.csv')\nlgbm_sub = pd.read_csv('../input/bagging-submissions-dataset/LGBM_submission.csv')\nxgb_sub = pd.read_csv('../input/bagging-submissions-dataset/XGB_Submission')","metadata":{"execution":{"iopub.status.busy":"2021-09-23T05:05:11.82115Z","iopub.execute_input":"2021-09-23T05:05:11.821377Z","iopub.status.idle":"2021-09-23T05:05:12.563709Z","shell.execute_reply.started":"2021-09-23T05:05:11.821355Z","shell.execute_reply":"2021-09-23T05:05:12.562396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Check for correlations","metadata":{}},{"cell_type":"markdown","source":"It's also useful to note that submission files with lower correlation tend to do better, so check their correlation first.","metadata":{}},{"cell_type":"code","source":"import matplotlib as plt\nimport plotly.figure_factory as ff\nimport plotly.express as px\n\ndata = np.corrcoef([cb_sub.claim, lgbm_sub.claim, xgb_sub.claim])\ngroup_labels = ['catboost', 'lgbm', 'xgboost']\nfig=px.imshow(data,x=group_labels, y=group_labels)\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-23T05:06:54.60055Z","iopub.execute_input":"2021-09-23T05:06:54.600768Z","iopub.status.idle":"2021-09-23T05:06:58.608215Z","shell.execute_reply.started":"2021-09-23T05:06:54.600747Z","shell.execute_reply":"2021-09-23T05:06:58.607129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Major types of Averaging","metadata":{}},{"cell_type":"markdown","source":"There are 3 major types of averaging used in competitions. These are:\n* **Simple Averaging**\n* **Rank Averaging**\n* **Weighted Averaging**","metadata":{}},{"cell_type":"markdown","source":"## Simple Averaging","metadata":{}},{"cell_type":"markdown","source":"Simple averaging is just adding up all the submission files and dividing by the number of files.\n\nFor example: **Final Submission = (Submission1 + Submission2 + Submission3) / 3**","metadata":{}},{"cell_type":"code","source":"simple_avg = (cb_sub + lgbm_sub + xgb_sub) / 3","metadata":{"execution":{"iopub.status.busy":"2021-09-23T05:27:19.339653Z","iopub.execute_input":"2021-09-23T05:27:19.33992Z","iopub.status.idle":"2021-09-23T05:27:19.350149Z","shell.execute_reply.started":"2021-09-23T05:27:19.33989Z","shell.execute_reply":"2021-09-23T05:27:19.349173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Rank averaging","metadata":{}},{"cell_type":"markdown","source":"Rank averaging is when you assign a rank to each submission based on performance.\n\nLet's imagine LGBM has the strongest performance, followed by XGBoost, then Catboost.\n\nWe would assign LGBM (3), XGBoost (2) and Catboost (1). Then, get their individual weights by dividing by the total (6).\n\nFor example: **Final Submission = LGBM * 3/6 + XGB * 2/6 + CB * 1/6**","metadata":{}},{"cell_type":"code","source":"rank_avg = lgbm_sub*3/6 + xgb_sub*2/6 + cb_sub*1/6","metadata":{"execution":{"iopub.status.busy":"2021-09-23T05:27:20.075116Z","iopub.execute_input":"2021-09-23T05:27:20.076045Z","iopub.status.idle":"2021-09-23T05:27:20.094977Z","shell.execute_reply.started":"2021-09-23T05:27:20.075987Z","shell.execute_reply":"2021-09-23T05:27:20.094197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Weighted Averaging","metadata":{}},{"cell_type":"markdown","source":"Weighted averaging is when you assign weights to each submission and see how they turn out on the leaderboard.\n\nThis method is mostly trial and error. Putting more weight on a submission causes that submission to have more effect on the final submission.\n\nJust make sure that the total of the weights add up to 1\n\nFor example: **Final Submission = Submission1 * 0.4 + Submission2 * 0.3 + Submission3 * 0.3**","metadata":{}},{"cell_type":"code","source":"weighted_avg = lgbm_sub*0.4 + xgb_sub*0.3 + cb_sub*0.3","metadata":{"execution":{"iopub.status.busy":"2021-09-23T05:27:21.38756Z","iopub.execute_input":"2021-09-23T05:27:21.387781Z","iopub.status.idle":"2021-09-23T05:27:21.400706Z","shell.execute_reply.started":"2021-09-23T05:27:21.38776Z","shell.execute_reply":"2021-09-23T05:27:21.399776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Power Averaging","metadata":{}},{"cell_type":"markdown","source":"This is a very situational type of averaging. It's applicable in TPS September as the leaderboard is based on optimizing for AUC. \n\nIt's also the opposite of normal averaging as it requires highly correlated models to perform well.\n\nFor example: **Final Submission = (Submission1^Power + Submission2^Power + Submission3^Power) / 3**\n\nI go in-depth on power averaging for TPS September in this notebook: https://www.kaggle.com/edrickkesuma/power-averaging-is-your-friend","metadata":{}},{"cell_type":"code","source":"power = 6\npower_avg = (lgbm_sub**power + xgb_sub**power + cb_sub**power) / 3","metadata":{"execution":{"iopub.status.busy":"2021-09-23T05:33:45.280569Z","iopub.execute_input":"2021-09-23T05:33:45.281293Z","iopub.status.idle":"2021-09-23T05:33:45.382417Z","shell.execute_reply.started":"2021-09-23T05:33:45.281237Z","shell.execute_reply":"2021-09-23T05:33:45.38138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Caveat","metadata":{}},{"cell_type":"markdown","source":"Do keep in mind that only combining submissions and blindly following the leaderboard score may cause you to **overfit** to the public LB. \n\nIf possible, get the submission files from your own models and make sure your oof (out of fold) predictions' score isn't too far from your leaderboard score.","metadata":{}}]}