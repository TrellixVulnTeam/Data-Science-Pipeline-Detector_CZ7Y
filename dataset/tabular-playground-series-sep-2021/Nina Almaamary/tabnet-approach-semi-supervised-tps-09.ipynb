{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports \n(ﾉ◕ヮ◕)ﾉ*:･ﾟ✧ Lets do dis ..  \n\n## Content\n1. [Exploring data](#Exploring-data)\n2. [Feature Selection](#Feature-Selection)\n3. [Preprocessing](#Preprocessing)\n    1. [Train valid split](#Train-valid-split)\n    2. [Add Features](#Add-Features)\n    3. [Missing values](#Impute)\n    4. [Scaler](#Scaler)\n    5. [Data Smoothing](#Data-Smoothing)\n4. [TabNet](#TabNet)\n    1. [Semi Supervised](#Semi-Supervised)\n        * [Unsupervised](#Unsupervised)\n        * [Supervised](#Supervised)\n5. [Submit](#Submit)","metadata":{}},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"! pip install pytorch-tabnet # if not installed","metadata":{"execution":{"iopub.status.busy":"2021-09-23T13:31:50.916421Z","iopub.execute_input":"2021-09-23T13:31:50.917468Z","iopub.status.idle":"2021-09-23T13:31:58.992288Z","shell.execute_reply.started":"2021-09-23T13:31:50.917363Z","shell.execute_reply":"2021-09-23T13:31:58.991387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\n\nimport pytorch_tabnet\nfrom pytorch_tabnet.tab_model import TabNetClassifier\nimport torch\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import roc_auc_score, accuracy_score\n\nrandom = 200\ntorch.manual_seed(random)\n\ntrain = '../input/tabular-playground-series-sep-2021/train.csv'\ntest = '../input/tabular-playground-series-sep-2021/test.csv'","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2021-09-23T13:31:58.994544Z","iopub.execute_input":"2021-09-23T13:31:58.995141Z","iopub.status.idle":"2021-09-23T13:32:01.023155Z","shell.execute_reply.started":"2021-09-23T13:31:58.995092Z","shell.execute_reply":"2021-09-23T13:32:01.022192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploring data ","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(train)\ntest_df = pd.read_csv(test)","metadata":{"execution":{"iopub.status.busy":"2021-09-23T13:32:01.024357Z","iopub.execute_input":"2021-09-23T13:32:01.024687Z","iopub.status.idle":"2021-09-23T13:32:38.544034Z","shell.execute_reply.started":"2021-09-23T13:32:01.024646Z","shell.execute_reply":"2021-09-23T13:32:38.543218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It looks like we have Nan values **(´。＿。｀)**. There are multiple approaches to deal with missing values \n* Drop Columns with Missing Values\n* Imputation \n* Interpolate\n\nWe will try both Imputation and Interpolation.  \nAnother thing to note here, lets get rid of the id because it creates [**data leakage.**](https://www.kaggle.com/alexisbcook/data-leakage) \n\n> *“ if any other feature whose value would not actually be available in practice at the time you’d want to use the model to make a prediction, is a feature that can introduce leakage to your model ”* ~ **Data Skeptic**","metadata":{}},{"cell_type":"code","source":"display(train_df.head())\n\n# Lets see the null values\nsum_na = train_df.isna().sum().sum()\nprint(f'Total Nan values {sum_na}')","metadata":{"execution":{"iopub.status.busy":"2021-09-23T13:32:38.545539Z","iopub.execute_input":"2021-09-23T13:32:38.545866Z","iopub.status.idle":"2021-09-23T13:32:38.808333Z","shell.execute_reply.started":"2021-09-23T13:32:38.54583Z","shell.execute_reply":"2021-09-23T13:32:38.807386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.pop('id')\ntest_id = test_df.pop('id')","metadata":{"execution":{"iopub.status.busy":"2021-09-23T13:32:38.811167Z","iopub.execute_input":"2021-09-23T13:32:38.811732Z","iopub.status.idle":"2021-09-23T13:32:38.821198Z","shell.execute_reply.started":"2021-09-23T13:32:38.811688Z","shell.execute_reply":"2021-09-23T13:32:38.820404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"When viewing different feature distribution, you could see that they are mostly **skewed**. We cannot use the **mean** when imputing, because our features are not symmetric. So we either replace with **median** or **mode**. [This article maybe useful to you](https://vitalflux.com/pandas-impute-missing-values-mean-median-mode/), it was useful to me.","metadata":{}},{"cell_type":"code","source":"train_df.hist(figsize=(30,30), bins=25, xlabelsize=0, ylabelsize=0, color='#cf1f1f')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-23T13:32:38.822164Z","iopub.execute_input":"2021-09-23T13:32:38.822409Z","iopub.status.idle":"2021-09-23T13:32:57.548799Z","shell.execute_reply.started":"2021-09-23T13:32:38.822383Z","shell.execute_reply":"2021-09-23T13:32:57.547766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Selection\nThere are multiple ways to select features:  \n* Filter Method\n* Wrapper Method\n* Embedded Method \n\nIn this notebook we will use the filter method","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\n\ntrain_corr = train_df.corr()\ntrain_mask = np.triu(np.ones_like(train_corr, dtype=bool))\n\nfig = plt.figure(figsize=(16, 16))\n\ntrain_corr1 = train_corr[train_corr > 0.001]\nsns.heatmap(train_corr, \n            square=True, \n            mask=train_mask,\n            annot=False,\n            cmap=plt.cm.Reds\n           )","metadata":{"execution":{"iopub.status.busy":"2021-09-23T13:32:57.549982Z","iopub.execute_input":"2021-09-23T13:32:57.550232Z","iopub.status.idle":"2021-09-23T13:33:36.476005Z","shell.execute_reply.started":"2021-09-23T13:32:57.550206Z","shell.execute_reply":"2021-09-23T13:33:36.475118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will pick the features that have a correlation higher than 0 with our target feature **o(\\*￣▽￣\\*)ブ**","metadata":{}},{"cell_type":"code","source":"cor_target = abs(train_corr[\"claim\"])\n\nrelevant_features = cor_target[cor_target>0]\nrelevant_features","metadata":{"execution":{"iopub.status.busy":"2021-09-23T13:33:36.477372Z","iopub.execute_input":"2021-09-23T13:33:36.477686Z","iopub.status.idle":"2021-09-23T13:33:36.4876Z","shell.execute_reply.started":"2021-09-23T13:33:36.477646Z","shell.execute_reply":"2021-09-23T13:33:36.486854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"relevent_train = train_df.loc[:, relevant_features.index]\nrelevent_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-23T13:33:36.489356Z","iopub.execute_input":"2021-09-23T13:33:36.4897Z","iopub.status.idle":"2021-09-23T13:33:36.787811Z","shell.execute_reply.started":"2021-09-23T13:33:36.489659Z","shell.execute_reply":"2021-09-23T13:33:36.787235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing \n\nOne thing to note here, if we are planing to impute or add features to our data, we need to first split it into train and val, to avoid [**data leakage**](https://www.kaggle.com/alexisbcook/data-leakage). Then deal with them separately.  ","metadata":{}},{"cell_type":"markdown","source":"### Train valid split","metadata":{}},{"cell_type":"code","source":"y = relevent_train['claim']\nX = relevent_train.drop('claim', axis=1)\n\nX_train, X_valid, y_train, y_valid = train_test_split(\n    X, y, \n    test_size=0.2,\n    train_size=0.8, \n    shuffle=True,\n    random_state=random)","metadata":{"execution":{"iopub.status.busy":"2021-09-23T13:33:36.788922Z","iopub.execute_input":"2021-09-23T13:33:36.789277Z","iopub.status.idle":"2021-09-23T13:33:38.409333Z","shell.execute_reply.started":"2021-09-23T13:33:36.789237Z","shell.execute_reply":"2021-09-23T13:33:38.408431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Add Features","metadata":{}},{"cell_type":"code","source":"pd.options.mode.chained_assignment = None\n\nf = [x for x in X_train.columns.values if x[0]==\"f\"]\n\nX_train['missing'] = X_train.loc[:,f].isna().sum(axis=1)\nX_train['abs_sum'] = X_train.loc[:,f].abs().sum(axis=1)\nX_train['median'] = X_train.loc[:,f].median(axis=1)\nX_train['var'] = X_train.loc[:,f].var(axis=1)\nX_train['std'] = X_train.loc[:,f].std(axis=1)\nX_train['mean'] = X_train.loc[:,f].mean(axis=1)\nX_train['max'] = X_train.loc[:,f].max(axis=1)\nX_train['min'] = X_train.loc[:,f].min(axis=1)\n\nX_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-23T13:33:38.410604Z","iopub.execute_input":"2021-09-23T13:33:38.410932Z","iopub.status.idle":"2021-09-23T13:33:48.696473Z","shell.execute_reply.started":"2021-09-23T13:33:38.410893Z","shell.execute_reply":"2021-09-23T13:33:48.695561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_valid['missing'] = X_valid.loc[:,f].isna().sum(axis=1)\nX_valid['abs_sum'] = X_valid.loc[:,f].abs().sum(axis=1)\nX_valid['median'] = X_valid.loc[:,f].median(axis=1)\nX_valid['var'] = X_valid.loc[:,f].var(axis=1)\nX_valid['std'] = X_valid.loc[:,f].std(axis=1)\nX_valid['mean'] = X_valid.loc[:,f].mean(axis=1)\nX_valid['max'] = X_valid.loc[:,f].max(axis=1)\nX_valid['min'] = X_valid.loc[:,f].min(axis=1)\n\npd.options.mode.chained_assignment = 'warn'\nX_valid.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-23T13:33:48.698118Z","iopub.execute_input":"2021-09-23T13:33:48.698435Z","iopub.status.idle":"2021-09-23T13:33:50.971227Z","shell.execute_reply.started":"2021-09-23T13:33:48.698397Z","shell.execute_reply":"2021-09-23T13:33:50.970374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Impute \nI'll use **median** because of the reasons stated previously.","metadata":{}},{"cell_type":"code","source":"my_imputer = SimpleImputer(strategy=\"median\")\ndef impute(X_t, X_v):\n    return pd.DataFrame(my_imputer.fit_transform(X_t)), pd.DataFrame(my_imputer.transform(X_v))\n\nX_train_imp, X_val_imp = impute(X_train, X_valid)","metadata":{"execution":{"iopub.status.busy":"2021-09-23T13:33:50.972485Z","iopub.execute_input":"2021-09-23T13:33:50.97273Z","iopub.status.idle":"2021-09-23T13:34:15.509188Z","shell.execute_reply.started":"2021-09-23T13:33:50.972702Z","shell.execute_reply":"2021-09-23T13:34:15.508613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Scaler\n[This](https://stackoverflow.com/questions/51841506/data-standardization-vs-normalization-vs-robust-scaler) helped me decide on which preprocessing approach is better.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import RobustScaler\n\ndef robust_scale(X_t, X_v):\n    scaler = RobustScaler()\n    \n    return pd.DataFrame(scaler.fit_transform(X_t)), pd.DataFrame(scaler.transform(X_v))\n\nX_train_imp_st, X_val_imp_st = robust_scale(X_train_imp, X_val_imp)","metadata":{"execution":{"iopub.status.busy":"2021-09-23T13:34:15.512744Z","iopub.execute_input":"2021-09-23T13:34:15.513506Z","iopub.status.idle":"2021-09-23T13:34:19.796427Z","shell.execute_reply.started":"2021-09-23T13:34:15.513472Z","shell.execute_reply":"2021-09-23T13:34:19.795444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(X_train_imp_st.head())\n\n# Lets see the null values\nna_sum = X_train_imp_st.isna().sum().sum()\nprint(f'Nan: {na_sum}')","metadata":{"execution":{"iopub.status.busy":"2021-09-23T13:34:19.797807Z","iopub.execute_input":"2021-09-23T13:34:19.798151Z","iopub.status.idle":"2021-09-23T13:34:20.018328Z","shell.execute_reply.started":"2021-09-23T13:34:19.798111Z","shell.execute_reply":"2021-09-23T13:34:20.017606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Fix Skewness\nWe will use quantile to automatically transfer our numeric inputs to have a standard probability distribution, [this](https://machinelearningmastery.com/quantile-transforms-for-machine-learning/) post was really helpful at understanding the use of Quantile Transforms and why it's useful.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import QuantileTransformer\n\ntrans = QuantileTransformer(n_quantiles=100, output_distribution='normal')","metadata":{"execution":{"iopub.status.busy":"2021-09-23T13:34:20.019394Z","iopub.execute_input":"2021-09-23T13:34:20.019635Z","iopub.status.idle":"2021-09-23T13:34:20.023902Z","shell.execute_reply.started":"2021-09-23T13:34:20.019594Z","shell.execute_reply":"2021-09-23T13:34:20.023007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Smoothing \nThere are 3 techniques to smooth data: \n* Binning\n* Regression \n* Outlier analysis   \n\n[This](https://machinelearningmastery.com/discretization-transforms-for-machine-learning/) was helpful. I'll be using Binning technique. (o′┏▽┓｀o) ","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import KBinsDiscretizer\n\nkbin = KBinsDiscretizer(n_bins=100, encode='ordinal',strategy='uniform')","metadata":{"execution":{"iopub.status.busy":"2021-09-23T13:34:20.025245Z","iopub.execute_input":"2021-09-23T13:34:20.025459Z","iopub.status.idle":"2021-09-23T13:34:20.034848Z","shell.execute_reply.started":"2021-09-23T13:34:20.025435Z","shell.execute_reply":"2021-09-23T13:34:20.034126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\n\npipe = Pipeline([\n    (\"scaler\", trans),\n    (\"binning\", kbin)\n])\n\ntrain_final = pd.DataFrame(pipe.fit_transform(X_train_imp_st))\nval_final = pd.DataFrame(pipe.transform(X_val_imp_st))","metadata":{"execution":{"iopub.status.busy":"2021-09-23T13:34:20.036051Z","iopub.execute_input":"2021-09-23T13:34:20.036918Z","iopub.status.idle":"2021-09-23T13:34:55.763909Z","shell.execute_reply.started":"2021-09-23T13:34:20.036882Z","shell.execute_reply":"2021-09-23T13:34:55.762798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_final.hist(figsize=(15,10), bins=64, color='#cf1f1f')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-23T13:34:55.765095Z","iopub.execute_input":"2021-09-23T13:34:55.765418Z","iopub.status.idle":"2021-09-23T13:35:20.777344Z","shell.execute_reply.started":"2021-09-23T13:34:55.765386Z","shell.execute_reply":"2021-09-23T13:35:20.776707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_final.hist(figsize=(15,10), bins=64, color='#cf1f1f')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-23T13:35:20.778194Z","iopub.execute_input":"2021-09-23T13:35:20.77872Z","iopub.status.idle":"2021-09-23T13:35:43.710676Z","shell.execute_reply.started":"2021-09-23T13:35:20.778687Z","shell.execute_reply":"2021-09-23T13:35:43.709736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TabNet ","metadata":{}},{"cell_type":"code","source":"Xtrain = train_final.to_numpy()\nXvalid = val_final.to_numpy()","metadata":{"execution":{"iopub.status.busy":"2021-09-23T13:35:43.711938Z","iopub.execute_input":"2021-09-23T13:35:43.712385Z","iopub.status.idle":"2021-09-23T13:35:43.717531Z","shell.execute_reply.started":"2021-09-23T13:35:43.712346Z","shell.execute_reply":"2021-09-23T13:35:43.716682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del X_train_imp_st, X_val_imp_st, trans, kbin, pipe, my_imputer","metadata":{"execution":{"iopub.status.busy":"2021-09-23T13:35:43.71904Z","iopub.execute_input":"2021-09-23T13:35:43.719447Z","iopub.status.idle":"2021-09-23T13:35:43.731324Z","shell.execute_reply.started":"2021-09-23T13:35:43.71941Z","shell.execute_reply":"2021-09-23T13:35:43.730413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Semi-Supervised","metadata":{}},{"cell_type":"markdown","source":"#### Unsupervised","metadata":{}},{"cell_type":"code","source":"from pytorch_tabnet.pretraining import TabNetPretrainer\n\nunsupervised_model = TabNetPretrainer(\n    optimizer_fn=torch.optim.Adam,\n    optimizer_params=dict(lr=2e-2),\n    mask_type='entmax',\n    )\n\n\nunsupervised_model.fit(\n    Xtrain,\n    eval_set=[Xvalid],\n    max_epochs=15 , patience=10,\n    batch_size=512, virtual_batch_size=256,\n    num_workers=0,\n    drop_last=False,\n    pretraining_ratio=0.8,\n\n)\n\nreconstructed_X, embedded_X = unsupervised_model.predict(Xtrain)\nassert(reconstructed_X.shape==embedded_X.shape)","metadata":{"execution":{"iopub.status.busy":"2021-09-23T13:35:43.732726Z","iopub.execute_input":"2021-09-23T13:35:43.733134Z","iopub.status.idle":"2021-09-23T14:06:47.277091Z","shell.execute_reply.started":"2021-09-23T13:35:43.733103Z","shell.execute_reply":"2021-09-23T14:06:47.276183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Save model (Optional)","metadata":{}},{"cell_type":"code","source":"# unsupervised_model.save_model('./pretrain')","metadata":{"execution":{"iopub.status.busy":"2021-09-23T14:06:47.278476Z","iopub.execute_input":"2021-09-23T14:06:47.278812Z","iopub.status.idle":"2021-09-23T14:06:47.282824Z","shell.execute_reply.started":"2021-09-23T14:06:47.278773Z","shell.execute_reply":"2021-09-23T14:06:47.281819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Supervised","metadata":{}},{"cell_type":"code","source":"model1 = TabNetClassifier(\n    optimizer_fn=torch.optim.Adam,\n    optimizer_params=dict(lr=2e-2),\n    scheduler_params={\"step_size\":5,\"gamma\":0.9},\n    scheduler_fn=torch.optim.lr_scheduler.StepLR,\n    mask_type='entmax'\n)","metadata":{"execution":{"iopub.status.busy":"2021-09-23T14:06:47.283937Z","iopub.execute_input":"2021-09-23T14:06:47.284143Z","iopub.status.idle":"2021-09-23T14:06:47.296855Z","shell.execute_reply.started":"2021-09-23T14:06:47.284119Z","shell.execute_reply":"2021-09-23T14:06:47.295891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model1.fit(\n    Xtrain, y_train,\n    eval_set=[(Xtrain, y_train), (Xvalid, y_valid)],\n    eval_name=['train', 'valid'],\n    eval_metric=['auc'],\n    max_epochs=15, patience=10,\n    batch_size=512, virtual_batch_size=256,\n    num_workers=0,\n    weights=1,\n    from_unsupervised=unsupervised_model,\n    drop_last=False\n)","metadata":{"execution":{"iopub.status.busy":"2021-09-23T14:08:01.147771Z","iopub.execute_input":"2021-09-23T14:08:01.14859Z","iopub.status.idle":"2021-09-23T14:43:52.931755Z","shell.execute_reply.started":"2021-09-23T14:08:01.14855Z","shell.execute_reply":"2021-09-23T14:43:52.930916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Save model (optional)","metadata":{}},{"cell_type":"code","source":"model1.save_model('./model_80')","metadata":{"execution":{"iopub.status.busy":"2021-09-23T14:43:52.933363Z","iopub.execute_input":"2021-09-23T14:43:52.933579Z","iopub.status.idle":"2021-09-23T14:43:52.954975Z","shell.execute_reply.started":"2021-09-23T14:43:52.933554Z","shell.execute_reply":"2021-09-23T14:43:52.954417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submit","metadata":{}},{"cell_type":"code","source":"feature = [f for f in relevant_features.index if 'f' in f]\n\nrelevent_test = test_df.loc[:, feature]\nrelevent_test.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-23T14:58:24.327523Z","iopub.execute_input":"2021-09-23T14:58:24.327837Z","iopub.status.idle":"2021-09-23T14:58:24.799172Z","shell.execute_reply.started":"2021-09-23T14:58:24.327807Z","shell.execute_reply":"2021-09-23T14:58:24.798542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"relevent_test['missing'] = relevent_test.loc[:,f].isna().sum(axis=1)\nrelevent_test['abs_sum'] = relevent_test.loc[:,f].abs().sum(axis=1)\nrelevent_test['median'] = relevent_test.loc[:,f].median(axis=1)\nrelevent_test['var'] = relevent_test.loc[:,f].var(axis=1)\nrelevent_test['std'] = relevent_test.loc[:,f].std(axis=1)\nrelevent_test['mean'] = relevent_test.loc[:,f].mean(axis=1)\nrelevent_test['max'] = relevent_test.loc[:,f].max(axis=1)\nrelevent_test['min'] = relevent_test.loc[:,f].min(axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-09-23T14:58:29.306524Z","iopub.execute_input":"2021-09-23T14:58:29.307338Z","iopub.status.idle":"2021-09-23T14:58:39.168039Z","shell.execute_reply.started":"2021-09-23T14:58:29.3073Z","shell.execute_reply":"2021-09-23T14:58:39.167094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_imputer = SimpleImputer(strategy=\"median\")\ntest_imp = pd.DataFrame(my_imputer.fit_transform(relevent_test))","metadata":{"execution":{"iopub.status.busy":"2021-09-23T14:58:39.169536Z","iopub.execute_input":"2021-09-23T14:58:39.169796Z","iopub.status.idle":"2021-09-23T14:58:58.642088Z","shell.execute_reply.started":"2021-09-23T14:58:39.169767Z","shell.execute_reply":"2021-09-23T14:58:58.641191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = RobustScaler()\nX_test = pd.DataFrame(scaler.fit_transform(test_imp))","metadata":{"execution":{"iopub.status.busy":"2021-09-23T14:58:58.643214Z","iopub.execute_input":"2021-09-23T14:58:58.643469Z","iopub.status.idle":"2021-09-23T14:59:01.369259Z","shell.execute_reply.started":"2021-09-23T14:58:58.643442Z","shell.execute_reply":"2021-09-23T14:59:01.368432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trans = QuantileTransformer(n_quantiles=64, output_distribution='normal')\nkbin = KBinsDiscretizer(n_bins=64, encode='ordinal',strategy='uniform')\n\npipe = Pipeline([\n    (\"scaler\", trans),\n    (\"binning\", kbin)\n])\n\ntest_final = pd.DataFrame(pipe.fit_transform(X_test))","metadata":{"execution":{"iopub.status.busy":"2021-09-23T14:59:01.371399Z","iopub.execute_input":"2021-09-23T14:59:01.371837Z","iopub.status.idle":"2021-09-23T14:59:19.687005Z","shell.execute_reply.started":"2021-09-23T14:59:01.371795Z","shell.execute_reply":"2021-09-23T14:59:19.686095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = model1.predict(test_final.to_numpy())","metadata":{"execution":{"iopub.status.busy":"2021-09-23T14:59:19.688166Z","iopub.execute_input":"2021-09-23T14:59:19.688419Z","iopub.status.idle":"2021-09-23T14:59:44.995097Z","shell.execute_reply.started":"2021-09-23T14:59:19.68839Z","shell.execute_reply":"2021-09-23T14:59:44.99423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame({\n    'id': test_id,\n    'claim': preds\n})\n\ndf = df.set_index('id')\ndf.to_csv('final.csv')","metadata":{"execution":{"iopub.status.busy":"2021-09-23T14:59:44.996863Z","iopub.execute_input":"2021-09-23T14:59:44.997074Z","iopub.status.idle":"2021-09-23T14:59:45.62327Z","shell.execute_reply.started":"2021-09-23T14:59:44.99705Z","shell.execute_reply":"2021-09-23T14:59:45.622538Z"},"trusted":true},"execution_count":null,"outputs":[]}]}