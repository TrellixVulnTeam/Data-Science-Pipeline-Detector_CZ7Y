{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2021-09-11T15:22:54.140329Z","iopub.execute_input":"2021-09-11T15:22:54.140696Z","iopub.status.idle":"2021-09-11T15:22:54.153691Z","shell.execute_reply.started":"2021-09-11T15:22:54.140612Z","shell.execute_reply":"2021-09-11T15:22:54.152552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Train=pd.read_csv(\"/kaggle/input/tabular-playground-series-sep-2021/train.csv\")\nTest=pd.read_csv(\"/kaggle/input/tabular-playground-series-sep-2021/test.csv\")\n","metadata":{"execution":{"iopub.status.busy":"2021-09-11T15:22:54.179145Z","iopub.execute_input":"2021-09-11T15:22:54.179393Z","iopub.status.idle":"2021-09-11T15:23:35.894243Z","shell.execute_reply.started":"2021-09-11T15:22:54.179369Z","shell.execute_reply":"2021-09-11T15:23:35.893319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Test1=pd.read_csv(\"/kaggle/input/tabular-playground-series-sep-2021/sample_solution.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-09-11T15:23:35.89581Z","iopub.execute_input":"2021-09-11T15:23:35.896231Z","iopub.status.idle":"2021-09-11T15:23:36.039084Z","shell.execute_reply.started":"2021-09-11T15:23:35.896177Z","shell.execute_reply":"2021-09-11T15:23:36.038186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(Train.shape)\nprint(Test.shape)\nprint(Train.dtypes)\nprint(Test.dtypes)\nprint(Train.dtypes.unique())\nprint(Test.dtypes.unique())\n","metadata":{"execution":{"iopub.status.busy":"2021-09-11T15:23:36.041299Z","iopub.execute_input":"2021-09-11T15:23:36.041697Z","iopub.status.idle":"2021-09-11T15:23:36.060386Z","shell.execute_reply.started":"2021-09-11T15:23:36.041658Z","shell.execute_reply":"2021-09-11T15:23:36.058964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(Train.isna().sum())\nprint(Test.isna().sum())","metadata":{"execution":{"iopub.status.busy":"2021-09-11T15:23:36.062934Z","iopub.execute_input":"2021-09-11T15:23:36.063456Z","iopub.status.idle":"2021-09-11T15:23:36.371932Z","shell.execute_reply.started":"2021-09-11T15:23:36.063415Z","shell.execute_reply":"2021-09-11T15:23:36.37092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_cols=Train.select_dtypes('number').columns\nobj_cols=Train.select_dtypes('object').columns\nprint(num_cols)\nprint(obj_cols)","metadata":{"execution":{"iopub.status.busy":"2021-09-11T15:23:36.3734Z","iopub.execute_input":"2021-09-11T15:23:36.373735Z","iopub.status.idle":"2021-09-11T15:23:36.642509Z","shell.execute_reply.started":"2021-09-11T15:23:36.373698Z","shell.execute_reply":"2021-09-11T15:23:36.641428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y=Train['claim']\nTrain=Train.drop(['claim'], axis=1)\nTrain.shape\n","metadata":{"execution":{"iopub.status.busy":"2021-09-11T15:23:36.644136Z","iopub.execute_input":"2021-09-11T15:23:36.644562Z","iopub.status.idle":"2021-09-11T15:23:36.913422Z","shell.execute_reply.started":"2021-09-11T15:23:36.644524Z","shell.execute_reply":"2021-09-11T15:23:36.912278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Fill missing values\nfrom sklearn.impute import SimpleImputer\n\n# Imputation\nmy_imputer = SimpleImputer()\nimputed_train = pd.DataFrame(my_imputer.fit_transform(Train))\nimputed_test = pd.DataFrame(my_imputer.transform(Test))\n\n# Imputation removed column names; put them back\nimputed_train.columns = Train.columns\nimputed_test.columns = Test.columns","metadata":{"execution":{"iopub.status.busy":"2021-09-11T15:23:36.914804Z","iopub.execute_input":"2021-09-11T15:23:36.915357Z","iopub.status.idle":"2021-09-11T15:23:41.262206Z","shell.execute_reply.started":"2021-09-11T15:23:36.915317Z","shell.execute_reply":"2021-09-11T15:23:41.261314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#standardization\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nstandard1 = scaler.fit_transform(imputed_train)\nstandard2 = scaler.transform(imputed_test)\n\n#normalization\n\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nnormal1 = scaler.fit_transform(standard1)\nnormal2 = scaler.transform(standard2)\n\nTrain = pd.DataFrame(normal1)\nprint(Train.shape)\nTest = pd.DataFrame(normal2)\n\n#useful_features= [c for c in df.columns if c not in (\"id\")]","metadata":{"execution":{"iopub.status.busy":"2021-09-11T15:23:41.264727Z","iopub.execute_input":"2021-09-11T15:23:41.265089Z","iopub.status.idle":"2021-09-11T15:23:45.096187Z","shell.execute_reply.started":"2021-09-11T15:23:41.265051Z","shell.execute_reply":"2021-09-11T15:23:45.095278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n# Use PCA\nfrom sklearn.decomposition import PCA\n \npca = PCA(n_components = 90)\n \nTrain = pca.fit_transform(normal1)\nTest = pca.transform(normal2)\n\nTrain = pd.DataFrame(Train)\nprint(Train.shape)\nTest = pd.DataFrame(Test)\nprint(Test.shape)\n\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2021-09-11T15:23:45.097746Z","iopub.execute_input":"2021-09-11T15:23:45.098074Z","iopub.status.idle":"2021-09-11T15:23:45.104281Z","shell.execute_reply.started":"2021-09-11T15:23:45.098039Z","shell.execute_reply":"2021-09-11T15:23:45.103176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nWhat Techniques to Use When\nGenerally, k-fold cross validation is the gold-standard for evaluating the performance of a machine learning algorithm on unseen data with k set to 3, 5, or 10.\nUse stratified cross validation to enforce class distributions when there are a large number of classes or an imbalance in instances for each class.\nUsing a train/test split is good for speed when using a slow algorithm and produces performance estimates with lower bias when using large datasets.\nThe best advice is to experiment and find a technique for your problem that is fast and produces reasonable estimates of performance that you can use to make decisions.\n\nIf in doubt, use 10-fold cross validation for regression problems and stratified 10-fold cross validation on classification problems.\n\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2021-09-11T15:23:45.105535Z","iopub.execute_input":"2021-09-11T15:23:45.10603Z","iopub.status.idle":"2021-09-11T15:23:45.115301Z","shell.execute_reply.started":"2021-09-11T15:23:45.105996Z","shell.execute_reply":"2021-09-11T15:23:45.114154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n\n#Using stratified K-fold split along with XGBClassifier\n\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_auc_score\nmodel= XGBClassifier(n_estimators = 2000, use_label_encoder= False,objective='binary:logistic', max_depth=8, learning_rate=0.01, alpha=0,tree_method='gpu_hist',gpu_id=0,predictor=\"gpu_predictor\", random_state =20)\nresult=cross_val_score(estimator=model,X=Train,y=y,scoring='roc_auc',cv=5) #https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html\nresult\n\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2021-09-11T15:23:45.116708Z","iopub.execute_input":"2021-09-11T15:23:45.117148Z","iopub.status.idle":"2021-09-11T15:23:45.124866Z","shell.execute_reply.started":"2021-09-11T15:23:45.117105Z","shell.execute_reply":"2021-09-11T15:23:45.123808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n#Using train,test split along with XGBClassifier\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_valid, y_train, y_valid = train_test_split(Train, y, test_size = 0.3, random_state = 0)\nprint(X_train.shape)\nprint(X_valid.shape)\nprint(y_train.shape)\nprint(y_valid.shape)\n\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.metrics import roc_auc_score\n\nparams= {'n_estimators' : 1500, \n         'use_label_encoder' : 'False',\n         'objective' :'binary:logistic', \n         'max_depth' : 6, \n         'learning_rate' : 0.02, \n         'alpha' : 10,\n         'tree_method' :'gpu_hist',\n         'gpu_id' : 0,\n         'predictor' :\"gpu_predictor\", \n         'random_state' : 20}\n\nmodel= XGBClassifier(**params)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_valid)\n\n#from sklearn.metrics import accuracy_score\nprint('XGBoost model accuracy score: {0:0.4f}'. format(roc_auc_score(y_valid, y_pred)))\n\"\"\"\n\n","metadata":{"execution":{"iopub.status.busy":"2021-09-11T15:23:45.126276Z","iopub.execute_input":"2021-09-11T15:23:45.126855Z","iopub.status.idle":"2021-09-11T15:23:45.13625Z","shell.execute_reply.started":"2021-09-11T15:23:45.126819Z","shell.execute_reply":"2021-09-11T15:23:45.135438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n#XGB classfier with hyperparameter tuning using Hyperopt \n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_valid, y_train, y_valid = train_test_split(Train, y, test_size = 0.3, random_state = 0)\nprint(X_train.shape)\nprint(X_valid.shape)\nprint(y_train.shape)\nprint(y_valid.shape)\n\nfrom hyperopt import hp\nfrom hyperopt import STATUS_OK, Trials, fmin, hp, tpe\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.metrics import roc_auc_score\n\nspace={'max_depth': hp.quniform(\"max_depth\", 3, 18, 1),\n       'learning_rate' : hp.quniform(\"learning_rate\", 0, 1, 0.01), \n        'alpha' : hp.uniform(\"alpha\", 1, 9),\n        'gamma': hp.uniform ('gamma', 1,9),\n        'reg_alpha' : hp.quniform('reg_alpha', 40,180,1),\n        'reg_lambda' : hp.uniform('reg_lambda', 0,1),\n        'colsample_bytree' : hp.uniform('colsample_bytree', 0.5,1),\n        'min_child_weight' : hp.quniform('min_child_weight', 0, 10, 1),\n        'n_estimators': hp.choice('n_estimators', range(50,1000)),\n        'seed': 0\n    }\n\ndef objective(space):\n    \n    clf=XGBClassifier(\n                    n_estimators =space['n_estimators'], use_label_encoder= False, max_depth = int(space['max_depth']), \n                    gamma = space['gamma'],reg_alpha = int(space['reg_alpha']), \n                    min_child_weight=int(space['min_child_weight']),\n                    tree_method='gpu_hist',gpu_id=0,predictor=\"gpu_predictor\", random_state =20,\n                    colsample_bytree=int(space['colsample_bytree']))\n    \n    evaluation = [( X_train, y_train), ( X_valid, y_valid)]\n    \n    clf.fit(X_train, y_train,\n            eval_set=evaluation, eval_metric=\"auc\",\n            early_stopping_rounds=10,verbose=False)\n    \n\n    pred = clf.predict(X_valid)\n    accuracy = roc_auc_score(y_valid, pred>0.5)\n    print (\"SCORE:\", accuracy)\n    return {'loss': -accuracy, 'status': STATUS_OK }\n\ntrials = Trials()\n\nbest_hyperparams = fmin(fn = objective,\n                        space = space,\n                        algo = tpe.suggest,\n                        max_evals = 100,\n                        trials = trials)\n\nprint(\"The best hyperparameters are : \",\"\\n\")\nprint(best_hyperparams)\n\n\nparams= {'n_estimators' : 184, \n         'use_label_encoder' : 'False',\n         'objective' :'binary:logistic', \n         'learning_rate': 0.33,\n         'max_depth' : 18, \n         'min_child_weight' : 8,\n         'alpha': 2.7457662489762638,\n         'reg_alpha' :41,\n         'reg_lambda': 0.5127805406197737,\n         'gamma': 1.2189238435974041,\n         'colsample_bytree': 0.5759927696082564,\n         'tree_method' :'gpu_hist',\n         'gpu_id' : 0,\n         'predictor' :\"gpu_predictor\", \n         'random_state' : 20}\n\nmodel= XGBClassifier(**params)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_valid)\n\n#from sklearn.metrics import accuracy_score\nprint('XGBoost model accuracy score: {0:0.4f}'. format(roc_auc_score(y_valid, y_pred)))\n\nPredict1 = model.predict(Test)\noutput = pd.DataFrame({'id': Test1.id,\n                       'claim': Predict1})\noutput.to_csv('submission.csv', index=False)\n\n\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2021-09-11T15:23:45.139074Z","iopub.execute_input":"2021-09-11T15:23:45.139343Z","iopub.status.idle":"2021-09-11T15:23:45.147831Z","shell.execute_reply.started":"2021-09-11T15:23:45.139319Z","shell.execute_reply":"2021-09-11T15:23:45.146979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#XGB classfier with hyperparameter tuning using Hyperopt, with cross validation \n\nfrom hyperopt import hp\nfrom hyperopt import STATUS_OK, Trials, fmin, hp, tpe\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import cross_val_score\nimport xgboost as xgb\n\nspace={'max_depth': hp.quniform(\"max_depth\", 3, 18, 1),\n       'learning_rate' : hp.quniform(\"learning_rate\", 0, 1, 0.01), \n        'alpha' : hp.uniform(\"alpha\", 1, 9),\n        'gamma': hp.uniform ('gamma', 1,9),\n        'reg_alpha' : hp.quniform('reg_alpha', 40,180,1),\n        'reg_lambda' : hp.uniform('reg_lambda', 0,1),\n        'colsample_bytree' : hp.uniform('colsample_bytree', 0.5,1),\n        'min_child_weight' : hp.quniform('min_child_weight', 0, 10, 1),\n        'n_estimators': hp.choice('n_estimators', range(100,2000)),\n    }\n\ndef objective(space):\n    \n    clf=XGBClassifier( \n                    use_label_encoder= False, \n                    max_depth = int(space['max_depth']),\n                    learning_rate =space['learning_rate'],\n                    alpha = space['alpha'],\n                    gamma = space['gamma'],\n                    reg_alpha = int(space['reg_alpha']),\n                    reg_lambda = int(space['reg_lambda']),\n                    colsample_bytree=int(space['colsample_bytree']),\n                    min_child_weight=int(space['min_child_weight']),\n                    n_estimators =space['n_estimators'],\n                    seed=0,\n                    objective='binary:logistic',\n                    tree_method='gpu_hist',\n                    gpu_id=0,\n                    predictor=\"gpu_predictor\", \n                    )\n    \n    \n    clf.fit(Train, y)\n\n    # Applying k-Fold Cross Validation\n    from sklearn.model_selection import cross_val_score\n    accuracies = cross_val_score(estimator = clf, X = Train, y = y,scoring='roc_auc', cv = 5)\n    CrossValMean = accuracies.mean()\n\n    print(\"CrossValMean:\", CrossValMean)\n\n    return{'loss':1-CrossValMean, 'status': STATUS_OK }\n    \n    \n    #result=cross_val_score(estimator=clf,X=Train,y=y,scoring='roc_auc',cv=5) #https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html\n\n","metadata":{"execution":{"iopub.status.busy":"2021-09-11T15:23:45.149217Z","iopub.execute_input":"2021-09-11T15:23:45.14971Z","iopub.status.idle":"2021-09-11T15:23:45.58561Z","shell.execute_reply.started":"2021-09-11T15:23:45.149673Z","shell.execute_reply":"2021-09-11T15:23:45.584617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trials = Trials()\n\nbest_hyperparams = fmin(fn = objective,\n                        space = space,\n                        algo = tpe.suggest,\n                        max_evals = 5,\n                        trials = trials)","metadata":{"execution":{"iopub.status.busy":"2021-09-11T15:23:45.587169Z","iopub.execute_input":"2021-09-11T15:23:45.587759Z","iopub.status.idle":"2021-09-11T15:29:46.105792Z","shell.execute_reply.started":"2021-09-11T15:23:45.587721Z","shell.execute_reply":"2021-09-11T15:29:46.104855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"The best hyperparameters are : \",\"\\n\")\nprint(best_hyperparams)","metadata":{"execution":{"iopub.status.busy":"2021-09-11T15:29:46.10708Z","iopub.execute_input":"2021-09-11T15:29:46.107456Z","iopub.status.idle":"2021-09-11T15:29:46.113146Z","shell.execute_reply.started":"2021-09-11T15:29:46.107418Z","shell.execute_reply":"2021-09-11T15:29:46.112067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fitting XGBoost to the Training set\nfrom xgboost import XGBClassifier\n\n#warnings.filterwarnings(action='ignore', category=DeprecationWarning)\nclf = XGBClassifier(n_estimators = best_hyperparams['n_estimators'],\n                            max_depth = int(best_hyperparams['max_depth']),\n                            learning_rate = best_hyperparams['learning_rate'],\n                            gamma = best_hyperparams['gamma'],\n                            min_child_weight = best_hyperparams['min_child_weight'],\n                            colsample_bytree = best_hyperparams['colsample_bytree'],\n                            alpha = best_hyperparams['alpha'],\n                            reg_alpha = best_hyperparams['reg_alpha'],\n                            reg_lambda = best_hyperparams['reg_lambda'],\n                            seed = 0,\n                            objective='binary:logistic',\n                            tree_method='gpu_hist',\n                            gpu_id=0,\n                            predictor=\"gpu_predictor\"\n                            )\n\n\nclf.fit(Train, y)\n\n# Applying k-Fold Cross Validation\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = clf, X = Train, y = y, cv = 5)\nprint(accuracies)\nCrossValMean = accuracies.mean()\nprint(\"Final CrossValMean: \", CrossValMean)","metadata":{"execution":{"iopub.status.busy":"2021-09-11T15:29:46.114366Z","iopub.execute_input":"2021-09-11T15:29:46.11485Z","iopub.status.idle":"2021-09-11T15:30:51.460508Z","shell.execute_reply.started":"2021-09-11T15:29:46.114814Z","shell.execute_reply":"2021-09-11T15:30:51.459621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Predict1 = clf.predict(Test)\noutput = pd.DataFrame({'id': Test1.id,\n                       'claim': Predict1})\noutput.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-11T15:30:51.461708Z","iopub.execute_input":"2021-09-11T15:30:51.462204Z","iopub.status.idle":"2021-09-11T15:30:53.614137Z","shell.execute_reply.started":"2021-09-11T15:30:51.462166Z","shell.execute_reply":"2021-09-11T15:30:53.613065Z"},"trusted":true},"execution_count":null,"outputs":[]}]}