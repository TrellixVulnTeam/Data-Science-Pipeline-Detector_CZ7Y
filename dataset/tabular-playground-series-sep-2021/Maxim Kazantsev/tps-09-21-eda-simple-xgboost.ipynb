{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom xgboost import XGBClassifier\nfrom sklearn.cluster import MiniBatchKMeans\n\n\n# Pandas setting to display more dataset rows and columns\npd.set_option('display.max_rows', 150)\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_colwidth', None)\npd.set_option('display.float_format', lambda x: '%.5f' % x)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-01T17:32:21.503413Z","iopub.execute_input":"2021-09-01T17:32:21.503864Z","iopub.status.idle":"2021-09-01T17:32:22.23091Z","shell.execute_reply.started":"2021-09-01T17:32:21.5038Z","shell.execute_reply":"2021-09-01T17:32:22.229152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Data import**","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/tabular-playground-series-sep-2021/train.csv\", low_memory=False)#, nrows=10000)\n# train[\"date_time\"] = pd.to_datetime(train[\"date_time\"], format=\"%Y-%m-%d %H:%M:%S\")\ntest = pd.read_csv(\"/kaggle/input/tabular-playground-series-sep-2021/test.csv\", low_memory=False)\n# test[\"date_time\"] = pd.to_datetime(test[\"date_time\"], format=\"%Y-%m-%d %H:%M:%S\")\ntrain.info(memory_usage=\"deep\")","metadata":{"execution":{"iopub.status.busy":"2021-09-01T17:36:59.213808Z","iopub.execute_input":"2021-09-01T17:36:59.214379Z","iopub.status.idle":"2021-09-01T17:38:03.583348Z","shell.execute_reply.started":"2021-09-01T17:36:59.214342Z","shell.execute_reply":"2021-09-01T17:38:03.582221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.info(memory_usage=\"deep\")","metadata":{"execution":{"iopub.status.busy":"2021-09-01T17:33:37.069922Z","iopub.execute_input":"2021-09-01T17:33:37.070237Z","iopub.status.idle":"2021-09-01T17:33:37.085993Z","shell.execute_reply.started":"2021-09-01T17:33:37.070203Z","shell.execute_reply":"2021-09-01T17:33:37.08514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **EDA**","metadata":{}},{"cell_type":"code","source":"# Colors to be used for plots\ncolors = [\"lightcoral\", \"sandybrown\", \"darkorange\", \"mediumseagreen\",\n          \"lightseagreen\", \"cornflowerblue\", \"mediumpurple\", \"palevioletred\",\n          \"lightskyblue\", \"sandybrown\", \"yellowgreen\", \"indianred\",\n          \"lightsteelblue\", \"mediumorchid\", \"deepskyblue\"]","metadata":{"execution":{"iopub.status.busy":"2021-09-01T17:33:37.087253Z","iopub.execute_input":"2021-09-01T17:33:37.087796Z","iopub.status.idle":"2021-09-01T17:33:37.101323Z","shell.execute_reply.started":"2021-09-01T17:33:37.087754Z","shell.execute_reply":"2021-09-01T17:33:37.100379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-01T17:33:37.102899Z","iopub.execute_input":"2021-09-01T17:33:37.103237Z","iopub.status.idle":"2021-09-01T17:33:37.190515Z","shell.execute_reply.started":"2021-09-01T17:33:37.103205Z","shell.execute_reply":"2021-09-01T17:33:37.189472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.columns.values","metadata":{"execution":{"iopub.status.busy":"2021-09-01T17:33:37.191854Z","iopub.execute_input":"2021-09-01T17:33:37.192167Z","iopub.status.idle":"2021-09-01T17:33:37.198284Z","shell.execute_reply.started":"2021-09-01T17:33:37.192134Z","shell.execute_reply":"2021-09-01T17:33:37.197623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(5, 5))\npie = ax.pie([len(train), len(test)],\n             labels=[\"Train dataset\", \"Test dataset\"],\n             colors=[\"salmon\", \"teal\"],\n             textprops={\"fontsize\": 15},\n             autopct='%1.1f%%')\nax.axis(\"equal\")\nax.set_title(\"Dataset length comparison\", fontsize=18)\nfig.set_facecolor('white')\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-09-01T17:33:37.199424Z","iopub.execute_input":"2021-09-01T17:33:37.199928Z","iopub.status.idle":"2021-09-01T17:33:37.318978Z","shell.execute_reply.started":"2021-09-01T17:33:37.199889Z","shell.execute_reply":"2021-09-01T17:33:37.317856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.describe().T","metadata":{"execution":{"iopub.status.busy":"2021-09-01T17:33:37.321508Z","iopub.execute_input":"2021-09-01T17:33:37.321896Z","iopub.status.idle":"2021-09-01T17:33:42.922737Z","shell.execute_reply.started":"2021-09-01T17:33:37.321863Z","shell.execute_reply":"2021-09-01T17:33:42.921512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(6, 6))\n\nbars = ax.bar(train[\"claim\"].value_counts().index,\n              train[\"claim\"].value_counts().values,\n              color=colors,\n              edgecolor=\"black\",\n              width=0.4)\nax.set_title(\"Claim (target) values distribution\", fontsize=20, pad=15)\nax.set_ylabel(\"Amount of values\", fontsize=14, labelpad=15)\nax.set_xlabel(\"Claim (target) value\", fontsize=14, labelpad=10)\nax.set_xticks(train[\"claim\"].value_counts().index)\nax.tick_params(axis=\"both\", labelsize=14)\nax.bar_label(bars, [f\"{x:2.2f}%\" for x in train[\"claim\"].value_counts().values/(len(train)/100)],\n                 padding=5, fontsize=15)\nax.bar_label(bars, [f\"{x:2d}\" for x in train[\"claim\"].value_counts().values],\n                 padding=-30, fontsize=15)\nax.margins(0.2, 0.12)\nax.grid(axis=\"y\")\n\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-09-01T17:33:42.924587Z","iopub.execute_input":"2021-09-01T17:33:42.924911Z","iopub.status.idle":"2021-09-01T17:33:43.13494Z","shell.execute_reply.started":"2021-09-01T17:33:42.924881Z","shell.execute_reply":"2021-09-01T17:33:43.133696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The target value classes are balanced which is good.","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(16, 6))\n\nbars = ax.bar(train.isna().sum().index,\n              train.isna().sum().values,\n              color=\"lightskyblue\",\n              edgecolor=\"black\",\n              width=0.7)\nax.set_title(\"Missing feature values distribution in the train dataset\", fontsize=20, pad=15)\nax.set_ylabel(\"Missing values\", fontsize=14, labelpad=15)\nax.set_xlabel(\"Feature\", fontsize=14, labelpad=10)\nax.set_xticks([x if i%2==0 else \"\" for i, x in enumerate(train.columns.values)])\nax.tick_params(axis=\"x\", rotation=90, labelsize=8)\nax.margins(0.005, 0.12)\nax.grid(axis=\"y\")\n\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-09-01T17:33:43.136734Z","iopub.execute_input":"2021-09-01T17:33:43.137179Z","iopub.status.idle":"2021-09-01T17:33:44.891946Z","shell.execute_reply.started":"2021-09-01T17:33:43.137124Z","shell.execute_reply":"2021-09-01T17:33:44.890692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(16, 6))\n\nbars = ax.bar(test.isna().sum().index,\n              test.isna().sum().values,\n              color=\"lightsteelblue\",\n              edgecolor=\"black\",\n              width=0.7)\nax.set_title(\"Missing feature values distributionin in the test dataset\", fontsize=20, pad=15)\nax.set_ylabel(\"Missing values\", fontsize=14, labelpad=15)\nax.set_xlabel(\"Feature\", fontsize=14, labelpad=10)\nax.set_xticks([x if i%2==0 else \"\" for i, x in enumerate(test.columns.values)])\nax.tick_params(axis=\"x\", rotation=90, labelsize=8)\nax.margins(0.005, 0.12)\nax.grid(axis=\"y\")\n\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-09-01T17:33:44.893744Z","iopub.execute_input":"2021-09-01T17:33:44.894372Z","iopub.status.idle":"2021-09-01T17:33:46.284707Z","shell.execute_reply.started":"2021-09-01T17:33:44.894316Z","shell.execute_reply":"2021-09-01T17:33:46.283544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see, both train and test datasets have missing values in every feature excepth for \"id\" and \"claim\". We should take care with them.","metadata":{}},{"cell_type":"markdown","source":"Let's check feature values distribution in the both datasets.","metadata":{}},{"cell_type":"code","source":"df = pd.concat([train.drop([\"id\", \"claim\"], axis=1), test.drop(\"id\", axis=1)], axis=0)\ncolumns = df.columns.values\n\ncols = 4\nrows = len(columns) // cols + 1\n\nfig, axs = plt.subplots(ncols=cols, nrows=rows, figsize=(16,130), sharex=False)\n\nplt.subplots_adjust(hspace = 0.3)\ni=0\n\nfor r in np.arange(0, rows, 1):\n    for c in np.arange(0, cols, 1):\n        if i >= len(columns):\n            axs[r, c].set_visible(False)\n        else:\n            hist1 = axs[r, c].hist(train[columns[i]].values,\n                                   range=(df[columns[i]].min(),\n                                          df[columns[i]].max()),\n                                   bins=40,\n                                   color=\"deepskyblue\",\n                                   edgecolor=\"black\",\n                                   alpha=0.7,\n                                   label=\"Train Dataset\")\n            hist2 = axs[r, c].hist(test[columns[i]].values,\n                                   range=(df[columns[i]].min(),\n                                          df[columns[i]].max()),\n                                   bins=40,\n                                   color=\"palevioletred\",\n                                   edgecolor=\"black\",\n                                   alpha=0.7,\n                                   label=\"Test Dataset\")\n            axs[r, c].set_title(columns[i], fontsize=12, pad=5)\n            axs[r, c].set_yticks(axs[r, c].get_yticks())\n            axs[r, c].set_yticklabels([str(int(i/1000))+\"k\" for i in axs[r, c].get_yticks()])\n            axs[r, c].tick_params(axis=\"y\", labelsize=10)\n            axs[r, c].tick_params(axis=\"x\", labelsize=10)\n            axs[r, c].grid(axis=\"y\")\n            axs[r, c].legend(fontsize=13)\n                                  \n        i+=1\n#plt.suptitle(\"Feature values distribution in both datasets\", y=0.99)\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-09-01T17:33:46.286335Z","iopub.execute_input":"2021-09-01T17:33:46.286738Z","iopub.status.idle":"2021-09-01T17:34:36.530326Z","shell.execute_reply.started":"2021-09-01T17:33:46.286699Z","shell.execute_reply":"2021-09-01T17:34:36.529199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see, the datasets are well balanced. So target distribution should probably be the same for test predictions.","metadata":{}},{"cell_type":"code","source":"print(\"Features with the leas amount of unique values:\")\ntrain.drop([\"id\", \"claim\"], axis=1).nunique().sort_values().head(5)","metadata":{"execution":{"iopub.status.busy":"2021-09-01T17:34:36.531644Z","iopub.execute_input":"2021-09-01T17:34:36.532104Z","iopub.status.idle":"2021-09-01T17:34:43.039243Z","shell.execute_reply.started":"2021-09-01T17:34:36.532065Z","shell.execute_reply":"2021-09-01T17:34:43.038185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are no categorical features in the dataset.\n\nLet's look at feature correlation.","metadata":{}},{"cell_type":"code","source":"# Plot dataframe\ndf = train.drop(\"id\", axis=1).corr().round(5)\n\n# Mask to hide upper-right part of plot as it is a duplicate\nmask = np.zeros_like(df)\nmask[np.triu_indices_from(mask)] = True\n\n# Making a plot\nplt.figure(figsize=(16,16))\nax = sns.heatmap(df, annot=False, mask=mask, cmap=\"RdBu\", annot_kws={\"weight\": \"bold\", \"fontsize\":13})\nax.set_title(\"Feature correlation heatmap\", fontsize=17)\nplt.setp(ax.get_xticklabels(), rotation=90, ha=\"right\",\n         rotation_mode=\"anchor\", weight=\"normal\")\nplt.setp(ax.get_yticklabels(), weight=\"normal\",\n         rotation_mode=\"anchor\", rotation=0, ha=\"right\")\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-09-01T17:34:43.04262Z","iopub.execute_input":"2021-09-01T17:34:43.04312Z","iopub.status.idle":"2021-09-01T17:35:23.484048Z","shell.execute_reply.started":"2021-09-01T17:34:43.043073Z","shell.execute_reply":"2021-09-01T17:35:23.48271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is very weak linear correlation between the features. There are some features with relatively low correlation with target value even comparing with other features:","metadata":{}},{"cell_type":"code","source":"df[(df[\"claim\"]>-0.001) & (df[\"claim\"]<0.001)][\"claim\"]","metadata":{"execution":{"iopub.status.busy":"2021-09-01T17:35:23.486017Z","iopub.execute_input":"2021-09-01T17:35:23.486486Z","iopub.status.idle":"2021-09-01T17:35:23.508227Z","shell.execute_reply.started":"2021-09-01T17:35:23.486421Z","shell.execute_reply":"2021-09-01T17:35:23.506777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Probably worth a try to drop them and check if it improves the result.","metadata":{}},{"cell_type":"markdown","source":"# **Data preprocessing**","metadata":{}},{"cell_type":"code","source":"features = [x for x in train.columns.values if x[0]==\"f\"]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The idea of adding a new feature below is taken from [this notebook](https://www.kaggle.com/hiro5299834/tps-sep-2021-single-lgbm) by [BIZEN](https://www.kaggle.com/hiro5299834).","metadata":{}},{"cell_type":"code","source":"# Counting amount of missing values in each row and adding it as a new feature\ntrain['n_missing'] = train[features].isna().sum(axis=1)\ntest['n_missing'] = test[features].isna().sum(axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Filling missing values with median of each column\nimputer = SimpleImputer(strategy=\"median\")\nfor col in features:\n    train[col] = imputer.fit_transform(np.array(train[col]).reshape(-1,1))\n    test[col] = imputer.transform(np.array(test[col]).reshape(-1,1))","metadata":{"execution":{"iopub.status.busy":"2021-09-01T17:35:23.509813Z","iopub.execute_input":"2021-09-01T17:35:23.51016Z","iopub.status.idle":"2021-09-01T17:35:23.514141Z","shell.execute_reply.started":"2021-09-01T17:35:23.510124Z","shell.execute_reply":"2021-09-01T17:35:23.513068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scaling all values to [0,1] range\nscaler = StandardScaler()\nfor col in features:\n    train[col] = scaler.fit_transform(np.array(train[col]).reshape(-1,1))\n    test[col] = scaler.transform(np.array(test[col]).reshape(-1,1))","metadata":{"execution":{"iopub.status.busy":"2021-09-01T17:38:03.585256Z","iopub.execute_input":"2021-09-01T17:38:03.585909Z","iopub.status.idle":"2021-09-01T17:38:07.441973Z","shell.execute_reply.started":"2021-09-01T17:38:03.585859Z","shell.execute_reply":"2021-09-01T17:38:07.440722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = train.drop([\"id\", \"claim\"], axis=1)\nX_test = test.drop(\"id\", axis=1)\ny = train[\"claim\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Model training**","metadata":{}},{"cell_type":"code","source":"# Model hyperparameters\nxgb_params = {'objective': 'binary:logistic',\n              'use_label_encoder': False,\n              'n_estimators': 2600,\n              'learning_rate': 0.04,\n              'subsample': 0.66,\n              'colsample_bytree': 0.1,\n              'max_depth': 8,\n              'booster': 'gbtree',\n              'gamma': 5.5,\n              'reg_alpha': 81.8,\n              'reg_lambda': 72.0,\n              'random_state': 42,\n              'tree_method': 'gpu_hist',\n              'n_jobs': 4}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Training a model on a full dataset\nmodel = XGBClassifier(**xgb_params)\nmodel.fit(X, y,\n          verbose=False)\n# Making probability of class \"1\" predictions\npreds = model.predict_proba(X_test)[:, 1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Feature importances**","metadata":{}},{"cell_type":"code","source":"# Making a DataFrame with feature importances\ndf = pd.DataFrame(columns=[\"Feature\", \"Importance\"])\ndf[\"Feature\"] = X.columns\ndf[\"Importance\"] = model.feature_importances_ / model.feature_importances_.sum()\ndf.sort_values(\"Importance\", axis=0, ascending=False, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = np.arange(0, len(df[\"Feature\"]))\nheight = 0.4\n\nfig, ax = plt.subplots(figsize=(16, 30))\nbars1 = ax.barh(x, df[\"Importance\"], height=height,\n                color=\"mediumorchid\", edgecolor=\"black\")\nax.set_title(\"Feature importances\", fontsize=30, pad=15)\nax.set_ylabel(\"Feature names\", fontsize=20, labelpad=15)\nax.set_xlabel(\"Feature importance\", fontsize=20, labelpad=15)\nax.set_yticks(x)\nax.set_yticklabels(df[\"Feature\"], fontsize=15)\nax.tick_params(axis=\"x\", labelsize=15)\nax.grid(axis=\"x\")\nax2 = ax.secondary_xaxis('top')\nax2.set_xlabel(\"Feature importance\", fontsize=20, labelpad=15)\nax2.tick_params(axis=\"x\", labelsize=15)\nplt.margins(0.04, 0.01)\nplt.gca().invert_yaxis()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Predictions submission**","metadata":{}},{"cell_type":"code","source":"predictions = pd.DataFrame()\npredictions[\"id\"] = test[\"id\"]\npredictions[\"claim\"] = preds\n\npredictions.to_csv('submission.csv', index=False, header=predictions.columns)\npredictions.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}