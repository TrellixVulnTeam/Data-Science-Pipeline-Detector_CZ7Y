{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Load Packages"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport shutil\nimport gc\n\nimport IPython\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\nimport librosa\nimport librosa.display\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import *\nfrom keras_preprocessing.image import ImageDataGenerator","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import viztools02","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load DataFrames"},{"metadata":{"trusted":true},"cell_type":"code","source":"path = '../input/freesound-audio-tagging/'\ntrain_path = path + 'audio_train/'\n\nprint(len(os.listdir(train_path)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/freesound-audio-tagging/train.csv\")\ntest = pd.read_csv(\"../input/freesound-audio-tagging/sample_submission.csv\")\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Train Shape:', train.shape)\nprint('Test Shape: ', test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Unique Labels"},{"metadata":{"trusted":true},"cell_type":"code","source":"uniq_labels = train.label.unique()\nprint(len(uniq_labels), '\\n')\nprint(uniq_labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Distibution of Classes"},{"metadata":{"trusted":true},"cell_type":"code","source":"category_group = train.groupby(['label', 'manually_verified']).count()\nplot = category_group.unstack().reindex(category_group.unstack().sum(axis=1).sort_values().index)\\\n          .plot(kind='bar', stacked=True, title=\"Number of Audio Samples per Category\", figsize=(16,10))\nplot.set_xlabel(\"Category\")\nplot.set_ylabel(\"Number of Samples\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Minimum samples per category = ', min(train.label.value_counts()))\nprint('Maximum samples per category = ', max(train.label.value_counts()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Analyze a Single Sound"},{"metadata":{"trusted":true},"cell_type":"code","source":"fname = train_path + '002d256b.wav'\nclip, sr = librosa.load(fname, sr=44100)\n\nprint(clip.shape)\nprint(sr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=[12, 6])\nplt.plot(clip)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=[12, 6])\nplt.plot(clip[2000:3000])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Subset DataFrame\n\nFor quicker training/experimentation."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_orig = train.copy()\ntrain = train_orig.sample(n=500, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Determine Number of Frames for Each File"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time \ntrain['nframes'] = train['fname'].apply(lambda f: librosa.load(train_path + f, sr=44100)[0].shape[0])\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=[12,4])\nplt.hist(train.nframes, bins=200)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(np.min(train.nframes))\nprint(np.max(train.nframes))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_, ax = plt.subplots(figsize=(20, 4))\nsns.violinplot(ax=ax, x=\"label\", y=\"nframes\", data=train)\nplt.xticks(rotation=90)\nplt.title('Distribution of audio frames, per label', fontsize=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Mel Spectrogram"},{"metadata":{"trusted":true},"cell_type":"code","source":"fname = train_path + '002d256b.wav'\n\nclip, sr = librosa.load(fname, sr=44100)\n\nS1 = librosa.feature.melspectrogram(y=clip, sr=44100)   # Numpy Array\nS2 = librosa.power_to_db(S1, ref=np.max)                # Numpy Array\n\nprint(S2.shape)\n\nlibrosa.display.specshow(S2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Display Several Spectrograms"},{"metadata":{"trusted":true},"cell_type":"code","source":"train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample = train.sample(16)\n\nplt.figure(figsize=[16,9])\n\nfor i in range(16):\n    fname = train_path + sample.fname.iloc[i]\n    clip, sr = librosa.load(fname, sr=44100)\n    S1 = librosa.feature.melspectrogram(y=clip, sr=44100)   # Numpy Array\n    S2 = librosa.power_to_db(S1, ref=np.max)                # Numpy Array\n    \n    plt.subplot(4, 4, i+1)\n    librosa.display.specshow(S2)\n    plt.title(f'{sample.label.iloc[i]} - {S2.shape[:2]} - {sample.nframes.iloc[i]}')\n\nplt.tight_layout()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rows = 3\ncols = 4\n    \nsample = train.sample(rows * cols)\n\nfor i in range(rows):\n    \n    plt.figure(figsize=[16,2])\n    for j in range(cols):\n        k = 4*i + j\n        wf = librosa.load( train_path + sample.fname.iloc[k] )[0] \n        plt.subplot(1, cols ,j+1)\n        plt.plot(wf)\n        plt.axis('off')\n        \n    plt.tight_layout()    \n    plt.show()\n    \n    plt.figure(figsize=[16,3])\n    for j in range(cols):\n        k = 4*i + j\n        fname = train_path + sample.fname.iloc[k]\n        clip, sr = librosa.load(fname, sr=44100)\n        S1 = librosa.feature.melspectrogram(y=clip, sr=44100)   # Numpy Array\n        S2 = librosa.power_to_db(S1, ref=np.max)                # Numpy Array\n        \n        plt.subplot(1, cols ,j+1)\n        librosa.display.specshow(S2)\n        plt.title(f'{sample.label.iloc[k]} - {S2.shape[:2]} - {sample.nframes.iloc[k]}')\n        \n        \n    plt.tight_layout() \n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create Spectrogram Files"},{"metadata":{"trusted":true},"cell_type":"code","source":"!mkdir train_images\n!mkdir test_images","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_spectrogram_file(fname, source, dest, dpi):\n    \n    path = source + fname\n    save_path  = dest + fname[:-4] + '.jpg'\n    \n    clip, sr = librosa.load(path, sr=44100)\n    \n    S1 = librosa.feature.melspectrogram(y=clip, sr=44100)\n    S2 = librosa.power_to_db(S1, ref=np.max)\n    \n    fig = plt.figure()\n    librosa.display.specshow(S2)   \n    plt.savefig(save_path, dpi=dpi, bbox_inches='tight',pad_inches=0)\n    \n    plt.cla()\n    plt.clf()\n    plt.close('all')\n    plt.close(fig)\n    del S1, S2, clip, fig\n    gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create training images"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nstart = 0\nend = 10000\nn = start\n\nsource = train_path\ndest = 'train_images/'\n\nfor f in train.fname.values[start:end]:\n    create_spectrogram_file(f, source, dest, 50)\n    n += 1\n    \n    if n % 50 == 0:\n        print(n, end=' ')\n    \n    if n % 1000 == 0:\n        gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Add New Columns to Training Set"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['image_file'] = train.fname.apply(lambda x : x[:-4] + '.jpg')\ntest['image_file'] = test.fname.apply(lambda x : x[:-4] + '.jpg')\n\nle = LabelEncoder()\nle.fit(train.label)\n\ntrain['enc_label'] = le.transform(train.label)\n#test['enc_label'] = le.transform(test.label)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Generators"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_datagen = ImageDataGenerator(rescale=1/255, validation_split=0.2)\n#test_datagen = ImageDataGenerator(rescale=1/255)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bs = 64\ntarget_size = (128, 128)\n\ntrain_generator = train_datagen.flow_from_dataframe(\n    dataframe = train,\n    directory = 'train_images/',\n    subset = \"training\",\n    x_col = 'image_file',\n    y_col = 'label',\n    batch_size = bs,\n    shuffle = True,\n    class_mode = 'categorical',\n    target_size = target_size)\n\nvalid_generator = train_datagen.flow_from_dataframe(\n    dataframe = train,\n    directory = 'train_images',\n    subset = \"validation\",\n    x_col = 'image_file',\n    y_col = 'label',\n    batch_size = bs,\n    seed = 1,\n    shuffle = False,\n    class_mode = 'categorical',\n    target_size = target_size)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tr_batches = len(train_generator)\nva_batches = len(valid_generator)\n\nprint(tr_batches)\nprint(va_batches)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Build CNN"},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(1)\n\ncnn = Sequential()\n\ncnn.add(Conv2D(16, (3,3), activation = 'relu', padding = 'same', input_shape=(128,128,3)))\ncnn.add(Conv2D(16, (3,3), activation = 'relu', padding = 'same'))\ncnn.add(MaxPooling2D(2,2))\ncnn.add(Dropout(0.25))\ncnn.add(BatchNormalization())\n\ncnn.add(Conv2D(32, (3,3), activation = 'relu', padding = 'same'))\ncnn.add(Conv2D(32, (3,3), activation = 'relu', padding = 'same'))\ncnn.add(MaxPooling2D(2,2))\ncnn.add(Dropout(0.5))\ncnn.add(BatchNormalization())\n\ncnn.add(Flatten())\ncnn.add(Dense(64, activation='relu'))\ncnn.add(Dropout(0.75))\ncnn.add(BatchNormalization())\n\ncnn.add(Dense(41, activation='softmax'))\n\ncnn.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train Network"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time \n\nopt = keras.optimizers.Adam(0.001)\ncnn.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n\nh1 = cnn.fit(train_generator, steps_per_epoch=tr_batches, epochs=20,\n                       validation_data=valid_generator, validation_steps=va_batches, \n                       verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"viztools02.vis_training([h1])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}