{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Exploratory Data Analysis Notebook","metadata":{}},{"cell_type":"markdown","source":"# EDA Description\n\nThe primary goal of this notebook is to simply analyze the Freesound_Audio training and test dataset. The mission is to summarize its key characteristics using statistical graphs and other data visualization methods.","metadata":{}},{"cell_type":"markdown","source":"# Importing Packages","metadata":{}},{"cell_type":"markdown","source":"In the code cells below, we are importing several different packages into the Kaggle notebook that will allow us to perform the necessary steps to acheive the best model. Few important packages that is needed for this project include; Librosa, IPython, and TensorFlow.","metadata":{}},{"cell_type":"code","source":"!pip install -U efficientnet -qq","metadata":{"execution":{"iopub.status.busy":"2022-04-28T09:40:55.357099Z","iopub.execute_input":"2022-04-28T09:40:55.35759Z","iopub.status.idle":"2022-04-28T09:41:07.171782Z","shell.execute_reply.started":"2022-04-28T09:40:55.357501Z","shell.execute_reply":"2022-04-28T09:41:07.17087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport math\nimport wave\n\nimport os\nimport cv2\n\nimport IPython.display as ipd \n\nimport librosa \nimport librosa.display\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\nfrom tensorflow.keras.models import Sequential, load_model\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras import backend as K\n\nimport efficientnet.tfkeras as efn\n\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' ","metadata":{"execution":{"iopub.status.busy":"2022-04-28T09:41:07.175305Z","iopub.execute_input":"2022-04-28T09:41:07.175516Z","iopub.status.idle":"2022-04-28T09:41:14.442695Z","shell.execute_reply.started":"2022-04-28T09:41:07.175489Z","shell.execute_reply":"2022-04-28T09:41:14.441902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading DataFrame\n\nIn this section, we are going to load the Freesound Audio training dataset into the Kaggle notebook. This will allow us to explore the data more properly. ","metadata":{}},{"cell_type":"code","source":"train_path = '../input/freesound-audio-tagging/audio_train/'\n\nprint(len(os.listdir(train_path)))","metadata":{"execution":{"iopub.status.busy":"2022-04-28T09:41:14.443938Z","iopub.execute_input":"2022-04-28T09:41:14.444188Z","iopub.status.idle":"2022-04-28T09:41:14.834313Z","shell.execute_reply.started":"2022-04-28T09:41:14.444155Z","shell.execute_reply":"2022-04-28T09:41:14.833518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"../input/freesound-audio-tagging/train.csv\")\n\nprint('The shape of the training data is: ', train.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-28T09:41:14.8365Z","iopub.execute_input":"2022-04-28T09:41:14.836968Z","iopub.status.idle":"2022-04-28T09:41:14.861695Z","shell.execute_reply.started":"2022-04-28T09:41:14.836929Z","shell.execute_reply":"2022-04-28T09:41:14.861002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the output displayed above, we can see that there are a total of 9473 total audio files located within the training dataset. The shape if the training data indicates 9473 by 3. The three are the number of columns within the dataset which in this case is are **fnames**, **labels**, and **manually_verifed**. We can further view that in the cell below.","metadata":{}},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-28T09:41:14.862876Z","iopub.execute_input":"2022-04-28T09:41:14.863115Z","iopub.status.idle":"2022-04-28T09:41:14.880288Z","shell.execute_reply.started":"2022-04-28T09:41:14.863082Z","shell.execute_reply":"2022-04-28T09:41:14.879619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After taking a moment to review the preview of the dataset, we can see that the fname column contains the individual audio files in a wav format. The label column contains the potential name of each of the given audio files. While the manually_verified column contains either a 0 or a 1 to each of the given files. The binary classification problem here tells us whether or not the label for the specified file is correct or not. The zero means that the label is not verified while the one means that the label is verified.","metadata":{}},{"cell_type":"markdown","source":"# Unique Labels","metadata":{}},{"cell_type":"code","source":"uniq_labels = train.label.unique()\nprint('There are a total of', len(uniq_labels), 'unique labels.\\n')\nprint(uniq_labels)","metadata":{"execution":{"iopub.status.busy":"2022-04-28T09:41:14.881625Z","iopub.execute_input":"2022-04-28T09:41:14.882279Z","iopub.status.idle":"2022-04-28T09:41:14.892586Z","shell.execute_reply.started":"2022-04-28T09:41:14.882245Z","shell.execute_reply":"2022-04-28T09:41:14.891686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After observing the unique label section above, we can see that there are a total of 41 unique labels within the Freesound Audio dataset. Several of which are musical instruments such as a trumpet, cello, and a flute. Some of which are animal noises such as a dog's bark, a cow's cowbell, and a cat's meow. While other noises are random real-world sounds like applause, knock, and keys_jangling. ","metadata":{}},{"cell_type":"markdown","source":"# Label Distribution","metadata":{}},{"cell_type":"markdown","source":"In this section, we will observe the total number of verified and not verified audio files. We will be using different data visualization methods to answer this.\n\nIn the first diagram below, we can see a bar chart was created to easily see the differences between the total number of verified (seen as 1) and non-verified (seen as 0) labels for all audio files. We can see that there are somewhere between 3500 to 3900 audio files where the labels are verified (seen in green). On the other hand, we can see that there are somewhere between 5600 to 5900 audio files where the labels are not verified (seen in blue).","metadata":{}},{"cell_type":"code","source":"train.manually_verified.value_counts().plot(kind='bar', xlabel='MGMT_value', ylabel='Count', \n                                     color=['#1E90FF', '#00C957'], edgecolor='black');","metadata":{"execution":{"iopub.status.busy":"2022-04-28T09:41:14.893999Z","iopub.execute_input":"2022-04-28T09:41:14.894251Z","iopub.status.idle":"2022-04-28T09:41:15.092542Z","shell.execute_reply.started":"2022-04-28T09:41:14.89422Z","shell.execute_reply":"2022-04-28T09:41:15.09187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the cells below, I created a pie chart to further see the differences between the verified and non-verified labels. This time we will go ahead and review the overall differences between the two through percentages. \n\nAs we can see in the pie chart below, there are about 39 percent of audio files with their correct labels (seen in blue). On the other hand, there are about 61 percent of audio files with labels that are not correctly verified (seen in red). ","metadata":{}},{"cell_type":"code","source":"print((train.manually_verified.value_counts() /len(train)).to_frame().T)","metadata":{"execution":{"iopub.status.busy":"2022-04-28T09:41:15.093656Z","iopub.execute_input":"2022-04-28T09:41:15.094984Z","iopub.status.idle":"2022-04-28T09:41:15.103893Z","shell.execute_reply.started":"2022-04-28T09:41:15.094943Z","shell.execute_reply":"2022-04-28T09:41:15.103018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels_count = train.manually_verified.value_counts()\n\nplt.pie(labels_count, labels=['Not Verified', 'Verified'], startangle=180, \n        autopct='%1.1f', colors=['#EE2C2C','#009ACD'], shadow=True)\n\nplt.figure(figsize=(16,16))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-28T09:41:15.105562Z","iopub.execute_input":"2022-04-28T09:41:15.105839Z","iopub.status.idle":"2022-04-28T09:41:15.221017Z","shell.execute_reply.started":"2022-04-28T09:41:15.105793Z","shell.execute_reply":"2022-04-28T09:41:15.22007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Audio Samples Per Category","metadata":{}},{"cell_type":"markdown","source":"Next, we will go ahead and create a graph that views the number of audio files per each category below. \n\nAfter reviewing the graph, we can see that there the minimum number of audio samples in a category is about 94 while the maximum is 300.\n\nWe can see here that there are actually several category that contains a maximum number of 300.\n\nThe orange area are the labels that are verified per audio file. While the blue area are the labels that are not verified per audio file. Even so here, we can see that there is a large number of audio files where the label is not verified.  ","metadata":{}},{"cell_type":"code","source":"category_group = train.groupby(['label', 'manually_verified']).count()\nplot = category_group.unstack().reindex(category_group.unstack().sum(axis=1).sort_values().index)\\\n          .plot(kind='bar', stacked=True, title=\"Number of Audio Samples per Category\", figsize=(16,10))\nplot.set_xlabel(\"Category\")\nplot.set_ylabel(\"Number of Samples\");","metadata":{"execution":{"iopub.status.busy":"2022-04-28T09:41:15.22499Z","iopub.execute_input":"2022-04-28T09:41:15.225691Z","iopub.status.idle":"2022-04-28T09:41:15.994801Z","shell.execute_reply.started":"2022-04-28T09:41:15.225641Z","shell.execute_reply":"2022-04-28T09:41:15.994114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploring Samples","metadata":{}},{"cell_type":"markdown","source":"In this section, I have provided a few random audio samples into this notebook. This will provide us with some examples of what some of these audio sound like and how it is meausured.\n\nIt is also worth noting that in this section, I will also share the process of how each audio file is then converted to what it is called a spectrogram. \n\nA spectrogram is a visual representation of the spectrum of frequencies of a signal as it varies with time. To simply put it, its pretty much another way of visually representing the strength of a signal over time at different frequencies, but in a format of an image. For the sake of this project, the idea is to transform this audio classification into a form of image classification. ","metadata":{}},{"cell_type":"markdown","source":"### Sample 1","metadata":{}},{"cell_type":"code","source":"gunshot = '../input/freesound-audio-tagging/audio_train/0048fd00.wav'\nipd.Audio(gunshot)","metadata":{"execution":{"iopub.status.busy":"2022-04-28T09:41:15.995876Z","iopub.execute_input":"2022-04-28T09:41:15.996232Z","iopub.status.idle":"2022-04-28T09:41:16.012665Z","shell.execute_reply.started":"2022-04-28T09:41:15.996185Z","shell.execute_reply":"2022-04-28T09:41:16.012053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"signal, sr = librosa.load(gunshot)\nprint(type(signal))\nprint(type(sr))","metadata":{"execution":{"iopub.status.busy":"2022-04-28T09:41:16.013588Z","iopub.execute_input":"2022-04-28T09:41:16.013844Z","iopub.status.idle":"2022-04-28T09:41:16.813117Z","shell.execute_reply.started":"2022-04-28T09:41:16.013796Z","shell.execute_reply":"2022-04-28T09:41:16.811559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(signal.shape)\nprint(sr)\nprint(len(signal) / sr)","metadata":{"execution":{"iopub.status.busy":"2022-04-28T09:41:16.814583Z","iopub.execute_input":"2022-04-28T09:41:16.814849Z","iopub.status.idle":"2022-04-28T09:41:16.819958Z","shell.execute_reply.started":"2022-04-28T09:41:16.814801Z","shell.execute_reply":"2022-04-28T09:41:16.819202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = [12,3])\nplt.subplot(2,1,1)\nplt.plot(signal)\nplt.subplot(2,1,2)\ninterval = range(2000, 3000)\nplt.plot(interval, signal[interval])\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-28T09:41:16.821304Z","iopub.execute_input":"2022-04-28T09:41:16.821792Z","iopub.status.idle":"2022-04-28T09:41:17.110437Z","shell.execute_reply.started":"2022-04-28T09:41:16.821756Z","shell.execute_reply":"2022-04-28T09:41:17.109759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x1 = librosa.feature.melspectrogram(y=signal, sr=22050)   \nx2 = librosa.power_to_db(x1, ref=np.max)   \n\nprint(x2.shape)\n\nlibrosa.display.specshow(x2, sr=22050, x_axis='time', y_axis='hz')\nplt.colorbar()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-28T09:41:17.111666Z","iopub.execute_input":"2022-04-28T09:41:17.112051Z","iopub.status.idle":"2022-04-28T09:41:17.38032Z","shell.execute_reply.started":"2022-04-28T09:41:17.112018Z","shell.execute_reply":"2022-04-28T09:41:17.379653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Sample 2","metadata":{}},{"cell_type":"code","source":"cello = '../input/freesound-audio-tagging/audio_train/0091fc7f.wav'\nipd.Audio(cello)","metadata":{"execution":{"iopub.status.busy":"2022-04-28T09:41:17.381686Z","iopub.execute_input":"2022-04-28T09:41:17.382159Z","iopub.status.idle":"2022-04-28T09:41:17.426258Z","shell.execute_reply.started":"2022-04-28T09:41:17.382122Z","shell.execute_reply":"2022-04-28T09:41:17.425679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"signal, sr = librosa.load(cello)\nprint(type(signal))\nprint(type(sr))","metadata":{"execution":{"iopub.status.busy":"2022-04-28T09:41:17.428447Z","iopub.execute_input":"2022-04-28T09:41:17.428686Z","iopub.status.idle":"2022-04-28T09:41:17.642333Z","shell.execute_reply.started":"2022-04-28T09:41:17.428656Z","shell.execute_reply":"2022-04-28T09:41:17.641528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(signal.shape)\nprint(sr)\nprint(len(signal) / sr)","metadata":{"execution":{"iopub.status.busy":"2022-04-28T09:41:17.643673Z","iopub.execute_input":"2022-04-28T09:41:17.644478Z","iopub.status.idle":"2022-04-28T09:41:17.649638Z","shell.execute_reply.started":"2022-04-28T09:41:17.644437Z","shell.execute_reply":"2022-04-28T09:41:17.648813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = [12,3])\nplt.subplot(2,1,1)\nplt.plot(signal)\nplt.subplot(2,1,2)\ninterval = range(2000, 3000)\nplt.plot(interval, signal[interval])\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-28T09:41:17.652052Z","iopub.execute_input":"2022-04-28T09:41:17.652539Z","iopub.status.idle":"2022-04-28T09:41:17.945362Z","shell.execute_reply.started":"2022-04-28T09:41:17.652504Z","shell.execute_reply":"2022-04-28T09:41:17.944687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x1 = librosa.feature.melspectrogram(y=signal, sr=22050)   \nx2 = librosa.power_to_db(x1, ref=np.max)   \n\nprint(x2.shape)\n\nlibrosa.display.specshow(x2, sr=22050, x_axis='time', y_axis='hz')\nplt.colorbar()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-28T09:41:17.946722Z","iopub.execute_input":"2022-04-28T09:41:17.946973Z","iopub.status.idle":"2022-04-28T09:41:18.255146Z","shell.execute_reply.started":"2022-04-28T09:41:17.94694Z","shell.execute_reply":"2022-04-28T09:41:18.25442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Label Encoder","metadata":{}},{"cell_type":"code","source":"labels = np.unique(train.label.values)\nlabel_encoder = {label:i for i, label in enumerate(labels)}\nprint(label_encoder['Cello'])\nprint(label_encoder['Gunshot_or_gunfire'])","metadata":{"execution":{"iopub.status.busy":"2022-04-28T09:41:18.256281Z","iopub.execute_input":"2022-04-28T09:41:18.25674Z","iopub.status.idle":"2022-04-28T09:41:18.272262Z","shell.execute_reply.started":"2022-04-28T09:41:18.256699Z","shell.execute_reply":"2022-04-28T09:41:18.271518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Displaying Multiple Spectrogram Images","metadata":{}},{"cell_type":"markdown","source":"To get an idea of what a spectrogam would look like for several audio labels. I decided to create a multi-sample images that display several random spectrogram images of random audio files each time it is ran. \n\nIf you notice, some audios below displays some signals in some form of pattern. Taken the knock for instance, we can see here that each time a knock occur within the audio the spectrogram would pick up that signal. The louder the noise is the lighter the colors get which is somewhat similar to a heat map. ","metadata":{}},{"cell_type":"code","source":"sample = train.sample(20)\n\nplt.figure(figsize=[20,9])\n\nfor i in range(20):\n    fname = train_path + sample.fname.iloc[i]\n    clip, sr = librosa.load(fname, sr=44100)\n    S1 = librosa.feature.melspectrogram(y=clip, sr=44100) \n    S2 = librosa.power_to_db(S1, ref=np.max)                \n    \n    plt.subplot(5, 4, i+1)\n    librosa.display.specshow(S2)\n    plt.title(f'{sample.label.iloc[i]} - {S2.shape[:2]} - {sample.fname.iloc[i]} ')\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-28T09:41:18.27335Z","iopub.execute_input":"2022-04-28T09:41:18.274073Z","iopub.status.idle":"2022-04-28T09:41:21.471874Z","shell.execute_reply.started":"2022-04-28T09:41:18.274038Z","shell.execute_reply":"2022-04-28T09:41:21.471238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Generator","metadata":{}},{"cell_type":"code","source":"SPEC_PATH = '../input/freesound-melpec-128-512-2sec/spectrograms'\nIMG_SIZE = (128,87)\n\nclass DataGenerator(keras.utils.Sequence):\n    \n    def __init__(self, df, batch_size=32, shuffle=True, is_train=True):\n        self.df = df\n        self.n = len(df)\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.is_train = is_train\n        self.on_epoch_end()\n        \n    def on_epoch_end(self):\n        self.indices = np.arange(self.n)\n        if self.shuffle == True:\n            np.random.shuffle(self.indices)   \n    \n    def __len__(self):\n        \n        return math.ceil( self.n / self.batch_size )\n    \n    def __getitem__(self, batch_index):\n        \n        start = batch_index * self.batch_size\n        end = (batch_index + 1) * self.batch_size\n        \n        indices = self.indices[start:end]\n        \n        return self.__data_generation(indices)\n    \n    def __data_generation(self, batch_indices):\n        batch_size = len(batch_indices)\n        \n        X = np.zeros(shape=(batch_size, IMG_SIZE[0], IMG_SIZE[1], 3))\n        y = np.zeros(batch_size)\n        \n        for i, idx in enumerate(batch_indices):\n            FILE = self.df.fname.values[idx]\n            LABEL = self.df.label.values[idx]\n            \n            SET = 'train_spec' if self.is_train else 'test_spec'\n            path = f'{SPEC_PATH}/{SET}/{FILE[:-4]}.npy'\n\n            try:\n                data_array = np.load(path)\n                resized = cv2.resize(data_array, (IMG_SIZE[1], IMG_SIZE[0]))\n                \n                for j in range(3):\n                    X[i,:,:,j] = resized \n                \n            except:\n                print('skipped')\n                \n            if self.is_train:\n                y[i] = label_encoder[LABEL]\n        if self.is_train:    \n            return X, y\n        return X\n    \nGENERATOR_TEST = True\n\nif GENERATOR_TEST:\n    temp_gen = DataGenerator(train, batch_size=8, shuffle=False)\n    X,y = temp_gen.__getitem__(0)\n\n    print(X.shape)\n    print(y)\n    \n    librosa.display.specshow(X[0, :, :, 0])","metadata":{"execution":{"iopub.status.busy":"2022-04-28T09:41:21.472931Z","iopub.execute_input":"2022-04-28T09:41:21.473264Z","iopub.status.idle":"2022-04-28T09:41:21.636901Z","shell.execute_reply.started":"2022-04-28T09:41:21.473233Z","shell.execute_reply":"2022-04-28T09:41:21.636067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df, valid_df = train_test_split(train, test_size=0.2, random_state=1, stratify=train.label)\n\nprint(train_df.shape)\nprint(valid_df.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-28T09:41:21.641465Z","iopub.execute_input":"2022-04-28T09:41:21.641751Z","iopub.status.idle":"2022-04-28T09:41:21.678239Z","shell.execute_reply.started":"2022-04-28T09:41:21.641714Z","shell.execute_reply":"2022-04-28T09:41:21.677476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader = DataGenerator(train_df, batch_size=64, shuffle=True)\nvalid_loader = DataGenerator(valid_df, batch_size=64, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-28T09:41:21.682667Z","iopub.execute_input":"2022-04-28T09:41:21.685048Z","iopub.status.idle":"2022-04-28T09:41:21.691938Z","shell.execute_reply.started":"2022-04-28T09:41:21.685003Z","shell.execute_reply":"2022-04-28T09:41:21.691139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TR_STEPS = len(train_loader)\nVA_STEPS = len(valid_loader)\n\nprint(TR_STEPS)\nprint(VA_STEPS)","metadata":{"execution":{"iopub.status.busy":"2022-04-28T09:41:21.697469Z","iopub.execute_input":"2022-04-28T09:41:21.699768Z","iopub.status.idle":"2022-04-28T09:41:21.71074Z","shell.execute_reply.started":"2022-04-28T09:41:21.699726Z","shell.execute_reply":"2022-04-28T09:41:21.709925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CNN Model","metadata":{}},{"cell_type":"code","source":"ENB1_model = efn.EfficientNetB1(input_shape=(128,87,3), include_top=False, weights='imagenet')\nENB1_model.trainable = True","metadata":{"execution":{"iopub.status.busy":"2022-04-28T09:41:21.715094Z","iopub.execute_input":"2022-04-28T09:41:21.71561Z","iopub.status.idle":"2022-04-28T09:41:27.321244Z","shell.execute_reply.started":"2022-04-28T09:41:21.715571Z","shell.execute_reply":"2022-04-28T09:41:27.320487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cnn = Sequential([\n    ENB1_model,\n    \n    Flatten(),\n    \n    Dense(64, activation='relu'),\n    Dropout(0.45),\n    \n    Dense(32, activation='relu'),\n    Dropout(0.45),\n    \n    Dense(41, activation='softmax')\n])\n\ncnn.summary()","metadata":{"execution":{"iopub.status.busy":"2022-04-28T09:41:27.325359Z","iopub.execute_input":"2022-04-28T09:41:27.325563Z","iopub.status.idle":"2022-04-28T09:41:28.079526Z","shell.execute_reply.started":"2022-04-28T09:41:27.325539Z","shell.execute_reply":"2022-04-28T09:41:28.078811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train Network","metadata":{}},{"cell_type":"markdown","source":"## Training Run 1","metadata":{}},{"cell_type":"code","source":"opt = tf.keras.optimizers.Adam(0.001)\ncnn.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2022-04-28T09:41:28.080612Z","iopub.execute_input":"2022-04-28T09:41:28.080874Z","iopub.status.idle":"2022-04-28T09:41:28.099851Z","shell.execute_reply.started":"2022-04-28T09:41:28.08082Z","shell.execute_reply":"2022-04-28T09:41:28.099011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time \n\nh1 = cnn.fit(train_loader, steps_per_epoch = TR_STEPS, epochs = 20, validation_data = valid_loader, \n             validation_steps = VA_STEPS, verbose = 1)","metadata":{"execution":{"iopub.status.busy":"2022-04-28T09:41:28.10101Z","iopub.execute_input":"2022-04-28T09:41:28.101264Z","iopub.status.idle":"2022-04-28T09:50:21.18575Z","shell.execute_reply.started":"2022-04-28T09:41:28.1012Z","shell.execute_reply":"2022-04-28T09:50:21.18503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def merge_history(hlist):\n    history = {}\n    for k in hlist[0].history.keys():\n        history[k] = sum([h.history[k] for h in hlist], [])\n    return history","metadata":{"execution":{"iopub.status.busy":"2022-04-28T09:50:21.187547Z","iopub.execute_input":"2022-04-28T09:50:21.187801Z","iopub.status.idle":"2022-04-28T09:50:21.194649Z","shell.execute_reply.started":"2022-04-28T09:50:21.187768Z","shell.execute_reply":"2022-04-28T09:50:21.193968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def vis_training(h, start=1):\n    epoch_range = range(start, len(h['loss'])+1)\n    s = slice(start-1, None)\n\n    plt.figure(figsize=[14,4])\n\n    n = int(len(h.keys()) / 2)\n\n    for i in range(n):\n        k = list(h.keys())[i]\n        plt.subplot(1,n,i+1)\n        plt.plot(epoch_range, h[k][s], label='Training')\n        plt.plot(epoch_range, h['val_' + k][s], label='Validation')\n        plt.xlabel('Epoch'); plt.ylabel(k); plt.title(k)\n        plt.grid()\n        plt.legend()\n\n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-28T09:50:21.195758Z","iopub.execute_input":"2022-04-28T09:50:21.196122Z","iopub.status.idle":"2022-04-28T09:50:21.205719Z","shell.execute_reply.started":"2022-04-28T09:50:21.196085Z","shell.execute_reply":"2022-04-28T09:50:21.204869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = merge_history([h1])\nvis_training(history)","metadata":{"execution":{"iopub.status.busy":"2022-04-28T09:50:21.207378Z","iopub.execute_input":"2022-04-28T09:50:21.207631Z","iopub.status.idle":"2022-04-28T09:50:21.573835Z","shell.execute_reply.started":"2022-04-28T09:50:21.207598Z","shell.execute_reply":"2022-04-28T09:50:21.573114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training Run 2","metadata":{}},{"cell_type":"code","source":"K.set_value(cnn.optimizer.learning_rate, 0.0001)","metadata":{"execution":{"iopub.status.busy":"2022-04-28T09:50:21.575064Z","iopub.execute_input":"2022-04-28T09:50:21.575387Z","iopub.status.idle":"2022-04-28T09:50:21.582521Z","shell.execute_reply.started":"2022-04-28T09:50:21.575349Z","shell.execute_reply":"2022-04-28T09:50:21.581728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time \n\nh2 = cnn.fit(train_loader, steps_per_epoch = TR_STEPS, epochs = 10, validation_data = valid_loader, \n             validation_steps = VA_STEPS, verbose = 1)","metadata":{"execution":{"iopub.status.busy":"2022-04-28T09:50:21.584144Z","iopub.execute_input":"2022-04-28T09:50:21.584844Z","iopub.status.idle":"2022-04-28T09:54:08.781764Z","shell.execute_reply.started":"2022-04-28T09:50:21.584788Z","shell.execute_reply":"2022-04-28T09:54:08.781087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def vis_training(h, start=1):\n    epoch_range = range(start, len(h['loss'])+1)\n    s = slice(start-1, None)\n\n    plt.figure(figsize=[14,4])\n\n    n = int(len(h.keys()) / 2)\n\n    for i in range(n):\n        k = list(h.keys())[i]\n        plt.subplot(1,n,i+1)\n        plt.plot(epoch_range, h[k][s], label='Training')\n        plt.plot(epoch_range, h['val_' + k][s], label='Validation')\n        plt.xlabel('Epoch'); plt.ylabel(k); plt.title(k)\n        plt.grid()\n        plt.legend()\n\n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-28T09:54:08.78468Z","iopub.execute_input":"2022-04-28T09:54:08.7849Z","iopub.status.idle":"2022-04-28T09:54:08.794528Z","shell.execute_reply.started":"2022-04-28T09:54:08.784875Z","shell.execute_reply":"2022-04-28T09:54:08.793799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = merge_history([h1, h2])\nvis_training(history, start=10)","metadata":{"execution":{"iopub.status.busy":"2022-04-28T09:54:08.795536Z","iopub.execute_input":"2022-04-28T09:54:08.795732Z","iopub.status.idle":"2022-04-28T09:54:09.174352Z","shell.execute_reply.started":"2022-04-28T09:54:08.795709Z","shell.execute_reply":"2022-04-28T09:54:09.173689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Save Model","metadata":{}},{"cell_type":"code","source":"cnn.save(f'Freesound_Audio_EfficientNet_B1_v01.h5')","metadata":{"execution":{"iopub.status.busy":"2022-04-28T09:54:09.175701Z","iopub.execute_input":"2022-04-28T09:54:09.175974Z","iopub.status.idle":"2022-04-28T09:54:10.075717Z","shell.execute_reply.started":"2022-04-28T09:54:09.17594Z","shell.execute_reply":"2022-04-28T09:54:10.074995Z"},"trusted":true},"execution_count":null,"outputs":[]}]}