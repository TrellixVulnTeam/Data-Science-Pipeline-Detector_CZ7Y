{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"51240721-fba1-e119-24d2-b3610b3dd4af"},"source":"## Imports and definitions"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e9d9727c-f58e-08a9-1089-f391de64c6bc"},"outputs":[],"source":"from multiprocessing import Pool, cpu_count\nfrom os.path import join\n\nimport numpy as np\nimport pandas as pd\nimport xgboost as xgb\nfrom sklearn.metrics import fbeta_score\n\nimport cv2\n\n# Definitions\nimage_format = 'jpg' # one of 'jpg' or 'tif'\nhistogram_size = 64\nxgb_tune_train_subsample = 0.35 # Percent of train used for tune (just to avoid Kaggle's limit)\ninput_folder = join('..', 'input')\ntrain_folder = join(input_folder, 'train-' + image_format)\ntest_folder = join(input_folder, 'test-' + image_format)"},{"cell_type":"markdown","metadata":{"_cell_guid":"65ed16a8-c9af-3f94-a95f-128b2edfb73d"},"source":"## Load data and apply one-hot encoding"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f9cc5785-2d57-baa7-6604-6cea27a9647a"},"outputs":[],"source":"# Load data\ndf_train = pd.read_csv(join(input_folder, 'train.csv'))\ndf_sub = pd.read_csv(join(input_folder, 'sample_submission.csv'))\n\n# One-hot encoding of 'tags'\ndf_train = pd.concat([df_train['image_name'], df_train.tags.str.get_dummies(sep=' ')], axis=1)\nall_labels = df_train.columns[1:].tolist()\n\ndf_train.sample(10, random_state=42)"},{"cell_type":"markdown","metadata":{"_cell_guid":"4f4f4c6a-14c7-d967-d636-2b793fe682c7"},"source":"## Util functions"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4bcd38d7-8439-75e1-3c4b-bbf3b601947c"},"outputs":[],"source":"def load_train_im(f):\n    return cv2.imread(join(train_folder, f + '.jpg'))\n\ndef load_test_im(f):\n    return cv2.imread(join(test_folder, f + '.jpg'))\n\ndef im2hist(im):\n    im_size = np.prod(im.shape[:2])\n\n    hist_b = cv2.calcHist([im], [0], None, [histogram_size], [0, 256]).ravel() / im_size\n    hist_g = cv2.calcHist([im], [1], None, [histogram_size], [0, 256]).ravel() / im_size\n    hist_r = cv2.calcHist([im], [2], None, [histogram_size], [0, 256]).ravel() / im_size\n\n    return np.r_[hist_b, hist_g, hist_r]\n\ndef train_file2hist(f):\n    return im2hist(load_train_im(f))\n\ndef test_file2hist(f):\n    return im2hist(load_test_im(f))\n\n# From here: https://www.kaggle.com/anokas/planet-understanding-the-amazon-from-space/fixed-f2-score-in-python/code\ndef f2_score(y_true, y_pred):\n    y_true, y_pred, = np.array(y_true), np.array(y_pred)\n    return fbeta_score(y_true, y_pred, beta=2, average='samples')"},{"cell_type":"markdown","metadata":{"_cell_guid":"9f51b08e-ab7f-8d60-31c0-a21a20b4b5b8"},"source":"## Histograms generation"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dcf06489-d6d3-01f6-fd88-73e49c985132"},"outputs":[],"source":"# For each sample, we create its histogram\npool = Pool(cpu_count())\ntry:\n    hist_train = pool.map(train_file2hist, df_train['image_name'])\n    X_train = np.array(hist_train)\n\n    hist_test = pool.map(test_file2hist, df_sub['image_name'])\n    X_test = np.array(hist_test)\nfinally:\n    pool.terminate()"},{"cell_type":"markdown","metadata":{"_cell_guid":"78401e86-7e54-9345-f231-13fe55de6493"},"source":"## XGB Hyperparameters"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a518d5d5-35f7-467d-d8c0-6ef557647f92"},"outputs":[],"source":"# XGB hyperparameters\nxgb_params = {\n    'objective': 'binary:logistic',\n    'eta': 0.3,\n    'max_depth': 5,\n    'subsample': 0.8,\n    'colsample_bytree': 0.8,\n    'silent': 1\n}"},{"cell_type":"markdown","metadata":{"_cell_guid":"0a8d9251-59fb-353c-43f5-006bf1e7d952"},"source":"## Hack to `xgb.cv`\n\nThis class emulates a callback to `xgb.cv` in order to access `dtest` and and `bst` (booster instance) of each fold.\n\nLatter, we use these instances to access the labels of the test and generate OOF (out-of-fold) predictions on it.\n\nThis hack will avoid a new CV train just to calculate the OOF predictions."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fd96107b-59bc-8507-0c33-246ded882332"},"outputs":[],"source":"class XGBCVHolder:\n    \"\"\"\n    This is a hack to XGBoost, which does not provide an API to access \n    the models trained over xgb.cv\n    \"\"\"\n    def __init__(self):\n        self.models = []\n        self.dtests = []\n        self.called = False\n\n    def __call__(self, env):\n        if not self.called:\n            self.called = True\n            for cvpack in env.cvfolds:\n                self.models.append(cvpack.bst)\n                self.dtests.append(cvpack.dtest)\n\n    def predict_oof(self, ntree_limit=0):\n        y = []\n        y_hat = []\n        for model, dtest in zip(self.models, self.dtests):\n            y.extend(dtest.get_label())\n            y_hat.extend(model.predict(dtest, ntree_limit=ntree_limit))\n\n        return np.array(y), np.array(y_hat)"},{"cell_type":"markdown","metadata":{"_cell_guid":"6912d5b8-2923-b045-3821-9e13bd2b3795"},"source":"## Tune of parameter `num_boost_round`\n\nThis step will generate a model for each label, tuning the best `num_boost_round` in each case."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"62b3976b-30f3-6579-b7f8-e038068c7ad3"},"outputs":[],"source":"# Generate dtest (submission)\ndtest = xgb.DMatrix(X_test)\ndf_sub['tags'] = ''\n\ntrain_subsample = np.random.choice(df_train.shape[0], int(df_train.shape[0] * xgb_tune_train_subsample))\n\n# Begin CV to find best num_boost_trees\ny_true, y_pred = [], []\nfor label in all_labels:\n    print(\"Tunning label '{}'...\".format(label))\n\n    # Perform cross-validation on train data\n    xgb_model = XGBCVHolder()\n    dtrain = xgb.DMatrix(X_train[train_subsample], df_train.iloc[train_subsample][label].values)\n    result = xgb.cv(xgb_params, dtrain, num_boost_round=200, nfold=2, metrics=['error', 'auc'],\n        early_stopping_rounds=20, verbose_eval=False, show_stdv=False, callbacks=[xgb_model])\n\n    print(result.iloc[-1][['test-auc-mean', 'test-error-mean']])\n\n    # Result of tunning\n    num_boost_round = result.shape[0]\n\n\n    # Get OOF predictions based on models trained on CV\n    y, y_hat = xgb_model.predict_oof(ntree_limit=num_boost_round)\n    y_true.append(y)\n    y_pred.append(y_hat)\n\n    # Train main model\n    print(\"Training model...\")\n    model = xgb.train(xgb_params, dtrain, num_boost_round=num_boost_round)\n    ytest_hat = model.predict(dtest) > 0.5\n\n    df_sub.loc[ytest_hat, 'tags'] = df_sub.loc[ytest_hat, 'tags'] + ' ' + label"},{"cell_type":"markdown","metadata":{"_cell_guid":"d729e4fa-2504-39c6-01ae-d9335f3584e2"},"source":"## Create `y_true` and `y_pred`, used for OOF f2 calculation"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4fe857e6-46e2-4342-67f9-11071c310ad6"},"outputs":[],"source":"# Join all OOF predictions and y_true\ny_true = np.c_[y_true].T\ny_pred = np.c_[y_pred].T\n\n# Calculate OOF f2 score\ncv_f2score = f2_score(y_true > 0.5, y_pred > 0.5)\nprint(\"Expected f2 in CV is %.4f\" % cv_f2score)"},{"cell_type":"markdown","metadata":{"_cell_guid":"7eba539f-6873-1cdd-b84d-c5e4a2838e41"},"source":"## Submission file"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"02b9cdc8-7f91-8b5f-c2f0-7a2b18f5f750"},"outputs":[],"source":"df_sub.to_csv('sub_%.4f.csv' % cv_f2score, index=False)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"633b9607-8294-8c57-9042-c342da79bc87"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}