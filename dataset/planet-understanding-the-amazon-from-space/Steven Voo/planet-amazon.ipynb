{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Multiple-label Classification with Planet Amazon Dataset\n\n`fastai` package will be used for this work."},{"metadata":{"trusted":true},"cell_type":"code","source":"%reload_ext autoreload\n%autoreload 2\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom fastai.vision import *","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = Path(\"../input/\")\npath.ls()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# look at the image-label file\ndf = pd.read_csv(path/\"train_v2.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The \"tags\" column consists of the labels for each training images. Each tags consists of multi-labels which describe the weather (clear, cloudy, partly_cloudy, haze, etc.) and the geographical substances (primary forest, agriculture, road, water, habitation, etc.) that can be found in each of the Amazon's satellite images."},{"metadata":{},"cell_type":"markdown","source":"## DataBunch Preparation\n\n\"[data block API](https://docs.fast.ai/data_block.html)\" procedure will be used to convert the raw images into model-readable DataBunch object."},{"metadata":{"trusted":true},"cell_type":"code","source":"# image transformation\ntfms = get_transforms(flip_vert = True, max_lighting = 0.1, max_zoom = 1.05, max_warp = 0.)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create ImageList\nnp.random.seed(7)\nsource = (ImageList.from_csv(path, \"train_v2.csv\", folder = \"train-jpg\", suffix = \".jpg\")\n         .split_by_rand_pct(0.2)\n         .label_from_df(cols = \"tags\", label_delim = \" \"))\n# label_delim to separate the words in \"tags\" column so as to generate multiple labels ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I will create data with rundown resolution first (128x128) so that I can fit the model faster and see how that model performs with lower quality images, as a baseline benchmark. (default image size provided by Kaggle is 256x256)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# data with size 128 (default 256)\ndata = (source.transform(tfms, size = 128)\n       .databunch()\n       .normalize(imagenet_stats))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# show the data\ndata.show_batch(rows = 4, figsize = (15,15))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What an interesting Amazon's satellite images!"},{"metadata":{},"cell_type":"markdown","source":"## Multiclassification\n\nPre-trained ResNet50 CNN will be used to fit the data. As for the metrics, `accuracy_thresh` will be used instead of `accuracy` because this is a multiclassification problem, the model should return multiple labels for each images as long as the probabilities of those labels are above a certain threshold. Apart from that, **F2-score** will also be used since it is the metric used by Kaggle in this competition. `fbeta` function from `fastai` will generates this metric."},{"metadata":{"trusted":true},"cell_type":"code","source":"# metrics\nacc_thresh = partial(accuracy_thresh, thresh = 0.2) # choose threshold = 0.2\nf2_score = partial(fbeta, beta = 2, thresh = 0.2) # fbeta where beta = 2 (F2) and threshold = 0.2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# download model\nlearn = cnn_learner(data, models.resnet50, metrics = [acc_thresh, f2_score], model_dir = \"/tmp/models\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# find good learning rate\nlearn.lr_find()\nlearn.recorder.plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The loss-line drops steepest around 0.01 (1e-02), so this will be used as the learning rate."},{"metadata":{"trusted":true},"cell_type":"code","source":"# baseline model with image size=128\nlearn.fit_one_cycle(cyc_len = 5, max_lr = slice(1e-2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This baseline model achieves an accuracy of 95.6% and F2-score of 0.924 which is pretty good."},{"metadata":{"trusted":true},"cell_type":"code","source":"# save this model\nlearn.save(\"baseline-rn50-128\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model Fine-tuning\n\nI will now fit the ResNet50 CNN with original image size, expect it to perform better."},{"metadata":{"trusted":true},"cell_type":"code","source":"# create 2nd data with original size\nnp.random.seed(7)\ndata2 = (source.transform(tfms, size = 256)\n        .databunch()\n        .normalize(imagenet_stats))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create another CNN model\nlearn2 = cnn_learner(data2, models.resnet50, metrics = [acc_thresh, f2_score], model_dir = \"/tmp/models\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot the learning rate of this model\nlearn2.lr_find()\nlearn2.recorder.plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Same as above, the loss-line drops steepest around 0.03 (3e-02) and I will use that as learning rate."},{"metadata":{"trusted":true},"cell_type":"code","source":"# baseline model with image size = 256\nlr = 3e-2\nlearn2.fit_one_cycle(cyc_len = 5, max_lr = slice(lr))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save baseline model for size=256\nlearn2.save(\"baseline-rn50-256\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot the learning rate\nlearn2.unfreeze()\nlearn2.lr_find()\nlearn2.recorder.plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"0.0001 (1e-04) seems to be a good turning point before the loss-line shoots up. I will use 0.0001/10 = 0.00001 as one of the learning rate."},{"metadata":{"trusted":true},"cell_type":"code","source":"# model 2 for image size = 256\nlearn2.fit_one_cycle(cyc_len = 10, max_lr = slice(1e-5, lr/10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot the training and validation loss of the model\nlearn2.recorder.plot_losses()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save the unfreezed model\nlearn2.save(\"stage-2-rn50-256\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The model is now ready for inference, `learn.export` is called to save all the information of the Learner object for inference: the stuff we need in the DataBunch (transforms, classes, normalization...), the model with its weights and all the callbacks the Learner was using."},{"metadata":{"trusted":true},"cell_type":"code","source":"# export the model\nlearn2.export(file = \"../working/export.pkl\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predictions and Submission\n\nThere are 2 set of images for testing provided by Kaggle, in the folders `test-jpg.tar.7z` and `test-jpg-additional.tar.7z`. All of these test images are compiled in the folder `test-jpg-v2` in the path. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#test = ImageList.from_folder(path/\"test-jpg\").add(ImageList.from_folder(path/\"test-jpg-additional\"))\ntest = ImageList.from_folder(path/\"test-jpg-v2\")\nlen(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"load_path = Path(\"../working/\")\n\nlearn = load_learner(load_path, test=test)\npredicts, _ = learn.get_preds(ds_type = DatasetType.Test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pick the labels as long as the probability is more than 0.2\nlabels_pred = [\" \".join([learn.data.classes[i] for i,p in enumerate(pred) if p > 0.2]) for pred in predicts]\n\nlabels_pred[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for img in learn.data.test_ds.items[:10]:\n    print(img.name)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pick up the images' names\nimage_names = [img.name[:-4] for img in learn.data.test_ds.items] # img.name[:-4] because I want to remove '.jpg' from the name","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create the dataframe of images' names and their tags (the format we have seen in train_v2.csv)\ndf2 = pd.DataFrame({\"image_name\":image_names, \"tags\":labels_pred})\ndf2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create the csv file for submission\ndf2.to_csv(\"submission.csv\", index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}