{"nbformat":4,"metadata":{"language_info":{"name":"python","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py","version":"3.6.1"},"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"}},"cells":[{"cell_type":"markdown","execution_state":"idle","outputs":[],"source":"### Open-CV and Tensor-flow","execution_count":null,"metadata":{"_uuid":"9f740373449ede3f6ceaab925373035affb7f24f","_cell_guid":"66141932-3234-99aa-7de3-93feabefb4b4"}},{"cell_type":"code","execution_state":"idle","outputs":[],"source":"import numpy as np \nimport pandas as pd \nimport seaborn as sn\nfrom tqdm import tqdm\nimport cv2 \nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom IPython.display import display\n%matplotlib inline\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))","execution_count":null,"metadata":{"_uuid":"4239639df2b484d824c253869e0202203e12597e","_execution_state":"idle","_cell_guid":"b946ed84-c02f-69d5-a9c1-cad5eb40ad8e"}},{"cell_type":"code","execution_state":"idle","outputs":[],"source":"train_labels = pd.read_csv('../input/train_v2.csv')\ndummies = train_labels['tags'].str.get_dummies(sep=' ')\nencoded_labels = pd.concat([train_labels, dummies], axis=1)\nencoded_labels.head()","execution_count":null,"metadata":{"_uuid":"375a6d0d6c06cb7f2a38be69b94943b5561608b7","_execution_state":"idle","_cell_guid":"6c433c75-8230-14d5-ba8c-586be7dd5402"}},{"source":"y = encoded_labels.mean()\nx = range(len(y))\nplt.figure(figsize=(25,20))\nplt.bar(x, y)\nplt.xticks(x, encoded_labels.columns[2:], fontsize= 30,  rotation=90)\nplt.yticks(fontsize=30)\nplt.show()","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_uuid":"8f34ad4dd5246fa30e655cd6133d783e5947156f","_execution_state":"idle","_cell_guid":"f32d7d17-87e4-4f9c-b9bc-b8dd40c14912","collapsed":false}},{"source":"%%time\nx_train_tif = []#np.array([])\n#x_train_tif = []\n\n#x_test = []#np.array([])\n\n\nfor idx, tags in tqdm(train_labels.values[:10000], miniters=1000):\n    #img_jpg = cv2.imread('../input/train-jpg/{}.jpg'.format(idx))\n    #x_train.append(cv2.resize(img_jpg, (32, 32)))\n    img_tif = cv2.imread('../input/train-tif-v2/{}.tif'.format(idx),-1)\n    x_train_tif.append(cv2.resize(img_tif,(32, 32)))\n    \n#for idx, tags in (train_labels.values[10000:20000]):\n#    img_jpg = cv2.imread('../input/train-jpg/{}.jpg'.format(idx))[:,:,1]\n#    x_test.append(cv2.resize(img_jpg, (64, 64)))\n    ","cell_type":"code","metadata":{"_uuid":"7129b9afd5826c86286547593d1b27c797140f95","_execution_state":"idle","collapsed":false,"_cell_guid":"de358204-cc9a-494e-9557-8bacf8d7a0c1"},"execution_count":null,"outputs":[]},{"source":"y_train = np.array(encoded_labels[encoded_labels.columns[-17:]][:10000])\ny_train = np.array(y_train, np.uint8)\nx_train = np.array(x_train_tif, np.float16) / 255.\n\nprint(x_train.shape)\nprint(y_train.shape)","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_uuid":"9f2cd19ab4d1e6a2b933226c2360411141b6b3c5","_execution_state":"busy","_cell_guid":"6953e759-1c42-4004-8130-1f2bd8cd3deb","collapsed":false}},{"source":"import keras as k\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\n\nsplit = 35000\nx_train, x_valid, y_train, y_valid = x_train[:split], x_train[split:], y_train[:split], y_train[split:]\nmodel = Sequential()\nmodel.add(Conv2D(10, kernel_size=(3, 3),\n                 activation='relu',\n                 input_shape=(32, 32, 4)))\nmodel.add(Conv2D(32, (6, 6), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.5))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(17, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy', # We NEED binary here, since categorical_crossentropy l1 norms the output before calculating loss.\n              optimizer='adam',\n              metrics=['accuracy'])    ","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_uuid":"9098a6f6dbf953f4d82c3d608203c92e687760d9","_execution_state":"idle","collapsed":false}},{"source":"from keras.preprocessing.image import ImageDataGenerator\n\ndatagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True)\n# fit parameters from data\ndatagen.fit(x_train)","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_uuid":"1c38e03de2ee6321053d83fb061aa5616d8aa5e7","_execution_state":"idle","collapsed":false}},{"source":"model.fit(x_train, y_train,\n          batch_size=100,\n          epochs=4,\n          verbose=1,\n          validation_data=(x_valid, y_valid))\n          \nfrom sklearn.metrics import fbeta_score\n\np_valid = model.predict(x_valid, batch_size=128)\nprint(y_valid)\nprint(p_valid)\nprint(fbeta_score(y_valid, np.array(p_valid) > 0.2, beta=2, average='samples'))","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_uuid":"5f45a79a16fa58fa2f0070580c6ad322230ad414","_execution_state":"idle","collapsed":false}},{"source":"batch_size = 16\n\n# this is the augmentation configuration we will use for training\ntrain_datagen = ImageDataGenerator(\n        rescale=1./255,\n        shear_range=0.2,\n        zoom_range=0.2,\n        horizontal_flip=True)\n\n# this is the augmentation configuration we will use for testing:\n# only rescaling\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\n# this is a generator that will read pictures found in\n# subfolers of 'data/train', and indefinitely generate\n# batches of augmented image data\ntrain_generator = train_datagen.flow_from_directory(\n        'data/train',  # this is the target directory\n        target_size=(150, 150),  # all images will be resized to 150x150\n        batch_size=batch_size,\n        class_mode='binary')  # since we use binary_crossentropy loss, we need binary labels\n\n# this is a similar generator, for validation data\nvalidation_generator = test_datagen.flow_from_directory(\n        'data/validation',\n        target_size=(150, 150),\n        batch_size=batch_size,\n        class_mode='binary')\nWe can now use these generators to train our model. Each epoch takes 20-30s on GPU and 300-400s on CPU. So it's definitely viable to run this model on CPU if you aren't in a hurry.\n\nmodel.fit_generator(\n        train_generator,\n        steps_per_epoch=2000 // batch_size,\n        epochs=50,\n        validation_data=validation_generator,\n        validation_steps=800 // batch_size)","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_uuid":"1d15da3f86da1a6c746d470b66ef1f0127be9505","_execution_state":"idle","collapsed":false}},{"source":"from __future__ import division\n\nimport six\nfrom keras.models import Model\nfrom keras.layers import (\n    Input,\n    Activation,\n    Dense,\n    Flatten\n)\nfrom keras.layers.convolutional import (\n    Conv2D,\n    MaxPooling2D,\n    AveragePooling2D\n)\nfrom keras.layers.merge import add\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.regularizers import l2\nfrom keras import backend as K","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_uuid":"4c6c3d9b0836896078f6354536eb9371672b357e","_execution_state":"idle","collapsed":false}},{"source":"def _bn_relu(input):\n    \"\"\"Helper to build a BN -> relu block\n    \"\"\"\n    norm = BatchNormalization(axis=CHANNEL_AXIS)(input)\n    return Activation(\"relu\")(norm)\n\n\ndef _conv_bn_relu(**conv_params):\n    \"\"\"Helper to build a conv -> BN -> relu block\n    \"\"\"\n    filters = conv_params[\"filters\"]\n    kernel_size = conv_params[\"kernel_size\"]\n    strides = conv_params.setdefault(\"strides\", (1, 1))\n    kernel_initializer = conv_params.setdefault(\"kernel_initializer\", \"he_normal\")\n    padding = conv_params.setdefault(\"padding\", \"same\")\n    kernel_regularizer = conv_params.setdefault(\"kernel_regularizer\", l2(1.e-4))\n\n    def f(input):\n        conv = Conv2D(filters=filters, kernel_size=kernel_size,\n                      strides=strides, padding=padding,\n                      kernel_initializer=kernel_initializer,\n                      kernel_regularizer=kernel_regularizer)(input)\n        return _bn_relu(conv)\n\n    return f\n\n\ndef _bn_relu_conv(**conv_params):\n    \"\"\"Helper to build a BN -> relu -> conv block.\n    This is an improved scheme proposed in http://arxiv.org/pdf/1603.05027v2.pdf\n    \"\"\"\n    filters = conv_params[\"filters\"]\n    kernel_size = conv_params[\"kernel_size\"]\n    strides = conv_params.setdefault(\"strides\", (1, 1))\n    kernel_initializer = conv_params.setdefault(\"kernel_initializer\", \"he_normal\")\n    padding = conv_params.setdefault(\"padding\", \"same\")\n    kernel_regularizer = conv_params.setdefault(\"kernel_regularizer\", l2(1.e-4))\n\n    def f(input):\n        activation = _bn_relu(input)\n        return Conv2D(filters=filters, kernel_size=kernel_size,\n                      strides=strides, padding=padding,\n                      kernel_initializer=kernel_initializer,\n                      kernel_regularizer=kernel_regularizer)(activation)\n\n    return f\n\n\ndef _shortcut(input, residual):\n    \"\"\"Adds a shortcut between input and residual block and merges them with \"sum\"\n    \"\"\"\n    # Expand channels of shortcut to match residual.\n    # Stride appropriately to match residual (width, height)\n    # Should be int if network architecture is correctly configured.\n    input_shape = K.int_shape(input)\n    residual_shape = K.int_shape(residual)\n    stride_width = int(round(input_shape[ROW_AXIS] / residual_shape[ROW_AXIS]))\n    stride_height = int(round(input_shape[COL_AXIS] / residual_shape[COL_AXIS]))\n    equal_channels = input_shape[CHANNEL_AXIS] == residual_shape[CHANNEL_AXIS]\n\n    shortcut = input\n    # 1 X 1 conv if shape is different. Else identity.\n    if stride_width > 1 or stride_height > 1 or not equal_channels:\n        shortcut = Conv2D(filters=residual_shape[CHANNEL_AXIS],\n                          kernel_size=(1, 1),\n                          strides=(stride_width, stride_height),\n                          padding=\"valid\",\n                          kernel_initializer=\"he_normal\",\n                          kernel_regularizer=l2(0.0001))(input)\n\n    return add([shortcut, residual])\n\n\ndef _residual_block(block_function, filters, repetitions, is_first_layer=False):\n    \"\"\"Builds a residual block with repeating bottleneck blocks.\n    \"\"\"\n    def f(input):\n        for i in range(repetitions):\n            init_strides = (1, 1)\n            if i == 0 and not is_first_layer:\n                init_strides = (2, 2)\n            input = block_function(filters=filters, init_strides=init_strides,\n                                   is_first_block_of_first_layer=(is_first_layer and i == 0))(input)\n        return input\n\n    return f\n\n\ndef basic_block(filters, init_strides=(1, 1), is_first_block_of_first_layer=False):\n    \"\"\"Basic 3 X 3 convolution blocks for use on resnets with layers <= 34.\n    Follows improved proposed scheme in http://arxiv.org/pdf/1603.05027v2.pdf\n    \"\"\"\n    def f(input):\n\n        if is_first_block_of_first_layer:\n            # don't repeat bn->relu since we just did bn->relu->maxpool\n            conv1 = Conv2D(filters=filters, kernel_size=(3, 3),\n                           strides=init_strides,\n                           padding=\"same\",\n                           kernel_initializer=\"he_normal\",\n                           kernel_regularizer=l2(1e-4))(input)\n        else:\n            conv1 = _bn_relu_conv(filters=filters, kernel_size=(3, 3),\n                                  strides=init_strides)(input)\n\n        residual = _bn_relu_conv(filters=filters, kernel_size=(3, 3))(conv1)\n        return _shortcut(input, residual)\n\n    return f\n\n\ndef bottleneck(filters, init_strides=(1, 1), is_first_block_of_first_layer=False):\n    \"\"\"Bottleneck architecture for > 34 layer resnet.\n    Follows improved proposed scheme in http://arxiv.org/pdf/1603.05027v2.pdf\n    Returns:\n        A final conv layer of filters * 4\n    \"\"\"\n    def f(input):\n\n        if is_first_block_of_first_layer:\n            # don't repeat bn->relu since we just did bn->relu->maxpool\n            conv_1_1 = Conv2D(filters=filters, kernel_size=(1, 1),\n                              strides=init_strides,\n                              padding=\"same\",\n                              kernel_initializer=\"he_normal\",\n                              kernel_regularizer=l2(1e-4))(input)\n        else:\n            conv_1_1 = _bn_relu_conv(filters=filters, kernel_size=(3, 3),\n                                     strides=init_strides)(input)\n\n        conv_3_3 = _bn_relu_conv(filters=filters, kernel_size=(3, 3))(conv_1_1)\n        residual = _bn_relu_conv(filters=filters * 4, kernel_size=(1, 1))(conv_3_3)\n        return _shortcut(input, residual)\n\n    return f\n\n\ndef _handle_dim_ordering():\n    global ROW_AXIS\n    global COL_AXIS\n    global CHANNEL_AXIS\n    if K.image_dim_ordering() == 'tf':\n        ROW_AXIS = 1\n        COL_AXIS = 2\n        CHANNEL_AXIS = 3\n    else:\n        CHANNEL_AXIS = 1\n        ROW_AXIS = 2\n        COL_AXIS = 3\n\n\ndef _get_block(identifier):\n    if isinstance(identifier, six.string_types):\n        res = globals().get(identifier)\n        if not res:\n            raise ValueError('Invalid {}'.format(identifier))\n        return res\n    return identifier","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_uuid":"8b73cec79f86f8f51e0339b0b58314eb37c8bd1d","_execution_state":"idle","collapsed":false}},{"source":"class ResnetBuilder(object):\n    @staticmethod\n    def build(input_shape, num_outputs, block_fn, repetitions):\n        \"\"\"Builds a custom ResNet like architecture.\n        Args:\n            input_shape: The input shape in the form (nb_channels, nb_rows, nb_cols)\n            num_outputs: The number of outputs at final softmax layer\n            block_fn: The block function to use. This is either `basic_block` or `bottleneck`.\n                The original paper used basic_block for layers < 50\n            repetitions: Number of repetitions of various block units.\n                At each block unit, the number of filters are doubled and the input size is halved\n        Returns:\n            The keras `Model`.\n        \"\"\"\n        _handle_dim_ordering()\n        if len(input_shape) != 3:\n            raise Exception(\"Input shape should be a tuple (nb_channels, nb_rows, nb_cols)\")\n\n        # Permute dimension order if necessary\n        if K.image_dim_ordering() == 'tf':\n            input_shape = (input_shape[1], input_shape[2], input_shape[0])\n\n        # Load function from str if needed.\n        block_fn = _get_block(block_fn)\n\n        input = Input(shape=input_shape)\n        conv1 = _conv_bn_relu(filters=64, kernel_size=(7, 7), strides=(2, 2))(input)\n        pool1 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding=\"same\")(conv1)\n\n        block = pool1\n        filters = 64\n        for i, r in enumerate(repetitions):\n            block = _residual_block(block_fn, filters=filters, repetitions=r, is_first_layer=(i == 0))(block)\n            filters *= 2\n\n        # Last activation\n        block = _bn_relu(block)\n\n        # Classifier block\n        block_shape = K.int_shape(block)\n        pool2 = AveragePooling2D(pool_size=(block_shape[ROW_AXIS], block_shape[COL_AXIS]),\n                                 strides=(1, 1))(block)\n        flatten1 = Flatten()(pool2)\n        dense = Dense(units=num_outputs, kernel_initializer=\"he_normal\",\n                      activation=\"softmax\")(flatten1)\n\n        model = Model(inputs=input, outputs=dense)\n        return model\n\n    @staticmethod\n    def build_resnet_18(input_shape, num_outputs):\n        return ResnetBuilder.build(input_shape, num_outputs, basic_block, [2, 2, 2, 2])\n\n    @staticmethod\n    def build_resnet_34(input_shape, num_outputs):\n        return ResnetBuilder.build(input_shape, num_outputs, basic_block, [3, 4, 6, 3])\n\n    @staticmethod\n    def build_resnet_50(input_shape, num_outputs):\n        return ResnetBuilder.build(input_shape, num_outputs, bottleneck, [3, 4, 6, 3])\n\n    @staticmethod\n    def build_resnet_101(input_shape, num_outputs):\n        return ResnetBuilder.build(input_shape, num_outputs, bottleneck, [3, 4, 23, 3])\n\n    @staticmethod\n    def build_resnet_152(input_shape, num_outputs):\n        return ResnetBuilder.build(input_shape, num_outputs, bottleneck, [3, 8, 36, 3])","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_uuid":"32ca6bb84898c5b1aaad0b1c62aeab795e8d513a","_execution_state":"idle","collapsed":false}},{"source":"resnet = ResnetBuilder.build_resnet_18([4,64,64], 17)\nresnet.compile(loss='binary_crossentropy', # We NEED binary here, since categorical_crossentropy l1 norms the output before calculating loss.\n              optimizer='adam',\n              metrics=['accuracy'])","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_uuid":"933f035583bcdaef68a02ef53fd5f9e9d7225af1","_execution_state":"idle","collapsed":false}},{"source":"split = 35000\nx_train, x_valid, y_train, y_valid = x_train[:split], x_train[split:], y_train[:split], y_train[split:]\n","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_uuid":"f5e84f660ea1bba53db948691e77b49e7b68330f","_execution_state":"idle","collapsed":false}},{"source":"resnet.fit(x_train, y_train,\n          batch_size=248,\n          epochs=1,\n          verbose=1,\n          validation_data=(x_valid, y_valid))\n          \nfrom sklearn.metrics import fbeta_score\n\np_valid = resnet.predict(x_valid, batch_size=128)\nprint(y_valid)\nprint(p_valid)\nprint(fbeta_score(y_valid, np.array(p_valid) > 0.2, beta=2, average='samples'))","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_uuid":"958444ca4ec183ff8ff662f2de8277b830803436","_execution_state":"idle","collapsed":false}},{"source":"%%time\nx_test = []\nfor idx, tags in (train_labels.values[20000:21000,:,:,1]):\n    img_jpg = cv2.imread('../input/train-jpg/{}.jpg'.format(idx))\n    x_test.append(cv2.resize(img_jpg, (64, 64)))\nx_test = np.array(x_test)\n","cell_type":"code","metadata":{"_uuid":"a82bbd4c7e7b572a83cd8b41c622f0a22d73b04d","_execution_state":"idle","collapsed":false,"_cell_guid":"9c8cf314-371d-4435-9b5c-a621bb499f8f"},"execution_count":null,"outputs":[]},{"source":"%%time\ny_test = np.array(encoded_labels.primary[20000:21000])","cell_type":"code","metadata":{"_uuid":"f082d6ed025ca0d5801ac42e8b601cd8b617d8ec","_execution_state":"idle","collapsed":false,"_cell_guid":"e344d9ae-ae0b-4923-b97a-f0322c88c637"},"execution_count":null,"outputs":[]},{"source":"x_train_jpg = np.array(x_train_jpg)\nx_test_jpg = np.array(x_test_jpg)\n\ny_primary_train = np.array(encoded_labels.primary[:10000])\ny_primary_test = np.array(encoded_labels.primary[10000:20000])\n\n#y_primary = encoded_labels.primary[:10000]\n","cell_type":"code","metadata":{"_uuid":"e61229b33d6f072ae71b24c9e034b917fc12cb2e","_execution_state":"idle","collapsed":false,"_cell_guid":"c401edb7-13ac-4cc5-ac20-fb3d3294a262"},"execution_count":null,"outputs":[]},{"source":"plt.hist(y_primary)","cell_type":"code","metadata":{"_uuid":"3bfc60863eef85b55138098e8d1d96a184265687","_execution_state":"idle","collapsed":false,"_cell_guid":"681d6dd5-b1e0-40c8-9330-bdb0bbf5f37b"},"execution_count":null,"outputs":[]},{"source":"data = np.fromfile('../input/train-jpg/',\ndtype=np.float32)\ndata.shape\n(60940800,)\ndata.reshape((50,1104,104))","cell_type":"code","metadata":{"_uuid":"8212f3d082cb87d9e0fe2d2b49673e96e51eaf62","_execution_state":"idle","_cell_guid":"46f3eb4f-3a23-45d5-b066-0f40a7a1217d","collapsed":false},"execution_count":null,"outputs":[]},{"source":"import mxnet as mx\nbatch_size = 100\ntrain_iter = mx.io.NDArrayIter(x_train_jpg, y_primary_train, batch_size, shuffle=True)\nval_iter = mx.io.NDArrayIter(x_test_jpg, y_primary_test, batch_size)","cell_type":"code","metadata":{"_uuid":"3af8fbc1c335c9e48ca69d81ebf0d11a373eeedd","_execution_state":"idle","collapsed":false,"_cell_guid":"a28daad3-456e-40d5-9303-7e2865ecb5b8"},"execution_count":null,"outputs":[]},{"source":"data = mx.sym.var('data')\n# first conv layer\nconv1 = mx.sym.Convolution(data=data, kernel=(5,5), num_filter=20)\ntanh1 = mx.sym.Activation(data=conv1, act_type=\"tanh\")\npool1 = mx.sym.Pooling(data=tanh1, pool_type=\"max\", kernel=(2,2), stride=(2,2))\n# second conv layer\nconv2 = mx.sym.Convolution(data=pool1, kernel=(5,5), num_filter=50)\ntanh2 = mx.sym.Activation(data=conv2, act_type=\"tanh\")\npool2 = mx.sym.Pooling(data=tanh2, pool_type=\"max\", kernel=(2,2), stride=(2,2))\n# first fullc layer\nflatten = mx.sym.flatten(data=pool2)\nfc1 = mx.symbol.FullyConnected(data=flatten, num_hidden=500)\ntanh3 = mx.sym.Activation(data=fc1, act_type=\"tanh\")\n# second fullc\nfc2 = mx.sym.FullyConnected(data=tanh3, num_hidden=1)\n# softmax loss\nlenet = mx.sym.SoftmaxOutput(data=fc2, name='softmax')","cell_type":"code","metadata":{"_uuid":"a6799d3338e5f4139cef8f9fc1bfbf1074a24683","_execution_state":"idle","_cell_guid":"3f004a9d-8c96-45d8-ac39-348e72eeaf55","collapsed":false},"execution_count":null,"outputs":[]},{"source":"a = train_iter.next()","cell_type":"code","metadata":{"_uuid":"38f4b2458d0a1d2765c07922978c99c9f2501de6","_execution_state":"idle","_cell_guid":"e1eb6c98-0b85-428c-83db-72dfed5f02bb","collapsed":false},"execution_count":null,"outputs":[]},{"source":"a.data","cell_type":"code","metadata":{"_uuid":"ae9ba00773e2ff7be3612b8637b7a37656440645","_execution_state":"idle","_cell_guid":"d4dfabf2-9aeb-4cd5-8911-535147ba911d","collapsed":false},"execution_count":null,"outputs":[]},{"source":"data = mx.sym.var('data')\n# Flatten the data from 4-D shape into 2-D (batch_size, num_channel*width*height)\ndata = mx.sym.flatten(data=data)\n# The first fully-connected layer and the corresponding activation function\nfc1  = mx.sym.FullyConnected(data=data, num_hidden=128)\nact1 = mx.sym.Activation(data=fc1, act_type=\"relu\")\n\n# The second fully-connected layer and the corresponding activation function\nfc2  = mx.sym.FullyConnected(data=act1, num_hidden = 64)\nact2 = mx.sym.Activation(data=fc2, act_type=\"relu\")\n\n# The second fully-connected layer and the corresponding activation function\nfc3  = mx.sym.FullyConnected(data=act1, num_hidden = 64)\nact3 = mx.sym.Activation(data=fc3, act_type=\"relu\")\n\nfc4  = mx.sym.FullyConnected(data=act3, num_hidden=1)\n# Softmax with cross entropy loss\nmlp  = mx.sym.SoftmaxOutput(data=fc3, name='softmax')","cell_type":"code","metadata":{"_uuid":"b3632d94601f94d068a67737d8aa12534e02a442","_execution_state":"idle","collapsed":false,"_cell_guid":"14ad17ff-42f0-4f9f-90f1-4dfc2b8bb9b9"},"execution_count":null,"outputs":[]},{"source":"import logging\nlogging.getLogger().setLevel(logging.DEBUG)  # logging to stdout\n# create a trainable module on CPU\nmlp_model = mx.mod.Module(symbol=lenet, context=mx.cpu())\nmlp_model.fit(train_iter,  # train data\n              eval_data=val_iter,  # validation data\n              optimizer='sgd',  # use SGD to train\n              optimizer_params={'learning_rate':0.1},  # use fixed learning rate\n              eval_metric='acc',  # report accuracy during training\n              batch_end_callback = mx.callback.Speedometer(batch_size, 100), # output progress for each 100 data batches\n              num_epoch=4)  # train for at most 10 dataset passes","cell_type":"code","metadata":{"_uuid":"78cd8364a771975278a272a92aa13e8af3ce7c8f","_execution_state":"idle","collapsed":false,"_cell_guid":"35e05edd-8681-4e62-a356-c9651e99984a"},"execution_count":null,"outputs":[]},{"source":"y_test.shape","cell_type":"code","metadata":{"_uuid":"5b84f955e6f4dbce2c13aae5b3a3782aa86a8a3b","_execution_state":"idle","collapsed":false,"_cell_guid":"9e5829dc-7203-4add-8403-16523f892b9b"},"execution_count":null,"outputs":[]},{"source":"test_iter = mx.io.NDArrayIter(x_test, y_test, batch_size)\n# predict accuracy of mlp\nacc = mx.metric.Accuracy()\nmlp_model.score(test_iter, acc)\nprint(acc)","cell_type":"code","metadata":{"_uuid":"41ed611aebec80bd99cedf3a20c43364a6379e9a","_execution_state":"idle","collapsed":false,"_cell_guid":"c6faaf98-6f2a-4b27-ac28-e8f0ad78c35d"},"execution_count":null,"outputs":[]},{"source":"","cell_type":"code","metadata":{"_uuid":"f31df82afbe717cd9f832b0668a443fe8edb2a45","_execution_state":"idle","collapsed":false,"_cell_guid":"3823e1ef-f2a5-41b7-9c50-b2a03f5cbaa8"},"execution_count":null,"outputs":[]},{"source":"x_train_tif[0].shape","cell_type":"code","metadata":{"_uuid":"e1991342a349a28265d225b3821be4ae8a64afd3","_execution_state":"idle","collapsed":false,"_cell_guid":"90c9959a-c9e7-480f-a87e-390efc9c2c49"},"execution_count":null,"outputs":[]},{"source":"img_jpg = cv2.imread('../input/train-jpg/train_1000.jpg')","cell_type":"code","metadata":{"_uuid":"8f9794500453265089669ba58ba8e2e9f3b67e55","_execution_state":"idle","collapsed":false,"_cell_guid":"08117381-ddf1-4838-87be-4bdf9032757a"},"execution_count":null,"outputs":[]},{"source":"img_jpg.shape","cell_type":"code","metadata":{"_uuid":"0d6f9f96452c46f214bf1e8765be07ed4826d473","_execution_state":"idle","collapsed":false,"_cell_guid":"71f936bd-98bd-4677-aee6-7b463e929850"},"execution_count":null,"outputs":[]},{"source":"img_tif = cv2.imread('../input/train-tif-v2/train_1000.tif', cv2.IMREAD_UNCHANGED)","cell_type":"code","metadata":{"_uuid":"2d0d2c8d46d384424b766405a583059e362f99d7","_execution_state":"idle","collapsed":false,"_cell_guid":"e8715863-de85-4abf-9afa-b243f25aff63"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_state":"idle","outputs":[],"source":"img.view()[0].shape","execution_count":null,"metadata":{"_uuid":"51e8853fa37f581f87c7b93d7b2f67dfddb81200","_execution_state":"idle","_cell_guid":"e7d13cf2-4185-5845-eac8-8616c2bcef85"}},{"cell_type":"code","execution_state":"idle","outputs":[],"source":"img_tif_to_bgr = cv2.cvtColor(img_jpg, cv2.COLOR_RGB2BGR)\nplt.imshow(img_tif[:,:,3])\nplt.axis('off')\n","execution_count":null,"metadata":{"_uuid":"43c948e403e02f0704edbab9dd5c938c76783ac4","_execution_state":"idle","_cell_guid":"7595de96-a273-acab-ed89-b01e4404a7d6"}},{"source":"img_bgr = cv2.cvtColor(img_jpg, cv2.COLOR_RGB2BGR)\nplt.imshow(img_bgr)\nplt.axis('off')","cell_type":"code","metadata":{"_uuid":"98f0173505b08629871dd18ba1115fedf9e69703","_execution_state":"idle","collapsed":false,"_cell_guid":"e3a3b61c-5ac2-4111-9998-16fe0246cef9"},"execution_count":null,"outputs":[]},{"source":"plt.imshow(img_jpg)\nplt.axis('off')","cell_type":"code","metadata":{"_uuid":"fd3abf1cf2b99243b485ba1f1d4649daa1248dae","_execution_state":"idle","collapsed":false,"_cell_guid":"caa9f498-a5a0-4cee-b824-2e89c10a03f1"},"execution_count":null,"outputs":[]},{"source":"img_blur = cv2.GaussianBlur(img_bgr,(5,5),0)\n#ret3,th3 = cv2.threshold(blur,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)","cell_type":"code","metadata":{"_uuid":"c53d0f22df9434332a73b03b465caa5ef3e8391f","_execution_state":"idle","collapsed":false,"_cell_guid":"b0c3c0d7-17ca-4289-8674-93f58d418221"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_state":"idle","outputs":[],"source":"titles = ['img_jpg','img_tif','img_bgr','img_blur']#,'TOZERO','TOZERO_INV']\nimages = [img_jpg,img_tif[:,:,3],img_bgr,img_blur]\n\nfor i in range(len(images)):\n    plt.figure(figsize=(20,10))\n    plt.subplot(2,3,i+1),plt.imshow(images[i])\n    plt.title(titles[i])\n    plt.axis('off')\n    #plt.xticks([]),plt.yticks([])\n\nplt.show()","execution_count":null,"metadata":{"_uuid":"56ac84228cad3127cd97f5b798d00081f9842ff1","_execution_state":"idle","_cell_guid":"dbedfb7a-4bb2-bfad-3e90-3078591e0097"}},{"source":"","cell_type":"code","metadata":{"_uuid":"50abff6e8ca31d79da7eec0778c54df3920ca6f3","_execution_state":"idle","collapsed":false,"_cell_guid":"50231008-4ad5-435f-9e22-71fa90397b0b"},"execution_count":null,"outputs":[]}],"nbformat_minor":0}