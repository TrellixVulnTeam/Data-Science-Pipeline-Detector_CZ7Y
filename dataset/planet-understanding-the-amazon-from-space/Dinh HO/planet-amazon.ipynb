{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"1fb55fff-f62e-b561-066d-1105c117ac11"},"source":"Learning the dataset"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d8f1f78b-612a-31d4-82e7-68d79863d186"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"494f2294-948a-61a1-61f6-3ef42ca08544"},"outputs":[],"source":"import sys\nimport os\nimport subprocess\nimport tensorflow as tf\n\n# Make sure you have all of these packages installed, e.g. via pip\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n#import rasterio\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport scipy\nfrom skimage import io\nfrom scipy import ndimage\nfrom IPython.display import display\n%matplotlib inline\n\n!ls -ha ../input"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"338f148f-1076-3848-4425-3e00d4128727"},"outputs":[],"source":"PLANET_KAGGLE_ROOT = os.path.abspath(\"../input/\")\nPLANET_KAGGLE_JPEG_DIR = os.path.join(PLANET_KAGGLE_ROOT, './train-jpg')\nPLANET_KAGGLE_LABEL_CSV = os.path.join(PLANET_KAGGLE_ROOT, './train.csv')\nassert os.path.exists(PLANET_KAGGLE_ROOT)\nassert os.path.exists(PLANET_KAGGLE_JPEG_DIR)\nassert os.path.exists(PLANET_KAGGLE_LABEL_CSV)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5aacd706-b4b9-29fd-ad84-d7fc49769151"},"outputs":[],"source":"labels_df = pd.read_csv(PLANET_KAGGLE_LABEL_CSV)\nlabels_df.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"16c02fb3-b45a-4e15-65ed-f4dcf726e998"},"outputs":[],"source":"# Build list with unique labels\nlabel_list = []\nfor tag_str in labels_df.tags.values:\n    labels = tag_str.split(' ')\n    for label in labels:\n        if label not in label_list:\n            label_list.append(label)\n           "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5fea2976-5512-b7ab-3ee7-9809c688f682"},"outputs":[],"source":"# Add onehot features for every label\nfor label in label_list:\n    labels_df[label] = labels_df['tags'].apply(lambda x: 1 if label in x.split(' ') else 0)\n# Display head\nlabels_df.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bade3bf1-5b4d-fda2-5964-0666817cc347"},"outputs":[],"source":"# Histogram of label instances\nlabels_df[label_list].sum().sort_values().plot.bar()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2c34997e-5eb9-63bb-faf0-ef73e2ce8e2f"},"outputs":[],"source":"def make_cooccurence_matrix(labels):\n    numeric_df = labels_df[labels]; \n    c_matrix = numeric_df.T.dot(numeric_df)\n    sns.heatmap(c_matrix)\n    return c_matrix\n    \n# Compute the co-ocurrence matrix\nmake_cooccurence_matrix(label_list)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1e84b8a6-cd78-be03-3466-16cda3308289"},"outputs":[],"source":"# Each image should have exactly one weather label:\nweather_labels = ['clear', 'partly_cloudy', 'haze', 'cloudy']\nmake_cooccurence_matrix(weather_labels)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"16957cdd-c64e-784e-55e7-be58c1c1c32d"},"outputs":[],"source":"# But the land labels may overlap:\nland_labels = ['primary', 'agriculture', 'water', 'cultivation', 'habitation']\nmake_cooccurence_matrix(land_labels)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1737b3f3-4bba-dcda-94e5-1244a49ba575"},"outputs":[],"source":"# The rarer labels have very little overlap:\nrare_labels = [l for l in label_list if labels_df[label_list].sum()[l] < 2000]\nmake_cooccurence_matrix(rare_labels)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ab7cd955-e663-843c-af71-3e5f80feb8c6"},"outputs":[],"source":"# Let's display an image and visualize the pixel values. Here we will pick an image, \n# load every single single band, then create RGB stack. These raw images are\n# 16-bit (from 0 to 65535), and contain red, green, blue, and Near infrared (NIR) channels.\n#In this example, we are discarding the NIR band just to simplify the steps to \n# visualize the image. However, you should probably keep it for ML classification.\n\n\nfrom six import string_types\n\ndef sample_images(tags, n=None):\n    \"\"\"Randomly sample n images with the specified tags.\"\"\"\n    condition = True\n    if isinstance(tags, string_types):\n        raise ValueError(\"Pass a list of tags, not a single tag.\")\n    for tag in tags:\n        condition = condition & labels_df[tag] == 1\n    if n is not None:\n        return labels_df[condition].sample(n)\n    else:\n        return labels_df[condition]  "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2997785e-e604-eb8f-7c2e-95da6b665b02"},"outputs":[],"source":"def find_image(filename):\n    \"\"\"Return a 4D (r, g, b, nir) numpy array with the data in the specified TIFF filename.\"\"\"\n    for dirname in os.listdir(PLANET_KAGGLE_ROOT):\n        path = os.path.abspath(os.path.join(PLANET_KAGGLE_ROOT, dirname, filename))\n        if os.path.exists(path):\n            print('Found image {}'.format(path))\n            return path\n        else:\n            print('Cannot find image at {}'.format(path))\n\ndef load_image(filename):\n    path = find_image(filename)\n    if path:\n        return io.imread(path)\n    else:\n        print('Load failed: could not find image {}'.format(path))\n        \ndef plot_rgbn_histo(r, g, b, n):\n    for slice_, name, color in ((r,'r', 'red'),(g,'g', 'green'),(b,'b', 'blue'), (nir, 'nir', 'magenta')):\n        plt.hist(slice_.ravel(), bins=100, \n                 range=[0,rgb_image.max()], \n                 label=name, color=color, histtype='step')\n    plt.legend()\n\ndef sample_to_fname(sample_df, row_idx, suffix='tif'):\n    '''Given a dataframe of sampled images, get the\n    corresponding filename.'''\n    fname = sample_df.get_value(sample_df.index[row_idx], 'image_name')\n    return '{}.{}'.format(fname, suffix)        "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"84f687dc-9236-316f-c4eb-27b9c7ec382b"},"outputs":[],"source":"# Let's look at an individual image. First, we'll plot a histogram of pixel values in each channel.\n# Note how the intensities are distributed in a relatively narrow region of the dynamic range\ns = sample_images(['primary', 'water', 'road'], n=1)\nimage_path = sample_to_fname(s, 0)\nrgbn_image = load_image(image_path)\nrgb_image = rgbn_image[:,:,:3]\nr, g, b, nir = rgbn_image[:, :, 0], rgbn_image[:, :, 1], rgbn_image[:, :, 2], rgbn_image[:, :, 3]\n\n# plot a histogram of rgbn values\nplot_rgbn_histo(r, g, b, nir)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"51235b23-5e4d-ed74-ca90-3c96d7641ce1"},"outputs":[],"source":"# We can look at each channel individually:\nfig = plt.figure()\nfig.set_size_inches(12, 4)\nfor i, (x, c) in enumerate(((r, 'r'), (g, 'g'), (b, 'b'), (nir, 'near-ir'))):\n    a = fig.add_subplot(1, 4, i+1)\n    a.set_title(c)\n    plt.imshow(x)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d9db47c0-9c57-2a44-544d-fe6a0ca58dfa"},"outputs":[],"source":"plt.imshow(rgb_image)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"950678f7-de2c-e7a0-82ec-455d14120b38"},"outputs":[],"source":"# Calibrate colors for visual inspection\n#  here we will employ the JPEG images provided in the data set,\n# which have already been color-corrected.\n# Get a list of reference images to extract data from:\n\n# Pull a list of 20000 image names\njpg_list = os.listdir(PLANET_KAGGLE_JPEG_DIR)[:20000]\n# Select a random sample of 100 among those\nnp.random.shuffle(jpg_list)\njpg_list = jpg_list[:100]\n\nprint(jpg_list)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5066fadf-c4af-bdb0-efa0-64d101a889f8"},"outputs":[],"source":"# Read each image (8-bit RGBA) and dump the pixels values to ref_colors,\n# which contains buckets for R, G and B\nref_colors = [[],[],[]]\nfor _file in jpg_list:\n    # keep only the first 3 bands, RGB\n    _img = mpimg.imread(os.path.join(PLANET_KAGGLE_JPEG_DIR, _file))[:,:,:3]\n    # Flatten 2-D to 1-D\n    _data = _img.reshape((-1,3))\n    # Dump pixel values to aggregation buckets\n    for i in range(3): \n        ref_colors[i] = ref_colors[i] + _data[:,i].tolist()\n    \nref_colors = np.array(ref_colors)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"eb249fd1-acf8-36c9-37ee-ac41a2bb28af"},"outputs":[],"source":"# Visualize the histogram of the reference data\nfor i,color in enumerate(['r','g','b']):\n    plt.hist(ref_colors[i], bins=30, range=[0,255], label=color, color=color, histtype='step')\nplt.legend()\nplt.title('Reference color histograms')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a12f0c99-c36f-f35f-502c-d64d0f102aed"},"outputs":[],"source":"# Compute the mean and variance for each channel in the reference data\nref_means = [np.mean(ref_colors[i]) for i in range(3)]\nref_stds = [np.std(ref_colors[i]) for i in range(3)]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"614c0c50-3540-3f57-42d7-47c4a5402259"},"outputs":[],"source":"# And now, we have a function that can calibrate any raw image reasonably well:\ndef calibrate_image(rgb_image):\n    # Transform test image to 32-bit floats to avoid \n    # surprises when doing arithmetic with it \n    calibrated_img = rgb_image.copy().astype('float32')\n\n    # Loop over RGB\n    for i in range(3):\n        # Subtract mean \n        calibrated_img[:,:,i] = calibrated_img[:,:,i]-np.mean(calibrated_img[:,:,i])\n        # Normalize variance\n        calibrated_img[:,:,i] = calibrated_img[:,:,i]/np.std(calibrated_img[:,:,i])\n        # Scale to reference \n        calibrated_img[:,:,i] = calibrated_img[:,:,i]*ref_stds[i] + ref_means[i]\n        # Clip any values going out of the valid range\n        calibrated_img[:,:,i] = np.clip(calibrated_img[:,:,i],0,255)\n\n    # Convert to 8-bit unsigned int\n    return calibrated_img.astype('uint8')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a980f334-bc59-0fed-39d2-40eefa94a59a"},"outputs":[],"source":"# Visualize the color histogram of the newly calibrated test image, and note that it's more\n# evenly distributed throughout the dynamic range, and is closer to the reference data.\ntest_image_calibrated = calibrate_image(rgb_image)\nfor i,color in enumerate(['r','g','b']):\n    plt.hist(test_image_calibrated[:,:,i].ravel(), bins=30, range=[0,255], \n             label=color, color=color, histtype='step')\nplt.legend()\nplt.title('Calibrated image color histograms')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"766ed748-64de-4af0-4112-96514f8d8db5"},"outputs":[],"source":"plt.imshow(test_image_calibrated)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8fa45fc2-9259-d213-6970-397be4e6ac98"},"outputs":[],"source":"#Putting it all together, to show several images with your tags of choice:\nsampled_images = sample_images(['clear', 'road', 'water'], n=1)\nfor i in range(len(sampled_images)):\n    tif = sample_to_fname(sampled_images, i, 'tif')\n    jpg = sample_to_fname(sampled_images, i, 'jpg')\n\n    try:\n        tif_img = load_image(tif)[:,:,:3]\n        jpg_img = load_image(jpg)[:,:,:3]\n\n        fig = plt.figure()\n        plt.imshow(calibrate_image(tif_img))\n\n        fig = plt.figure()\n        plt.imshow(calibrate_image(jpg_img))\n                \n    except:\n        continue"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ca60dfda-6b79-df7a-1fef-8a2ef9cf7dde"},"outputs":[],"source":"# Image clustering\n# For our purpose we will just use the pixel intensities and compute pairwise distances\n\nimport cv2\nfrom glob import glob\nimage_paths = sorted(glob('../input/train-jpg/*.jpg'))[0:1000]\nimage_names = list(map(lambda row: row.split(\"/\")[-1][:-4], image_paths))\n\nn_imgs = 600\n\nall_imgs = []\n\nfor i in range(n_imgs):\n    img = plt.imread(image_paths[i])\n    img = cv2.resize(img, (100, 100), cv2.INTER_LINEAR).astype('float')\n#    img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY).astype('float')\n    img = cv2.normalize(img, None, 0.0, 1.0, cv2.NORM_MINMAX)\n    img = img.reshape(1, -1)\n    all_imgs.append(img)\n\nimg_mat = np.vstack(all_imgs)\nimg_mat.shape\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"33f03fad-91a0-9207-191b-2bb1b9be8145"},"outputs":[],"source":"# We can see frmo the line spectrum in the clustermap, that there are a few images \n# that are very dissimilar to all other images by using the pixel intensities.\n# Also there is a block-like structure to it, maybe that already tells us\n#something about the tags themselves.\nfrom scipy.spatial.distance import pdist, squareform\n\nsq_dists = squareform(pdist(img_mat))\nprint(sq_dists.shape)\nsns.clustermap(\n    sq_dists,\n    figsize=(12,12),\n    cmap=plt.get_cmap('viridis')\n)\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5c0b161b-8ac0-6844-0ce1-c9e5e9ad0fb1"},"outputs":[],"source":"# Let's have a look at t-SNE embedding of the images to get a nice visualization\n# of the distances in three dimensions.\n%matplotlib inline\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\nfrom sklearn.manifold import TSNE\ntsne = TSNE(\n    n_components=3,\n    init='random', # pca\n    random_state=101,\n    method='barnes_hut',\n    n_iter=500,\n    verbose=2\n).fit_transform(img_mat)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3d0a628b-bd8f-e178-9d6a-740f710bfc97"},"outputs":[],"source":"trace1 = go.Scatter3d(\n    x=tsne[:,0],\n    y=tsne[:,1],\n    z=tsne[:,2],\n    mode='markers',\n    marker=dict(\n        sizemode='diameter',\n        #color = preprocessing.LabelEncoder().fit_transform(all_image_types),\n        #colorscale = 'Portland',\n        #colorbar = dict(title = 'images'),\n        line=dict(color='rgb(255, 255, 255)'),\n        opacity=0.9\n    )\n)\n\ndata=[trace1]\nlayout=dict(height=800, width=800, title='3D embedding of images')\nfig=dict(data=data, layout=layout)\npy.iplot(fig, filename='3DBubble')\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"4d617e73-94fb-4975-f929-ba94f827cdbc"},"source":"**Tracking down outliers**\n\nLet's try to find the most common image, as indicated by the average distance to all other images and the least common image by the same metric.\nAs we expected, the image that has least distance to all others is an image with rainforest on it. The image with the maximal average distance to all others shows only clouds and seems to be a little overexposed."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"aa03a972-edc2-023a-5318-48873f01f8bb"},"outputs":[],"source":"mask = np.zeros_like(sq_dists, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# upper triangle of matrix set to np.nan\nsq_dists[np.triu_indices_from(mask)] = np.nan\nsq_dists[0, 0] = np.nan\n\nfig = plt.figure(figsize=(12,8))\n# maximally dissimilar image\nax = fig.add_subplot(1,2,1)\nmaximally_dissimilar_image_idx = np.nanargmax(np.nanmean(sq_dists, axis=1))\nplt.imshow(plt.imread(image_paths[maximally_dissimilar_image_idx]))\nplt.title('maximally dissimilar')\n\n# maximally similar image\nax = fig.add_subplot(1,2,2)\nmaximally_similar_image_idx = np.nanargmin(np.nanmean(sq_dists, axis=1))\nplt.imshow(plt.imread(image_paths[maximally_similar_image_idx]))\nplt.title('maximally similar')\n\n# # now compute the mean image\n#ax = fig.add_subplot(1,3,3)\n#mean_img = gray_imgs_mat.mean(axis=0).reshape(rescaled_dim, rescaled_dim, 3)\n#plt.imshow(cv2.normalize(mean_img, None, 0.0, 1.0, cv2.NORM_MINMAX))\n#plt.title('mean image')"},{"cell_type":"markdown","metadata":{"_cell_guid":"c57cd81b-564c-59ba-b849-1cf431a6a5cd"},"source":"**Image scatter plot**\n\nThis scatter plot shows a 2D embedding but with the actual images overlaid."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"72b33403-33e6-bcce-eca0-f828f2a8edb0"},"outputs":[],"source":"from sklearn.manifold import TSNE\ntsne = TSNE(\n    n_components=2,\n    init='random', # pca\n    random_state=101,\n    method='barnes_hut',\n    n_iter=500,\n    verbose=2\n).fit_transform(img_mat)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a3f538af-681a-68b4-77c3-5d8ec12859e7"},"outputs":[],"source":"from matplotlib.offsetbox import OffsetImage, AnnotationBbox\ndef imscatter(x, y, images, ax=None, zoom=0.1):\n    ax = plt.gca()\n    images = [OffsetImage(image, zoom=zoom) for image in images]\n    artists = []\n    for x0, y0, im0 in zip(x, y, images):\n        ab = AnnotationBbox(im0, (x0, y0), xycoords='data', frameon=False)\n        artists.append(ax.add_artist(ab))\n    ax.update_datalim(np.column_stack([x, y]))\n    ax.autoscale()\n    #return artists\n\nnimgs = 500\nplt.figure(figsize=(13,10))\nimscatter(tsne[0:nimgs,0], tsne[0:nimgs,1], [plt.imread(image_paths[i]) for i in range(nimgs)])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c6c3cb69-e864-5efd-c075-69514e6568c7"},"outputs":[],"source":"# We're now going to compute the NDVI for a few images and rank them by it to see\n#how well we can identify the amount of vegetation in the image.\nimage_paths = sorted(glob('../input/train-tif/*.tif'))[0:1000]\nimgs = [io.imread(path) / io.imread(path).max() for path in image_paths]\n#r, g, b, nir = img[:, :, 0], img[:, :, 1], img[:, :, 2], img[:, :, 3]\nndvis = [(img[:,:,3] - img[:,:,0])/((img[:,:,3] + img[:,:,0])) for img in imgs]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"54c7e26d-6823-99c7-0def-acd115ef03a0"},"outputs":[],"source":"plt.figure(figsize=(12,8))\nplt.subplot(121)\nplt.imshow(ndvis[32], cmap='jet')\nplt.colorbar()\nplt.title('NDVI index of cloudy image')\nplt.subplot(122)\nplt.imshow(imgs[32])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7fb89446-2246-15a0-ecb2-0f0c82f52acc"},"outputs":[],"source":"plt.figure(figsize=(12,8))\nplt.subplot(121)\nplt.imshow(ndvis[800], cmap='jet')\nplt.colorbar()\nplt.title('NDVI index of image with lots of vegetation')\nplt.subplot(122)\nplt.imshow(imgs[800])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"273b63ee-aee6-109f-0669-84355e723305"},"outputs":[],"source":"mndvis = np.nan_to_num([ndvi.mean() for ndvi in ndvis])\nplt.figure(figsize=(12,8))\nsns.distplot(mndvis)\nplt.title('distribution of mean NDVIs')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"54a01f16-09be-9768-4eda-d086def401fd"},"outputs":[],"source":"sorted_idcs = np.argsort(mndvis)\nprint(len(sorted_idcs))\nplt.figure(figsize=(12,8))\nplt.subplot(221)\nplt.imshow(ndvis[sorted_idcs[0]], cmap='jet')\nplt.subplot(222)\nplt.imshow(ndvis[sorted_idcs[50]], cmap='jet')\nplt.subplot(223)\nplt.imshow(ndvis[sorted_idcs[-30]], cmap='jet')\nplt.subplot(224)\nplt.imshow(ndvis[sorted_idcs[-11]], cmap='jet')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"212b8b44-4f63-949d-8ddc-673d4ff6760c"},"outputs":[],"source":"import tensorflow as tf"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2b78ca73-b105-ea93-4566-83d52de7486c"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}