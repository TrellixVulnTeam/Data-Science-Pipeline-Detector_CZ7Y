{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Planet: Understanding the Amazon from Space\n\nContributors: Yuem Park\n\nThis notebook uses data from this closed Kaggle competition: https://www.kaggle.com/c/planet-understanding-the-amazon-from-space\n\nAs this was primarily a learning experience, much of the code was inspired by bits and pieces from the following notebooks:\n\n* https://www.kaggle.com/robinkraft/getting-started-with-the-data-now-with-docs\n* https://www.kaggle.com/anokas/data-exploration-analysis\n* https://github.com/EKami/planet-amazon-deforestation/blob/master/notebooks/amazon_forest_notebook_preview.ipynb\n* https://www.tensorflow.org/tutorials/load_data/images\n* https://www.tensorflow.org/tutorials/images/transfer_learning\n* https://www.tensorflow.org/guide/data\n* https://cs230-stanford.github.io/tensorflow-input-data.html"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm_notebook","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The input directory structure:"},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls -lha ../input/planet-understanding-the-amazon-from-space","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"File sizes:"},{"metadata":{"trusted":true},"cell_type":"code","source":"input_path = '../input/planet-understanding-the-amazon-from-space/'\n\nprint('File sizes:')\nprint('')\nfor f in os.listdir(input_path):\n    if not os.path.isdir(input_path + f):\n        print(f.ljust(30) + str(round(os.path.getsize(input_path + f) / 1000000, 2)) + 'MB')\n    else:\n        sizes = [os.path.getsize(input_path+f+'/'+x)/1000000 for x in os.listdir(input_path + f)]\n        print(f.ljust(30) + str(round(sum(sizes), 2)) + 'MB' + ' ({} files)'.format(len(sizes)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It appears as if there are 40,479 training images, with the jpg's being much smaller than the tif's."},{"metadata":{},"cell_type":"markdown","source":"## Training data table"},{"metadata":{},"cell_type":"markdown","source":"The training data table:"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(input_path + 'train_v2.csv')\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's binarize this to make it easier to deal with:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# build list with unique labels\nlabel_list = []\nfor tag_str in train_df['tags'].values:\n    labels = tag_str.split(' ')\n    for label in labels:\n        if label not in label_list:\n            label_list.append(label)\n            \nlabel_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# binarize the labels\nfor label in label_list:\n    train_df[label] = train_df['tags'].apply(lambda x : 1 if label in x.split(' ') else 0)\n    \ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"How many of each label are there?"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[label_list].sum().sort_values().plot.bar()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"How much overlap is there?"},{"metadata":{"trusted":true},"cell_type":"code","source":"# a function to quickly plot the co-occurences\ndef show_cooccurence_matrix(labels):\n    numeric_df = train_df[labels]\n    co_matrix = numeric_df.T.dot(numeric_df)\n    \n    fig, ax = plt.subplots(figsize=(7,7))\n    im = ax.imshow(co_matrix)\n    ax.set_xticks(np.arange(len(labels)))\n    ax.set_yticks(np.arange(len(labels)))\n    ax.set_xticklabels(labels, rotation=45, ha='right', rotation_mode='anchor')\n    ax.set_yticklabels(labels)\n    cbar = fig.colorbar(im, ax=ax) # not sure why this isn't working...\n    plt.show(fig)\n    \n# compute the co-ocurrence matrix\nshow_cooccurence_matrix(label_list)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Each image should have only one weather label:"},{"metadata":{"trusted":true},"cell_type":"code","source":"weather_labels = ['clear', 'partly_cloudy', 'haze', 'cloudy']\nshow_cooccurence_matrix(weather_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"But land labels can overlap:"},{"metadata":{"trusted":true},"cell_type":"code","source":"land_labels = ['primary', 'agriculture', 'water', 'cultivation', 'habitation']\nshow_cooccurence_matrix(land_labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Given this data format, a plan could be to train two models: the first is a single label classifier that will assign a single weather label to each image, and the second is a multi-label classifier that will assign at least one land label to each image."},{"metadata":{},"cell_type":"markdown","source":"## Training images"},{"metadata":{},"cell_type":"markdown","source":"First, inspect the jpg's:"},{"metadata":{"trusted":true},"cell_type":"code","source":"import cv2\n\n# read it in unchanged, to make sure we aren't losing any information\nimg = cv2.imread(input_path + 'train-jpg/{}.jpg'.format(train_df['image_name'][0]), cv2.IMREAD_UNCHANGED)\nnp.shape(img)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like RGB (but note that, from the example notebook, the order is BGR). Let's just plot a few:"},{"metadata":{"trusted":true},"cell_type":"code","source":"jpg_channels = ['B','G','R']\n\nfig, ax = plt.subplots(nrows=2, ncols=3, sharex=True, sharey=True, figsize=(20,12))\n\nax = ax.flatten()\n\nfor i in range(6):\n    img = cv2.imread(input_path + 'train-jpg/{}.jpg'.format(train_df['image_name'][i]), cv2.IMREAD_UNCHANGED)\n    ax[i].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n    ax[i].set_title('{} - {}'.format(train_df['image_name'][i], train_df['tags'][i]))\n    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What's the data type of the jpeg's?"},{"metadata":{"trusted":true},"cell_type":"code","source":"type(img[0,0,0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A histogram of values:"},{"metadata":{"trusted":true},"cell_type":"code","source":"jpg_channel_colors = ['b','g','r']\n\nfig, ax = plt.subplots()\n\nfor i in range(len(jpg_channels)):\n    ax.hist(img[:,:,i].flatten(), bins=100, range=[np.min(img), np.max(img)],\n            label=jpg_channels[i], color=jpg_channel_colors[i], alpha=0.5)\n    ax.legend()\n    \nax.set_xlim(0,255)\n    \nplt.show(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's look at the tiff's:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# read it in unchanged, to make sure we aren't losing any information\nimg = cv2.imread(input_path + 'train-tif-v2/{}.tif'.format(train_df['image_name'][0]), cv2.IMREAD_UNCHANGED)\nnp.shape(img)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This should be BGR and near infrared (NIR). Let's plot an image to see these 4 channels:"},{"metadata":{"trusted":true},"cell_type":"code","source":"tif_channels = ['B','G','R','NIR']\n\nfig, ax = plt.subplots(nrows=1, ncols=4, sharex=True, sharey=True, figsize=(20,15))\n\nax = ax.flatten()\n\nimg = cv2.imread(input_path + 'train-tif-v2/{}.tif'.format(train_df['image_name'][1]), cv2.IMREAD_UNCHANGED)\n\nfor i in range(len(tif_channels)):\n    ax[i].imshow(img[:,:,i])\n    ax[i].set_title('{} - {}'.format(train_df['image_name'][1], tif_channels[i]))\n    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What's the data type of the tiff's?"},{"metadata":{"trusted":true},"cell_type":"code","source":"type(img[0,0,0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A histogram of values:"},{"metadata":{"trusted":true},"cell_type":"code","source":"tif_channel_colors = ['b','g','r','magenta']\n\nfig, ax = plt.subplots()\n\nfor i in range(len(tif_channels)):\n    ax.hist(img[:,:,i].flatten(), bins=100, range=[np.min(img), np.max(img)],\n            label=tif_channels[i], color=tif_channel_colors[i], alpha=0.5)\n    ax.legend()\n    \nplt.show(fig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"A little unclear how to interpret this histogram... the \"getting started\" notebook posted by the competition hosts says this of a similar histogram:\n\n> Note how the intensities are distributed in a relatively narrow region of the dynamic range.\n\nNot sure what is meant by \"intensity\" or \"dynamic range\" - perhaps one way to think of it for now is that it's just something related to wavelength?"},{"metadata":{},"cell_type":"markdown","source":"## Transfer learning\n\nSome overview notes on what we're about to do...\n\nIn general, there are two types of transfer learning in the context of deep learning:\n\n* via feature extraction\n    * treating the network as an arbitrary feature extractor\n* via fine-tuning\n    * removing the fully-connected layers of an existing network, placing a new set of fully-connected layers on top of the network, and then fine-tuning these weights (and optionally previous layers) to recognize the new object classes\n    \n** Feature extraction **\n\nWhen treating networks as a feature extractor, we essentially 'chop off' the network at our pre-specified layer (typically prior to the fully-connected layers when actual classification predictions are made). We propagate some input through this 'shortened' network, get the output array, flatten it, and use that as the feature vector for the original input in another classification algorithm.\n\nThe two most common machine learning models for transfer learning via feature extraction are logistic regression and linear SVM. These are preferred because:\n\n* CNN's are non-linear models capable of learning non-linear features — we are assuming that the features learned by the CNN are already robust and discriminative.\n* Feature vectors tend to be very large and have high dimensionality. We therefore need a fast model that can be trained on top of the features. Linear models tend to be very fast to train."},{"metadata":{},"cell_type":"markdown","source":"### Data preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\ntf.__version__","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We could use `tf.keras.preprocessing` to load the images, but it has 3 downsides:\n\n1. it's slow\n2. it lacks fine-grained control\n3. it's not well integrated with the rest of TensorFlow\n\nAlternatively, we could use `tf.data.Dataset`. From the documentation:\n\n> The `tf.data` API enables you to build complex input pipelines from simple, reusable pieces. For example, the pipeline for an image model might aggregate data from files in a distributed file system, apply random perturbations to each image, and merge randomly selected images into a batch for training.\n\nThis API allows the user to only call the data (and apply transformations, etc.) when the data is actually needed, instead of keeping all the images in RAM indefinitely.\n\nSome notes when trying to implement this pipeline:\n\n* `decode_tiff` is currently not implemented in TensorFlow - therefore we need to wrap `cv2.imread()`\n* although `decode_jpeg` is implemented in TensorFlow, the function was unable to correctly extract the information stored in the jpeg's used in this competition, and just returned a matrix of zeros. Some digging suggested that jpeg's stored on Kaggle's servers use a different encoding to that which is expected by `decode_jpeg`."},{"metadata":{},"cell_type":"markdown","source":"First, get a list of the paths to the images. We'll use the jpeg's because the neural network that we will be using (VGG16) expects 3 channels:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# extract image names from the .csv with the labels\npath_to_images = train_df['image_name'].copy().values\n\n# convert to path\nfor i in range(len(path_to_images)):\n    path_to_images[i] = input_path + 'train-jpg/' + path_to_images[i] + '.jpg'\n\npath_to_images[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then get the labels:"},{"metadata":{"trusted":true},"cell_type":"code","source":"weather_labels_array = train_df[weather_labels].copy().values.astype(bool)\n\nweather_labels_array[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[weather_labels].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Merge the two into a Dataset:"},{"metadata":{"trusted":true},"cell_type":"code","source":"weather_ds = tf.data.Dataset.from_tensor_slices((path_to_images, weather_labels_array))\n\n# note that the `numpy()` function is required to grab the actual values from the Dataset\nfor path, label in weather_ds.take(5):\n    print(\"path  : \", path.numpy().decode('utf-8'))\n    print(\"label : \", label.numpy())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Define functions to get the actual images from the paths, and map these onto the Dataset:\n\nNote that this is where the most issues were experienced. The code here, which seems simple enough, will fail with an opaque error message even if the slightest thing is done incorrectly. Take note of the comments carefully..."},{"metadata":{"trusted":true},"cell_type":"code","source":"# this function wraps `cv2.imread` - we treat it as a 'standalone' function, and therefore can use\n# eager execution (i.e. the use of `numpy()`) to get a string of the path.\n# note that no tensorflow functions are used here\ndef cv2_imread(path, label):\n    # read in the image, getting the string of the path via eager execution\n    img = cv2.imread(path.numpy().decode('utf-8'), cv2.IMREAD_UNCHANGED)\n    # change from BGR to RGB\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    return img, label\n\n# this function assumes that the image has been read in, and does some transformations on it\n# note that only tensorflow functions are used here\ndef tf_cleanup(img, label):\n    # convert to Tensor\n    img = tf.convert_to_tensor(img)\n    # unclear why, but the jpeg is read in as uint16 - convert to uint8\n    img = tf.dtypes.cast(img, tf.uint8)\n    # set the shape of the Tensor\n    img.set_shape((256, 256, 3))\n    # convert to float32, scaling from uint8 (0-255) to float32 (0-1)\n    img = tf.image.convert_image_dtype(img, tf.float32)\n    # resize the image\n    img = tf.image.resize(img, [256, 256])\n    return img, label\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE\n\n# map the cv2 wrapper function using `tf.py_function`\nweather_ds = weather_ds.map(lambda path, label: tuple(tf.py_function(cv2_imread, [path, label], [tf.uint16, label.dtype])),\n                            num_parallel_calls=AUTOTUNE)\n\n# map the TensorFlow transformation function - no need to wrap\nweather_ds = weather_ds.map(tf_cleanup, num_parallel_calls=AUTOTUNE)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check to make sure everything was read in correctly:"},{"metadata":{"trusted":true},"cell_type":"code","source":"for image, label in weather_ds.take(1):\n    print(\"image shape : \", image.numpy().shape)\n    print(\"label       : \", label.numpy())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(nrows=3, ncols=4, sharex=True, sharey=True, figsize=(20,15))\n\ni = 0\n\nfor image, label in weather_ds.take(3):\n    ax[i,0].imshow(image[:,:,0])\n    ax[i,0].set_title('{} - {}'.format(label.numpy(), 'R'))\n    ax[i,1].imshow(image[:,:,1])\n    ax[i,1].set_title('{} - {}'.format(label.numpy(), 'G'))\n    ax[i,2].imshow(image[:,:,2])\n    ax[i,2].set_title('{} - {}'.format(label.numpy(), 'B'))\n    ax[i,3].imshow(image)\n    ax[i,3].set_title('{} - {}'.format(label.numpy(), 'RGB'))\n    \n    i = i+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(nrows=3, ncols=3, sharex=True, sharey=True, figsize=(20,15))\n\nax[0,0].set_xlim(0,1)\n\ni = 0\n\nfor image, label in weather_ds.take(3):\n    ax[i,0].hist(image[:,:,0].numpy().flatten())\n    ax[i,0].set_title('{} - {}'.format(label.numpy(), 'R'))\n    ax[i,1].hist(image[:,:,1].numpy().flatten())\n    ax[i,1].set_title('{} - {}'.format(label.numpy(), 'G'))\n    ax[i,2].hist(image[:,:,2].numpy().flatten())\n    ax[i,2].set_title('{} - {}'.format(label.numpy(), 'B'))\n    \n    i = i+1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Split into training and validation. Note that the buffer size for shuffling defines how random the Dataset becomes - a buffer size that's equal to the number of instances will result in a uniform shuffling over the entire Dataset, and a buffer size equal to 1 will result in no shuffling."},{"metadata":{"trusted":true},"cell_type":"code","source":"n_all = len(path_to_images)\nn_train = int(n_all * 0.8)\nn_val = n_all - n_train\n\n# shuffle the Dataset\nSHUFFLE_BUFFER_SIZE = 1000\nweather_ds = weather_ds.shuffle(SHUFFLE_BUFFER_SIZE)\n\n# n_train will be used for training, the rest will be used for validation\ntrain_weather_ds = weather_ds.take(n_train)\nval_weather_ds = weather_ds.skip(n_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Batch the data. It is unclear, but using a batch size of 32 seems like a common practice, and using powers of 2 is preferred when using a GPU."},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 32\n\ntrain_weather_batches_ds = train_weather_ds.batch(BATCH_SIZE)\nval_weather_batches_ds = val_weather_ds.batch(BATCH_SIZE)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Inspect a batch of data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"for image_batch, label_batch in train_weather_batches_ds.take(1):\n    print(image_batch.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model setup"},{"metadata":{"trusted":true},"cell_type":"code","source":"IMG_SHAPE = (256, 256, 3)\n\n# create the base model from the pre-trained model VGG16\n# note that, if using a Kaggle server, internet has to be turned on\nweather_VGG16 = tf.keras.applications.VGG16(input_shape=IMG_SHAPE,\n                                            include_top=False,\n                                            weights='imagenet')\n\n# freeze the convolutional base\nweather_VGG16.trainable = False\n\n# look at the model architecture\nweather_VGG16.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}