{"cells":[{"outputs":[],"source":"This is based on our official notebook - not something I can take credit for :)\nhttps://github.com/planetlabs/planet-amazon-deforestation/blob/master/planet_chip_examples.ipynb\n\n# *Planet: Understanding the Amazon from Space* challenge\n\nThis notebook will show you how to do some basic manipulation of the images and label files.","cell_type":"markdown","execution_count":null,"metadata":{"_uuid":"75def13d5794b388aa691a3abae107fd2048f6de","_cell_guid":"f37a9596-11de-4330-6e6d-2fda24efdba3"}},{"outputs":[],"source":"import sys\nimport os\nimport subprocess\n\nfrom six import string_types\n\n# Make sure you have all of these packages installed, e.g. via pip\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport scipy\nfrom skimage import io\nfrom scipy import ndimage\nfrom IPython.display import display\n%matplotlib inline","cell_type":"code","execution_count":null,"metadata":{"_uuid":"6d44514dbc41286cfeae0b48ce8e70b126b9efdf","_cell_guid":"9018e828-1983-15e2-6566-b93fa759ca04","trusted":false}},{"outputs":[],"source":"!ls -lha ../input","cell_type":"code","execution_count":null,"metadata":{"_uuid":"d8ed81558caf1d55194d1658b0ffaf8b1cb8aead","_cell_guid":"6e51259c-c390-6751-a321-2ede2b312a7c","trusted":false}},{"outputs":[],"source":"!ls -lha ../input/test-tif-v2 | wc -l","cell_type":"code","execution_count":null,"metadata":{"_uuid":"25e50e7d9cbdaa8f5204d58416fd7408ab1adad6","_cell_guid":"a4f4f743-8c83-28b6-b759-1f85d91bd4f9","trusted":false}},{"outputs":[],"source":"## Setup\nSet `PLANET_KAGGLE_ROOT` to the proper directory where we've got the TIFF and JPEG zip files, and accompanying CSVs.","cell_type":"markdown","execution_count":null,"metadata":{"_uuid":"a25b7eb35ab59dde4a42d490bbeff5faa705ef79","_cell_guid":"de648e34-cf96-3c7d-37ad-59bc2b8360f7"}},{"outputs":[],"source":"PLANET_KAGGLE_ROOT = os.path.abspath(\"../input/\")\nPLANET_KAGGLE_JPEG_DIR = os.path.join(PLANET_KAGGLE_ROOT, 'train-jpg')\nPLANET_KAGGLE_LABEL_CSV = os.path.join(PLANET_KAGGLE_ROOT, 'train_v2.csv')\nassert os.path.exists(PLANET_KAGGLE_ROOT)\nassert os.path.exists(PLANET_KAGGLE_JPEG_DIR)\nassert os.path.exists(PLANET_KAGGLE_LABEL_CSV)","cell_type":"code","execution_count":null,"metadata":{"_uuid":"46330358f73b8509d2b7ffccc12ecbeb79ab899e","_cell_guid":"bace665c-f34b-09ff-7808-9efa412c65ca","trusted":false}},{"outputs":[],"source":"## Inspect image labels\nThe labels are in a CSV entitled `train.csv`. Note that each image can be tagged with multiple tags. We'll convert them to a \"one hot\" style representation where each label is a column:","cell_type":"markdown","execution_count":null,"metadata":{"_uuid":"92f228543b19f6184c3faf2b5a5e60e790666e75","_cell_guid":"3468c4df-3507-4650-e1ef-33348b7507ec"}},{"outputs":[],"source":"!ls -lha /kaggle/input/train_v2.csv","cell_type":"code","execution_count":null,"metadata":{"_uuid":"62a491f46924c3150376cb00b11a3be346ef9125","_cell_guid":"d0c7574c-940e-7d28-e0f3-4f333a83300a","trusted":false}},{"outputs":[],"source":"labels_df = pd.read_csv(PLANET_KAGGLE_LABEL_CSV)\nlabels_df.head()","cell_type":"code","execution_count":null,"metadata":{"_uuid":"46ace61963731d327faca232661d7afa7c28ddf2","_cell_guid":"6a90f711-8222-e7b0-9635-a00456ed64b0","trusted":false}},{"outputs":[],"source":"# Build list with unique labels\nlabel_list = []\nfor tag_str in labels_df.tags.values:\n    labels = tag_str.split(' ')\n    for label in labels:\n        if label not in label_list:\n            label_list.append(label)","cell_type":"code","execution_count":null,"metadata":{"_uuid":"dfa3766a305377dedc63abb8069751b7c60386e2","_cell_guid":"f556c7f8-5e52-aab8-ca43-f5184a85836e","trusted":false}},{"outputs":[],"source":"# Add onehot features for every label\nfor label in label_list:\n    labels_df[label] = labels_df['tags'].apply(lambda x: 1 if label in x.split(' ') else 0)\n# Display head\nlabels_df.head()","cell_type":"code","execution_count":null,"metadata":{"_uuid":"594c3b580181023826f9e6d431c727d61859d2b9","_cell_guid":"e9f1be5a-b833-e0c3-bbf3-a95a72d02335","trusted":false}},{"outputs":[],"source":"# Histogram of label instances\nlabels_df[label_list].sum().sort_values().plot.bar()","cell_type":"code","execution_count":null,"metadata":{"_uuid":"81e92e4ec80074284ab5c7d38a7b90b4f16ade5a","_cell_guid":"918adbb3-fd30-0d4d-6da2-12b49c6fa3bd","trusted":false}},{"outputs":[],"source":"def make_cooccurence_matrix(labels):\n    numeric_df = labels_df[labels]; \n    c_matrix = numeric_df.T.dot(numeric_df)\n    sns.heatmap(c_matrix)\n    return c_matrix\n    \n# Compute the co-ocurrence matrix\nmake_cooccurence_matrix(label_list)","cell_type":"code","execution_count":null,"metadata":{"_uuid":"b71afd441ffb87288bff1e076cd5bced19a0b826","_cell_guid":"345bfce6-c8d5-1c83-4e69-c2ba2ddf256c","trusted":false}},{"outputs":[],"source":"Each image should have exactly one weather label:","cell_type":"markdown","execution_count":null,"metadata":{"_uuid":"01576e7e3cafccb0ad2d393cf09386bb08914497","_cell_guid":"c130a544-2ae7-71c9-04f1-51c3e69ce842"}},{"outputs":[],"source":"weather_labels = ['clear', 'partly_cloudy', 'haze', 'cloudy']\nmake_cooccurence_matrix(weather_labels)","cell_type":"code","execution_count":null,"metadata":{"_uuid":"51aa3d31fba3507556bf56fa9f955ee12c3d5d7d","_cell_guid":"4c40a10f-83cc-5ab3-7c95-f8e26a08068f","trusted":false}},{"outputs":[],"source":"But the land labels may overlap:","cell_type":"markdown","execution_count":null,"metadata":{"_uuid":"50e7322a69ebf04c8f76639bb2f6c59eee357e0b","_cell_guid":"f61ee506-ee56-9ba1-db45-1da151978e5c"}},{"outputs":[],"source":"land_labels = ['primary', 'agriculture', 'water', 'cultivation', 'habitation']\nmake_cooccurence_matrix(land_labels)","cell_type":"code","execution_count":null,"metadata":{"_uuid":"9adbc18ca0fc5852e213c731e61526caf50780ed","_cell_guid":"b503e3da-a478-ac7f-41e5-1893394f3a68","trusted":false}},{"outputs":[],"source":"The rarer labels have very little overlap:","cell_type":"markdown","execution_count":null,"metadata":{"_uuid":"5d505a938f330334bf464c4bf5a1d34912c07d70","_cell_guid":"3e06f07e-3c31-067b-78dc-323505ca5d6c"}},{"outputs":[],"source":"rare_labels = [l for l in label_list if labels_df[label_list].sum()[l] < 2000]\nmake_cooccurence_matrix(rare_labels)","cell_type":"code","execution_count":null,"metadata":{"_uuid":"7814e5f1890072e27455f3065e00797fbe6b4458","_cell_guid":"8703df07-3d6b-abfe-8614-28d4ff068e4a","trusted":false}},{"outputs":[],"source":"## Inspect images\nLet's display an image and visualize the pixel values. Here we will pick an image, load every single single band, then create RGB stack. These raw images are 16-bit (from 0 to 65535), and contain red, green, blue, and [Near infrared (NIR)](https://en.wikipedia.org/wiki/Infrared#Regions_within_the_infrared) channels. In this example, we are discarding the NIR band just to simplify the steps to visualize the image. However, you should probably keep it for ML classification.\n\nThe files can be easily read into numpy arrays with the skimage.","cell_type":"code","execution_count":null,"metadata":{"_uuid":"e2ece6ec7741197913480afb0b694aa1b23d4ffd","_cell_guid":"cbd4cc2b-dc23-5d51-813d-58191d04e36a","trusted":false}},{"outputs":[],"source":"def sample_images(tags, n=None):\n    \"\"\"Randomly sample n images with the specified tags.\"\"\"\n    condition = True\n    if isinstance(tags, string_types):\n        raise ValueError(\"Pass a list of tags, not a single tag.\")\n    for tag in tags:\n        condition = condition & labels_df[tag] == 1\n    if n is not None:\n        return labels_df[condition].sample(n)\n    else:\n        return labels_df[condition]","cell_type":"code","execution_count":null,"metadata":{"_uuid":"d3b4c5e61beba2fb9cd2fe5d9dd15b31dbe3c514","_cell_guid":"ee7b99e6-53d9-3443-67db-3ab30742d448","trusted":false}},{"outputs":[],"source":"def load_image(filename):\n    '''Look through the directory tree to find the image you specified\n    (e.g. train_10.tif vs. train_10.jpg)'''\n    for dirname in os.listdir(PLANET_KAGGLE_ROOT):\n        path = os.path.abspath(os.path.join(PLANET_KAGGLE_ROOT, dirname, filename))\n        if os.path.exists(path):\n            print('Found image {}'.format(path))\n            return io.imread(path)\n    # if you reach this line, you didn't find the image you're looking for\n    print('Load failed: could not find image {}'.format(path))\n    \ndef sample_to_fname(sample_df, row_idx, suffix='tif'):\n    '''Given a dataframe of sampled images, get the\n    corresponding filename.'''\n    fname = sample_df.get_value(sample_df.index[row_idx], 'image_name')\n    return '{}.{}'.format(fname, suffix)","cell_type":"code","execution_count":null,"metadata":{"_uuid":"27335c09257fbce18d40fe22ef0f5cdeb3697513","_cell_guid":"d4165497-7319-fe38-b113-c572493c9c8c","trusted":false}},{"outputs":[],"source":"Let's look at an individual image. First, we'll plot a histogram of pixel values in each channel. Note how the intensities are distributed in a relatively narrow region of the dynamic range","cell_type":"markdown","execution_count":null,"metadata":{"_uuid":"484c7d1431aae9b647f6eaddeffba751d2c58618","_cell_guid":"7dfe886c-8f3c-5375-3c3c-e950170249a5"}},{"outputs":[],"source":"def plot_rgbn_histo(r, g, b, n):\n    for slice_, name, color in ((r,'r', 'red'),(g,'g', 'green'),(b,'b', 'blue'), (nir, 'nir', 'magenta')):\n        plt.hist(slice_.ravel(), bins=100, \n                 range=[0,rgb_image.max()], \n                 label=name, color=color, histtype='step')\n    plt.legend()","cell_type":"code","execution_count":null,"metadata":{"_uuid":"5bcdbd189ad6d6f0f34f2f959b45f59e4a451a5e","_cell_guid":"d393a9b7-ad0f-c933-d892-ba7b618f94f2","trusted":false}},{"outputs":[],"source":"s = sample_images(['primary', 'water', 'road'], n=1)\nfname = sample_to_fname(s, 0)\n\n# find the image in the data directory and load it\n# note the initial bgrn band ordering\nbgrn_image = load_image(fname)\n\n# extract the rgb values\nbgr_image = bgrn_image[:,:,:3]\nrgb_image = bgr_image[:, :, [2,1,0]]\n\n# extract the different bands\nb, g, r, nir = bgrn_image[:, :, 0], bgrn_image[:, :, 1], bgrn_image[:, :, 2], bgrn_image[:, :, 3]\n\n# plot a histogram of rgbn values\nplot_rgbn_histo(r, g, b, nir)","cell_type":"code","execution_count":null,"metadata":{"_uuid":"4b4b5b1b485312985ca04e7b14b06cef84d4d659","_cell_guid":"e61ff246-835d-ab3d-e1a3-6693896ab18a","trusted":false}},{"outputs":[],"source":"We can look at each channel individually:","cell_type":"markdown","execution_count":null,"metadata":{"_uuid":"e90349974c6322a6e696456223146d9024aba929","_cell_guid":"8d7cd8aa-f9a9-5a53-9f62-78597f9c65d9"}},{"outputs":[],"source":"# Plot the bands\nfig = plt.figure()\nfig.set_size_inches(12, 4)\nfor i, (x, c) in enumerate(((r, 'r'), (g, 'g'), (b, 'b'), (nir, 'near-ir'))):\n    a = fig.add_subplot(1, 4, i+1)\n    a.set_title(c)\n    plt.imshow(x)","cell_type":"code","execution_count":null,"metadata":{"_uuid":"bcfb570a2da0773384cecb50d887dde151e617d6","_cell_guid":"576746f5-ce04-bb53-25e7-740d20c2cb3b","trusted":false}},{"outputs":[],"source":"But, when we try to look at the RGB image, something funny's going on!|","cell_type":"markdown","execution_count":null,"metadata":{"_uuid":"a2af4bc60b6d8f6e918dfb3dda08ee326c2ecc7e","_cell_guid":"de25ec51-d364-09d5-c512-9e1dbc952bf1"}},{"outputs":[],"source":"plt.imshow(rgb_image)","cell_type":"code","execution_count":null,"metadata":{"_uuid":"43c0dccca83e811e69748bc4d2b8c0fd49bd76fa","_cell_guid":"9aa21e0f-4a5c-58a4-6790-76caf192ea13","trusted":false}},{"outputs":[],"source":"### Calibrate colors for visual inspection","cell_type":"markdown","execution_count":null,"metadata":{"_uuid":"3bd85b5a5e8cf16d31fb728d50dce6b7792353fa","_cell_guid":"46156d4c-b583-2650-0e2b-a30f932e91a6"}},{"outputs":[],"source":"Yikes! That does not look pretty. This is not a problem for analytic purposes, but we can try some transformations to make the image look better for visual inspection.\n\nOne way of doing this is to normalize the image channels to a reference color curve. We'll show here how to estimate a reference color curve from other normalized images. We could choose a third party aerial image of a canopy , but here we will employ the JPEG images provided in the data set, which have already been color-corrected.  \n\nIn essence, the idea is to transform the pixel values of the test image so that their average and variance match the reference image data.\n\nGet a list of reference images to extract data from:","cell_type":"markdown","execution_count":null,"metadata":{"_uuid":"9f4b8ea7e5a7cb67e01d0c4841eb35ed1e7004e8","_cell_guid":"d1105f35-9d6d-8f24-44db-53681e6fb679"}},{"outputs":[],"source":"# Pull a list of 20000 image names\njpg_list = os.listdir(PLANET_KAGGLE_JPEG_DIR)[:20000]\n# Select a random sample of 100 among those\nnp.random.shuffle(jpg_list)\njpg_list = jpg_list[:100]","cell_type":"code","execution_count":null,"metadata":{"_uuid":"d8d21e039874c50cc01fea4c68e7979876ef2ff3","_cell_guid":"b3d72c97-c925-ed98-2fd9-d7b4e1ffc629","trusted":false}},{"outputs":[],"source":"print(jpg_list)","cell_type":"code","execution_count":null,"metadata":{"_uuid":"2f4a0e73113c72d94fa7f116ab83916417e7b4d7","_cell_guid":"ead55f22-556d-bd82-b8da-8a4c87533537","trusted":false}},{"outputs":[],"source":"Read each image (8-bit RGBA) and dump the pixels values to ref_colors, which contains buckets for R, G and B","cell_type":"markdown","execution_count":null,"metadata":{"_uuid":"5b7d2bbed8d561b38d718cfa30608323443057d7","_cell_guid":"aeb21a56-9225-428d-33fb-0f9b9b79e8db"}},{"outputs":[],"source":"ref_colors = [[],[],[]]\nfor _file in jpg_list:\n    # keep only the first 3 bands, RGB\n    _img = mpimg.imread(os.path.join(PLANET_KAGGLE_JPEG_DIR, _file))[:,:,:3]\n    # Flatten 2-D to 1-D\n    _data = _img.reshape((-1,3))\n    # Dump pixel values to aggregation buckets\n    for i in range(3): \n        ref_colors[i] = ref_colors[i] + _data[:,i].tolist()\n    \nref_colors = np.array(ref_colors)","cell_type":"code","execution_count":null,"metadata":{"_uuid":"6cc6156d82576ca20d31ab80a8b25cf732aca06e","_cell_guid":"86e2b276-cf76-856e-2c3c-d5f5f2da0c03","trusted":false}},{"outputs":[],"source":"Visualize the histogram of the reference data","cell_type":"markdown","execution_count":null,"metadata":{"_uuid":"bbe9f12f6e114b08030e7fbeec51e9f142e5d3b9","_cell_guid":"0e0c4f47-41a1-9913-6019-7378c4999afc"}},{"outputs":[],"source":"for i,color in enumerate(['r','g','b']):\n    plt.hist(ref_colors[i], bins=30, range=[0,255], label=color, color=color, histtype='step')\nplt.legend()\nplt.title('Reference color histograms')","cell_type":"code","execution_count":null,"metadata":{"_uuid":"161df40e3a525480d219cbae9e06ce442dc4bfd9","_cell_guid":"e0a6d26e-b51a-86bd-f312-87948475c98c","trusted":false}},{"outputs":[],"source":"Compute the mean and variance for each channel in the reference data","cell_type":"markdown","execution_count":null,"metadata":{"_uuid":"4952f7a6ccea7f0dec45dcd9d8b12bf94956249b","_cell_guid":"4db8b536-c997-dc10-564e-a5e2989b53e7"}},{"outputs":[],"source":"ref_means = [np.mean(ref_colors[i]) for i in range(3)]\nref_stds = [np.std(ref_colors[i]) for i in range(3)]","cell_type":"code","execution_count":null,"metadata":{"_uuid":"290dc32642f55bfc3b9b7f1cd94e3f61e34ca1cb","_cell_guid":"d80f17c6-99e3-a0ad-ae0f-63d8c9546c88","trusted":false}},{"outputs":[],"source":"And now, we have a function that can calibrate any raw image reasonably well:","cell_type":"markdown","execution_count":null,"metadata":{"_uuid":"3b4e5e16c1b2630f8e857e0220f3772e6c1ff898","_cell_guid":"9f2903ab-f1d6-55a8-8615-0387a8784ee3"}},{"outputs":[],"source":"def calibrate_image(rgb_image):\n    # Transform test image to 32-bit floats to avoid \n    # surprises when doing arithmetic with it \n    calibrated_img = rgb_image.copy().astype('float32')\n\n    # Loop over RGB\n    for i in range(3):\n        # Subtract mean \n        calibrated_img[:,:,i] = calibrated_img[:,:,i]-np.mean(calibrated_img[:,:,i])\n        # Normalize variance\n        calibrated_img[:,:,i] = calibrated_img[:,:,i]/np.std(calibrated_img[:,:,i])\n        # Scale to reference \n        calibrated_img[:,:,i] = calibrated_img[:,:,i]*ref_stds[i] + ref_means[i]\n        # Clip any values going out of the valid range\n        calibrated_img[:,:,i] = np.clip(calibrated_img[:,:,i],0,255)\n\n    # Convert to 8-bit unsigned int\n    return calibrated_img.astype('uint8')","cell_type":"code","execution_count":null,"metadata":{"_uuid":"03887e1270b6de1cafd4e57dbd1e4ba257f91739","_cell_guid":"ae8ece5e-8de5-156b-fb53-fe9516f149c2","trusted":false}},{"outputs":[],"source":"Visualize the color histogram of the newly calibrated test image, and note that it's more evenly distributed throughout the dynamic range, and is closer to the reference data.","cell_type":"markdown","execution_count":null,"metadata":{"_uuid":"57ba52acfa2dfb2f702fff93dc7c905cf27cd52c","_cell_guid":"a074ce12-da93-2b88-7437-de12c7948b68"}},{"outputs":[],"source":"test_image_calibrated = calibrate_image(rgb_image)\nfor i,color in enumerate(['r','g','b']):\n    plt.hist(test_image_calibrated[:,:,i].ravel(), bins=30, range=[0,255], \n             label=color, color=color, histtype='step')\nplt.legend()\nplt.title('Calibrated image color histograms')","cell_type":"code","execution_count":null,"metadata":{"_uuid":"8aaf5d04c0e8e1592809ef7873ff6c7c822f5b74","_cell_guid":"257d4945-f0c9-2077-7fcb-f6123dfd4073","trusted":false}},{"outputs":[],"source":"And now we have something we can recognize!","cell_type":"markdown","execution_count":null,"metadata":{"_uuid":"f32b881c22e618ccc0bcce68ab24bf22c713f777","_cell_guid":"5174e99f-3540-5adf-f5d1-63573e840115"}},{"outputs":[],"source":"plt.imshow(test_image_calibrated)","cell_type":"code","execution_count":null,"metadata":{"_uuid":"3341b73af626098834a7f0f13be6abe126fcff97","_cell_guid":"7e7a5dd9-e0a5-49b2-836a-d2fdb098eab4","trusted":false}},{"outputs":[],"source":"Putting it all together, to show several images with your tags of choice. You may notice that the jpgs and tifs look a bit different. You can read about why that is here:\n    \n[A Hands-on Guide to Color Correction](https://www.planet.com/pulse/color-correction/)","cell_type":"markdown","execution_count":null,"metadata":{"_uuid":"14217d0bd747a1148888dde9a2f7c4e0e122a52e","_cell_guid":"38c08d57-7f43-8c83-8025-2550ed9746bd"}},{"outputs":[],"source":"sampled_images = sample_images(['clear', 'road', 'water'], n=3)\n\nfor i in range(len(sampled_images)):\n    tif = sample_to_fname(sampled_images, i, 'tif')\n    jpg = sample_to_fname(sampled_images, i, 'jpg')\n\n    try:\n        tif_img = load_image(tif)[:,:,:3]\n        jpg_img = load_image(jpg)[:,:,:3]\n\n        fig = plt.figure()\n        plt.imshow(calibrate_image(tif_img))\n\n        fig = plt.figure()\n        plt.imshow(calibrate_image(jpg_img))\n    except:\n        continue\n        \n        ","cell_type":"code","execution_count":null,"metadata":{"_uuid":"00a2daaa95c38031ee95ecdb57cb62e1a9b0eff8","_cell_guid":"432936ae-a00b-cfba-ce56-982a665c8cca","trusted":false}},{"outputs":[],"source":"## Image modification","cell_type":"markdown","execution_count":null,"metadata":{"_uuid":"10520c449a6588d25511067487f9f94e79cfaf0d","_cell_guid":"a96d680f-e9d6-a322-adba-5297198606cc"}},{"outputs":[],"source":"You might want to rotate, flip, or otherwise modify the images for training purposes. Note that the dimensions of the image changes:","cell_type":"markdown","execution_count":null,"metadata":{"_uuid":"aaae0280ebf48afa30433c891f01517017e08a15","_cell_guid":"2cc0df6e-abcf-752e-8284-236c7547ab92"}},{"outputs":[],"source":"fig = plt.figure()\nfig.set_size_inches(12, 4)\nfor i, (x, c) in enumerate(((r, 'r'), (g, 'g'), (b, 'b'), (nir, 'near-ir'))):\n    a = fig.add_subplot(1, 4, i+1)\n    a.set_title(c)\n    plt.imshow(x)","cell_type":"code","execution_count":null,"metadata":{"_uuid":"b46aad155c1820a257c8b14a7d6cac09f37aa1b6","_cell_guid":"bc6692e1-db28-234d-a114-638be512004f","trusted":false}},{"outputs":[],"source":"rotated = scipy.ndimage.rotate(rgb_image, angle=45)\nplt.imshow(rgb_image)\nplt.imshow(calibrate_image(rotated))\nrotated.shape","cell_type":"code","execution_count":null,"metadata":{"_uuid":"27ee30e6c2ba42b2f8dd6a11d7403c6f704d8f8d","_cell_guid":"84144dc3-256f-0ba4-43ae-00163898d4ca","trusted":false}},{"outputs":[],"source":"Original Notebook by Jesus Martinez Manso and Benjamin Goldenberg\n\n(C) Planet 2017","cell_type":"markdown","execution_count":null,"metadata":{"_uuid":"69406c8e03f6f8f2e489a9734e073a8019b3b454","_cell_guid":"64ff4caf-cebb-857a-8292-4b6af9edfec2"}},{"outputs":[],"source":"","cell_type":"code","execution_count":null,"metadata":{"_uuid":"797a1eadc6e7d539768600f2c20091686aa46040","_cell_guid":"e3b10fd3-3ff3-0f6c-5f08-c6cde6ceab98","trusted":false}}],"nbformat":4,"nbformat_minor":0,"metadata":{"_change_revision":0,"language_info":{"version":"3.6.1","name":"python","codemirror_mode":{"version":3,"name":"ipython"},"nbconvert_exporter":"python","pygments_lexer":"ipython3","mimetype":"text/x-python","file_extension":".py"},"_is_fork":false,"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"}}}