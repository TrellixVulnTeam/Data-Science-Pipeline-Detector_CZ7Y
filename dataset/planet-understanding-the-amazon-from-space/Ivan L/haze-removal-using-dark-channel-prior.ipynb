{"cells":[{"metadata":{"_uuid":"134ccc5bec0cee5e0bfc4d14dcadc331b74c8aea","collapsed":false,"_cell_guid":"2e21d4f0-da08-4af1-a484-719db8081f6c","_execution_state":"idle"},"outputs":[],"cell_type":"markdown","source":"The code is taken from BLKStone's Github:\n\nhttps://github.com/BLKStone/Single-Image-Haze-Removal-Using-Dark-Channel-Prior\n\nThis is published to help Heng, and others, in his quest to produce the top results. You can visually inspect the result of the filter on a few hazy images at the bottom using default settings - they look good to me.\n\nPaper: http://kaiminghe.com/publications/pami10dehaze.pdf","execution_count":null},{"metadata":{"_uuid":"3514ddcdcfd2eb6a148d299826f90ddd3fba0ddf","collapsed":false,"_cell_guid":"d076c9e2-d13a-475e-9782-d2577f962434","_execution_state":"idle","trusted":false},"outputs":[],"cell_type":"code","source":"import math\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport time\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom IPython.display import display","execution_count":1},{"metadata":{"_uuid":"7658527fc9942722573fa7d51a5b1c6b402b1fb3","collapsed":false,"_cell_guid":"aee3c28d-1281-4057-957c-b4b51919bf5a","_execution_state":"idle","trusted":false},"outputs":[],"cell_type":"code","source":"class Node(object):\n    def __init__(self,x,y,value):\n        self.x = x\n        self.y = y\n        self.value = value\n\n    def printInfo(self):\n        print('%s:%s:%s' %(self.x,self.y,self.value))\n        \ndef getMinChannel(img):\n\n    # 输入检查\n    if len(img.shape)==3 and img.shape[2]==3:\n        pass\n    else:\n        print(\"bad image shape, input must be color image\")\n        return None\n    \n    return np.min(img, axis=2)\n   \ndef getDarkChannel(img,blockSize = 3):\n\n    # 输入检查\n    if len(img.shape)==2:\n        pass\n    else:\n        print(\"bad image shape, input image must be two demensions\")\n        return None\n\n    # blockSize检查\n    if blockSize % 2 == 0 or blockSize < 3:\n        print('blockSize is not odd or too small')\n        return None\n\n    # 计算addSize\n    A = int((blockSize-1)/2) #AddSize\n\n    #New height and new width\n    H = img.shape[0] + blockSize - 1\n    W = img.shape[1] + blockSize - 1\n\n    # 中间结果\n    imgMiddle = 255 * np.ones((H,W))    \n\n    imgMiddle[A:H-A, A:W-A] = img\n    \n    imgDark = np.zeros_like(img, np.uint8)    \n    \n    localMin = 255\n    for i in range(A, H-A):\n        for j in range(A, W-A):\n            x = range(i-A, i+A+1)\n            y = range(j-A, j+A+1)\n            imgDark[i-A,j-A] = np.min(imgMiddle[x,y])                            \n            \n    return imgDark\n\ndef getAtomsphericLight(darkChannel,img,meanMode = False, percent = 0.001):\n\n    size = darkChannel.shape[0]*darkChannel.shape[1]\n    height = darkChannel.shape[0]\n    width = darkChannel.shape[1]\n\n    nodes = []\n\n    # 用一个链表结构(list)存储数据\n    for i in range(0,height):\n        for j in range(0,width):\n            oneNode = Node(i,j,darkChannel[i,j])\n            nodes.append(oneNode)\t\n\n    # 排序\n    nodes = sorted(nodes, key = lambda node: node.value,reverse = True)\n\n    atomsphericLight = 0\n\n    # 原图像像素过少时，只考虑第一个像素点\n    if int(percent*size) == 0:\n        for i in range(0,3):\n            if img[nodes[0].x,nodes[0].y,i] > atomsphericLight:\n                atomsphericLight = img[nodes[0].x,nodes[0].y,i]\n        return atomsphericLight\n\n    # 开启均值模式\n    if meanMode:\n        sum = 0\n        for i in range(0,int(percent*size)):\n            for j in range(0,3):\n                sum = sum + img[nodes[i].x,nodes[i].y,j]\n        atomsphericLight = int(sum/(int(percent*size)*3))\n        return atomsphericLight\n\n    # 获取暗通道前0.1%(percent)的位置的像素点在原图像中的最高亮度值\n    for i in range(0,int(percent*size)):\n        for j in range(0,3):\n            if img[nodes[i].x,nodes[i].y,j] > atomsphericLight:\n                atomsphericLight = img[nodes[i].x,nodes[i].y,j]\n    return atomsphericLight\n\ndef getRecoverScene(img, omega=0.95, t0=0.1, blockSize=15, meanMode=False, percent=0.001, refine=True):\n\n    imgGray = getMinChannel(img)\n    imgDark = getDarkChannel(imgGray, blockSize = blockSize)\n    atomsphericLight = getAtomsphericLight(imgDark,img,meanMode = meanMode,percent= percent)\n\n    imgDark = np.float64(imgDark)\n    transmission = 1 - omega * imgDark / atomsphericLight\n\n    # 防止出现t小于0的情况\n    # 对t限制最小值为0.1\n    transmission[transmission<0.1] = 0.1     \n    \n    if refine:        \n        normI = (img - img.min()) / (img.max() - img.min())  # normalize I\n        transmission = guided_filter(normI, transmission, r=40, eps=1e-3)\n\n    sceneRadiance = np.zeros(img.shape)\n    img = np.float64(img)\n    \n    for i in range(3):        \n        SR = (img[:,:,i] - atomsphericLight)/transmission + atomsphericLight\n\n        # 限制透射率 在0～255                  \n        SR[SR>255] = 255\n        SR[SR<0] = 0                    \n        sceneRadiance[:,:,i] = SR  \n            \n    sceneRadiance = np.uint8(sceneRadiance)\n\n    return sceneRadiance","execution_count":2},{"metadata":{"_uuid":"1f17cc029e489ae9b701e56137c5e7b50f2f62f2","collapsed":false,"_cell_guid":"728d78dd-49b3-4851-a583-89f8e465a06a","_execution_state":"idle"},"outputs":[],"source":"The following code comes from joyeecheung, https://github.com/joyeecheung/dark-channel-prior-dehazing\n\nThe guilded-filter helps to damp down the artefacts when the Dark-channel prior filter is applied to an image","cell_type":"markdown","execution_count":null},{"metadata":{"_uuid":"a0ca324d5b84dd7eeb1270ce72941093c146e714","collapsed":false,"_cell_guid":"349d6c54-6ae1-40db-898e-a4df16ee28a0","_execution_state":"idle","trusted":false},"outputs":[],"source":"\"\"\"Implementation for Guided Image Filtering\nReference:\nhttp://research.microsoft.com/en-us/um/people/kahe/eccv10/\n\"\"\"\n\nfrom itertools import combinations_with_replacement\nfrom collections import defaultdict\n\nimport numpy as np\nfrom numpy.linalg import inv\n\nR, G, B = 0, 1, 2  # index for convenience\n\n\ndef boxfilter(I, r):\n    \"\"\"Fast box filter implementation.\n    Parameters\n    ----------\n    I:  a single channel/gray image data normalized to [0.0, 1.0]\n    r:  window radius\n    Return\n    -----------\n    The filtered image data.\n    \"\"\"        \n    M, N = I.shape\n    dest = np.zeros((M, N))\n\n    # cumulative sum over Y axis\n    sumY = np.cumsum(I, axis=0)\n    # difference over Y axis\n    dest[:r + 1] = sumY[r: 2 * r + 1]\n    dest[r + 1:M - r] = sumY[2 * r + 1:] - sumY[:M - 2 * r - 1]\n    dest[-r:] = np.tile(sumY[-1], (r, 1)) - sumY[M - 2 * r - 1:M - r - 1]\n\n    # cumulative sum over X axis\n    sumX = np.cumsum(dest, axis=1)\n    # difference over Y axis\n    dest[:, :r + 1] = sumX[:, r:2 * r + 1]\n    dest[:, r + 1:N - r] = sumX[:, 2 * r + 1:] - sumX[:, :N - 2 * r - 1]\n    dest[:, -r:] = np.tile(sumX[:, -1][:, None], (1, r)) - \\\n        sumX[:, N - 2 * r - 1:N - r - 1]\n\n    return dest\n\n\ndef guided_filter(I, p, r=40, eps=1e-3):\n    \"\"\"Refine a filter under the guidance of another (RGB) image.\n    Parameters\n    -----------\n    I:   an M * N * 3 RGB image for guidance.\n    p:   the M * N filter to be guided\n    r:   the radius of the guidance\n    eps: epsilon for the guided filter\n    Return\n    -----------\n    The guided filter.\n    \"\"\"    \n    M, N = p.shape\n    base = boxfilter(np.ones((M, N)), r)\n\n    # each channel of I filtered with the mean filter\n    means = [boxfilter(I[:, :, i], r) / base for i in range(3)]\n    # p filtered with the mean filter\n    mean_p = boxfilter(p, r) / base\n    # filter I with p then filter it with the mean filter\n    means_IP = [boxfilter(I[:, :, i] * p, r) / base for i in range(3)]\n    # covariance of (I, p) in each local patch\n    covIP = [means_IP[i] - means[i] * mean_p for i in range(3)]\n\n    # variance of I in each local patch: the matrix Sigma in ECCV10 eq.14\n    var = defaultdict(dict)\n    for i, j in combinations_with_replacement(range(3), 2):\n        var[i][j] = boxfilter(\n            I[:, :, i] * I[:, :, j], r) / base - means[i] * means[j]\n\n    a = np.zeros((M, N, 3))\n    for y, x in np.ndindex(M, N):\n        #         rr, rg, rb\n        # Sigma = rg, gg, gb\n        #         rb, gb, bb\n        Sigma = np.array([[var[R][R][y, x], var[R][G][y, x], var[R][B][y, x]],\n                          [var[R][G][y, x], var[G][G][y, x], var[G][B][y, x]],\n                          [var[R][B][y, x], var[G][B][y, x], var[B][B][y, x]]])\n        cov = np.array([c[y, x] for c in covIP])\n        a[y, x] = np.dot(cov, inv(Sigma + eps * np.eye(3)))  # eq 14\n\n    # ECCV10 eq.15\n    b = mean_p - a[:, :, R] * means[R] - \\\n        a[:, :, G] * means[G] - a[:, :, B] * means[B]\n\n    # ECCV10 eq.16\n    q = (boxfilter(a[:, :, R], r) * I[:, :, R] + boxfilter(a[:, :, G], r) *\n         I[:, :, G] + boxfilter(a[:, :, B], r) * I[:, :, B] + boxfilter(b, r)) / base\n\n    return q","cell_type":"code","execution_count":3},{"metadata":{"_uuid":"1f418354bd93e46cd54b46132ba69f9f67e549d7","collapsed":false,"_cell_guid":"d3a1361f-250f-41e7-bb50-155d582a4c3a","_execution_state":"idle","trusted":false},"outputs":[],"cell_type":"code","source":"df_train = pd.read_csv('../input/train_v2.csv')","execution_count":4},{"metadata":{"_uuid":"cffd2123001eaf02bcb22002af44b1973293cb4a","collapsed":false,"_execution_state":"idle"},"outputs":[],"cell_type":"markdown","source":"Let's look at the results on some hazy images. \n\nFor each hazy image, from left to right we will plot the original image, then dark channel prior with refinement, then without. For the satellite images, refinement does not seem to have much impact, but it definitely helps to reduce artefacts in general.","execution_count":null},{"metadata":{"_uuid":"b2698643c82049d09538e2a001c0bb234634c361","collapsed":false,"_cell_guid":"8369ddd9-93b8-4d93-9e41-0ec94ced4f0c","_execution_state":"idle","trusted":false},"outputs":[],"cell_type":"code","source":"Hazy_img_idx = [104, 3007, 3794, 23710, 38469]\n\nfor i in Hazy_img_idx:    \n     \n    path = '../input/train-jpg/'\n    filename = 'train_{}.jpg'.format(i)\n    img = cv2.imread(path+filename)  #0-255    \n    \n    dehazed_img1 = getRecoverScene(img, refine=True)\n    dehazed_img2 = getRecoverScene(img, refine=False)\n\n    fig = plt.figure()\n    fig.set_size_inches(12, 4)\n    fig.suptitle(filename + '   Tags: ' + df_train['tags'][i], fontsize=12)\n\n    plt.subplot(131)\n    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n\n    plt.subplot(132)\n    plt.imshow(cv2.cvtColor(dehazed_img1, cv2.COLOR_BGR2RGB))\n    \n    plt.subplot(133)\n    plt.imshow(cv2.cvtColor(dehazed_img2, cv2.COLOR_BGR2RGB))\n    \n    plt.show()\n","execution_count":5}],"nbformat_minor":0,"metadata":{"language_info":{"mimetype":"text/x-python","nbconvert_exporter":"python","codemirror_mode":{"version":3,"name":"ipython"},"file_extension":".py","name":"python","pygments_lexer":"ipython3","version":"3.6.1"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4}