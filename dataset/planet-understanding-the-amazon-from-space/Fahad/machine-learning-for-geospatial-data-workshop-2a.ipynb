{"cells":[{"metadata":{"_uuid":"0a3123149dc8b38430a40a1cbd92f50b49d8f104"},"cell_type":"markdown","source":"# Data exploration and basic ConvNN for Image classification\n\n*University of Waterloo Geospatial Club - Winter 2019*\n\nWe hope that you will learn something new from the content of this workshop. If you have questions after the workshop, then please feel free to message the Geospatial Club or the workshop presenters.\n\nJuan Carrillo at jmcarril@uwaterloo.ca and \nJaydeep Mistry at jaydeep.mistry@uwaterloo.ca\n\n[Slides](http://bit.ly/geoml-1) and [Kernel](https://www.kaggle.com/jrmistry/geospatial-club-ml-for-geo-data-workshop-1) of the first workshop on vector data.\n\n\n\n| *References* |\n------------ |\n[[1]](https://www.kaggle.com/c/planet-understanding-the-amazon-from-space)  | Planet: Understanding the Amazon from Space. Featured Kaggle Competition.\n[[2]](https://www.kaggle.com/philschmidt/multilabel-classification-rainforest-eda) | Kernel by Philipp Schmidt\n[[3]](https://www.kaggle.com/ekami66/0-92837-on-private-lb-solution-with-keras) | Kernel by Tuatini Godard\n[[4]](https://www.kaggle.com/anokas/simple-keras-starter?fbclid=IwAR0ohl2rTL4jajyvL26SDKN8s4ZuZRSU2ir4KShM0Cw1-XRBH5DJ_6zTIpU) | Source code by Mikel Bober\n[[5]](https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/) | Source code by Jason Brownlee\n[[6]](https://gisgeography.com/ndvi-normalized-difference-vegetation-index/) | Normalized Difference Vegetation Index (NDVI)\n[[7]](https://lvdmaaten.github.io/tsne/) | t-Distributed Stochastic Neighbor Embedding (t-SNE)\n[[8]](https://matplotlib.org/) | Matplotlib documentation\n[[9]](https://tqdm.github.io/) | tqdm documentation\n[[10]](https://docs.scipy.org/doc/) | Scipy documentation"},{"metadata":{"_uuid":"bd9ef1ac33bd100f4636034b988ad00757889a04"},"cell_type":"markdown","source":"## Dataset: Satellite data from the Amazon rainforest\n\nSquare tiles (256x256pix.) of high-resolution, four band (RGB + IR) images from Planet Flock 2 Satellites. Each tile can have multiple (common and less common) labels, but only one of the cloud cover labels.\n\nCommon labels | Less Common Labels | Cloud Cover Labels\n------------ | ------------- | -------------\nPrimary Rain Forest | Slash and Burn | Clear\nWater (Rivers & Lakes) | Selective Logging | Partly cloudy\nHabitation | Blooming | Cloudy\nAgriculture | Conventional Mining | Haze\nRoad | Artisinal Mining |\nCultivation | Blow Down |\nBare Ground | |\n\nMore details about the dataset [here](https://www.kaggle.com/c/planet-understanding-the-amazon-from-space/data)"},{"metadata":{"_uuid":"7d138912d3bfcafddd9609d45e673b4a75f06d40"},"cell_type":"markdown","source":"## Import required libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Import required libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom subprocess import check_output\nimport matplotlib.pyplot as plt\nfrom scipy.stats import bernoulli\nimport seaborn as sns\nimport cv2\nfrom glob import glob # handles pathnames \n%matplotlib inline\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nfrom sklearn.manifold import TSNE\nfrom matplotlib.offsetbox import OffsetImage, AnnotationBbox\nfrom skimage import io\nimport tifffile\n\nimport os\nimport gc\nimport keras as k\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\n\nimport cv2\nfrom tqdm import tqdm\nfrom sklearn.metrics import fbeta_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c8af27cd5daf8c703f7181b62232e1d51930c786"},"cell_type":"code","source":"# Print filenames of input datasets\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5c1d95159ef9bad525153beb7b4939f0e3f8ac9b"},"cell_type":"markdown","source":"## Preview some images\nMore than 40.000 images in the training set."},{"metadata":{"trusted":true,"_uuid":"224e009e1dabb218af959d2b31d012e737699c23"},"cell_type":"code","source":"labels_df = pd.read_csv('../input/train_v2.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9dd69d749945ef82888ea73fd269acb94417ff28"},"cell_type":"code","source":"# Pick nine images (jpg)\nall_image_paths_jpg = sorted(glob('../input/train-jpg/*.jpg'))\nnine_imgs_paths_jpg = []\nnine_imgs_paths_jpg.append(all_image_paths_jpg[11006])\nnine_imgs_paths_jpg.append(all_image_paths_jpg[15000])\nnine_imgs_paths_jpg.append(all_image_paths_jpg[4005])\nnine_imgs_paths_jpg.append(all_image_paths_jpg[12007])\nnine_imgs_paths_jpg.append(all_image_paths_jpg[8002])\nnine_imgs_paths_jpg.append(all_image_paths_jpg[14001])\nnine_imgs_paths_jpg.append(all_image_paths_jpg[16004])\nnine_imgs_paths_jpg.append(all_image_paths_jpg[13003])\nnine_imgs_paths_jpg.append(all_image_paths_jpg[9006])\n\nnine_image_names = list(map(lambda row: row.split(\"/\")[-1][:-4], nine_imgs_paths_jpg))\n\n# Plot them in a 3 by 3 grid\nplt.figure(figsize=(12,8))\nfor i in range(9):\n    plt.subplot(3,3,i+1)\n    plt.subplots_adjust(top = 0.99, bottom=0.01, hspace=0.5, wspace=0.4)\n    plt.imshow(plt.imread(nine_imgs_paths_jpg[i]))\n    plt.title(str(labels_df[labels_df.image_name == nine_image_names[i]].tags.values))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"99293bbf3821053d71d2393c3ec39be40640a6fe"},"cell_type":"markdown","source":"## Explore NDVI index for different classes\nThe Normalized Difference Vegetation Index (NDVI) help us to assess vegetation by using information from the Infrared band. More on the NDVI index [here](https://gisgeography.com/ndvi-normalized-difference-vegetation-index/)."},{"metadata":{"trusted":true,"_uuid":"c5328ed50f5e4514d783e35c2126e06faac8da79"},"cell_type":"code","source":"# Pick nine images (tif)\nall_image_paths_tif = sorted(glob('../input/train-tif-v2/*.tif'))\nnine_imgs_paths_tif = []\nnine_imgs_paths_tif.append(all_image_paths_tif[11006])\nnine_imgs_paths_tif.append(all_image_paths_tif[15000])\nnine_imgs_paths_tif.append(all_image_paths_tif[4005])\nnine_imgs_paths_tif.append(all_image_paths_tif[12007])\nnine_imgs_paths_tif.append(all_image_paths_tif[8002])\nnine_imgs_paths_tif.append(all_image_paths_tif[14001])\nnine_imgs_paths_tif.append(all_image_paths_tif[16004])\nnine_imgs_paths_tif.append(all_image_paths_tif[13003])\nnine_imgs_paths_tif.append(all_image_paths_tif[9006])\n\nnine_image_names = list(map(lambda row: row.split(\"/\")[-1][:-4], nine_imgs_paths_tif))\n\n# Calculate NDVI\nimgs = [io.imread(path) / io.imread(path).max() for path in nine_imgs_paths_tif]\n#r, g, b, nir = img[:, :, 0], img[:, :, 1], img[:, :, 2], img[:, :, 3]\nndvis = [(img[:,:,3] - img[:,:,0])/((img[:,:,3] + img[:,:,0])) for img in imgs]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1d9b9a427db5f65e5e2e1ee4397c7c61453629cd"},"cell_type":"code","source":"# Plot RGB image and NDVI\nimg_id = 0\nplt.figure(figsize=(12,3.5))\nplt.subplot(121)\nplt.title(str(labels_df[labels_df.image_name == nine_image_names[img_id]].tags.values))\nplt.imshow(plt.imread(nine_imgs_paths_jpg[img_id]))\nplt.subplot(122)\nplt.title('NDVI index')\nplt.imshow(ndvis[img_id], cmap='jet')\nplt.colorbar()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c8b4772a7f98dc87a63ba5970f8aae1be1287d63"},"cell_type":"code","source":"# Plot RGB image and NDVI\nimg_id = 4\nplt.figure(figsize=(12,3.5))\nplt.subplot(121)\nplt.title(str(labels_df[labels_df.image_name == nine_image_names[img_id]].tags.values))\nplt.imshow(plt.imread(nine_imgs_paths_jpg[img_id]))\nplt.subplot(122)\nplt.title('NDVI index')\nplt.imshow(ndvis[img_id], cmap='jet')\nplt.colorbar()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9c44808c558b9eeaacdd6b6ab447b795d13d879a"},"cell_type":"code","source":"# Plot RGB image and NDVI\nimg_id = 6\nplt.figure(figsize=(12,3.5))\nplt.subplot(121)\nplt.title(str(labels_df[labels_df.image_name == nine_image_names[img_id]].tags.values))\nplt.imshow(plt.imread(nine_imgs_paths_jpg[img_id]))\nplt.subplot(122)\nplt.title('NDVI index')\nplt.imshow(ndvis[img_id], cmap='jet')\nplt.colorbar()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ecec5acde91a83683768e87627ddfa93f1bc7b8f"},"cell_type":"markdown","source":"## Inspect labels"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Print shape\nprint(labels_df.shape)\n# How does the training labels file look like?\nlabels_df.head()\n#labels_df.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"289eac1f4a78b9725d067005983b2e50ad1a0fc6"},"cell_type":"code","source":"# Print all unique classes\nfrom itertools import chain\nlabels_list = list(chain.from_iterable([tags.split(\" \") \n                                        for tags in labels_df['tags'].values]))\nnum_labels = len(labels_list)\nuniq_labels = set(labels_list) \nnum_uniq_labels = len(uniq_labels) \nprint(\"There are {} labels in the training set\".format(num_labels))\nprint(\"distributed among {} classes:\".format(num_uniq_labels))\nprint(uniq_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0bbe2007b5334d448de722440ab432ec385a89d9"},"cell_type":"code","source":"# Histogram of label instances\nlabels_s = pd.Series(labels_list).value_counts() # To sort them by count\nfig, ax = plt.subplots(figsize=(16, 8))\nsns.barplot(x=labels_s, y=labels_s.index, orient='h')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7374ba8b5ace1d490070ff20e64e222477a9b8b9"},"cell_type":"markdown","source":"## Image clustering\nUse a t-SNE plot to identify clusters of images. More details on t-SNE [here](https://lvdmaaten.github.io/tsne/)."},{"metadata":{"trusted":true,"_uuid":"5a8db227d43c90e30aa8a39616dd30acb9839e13"},"cell_type":"code","source":"# Subset of images for clustering\nn_imgs = 350\nclust_imgs = []\nimage_paths = sorted(glob('../input/train-jpg/*.jpg'))[0:n_imgs]\n\n# Image preprocessing\nfor i in range(n_imgs):\n    img = plt.imread(image_paths[i])\n    img = cv2.resize(img, (100, 100), cv2.INTER_LINEAR).astype('float')\n#    img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY).astype('float')\n    img = cv2.normalize(img, None, 0.0, 1.0, cv2.NORM_MINMAX)\n    img = img.reshape(1, -1)\n    clust_imgs.append(img)\n\n# Convert into a Numpy array\nimg_mat = np.vstack(clust_imgs)\n# Number of images, (100pix by 100pix) by 4 bands\nimg_mat.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"83c9f4ff33d7bbea9f8093e012e98f67f8440cd1"},"cell_type":"code","source":"# Fit a t-SNE manifold to the subset of images\ntsne = TSNE(\n    n_components=2,\n    init='random', # pca\n    random_state=101,\n    method='barnes_hut',\n    n_iter=500,\n    verbose=2\n).fit_transform(img_mat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c257fa287cdb06f54be53e7da18a3517a888e955"},"cell_type":"code","source":"# Plot the subset of images in a two dimensional representation\ndef imscatter(x, y, images, ax=None, zoom=0.1):\n    ax = plt.gca()\n    images = [OffsetImage(image, zoom=zoom) for image in images]\n    artists = []\n    for x0, y0, im0 in zip(x, y, images):\n        ab = AnnotationBbox(im0, (x0, y0), xycoords='data', frameon=False)\n        artists.append(ax.add_artist(ab))\n    ax.update_datalim(np.column_stack([x, y]))\n    ax.autoscale()\n\nplt.figure(figsize=(13,10))\nimscatter(tsne[0:n_imgs,0], tsne[0:n_imgs,1], [plt.imread(image_paths[i]) for i in range(n_imgs)])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e7bd07ce494f0d5eb21c7f0cf84776a0b7903ead"},"cell_type":"markdown","source":"## Basic image classification with Convolutional Neural Networks"},{"metadata":{"_uuid":"f47f1113f63799eba336a70a0814847c7a570865"},"cell_type":"markdown","source":"### Using jpg images (RGB)"},{"metadata":{"trusted":true,"_uuid":"08e597de70b04668916ae441a0d201dc4504b05e"},"cell_type":"code","source":"# Preprocess labels\nx_train = []\nx_test = []\ny_train = []\n\ndf_train = pd.read_csv('../input/train_v2.csv')\n\nflatten = lambda l: [item for sublist in l for item in sublist]\nlabels = list(set(flatten([l.split(' ') for l in df_train['tags'].values])))\n\nlabel_map = {l: i for i, l in enumerate(labels)}\ninv_label_map = {i: l for l, i in label_map.items()}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"37fd5728904f918fe00be705ef48a282ac83ebc8"},"cell_type":"code","source":"# Prepare array of images and vector representation of labels\nfor f, tags in tqdm(df_train.values, miniters=1000):\n    img = cv2.imread('../input/train-jpg/{}.jpg'.format(f))\n    targets = np.zeros(17)\n    for t in tags.split(' '):\n        targets[label_map[t]] = 1 \n    x_train.append(cv2.resize(img, (32, 32)))\n    y_train.append(targets)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"988daaca4e0f18567b9d4b636956bb4dbb7a35f3"},"cell_type":"code","source":"# Maximum and minimum values in the array of training images\nmax_value = np.amax(x_train)\nmin_value = np.amin(x_train)\n\nprint('Min value in the training images array: ', min_value)\nprint('Max value in the training images array: ', max_value)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"31c02e37a87371f41f9e87550572a465e49ac0e2"},"cell_type":"code","source":"# Normalize images into the interval [0,1]\ny_train = np.array(y_train, np.uint8)\nx_train = np.array(x_train, np.float16) / 255.\nprint(\"Shape of the training images array is: \")\nprint(x_train.shape)\nprint(\"Shape of the training labels array is: \")\nprint(y_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"897efd99d79a21660d29247f290a363946af1767"},"cell_type":"code","source":"# Split between training and validation sets\nsplit = 35000 # Approx. 86% for training and 14% for validation\nx_train, x_valid, y_train, y_valid = (x_train[:split], x_train[split:], \n                                      y_train[:split], y_train[split:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a3e39162ac37be3ecb759d9751fbe363600ca6cc"},"cell_type":"code","source":"# Create structure of Convolutional Neural Network\nmodel = Sequential()\n\nmodel.add(Conv2D(32, kernel_size=(3, 3),\n                 activation='relu',\n                 input_shape=(32, 32, 3)))\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(17, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy', \n              # We NEED binary here, since categorical_crossentropy \n              # l1 norms the output before calculating loss.\n              optimizer='adam',\n              metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ab3a7f3af1a976379495f3a9cba4f6d8439facc0","trusted":true},"cell_type":"code","source":"# Train model              \nhistory = model.fit(x_train, y_train,\n          batch_size=128,\n          epochs=12,\n          verbose=1,\n          validation_data=(x_valid, y_valid))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"53c7ac93279d6c23a166e2b4912b289a533ab983"},"cell_type":"code","source":"# Plot model accuracy and loss\n\nfig, axs = plt.subplots(1, 2, figsize=(12, 3), sharey=False)\nplt.subplots_adjust(top = 0.99, bottom=0.01, hspace=0.5, wspace=0.4)\n\naxs[0].plot(history.history['acc'])\naxs[0].plot(history.history['val_acc'])\naxs[0].set_title('model accuracy')\naxs[0].set_ylabel('accuracy')\naxs[0].set_xlabel('epoch')\naxs[0].legend(['train', 'test'], loc='upper left')\n\naxs[1].plot(history.history['loss'])\naxs[1].plot(history.history['val_loss'])\naxs[1].set_title('model loss')\naxs[1].set_ylabel('loss')\naxs[1].set_xlabel('epoch')\naxs[1].legend(['train', 'test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"125685cf069cae43aca217f2d1aca501a043a109"},"cell_type":"code","source":"# Use the model to predict\np_valid = model.predict(x_valid, batch_size=128)\n\n#np.set_printoptions(precision=2)\nnp.set_printoptions(formatter={'float': '{: 0.3f}'.format})\n\nprint(\"## For a sample of 3 images and the first 5 labels (of 17)\\n\")\nprint(\"True vectorized labels\")\nprint(np.round(y_valid[0:3,0:7]/1.0,2))\nprint(\"-------------------------------------------------------\")\nprint(\"Predicted vectorized labels\")\nprint(np.round(p_valid[0:3,0:7],2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"a143949ef767c0d16e435453a5b4bdd7cd89ac5c"},"cell_type":"code","source":"# Print validation accuracy\nprint(fbeta_score(y_valid, np.array(p_valid) > 0.2, beta=2, average='samples'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"deda796d330570c612d191598d98ad9aa032ec09"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}