{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"a20668ea-6a7f-0ddc-1837-074b09e7707c"},"source":"As far as I concerned, this game is a muti-classify problem.\nI will solve in this way:\n\n 1.  analyse train pictures and train.csv;\n 2. use CNN and CV train model;\n 3. pridict test labels."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c32dbbc7-88f5-2a6e-4ef2-19899c255ca0"},"outputs":[],"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6a5dfbb2-1e59-ded7-ac1d-cb61220ecb91"},"outputs":[],"source":"for f in os.listdir('../input'):\n    if not os.path.isdir('../input/' + f):\n        print(f.ljust(30) + str(round(os.path.getsize('../input/' + f) / 1000000, 2)) + 'MB')\n    else:\n        sizes = [os.path.getsize('../input/'+f+'/'+x)/1000000 for x in os.listdir('../input/' + f)]\n        print(f.ljust(30) + str(round(sum(sizes), 2)) + 'MB' + ' ({} files)'.format(len(sizes)))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f48facb8-16c0-22b4-3259-46564291363d"},"outputs":[],"source":"df_train = pd.read_csv('../input/train.csv')\n\ndf_train.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"a5c4a93f-2313-ef93-b647-5a12a3a8a6f2"},"source":"we can see the labels in  df_train, has **3 rules**:\n\n 1. every label should has a atmospheric label, as 'haze ,'clear','cloudy','partly_cloudy ',if not ,this label would be a noise.\n\n 2. when atmospheric label is 'cloudy', there is no other lable, if not, as a noise.\n\n 3. besides atmospheric labels, the left labels has some relationship, such as 'road' and 'habitation'\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"3f0869a2-ffac-f415-1b0c-361277c45c05"},"source":"First, let's deal with a original data, dorp out noise data."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0b964f6f-3adf-c4c1-c2c2-cad0c718107e"},"outputs":[],"source":"train_labels = df_train['tags'].apply(lambda x: x.split(' '))\ntrain_length=len(train_labels)\ntrain_labels\n             "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0ef0bdcb-0e83-0e61-88c5-1813f162874c"},"outputs":[],"source":"flatten = lambda l: [item for sublist in l for item in sublist]\n\nlabels17 = list(set(flatten([l.split(' ') for l in df_train['tags'].values])))\nlabels17\n\n\n\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a6d0de7e-76a1-028c-121c-a59e5274c1e2"},"outputs":[],"source":"atmos_label=['haze','clear','cloudy','partly_cloudy']\ncommon_label=[]\nfor i in range(len(atmos_label)):\n    for j in range(len(labels17)):\n        if labels17[j]!=atmos_label[i]:\n            common_label.append(labels17[j])\ncommon_label=np.array(common_label)\ncommon_label=np.unique(common_label)\ncommon_label=list(common_label)\ncommon_label\nlen(train_labels[1])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"034d2fad-b25c-80ad-34d9-0a7e1b6ba25f"},"outputs":[],"source":"valible_train_haze=[]\nvalible_train_clear=[]\nvalible_train_cloudy=[]\nvalible_train_partly_cloudy=[]\nfor i in range(train_length):\n    for j in range(len(train_labels[i])):\n        if train_labels[i][j] == atmos_label[0]:\n            valible_train_haze.append(i)\n        elif train_labels[i][j] == atmos_label[1]:\n             valible_train_clear.append(i)\n        elif train_labels[i][j] == atmos_label[2]:   \n             valible_train_cloudy.append(i)\n        elif train_labels[i][j] == atmos_label[3]: \n              valible_train_partly_cloudy.append(i)\nprint(len(valible_train_haze))\nprint(len(valible_train_clear))  \nprint(len(valible_train_cloudy))\nprint(len(valible_train_partly_cloudy))  "},{"cell_type":"markdown","metadata":{"_cell_guid":"7478e43a-0057-b18f-1c3c-44f1c42aeace"},"source":"now we split train data into four parts according atmospheric label,\n\n\nthe number is HAZE:2695 CLEAR:28203 CLOUDY:2330 PARTLY_CLOUDY:7251.\n\n\nif the the label is  'CLOUDY', there should common label, drop out noise. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1e3f8a9e-993f-481d-ff35-7640f50855a9"},"outputs":[],"source":"valible_train_cloudy_dropout=[]\nprint(len(valible_train_cloudy))\nfor i in valible_train_cloudy:\n    if(len(train_labels[i]))>1:\n       continue\n    else:\n       valible_train_cloudy_dropout.append(i)\nprint(len(valible_train_cloudy_dropout))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3440b13c-491f-776e-98c2-dc42a7f476a2"},"outputs":[],"source":"import gc\n\nimport keras as k\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\n\nimport cv2\nfrom tqdm import tqdm\n\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.cross_validation import KFold\nfrom sklearn.metrics import fbeta_score\nimport time"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"98544fd2-a5fb-cae4-556c-4c2058e322c0"},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1e53353-a5ef-6373-6726-4db26b2df195"},"outputs":[],"source":"\nfrom collections import Counter, defaultdict\ncounts = defaultdict(int)\nfor l in train_labels:\n    for l2 in l:\n        counts[l2] += 1\ncounts"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"aba99dc5-0933-e601-1138-9a1f647d3eb9"},"outputs":[],"source":"x_train = []\nx_test = []\ny_train = []\n\ndf_train = pd.read_csv('../input/train.csv')\ndf_test = pd.read_csv('../input/sample_submission.csv')\n\n\nfor f, tags in tqdm(df_train.values[0:10000], miniters=1000):\n    img = cv2.imread('../input/train-jpg/{}.jpg'.format(f))\n    targets = np.zeros(4)\n    for t in tags.split(' '):\n            if t in atmos_label:\n                num_index=atmos_label.index(t)\n                targets[num_index] = 1 \n    x_train.append(cv2.resize(img, (32, 32)))\n    y_train.append(targets)\n    \ntargets\n\nfor f, tags in tqdm(df_test.values[0:10000], miniters=1000):\n    img = cv2.imread('../input/test-jpg-v2/{}.jpg'.format(f))\n    x_test.append(cv2.resize(img, (32, 32)))\n    \ny_train = np.array(y_train, np.uint8)\nx_train = np.array(x_train, np.float32) / 255.\nx_test  = np.array(x_test, np.float32) / 255.\n\nprint(x_train.shape)\nprint(y_train.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"70bd4bb6-8e79-2588-a223-db662853df76"},"outputs":[],"source":"from keras.layers import BatchNormalization\n\nnfolds = 2\n\nnum_fold = 0\nsum_score = 0\n\nyfull_test = []\nyfull_train =[]\n\nkf = KFold(len(y_train), n_folds=nfolds, shuffle=True, random_state=1)\n\nfor train_index, test_index in kf:\n        start_time_model_fitting = time.time()\n        \n        X_train = x_train[train_index]\n        Y_train = y_train[train_index]\n        X_valid = x_train[test_index]\n        Y_valid = y_train[test_index]\n\n        num_fold += 1\n        print('Start KFold number {} from {}'.format(num_fold, nfolds))\n        print('Split train: ', len(X_train), len(Y_train))\n        print('Split valid: ', len(X_valid), len(Y_valid))\n        \n        kfold_weights_path = os.path.join('', 'weights_kfold_' + str(num_fold) + '.h5')\n        \n        model = Sequential()\n        model.add(Conv2D(32, 3, 3, activation='relu', input_shape=(32, 32, 3)))\n        model.add(Conv2D(32, 3, 3, activation='relu'))\n        BatchNormalization(axis=1)\n        model.add(MaxPooling2D(pool_size=(2, 2)))\n        BatchNormalization(axis=1)\n        model.add(Conv2D(64, 3, 3, activation='relu'))\n        model.add(Conv2D(64, 3, 3, activation='relu'))\n        BatchNormalization(axis=1)\n        model.add(MaxPooling2D(pool_size=(2, 2)))\n        model.add(Flatten())\n        BatchNormalization()\n        model.add(Dense(128, activation='relu'))\n        BatchNormalization()\n        model.add(Dropout(0.5))\n        model.add(Dense(4, activation='softmax'))\n\n        model.compile(loss='categorical_crossentropy', \n                      optimizer='adam',\n                      metrics=['accuracy'])\n        callbacks = [\n            EarlyStopping(monitor='val_loss', patience=2, verbose=0),\n            ModelCheckpoint(kfold_weights_path, monitor='val_loss', save_best_only=True, verbose=0)]\n        \n        model.fit(x = X_train, y= Y_train, validation_data=(X_valid, Y_valid),\n                  batch_size=128,verbose=2, nb_epoch=7,callbacks=callbacks,\n                  shuffle=True)\n        \n        if os.path.isfile(kfold_weights_path):\n            model.load_weights(kfold_weights_path)\n        \n        p_valid = model.predict(X_valid, batch_size = 128, verbose=2)\n        print(fbeta_score(Y_valid, np.array(p_valid) > 0.2, beta=2, average='samples'))\n        \n        p_test = model.predict(x_train, batch_size = 128, verbose=2)\n        yfull_train.append(p_test)\n        \n        p_test = model.predict(x_test, batch_size = 128, verbose=2)\n        yfull_test.append(p_test)\n        "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4eaddfe6-d102-746e-49eb-37945e030e1e"},"outputs":[],"source":"yfull_train\nresult_haze=[]\nresult_clear=[]\nresult_cloudy=[]\nresult_partly_cloudy=[]\n\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"20fa2928-89ea-4293-48f8-39db1bca1eba"},"outputs":[],"source":"yfull_test\nresult = np.array(yfull_test[0])\nfor i in range(1, nfolds):\n    result += np.array(yfull_test[i])\nresult /= nfolds\nresult = pd.DataFrame(result, columns = atmos_label)\nresult"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4e8a9952-4ea5-03fe-3ce4-ecdbb986b9f3"},"outputs":[],"source":"from tqdm import tqdm\n\npreds = []\nfor i in tqdm(range(result.shape[0]), miniters=1000):\n    a = result.ix[[i]]\n    \n    a = a.apply(lambda x: x > 0.2, axis=1)\n    a = a.transpose()\n    print(a)\n    a = a.loc[a[i] == True]\n    print(a)\n    ' '.join(list(a.index))\n    preds.append(' '.join(list(a.index)))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8dccf4b6-0827-2edc-1c27-c70f1f851d5d"},"outputs":[],"source":"df_test['tags'] = preds\ndf_test"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"03b1c058-c12b-98df-99ed-7b9b0ca2a038"},"outputs":[],"source":"df_test.to_csv('sample_submission1.csv', index=False)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"492964b4-21f4-061d-869f-65b6e74c53ea"},"outputs":[],"source":"from subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"be965b8c-40c6-d2bf-114b-7704dc7c802f"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}