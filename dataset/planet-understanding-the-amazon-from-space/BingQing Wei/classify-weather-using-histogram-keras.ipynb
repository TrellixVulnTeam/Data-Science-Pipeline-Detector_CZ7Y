{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"25b94b94-b0c3-0181-1ef1-eb9e74fa3998"},"source":"Up-vote if you like this script :)\n\n**About author:**\n\nStill a student.\n\n**Theory:**\n\nThe competition challenges us to classify multi-labeled data.\n\nThrough careful examination, one can find these labels consist of 2 kinds: geological and weather.\n\nWhile geological data relates to the image's texture,  weather is largely a matter of 'color'.\n\nSo the primary goal of this notebook, is to classify 4 weather labels: 'clear', 'haze', 'cloudy', 'partly_cloudy'.\n\n**Limitations:**\n\nOnly used 20,000 samples, because Kaggle server and my own laptop won't allow me to train it in a reasonable short time.\n\nThe length of histograms is limited to 50 and images are scaled down to (50, 50, 3). In implementing your code, you may use original images with larger histogram range.\n\nDidn't use 'primary' weather label. Because, I forgot...and it may harm this simple network's performance.\n\n**Final ouput:**\n\nThe output of this network has shape of (sample-size, 4).\n\nI used a **threshold of 0.25** to select predicted class labels.\n\nThe final output is put into a csv file, 'predict.csv'.\n\n**Words from author:**\n\nIf you find any ways that could improve performance, comment below."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d93c51fb-e3e5-c0ba-8647-c251efd2e2b4"},"outputs":[],"source":"import numpy as np\nimport pandas as pd\nimport pickle\nimport skimage\nfrom skimage import io, transform, exposure\nfrom matplotlib import pyplot as plt\n\nMAX_NUM = 20000"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6be6ef68-b701-39a2-3a44-7136a0c6b1b9"},"outputs":[],"source":"csv = pd.read_csv(r'../input/train_v2.csv')\nser = csv['tags'].map(lambda x : x.split())\ntags = []\nfor s in ser:\n    tags = tags + s\nser = pd.Series(tags)\nser.value_counts()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b5a49ad3-30ea-b07b-195f-51ef7a4ec73f"},"outputs":[],"source":"sel_tags = ['clear', 'haze', 'cloudy', 'partly_cloudy']\ndef is_within_labels(x):\n    x = x.split()\n    for i in x:\n        if i in sel_tags:\n            return True\n    return False\nsel_rows = csv['tags'].map(is_within_labels)\nnp.sum(sel_rows)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"820c6f79-ba26-066a-6bf0-d16cdb7c7685"},"outputs":[],"source":"sel_csv = csv.loc[sel_rows]\ndef trans_tags(x):\n    x = x.split()\n    y = []\n    for i in x:\n        if i in sel_tags:\n            y.append(i)\n    return \" \".join(y)\nsel_csv['tags'] = sel_csv['tags'].map(trans_tags)\nsel_csv = sel_csv.drop(sel_csv.index[MAX_NUM:])\nsel_csv.to_csv(r'weather_sel.csv', index=False)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"853c9bee-6b7e-98a9-b543-f3312fe0df94"},"outputs":[],"source":"#transforming tags\ncounts = 0\ntags_df = pd.DataFrame(columns=sel_tags)\nfor index in sel_csv.index:\n    row = sel_csv.ix[index]\n    new_row = []\n    for i in sel_tags:    \n        if i in row['tags']:\n            new_row.append(int(1))\n        else: new_row.append(int(0))\n    tags_df.loc[index] = new_row\n    counts = counts + 1\n    if(counts == MAX_NUM): break\ntags_df.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"421c4a83-61ee-193b-bd96-d8979ec27d64"},"outputs":[],"source":"#scaling images\nfrom skimage import io\nimport skimage\nfrom skimage.transform import *\nimport matplotlib.pyplot as plt\nimport os"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"73f84092-fee7-da42-6513-c9de05f1120c"},"outputs":[],"source":"ims = []\nbase_path = r'../input/train-jpg'\ncoutns = 0\nfor index in sel_csv.index:\n    im_path = os.path.join(base_path, sel_csv.ix[index]['image_name'] + '.jpg')\n    ti = io.imread(im_path)\n    ti = resize(ti, (50, 50), mode='edge')\n    ims.append(ti)\n    counts = counts + 1\n    if(counts == MAX_NUM): break\nims = np.array(ims)\ntags_arr = tags_df.values[:MAX_NUM]\nprint(ims.shape)\nprint(tags_arr.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ea56573b-6d1f-5d30-35a5-c2045855ef89"},"outputs":[],"source":"#we select train_2 and train_88 to illustrate histogram usages\nti1 = io.imread(r'../input/train-jpg/train_2.jpg')\nti2 = io.imread(r'../input/train-jpg/train_88.jpg')\nti1 = transform.resize(ti1, (50, 50), mode='edge')\nti2 = transform.resize(ti2, (50, 50), mode='edge')\n\nplt.subplot(1,2,1)\nplt.imshow(ti1)\nplt.subplot(1,2,2)\nplt.imshow(ti2)\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b409a939-3b5f-0bca-a9cf-2edc8a697e9f"},"outputs":[],"source":"NUM_OF_BINS = 50\nti1_hist = exposure.histogram(ti1, nbins=NUM_OF_BINS)\nti2_hist = exposure.histogram(ti2, nbins=NUM_OF_BINS)\n\nxaxis = np.arange(NUM_OF_BINS)\nplt.bar(xaxis, +ti1_hist[0], facecolor='#9999ff', edgecolor='white')\nplt.bar(xaxis, -ti2_hist[0], facecolor='#ff9999', edgecolor='white')\n#plt.ylim(-200, +200)\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7676600c-9065-0aeb-8bb1-507166aebd8c"},"outputs":[],"source":"ims_hist = []\nfor im in ims:\n    ims_hist.append(exposure.histogram(im, nbins=NUM_OF_BINS)[0])\nims_hist = np.array(ims_hist)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"27f2c7ba-5760-35a3-182b-6fe49714f139"},"outputs":[],"source":"#Finally, build the neural network\nimport numpy as np\nimport tensorflow as tf\nimport sklearn\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Input, Activation, Flatten, Lambda, Dropout\nfrom keras.layers.convolutional import Conv2D, MaxPooling2D\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom keras import optimizers, regularizers, initializers, metrics, activations, losses\nimport keras\nimport pandas as pd\n\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nsession = tf.Session(config=config)\n\ndata = ims_hist\ntarget = tags_arr\n\nprint(data.shape)\nprint(target.shape)\n\nseed = 3\nnp.random.seed(seed=seed)\nbatch_size = 10\n\ndef threshold_binarize(x, threshold):\n    ge = tf.greater_equal(x, tf.constant(threshold))\n    y = tf.where(ge, x=tf.ones_like(x), y=tf.zeros_like(x))\n    return y\n\ndef threshold_metrics(y_true, y_pred):\n    y_pred = threshold_binarize(y_pred, threshold=0.25)\n    return metrics.binary_accuracy(y_true, y_pred)\n\ndef pre_train(X_train, hidden_layers,\n              decoder_activation='sigmoid',\n              batch_size=100, pre_train_epoch=1):\n\n    for i in np.arange(len(hidden_layers)):\n        print('Pre-training the layer: Input {} -> Output {}'.format(hidden_layers[i].input_shape, hidden_layers[i].output_shape))\n\n        # Create AE and training\n        encoded = hidden_layers[i]\n        decoded = Dense(encoded.input_shape[1], activation=decoder_activation)\n        ae_model = Sequential()\n        ae_model.add(encoded)\n        ae_model.add(decoded)\n\n        ae_model.compile(loss='mse', optimizer='rmsprop')\n        if i == 0:\n            # Train the simple autoencoder\n            ae_model.fit(X_train, X_train,\n                         batch_size=batch_size, nb_epoch=pre_train_epoch, verbose=False)\n        else:\n            pre_model = Sequential()\n            for i in range(i):\n                pre_model.add(hidden_layers[i])\n\n            X_data = pre_model.predict(X_train)\n            ae_model.fit(X_train, X_data,\n                         batch_size=batch_size, nb_epoch=pre_train_epoch, verbose=False)\n\ninput_shape = data.shape[1:]\nlayers = [Dense(1000, activation='tanh',\n                input_shape=input_shape,\n                kernel_initializer=initializers.RandomNormal(stddev=10)),\n          Dense(100, activation='tanh',\n                kernel_initializer=initializers.RandomNormal(stddev=10)),\n          Dense(4, activation='sigmoid',\n                kernel_initializer=initializers.RandomNormal(stddev=10))]\n\nmodel = Sequential()\nfor layer in layers:\n    model.add(layer)\n\npre_train(data, layers)\n\nopt = optimizers.Adam(lr=0.01)\nmodel.compile(loss=losses.binary_crossentropy, optimizer=opt, metrics=[threshold_metrics])\n\n#Fit model on training data\nmodel.fit(data, target, batch_size=batch_size, nb_epoch=1, verbose=True)\n\nresult = model.predict(x=data[400:], batch_size=batch_size, verbose=True)\n\npredict = pd.DataFrame(data=result, columns=['clear', 'haze', 'cloudy', 'partly_cloudy'])\ndef binarize(row):\n    row[row >= 0.25] = 1\n    row[row < 0.25] = 0\n    return row\nfor index in predict.index:\n    predict.ix[index] = binarize(predict.ix[index])\npredict.head()\n\npredict.to_csv(r'predict.csv', index=False)"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}