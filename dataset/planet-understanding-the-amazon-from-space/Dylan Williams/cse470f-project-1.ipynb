{"cells":[{"metadata":{"_uuid":"e97f90942a751fd14b57f2651bc16df17ba4726f"},"cell_type":"markdown","source":"WONT BE GRADED UNTIL THE  10TH"},{"metadata":{"trusted":true,"_uuid":"04c0fcaa1371b328a68acf502479897bc78517a6"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"520837bee8259be7688a52add0e441c4f1be2cda"},"cell_type":"markdown","source":"Last Layer Activation should be sigmoid. \nLoss Function should be binary_crossentropy"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport gc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\npal = sns.color_palette()\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\nfrom torch.utils.data.dataset import Dataset\n\nprint('# File sizes')\nfor f in os.listdir('../input'):\n    if not os.path.isdir('../input/' + f):\n        print(f.ljust(30) + str(round(os.path.getsize('../input/' + f) / 1000000, 2)) + 'MB')\n    else:\n        sizes = [os.path.getsize('../input/'+f+'/'+x)/1000000 for x in os.listdir('../input/' + f)]\n        print(f.ljust(30) + str(round(sum(sizes), 2)) + 'MB' + ' ({} files)'.format(len(sizes)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1789767e1169a29590ebd30456d5a16fc39856c5"},"cell_type":"code","source":"from torchvision import transforms\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom PIL import *\nfrom torch import from_numpy\nimport torch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6c767682cda5948d5facb7b1ff4e0b79a3b8a570"},"cell_type":"code","source":"IMG_PATH = '../input/planet-understanding-the-amazon-from-space/train-jpg/'\nIMG_EXT = '.jpg'\nTRAIN_DATA = '../input/planet-understanding-the-amazon-from-space/train_v2.csv'\nTEST_DATA = '../'\nTEST_PATH = '../input/planet-understanding-the-amazon-from-space/test-jpg-additional/'\nTEST_DATA = '../input/planet-understanding-the-amazon-from-space/sample_submission_v2.csv'\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f8a3c194b786f657d68c58afccf6e7b821f5e090"},"cell_type":"code","source":"class KaggleAmazonDataset(Dataset):\n    \"\"\"Dataset wrapping images and target labels for Kaggle - Planet Amazon from Space competition.\n\n    Arguments:\n        A CSV file path\n        Path to image folder\n        Extension of images\n        PIL transforms\n    \"\"\"\n\n    def __init__(self, csv_path, img_path, img_ext, transform=None):\n    \n        tmp_df = pd.read_csv(csv_path)\n        assert tmp_df['image_name'].apply(lambda x: os.path.isfile(img_path + x + img_ext)).all(), \\\n\"Some images referenced in the CSV file were not found\"\n        \n        self.mlb = MultiLabelBinarizer()\n        self.img_path = img_path\n        self.img_ext = img_ext\n        self.transform = transform\n\n        self.X_train = tmp_df['image_name']\n        self.y_train = self.mlb.fit_transform(tmp_df['tags'].str.split()).astype(np.float32)\n\n    def __getitem__(self, index):\n        img = Image.open(self.img_path + self.X_train[index] + self.img_ext)\n        img = img.convert('RGB')\n        if self.transform is not None:\n            img = self.transform(img)\n        \n        label = torch.from_numpy(self.y_train[index])\n        return img, label\n\n    def __len__(self):\n        return len(self.X_train.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4fb6a7011794b260f7adbc69f1430bfc975532e9"},"cell_type":"code","source":"normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                     std=[0.229, 0.224, 0.225])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"754898463d24aa31f7388c584633e6a9e7ef9430"},"cell_type":"code","source":"#got this from https://gist.github.com/Fuchai/12f2321e6c8fa53058f5eb23aeddb6ab. Helps give me validation data because this dataset comes with none\nclass GenHelper(Dataset):\n    def __init__(self, mother, length, mapping):\n        # here is a mapping from this index to the mother ds index\n        self.mapping=mapping\n        self.length=length\n        self.mother=mother\n\n    def __getitem__(self, index):\n        return self.mother[self.mapping[index]]\n\n    def __len__(self):\n        return self.length\n\n\ndef train_valid_split(ds, split_fold=10, random_seed=None):\n    '''\n    This is a pytorch generic function that takes a data.Dataset object and splits it to validation and training\n    efficiently.\n    :return:\n    '''\n    if random_seed!=None:\n        np.random.seed(random_seed)\n\n    dslen=len(ds)\n    indices= list(range(dslen))\n    valid_size=dslen//split_fold\n    np.random.shuffle(indices)\n    train_mapping=indices[valid_size:]\n    valid_mapping=indices[:valid_size]\n    train=GenHelper(ds, dslen - valid_size, train_mapping)\n    valid=GenHelper(ds, valid_size, valid_mapping)\n\n    return train, valid","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"69a68e43a99d8ae7eb197c994283bc927a1b3660"},"cell_type":"code","source":"transformations = transforms.Compose([transforms.Resize(32),transforms.ToTensor()])\ntransformation_augmented = transforms.Compose([transforms.RandomResizedCrop(224),transforms.ToTensor(),normalize])\ntransformation_raw = transforms.Compose([transforms.Resize(224),transforms.ToTensor(),normalize])\n\ndset_whole = KaggleAmazonDataset(TRAIN_DATA,IMG_PATH,IMG_EXT,transformations)\ntraining_data, valid_data = train_valid_split(dset_whole)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"15e4055f4393afe236555f1e5da279724666ba2d"},"cell_type":"code","source":"im, target = valid_data[0]\ntarget","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3975bba6cc5620b6317d42924548c99cabd67f3e"},"cell_type":"code","source":"def plot_sample(im, target):\n    plt.imshow(im.numpy().transpose(1,2,0))\n    #TODO show multple names\n    plt.text(0,0, \"Forest\\nRiver\", verticalalignment='top', color='yellow')\n    \nplot_sample(im, target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"30c21fc5964a41e1ec9af4308b5785e7c5fed869"},"cell_type":"code","source":"import torch","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('../input/planet-understanding-the-amazon-from-space/train_v2.csv')\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"05256b1436afc335cd944844f6025d5fc751c2d0"},"cell_type":"markdown","source":"Here we can see the frequency of the labels in the dataset. As we can tell, a majority of the images should have a primary tag included. This makes sense since we are looking at images of a rainforest."},{"metadata":{"trusted":true,"_uuid":"ce4f9ac74603e594d1b9c8146d3c2a0244a0612a"},"cell_type":"code","source":"labels = df_train['tags'].apply(lambda x: x.split(' '))\nfrom collections import Counter, defaultdict\ncounts = defaultdict(int)\nfor l in labels:\n    for l2 in l:\n        counts[l2] += 1\n\ndata=[go.Bar(x=list(counts.keys()), y=list(counts.values()))]\nlayout=dict(height=800, width=800, title='Distribution of training labels')\nfig=dict(data=data, layout=layout)\npy.iplot(data, filename='train-label-dist')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eab70791b6053c2bfa5c07799d33c1674865cadf"},"cell_type":"markdown","source":"Now we should use a co-occurence matrix from Seaborn to see how often what tags appear with other tags."},{"metadata":{"trusted":true,"_uuid":"b6afa89533b36b30d6f6e655b7791bfb59136d01","scrolled":true},"cell_type":"code","source":"com = np.zeros([len(counts)]*2)\nfor i, l in enumerate(list(counts.keys())):\n    for i2, l2 in enumerate(list(counts.keys())):\n        c = 0\n        cy = 0\n        for row in labels.values:\n            if l in row:\n                c += 1\n                if l2 in row: cy += 1\n        com[i, i2] = cy / c\n\ndata=[go.Heatmap(z=com, x=list(counts.keys()), y=list(counts.keys()))]\nlayout=go.Layout(height=800, width=800, title='Co-occurence matrix of training labels')\nfig=dict(data=data, layout=layout)\npy.iplot(data, filename='train-com')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ddd14f15db2ff3dcf4a1912309e72b44e361d77c"},"cell_type":"markdown","source":"It is important to note that we should never see the cloudy tag along with another tag. Hopefully we can at least get the CNN to not do this. This matrix also confirms that the primary tag will be used very often in conjunction with other tags. Maybe we can take this into account while training to make training more efficient?"},{"metadata":{"trusted":true,"_uuid":"8e8184104706746f54de9450484e090e03df4be8"},"cell_type":"markdown","source":"Now we set up our CNN."},{"metadata":{"trusted":true,"_uuid":"9dec5f91a8d2bacffbc878fadd95cb603b3f7fb1"},"cell_type":"code","source":"import skimage.io\n\nnew_style = {'grid': False}\nplt.rc('axes', **new_style)\n_, ax = plt.subplots(3, 3, sharex='col', sharey='row', figsize=(20, 20))\ni = 0\nfor f, l in df_train[:9].values:\n    img = skimage.io.imread('../input/planet-understanding-the-amazon-from-space/train-jpg/{}.jpg'.format(f))\n    ax[i // 3, i % 3].imshow(img)\n    ax[i // 3, i % 3].set_title('{} - {}'.format(f, l))\n    #ax[i // 4, i % 4].show()\n    i += 1\n    \nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2780ab3fe9fe92df89defb73985ed74c0f3e687e"},"cell_type":"code","source":"import torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torchvision.transforms import ToTensor\nfrom torch.utils.data import DataLoader\nfrom IPython.core.debugger import set_trace","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"378128de1140dce8c67e99ea78107a2ce8636937"},"cell_type":"code","source":"BATCH_SIZE = 256\nLEARNING_RATE = 0.01\nWORKERS = 4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bcf2d11e6952f0346bd59979f6b9fc6b9518496e"},"cell_type":"code","source":"train_loader = DataLoader(training_data, batch_size = BATCH_SIZE, num_workers=WORKERS, shuffle = True)\ntest_loader = DataLoader(valid_data, batch_size = BATCH_SIZE, num_workers=WORKERS, shuffle = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2644740c15d2a0ca8c3beec681ec5e606cd99ff9"},"cell_type":"code","source":"import torch.cuda\ntorch.cuda.is_available()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8c3336e93a48fb2787501cc7dd685d9bfc86f0ca"},"cell_type":"code","source":"if torch.cuda.is_available():\n    def togpu(x):\n        return x.cuda()\n    def tocpu(x):\n        return x.cpu()\nelse:\n    def togpu(x):\n        return x\n    def tocpu(x):\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5fab2f94fbf40baccc396f104c624d149a5daba0"},"cell_type":"code","source":"class SimpleCNN(nn.Module):\n    def __init__(self, shape=(256,256), num_classes=17):\n        super().__init__()\n        \n        #num_inputs = np.product(shape)\n        self.layer1 = nn.Conv2d(3, 32, kernel_size=3) \n        self.layer2 = nn.Conv2d(32, 64, kernel_size=3)\n        self.conv2_drop = nn.Dropout2d()\n        self.layer3 = nn.Linear(2304, 256)\n        self.layer4 = nn.Linear(256, 17)\n        \n    \n    def forward(self, x):        \n        \n        #set_trace()\n        x = self.layer1(x)\n        x = F.max_pool2d(x, 2)\n        x = F.relu(x)\n        \n       \n        \n        x = self.layer2(x)\n        x = self.conv2_drop(x)\n        x = F.max_pool2d(x, 2)\n        x = F.relu(x)\n\n        \n        # Flatten\n        x = x.view(x.size(0),-1)\n        \n        x = self.layer3(x)\n        x = F.relu(x)\n        \n        x = F.dropout(x, training=self.training)\n        \n        x = self.layer4(x)\n        \n        y = torch.sigmoid(x)\n        \n        return y  # Will learn to treat 'a' as the natural parameters of a multinomial distr. ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"afc3a6fd87fc2064f4739543a8f164b20d8b799a"},"cell_type":"markdown","source":"Training Loop"},{"metadata":{"trusted":true,"_uuid":"4596985ca26e2f26530f562f420803b1e3b96ec0"},"cell_type":"code","source":"net = SimpleCNN(shape=(256,256), num_classes= 17)\nnet = togpu(net)\noptimizer = torch.optim.SGD(params = net.parameters(), lr = LEARNING_RATE, momentum = 0.5)\nstart_epoch = 0\nnum_epochs = 30\nbest_eval_loss = float('inf')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"410354fce896a6915f0abde5821b043a4b164ec6"},"cell_type":"code","source":"import time\nimport tqdm\nimport sys\nimport shutil","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"08b570d0971ad66727f6723465aff946e25d7dc5"},"cell_type":"code","source":"#model = torch.load('../input/cse470f-project-1/simplecnn-checkpoint.pth.tar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"237a5a79ce379d6c86bea03ed134431a9f6747eb"},"cell_type":"code","source":"#model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b1acf7f434bd8e9919c31458fb073fcd6a04a077"},"cell_type":"code","source":"def compute_eval_loss(net, loader):\n    # Evaluate the model\n    with torch.no_grad():\n        eval_loss = 0.0\n        for i, data in tqdm.tqdm(enumerate(loader),\n                                 file = sys.stdout,\n                                 desc='Evaluating',\n                                 total=len(loader),\n                                 leave=False):\n            inputs, labels = data\n            inputs, labels = togpu(inputs), togpu(labels)\n            outputs = net(inputs)               # Predict\n            loss =  F.binary_cross_entropy(outputs, labels)   # Grade / Evaluate\n            eval_loss += loss.item()\n    eval_loss /= len(test_loader)\n    return eval_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f92d7500e59ef1250b43d2667eb639e611c29aa5"},"cell_type":"code","source":"for epoch in tqdm.tnrange(start_epoch, num_epochs):\n    \n    running_loss = 0.0\n    tstart = time.time()\n    \n    # Update the model parameters\n    for i, data in tqdm.tqdm(enumerate(train_loader),\n                             file = sys.stdout,\n                             desc='Updating',\n                             total=len(train_loader), \n                             leave=False):\n        # get the inputs\n        inputs, labels = data\n        \n        # Move them to the GPU\n        inputs = togpu(inputs)\n        labels = togpu(labels)\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = net(inputs)               # Predict\n        loss = F.binary_cross_entropy(outputs, labels)   # Grade / Evaluate\n        loss.backward()                     # Determine how each parameter effected the loss\n        optimizer.step()                    # Update parameters \n\n        # print statistics\n        running_loss += loss.item()\n    running_loss /= len(train_loader)\n    \n\n    eval_loss = compute_eval_loss(net, test_loader)\n    \n    tend = time.time()\n    \n    # Save parameters\n    torch.save(dict(epoch=epoch, \n                         loss=eval_loss,\n                         parameters=net.state_dict(),\n                         optimizer=optimizer.state_dict()),\n                   'simplecnn-checkpoint.pth.tar')\n    \n    if eval_loss < best_eval_loss:\n        best_eval_loss = eval_loss\n        best_epoch = epoch\n        shutil.copyfile('simplecnn-checkpoint.pth.tar', 'simplecnn-best.pth.tar')\n        \n    print(\"Epoch {: 4}   loss: {: 2.5f}  test-loss: {: 2.5}  time: {}\".format(epoch,\n                                                                                running_loss,\n                                                                                eval_loss,\n                                                                                tend-tstart))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"26868b1e369356ade382336f7f7dc0363b13af79"},"cell_type":"code","source":"predictions = np.zeros((len(valid_data),17))\ntargets = np.zeros((len(valid_data),17))\n\nfor i  in tqdm.tnrange(len(valid_data)):\n    for j in range(17): \n        set_trace()\n        x, t = valid_data[i]\n        p = tocpu(net(togpu(x[None,...]))).argmax(1)[0]\n        predictions[i][j] = p\n        targets[i][j] = t[i][j]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ff2b3c75131e587bd7330418774ba99fa325cdb2"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"57f85390eb158f0cd41f863da78856b5d9b77e61"},"cell_type":"code","source":"%debug","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f110de7ea17e1a95f59ee43ec3a7c41cbce32f01"},"cell_type":"code","source":"from sklearn.metrics import classification_report","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a461ce095c5f9530ddcd0df9f8e078e819bacb84"},"cell_type":"code","source":"print(classification_report(targets, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7c7071485a879adfb88fced91e0f4fad8d6bab97"},"cell_type":"code","source":"%debug","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"28d285cffdeef6024d873e1ab2295f48845c4790"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}