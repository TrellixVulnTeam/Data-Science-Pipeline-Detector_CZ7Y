{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"01068536-7699-64a9-c695-20023c541de2"},"source":"# Who can save the rainforest?\n\nIn this competition we are given a **multilabel classification** problem, where we have to decide, given an image, which labels belong to it. From the evaluation section of the competition:\n\nFor each image listed in the test set, predict a space-delimited list of tags which you believe are associated with the image. There are 17 possible tags: agriculture, artisinal_mine, bare_ground, blooming, blow_down, clear, cloudy, conventional_mine, cultivation, habitation, haze, partly_cloudy, primary, road, selective_logging, slash_burn, water.\n\nIn this notebook we will:\n\n* generate a fun bernoulli trial sample submission\n* look at the actual images to get a first impression of the data\n* cluster the images according to their pixel intensities to find potentially formed groups\n* compute standard NDVI values for images and rank them by this index. \n\nA standard approach to multilabel classification is to learn as many OVA (one vs all) models as there are distinct labels and then assign labels by the classifier output of each of the models, we'll get to that later.\n\n**If you like it, please upvote this :)**\n\nLet's dive right into the data."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3e2d6bad-160f-2466-857f-a23bac3252eb"},"outputs":[],"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom subprocess import check_output\nimport matplotlib.pyplot as plt\nfrom scipy.stats import bernoulli\nimport seaborn as sns\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n#print(check_output([\"ls\", \"../input/train-jpg\"]).decode(\"utf8\"))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0b35a5f2-9aa1-01e0-62fe-f7fbcd259324"},"outputs":[],"source":"sample = pd.read_csv('../input/sample_submission_v2.csv')\nprint(sample.shape)\nsample.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"28193050-bd95-5ecc-0acd-e407c58b5b19"},"outputs":[],"source":"df = pd.read_csv('../input/train_v2.csv')\ndf.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"08955642-f7fe-3c1c-68a2-88eb1035baff"},"source":"So, we are given around 40.000 training images."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d4bcd9aa-5156-011b-74d7-084820f6df8e"},"outputs":[],"source":"df.shape"},{"cell_type":"markdown","metadata":{"_cell_guid":"d3d7c9cd-ef05-ff3f-3aee-2bcbe097777e"},"source":"# Tag counts\n\nFirst, let's count all of the tags."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1eb1b2f8-c1ae-c4bc-102a-9db91c60d71f"},"outputs":[],"source":"all_tags = [item for sublist in list(df['tags'].apply(lambda row: row.split(\" \")).values) for item in sublist]\nprint('total of {} non-unique tags in all training images'.format(len(all_tags)))\nprint('average number of labels per image {}'.format(1.0*len(all_tags)/df.shape[0]))"},{"cell_type":"markdown","metadata":{"_cell_guid":"4b328d0d-9b12-4b96-0471-e1dd31242985"},"source":"Now, lets do the actual counting. We're going to use pandas dataframe groupby method for that. In total, as we found in the description above, there are 17 distinct tags."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8e499bc7-c231-437d-9bf5-c977c3d901d2"},"outputs":[],"source":"tags_counted_and_sorted = pd.DataFrame({'tag': all_tags}).groupby('tag').size().reset_index().sort_values(0, ascending=False)\ntags_counted_and_sorted.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"17b7ae1f-6d0a-06f0-9b06-a4b15f57bb5f"},"source":"There are only a few tags, that occur very often in the data:\n\n* primary\n* clear\n* agriculture\n* road\n* and water."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"da7d4cb0-9342-1b01-6544-9fcfecb630d2"},"outputs":[],"source":"tags_counted_and_sorted.plot.barh(x='tag', y=0, figsize=(12,8))"},{"cell_type":"markdown","metadata":{"_cell_guid":"d917a637-7fc7-af23-9ead-4e7ff2d4560e"},"source":"From this tag distribution it will most likely be relatively easy to predict the often occuring tags and comparatively very hard to get the low sampled tags correct."},{"cell_type":"markdown","metadata":{"_cell_guid":"9a65e304-4284-30ba-3ba6-2e4a289323ee"},"source":"# Submission from training tag counts\n\nLet's do something fun. We'll take the training tag distribution and sample from it as a prior for our test data. For that we will configure a bernoulli distribution for each sample with the observed training frequency and sample from that for each test image. With that we'll generate a submission without ever looking at the images. :)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"288bded4-c859-4404-9974-6dbdfa89570e"},"outputs":[],"source":"tag_probas = tags_counted_and_sorted[0].values/tags_counted_and_sorted[0].values.sum()\nindicators = np.hstack([bernoulli.rvs(p, 0, sample.shape[0]).reshape(sample.shape[0], 1) for p in tag_probas])\nindicators = np.array(indicators)\nindicators.shape"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"424a7d1a-801a-654c-0af8-4b9c502fedfa"},"outputs":[],"source":"indicators[:10,:]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d2bed920-9a5c-275d-f262-9d3d9d7052b9"},"outputs":[],"source":"sorted_tags = tags_counted_and_sorted['tag'].values\nall_test_tags = []\nfor index in range(indicators.shape[0]):\n    all_test_tags.append(' '.join(list(sorted_tags[np.where(indicators[index, :] == 1)[0]])))\nlen(all_test_tags)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2dfd33fb-261b-282d-dd2b-02a8aca8ef6a"},"outputs":[],"source":"sample['tags'] = all_test_tags\nsample.head()\nsample.to_csv('bernoulli_submission.csv', index=False)"},{"cell_type":"markdown","metadata":{"_cell_guid":"678d5bea-eba1-409e-73c0-1cad672042f5"},"source":"Ok, enough for the fun part, lets get serious :)."},{"cell_type":"markdown","metadata":{"_cell_guid":"b3ccdd2d-51c5-1ba8-79ec-21ddd4477c8c"},"source":"# Looking at the actual images"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3695b343-5ce4-fe6b-3eee-ce1d788f7383"},"outputs":[],"source":"from glob import glob\nimage_paths = sorted(glob('../input/train-jpg/*.jpg'))[0:1000]\nimage_names = list(map(lambda row: row.split(\"/\")[-1][:-4], image_paths))\nimage_names[0:10]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c8ca0894-ca11-70a3-952d-45e84e515b50"},"outputs":[],"source":"plt.figure(figsize=(12,8))\nfor i in range(6):\n    plt.subplot(2,3,i+1)\n    plt.imshow(plt.imread(image_paths[i]))\n    plt.title(str(df[df.image_name == image_names[i]].tags.values))"},{"cell_type":"markdown","metadata":{"_cell_guid":"91b7640a-e004-a986-c825-05089f536e99"},"source":"It seems, that all of the images are of the same size, which would make preprocessing them much easier."},{"cell_type":"markdown","metadata":{"_cell_guid":"cedbd3a2-4258-f57f-a7e4-3aa748fd668a"},"source":"# Building multilabel models with the actual images\n\nNow, we are going to build multilabel models in a classic OVR setting, where we train one model for each tag and that model is trained to distinguish between that tag and all the other tags.\n\nThere are a few things going on here:\n\n* encoding of tags as multi label indicators\n* flattening, encoding and scaling of images\n* fitting OVR models\n* evaluating F2 scores.\n\nWe are training the models on only a subset of samples to save runtime for the kernel and are rescaling the images quite heavily to 20x20x4 = 1600 dimensions."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7ae6519c-2992-e3d9-e0d6-98de7e8f9362"},"outputs":[],"source":"from sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.preprocessing import MultiLabelBinarizer, MinMaxScaler\nfrom sklearn.metrics import fbeta_score, precision_score, make_scorer, average_precision_score\nimport cv2\nimport warnings\n\nn_samples = 5000\nrescaled_dim = 20\n\ndf['split_tags'] = df['tags'].map(lambda row: row.split(\" \"))\nlb = MultiLabelBinarizer()\ny = lb.fit_transform(df['split_tags'])\ny = y[:n_samples]\nX = np.squeeze(np.array([cv2.resize(plt.imread('../input/train-jpg/{}.jpg'.format(name)), (rescaled_dim, rescaled_dim), cv2.INTER_LINEAR).reshape(1, -1) for name in df.head(n_samples)['image_name'].values]))\nX = MinMaxScaler().fit_transform(X)\n\nprint(X.shape, y.shape, lb.classes_)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n\nclf = OneVsRestClassifier(LogisticRegression(C=10, penalty='l2'))\n\nwith warnings.catch_warnings():\n    warnings.simplefilter('ignore')\n    clf.fit(X_train, y_train)\n\nscore = fbeta_score(y_test, clf.predict(X_test), beta=2, average=None)\navg_sample_score = fbeta_score(y_test, clf.predict(X_test), beta=2, average='samples')\nprint('Average F2 test score {}'.format(avg_sample_score))\nprint('F2 test scores per tag:')\n[(lb.classes_[l], score[l]) for l in score.argsort()[::-1]]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"db00b3e0-3849-aa72-6a47-1d37f16e8af8"},"outputs":[],"source":"X_sub = np.squeeze(np.array([cv2.resize(plt.imread('../input/test-jpg/{}.jpg'.format(name)), (rescaled_dim, rescaled_dim), cv2.INTER_LINEAR).reshape(1, -1) for name in sample['image_name'].values]))\nX_sub = MinMaxScaler().fit_transform(X_sub)\nX_sub.shape"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fecd9f3d-989b-2627-9352-09e812a661f1"},"outputs":[],"source":"y_sub = clf.predict(X_sub)\nall_test_tags = []\nfor index in range(y_sub.shape[0]):\n    all_test_tags.append(' '.join(list(lb.classes_[np.where(y_sub[index, :] == 1)[0]])))\nall_test_tags[0:20]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3c9021bc-0cb6-fda6-59ba-dc5740b5844e"},"outputs":[],"source":"test_imgs = [plt.imread('../input/test-jpg/{}.jpg'.format(name)) for name in sample.head(20)['image_name'].values]\nplt.imshow(test_imgs[7])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b111d94c-ef52-1832-a7d8-d2a7073a531c"},"outputs":[],"source":"sample['tags'] = all_test_tags\nsample.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"308f57e6-e53a-a16a-d3d8-0227fb1fdc14"},"outputs":[],"source":"sample.to_csv('ovr_f2_{}.csv'.format(avg_sample_score), index=False)"},{"cell_type":"markdown","metadata":{"_cell_guid":"c00595fc-2096-3356-e195-0660bfc31c97"},"source":"# Image clustering\n\nWithout having to look at all of the images, a common technique is to cluster images by their native representation (pixel intensities) or some encoded version of it, e.g. by computing activations of a vision-based neural network.\n\nFor our purpose we will just use the pixel intensities and compute pairwise distances."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"29e139a7-af08-f405-2284-1b0aae8c9255"},"outputs":[],"source":"import cv2\n\nn_imgs = 600\n\nall_imgs = []\n\nfor i in range(n_imgs):\n    img = plt.imread(image_paths[i])\n    img = cv2.resize(img, (100, 100), cv2.INTER_LINEAR).astype('float')\n#    img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY).astype('float')\n    img = cv2.normalize(img, None, 0.0, 1.0, cv2.NORM_MINMAX)\n    img = img.reshape(1, -1)\n    all_imgs.append(img)\n\nimg_mat = np.vstack(all_imgs)\nimg_mat.shape"},{"cell_type":"markdown","metadata":{"_cell_guid":"7224bb6a-2ab0-6ca8-d7b5-424c80599c2e"},"source":"We can see frmo the line spectrum in the clustermap, that there are a few images that are very **dissimilar** to all other images by using the pixel intensities.\n\nAlso there is a block-like structure to it, maybe that already tells us something about the tags themselves."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"189431de-a11b-acd3-635f-e0008405e7bc"},"outputs":[],"source":"from scipy.spatial.distance import pdist, squareform\n\nsq_dists = squareform(pdist(img_mat))\nprint(sq_dists.shape)\nsns.clustermap(\n    sq_dists,\n    figsize=(12,12),\n    cmap=plt.get_cmap('viridis')\n)"},{"cell_type":"markdown","metadata":{"_cell_guid":"5e89343f-bcc9-c8d2-efdb-53f54867dfa2"},"source":"Let's have a look at t-SNE embedding of the images to get a nice visualization of the distances in three dimensions.\nIt looks like there are a few outliers, which we already suspected from the clustermap view above. Other than that, the 3d embedding doesn't tell us much."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c4e7dafd-d90d-e28e-c372-ba5fc182b81d"},"outputs":[],"source":"%matplotlib inline\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a3b7cab3-b160-26ed-ccd7-7c4363bcc2b2"},"outputs":[],"source":"from sklearn.manifold import TSNE\ntsne = TSNE(\n    n_components=3,\n    init='random', # pca\n    random_state=101,\n    method='barnes_hut',\n    n_iter=500,\n    verbose=2\n).fit_transform(img_mat)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cb552a6d-1944-4e01-4ee1-7bcd154e0e6b"},"outputs":[],"source":"trace1 = go.Scatter3d(\n    x=tsne[:,0],\n    y=tsne[:,1],\n    z=tsne[:,2],\n    mode='markers',\n    marker=dict(\n        sizemode='diameter',\n        #color = preprocessing.LabelEncoder().fit_transform(all_image_types),\n        #colorscale = 'Portland',\n        #colorbar = dict(title = 'images'),\n        line=dict(color='rgb(255, 255, 255)'),\n        opacity=0.9\n    )\n)\n\ndata=[trace1]\nlayout=dict(height=800, width=800, title='3D embedding of images')\nfig=dict(data=data, layout=layout)\npy.iplot(fig, filename='3DBubble')"},{"cell_type":"markdown","metadata":{"_cell_guid":"bf086c67-2699-322a-10cd-f2bbdf5756fa"},"source":"# Tracking down outliers\n\nLet's try to find the **most common image**, as indicated by the average distance to all other images and the **least common image** by the same metric.\n\nAs we expected, the image that has least distance to all others is an image with rainforest on it. The image with the maximal average distance to all others shows only clouds and seems to be a little overexposed."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0a0345fa-3561-a320-4318-df6e6d61098f"},"outputs":[],"source":"mask = np.zeros_like(sq_dists, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# upper triangle of matrix set to np.nan\nsq_dists[np.triu_indices_from(mask)] = np.nan\nsq_dists[0, 0] = np.nan\n\nfig = plt.figure(figsize=(12,8))\n# maximally dissimilar image\nax = fig.add_subplot(1,2,1)\nmaximally_dissimilar_image_idx = np.nanargmax(np.nanmean(sq_dists, axis=1))\nplt.imshow(plt.imread(image_paths[maximally_dissimilar_image_idx]))\nplt.title('maximally dissimilar')\n\n# maximally similar image\nax = fig.add_subplot(1,2,2)\nmaximally_similar_image_idx = np.nanargmin(np.nanmean(sq_dists, axis=1))\nplt.imshow(plt.imread(image_paths[maximally_similar_image_idx]))\nplt.title('maximally similar')\n\n# # now compute the mean image\n#ax = fig.add_subplot(1,3,3)\n#mean_img = gray_imgs_mat.mean(axis=0).reshape(rescaled_dim, rescaled_dim, 3)\n#plt.imshow(cv2.normalize(mean_img, None, 0.0, 1.0, cv2.NORM_MINMAX))\n#plt.title('mean image')"},{"cell_type":"markdown","metadata":{"_cell_guid":"73f1ffe8-d95b-d520-8f74-085ad8d38480"},"source":"## Image scatter plot\n\nThis scatter plot shows a 2D embedding but with the actual images overlaid."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"eefd07e1-a896-481f-d9cd-37dc570e116e"},"outputs":[],"source":"from sklearn.manifold import TSNE\ntsne = TSNE(\n    n_components=2,\n    init='random', # pca\n    random_state=101,\n    method='barnes_hut',\n    n_iter=500,\n    verbose=2\n).fit_transform(img_mat)"},{"cell_type":"markdown","metadata":{"_cell_guid":"9e01df7c-c3ce-a04b-75cb-9452aaa49445"},"source":"There is the large chunk of pure rainforest images that is present in the data. The tag distribution already hinted in that direction earlier."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ce1a40d1-85c3-3717-815a-bcd6ff580c4d"},"outputs":[],"source":"from matplotlib.offsetbox import OffsetImage, AnnotationBbox\ndef imscatter(x, y, images, ax=None, zoom=0.1):\n    ax = plt.gca()\n    images = [OffsetImage(image, zoom=zoom) for image in images]\n    artists = []\n    for x0, y0, im0 in zip(x, y, images):\n        ab = AnnotationBbox(im0, (x0, y0), xycoords='data', frameon=False)\n        artists.append(ax.add_artist(ab))\n    ax.update_datalim(np.column_stack([x, y]))\n    ax.autoscale()\n    #return artists\n\nnimgs = 500\nplt.figure(figsize=(13,10))\nimscatter(tsne[0:nimgs,0], tsne[0:nimgs,1], [plt.imread(image_paths[i]) for i in range(nimgs)])"},{"cell_type":"markdown","metadata":{"_cell_guid":"3f0a7fb5-1bb1-fee9-59e9-784c08148b0c"},"source":"# GeoTiff Images\n\nIn remote sensing a standard measure to use for determining whether there is vegetation present is [NDVI][1], the \"Normalized Difference Vegetation Index\". There are several weak points to the original index and some extensions of it.\n\nWe're now going to compute the NDVI for a few images and rank them by it to see how well we can identify the amount of vegetation in the image.\n\n  [1]: https://en.wikipedia.org/wiki/Normalized_Difference_Vegetation_Index"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b968abe1-217d-71c9-3b29-12c469464777"},"outputs":[],"source":"from skimage import io\nimage_paths = sorted(glob('../input/train-tif/*.tif'))[0:1000]\nimgs = [io.imread(path) / io.imread(path).max() for path in image_paths]\n#r, g, b, nir = img[:, :, 0], img[:, :, 1], img[:, :, 2], img[:, :, 3]\nndvis = [(img[:,:,3] - img[:,:,0])/((img[:,:,3] + img[:,:,0])) for img in imgs]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b2ecee31-b640-e28f-5c87-d50b43a20df8"},"outputs":[],"source":"plt.figure(figsize=(12,8))\nplt.subplot(121)\nplt.imshow(ndvis[2], cmap='jet')\nplt.colorbar()\nplt.title('NDVI index of cloudy image')\nplt.subplot(122)\nplt.imshow(imgs[32])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"18b48159-efee-e51f-0b07-317bbb373ae7"},"outputs":[],"source":"plt.figure(figsize=(12,8))\nplt.subplot(121)\nplt.imshow(ndvis[10], cmap='jet')\nplt.colorbar()\nplt.title('NDVI index of image with lots of vegetation')\nplt.subplot(122)\nplt.imshow(imgs[10])"},{"cell_type":"markdown","metadata":{"_cell_guid":"1bdeb27b-ca9c-c798-b7ed-483ff0a06997"},"source":"The mean NDVI distribution has its mode around 0.2, which is not unexpected because a lot of images show actually trees. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7bb62580-5b9a-47b0-b3d1-0f85602c42aa"},"outputs":[],"source":"import seaborn as sns\nmndvis = np.nan_to_num([ndvi.mean() for ndvi in ndvis])\nplt.figure(figsize=(12,8))\nsns.distplot(mndvis)\nplt.title('distribution of mean NDVIs')"},{"cell_type":"markdown","metadata":{"_cell_guid":"4988741c-dc2e-2ebc-0bd0-cf8206d6a800"},"source":"# NDVI ranking of images\n\nLet's plot a few images which have very low and very high NDVI values."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d7920085-5439-e259-e97c-8ecc0a852768"},"outputs":[],"source":"sorted_idcs = np.argsort(mndvis)\nprint(len(sorted_idcs))\nplt.figure(figsize=(12,8))\nplt.subplot(221)\nplt.imshow(imgs[sorted_idcs[0]])\nplt.subplot(222)\nplt.imshow(imgs[sorted_idcs[50]])\nplt.subplot(223)\nplt.imshow(imgs[sorted_idcs[-30]])\nplt.subplot(224)\nplt.imshow(imgs[sorted_idcs[-11]])"},{"cell_type":"markdown","metadata":{"_cell_guid":"734f7f61-8373-ec1e-c8cf-a6863f69251c"},"source":"Median NDVI value image can also be interesting to look at, it actually shows a mix of vegetation and roads."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"677264b5-f2ec-52cd-be0b-9e08c1034832"},"outputs":[],"source":"plt.imshow(imgs[sorted_idcs[500]])\nplt.title('image with median NDVI value')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1d823049-c6ec-7953-fc43-953a291556c5"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}