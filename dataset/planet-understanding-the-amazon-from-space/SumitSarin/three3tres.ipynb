{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Author: Sumit Sarin"},{"metadata":{},"cell_type":"markdown","source":"SUMMARY:<br>\n* This is a multilabel classification problem. Hence, classes are encoded with binary values, each class gets a column containing truth value depicting if the image belongs to the label or not.\n* The dataset comprises of Images. Images have a large no of features, deep learning is particularly suited for these kinds of tasks. \n* Since images have a lot of local information, we can make do with lesser number of weights than a fully connected layer by using a CNN.\n* I tried two popular CNN architectures used in image classification: VGG16 and ResNet50.\n* There is a huge class imbalance. Some labels occur on more than 37512 images, while some occur on as low as 99.\n* Hence, I did not make a validation and train split, because some classes/combinations in that case may never appear in the training data due to randomness of split.\n* Instead, I used cross validation, and Out Of Fold(OOF) Approach. For every iteration of cross validation, I opbtained the sigmoid values on the test set and then averaged them all.\n* Also, I used sigmoid activation in the output layer because softmax gives probabilities and is not used in multilabel classification problems.\n* Higher resolution images will contain more data and give better results. But it takes a toll on the memory. Hence I generate data in batches using the ImageDataGenerator class provided by keras.\n* I track F2 scores of the validation set after each epoch, and save the best F2 scores model for each cross validation iteration."},{"metadata":{},"cell_type":"markdown","source":"IMPORTS"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport cv2\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import KFold\n\nfrom tensorflow.keras.layers import Dropout, Flatten, Dense, InputLayer\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, Callback, ReduceLROnPlateau\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.applications.vgg16 import VGG16\nfrom tensorflow.keras.applications.resnet import ResNet50\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.backend import clear_session\nimport tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hyperparameters, etc."},{"metadata":{"trusted":true},"cell_type":"code","source":"INPUT_SHAPE = (128, 128, 3) # Image Dimensions\nBATCH_SIZE = 128\nDROPOUT_RATE = 0.5\nEPOCHS = 24\nLR = 0.0001 # Learning Rate\nREG_STRENGTH = 0.01 # Regularization Strength\nNFOLDS = 5 # No of folds for cross validation\nWORKERS = 4 # Multithreading no of threads\nMAXQ = 10 # Max Queue size for multithreading\nTHRES = [0.2] * 17 # Threshold for truth value of label, applied on sigmoid output.\n\nTRAIN_PATH = '/kaggle/input/planet-understanding-the-amazon-from-space/train-jpg'\nTEST_PATH = '/kaggle/input/planet-understanding-the-amazon-from-space/test-jpg-v2'\n\nTRAIN_CSV_PATH = '/kaggle/input/planet-understanding-the-amazon-from-space/train_v2.csv'\nTEST_CSV_PATH = '/kaggle/input/planet-understanding-the-amazon-from-space/sample_submission_v2.csv'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Construct dataframes holding training and test data information"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv(TRAIN_CSV_PATH)\ndf_test = pd.read_csv(TEST_CSV_PATH)\n\ndf_train['image_name'] = df_train['image_name'].astype(str) + '.jpg'\ndf_test['image_name'] = df_test['image_name'].astype(str) + '.jpg'\n\ndf_test['tags'] = df_test['tags'].apply(lambda x: x.split(' '))\n\nprint(df_train.head())\nprint(df_test.head())\n\nX_train_files = np.array(df_train['image_name'].tolist())\nX_train_files.reshape((X_train_files.shape[0], 1))\n\ny_train = np.array(df_train['tags'].tolist())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create list of all labels in the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = []\n\nfor tag in df_train['tags'].values:\n    labels_in_tag = tag.split(' ')\n    for label in labels_in_tag:\n        if label not in labels:\n            labels.append(label)\n        \nlabels.sort()\nprint(labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plot a few images and their labels. The images are 256x256. Below we see quality of images on 32x32, 64x64, 128x128, 256x256."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 12))\nres = [32, 64, 128, 256]\nNIMGS = 5\n\nfor i in range(len(res)):\n    for j in range(NIMGS):\n        img = cv2.imread(os.path.join(TRAIN_PATH,df_train['image_name'][j+1]))\n        img = cv2.resize(img, (res[i], res[i]))\n        plt.subplot(len(res), NIMGS, i*NIMGS+j+1)\n        plt.imshow(img)\n        plt.title(df_train['tags'][j+1] + \"\\n\" + str(res[i]) + \"x\" + str(res[i]), rotation=18)\n        plt.axis('off')\n    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"INFERENCE:\n* In 32 bit resolution, it's hard for even me to make out that a road is present in 4th column.\n* The presence of water (or a wave?) in column 5 is unclearish (broken) in 64 bit, while it becomes clear in 128 bit and 256 bit images.\n* In 128 bit and 256 bit, there isnt much of a difference. Plus the added cost of more training time (and memory) wouldnt be worthwile I think.\n* Hence I believe 128 bit resolution is a good bet."},{"metadata":{},"cell_type":"markdown","source":"Plot counts of each label"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8, 8))\n\nlabels_count = {}\n\nfor tag in df_train['tags'].values:\n    labels_in_tag = tag.split(' ')\n    for label in labels_in_tag:\n        if label in labels_count:\n            labels_count[label] += 1\n        else:\n            labels_count[label] = 0\n            \nmin_label = min(labels_count, key=labels_count.get)\nmax_label = max(labels_count, key=labels_count.get)\nprint(min_label+\" is tagged least no of times: \"+str(labels_count[min_label]))\nprint(max_label+\" is tagged max no of times: \"+str(labels_count[max_label]))\n            \nplt.bar(range(len(labels_count)), list(labels_count.values()), align='center')\nplt.xticks(range(len(labels_count)), list(labels_count.keys()), rotation=90)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"INFERENCE:\n* Because some classes have too few labels, it is possible that if I split into training and validation set, some classes may get entirely lost from one of the sets. This would be bad.\n* Hence it's best to use cross validation here, and using OOF would ensure that we are protected from the above problem.\n* In OOF what we do is basically take predictions on test set from model trained on each fold of cross validation, and average them out.\n* Accuracy is a bad metric here, and hence F2 is being used."},{"metadata":{},"cell_type":"markdown","source":"Function to create the CNN model.\n* Earlier I had included two dense layers at the end, but I found that there was no improvement being offered by them, just takin additional space.\n* I tried two different popular architectures: VGG16 and ResNet50 with weights trained on Imagenet dataset. The weights were finetuned on our dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model():\n    model = Sequential()\n    model.add(InputLayer(INPUT_SHAPE))\n    model.add(VGG16(weights='imagenet', include_top=False))\n    model.add(Flatten())\n#     model.add(Dense(4096, activation='relu'))\n#     model.add(Dropout(DROPOUT_RATE))\n#     model.add(Dense(4096, activation='relu'))\n#     model.add(Dropout(DROPOUT_RATE))\n    model.add(Dense(17, activation='sigmoid'))\n    return model\n\nclear_session()\n\nmodel = create_model()\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Callback function so I can measure the F2 score of the validation set after every epoch and save the best model accordingly."},{"metadata":{"trusted":true},"cell_type":"code","source":"def f2_score(y_true, y_pred):\n    y_true = tf.cast(y_true, \"int32\")\n    y_pred = tf.cast(tf.round(y_pred), \"int32\") # implicit 0.5 threshold via tf.round\n    y_correct = y_true * y_pred\n    sum_true = tf.reduce_sum(y_true, axis=1)\n    sum_pred = tf.reduce_sum(y_pred, axis=1)\n    sum_correct = tf.reduce_sum(y_correct, axis=1)\n    precision = sum_correct / sum_pred\n    recall = sum_correct / sum_true\n    f_score = 5 * precision * recall / (4 * precision + recall)\n    f_score = tf.where(tf.math.is_nan(f_score), tf.zeros_like(f_score), f_score)\n    return tf.reduce_mean(f_score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, I iterate through each fold, \n* mention normalization\n* mention generators\n* mention multiprocessing\n* mention augmentation"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_fold = 0\n\ny_test = []\n\nfolds = KFold(n_splits=NFOLDS, shuffle=True, random_state=1).split(X_train_files, y_train)\n\nfor train_index, val_index in folds:\n    X_train_files_fold = X_train_files[train_index]\n    y_train_fold = y_train[train_index]\n    X_val_files_fold = X_train_files[val_index]\n    y_val_fold = np.array(y_train[val_index])\n    \n    train_df = pd.DataFrame(list(zip(X_train_files_fold, y_train_fold)), columns = ['image_name', 'tags'])\n    val_df = pd.DataFrame(list(zip(X_val_files_fold, y_val_fold)), columns = ['image_name', 'tags'])\n    \n    train_df['tags'] = train_df['tags'].apply(lambda x: x.split(' '))\n    val_df['tags'] = val_df['tags'].apply(lambda x: x.split(' '))\n\n    train_datagen = ImageDataGenerator(\n        rescale=1./255,\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        horizontal_flip=True,\n        vertical_flip=True\n    )\n    \n    train_generator = train_datagen.flow_from_dataframe(\n        train_df,\n        directory=TRAIN_PATH,\n        x_col='image_name',\n        y_col='tags',\n        target_size=(INPUT_SHAPE[0], INPUT_SHAPE[1]),\n        class_mode='categorical',\n        batch_size=BATCH_SIZE,\n        classes=labels,\n    )\n    \n    val_datagen = ImageDataGenerator(\n        rescale=1./255\n    )\n    \n    val_generator = val_datagen.flow_from_dataframe(\n        val_df,\n        directory=TRAIN_PATH,\n        x_col='image_name',\n        y_col='tags',\n        target_size=(INPUT_SHAPE[0], INPUT_SHAPE[1]),\n        class_mode='categorical',\n        batch_size=BATCH_SIZE,\n        classes=labels,\n    )\n    \n    test_datagen = ImageDataGenerator(\n        rescale=1./255\n    )\n    \n    test_generator = test_datagen.flow_from_dataframe(\n        df_test,\n        directory=TEST_PATH,\n        x_col='image_name',\n        y_col='tags',\n        target_size=(INPUT_SHAPE[0], INPUT_SHAPE[1]),\n        class_mode='categorical',\n        batch_size=BATCH_SIZE,\n        classes=labels,\n        shuffle=False,\n    )\n\n    model_path_of_fold = os.path.join('', 'weights_of_fold_' + str(num_fold) + '.h5')\n    \n    clear_session()\n    model = create_model()\n    \n    adam = Adam(learning_rate=LR)\n    model.compile(loss='binary_crossentropy', optimizer=adam, metrics=[f2_score])\n    \n    callbacks = [\n        ModelCheckpoint(model_path_of_fold, monitor='val_f2_score', save_best_only=True, mode='max'),\n        ReduceLROnPlateau(monitor='loss', factor=0.1, patience=3, mode='min', min_lr=0.000001)\n    ]\n    \n    model.fit_generator(train_generator, epochs=EPOCHS, validation_data=val_generator, callbacks=callbacks,\n                       workers=WORKERS, use_multiprocessing=True, max_queue_size=MAXQ)\n\n    model.load_weights(model_path_of_fold)\n\n    p_test = model.predict_generator(test_generator, workers=WORKERS, use_multiprocessing=True, max_queue_size=MAXQ)\n    y_test.append(p_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Take average of all predictions (OOF) generated during each fold of validation on the test set. <br> \nAttach predictions to the test dataframe."},{"metadata":{"trusted":true},"cell_type":"code","source":"result = np.array(y_test[0])\nfor i in range(1, NFOLDS):\n    result += np.array(y_test[i])\nresult /= NFOLDS\nresult = pd.DataFrame(result, columns = labels)\nresult.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Construct the csv file of predictions on test set, convert the binary labels to their respective labels."},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = []\nfor i in range(result.shape[0]):\n    a = result.ix[[i]]\n    a = a.apply(lambda x: x > THRES, axis=1)\n    a = a.transpose()\n    a = a.loc[a[i] == True]\n    ' '.join(list(a.index))\n    preds.append(' '.join(list(a.index)))\n    \ndf_test['tags'] = preds\ndf_test['image_name'] = df_test['image_name'].astype(str).str.slice(stop=-4)\ndf_test.to_csv('submit.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}