{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"f506f8c6-5db2-0d45-4e5a-31be91e91129"},"source":"In this notebook, I will use vgg16 model to extract features ( by removing last fully connected layers). Then features can be use as input for XGBoost or random forest or neural networks"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5b7d697a-953b-0908-e530-3f4f5ba10bcb"},"outputs":[],"source":"import tensorflow as tf\nfrom keras.applications.vgg16 import VGG16\nfrom keras.models import Model\nfrom keras.preprocessing import image\nfrom keras.applications.vgg16 import preprocess_input\nfrom keras.layers import Flatten, Input\nimport pandas as pd\nfrom tqdm import tqdm\nimport numpy as np"},{"cell_type":"markdown","metadata":{"_cell_guid":"8cc90d18-231d-a9e8-d0b9-3f3259298388"},"source":"Function to convert data to tf feature"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0f0e80d4-3857-c8dc-4b5f-33334d7202ce"},"outputs":[],"source":"def _bytes_feature(value):\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\ndef _float_feature(value):\n    \"\"\"Wrapper for inserting float features into Example proto.\"\"\"    \n    if not isinstance(value, list):\n        value = [value]    \n    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n\ndef _int64_feature(value):\n    if not isinstance(value, list):\n        value = [value]\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))"},{"cell_type":"markdown","metadata":{"_cell_guid":"0dbec1fb-f022-5f2d-d69f-25e6d3c3896c"},"source":"Load Data"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"87c0d97d-9cdb-19f7-bd90-a90ec0908238"},"outputs":[],"source":"train_path = \"../input/train-jpg/\"\ntest_path = \"../input/test-jpg/\"\ntrain = pd.read_csv(\"../input/train_v2.csv\")\ntest = pd.read_csv(\"../input/sample_submission_v2.csv\")"},{"cell_type":"markdown","metadata":{"_cell_guid":"2be58605-244c-b295-80f7-1d0e6e9c67d5"},"source":"Construct model in keras. Convert the image to features then save to tf records file"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"70672595-cd59-1925-3cd7-17204f3f52bc"},"outputs":[],"source":"# use vgg 16 model extract feature from fc1 layer\nbase_model = VGG16(weights='imagenet', pooling = max)\nmodel = Model(inputs=base_model.input, outputs=base_model.get_layer('fc1').output)\n\nflatten = lambda l: [item for sublist in l for item in sublist]\nlabels = list(set(flatten([l.split(' ') for l in train['tags'].values])))\n\nlabel_map = {l: i for i, l in enumerate(labels)}\ninv_label_map = {i: l for l, i in label_map.items()}\n\ntfrecords_filename = \"vgg16_fc1_train.tfrecord\"\nwriter = tf.python_io.TFRecordWriter(tfrecords_filename)\nfor f, tags in tqdm(train.values[:], miniters=1000):\n    # preprocess input image\n    img_path = train_path + \"{}.jpg\".format(f)\n    img = image.load_img(img_path, target_size=(224, 224))\n    x = image.img_to_array(img)\n    x = np.expand_dims(x, axis=0)\n    x = preprocess_input(x)\n\n    # generate feature [4096]\n    features = model.predict(x)\n    np.squeeze(features)\n\n    targets = []\n    for t in tags.split(' '):\n        targets.append(label_map[t])\n\n    example = tf.train.Example(features=tf.train.Features(feature={\n        'video_id': _bytes_feature(f.encode('utf-8')),\n        'labels': _int64_feature(targets),\n        'rgb': _float_feature(features.tolist()[0])}))\n\n    writer.write(example.SerializeToString())\n\nwriter.close()"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}