{"metadata":{"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"},"language_info":{"name":"python","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"version":"3.6.1","pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":2,"cells":[{"execution_count":null,"metadata":{"_cell_guid":"5c38e99c-0da8-4831-97ee-ea2813d6c33e","collapsed":false,"_execution_state":"idle","_uuid":"9d323dba3f6216eb9ee8efbd411a52dc9ad53d34"},"source":"I just uploaded this from home.  Obviously it won't run on Kaggle, but it shows what I did.  This is the final version.  Earlier versions did not have TTA.  And very early versions used the model from Peter Giannakopoulos' old kernel (https://www.kaggle.com/petrosgk/1st-try-with-keras-0-918-lb) instead of VGG19.  I ran multiple versions using slightly different learning rate parameters, took intuitively weighted averages of the resulting probabilities (saved in this version as vgg19prob.csv), and applied thresholds to them.   The threhsolds were initially all 0.2.  Later I tried Heng CherKeng's thresholds from this discussion thread https://www.kaggle.com/c/planet-understanding-the-amazon-from-space/discussion/32475.  They didn't work very well by themselves, so I averaged them in with 0.2 and later started averaging in the optimal thresholds from my validation set (the output of optimize_f2_thresholds).","outputs":[],"cell_type":"markdown"},{"source":"import numpy as np  # linear algebra\nimport pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D, BatchNormalization\nfrom keras.optimizers import Adam\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\nfrom keras.preprocessing import image as image_utils\nfrom keras import applications\nimport cv2\nimport gc\nfrom tqdm import tqdm\n\nfrom sklearn.metrics import fbeta_score","metadata":{"_cell_guid":"d3c4d3b8-4400-4dba-9c4f-631862f92bdc","_uuid":"790b0f6f1402b4985d5d8cadddec6ef8cbe99b06","trusted":false},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"# Params\ninput_size = 128\ninput_channels = 3\n\nepochs = 40\nbatch_size = 128\nlearning_rate = 4.7e-5\nlr_decay = .0029\n\nvalid_data_size = 5000  # Samples to withhold for validation\n\nmodel = Sequential()\nmodel.add( BatchNormalization( \n                  input_shape=(input_size, input_size, input_channels) ) )\n\nvggmod = applications.VGG19(include_top=False, input_shape=(input_size, input_size, input_channels))\nmodel.add( Sequential(layers=vggmod.layers) )\nmodel.add(Dropout(0.2))\n\n# top_model = Sequential()\nmodel.add(Flatten(input_shape=model.output_shape[1:]))\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(17, activation='sigmoid'))\n\n# model.add(top_model)","metadata":{"_cell_guid":"741f7087-8eee-4b8d-8e04-a7d8fed7b7c4","collapsed":true,"_uuid":"be8a8e28fed63b8e6500d1bfe55dc456ea609e7f","trusted":false},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"def optimise_f2_thresholds(y, p, verbose=True, resolution=100):\n  def mf(x):\n    p2 = np.zeros_like(p)\n    for i in range(17):\n      p2[:, i] = (p[:, i] > x[i]).astype(np.int)\n    score = fbeta_score(y, p2, beta=2, average='samples')\n    return score\n\n  x = [0.2]*17\n  for i in range(17):\n    best_i2 = 0\n    best_score = 0\n    for i2 in range(resolution):\n      i2 /= resolution\n      x[i] = i2\n      score = mf(x)\n      if score > best_score:\n        best_i2 = i2\n        best_score = score\n    x[i] = best_i2\n    if verbose:\n      print(i, best_i2, best_score)\n\n  return x","metadata":{"_cell_guid":"71ada2a7-b364-4f7e-ac0d-e32538469642","collapsed":true,"_uuid":"0bb014c3b9e19f8c87354f44f03f5f039b8a0b90","trusted":false},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"df_train_data = pd.read_csv(r\"d:\\bigdata\\amazon\\train_v2.csv\")","metadata":{"_cell_guid":"8b8a9e84-eca8-477e-a23e-6c4e3a5b6bdf","collapsed":true,"_uuid":"0e6e9543c04a9daf85ce43164f8ae0c6369c8e06","trusted":false},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"flatten = lambda l: [item for sublist in l for item in sublist]\nlabels = list(set(flatten([l.split(' ') for l in df_train_data['tags'].values])))\n\nlabel_map = {l: i for i, l in enumerate(labels)}\ninv_label_map = {i: l for l, i in label_map.items()}","metadata":{"_cell_guid":"71d58c08-103a-44e1-b442-381cdebe2833","collapsed":true,"_uuid":"11e6ea92639b502c5bdc76d169c5638008ef6ec3","trusted":false},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"x_valid = []\ny_valid = []\n\ndf_valid = df_train_data[:valid_data_size]\n\nfor f, tags in tqdm(df_valid.values, miniters=100):\n    img = cv2.resize(cv2.imread(r\"d:\\bigdata\\amazon\\train-jpg\\{}.jpg\".format(f)), (input_size, input_size))\n    targets = np.zeros(17)\n    for t in tags.split(' '):\n        targets[label_map[t]] = 1\n    x_valid.append(img)\n    y_valid.append(targets)\n\ny_valid = np.array(y_valid, np.uint8)\nx_valid = np.array(x_valid, np.float32)\n\ngc.collect()\n","metadata":{"_cell_guid":"362c8001-d47f-4f85-a3bb-af0664a5545b","_uuid":"76b46c564484d8f699b797acf276311f20332ae0","trusted":false},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"x_train = []\ny_train = []\n\ndf_train = df_train_data[valid_data_size:]\n\nfor f, tags in tqdm(df_train.values, miniters=1000):\n    img = cv2.resize(cv2.imread(r\"d:\\bigdata\\amazon\\train-jpg\\{}.jpg\".format(f)), (input_size, input_size))\n    targets = np.zeros(17)\n    for t in tags.split(' '):\n        targets[label_map[t]] = 1\n    x_train.append(img)\n    y_train.append(targets)\n    img = cv2.flip(img, 0)  # flip vertically\n    x_train.append(img)\n    y_train.append(targets)\n    img = cv2.flip(img, 1)  # flip horizontally\n    x_train.append(img)\n    y_train.append(targets)\n    img = cv2.flip(img, 0)  # flip vertically\n    x_train.append(img)\n    y_train.append(targets)\n\ny_train = np.array(y_train, np.uint8)\nx_train = np.array(x_train, np.float32)\n\ngc.collect()","metadata":{"_cell_guid":"d83da512-bd8d-4d8e-a168-a1cd3d5520a9","_uuid":"352053fc545e80edf6a945fc33a606840637c104","trusted":false},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"df_test_data = pd.read_csv(r\"d:\\bigdata\\amazon\\sample_submission_v2.csv\")\n","metadata":{"_cell_guid":"eb88d32c-2e11-40e9-8ebe-490ae77de381","collapsed":true,"_uuid":"79294192592a913854e9eac94c6a9186bbed616e","trusted":false},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"x_test = []\nx_test1 = []\nx_test2 = []\nx_test3 = []\n\nfor f, tags in tqdm(df_test_data.values, miniters=1000):\n    img = cv2.resize(cv2.imread(r\"d:\\bigdata\\amazon\\test-jpg\\{}.jpg\".format(f)), (input_size, input_size))\n    x_test.append(img)\n    img = cv2.flip(img, 0)  # flip vertically\n    x_test1.append(img)\n    img = cv2.flip(img, 1)  # flip horizontally\n    x_test2.append(img)\n    img = cv2.flip(img, 0)  # flip vertically\n    x_test3.append(img)\n\n    \nx_test = np.array(x_test, np.float32)\nx_test1 = np.array(x_test1, np.float32)\nx_test2 = np.array(x_test2, np.float32)\nx_test3 = np.array(x_test3, np.float32)\n\ngc.collect()\n","metadata":{"_cell_guid":"f23e8ce4-6ef1-4ea4-97c1-744f3c46f1c4","_uuid":"1bff3a6b25a3f727d4eba8d8e04db2485fd25c15","trusted":false},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"callbacks = [EarlyStopping(monitor='val_loss',\n                           patience=3,\n                           verbose=0),\n             TensorBoard(log_dir='logs'),\n             ModelCheckpoint('weights.h5',\n                             save_best_only=True)]\n\nopt = Adam(lr=learning_rate, decay=lr_decay)\n\nmodel.compile(loss='binary_crossentropy',\n              # We NEED binary here, since categorical_crossentropy l1 norms the output before calculating loss.\n              optimizer=opt,\n              metrics=['accuracy'])\n","metadata":{"_cell_guid":"0168622f-4c8c-48b1-a4d7-c2ceb3ff6f17","collapsed":true,"_uuid":"462723ca8da26e69b7a6843228f13931eb282550","trusted":false},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"model.fit(x_train,\n          y_train,\n          batch_size=batch_size,\n          epochs=epochs,\n          verbose=2,\n          callbacks=callbacks,\n          validation_data=(x_valid, y_valid))\n","metadata":{"_cell_guid":"547afc93-d765-499c-9f87-d8266a2f5585","_uuid":"61ffbd28c803dad3948ed2f9c281918faaca0541","scrolled":true,"trusted":false},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"p_valid = model.predict(x_valid, batch_size=batch_size)\nprint(fbeta_score(y_valid, np.array(p_valid) > 0.2, beta=2, average='samples'))","metadata":{"_cell_guid":"07326797-53e0-436e-8ef9-f00e08791935","_uuid":"e5b590abd95a8b5deb1488bf6316e90f954ea3ec","trusted":false},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"optimise_f2_thresholds(y_valid, p_valid)","metadata":{"_cell_guid":"df7fab62-745f-4430-837a-383b3a69c5a6","_uuid":"0f54d3787bdfc688ccd6c15bbf33ee78ad7dc87f","scrolled":true,"trusted":false},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"labels","metadata":{"_cell_guid":"c69090ef-17f7-4d1f-a9d2-a88ea4eb9fab","_uuid":"e155283a4f9d85dec61aec40060359053c07a77a","trusted":false},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"y_test = []\n\np_test0 = model.predict(x_test, batch_size=batch_size, verbose=2)\np_test1 = model.predict(x_test1, batch_size=batch_size, verbose=2)\np_test2 = model.predict(x_test2, batch_size=batch_size, verbose=2)\np_test3 = model.predict(x_test3, batch_size=batch_size, verbose=2)\np_test = .31*p_test0 + .23*p_test1 + .23*p_test2 + .23*p_test3\n\ny_test.append(p_test)\n","metadata":{"_cell_guid":"b301377b-edc4-4b7e-951a-6696fab0a3aa","collapsed":true,"_uuid":"08b577652ed9e366baae94a2c563aacb25a3e3e5","trusted":false},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"","metadata":{"_cell_guid":"58be9e11-c8a0-4817-b127-f08c237861c5","collapsed":true,"_uuid":"263fa7a636ff8d32a42739d613ed87ce3655605b","trusted":false},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"result = np.array(y_test[0])\nresult = pd.DataFrame(result, columns=labels)\n\npreds = []\n\nfor i in tqdm(range(result.shape[0]), miniters=1000):\n    a = result.iloc[[i]]\n    a = a.apply(lambda x: x > 0.2, axis=1)\n    a = a.transpose()\n    a = a.loc[a[i] == True]\n    ' '.join(list(a.index))\n    preds.append(' '.join(list(a.index)))\n","metadata":{"_cell_guid":"2d2fd1dd-fdca-4c66-bb98-90bc1c3236e0","_uuid":"c205ccc06af2cad30d186b9aaa857ebfe67a1964","scrolled":true,"trusted":false},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"df_test_data['tags'] = preds\ndf_test_data.to_csv('vgg19sub.csv', index=False)\nresult.index = df_test_data.iloc[:,0].values\nresult.to_csv('vgg19prob.csv', index=False)","metadata":{"_cell_guid":"cfcb8c40-3545-4a52-9675-9af711ab3bbd","collapsed":true,"_uuid":"511af64927828f382935be0d306a7f5483593827","trusted":false},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"p_valid.shape","metadata":{"_cell_guid":"22cebd16-cf52-4283-b4bf-bd01a7480b47","_uuid":"c77d8d7bd666ce4538c48ea15e761535574c1161","trusted":false},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"","metadata":{"_cell_guid":"6780ca0c-e210-4410-807f-604e37cbd5b2","collapsed":true,"_uuid":"5be8c9a609af2d0232b73d5dd0e5a3754e20831c","trusted":false},"execution_count":null,"cell_type":"code","outputs":[]}],"nbformat":4}