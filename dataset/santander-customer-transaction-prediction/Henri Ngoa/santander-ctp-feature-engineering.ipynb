{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Feature Engineering for the Kaggle competition :  [Customer Transaction Prediction](https://www.kaggle.com/c/santander-customer-transaction-prediction/overview) of SANTANDER\n---\n### Competition introduction\n*At Santander our mission is to help people and businesses prosper. We are always looking for ways to help our customers understand their financial health and identify which products and services might help them achieve their monetary goals.*\n\n*Our data science team is continually challenging our machine learning algorithms, working with the global data science community to make sure we can more accurately identify new ways to solve our most common challenge, binary classification problems such as: is a customer satisfied? Will a customer buy this product? Can a customer pay this loan?*\n\n*In this challenge, we invite Kagglers to help us identify which customers will make a specific transaction in the future, irrespective of the amount of money transacted. The data provided for this competition has the same structure as the real data we have available to solve this problem.*\n\n\n### Data\n- We are provided with an anonymized dataset containing numeric feature variables, the binary target column, and a string `ID_code` column.\n\n- The task is to predict the value of target column in the test set.\n\n- File descriptions\n\n    `train.csv` - the training set.\n    \n    `test.csv` - the test set. The test set contains some rows which are not included in scoring.\n    \n    `sample_submission.csv` - a sample submission file in the correct format.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_selection import mutual_info_classif\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom xgboost import XGBClassifier\nfrom sklearn.cluster import KMeans\nfrom sklearn.naive_bayes import GaussianNB\nimport lightgbm\n%matplotlib inline","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_formatting():\n    '''\n    Set up the default plotting settings.\n    '''\n    \n    plt.rc(\n        'figure',\n        figsize=(12,6),\n        titleweight='bold',\n        titlesize=25\n    )\n    plt.rc(\n        'axes',\n        labelweight='ultralight',\n        titleweight='ultralight',\n        titlelocation='left',\n        titlecolor='k',\n        titley=1.03,\n        titlesize=16,\n        grid=True\n    )\n    plt.rc(\n        'axes.spines',\n        right=False,\n        left=False,\n        top=False   \n    )\n    plt.rc(\n        'grid',\n        color='k',\n        linestyle=(0,15,2,0),\n        alpha=0.5\n    )\n    plt.rc('axes.grid', axis='y')\n    plt.rc('ytick.major', width=0)\n    plt.rc('font', family='monospace')\n    \nplot_formatting() # Setting our default settings","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/santander-customer-transaction-prediction/train.csv')\ntest = pd.read_csv('../input/test-ctr/real_test.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_index = test['ID_code']\n\n# Separate features from the label\ny = train['target']\ntrain = train.loc[:, 'var_0':'var_199']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualizing data with PCA","metadata":{}},{"cell_type":"code","source":"# Standardizing the data for PCA\nX_stand = StandardScaler().fit_transform(train)\n\n# Two components to visualize\npca = PCA(n_components=2).fit(X_stand)\nX_pca = pca.transform(X_stand)\n\nsns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], c=y.map({0: 'tomato', 1: 'steelblue'}), alpha=0.3)\nplt.suptitle('Training set visualization', x=0.33, y=0.9)\nplt.title('The Data is not easily linearly separable', y=0.87)\nplt.tick_params(labelbottom=False, labelleft=False)\nplt.xlabel('PCA 1st component')\nplt.ylabel('PCA 2nd component');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Baseline score","metadata":{}},{"cell_type":"code","source":"def score_dataset(X, y, model = XGBClassifier(objective = 'binary:logistic', tree_method='gpu_hist',\n                                              eval_metric = 'auc', use_label_encoder=False, random_state=0)):\n    '''\n    Used to score the dataset each time new features are added to it.\n    To assess the relevance of new features.\n    '''\n    \n    score = cross_val_score(\n        model, X.values, y.values, cv=5, scoring=\"roc_auc\"\n    )\n    print('CV scores: ', score)\n    print('Mean score: ', score.mean())\n    return ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def score_lightgbm(X, y):\n    \n    '''\n    Used to score the dataset each time new features are added to it.\n    To assess the relevance of new features.\n    '''\n    \n    folds = StratifiedKFold(n_splits=3, shuffle=True, random_state=14)\n    oof = np.zeros(len(X))\n    \n    param = {\n    'boost': 'gbdt',\n    'metric':'auc'\n}\n\n    for fold_, (trn_idx, val_idx) in enumerate(folds.split(X.values, y.values)):\n        \n        trn_data = lightgbm.Dataset(X.iloc[trn_idx], label=y.iloc[trn_idx])\n        val_data = lightgbm.Dataset(X.iloc[val_idx], label=y.iloc[val_idx])\n\n        clf = lightgbm.train(param, train_set=trn_data, num_boost_round=2500,\n                             valid_sets = [trn_data, val_data],\n                             early_stopping_rounds = 300, verbose_eval=False);\n        \n        oof[val_idx] = clf.predict(X.iloc[val_idx], num_iteration=clf.best_iteration)\n\n    print(\"CV score: {:<8.5f}\".format(roc_auc_score(y, oof)))\n    return","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# score_dataset(train, y)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score_lightgbm(train, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Mutual Information scores of features with the target","metadata":{}},{"cell_type":"code","source":"mi_scores = mutual_info_classif(train, y, random_state=0)\nmi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=train.columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Drop uninformative features","metadata":{}},{"cell_type":"code","source":"def drop_uninformative(df):\n    '''\n    Keeps features with non-zero MI scores.\n    '''\n    return df.loc[:, mi_scores > 0] ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = train.copy()\nX = drop_uninformative(X)\n\nscore_lightgbm(X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The score drops. It is preferable to keep those features even though they have zero MI scores.","metadata":{}},{"cell_type":"code","source":"print('Percentage of duplicates Train:')\ntrain.loc[:, 'var_0':'var_199'].apply(lambda x: 1 - len(x.unique()) / len(train), axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Percentage of duplicates Test:')\ntest.loc[:, 'var_0':'var_199'].apply(lambda x: 1 - len(x.unique()) / len(train), axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Count features","metadata":{}},{"cell_type":"code","source":"def count_features(df):\n    return df.apply(lambda x: x.groupby(x).transform('count'), axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.concat([train, test.loc[:, 'var_0':'var_199']], axis=0).reset_index().drop('index', axis=1)\nX = df.copy()\nnew_columns = count_features(X)\nX = X.join(new_columns.add_prefix('Count_'))\nX = X.iloc[:len(train)]\nscore_lightgbm(X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is a good increase but there might be an interaction between raw and count features. If so, it will probably increase even more our score. To see this, let's combine them. The following \"unique_columns\" function will create two new columns for each feature:\n- The first column will only keep unique entries and impute Nan to non unique entries.\n- The second column will keep non-unique entries and impute Nan to unique entries.","metadata":{}},{"cell_type":"code","source":"def unique_columns(df):\n    '''\n    Returns a transformed version of the dataframe.\n    Where each raw feature is replace by two new corresponding columns:\n        - One column will impute NaN to unique entries and keep non-unique ones.\n        - The other one will not impute NaN only to unique entries.\n    '''\n    \n    #Calculate counts to see the frequency of each entry\n    unique_df = df.apply(lambda x: x.groupby(x).transform('count'), axis=0)\n    \n    #Impute NaNs \n    unique_columns = df.where(unique_df == 1)\n    non_unique = df.where(unique_df > 1)\n    \n    return non_unique.add_prefix('NU_').join(unique_columns.add_prefix('U_'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df.copy()\nX = unique_columns(X)\nX = X.iloc[:len(train)]\nscore_lightgbm(X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Gaussian Naive Bayes Predict_proba\n\nThe EDA showed that data points are samples from a mutlivariate gaussian distribution. It also showed that the features are independent. Therefore, all critical assumptions of a Gaussian Naive Bayes model are reunited here. We could use a little part of our dataset to train a Gaussian NB model and give new features (being the predic_proba of this model) to the remaining data.","metadata":{}},{"cell_type":"code","source":"# def GNB_features(X_train, X, y_train, test_df=None):\n#     X = X.copy()\n#     if test_df:\n#         test_df = test_df.copy()\n#     for i, feature in enumerate(X_train.columns):\n#         clf = GaussianNB().fit(X_train[[feature]], y_train)\n#         X['GNB_' + str(i)] =  clf.predict(X[[feature]])\n#         if test_df:\n#             test_df['GNB_' + str(i)] =  clf.predict(test_df[[feature]])\n    \n#     return X, test_df\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## First principal component:","metadata":{}},{"cell_type":"code","source":"def PCA_feature(df):\n    \n    Stand_df = StandardScaler().fit_transform(df)\n    pca = PCA(n_components=1).fit(Stand_df)\n    X_pca = pca.transform(Stand_df)\n    \n    return X_pca, pca","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = train.copy()\n\nX['PC1'] = PCA_feature(X)[0]\nscore_lightgbm(X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Add clustering features\nWe could either add the cluster labels as a column or distances of each data point from cluster centroids.\n\nAfter tuning, the optimal number of clusters is 3.\n","metadata":{}},{"cell_type":"code","source":"def cluster_labels(df, features, n_clusters=20):\n    # After tuning, the optimal number of clusters chosen 3\n    \n    X = df.copy()\n    X = X.loc[:, features]\n    X_scaled = StandardScaler().fit_transform(X)\n    kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=0) \n    \n    return kmeans.fit_predict(X_scaled)\n\n\ndef centroid_distance(df, features, n_clusters=3):\n    # After tuning, the optimal number of clusters chosen 3\n    \n    X = df.copy()\n    X = X.loc[:, features]\n    X_scaled = StandardScaler().fit_transform(X)\n    kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=0).fit(X_scaled)\n    \n    X_cd = kmeans.transform(X_scaled)\n    X_cd = pd.DataFrame(\n        X_cd, columns=[f\"Centroid_{i}\" for i in range(X_cd.shape[1])]\n    )\n    return kmeans, X_cd","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = train.copy()\nX['Cluster'] = cluster_labels(X, X.columns)\nscore_lightgbm(X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = train.copy()\nkmeans, dist = centroid_distance(X, X.columns)\nX = X.join(dist)\nscore_lightgbm(X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset transformation","metadata":{}},{"cell_type":"code","source":"df = pd.concat([train, test.loc[:, 'var_0':'var_199']], axis=0).reset_index().drop('index', axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df.copy()\n\n_, dist = centroid_distance(X, X.columns)\nnew_columns = unique_columns(X)\nX_pca, _ = PCA_feature(X)\n\nX = new_columns.join(dist)\nX['PC1'] = X_pca\n\ntrain = X.iloc[:len(train)]\ntest = X.iloc[len(train):len(train) + len(test)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train.columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score_lightgbm(train, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# score_dataset(train, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['target'] = y.copy()\ntest = test.reset_index().drop('index', axis=1)\ntest['ID_code'] = test_index","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.to_csv('final_train.csv', index=False)\ntest.to_csv('final_test.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]}]}