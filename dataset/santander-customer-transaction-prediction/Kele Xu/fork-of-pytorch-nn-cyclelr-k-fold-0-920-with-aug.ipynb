{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport time\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors.kde import KernelDensity\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.autograd import Variable\nfrom torch.utils.data import Dataset, DataLoader\n\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0508722b8fa65e54571703633e6c878477e0443f"},"cell_type":"markdown","source":"## Load Data"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#Load data\ntrain_df = pd.read_csv('../input/train.csv')\ntest_df = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"996ed97f55d61b4634a2b942eb34ccb7b3930aca"},"cell_type":"code","source":"train_features = train_df.drop(['target','ID_code'], axis = 1)\ntest_features = test_df.drop(['ID_code'],axis = 1)\ntrain_target = train_df['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4017f7c59c68671f6cb070fc6a1f4c689cbbc40c"},"cell_type":"code","source":"df_test = test_features.values\n\nunique_samples = []\nunique_count = np.zeros_like(df_test)\nbasic_features = [c for c in train_df.columns if c not in ['ID_code', 'target']]\nfor feature in range(df_test.shape[1]):\n    _, index_, count_ = np.unique(df_test[:, feature], return_counts=True, return_index=True)\n    unique_count[index_[count_ == 1], feature] += 1\n\n# 具有唯一值的样本是真实的，其他样本是假的\nreal_samples_indexes = np.argwhere(np.sum(unique_count, axis=1) > 0)[:, 0]\nsynthetic_samples_indexes = np.argwhere(np.sum(unique_count, axis=1) == 0)[:, 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_data(train_df, test_df):\n    idx = [c for c in train_df.columns if c not in ['ID_code', 'target']]\n    df = pd.concat([train_df,test_df.ix[real_samples_indexes]])\n    for feat in idx:\n        temp = df[feat].value_counts(dropna=True)\n        train_df[\"count_\"+feat] = train_df[feat].map(temp) \n        test_df[\"count_\"+feat] = test_df[feat].map(temp)       \n                \n        train_df[\"sum_\"+feat] = ((train_df[feat]-df[feat].mean())*train_df[\"count_\"+feat].map(lambda x:int(x>1))).astype(np.float32)\n        test_df[\"sum_\"+feat] = ((test_df[feat]-df[feat].mean())*test_df[\"count_\"+feat].map(lambda x:int(x>1))).astype(np.float32)        \n        \n        train_df[\"copy_\"+feat]=train_df[feat] * (train_df[\"count_\"+feat] > 1).astype(int)\n        test_df[\"copy_\"+feat]=test_df[feat] * (test_df[\"count_\"+feat] > 1).astype(int)\n        train_df[\"copy_\"+feat] = train_df[\"copy_\"+feat].replace(0,df[feat].median()) \n        test_df[\"copy_\"+feat] = test_df[\"copy_\"+feat].replace(0,df[feat].median()) \n\n    return train_df, test_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ed0023ee6fac5fc4ebe09b087ac060294d0ea27f"},"cell_type":"code","source":"train_features , test_features = process_data(train_features,test_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_name = test_features.columns.tolist()\nfeature_name.sort()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\ntrain_features = train_features[feature_name]\ntest_features = test_features[feature_name]\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"84f3159307f8a5c5f2993a6d874aafe776490e08"},"cell_type":"code","source":"#### Scaling feature #####\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler\nsc = StandardScaler()\ntrain_features = sc.fit_transform(train_features)\ntest_features = sc.transform(test_features)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1bdb1f695eb045d4dc6710234c7df355ecb9679a"},"cell_type":"markdown","source":"## Split K- fold validation"},{"metadata":{"trusted":true,"_uuid":"b7832757a0bcae1b027da430bfa0192dd2eaef7c"},"cell_type":"code","source":"# Implement K-fold validation to improve results\nn_splits = 10 # Number of K-fold Splits\n\nsplits = list(StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=4590).split(train_features, train_target))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"55c0a7544d135109579772b0140569d9ea8f9ed8"},"cell_type":"markdown","source":"## Cycling learning rate\n\n*copy from ==> https://github.com/anandsaha/pytorch.cyclic.learning.rate/blob/master/cls.py"},{"metadata":{"trusted":true,"_uuid":"8ffbc25621cbd735e0ba5f7bbbc4f7d8da37c9d9"},"cell_type":"code","source":"class CyclicLR(object):\n    def __init__(self, optimizer, base_lr=1e-3, max_lr=6e-3,\n                 step_size=2000, mode='triangular', gamma=1.,\n                 scale_fn=None, scale_mode='cycle', last_batch_iteration=-1):\n\n        if not isinstance(optimizer, Optimizer):\n            raise TypeError('{} is not an Optimizer'.format(\n                type(optimizer).__name__))\n        self.optimizer = optimizer\n\n        if isinstance(base_lr, list) or isinstance(base_lr, tuple):\n            if len(base_lr) != len(optimizer.param_groups):\n                raise ValueError(\"expected {} base_lr, got {}\".format(\n                    len(optimizer.param_groups), len(base_lr)))\n            self.base_lrs = list(base_lr)\n        else:\n            self.base_lrs = [base_lr] * len(optimizer.param_groups)\n\n        if isinstance(max_lr, list) or isinstance(max_lr, tuple):\n            if len(max_lr) != len(optimizer.param_groups):\n                raise ValueError(\"expected {} max_lr, got {}\".format(\n                    len(optimizer.param_groups), len(max_lr)))\n            self.max_lrs = list(max_lr)\n        else:\n            self.max_lrs = [max_lr] * len(optimizer.param_groups)\n\n        self.step_size = step_size\n\n        if mode not in ['triangular', 'triangular2', 'exp_range'] \\\n                and scale_fn is None:\n            raise ValueError('mode is invalid and scale_fn is None')\n\n        self.mode = mode\n        self.gamma = gamma\n\n        if scale_fn is None:\n            if self.mode == 'triangular':\n                self.scale_fn = self._triangular_scale_fn\n                self.scale_mode = 'cycle'\n            elif self.mode == 'triangular2':\n                self.scale_fn = self._triangular2_scale_fn\n                self.scale_mode = 'cycle'\n            elif self.mode == 'exp_range':\n                self.scale_fn = self._exp_range_scale_fn\n                self.scale_mode = 'iterations'\n        else:\n            self.scale_fn = scale_fn\n            self.scale_mode = scale_mode\n\n        self.batch_step(last_batch_iteration + 1)\n        self.last_batch_iteration = last_batch_iteration\n\n    def batch_step(self, batch_iteration=None):\n        if batch_iteration is None:\n            batch_iteration = self.last_batch_iteration + 1\n        self.last_batch_iteration = batch_iteration\n        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n            param_group['lr'] = lr\n\n    def _triangular_scale_fn(self, x):\n        return 1.\n\n    def _triangular2_scale_fn(self, x):\n        return 1 / (2. ** (x - 1))\n\n    def _exp_range_scale_fn(self, x):\n        return self.gamma**(x)\n\n    def get_lr(self):\n        step_size = float(self.step_size)\n        cycle = np.floor(1 + self.last_batch_iteration / (2 * step_size))\n        x = np.abs(self.last_batch_iteration / step_size - 2 * cycle + 1)\n\n        lrs = []\n        param_lrs = zip(self.optimizer.param_groups, self.base_lrs, self.max_lrs)\n        for param_group, base_lr, max_lr in param_lrs:\n            base_height = (max_lr - base_lr) * np.maximum(0, (1 - x))\n            if self.scale_mode == 'cycle':\n                lr = base_lr + base_height * self.scale_fn(cycle)\n            else:\n                lr = base_lr + base_height * self.scale_fn(self.last_batch_iteration)\n            lrs.append(lr)\n        return lrs","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5a1cde968ad94fe9a8876e69bd1792fc9c26a028"},"cell_type":"markdown","source":"## Build Simple NN model (Pytorch)\n\n* add flatten layer before fc layer (improve to 0.89+)\n* https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/82863\n\n* Model structure\n* (batch_size, 200) ==> Flatten ==> (batch_size* 200,1) ==> fc1 ==> (batch_size* 200, hidden_layer) ==>Reshape ==>(batch_size, hidden_layer * 200) ==> fc2 ==> (batch_size, 1)"},{"metadata":{"trusted":true,"_uuid":"925b6d89e56800f25609f2ac5c3cbf20608ba799"},"cell_type":"code","source":"class Simple_NN(nn.Module):\n    def __init__(self ,input_dim ,hidden_dim, dropout = 0.5):\n        super(Simple_NN, self).__init__()\n        \n        self.inpt_dim = input_dim\n        self.hidden_dim = hidden_dim\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(dropout)\n        self.fc1 = nn.Linear(1, hidden_dim)\n        self.fc2 = nn.Linear(int(hidden_dim*input_dim), 1)\n    \n    def forward(self, x):\n        b_size = x.size(0)\n        x = x.view(-1, 1)\n        y = self.fc1(x)\n        y = self.relu(y)\n        y = y.view(b_size, -1)\n        \n        out= self.fc2(y)\n        \n        return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6aafb908b6967fe72a35603de367372145ee722d"},"cell_type":"code","source":"def sigmoid(x):\n    return 1 / (1 + np.exp(-x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7e655f4e8dfe29138ecd19840c9523c087f86d99"},"cell_type":"code","source":"class EarlyStopping:\n    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n    def __init__(self, patience=2, verbose=False):\n        \"\"\"\n        Args:\n            patience (int): How long to wait after last time validation loss improved.\n                            Default: 7\n            verbose (bool): If True, prints a message for each validation loss improvement. \n                            Default: False\n        \"\"\"\n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.val_auc_min = 0\n\n    def __call__(self, val_auc, model):\n\n        score = val_auc\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(val_auc, model)\n        elif score < self.best_score:\n            self.counter += 1\n            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(val_auc, model)\n            self.counter = 0\n\n    def save_checkpoint(self, val_auc, model):\n        '''Saves model when validation loss decrease.'''\n        if self.verbose:\n            print(f'Validation loss decreased ({self.val_auc_min:.6f} --> {val_auc:.6f}).  Saving model ...')\n        torch.save(model.state_dict(), 'checkpoint.pt')\n        self.val_auc_min = val_auc","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"867d8ce51beef40c85d5751f89d832af9b5d8ada"},"cell_type":"markdown","source":"\n\n\n## Start training\n* Epoch = 40\n* Batch size = 256\n* Cycling step = 150"},{"metadata":{"trusted":true},"cell_type":"code","source":"def augment(x,y,t=2):\n    xs,xn = [],[]\n    for i in range(t):\n        mask = y>0\n        x1 = x[mask].copy()\n        ids = np.arange(x1.shape[0])\n        for c in range(x1.shape[1]):\n            np.random.shuffle(ids)\n            x1[:,c] = x1[ids][:,c]\n        xs.append(x1)\n\n    for i in range(t//2):\n        mask = y==0\n        x1 = x[mask].copy()\n        ids = np.arange(x1.shape[0])\n        for c in range(x1.shape[1]):\n            np.random.shuffle(ids)\n            x1[:,c] = x1[ids][:,c]\n        xn.append(x1)\n\n    xs = np.vstack(xs)\n    xn = np.vstack(xn)\n    ys = np.ones(xs.shape[0])\n    yn = np.zeros(xn.shape[0])\n    x = np.vstack([x,xs,xn])\n    y = np.concatenate([y,ys,yn])\n    return x,y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"70429be47146b43f3129eacc3812d10d6836cd8a","scrolled":false},"cell_type":"code","source":"from torch.optim.optimizer import Optimizer\n## Hyperparameter\nn_epochs = 20000\nbatch_size = 256\n\n## Build tensor data for torch\ntrain_preds = np.zeros((len(train_features)))\ntest_preds = np.zeros((len(test_features)))\n\nx_test = np.array(test_features)\nx_test_cuda = torch.tensor(x_test, dtype=torch.float).cuda()\ntest = torch.utils.data.TensorDataset(x_test_cuda)\ntest_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n\navg_losses_f = []\navg_val_losses_f = []\nval_auc=[]\n## Start K-fold validation\nfor i, (train_idx, valid_idx) in enumerate(splits):\n    valid_preds = []\n    x_train = np.array(train_features)\n    y_train = np.array(train_target)   \n    #x_train , y_train = augment(x_train , y_train)\n    \n    x_train_fold = torch.tensor(x_train[train_idx.astype(int)], dtype=torch.float).cuda()\n    y_train_fold = torch.tensor(y_train[train_idx.astype(int), np.newaxis], dtype=torch.float32).cuda()\n    \n    x_val_fold = torch.tensor(x_train[valid_idx.astype(int)], dtype=torch.float).cuda()\n    y_val_fold = torch.tensor(y_train[valid_idx.astype(int), np.newaxis], dtype=torch.float32).cuda()\n    \n    ##Loss function\n    #loss_fn = FocalLoss(2)\n    loss_fn = torch.nn.BCEWithLogitsLoss()\n    \n    #Build model, initial weight and optimizer\n    model = Simple_NN(train_features.shape[1],32)\n    model.cuda()\n    optimizer = torch.optim.Adam(model.parameters(), lr = 0.001,weight_decay=1e-5) # Using Adam optimizer\n    \n    \n    ######################Cycling learning rate########################\n\n    step_size = 2000\n    base_lr, max_lr = 0.0001, 0.002  \n    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), \n                             lr=max_lr)\n    \n    scheduler = CyclicLR(optimizer, base_lr=base_lr, max_lr=max_lr,\n               step_size=step_size, mode='exp_range',\n               gamma=0.99994)\n\n    ###################################################################\n\n    train = torch.utils.data.TensorDataset(x_train_fold, y_train_fold)\n    valid = torch.utils.data.TensorDataset(x_val_fold, y_val_fold)\n    \n    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n    valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n    \n    print(f'Fold {i + 1}')\n    early_stopping = EarlyStopping(patience=8, verbose=True)\n    for epoch in range(n_epochs):\n        start_time = time.time()\n        model.train()\n        avg_loss = 0.\n        #avg_auc = 0.\n        for i, (x_batch, y_batch) in enumerate(train_loader):\n            y_pred = model(x_batch)\n            ###################tuning learning rate###############\n            if scheduler:\n                #print('cycle_LR')\n                scheduler.batch_step()\n\n            ######################################################\n            loss = loss_fn(y_pred, y_batch)\n\n            optimizer.zero_grad()\n            loss.backward()\n\n            optimizer.step()\n            avg_loss += loss.item()/len(train_loader)\n            #avg_auc += round(roc_auc_score(y_batch.cpu(),y_pred.detach().cpu()),4) / len(train_loader)\n        model.eval()\n        \n        valid_preds_fold = np.zeros((x_val_fold.size(0)))\n        test_preds_fold = np.zeros((len(test_features)))\n        \n        avg_val_loss = 0.\n        avg_val_auc = 0.\n        for i, (x_batch, y_batch) in enumerate(valid_loader):\n            y_pred = model(x_batch).detach()\n            \n            avg_val_auc += round(roc_auc_score(y_batch.cpu(),sigmoid(y_pred.cpu().numpy())[:, 0]),4) / len(valid_loader)\n            avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)\n            valid_preds_fold[i * batch_size:(i+1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n        valid_preds.append(valid_preds_fold)\n            \n        elapsed_time = time.time() - start_time \n        print('Epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t time={:.2f}s'.format(\n            epoch + 1, n_epochs, avg_loss, avg_val_loss, elapsed_time))\n        \n        early_stopping(avg_val_auc, model)\n        if early_stopping.early_stop:\n            valid_preds_fold = valid_preds[-9]\n            print(\"Early stopping\")\n            break\n    model.load_state_dict(torch.load('checkpoint.pt')) \n    avg_losses_f.append(avg_loss)\n    avg_val_losses_f.append(avg_val_loss) \n    \n    for i, (x_batch,) in enumerate(test_loader):\n        y_pred = model(x_batch).detach()\n\n        test_preds_fold[i * batch_size:(i+1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n        \n    train_preds[valid_idx] = valid_preds_fold\n    test_preds += test_preds_fold / len(splits)\n    val_auc.append(roc_auc_score(train_target[valid_idx],train_preds[valid_idx]))\n    print(val_auc)\nprint(np.mean(val_auc))\nauc  =  round(roc_auc_score(train_target,train_preds),4)        \nprint('All \\t loss={:.4f} \\t val_loss={:.4f} \\t auc={:.4f}'.format(np.average(avg_losses_f),np.average(avg_val_losses_f),auc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cdaf09d52b7b46fb5a33cc69e37889d29742b0ed"},"cell_type":"code","source":"test_ID = test_df['ID_code'].values\nsubmission_nn = pd.DataFrame({ \"ID_code\": test_ID, \"target\": test_preds})\nsubmission_nn.to_csv('test_preds_submission_nn_10fold.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d87c62628cd9f1a2f0585fe9bd93ee3e3b1de0dd"},"cell_type":"code","source":"train_ID = train_df['ID_code'].values\nsubmission_nn = pd.DataFrame({ \"ID_code\": train_ID, \"target\": train_preds})\nsubmission_nn.to_csv('train_preds_submission_nn_10fold.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}