{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Assessment of a plain MLP+mixup on SCTP\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom scipy.stats import beta\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader,TensorDataset\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning.loggers import TensorBoardLogger,CSVLogger\nfrom pytorch_lightning.callbacks.early_stopping import EarlyStopping","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-03T19:19:44.644707Z","iopub.execute_input":"2021-06-03T19:19:44.645309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# needed for deterministic output\npl.seed_everything(2)\n\n# device in which the model will be trained\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***\n## data preparation","metadata":{}},{"cell_type":"code","source":"dataset = pd.read_csv(\"../input/santander-customer-transaction-prediction/train.csv\")\ndataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.groupby(\"target\")[\"ID_code\"].count() / len(dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dataset stratified split: train 60% - valid 20% - test 20%\n\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=2)\nsplit = skf.split(dataset, dataset.target)\n_,valid_index = next(split)\n_,test_index = next(split)\n\ntrain_dset = dataset.drop(valid_index).drop(test_index).reset_index(drop=True)\nvalid_dset = dataset.loc[valid_index].reset_index(drop=True)\ntest_dset = dataset.loc[test_index].reset_index(drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(train_dset.groupby(\"target\")[\"ID_code\"].count() / len(train_dset))\ndisplay(valid_dset.groupby(\"target\")[\"ID_code\"].count() / len(valid_dset))\ndisplay(test_dset.groupby(\"target\")[\"ID_code\"].count() / len(test_dset))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_features = dataset.columns[2:].tolist()\ntarget = \"target\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# parsing inputs as pytorch tensor dataset\n\ntrain_tensor_dset = TensorDataset(\n    torch.tensor(train_dset[input_features].values, dtype=torch.float),\n    torch.tensor(train_dset[target].values.reshape(-1,1), dtype=torch.float)\n)\n\nvalid_tensor_dset = TensorDataset(\n    torch.tensor(valid_dset[input_features].values, dtype=torch.float),\n    torch.tensor(valid_dset[target].values.reshape(-1,1), dtype=torch.float)\n)\n\ntest_tensor_dset = TensorDataset(\n    torch.tensor(test_dset[input_features].values, dtype=torch.float),\n    torch.tensor(test_dset[target].values.reshape(-1,1), dtype=torch.float) \n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***\n## 3-layers MLP without mixup","metadata":{}},{"cell_type":"code","source":"class DNN(pl.LightningModule):\n\n    def __init__(self, input_dim, output_dim, nn_depth, nn_width, dropout, momentum):\n        super().__init__()\n\n        self.bn_in = nn.BatchNorm1d(input_dim, momentum=momentum)\n        self.dp_in = nn.Dropout(dropout)\n        self.ln_in = nn.Linear(input_dim, nn_width, bias=False)\n\n        self.bnorms = nn.ModuleList([nn.BatchNorm1d(nn_width, momentum=momentum) for i in range(nn_depth-1)])\n        self.dropouts = nn.ModuleList([nn.Dropout(dropout) for i in range(nn_depth-1)])\n        self.linears = nn.ModuleList([nn.Linear(nn_width, nn_width, bias=False) for i in range(nn_depth-1)])\n        \n        self.bn_out = nn.BatchNorm1d(nn_width, momentum=momentum)\n        self.dp_out = nn.Dropout(dropout/2)\n        self.ln_out = nn.Linear(nn_width, output_dim, bias=False)\n\n        self.loss = nn.BCEWithLogitsLoss()\n\n    def forward(self, x):\n        x = self.bn_in(x)\n        x = self.dp_in(x)\n        x = nn.functional.relu(self.ln_in(x))\n\n        for bn_layer,dp_layer,ln_layer in zip(self.bnorms,self.dropouts,self.linears):\n            x = bn_layer(x)\n            x = dp_layer(x)\n            x = ln_layer(x)\n            x = nn.functional.relu(x)\n            \n        x = self.bn_out(x)\n        x = self.dp_out(x)\n        x = self.ln_out(x)\n        return x\n\n    def training_step(self, batch, batch_idx):\n        X, y = batch\n        y_hat = self.forward(X)\n        loss = self.loss(y_hat, y)\n        self.log('train_loss', loss)\n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        X, y = batch\n        y_hat = self.forward(X)\n        loss = self.loss(y_hat, y)\n        self.log('valid_loss', loss)\n        \n    def test_step(self, batch, batch_idx):\n        X, y = batch\n        y_logit = self.forward(X)\n        y_probs = torch.sigmoid(y_logit).detach().cpu().numpy()\n        loss = self.loss(y_logit, y)\n        metric = roc_auc_score(y.cpu().numpy(), y_probs)\n        self.log('test_loss', loss)\n        self.log('test_metric', metric)\n        \n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=5e-3, weight_decay=1e-4)\n        scheduler = {\n            'scheduler': ReduceLROnPlateau(\n                optimizer, \n                mode=\"min\", \n                factor=0.5, \n                patience=5, \n                min_lr=1e-5),\n            'interval': 'epoch',\n            'frequency': 1,\n            'reduce_on_plateau': True,\n            'monitor': 'valid_loss',\n        }\n        return [optimizer], [scheduler]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = DNN(\n    input_dim=len(input_features), \n    output_dim=1, \n    nn_depth=3, \n    nn_width=128, \n    dropout=0.2, \n    momentum=0.1\n)\n\nlogger = logger = CSVLogger(\"logs\", name=\"mlp_wo_mixup\")\n\nearly_stop_callback = EarlyStopping(\n   monitor='valid_loss',\n   min_delta=.0,\n   patience=20,\n   verbose=True,\n   mode='min'\n)\n\ntrainer = pl.Trainer(\n    callbacks=[early_stop_callback], \n    min_epochs=10, \n    max_epochs=200, \n    gpus=0, \n    logger=logger, \n    deterministic=True\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summarize()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.fit(\n    model, \n    DataLoader(train_tensor_dset, batch_size=1024, shuffle=True, num_workers=4),\n    DataLoader(valid_tensor_dset, batch_size=1024, shuffle=False, num_workers=4)\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# AUC on validation dataset\ntrainer.test(model, DataLoader(valid_tensor_dset, batch_size=1024, shuffle=False, num_workers=4))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# AUC on test dataset\ntrainer.test(model, DataLoader(test_tensor_dset, batch_size=1024, shuffle=False, num_workers=4))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics = pd.read_csv(\"logs/mlp_wo_mixup/version_0/metrics.csv\")\n\ndf1 = metrics.loc[:,[\"step\",\"train_loss\"]].dropna()\ndf2 = metrics.loc[:,[\"step\",\"valid_loss\"]].dropna()\n\nplt.figure(figsize=(12,5))\nplt.plot(df1.step, df1.train_loss, \"o-\", label=\"train_loss\")\nplt.plot(df2.step, df2.valid_loss, \"o-\", label=\"valid_loss\")\nplt.grid()\nplt.legend(loc=\"best\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***\n## 3-layers MLP with mixup","metadata":{}},{"cell_type":"code","source":"alpha = 0.25\n\nx = np.linspace(beta.ppf(0.01, alpha, alpha), beta.ppf(0.99, alpha, alpha), 100)\nplt.plot(x, beta.pdf(x, alpha, alpha), 'r-', lw=5, alpha=0.6, label='beta pdf')\nplt.grid()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DNN(pl.LightningModule):\n\n    def __init__(self, input_dim, output_dim, nn_depth, nn_width, dropout, momentum, alpha=0.8):\n        super().__init__()\n        \n        self.alpha = alpha\n        \n        self.bn_in = nn.BatchNorm1d(input_dim, momentum=momentum)\n        self.dp_in = nn.Dropout(dropout)\n        self.ln_in = nn.Linear(input_dim, nn_width, bias=False)\n\n        self.bnorms = nn.ModuleList([nn.BatchNorm1d(nn_width, momentum=momentum) for i in range(nn_depth-1)])\n        self.dropouts = nn.ModuleList([nn.Dropout(dropout) for i in range(nn_depth-1)])\n        self.linears = nn.ModuleList([nn.Linear(nn_width, nn_width, bias=False) for i in range(nn_depth-1)])\n        \n        self.bn_out = nn.BatchNorm1d(nn_width, momentum=momentum)\n        self.dp_out = nn.Dropout(dropout/2)\n        self.ln_out = nn.Linear(nn_width, output_dim, bias=False)\n\n        self.loss = nn.BCEWithLogitsLoss()\n\n    def forward(self, x):\n        x = self.bn_in(x)\n        x = self.dp_in(x)\n        x = nn.functional.relu(self.ln_in(x))\n\n        for bn_layer,dp_layer,ln_layer in zip(self.bnorms,self.dropouts,self.linears):\n            x = bn_layer(x)\n            x = dp_layer(x)\n            x = ln_layer(x)\n            x = nn.functional.relu(x)\n            \n        x = self.bn_out(x)\n        x = self.dp_out(x)\n        x = self.ln_out(x)\n        return x\n\n    def training_step(self, batch, batch_idx):\n        X, y = batch\n        \n        lam = np.random.beta(self.alpha,self.alpha)\n        lam = torch.FloatTensor([lam]).to(self.device)\n        n = len(batch)//2\n        X = lam * X[:n,:] + (1-lam) * X[n:,:]\n        y = lam * y[:n,:] + (1-lam) * y[n:,:]\n        \n        y_hat = self.forward(X)\n        loss = self.loss(y_hat, y)\n        self.log('train_loss', loss)\n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        X, y = batch\n        y_hat = self.forward(X)\n        loss = self.loss(y_hat, y)\n        self.log('valid_loss', loss)\n        \n    def test_step(self, batch, batch_idx):\n        X, y = batch\n        y_logit = self.forward(X)\n        y_probs = torch.sigmoid(y_logit).detach().cpu().numpy()\n        loss = self.loss(y_logit, y)\n        metric = roc_auc_score(y.cpu().numpy(), y_probs)\n        self.log('test_loss', loss)\n        self.log('test_metric', metric)\n        \n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=5e-3, weight_decay=1e-4)\n        scheduler = {\n            'scheduler': ReduceLROnPlateau(\n                optimizer, \n                mode=\"min\", \n                factor=0.5, \n                patience=5, \n                min_lr=1e-5),\n            'interval': 'epoch',\n            'frequency': 1,\n            'reduce_on_plateau': True,\n            'monitor': 'valid_loss',\n        }\n        return [optimizer], [scheduler]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = DNN(\n    input_dim=len(input_features), \n    output_dim=1, \n    nn_depth=3, \n    nn_width=128, \n    dropout=0.2, \n    momentum=0.1,\n    alpha=0.25,\n)\n\nlogger = logger = CSVLogger(\"logs\", name=\"mlp_w_mixup\")\n\nearly_stop_callback = EarlyStopping(\n   monitor='valid_loss',\n   min_delta=.0,\n   patience=20,\n   verbose=True,\n   mode='min'\n)\n\ntrainer = pl.Trainer(\n    callbacks=[early_stop_callback], \n    min_epochs=10, \n    max_epochs=200, \n    gpus=0, \n    logger=logger,\n    deterministic=True\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summarize()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.fit(\n    model, \n    DataLoader(train_tensor_dset, batch_size=1024, shuffle=True, num_workers=4, drop_last=True),\n    DataLoader(valid_tensor_dset, batch_size=1024, shuffle=False, num_workers=4)\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# AUC on validation dataset\ntrainer.test(model, DataLoader(valid_tensor_dset, batch_size=1024, shuffle=False, num_workers=4))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# AUC on test dataset\ntrainer.test(model, DataLoader(test_tensor_dset, batch_size=1024, shuffle=False, num_workers=4))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics = pd.read_csv(\"logs/mlp_w_mixup/version_0/metrics.csv\")\n\ndf1 = metrics.loc[:,[\"step\",\"train_loss\"]].dropna()\ndf2 = metrics.loc[:,[\"step\",\"valid_loss\"]].dropna()\n\nplt.figure(figsize=(12,5))\nplt.plot(df1.step, df1.train_loss, \"o-\", label=\"train_loss\")\nplt.plot(df2.step, df2.valid_loss, \"o-\", label=\"valid_loss\")\nplt.grid()\nplt.legend(loc=\"best\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***","metadata":{}}]}