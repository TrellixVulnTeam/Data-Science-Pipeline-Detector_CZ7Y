{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import scale, StandardScaler\nimport matplotlib.pyplot as plt\n!pip install kneed\n# kneed is not installed in kaggle. uncomment the above line.\nfrom kneed import KneeLocator\nfrom sklearn.linear_model import LassoCV\nimport xgboost as xg\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"dataset = pd.read_csv(\"/kaggle/input/santander-customer-transaction-prediction/train.csv\", index_col = 0)\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will see if there is any missing values in any of the columns.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_info = dataset.drop(\"target\", axis = 1).isna().sum()\nmissing_info[missing_info > 0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is no missing value in any column. We will see if we have any categorical variables in the data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dtype_column = dataset.drop(\"target\", axis = 1).dtypes\nlen(dtype_column[dtype_column == 'float64'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is no categorical variables in the data. We would also like to see the distribution of target variable across diferent class to see if there is any class which is underrepresnted which is often the case in retail modeling.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.target.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the class 1 is under represented in the data. We might need to add the a class weight later to take care of that. For now we will leave it as like.\n\n* Now we can do some plotting and correlation analysis but it's difficult to visualize the data because of the size of the dataset. So we would like to work on some dimensionality reduction. \n* One way to do that is using the lasso regression.\n* Another thing we would like to see is how many dimensions are enough to repsent the data. We would do some PCA to get the idea about that.\n\n### PCA","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scaling the data is quite important in PCA. So let's do that first.\nX = scale(dataset.drop('target', axis = 1).values) \n\n# Fitting a PCA\npca200comp = PCA(n_components = 200).fit(X)\n\n# Plotting the variance explained by each component.\nfig, (ax1,ax2) = plt.subplots(1,2, figsize = (14,5))\nax1.plot(list(range(1,201)),pca200comp.explained_variance_ratio_, marker = \"o\")\nax1.set_xlabel('Principal Component')\nax1.set_ylabel('Prop. Variance Explained')\n\n# Plotting the cumulative variance explained by each component.\nax2.plot(list(range(1,201)),pca200comp.explained_variance_ratio_.cumsum(), marker = \"o\")\nax2.set_xlabel('Principal Component')\nax2.set_ylabel('Cumulative Prop. Variance Explained')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The plot is quite interesting. \n* Our first component has explained only 6% of the variance.\n* The drop in variance explained is linear. So we don't see any elbow shape curve that we can use.\nLet's try the knee locator to check if there is any knee in the plot that is not visually clear.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can find the elbow using KneeLocator.\nkl = KneeLocator(range(1, 201), pca200comp.explained_variance_ratio_, curve=\"convex\", direction=\"decreasing\")\nkl.elbow","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let's see what lasso regression tells us.\n\n### Lasso Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Code for fitting ridge regression model for different values of lambda.\nX = dataset.drop('target', axis = 1).values\ny = np.array(dataset['target'])\n\n# Scaling the variables\nscaler = StandardScaler().fit(X)\nX = scaler.transform(X)\n\n#lambda_range = np.linspace(1,1000,100) # Setting a range of lambda\n\ncv_Lasso = LassoCV().fit(X,y) # CV is 5 fold by default.\nprint(cv_Lasso.alpha_)\nprint(len(cv_Lasso.coef_[cv_Lasso.coef_ == 0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Lasso_coef = pd.DataFrame(cv_Lasso.coef_, index = dataset.drop('target', axis = 1).columns, columns = [\"Coef\"])\nLasso_coef[\"abs_coef\"] = np.abs(Lasso_coef[\"Coef\"])\nLasso_coef.sort_values([\"abs_coef\"], ascending = False, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's try 2 attempts:-\n1. Trying XGboost with all the variables with default setting. \n2. Trying XGboost with top variables from Lasso with default setting. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train and test split of the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 7)\n\n# Running a XGBoost with default settings.\nmodel = xg.XGBClassifier()\nmodel.fit(X_train, y_train)\n\n# Check the accuracy of the model on train and test dataset.\naccuracy_train = accuracy_score(y_train, model.predict(X_train))\nprint(\"Accuracy on train dataset %.2f%%\" % (accuracy_train * 100))\n\naccuracy_test = accuracy_score(y_test, model.predict(X_test))\nprint(\"Accuracy on test dataset %.2f%%\" % (accuracy_test * 100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a confusion matrix \nprint(confusion_matrix(y_test, model.predict(X_test)))\nprint(classification_report(y_test, model.predict(X_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Selecting the top 25 variables.\ndata_top_lasso = dataset[Lasso_coef[:75].index]\ndata_top_lasso = data_top_lasso.values\n\n# Train and test split of the data\nX_train, X_test, y_train, y_test = train_test_split(data_top_lasso, y, test_size = 0.33, random_state = 7)\n\n# Running a XGBoost with default settings.\nmodel = xg.XGBClassifier(tree_method='gpu_hist')\nmodel.fit(X_train, y_train)\n\n# Check the accuracy of the model on train and test dataset.\naccuracy_train = accuracy_score(y_train, model.predict(X_train))\nprint(\"Accuracy on train dataset %.2f%%\" % (accuracy_train * 100))\n\naccuracy_test = accuracy_score(y_test, model.predict(X_test))\nprint(\"Accuracy on test dataset %.2f%%\" % (accuracy_test * 100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a confusion matrix \nprint(confusion_matrix(y_test, model.predict(X_test)))\nprint(classification_report(y_test, model.predict(X_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that adding more features is not improving the accuracy too much\nLet's try some tuning and fit again","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train and test split of the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 7)\n\nclassifier = xg.XGBClassifier(n_thread = -1, tree_method='gpu_hist')\nparam_grid = {\n    \"n_estimators\" : np.arange(100, 500, 50),\n    \"max_depth\" : np.arange(1, 20, 3),\n    \"colsample_bytree\": np.arange(0.5,1, 0.1),\n    \"criterion\": [\"gini\",'entropy']\n}\nmodel = RandomizedSearchCV(estimator = classifier,\n                          param_distributions = param_grid,\n                          n_iter = 10,\n                          scoring = \"accuracy\",\n                          verbose = 10,\n                          n_jobs = -1,\n                          cv = 5)\nmodel.fit(X_train, y_train)\nmodel.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(model.best_estimator_.get_params())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_test = accuracy_score(y_test, model.predict(X_test))\nprint(\"Accuracy on test dataset %.2f%%\" % (accuracy_test * 100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}