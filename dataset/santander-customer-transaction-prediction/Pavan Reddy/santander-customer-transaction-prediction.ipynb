{"cells":[{"metadata":{},"cell_type":"markdown","source":"<img src = \"https://www.santanderbank.com/us/documents/22507/2202391/DebitCardPage-Debit-Prmier-Cards_437x336.png/68eaaf59-35ec-44c7-82f8-2cf46a38d733?t=1554058009547\" width=\"400\"></img>"},{"metadata":{},"cell_type":"markdown","source":"# Introduction\nSantander always looking for ways to help their customers to understand financial health and identify which products and services might help them achieve their monetary goals.\n\nProblem Statement:\nis a customer satisfied? Will a customer buy this product? Can a customer pay this loan?\n\nDataset contains numeric feature variables, the binary target column, and a string ID_code column.\nThe task is to predict the value of target column in the test set.\n\nData: \n1. train.csv - the training set.\n2. test.csv - the test set.  \n3. sample_submission.csv  \n\nData reference: (https://www.kaggle.com/c/santander-customer-transaction-prediction/data)"},{"metadata":{},"cell_type":"markdown","source":"# <a id='0'>Content</a>\n\n- <a href='#1'>1. Read the data</a>\n- <a href='#2'>2. Data Understanding</a>\n - <a href='#5'>2.1 Missing values</a>\n - <a href='#6'>2.2 Statistics</a>\n- <a href='#3'>3. Data Exploration</a>\n - <a href='#7'>3.1 Distribution of Train vs Test</a>\n - <a href='#8'>3.2 Distribution of Y variable</a>\n - <a href='#9'>3.3 Distribution of X variables</a>\n - <a href='#10'>3.4 Correlation</a>\n - <a href='#11'>3.5 Repeated values</a>\n- <a href='#4'>4. Additional Features</a>\n- <a href='#5'>5. Model</a>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Input path\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Import necessary libraries\n\nimport numpy as np\nimport pandas as pd\n\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score, roc_curve\n\nfrom sklearn.model_selection import train_test_split\n\nimport lightgbm as lgb\nimport skopt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id='1'>1. Read the data</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read the data\n\ntrain = pd.read_csv('/kaggle/input/santander-customer-transaction-prediction/train.csv')\ntest  = pd.read_csv('/kaggle/input/santander-customer-transaction-prediction/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('train shape:', train.shape)\nprint('test shape:', test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id='2'>2. Data Understanding</a>"},{"metadata":{},"cell_type":"markdown","source":"### <a id='5'>2.1 Missing values</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finding missing values in train and test data\n\ndef func(df):\n    a = df.isnull().sum()\n    b = df.count()\n    c = (a/b) * 100\n    d = pd.DataFrame(a, columns = ['Missingvalue%'])\n    return d['Missingvalue%'].sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('missing values in train data:', func(train))\nprint('missing values in test data:', func(test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Summary:\nThere is no missing value in train and test dataset"},{"metadata":{},"cell_type":"markdown","source":"### <a id='6'>2.2 Statistics</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Summary: \nmin, max, mean and std dev vaues seems to be similar for train and test data"},{"metadata":{},"cell_type":"markdown","source":"# <a id='3'>3. Data Exploration</a>"},{"metadata":{},"cell_type":"markdown","source":"### <a id='7'>3.1 Distribution of Train vs Test</a>"},{"metadata":{},"cell_type":"markdown","source":"#### Let's try to plot Train[variables] vs Test[variables] for few features"},{"metadata":{"trusted":true},"cell_type":"code","source":"features = ['var_0', 'var_1','var_2','var_3', 'var_4', 'var_5', 'var_6', 'var_7', 'var_8', 'var_9',]  \n\ni = 0\nfig, ax = plt.subplots(figsize=(12,12))\n\nfor feature in features:    \n    i = i + 1\n    plt.subplot(4,4,i)     \n    plt.scatter(train[feature], test[feature])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <a id='8'>3.2 Distribution of Y variable</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Y distribution in train data\n\nsns.countplot(train.target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('% of 1 in train data:', (train.target.value_counts()[1]/train.shape[0]) * 100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Summary: \nData is highly imbalanced, only 10% of 1's in target(Y) column"},{"metadata":{},"cell_type":"markdown","source":"### <a id='9'>3.3 Distribution of X variables</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# function to generate subplots for ('X' variables vs 'Y')\n\ndef plot_feature_distribution(df1, df2, label1, label2, features):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(10,10,figsize=(18,22))\n\n    for feature in features:\n        i += 1\n        plt.subplot(10,10,i)\n        sns.distplot(df1[feature], hist=False,label=label1)\n        sns.distplot(df2[feature], hist=False,label=label2)\n        plt.xlabel(feature, fontsize=9)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=6, pad=-6)\n        plt.tick_params(axis='y', which='major', labelsize=6)\n    plt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets plot for few variables.\n\ndf1 = train.loc[train['target'] == 0]\ndf2 = train.loc[train['target'] == 1]\nfeatures = train.columns.values[2:102]\nplot_feature_distribution(df1, df2, '0', '1', features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Summary: \nMost of the variables seems to be normally distributed."},{"metadata":{},"cell_type":"markdown","source":"### <a id='10'>3.4 Correlation</a>"},{"metadata":{},"cell_type":"markdown","source":"#### Relationship between X variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"correlations = train[features].corr().abs().unstack().sort_values(kind=\"quicksort\").reset_index()\ncorrelations.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Summary:\nIt's clear that, relationship between X variables is very low."},{"metadata":{},"cell_type":"markdown","source":"### <a id='11'>3.5 Repeated values</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"features = train.columns.values[2:202]\n\nunique_max_train = []\nfor feature in features:\n    values = train[feature].value_counts()\n    unique_max_train.append([feature, values.max(), values.idxmax()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.transpose((pd.DataFrame(unique_max_train, columns=['Feature', 'Count', 'Value'])).\\\n            sort_values(by = 'Count', ascending=False).head(10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id='4'>4. Additional Features</a>"},{"metadata":{},"cell_type":"markdown","source":"#### As we dont have much information on columns, lets try to create aggregated columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"idx = features = train.columns.values[2:202]\nfor df in [test, train]:\n    df['sum'] = df[idx].sum(axis=1)  \n    df['min'] = df[idx].min(axis=1)\n    df['max'] = df[idx].max(axis=1)\n    df['avg'] = df[idx].mean(axis=1)\n    df['std'] = df[idx].std(axis=1)     \n    df['med'] = df[idx].median(axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id='5'>5. Model</a>"},{"metadata":{},"cell_type":"markdown","source":"### LightGBM (Leaf-Wise growth)\n\nhttps://lightgbm.readthedocs.io/en/latest/Features.html\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X columns\nfeatures = [c for c in train.columns if c not in ['ID_code', 'target']]\n\n# Y volumn\ny = train['target']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Some of the parameters of 'LightGBM'"},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {      'learning_rate': 0.01,\n                'max_depth': -1,\n                'num_leaves': 12,\n                'feature_fraction': 0.1,\n                'subsample': 0.2,\n                'objective': 'binary',\n                 'metric': 'auc',\n                 'is_unbalance': True,\n                 'bagging_freq': 5,\n                 'boosting': 'gbdt' }                 ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"folds = StratifiedKFold(n_splits = 5, shuffle = False)\n\noof = np.zeros(len(train))\n#predictions = np.zeros(len(test))\n\nfor fold_, (idxT, idxV) in enumerate(folds.split(train.values, y.values)):\n    print(\"Fold {}\".format(fold_))\n    \n    X_train = train.iloc[idxT][features]\n    y_train = y.iloc[idxT] \n    X_val =   train.iloc[idxV][features] \n    y_val = y.iloc[idxV]\n        \n    train_data = lgb.Dataset(X_train, y_train)\n    val_data   = lgb.Dataset(X_val, y_val)\n    \n    clf = lgb.train(params =  params ,                    \n                    train_set = train_data, \n                    valid_sets = [train_data, val_data], \n                    num_boost_round = 20000,\n                    verbose_eval = 1000, \n                    early_stopping_rounds = 5000)\n    \n    oof[idxV] = clf.predict(X_val, num_iteration=clf.best_iteration)\n    \nprint(\"CV score: {:<8.5f}\".format(roc_auc_score(y, oof)))   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\n Validation accuracy is around 90% and can be further improved with below\n \n 1) Tuning Hyperparameters\n \n 2) Testing with other models\n \n 3) Ensemble of different models"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}