{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\ndf = pd.read_csv('../input/santander-customer-transaction-prediction/train.csv',index_col=0)\n\nzeros_index = df.loc[df.target == 0,:].index\nones_index = df.loc[df.target == 1,:].index\n\nfinal_list = list(zeros_index)\nfinal_list.extend(list(np.random.choice(ones_index, 2000, replace=False)))\ndf = df.loc[final_list,:]\ndf.to_csv('undersampled.csv',index=False)\ndel zeros_index,ones_index","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Note hclip and lclip isnt being used in the data_query but its still passed","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Run below commands in google colab\n# install Java8\n!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n# download spark3.0.0\n!wget -q https://downloads.apache.org/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n# unzip it\n!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n# install findspark \n!pip install -q findspark","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nos.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\nos.environ[\"SPARK_HOME\"] = \"./spark-3.1.1-bin-hadoop3.2\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import findspark\nfindspark.init()\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('Santander Fast Sample').getOrCreate()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_customer=spark.read.csv('./undersampled.csv', header=True, inferSchema=True)\n#print(data_customer.columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Getting lcip and hclip before beginning sampling\n\nperc_query = '''\nselect\n    percentile_approx({column}, array(0.95, 0.01)) as perc\nfrom {data_table}\nwhere {date_column} = '{ing_date}'\nand {segment_column} = \"{segment}\"\nand model_name= '{model_name}'\n'''.format(column='frm_dnb_employee_site_cnt', db='ws_mkt_dst',\n            data_table='tb_sb_gbt_meta_purch_mdls_ads_uid015_v004', ing_date='2021-01-01',\n            segment_column='lcm_segment_sb_chnl',\n            date_column='selection_date',\n            segment='development', model_name='sb_metapurchase_sb_chnl_30d')\n\n# frm_dnb_employee_site_cnt is used by Bucketizer to create distinct bins in case of use of fast_sampler.py\n# ws_mkt_dst.tb_sb_gbt_meta_purch_mdls_ads_uid015_v004 is the Master ADS for UK. This is the main database for UK Metapurchase\n# lcm_segment_sb_chnl='development'  Chose any One of the LCM segment\n# sb_metapurchase_sb_chnl_30d The model name tells we are training a Metapurchase Model which predicts for 30 days\nperc_query_completed = '''\nselect percentile_approx(frm_dnb_employee_site_cnt, array(0.95, 0.01)) as perc \nfrom ws_mkt_dst.tb_sb_gbt_meta_purch_mdls_ads_uid015_v004\nwhere selection_date='2021-01-01'\nand\nlcm_segment_sb_chnl='development'\nand\nmodel_name='sb_metapurchase_sb_chnl_30d'\n'''\n\noutput = [30.0,0.0] # spark2array(pquery, 'perc')[0]) in the original code\n\nhclip, lclip = list(map(int, output)) # hclip, lclip = list(map(int, spark2array(pquery, 'perc')[0]))\nprint(hclip,lclip)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# transformed, c_vals = compute_clusters(df_zero, n, clips, k)\n# Enetering Compute Clusters   \n\n# Sampling Function \n#sampling(data_query, clips, k, tgt_column, sample_column, rid_name,segment, date, type, split, gamma, segment_column,date_column, force=False) \nk= 10\ntgt_column  = 'target'\nsplit = 0.95\ndata_customer = data_customer.na.fill(k + 1)\ndf_zero = data_customer.filter('{} = 0'.format(tgt_column))\ndf_ones = data_customer.filter('{} = 1'.format(tgt_column))\nn_ones = df_ones.count()\nn = df_zero.count()\nz_perc = float(n_ones * split) / float((1 - split) * n) # percentage by which you have to downsample  the majority class according to split (0.95)\nprint(z_perc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Entering Computer Clusters Subfunction : compute_clusters(df, n, clips, k, remove_clusters=False, min_req_clusters=5)\n#transformed, c_vals = compute_clusters(df_zero, n, clips, k)\n\nk = 10\nmin_req_clusters=5\nclips  = (lclip,hclip)\n\nsplits = np.linspace(clips[0], clips[1],11) # array([30., 27., 24., 21., 18., 15., 12.,  9.,  6.,  3.,  0.])\nsplits  = splits[1:-1] # array([27., 24., 21., 18., 15., 12.,  9.,  6.,  3.]) exlcudes the first and last values that is hclip and lclip\nsplits = np.insert(splits, [0, len(splits)], [-float('inf'), float('inf')]) #add NegativeInfinity and PositiveInfinity as the bounds of your splits to prevent out of Bucketizer bounds exception.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Bucketizer\nfrom pyspark.ml.feature import Bucketizer\nfrom pyspark.sql import functions as F\nbucketizer = Bucketizer(splits=splits, inputCol=\"var_45\", outputCol=\"clusters_ids\") # choosing a variable with high variance here out of the dataset\ndf = bucketizer.transform(df_zero)\ncm = df.groupBy('clusters_ids').agg(F.count('*'))\ncm.show()\ncluster_counts = np.array([[x['clusters_ids'], x['count(1)']] for x in cm.collect()])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"remove_clusters=False\n\n\n# if remove_clusters is set to true , default = False running remove_unpopulated_clusters(cluster_counts, n = df_zero.count(), thresh=0.02)\nthresh = 0.2\ncluster_perc = cluster_counts[:, 1] / float(n)\ncluster_sorted_idx = np.argsort(cluster_perc)\n\n# clusters that contain less then `trash_thresh`% of the dataset\ntrash_mask = np.cumsum(cluster_perc[cluster_sorted_idx]) > thresh\n# clusters have more then 1% of the dataset\nbig_clusters = cluster_perc[cluster_sorted_idx] > 0.01\nkept_clusters = trash_mask | big_clusters\n\n# sort clusters based on percantage\nvals = cluster_counts[cluster_sorted_idx]\nc_vals = vals[kept_clusters] # Clusters that remain\n\nif len(c_vals) < min_req_clusters:\n    raise RuntimeError(\"Number of clusters is less then five\")\nelse:\n    c_vals = cluster_counts\n    c_vals[:, 0] = list(map(int,c_vals[:, 0])) # just convert clusters to integers\n    print(c_vals)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Get Sample Fractions\ngamma = 0.2\nforce = False\nperc = z_perc\nclusters = cluster_counts\n\nprev_dist_perc = gamma * perc\nuni_dist_perc = perc * (1 - gamma)\nprint(prev_dist_perc,uni_dist_perc)\nn_uni_dist = float(sum(clusters[:, 1]) * uni_dist_perc)\nn_clusters = len(clusters)\n\nfractions = {x[0]: min([prev_dist_perc + (n_uni_dist / float((x[1] * n_clusters))), 1.0]) for x in clusters}\nfractions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sampled_data  = df.stat.sampleBy('clusters_ids', fractions={8.0: 1.0,\n                                                            0.0: 0.06491368862359255,\n                                                            7.0: 1.0,\n                                                            1.0: 0.4356199635996894,\n                                                            4.0: 0.572786234615266,\n                                                            3.0: 0.52717104788937,\n                                                            2.0: 0.48053818664338255,\n                                                            6.0: 0.7962134763671148,\n                                                            5.0: 0.6636679533882314,\n                                                            9.0: 0.6916791040480812}, seed=1)\nsampled_data.groupBy('target').agg(F.count('*')).show() # reduced from 179902","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Combining the responders and non_responders\nrandom_cols = ['target', 'var_0', 'var_1', 'var_2', 'var_3', 'var_4', 'var_5', 'var_6', 'var_7']\npurchasers_data = df_ones.select(random_cols)\nsampled_data = sampled_data.select(random_cols).union(purchasers_data)\nsampled_data.groupBy('target').agg(F.count('*')).show()\nprint(1- (2000/37220), 'is the required split as set')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}