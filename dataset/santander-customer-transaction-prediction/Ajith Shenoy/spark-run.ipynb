{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom imblearn.under_sampling import RandomUnderSampler\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/santander-customer-transaction-prediction/train.csv',index_col=0)\n\nzeros_index = df.loc[df.target == 0,:].index\nones_index = df.loc[df.target == 1,:].index\n\nfinal_list = list(zeros_index)\nfinal_list.extend(list(np.random.choice(ones_index, 2000, replace=False)))\ndf = df.loc[final_list,:]\ndf.to_csv('undersampled.csv',index=False)\ndel zeros_index,ones_index","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Run below commands in google colab\n# install Java8\n!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n# download spark3.0.0\n!wget -q https://downloads.apache.org/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n# unzip it\n!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n# install findspark \n!pip install -q findspark","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nos.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\nos.environ[\"SPARK_HOME\"] = \"./spark-3.1.1-bin-hadoop3.2\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import findspark\nfindspark.init()\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('Santander Customer Classification').getOrCreate()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_customer=spark.read.csv('./undersampled.csv', header=True, inferSchema=True)\nprint(data_customer.columns)\ndata_customer=data_customer.na.fill(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tgt_column  = 'target'\nsplit = 0.95\ndf_zero = data_customer.filter('{} = 0'.format(tgt_column))\ndf_ones = data_customer.filter('{} = 1'.format(tgt_column))\nn_ones = df_ones.count()\nn = df_zero.count()\nz_perc = float(n_ones * split) / float((1 - split) * n)\nprint(z_perc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Vector Assembler Excluding the target Column , the original function in code also involves standarization\nnum_cols = data_customer.columns[1:] # float_cols used in original code \nfrom pyspark.ml.feature import VectorAssembler\nassemble = VectorAssembler(inputCols=num_cols, outputCol='features')\nassembled_data=assemble.transform(df_zero)\n\n#Standardizing Data\nscale=StandardScaler(inputCol='features',outputCol='standardized',withMean=True,withStd=True)\ndata_scale=scale.fit(assembled_data)\ndata_scale_output = data_scale.transform(assembled_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pyspark.ml.clustering import KMeans\nfrom pyspark.ml.evaluation import ClusteringEvaluator\nfrom pyspark.sql import functions as F","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compute Clusters and the steps that follow \nkmeans = KMeans(k=5,featuresCol='standardized',\n                        maxIter=100, seed=10, initSteps=10)\nmodel = kmeans.fit(data_scale_output.select('standardized'))\ntransformed = model.transform(data_scale_output)\ncm = transformed.groupBy('prediction').agg(F.count('*'))\ncm.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"thresh = 0.02 # atleast these many % of data should be there in each cluster\n#  returns total records per cluster [ [1,76], ...]\ncluster_counts =  np.array( [ [x['prediction'], x['count(1)'] ] for x in cm.collect()] )\ncluster_perc = cluster_counts[:, 1] / float(n) # get percentages\nprint(cluster_perc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Removing Unpopulated Clusters\ncluster_sorted_idx = np.argsort(cluster_perc)\ntrash_mask = np.cumsum(cluster_perc[cluster_sorted_idx]) > thresh # [False,  True,  True,  True,  True]\nbig_clusters = cluster_perc[cluster_sorted_idx] > 0.01\nkept_clusters = trash_mask | big_clusters\nvals = cluster_counts[cluster_sorted_idx]\nclusters = vals[kept_clusters]\nlen(clusters)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#get_sample_fractions(z_perc, gamma, n, c_vals, force)\ngamma = 0.2\nforce = False\nperc = z_perc\n\nprev_dist_perc = gamma * perc\nuni_dist_perc = perc * (1 - gamma)\nprint(prev_dist_perc,uni_dist_perc)\nn_uni_dist = float(sum(clusters[:, 1]) * uni_dist_perc)\nn_clusters = len(clusters)\n\nfractions = {x[0]: min([prev_dist_perc + (n_uni_dist / float((x[1] * n_clusters))), 1.0]) for x in clusters}\nfractions\nsampled_data  = transformed.stat.sampleBy('prediction', fractions={0: 0.2122923328983849,\n                                                                 1: 0.21198470311434814,\n                                                                 4: 0.2106494748652401,\n                                                                 3: 0.20999896400126306})\nsampled_data.groupBy('target').agg(F.count('*')).show() # reduced from 179902","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Combining the responders and non_responders\nrandom_cols = ['target', 'var_0', 'var_1', 'var_2', 'var_3', 'var_4', 'var_5', 'var_6', 'var_7', 'var_8', 'var_9', 'var_10']\npurchasers_data = df_ones.select(random_cols)\nsampled_data = sampled_data.select(random_cols).union(purchasers_data)\nsampled_data.groupBy('target').agg(F.count('*')).show()\nprint(1- (2000/37865), 'is the required split as set')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}