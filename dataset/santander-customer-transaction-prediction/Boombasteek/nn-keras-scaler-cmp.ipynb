{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# let's get the dataset\n\nimport pandas as pd\ndata = pd.read_csv('../input/train.csv')","execution_count":11,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# have a look at the first few rows\n\ndata.head()","execution_count":12,"outputs":[{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"   ID_code  target    var_0   var_1   ...     var_196  var_197  var_198  var_199\n0  train_0       0   8.9255 -6.7863   ...      7.8784   8.5635  12.7803  -1.0914\n1  train_1       0  11.5006 -4.1473   ...      8.1267   8.7889  18.3560   1.9518\n2  train_2       0   8.6093 -2.7457   ...     -6.5213   8.2675  14.7222   0.3965\n3  train_3       0  11.0604 -2.1518   ...     -2.9275  10.2922  17.9697  -8.9996\n4  train_4       0   9.8369 -1.4834   ...      3.9267   9.5031  17.9974  -8.8104\n\n[5 rows x 202 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID_code</th>\n      <th>target</th>\n      <th>var_0</th>\n      <th>var_1</th>\n      <th>var_2</th>\n      <th>var_3</th>\n      <th>var_4</th>\n      <th>var_5</th>\n      <th>var_6</th>\n      <th>var_7</th>\n      <th>var_8</th>\n      <th>var_9</th>\n      <th>var_10</th>\n      <th>var_11</th>\n      <th>var_12</th>\n      <th>var_13</th>\n      <th>var_14</th>\n      <th>var_15</th>\n      <th>var_16</th>\n      <th>var_17</th>\n      <th>var_18</th>\n      <th>var_19</th>\n      <th>var_20</th>\n      <th>var_21</th>\n      <th>var_22</th>\n      <th>var_23</th>\n      <th>var_24</th>\n      <th>var_25</th>\n      <th>var_26</th>\n      <th>var_27</th>\n      <th>var_28</th>\n      <th>var_29</th>\n      <th>var_30</th>\n      <th>var_31</th>\n      <th>var_32</th>\n      <th>var_33</th>\n      <th>var_34</th>\n      <th>var_35</th>\n      <th>var_36</th>\n      <th>var_37</th>\n      <th>...</th>\n      <th>var_160</th>\n      <th>var_161</th>\n      <th>var_162</th>\n      <th>var_163</th>\n      <th>var_164</th>\n      <th>var_165</th>\n      <th>var_166</th>\n      <th>var_167</th>\n      <th>var_168</th>\n      <th>var_169</th>\n      <th>var_170</th>\n      <th>var_171</th>\n      <th>var_172</th>\n      <th>var_173</th>\n      <th>var_174</th>\n      <th>var_175</th>\n      <th>var_176</th>\n      <th>var_177</th>\n      <th>var_178</th>\n      <th>var_179</th>\n      <th>var_180</th>\n      <th>var_181</th>\n      <th>var_182</th>\n      <th>var_183</th>\n      <th>var_184</th>\n      <th>var_185</th>\n      <th>var_186</th>\n      <th>var_187</th>\n      <th>var_188</th>\n      <th>var_189</th>\n      <th>var_190</th>\n      <th>var_191</th>\n      <th>var_192</th>\n      <th>var_193</th>\n      <th>var_194</th>\n      <th>var_195</th>\n      <th>var_196</th>\n      <th>var_197</th>\n      <th>var_198</th>\n      <th>var_199</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>train_0</td>\n      <td>0</td>\n      <td>8.9255</td>\n      <td>-6.7863</td>\n      <td>11.9081</td>\n      <td>5.0930</td>\n      <td>11.4607</td>\n      <td>-9.2834</td>\n      <td>5.1187</td>\n      <td>18.6266</td>\n      <td>-4.9200</td>\n      <td>5.7470</td>\n      <td>2.9252</td>\n      <td>3.1821</td>\n      <td>14.0137</td>\n      <td>0.5745</td>\n      <td>8.7989</td>\n      <td>14.5691</td>\n      <td>5.7487</td>\n      <td>-7.2393</td>\n      <td>4.2840</td>\n      <td>30.7133</td>\n      <td>10.5350</td>\n      <td>16.2191</td>\n      <td>2.5791</td>\n      <td>2.4716</td>\n      <td>14.3831</td>\n      <td>13.4325</td>\n      <td>-5.1488</td>\n      <td>-0.4073</td>\n      <td>4.9306</td>\n      <td>5.9965</td>\n      <td>-0.3085</td>\n      <td>12.9041</td>\n      <td>-3.8766</td>\n      <td>16.8911</td>\n      <td>11.1920</td>\n      <td>10.5785</td>\n      <td>0.6764</td>\n      <td>7.8871</td>\n      <td>...</td>\n      <td>15.4576</td>\n      <td>5.3133</td>\n      <td>3.6159</td>\n      <td>5.0384</td>\n      <td>6.6760</td>\n      <td>12.6644</td>\n      <td>2.7004</td>\n      <td>-0.6975</td>\n      <td>9.5981</td>\n      <td>5.4879</td>\n      <td>-4.7645</td>\n      <td>-8.4254</td>\n      <td>20.8773</td>\n      <td>3.1531</td>\n      <td>18.5618</td>\n      <td>7.7423</td>\n      <td>-10.1245</td>\n      <td>13.7241</td>\n      <td>-3.5189</td>\n      <td>1.7202</td>\n      <td>-8.4051</td>\n      <td>9.0164</td>\n      <td>3.0657</td>\n      <td>14.3691</td>\n      <td>25.8398</td>\n      <td>5.8764</td>\n      <td>11.8411</td>\n      <td>-19.7159</td>\n      <td>17.5743</td>\n      <td>0.5857</td>\n      <td>4.4354</td>\n      <td>3.9642</td>\n      <td>3.1364</td>\n      <td>1.6910</td>\n      <td>18.5227</td>\n      <td>-2.3978</td>\n      <td>7.8784</td>\n      <td>8.5635</td>\n      <td>12.7803</td>\n      <td>-1.0914</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>train_1</td>\n      <td>0</td>\n      <td>11.5006</td>\n      <td>-4.1473</td>\n      <td>13.8588</td>\n      <td>5.3890</td>\n      <td>12.3622</td>\n      <td>7.0433</td>\n      <td>5.6208</td>\n      <td>16.5338</td>\n      <td>3.1468</td>\n      <td>8.0851</td>\n      <td>-0.4032</td>\n      <td>8.0585</td>\n      <td>14.0239</td>\n      <td>8.4135</td>\n      <td>5.4345</td>\n      <td>13.7003</td>\n      <td>13.8275</td>\n      <td>-15.5849</td>\n      <td>7.8000</td>\n      <td>28.5708</td>\n      <td>3.4287</td>\n      <td>2.7407</td>\n      <td>8.5524</td>\n      <td>3.3716</td>\n      <td>6.9779</td>\n      <td>13.8910</td>\n      <td>-11.7684</td>\n      <td>-2.5586</td>\n      <td>5.0464</td>\n      <td>0.5481</td>\n      <td>-9.2987</td>\n      <td>7.8755</td>\n      <td>1.2859</td>\n      <td>19.3710</td>\n      <td>11.3702</td>\n      <td>0.7399</td>\n      <td>2.7995</td>\n      <td>5.8434</td>\n      <td>...</td>\n      <td>29.4846</td>\n      <td>5.8683</td>\n      <td>3.8208</td>\n      <td>15.8348</td>\n      <td>-5.0121</td>\n      <td>15.1345</td>\n      <td>3.2003</td>\n      <td>9.3192</td>\n      <td>3.8821</td>\n      <td>5.7999</td>\n      <td>5.5378</td>\n      <td>5.0988</td>\n      <td>22.0330</td>\n      <td>5.5134</td>\n      <td>30.2645</td>\n      <td>10.4968</td>\n      <td>-7.2352</td>\n      <td>16.5721</td>\n      <td>-7.3477</td>\n      <td>11.0752</td>\n      <td>-5.5937</td>\n      <td>9.4878</td>\n      <td>-14.9100</td>\n      <td>9.4245</td>\n      <td>22.5441</td>\n      <td>-4.8622</td>\n      <td>7.6543</td>\n      <td>-15.9319</td>\n      <td>13.3175</td>\n      <td>-0.3566</td>\n      <td>7.6421</td>\n      <td>7.7214</td>\n      <td>2.5837</td>\n      <td>10.9516</td>\n      <td>15.4305</td>\n      <td>2.0339</td>\n      <td>8.1267</td>\n      <td>8.7889</td>\n      <td>18.3560</td>\n      <td>1.9518</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>train_2</td>\n      <td>0</td>\n      <td>8.6093</td>\n      <td>-2.7457</td>\n      <td>12.0805</td>\n      <td>7.8928</td>\n      <td>10.5825</td>\n      <td>-9.0837</td>\n      <td>6.9427</td>\n      <td>14.6155</td>\n      <td>-4.9193</td>\n      <td>5.9525</td>\n      <td>-0.3249</td>\n      <td>-11.2648</td>\n      <td>14.1929</td>\n      <td>7.3124</td>\n      <td>7.5244</td>\n      <td>14.6472</td>\n      <td>7.6782</td>\n      <td>-1.7395</td>\n      <td>4.7011</td>\n      <td>20.4775</td>\n      <td>17.7559</td>\n      <td>18.1377</td>\n      <td>1.2145</td>\n      <td>3.5137</td>\n      <td>5.6777</td>\n      <td>13.2177</td>\n      <td>-7.9940</td>\n      <td>-2.9029</td>\n      <td>5.8463</td>\n      <td>6.1439</td>\n      <td>-11.1025</td>\n      <td>12.4858</td>\n      <td>-2.2871</td>\n      <td>19.0422</td>\n      <td>11.0449</td>\n      <td>4.1087</td>\n      <td>4.6974</td>\n      <td>6.9346</td>\n      <td>...</td>\n      <td>13.2070</td>\n      <td>5.8442</td>\n      <td>4.7086</td>\n      <td>5.7141</td>\n      <td>-1.0410</td>\n      <td>20.5092</td>\n      <td>3.2790</td>\n      <td>-5.5952</td>\n      <td>7.3176</td>\n      <td>5.7690</td>\n      <td>-7.0927</td>\n      <td>-3.9116</td>\n      <td>7.2569</td>\n      <td>-5.8234</td>\n      <td>25.6820</td>\n      <td>10.9202</td>\n      <td>-0.3104</td>\n      <td>8.8438</td>\n      <td>-9.7009</td>\n      <td>2.4013</td>\n      <td>-4.2935</td>\n      <td>9.3908</td>\n      <td>-13.2648</td>\n      <td>3.1545</td>\n      <td>23.0866</td>\n      <td>-5.3000</td>\n      <td>5.3745</td>\n      <td>-6.2660</td>\n      <td>10.1934</td>\n      <td>-0.8417</td>\n      <td>2.9057</td>\n      <td>9.7905</td>\n      <td>1.6704</td>\n      <td>1.6858</td>\n      <td>21.6042</td>\n      <td>3.1417</td>\n      <td>-6.5213</td>\n      <td>8.2675</td>\n      <td>14.7222</td>\n      <td>0.3965</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>train_3</td>\n      <td>0</td>\n      <td>11.0604</td>\n      <td>-2.1518</td>\n      <td>8.9522</td>\n      <td>7.1957</td>\n      <td>12.5846</td>\n      <td>-1.8361</td>\n      <td>5.8428</td>\n      <td>14.9250</td>\n      <td>-5.8609</td>\n      <td>8.2450</td>\n      <td>2.3061</td>\n      <td>2.8102</td>\n      <td>13.8463</td>\n      <td>11.9704</td>\n      <td>6.4569</td>\n      <td>14.8372</td>\n      <td>10.7430</td>\n      <td>-0.4299</td>\n      <td>15.9426</td>\n      <td>13.7257</td>\n      <td>20.3010</td>\n      <td>12.5579</td>\n      <td>6.8202</td>\n      <td>2.7229</td>\n      <td>12.1354</td>\n      <td>13.7367</td>\n      <td>0.8135</td>\n      <td>-0.9059</td>\n      <td>5.9070</td>\n      <td>2.8407</td>\n      <td>-15.2398</td>\n      <td>10.4407</td>\n      <td>-2.5731</td>\n      <td>6.1796</td>\n      <td>10.6093</td>\n      <td>-5.9158</td>\n      <td>8.1723</td>\n      <td>2.8521</td>\n      <td>...</td>\n      <td>31.8833</td>\n      <td>5.9684</td>\n      <td>7.2084</td>\n      <td>3.8899</td>\n      <td>-11.0882</td>\n      <td>17.2502</td>\n      <td>2.5881</td>\n      <td>-2.7018</td>\n      <td>0.5641</td>\n      <td>5.3430</td>\n      <td>-7.1541</td>\n      <td>-6.1920</td>\n      <td>18.2366</td>\n      <td>11.7134</td>\n      <td>14.7483</td>\n      <td>8.1013</td>\n      <td>11.8771</td>\n      <td>13.9552</td>\n      <td>-10.4701</td>\n      <td>5.6961</td>\n      <td>-3.7546</td>\n      <td>8.4117</td>\n      <td>1.8986</td>\n      <td>7.2601</td>\n      <td>-0.4639</td>\n      <td>-0.0498</td>\n      <td>7.9336</td>\n      <td>-12.8279</td>\n      <td>12.4124</td>\n      <td>1.8489</td>\n      <td>4.4666</td>\n      <td>4.7433</td>\n      <td>0.7178</td>\n      <td>1.4214</td>\n      <td>23.0347</td>\n      <td>-1.2706</td>\n      <td>-2.9275</td>\n      <td>10.2922</td>\n      <td>17.9697</td>\n      <td>-8.9996</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>train_4</td>\n      <td>0</td>\n      <td>9.8369</td>\n      <td>-1.4834</td>\n      <td>12.8746</td>\n      <td>6.6375</td>\n      <td>12.2772</td>\n      <td>2.4486</td>\n      <td>5.9405</td>\n      <td>19.2514</td>\n      <td>6.2654</td>\n      <td>7.6784</td>\n      <td>-9.4458</td>\n      <td>-12.1419</td>\n      <td>13.8481</td>\n      <td>7.8895</td>\n      <td>7.7894</td>\n      <td>15.0553</td>\n      <td>8.4871</td>\n      <td>-3.0680</td>\n      <td>6.5263</td>\n      <td>11.3152</td>\n      <td>21.4246</td>\n      <td>18.9608</td>\n      <td>10.1102</td>\n      <td>2.7142</td>\n      <td>14.2080</td>\n      <td>13.5433</td>\n      <td>3.1736</td>\n      <td>-3.3423</td>\n      <td>5.9015</td>\n      <td>7.9352</td>\n      <td>-3.1582</td>\n      <td>9.4668</td>\n      <td>-0.0083</td>\n      <td>19.3239</td>\n      <td>12.4057</td>\n      <td>0.6329</td>\n      <td>2.7922</td>\n      <td>5.8184</td>\n      <td>...</td>\n      <td>33.5107</td>\n      <td>5.6953</td>\n      <td>5.4663</td>\n      <td>18.2201</td>\n      <td>6.5769</td>\n      <td>21.2607</td>\n      <td>3.2304</td>\n      <td>-1.7759</td>\n      <td>3.1283</td>\n      <td>5.5518</td>\n      <td>1.4493</td>\n      <td>-2.6627</td>\n      <td>19.8056</td>\n      <td>2.3705</td>\n      <td>18.4685</td>\n      <td>16.3309</td>\n      <td>-3.3456</td>\n      <td>13.5261</td>\n      <td>1.7189</td>\n      <td>5.1743</td>\n      <td>-7.6938</td>\n      <td>9.7685</td>\n      <td>4.8910</td>\n      <td>12.2198</td>\n      <td>11.8503</td>\n      <td>-7.8931</td>\n      <td>6.4209</td>\n      <td>5.9270</td>\n      <td>16.0201</td>\n      <td>-0.2829</td>\n      <td>-1.4905</td>\n      <td>9.5214</td>\n      <td>-0.1508</td>\n      <td>9.1942</td>\n      <td>13.2876</td>\n      <td>-1.5121</td>\n      <td>3.9267</td>\n      <td>9.5031</td>\n      <td>17.9974</td>\n      <td>-8.8104</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true,"_uuid":"77cbcf38309f0f9548e7add3d5e07d0a612c4cdb"},"cell_type":"code","source":"# check for missing values\n\ndata.isnull().sum().sum()","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"0"},"metadata":{}}]},{"metadata":{"trusted":true,"_uuid":"c670a924a858bdeb38d3a78d2bcb4e077e4bf60c"},"cell_type":"code","source":"# check for non-numerical variables\n\n(data.dtypes == 'O').sum()","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"1"},"metadata":{}}]},{"metadata":{"trusted":true,"_uuid":"c44ac53161566bf0ac2b47c22f5d30f35b0dd52f"},"cell_type":"code","source":"# so only the 'ID_code' column","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0872fd6a28ae13d4186e5092c4a94d948472ad7a"},"cell_type":"code","source":"# check for possible numerically encoded categorical variables\n\n(data.dtypes == 'int64').sum()","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"1"},"metadata":{}}]},{"metadata":{"trusted":true,"_uuid":"c7505bd28182da322095fabc362885c358784599"},"cell_type":"code","source":"# so only the 'target' column","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0d1213dfaf008ec20d0a9108999588d7f60cc87b"},"cell_type":"code","source":"# next it's time to standardize the values, one of the important steps\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, \\\n                    RobustScaler, PowerTransformer, QuantileTransformer\nstd_scaler = StandardScaler()\nmm_scaler = MinMaxScaler()\nmabs_scaler = MaxAbsScaler()\nr_scaler = RobustScaler()\np_transform = PowerTransformer()\nq_transform = QuantileTransformer()","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def scale(x, model):\n    x[x.columns[2:]] = model.fit_transform(x[x.columns[2:]])\n    return x","execution_count":18,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d5212d262348046e87593bc9440bff6c8b18b682"},"cell_type":"code","source":"# let's have a look if that worked fine\ndata_std = scale(data, std_scaler)\ndata_std.head()","execution_count":20,"outputs":[{"output_type":"execute_result","execution_count":20,"data":{"text/plain":"   ID_code  target     var_0    ...      var_197   var_198   var_199\n0  train_0       0 -0.577102    ...    -0.373968 -1.026398  0.214135\n1  train_1       0  0.269959    ...    -0.129400  0.825417  0.505685\n2  train_2       0 -0.681113    ...    -0.695141 -0.381449  0.356681\n3  train_3       0  0.125158    ...     1.501744  0.697118 -0.543502\n4  train_4       0 -0.277303    ...     0.645537  0.706318 -0.525375\n\n[5 rows x 202 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID_code</th>\n      <th>target</th>\n      <th>var_0</th>\n      <th>var_1</th>\n      <th>var_2</th>\n      <th>var_3</th>\n      <th>var_4</th>\n      <th>var_5</th>\n      <th>var_6</th>\n      <th>var_7</th>\n      <th>var_8</th>\n      <th>var_9</th>\n      <th>var_10</th>\n      <th>var_11</th>\n      <th>var_12</th>\n      <th>var_13</th>\n      <th>var_14</th>\n      <th>var_15</th>\n      <th>var_16</th>\n      <th>var_17</th>\n      <th>var_18</th>\n      <th>var_19</th>\n      <th>var_20</th>\n      <th>var_21</th>\n      <th>var_22</th>\n      <th>var_23</th>\n      <th>var_24</th>\n      <th>var_25</th>\n      <th>var_26</th>\n      <th>var_27</th>\n      <th>var_28</th>\n      <th>var_29</th>\n      <th>var_30</th>\n      <th>var_31</th>\n      <th>var_32</th>\n      <th>var_33</th>\n      <th>var_34</th>\n      <th>var_35</th>\n      <th>var_36</th>\n      <th>var_37</th>\n      <th>...</th>\n      <th>var_160</th>\n      <th>var_161</th>\n      <th>var_162</th>\n      <th>var_163</th>\n      <th>var_164</th>\n      <th>var_165</th>\n      <th>var_166</th>\n      <th>var_167</th>\n      <th>var_168</th>\n      <th>var_169</th>\n      <th>var_170</th>\n      <th>var_171</th>\n      <th>var_172</th>\n      <th>var_173</th>\n      <th>var_174</th>\n      <th>var_175</th>\n      <th>var_176</th>\n      <th>var_177</th>\n      <th>var_178</th>\n      <th>var_179</th>\n      <th>var_180</th>\n      <th>var_181</th>\n      <th>var_182</th>\n      <th>var_183</th>\n      <th>var_184</th>\n      <th>var_185</th>\n      <th>var_186</th>\n      <th>var_187</th>\n      <th>var_188</th>\n      <th>var_189</th>\n      <th>var_190</th>\n      <th>var_191</th>\n      <th>var_192</th>\n      <th>var_193</th>\n      <th>var_194</th>\n      <th>var_195</th>\n      <th>var_196</th>\n      <th>var_197</th>\n      <th>var_198</th>\n      <th>var_199</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>train_0</td>\n      <td>0</td>\n      <td>-0.577102</td>\n      <td>-1.273737</td>\n      <td>0.451707</td>\n      <td>-0.833709</td>\n      <td>0.235571</td>\n      <td>-0.536430</td>\n      <td>-0.334926</td>\n      <td>0.608751</td>\n      <td>-1.561580</td>\n      <td>-1.473796</td>\n      <td>0.460091</td>\n      <td>1.076623</td>\n      <td>-0.054077</td>\n      <td>-1.714773</td>\n      <td>0.561098</td>\n      <td>-0.009778</td>\n      <td>-1.401636</td>\n      <td>-0.229802</td>\n      <td>-1.395940</td>\n      <td>2.285292</td>\n      <td>-0.468989</td>\n      <td>-0.126734</td>\n      <td>-0.606166</td>\n      <td>-1.039947</td>\n      <td>1.005683</td>\n      <td>-0.823003</td>\n      <td>-0.184673</td>\n      <td>0.479492</td>\n      <td>-0.768964</td>\n      <td>0.360340</td>\n      <td>0.926437</td>\n      <td>1.162587</td>\n      <td>-1.299824</td>\n      <td>0.489773</td>\n      <td>-0.447275</td>\n      <td>1.300500</td>\n      <td>-0.484245</td>\n      <td>0.897088</td>\n      <td>...</td>\n      <td>-0.808962</td>\n      <td>-1.468275</td>\n      <td>-1.230618</td>\n      <td>-1.133356</td>\n      <td>1.749415</td>\n      <td>-1.323871</td>\n      <td>-0.711244</td>\n      <td>0.442890</td>\n      <td>1.500647</td>\n      <td>-0.400904</td>\n      <td>-1.075697</td>\n      <td>-1.411980</td>\n      <td>0.122226</td>\n      <td>0.642079</td>\n      <td>-0.231051</td>\n      <td>-1.347890</td>\n      <td>-0.974846</td>\n      <td>0.700359</td>\n      <td>-0.291946</td>\n      <td>-0.311275</td>\n      <td>-1.076447</td>\n      <td>-0.779321</td>\n      <td>0.261797</td>\n      <td>1.251424</td>\n      <td>1.404031</td>\n      <td>2.086632</td>\n      <td>0.900016</td>\n      <td>-0.810464</td>\n      <td>0.556997</td>\n      <td>-0.164257</td>\n      <td>0.263374</td>\n      <td>-1.149158</td>\n      <td>0.817469</td>\n      <td>-0.411013</td>\n      <td>0.168705</td>\n      <td>-1.578117</td>\n      <td>1.022131</td>\n      <td>-0.373968</td>\n      <td>-1.026398</td>\n      <td>0.214135</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>train_1</td>\n      <td>0</td>\n      <td>0.269959</td>\n      <td>-0.622138</td>\n      <td>1.190360</td>\n      <td>-0.688846</td>\n      <td>0.790975</td>\n      <td>1.539900</td>\n      <td>0.244461</td>\n      <td>-0.003525</td>\n      <td>0.858974</td>\n      <td>0.419300</td>\n      <td>-0.144987</td>\n      <td>1.893408</td>\n      <td>-0.000409</td>\n      <td>-0.025160</td>\n      <td>-0.935586</td>\n      <td>-2.120000</td>\n      <td>1.757336</td>\n      <td>-1.473077</td>\n      <td>-0.948119</td>\n      <td>2.017368</td>\n      <td>-1.678317</td>\n      <td>-1.771135</td>\n      <td>1.491237</td>\n      <td>0.668183</td>\n      <td>-0.954799</td>\n      <td>0.782756</td>\n      <td>-1.302434</td>\n      <td>-0.932390</td>\n      <td>-0.621140</td>\n      <td>-1.722433</td>\n      <td>-0.202250</td>\n      <td>-1.165592</td>\n      <td>0.695096</td>\n      <td>1.063517</td>\n      <td>-0.118257</td>\n      <td>-0.599010</td>\n      <td>0.196242</td>\n      <td>-0.011334</td>\n      <td>...</td>\n      <td>0.480256</td>\n      <td>1.078324</td>\n      <td>-1.086283</td>\n      <td>0.918394</td>\n      <td>-0.392137</td>\n      <td>-0.832228</td>\n      <td>0.640994</td>\n      <td>1.727411</td>\n      <td>-0.339675</td>\n      <td>0.443627</td>\n      <td>1.252712</td>\n      <td>1.102749</td>\n      <td>0.255461</td>\n      <td>1.037661</td>\n      <td>1.408807</td>\n      <td>-0.395487</td>\n      <td>-0.590320</td>\n      <td>1.783707</td>\n      <td>-0.738204</td>\n      <td>3.031050</td>\n      <td>-0.542085</td>\n      <td>-0.435700</td>\n      <td>-1.743656</td>\n      <td>0.146464</td>\n      <td>1.050349</td>\n      <td>-0.186013</td>\n      <td>-0.412563</td>\n      <td>-0.483543</td>\n      <td>-0.522151</td>\n      <td>-1.129387</td>\n      <td>0.966611</td>\n      <td>0.093605</td>\n      <td>0.443623</td>\n      <td>1.908764</td>\n      <td>-0.817594</td>\n      <td>1.522342</td>\n      <td>1.067654</td>\n      <td>-0.129400</td>\n      <td>0.825417</td>\n      <td>0.505685</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>train_2</td>\n      <td>0</td>\n      <td>-0.681113</td>\n      <td>-0.276066</td>\n      <td>0.516988</td>\n      <td>0.536516</td>\n      <td>-0.305477</td>\n      <td>-0.511033</td>\n      <td>1.769839</td>\n      <td>-0.564749</td>\n      <td>-1.561370</td>\n      <td>-1.307408</td>\n      <td>-0.130752</td>\n      <td>-1.343197</td>\n      <td>0.888789</td>\n      <td>-0.262491</td>\n      <td>-0.005875</td>\n      <td>0.179918</td>\n      <td>-0.647163</td>\n      <td>0.589523</td>\n      <td>-1.342815</td>\n      <td>1.005285</td>\n      <td>0.759841</td>\n      <td>0.107340</td>\n      <td>-1.085317</td>\n      <td>0.937877</td>\n      <td>-1.299019</td>\n      <td>-1.575276</td>\n      <td>-0.665103</td>\n      <td>-1.158352</td>\n      <td>0.399967</td>\n      <td>0.416687</td>\n      <td>-0.428711</td>\n      <td>0.968919</td>\n      <td>-0.685601</td>\n      <td>0.987446</td>\n      <td>-0.718871</td>\n      <td>0.051395</td>\n      <td>0.804550</td>\n      <td>0.473703</td>\n      <td>...</td>\n      <td>-1.015814</td>\n      <td>0.967742</td>\n      <td>-0.460899</td>\n      <td>-1.004946</td>\n      <td>0.335468</td>\n      <td>0.237541</td>\n      <td>0.853879</td>\n      <td>-0.185182</td>\n      <td>0.766417</td>\n      <td>0.359986</td>\n      <td>-1.601890</td>\n      <td>-0.572671</td>\n      <td>-1.448003</td>\n      <td>-0.862364</td>\n      <td>0.766677</td>\n      <td>-0.249092</td>\n      <td>0.331276</td>\n      <td>-1.156053</td>\n      <td>-1.012477</td>\n      <td>-0.067934</td>\n      <td>-0.294957</td>\n      <td>-0.506407</td>\n      <td>-1.560110</td>\n      <td>-1.254681</td>\n      <td>1.108568</td>\n      <td>-0.278666</td>\n      <td>-1.127290</td>\n      <td>0.351547</td>\n      <td>-1.314146</td>\n      <td>-1.626240</td>\n      <td>-0.072093</td>\n      <td>0.777997</td>\n      <td>-0.174131</td>\n      <td>-0.412316</td>\n      <td>1.151591</td>\n      <td>2.297370</td>\n      <td>-1.617906</td>\n      <td>-0.695141</td>\n      <td>-0.381449</td>\n      <td>0.356681</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>train_3</td>\n      <td>0</td>\n      <td>0.125158</td>\n      <td>-0.129426</td>\n      <td>-0.667575</td>\n      <td>0.195355</td>\n      <td>0.927992</td>\n      <td>0.410672</td>\n      <td>0.500633</td>\n      <td>-0.474201</td>\n      <td>-1.843910</td>\n      <td>0.548767</td>\n      <td>0.347543</td>\n      <td>1.014331</td>\n      <td>-0.934857</td>\n      <td>0.741492</td>\n      <td>-0.480762</td>\n      <td>0.641408</td>\n      <td>0.551235</td>\n      <td>0.784619</td>\n      <td>0.088977</td>\n      <td>0.160959</td>\n      <td>1.192958</td>\n      <td>-0.573410</td>\n      <td>0.883010</td>\n      <td>-0.562999</td>\n      <td>0.410618</td>\n      <td>0.242366</td>\n      <td>0.822099</td>\n      <td>0.152265</td>\n      <td>0.477453</td>\n      <td>-0.846035</td>\n      <td>-0.948135</td>\n      <td>0.022063</td>\n      <td>-0.796119</td>\n      <td>-1.988413</td>\n      <td>-1.523135</td>\n      <td>-1.884007</td>\n      <td>1.918310</td>\n      <td>-1.340964</td>\n      <td>...</td>\n      <td>0.700720</td>\n      <td>1.537629</td>\n      <td>1.300009</td>\n      <td>-1.351618</td>\n      <td>-1.505430</td>\n      <td>-0.411123</td>\n      <td>-1.015017</td>\n      <td>0.185862</td>\n      <td>-1.407938</td>\n      <td>-0.793123</td>\n      <td>-1.615767</td>\n      <td>-0.996695</td>\n      <td>-0.182207</td>\n      <td>2.076768</td>\n      <td>-0.765424</td>\n      <td>-1.223761</td>\n      <td>1.953266</td>\n      <td>0.788267</td>\n      <td>-1.102129</td>\n      <td>1.109222</td>\n      <td>-0.192528</td>\n      <td>-1.220110</td>\n      <td>0.131590</td>\n      <td>-0.337210</td>\n      <td>-1.418782</td>\n      <td>0.832451</td>\n      <td>-0.325002</td>\n      <td>-0.215372</td>\n      <td>-0.751604</td>\n      <td>1.129547</td>\n      <td>0.270216</td>\n      <td>-0.891456</td>\n      <td>-0.818468</td>\n      <td>-0.478548</td>\n      <td>1.607869</td>\n      <td>-0.789517</td>\n      <td>-0.959020</td>\n      <td>1.501744</td>\n      <td>0.697118</td>\n      <td>-0.543502</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>train_4</td>\n      <td>0</td>\n      <td>-0.277303</td>\n      <td>0.035610</td>\n      <td>0.817683</td>\n      <td>-0.077829</td>\n      <td>0.738607</td>\n      <td>0.955574</td>\n      <td>0.613372</td>\n      <td>0.791544</td>\n      <td>1.794753</td>\n      <td>0.090006</td>\n      <td>-1.788863</td>\n      <td>-1.490109</td>\n      <td>-0.925386</td>\n      <td>-0.138103</td>\n      <td>0.112013</td>\n      <td>1.171149</td>\n      <td>-0.330867</td>\n      <td>0.391612</td>\n      <td>-1.110346</td>\n      <td>-0.140479</td>\n      <td>1.384169</td>\n      <td>0.207760</td>\n      <td>2.038227</td>\n      <td>-0.579511</td>\n      <td>0.959326</td>\n      <td>-0.434959</td>\n      <td>1.220617</td>\n      <td>-1.446727</td>\n      <td>0.470432</td>\n      <td>1.101452</td>\n      <td>0.568668</td>\n      <td>-0.428840</td>\n      <td>0.194985</td>\n      <td>1.052620</td>\n      <td>1.793625</td>\n      <td>-0.619668</td>\n      <td>0.193903</td>\n      <td>-0.022447</td>\n      <td>...</td>\n      <td>0.850294</td>\n      <td>0.284519</td>\n      <td>0.072839</td>\n      <td>1.371697</td>\n      <td>1.731257</td>\n      <td>0.387118</td>\n      <td>0.722415</td>\n      <td>0.304598</td>\n      <td>-0.582369</td>\n      <td>-0.227938</td>\n      <td>0.328676</td>\n      <td>-0.340447</td>\n      <td>-0.001325</td>\n      <td>0.510917</td>\n      <td>-0.244125</td>\n      <td>1.621725</td>\n      <td>-0.072667</td>\n      <td>0.625042</td>\n      <td>0.318535</td>\n      <td>0.922795</td>\n      <td>-0.941250</td>\n      <td>-0.231087</td>\n      <td>0.465436</td>\n      <td>0.771124</td>\n      <td>-0.097269</td>\n      <td>-0.827452</td>\n      <td>-0.799239</td>\n      <td>1.404967</td>\n      <td>0.162989</td>\n      <td>-1.053901</td>\n      <td>-1.036191</td>\n      <td>0.688988</td>\n      <td>-1.405987</td>\n      <td>1.468536</td>\n      <td>-1.501101</td>\n      <td>-0.958473</td>\n      <td>0.297627</td>\n      <td>0.645537</td>\n      <td>0.706318</td>\n      <td>-0.525375</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_mm = scale(data, mm_scaler)\ndata_mm.head()","execution_count":21,"outputs":[{"output_type":"execute_result","execution_count":21,"data":{"text/plain":"   ID_code  target     var_0    ...      var_197   var_198   var_199\n0  train_0       0  0.427853    ...     0.430958  0.327658  0.560645\n1  train_1       0  0.557212    ...     0.468277  0.609546  0.605827\n2  train_2       0  0.411969    ...     0.381950  0.425833  0.582736\n3  train_3       0  0.535099    ...     0.717176  0.590016  0.443232\n4  train_4       0  0.473637    ...     0.586526  0.591416  0.446041\n\n[5 rows x 202 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID_code</th>\n      <th>target</th>\n      <th>var_0</th>\n      <th>var_1</th>\n      <th>var_2</th>\n      <th>var_3</th>\n      <th>var_4</th>\n      <th>var_5</th>\n      <th>var_6</th>\n      <th>var_7</th>\n      <th>var_8</th>\n      <th>var_9</th>\n      <th>var_10</th>\n      <th>var_11</th>\n      <th>var_12</th>\n      <th>var_13</th>\n      <th>var_14</th>\n      <th>var_15</th>\n      <th>var_16</th>\n      <th>var_17</th>\n      <th>var_18</th>\n      <th>var_19</th>\n      <th>var_20</th>\n      <th>var_21</th>\n      <th>var_22</th>\n      <th>var_23</th>\n      <th>var_24</th>\n      <th>var_25</th>\n      <th>var_26</th>\n      <th>var_27</th>\n      <th>var_28</th>\n      <th>var_29</th>\n      <th>var_30</th>\n      <th>var_31</th>\n      <th>var_32</th>\n      <th>var_33</th>\n      <th>var_34</th>\n      <th>var_35</th>\n      <th>var_36</th>\n      <th>var_37</th>\n      <th>...</th>\n      <th>var_160</th>\n      <th>var_161</th>\n      <th>var_162</th>\n      <th>var_163</th>\n      <th>var_164</th>\n      <th>var_165</th>\n      <th>var_166</th>\n      <th>var_167</th>\n      <th>var_168</th>\n      <th>var_169</th>\n      <th>var_170</th>\n      <th>var_171</th>\n      <th>var_172</th>\n      <th>var_173</th>\n      <th>var_174</th>\n      <th>var_175</th>\n      <th>var_176</th>\n      <th>var_177</th>\n      <th>var_178</th>\n      <th>var_179</th>\n      <th>var_180</th>\n      <th>var_181</th>\n      <th>var_182</th>\n      <th>var_183</th>\n      <th>var_184</th>\n      <th>var_185</th>\n      <th>var_186</th>\n      <th>var_187</th>\n      <th>var_188</th>\n      <th>var_189</th>\n      <th>var_190</th>\n      <th>var_191</th>\n      <th>var_192</th>\n      <th>var_193</th>\n      <th>var_194</th>\n      <th>var_195</th>\n      <th>var_196</th>\n      <th>var_197</th>\n      <th>var_198</th>\n      <th>var_199</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>train_0</td>\n      <td>0</td>\n      <td>0.427853</td>\n      <td>0.324824</td>\n      <td>0.568059</td>\n      <td>0.388041</td>\n      <td>0.550670</td>\n      <td>0.467321</td>\n      <td>0.454298</td>\n      <td>0.594255</td>\n      <td>0.270395</td>\n      <td>0.247420</td>\n      <td>0.600396</td>\n      <td>0.676400</td>\n      <td>0.474711</td>\n      <td>0.232357</td>\n      <td>0.559134</td>\n      <td>0.535530</td>\n      <td>0.295319</td>\n      <td>0.498814</td>\n      <td>0.285205</td>\n      <td>0.906070</td>\n      <td>0.434859</td>\n      <td>0.444895</td>\n      <td>0.396726</td>\n      <td>0.344246</td>\n      <td>0.576530</td>\n      <td>0.368293</td>\n      <td>0.478336</td>\n      <td>0.612068</td>\n      <td>0.424169</td>\n      <td>0.601423</td>\n      <td>0.683647</td>\n      <td>0.596934</td>\n      <td>0.323936</td>\n      <td>0.548632</td>\n      <td>0.396180</td>\n      <td>0.712154</td>\n      <td>0.392624</td>\n      <td>0.695207</td>\n      <td>...</td>\n      <td>0.347928</td>\n      <td>0.316133</td>\n      <td>0.314657</td>\n      <td>0.339334</td>\n      <td>0.830517</td>\n      <td>0.256340</td>\n      <td>0.358433</td>\n      <td>0.616147</td>\n      <td>0.737390</td>\n      <td>0.434280</td>\n      <td>0.311849</td>\n      <td>0.347898</td>\n      <td>0.497048</td>\n      <td>0.622578</td>\n      <td>0.463174</td>\n      <td>0.255534</td>\n      <td>0.384752</td>\n      <td>0.560712</td>\n      <td>0.435670</td>\n      <td>0.449490</td>\n      <td>0.391869</td>\n      <td>0.400996</td>\n      <td>0.526098</td>\n      <td>0.672579</td>\n      <td>0.705287</td>\n      <td>0.799861</td>\n      <td>0.600263</td>\n      <td>0.397474</td>\n      <td>0.528213</td>\n      <td>0.476595</td>\n      <td>0.569515</td>\n      <td>0.342943</td>\n      <td>0.568958</td>\n      <td>0.448173</td>\n      <td>0.510975</td>\n      <td>0.300318</td>\n      <td>0.678981</td>\n      <td>0.430958</td>\n      <td>0.327658</td>\n      <td>0.560645</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>train_1</td>\n      <td>0</td>\n      <td>0.557212</td>\n      <td>0.428639</td>\n      <td>0.681235</td>\n      <td>0.410417</td>\n      <td>0.628408</td>\n      <td>0.795072</td>\n      <td>0.536604</td>\n      <td>0.500584</td>\n      <td>0.660911</td>\n      <td>0.573056</td>\n      <td>0.515922</td>\n      <td>0.789061</td>\n      <td>0.483072</td>\n      <td>0.508937</td>\n      <td>0.317515</td>\n      <td>0.223730</td>\n      <td>0.761884</td>\n      <td>0.339565</td>\n      <td>0.352288</td>\n      <td>0.861046</td>\n      <td>0.241322</td>\n      <td>0.216962</td>\n      <td>0.696636</td>\n      <td>0.589786</td>\n      <td>0.293071</td>\n      <td>0.605293</td>\n      <td>0.312507</td>\n      <td>0.383447</td>\n      <td>0.441458</td>\n      <td>0.297558</td>\n      <td>0.505446</td>\n      <td>0.318052</td>\n      <td>0.651231</td>\n      <td>0.637080</td>\n      <td>0.447521</td>\n      <td>0.453532</td>\n      <td>0.487657</td>\n      <td>0.538568</td>\n      <td>...</td>\n      <td>0.560954</td>\n      <td>0.696974</td>\n      <td>0.336200</td>\n      <td>0.655976</td>\n      <td>0.466594</td>\n      <td>0.327905</td>\n      <td>0.569433</td>\n      <td>0.791123</td>\n      <td>0.453524</td>\n      <td>0.544520</td>\n      <td>0.641650</td>\n      <td>0.682684</td>\n      <td>0.514816</td>\n      <td>0.679310</td>\n      <td>0.714618</td>\n      <td>0.411922</td>\n      <td>0.443285</td>\n      <td>0.742758</td>\n      <td>0.370901</td>\n      <td>0.888593</td>\n      <td>0.472684</td>\n      <td>0.453505</td>\n      <td>0.200745</td>\n      <td>0.499164</td>\n      <td>0.652852</td>\n      <td>0.498052</td>\n      <td>0.431177</td>\n      <td>0.451118</td>\n      <td>0.357380</td>\n      <td>0.333571</td>\n      <td>0.668079</td>\n      <td>0.536531</td>\n      <td>0.523717</td>\n      <td>0.756190</td>\n      <td>0.350211</td>\n      <td>0.765154</td>\n      <td>0.686614</td>\n      <td>0.468277</td>\n      <td>0.609546</td>\n      <td>0.605827</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>train_2</td>\n      <td>0</td>\n      <td>0.411969</td>\n      <td>0.483777</td>\n      <td>0.578061</td>\n      <td>0.599690</td>\n      <td>0.474941</td>\n      <td>0.471329</td>\n      <td>0.753295</td>\n      <td>0.414724</td>\n      <td>0.270429</td>\n      <td>0.276041</td>\n      <td>0.517909</td>\n      <td>0.342628</td>\n      <td>0.621608</td>\n      <td>0.470087</td>\n      <td>0.467604</td>\n      <td>0.563559</td>\n      <td>0.406751</td>\n      <td>0.603760</td>\n      <td>0.293163</td>\n      <td>0.690967</td>\n      <td>0.631517</td>\n      <td>0.477340</td>\n      <td>0.328212</td>\n      <td>0.628554</td>\n      <td>0.243301</td>\n      <td>0.257262</td>\n      <td>0.407060</td>\n      <td>0.346858</td>\n      <td>0.560886</td>\n      <td>0.609644</td>\n      <td>0.469692</td>\n      <td>0.573736</td>\n      <td>0.424708</td>\n      <td>0.625353</td>\n      <td>0.353799</td>\n      <td>0.542086</td>\n      <td>0.572610</td>\n      <td>0.622202</td>\n      <td>...</td>\n      <td>0.313748</td>\n      <td>0.680436</td>\n      <td>0.429542</td>\n      <td>0.359151</td>\n      <td>0.590239</td>\n      <td>0.483623</td>\n      <td>0.602651</td>\n      <td>0.530592</td>\n      <td>0.624137</td>\n      <td>0.533602</td>\n      <td>0.237317</td>\n      <td>0.459635</td>\n      <td>0.287649</td>\n      <td>0.406818</td>\n      <td>0.616158</td>\n      <td>0.435960</td>\n      <td>0.583572</td>\n      <td>0.248760</td>\n      <td>0.331094</td>\n      <td>0.481460</td>\n      <td>0.510060</td>\n      <td>0.442701</td>\n      <td>0.230522</td>\n      <td>0.279265</td>\n      <td>0.661484</td>\n      <td>0.485748</td>\n      <td>0.339107</td>\n      <td>0.588145</td>\n      <td>0.232005</td>\n      <td>0.259942</td>\n      <td>0.522496</td>\n      <td>0.643141</td>\n      <td>0.448960</td>\n      <td>0.448000</td>\n      <td>0.671183</td>\n      <td>0.881350</td>\n      <td>0.236337</td>\n      <td>0.381950</td>\n      <td>0.425833</td>\n      <td>0.582736</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>train_3</td>\n      <td>0</td>\n      <td>0.535099</td>\n      <td>0.507140</td>\n      <td>0.396562</td>\n      <td>0.546993</td>\n      <td>0.647586</td>\n      <td>0.616822</td>\n      <td>0.572995</td>\n      <td>0.428577</td>\n      <td>0.224846</td>\n      <td>0.595326</td>\n      <td>0.584683</td>\n      <td>0.667808</td>\n      <td>0.337487</td>\n      <td>0.634434</td>\n      <td>0.390940</td>\n      <td>0.631747</td>\n      <td>0.583749</td>\n      <td>0.628749</td>\n      <td>0.507645</td>\n      <td>0.549079</td>\n      <td>0.700831</td>\n      <td>0.382980</td>\n      <td>0.609665</td>\n      <td>0.412806</td>\n      <td>0.490492</td>\n      <td>0.525535</td>\n      <td>0.627699</td>\n      <td>0.559081</td>\n      <td>0.569948</td>\n      <td>0.425420</td>\n      <td>0.387684</td>\n      <td>0.460316</td>\n      <td>0.406576</td>\n      <td>0.166598</td>\n      <td>0.228298</td>\n      <td>0.278578</td>\n      <td>0.728151</td>\n      <td>0.309300</td>\n      <td>...</td>\n      <td>0.597383</td>\n      <td>0.765663</td>\n      <td>0.692366</td>\n      <td>0.305650</td>\n      <td>0.277408</td>\n      <td>0.389202</td>\n      <td>0.311033</td>\n      <td>0.581135</td>\n      <td>0.288747</td>\n      <td>0.383082</td>\n      <td>0.235352</td>\n      <td>0.403185</td>\n      <td>0.456450</td>\n      <td>0.828334</td>\n      <td>0.381238</td>\n      <td>0.275916</td>\n      <td>0.830474</td>\n      <td>0.575484</td>\n      <td>0.318082</td>\n      <td>0.636110</td>\n      <td>0.525551</td>\n      <td>0.333638</td>\n      <td>0.504974</td>\n      <td>0.423255</td>\n      <td>0.286793</td>\n      <td>0.633305</td>\n      <td>0.442457</td>\n      <td>0.495121</td>\n      <td>0.321057</td>\n      <td>0.668326</td>\n      <td>0.570474</td>\n      <td>0.383085</td>\n      <td>0.370986</td>\n      <td>0.439205</td>\n      <td>0.745555</td>\n      <td>0.418549</td>\n      <td>0.346810</td>\n      <td>0.717176</td>\n      <td>0.590016</td>\n      <td>0.443232</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>train_4</td>\n      <td>0</td>\n      <td>0.473637</td>\n      <td>0.533434</td>\n      <td>0.624133</td>\n      <td>0.504796</td>\n      <td>0.621079</td>\n      <td>0.702836</td>\n      <td>0.589011</td>\n      <td>0.622220</td>\n      <td>0.811883</td>\n      <td>0.516413</td>\n      <td>0.286423</td>\n      <td>0.322364</td>\n      <td>0.338962</td>\n      <td>0.490449</td>\n      <td>0.486635</td>\n      <td>0.710020</td>\n      <td>0.453467</td>\n      <td>0.578410</td>\n      <td>0.327987</td>\n      <td>0.498423</td>\n      <td>0.731432</td>\n      <td>0.491260</td>\n      <td>0.774851</td>\n      <td>0.410433</td>\n      <td>0.569827</td>\n      <td>0.425566</td>\n      <td>0.686822</td>\n      <td>0.300163</td>\n      <td>0.569127</td>\n      <td>0.709548</td>\n      <td>0.627161</td>\n      <td>0.406305</td>\n      <td>0.569181</td>\n      <td>0.635400</td>\n      <td>0.745858</td>\n      <td>0.450720</td>\n      <td>0.487330</td>\n      <td>0.536652</td>\n      <td>...</td>\n      <td>0.622098</td>\n      <td>0.578261</td>\n      <td>0.509205</td>\n      <td>0.725934</td>\n      <td>0.827431</td>\n      <td>0.505396</td>\n      <td>0.582137</td>\n      <td>0.597309</td>\n      <td>0.416089</td>\n      <td>0.456858</td>\n      <td>0.510767</td>\n      <td>0.490551</td>\n      <td>0.480572</td>\n      <td>0.603767</td>\n      <td>0.461170</td>\n      <td>0.743154</td>\n      <td>0.522083</td>\n      <td>0.548056</td>\n      <td>0.524274</td>\n      <td>0.611618</td>\n      <td>0.412316</td>\n      <td>0.484773</td>\n      <td>0.559135</td>\n      <td>0.597199</td>\n      <td>0.482713</td>\n      <td>0.412869</td>\n      <td>0.381366</td>\n      <td>0.760998</td>\n      <td>0.465840</td>\n      <td>0.344757</td>\n      <td>0.387371</td>\n      <td>0.629275</td>\n      <td>0.299888</td>\n      <td>0.697737</td>\n      <td>0.238801</td>\n      <td>0.393218</td>\n      <td>0.557507</td>\n      <td>0.586526</td>\n      <td>0.591416</td>\n      <td>0.446041</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_mabs = scale(data, mabs_scaler)\ndata_mabs.head()","execution_count":22,"outputs":[{"output_type":"execute_result","execution_count":22,"data":{"text/plain":"   ID_code  target     var_0    ...      var_197   var_198   var_199\n0  train_0       0  0.427853    ...     0.430958  0.327658  0.560645\n1  train_1       0  0.557212    ...     0.468277  0.609546  0.605827\n2  train_2       0  0.411969    ...     0.381950  0.425833  0.582736\n3  train_3       0  0.535099    ...     0.717176  0.590016  0.443232\n4  train_4       0  0.473637    ...     0.586526  0.591416  0.446041\n\n[5 rows x 202 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID_code</th>\n      <th>target</th>\n      <th>var_0</th>\n      <th>var_1</th>\n      <th>var_2</th>\n      <th>var_3</th>\n      <th>var_4</th>\n      <th>var_5</th>\n      <th>var_6</th>\n      <th>var_7</th>\n      <th>var_8</th>\n      <th>var_9</th>\n      <th>var_10</th>\n      <th>var_11</th>\n      <th>var_12</th>\n      <th>var_13</th>\n      <th>var_14</th>\n      <th>var_15</th>\n      <th>var_16</th>\n      <th>var_17</th>\n      <th>var_18</th>\n      <th>var_19</th>\n      <th>var_20</th>\n      <th>var_21</th>\n      <th>var_22</th>\n      <th>var_23</th>\n      <th>var_24</th>\n      <th>var_25</th>\n      <th>var_26</th>\n      <th>var_27</th>\n      <th>var_28</th>\n      <th>var_29</th>\n      <th>var_30</th>\n      <th>var_31</th>\n      <th>var_32</th>\n      <th>var_33</th>\n      <th>var_34</th>\n      <th>var_35</th>\n      <th>var_36</th>\n      <th>var_37</th>\n      <th>...</th>\n      <th>var_160</th>\n      <th>var_161</th>\n      <th>var_162</th>\n      <th>var_163</th>\n      <th>var_164</th>\n      <th>var_165</th>\n      <th>var_166</th>\n      <th>var_167</th>\n      <th>var_168</th>\n      <th>var_169</th>\n      <th>var_170</th>\n      <th>var_171</th>\n      <th>var_172</th>\n      <th>var_173</th>\n      <th>var_174</th>\n      <th>var_175</th>\n      <th>var_176</th>\n      <th>var_177</th>\n      <th>var_178</th>\n      <th>var_179</th>\n      <th>var_180</th>\n      <th>var_181</th>\n      <th>var_182</th>\n      <th>var_183</th>\n      <th>var_184</th>\n      <th>var_185</th>\n      <th>var_186</th>\n      <th>var_187</th>\n      <th>var_188</th>\n      <th>var_189</th>\n      <th>var_190</th>\n      <th>var_191</th>\n      <th>var_192</th>\n      <th>var_193</th>\n      <th>var_194</th>\n      <th>var_195</th>\n      <th>var_196</th>\n      <th>var_197</th>\n      <th>var_198</th>\n      <th>var_199</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>train_0</td>\n      <td>0</td>\n      <td>0.427853</td>\n      <td>0.324824</td>\n      <td>0.568059</td>\n      <td>0.388041</td>\n      <td>0.550670</td>\n      <td>0.467321</td>\n      <td>0.454298</td>\n      <td>0.594255</td>\n      <td>0.270395</td>\n      <td>0.247420</td>\n      <td>0.600396</td>\n      <td>0.676400</td>\n      <td>0.474711</td>\n      <td>0.232357</td>\n      <td>0.559134</td>\n      <td>0.535530</td>\n      <td>0.295319</td>\n      <td>0.498814</td>\n      <td>0.285205</td>\n      <td>0.906070</td>\n      <td>0.434859</td>\n      <td>0.444895</td>\n      <td>0.396726</td>\n      <td>0.344246</td>\n      <td>0.576530</td>\n      <td>0.368293</td>\n      <td>0.478336</td>\n      <td>0.612068</td>\n      <td>0.424169</td>\n      <td>0.601423</td>\n      <td>0.683647</td>\n      <td>0.596934</td>\n      <td>0.323936</td>\n      <td>0.548632</td>\n      <td>0.396180</td>\n      <td>0.712154</td>\n      <td>0.392624</td>\n      <td>0.695207</td>\n      <td>...</td>\n      <td>0.347928</td>\n      <td>0.316133</td>\n      <td>0.314657</td>\n      <td>0.339334</td>\n      <td>0.830517</td>\n      <td>0.256340</td>\n      <td>0.358433</td>\n      <td>0.616147</td>\n      <td>0.737390</td>\n      <td>0.434280</td>\n      <td>0.311849</td>\n      <td>0.347898</td>\n      <td>0.497048</td>\n      <td>0.622578</td>\n      <td>0.463174</td>\n      <td>0.255534</td>\n      <td>0.384752</td>\n      <td>0.560712</td>\n      <td>0.435670</td>\n      <td>0.449490</td>\n      <td>0.391869</td>\n      <td>0.400996</td>\n      <td>0.526098</td>\n      <td>0.672579</td>\n      <td>0.705287</td>\n      <td>0.799861</td>\n      <td>0.600263</td>\n      <td>0.397474</td>\n      <td>0.528213</td>\n      <td>0.476595</td>\n      <td>0.569515</td>\n      <td>0.342943</td>\n      <td>0.568958</td>\n      <td>0.448173</td>\n      <td>0.510975</td>\n      <td>0.300318</td>\n      <td>0.678981</td>\n      <td>0.430958</td>\n      <td>0.327658</td>\n      <td>0.560645</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>train_1</td>\n      <td>0</td>\n      <td>0.557212</td>\n      <td>0.428639</td>\n      <td>0.681235</td>\n      <td>0.410417</td>\n      <td>0.628408</td>\n      <td>0.795072</td>\n      <td>0.536604</td>\n      <td>0.500584</td>\n      <td>0.660911</td>\n      <td>0.573056</td>\n      <td>0.515922</td>\n      <td>0.789061</td>\n      <td>0.483072</td>\n      <td>0.508937</td>\n      <td>0.317515</td>\n      <td>0.223730</td>\n      <td>0.761884</td>\n      <td>0.339565</td>\n      <td>0.352288</td>\n      <td>0.861046</td>\n      <td>0.241322</td>\n      <td>0.216962</td>\n      <td>0.696636</td>\n      <td>0.589786</td>\n      <td>0.293071</td>\n      <td>0.605293</td>\n      <td>0.312507</td>\n      <td>0.383447</td>\n      <td>0.441458</td>\n      <td>0.297558</td>\n      <td>0.505446</td>\n      <td>0.318052</td>\n      <td>0.651231</td>\n      <td>0.637080</td>\n      <td>0.447521</td>\n      <td>0.453532</td>\n      <td>0.487657</td>\n      <td>0.538568</td>\n      <td>...</td>\n      <td>0.560954</td>\n      <td>0.696974</td>\n      <td>0.336200</td>\n      <td>0.655976</td>\n      <td>0.466594</td>\n      <td>0.327905</td>\n      <td>0.569433</td>\n      <td>0.791123</td>\n      <td>0.453524</td>\n      <td>0.544520</td>\n      <td>0.641650</td>\n      <td>0.682684</td>\n      <td>0.514816</td>\n      <td>0.679310</td>\n      <td>0.714618</td>\n      <td>0.411922</td>\n      <td>0.443285</td>\n      <td>0.742758</td>\n      <td>0.370901</td>\n      <td>0.888593</td>\n      <td>0.472684</td>\n      <td>0.453505</td>\n      <td>0.200745</td>\n      <td>0.499164</td>\n      <td>0.652852</td>\n      <td>0.498052</td>\n      <td>0.431177</td>\n      <td>0.451118</td>\n      <td>0.357380</td>\n      <td>0.333571</td>\n      <td>0.668079</td>\n      <td>0.536531</td>\n      <td>0.523717</td>\n      <td>0.756190</td>\n      <td>0.350211</td>\n      <td>0.765154</td>\n      <td>0.686614</td>\n      <td>0.468277</td>\n      <td>0.609546</td>\n      <td>0.605827</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>train_2</td>\n      <td>0</td>\n      <td>0.411969</td>\n      <td>0.483777</td>\n      <td>0.578061</td>\n      <td>0.599690</td>\n      <td>0.474941</td>\n      <td>0.471329</td>\n      <td>0.753295</td>\n      <td>0.414724</td>\n      <td>0.270429</td>\n      <td>0.276041</td>\n      <td>0.517909</td>\n      <td>0.342628</td>\n      <td>0.621608</td>\n      <td>0.470087</td>\n      <td>0.467604</td>\n      <td>0.563559</td>\n      <td>0.406751</td>\n      <td>0.603760</td>\n      <td>0.293163</td>\n      <td>0.690967</td>\n      <td>0.631517</td>\n      <td>0.477340</td>\n      <td>0.328212</td>\n      <td>0.628554</td>\n      <td>0.243301</td>\n      <td>0.257262</td>\n      <td>0.407060</td>\n      <td>0.346858</td>\n      <td>0.560886</td>\n      <td>0.609644</td>\n      <td>0.469692</td>\n      <td>0.573736</td>\n      <td>0.424708</td>\n      <td>0.625353</td>\n      <td>0.353799</td>\n      <td>0.542086</td>\n      <td>0.572610</td>\n      <td>0.622202</td>\n      <td>...</td>\n      <td>0.313748</td>\n      <td>0.680436</td>\n      <td>0.429542</td>\n      <td>0.359151</td>\n      <td>0.590239</td>\n      <td>0.483623</td>\n      <td>0.602651</td>\n      <td>0.530592</td>\n      <td>0.624137</td>\n      <td>0.533602</td>\n      <td>0.237317</td>\n      <td>0.459635</td>\n      <td>0.287649</td>\n      <td>0.406818</td>\n      <td>0.616158</td>\n      <td>0.435960</td>\n      <td>0.583572</td>\n      <td>0.248760</td>\n      <td>0.331094</td>\n      <td>0.481460</td>\n      <td>0.510060</td>\n      <td>0.442701</td>\n      <td>0.230522</td>\n      <td>0.279265</td>\n      <td>0.661484</td>\n      <td>0.485748</td>\n      <td>0.339107</td>\n      <td>0.588145</td>\n      <td>0.232005</td>\n      <td>0.259942</td>\n      <td>0.522496</td>\n      <td>0.643141</td>\n      <td>0.448960</td>\n      <td>0.448000</td>\n      <td>0.671183</td>\n      <td>0.881350</td>\n      <td>0.236337</td>\n      <td>0.381950</td>\n      <td>0.425833</td>\n      <td>0.582736</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>train_3</td>\n      <td>0</td>\n      <td>0.535099</td>\n      <td>0.507140</td>\n      <td>0.396562</td>\n      <td>0.546993</td>\n      <td>0.647586</td>\n      <td>0.616822</td>\n      <td>0.572995</td>\n      <td>0.428577</td>\n      <td>0.224846</td>\n      <td>0.595326</td>\n      <td>0.584683</td>\n      <td>0.667808</td>\n      <td>0.337487</td>\n      <td>0.634434</td>\n      <td>0.390940</td>\n      <td>0.631747</td>\n      <td>0.583749</td>\n      <td>0.628749</td>\n      <td>0.507645</td>\n      <td>0.549079</td>\n      <td>0.700831</td>\n      <td>0.382980</td>\n      <td>0.609665</td>\n      <td>0.412806</td>\n      <td>0.490492</td>\n      <td>0.525535</td>\n      <td>0.627699</td>\n      <td>0.559081</td>\n      <td>0.569948</td>\n      <td>0.425420</td>\n      <td>0.387684</td>\n      <td>0.460316</td>\n      <td>0.406576</td>\n      <td>0.166598</td>\n      <td>0.228298</td>\n      <td>0.278578</td>\n      <td>0.728151</td>\n      <td>0.309300</td>\n      <td>...</td>\n      <td>0.597383</td>\n      <td>0.765663</td>\n      <td>0.692366</td>\n      <td>0.305650</td>\n      <td>0.277408</td>\n      <td>0.389202</td>\n      <td>0.311033</td>\n      <td>0.581135</td>\n      <td>0.288747</td>\n      <td>0.383082</td>\n      <td>0.235352</td>\n      <td>0.403185</td>\n      <td>0.456450</td>\n      <td>0.828334</td>\n      <td>0.381238</td>\n      <td>0.275916</td>\n      <td>0.830474</td>\n      <td>0.575484</td>\n      <td>0.318082</td>\n      <td>0.636110</td>\n      <td>0.525551</td>\n      <td>0.333638</td>\n      <td>0.504974</td>\n      <td>0.423255</td>\n      <td>0.286793</td>\n      <td>0.633305</td>\n      <td>0.442457</td>\n      <td>0.495121</td>\n      <td>0.321057</td>\n      <td>0.668326</td>\n      <td>0.570474</td>\n      <td>0.383085</td>\n      <td>0.370986</td>\n      <td>0.439205</td>\n      <td>0.745555</td>\n      <td>0.418549</td>\n      <td>0.346810</td>\n      <td>0.717176</td>\n      <td>0.590016</td>\n      <td>0.443232</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>train_4</td>\n      <td>0</td>\n      <td>0.473637</td>\n      <td>0.533434</td>\n      <td>0.624133</td>\n      <td>0.504796</td>\n      <td>0.621079</td>\n      <td>0.702836</td>\n      <td>0.589011</td>\n      <td>0.622220</td>\n      <td>0.811883</td>\n      <td>0.516413</td>\n      <td>0.286423</td>\n      <td>0.322364</td>\n      <td>0.338962</td>\n      <td>0.490449</td>\n      <td>0.486635</td>\n      <td>0.710020</td>\n      <td>0.453467</td>\n      <td>0.578410</td>\n      <td>0.327987</td>\n      <td>0.498423</td>\n      <td>0.731432</td>\n      <td>0.491260</td>\n      <td>0.774851</td>\n      <td>0.410433</td>\n      <td>0.569827</td>\n      <td>0.425566</td>\n      <td>0.686822</td>\n      <td>0.300163</td>\n      <td>0.569127</td>\n      <td>0.709548</td>\n      <td>0.627161</td>\n      <td>0.406305</td>\n      <td>0.569181</td>\n      <td>0.635400</td>\n      <td>0.745858</td>\n      <td>0.450720</td>\n      <td>0.487330</td>\n      <td>0.536652</td>\n      <td>...</td>\n      <td>0.622098</td>\n      <td>0.578261</td>\n      <td>0.509205</td>\n      <td>0.725934</td>\n      <td>0.827431</td>\n      <td>0.505396</td>\n      <td>0.582137</td>\n      <td>0.597309</td>\n      <td>0.416089</td>\n      <td>0.456858</td>\n      <td>0.510767</td>\n      <td>0.490551</td>\n      <td>0.480572</td>\n      <td>0.603767</td>\n      <td>0.461170</td>\n      <td>0.743154</td>\n      <td>0.522083</td>\n      <td>0.548056</td>\n      <td>0.524274</td>\n      <td>0.611618</td>\n      <td>0.412316</td>\n      <td>0.484773</td>\n      <td>0.559135</td>\n      <td>0.597199</td>\n      <td>0.482713</td>\n      <td>0.412869</td>\n      <td>0.381366</td>\n      <td>0.760998</td>\n      <td>0.465840</td>\n      <td>0.344757</td>\n      <td>0.387371</td>\n      <td>0.629275</td>\n      <td>0.299888</td>\n      <td>0.697737</td>\n      <td>0.238801</td>\n      <td>0.393218</td>\n      <td>0.557507</td>\n      <td>0.586526</td>\n      <td>0.591416</td>\n      <td>0.446041</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_r = scale(data, r_scaler)\ndata_r.head()","execution_count":23,"outputs":[{"output_type":"execute_result","execution_count":23,"data":{"text/plain":"   ID_code  target     var_0    ...      var_197   var_198   var_199\n0  train_0       0 -0.371543    ...    -0.242223 -0.744683  0.107705\n1  train_1       0  0.226713    ...    -0.074077  0.571886  0.297368\n2  train_2       0 -0.445003    ...    -0.463036 -0.286149  0.200436\n3  train_3       0  0.124444    ...     1.047370  0.480670 -0.385163\n4  train_4       0 -0.159803    ...     0.458709  0.487211 -0.373372\n\n[5 rows x 202 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID_code</th>\n      <th>target</th>\n      <th>var_0</th>\n      <th>var_1</th>\n      <th>var_2</th>\n      <th>var_3</th>\n      <th>var_4</th>\n      <th>var_5</th>\n      <th>var_6</th>\n      <th>var_7</th>\n      <th>var_8</th>\n      <th>var_9</th>\n      <th>var_10</th>\n      <th>var_11</th>\n      <th>var_12</th>\n      <th>var_13</th>\n      <th>var_14</th>\n      <th>var_15</th>\n      <th>var_16</th>\n      <th>var_17</th>\n      <th>var_18</th>\n      <th>var_19</th>\n      <th>var_20</th>\n      <th>var_21</th>\n      <th>var_22</th>\n      <th>var_23</th>\n      <th>var_24</th>\n      <th>var_25</th>\n      <th>var_26</th>\n      <th>var_27</th>\n      <th>var_28</th>\n      <th>var_29</th>\n      <th>var_30</th>\n      <th>var_31</th>\n      <th>var_32</th>\n      <th>var_33</th>\n      <th>var_34</th>\n      <th>var_35</th>\n      <th>var_36</th>\n      <th>var_37</th>\n      <th>...</th>\n      <th>var_160</th>\n      <th>var_161</th>\n      <th>var_162</th>\n      <th>var_163</th>\n      <th>var_164</th>\n      <th>var_165</th>\n      <th>var_166</th>\n      <th>var_167</th>\n      <th>var_168</th>\n      <th>var_169</th>\n      <th>var_170</th>\n      <th>var_171</th>\n      <th>var_172</th>\n      <th>var_173</th>\n      <th>var_174</th>\n      <th>var_175</th>\n      <th>var_176</th>\n      <th>var_177</th>\n      <th>var_178</th>\n      <th>var_179</th>\n      <th>var_180</th>\n      <th>var_181</th>\n      <th>var_182</th>\n      <th>var_183</th>\n      <th>var_184</th>\n      <th>var_185</th>\n      <th>var_186</th>\n      <th>var_187</th>\n      <th>var_188</th>\n      <th>var_189</th>\n      <th>var_190</th>\n      <th>var_191</th>\n      <th>var_192</th>\n      <th>var_193</th>\n      <th>var_194</th>\n      <th>var_195</th>\n      <th>var_196</th>\n      <th>var_197</th>\n      <th>var_198</th>\n      <th>var_199</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>train_0</td>\n      <td>0</td>\n      <td>-0.371543</td>\n      <td>-0.849081</td>\n      <td>0.350032</td>\n      <td>-0.564165</td>\n      <td>0.148216</td>\n      <td>-0.367026</td>\n      <td>-0.215656</td>\n      <td>0.420577</td>\n      <td>-1.011036</td>\n      <td>-0.957762</td>\n      <td>0.305583</td>\n      <td>0.773493</td>\n      <td>-0.043671</td>\n      <td>-1.114937</td>\n      <td>0.366513</td>\n      <td>-0.008174</td>\n      <td>-0.966624</td>\n      <td>-0.162739</td>\n      <td>-0.922003</td>\n      <td>1.501988</td>\n      <td>-0.287712</td>\n      <td>-0.087983</td>\n      <td>-0.412584</td>\n      <td>-0.697739</td>\n      <td>0.682295</td>\n      <td>-0.564694</td>\n      <td>-0.115641</td>\n      <td>0.311940</td>\n      <td>-0.548520</td>\n      <td>0.288457</td>\n      <td>0.576435</td>\n      <td>0.841940</td>\n      <td>-0.851126</td>\n      <td>0.348616</td>\n      <td>-0.299471</td>\n      <td>0.903673</td>\n      <td>-0.340591</td>\n      <td>0.581356</td>\n      <td>...</td>\n      <td>-0.496664</td>\n      <td>-0.995956</td>\n      <td>-0.852672</td>\n      <td>-0.757186</td>\n      <td>1.106464</td>\n      <td>-0.907538</td>\n      <td>-0.478894</td>\n      <td>0.302331</td>\n      <td>1.120384</td>\n      <td>-0.275810</td>\n      <td>-0.750177</td>\n      <td>-0.992302</td>\n      <td>0.092748</td>\n      <td>0.431413</td>\n      <td>-0.155465</td>\n      <td>-0.900226</td>\n      <td>-0.683989</td>\n      <td>0.481938</td>\n      <td>-0.202353</td>\n      <td>-0.210354</td>\n      <td>-0.745271</td>\n      <td>-0.529248</td>\n      <td>0.172926</td>\n      <td>0.860504</td>\n      <td>0.965631</td>\n      <td>1.451151</td>\n      <td>0.656221</td>\n      <td>-0.535886</td>\n      <td>0.399552</td>\n      <td>-0.106880</td>\n      <td>0.190533</td>\n      <td>-0.776912</td>\n      <td>0.599643</td>\n      <td>-0.303378</td>\n      <td>0.118478</td>\n      <td>-1.112383</td>\n      <td>0.643194</td>\n      <td>-0.242223</td>\n      <td>-0.744683</td>\n      <td>0.107705</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>train_1</td>\n      <td>0</td>\n      <td>0.226713</td>\n      <td>-0.416363</td>\n      <td>0.864155</td>\n      <td>-0.467749</td>\n      <td>0.527324</td>\n      <td>0.979489</td>\n      <td>0.190804</td>\n      <td>0.014925</td>\n      <td>0.523831</td>\n      <td>0.231733</td>\n      <td>-0.111621</td>\n      <td>1.356555</td>\n      <td>-0.005922</td>\n      <td>-0.026486</td>\n      <td>-0.597899</td>\n      <td>-1.428478</td>\n      <td>1.275230</td>\n      <td>-1.026186</td>\n      <td>-0.624927</td>\n      <td>1.325749</td>\n      <td>-1.055828</td>\n      <td>-1.256152</td>\n      <td>1.040491</td>\n      <td>0.471547</td>\n      <td>-0.579970</td>\n      <td>0.561012</td>\n      <td>-0.919817</td>\n      <td>-0.613938</td>\n      <td>-0.443401</td>\n      <td>-1.213627</td>\n      <td>-0.153128</td>\n      <td>-0.825920</td>\n      <td>0.449260</td>\n      <td>0.722049</td>\n      <td>-0.080039</td>\n      <td>-0.431143</td>\n      <td>0.134638</td>\n      <td>-0.016755</td>\n      <td>...</td>\n      <td>0.332025</td>\n      <td>0.730327</td>\n      <td>-0.752482</td>\n      <td>0.664459</td>\n      <td>-0.282063</td>\n      <td>-0.568214</td>\n      <td>0.442581</td>\n      <td>1.216111</td>\n      <td>-0.203738</td>\n      <td>0.311982</td>\n      <td>0.870981</td>\n      <td>0.769315</td>\n      <td>0.187665</td>\n      <td>0.704931</td>\n      <td>0.951003</td>\n      <td>-0.270468</td>\n      <td>-0.423478</td>\n      <td>1.172760</td>\n      <td>-0.503017</td>\n      <td>2.274010</td>\n      <td>-0.378730</td>\n      <td>-0.284569</td>\n      <td>-1.152364</td>\n      <td>0.122980</td>\n      <td>0.726688</td>\n      <td>-0.135205</td>\n      <td>-0.278625</td>\n      <td>-0.322572</td>\n      <td>-0.328894</td>\n      <td>-0.748774</td>\n      <td>0.686540</td>\n      <td>0.085795</td>\n      <td>0.331306</td>\n      <td>1.344064</td>\n      <td>-0.530228</td>\n      <td>1.103135</td>\n      <td>0.672394</td>\n      <td>-0.074077</td>\n      <td>0.571886</td>\n      <td>0.297368</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>train_2</td>\n      <td>0</td>\n      <td>-0.445003</td>\n      <td>-0.186541</td>\n      <td>0.395469</td>\n      <td>0.347815</td>\n      <td>-0.221094</td>\n      <td>-0.350556</td>\n      <td>1.260908</td>\n      <td>-0.356903</td>\n      <td>-1.010902</td>\n      <td>-0.853215</td>\n      <td>-0.101807</td>\n      <td>-0.953897</td>\n      <td>0.619541</td>\n      <td>-0.179374</td>\n      <td>0.001175</td>\n      <td>0.119503</td>\n      <td>-0.431191</td>\n      <td>0.406277</td>\n      <td>-0.886761</td>\n      <td>0.660008</td>\n      <td>0.492790</td>\n      <td>0.078302</td>\n      <td>-0.744539</td>\n      <td>0.656165</td>\n      <td>-0.801597</td>\n      <td>-1.092070</td>\n      <td>-0.461288</td>\n      <td>-0.762118</td>\n      <td>0.282725</td>\n      <td>0.329095</td>\n      <td>-0.299508</td>\n      <td>0.703201</td>\n      <td>-0.450746</td>\n      <td>0.672537</td>\n      <td>-0.480606</td>\n      <td>0.025906</td>\n      <td>0.559460</td>\n      <td>0.302597</td>\n      <td>...</td>\n      <td>-0.629626</td>\n      <td>0.655365</td>\n      <td>-0.318371</td>\n      <td>-0.668212</td>\n      <td>0.189697</td>\n      <td>0.170122</td>\n      <td>0.587650</td>\n      <td>-0.144465</td>\n      <td>0.592102</td>\n      <td>0.253768</td>\n      <td>-1.116540</td>\n      <td>-0.404349</td>\n      <td>-1.025883</td>\n      <td>-0.608806</td>\n      <td>0.517736</td>\n      <td>-0.173667</td>\n      <td>0.200890</td>\n      <td>-0.701847</td>\n      <td>-0.687807</td>\n      <td>-0.029478</td>\n      <td>-0.209215</td>\n      <td>-0.334916</td>\n      <td>-1.031068</td>\n      <td>-0.812237</td>\n      <td>0.766020</td>\n      <td>-0.199879</td>\n      <td>-0.787668</td>\n      <td>0.222319</td>\n      <td>-0.863506</td>\n      <td>-1.079223</td>\n      <td>-0.046079</td>\n      <td>0.560891</td>\n      <td>-0.112102</td>\n      <td>-0.304303</td>\n      <td>0.764938</td>\n      <td>1.656951</td>\n      <td>-1.050161</td>\n      <td>-0.463036</td>\n      <td>-0.286149</td>\n      <td>0.200436</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>train_3</td>\n      <td>0</td>\n      <td>0.124444</td>\n      <td>-0.089159</td>\n      <td>-0.429020</td>\n      <td>0.120748</td>\n      <td>0.620850</td>\n      <td>0.247176</td>\n      <td>0.370517</td>\n      <td>-0.296912</td>\n      <td>-1.190060</td>\n      <td>0.313081</td>\n      <td>0.227981</td>\n      <td>0.729025</td>\n      <td>-0.663212</td>\n      <td>0.467393</td>\n      <td>-0.304826</td>\n      <td>0.430113</td>\n      <td>0.419286</td>\n      <td>0.541770</td>\n      <td>0.063061</td>\n      <td>0.104616</td>\n      <td>0.767889</td>\n      <td>-0.405298</td>\n      <td>0.619113</td>\n      <td>-0.371249</td>\n      <td>0.299160</td>\n      <td>0.182175</td>\n      <td>0.608684</td>\n      <td>0.097352</td>\n      <td>0.337827</td>\n      <td>-0.581574</td>\n      <td>-0.635253</td>\n      <td>0.024892</td>\n      <td>-0.522787</td>\n      <td>-1.264361</td>\n      <td>-1.016993</td>\n      <td>-1.334132</td>\n      <td>1.337273</td>\n      <td>-0.892191</td>\n      <td>...</td>\n      <td>0.473736</td>\n      <td>1.041680</td>\n      <td>0.903966</td>\n      <td>-0.908418</td>\n      <td>-1.003894</td>\n      <td>-0.277575</td>\n      <td>-0.685899</td>\n      <td>0.119487</td>\n      <td>-0.972358</td>\n      <td>-0.548794</td>\n      <td>-1.126202</td>\n      <td>-0.701387</td>\n      <td>-0.124130</td>\n      <td>1.423403</td>\n      <td>-0.516025</td>\n      <td>-0.818149</td>\n      <td>1.299764</td>\n      <td>0.537995</td>\n      <td>-0.748211</td>\n      <td>0.845507</td>\n      <td>-0.138955</td>\n      <td>-0.843117</td>\n      <td>0.086880</td>\n      <td>-0.199856</td>\n      <td>-0.941423</td>\n      <td>0.575705</td>\n      <td>-0.216262</td>\n      <td>-0.147592</td>\n      <td>-0.483779</td>\n      <td>0.753610</td>\n      <td>0.195359</td>\n      <td>-0.598020</td>\n      <td>-0.574591</td>\n      <td>-0.351340</td>\n      <td>1.065039</td>\n      <td>-0.548868</td>\n      <td>-0.627542</td>\n      <td>1.047370</td>\n      <td>0.480670</td>\n      <td>-0.385163</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>train_4</td>\n      <td>0</td>\n      <td>-0.159803</td>\n      <td>0.020439</td>\n      <td>0.604761</td>\n      <td>-0.061074</td>\n      <td>0.491579</td>\n      <td>0.600549</td>\n      <td>0.449607</td>\n      <td>0.541684</td>\n      <td>1.117206</td>\n      <td>0.024827</td>\n      <td>-1.245081</td>\n      <td>-1.058771</td>\n      <td>-0.656551</td>\n      <td>-0.099244</td>\n      <td>0.077138</td>\n      <td>0.786660</td>\n      <td>-0.206722</td>\n      <td>0.268829</td>\n      <td>-0.732545</td>\n      <td>-0.093668</td>\n      <td>0.889338</td>\n      <td>0.149640</td>\n      <td>1.419444</td>\n      <td>-0.382552</td>\n      <td>0.652448</td>\n      <td>-0.292659</td>\n      <td>0.895399</td>\n      <td>-0.951227</td>\n      <td>0.332834</td>\n      <td>0.822943</td>\n      <td>0.345179</td>\n      <td>-0.298126</td>\n      <td>0.123263</td>\n      <td>0.714956</td>\n      <td>1.195050</td>\n      <td>-0.445660</td>\n      <td>0.133004</td>\n      <td>-0.024071</td>\n      <td>...</td>\n      <td>0.569880</td>\n      <td>0.192224</td>\n      <td>0.052125</td>\n      <td>0.978550</td>\n      <td>1.094691</td>\n      <td>0.273358</td>\n      <td>0.498065</td>\n      <td>0.203953</td>\n      <td>-0.378357</td>\n      <td>-0.155426</td>\n      <td>0.227620</td>\n      <td>-0.241672</td>\n      <td>0.004731</td>\n      <td>0.340724</td>\n      <td>-0.164287</td>\n      <td>1.063376</td>\n      <td>-0.072776</td>\n      <td>0.433910</td>\n      <td>0.208957</td>\n      <td>0.706935</td>\n      <td>-0.652534</td>\n      <td>-0.138872</td>\n      <td>0.307499</td>\n      <td>0.539920</td>\n      <td>-0.048627</td>\n      <td>-0.582944</td>\n      <td>-0.554024</td>\n      <td>0.909670</td>\n      <td>0.133589</td>\n      <td>-0.698569</td>\n      <td>-0.726076</td>\n      <td>0.499102</td>\n      <td>-0.996298</td>\n      <td>1.031426</td>\n      <td>-0.979782</td>\n      <td>-0.669600</td>\n      <td>0.178488</td>\n      <td>0.458709</td>\n      <td>0.487211</td>\n      <td>-0.373372</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_p = scale(data, p_transform)\ndata_p.head()","execution_count":33,"outputs":[{"output_type":"execute_result","execution_count":33,"data":{"text/plain":"   ID_code  target     var_0    ...      var_197   var_198   var_199\n0  train_0       0 -0.548273    ...    -0.377830 -1.024994  0.173334\n1  train_1       0  0.317114    ...    -0.133905  0.818971  0.476058\n2  train_2       0 -0.660209    ...    -0.697303 -0.411488  0.320330\n3  train_3       0  0.175355    ...     1.507528  0.682927 -0.573633\n4  train_4       0 -0.232093    ...     0.643179  0.692646 -0.556359\n\n[5 rows x 202 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID_code</th>\n      <th>target</th>\n      <th>var_0</th>\n      <th>var_1</th>\n      <th>var_2</th>\n      <th>var_3</th>\n      <th>var_4</th>\n      <th>var_5</th>\n      <th>var_6</th>\n      <th>var_7</th>\n      <th>var_8</th>\n      <th>var_9</th>\n      <th>var_10</th>\n      <th>var_11</th>\n      <th>var_12</th>\n      <th>var_13</th>\n      <th>var_14</th>\n      <th>var_15</th>\n      <th>var_16</th>\n      <th>var_17</th>\n      <th>var_18</th>\n      <th>var_19</th>\n      <th>var_20</th>\n      <th>var_21</th>\n      <th>var_22</th>\n      <th>var_23</th>\n      <th>var_24</th>\n      <th>var_25</th>\n      <th>var_26</th>\n      <th>var_27</th>\n      <th>var_28</th>\n      <th>var_29</th>\n      <th>var_30</th>\n      <th>var_31</th>\n      <th>var_32</th>\n      <th>var_33</th>\n      <th>var_34</th>\n      <th>var_35</th>\n      <th>var_36</th>\n      <th>var_37</th>\n      <th>...</th>\n      <th>var_160</th>\n      <th>var_161</th>\n      <th>var_162</th>\n      <th>var_163</th>\n      <th>var_164</th>\n      <th>var_165</th>\n      <th>var_166</th>\n      <th>var_167</th>\n      <th>var_168</th>\n      <th>var_169</th>\n      <th>var_170</th>\n      <th>var_171</th>\n      <th>var_172</th>\n      <th>var_173</th>\n      <th>var_174</th>\n      <th>var_175</th>\n      <th>var_176</th>\n      <th>var_177</th>\n      <th>var_178</th>\n      <th>var_179</th>\n      <th>var_180</th>\n      <th>var_181</th>\n      <th>var_182</th>\n      <th>var_183</th>\n      <th>var_184</th>\n      <th>var_185</th>\n      <th>var_186</th>\n      <th>var_187</th>\n      <th>var_188</th>\n      <th>var_189</th>\n      <th>var_190</th>\n      <th>var_191</th>\n      <th>var_192</th>\n      <th>var_193</th>\n      <th>var_194</th>\n      <th>var_195</th>\n      <th>var_196</th>\n      <th>var_197</th>\n      <th>var_198</th>\n      <th>var_199</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>train_0</td>\n      <td>0</td>\n      <td>-0.548273</td>\n      <td>-1.281568</td>\n      <td>0.497638</td>\n      <td>-0.834279</td>\n      <td>0.224667</td>\n      <td>-0.539091</td>\n      <td>-0.306687</td>\n      <td>0.621745</td>\n      <td>-1.525186</td>\n      <td>-1.428374</td>\n      <td>0.450281</td>\n      <td>1.073821</td>\n      <td>-0.083774</td>\n      <td>-1.646239</td>\n      <td>0.567452</td>\n      <td>-0.002036</td>\n      <td>-1.450228</td>\n      <td>-0.240518</td>\n      <td>-1.416992</td>\n      <td>2.246311</td>\n      <td>-0.456943</td>\n      <td>-0.127623</td>\n      <td>-0.588674</td>\n      <td>-1.042613</td>\n      <td>1.005825</td>\n      <td>-0.816228</td>\n      <td>-0.150827</td>\n      <td>0.482651</td>\n      <td>-0.772335</td>\n      <td>0.396707</td>\n      <td>0.925472</td>\n      <td>1.156956</td>\n      <td>-1.299919</td>\n      <td>0.508490</td>\n      <td>-0.452285</td>\n      <td>1.303937</td>\n      <td>-0.492721</td>\n      <td>0.898334</td>\n      <td>...</td>\n      <td>-0.801403</td>\n      <td>-1.474932</td>\n      <td>-1.237846</td>\n      <td>-1.158952</td>\n      <td>1.794701</td>\n      <td>-1.320721</td>\n      <td>-0.717202</td>\n      <td>0.433893</td>\n      <td>1.444455</td>\n      <td>-0.403958</td>\n      <td>-1.081219</td>\n      <td>-1.415528</td>\n      <td>0.129183</td>\n      <td>0.635327</td>\n      <td>-0.253983</td>\n      <td>-1.377960</td>\n      <td>-0.975126</td>\n      <td>0.717733</td>\n      <td>-0.294316</td>\n      <td>-0.271998</td>\n      <td>-1.079717</td>\n      <td>-0.768280</td>\n      <td>0.260973</td>\n      <td>1.240331</td>\n      <td>1.375142</td>\n      <td>2.109849</td>\n      <td>0.903486</td>\n      <td>-0.812264</td>\n      <td>0.564008</td>\n      <td>-0.165667</td>\n      <td>0.274116</td>\n      <td>-1.172965</td>\n      <td>0.822370</td>\n      <td>-0.429567</td>\n      <td>0.169960</td>\n      <td>-1.620820</td>\n      <td>1.023320</td>\n      <td>-0.377830</td>\n      <td>-1.024994</td>\n      <td>0.173334</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>train_1</td>\n      <td>0</td>\n      <td>0.317114</td>\n      <td>-0.615970</td>\n      <td>1.172732</td>\n      <td>-0.689951</td>\n      <td>0.787508</td>\n      <td>1.545231</td>\n      <td>0.275588</td>\n      <td>0.018183</td>\n      <td>0.854196</td>\n      <td>0.382655</td>\n      <td>-0.157832</td>\n      <td>1.859704</td>\n      <td>-0.030142</td>\n      <td>-0.068434</td>\n      <td>-0.935107</td>\n      <td>-2.144525</td>\n      <td>1.685683</td>\n      <td>-1.460444</td>\n      <td>-0.948235</td>\n      <td>1.988151</td>\n      <td>-1.707462</td>\n      <td>-1.769324</td>\n      <td>1.456847</td>\n      <td>0.675818</td>\n      <td>-0.956280</td>\n      <td>0.796627</td>\n      <td>-1.333828</td>\n      <td>-0.932139</td>\n      <td>-0.626535</td>\n      <td>-1.812543</td>\n      <td>-0.231434</td>\n      <td>-1.172759</td>\n      <td>0.695158</td>\n      <td>1.060039</td>\n      <td>-0.124645</td>\n      <td>-0.601743</td>\n      <td>0.185302</td>\n      <td>-0.003266</td>\n      <td>...</td>\n      <td>0.502937</td>\n      <td>1.077143</td>\n      <td>-1.089616</td>\n      <td>0.927840</td>\n      <td>-0.410407</td>\n      <td>-0.833262</td>\n      <td>0.634287</td>\n      <td>1.750378</td>\n      <td>-0.292492</td>\n      <td>0.440756</td>\n      <td>1.242110</td>\n      <td>1.101811</td>\n      <td>0.261991</td>\n      <td>1.039881</td>\n      <td>1.435382</td>\n      <td>-0.370648</td>\n      <td>-0.601531</td>\n      <td>1.718931</td>\n      <td>-0.739252</td>\n      <td>2.763831</td>\n      <td>-0.533976</td>\n      <td>-0.403211</td>\n      <td>-1.741955</td>\n      <td>0.166307</td>\n      <td>1.047400</td>\n      <td>-0.193277</td>\n      <td>-0.395344</td>\n      <td>-0.488078</td>\n      <td>-0.514712</td>\n      <td>-1.128920</td>\n      <td>0.966836</td>\n      <td>0.139240</td>\n      <td>0.456896</td>\n      <td>1.968141</td>\n      <td>-0.817239</td>\n      <td>1.490942</td>\n      <td>1.069802</td>\n      <td>-0.133905</td>\n      <td>0.818971</td>\n      <td>0.476058</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>train_2</td>\n      <td>0</td>\n      <td>-0.660209</td>\n      <td>-0.265898</td>\n      <td>0.559289</td>\n      <td>0.534933</td>\n      <td>-0.315908</td>\n      <td>-0.513810</td>\n      <td>1.710191</td>\n      <td>-0.551262</td>\n      <td>-1.524992</td>\n      <td>-1.280003</td>\n      <td>-0.143659</td>\n      <td>-1.356473</td>\n      <td>0.886580</td>\n      <td>-0.302845</td>\n      <td>0.003886</td>\n      <td>0.187375</td>\n      <td>-0.627288</td>\n      <td>0.582803</td>\n      <td>-1.361068</td>\n      <td>1.004812</td>\n      <td>0.766166</td>\n      <td>0.106447</td>\n      <td>-1.096219</td>\n      <td>0.939065</td>\n      <td>-1.331777</td>\n      <td>-1.635250</td>\n      <td>-0.649088</td>\n      <td>-1.160068</td>\n      <td>0.392368</td>\n      <td>0.451112</td>\n      <td>-0.453159</td>\n      <td>0.969222</td>\n      <td>-0.685538</td>\n      <td>0.987657</td>\n      <td>-0.721695</td>\n      <td>0.046779</td>\n      <td>0.801491</td>\n      <td>0.479769</td>\n      <td>...</td>\n      <td>-1.020216</td>\n      <td>0.967807</td>\n      <td>-0.452034</td>\n      <td>-1.015385</td>\n      <td>0.316318</td>\n      <td>0.233712</td>\n      <td>0.851591</td>\n      <td>-0.196503</td>\n      <td>0.790304</td>\n      <td>0.356824</td>\n      <td>-1.633532</td>\n      <td>-0.570521</td>\n      <td>-1.456028</td>\n      <td>-0.865062</td>\n      <td>0.759224</td>\n      <td>-0.220518</td>\n      <td>0.315452</td>\n      <td>-1.172533</td>\n      <td>-1.012256</td>\n      <td>-0.022550</td>\n      <td>-0.283805</td>\n      <td>-0.477489</td>\n      <td>-1.558876</td>\n      <td>-1.268307</td>\n      <td>1.101734</td>\n      <td>-0.285546</td>\n      <td>-1.136040</td>\n      <td>0.346267</td>\n      <td>-1.322660</td>\n      <td>-1.623975</td>\n      <td>-0.060498</td>\n      <td>0.796244</td>\n      <td>-0.157759</td>\n      <td>-0.430843</td>\n      <td>1.151105</td>\n      <td>2.204624</td>\n      <td>-1.602690</td>\n      <td>-0.697303</td>\n      <td>-0.411488</td>\n      <td>0.320330</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>train_3</td>\n      <td>0</td>\n      <td>0.175355</td>\n      <td>-0.118417</td>\n      <td>-0.642989</td>\n      <td>0.193105</td>\n      <td>0.927382</td>\n      <td>0.407445</td>\n      <td>0.524980</td>\n      <td>-0.458394</td>\n      <td>-1.785050</td>\n      <td>0.519162</td>\n      <td>0.336337</td>\n      <td>1.013359</td>\n      <td>-0.938387</td>\n      <td>0.727104</td>\n      <td>-0.473655</td>\n      <td>0.645640</td>\n      <td>0.579824</td>\n      <td>0.781203</td>\n      <td>0.109192</td>\n      <td>0.171587</td>\n      <td>1.186202</td>\n      <td>-0.573974</td>\n      <td>0.889946</td>\n      <td>-0.553662</td>\n      <td>0.442790</td>\n      <td>0.276833</td>\n      <td>0.833253</td>\n      <td>0.156422</td>\n      <td>0.470600</td>\n      <td>-0.840453</td>\n      <td>-0.951029</td>\n      <td>0.037318</td>\n      <td>-0.796078</td>\n      <td>-2.061238</td>\n      <td>-1.515007</td>\n      <td>-1.873313</td>\n      <td>1.947529</td>\n      <td>-1.347818</td>\n      <td>...</td>\n      <td>0.715403</td>\n      <td>1.530226</td>\n      <td>1.291863</td>\n      <td>-1.406168</td>\n      <td>-1.480123</td>\n      <td>-0.414448</td>\n      <td>-1.014237</td>\n      <td>0.174580</td>\n      <td>-1.479234</td>\n      <td>-0.794295</td>\n      <td>-1.648198</td>\n      <td>-0.996949</td>\n      <td>-0.175426</td>\n      <td>2.116614</td>\n      <td>-0.774951</td>\n      <td>-1.244021</td>\n      <td>2.002443</td>\n      <td>0.801109</td>\n      <td>-1.101427</td>\n      <td>1.101199</td>\n      <td>-0.180589</td>\n      <td>-1.250325</td>\n      <td>0.130711</td>\n      <td>-0.319669</td>\n      <td>-1.458336</td>\n      <td>0.830742</td>\n      <td>-0.306063</td>\n      <td>-0.221215</td>\n      <td>-0.747605</td>\n      <td>1.130029</td>\n      <td>0.280909</td>\n      <td>-0.889432</td>\n      <td>-0.814567</td>\n      <td>-0.495622</td>\n      <td>1.605905</td>\n      <td>-0.782465</td>\n      <td>-0.959409</td>\n      <td>1.507528</td>\n      <td>0.682927</td>\n      <td>-0.573633</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>train_4</td>\n      <td>0</td>\n      <td>-0.232093</td>\n      <td>0.046879</td>\n      <td>0.837952</td>\n      <td>-0.080167</td>\n      <td>0.734148</td>\n      <td>0.955458</td>\n      <td>0.633432</td>\n      <td>0.798945</td>\n      <td>1.859494</td>\n      <td>0.043330</td>\n      <td>-1.762973</td>\n      <td>-1.509128</td>\n      <td>-0.929412</td>\n      <td>-0.180678</td>\n      <td>0.121634</td>\n      <td>1.168080</td>\n      <td>-0.295327</td>\n      <td>0.382370</td>\n      <td>-1.117330</td>\n      <td>-0.129836</td>\n      <td>1.370255</td>\n      <td>0.206903</td>\n      <td>1.954540</td>\n      <td>-0.570488</td>\n      <td>0.962737</td>\n      <td>-0.407568</td>\n      <td>1.205228</td>\n      <td>-1.451412</td>\n      <td>0.463507</td>\n      <td>1.094184</td>\n      <td>0.549966</td>\n      <td>-0.417081</td>\n      <td>0.195112</td>\n      <td>1.049683</td>\n      <td>1.807234</td>\n      <td>-0.622276</td>\n      <td>0.182949</td>\n      <td>-0.014385</td>\n      <td>...</td>\n      <td>0.858159</td>\n      <td>0.289683</td>\n      <td>0.084757</td>\n      <td>1.335249</td>\n      <td>1.775292</td>\n      <td>0.383747</td>\n      <td>0.717277</td>\n      <td>0.294160</td>\n      <td>-0.551210</td>\n      <td>-0.231463</td>\n      <td>0.345879</td>\n      <td>-0.337460</td>\n      <td>0.005760</td>\n      <td>0.501983</td>\n      <td>-0.266892</td>\n      <td>1.577887</td>\n      <td>-0.090824</td>\n      <td>0.645946</td>\n      <td>0.316214</td>\n      <td>0.930402</td>\n      <td>-0.941078</td>\n      <td>-0.190938</td>\n      <td>0.464764</td>\n      <td>0.778567</td>\n      <td>-0.063590</td>\n      <td>-0.829414</td>\n      <td>-0.793614</td>\n      <td>1.411094</td>\n      <td>0.173476</td>\n      <td>-1.053664</td>\n      <td>-1.038206</td>\n      <td>0.712982</td>\n      <td>-1.424261</td>\n      <td>1.497337</td>\n      <td>-1.502694</td>\n      <td>-0.959885</td>\n      <td>0.288478</td>\n      <td>0.643179</td>\n      <td>0.692646</td>\n      <td>-0.556359</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_q = scale(data, q_transform)\ndata_q.describe()","execution_count":35,"outputs":[{"output_type":"execute_result","execution_count":35,"data":{"text/plain":"              target         var_0      ...            var_198       var_199\ncount  200000.000000  2.000000e+05      ...       2.000000e+05  2.000000e+05\nmean        0.100490  5.009133e-01      ...       4.999995e-01  5.000022e-01\nstd         0.300653  2.888606e-01      ...       2.886509e-01  2.886658e-01\nmin         0.000000  1.000000e-07      ...       1.000000e-07  1.000000e-07\n25%         0.000000  2.511622e-01      ...       2.499140e-01  2.505416e-01\n50%         0.000000  5.012680e-01      ...       5.000326e-01  4.993915e-01\n75%         0.000000  7.515174e-01      ...       7.497028e-01  7.505009e-01\nmax         1.000000  9.999999e-01      ...       9.999999e-01  9.999999e-01\n\n[8 rows x 201 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>target</th>\n      <th>var_0</th>\n      <th>var_1</th>\n      <th>var_2</th>\n      <th>var_3</th>\n      <th>var_4</th>\n      <th>var_5</th>\n      <th>var_6</th>\n      <th>var_7</th>\n      <th>var_8</th>\n      <th>var_9</th>\n      <th>var_10</th>\n      <th>var_11</th>\n      <th>var_12</th>\n      <th>var_13</th>\n      <th>var_14</th>\n      <th>var_15</th>\n      <th>var_16</th>\n      <th>var_17</th>\n      <th>var_18</th>\n      <th>var_19</th>\n      <th>var_20</th>\n      <th>var_21</th>\n      <th>var_22</th>\n      <th>var_23</th>\n      <th>var_24</th>\n      <th>var_25</th>\n      <th>var_26</th>\n      <th>var_27</th>\n      <th>var_28</th>\n      <th>var_29</th>\n      <th>var_30</th>\n      <th>var_31</th>\n      <th>var_32</th>\n      <th>var_33</th>\n      <th>var_34</th>\n      <th>var_35</th>\n      <th>var_36</th>\n      <th>var_37</th>\n      <th>var_38</th>\n      <th>...</th>\n      <th>var_160</th>\n      <th>var_161</th>\n      <th>var_162</th>\n      <th>var_163</th>\n      <th>var_164</th>\n      <th>var_165</th>\n      <th>var_166</th>\n      <th>var_167</th>\n      <th>var_168</th>\n      <th>var_169</th>\n      <th>var_170</th>\n      <th>var_171</th>\n      <th>var_172</th>\n      <th>var_173</th>\n      <th>var_174</th>\n      <th>var_175</th>\n      <th>var_176</th>\n      <th>var_177</th>\n      <th>var_178</th>\n      <th>var_179</th>\n      <th>var_180</th>\n      <th>var_181</th>\n      <th>var_182</th>\n      <th>var_183</th>\n      <th>var_184</th>\n      <th>var_185</th>\n      <th>var_186</th>\n      <th>var_187</th>\n      <th>var_188</th>\n      <th>var_189</th>\n      <th>var_190</th>\n      <th>var_191</th>\n      <th>var_192</th>\n      <th>var_193</th>\n      <th>var_194</th>\n      <th>var_195</th>\n      <th>var_196</th>\n      <th>var_197</th>\n      <th>var_198</th>\n      <th>var_199</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>200000.000000</td>\n      <td>2.000000e+05</td>\n      <td>2.000000e+05</td>\n      <td>2.000000e+05</td>\n      <td>2.000000e+05</td>\n      <td>2.000000e+05</td>\n      <td>2.000000e+05</td>\n      <td>2.000000e+05</td>\n      <td>2.000000e+05</td>\n      <td>2.000000e+05</td>\n      <td>2.000000e+05</td>\n      <td>2.000000e+05</td>\n      <td>2.000000e+05</td>\n      <td>2.000000e+05</td>\n      <td>2.000000e+05</td>\n      <td>2.000000e+05</td>\n      <td>2.000000e+05</td>\n      <td>2.000000e+05</td>\n      <td>2.000000e+05</td>\n      <td>2.000000e+05</td>\n      <td>2.000000e+05</td>\n      <td>2.000000e+05</td>\n      <td>2.000000e+05</td>\n      <td>2.000000e+05</td>\n      <td>2.000000e+05</td>\n      <td>2.000000e+05</td>\n      <td>2.000000e+05</td>\n      <td>2.000000e+05</td>\n      <td>2.000000e+05</td>\n      <td>2.000000e+05</td>\n      <td>2.000000e+05</td>\n      <td>2.000000e+05</td>\n      <td>2.000000e+05</td>\n      <td>2.000000e+05</td>\n      <td>2.000000e+05</td>\n      <td>2.000000e+05</td>\n      <td>2.000000e+05</td>\n      <td>2.000000e+05</td>\n      <td>2.000000e+05</td>\n      <td>2.000000e+05</td>\n      <td>...</td>\n      <td>2.000000e+05</td>\n      <td>2.000000e+05</td>\n      <td>2.000000e+05</td>\n      <td>2.000000e+05</td>\n      <td>2.000000e+05</td>\n      <td>2.000000e+05</td>\n      <td>2.000000e+05</td>\n      <td>2.000000e+05</td>\n      <td>2.000000e+05</td>\n      <td>2.000000e+05</td>\n      <td>2.000000e+05</td>\n      <td>2.000000e+05</td>\n      <td>2.000000e+05</td>\n      <td>2.000000e+05</td>\n      <td>2.000000e+05</td>\n      <td>2.000000e+05</td>\n      <td>2.000000e+05</td>\n      <td>2.000000e+05</td>\n      <td>2.000000e+05</td>\n      <td>2.000000e+05</td>\n      <td>2.000000e+05</td>\n      <td>2.000000e+05</td>\n      <td>2.000000e+05</td>\n      <td>2.000000e+05</td>\n      <td>2.000000e+05</td>\n      <td>2.000000e+05</td>\n      <td>2.000000e+05</td>\n      <td>2.000000e+05</td>\n      <td>2.000000e+05</td>\n      <td>2.000000e+05</td>\n      <td>2.000000e+05</td>\n      <td>2.000000e+05</td>\n      <td>2.000000e+05</td>\n      <td>2.000000e+05</td>\n      <td>2.000000e+05</td>\n      <td>2.000000e+05</td>\n      <td>2.000000e+05</td>\n      <td>2.000000e+05</td>\n      <td>2.000000e+05</td>\n      <td>2.000000e+05</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.100490</td>\n      <td>5.009133e-01</td>\n      <td>5.005027e-01</td>\n      <td>4.996791e-01</td>\n      <td>5.005155e-01</td>\n      <td>4.993640e-01</td>\n      <td>5.001473e-01</td>\n      <td>5.013534e-01</td>\n      <td>5.001953e-01</td>\n      <td>4.998141e-01</td>\n      <td>5.018727e-01</td>\n      <td>5.005254e-01</td>\n      <td>4.999829e-01</td>\n      <td>5.006096e-01</td>\n      <td>4.989463e-01</td>\n      <td>4.996569e-01</td>\n      <td>4.996233e-01</td>\n      <td>4.995296e-01</td>\n      <td>5.006344e-01</td>\n      <td>4.996193e-01</td>\n      <td>5.002209e-01</td>\n      <td>5.006074e-01</td>\n      <td>4.999046e-01</td>\n      <td>4.986301e-01</td>\n      <td>5.008364e-01</td>\n      <td>5.001080e-01</td>\n      <td>5.000397e-01</td>\n      <td>5.003332e-01</td>\n      <td>5.013361e-01</td>\n      <td>5.006328e-01</td>\n      <td>5.013786e-01</td>\n      <td>4.996851e-01</td>\n      <td>4.999806e-01</td>\n      <td>5.001897e-01</td>\n      <td>5.011813e-01</td>\n      <td>5.006689e-01</td>\n      <td>4.992723e-01</td>\n      <td>5.009416e-01</td>\n      <td>5.002937e-01</td>\n      <td>4.994987e-01</td>\n      <td>...</td>\n      <td>5.003735e-01</td>\n      <td>5.000024e-01</td>\n      <td>4.989168e-01</td>\n      <td>5.006420e-01</td>\n      <td>4.998312e-01</td>\n      <td>4.989054e-01</td>\n      <td>4.992401e-01</td>\n      <td>4.991870e-01</td>\n      <td>4.997805e-01</td>\n      <td>5.008075e-01</td>\n      <td>5.006631e-01</td>\n      <td>4.999429e-01</td>\n      <td>4.994735e-01</td>\n      <td>4.993279e-01</td>\n      <td>4.990742e-01</td>\n      <td>4.998232e-01</td>\n      <td>4.998551e-01</td>\n      <td>5.004011e-01</td>\n      <td>5.009555e-01</td>\n      <td>4.999270e-01</td>\n      <td>5.003359e-01</td>\n      <td>4.998615e-01</td>\n      <td>5.005585e-01</td>\n      <td>4.986559e-01</td>\n      <td>4.996399e-01</td>\n      <td>4.997836e-01</td>\n      <td>4.993615e-01</td>\n      <td>4.998608e-01</td>\n      <td>4.994400e-01</td>\n      <td>4.993479e-01</td>\n      <td>4.997857e-01</td>\n      <td>5.003523e-01</td>\n      <td>4.998238e-01</td>\n      <td>4.997767e-01</td>\n      <td>5.018578e-01</td>\n      <td>5.009230e-01</td>\n      <td>5.007469e-01</td>\n      <td>5.003702e-01</td>\n      <td>4.999995e-01</td>\n      <td>5.000022e-01</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.300653</td>\n      <td>2.888606e-01</td>\n      <td>2.890426e-01</td>\n      <td>2.885573e-01</td>\n      <td>2.884554e-01</td>\n      <td>2.886660e-01</td>\n      <td>2.884064e-01</td>\n      <td>2.887558e-01</td>\n      <td>2.886723e-01</td>\n      <td>2.888769e-01</td>\n      <td>2.890080e-01</td>\n      <td>2.883223e-01</td>\n      <td>2.888288e-01</td>\n      <td>2.887290e-01</td>\n      <td>2.886902e-01</td>\n      <td>2.884510e-01</td>\n      <td>2.883790e-01</td>\n      <td>2.883699e-01</td>\n      <td>2.884560e-01</td>\n      <td>2.882669e-01</td>\n      <td>2.890106e-01</td>\n      <td>2.888112e-01</td>\n      <td>2.885361e-01</td>\n      <td>2.884915e-01</td>\n      <td>2.885058e-01</td>\n      <td>2.891534e-01</td>\n      <td>2.884339e-01</td>\n      <td>2.884887e-01</td>\n      <td>2.884770e-01</td>\n      <td>2.888619e-01</td>\n      <td>2.888948e-01</td>\n      <td>2.884059e-01</td>\n      <td>2.887925e-01</td>\n      <td>2.888432e-01</td>\n      <td>2.888900e-01</td>\n      <td>2.886626e-01</td>\n      <td>2.886378e-01</td>\n      <td>2.887172e-01</td>\n      <td>2.889192e-01</td>\n      <td>2.887959e-01</td>\n      <td>...</td>\n      <td>2.889990e-01</td>\n      <td>2.886488e-01</td>\n      <td>2.890886e-01</td>\n      <td>2.888771e-01</td>\n      <td>2.894441e-01</td>\n      <td>2.885058e-01</td>\n      <td>2.883470e-01</td>\n      <td>2.884129e-01</td>\n      <td>2.885234e-01</td>\n      <td>2.889513e-01</td>\n      <td>2.894146e-01</td>\n      <td>2.882888e-01</td>\n      <td>2.886921e-01</td>\n      <td>2.885660e-01</td>\n      <td>2.893783e-01</td>\n      <td>2.885987e-01</td>\n      <td>2.887341e-01</td>\n      <td>2.882447e-01</td>\n      <td>2.889596e-01</td>\n      <td>2.888574e-01</td>\n      <td>2.883948e-01</td>\n      <td>2.886768e-01</td>\n      <td>2.885364e-01</td>\n      <td>2.885821e-01</td>\n      <td>2.888837e-01</td>\n      <td>2.884035e-01</td>\n      <td>2.888563e-01</td>\n      <td>2.883651e-01</td>\n      <td>2.886734e-01</td>\n      <td>2.890006e-01</td>\n      <td>2.879172e-01</td>\n      <td>2.887139e-01</td>\n      <td>2.882666e-01</td>\n      <td>2.884614e-01</td>\n      <td>2.887687e-01</td>\n      <td>2.890083e-01</td>\n      <td>2.886203e-01</td>\n      <td>2.886649e-01</td>\n      <td>2.886509e-01</td>\n      <td>2.886658e-01</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>1.000000e-07</td>\n      <td>1.000000e-07</td>\n      <td>1.000000e-07</td>\n      <td>1.000000e-07</td>\n      <td>1.000000e-07</td>\n      <td>1.000000e-07</td>\n      <td>1.000000e-07</td>\n      <td>1.000000e-07</td>\n      <td>1.000000e-07</td>\n      <td>1.000000e-07</td>\n      <td>1.000000e-07</td>\n      <td>1.000000e-07</td>\n      <td>1.000000e-07</td>\n      <td>1.000000e-07</td>\n      <td>1.000000e-07</td>\n      <td>1.000000e-07</td>\n      <td>1.000000e-07</td>\n      <td>1.000000e-07</td>\n      <td>1.000000e-07</td>\n      <td>1.000000e-07</td>\n      <td>1.000000e-07</td>\n      <td>1.000000e-07</td>\n      <td>1.000000e-07</td>\n      <td>1.000000e-07</td>\n      <td>1.000000e-07</td>\n      <td>1.000000e-07</td>\n      <td>1.000000e-07</td>\n      <td>1.000000e-07</td>\n      <td>1.000000e-07</td>\n      <td>1.000000e-07</td>\n      <td>1.000000e-07</td>\n      <td>1.000000e-07</td>\n      <td>1.000000e-07</td>\n      <td>1.000000e-07</td>\n      <td>1.000000e-07</td>\n      <td>1.000000e-07</td>\n      <td>1.000000e-07</td>\n      <td>1.000000e-07</td>\n      <td>1.000000e-07</td>\n      <td>...</td>\n      <td>1.000000e-07</td>\n      <td>1.000000e-07</td>\n      <td>1.000000e-07</td>\n      <td>1.000000e-07</td>\n      <td>1.000000e-07</td>\n      <td>1.000000e-07</td>\n      <td>1.000000e-07</td>\n      <td>1.000000e-07</td>\n      <td>1.000000e-07</td>\n      <td>1.000000e-07</td>\n      <td>1.000000e-07</td>\n      <td>1.000000e-07</td>\n      <td>1.000000e-07</td>\n      <td>1.000000e-07</td>\n      <td>1.000000e-07</td>\n      <td>1.000000e-07</td>\n      <td>1.000000e-07</td>\n      <td>1.000000e-07</td>\n      <td>1.000000e-07</td>\n      <td>1.000000e-07</td>\n      <td>1.000000e-07</td>\n      <td>1.000000e-07</td>\n      <td>1.000000e-07</td>\n      <td>1.000000e-07</td>\n      <td>1.000000e-07</td>\n      <td>1.000000e-07</td>\n      <td>1.000000e-07</td>\n      <td>1.000000e-07</td>\n      <td>1.000000e-07</td>\n      <td>1.000000e-07</td>\n      <td>1.000000e-07</td>\n      <td>1.000000e-07</td>\n      <td>1.000000e-07</td>\n      <td>1.000000e-07</td>\n      <td>1.000000e-07</td>\n      <td>1.000000e-07</td>\n      <td>1.000000e-07</td>\n      <td>1.000000e-07</td>\n      <td>1.000000e-07</td>\n      <td>1.000000e-07</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.000000</td>\n      <td>2.511622e-01</td>\n      <td>2.495506e-01</td>\n      <td>2.498471e-01</td>\n      <td>2.506062e-01</td>\n      <td>2.496202e-01</td>\n      <td>2.512266e-01</td>\n      <td>2.512513e-01</td>\n      <td>2.505160e-01</td>\n      <td>2.498162e-01</td>\n      <td>2.518050e-01</td>\n      <td>2.517004e-01</td>\n      <td>2.498736e-01</td>\n      <td>2.507507e-01</td>\n      <td>2.491337e-01</td>\n      <td>2.507575e-01</td>\n      <td>2.497113e-01</td>\n      <td>2.502373e-01</td>\n      <td>2.511575e-01</td>\n      <td>2.508860e-01</td>\n      <td>2.502096e-01</td>\n      <td>2.505266e-01</td>\n      <td>2.495769e-01</td>\n      <td>2.484592e-01</td>\n      <td>2.516899e-01</td>\n      <td>2.494240e-01</td>\n      <td>2.497498e-01</td>\n      <td>2.503361e-01</td>\n      <td>2.518346e-01</td>\n      <td>2.499887e-01</td>\n      <td>2.510955e-01</td>\n      <td>2.507162e-01</td>\n      <td>2.498466e-01</td>\n      <td>2.500247e-01</td>\n      <td>2.510613e-01</td>\n      <td>2.509504e-01</td>\n      <td>2.489085e-01</td>\n      <td>2.507363e-01</td>\n      <td>2.497015e-01</td>\n      <td>2.491037e-01</td>\n      <td>...</td>\n      <td>2.490866e-01</td>\n      <td>2.496246e-01</td>\n      <td>2.488581e-01</td>\n      <td>2.513839e-01</td>\n      <td>2.482396e-01</td>\n      <td>2.492750e-01</td>\n      <td>2.498054e-01</td>\n      <td>2.493434e-01</td>\n      <td>2.496257e-01</td>\n      <td>2.507052e-01</td>\n      <td>2.499966e-01</td>\n      <td>2.501991e-01</td>\n      <td>2.499269e-01</td>\n      <td>2.494638e-01</td>\n      <td>2.480395e-01</td>\n      <td>2.488969e-01</td>\n      <td>2.495542e-01</td>\n      <td>2.508470e-01</td>\n      <td>2.501683e-01</td>\n      <td>2.492586e-01</td>\n      <td>2.509175e-01</td>\n      <td>2.497113e-01</td>\n      <td>2.505337e-01</td>\n      <td>2.490694e-01</td>\n      <td>2.493451e-01</td>\n      <td>2.502444e-01</td>\n      <td>2.495436e-01</td>\n      <td>2.505795e-01</td>\n      <td>2.481064e-01</td>\n      <td>2.488774e-01</td>\n      <td>2.506057e-01</td>\n      <td>2.500032e-01</td>\n      <td>2.506979e-01</td>\n      <td>2.502807e-01</td>\n      <td>2.517292e-01</td>\n      <td>2.502332e-01</td>\n      <td>2.510326e-01</td>\n      <td>2.504576e-01</td>\n      <td>2.499140e-01</td>\n      <td>2.505416e-01</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.000000</td>\n      <td>5.012680e-01</td>\n      <td>5.015864e-01</td>\n      <td>5.000934e-01</td>\n      <td>5.001663e-01</td>\n      <td>4.988541e-01</td>\n      <td>5.005539e-01</td>\n      <td>5.023542e-01</td>\n      <td>5.001550e-01</td>\n      <td>4.991854e-01</td>\n      <td>5.027612e-01</td>\n      <td>5.009623e-01</td>\n      <td>4.996766e-01</td>\n      <td>5.007007e-01</td>\n      <td>4.985933e-01</td>\n      <td>4.992111e-01</td>\n      <td>5.006394e-01</td>\n      <td>4.996647e-01</td>\n      <td>5.013275e-01</td>\n      <td>4.986859e-01</td>\n      <td>5.006481e-01</td>\n      <td>5.017670e-01</td>\n      <td>5.000415e-01</td>\n      <td>4.978353e-01</td>\n      <td>5.006279e-01</td>\n      <td>4.999139e-01</td>\n      <td>5.007230e-01</td>\n      <td>5.011054e-01</td>\n      <td>5.021879e-01</td>\n      <td>5.007834e-01</td>\n      <td>5.021620e-01</td>\n      <td>5.008482e-01</td>\n      <td>5.000084e-01</td>\n      <td>4.999493e-01</td>\n      <td>5.019390e-01</td>\n      <td>5.011122e-01</td>\n      <td>4.989974e-01</td>\n      <td>5.019557e-01</td>\n      <td>4.996406e-01</td>\n      <td>4.997450e-01</td>\n      <td>...</td>\n      <td>4.999648e-01</td>\n      <td>5.003337e-01</td>\n      <td>4.978520e-01</td>\n      <td>5.000500e-01</td>\n      <td>4.994632e-01</td>\n      <td>4.982119e-01</td>\n      <td>4.983165e-01</td>\n      <td>4.990091e-01</td>\n      <td>4.997373e-01</td>\n      <td>5.018352e-01</td>\n      <td>5.009240e-01</td>\n      <td>5.003542e-01</td>\n      <td>4.990717e-01</td>\n      <td>4.989399e-01</td>\n      <td>4.991002e-01</td>\n      <td>4.998266e-01</td>\n      <td>4.997493e-01</td>\n      <td>5.008048e-01</td>\n      <td>5.022381e-01</td>\n      <td>5.009768e-01</td>\n      <td>5.004203e-01</td>\n      <td>5.008728e-01</td>\n      <td>5.012073e-01</td>\n      <td>4.983003e-01</td>\n      <td>5.007097e-01</td>\n      <td>4.998911e-01</td>\n      <td>4.983880e-01</td>\n      <td>5.000108e-01</td>\n      <td>4.992506e-01</td>\n      <td>4.990163e-01</td>\n      <td>4.998402e-01</td>\n      <td>5.007819e-01</td>\n      <td>4.995127e-01</td>\n      <td>4.998888e-01</td>\n      <td>5.030793e-01</td>\n      <td>5.019228e-01</td>\n      <td>5.004527e-01</td>\n      <td>5.007407e-01</td>\n      <td>5.000326e-01</td>\n      <td>4.993915e-01</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.000000</td>\n      <td>7.515174e-01</td>\n      <td>7.514817e-01</td>\n      <td>7.489763e-01</td>\n      <td>7.502741e-01</td>\n      <td>7.494549e-01</td>\n      <td>7.493301e-01</td>\n      <td>7.517082e-01</td>\n      <td>7.499113e-01</td>\n      <td>7.504752e-01</td>\n      <td>7.529820e-01</td>\n      <td>7.502532e-01</td>\n      <td>7.507984e-01</td>\n      <td>7.510367e-01</td>\n      <td>7.484332e-01</td>\n      <td>7.496003e-01</td>\n      <td>7.488145e-01</td>\n      <td>7.486110e-01</td>\n      <td>7.509928e-01</td>\n      <td>7.491127e-01</td>\n      <td>7.509998e-01</td>\n      <td>7.507508e-01</td>\n      <td>7.496155e-01</td>\n      <td>7.475686e-01</td>\n      <td>7.512715e-01</td>\n      <td>7.503261e-01</td>\n      <td>7.494161e-01</td>\n      <td>7.494788e-01</td>\n      <td>7.506864e-01</td>\n      <td>7.509732e-01</td>\n      <td>7.522790e-01</td>\n      <td>7.485744e-01</td>\n      <td>7.508170e-01</td>\n      <td>7.507508e-01</td>\n      <td>7.519674e-01</td>\n      <td>7.506006e-01</td>\n      <td>7.490444e-01</td>\n      <td>7.512281e-01</td>\n      <td>7.511981e-01</td>\n      <td>7.485615e-01</td>\n      <td>...</td>\n      <td>7.513492e-01</td>\n      <td>7.496041e-01</td>\n      <td>7.496311e-01</td>\n      <td>7.511922e-01</td>\n      <td>7.506523e-01</td>\n      <td>7.486502e-01</td>\n      <td>7.489797e-01</td>\n      <td>7.487839e-01</td>\n      <td>7.486389e-01</td>\n      <td>7.505411e-01</td>\n      <td>7.515596e-01</td>\n      <td>7.487962e-01</td>\n      <td>7.485978e-01</td>\n      <td>7.486060e-01</td>\n      <td>7.494171e-01</td>\n      <td>7.496989e-01</td>\n      <td>7.498208e-01</td>\n      <td>7.490762e-01</td>\n      <td>7.515866e-01</td>\n      <td>7.494000e-01</td>\n      <td>7.498868e-01</td>\n      <td>7.490473e-01</td>\n      <td>7.494671e-01</td>\n      <td>7.483482e-01</td>\n      <td>7.500142e-01</td>\n      <td>7.495284e-01</td>\n      <td>7.495914e-01</td>\n      <td>7.486788e-01</td>\n      <td>7.499488e-01</td>\n      <td>7.497497e-01</td>\n      <td>7.489888e-01</td>\n      <td>7.504178e-01</td>\n      <td>7.486463e-01</td>\n      <td>7.495756e-01</td>\n      <td>7.522302e-01</td>\n      <td>7.515000e-01</td>\n      <td>7.505947e-01</td>\n      <td>7.502908e-01</td>\n      <td>7.497028e-01</td>\n      <td>7.505009e-01</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1.000000</td>\n      <td>9.999999e-01</td>\n      <td>9.999999e-01</td>\n      <td>9.999999e-01</td>\n      <td>9.999999e-01</td>\n      <td>9.999999e-01</td>\n      <td>9.999999e-01</td>\n      <td>9.999999e-01</td>\n      <td>9.999999e-01</td>\n      <td>9.999999e-01</td>\n      <td>9.999999e-01</td>\n      <td>9.999999e-01</td>\n      <td>9.999999e-01</td>\n      <td>9.999999e-01</td>\n      <td>9.999999e-01</td>\n      <td>9.999999e-01</td>\n      <td>9.999999e-01</td>\n      <td>9.999999e-01</td>\n      <td>9.999999e-01</td>\n      <td>9.999999e-01</td>\n      <td>9.999999e-01</td>\n      <td>9.999999e-01</td>\n      <td>9.999999e-01</td>\n      <td>9.999999e-01</td>\n      <td>9.999999e-01</td>\n      <td>9.999999e-01</td>\n      <td>9.999999e-01</td>\n      <td>9.999999e-01</td>\n      <td>9.999999e-01</td>\n      <td>9.999999e-01</td>\n      <td>9.999999e-01</td>\n      <td>9.999999e-01</td>\n      <td>9.999999e-01</td>\n      <td>9.999999e-01</td>\n      <td>9.999999e-01</td>\n      <td>9.999999e-01</td>\n      <td>9.999999e-01</td>\n      <td>9.999999e-01</td>\n      <td>9.999999e-01</td>\n      <td>9.999999e-01</td>\n      <td>...</td>\n      <td>9.999999e-01</td>\n      <td>9.999999e-01</td>\n      <td>9.999999e-01</td>\n      <td>9.999999e-01</td>\n      <td>9.999999e-01</td>\n      <td>9.999999e-01</td>\n      <td>9.999999e-01</td>\n      <td>9.999999e-01</td>\n      <td>9.999999e-01</td>\n      <td>9.999999e-01</td>\n      <td>9.999999e-01</td>\n      <td>9.999999e-01</td>\n      <td>9.999999e-01</td>\n      <td>9.999999e-01</td>\n      <td>9.999999e-01</td>\n      <td>9.999999e-01</td>\n      <td>9.999999e-01</td>\n      <td>9.999999e-01</td>\n      <td>9.999999e-01</td>\n      <td>9.999999e-01</td>\n      <td>9.999999e-01</td>\n      <td>9.999999e-01</td>\n      <td>9.999999e-01</td>\n      <td>9.999999e-01</td>\n      <td>9.999999e-01</td>\n      <td>9.999999e-01</td>\n      <td>9.999999e-01</td>\n      <td>9.999999e-01</td>\n      <td>9.999999e-01</td>\n      <td>9.999999e-01</td>\n      <td>9.999999e-01</td>\n      <td>9.999999e-01</td>\n      <td>9.999999e-01</td>\n      <td>9.999999e-01</td>\n      <td>9.999999e-01</td>\n      <td>9.999999e-01</td>\n      <td>9.999999e-01</td>\n      <td>9.999999e-01</td>\n      <td>9.999999e-01</td>\n      <td>9.999999e-01</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true,"_uuid":"aa0ded4e2fa7a6df630bf419e4cc7f67680526b8"},"cell_type":"code","source":"# Now for the model\n# Let us use an ANN with 1 hidden layer\n# Input Layer has 200 units\n# Output Layers has 1 unit\n# Hidden Layers should have 100 units\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.losses import sigmoid_cross_entropy\nfrom tensorflow.train import AdamOptimizer\n\nclassifier = Sequential()\nclassifier.add(Dense(units = 100, input_shape=(200,), activation = 'relu'))\nclassifier.add(Dense(units = 1, activation = 'sigmoid'))\nadam = AdamOptimizer()\nclassifier.compile(optimizer = adam, loss = sigmoid_cross_entropy, metrics = ['accuracy'])","execution_count":24,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"94dfdd0031473bd924cd3477c4cea1249bf2f22d"},"cell_type":"code","source":"# checking number of training examples to decide batch_size, epochs and validation_split \n\nlen(data)","execution_count":25,"outputs":[{"output_type":"execute_result","execution_count":25,"data":{"text/plain":"200000"},"metadata":{}}]},{"metadata":{"trusted":true,"_uuid":"9563c8327e57d4a0ca096a3a0cadffbbfe1b1365"},"cell_type":"code","source":"# setting up the callback to save weights with best 'val_acc'\n\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n\ncheckpoint = ModelCheckpoint('./weights.keras', 'val_acc', 1, True, True, 'auto', 1)","execution_count":27,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7e9a5fcdd95ade119d016882038ec29e1494ca52","_kg_hide-input":false},"cell_type":"code","source":"# time to train\nclassifier.fit(data_mm[data_mm.columns[2:]].values, data_mm['target'].values, batch_size = 500, epochs = 50, callbacks = [checkpoint], validation_split = 0.2)","execution_count":28,"outputs":[{"output_type":"stream","text":"Train on 160000 samples, validate on 40000 samples\nEpoch 1/50\n158500/160000 [============================>.] - ETA: 0s - loss: 0.7061 - acc: 0.8975\nEpoch 00001: val_acc improved from -inf to 0.89878, saving model to ./weights.keras\n160000/160000 [==============================] - 4s 23us/step - loss: 0.7060 - acc: 0.8975 - val_loss: 0.6941 - val_acc: 0.8988\nEpoch 2/50\n159500/160000 [============================>.] - ETA: 0s - loss: 0.6937 - acc: 0.8997\nEpoch 00002: val_acc did not improve from 0.89878\n160000/160000 [==============================] - 3s 21us/step - loss: 0.6937 - acc: 0.8997 - val_loss: 0.6934 - val_acc: 0.8988\nEpoch 3/50\n158500/160000 [============================>.] - ETA: 0s - loss: 0.6933 - acc: 0.8996\nEpoch 00003: val_acc did not improve from 0.89878\n160000/160000 [==============================] - 3s 21us/step - loss: 0.6933 - acc: 0.8997 - val_loss: 0.6933 - val_acc: 0.8988\nEpoch 4/50\n159000/160000 [============================>.] - ETA: 0s - loss: 0.6933 - acc: 0.8997\nEpoch 00004: val_acc did not improve from 0.89878\n160000/160000 [==============================] - 3s 21us/step - loss: 0.6933 - acc: 0.8997 - val_loss: 0.6932 - val_acc: 0.8988\nEpoch 5/50\n158000/160000 [============================>.] - ETA: 0s - loss: 0.6932 - acc: 0.8997\nEpoch 00005: val_acc did not improve from 0.89878\n160000/160000 [==============================] - 3s 21us/step - loss: 0.6932 - acc: 0.8997 - val_loss: 0.6932 - val_acc: 0.8988\nEpoch 6/50\n159000/160000 [============================>.] - ETA: 0s - loss: 0.6932 - acc: 0.8998\nEpoch 00006: val_acc did not improve from 0.89878\n160000/160000 [==============================] - 3s 21us/step - loss: 0.6932 - acc: 0.8997 - val_loss: 0.6932 - val_acc: 0.8988\nEpoch 7/50\n159500/160000 [============================>.] - ETA: 0s - loss: 0.6932 - acc: 0.8996\nEpoch 00007: val_acc did not improve from 0.89878\n160000/160000 [==============================] - 3s 21us/step - loss: 0.6932 - acc: 0.8997 - val_loss: 0.6932 - val_acc: 0.8988\nEpoch 8/50\n158500/160000 [============================>.] - ETA: 0s - loss: 0.6932 - acc: 0.8997\nEpoch 00008: val_acc did not improve from 0.89878\n160000/160000 [==============================] - 3s 21us/step - loss: 0.6932 - acc: 0.8997 - val_loss: 0.6932 - val_acc: 0.8988\nEpoch 9/50\n157500/160000 [============================>.] - ETA: 0s - loss: 0.6932 - acc: 0.8996\nEpoch 00009: val_acc did not improve from 0.89878\n160000/160000 [==============================] - 3s 21us/step - loss: 0.6932 - acc: 0.8997 - val_loss: 0.6932 - val_acc: 0.8988\nEpoch 10/50\n158500/160000 [============================>.] - ETA: 0s - loss: 0.6932 - acc: 0.8998\nEpoch 00010: val_acc did not improve from 0.89878\n160000/160000 [==============================] - 3s 21us/step - loss: 0.6932 - acc: 0.8997 - val_loss: 0.6932 - val_acc: 0.8988\nEpoch 11/50\n157000/160000 [============================>.] - ETA: 0s - loss: 0.6932 - acc: 0.8997\nEpoch 00011: val_acc did not improve from 0.89878\n160000/160000 [==============================] - 3s 21us/step - loss: 0.6932 - acc: 0.8997 - val_loss: 0.6932 - val_acc: 0.8988\nEpoch 12/50\n158500/160000 [============================>.] - ETA: 0s - loss: 0.6932 - acc: 0.8996\nEpoch 00012: val_acc did not improve from 0.89878\n160000/160000 [==============================] - 3s 21us/step - loss: 0.6932 - acc: 0.8997 - val_loss: 0.6932 - val_acc: 0.8988\nEpoch 13/50\n158500/160000 [============================>.] - ETA: 0s - loss: 0.6932 - acc: 0.8997\nEpoch 00013: val_acc did not improve from 0.89878\n160000/160000 [==============================] - 3s 22us/step - loss: 0.6932 - acc: 0.8997 - val_loss: 0.6932 - val_acc: 0.8988\nEpoch 14/50\n158000/160000 [============================>.] - ETA: 0s - loss: 0.6932 - acc: 0.8997\nEpoch 00014: val_acc did not improve from 0.89878\n160000/160000 [==============================] - 4s 22us/step - loss: 0.6932 - acc: 0.8997 - val_loss: 0.6932 - val_acc: 0.8988\nEpoch 15/50\n159000/160000 [============================>.] - ETA: 0s - loss: 0.6932 - acc: 0.8998\nEpoch 00015: val_acc did not improve from 0.89878\n160000/160000 [==============================] - 3s 22us/step - loss: 0.6932 - acc: 0.8997 - val_loss: 0.6932 - val_acc: 0.8988\nEpoch 16/50\n159500/160000 [============================>.] - ETA: 0s - loss: 0.6932 - acc: 0.8996\nEpoch 00016: val_acc did not improve from 0.89878\n160000/160000 [==============================] - 3s 21us/step - loss: 0.6932 - acc: 0.8997 - val_loss: 0.6932 - val_acc: 0.8988\nEpoch 17/50\n158500/160000 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.8996\nEpoch 00017: val_acc did not improve from 0.89878\n160000/160000 [==============================] - 3s 21us/step - loss: 0.6931 - acc: 0.8997 - val_loss: 0.6931 - val_acc: 0.8988\nEpoch 18/50\n159000/160000 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.8996\nEpoch 00018: val_acc did not improve from 0.89878\n160000/160000 [==============================] - 3s 21us/step - loss: 0.6931 - acc: 0.8997 - val_loss: 0.6931 - val_acc: 0.8988\nEpoch 19/50\n159000/160000 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.8998\nEpoch 00019: val_acc did not improve from 0.89878\n160000/160000 [==============================] - 3s 21us/step - loss: 0.6931 - acc: 0.8997 - val_loss: 0.6931 - val_acc: 0.8988\nEpoch 20/50\n158500/160000 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.8996\nEpoch 00020: val_acc did not improve from 0.89878\n160000/160000 [==============================] - 3s 21us/step - loss: 0.6931 - acc: 0.8997 - val_loss: 0.6931 - val_acc: 0.8988\nEpoch 21/50\n158000/160000 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.8997\nEpoch 00021: val_acc did not improve from 0.89878\n160000/160000 [==============================] - 3s 21us/step - loss: 0.6931 - acc: 0.8997 - val_loss: 0.6931 - val_acc: 0.8988\nEpoch 22/50\n158000/160000 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.8997\nEpoch 00022: val_acc did not improve from 0.89878\n160000/160000 [==============================] - 3s 21us/step - loss: 0.6931 - acc: 0.8997 - val_loss: 0.6931 - val_acc: 0.8988\nEpoch 23/50\n157500/160000 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.8997\nEpoch 00023: val_acc did not improve from 0.89878\n160000/160000 [==============================] - 3s 21us/step - loss: 0.6931 - acc: 0.8997 - val_loss: 0.6931 - val_acc: 0.8988\nEpoch 24/50\n157500/160000 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.8997\nEpoch 00024: val_acc did not improve from 0.89878\n160000/160000 [==============================] - 3s 21us/step - loss: 0.6931 - acc: 0.8997 - val_loss: 0.6931 - val_acc: 0.8988\nEpoch 25/50\n157500/160000 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.8997\nEpoch 00025: val_acc did not improve from 0.89878\n160000/160000 [==============================] - 3s 21us/step - loss: 0.6931 - acc: 0.8997 - val_loss: 0.6931 - val_acc: 0.8988\nEpoch 26/50\n159000/160000 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.8997\nEpoch 00026: val_acc did not improve from 0.89878\n160000/160000 [==============================] - 3s 21us/step - loss: 0.6931 - acc: 0.8997 - val_loss: 0.6931 - val_acc: 0.8988\nEpoch 27/50\n158000/160000 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.8996\nEpoch 00027: val_acc did not improve from 0.89878\n160000/160000 [==============================] - 3s 21us/step - loss: 0.6931 - acc: 0.8997 - val_loss: 0.6931 - val_acc: 0.8988\nEpoch 28/50\n157000/160000 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.8997\nEpoch 00028: val_acc did not improve from 0.89878\n160000/160000 [==============================] - 3s 21us/step - loss: 0.6931 - acc: 0.8997 - val_loss: 0.6931 - val_acc: 0.8988\nEpoch 29/50\n157500/160000 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.8998\nEpoch 00029: val_acc did not improve from 0.89878\n160000/160000 [==============================] - 3s 22us/step - loss: 0.6931 - acc: 0.8997 - val_loss: 0.6931 - val_acc: 0.8988\nEpoch 30/50\n158000/160000 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.8998\nEpoch 00030: val_acc did not improve from 0.89878\n160000/160000 [==============================] - 4s 22us/step - loss: 0.6931 - acc: 0.8997 - val_loss: 0.6931 - val_acc: 0.8988\n","name":"stdout"},{"output_type":"stream","text":"Epoch 31/50\n158000/160000 [============================>.] - ETA: 0s - loss: 0.6921 - acc: 0.9036\nEpoch 00031: val_acc improved from 0.89878 to 0.90538, saving model to ./weights.keras\n160000/160000 [==============================] - 4s 23us/step - loss: 0.6921 - acc: 0.9037 - val_loss: 0.6910 - val_acc: 0.9054\nEpoch 32/50\n158000/160000 [============================>.] - ETA: 0s - loss: 0.6896 - acc: 0.9116\nEpoch 00032: val_acc improved from 0.90538 to 0.90940, saving model to ./weights.keras\n160000/160000 [==============================] - 4s 23us/step - loss: 0.6895 - acc: 0.9117 - val_loss: 0.6905 - val_acc: 0.9094\nEpoch 33/50\n159000/160000 [============================>.] - ETA: 0s - loss: 0.6883 - acc: 0.9150\nEpoch 00033: val_acc did not improve from 0.90940\n160000/160000 [==============================] - 4s 22us/step - loss: 0.6883 - acc: 0.9151 - val_loss: 0.6905 - val_acc: 0.9092\nEpoch 34/50\n159000/160000 [============================>.] - ETA: 0s - loss: 0.6875 - acc: 0.9176\nEpoch 00034: val_acc did not improve from 0.90940\n160000/160000 [==============================] - 3s 22us/step - loss: 0.6875 - acc: 0.9176 - val_loss: 0.6904 - val_acc: 0.9085\nEpoch 35/50\n159500/160000 [============================>.] - ETA: 0s - loss: 0.6869 - acc: 0.9192\nEpoch 00035: val_acc did not improve from 0.90940\n160000/160000 [==============================] - 4s 22us/step - loss: 0.6869 - acc: 0.9191 - val_loss: 0.6906 - val_acc: 0.9093\nEpoch 36/50\n157500/160000 [============================>.] - ETA: 0s - loss: 0.6863 - acc: 0.9207\nEpoch 00036: val_acc improved from 0.90940 to 0.90990, saving model to ./weights.keras\n160000/160000 [==============================] - 3s 21us/step - loss: 0.6863 - acc: 0.9207 - val_loss: 0.6905 - val_acc: 0.9099\nEpoch 37/50\n157500/160000 [============================>.] - ETA: 0s - loss: 0.6858 - acc: 0.9223\nEpoch 00037: val_acc did not improve from 0.90990\n160000/160000 [==============================] - 3s 21us/step - loss: 0.6858 - acc: 0.9222 - val_loss: 0.6906 - val_acc: 0.9099\nEpoch 38/50\n159000/160000 [============================>.] - ETA: 0s - loss: 0.6854 - acc: 0.9231\nEpoch 00038: val_acc did not improve from 0.90990\n160000/160000 [==============================] - 3s 21us/step - loss: 0.6854 - acc: 0.9231 - val_loss: 0.6907 - val_acc: 0.9098\nEpoch 39/50\n157000/160000 [============================>.] - ETA: 0s - loss: 0.6850 - acc: 0.9242\nEpoch 00039: val_acc did not improve from 0.90990\n160000/160000 [==============================] - 3s 21us/step - loss: 0.6850 - acc: 0.9242 - val_loss: 0.6907 - val_acc: 0.9097\nEpoch 40/50\n159000/160000 [============================>.] - ETA: 0s - loss: 0.6846 - acc: 0.9251\nEpoch 00040: val_acc did not improve from 0.90990\n160000/160000 [==============================] - 3s 21us/step - loss: 0.6846 - acc: 0.9250 - val_loss: 0.6907 - val_acc: 0.9098\nEpoch 41/50\n157500/160000 [============================>.] - ETA: 0s - loss: 0.6843 - acc: 0.9255\nEpoch 00041: val_acc did not improve from 0.90990\n160000/160000 [==============================] - 3s 21us/step - loss: 0.6843 - acc: 0.9256 - val_loss: 0.6909 - val_acc: 0.9097\nEpoch 42/50\n159000/160000 [============================>.] - ETA: 0s - loss: 0.6840 - acc: 0.9262\nEpoch 00042: val_acc did not improve from 0.90990\n160000/160000 [==============================] - 3s 21us/step - loss: 0.6840 - acc: 0.9262 - val_loss: 0.6909 - val_acc: 0.9091\nEpoch 43/50\n159000/160000 [============================>.] - ETA: 0s - loss: 0.6838 - acc: 0.9268\nEpoch 00043: val_acc did not improve from 0.90990\n160000/160000 [==============================] - 3s 21us/step - loss: 0.6838 - acc: 0.9268 - val_loss: 0.6910 - val_acc: 0.9098\nEpoch 44/50\n159500/160000 [============================>.] - ETA: 0s - loss: 0.6835 - acc: 0.9275\nEpoch 00044: val_acc did not improve from 0.90990\n160000/160000 [==============================] - 3s 21us/step - loss: 0.6835 - acc: 0.9274 - val_loss: 0.6910 - val_acc: 0.9097\nEpoch 45/50\n158500/160000 [============================>.] - ETA: 0s - loss: 0.6833 - acc: 0.9279\nEpoch 00045: val_acc improved from 0.90990 to 0.91008, saving model to ./weights.keras\n160000/160000 [==============================] - 3s 20us/step - loss: 0.6833 - acc: 0.9278 - val_loss: 0.6911 - val_acc: 0.9101\nEpoch 46/50\n157000/160000 [============================>.] - ETA: 0s - loss: 0.6831 - acc: 0.9283\nEpoch 00046: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 20us/step - loss: 0.6831 - acc: 0.9282 - val_loss: 0.6911 - val_acc: 0.9096\nEpoch 47/50\n158000/160000 [============================>.] - ETA: 0s - loss: 0.6829 - acc: 0.9286\nEpoch 00047: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 20us/step - loss: 0.6829 - acc: 0.9286 - val_loss: 0.6911 - val_acc: 0.9097\nEpoch 48/50\n159500/160000 [============================>.] - ETA: 0s - loss: 0.6828 - acc: 0.9290\nEpoch 00048: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 20us/step - loss: 0.6828 - acc: 0.9289 - val_loss: 0.6911 - val_acc: 0.9094\nEpoch 49/50\n159500/160000 [============================>.] - ETA: 0s - loss: 0.6826 - acc: 0.9292\nEpoch 00049: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 20us/step - loss: 0.6826 - acc: 0.9292 - val_loss: 0.6911 - val_acc: 0.9098\nEpoch 50/50\n157500/160000 [============================>.] - ETA: 0s - loss: 0.6825 - acc: 0.9295\nEpoch 00050: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 20us/step - loss: 0.6825 - acc: 0.9294 - val_loss: 0.6913 - val_acc: 0.9094\n","name":"stdout"},{"output_type":"execute_result","execution_count":28,"data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7f7bd6411358>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier.fit(data_mabs[data_mabs.columns[2:]].values, data_mabs['target'].values, batch_size = 500, epochs = 50, callbacks = [checkpoint], validation_split = 0.2)","execution_count":31,"outputs":[{"output_type":"stream","text":"Train on 160000 samples, validate on 40000 samples\nEpoch 1/50\n158500/160000 [============================>.] - ETA: 0s - loss: 0.6809 - acc: 0.9329\nEpoch 00001: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 4s 22us/step - loss: 0.6809 - acc: 0.9329 - val_loss: 0.6919 - val_acc: 0.9090\nEpoch 2/50\n158000/160000 [============================>.] - ETA: 0s - loss: 0.6809 - acc: 0.9331\nEpoch 00002: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 4s 24us/step - loss: 0.6809 - acc: 0.9329 - val_loss: 0.6918 - val_acc: 0.9091\nEpoch 3/50\n158000/160000 [============================>.] - ETA: 0s - loss: 0.6809 - acc: 0.9329\nEpoch 00003: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 4s 22us/step - loss: 0.6809 - acc: 0.9329 - val_loss: 0.6919 - val_acc: 0.9089\nEpoch 4/50\n158500/160000 [============================>.] - ETA: 0s - loss: 0.6809 - acc: 0.9330\nEpoch 00004: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 22us/step - loss: 0.6809 - acc: 0.9330 - val_loss: 0.6920 - val_acc: 0.9087\nEpoch 5/50\n158500/160000 [============================>.] - ETA: 0s - loss: 0.6808 - acc: 0.9330\nEpoch 00005: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 4s 22us/step - loss: 0.6808 - acc: 0.9330 - val_loss: 0.6921 - val_acc: 0.9091\nEpoch 6/50\n158000/160000 [============================>.] - ETA: 0s - loss: 0.6808 - acc: 0.9330\nEpoch 00006: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 4s 22us/step - loss: 0.6808 - acc: 0.9330 - val_loss: 0.6922 - val_acc: 0.9087\nEpoch 7/50\n158000/160000 [============================>.] - ETA: 0s - loss: 0.6809 - acc: 0.9331\nEpoch 00007: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 4s 22us/step - loss: 0.6808 - acc: 0.9331 - val_loss: 0.6919 - val_acc: 0.9088\nEpoch 8/50\n158000/160000 [============================>.] - ETA: 0s - loss: 0.6808 - acc: 0.9331\nEpoch 00008: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 4s 22us/step - loss: 0.6808 - acc: 0.9331 - val_loss: 0.6920 - val_acc: 0.9089\nEpoch 9/50\n158500/160000 [============================>.] - ETA: 0s - loss: 0.6809 - acc: 0.9329\nEpoch 00009: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 22us/step - loss: 0.6809 - acc: 0.9329 - val_loss: 0.6920 - val_acc: 0.9080\nEpoch 10/50\n158500/160000 [============================>.] - ETA: 0s - loss: 0.6811 - acc: 0.9328\nEpoch 00010: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 22us/step - loss: 0.6811 - acc: 0.9327 - val_loss: 0.6920 - val_acc: 0.9082\nEpoch 11/50\n159500/160000 [============================>.] - ETA: 0s - loss: 0.6808 - acc: 0.9331\nEpoch 00011: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 4s 22us/step - loss: 0.6808 - acc: 0.9331 - val_loss: 0.6923 - val_acc: 0.9083\nEpoch 12/50\n158000/160000 [============================>.] - ETA: 0s - loss: 0.6808 - acc: 0.9331\nEpoch 00012: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 4s 23us/step - loss: 0.6808 - acc: 0.9332 - val_loss: 0.6922 - val_acc: 0.9085\nEpoch 13/50\n158000/160000 [============================>.] - ETA: 0s - loss: 0.6808 - acc: 0.9334\nEpoch 00013: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 4s 23us/step - loss: 0.6808 - acc: 0.9332 - val_loss: 0.6923 - val_acc: 0.9081\nEpoch 14/50\n158000/160000 [============================>.] - ETA: 0s - loss: 0.6807 - acc: 0.9332\nEpoch 00014: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 4s 22us/step - loss: 0.6808 - acc: 0.9333 - val_loss: 0.6922 - val_acc: 0.9084\nEpoch 15/50\n157500/160000 [============================>.] - ETA: 0s - loss: 0.6807 - acc: 0.9332\nEpoch 00015: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 22us/step - loss: 0.6807 - acc: 0.9333 - val_loss: 0.6922 - val_acc: 0.9084\nEpoch 16/50\n159000/160000 [============================>.] - ETA: 0s - loss: 0.6808 - acc: 0.9333\nEpoch 00016: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 22us/step - loss: 0.6807 - acc: 0.9333 - val_loss: 0.6921 - val_acc: 0.9083\nEpoch 17/50\n159000/160000 [============================>.] - ETA: 0s - loss: 0.6807 - acc: 0.9333\nEpoch 00017: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 22us/step - loss: 0.6807 - acc: 0.9333 - val_loss: 0.6922 - val_acc: 0.9082\nEpoch 18/50\n159000/160000 [============================>.] - ETA: 0s - loss: 0.6808 - acc: 0.9333\nEpoch 00018: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 21us/step - loss: 0.6808 - acc: 0.9333 - val_loss: 0.6921 - val_acc: 0.9086\nEpoch 19/50\n159000/160000 [============================>.] - ETA: 0s - loss: 0.6808 - acc: 0.9332\nEpoch 00019: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 22us/step - loss: 0.6808 - acc: 0.9332 - val_loss: 0.6922 - val_acc: 0.9077\nEpoch 20/50\n158000/160000 [============================>.] - ETA: 0s - loss: 0.6808 - acc: 0.9333\nEpoch 00020: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 22us/step - loss: 0.6808 - acc: 0.9333 - val_loss: 0.6921 - val_acc: 0.9085\nEpoch 21/50\n159000/160000 [============================>.] - ETA: 0s - loss: 0.6808 - acc: 0.9334\nEpoch 00021: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 4s 22us/step - loss: 0.6807 - acc: 0.9334 - val_loss: 0.6922 - val_acc: 0.9081\nEpoch 22/50\n157500/160000 [============================>.] - ETA: 0s - loss: 0.6807 - acc: 0.9333\nEpoch 00022: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 22us/step - loss: 0.6807 - acc: 0.9334 - val_loss: 0.6924 - val_acc: 0.9081\nEpoch 23/50\n158000/160000 [============================>.] - ETA: 0s - loss: 0.6807 - acc: 0.9334\nEpoch 00023: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 22us/step - loss: 0.6807 - acc: 0.9334 - val_loss: 0.6922 - val_acc: 0.9083\nEpoch 24/50\n159000/160000 [============================>.] - ETA: 0s - loss: 0.6807 - acc: 0.9335\nEpoch 00024: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 22us/step - loss: 0.6807 - acc: 0.9335 - val_loss: 0.6923 - val_acc: 0.9085\nEpoch 25/50\n158000/160000 [============================>.] - ETA: 0s - loss: 0.6806 - acc: 0.9336\nEpoch 00025: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 4s 23us/step - loss: 0.6807 - acc: 0.9335 - val_loss: 0.6923 - val_acc: 0.9081\nEpoch 26/50\n158000/160000 [============================>.] - ETA: 0s - loss: 0.6807 - acc: 0.9334\nEpoch 00026: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 4s 24us/step - loss: 0.6807 - acc: 0.9335 - val_loss: 0.6922 - val_acc: 0.9083\nEpoch 27/50\n158000/160000 [============================>.] - ETA: 0s - loss: 0.6807 - acc: 0.9336\nEpoch 00027: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 4s 24us/step - loss: 0.6806 - acc: 0.9335 - val_loss: 0.6922 - val_acc: 0.9083\nEpoch 28/50\n158000/160000 [============================>.] - ETA: 0s - loss: 0.6806 - acc: 0.9335\nEpoch 00028: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 4s 24us/step - loss: 0.6806 - acc: 0.9335 - val_loss: 0.6921 - val_acc: 0.9084\nEpoch 29/50\n158500/160000 [============================>.] - ETA: 0s - loss: 0.6807 - acc: 0.9335\nEpoch 00029: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 4s 23us/step - loss: 0.6806 - acc: 0.9335 - val_loss: 0.6924 - val_acc: 0.9080\nEpoch 30/50\n158000/160000 [============================>.] - ETA: 0s - loss: 0.6807 - acc: 0.9335\nEpoch 00030: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 4s 22us/step - loss: 0.6807 - acc: 0.9334 - val_loss: 0.6922 - val_acc: 0.9085\n","name":"stdout"},{"output_type":"stream","text":"Epoch 31/50\n158000/160000 [============================>.] - ETA: 0s - loss: 0.6809 - acc: 0.9331\nEpoch 00031: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 4s 22us/step - loss: 0.6809 - acc: 0.9330 - val_loss: 0.6923 - val_acc: 0.9081\nEpoch 32/50\n158000/160000 [============================>.] - ETA: 0s - loss: 0.6807 - acc: 0.9336\nEpoch 00032: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 4s 22us/step - loss: 0.6807 - acc: 0.9335 - val_loss: 0.6924 - val_acc: 0.9081\nEpoch 33/50\n158000/160000 [============================>.] - ETA: 0s - loss: 0.6806 - acc: 0.9336\nEpoch 00033: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 4s 22us/step - loss: 0.6806 - acc: 0.9336 - val_loss: 0.6923 - val_acc: 0.9084\nEpoch 34/50\n158500/160000 [============================>.] - ETA: 0s - loss: 0.6806 - acc: 0.9336\nEpoch 00034: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 4s 22us/step - loss: 0.6806 - acc: 0.9336 - val_loss: 0.6923 - val_acc: 0.9080\nEpoch 35/50\n159000/160000 [============================>.] - ETA: 0s - loss: 0.6806 - acc: 0.9336\nEpoch 00035: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 4s 22us/step - loss: 0.6806 - acc: 0.9336 - val_loss: 0.6923 - val_acc: 0.9083\nEpoch 36/50\n159000/160000 [============================>.] - ETA: 0s - loss: 0.6806 - acc: 0.9336\nEpoch 00036: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 22us/step - loss: 0.6806 - acc: 0.9337 - val_loss: 0.6924 - val_acc: 0.9079\nEpoch 37/50\n158500/160000 [============================>.] - ETA: 0s - loss: 0.6806 - acc: 0.9336\nEpoch 00037: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 22us/step - loss: 0.6806 - acc: 0.9337 - val_loss: 0.6924 - val_acc: 0.9079\nEpoch 38/50\n158500/160000 [============================>.] - ETA: 0s - loss: 0.6806 - acc: 0.9336\nEpoch 00038: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 4s 22us/step - loss: 0.6806 - acc: 0.9337 - val_loss: 0.6923 - val_acc: 0.9082\nEpoch 39/50\n158000/160000 [============================>.] - ETA: 0s - loss: 0.6806 - acc: 0.9336\nEpoch 00039: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 4s 23us/step - loss: 0.6806 - acc: 0.9337 - val_loss: 0.6923 - val_acc: 0.9086\nEpoch 40/50\n158000/160000 [============================>.] - ETA: 0s - loss: 0.6808 - acc: 0.9333\nEpoch 00040: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 4s 24us/step - loss: 0.6808 - acc: 0.9333 - val_loss: 0.6922 - val_acc: 0.9086\nEpoch 41/50\n158500/160000 [============================>.] - ETA: 0s - loss: 0.6806 - acc: 0.9336\nEpoch 00041: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 4s 23us/step - loss: 0.6807 - acc: 0.9336 - val_loss: 0.6923 - val_acc: 0.9083\nEpoch 42/50\n158000/160000 [============================>.] - ETA: 0s - loss: 0.6806 - acc: 0.9337\nEpoch 00042: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 22us/step - loss: 0.6806 - acc: 0.9338 - val_loss: 0.6923 - val_acc: 0.9084\nEpoch 43/50\n159500/160000 [============================>.] - ETA: 0s - loss: 0.6806 - acc: 0.9337\nEpoch 00043: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 22us/step - loss: 0.6806 - acc: 0.9338 - val_loss: 0.6922 - val_acc: 0.9086\nEpoch 44/50\n158500/160000 [============================>.] - ETA: 0s - loss: 0.6806 - acc: 0.9337\nEpoch 00044: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 4s 22us/step - loss: 0.6805 - acc: 0.9338 - val_loss: 0.6922 - val_acc: 0.9084\nEpoch 45/50\n158500/160000 [============================>.] - ETA: 0s - loss: 0.6806 - acc: 0.9337\nEpoch 00045: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 22us/step - loss: 0.6805 - acc: 0.9338 - val_loss: 0.6923 - val_acc: 0.9084\nEpoch 46/50\n158500/160000 [============================>.] - ETA: 0s - loss: 0.6805 - acc: 0.9339\nEpoch 00046: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 22us/step - loss: 0.6805 - acc: 0.9338 - val_loss: 0.6923 - val_acc: 0.9083\nEpoch 47/50\n158000/160000 [============================>.] - ETA: 0s - loss: 0.6805 - acc: 0.9338\nEpoch 00047: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 4s 22us/step - loss: 0.6805 - acc: 0.9338 - val_loss: 0.6923 - val_acc: 0.9085\nEpoch 48/50\n158500/160000 [============================>.] - ETA: 0s - loss: 0.6805 - acc: 0.9338\nEpoch 00048: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 22us/step - loss: 0.6805 - acc: 0.9338 - val_loss: 0.6923 - val_acc: 0.9085\nEpoch 49/50\n158500/160000 [============================>.] - ETA: 0s - loss: 0.6805 - acc: 0.9340\nEpoch 00049: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 4s 22us/step - loss: 0.6805 - acc: 0.9339 - val_loss: 0.6924 - val_acc: 0.9080\nEpoch 50/50\n158000/160000 [============================>.] - ETA: 0s - loss: 0.6807 - acc: 0.9336\nEpoch 00050: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 4s 24us/step - loss: 0.6807 - acc: 0.9336 - val_loss: 0.6923 - val_acc: 0.9080\n","name":"stdout"},{"output_type":"execute_result","execution_count":31,"data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7f7bd65c4400>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier.fit(data_r[data_r.columns[2:]].values, data_r['target'].values, batch_size = 500, epochs = 50, callbacks = [checkpoint], validation_split = 0.2)","execution_count":32,"outputs":[{"output_type":"stream","text":"Train on 160000 samples, validate on 40000 samples\nEpoch 1/50\n157500/160000 [============================>.] - ETA: 0s - loss: 0.6806 - acc: 0.9338\nEpoch 00001: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 21us/step - loss: 0.6806 - acc: 0.9337 - val_loss: 0.6923 - val_acc: 0.9084\nEpoch 2/50\n158500/160000 [============================>.] - ETA: 0s - loss: 0.6806 - acc: 0.9339\nEpoch 00002: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 21us/step - loss: 0.6805 - acc: 0.9339 - val_loss: 0.6925 - val_acc: 0.9081\nEpoch 3/50\n159500/160000 [============================>.] - ETA: 0s - loss: 0.6805 - acc: 0.9339\nEpoch 00003: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 21us/step - loss: 0.6805 - acc: 0.9339 - val_loss: 0.6925 - val_acc: 0.9082\nEpoch 4/50\n159000/160000 [============================>.] - ETA: 0s - loss: 0.6805 - acc: 0.9339\nEpoch 00004: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 20us/step - loss: 0.6805 - acc: 0.9339 - val_loss: 0.6924 - val_acc: 0.9080\nEpoch 5/50\n158000/160000 [============================>.] - ETA: 0s - loss: 0.6804 - acc: 0.9340\nEpoch 00005: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 20us/step - loss: 0.6805 - acc: 0.9340 - val_loss: 0.6926 - val_acc: 0.9079\nEpoch 6/50\n159000/160000 [============================>.] - ETA: 0s - loss: 0.6805 - acc: 0.9340\nEpoch 00006: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 20us/step - loss: 0.6805 - acc: 0.9340 - val_loss: 0.6926 - val_acc: 0.9081\nEpoch 7/50\n158500/160000 [============================>.] - ETA: 0s - loss: 0.6804 - acc: 0.9340\nEpoch 00007: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 21us/step - loss: 0.6805 - acc: 0.9340 - val_loss: 0.6924 - val_acc: 0.9083\nEpoch 8/50\n158000/160000 [============================>.] - ETA: 0s - loss: 0.6805 - acc: 0.9341\nEpoch 00008: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 20us/step - loss: 0.6805 - acc: 0.9340 - val_loss: 0.6924 - val_acc: 0.9083\nEpoch 9/50\n159500/160000 [============================>.] - ETA: 0s - loss: 0.6804 - acc: 0.9340\nEpoch 00009: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 20us/step - loss: 0.6805 - acc: 0.9340 - val_loss: 0.6924 - val_acc: 0.9082\nEpoch 10/50\n158000/160000 [============================>.] - ETA: 0s - loss: 0.6804 - acc: 0.9340\nEpoch 00010: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 20us/step - loss: 0.6804 - acc: 0.9340 - val_loss: 0.6924 - val_acc: 0.9083\nEpoch 11/50\n159000/160000 [============================>.] - ETA: 0s - loss: 0.6804 - acc: 0.9340\nEpoch 00011: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 20us/step - loss: 0.6804 - acc: 0.9340 - val_loss: 0.6925 - val_acc: 0.9081\nEpoch 12/50\n159500/160000 [============================>.] - ETA: 0s - loss: 0.6806 - acc: 0.9337\nEpoch 00012: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 20us/step - loss: 0.6806 - acc: 0.9338 - val_loss: 0.6926 - val_acc: 0.9080\nEpoch 13/50\n157500/160000 [============================>.] - ETA: 0s - loss: 0.6807 - acc: 0.9338\nEpoch 00013: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 21us/step - loss: 0.6806 - acc: 0.9338 - val_loss: 0.6924 - val_acc: 0.9083\nEpoch 14/50\n158000/160000 [============================>.] - ETA: 0s - loss: 0.6804 - acc: 0.9341\nEpoch 00014: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 21us/step - loss: 0.6805 - acc: 0.9341 - val_loss: 0.6924 - val_acc: 0.9082\nEpoch 15/50\n159000/160000 [============================>.] - ETA: 0s - loss: 0.6804 - acc: 0.9340\nEpoch 00015: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 21us/step - loss: 0.6804 - acc: 0.9341 - val_loss: 0.6924 - val_acc: 0.9080\nEpoch 16/50\n157500/160000 [============================>.] - ETA: 0s - loss: 0.6804 - acc: 0.9342\nEpoch 00016: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 21us/step - loss: 0.6804 - acc: 0.9342 - val_loss: 0.6923 - val_acc: 0.9082\nEpoch 17/50\n159500/160000 [============================>.] - ETA: 0s - loss: 0.6804 - acc: 0.9342\nEpoch 00017: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 21us/step - loss: 0.6804 - acc: 0.9342 - val_loss: 0.6924 - val_acc: 0.9082\nEpoch 18/50\n158000/160000 [============================>.] - ETA: 0s - loss: 0.6804 - acc: 0.9342\nEpoch 00018: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 22us/step - loss: 0.6804 - acc: 0.9342 - val_loss: 0.6924 - val_acc: 0.9085\nEpoch 19/50\n158000/160000 [============================>.] - ETA: 0s - loss: 0.6804 - acc: 0.9342\nEpoch 00019: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 22us/step - loss: 0.6804 - acc: 0.9342 - val_loss: 0.6925 - val_acc: 0.9078\nEpoch 20/50\n159500/160000 [============================>.] - ETA: 0s - loss: 0.6804 - acc: 0.9342\nEpoch 00020: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 22us/step - loss: 0.6804 - acc: 0.9342 - val_loss: 0.6928 - val_acc: 0.9079\nEpoch 21/50\n158000/160000 [============================>.] - ETA: 0s - loss: 0.6805 - acc: 0.9342\nEpoch 00021: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 20us/step - loss: 0.6805 - acc: 0.9341 - val_loss: 0.6923 - val_acc: 0.9082\nEpoch 22/50\n159000/160000 [============================>.] - ETA: 0s - loss: 0.6804 - acc: 0.9341\nEpoch 00022: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 20us/step - loss: 0.6805 - acc: 0.9341 - val_loss: 0.6929 - val_acc: 0.9076\nEpoch 23/50\n159500/160000 [============================>.] - ETA: 0s - loss: 0.6804 - acc: 0.9342\nEpoch 00023: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 21us/step - loss: 0.6804 - acc: 0.9342 - val_loss: 0.6925 - val_acc: 0.9083\nEpoch 24/50\n159500/160000 [============================>.] - ETA: 0s - loss: 0.6804 - acc: 0.9343\nEpoch 00024: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 20us/step - loss: 0.6804 - acc: 0.9343 - val_loss: 0.6923 - val_acc: 0.9081\nEpoch 25/50\n159000/160000 [============================>.] - ETA: 0s - loss: 0.6803 - acc: 0.9342\nEpoch 00025: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 20us/step - loss: 0.6803 - acc: 0.9343 - val_loss: 0.6924 - val_acc: 0.9084\nEpoch 26/50\n159500/160000 [============================>.] - ETA: 0s - loss: 0.6803 - acc: 0.9343\nEpoch 00026: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 20us/step - loss: 0.6803 - acc: 0.9343 - val_loss: 0.6924 - val_acc: 0.9083\nEpoch 27/50\n159500/160000 [============================>.] - ETA: 0s - loss: 0.6803 - acc: 0.9343\nEpoch 00027: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 20us/step - loss: 0.6803 - acc: 0.9343 - val_loss: 0.6924 - val_acc: 0.9083\nEpoch 28/50\n159000/160000 [============================>.] - ETA: 0s - loss: 0.6804 - acc: 0.9343\nEpoch 00028: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 20us/step - loss: 0.6803 - acc: 0.9343 - val_loss: 0.6925 - val_acc: 0.9083\nEpoch 29/50\n159000/160000 [============================>.] - ETA: 0s - loss: 0.6804 - acc: 0.9342\nEpoch 00029: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 20us/step - loss: 0.6804 - acc: 0.9342 - val_loss: 0.6924 - val_acc: 0.9081\nEpoch 30/50\n159000/160000 [============================>.] - ETA: 0s - loss: 0.6804 - acc: 0.9342\nEpoch 00030: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 20us/step - loss: 0.6804 - acc: 0.9342 - val_loss: 0.6928 - val_acc: 0.9075\n","name":"stdout"},{"output_type":"stream","text":"Epoch 31/50\n159000/160000 [============================>.] - ETA: 0s - loss: 0.6804 - acc: 0.9342\nEpoch 00031: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 20us/step - loss: 0.6804 - acc: 0.9343 - val_loss: 0.6925 - val_acc: 0.9079\nEpoch 32/50\n159000/160000 [============================>.] - ETA: 0s - loss: 0.6804 - acc: 0.9344\nEpoch 00032: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 20us/step - loss: 0.6803 - acc: 0.9344 - val_loss: 0.6925 - val_acc: 0.9080\nEpoch 33/50\n157000/160000 [============================>.] - ETA: 0s - loss: 0.6803 - acc: 0.9343\nEpoch 00033: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 20us/step - loss: 0.6803 - acc: 0.9344 - val_loss: 0.6924 - val_acc: 0.9082\nEpoch 34/50\n158500/160000 [============================>.] - ETA: 0s - loss: 0.6803 - acc: 0.9345\nEpoch 00034: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 21us/step - loss: 0.6803 - acc: 0.9344 - val_loss: 0.6924 - val_acc: 0.9083\nEpoch 35/50\n159000/160000 [============================>.] - ETA: 0s - loss: 0.6803 - acc: 0.9344\nEpoch 00035: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 21us/step - loss: 0.6803 - acc: 0.9344 - val_loss: 0.6925 - val_acc: 0.9079\nEpoch 36/50\n158500/160000 [============================>.] - ETA: 0s - loss: 0.6803 - acc: 0.9344\nEpoch 00036: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 21us/step - loss: 0.6803 - acc: 0.9344 - val_loss: 0.6925 - val_acc: 0.9079\nEpoch 37/50\n159500/160000 [============================>.] - ETA: 0s - loss: 0.6803 - acc: 0.9344\nEpoch 00037: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 20us/step - loss: 0.6803 - acc: 0.9344 - val_loss: 0.6925 - val_acc: 0.9081\nEpoch 38/50\n159500/160000 [============================>.] - ETA: 0s - loss: 0.6803 - acc: 0.9345\nEpoch 00038: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 20us/step - loss: 0.6803 - acc: 0.9344 - val_loss: 0.6924 - val_acc: 0.9082\nEpoch 39/50\n158500/160000 [============================>.] - ETA: 0s - loss: 0.6803 - acc: 0.9345\nEpoch 00039: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 20us/step - loss: 0.6803 - acc: 0.9344 - val_loss: 0.6924 - val_acc: 0.9083\nEpoch 40/50\n159500/160000 [============================>.] - ETA: 0s - loss: 0.6803 - acc: 0.9344\nEpoch 00040: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 21us/step - loss: 0.6803 - acc: 0.9344 - val_loss: 0.6923 - val_acc: 0.9083\nEpoch 41/50\n157500/160000 [============================>.] - ETA: 0s - loss: 0.6806 - acc: 0.9338\nEpoch 00041: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 21us/step - loss: 0.6806 - acc: 0.9338 - val_loss: 0.6925 - val_acc: 0.9078\nEpoch 42/50\n158000/160000 [============================>.] - ETA: 0s - loss: 0.6804 - acc: 0.9344\nEpoch 00042: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 20us/step - loss: 0.6804 - acc: 0.9343 - val_loss: 0.6928 - val_acc: 0.9078\nEpoch 43/50\n158500/160000 [============================>.] - ETA: 0s - loss: 0.6803 - acc: 0.9343\nEpoch 00043: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 21us/step - loss: 0.6803 - acc: 0.9344 - val_loss: 0.6926 - val_acc: 0.9078\nEpoch 44/50\n159000/160000 [============================>.] - ETA: 0s - loss: 0.6803 - acc: 0.9347\nEpoch 00044: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 21us/step - loss: 0.6803 - acc: 0.9345 - val_loss: 0.6926 - val_acc: 0.9081\nEpoch 45/50\n159500/160000 [============================>.] - ETA: 0s - loss: 0.6803 - acc: 0.9346\nEpoch 00045: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 21us/step - loss: 0.6803 - acc: 0.9345 - val_loss: 0.6926 - val_acc: 0.9080\nEpoch 46/50\n159500/160000 [============================>.] - ETA: 0s - loss: 0.6803 - acc: 0.9345\nEpoch 00046: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 21us/step - loss: 0.6803 - acc: 0.9345 - val_loss: 0.6927 - val_acc: 0.9079\nEpoch 47/50\n157500/160000 [============================>.] - ETA: 0s - loss: 0.6803 - acc: 0.9345\nEpoch 00047: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 20us/step - loss: 0.6803 - acc: 0.9345 - val_loss: 0.6926 - val_acc: 0.9079\nEpoch 48/50\n158500/160000 [============================>.] - ETA: 0s - loss: 0.6802 - acc: 0.9346\nEpoch 00048: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 20us/step - loss: 0.6802 - acc: 0.9345 - val_loss: 0.6926 - val_acc: 0.9078\nEpoch 49/50\n159000/160000 [============================>.] - ETA: 0s - loss: 0.6803 - acc: 0.9345\nEpoch 00049: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 20us/step - loss: 0.6802 - acc: 0.9346 - val_loss: 0.6927 - val_acc: 0.9075\nEpoch 50/50\n158500/160000 [============================>.] - ETA: 0s - loss: 0.6803 - acc: 0.9346\nEpoch 00050: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 21us/step - loss: 0.6802 - acc: 0.9346 - val_loss: 0.6927 - val_acc: 0.9075\n","name":"stdout"},{"output_type":"execute_result","execution_count":32,"data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7f7bd431e400>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier.fit(data_p[data_p.columns[2:]].values, data_p['target'].values, batch_size = 500, epochs = 50, callbacks = [checkpoint], validation_split = 0.2)","execution_count":34,"outputs":[{"output_type":"stream","text":"Train on 160000 samples, validate on 40000 samples\nEpoch 1/50\n159500/160000 [============================>.] - ETA: 0s - loss: 0.6810 - acc: 0.9329\nEpoch 00001: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 21us/step - loss: 0.6810 - acc: 0.9330 - val_loss: 0.6925 - val_acc: 0.9076\nEpoch 2/50\n159000/160000 [============================>.] - ETA: 0s - loss: 0.6805 - acc: 0.9340\nEpoch 00002: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 4s 22us/step - loss: 0.6805 - acc: 0.9340 - val_loss: 0.6926 - val_acc: 0.9078\nEpoch 3/50\n159000/160000 [============================>.] - ETA: 0s - loss: 0.6804 - acc: 0.9343\nEpoch 00003: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 22us/step - loss: 0.6804 - acc: 0.9342 - val_loss: 0.6932 - val_acc: 0.9068\nEpoch 4/50\n159000/160000 [============================>.] - ETA: 0s - loss: 0.6803 - acc: 0.9345\nEpoch 00004: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 22us/step - loss: 0.6803 - acc: 0.9345 - val_loss: 0.6930 - val_acc: 0.9072\nEpoch 5/50\n158000/160000 [============================>.] - ETA: 0s - loss: 0.6802 - acc: 0.9346\nEpoch 00005: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 21us/step - loss: 0.6803 - acc: 0.9345 - val_loss: 0.6931 - val_acc: 0.9070\nEpoch 6/50\n157000/160000 [============================>.] - ETA: 0s - loss: 0.6802 - acc: 0.9345\nEpoch 00006: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 21us/step - loss: 0.6802 - acc: 0.9346 - val_loss: 0.6931 - val_acc: 0.9071\nEpoch 7/50\n159500/160000 [============================>.] - ETA: 0s - loss: 0.6803 - acc: 0.9345\nEpoch 00007: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 22us/step - loss: 0.6802 - acc: 0.9346 - val_loss: 0.6931 - val_acc: 0.9070\nEpoch 8/50\n159500/160000 [============================>.] - ETA: 0s - loss: 0.6802 - acc: 0.9346\nEpoch 00008: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 22us/step - loss: 0.6802 - acc: 0.9346 - val_loss: 0.6932 - val_acc: 0.9068\nEpoch 9/50\n159500/160000 [============================>.] - ETA: 0s - loss: 0.6802 - acc: 0.9346\nEpoch 00009: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 21us/step - loss: 0.6802 - acc: 0.9346 - val_loss: 0.6930 - val_acc: 0.9073\nEpoch 10/50\n159000/160000 [============================>.] - ETA: 0s - loss: 0.6802 - acc: 0.9347\nEpoch 00010: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 21us/step - loss: 0.6802 - acc: 0.9346 - val_loss: 0.6931 - val_acc: 0.9072\nEpoch 11/50\n157500/160000 [============================>.] - ETA: 0s - loss: 0.6802 - acc: 0.9346\nEpoch 00011: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 22us/step - loss: 0.6802 - acc: 0.9346 - val_loss: 0.6933 - val_acc: 0.9069\nEpoch 12/50\n159500/160000 [============================>.] - ETA: 0s - loss: 0.6804 - acc: 0.9343\nEpoch 00012: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 21us/step - loss: 0.6804 - acc: 0.9344 - val_loss: 0.6929 - val_acc: 0.9074\nEpoch 13/50\n159500/160000 [============================>.] - ETA: 0s - loss: 0.6804 - acc: 0.9343\nEpoch 00013: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 22us/step - loss: 0.6804 - acc: 0.9343 - val_loss: 0.6934 - val_acc: 0.9071\nEpoch 14/50\n159000/160000 [============================>.] - ETA: 0s - loss: 0.6803 - acc: 0.9344\nEpoch 00014: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 22us/step - loss: 0.6803 - acc: 0.9344 - val_loss: 0.6929 - val_acc: 0.9074\nEpoch 15/50\n159000/160000 [============================>.] - ETA: 0s - loss: 0.6803 - acc: 0.9345\nEpoch 00015: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 4s 22us/step - loss: 0.6803 - acc: 0.9345 - val_loss: 0.6929 - val_acc: 0.9075\nEpoch 16/50\n157000/160000 [============================>.] - ETA: 0s - loss: 0.6802 - acc: 0.9347\nEpoch 00016: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 21us/step - loss: 0.6802 - acc: 0.9346 - val_loss: 0.6931 - val_acc: 0.9071\nEpoch 17/50\n159000/160000 [============================>.] - ETA: 0s - loss: 0.6802 - acc: 0.9346\nEpoch 00017: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 21us/step - loss: 0.6802 - acc: 0.9347 - val_loss: 0.6930 - val_acc: 0.9070\nEpoch 18/50\n158500/160000 [============================>.] - ETA: 0s - loss: 0.6802 - acc: 0.9347\nEpoch 00018: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 21us/step - loss: 0.6802 - acc: 0.9347 - val_loss: 0.6930 - val_acc: 0.9071\nEpoch 19/50\n159000/160000 [============================>.] - ETA: 0s - loss: 0.6802 - acc: 0.9347\nEpoch 00019: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 20us/step - loss: 0.6802 - acc: 0.9347 - val_loss: 0.6930 - val_acc: 0.9071\nEpoch 20/50\n159000/160000 [============================>.] - ETA: 0s - loss: 0.6802 - acc: 0.9347\nEpoch 00020: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 4s 23us/step - loss: 0.6802 - acc: 0.9347 - val_loss: 0.6929 - val_acc: 0.9072\nEpoch 21/50\n159500/160000 [============================>.] - ETA: 0s - loss: 0.6802 - acc: 0.9348\nEpoch 00021: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 4s 23us/step - loss: 0.6802 - acc: 0.9347 - val_loss: 0.6930 - val_acc: 0.9072\nEpoch 22/50\n159000/160000 [============================>.] - ETA: 0s - loss: 0.6802 - acc: 0.9346\nEpoch 00022: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 21us/step - loss: 0.6802 - acc: 0.9347 - val_loss: 0.6930 - val_acc: 0.9073\nEpoch 23/50\n158000/160000 [============================>.] - ETA: 0s - loss: 0.6802 - acc: 0.9347\nEpoch 00023: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 21us/step - loss: 0.6802 - acc: 0.9347 - val_loss: 0.6930 - val_acc: 0.9072\nEpoch 24/50\n159500/160000 [============================>.] - ETA: 0s - loss: 0.6802 - acc: 0.9347\nEpoch 00024: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 21us/step - loss: 0.6802 - acc: 0.9347 - val_loss: 0.6930 - val_acc: 0.9072\nEpoch 25/50\n157500/160000 [============================>.] - ETA: 0s - loss: 0.6802 - acc: 0.9346\nEpoch 00025: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 21us/step - loss: 0.6802 - acc: 0.9347 - val_loss: 0.6930 - val_acc: 0.9072\nEpoch 26/50\n158500/160000 [============================>.] - ETA: 0s - loss: 0.6802 - acc: 0.9348\nEpoch 00026: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 4s 22us/step - loss: 0.6802 - acc: 0.9347 - val_loss: 0.6930 - val_acc: 0.9071\nEpoch 27/50\n159500/160000 [============================>.] - ETA: 0s - loss: 0.6802 - acc: 0.9347\nEpoch 00027: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 4s 22us/step - loss: 0.6802 - acc: 0.9347 - val_loss: 0.6931 - val_acc: 0.9072\nEpoch 28/50\n158000/160000 [============================>.] - ETA: 0s - loss: 0.6802 - acc: 0.9346\nEpoch 00028: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 20us/step - loss: 0.6802 - acc: 0.9347 - val_loss: 0.6930 - val_acc: 0.9073\nEpoch 29/50\n157500/160000 [============================>.] - ETA: 0s - loss: 0.6802 - acc: 0.9347\nEpoch 00029: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 4s 22us/step - loss: 0.6802 - acc: 0.9347 - val_loss: 0.6930 - val_acc: 0.9071\nEpoch 30/50\n158500/160000 [============================>.] - ETA: 0s - loss: 0.6802 - acc: 0.9347\nEpoch 00030: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 21us/step - loss: 0.6802 - acc: 0.9347 - val_loss: 0.6930 - val_acc: 0.9071\n","name":"stdout"},{"output_type":"stream","text":"Epoch 31/50\n159500/160000 [============================>.] - ETA: 0s - loss: 0.6802 - acc: 0.9347\nEpoch 00031: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 19us/step - loss: 0.6802 - acc: 0.9347 - val_loss: 0.6930 - val_acc: 0.9071\nEpoch 32/50\n159500/160000 [============================>.] - ETA: 0s - loss: 0.6807 - acc: 0.9336\nEpoch 00032: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 19us/step - loss: 0.6807 - acc: 0.9336 - val_loss: 0.6932 - val_acc: 0.9067\nEpoch 33/50\n157000/160000 [============================>.] - ETA: 0s - loss: 0.6805 - acc: 0.9342\nEpoch 00033: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 19us/step - loss: 0.6805 - acc: 0.9341 - val_loss: 0.6930 - val_acc: 0.9070\nEpoch 34/50\n158500/160000 [============================>.] - ETA: 0s - loss: 0.6803 - acc: 0.9344\nEpoch 00034: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 21us/step - loss: 0.6803 - acc: 0.9344 - val_loss: 0.6932 - val_acc: 0.9070\nEpoch 35/50\n158000/160000 [============================>.] - ETA: 0s - loss: 0.6802 - acc: 0.9347\nEpoch 00035: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 20us/step - loss: 0.6802 - acc: 0.9347 - val_loss: 0.6931 - val_acc: 0.9072\nEpoch 36/50\n159500/160000 [============================>.] - ETA: 0s - loss: 0.6802 - acc: 0.9347\nEpoch 00036: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 19us/step - loss: 0.6802 - acc: 0.9347 - val_loss: 0.6931 - val_acc: 0.9071\nEpoch 37/50\n159500/160000 [============================>.] - ETA: 0s - loss: 0.6802 - acc: 0.9347\nEpoch 00037: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 19us/step - loss: 0.6802 - acc: 0.9347 - val_loss: 0.6931 - val_acc: 0.9070\nEpoch 38/50\n159500/160000 [============================>.] - ETA: 0s - loss: 0.6802 - acc: 0.9347\nEpoch 00038: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 19us/step - loss: 0.6802 - acc: 0.9347 - val_loss: 0.6932 - val_acc: 0.9071\nEpoch 39/50\n159500/160000 [============================>.] - ETA: 0s - loss: 0.6802 - acc: 0.9348\nEpoch 00039: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 4s 22us/step - loss: 0.6802 - acc: 0.9347 - val_loss: 0.6931 - val_acc: 0.9072\nEpoch 40/50\n159500/160000 [============================>.] - ETA: 0s - loss: 0.6802 - acc: 0.9348\nEpoch 00040: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 19us/step - loss: 0.6802 - acc: 0.9347 - val_loss: 0.6932 - val_acc: 0.9068\nEpoch 41/50\n159000/160000 [============================>.] - ETA: 0s - loss: 0.6802 - acc: 0.9348\nEpoch 00041: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 19us/step - loss: 0.6802 - acc: 0.9348 - val_loss: 0.6932 - val_acc: 0.9069\nEpoch 42/50\n159500/160000 [============================>.] - ETA: 0s - loss: 0.6801 - acc: 0.9349\nEpoch 00042: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 18us/step - loss: 0.6802 - acc: 0.9348 - val_loss: 0.6931 - val_acc: 0.9070\nEpoch 43/50\n157500/160000 [============================>.] - ETA: 0s - loss: 0.6801 - acc: 0.9347\nEpoch 00043: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 19us/step - loss: 0.6802 - acc: 0.9348 - val_loss: 0.6930 - val_acc: 0.9072\nEpoch 44/50\n159500/160000 [============================>.] - ETA: 0s - loss: 0.6801 - acc: 0.9348\nEpoch 00044: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 18us/step - loss: 0.6802 - acc: 0.9348 - val_loss: 0.6931 - val_acc: 0.9073\nEpoch 45/50\n157500/160000 [============================>.] - ETA: 0s - loss: 0.6802 - acc: 0.9349\nEpoch 00045: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 18us/step - loss: 0.6802 - acc: 0.9348 - val_loss: 0.6931 - val_acc: 0.9073\nEpoch 46/50\n159500/160000 [============================>.] - ETA: 0s - loss: 0.6802 - acc: 0.9348\nEpoch 00046: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 18us/step - loss: 0.6802 - acc: 0.9348 - val_loss: 0.6930 - val_acc: 0.9073\nEpoch 47/50\n157000/160000 [============================>.] - ETA: 0s - loss: 0.6801 - acc: 0.9349\nEpoch 00047: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 18us/step - loss: 0.6802 - acc: 0.9348 - val_loss: 0.6930 - val_acc: 0.9072\nEpoch 48/50\n159500/160000 [============================>.] - ETA: 0s - loss: 0.6802 - acc: 0.9348\nEpoch 00048: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 19us/step - loss: 0.6802 - acc: 0.9348 - val_loss: 0.6930 - val_acc: 0.9073\nEpoch 49/50\n159500/160000 [============================>.] - ETA: 0s - loss: 0.6802 - acc: 0.9347\nEpoch 00049: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 18us/step - loss: 0.6802 - acc: 0.9348 - val_loss: 0.6930 - val_acc: 0.9073\nEpoch 50/50\n159500/160000 [============================>.] - ETA: 0s - loss: 0.6801 - acc: 0.9348\nEpoch 00050: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 19us/step - loss: 0.6802 - acc: 0.9348 - val_loss: 0.6930 - val_acc: 0.9072\n","name":"stdout"},{"output_type":"execute_result","execution_count":34,"data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7f7bd431e8d0>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier.fit(data_q[data_q.columns[2:]].values, data_q['target'].values, batch_size = 500, epochs = 50, callbacks = [checkpoint], validation_split = 0.2)","execution_count":null,"outputs":[{"output_type":"stream","text":"Train on 160000 samples, validate on 40000 samples\nEpoch 1/50\n159500/160000 [============================>.] - ETA: 0s - loss: 0.6929 - acc: 0.9007\nEpoch 00001: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 4s 22us/step - loss: 0.6929 - acc: 0.9007 - val_loss: 0.6923 - val_acc: 0.9014\nEpoch 2/50\n158000/160000 [============================>.] - ETA: 0s - loss: 0.6908 - acc: 0.9081\nEpoch 00002: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 4s 23us/step - loss: 0.6908 - acc: 0.9082 - val_loss: 0.6917 - val_acc: 0.9071\nEpoch 3/50\n158000/160000 [============================>.] - ETA: 0s - loss: 0.6885 - acc: 0.9146\nEpoch 00003: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 4s 23us/step - loss: 0.6885 - acc: 0.9146 - val_loss: 0.6916 - val_acc: 0.9055\nEpoch 4/50\n158000/160000 [============================>.] - ETA: 0s - loss: 0.6869 - acc: 0.9189\nEpoch 00004: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 4s 23us/step - loss: 0.6869 - acc: 0.9188 - val_loss: 0.6919 - val_acc: 0.9056\nEpoch 5/50\n158500/160000 [============================>.] - ETA: 0s - loss: 0.6861 - acc: 0.9206\nEpoch 00005: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 4s 22us/step - loss: 0.6861 - acc: 0.9207 - val_loss: 0.6932 - val_acc: 0.9065\nEpoch 6/50\n158500/160000 [============================>.] - ETA: 0s - loss: 0.6855 - acc: 0.9225\nEpoch 00006: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 4s 22us/step - loss: 0.6855 - acc: 0.9225 - val_loss: 0.6929 - val_acc: 0.9061\nEpoch 7/50\n159000/160000 [============================>.] - ETA: 0s - loss: 0.6847 - acc: 0.9245\nEpoch 00007: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 22us/step - loss: 0.6847 - acc: 0.9245 - val_loss: 0.6928 - val_acc: 0.9056\nEpoch 8/50\n159000/160000 [============================>.] - ETA: 0s - loss: 0.6844 - acc: 0.9253\nEpoch 00008: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 22us/step - loss: 0.6844 - acc: 0.9253 - val_loss: 0.6928 - val_acc: 0.9051\nEpoch 9/50\n157500/160000 [============================>.] - ETA: 0s - loss: 0.6840 - acc: 0.9266\nEpoch 00009: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 21us/step - loss: 0.6840 - acc: 0.9264 - val_loss: 0.6931 - val_acc: 0.9052\nEpoch 10/50\n158000/160000 [============================>.] - ETA: 0s - loss: 0.6837 - acc: 0.9270\nEpoch 00010: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 20us/step - loss: 0.6837 - acc: 0.9270 - val_loss: 0.6937 - val_acc: 0.9050\nEpoch 11/50\n159500/160000 [============================>.] - ETA: 0s - loss: 0.6833 - acc: 0.9281\nEpoch 00011: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 20us/step - loss: 0.6834 - acc: 0.9281 - val_loss: 0.6926 - val_acc: 0.9050\nEpoch 12/50\n158000/160000 [============================>.] - ETA: 0s - loss: 0.6831 - acc: 0.9289\nEpoch 00012: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 22us/step - loss: 0.6831 - acc: 0.9288 - val_loss: 0.6940 - val_acc: 0.9049\nEpoch 13/50\n157500/160000 [============================>.] - ETA: 0s - loss: 0.6829 - acc: 0.9292\nEpoch 00013: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 4s 22us/step - loss: 0.6829 - acc: 0.9291 - val_loss: 0.6934 - val_acc: 0.9048\nEpoch 14/50\n158000/160000 [============================>.] - ETA: 0s - loss: 0.6829 - acc: 0.9293\nEpoch 00014: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 4s 22us/step - loss: 0.6828 - acc: 0.9294 - val_loss: 0.6937 - val_acc: 0.9049\nEpoch 15/50\n158500/160000 [============================>.] - ETA: 0s - loss: 0.6828 - acc: 0.9295\nEpoch 00015: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 4s 22us/step - loss: 0.6828 - acc: 0.9296 - val_loss: 0.6940 - val_acc: 0.9046\nEpoch 16/50\n159000/160000 [============================>.] - ETA: 0s - loss: 0.6823 - acc: 0.9307\nEpoch 00016: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 3s 21us/step - loss: 0.6823 - acc: 0.9307 - val_loss: 0.6936 - val_acc: 0.9047\nEpoch 17/50\n158500/160000 [============================>.] - ETA: 0s - loss: 0.6822 - acc: 0.9309\nEpoch 00017: val_acc did not improve from 0.91008\n160000/160000 [==============================] - 4s 22us/step - loss: 0.6822 - acc: 0.9309 - val_loss: 0.6947 - val_acc: 0.9044\nEpoch 18/50\n111000/160000 [===================>..........] - ETA: 1s - loss: 0.6821 - acc: 0.9310","name":"stdout"}]},{"metadata":{"trusted":true,"_uuid":"46e5962ab16842e5ae752208c6fd9df2632f161c"},"cell_type":"code","source":"# checking if the weights were saved\n\nimport os\nprint(os.listdir('./'))","execution_count":29,"outputs":[{"output_type":"stream","text":"['weights.keras', '__notebook_source__.ipynb', '.ipynb_checkpoints']\n","name":"stdout"}]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}