{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Content\n#### 1)Libraries\n#### 2)Data extraction\n#### 3)Data exploration\n#### 4)Unbalanced Data and Resampling\n#### 5)Feature selection\n#### 6) Classification models\n#### 7)Detection of the most influential variables"},{"metadata":{},"cell_type":"markdown","source":"### Loading the libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.utils import resample\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix,recall_score,roc_auc_score,roc_curve,precision_score,f1_score,auc,accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Extraction"},{"metadata":{"trusted":true},"cell_type":"code","source":"customer_data = pd.read_csv('../input/santander-customer-transaction-prediction/train.csv')\ntest_data = pd.read_csv('../input/santander-customer-transaction-prediction/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(customer_data.shape)\nprint(test_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"customer_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Exploration"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is an anonymised dataset with 199 discrete numeric variables, with a dependent variable labeled as a binary variable and a column in string format with an identifier label. Two training datasets are provided, a training dataset and evaluation dataset, but no target variable so that for our purpose we won't use it to train the models. The task that is requested in this challenge is to predict the value of the target column in the test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"customer_data['target'].value_counts().plot.bar()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We have imbalance data."},{"metadata":{},"cell_type":"markdown","source":"### Missing Value Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(customer_data.isnull().sum().any())\nprint(test_data.isnull().sum().any())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is no missing value present"},{"metadata":{},"cell_type":"markdown","source":"## Checking the distribution\nGet an idea of this data distribution, we review in the training dataset that we will work with, we review the histogram of the mean values of each record based on the binary target variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ncolumns = list(customer_data.columns)\ncolumns.remove('target')\ncolumns.remove('ID_code')\n\ntarget0_data = customer_data[customer_data['target']==0]\ntarget1_data = customer_data[customer_data['target']==1]\nplt.figure(figsize=(14,8))\nplt.title(\"Distribution of mean row data based on target \")\nsns.distplot(target0_data[columns].mean(axis=1),color='blue',kde=True,bins=100,label='target_0')\nsns.distplot(target1_data[columns].mean(axis=1),color='red',kde=True,bins=100,label='target_1')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see that there is a small variation in the mean of all feature that could explain the target variable."},{"metadata":{},"cell_type":"markdown","source":"####  We will look for correlation variables to decrease high dimensionality. We have tried to numerically show it as visually the plot would be too large. "},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix = customer_data[columns].corr()\nsns.heatmap(corr_matrix)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We try to detect potential correlated variables to decrease high dimensionality. As the correlation matrix is too large visually as seen above, we tried to numerically detect the existence of correlations above 0.5 and below -0.5."},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = customer_data.corr()\nhigh_corr = np.where(corr>0.5)\nhigh_corr = [(corr(x),corr(y)) for x,y in zip(*high_corr) if x!=y and x<y]\nif len(high_corr)==0:\n    print(\"There are no correlated variables\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## PCA"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\nratio={}\nfor i in range(190,200):\n    pca=PCA(n_components=i).fit(customer_data[columns])\n    ratio[i]=sum(pca.explained_variance_ratio_)\n    \npd.Series(ratio).plot()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### It is observed that we require whole 200 features to explain the variance. So we will not be applying PCA here."},{"metadata":{},"cell_type":"markdown","source":"## Outlier"},{"metadata":{"trusted":true},"cell_type":"code","source":"def boxplot_func(data_frame,col):\n    sns.set(style=\"whitegrid\")\n    plt.title(\"Outliers\")\n    fig, ax = plt.subplots(10,10,figsize=(18,24))\n    counter=0\n    for c in col:\n        counter+=1\n        plt.subplot(10,10,counter)\n        sns.boxplot(data_frame[c])\n        plt.xlabel(c)\n        plt.tick_params(axis='x', labelsize=7, pad= -7)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col = customer_data.columns.values[2:102]\nboxplot_func(customer_data,col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col=customer_data.columns.values[102:]\nboxplot_func(customer_data,col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"copy_train_data = customer_data.copy()\ncopy_test_data = test_data.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def outlier_removal(df):\n    for i in columns:\n        q75, q25 =np.percentile(df.loc[:,i],[75,25])\n        iqr  = q75-q25\n        min  = q25 - (iqr*1.5)\n        max  = q75 + (iqr*1.5)\n        df = df.drop(df[df.loc[:,i]<min].index) \n        df = df.drop(df[df.loc[:,i]>max].index)\n        return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"customer_data = outlier_removal(customer_data)\n#test_data = outlier_removal(test_data) ## We don't need to remove outliers from test data\nprint(\"Total number of observations dropped in train set:\",copy_train_data.shape[0]-customer_data.shape[0])\ncustomer_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"customer_data['target'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Resampling\nNote we are dealing with a data set very unbalanced, where there is only 10% of records categorized with target 1, so those customers who have made a financial transaction. So we will try sampling the data"},{"metadata":{},"cell_type":"markdown","source":"## 1. Under Sampling"},{"metadata":{"trusted":true},"cell_type":"code","source":"class_0,class_1 = customer_data.target.value_counts()\n\ndf_class_0 = customer_data[customer_data['target']==0]\ndf_class_1 = customer_data[customer_data['target']==1]\n\nunder_df_0 = df_class_0.sample(class_1)\ndf_train_under = pd.concat([under_df_0,df_class_1],axis=0)\n\nprint(df_train_under.target.value_counts())\ndf_train_under.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Oversampling"},{"metadata":{"trusted":true},"cell_type":"code","source":"over_df = resample(df_class_1, replace=True, n_samples=179813,random_state=123)\n\ndf_train_over = pd.concat([over_df,df_class_0],axis=0)\n\nlen(df_train_over)\nprint(df_train_over.target.value_counts())\ndf_train_over.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\nX_train_under = df_train_under[columns]\ny_train_under = df_train_under['target']\n\nX_train_over = df_train_over[columns]\ny_train_over = df_train_over['target']\nprint(X_train_under.shape)\ntrain_x,test_x,train_y,test_y = train_test_split(X_train_under,y_train_under,train_size=0.8,random_state=42,stratify=y_train_under)\ntrain_y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_y.value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Classification Models"},{"metadata":{"trusted":true},"cell_type":"code","source":"def metrics(y_true,y_pred):\n    print(\"Confusion Matrix\")\n    print(confusion_matrix(y_true,y_pred))\n    \n    print(\"Accuracy:\", accuracy_score(y_true,y_pred))\n    print(\"Precision:\", precision_score(y_true,y_pred))\n    print(\"F1 Score:\", f1_score(y_true,y_pred))\n    print(\"Recall:\", recall_score(y_true,y_pred))\n    \n    false_positive_rate,recall,thresholds = roc_curve(y_true,y_pred)\n    roc_auc = auc(false_positive_rate,recall)\n    \n    print(\"ROC:\",roc_auc)\n    \n    plt.plot(false_positive_rate,recall,'b')\n    plt.plot([0,1],[0,1],'r--')\n    plt.title(\"AUC=%0.2f\"%roc_auc)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Testing with Undersampling Data\n\n### 1) Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"logistic_model = LogisticRegression().fit(train_x,train_y)\nlogistic_predict = logistic_model.predict(train_x)\n\nprint(\"Metrics:\")\nmetrics(train_y,logistic_predict)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"logistic_predict_test = logistic_model.predict(test_x)\nprint(\"Metrics for test:\")\nmetrics(test_y,logistic_predict_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2) Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"tree = RandomForestClassifier(n_estimators=10,max_depth=7,random_state=1).fit(train_x,train_y)\ntree_train_predict = tree.predict(train_x)\n    \nprint(\"Metrics:\")\nmetrics(train_y,tree_train_predict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tree_test_predict = tree.predict(test_x)\nprint(\"Metrics:\")\nmetrics(test_y,tree_test_predict)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3) Naive Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"naive = GaussianNB().fit(train_x,train_y)\nnaive_train_predict = naive.predict(train_x)\nprint(\"Metrics:\")\nmetrics(train_y,naive_train_predict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"naive_test_predict = naive.predict(test_x)\nprint(\"Metrics:\")\nmetrics(test_y,naive_test_predict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = []\nmodels.append((\"LogisticRegression\",LogisticRegression()))\nmodels.append((\"Random Forest\",RandomForestClassifier(n_estimators=10,max_depth=7,random_state=1)))\nmodels.append((\"NaiveBayes\",GaussianNB()))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_test(x_data,y_data):\n    for name,model in models:\n        train_x,test_x,train_y,test_y = train_test_split(x_data,y_data,train_size=0.75,random_state=42,stratify=y_data)\n        print(\"#\"*10,\"Validation for %s \"%name,\"#\"*10)\n        model.fit(train_x,train_y)\n        metrics(train_y,model.predict(train_x))\n        pred = model.predict(test_x)\n        print(\"Testing Metrics of %s\"%name)\n        metrics(test_y,pred)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Oversampling Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_test(X_train_over,y_train_over)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.inspection import permutation_importance\n\nimps = permutation_importance(naive, test_x, test_y)\nimportances = imps.importances_mean\nstd = imps.importances_std\nindices = np.argsort(importances)[::-1]\n\n# Print the feature ranking\nprint(\"Feature ranking:\")\nfor f in range(test_x.shape[1]):\n    print(\"%d. (%f)\" % (f + 1, importances[indices[f]]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature = pd.DataFrame({\"imp\":importances,\"col\":columns})\nfeature = feature.sort_values(['imp','col'],ascending=[True,False]).iloc[-30:]\nfeature.plot(kind='barh',x='col',y='imp',figsize=(10,7),legend=None)\nplt.title(\"Feature Importances\")\nplt.ylabel(\"Features\")\nplt.xlabel(\"Importances\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.drop(['ID_code'],axis=1,inplace=True)\n\npredict = naive.predict(test_data)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.Series(predict).value_counts().plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission = pd.read_csv('../input/santander-customer-transaction-prediction/sample_submission.csv')\nsample_submission['target'] = predict\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(sample_submission.head())\nsample_submission.to_csv('submission_1.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}