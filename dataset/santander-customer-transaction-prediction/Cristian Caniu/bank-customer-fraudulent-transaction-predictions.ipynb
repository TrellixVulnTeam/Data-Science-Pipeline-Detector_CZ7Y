{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fraudulent Transaction Predictions\n\nTest models to predict bank customer fraudulent transactions. This is a classification task typically for imbalanced data where the class of interest is in less proportion.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport time\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif, chi2\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\n\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2021-10-01T17:49:05.102185Z","iopub.execute_input":"2021-10-01T17:49:05.102868Z","iopub.status.idle":"2021-10-01T17:49:05.115695Z","shell.execute_reply.started":"2021-10-01T17:49:05.102768Z","shell.execute_reply":"2021-10-01T17:49:05.114906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Import data","metadata":{}},{"cell_type":"code","source":"# Load data\ntrain_df = pd.read_csv('../input/santander-customer-transaction-prediction/train.csv')\n\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-01T17:49:05.393197Z","iopub.execute_input":"2021-10-01T17:49:05.393811Z","iopub.status.idle":"2021-10-01T17:49:11.577841Z","shell.execute_reply.started":"2021-10-01T17:49:05.393774Z","shell.execute_reply":"2021-10-01T17:49:11.577032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The dataframe cosists of 200 features and 1 target. The predictor features are unnamed and the target variable has 2 possible values, 0 or 1. Typically, data for fraudulent transaction predictions is higly unbalanced, with a sizable difference in the number of samples per category. Let us see this in the code below.\n\n### Sample population\n\nLet us see the number of sample per class in the train dataset","metadata":{}},{"cell_type":"code","source":"def sample_population(y):\n    # Unique class values and counts (already sorted)\n    mics, counts = np.unique(y, return_counts=True)\n    # Samples per class\n    samples = {mic: count for mic, count in zip(mics, counts)}\n    \n    return samples\n\nsample_population(train_df.target)","metadata":{"execution":{"iopub.status.busy":"2021-10-01T17:49:11.579462Z","iopub.execute_input":"2021-10-01T17:49:11.580277Z","iopub.status.idle":"2021-10-01T17:49:11.591554Z","shell.execute_reply.started":"2021-10-01T17:49:11.580238Z","shell.execute_reply":"2021-10-01T17:49:11.59084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In categorical problems, the imbalance typically leads to an overffiting and a wrong interpretation of the accuracy metric. In these cases, the accuracy seems almost reach 100%, however, this accuracy is just for the most represented category leaving the less represented category unlearned. There is more than one solution to overcome this sitiation, for instance, by introducing class weights or by doing a virtual oversampling of the less represented category. Here, we just inform to models to work with balanced class weights.\n\n### $K$-best features \n\nDistributions ans histplots show how the data is distribuited in the space of input variables. For binary classification, thera are two distributions per input variable, this can be seen in one dimesion or two dimensions. However, the features selection made by the SelectKBest() function from sci-kit learn is an univariate linear regression test that analyzes the variables one at the time. \n\nThe method uses the f_classif() function to compute the ANOVA $F$-value for the provided samples. It represents a level of distance between samples of different classes divided by the sum of compactness of each class. For two variables, $x$ and $y$, the formula for the $F$-value is as follows:\n\n$F = (n(\\bar{x}_0-\\bar{x})^2+m(\\bar{x}_1-\\bar{x})^2)/ \\frac{1}{(n-1)+(m-1)}\\sum_{i,j}^{n,m}((x_i-\\bar{x}_0)^2 + (x_j-\\bar{x}_1)^2)$.\n\nThe formula includes the factors $(n-1)$ and $(m-1)$ for the level of compactness that take into account the correction for the true parameter.\n\nLet us include the correlation formula for comparison.\n\nr = $\\sum_{i,j}(x_i-\\bar{x}))(y_j-\\bar{y})/\\sigma_i \\sigma_j$.\n\nThe search for the best predictors must be donde leaving out the test dataframe for validation purposes.","metadata":{}},{"cell_type":"code","source":"# Define the train and test dataframes\ntrain_df, test_df = train_test_split(train_df, test_size=.25, random_state=1)\n\nprint(train_df.shape)\nprint(test_df.shape)","metadata":{"execution":{"iopub.status.busy":"2021-10-01T17:49:11.592712Z","iopub.execute_input":"2021-10-01T17:49:11.593301Z","iopub.status.idle":"2021-10-01T17:49:12.085187Z","shell.execute_reply.started":"2021-10-01T17:49:11.593254Z","shell.execute_reply":"2021-10-01T17:49:12.0843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function to plot n hisplots\ndef hist_plots(X, y, n_features):\n    \n    def single_plot(data_0, data_1, x_labels, ylabel):\n        xlabel = x_labels[0]\n        fig, axes = plt.subplots(figsize=(6, 4))\n        axes.hist(data_0.iloc[:,0], bins=10, density=True, histtype='step')\n        axes.hist(data_1.iloc[:,0], bins=10, density=True, histtype='step')\n        axes.set(xlabel=xlabel, ylabel=ylabel)\n        axes.legend(labels=[0,1])\n        plt.tight_layout()\n        plt.show()\n    \n    def double_plot(data_0, data_1, x_labels, ylabel):\n        fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n        for i, xlabel in enumerate(x_labels):\n            axes[i].hist(data_0.iloc[:,i], bins=10, density=True, histtype='step')\n            axes[i].hist(data_1.iloc[:,i], bins=10, density=True, histtype='step')\n            axes[i].set(xlabel=xlabel, ylabel=ylabel)\n            axes[i].legend(labels=[0,1])\n        plt.tight_layout()\n        plt.show()\n    \n    def multi_plot(data_0, data_1, x_labels, ylabel):\n        n = len(x_labels)//2\n        fig, axes = plt.subplots(n, 2, figsize=(12, 4*n))\n        count=0\n        for i in range(n):\n            for j in range(2):\n                xlabel = x_labels[count]\n                axes[i][j].hist(data_0.iloc[:,count], bins=10, density=True, histtype='step')\n                axes[i][j].hist(data_1.iloc[:,count], bins=10, density=True, histtype='step')\n                axes[i][j].set(xlabel=xlabel, ylabel=ylabel)\n                axes[i][j].legend(labels=[0,1])\n                count += 1\n        plt.tight_layout()\n        plt.show()\n    \n    data_0 = X[y==0]\n    data_1 = X[y==1]\n\n    ylabel = y.name\n    x_labels = X.columns[:n_features]\n    \n    if len(x_labels)==1:\n        single_plot(data_0, data_1, x_labels, ylabel)\n        \n    elif len(x_labels)==2:\n        double_plot(data_0, data_1, x_labels, ylabel)\n        \n    elif len(x_labels)==3:\n        double_plot(data_0, data_1, x_labels[:2], ylabel)\n        single_plot(data_0, data_1, x_labels[-1], ylabel)\n        \n    else:\n        multi_plot(data_0, data_1, x_labels, ylabel)\n        if len(x_labels)%2!=0:\n            single_plot(data_0, data_1, x_labels[-1], ylabel)","metadata":{"execution":{"iopub.status.busy":"2021-10-01T17:49:12.086763Z","iopub.execute_input":"2021-10-01T17:49:12.087069Z","iopub.status.idle":"2021-10-01T17:49:12.100575Z","shell.execute_reply.started":"2021-10-01T17:49:12.087041Z","shell.execute_reply":"2021-10-01T17:49:12.099898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the input and target variables\nX_train, y_train = train_df.iloc[:,2:], train_df.target\n# plot 2 hisplots to see their distributions\nhist_plots(X_train, y_train, 2)","metadata":{"execution":{"iopub.status.busy":"2021-10-01T17:49:12.10199Z","iopub.execute_input":"2021-10-01T17:49:12.10226Z","iopub.status.idle":"2021-10-01T17:49:12.695545Z","shell.execute_reply.started":"2021-10-01T17:49:12.102227Z","shell.execute_reply":"2021-10-01T17:49:12.694907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The histplots of the first two predictors look much more the same.\n\nNow, select the $k$ best predictors and plot four histograms.","metadata":{}},{"cell_type":"code","source":"def k_best_transform(X_train, y_train, X_test, y_test, k):\n    X_columns = X_train.columns\n    k_best = SelectKBest(f_classif, k=k)\n    X_train = k_best.fit_transform(X_train, y_train)\n    indices = k_best.get_support()\n    scores = k_best.scores_\n    sel_scores = scores[indices==True]\n    sel_columns = X_columns[indices==True]\n    X_train = pd.DataFrame(X_train, columns=sel_columns)\n    X_test = k_best.transform(X_test)\n    X_test = pd.DataFrame(X_test, columns=sel_columns)\n    y_train = y_train.reset_index(drop=True)\n    y_test = y_test.reset_index(drop=True)\n\n    return X_train, y_train, X_test, y_test\n\n# Define features and target for test data\nX_test, y_test = test_df.iloc[:,2:], test_df.target\n\n# Transform the train and test datasets based on the train dataset\nX_train, y_train, X_test, y_test = k_best_transform(X_train, y_train, X_test, y_test, 100)","metadata":{"execution":{"iopub.status.busy":"2021-10-01T17:49:12.696567Z","iopub.execute_input":"2021-10-01T17:49:12.696792Z","iopub.status.idle":"2021-10-01T17:49:13.591139Z","shell.execute_reply.started":"2021-10-01T17:49:12.696768Z","shell.execute_reply":"2021-10-01T17:49:13.590376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hist_plots(X_train, y_train, 4)","metadata":{"execution":{"iopub.status.busy":"2021-10-01T17:49:13.592461Z","iopub.execute_input":"2021-10-01T17:49:13.592782Z","iopub.status.idle":"2021-10-01T17:49:14.274563Z","shell.execute_reply.started":"2021-10-01T17:49:13.592746Z","shell.execute_reply":"2021-10-01T17:49:14.273669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After leaving out 100 predictors the variables var3 and var4 are out of the sample.","metadata":{}},{"cell_type":"markdown","source":"## Model assesment and model selection\n\nFirst, define a series of models.","metadata":{}},{"cell_type":"code","source":"# specify models\nmodels = {'DTC': DecisionTreeClassifier(criterion='entropy', splitter='random', max_depth=5, class_weight='balanced'),\n          'RFC': RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1, class_weight='balanced'),\n          'PCN': SGDClassifier(loss='perceptron', eta0=1.0, learning_rate=\"constant\", penalty=None, class_weight='balanced', max_iter=1000, tol=0.001, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5),\n          'SVC': SGDClassifier(loss='hinge', class_weight='balanced', max_iter=1000, tol=0.001, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5),\n          'LRC': LogisticRegression(class_weight='balanced'),\n          }","metadata":{"execution":{"iopub.status.busy":"2021-10-01T17:49:14.277429Z","iopub.execute_input":"2021-10-01T17:49:14.278073Z","iopub.status.idle":"2021-10-01T17:49:14.284493Z","shell.execute_reply.started":"2021-10-01T17:49:14.278033Z","shell.execute_reply":"2021-10-01T17:49:14.283795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then, compute the recall of model predictions","metadata":{}},{"cell_type":"code","source":"# Model assesment\ndef model_assesment(X_train, y_train, X_test, y_test):\n    # Define an empty list to save the test metric of different models\n    score_t = []\n    trainning_time = []\n    for model in models.values():\n        # Always scale the input. The most convenient way is to use a pipeline.\n        model = make_pipeline(StandardScaler(), model)\n        # start time\n        starttime = time.time()\n        # fit the model\n        model.fit(X_train, y_train)\n        # end time\n        trainning_time.append(time.time() - starttime)\n        # make predictions and compute the root mean squared error\n        predictions = model.predict(X_test)\n        # compute the score over an independent test sample\n        score_t.append(recall_score(y_test, predictions))\n    return score_t, trainning_time\n\nrecall_t, trainning_time = model_assesment(X_train, y_train, X_test, y_test)\n\nfor i, model in enumerate(models.keys()):\n    print('%s = %.2f,  Time = %.2f' % (model, recall_t[i], trainning_time[i]))","metadata":{"execution":{"iopub.status.busy":"2021-10-01T17:49:14.285598Z","iopub.execute_input":"2021-10-01T17:49:14.285829Z","iopub.status.idle":"2021-10-01T17:49:24.729916Z","shell.execute_reply.started":"2021-10-01T17:49:14.285805Z","shell.execute_reply":"2021-10-01T17:49:24.729104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Results\n\n### $k$-parameter\n\nLet us evaluate the model's quality for different numbers of model predictors and plot the recall for all models and $k$-values.","metadata":{}},{"cell_type":"code","source":"def k_features_assesment(X_train, y_train, X_test, y_test, k):\n    X_train, y_train, X_test, y_test = k_best_transform(X_train, y_train, X_test, y_test, k)\n    score_t, _ =  model_assesment(X_train, y_train, X_test, y_test)\n    return score_t\n\ndef k_scores_plot(data, grid, labels):\n    data = np.array(data).transpose()\n    fig, ax = plt.subplots(figsize=(6,4))\n    for i, label in enumerate(labels):\n        ax.scatter(grid, data[i],  marker='^', label=label)\n    ax.set_title('')\n    ax.set_xlabel('k')\n    ax.set_ylabel('Recall')\n    plt.legend()\n    plt.show()\n    \nk_grid = [5, 10, 20, 30, 40, 80, 120, 160, 200]\nrecall_k = []\nfor k in k_grid:\n    X_train, y_train = train_df.iloc[:,2:], train_df.target\n    X_test, y_test = test_df.iloc[:,2:], test_df.target\n    recall_k.append(k_features_assesment(X_train, y_train, X_test, y_test, k))\n    \nk_scores_plot(recall_k, k_grid, models.keys())","metadata":{"execution":{"iopub.status.busy":"2021-10-01T17:49:24.732389Z","iopub.execute_input":"2021-10-01T17:49:24.733208Z","iopub.status.idle":"2021-10-01T17:50:52.471875Z","shell.execute_reply.started":"2021-10-01T17:49:24.73317Z","shell.execute_reply":"2021-10-01T17:50:52.471097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Cross-validation\n\nHere, we validate the model using cross-validation.","metadata":{}},{"cell_type":"code","source":"def cv_testing_models(X, y, models, scoring, n_splits):\n    # define model evaluation method (n_splits = 1/test_size)\n    cv = RepeatedKFold(n_splits=n_splits, n_repeats=1, random_state=0)\n    # evaluate the models\n    metric_per_model = []\n    for model in models.values():\n        model = make_pipeline(StandardScaler(), model)\n        metric = cross_val_score(model, X, y, scoring=scoring, cv=cv, n_jobs=-1)\n        metric = np.abs(metric)\n        metric_per_model.append(metric)\n    return metric_per_model\n\n# load the whole data again\ntrain_df = pd.read_csv('../input/santander-customer-transaction-prediction/train.csv')\nX_train, y_train = train_df.iloc[:,2:], train_df.target\n\nstarttime = time.time()\nrecall_cv = cv_testing_models(X_train, y_train, models, scoring='recall', n_splits=4)\nprint('Time: {:0.2f} seconds'.format(time.time() - starttime))","metadata":{"execution":{"iopub.status.busy":"2021-10-01T17:50:52.473009Z","iopub.execute_input":"2021-10-01T17:50:52.473225Z","iopub.status.idle":"2021-10-01T17:51:38.054183Z","shell.execute_reply.started":"2021-10-01T17:50:52.473199Z","shell.execute_reply":"2021-10-01T17:51:38.053133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Test vs cross-validation recall\n\nLet us show the recall obtained from cross-validation for test assessment considering the 200 features in a box plot.","metadata":{}},{"cell_type":"code","source":"# define a custom box whisker plot\ndef box_whisker_plot(data, labels, metric_t):\n    mean = np.mean(data, axis=1)\n    y = np.array(range(1,len(mean)+1))\n    fig, ax = plt.subplots(figsize=(6,4))\n    ax.set_title('')\n    ax.set_xlabel('Recall')\n    ax.set_ylabel('Model')\n    ax.boxplot(data, labels=labels, vert=False, whis=(0,100))\n    ax.scatter(metric_t, y,  marker='^', label='Test recall')\n    ax.scatter(mean, y,  marker='^', label='C-V recall')   \n    plt.legend()\n    plt.show()\n\nbox_whisker_plot(recall_cv, models.keys(), recall_k[-1])","metadata":{"execution":{"iopub.status.busy":"2021-10-01T17:51:38.055751Z","iopub.execute_input":"2021-10-01T17:51:38.055989Z","iopub.status.idle":"2021-10-01T17:51:38.308176Z","shell.execute_reply.started":"2021-10-01T17:51:38.05596Z","shell.execute_reply":"2021-10-01T17:51:38.307363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As final remarks:\n\n- Cross-validation estimates are in agreement with the test recall.\n- The Logistic Regression Classifier performs better in both validation schemes","metadata":{}}]}