{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom catboost import CatBoostClassifier,Pool\nfrom IPython.display import display\nimport matplotlib.patches as patch\nimport matplotlib.pyplot as plt\nfrom sklearn.svm import NuSVR\nfrom scipy.stats import norm\nfrom sklearn import svm\nimport lightgbm as lgb\nimport xgboost as xgb\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport warnings\nimport time\nimport glob\nimport sys\nimport os\nimport gc\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.mixture import GaussianMixture\nfrom hyperopt import hp, tpe","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Setup"},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\n%precision 4\nwarnings.filterwarnings('ignore')\nplt.style.use('ggplot')\nnp.set_printoptions(suppress=True)\npd.set_option(\"display.precision\", 15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score, roc_curve","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import Dataset to play with it\ntrain_df = pd.read_csv(\"../input/train.csv\")\ntest_df = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data = pd.concat([train_df, test_df], axis=0, sort=False)\nscaler = StandardScaler()\nfor i in range(200):\n    all_data['var_' + str(i)] = scaler.fit_transform(np.array(all_data['var_' + str(i)]).reshape(-1, 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scale_train_df = all_data[all_data.target.notna()]\nscale_test_df = all_data[all_data.target.isna()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nidx = features = train_df.columns.values[2:202]\n\ntrain_df['sum'] = scale_train_df[idx].sum(axis=1)  \ntrain_df['min'] = scale_train_df[idx].min(axis=1)\ntrain_df['max'] = scale_train_df[idx].max(axis=1)\ntrain_df['mean'] = scale_train_df[idx].mean(axis=1)\ntrain_df['std'] = scale_train_df[idx].std(axis=1)\ntrain_df['skew'] = scale_train_df[idx].skew(axis=1)\ntrain_df['kurt'] = scale_train_df[idx].kurtosis(axis=1)\ntrain_df['med'] = scale_train_df[idx].median(axis=1)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df['sum'] = scale_test_df[idx].sum(axis=1)  \ntest_df['min'] = scale_test_df[idx].min(axis=1)\ntest_df['max'] = scale_test_df[idx].max(axis=1)\ntest_df['mean'] = scale_test_df[idx].mean(axis=1)\ntest_df['std'] = scale_test_df[idx].std(axis=1)\ntest_df['skew'] = scale_test_df[idx].skew(axis=1)\ntest_df['kurt'] = scale_test_df[idx].kurtosis(axis=1)\ntest_df['med'] = scale_test_df[idx].median(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_df['scale_std'] = scale_train_df[idx].std(axis=1)\n# test_df['scale_std'] = scale_test_df[idx].std(axis=1)\n\n# train_df['scale_25'] = np.percentile(scale_train_df[idx], 25)\n# test_df['scale_25'] = np.percentile(scale_test_df[idx], 25)\n\n# train_df['scale_75'] = np.percentile(scale_train_df[idx], 75)\n# test_df['scale_75'] = np.percentile(scale_test_df[idx], 75)\n\n# train_df['scale_max'] = np.max(scale_train_df[idx])\n# test_df['scale_max'] = np.max(scale_test_df[idx])\n\n# train_df['scale_min'] = np.min(scale_train_df[idx])\n# test_df['scale_min'] = np.min(scale_test_df[idx])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# features = [c for c in train_df.columns if c not in ['ID_code', 'target']]\n# for feature in features:\n#     train_df['r2_'+feature] = np.round(train_df[feature], 2)\n#     test_df['r2_'+feature] = np.round(test_df[feature], 2)\n#     train_df['r1_'+feature] = np.round(train_df[feature], 1)\n#     test_df['r1_'+feature] = np.round(test_df[feature], 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_df['var_108_cluster'] = train_df['var_108'].apply(lambda x: 0 if x < 14.2 else 1)\n# train_df['var_141_cluster'] = train_df['var_141'].apply(lambda x: 0 if x < 8.9 else 1)\n# train_df['var_181_cluster'] = train_df['var_181'].apply(lambda x: 0 if x < 10.8 else 1)\n# train_df['var_83_cluster'] = train_df['var_83'].apply(lambda x: 0 if x < 2.5 else 1)\n# train_df['var_123_cluster'] = train_df['var_123'].apply(lambda x: 0 if x < -7.2 else (1 if x < 6 else 2))\n\n# test_df['var_108_cluster'] = test_df['var_108'].apply(lambda x: 0 if x < 14.2 else 1)\n# test_df['var_141_cluster'] = test_df['var_141'].apply(lambda x: 0 if x < 8.9 else 1)\n# test_df['var_181_cluster'] = test_df['var_181'].apply(lambda x: 0 if x < 10.8 else 1)\n# test_df['var_83_cluster'] = test_df['var_83'].apply(lambda x: 0 if x < 2.5 else 1)\n# test_df['var_123_cluster'] = test_df['var_123'].apply(lambda x: 0 if x < -7.2 else (1 if x < 6 else 2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reverse_list = [0,1,2,3,4,5,6,7,8,11,15,16,18,19,\n#             22,24,25,26,27,41,29,\n#             32,35,37,40,48,49,47,\n#             55,51,52,53,60,61,62,103,65,66,67,69,\n#             70,71,74,78,79,\n#             82,84,89,90,91,94,95,96,97,99,\n#             105,106,110,111,112,118,119,125,128,\n#             130,133,134,135,137,138,\n#             140,144,145,147,151,155,157,159,\n#             161,162,163,164,167,168,\n#             170,171,173,175,176,179,\n#             180,181,184,185,187,189,\n#             190,191,195,196,199]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# feature_cluster_config = {\n#     'var_108': 3,\n#     'var_41': 4,\n#     'var_181': 4,\n#     'var_83': 4,\n#     'var_123': 4\n# }\n# n_cluster = 3\n\n# for feat_val in reverse_list:\n#     feature = 'var_' + str(feat_val)\n#     clus_model = GaussianMixture(n_components=n_cluster, random_state=0, max_iter=10000, verbose=True)\n#     clus_model.fit(train_df[[feature]])\n\n#     train_proba_predict = clus_model.predict_proba(train_df[[feature]])\n#     train_proba_df = pd.DataFrame(train_proba_predict)\n#     train_proba_df.columns = [feature + '_clus_' + str(i) for i in range(n_cluster)]\n#     train_df = pd.concat([train_df, train_proba_df], axis=1, sort=False)\n    \n#     test_proba_predict = clus_model.predict_proba(test_df[[feature]])\n#     test_proba_df = pd.DataFrame(test_proba_predict)\n#     test_proba_df.columns = [feature + '_clus_' + str(i) for i in range(n_cluster)]\n#     test_df = pd.concat([test_df, test_proba_df], axis=1, sort=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"features = [c for c in train_df.columns if c not in ['ID_code', 'target']]\ntarget = train_df['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from bayes_opt import BayesianOptimization\nfrom sklearn.model_selection import KFold, StratifiedKFold\n\ndef objective(params):\n    fold__ = 10\n    param = {\n        'num_leaves': int(params['num_leaves']),\n        'min_data_in_leaf': int(params['min_data_in_leaf']), \n        'min_sum_hessian_in_leaf': int(params['min_sum_hessian_in_leaf']),\n        'max_depth': int(params['max_depth']),\n        'learning_rate': 0.015,\n        \"feature_fraction\": params['feature_fraction'],\n        'bagging_freq': 5,\n        \"bagging_fraction\": params['bagging_fraction'] ,\n        \"bagging_seed\": 11,\n        'metric':'auc',\n        'boosting' : 'gbdt',\n        'boost_from_average':'false',\n        \"verbosity\": -1,\n        'num_threads': 8,\n        'tree_learner': 'serial',\n        'objective': 'binary', \n        'seed':2019,\n        'bagging_seed':2019,\n        'drop_seed':2019,\n        }\n    folds = StratifiedKFold(n_splits=fold__, shuffle=False, random_state=44000)\n    oof = np.zeros(len(train_df))\n    # predictions = np.zeros(len(test_df))\n    feature_importance_df = pd.DataFrame()\n    for fold_, (trn_idx, val_idx) in enumerate(folds.split(train_df.values, target.values)):\n        print(\"Fold {}\".format(fold_))\n        trn_data = lgb.Dataset(train_df.iloc[trn_idx][features], label=target.iloc[trn_idx])\n        val_data = lgb.Dataset(train_df.iloc[val_idx][features], label=target.iloc[val_idx])\n\n        num_round = 30000\n        clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], \\\n                        early_stopping_rounds = 3000)\n        \n        oof[val_idx] = clf.predict(train_df.iloc[val_idx][features], num_iteration=clf.best_iteration)\n        \n        del clf, trn_idx, val_idx\n        gc.collect()\n    return  -roc_auc_score(target, oof)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from hyperopt.fmin import fmin\n\nspace = {\n    'num_leaves': hp.quniform('num_leaves', 2, 12, 2),\n    'max_depth': hp.quniform('max_depth', -1, 10, 1),\n    'min_sum_hessian_in_leaf': hp.quniform('min_sum_hessian_in_leaf', 5, 15, 1),\n    'feature_fraction': hp.quniform('feature_fraction', 0.02, 0.1, 0.005),\n    'bagging_fraction': hp.quniform('bagging_fraction', 0.1, 0.5, 0.05),\n    'min_data_in_leaf': hp.quniform('min_data_in_leaf', 50, 100, 5)\n\n}\nbest = fmin(fn=objective,\n            space=space,\n            algo=tpe.suggest,\n            max_evals=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Hyperopt estimated optimum {}\".format(best))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from bayes_opt import BayesianOptimization\n# from sklearn.model_selection import KFold, StratifiedKFold\n# fold__ = 10\n# def LGB_CV(\n#           max_depth,\n#           num_leaves,\n#           min_data_in_leaf,\n#           feature_fraction,\n#           bagging_fraction,\n#           min_sum_hessian_in_leaf\n#          ):\n    \n#     folds = StratifiedKFold(n_splits=fold__, shuffle=False, random_state=44000)\n#     oof = np.zeros(len(train_df))\n#     # predictions = np.zeros(len(test_df))\n#     feature_importance_df = pd.DataFrame()\n\n#     for fold_, (trn_idx, val_idx) in enumerate(folds.split(train_df.values, target.values)):\n#         print(\"Fold {}\".format(fold_))\n#         param = {\n#             'num_leaves': int(num_leaves),\n#             'min_data_in_leaf': int(min_data_in_leaf), \n#             'min_sum_hessian_in_leaf': min_sum_hessian_in_leaf,\n#             'max_depth': int(max_depth),\n#             'learning_rate': 0.01,\n#             \"feature_fraction\": feature_fraction,\n#             'bagging_freq': 5,\n#             \"bagging_fraction\": bagging_fraction ,\n#             \"bagging_seed\": 11,\n#             'metric':'auc',\n#             'boosting' : 'gbdt',\n#             'boost_from_average':'false',\n#             \"verbosity\": -1,\n#             'num_threads': 8,\n#             'tree_learner': 'serial',\n#             'objective': 'binary', \n#             'seed':int(2**fold__),\n#             'bagging_seed':int(2**fold__),\n#             'drop_seed':int(2**fold__)\n#         }\n#         trn_data = lgb.Dataset(train_df.iloc[trn_idx][features], label=target.iloc[trn_idx])\n#         val_data = lgb.Dataset(train_df.iloc[val_idx][features], label=target.iloc[val_idx])\n\n#         num_round = 30000\n#         clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=1000, \\\n#                         early_stopping_rounds = 3000)\n        \n#         oof[val_idx] = clf.predict(train_df.iloc[val_idx][features], num_iteration=clf.best_iteration)\n        \n#         del clf, trn_idx, val_idx\n#         gc.collect()\n        \n#     return roc_auc_score(target, oof)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# LGB_BO = BayesianOptimization(LGB_CV, {\n#     'max_depth': (2, 10),\n#     'num_leaves': (5, 130),\n#     'min_data_in_leaf': (50, 150),\n#     'feature_fraction': (0.2, 0.5),\n#     'bagging_fraction': (0.2, 0.5),\n#     'min_sum_hessian_in_leaf':(5, 15)\n# })\n\n# print('-'*126)\n\n# with warnings.catch_warnings():\n#     warnings.filterwarnings('ignore')\n#     LGB_BO.maximize(init_points=2, n_iter=10, acq='ei', xi=0.0)\n    \n# LGB_BO.max","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(LGB_BO.max)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# param = {\n#     'bagging_freq': 5,\n#     'bagging_fraction': 0.4,\n#     'boost_from_average':'false',\n#     'boost': 'gbdt',\n#     'feature_fraction': 0.05,\n#     'learning_rate': 0.01,\n#     'max_depth': 7,  \n#     'metric':'auc',\n#     'min_data_in_leaf': 80,\n#     'min_sum_hessian_in_leaf': 10.0,\n#     'num_leaves': 21,\n#     'num_threads': 8,\n#     'tree_learner': 'serial',\n#     'objective': 'binary', \n#     'verbosity': 1\n# }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# folds = StratifiedKFold(n_splits=10, shuffle=False, random_state=44000)\n# oof = np.zeros(len(train_df))\n# predictions = np.zeros(len(test_df))\n# feature_importance_df = pd.DataFrame()\n\n# for fold_, (trn_idx, val_idx) in enumerate(folds.split(train_df.values, target.values)):\n#     print(\"Fold {}\".format(fold_))\n#     trn_data = lgb.Dataset(train_df.iloc[trn_idx][features], label=target.iloc[trn_idx])\n#     val_data = lgb.Dataset(train_df.iloc[val_idx][features], label=target.iloc[val_idx])\n\n#     num_round = 1000000\n#     clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=1000, \\\n#                     early_stopping_rounds = 3000)\n#     oof[val_idx] = clf.predict(train_df.iloc[val_idx][features], num_iteration=clf.best_iteration)\n    \n#     fold_importance_df = pd.DataFrame()\n#     fold_importance_df[\"Feature\"] = features\n#     fold_importance_df[\"importance\"] = clf.feature_importance()\n#     fold_importance_df[\"fold\"] = fold_ + 1\n#     feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n#     predictions += clf.predict(test_df[features], num_iteration=clf.best_iteration) / folds.n_splits\n\n# print(\"CV score: {:<8.5f}\".format(roc_auc_score(target, oof)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cols = (feature_importance_df[[\"Feature\", \"importance\"]]\n#         .groupby(\"Feature\")\n#         .mean()\n#         .sort_values(by=\"importance\", ascending=False)[:150].index)\n# best_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n\n# plt.figure(figsize=(14,28))\n# sns.barplot(x=\"importance\", y=\"Feature\", data=best_features.sort_values(by=\"importance\",ascending=False))\n# plt.title('Features importance (averaged/folds)')\n# plt.tight_layout()\n# plt.savefig('FI.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sub_df = pd.DataFrame({\"ID_code\":test_df[\"ID_code\"].values})\n# sub_df[\"target\"] = predictions\n# sub_df.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}