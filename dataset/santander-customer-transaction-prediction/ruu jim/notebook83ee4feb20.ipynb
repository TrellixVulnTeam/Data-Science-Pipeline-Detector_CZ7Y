{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-13T07:26:30.448131Z","iopub.execute_input":"2021-09-13T07:26:30.448428Z","iopub.status.idle":"2021-09-13T07:26:30.460149Z","shell.execute_reply.started":"2021-09-13T07:26:30.448395Z","shell.execute_reply":"2021-09-13T07:26:30.458995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Import Library","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nsns.set()\n\n# sklearn models & tools\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nimport lightgbm as lgb\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import make_scorer\n# from sklearn.mixture import GaussianMixture\n# from sklearn.preprocessing import RobustScaler\n# from sklearn.decomposition import PCA\n\n# ignore warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport time","metadata":{"execution":{"iopub.status.busy":"2021-09-13T07:26:30.462337Z","iopub.execute_input":"2021-09-13T07:26:30.46264Z","iopub.status.idle":"2021-09-13T07:26:32.925975Z","shell.execute_reply.started":"2021-09-13T07:26:30.462602Z","shell.execute_reply":"2021-09-13T07:26:32.925125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load Data","metadata":{}},{"cell_type":"code","source":"%%time\nsubmission = pd.read_csv('/kaggle/input/santander-customer-transaction-prediction/sample_submission.csv')\ntrain = pd.read_csv('/kaggle/input/santander-customer-transaction-prediction/train.csv')\ntest = pd.read_csv('/kaggle/input/santander-customer-transaction-prediction/test.csv')","metadata":{"execution":{"iopub.status.busy":"2021-09-13T07:26:32.927682Z","iopub.execute_input":"2021-09-13T07:26:32.927964Z","iopub.status.idle":"2021-09-13T07:26:51.166845Z","shell.execute_reply.started":"2021-09-13T07:26:32.927927Z","shell.execute_reply":"2021-09-13T07:26:51.165776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check data\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-13T07:26:51.168286Z","iopub.execute_input":"2021-09-13T07:26:51.168596Z","iopub.status.idle":"2021-09-13T07:26:51.212238Z","shell.execute_reply.started":"2021-09-13T07:26:51.168556Z","shell.execute_reply":"2021-09-13T07:26:51.211296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-13T07:26:51.214497Z","iopub.execute_input":"2021-09-13T07:26:51.214705Z","iopub.status.idle":"2021-09-13T07:26:51.244644Z","shell.execute_reply.started":"2021-09-13T07:26:51.214682Z","shell.execute_reply":"2021-09-13T07:26:51.243657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-13T07:26:51.245783Z","iopub.execute_input":"2021-09-13T07:26:51.246222Z","iopub.status.idle":"2021-09-13T07:26:51.256295Z","shell.execute_reply.started":"2021-09-13T07:26:51.246175Z","shell.execute_reply":"2021-09-13T07:26:51.255471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-13T07:26:51.257555Z","iopub.execute_input":"2021-09-13T07:26:51.258308Z","iopub.status.idle":"2021-09-13T07:26:51.270706Z","shell.execute_reply.started":"2021-09-13T07:26:51.258268Z","shell.execute_reply":"2021-09-13T07:26:51.26967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2021-09-13T07:26:51.272134Z","iopub.execute_input":"2021-09-13T07:26:51.272586Z","iopub.status.idle":"2021-09-13T07:26:51.302811Z","shell.execute_reply.started":"2021-09-13T07:26:51.272515Z","shell.execute_reply":"2021-09-13T07:26:51.302212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.info()","metadata":{"execution":{"iopub.status.busy":"2021-09-13T07:26:51.304415Z","iopub.execute_input":"2021-09-13T07:26:51.304741Z","iopub.status.idle":"2021-09-13T07:26:51.327291Z","shell.execute_reply.started":"2021-09-13T07:26:51.304713Z","shell.execute_reply":"2021-09-13T07:26:51.326417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### train\n- ID_code : (object) string data type\n- target : int => target variable\n- var_0 ~ var_199 : (float) 200 numerical variables  \n\n#### test\n- ID_code : (object) string data type\n- var_0 ~ var_199 : (float) 200 numerical variables\n\n#### Except for ID_code, all are numerical variables.","metadata":{}},{"cell_type":"markdown","source":"### Check null data","metadata":{}},{"cell_type":"code","source":"train.isnull().sum().sum()","metadata":{"execution":{"iopub.status.busy":"2021-09-13T07:26:51.32836Z","iopub.execute_input":"2021-09-13T07:26:51.329063Z","iopub.status.idle":"2021-09-13T07:26:51.432121Z","shell.execute_reply.started":"2021-09-13T07:26:51.329026Z","shell.execute_reply":"2021-09-13T07:26:51.431245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.isnull().sum().sum()","metadata":{"execution":{"iopub.status.busy":"2021-09-13T07:26:51.434413Z","iopub.execute_input":"2021-09-13T07:26:51.43462Z","iopub.status.idle":"2021-09-13T07:26:51.530438Z","shell.execute_reply.started":"2021-09-13T07:26:51.434598Z","shell.execute_reply":"2021-09-13T07:26:51.529565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is no missing data in both train and test datasets.","metadata":{}},{"cell_type":"markdown","source":"## EDA\n- 데이터 파악을 위해 데이터를 샅샅이 탐색해보도록 하겠습니다.\n- We will explore the data to understand the data.\n\n- 모델링의 목적에 맞는 EDA를 진행해서 어떤 식으로 모델링을 해야 할 지 정해보도록 하겠습니다.","metadata":{}},{"cell_type":"markdown","source":"### 모델링의 목적 = 분석 목적\n- Purpose of modeling = Purpose of analysis\n\n- train dataset의 익명의 수치형 데이터들을 통해 test data의 target이 0일지 1일지 예측하는 분류문제\n- Classification problem predicting whether the target of test dataset is 0 or 1 through anonymous numerical data of train dataset.\n \n \n- 중요한 포인트는 피처의 정보는 오직 수치로만 확인 가능하다는 점이다. 직관적으로 피처 간의 관계를 파악하기 힘들다.\n- The important point is that feature information can only be checked with numbers. It is difficult to intuitively grasp the relationship between features.\n\n\n- 이러한 데이터셋과 분석 목적을 가질 때 어떤 모델링을 해야 할까? 그리고 이 모델링의 목적에 맞는 EDA는 어떻게 진행하면 좋을까?\n- What modeling should we do when we have these datasets and analysis purposes? And how should we proceed with the EDA that fits the purpose of this modeling?\n\n\n- **지수님이 설명해주신 lightgbm 동작원리와 관련해서 설명을 적으면 좋을 것 같습니다.**","metadata":{}},{"cell_type":"markdown","source":"### target data ","metadata":{}},{"cell_type":"code","source":"sns.countplot(train.target)","metadata":{"execution":{"iopub.status.busy":"2021-09-13T07:26:51.531497Z","iopub.execute_input":"2021-09-13T07:26:51.531793Z","iopub.status.idle":"2021-09-13T07:26:51.766227Z","shell.execute_reply.started":"2021-09-13T07:26:51.531766Z","shell.execute_reply":"2021-09-13T07:26:51.765416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.loc[train.target==1].shape[0] / train.loc[train.target==0].shape[0]","metadata":{"execution":{"iopub.status.busy":"2021-09-13T07:26:51.767491Z","iopub.execute_input":"2021-09-13T07:26:51.767703Z","iopub.status.idle":"2021-09-13T07:26:51.946154Z","shell.execute_reply.started":"2021-09-13T07:26:51.767679Z","shell.execute_reply":"2021-09-13T07:26:51.945425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[\"target\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-09-13T09:54:08.939339Z","iopub.execute_input":"2021-09-13T09:54:08.939655Z","iopub.status.idle":"2021-09-13T09:54:08.951145Z","shell.execute_reply.started":"2021-09-13T09:54:08.93962Z","shell.execute_reply":"2021-09-13T09:54:08.950127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### class imbalanced\n\n- We have to solve the problem about imbalanced class.\n- 거래를 할 고객이 거래를 하지 않을 고객보다 훨씩 적다.\n- There are far fewer customers who will transaction than those who will not.","metadata":{}},{"cell_type":"code","source":"train.describe()","metadata":{"execution":{"iopub.status.busy":"2021-09-13T07:26:51.993525Z","iopub.execute_input":"2021-09-13T07:26:51.99372Z","iopub.status.idle":"2021-09-13T07:26:53.88448Z","shell.execute_reply.started":"2021-09-13T07:26:51.993698Z","shell.execute_reply":"2021-09-13T07:26:53.883842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.describe()","metadata":{"execution":{"iopub.status.busy":"2021-09-13T07:26:53.885556Z","iopub.execute_input":"2021-09-13T07:26:53.886046Z","iopub.status.idle":"2021-09-13T07:26:55.768273Z","shell.execute_reply.started":"2021-09-13T07:26:53.886013Z","shell.execute_reply":"2021-09-13T07:26:55.767676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- train, test 데이터의 집계값이 비슷해 보인다.\n- The aggregated values of train and test data look similar.\n\n- 피처 간 집계값들은 조금 차이가 있어 보인다.\n- There seems to be a difference in the aggregated values between features.","metadata":{}},{"cell_type":"markdown","source":"#### feature correlation","metadata":{}},{"cell_type":"code","source":"(train.drop([\"target\", \"ID_code\"], axis=1).corr()).mean().mean()","metadata":{"execution":{"iopub.status.busy":"2021-09-13T07:26:55.76931Z","iopub.execute_input":"2021-09-13T07:26:55.769895Z","iopub.status.idle":"2021-09-13T07:27:17.595976Z","shell.execute_reply.started":"2021-09-13T07:26:55.769864Z","shell.execute_reply":"2021-09-13T07:27:17.595152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.drop([\"target\", \"ID_code\"], axis=1).corr()","metadata":{"execution":{"iopub.status.busy":"2021-09-13T07:27:17.597184Z","iopub.execute_input":"2021-09-13T07:27:17.59747Z","iopub.status.idle":"2021-09-13T07:27:39.455004Z","shell.execute_reply.started":"2021-09-13T07:27:17.597425Z","shell.execute_reply":"2021-09-13T07:27:39.454092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 코드 그대로 가져옴\ntrain_correlations = train.drop([\"target\"], axis=1).corr()\ntrain_correlations = train_correlations.values.flatten()\ntrain_correlations = train_correlations[train_correlations != 1]\n\ntest_correlations = test.corr()\ntest_correlations = test_correlations.values.flatten()\ntest_correlations = test_correlations[test_correlations != 1]\n\nplt.figure(figsize=(20,5))\nsns.distplot(train_correlations, color=\"Red\", label=\"train\")\nsns.distplot(test_correlations, color=\"Green\", label=\"test\")\nplt.xlabel(\"Correlation values found in train (except 1)\")\nplt.ylabel(\"Density\")\nplt.title(\"Are there correlations between features?\"); \nplt.legend()","metadata":{"execution":{"iopub.status.busy":"2021-09-13T07:27:39.45636Z","iopub.execute_input":"2021-09-13T07:27:39.456581Z","iopub.status.idle":"2021-09-13T07:28:24.189684Z","shell.execute_reply.started":"2021-09-13T07:27:39.456556Z","shell.execute_reply":"2021-09-13T07:28:24.188673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"모든 변수가 선형 상관관계를 가지지 않는 것으로 보인다.","metadata":{}},{"cell_type":"markdown","source":"#### 중요 변수 뽑기","metadata":{}},{"cell_type":"markdown","source":"EDA를 통해 피처들의 히스토그램을 살펴볼 때 200개의 모든 변수를 확인하는 것보다 몇 개를 뽑아서 확인하는 것이 더 효율적이라고 생각.\n\n- 그냥 뽑는 것보다 중요 변수 선택 방법을 이용해서 뽑는 것이 더 효과적이라고 생각.\n- 모든 변수들이 선형 상관관계를 가지지 않기 때문에 nonlinear model을 사용해서 중요 변수를 뽑아보자.\n- random forest를 사용하고, 모델의 내장함수인 feature_importances_를 사용해서 중요 변수를 뽑을 것이다.","metadata":{}},{"cell_type":"code","source":"%%time\nparameters = {'min_samples_leaf': [20, 25]}\nforest = RandomForestClassifier(max_depth=15, n_estimators=15)\ngrid = GridSearchCV(forest, parameters, cv=3, n_jobs=-1, verbose=2, scoring=make_scorer(roc_auc_score))","metadata":{"execution":{"iopub.status.busy":"2021-09-13T10:01:38.522969Z","iopub.execute_input":"2021-09-13T10:01:38.52378Z","iopub.status.idle":"2021-09-13T10:01:38.533903Z","shell.execute_reply.started":"2021-09-13T10:01:38.523736Z","shell.execute_reply":"2021-09-13T10:01:38.532581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 모델 학습\ngrid.fit(train.drop([\"target\", \"ID_code\"], axis=1).values, train.target.values)","metadata":{"execution":{"iopub.status.busy":"2021-09-13T10:01:42.515239Z","iopub.execute_input":"2021-09-13T10:01:42.515794Z","iopub.status.idle":"2021-09-13T10:04:39.697597Z","shell.execute_reply.started":"2021-09-13T10:01:42.515756Z","shell.execute_reply":"2021-09-13T10:04:39.696521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid.best_params_","metadata":{"execution":{"iopub.status.busy":"2021-09-13T10:04:44.350483Z","iopub.execute_input":"2021-09-13T10:04:44.351287Z","iopub.status.idle":"2021-09-13T10:04:44.357379Z","shell.execute_reply.started":"2021-09-13T10:04:44.351235Z","shell.execute_reply":"2021-09-13T10:04:44.356383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid.best_score_","metadata":{"execution":{"iopub.status.busy":"2021-09-13T10:04:45.140727Z","iopub.execute_input":"2021-09-13T10:04:45.14103Z","iopub.status.idle":"2021-09-13T10:04:45.147358Z","shell.execute_reply.started":"2021-09-13T10:04:45.141001Z","shell.execute_reply":"2021-09-13T10:04:45.146588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 모델의 내장 함수인 feature_importances_\ngrid.best_estimator_.feature_importances_ \n\n# 어레이 형태로 반환 # shape : 200","metadata":{"execution":{"iopub.status.busy":"2021-09-13T10:04:56.371106Z","iopub.execute_input":"2021-09-13T10:04:56.371561Z","iopub.status.idle":"2021-09-13T10:04:56.383772Z","shell.execute_reply.started":"2021-09-13T10:04:56.37153Z","shell.execute_reply":"2021-09-13T10:04:56.382862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_top = 5\n\n# 변수 중요도\nimportances = grid.best_estimator_.feature_importances_\n\n# 변수 중요도 상위 5개의 인덱스\nidx = np.argsort(importances)[::-1][0:n_top] \n\n# 변수 이름\nfeature_names = train.drop([\"target\", \"ID_code\"], axis=1).columns.values \n\n# 변수 중요도 기준 상위 5개 변수의 중요도 시각화\nplt.figure(figsize=(20,5))\nsns.barplot(x=feature_names[idx], y=importances[idx])\nplt.title(\"What are the top important features?\")","metadata":{"execution":{"iopub.status.busy":"2021-09-13T10:04:58.678464Z","iopub.execute_input":"2021-09-13T10:04:58.678967Z","iopub.status.idle":"2021-09-13T10:04:59.105385Z","shell.execute_reply.started":"2021-09-13T10:04:58.678936Z","shell.execute_reply":"2021-09-13T10:04:59.104564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Q. 중요 변수를 몇개를 뽑아서 탐색하는 것이 전체 EDA에 어떤 도움이 되는지?","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(n_top, 2, figsize=(20, 5*n_top))\n\nfor n in range(n_top):\n    sns.distplot(train.loc[train.target==0, feature_names[idx][n]], ax=ax[n,0], color=\"Orange\", norm_hist=True)\n    sns.distplot(train.loc[train.target==1, feature_names[idx][n]], ax=ax[n,0], color=\"Red\", norm_hist=True)\n    sns.distplot(test.loc[:, feature_names[idx][n]], ax=ax[n,1], color=\"Mediumseagreen\", norm_hist=True)\n    ax[n,0].set_title(\"Train {}\".format(feature_names[idx][n]))\n    ax[n,1].set_title(\"Test {}\".format(feature_names[idx][n]))\n    ax[n,0].set_xlabel(\"\")\n    ax[n,1].set_xlabel(\"\")","metadata":{"execution":{"iopub.status.busy":"2021-09-13T10:05:14.793836Z","iopub.execute_input":"2021-09-13T10:05:14.794267Z","iopub.status.idle":"2021-09-13T10:05:30.537206Z","shell.execute_reply.started":"2021-09-13T10:05:14.794235Z","shell.execute_reply":"2021-09-13T10:05:30.536273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- describe() 함수에서도 확인했듯이, train과 test의 분포가 비슷하다\n- train에서 target=0, 1의 pdf가 대체로 비슷한데, 완전 다르게 target=1에서 갑자기 솟아오른 지점이 있다.(갑자기 누적된 부분)\n\n\n- significant different distribution for the two target values.\n- target=1과 target=0의 분포 모양이 달라야 확실하게 구분 가능하고, 이러한 변수를 사용해야 예측 정확도를 높일 수 있다.\n- 선택된 변수들은 target에 따라 데이터의 분포가 다른 변수들이다.\n\n- train의 target=1인 데이터들은 ","metadata":{}},{"cell_type":"markdown","source":"- train, test의 분포는 완전 비슷함","metadata":{}},{"cell_type":"code","source":"top = train.loc[:, feature_names[idx]]\ntop.describe()","metadata":{"execution":{"iopub.status.busy":"2021-09-13T10:05:43.630792Z","iopub.execute_input":"2021-09-13T10:05:43.631127Z","iopub.status.idle":"2021-09-13T10:05:43.697089Z","shell.execute_reply.started":"2021-09-13T10:05:43.631088Z","shell.execute_reply":"2021-09-13T10:05:43.696129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# scatter plot\ntop = top.join(train.target)\nsns.pairplot(top, hue=\"target\")","metadata":{"execution":{"iopub.status.busy":"2021-09-13T10:05:54.989621Z","iopub.execute_input":"2021-09-13T10:05:54.990223Z","iopub.status.idle":"2021-09-13T10:10:39.709506Z","shell.execute_reply.started":"2021-09-13T10:05:54.990184Z","shell.execute_reply":"2021-09-13T10:10:39.708441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- target=1인 데이터들이 갑자기 누적되어서 거의 넘어가지 않는 날카로운 지점이 있다.\n- 예를 들어, 81번 변수에서는 10에, 12번 변수에서는 13.5에 데이터가 누적되어 날카롭게 솟은 지점이 있다.(limit)","metadata":{}},{"cell_type":"code","source":"top","metadata":{"execution":{"iopub.status.busy":"2021-09-13T10:10:39.711684Z","iopub.execute_input":"2021-09-13T10:10:39.711941Z","iopub.status.idle":"2021-09-13T10:10:39.728566Z","shell.execute_reply.started":"2021-09-13T10:10:39.71191Z","shell.execute_reply":"2021-09-13T10:10:39.727712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.columns.values[2:202]","metadata":{"execution":{"iopub.status.busy":"2021-09-13T10:10:39.729653Z","iopub.execute_input":"2021-09-13T10:10:39.729882Z","iopub.status.idle":"2021-09-13T10:10:39.736526Z","shell.execute_reply.started":"2021-09-13T10:10:39.729855Z","shell.execute_reply":"2021-09-13T10:10:39.7357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"로우별 평균값의 분포 확인(train, test)","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(16,6))\nfeatures = train.columns.values[2:202]\nplt.title(\"Distribution of mean values per row in the train and test set\")\nsns.distplot(train[features].mean(axis=1), color=\"green\", kde=True, bins=120, label='train')\nsns.distplot(test[features].mean(axis=1), color=\"blue\", kde=True, bins=120, label='test')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-13T07:56:29.091004Z","iopub.execute_input":"2021-09-13T07:56:29.091843Z","iopub.status.idle":"2021-09-13T07:56:32.542808Z","shell.execute_reply.started":"2021-09-13T07:56:29.091802Z","shell.execute_reply":"2021-09-13T07:56:32.541994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- 여기는 집계값 확인을 위한 EDA인데, 넣을지 말지 고민됨..","metadata":{}},{"cell_type":"markdown","source":"## feature engineering\n\n- 기존 변수를 기반으로 새로운 변수 생성\n- 기존 변수의 특성을 반영하는(가지고 있는) 변수 생성\n- target=1, 0을 구분하는데 더욱 도움이 되는 변수 \n","metadata":{}},{"cell_type":"markdown","source":"### 1)\n#### 반올림 및 분위수 기반 binning : label encoding\n\n1. 반올림 : np.round\n    - 그냥 반올림한 것, 반올림x10, 반올림x100\n\n\n2. 분위수 기반 binning : pd.qcut\n    - 앞서 뽑은 5개의 중요 변수의 데이터(수치형)의 정확한 값 말고 데이터들의 범위(범주)를 알기 위해 (values range than actual values)\n    - qcut함수를 이용해서 분위수 기반의 bin을 만들고, LabelEncoder로 라벨링하여 새로운 변수 생성\n    - We will use qcut to create 10 equally sized bins i.e quartiles\n    - qcut function can be used to generate equally sized quantiles bins for your data\n\n===============================================================================\n\n- Histograms are example of data binning that helps to visualize your data distribution in equal intervals\n\n- qcut : bin을 만들기 위한 분위수 기반의 함수\n- qcut is a quantile based function to create bins\n\n- Quantile is to divide the data into equal number of subgroups or probability distributions of equal probability into continuous interval\n- Quantile은 데이터를 동일한 수의 subgroups으로 나누거나 같은 확률의 확률 분포를 연속 구간으로 나누는 것입니다.","metadata":{}},{"cell_type":"code","source":"%%time\noriginal_features = train.drop([\"target\", \"ID_code\"], axis=1).columns.values\n\n# 5개의 중요 변수 qcut한 변수 생성\nencoder = LabelEncoder()\nfor your_feature in top.drop(\"target\", axis=1).columns.values:\n    train[your_feature + \"_qbinned\"] = pd.qcut(\n        train.loc[:, your_feature].values,\n        q=10,\n        labels=False\n    )\n    train[your_feature + \"_qbinned\"] = encoder.fit_transform(\n        train[your_feature + \"_qbinned\"].values.reshape(-1, 1)\n    )\n    \n# 5개의 중요 변수 반올림한 변수 생성\nencoder = LabelEncoder()\nfor your_feature in top.drop(\"target\", axis=1).columns.values:\n    train[your_feature + \"_rounded\"] = np.round(train.loc[:, your_feature].values)\n    train[your_feature + \"_rounded_10\"] = np.round(10*train.loc[:, your_feature].values)\n    train[your_feature + \"_rounded_100\"] = np.round(100*train.loc[:, your_feature].values)\n\n    \n# test에도 같이 적용\n\nencoder = LabelEncoder()\nfor your_feature in top.drop(\"target\", axis=1).columns.values:\n    test[your_feature + \"_qbinned\"] = pd.qcut(\n        test.loc[:, your_feature].values,\n        q=10,\n        labels=False\n    )\n    test[your_feature + \"_qbinned\"] = encoder.fit_transform(\n        test[your_feature + \"_qbinned\"].values.reshape(-1, 1)\n    )\n\nencoder = LabelEncoder()\nfor your_feature in top.drop(\"target\", axis=1).columns.values:\n    test[your_feature + \"_rounded\"] = np.round(test.loc[:, your_feature].values)\n    test[your_feature + \"_rounded_10\"] = np.round(10*test.loc[:, your_feature].values)\n    test[your_feature + \"_rounded_100\"] = np.round(100*test.loc[:, your_feature].values)","metadata":{"execution":{"iopub.status.busy":"2021-09-13T10:15:36.086847Z","iopub.execute_input":"2021-09-13T10:15:36.087243Z","iopub.status.idle":"2021-09-13T10:15:36.716404Z","shell.execute_reply.started":"2021-09-13T10:15:36.087203Z","shell.execute_reply":"2021-09-13T10:15:36.71542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-13T10:15:40.49107Z","iopub.execute_input":"2021-09-13T10:15:40.491408Z","iopub.status.idle":"2021-09-13T10:15:40.521398Z","shell.execute_reply.started":"2021-09-13T10:15:40.491379Z","shell.execute_reply":"2021-09-13T10:15:40.520699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.columns.values[2:202]","metadata":{"execution":{"iopub.status.busy":"2021-09-13T10:16:10.740126Z","iopub.execute_input":"2021-09-13T10:16:10.740473Z","iopub.status.idle":"2021-09-13T10:16:10.747503Z","shell.execute_reply.started":"2021-09-13T10:16:10.740438Z","shell.execute_reply":"2021-09-13T10:16:10.746562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2)\n#### row별 집계값 : sum, min, max 등 \n- 기존 피처에 대해 몇 개의 집계값을 계산해서 새로운 피처로 추가\n- 왜?","metadata":{}},{"cell_type":"code","source":"%%time\nidx = features = train.columns.values[2:202] # 원래 피처(var_0 ~ var_199)\nfor df in [test, train]:\n    df['sum'] = df[idx].sum(axis=1)  \n    df['min'] = df[idx].min(axis=1)\n    df['max'] = df[idx].max(axis=1)\n    df['mean'] = df[idx].mean(axis=1)\n    df['std'] = df[idx].std(axis=1)\n    df['skew'] = df[idx].skew(axis=1)\n    df['kurt'] = df[idx].kurtosis(axis=1)\n    df['med'] = df[idx].median(axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-09-13T10:16:22.36263Z","iopub.execute_input":"2021-09-13T10:16:22.363354Z","iopub.status.idle":"2021-09-13T10:16:34.020102Z","shell.execute_reply.started":"2021-09-13T10:16:22.363306Z","shell.execute_reply":"2021-09-13T10:16:34.019032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 추가한 변수 확인\ntrain[train.columns[222:]].head()","metadata":{"execution":{"iopub.status.busy":"2021-09-13T10:16:37.48829Z","iopub.execute_input":"2021-09-13T10:16:37.489111Z","iopub.status.idle":"2021-09-13T10:16:37.756384Z","shell.execute_reply.started":"2021-09-13T10:16:37.489071Z","shell.execute_reply":"2021-09-13T10:16:37.755784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test[test.columns[222:]].head()","metadata":{"execution":{"iopub.status.busy":"2021-09-13T10:16:45.027794Z","iopub.execute_input":"2021-09-13T10:16:45.028655Z","iopub.status.idle":"2021-09-13T10:16:45.298952Z","shell.execute_reply.started":"2021-09-13T10:16:45.028607Z","shell.execute_reply":"2021-09-13T10:16:45.297852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-13T10:16:48.290471Z","iopub.execute_input":"2021-09-13T10:16:48.290755Z","iopub.status.idle":"2021-09-13T10:16:48.296891Z","shell.execute_reply.started":"2021-09-13T10:16:48.290728Z","shell.execute_reply":"2021-09-13T10:16:48.296304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modeling","metadata":{}},{"cell_type":"code","source":"features = [c for c in train.columns if c not in ['ID_code', 'target']]\ntarget = train['target']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 파라미터\nparams = {'objective' : \"binary\",  \n               'boost':\"gbdt\", # gbdt : Gradient Boosting Desicion Tree # 실행하고자 하는 알고리즘 타입 정의\n               'metric':\"auc\", # 성능평가 지표\n               'boost_from_average':\"false\",\n               'num_threads':8,\n               'learning_rate' : 0.01, # 최종 결과에 대한 각각의 Tree에 영향을 미치는 변수\n               'num_leaves' : 13, # 10, 13, 15 # 전체 Tree의 leave 수. Tree 모델의 복잡성을 컨트롤하는 주요 파라미터. # 디폴트 31\n               'max_depth':-1,  # tree의 최대 깊이 # 0보다 작은 값은 깊이에 제한이 없음 # 디폴트 -1\n               'tree_learner' : \"serial\",\n               'feature_fraction' : 0.05, # 모델이 tree를 만들 때 매번 각각의 iteration에서 파라미터 중 5%를 랜덤하게 선택\n               'bagging_freq' : 5,\n               'bagging_fraction' : 0.4, # 매번 iteration을 돌 때 사용되는 데이터의 일부를 선택하는데 트레이닝 속도를 높이고 과적합을 방지할 때 주로 사용\n               'min_data_in_leaf' : 80, # 70, 80, 90 # Leaf가 가지고 있는 최소한의 레코드 수, 과적합 해결에 사용, 디폴트 20(최적값)\n               'min_sum_hessian_in_leaf' : 10.0,\n               'verbosity' : 1}\n\n# 일반적으로 n_estimators를 크게 하고 learning_rate를 작게 해서 예측 성능을 향상시킬 수 있으나, 과적합 이슈와 학습시간이 길어지는 부정적인 영향도 고려해야 함","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Hyper Parameter Tunning\n- 성능 높이고 과적합을 줄이는 방향으로 주요 파라미터를 튜닝했다.\n- learning_rate : 0.01로 낮을 때 성능이 좋았다.\n- num_leaves : 10, 13, 15\n- min_data_in_leaf : 70, 80, 90\n\n#### kfold\n- class imbalanced 문제를 고려해서 StratifiedKFold 방식을 사용했다.\n\n#### LightGBM\n- Dataset이 피처의 수가 많고, 모두 수치형 데이터 타입으로 이루어져 있다.\n- 데이터의 수가 LightGBM을 사용하기에 적합하다.(과적합x)","metadata":{}},{"cell_type":"code","source":"%%time\n\nfolds = StratifiedKFold(n_splits=5, shuffle=False, random_state=44000)\noof = np.zeros(len(train))\npredictions = np.zeros(len(test))\nfeature_importance_df = pd.DataFrame()\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train.values, target.values)):\n    print(\"Fold {}\".format(fold_))\n    trn_data = lgb.Dataset(train.iloc[trn_idx][features], label=target.iloc[trn_idx])\n    val_data = lgb.Dataset(train.iloc[val_idx][features], label=target.iloc[val_idx])\n\n    num_round = 1000000 # 다폴트 : 100\n    clf = lgb.train(params, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds = 3000)\n    oof[val_idx] = clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n    # early_stopping_rounds : The index of iteration that has the best performance will be saved in the best_iteration field if early stopping logic is enabled by setting early_stopping_rounds\n    # verbose_eval : If int, the eval metric on the valid set is printed at every verbose_eval boosting stage. \n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"Feature\"] = features\n    fold_importance_df[\"importance\"] = clf.feature_importance()\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    predictions += clf.predict(test[features], num_iteration=clf.best_iteration) / folds.n_splits\n\nprint(\"CV score: {:<8.5f}\".format(roc_auc_score(target, oof)))","metadata":{},"execution_count":null,"outputs":[]}]}