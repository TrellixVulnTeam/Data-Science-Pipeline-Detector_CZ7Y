{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"1. [Import Libraries](#1)\n2. [Reading the Dataset](#2)\n3. [Data Pre-processing](#3)\n4. [Modeling](#4)\n    - 4.1. [Logistic Regression](#7)\n    - 4.2. [LightGBM](#8)\n    - 4.3. [Wrap-up and get the output](#9)\n5. [Conclusions](#5)\n6. [References](#6)","metadata":{}},{"cell_type":"markdown","source":"# 1. Import Libraries <a id = 1> </a>","metadata":{}},{"cell_type":"code","source":"import gc\n\nimport lightgbm as lgb\n\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mtick\nfrom matplotlib.ticker import PercentFormatter\n\nimport numpy as np \n\nimport pandas as pd\n\nfrom scipy import stats\n\nimport seaborn as sns\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import Model, Sequential\nfrom tensorflow.keras.layers import BatchNormalization, Dense, LeakyReLU\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-19T15:53:06.440688Z","iopub.execute_input":"2022-05-19T15:53:06.441033Z","iopub.status.idle":"2022-05-19T15:53:15.040224Z","shell.execute_reply.started":"2022-05-19T15:53:06.440917Z","shell.execute_reply":"2022-05-19T15:53:15.039471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Reading the Dataset <a id = 2> </a>","metadata":{}},{"cell_type":"markdown","source":"#### <a href = 'https://www.kaggle.com/c/santander-customer-transaction-prediction'>Link to the dataset in Kaggle.</a>","metadata":{}},{"cell_type":"code","source":"raw_train = pd.read_csv('../input/santander-customer-transaction-prediction/train.csv')\nraw_test = pd.read_csv('../input/santander-customer-transaction-prediction/test.csv')\nraw_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-19T15:53:15.042139Z","iopub.execute_input":"2022-05-19T15:53:15.042382Z","iopub.status.idle":"2022-05-19T15:53:34.37212Z","shell.execute_reply.started":"2022-05-19T15:53:15.042349Z","shell.execute_reply":"2022-05-19T15:53:34.371431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Shape of training dataset: {raw_train.shape}')\nprint(f'Shape of testing dataset: {raw_test.shape}')\nprint(20 * \"-\")\nprint(f'Number of NaN values in training dataset: {raw_train.isnull().values.sum()}')\nprint(f'Number of NaN values in testing dataset: {raw_test.isnull().values.sum()}')\nprint(20 * \"-\")\nprint(f\"Number of training values that aren't float64: {sum(raw_train.iloc[:, 2:].dtypes != 'float64')}\")\nprint(f\"Number of testing values that aren't float64: {sum(raw_test.iloc[:, 2:].dtypes != 'float64')}\")","metadata":{"execution":{"iopub.status.busy":"2022-05-19T15:53:34.373528Z","iopub.execute_input":"2022-05-19T15:53:34.373825Z","iopub.status.idle":"2022-05-19T15:53:34.770102Z","shell.execute_reply.started":"2022-05-19T15:53:34.373787Z","shell.execute_reply":"2022-05-19T15:53:34.768771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Pre-processing <a id = 3> </a>","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (22, 5))\nsns.countplot(data = raw_train, x = 'target')\nplt.xlabel('Target', fontsize = 15)\nplt.ylabel('Count', fontsize = 15)\nplt.title('Class distribution', fontsize = 20)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T15:53:34.772227Z","iopub.execute_input":"2022-05-19T15:53:34.772491Z","iopub.status.idle":"2022-05-19T15:53:34.997119Z","shell.execute_reply.started":"2022-05-19T15:53:34.77244Z","shell.execute_reply":"2022-05-19T15:53:34.996368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_0, num_1 = np.bincount(raw_train['target'])\nprint(f'Number of samples belonging to class zero: {num_0} ({num_0 / raw_train.shape[0]:0.2%} of total)')\nprint(f'Number of samples belonging to class one: {num_1} ({num_1 / raw_train.shape[0]:0.2%} of total)')\n\ndel num_0, num_1\n_ = gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-05-19T15:53:34.998356Z","iopub.execute_input":"2022-05-19T15:53:34.998611Z","iopub.status.idle":"2022-05-19T15:53:35.182025Z","shell.execute_reply.started":"2022-05-19T15:53:34.998577Z","shell.execute_reply":"2022-05-19T15:53:35.180028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = raw_train['target']\ntrain_len = len(raw_train)\nid_test = raw_test['ID_code']\n\nmerged_df = pd.concat([raw_train, raw_test], axis = 0).drop(columns = ['ID_code', 'target'])\n\ndel raw_train, raw_test\n_ = gc.collect()\n\nmerged_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-19T15:53:35.183283Z","iopub.execute_input":"2022-05-19T15:53:35.184047Z","iopub.status.idle":"2022-05-19T15:53:36.208467Z","shell.execute_reply.started":"2022-05-19T15:53:35.184007Z","shell.execute_reply":"2022-05-19T15:53:36.207796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_minmax = MinMaxScaler().fit_transform(merged_df)\nmerged_minmax_df = pd.DataFrame(merged_minmax)\nmerged_minmax_df = merged_minmax_df.add_prefix('var_')","metadata":{"execution":{"iopub.status.busy":"2022-05-19T15:53:36.209722Z","iopub.execute_input":"2022-05-19T15:53:36.210396Z","iopub.status.idle":"2022-05-19T15:53:37.09432Z","shell.execute_reply.started":"2022-05-19T15:53:36.210354Z","shell.execute_reply":"2022-05-19T15:53:37.093591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def stat_detector(df):\n    \n    '''\n    argument:\n        df: a DataFrame.\n    outputs:\n        outlier_mask: 0-1 mask; 0: not outlier, 1: is outlier.\n        Three plot about mean values, std values and outliers for each feature. \n    '''\n    plt.figure(figsize=(22, 10))\n    \n    ax1 = plt.subplot2grid((22, 22), (0, 0), rowspan = 9, colspan = 10)\n    \n    plt.title(f'Distribution of mean values for each feature', fontsize = 20)\n    ax1 = sns.distplot(df.mean(), kde = True, bins = 50, color = 'red')\n    plt.ylabel('density', fontsize = 15)\n\n    ax2 = plt.subplot2grid((22, 22), (0, 12), rowspan = 9, colspan = 10)\n    \n    plt.title(f'Distribution of standard deviation values for each feature', fontsize = 20)\n    ax2 = sns.distplot(df.std(), kde = True, bins = 50, color = 'green')\n    plt.ylabel('density', fontsize = 15)\n\n    ax3 = plt.subplot2grid((22, 22), (11, 0), rowspan = 11, colspan = 22)\n\n    total_data = df.shape[0] * df.shape[1]\n\n    outlier_mask = (np.abs(stats.zscore(df)) > 3.0) * 1\n    sum_outlier_col = outlier_mask.sum()\n    total_outlier = sum(sum_outlier_col)\n    outlier_share = (total_outlier / total_data) * 100\n    share_outlier_col = (sum_outlier_col / df.shape[0]) * 100\n\n    ax3 = plt.axhline(y = outlier_share, color = 'red', linestyle = '--', label = 'Average')\n    ax3 = share_outlier_col.plot.bar()\n    plt.title(f'Percentage of outliers for each feature', fontsize = 20)\n    plt.suptitle(f'total samples: {total_data:,}, total outliers: {total_outlier:,}, share of outliers from total data: {outlier_share:0.3}%',\n                 y = 0.48, fontsize = 15)\n    plt.xlabel('Features', fontsize = 15)\n    plt.ylabel('% of outliers', fontsize = 15)\n    plt.legend(['Average'], fontsize = 15)\n    plt.xticks([])\n    ax3.yaxis.set_major_formatter(mtick.PercentFormatter(decimals = 3))\n\n    return outlier_mask\n\n_ = stat_detector(merged_minmax_df)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T15:53:37.095816Z","iopub.execute_input":"2022-05-19T15:53:37.09624Z","iopub.status.idle":"2022-05-19T15:53:42.414204Z","shell.execute_reply.started":"2022-05-19T15:53:37.0962Z","shell.execute_reply":"2022-05-19T15:53:42.413496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_minmax_df.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T15:53:42.415441Z","iopub.execute_input":"2022-05-19T15:53:42.415694Z","iopub.status.idle":"2022-05-19T15:53:42.443158Z","shell.execute_reply.started":"2022-05-19T15:53:42.41566Z","shell.execute_reply":"2022-05-19T15:53:42.442468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the first step, the dataset is scaled. Then, 64 features are added to the dataset in two steps. Data obtained from PCA is first added, followed by data received from Denoising Autoencoder. \n\n|method|Number of initial features|Number of features after transformation|\n|:-:|:-:|:-:|\n|Scaled dataset|200|200|\n|Scaled dataset + PCA|200|232|\n|Scaled dataset + PCA + DAE|232|264|\n","metadata":{}},{"cell_type":"code","source":"merged_pca = PCA(n_components = 0.2, random_state = 0).fit_transform(merged_minmax)\nmerged_pca_scaled = MinMaxScaler().fit_transform(merged_pca)\nmerged_pca_df = pd.DataFrame(merged_pca_scaled)\nmerged_pca_df = merged_pca_df.add_prefix('pca_')\nprint(f'Shape of dataset after transformation using PCA: {merged_pca.shape}')\n\nmerged_minmax_pca_df = pd.concat([merged_minmax_df, merged_pca_df], axis = 1)\nprint(f'Shape of dataset after concatenating: {merged_minmax_pca_df.shape}\\n')\n\ndel merged_minmax, merged_minmax_df, merged_pca, merged_pca_scaled #merged_minmax_df\n_ = gc.collect()\n\nmerged_minmax_pca_df.head(2)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T15:53:42.446267Z","iopub.execute_input":"2022-05-19T15:53:42.446467Z","iopub.status.idle":"2022-05-19T15:53:54.942279Z","shell.execute_reply.started":"2022-05-19T15:53:42.446442Z","shell.execute_reply":"2022-05-19T15:53:54.941542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_ = stat_detector(merged_minmax_pca_df)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T15:53:54.946109Z","iopub.execute_input":"2022-05-19T15:53:54.947736Z","iopub.status.idle":"2022-05-19T15:54:01.283455Z","shell.execute_reply.started":"2022-05-19T15:53:54.947694Z","shell.execute_reply":"2022-05-19T15:54:01.282781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_dim = merged_pca_df.shape[1] \n\nencoder_decoder = Sequential(\n    [\n        Dense(64, input_shape = (input_dim, )),\n        BatchNormalization(),\n        LeakyReLU(),\n        Dense(32),\n        BatchNormalization(),\n        LeakyReLU(),\n        Dense(16),\n        Dense(32),\n        BatchNormalization(),\n        LeakyReLU(),\n        Dense(64),\n        BatchNormalization(),\n        LeakyReLU(),\n        Dense(input_dim, activation = 'linear')\n        \n    ]\n)\n\nauto_encoder = Model(inputs = encoder_decoder.input, outputs = encoder_decoder.output)\nauto_encoder.summary()","metadata":{"execution":{"iopub.status.busy":"2022-05-19T15:54:01.284908Z","iopub.execute_input":"2022-05-19T15:54:01.285382Z","iopub.status.idle":"2022-05-19T15:54:04.002257Z","shell.execute_reply.started":"2022-05-19T15:54:01.285343Z","shell.execute_reply":"2022-05-19T15:54:04.001368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"auto_encoder.compile(loss = 'mse', optimizer = 'adam')\n\nepochs = 10\nindex_merged_pca = np.random.choice(np.arange(merged_pca_df.shape[0]), int(merged_pca_df.shape[0] / 4))\n\nauto_encoder_history = auto_encoder.fit(\n    merged_pca_df.iloc[index_merged_pca].values, #Can we add noise? + np.random.normal(0.5, 0.15, size = merged_pca_df.iloc[index_merged_pca].shape)\n    merged_pca_df.iloc[index_merged_pca].values,\n    batch_size = 32,\n    epochs = epochs\n)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T15:54:04.00385Z","iopub.execute_input":"2022-05-19T15:54:04.004233Z","iopub.status.idle":"2022-05-19T15:55:39.264426Z","shell.execute_reply.started":"2022-05-19T15:54:04.004187Z","shell.execute_reply":"2022-05-19T15:55:39.2637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_dae = auto_encoder.predict(merged_pca_df.values)\nmerged_dae_df = pd.DataFrame(merged_dae)\nmerged_dae_df = merged_dae_df.add_prefix('dae_')\nmerged_pca_dae = pd.concat([merged_minmax_pca_df, merged_dae_df], axis = 1)\n\ndel merged_dae, merged_pca_df, merged_dae_df, auto_encoder_history, index_merged_pca\n_ = gc.collect()\n\nmerged_pca_dae.head(2)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T15:55:39.265878Z","iopub.execute_input":"2022-05-19T15:55:39.266143Z","iopub.status.idle":"2022-05-19T15:55:54.759622Z","shell.execute_reply.started":"2022-05-19T15:55:39.266107Z","shell.execute_reply":"2022-05-19T15:55:54.758795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_ = stat_detector(merged_pca_dae)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T15:55:54.760874Z","iopub.execute_input":"2022-05-19T15:55:54.761314Z","iopub.status.idle":"2022-05-19T15:56:01.557427Z","shell.execute_reply.started":"2022-05-19T15:55:54.761276Z","shell.execute_reply":"2022-05-19T15:56:01.556789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_dev, y_train, y_dev = train_test_split(merged_pca_dae[:train_len], y, test_size = 0.3, shuffle = True, stratify = y)\n\nstnd_scaler = StandardScaler().fit(X_train)\nX_train_scaled = stnd_scaler.transform(X_train)\nX_dev_scaled = stnd_scaler.transform(X_dev)\nX_test_scaled = stnd_scaler.transform(merged_pca_dae[train_len:])\n\ndel merged_pca_dae, stnd_scaler\n_ = gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-05-19T15:56:01.558646Z","iopub.execute_input":"2022-05-19T15:56:01.559414Z","iopub.status.idle":"2022-05-19T15:56:03.411816Z","shell.execute_reply.started":"2022-05-19T15:56:01.559373Z","shell.execute_reply":"2022-05-19T15:56:03.409726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Modeling <a id = 4> </a>","metadata":{}},{"cell_type":"markdown","source":"## 4.1. Logistic Regression <a id = 7> </a>","metadata":{}},{"cell_type":"code","source":"param_grid_lr = {\n    'C': [0.001, 0.01]\n}\n\nlr = LogisticRegression(penalty = 'l2', solver = 'sag', class_weight = 'balanced', random_state = 0)\n\ngrid_lr = GridSearchCV(\n    estimator = lr, \n    param_grid = param_grid_lr, \n    scoring='roc_auc',\n    cv = 3, \n    refit = True, \n    n_jobs = -1\n)\n\ngrid_lr.fit(X_train_scaled, y_train)\n\nprint(f\"Best parameters: {grid_lr.best_params_}\")","metadata":{"execution":{"iopub.status.busy":"2022-05-19T15:56:03.414689Z","iopub.execute_input":"2022-05-19T15:56:03.415425Z","iopub.status.idle":"2022-05-19T15:57:20.366392Z","shell.execute_reply.started":"2022-05-19T15:56:03.415355Z","shell.execute_reply":"2022-05-19T15:57:20.365487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_lr = grid_lr.predict_proba(X_dev_scaled)[:, 1]\ny_pred_lr_final = grid_lr.predict_proba(X_test_scaled)[:, 1]\n\nprint(f'AUC score for dev dataset using LogisticRegression: {roc_auc_score(y_dev, y_pred_lr):0.3f}')","metadata":{"execution":{"iopub.status.busy":"2022-05-19T15:57:20.368223Z","iopub.execute_input":"2022-05-19T15:57:20.368678Z","iopub.status.idle":"2022-05-19T15:57:20.52458Z","shell.execute_reply.started":"2022-05-19T15:57:20.368633Z","shell.execute_reply":"2022-05-19T15:57:20.523754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.2. LightGBM <a id = 8> </a>","metadata":{}},{"cell_type":"code","source":"lgb_model = lgb.LGBMClassifier(\n    objective = 'binary',\n    learning_rate = 0.07,\n    num_leaves = 20,\n    n_estimators = 500, \n    max_depth = 8,\n    class_weight = 'balanced',\n    subsample = 1,\n    colsample_bytree = 1,\n    metric = 'auc',\n    device = 'gpu',\n    gpu_platform_id = 0,\n    gpu_device_id = 0,\n    random_state = 0                          \n)\n\nparam_grid_lgb = {\n    'reg_alpha': [0.1, 1],\n    'reg_lambda': [0.1, 1]\n}\n\ngrid_lgb = GridSearchCV(\n    estimator = lgb_model, \n    param_grid = param_grid_lgb,\n    scoring='roc_auc',\n    cv = 3, \n    refit = True, \n    n_jobs = -1\n)\n\ngrid_lgb.fit(X_train_scaled, y_train)\n\nprint(f\"Best parameters: {grid_lgb.best_params_}\")","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:17:59.463906Z","iopub.execute_input":"2022-05-19T16:17:59.46454Z","iopub.status.idle":"2022-05-19T16:22:39.261907Z","shell.execute_reply.started":"2022-05-19T16:17:59.464499Z","shell.execute_reply":"2022-05-19T16:22:39.261025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_lgb = grid_lgb.predict_proba(X_dev_scaled)[:, 1]\ny_pred_lgb_final = grid_lgb.predict_proba(X_test_scaled)[:, 1]\n\nprint(f'AUC score for dev dataset using LightGBM: {roc_auc_score(y_dev, y_pred_lgb):0.3f}')","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:09:21.776655Z","iopub.execute_input":"2022-05-19T16:09:21.778337Z","iopub.status.idle":"2022-05-19T16:09:30.354196Z","shell.execute_reply.started":"2022-05-19T16:09:21.7783Z","shell.execute_reply":"2022-05-19T16:09:30.352757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.3. Wrap-up and get the output <a id = 9> </a>","metadata":{}},{"cell_type":"code","source":"AUC_df = pd.DataFrame(\n    dict(\n    model = ['Logistic Regression', 'LightGBM'],\n    score = [roc_auc_score(y_dev, y_pred_lr), roc_auc_score(y_dev, y_pred_lgb)]\n    )\n)\n\n\nplt.figure(figsize = (22, 5))\nax = sns.barplot(data = AUC_df, x = 'model', y = 'score')\nax.bar_label(ax.containers[0], fontsize = 15)\nax.set_xlabel('AUC-ROC score', fontsize = 15)\nax.set_ylabel('Model', fontsize = 15)\nax.set_ylim([0.7, 0.92])","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:02:00.558487Z","iopub.execute_input":"2022-05-19T16:02:00.558757Z","iopub.status.idle":"2022-05-19T16:02:00.805553Z","shell.execute_reply.started":"2022-05-19T16:02:00.558727Z","shell.execute_reply":"2022-05-19T16:02:00.804814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output = pd.DataFrame(\n    {\n    'ID_code': [],\n    'target': []   \n    }\n)\n\noutput['ID_code'] = id_test\noutput['target'] = y_pred_lgb_final\n\noutput","metadata":{"execution":{"iopub.status.busy":"2022-05-19T15:57:20.54491Z","iopub.status.idle":"2022-05-19T15:57:20.545994Z","shell.execute_reply.started":"2022-05-19T15:57:20.545636Z","shell.execute_reply":"2022-05-19T15:57:20.545671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output.to_csv('./submission.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T15:57:20.54722Z","iopub.status.idle":"2022-05-19T15:57:20.547856Z","shell.execute_reply.started":"2022-05-19T15:57:20.547569Z","shell.execute_reply":"2022-05-19T15:57:20.547601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Conclusions <a id = 5> </a>","metadata":{}},{"cell_type":"markdown","source":"It is assumed that the optimal error rate is 0.92 (best AUC-ROC). Considering that our best AUC-ROC is 0.89, there is a considerable bias that can be avoided.</br>\nBesides these, I also just scaled the dataset and trained it with the LightGBM model without any further pre-processing. I obtained an AUC-ROC score of 0.88! In other words, all this pre-processing on the notebook hasn't really improved the score.Besides these, I also just scaled the dataset and trained it with the LightGBM model without any further pre-processing. I obtained an AUC-ROC score of 0.88! In other words, all this pre-processing on the notebook hasn't really improved the score.","metadata":{}},{"cell_type":"markdown","source":"# 6. References <a id = 6> </a>","metadata":{}},{"cell_type":"markdown","source":"- <a href = 'https://ekamperi.github.io/machine%20learning/2021/01/21/encoder-decoder-model.html'>The encoder-decoder model as a dimensionality reduction technique</a>","metadata":{}}]}