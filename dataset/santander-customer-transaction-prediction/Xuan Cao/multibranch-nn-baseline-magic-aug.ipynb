{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom tqdm import tqdm, tqdm_notebook\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\nimport gc\n\n\n# Any results you write to the current directory are saved as output.\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom keras import layers\nfrom keras import backend as K\nfrom keras import regularizers\nfrom keras.constraints import max_norm\nfrom keras.models import Sequential\nfrom keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\nfrom keras.models import load_model\nfrom keras.models import Model\nfrom keras.initializers import glorot_uniform\nfrom keras.layers import Input,Dense,Activation,ZeroPadding2D,BatchNormalization,Flatten,Conv2D,AveragePooling2D,MaxPooling2D,Dropout,concatenate\nfrom sklearn import preprocessing\n\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve\n#from sklearn.metrics import auc\nfrom sklearn.metrics import roc_auc_score\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define helper functions. auc, plot_history\ndef auc(y_true, y_pred):\n    #auc = tf.metrics.auc(y_true, y_pred)[1]\n    y_pred = y_pred.ravel()\n    y_true = y_true.ravel()\n    return roc_auc_score(y_true, y_pred)\n\ndef auc_2(y_true, y_pred):\n    return tf.py_func(roc_auc_score, (y_true, y_pred), tf.double)\n\ndef plot_history(histories, key='binary_crossentropy'):\n    plt.figure(figsize=(16,10))\n    #plt.plot([0, 1], [0, 1], 'k--')\n    for name, history in histories:\n        val = plt.plot(history.epoch, history.history['val_'+key], '--', label=name.title()+' Val')\n\n    plt.plot(history.epoch, history.history[key], color=val[0].get_color(), label=name.title()+' Train')\n\n    plt.xlabel('Epochs')\n    plt.ylabel(key.replace('_',' ').title())\n    plt.legend()\n\n    plt.xlim([0,max(history.epoch)])\n    plt.ylim([0, 0.4])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def shuffle_col_vals_fix(x1, groups):\n    group_size = x1.shape[1]//groups\n    xs = [x1[:, i*group_size:(i+1)*group_size] for i in range(groups)]\n    rand_x = np.array([np.random.choice(x1.shape[0], size=x1.shape[0], replace=False) for i in range(group_size)]).T\n    grid = np.indices(xs[0].shape)\n    rand_y = grid[1]\n    res = [x[(rand_x, rand_y)] for x in xs]\n    return np.hstack(res)\n\ndef augment_fix_fast(x,y,groups,t1=2, t0=2):\n    # In order to make the sync version augment work, the df should be the form of:\n    # var_1, var_2, var_3 | var_1_count, var_2_count, var_3_count | var_1_rolling, var_2_rolling, var_3_rolling\n    # for the example above, 3 groups of feature, groups = 3\n    xs,xn = [],[]\n    for i in range(t1):\n        mask = y>0\n        x1 = x[mask].copy()\n        x1 = shuffle_col_vals_fix(x1, groups)\n        xs.append(x1)\n\n    for i in range(t0):\n        mask = (y==0)\n        x1 = x[mask].copy()\n        x1 = shuffle_col_vals_fix(x1, groups)\n        xn.append(x1)\n\n    xs = np.vstack(xs); xn = np.vstack(xn)\n    ys = np.ones(xs.shape[0]);yn = np.zeros(xn.shape[0])\n    x = np.vstack([x,xs,xn]); y = np.concatenate([y,ys,yn])\n    return x,y","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# load data \ntrain_df = pd.read_csv('../input/train.csv')\ntest_df =  pd.read_csv(\"../input/test.csv\")\nbase_features = [x for x in train_df.columns.values.tolist() if x.startswith('var_')]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"# mark real vs fake\ntrain_df['real'] = 1\n\nfor col in base_features:\n    test_df[col] = test_df[col].map(test_df[col].value_counts())\na = test_df[base_features].min(axis=1)\n\ntest_df = pd.read_csv('../input/test.csv')\ntest_df['real'] = (a == 1).astype('int')\n\ntrain = train_df.append(test_df).reset_index(drop=True)\ndel test_df, train_df; gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"# count features\nfor col in tqdm(base_features):\n    train[col + 'size'] = train[col].map(train.loc[train.real==1, col].value_counts())\ncnt_features = [col + 'size' for col in base_features]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# magice features 1\nfor col in tqdm(base_features):\n#        train[col+'size'] = train.groupby(col)['target'].transform('size')\n    train.loc[train[col+'size']>1,col+'no_noise'] = train.loc[train[col+'size']>1,col]\nnoise1_features = [col + 'no_noise' for col in base_features]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fill NA as mean, inspired by lightgbm\ntrain[noise1_features] = train[noise1_features].fillna(train[noise1_features].mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# magice features 2\nfor col in tqdm(base_features):\n#        train[col+'size'] = train.groupby(col)['target'].transform('size')\n    train.loc[train[col+'size']>2,col+'no_noise2'] = train.loc[train[col+'size']>2,col]\nnoise2_features = [col + 'no_noise2' for col in base_features]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fill NA as mean\ntrain[noise2_features] = train[noise2_features].fillna(train[noise2_features].mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train[train['target'].notnull()]\ntest_df = train[train['target'].isnull()]\nall_features = base_features + noise1_features + noise2_features\ncols_info = [base_features, noise1_features, noise2_features]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.to_csv('train_all.csv', index=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"39098e416885d4b96182c53292355a0e49cb0086"},"cell_type":"code","source":"scaler = preprocessing.StandardScaler().fit(train_df[all_features].values)\ndf_trn = pd.DataFrame(scaler.transform(train_df[all_features].values), columns=all_features)\ndf_tst = pd.DataFrame(scaler.transform(test_df[all_features].values), columns=all_features)\ny = train_df['target'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_keras_data(dataset, cols_info):\n    X = {}\n    base_feats, noise_feats, noise2_feats = cols_info\n    X['base'] = np.reshape(np.array(dataset[base_feats].values), (-1, len(base_feats), 1))\n    X['noise1'] = np.reshape(np.array(dataset[noise_feats].values), (-1, len(noise_feats), 1))\n    X['noise2'] = np.reshape(np.array(dataset[noise2_feats].values), (-1, len(noise2_feats), 1))\n    return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#X = get_keras_data(df_trn[all_features], cols_info)\nX_test = get_keras_data(df_tst[all_features], cols_info)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3afd722cdfbd3a200f5b33dcff2fe33635d02002"},"cell_type":"code","source":"# define network structure -> 2D CNN\ndef Convnet(cols_info, classes=1):\n    base_feats, noise1_feats, noise2_feats = cols_info\n    \n    # base_feats\n    X_base_input = Input(shape=(len(base_feats), 1), name='base')\n    X_base = Dense(16)(X_base_input)\n    X_base = Activation('relu')(X_base)\n    X_base = Flatten(name='base_last')(X_base)\n    \n    # noise1\n    X_noise1_input = Input(shape=(len(noise1_feats), 1), name='noise1')\n    X_noise1 = Dense(16)(X_noise1_input)\n    X_noise1 = Activation('relu')(X_noise1)\n    X_noise1 = Flatten(name='nose1_last')(X_noise1)\n\n    # noise2\n    X_noise2_input = Input(shape=(len(noise2_feats), 1), name='noise2')\n    X_noise2 = Dense(16)(X_noise2_input)\n    X_noise2 = Activation('relu')(X_noise2)\n    X_noise2 = Flatten(name='nose2_last')(X_noise2)    \n    \n    X = concatenate([X_base, X_noise1, X_noise2])\n    X = Dense(classes, activation='sigmoid')(X)\n    \n    model = Model(inputs=[X_base_input, X_noise1_input, X_noise2_input],outputs=X)\n    \n    return model\nmodel = Convnet(cols_info)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d2579e2c0abf8be1f0bbe1eec545394475e37568"},"cell_type":"code","source":"try:\n    del train, df_tst\nexcept:\n    pass\ngc.collect()","execution_count":22,"outputs":[{"output_type":"execute_result","execution_count":22,"data":{"text/plain":"8"},"metadata":{}}]},{"metadata":{"trusted":true,"_uuid":"301805e7d06a14a7ac9087079a2eb1a839626519"},"cell_type":"code","source":"# parameters\nSEED = 2019\nn_folds = 5\ndebug_flag = True\nfolds = 5\nskf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=SEED)","execution_count":28,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"471b1116f2311ea8f757ee041ec9052aebc9ca57"},"cell_type":"code","source":"#transformed_shape = tuple([-1] + list(shape))\n#X_test = np.reshape(X_test, transformed_shape)\n\ni = 0\nresult = pd.DataFrame({\"ID_code\": test_df.ID_code.values})\nval_aucs = []\nvalid_X = train_df[['target']]\nvalid_X['predict'] = 0\nfor train_idx, val_idx in skf.split(df_trn, y):\n    if i == folds:\n        break\n    i += 1    \n    X_train, y_train = df_trn.iloc[train_idx], y[train_idx]\n    X_valid, y_valid = df_trn.iloc[val_idx], y[val_idx]\n    \n    #aug\n    X_train, y_train = augment_fix_fast(X_train.values, y_train, groups=3, t1=2, t0=2)\n    X_train = pd.DataFrame(X_train, columns=all_features)\n    \n    X_train = get_keras_data(X_train, cols_info)\n    X_valid = get_keras_data(X_valid, cols_info)\n    #X_train = np.reshape(X_train, transformed_shape)\n    #X_valid = np.reshape(X_valid, transformed_shape)\n    \n    model_name = 'NN_fold{}.h5'.format(str(i))\n    \n    model = Convnet(cols_info)\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', 'binary_crossentropy', auc_2])\n    checkpoint = ModelCheckpoint(model_name, monitor='val_auc_2', verbose=1, \n                                 save_best_only=True, mode='max', save_weights_only = True)\n    reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, \n                                       verbose=1, mode='min', epsilon=0.0001)\n    earlystop = EarlyStopping(monitor='val_auc_2', mode='max', patience=10, verbose=1)\n    history = model.fit(X_train, y_train, \n                        epochs=300, \n                        batch_size=1024 * 2, \n                        validation_data=(X_valid, y_valid), \n                        callbacks=[checkpoint, reduceLROnPlat, earlystop])\n    #train_history = pd.DataFrame(history.history)\n    #train_history.to_csv('train_profile_fold{}.csv'.format(str(i)), index=None)\n    del X_train\n    gc.collect()\n    \n    # load and predict\n    model.load_weights(model_name)\n    \n    #predict\n    y_pred_keras = model.predict(X_valid).ravel()\n    \n    # AUC\n    valid_X['predict'].iloc[val_idx] = y_pred_keras\n    \n    fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_valid, y_pred_keras)\n    auc_valid = roc_auc_score(y_valid, y_pred_keras)\n    val_aucs.append(auc_valid)\n    \n    prediction = model.predict(X_test)\n    result[\"fold{}\".format(str(i))] = prediction","execution_count":29,"outputs":[{"output_type":"stream","text":"WARNING:tensorflow:From <ipython-input-2-4c2cc91224d8>:9: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\ntf.py_func is deprecated in TF V2. Instead, use\n    tf.py_function, which takes a python function which manipulates tf eager\n    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n    an ndarray (just call tensor.numpy()) but having access to eager tensors\n    means `tf.py_function`s can use accelerators such as GPUs as well as\n    being differentiable using a gradient tape.\n    \nWARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\nTrain on 479997 samples, validate on 40001 samples\nEpoch 1/300\n479997/479997 [==============================] - 7s 15us/step - loss: 0.2590 - acc: 0.9070 - binary_crossentropy: 0.2590 - auc_2: 0.8162 - val_loss: 0.2232 - val_acc: 0.9173 - val_binary_crossentropy: 0.2232 - val_auc_2: 0.8723\n\nEpoch 00001: val_auc_2 improved from -inf to 0.87227, saving model to NN_fold1.h5\nEpoch 2/300\n479997/479997 [==============================] - 6s 12us/step - loss: 0.2138 - acc: 0.9207 - binary_crossentropy: 0.2138 - auc_2: 0.8847 - val_loss: 0.2115 - val_acc: 0.9221 - val_binary_crossentropy: 0.2115 - val_auc_2: 0.8873\n\nEpoch 00002: val_auc_2 improved from 0.87227 to 0.88729, saving model to NN_fold1.h5\nEpoch 3/300\n479997/479997 [==============================] - 6s 12us/step - loss: 0.2017 - acc: 0.9251 - binary_crossentropy: 0.2017 - auc_2: 0.8995 - val_loss: 0.2009 - val_acc: 0.9261 - val_binary_crossentropy: 0.2009 - val_auc_2: 0.9016\n\nEpoch 00003: val_auc_2 improved from 0.88729 to 0.90161, saving model to NN_fold1.h5\nEpoch 4/300\n479997/479997 [==============================] - 5s 11us/step - loss: 0.1926 - acc: 0.9286 - binary_crossentropy: 0.1926 - auc_2: 0.9091 - val_loss: 0.1933 - val_acc: 0.9281 - val_binary_crossentropy: 0.1933 - val_auc_2: 0.9086\n\nEpoch 00004: val_auc_2 improved from 0.90161 to 0.90862, saving model to NN_fold1.h5\nEpoch 5/300\n479997/479997 [==============================] - 6s 12us/step - loss: 0.1883 - acc: 0.9300 - binary_crossentropy: 0.1883 - auc_2: 0.9137 - val_loss: 0.1900 - val_acc: 0.9299 - val_binary_crossentropy: 0.1900 - val_auc_2: 0.9120\n\nEpoch 00005: val_auc_2 improved from 0.90862 to 0.91198, saving model to NN_fold1.h5\nEpoch 6/300\n479997/479997 [==============================] - 6s 12us/step - loss: 0.1851 - acc: 0.9313 - binary_crossentropy: 0.1851 - auc_2: 0.9164 - val_loss: 0.1878 - val_acc: 0.9305 - val_binary_crossentropy: 0.1878 - val_auc_2: 0.9145\n\nEpoch 00006: val_auc_2 improved from 0.91198 to 0.91446, saving model to NN_fold1.h5\nEpoch 7/300\n479997/479997 [==============================] - 6s 12us/step - loss: 0.1831 - acc: 0.9320 - binary_crossentropy: 0.1831 - auc_2: 0.9187 - val_loss: 0.1860 - val_acc: 0.9314 - val_binary_crossentropy: 0.1860 - val_auc_2: 0.9161\n\nEpoch 00007: val_auc_2 improved from 0.91446 to 0.91615, saving model to NN_fold1.h5\nEpoch 8/300\n479997/479997 [==============================] - 6s 12us/step - loss: 0.1809 - acc: 0.9328 - binary_crossentropy: 0.1809 - auc_2: 0.9204 - val_loss: 0.1841 - val_acc: 0.9321 - val_binary_crossentropy: 0.1841 - val_auc_2: 0.9179\n\nEpoch 00008: val_auc_2 improved from 0.91615 to 0.91786, saving model to NN_fold1.h5\nEpoch 9/300\n479997/479997 [==============================] - 6s 12us/step - loss: 0.1794 - acc: 0.9334 - binary_crossentropy: 0.1794 - auc_2: 0.9218 - val_loss: 0.1827 - val_acc: 0.9323 - val_binary_crossentropy: 0.1827 - val_auc_2: 0.9189\n\nEpoch 00009: val_auc_2 improved from 0.91786 to 0.91892, saving model to NN_fold1.h5\nEpoch 10/300\n479997/479997 [==============================] - 6s 12us/step - loss: 0.1781 - acc: 0.9339 - binary_crossentropy: 0.1781 - auc_2: 0.9230 - val_loss: 0.1816 - val_acc: 0.9327 - val_binary_crossentropy: 0.1816 - val_auc_2: 0.9197\n\nEpoch 00010: val_auc_2 improved from 0.91892 to 0.91971, saving model to NN_fold1.h5\nEpoch 11/300\n479997/479997 [==============================] - 6s 11us/step - loss: 0.1769 - acc: 0.9344 - binary_crossentropy: 0.1769 - auc_2: 0.9240 - val_loss: 0.1808 - val_acc: 0.9336 - val_binary_crossentropy: 0.1808 - val_auc_2: 0.9206\n\nEpoch 00011: val_auc_2 improved from 0.91971 to 0.92059, saving model to NN_fold1.h5\nEpoch 12/300\n479997/479997 [==============================] - 6s 12us/step - loss: 0.1763 - acc: 0.9346 - binary_crossentropy: 0.1763 - auc_2: 0.9249 - val_loss: 0.1800 - val_acc: 0.9334 - val_binary_crossentropy: 0.1800 - val_auc_2: 0.9211\n\nEpoch 00012: val_auc_2 improved from 0.92059 to 0.92105, saving model to NN_fold1.h5\nEpoch 13/300\n479997/479997 [==============================] - 6s 12us/step - loss: 0.1753 - acc: 0.9352 - binary_crossentropy: 0.1753 - auc_2: 0.9257 - val_loss: 0.1811 - val_acc: 0.9328 - val_binary_crossentropy: 0.1811 - val_auc_2: 0.9213\n\nEpoch 00013: val_auc_2 improved from 0.92105 to 0.92126, saving model to NN_fold1.h5\nEpoch 14/300\n479997/479997 [==============================] - 6s 12us/step - loss: 0.1747 - acc: 0.9352 - binary_crossentropy: 0.1747 - auc_2: 0.9263 - val_loss: 0.1793 - val_acc: 0.9344 - val_binary_crossentropy: 0.1793 - val_auc_2: 0.9216\n\nEpoch 00014: val_auc_2 improved from 0.92126 to 0.92156, saving model to NN_fold1.h5\nEpoch 15/300\n479997/479997 [==============================] - 6s 12us/step - loss: 0.1744 - acc: 0.9354 - binary_crossentropy: 0.1744 - auc_2: 0.9268 - val_loss: 0.1789 - val_acc: 0.9339 - val_binary_crossentropy: 0.1789 - val_auc_2: 0.9220\n\nEpoch 00015: val_auc_2 improved from 0.92156 to 0.92196, saving model to NN_fold1.h5\nEpoch 16/300\n479997/479997 [==============================] - 5s 11us/step - loss: 0.1737 - acc: 0.9357 - binary_crossentropy: 0.1737 - auc_2: 0.9272 - val_loss: 0.1788 - val_acc: 0.9338 - val_binary_crossentropy: 0.1788 - val_auc_2: 0.9225\n\nEpoch 00016: val_auc_2 improved from 0.92196 to 0.92249, saving model to NN_fold1.h5\nEpoch 17/300\n479997/479997 [==============================] - 5s 11us/step - loss: 0.1733 - acc: 0.9358 - binary_crossentropy: 0.1733 - auc_2: 0.9276 - val_loss: 0.1800 - val_acc: 0.9339 - val_binary_crossentropy: 0.1800 - val_auc_2: 0.9223\n\nEpoch 00017: val_auc_2 did not improve from 0.92249\nEpoch 18/300\n479997/479997 [==============================] - 6s 12us/step - loss: 0.1733 - acc: 0.9358 - binary_crossentropy: 0.1733 - auc_2: 0.9278 - val_loss: 0.1780 - val_acc: 0.9342 - val_binary_crossentropy: 0.1780 - val_auc_2: 0.9228\n\nEpoch 00018: val_auc_2 improved from 0.92249 to 0.92279, saving model to NN_fold1.h5\nEpoch 19/300\n479997/479997 [==============================] - 5s 11us/step - loss: 0.1724 - acc: 0.9361 - binary_crossentropy: 0.1724 - auc_2: 0.9282 - val_loss: 0.1782 - val_acc: 0.9346 - val_binary_crossentropy: 0.1782 - val_auc_2: 0.9225\n\nEpoch 00019: val_auc_2 did not improve from 0.92279\nEpoch 20/300\n479997/479997 [==============================] - 5s 11us/step - loss: 0.1723 - acc: 0.9361 - binary_crossentropy: 0.1723 - auc_2: 0.9284 - val_loss: 0.1780 - val_acc: 0.9343 - val_binary_crossentropy: 0.1780 - val_auc_2: 0.9226\n\nEpoch 00020: val_auc_2 did not improve from 0.92279\nEpoch 21/300\n479997/479997 [==============================] - 5s 11us/step - loss: 0.1720 - acc: 0.9363 - binary_crossentropy: 0.1720 - auc_2: 0.9285 - val_loss: 0.1778 - val_acc: 0.9343 - val_binary_crossentropy: 0.1778 - val_auc_2: 0.9230\n\nEpoch 00021: val_auc_2 improved from 0.92279 to 0.92301, saving model to NN_fold1.h5\nEpoch 22/300\n479997/479997 [==============================] - 6s 12us/step - loss: 0.1717 - acc: 0.9365 - binary_crossentropy: 0.1717 - auc_2: 0.9287 - val_loss: 0.1780 - val_acc: 0.9343 - val_binary_crossentropy: 0.1780 - val_auc_2: 0.9228\n\nEpoch 00022: val_auc_2 did not improve from 0.92301\nEpoch 23/300\n479997/479997 [==============================] - 6s 12us/step - loss: 0.1720 - acc: 0.9363 - binary_crossentropy: 0.1720 - auc_2: 0.9289 - val_loss: 0.1777 - val_acc: 0.9345 - val_binary_crossentropy: 0.1777 - val_auc_2: 0.9229\n\nEpoch 00023: val_auc_2 did not improve from 0.92301\nEpoch 24/300\n","name":"stdout"},{"output_type":"stream","text":"479997/479997 [==============================] - 5s 11us/step - loss: 0.1715 - acc: 0.9365 - binary_crossentropy: 0.1715 - auc_2: 0.9290 - val_loss: 0.1784 - val_acc: 0.9342 - val_binary_crossentropy: 0.1784 - val_auc_2: 0.9228\n\nEpoch 00024: val_auc_2 did not improve from 0.92301\nEpoch 25/300\n479997/479997 [==============================] - 5s 11us/step - loss: 0.1714 - acc: 0.9366 - binary_crossentropy: 0.1714 - auc_2: 0.9293 - val_loss: 0.1779 - val_acc: 0.9346 - val_binary_crossentropy: 0.1779 - val_auc_2: 0.9227\n\nEpoch 00025: val_auc_2 did not improve from 0.92301\nEpoch 26/300\n479997/479997 [==============================] - 6s 12us/step - loss: 0.1711 - acc: 0.9367 - binary_crossentropy: 0.1711 - auc_2: 0.9294 - val_loss: 0.1788 - val_acc: 0.9337 - val_binary_crossentropy: 0.1788 - val_auc_2: 0.9227\n\nEpoch 00026: val_auc_2 did not improve from 0.92301\nEpoch 27/300\n479997/479997 [==============================] - 6s 12us/step - loss: 0.1711 - acc: 0.9367 - binary_crossentropy: 0.1711 - auc_2: 0.9296 - val_loss: 0.1778 - val_acc: 0.9349 - val_binary_crossentropy: 0.1778 - val_auc_2: 0.9229\n\nEpoch 00027: val_auc_2 did not improve from 0.92301\n\nEpoch 00027: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\nEpoch 28/300\n479997/479997 [==============================] - 6s 12us/step - loss: 0.1703 - acc: 0.9370 - binary_crossentropy: 0.1703 - auc_2: 0.9301 - val_loss: 0.1778 - val_acc: 0.9345 - val_binary_crossentropy: 0.1778 - val_auc_2: 0.9229\n\nEpoch 00028: val_auc_2 did not improve from 0.92301\nEpoch 29/300\n479997/479997 [==============================] - 6s 12us/step - loss: 0.1700 - acc: 0.9372 - binary_crossentropy: 0.1700 - auc_2: 0.9302 - val_loss: 0.1777 - val_acc: 0.9344 - val_binary_crossentropy: 0.1777 - val_auc_2: 0.9230\n\nEpoch 00029: val_auc_2 improved from 0.92301 to 0.92304, saving model to NN_fold1.h5\nEpoch 30/300\n479997/479997 [==============================] - 6s 11us/step - loss: 0.1700 - acc: 0.9371 - binary_crossentropy: 0.1700 - auc_2: 0.9302 - val_loss: 0.1777 - val_acc: 0.9345 - val_binary_crossentropy: 0.1777 - val_auc_2: 0.9228\n\nEpoch 00030: val_auc_2 did not improve from 0.92304\nEpoch 31/300\n479997/479997 [==============================] - 6s 12us/step - loss: 0.1698 - acc: 0.9372 - binary_crossentropy: 0.1698 - auc_2: 0.9303 - val_loss: 0.1776 - val_acc: 0.9346 - val_binary_crossentropy: 0.1776 - val_auc_2: 0.9229\n\nEpoch 00031: val_auc_2 did not improve from 0.92304\n\nEpoch 00031: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\nEpoch 32/300\n479997/479997 [==============================] - 6s 11us/step - loss: 0.1694 - acc: 0.9374 - binary_crossentropy: 0.1694 - auc_2: 0.9307 - val_loss: 0.1776 - val_acc: 0.9348 - val_binary_crossentropy: 0.1776 - val_auc_2: 0.9230\n\nEpoch 00032: val_auc_2 did not improve from 0.92304\nEpoch 33/300\n479997/479997 [==============================] - 5s 11us/step - loss: 0.1694 - acc: 0.9373 - binary_crossentropy: 0.1694 - auc_2: 0.9308 - val_loss: 0.1775 - val_acc: 0.9347 - val_binary_crossentropy: 0.1775 - val_auc_2: 0.9230\n\nEpoch 00033: val_auc_2 did not improve from 0.92304\nEpoch 34/300\n479997/479997 [==============================] - 5s 11us/step - loss: 0.1694 - acc: 0.9374 - binary_crossentropy: 0.1694 - auc_2: 0.9307 - val_loss: 0.1775 - val_acc: 0.9346 - val_binary_crossentropy: 0.1775 - val_auc_2: 0.9229\n\nEpoch 00034: val_auc_2 did not improve from 0.92304\nEpoch 35/300\n479997/479997 [==============================] - 6s 11us/step - loss: 0.1693 - acc: 0.9374 - binary_crossentropy: 0.1693 - auc_2: 0.9307 - val_loss: 0.1778 - val_acc: 0.9346 - val_binary_crossentropy: 0.1778 - val_auc_2: 0.9229\n\nEpoch 00035: val_auc_2 did not improve from 0.92304\nEpoch 36/300\n479997/479997 [==============================] - 6s 12us/step - loss: 0.1693 - acc: 0.9374 - binary_crossentropy: 0.1693 - auc_2: 0.9309 - val_loss: 0.1775 - val_acc: 0.9348 - val_binary_crossentropy: 0.1775 - val_auc_2: 0.9229\n\nEpoch 00036: val_auc_2 did not improve from 0.92304\nEpoch 37/300\n479997/479997 [==============================] - 5s 11us/step - loss: 0.1693 - acc: 0.9373 - binary_crossentropy: 0.1693 - auc_2: 0.9308 - val_loss: 0.1775 - val_acc: 0.9346 - val_binary_crossentropy: 0.1775 - val_auc_2: 0.9229\n\nEpoch 00037: val_auc_2 did not improve from 0.92304\n\nEpoch 00037: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\nEpoch 38/300\n479997/479997 [==============================] - 5s 11us/step - loss: 0.1690 - acc: 0.9374 - binary_crossentropy: 0.1690 - auc_2: 0.9311 - val_loss: 0.1775 - val_acc: 0.9346 - val_binary_crossentropy: 0.1775 - val_auc_2: 0.9229\n\nEpoch 00038: val_auc_2 did not improve from 0.92304\nEpoch 39/300\n479997/479997 [==============================] - 5s 11us/step - loss: 0.1689 - acc: 0.9375 - binary_crossentropy: 0.1689 - auc_2: 0.9310 - val_loss: 0.1776 - val_acc: 0.9345 - val_binary_crossentropy: 0.1776 - val_auc_2: 0.9229\n\nEpoch 00039: val_auc_2 did not improve from 0.92304\nEpoch 00039: early stopping\nTrain on 479997 samples, validate on 40001 samples\nEpoch 1/300\n479997/479997 [==============================] - 6s 13us/step - loss: 0.2591 - acc: 0.9066 - binary_crossentropy: 0.2591 - auc_2: 0.8149 - val_loss: 0.2219 - val_acc: 0.9182 - val_binary_crossentropy: 0.2219 - val_auc_2: 0.8751\n\nEpoch 00001: val_auc_2 improved from -inf to 0.87512, saving model to NN_fold2.h5\nEpoch 2/300\n143360/479997 [=======>......................] - ETA: 3s - loss: 0.2179 - acc: 0.9199 - binary_crossentropy: 0.2179 - auc_2: 0.8778","name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-29-0e763942d56c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m                         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m                         callbacks=[checkpoint, reduceLROnPlat, earlystop])\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0;31m#train_history = pd.DataFrame(history.history)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m#train_history.to_csv('train_profile_fold{}.csv'.format(str(i)), index=None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"trusted":true,"_uuid":"cf12c8076b868e0f1228fd2884b14f86a87c0c0a"},"cell_type":"code","source":"for i in range(len(val_aucs)):\n    print('Fold_%d AUC: %.6f' % (i+1, val_aucs[i]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f5b28c3ad1a96e4edd711546667cbac1527d57c8"},"cell_type":"code","source":"# summary on results\nauc_mean = np.mean(val_aucs)\nauc_std = np.std(val_aucs)\nauc_all = roc_auc_score(valid_X.target, valid_X.predict)\nprint('%d-fold auc mean: %.9f, std: %.9f. All auc: %6f.' % (n_folds, auc_mean, auc_std, auc_all))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e978483f1836a293a76bcf54d27cec81905a3667"},"cell_type":"code","source":"y_all = result.values[:, 1:]\nresult['target'] = np.mean(y_all, axis = 1)\nto_submit = result[['ID_code', 'target']]\nto_submit.to_csv('NN_submission.csv', index=None)\nresult.to_csv('NN_all_prediction.csv', index=None)\nvalid_X['ID_code'] = train_df['ID_code']\nvalid_X = valid_X[['ID_code', 'target', 'predict']].to_csv('NN_oof.csv', index=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bc08d7dc3cce9901126e935471f94203e48804ea"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}