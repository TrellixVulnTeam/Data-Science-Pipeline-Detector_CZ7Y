{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Training Notebook for Santander Dataset**","metadata":{}},{"cell_type":"markdown","source":"Second Part: https://www.kaggle.com/akhilnasser/santander-customer-transaction-training-2","metadata":{}},{"cell_type":"markdown","source":"**Recommended: GPU**","metadata":{}},{"cell_type":"markdown","source":"## **1. Required Libraries & Setup**","metadata":{}},{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:44:09.088127Z","iopub.execute_input":"2021-08-16T16:44:09.088563Z","iopub.status.idle":"2021-08-16T16:44:09.834484Z","shell.execute_reply.started":"2021-08-16T16:44:09.088523Z","shell.execute_reply":"2021-08-16T16:44:09.833365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!nvcc --version","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:44:09.836612Z","iopub.execute_input":"2021-08-16T16:44:09.837092Z","iopub.status.idle":"2021-08-16T16:44:10.406311Z","shell.execute_reply.started":"2021-08-16T16:44:09.837037Z","shell.execute_reply":"2021-08-16T16:44:10.226603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# General Data Manipulation Libraries\nimport numpy as np; print('Numpy Version:', np.__version__)\nimport pandas as pd; print('Pandas Version:', pd.__version__)\n\n# Model & Helper Libraries\nimport xgboost; print('XGBoost Version:', xgboost.__version__)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold\nimport torch; print('PyTorch Version:', torch.__version__)\n\n# Plotting Tools\nimport matplotlib.pyplot as plt\nimport plotly; print('Plotly Version:', plotly.__version__)\nfrom xgboost import plot_importance\nfrom optuna.visualization import plot_optimization_history\nfrom optuna.visualization import plot_param_importances\n\n# Hyper-parameter Optimization\nimport optuna; print('Optuna Version:', optuna.__version__)","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:44:10.407969Z","iopub.status.idle":"2021-08-16T16:44:10.408466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if torch.cuda.is_available():\n    import cudf; print('cuDF Version:', cudf.__version__)","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:44:10.409677Z","iopub.status.idle":"2021-08-16T16:44:10.410155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **2. Short EDA of Data**","metadata":{}},{"cell_type":"code","source":"# Load Data\ninput_dir = '/kaggle/input/santander-customer-transaction-prediction/'\nif torch.cuda.is_available():\n    df_train = cudf.read_csv(input_dir + '/train.csv')\nelse:\n    df_train = pd.read_csv(input_dir + '/train.csv')\ndf_train","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:44:10.411138Z","iopub.status.idle":"2021-08-16T16:44:10.411543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'There are {len(df_train)} rows and {len(df_train.columns)} columns.')","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:44:10.412357Z","iopub.status.idle":"2021-08-16T16:44:10.412746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.describe()","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:44:10.413875Z","iopub.status.idle":"2021-08-16T16:44:10.414282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check for NaN values\nprint(f'Are there Nan values? {df_train.isnull().values.any()}')","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:44:10.415138Z","iopub.status.idle":"2021-08-16T16:44:10.415527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since there are no unique identifiable characteristics among the column labels we now proceed with the rest of the Data pipeline.","metadata":{}},{"cell_type":"markdown","source":"## **3. Data Preperation**","metadata":{}},{"cell_type":"code","source":"var_colums = [c for c in df_train.columns if c not in ['ID_code','target']]\nX = df_train.loc[:, var_colums]\ny = df_train.loc[:, 'target']\n\n# We are performing a 80-20 split for Training and Validation\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)\nX_train.shape, X_valid.shape, y_train.shape, y_valid.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:44:10.416597Z","iopub.status.idle":"2021-08-16T16:44:10.41701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **4. Model Setup & Training**","metadata":{}},{"cell_type":"code","source":"# View of Xgboost Parameters\nxgboost.XGBClassifier().get_params()","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:44:10.418182Z","iopub.status.idle":"2021-08-16T16:44:10.418578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **4.1 XGBoost Parameter Selection**\n\n1. Learning Rate: Weightage of each tree in the XGBoost Classifier.\n2. Maximum Depth: The maximum depth of each tree in the XGBoost Classifier.\n3. Number of Estimators: The Maximum number of trees to be created.\n4. Subsample: The sampling percentage of the Training data used to create a Tree. Each Tree is trained on a new subsample of the trainign data.\n5. Colsample By Tree: Percentage of Features to be used while building a tree in the model. Similar to Subsample. Each Tree is trained on a new subset of the original feature space.\n6. Evaluation Metric: Evaluation Metric for the model.\n7. Use Label Encoding: The target labels have to be encoded as integers startign with 0. This will be removed soon in a new release.\n8. Verbosity: Verbosity of printing messages.\n9. Early Stopping Rounds: The stopping Criteria for the training phase. If the Validation score does not improve for the specified number of iterations the training is stopped.","metadata":{}},{"cell_type":"markdown","source":"### **4.2 Cross-validation with XGBoost**\n\nRefer: <a href = 'https://blog.cambridgespark.com/hyperparameter-tuning-in-xgboost-4ff9100a3b2f'>Hyperparameter tuning in XGBoost</a>\n\nThe cross-validation function is splitting the train dataset into `nfolds` and iteratively keeps one of the folds for validation purposes. `cv` returns a table where the rows correspond to the number of boosting trees used. The 4 columns correspond to the mean and standard deviation of MAE on the validation dataset and on the train dataset.","metadata":{}},{"cell_type":"code","source":"# Model instantiation\n\n# GPU Parameter\ndevice_method = 'gpu_hist' if torch.cuda.is_available() else 'auto'\nmodel_xgboost = xgboost.XGBClassifier(learning_rate=0.1,\n                                      max_depth=5,\n                                      n_estimators=5000,\n                                      subsample=0.5,\n                                      colsample_bytree=0.5,\n                                      eval_metric='auc',\n                                      use_label_encoder=False,\n                                      tree_method = device_method,\n                                      verbosity=1)\n# Validation Set\neval_set = [(X_valid, y_valid)]\n\n# Creating the DMatrix\nd_matrix = xgboost.DMatrix(data=X_train, label=y_train)\n\nxgb_param = model_xgboost.get_xgb_params()\n\ncv_folds = 10\nearly_stopping_rounds = 10\n# Cross-validation with 10 folds\ncvresult = xgboost.cv(xgb_param, d_matrix, num_boost_round=model_xgboost.get_params()['n_estimators'], \n            nfold=cv_folds, metrics='auc', early_stopping_rounds=early_stopping_rounds, verbose_eval=True)\n\nmodel_xgboost.set_params(n_estimators=cvresult.shape[0])","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:44:10.419397Z","iopub.status.idle":"2021-08-16T16:44:10.419816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training\nmodel_xgboost.fit(X_train,\n                  y_train,\n                  early_stopping_rounds=10,\n                  eval_set=eval_set,                  \n                  verbose=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:44:10.42071Z","iopub.status.idle":"2021-08-16T16:44:10.421141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print Results\nprint(\"AUC Train Mean Score: {:.4f} with Standard Deviation {:.4f}\\nAUC Valid Mean Score: {:.4f} with Standard Deviation {:.4f}\".format(cvresult['train-auc-mean'].iloc[-1],\n                                                    cvresult['train-auc-std'].iloc[-1], cvresult['test-auc-mean'].iloc[-1], cvresult['test-auc-std'].iloc[-1]))","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:44:10.422454Z","iopub.status.idle":"2021-08-16T16:44:10.423099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print Results on Test-Data\ny_train_pred = model_xgboost.predict_proba(X_train)[:,1]\ny_valid_pred = model_xgboost.predict_proba(X_valid)[:,1]\n\nif torch.cuda.is_available():\n    y_train = y_train.to_array()\n    y_valid = y_valid.to_array()\n\nprint(\"AUC Train: {:.4f}\\nAUC Test: {:.4f}\".format(roc_auc_score(y_train, y_train_pred),\n                                                    roc_auc_score(y_valid, y_valid_pred)))","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:44:10.424267Z","iopub.status.idle":"2021-08-16T16:44:10.424904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **4.3 Plot of Results of Training**","metadata":{}},{"cell_type":"markdown","source":"We have multiple choices for ranking feature Importance.\n\nRefer: <a href = 'https://towardsdatascience.com/be-careful-when-interpreting-your-features-importance-in-xgboost-6e16132588e7'> The Multiple faces of ‘Feature importance’ in XGBoost </a>\n\n\n* Gain:  Implies the relative contribution of the corresponding feature to the model calculated by taking each feature’s contribution for each tree in the model. A higher value of this metric when compared to another feature implies it is more important for generating a prediction.\n* Coverage: Metric means the relative number of observations related to this feature. How many times is this feature used in the classification process for all constructed trees. Expressed as a percentage for all features’ cover metrics.\n* Frequency (R)/Weight (python): Percentage representing the relative number of times a particular feature occurs in the trees of the model. \n\n*The Gain is the most relevant attribute to interpret the relative importance of each feature.*","metadata":{}},{"cell_type":"code","source":"# Feature Importance Plot\nplot_importance(model_xgboost, max_num_features=15, importance_type='gain')\nplt.figure(figsize = (25, 16))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:44:10.426113Z","iopub.status.idle":"2021-08-16T16:44:10.426728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **5. Hyper-parameter Optimization**","metadata":{}},{"cell_type":"code","source":"def objective(trial, X_train, y_train, X_valid, y_valid):\n    \n    # Model Parameters to be optimized\n    xgboost_params = {\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-7, 0.3, log=True),\n        \"n_estimators\": trial.suggest_int(name=\"n_estimators\", low=100, high=2000, step=100),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 8), \n        \"subsample\": trial.suggest_categorical(name=\"subsample\", choices=[0.4, 0.5, 0.6]),\n        \"colsample_bytree\": trial.suggest_categorical(name=\"colsample_bytree\", choices=[0.4, 0.5, 0.6]),\n        \"random_state\": 1121217\n    }\n    \n    # Model Initialisation\n    model_xgboost = xgboost.XGBClassifier(eval_metric='auc', use_label_encoder=False,\n                                      tree_method = device_method, verbosity=0, **xgboost_params)\n    eval_set = [(X_valid, y_valid)]\n    \n    # Model Training\n    model_xgboost.fit(X_train, y_train, early_stopping_rounds=10, eval_set=eval_set, verbose=False)\n    \n    # Model Prediction\n    y_valid_pred = model_xgboost.predict_proba(X_valid)[:,1]\n    \n    # Optimization Metric    \n    return roc_auc_score(y_valid, y_valid_pred)","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:44:10.42826Z","iopub.status.idle":"2021-08-16T16:44:10.428913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create Study Object for Optuna\nstudy = optuna.create_study(direction=\"maximize\")\n# Optimize\nstudy.optimize(lambda trial: objective(trial, X_train, y_train, X_valid, y_valid), n_trials=100)","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:44:10.430118Z","iopub.status.idle":"2021-08-16T16:44:10.430748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Optimized roc_auc_score: {study.best_value:.5f}\")","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:44:10.431951Z","iopub.status.idle":"2021-08-16T16:44:10.432565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Best params:\")\n\nfor key, value in study.best_params.items():\n    print(f\"\\t{key}: {value}\")","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:44:10.43383Z","iopub.status.idle":"2021-08-16T16:44:10.434459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **5.1 Plots of Results**","metadata":{}},{"cell_type":"code","source":"# Check if Plotly library is available\noptuna.visualization.is_available()","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:44:10.435869Z","iopub.status.idle":"2021-08-16T16:44:10.43649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Optimization History Plot\nplot_optimization_history(study)","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:44:10.437652Z","iopub.status.idle":"2021-08-16T16:44:10.438305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot Hyperparameter Importance\nplot_param_importances(study)","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:44:10.439506Z","iopub.status.idle":"2021-08-16T16:44:10.440152Z"},"trusted":true},"execution_count":null,"outputs":[]}]}