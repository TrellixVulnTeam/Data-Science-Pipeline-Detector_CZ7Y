{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import gc\nimport os\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nimport time\nfrom multiprocessing import Pool\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\n\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.utils.data\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)\n\nwarnings.filterwarnings('ignore')\n\nPATH=\"../input/\"#santander-customer-transaction-prediction/\"\nN_SPLITS = 10\nSEED_SKF = 4221\n\n\n","execution_count":8,"outputs":[{"output_type":"stream","text":"cuda\n","name":"stdout"}]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"def merge_train_test(df_train, df_test):\n    if \"target\" not in df_test.columns.values:\n        df_test[\"target\"] = -1\n    res = pd.concat([df_train, df_test])\n    res.reset_index(inplace=True, drop=True)\n    return res\n\ndef split_train_test(df):\n    df_train = df[df[\"target\"] >= 0]\n    df_test = df[df[\"target\"] <= -1]\n    df_train.reset_index(inplace=True, drop=True)\n    df_test.reset_index(inplace=True, drop=True)\n    assert list(df_train[\"ID_code\"].values) == [f\"train_{i}\" for i in range(200000)]\n    assert list(df_test[\"ID_code\"].values) == [f\"test_{i}\" for i in range(200000)]\n    return df_train, df_test","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntrain_df = pd.read_csv(PATH+\"train.csv\")\ntest_df = pd.read_csv(PATH+\"test.csv\")","execution_count":3,"outputs":[{"output_type":"stream","text":"CPU times: user 13.7 s, sys: 1.14 s, total: 14.8 s\nWall time: 14.8 s\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CountEncoder:\n    def fit(self, series):\n        self.counts = series.groupby(series).count()\n    \n    def transform(self, series):\n        return series.map(self.counts).fillna(0).astype(np.int16)","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# separate into real and fake\n\ndf_cnt = pd.DataFrame()\nfor v in range(200):\n    sr = test_df[f\"var_{v}\"]\n    enc = CountEncoder()\n    enc.fit(sr)\n    df_cnt[f\"cnt_{v}\"] = enc.transform(sr)\ntest_df[\"target\"] = -df_cnt.min(1)  # target==-1 -> real, target==-2 -> fake\ndel df_cnt","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_merged = merge_train_test(train_df, test_df)\ndf_merged.tail()","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"            ID_code  target    var_0   ...     var_97  var_98  var_99\n399995  test_199995      -1  13.1678   ...     8.3491  1.4743 -2.3265\n399996  test_199996      -1   9.7171   ...    25.7310  1.7975 -2.0057\n399997  test_199997      -2  11.6360   ...    -4.9342  1.6797 -0.3975\n399998  test_199998      -2  13.5745   ...    26.8269  2.7603  0.3056\n399999  test_199999      -1  10.4664   ...    12.8029  0.9685 -0.6401\n\n[5 rows x 202 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID_code</th>\n      <th>target</th>\n      <th>var_0</th>\n      <th>var_1</th>\n      <th>var_10</th>\n      <th>var_100</th>\n      <th>var_101</th>\n      <th>var_102</th>\n      <th>var_103</th>\n      <th>var_104</th>\n      <th>var_105</th>\n      <th>var_106</th>\n      <th>var_107</th>\n      <th>var_108</th>\n      <th>var_109</th>\n      <th>var_11</th>\n      <th>var_110</th>\n      <th>var_111</th>\n      <th>var_112</th>\n      <th>var_113</th>\n      <th>var_114</th>\n      <th>var_115</th>\n      <th>var_116</th>\n      <th>var_117</th>\n      <th>var_118</th>\n      <th>var_119</th>\n      <th>var_12</th>\n      <th>var_120</th>\n      <th>var_121</th>\n      <th>var_122</th>\n      <th>var_123</th>\n      <th>var_124</th>\n      <th>var_125</th>\n      <th>var_126</th>\n      <th>var_127</th>\n      <th>var_128</th>\n      <th>var_129</th>\n      <th>var_13</th>\n      <th>var_130</th>\n      <th>var_131</th>\n      <th>...</th>\n      <th>var_63</th>\n      <th>var_64</th>\n      <th>var_65</th>\n      <th>var_66</th>\n      <th>var_67</th>\n      <th>var_68</th>\n      <th>var_69</th>\n      <th>var_7</th>\n      <th>var_70</th>\n      <th>var_71</th>\n      <th>var_72</th>\n      <th>var_73</th>\n      <th>var_74</th>\n      <th>var_75</th>\n      <th>var_76</th>\n      <th>var_77</th>\n      <th>var_78</th>\n      <th>var_79</th>\n      <th>var_8</th>\n      <th>var_80</th>\n      <th>var_81</th>\n      <th>var_82</th>\n      <th>var_83</th>\n      <th>var_84</th>\n      <th>var_85</th>\n      <th>var_86</th>\n      <th>var_87</th>\n      <th>var_88</th>\n      <th>var_89</th>\n      <th>var_9</th>\n      <th>var_90</th>\n      <th>var_91</th>\n      <th>var_92</th>\n      <th>var_93</th>\n      <th>var_94</th>\n      <th>var_95</th>\n      <th>var_96</th>\n      <th>var_97</th>\n      <th>var_98</th>\n      <th>var_99</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>399995</th>\n      <td>test_199995</td>\n      <td>-1</td>\n      <td>13.1678</td>\n      <td>1.0136</td>\n      <td>2.6802</td>\n      <td>0.0951</td>\n      <td>9.7517</td>\n      <td>28.6119</td>\n      <td>1.7091</td>\n      <td>13.6924</td>\n      <td>5.9843</td>\n      <td>7.0253</td>\n      <td>22.2816</td>\n      <td>14.2617</td>\n      <td>25.2567</td>\n      <td>6.1565</td>\n      <td>1.9588</td>\n      <td>6.5321</td>\n      <td>2.9930</td>\n      <td>13.3917</td>\n      <td>0.4961</td>\n      <td>-0.6465</td>\n      <td>0.2973</td>\n      <td>9.7944</td>\n      <td>3.2861</td>\n      <td>-1.2859</td>\n      <td>14.3201</td>\n      <td>29.1451</td>\n      <td>13.9596</td>\n      <td>-3.4051</td>\n      <td>13.9743</td>\n      <td>3.3887</td>\n      <td>12.2799</td>\n      <td>13.9287</td>\n      <td>4.0643</td>\n      <td>-0.3375</td>\n      <td>19.0097</td>\n      <td>17.4594</td>\n      <td>12.9933</td>\n      <td>0.0775</td>\n      <td>...</td>\n      <td>-1.3391</td>\n      <td>6.1586</td>\n      <td>-1.8552</td>\n      <td>4.7364</td>\n      <td>15.6292</td>\n      <td>5.0223</td>\n      <td>-2.6639</td>\n      <td>14.7625</td>\n      <td>11.0281</td>\n      <td>0.0306</td>\n      <td>1.8960</td>\n      <td>24.9906</td>\n      <td>32.5007</td>\n      <td>8.3094</td>\n      <td>7.6126</td>\n      <td>25.6503</td>\n      <td>7.2437</td>\n      <td>15.0479</td>\n      <td>-2.7239</td>\n      <td>7.7879</td>\n      <td>13.9172</td>\n      <td>-9.0753</td>\n      <td>4.8331</td>\n      <td>4.4553</td>\n      <td>15.6388</td>\n      <td>5.5637</td>\n      <td>4.2547</td>\n      <td>12.6840</td>\n      <td>0.0995</td>\n      <td>6.9937</td>\n      <td>-1.8135</td>\n      <td>6.8214</td>\n      <td>9.3799</td>\n      <td>11.1513</td>\n      <td>9.6868</td>\n      <td>-0.1093</td>\n      <td>23.1655</td>\n      <td>8.3491</td>\n      <td>1.4743</td>\n      <td>-2.3265</td>\n    </tr>\n    <tr>\n      <th>399996</th>\n      <td>test_199996</td>\n      <td>-1</td>\n      <td>9.7171</td>\n      <td>-9.1462</td>\n      <td>3.2618</td>\n      <td>-6.1583</td>\n      <td>20.4441</td>\n      <td>21.8421</td>\n      <td>1.7256</td>\n      <td>8.5803</td>\n      <td>4.9388</td>\n      <td>8.8325</td>\n      <td>7.6675</td>\n      <td>14.0468</td>\n      <td>17.1160</td>\n      <td>-2.0445</td>\n      <td>6.3677</td>\n      <td>5.4146</td>\n      <td>1.9411</td>\n      <td>2.6129</td>\n      <td>3.4406</td>\n      <td>3.4260</td>\n      <td>2.3695</td>\n      <td>-8.9106</td>\n      <td>-13.8603</td>\n      <td>-2.0931</td>\n      <td>13.8246</td>\n      <td>31.8407</td>\n      <td>13.8584</td>\n      <td>-3.2260</td>\n      <td>6.3128</td>\n      <td>5.8228</td>\n      <td>12.7894</td>\n      <td>12.2272</td>\n      <td>-5.2897</td>\n      <td>1.6418</td>\n      <td>21.5449</td>\n      <td>6.6547</td>\n      <td>12.9118</td>\n      <td>0.9432</td>\n      <td>...</td>\n      <td>-0.1774</td>\n      <td>5.5691</td>\n      <td>2.9599</td>\n      <td>5.3306</td>\n      <td>14.8157</td>\n      <td>5.0154</td>\n      <td>-3.0839</td>\n      <td>18.8862</td>\n      <td>11.1069</td>\n      <td>0.3795</td>\n      <td>8.5277</td>\n      <td>4.1178</td>\n      <td>41.1888</td>\n      <td>14.1070</td>\n      <td>7.8092</td>\n      <td>14.2418</td>\n      <td>4.2794</td>\n      <td>15.5445</td>\n      <td>5.0915</td>\n      <td>12.1609</td>\n      <td>15.5275</td>\n      <td>-4.6090</td>\n      <td>7.3667</td>\n      <td>8.6763</td>\n      <td>18.6180</td>\n      <td>10.0517</td>\n      <td>19.1785</td>\n      <td>6.2420</td>\n      <td>7.0502</td>\n      <td>6.3545</td>\n      <td>-28.9851</td>\n      <td>6.7225</td>\n      <td>17.3362</td>\n      <td>10.8515</td>\n      <td>16.2477</td>\n      <td>0.5921</td>\n      <td>22.7872</td>\n      <td>25.7310</td>\n      <td>1.7975</td>\n      <td>-2.0057</td>\n    </tr>\n    <tr>\n      <th>399997</th>\n      <td>test_199997</td>\n      <td>-2</td>\n      <td>11.6360</td>\n      <td>2.2769</td>\n      <td>9.8596</td>\n      <td>-3.0749</td>\n      <td>3.1074</td>\n      <td>12.0068</td>\n      <td>1.6774</td>\n      <td>11.3799</td>\n      <td>4.2380</td>\n      <td>11.1093</td>\n      <td>25.8779</td>\n      <td>14.2004</td>\n      <td>18.5040</td>\n      <td>-0.3412</td>\n      <td>0.2553</td>\n      <td>5.2585</td>\n      <td>3.1487</td>\n      <td>12.9586</td>\n      <td>3.6474</td>\n      <td>3.4343</td>\n      <td>3.0913</td>\n      <td>0.1812</td>\n      <td>-15.1198</td>\n      <td>5.7577</td>\n      <td>14.0675</td>\n      <td>30.2584</td>\n      <td>10.6134</td>\n      <td>11.0362</td>\n      <td>-1.0272</td>\n      <td>7.6308</td>\n      <td>12.1679</td>\n      <td>13.4871</td>\n      <td>6.6516</td>\n      <td>-8.1981</td>\n      <td>10.2774</td>\n      <td>13.9975</td>\n      <td>11.1849</td>\n      <td>0.5870</td>\n      <td>...</td>\n      <td>-4.8658</td>\n      <td>7.0998</td>\n      <td>2.4941</td>\n      <td>5.0471</td>\n      <td>4.4730</td>\n      <td>5.0294</td>\n      <td>-7.1495</td>\n      <td>18.3794</td>\n      <td>11.0807</td>\n      <td>0.2335</td>\n      <td>10.5464</td>\n      <td>12.5727</td>\n      <td>45.7036</td>\n      <td>23.5307</td>\n      <td>-4.1993</td>\n      <td>19.3466</td>\n      <td>5.8432</td>\n      <td>12.7485</td>\n      <td>1.6603</td>\n      <td>12.0411</td>\n      <td>15.2880</td>\n      <td>-3.4333</td>\n      <td>4.0775</td>\n      <td>-7.9256</td>\n      <td>26.2420</td>\n      <td>13.7303</td>\n      <td>3.0444</td>\n      <td>11.8603</td>\n      <td>3.0822</td>\n      <td>5.7341</td>\n      <td>-16.0234</td>\n      <td>7.1124</td>\n      <td>7.4361</td>\n      <td>10.7057</td>\n      <td>15.3976</td>\n      <td>-0.6755</td>\n      <td>6.7713</td>\n      <td>-4.9342</td>\n      <td>1.6797</td>\n      <td>-0.3975</td>\n    </tr>\n    <tr>\n      <th>399998</th>\n      <td>test_199998</td>\n      <td>-2</td>\n      <td>13.5745</td>\n      <td>-0.5134</td>\n      <td>5.5000</td>\n      <td>1.5030</td>\n      <td>12.7682</td>\n      <td>35.0019</td>\n      <td>1.4541</td>\n      <td>9.6170</td>\n      <td>4.1599</td>\n      <td>9.2192</td>\n      <td>25.7227</td>\n      <td>13.9813</td>\n      <td>16.1063</td>\n      <td>-13.1346</td>\n      <td>10.8259</td>\n      <td>4.6892</td>\n      <td>1.4154</td>\n      <td>2.5782</td>\n      <td>4.7853</td>\n      <td>-2.7854</td>\n      <td>-0.6011</td>\n      <td>16.0398</td>\n      <td>9.9258</td>\n      <td>-1.0264</td>\n      <td>14.3051</td>\n      <td>16.3168</td>\n      <td>11.5549</td>\n      <td>4.0843</td>\n      <td>12.0524</td>\n      <td>3.7198</td>\n      <td>12.5590</td>\n      <td>13.6529</td>\n      <td>-1.9074</td>\n      <td>2.4535</td>\n      <td>14.3782</td>\n      <td>4.2644</td>\n      <td>11.7188</td>\n      <td>0.1629</td>\n      <td>...</td>\n      <td>-1.3938</td>\n      <td>5.0658</td>\n      <td>2.4233</td>\n      <td>7.4078</td>\n      <td>4.2432</td>\n      <td>5.0181</td>\n      <td>-4.3118</td>\n      <td>16.8280</td>\n      <td>27.3033</td>\n      <td>0.9399</td>\n      <td>7.0138</td>\n      <td>12.4946</td>\n      <td>32.2448</td>\n      <td>21.2218</td>\n      <td>15.5314</td>\n      <td>18.9030</td>\n      <td>2.0030</td>\n      <td>15.6688</td>\n      <td>5.3208</td>\n      <td>2.3608</td>\n      <td>9.1805</td>\n      <td>-4.3252</td>\n      <td>-2.6203</td>\n      <td>-1.8899</td>\n      <td>16.7015</td>\n      <td>10.2513</td>\n      <td>1.4672</td>\n      <td>11.7386</td>\n      <td>2.0682</td>\n      <td>8.9032</td>\n      <td>-8.2107</td>\n      <td>6.7505</td>\n      <td>18.3474</td>\n      <td>11.3160</td>\n      <td>10.6454</td>\n      <td>-0.9224</td>\n      <td>11.8991</td>\n      <td>26.8269</td>\n      <td>2.7603</td>\n      <td>0.3056</td>\n    </tr>\n    <tr>\n      <th>399999</th>\n      <td>test_199999</td>\n      <td>-1</td>\n      <td>10.4664</td>\n      <td>1.8070</td>\n      <td>8.4796</td>\n      <td>0.3854</td>\n      <td>12.1250</td>\n      <td>27.8602</td>\n      <td>1.2240</td>\n      <td>9.9291</td>\n      <td>5.4120</td>\n      <td>9.7640</td>\n      <td>27.7455</td>\n      <td>14.1016</td>\n      <td>25.9169</td>\n      <td>-5.8960</td>\n      <td>3.3216</td>\n      <td>5.5989</td>\n      <td>5.0560</td>\n      <td>10.9571</td>\n      <td>1.1325</td>\n      <td>-0.7894</td>\n      <td>3.8041</td>\n      <td>23.0863</td>\n      <td>-24.5122</td>\n      <td>4.6938</td>\n      <td>13.8333</td>\n      <td>31.4476</td>\n      <td>9.4732</td>\n      <td>5.5884</td>\n      <td>18.5127</td>\n      <td>0.1348</td>\n      <td>12.5160</td>\n      <td>12.8744</td>\n      <td>1.9396</td>\n      <td>-3.0250</td>\n      <td>10.3312</td>\n      <td>2.4590</td>\n      <td>11.2080</td>\n      <td>-0.2407</td>\n      <td>...</td>\n      <td>2.0655</td>\n      <td>7.1480</td>\n      <td>-2.7835</td>\n      <td>5.1201</td>\n      <td>20.6250</td>\n      <td>5.0179</td>\n      <td>2.5964</td>\n      <td>14.4892</td>\n      <td>19.1312</td>\n      <td>0.8608</td>\n      <td>2.1197</td>\n      <td>14.4495</td>\n      <td>25.5517</td>\n      <td>18.2896</td>\n      <td>-0.6072</td>\n      <td>19.7737</td>\n      <td>8.0756</td>\n      <td>15.2295</td>\n      <td>-0.5902</td>\n      <td>8.0110</td>\n      <td>11.3898</td>\n      <td>-3.7269</td>\n      <td>-4.8575</td>\n      <td>2.0850</td>\n      <td>17.8978</td>\n      <td>7.3186</td>\n      <td>9.8698</td>\n      <td>10.1636</td>\n      <td>5.3950</td>\n      <td>7.8362</td>\n      <td>-20.9045</td>\n      <td>6.7966</td>\n      <td>9.3417</td>\n      <td>10.2155</td>\n      <td>11.5941</td>\n      <td>1.3084</td>\n      <td>7.8346</td>\n      <td>12.8029</td>\n      <td>0.9685</td>\n      <td>-0.6401</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n# count encoding\n\ncount_enc = [None] * 200\ndf_real = df_merged[df_merged[\"target\"]!=-2]\nfor v in range(200):\n    enc = CountEncoder()\n    enc.fit(df_real[f\"var_{v}\"])\n    count_enc[v] = enc.transform(df_merged[f\"var_{v}\"])\n    \nfor v in range(200):\n    df_merged[f\"cnt_{v}\"] = count_enc[v]\n\ndel df_real","execution_count":7,"outputs":[{"output_type":"stream","text":"CPU times: user 13.1 s, sys: 556 ms, total: 13.7 s\nWall time: 13.7 s\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# normalize\n\nfor v in range(200):\n    df_merged[f\"var_{v}_minmax\"] = StandardScaler().fit_transform(df_merged[f\"var_{v}\"].values.reshape(-1, 1))\n    df_merged[f\"cnt_{v}_minmax\"] = MinMaxScaler().fit_transform(df_merged[f\"cnt_{v}\"].values.reshape(-1, 1))\ndf_merged.drop(columns=[f\"var_{v}\" for v in range(200)]+[f\"cnt_{v}\" for v in range(200)], inplace=True)","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df, test_df = split_train_test(df_merged)\ntarget = train_df['target']\ngc.collect()\nprint(train_df.shape)\ntest_df.head()","execution_count":10,"outputs":[{"output_type":"stream","text":"(200000, 402)\n","name":"stdout"},{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"  ID_code  target       ...        var_199_minmax  cnt_199_minmax\n0  test_0      -2       ...             -0.521508        0.428571\n1  test_1      -2       ...             -1.697920        0.285714\n2  test_2      -2       ...             -1.909412        0.000000\n3  test_3      -1       ...             -0.088724        0.000000\n4  test_4      -2       ...             -0.566131        0.285714\n\n[5 rows x 402 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID_code</th>\n      <th>target</th>\n      <th>var_0_minmax</th>\n      <th>cnt_0_minmax</th>\n      <th>var_1_minmax</th>\n      <th>cnt_1_minmax</th>\n      <th>var_2_minmax</th>\n      <th>cnt_2_minmax</th>\n      <th>var_3_minmax</th>\n      <th>cnt_3_minmax</th>\n      <th>var_4_minmax</th>\n      <th>cnt_4_minmax</th>\n      <th>var_5_minmax</th>\n      <th>cnt_5_minmax</th>\n      <th>var_6_minmax</th>\n      <th>cnt_6_minmax</th>\n      <th>var_7_minmax</th>\n      <th>cnt_7_minmax</th>\n      <th>var_8_minmax</th>\n      <th>cnt_8_minmax</th>\n      <th>var_9_minmax</th>\n      <th>cnt_9_minmax</th>\n      <th>var_10_minmax</th>\n      <th>cnt_10_minmax</th>\n      <th>var_11_minmax</th>\n      <th>cnt_11_minmax</th>\n      <th>var_12_minmax</th>\n      <th>cnt_12_minmax</th>\n      <th>var_13_minmax</th>\n      <th>cnt_13_minmax</th>\n      <th>var_14_minmax</th>\n      <th>cnt_14_minmax</th>\n      <th>var_15_minmax</th>\n      <th>cnt_15_minmax</th>\n      <th>var_16_minmax</th>\n      <th>cnt_16_minmax</th>\n      <th>var_17_minmax</th>\n      <th>cnt_17_minmax</th>\n      <th>var_18_minmax</th>\n      <th>cnt_18_minmax</th>\n      <th>...</th>\n      <th>var_180_minmax</th>\n      <th>cnt_180_minmax</th>\n      <th>var_181_minmax</th>\n      <th>cnt_181_minmax</th>\n      <th>var_182_minmax</th>\n      <th>cnt_182_minmax</th>\n      <th>var_183_minmax</th>\n      <th>cnt_183_minmax</th>\n      <th>var_184_minmax</th>\n      <th>cnt_184_minmax</th>\n      <th>var_185_minmax</th>\n      <th>cnt_185_minmax</th>\n      <th>var_186_minmax</th>\n      <th>cnt_186_minmax</th>\n      <th>var_187_minmax</th>\n      <th>cnt_187_minmax</th>\n      <th>var_188_minmax</th>\n      <th>cnt_188_minmax</th>\n      <th>var_189_minmax</th>\n      <th>cnt_189_minmax</th>\n      <th>var_190_minmax</th>\n      <th>cnt_190_minmax</th>\n      <th>var_191_minmax</th>\n      <th>cnt_191_minmax</th>\n      <th>var_192_minmax</th>\n      <th>cnt_192_minmax</th>\n      <th>var_193_minmax</th>\n      <th>cnt_193_minmax</th>\n      <th>var_194_minmax</th>\n      <th>cnt_194_minmax</th>\n      <th>var_195_minmax</th>\n      <th>cnt_195_minmax</th>\n      <th>var_196_minmax</th>\n      <th>cnt_196_minmax</th>\n      <th>var_197_minmax</th>\n      <th>cnt_197_minmax</th>\n      <th>var_198_minmax</th>\n      <th>cnt_198_minmax</th>\n      <th>var_199_minmax</th>\n      <th>cnt_199_minmax</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>test_0</td>\n      <td>-2</td>\n      <td>0.130422</td>\n      <td>0.230769</td>\n      <td>2.325119</td>\n      <td>0.090909</td>\n      <td>0.850188</td>\n      <td>0.1875</td>\n      <td>1.287498</td>\n      <td>0.235294</td>\n      <td>0.219369</td>\n      <td>0.055556</td>\n      <td>0.340370</td>\n      <td>0.111111</td>\n      <td>0.505104</td>\n      <td>0.344828</td>\n      <td>0.505660</td>\n      <td>0.090909</td>\n      <td>0.555971</td>\n      <td>0.000000</td>\n      <td>1.006658</td>\n      <td>0.761905</td>\n      <td>-0.437376</td>\n      <td>0.1</td>\n      <td>-0.184102</td>\n      <td>0.222222</td>\n      <td>-0.282638</td>\n      <td>0.189655</td>\n      <td>-1.767273</td>\n      <td>0.000000</td>\n      <td>0.002507</td>\n      <td>0.1875</td>\n      <td>-0.178100</td>\n      <td>0.377778</td>\n      <td>-0.636837</td>\n      <td>0.1250</td>\n      <td>-1.989929</td>\n      <td>0.000000</td>\n      <td>0.041496</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>-0.343983</td>\n      <td>0.000000</td>\n      <td>0.224065</td>\n      <td>0.241379</td>\n      <td>-1.805139</td>\n      <td>0.000</td>\n      <td>-1.073747</td>\n      <td>0.3</td>\n      <td>1.997741</td>\n      <td>0.000000</td>\n      <td>0.594309</td>\n      <td>0.222222</td>\n      <td>0.097825</td>\n      <td>0.222222</td>\n      <td>-1.145956</td>\n      <td>0.000000</td>\n      <td>-0.535275</td>\n      <td>0.363636</td>\n      <td>0.930328</td>\n      <td>0.407407</td>\n      <td>-1.178258</td>\n      <td>0.076923</td>\n      <td>1.455295</td>\n      <td>0.076923</td>\n      <td>-2.269410</td>\n      <td>0.045455</td>\n      <td>-0.219363</td>\n      <td>0.6</td>\n      <td>-1.365300</td>\n      <td>0.076923</td>\n      <td>1.822121</td>\n      <td>0.10</td>\n      <td>0.379477</td>\n      <td>0.444444</td>\n      <td>1.964372</td>\n      <td>0.111111</td>\n      <td>-0.132151</td>\n      <td>0.230769</td>\n      <td>-0.521508</td>\n      <td>0.428571</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>test_1</td>\n      <td>-2</td>\n      <td>-0.703966</td>\n      <td>0.461538</td>\n      <td>0.712000</td>\n      <td>0.363636</td>\n      <td>0.224987</td>\n      <td>0.5625</td>\n      <td>-0.784449</td>\n      <td>0.470588</td>\n      <td>-1.160614</td>\n      <td>0.388889</td>\n      <td>0.133003</td>\n      <td>0.111111</td>\n      <td>0.701835</td>\n      <td>0.344828</td>\n      <td>0.612082</td>\n      <td>0.363636</td>\n      <td>-1.408266</td>\n      <td>0.076923</td>\n      <td>-1.292635</td>\n      <td>0.428571</td>\n      <td>-0.320404</td>\n      <td>0.1</td>\n      <td>0.490471</td>\n      <td>0.000000</td>\n      <td>0.471313</td>\n      <td>0.165517</td>\n      <td>-1.288023</td>\n      <td>0.181818</td>\n      <td>-0.904411</td>\n      <td>0.1875</td>\n      <td>-0.942826</td>\n      <td>0.333333</td>\n      <td>-0.904638</td>\n      <td>0.5000</td>\n      <td>1.539034</td>\n      <td>0.000000</td>\n      <td>1.771654</td>\n      <td>0.222222</td>\n      <td>...</td>\n      <td>0.599008</td>\n      <td>0.222222</td>\n      <td>1.400615</td>\n      <td>0.172414</td>\n      <td>-1.234721</td>\n      <td>0.125</td>\n      <td>-0.653158</td>\n      <td>0.2</td>\n      <td>1.337022</td>\n      <td>0.000000</td>\n      <td>-1.031481</td>\n      <td>0.111111</td>\n      <td>0.891677</td>\n      <td>0.111111</td>\n      <td>0.138503</td>\n      <td>0.333333</td>\n      <td>0.136037</td>\n      <td>0.090909</td>\n      <td>0.235932</td>\n      <td>0.259259</td>\n      <td>1.625330</td>\n      <td>0.076923</td>\n      <td>0.458483</td>\n      <td>0.384615</td>\n      <td>-0.666980</td>\n      <td>0.318182</td>\n      <td>1.702963</td>\n      <td>0.0</td>\n      <td>-0.770858</td>\n      <td>0.230769</td>\n      <td>0.430333</td>\n      <td>0.85</td>\n      <td>-0.693959</td>\n      <td>0.111111</td>\n      <td>1.043247</td>\n      <td>0.370370</td>\n      <td>1.082903</td>\n      <td>0.153846</td>\n      <td>-1.697920</td>\n      <td>0.285714</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>test_2</td>\n      <td>-2</td>\n      <td>-1.707028</td>\n      <td>0.153846</td>\n      <td>-2.158612</td>\n      <td>0.000000</td>\n      <td>-0.216359</td>\n      <td>0.5625</td>\n      <td>0.124768</td>\n      <td>0.352941</td>\n      <td>-0.502880</td>\n      <td>0.222222</td>\n      <td>1.889479</td>\n      <td>0.111111</td>\n      <td>-0.597304</td>\n      <td>0.379310</td>\n      <td>1.086202</td>\n      <td>0.090909</td>\n      <td>0.372833</td>\n      <td>0.307692</td>\n      <td>0.629023</td>\n      <td>0.285714</td>\n      <td>-0.924394</td>\n      <td>0.2</td>\n      <td>0.036017</td>\n      <td>0.000000</td>\n      <td>-1.832103</td>\n      <td>0.044828</td>\n      <td>-1.017940</td>\n      <td>0.000000</td>\n      <td>1.473784</td>\n      <td>0.0625</td>\n      <td>-0.876507</td>\n      <td>0.533333</td>\n      <td>0.218033</td>\n      <td>0.3750</td>\n      <td>1.241495</td>\n      <td>0.111111</td>\n      <td>0.759725</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>-0.067008</td>\n      <td>0.444444</td>\n      <td>-1.114955</td>\n      <td>0.137931</td>\n      <td>-0.394631</td>\n      <td>0.000</td>\n      <td>1.053316</td>\n      <td>0.0</td>\n      <td>1.259352</td>\n      <td>0.222222</td>\n      <td>0.103865</td>\n      <td>0.555556</td>\n      <td>-1.268443</td>\n      <td>0.166667</td>\n      <td>0.712406</td>\n      <td>0.333333</td>\n      <td>-0.973512</td>\n      <td>0.090909</td>\n      <td>0.441721</td>\n      <td>0.333333</td>\n      <td>-0.869365</td>\n      <td>0.076923</td>\n      <td>1.172249</td>\n      <td>0.153846</td>\n      <td>0.036107</td>\n      <td>0.363636</td>\n      <td>-0.287168</td>\n      <td>0.1</td>\n      <td>-1.597908</td>\n      <td>0.230769</td>\n      <td>1.585120</td>\n      <td>0.05</td>\n      <td>-1.725709</td>\n      <td>0.000000</td>\n      <td>-2.006472</td>\n      <td>0.074074</td>\n      <td>1.337503</td>\n      <td>0.076923</td>\n      <td>-1.909412</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>test_3</td>\n      <td>-1</td>\n      <td>-0.701662</td>\n      <td>0.230769</td>\n      <td>0.075083</td>\n      <td>0.636364</td>\n      <td>0.496960</td>\n      <td>0.3750</td>\n      <td>-0.106186</td>\n      <td>0.411765</td>\n      <td>-1.377678</td>\n      <td>0.333333</td>\n      <td>1.046537</td>\n      <td>0.111111</td>\n      <td>-0.545666</td>\n      <td>0.517241</td>\n      <td>1.177484</td>\n      <td>0.272727</td>\n      <td>0.928549</td>\n      <td>0.230769</td>\n      <td>-0.089603</td>\n      <td>0.333333</td>\n      <td>-0.067821</td>\n      <td>0.2</td>\n      <td>-0.303197</td>\n      <td>0.222222</td>\n      <td>0.154054</td>\n      <td>0.224138</td>\n      <td>1.071498</td>\n      <td>0.090909</td>\n      <td>0.546658</td>\n      <td>0.4375</td>\n      <td>0.393016</td>\n      <td>0.355556</td>\n      <td>0.278633</td>\n      <td>0.1875</td>\n      <td>-1.434007</td>\n      <td>0.111111</td>\n      <td>-1.672483</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>2.325036</td>\n      <td>0.333333</td>\n      <td>-0.188861</td>\n      <td>0.448276</td>\n      <td>0.592055</td>\n      <td>0.375</td>\n      <td>-0.806681</td>\n      <td>0.3</td>\n      <td>-0.496115</td>\n      <td>0.111111</td>\n      <td>1.616320</td>\n      <td>0.000000</td>\n      <td>-1.771100</td>\n      <td>0.000000</td>\n      <td>-0.818464</td>\n      <td>0.000000</td>\n      <td>-0.488132</td>\n      <td>0.545455</td>\n      <td>0.573143</td>\n      <td>0.333333</td>\n      <td>1.395658</td>\n      <td>0.153846</td>\n      <td>0.538404</td>\n      <td>0.076923</td>\n      <td>-0.181783</td>\n      <td>0.272727</td>\n      <td>0.063700</td>\n      <td>0.2</td>\n      <td>-0.894858</td>\n      <td>0.230769</td>\n      <td>2.310884</td>\n      <td>0.15</td>\n      <td>0.304491</td>\n      <td>0.111111</td>\n      <td>0.346488</td>\n      <td>0.333333</td>\n      <td>-0.947946</td>\n      <td>0.461538</td>\n      <td>-0.088724</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>test_4</td>\n      <td>-2</td>\n      <td>0.341126</td>\n      <td>0.153846</td>\n      <td>0.369131</td>\n      <td>0.181818</td>\n      <td>1.296046</td>\n      <td>0.1250</td>\n      <td>0.467879</td>\n      <td>0.176471</td>\n      <td>-1.218584</td>\n      <td>0.166667</td>\n      <td>-0.448353</td>\n      <td>0.000000</td>\n      <td>1.672087</td>\n      <td>0.103448</td>\n      <td>-1.734057</td>\n      <td>0.181818</td>\n      <td>0.812587</td>\n      <td>0.076923</td>\n      <td>-0.344251</td>\n      <td>0.285714</td>\n      <td>0.857384</td>\n      <td>0.0</td>\n      <td>-0.004296</td>\n      <td>0.333333</td>\n      <td>0.410281</td>\n      <td>0.162069</td>\n      <td>0.093146</td>\n      <td>0.181818</td>\n      <td>-1.246926</td>\n      <td>0.0625</td>\n      <td>0.018669</td>\n      <td>0.733333</td>\n      <td>0.993291</td>\n      <td>0.2500</td>\n      <td>1.314888</td>\n      <td>0.000000</td>\n      <td>0.351337</td>\n      <td>0.222222</td>\n      <td>...</td>\n      <td>0.261308</td>\n      <td>0.555556</td>\n      <td>0.198969</td>\n      <td>0.206897</td>\n      <td>-0.912308</td>\n      <td>0.000</td>\n      <td>0.154327</td>\n      <td>0.1</td>\n      <td>-0.939631</td>\n      <td>0.222222</td>\n      <td>2.080710</td>\n      <td>0.000000</td>\n      <td>-0.191056</td>\n      <td>0.333333</td>\n      <td>-1.254432</td>\n      <td>0.166667</td>\n      <td>-0.985247</td>\n      <td>0.181818</td>\n      <td>0.902671</td>\n      <td>0.407407</td>\n      <td>0.222537</td>\n      <td>0.153846</td>\n      <td>0.570048</td>\n      <td>0.230769</td>\n      <td>-0.434962</td>\n      <td>0.318182</td>\n      <td>0.012746</td>\n      <td>0.3</td>\n      <td>0.496773</td>\n      <td>0.307692</td>\n      <td>-0.103620</td>\n      <td>0.50</td>\n      <td>-1.368411</td>\n      <td>0.111111</td>\n      <td>-1.760723</td>\n      <td>0.111111</td>\n      <td>-0.645869</td>\n      <td>0.076923</td>\n      <td>-0.566131</td>\n      <td>0.285714</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# nn model\n\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.hidden_size = 64\n        self.relu = nn.ReLU()\n        self.conv1 = nn.Conv1d(2, self.hidden_size, kernel_size=1)\n        self.conv2 = nn.Conv1d(self.hidden_size, self.hidden_size*2, kernel_size=1)\n        self.conv3 = nn.Conv1d(self.hidden_size*2, self.hidden_size*4, kernel_size=1)\n        self.conv4 = nn.Conv1d(self.hidden_size*4, self.hidden_size*8, kernel_size=1)\n        self.conv5 = nn.Conv1d(self.hidden_size*8, self.hidden_size*16, kernel_size=1)\n        self.conv6 = nn.Conv1d(self.hidden_size*16, self.hidden_size*32, kernel_size=1)\n        \n        self.fc = nn.Linear(self.hidden_size*32*200, 2)\n        \n    def forward(self, x_):\n        x = self.conv1(x_)\n        x = self.relu(x)\n        \n        x = self.conv2(x)\n        x = self.relu(x)\n        \n        x = self.conv3(x)\n        x = self.relu(x)\n        \n        x = self.conv4(x)\n        x = self.relu(x)\n        \n        x = self.conv5(x)\n        x = self.relu(x)\n        \n        x = self.conv6(x)\n        x = self.relu(x)\n        \n        x = x.view(x.shape[0], -1)\n        x = self.fc(x)\n        return x","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dataset\n\nclass TrainData(torch.utils.data.Dataset):\n    def __init__(self, trn_X, trn_y):\n        self.trn_X = trn_X\n        self.trn_y = trn_y\n        \n    def __len__(self):\n        return self.trn_X.shape[0]\n        \n    def __getitem__(self, idx):\n        return self.trn_X[idx], self.trn_y[idx], idx\n    \n    def shuffle(self):\n        trn_X = self.trn_X.to(\"cpu\").numpy()\n        trn_y = self.trn_y.to(\"cpu\").numpy()\n        trn_X_pos = trn_X[trn_y==1].transpose(2,0,1)\n        trn_X_neg = trn_X[trn_y==0].transpose(2,0,1)\n        for c in trn_X_pos:\n            np.random.shuffle(c)\n        for c in trn_X_neg:\n            np.random.shuffle(c)\n        trn_X[trn_y==1] = trn_X_pos.transpose(1,2,0)\n        trn_X[trn_y==0] = trn_X_neg.transpose(1,2,0)\n        self.trn_X = torch.from_numpy(trn_X).to(device)\n    \nclass ValidData(torch.utils.data.Dataset):\n    def __init__(self, val_X, val_y):\n        self.val_X = val_X\n        self.val_y = val_y\n        \n    def __len__(self):\n        return self.val_X.shape[0]\n        \n    def __getitem__(self, idx):\n        return self.val_X[idx], self.val_y[idx], idx\n    \nclass TestData(torch.utils.data.Dataset):\n    def __init__(self, test_X):\n        self.test_X = test_X\n        \n    def __len__(self):\n        return self.test_X.shape[0]\n        \n    def __getitem__(self, idx):\n        return self.test_X[idx], -1, idx\n    ","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.special import logit, expit\n\nBATCH_SIZE = 256\nEARLY_STOPPING = 20\nskf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED_SKF)\noof = np.zeros(len(train_df))\nfold_oof = np.zeros((N_SPLITS, len(train_df)))\nfold_preds = np.zeros((N_SPLITS, len(test_df)))\npredictions = np.zeros(len(test_df))\n\nloss_func = nn.CrossEntropyLoss()\n\nfor fold_, (trn_idx, val_idx) in enumerate(skf.split(train_df.values, target.values)):\n    print(\"fold n°{}\".format(fold_))\n    \n    features = [f\"var_{v}_minmax\" for v in range(200)] + [f\"cnt_{v}_minmax\" for v in range(200)]\n    \n    trn_X_npy, trn_y_npy = train_df.iloc[trn_idx][features].values.astype(np.float32), target.iloc[trn_idx].values\n    val_X_npy, val_y_npy = train_df.iloc[val_idx][features].values.astype(np.float32), target.iloc[val_idx].values\n    trn_X, trn_y = torch.tensor(trn_X_npy.reshape(-1, 2, 200)).to(device), torch.tensor(trn_y_npy).to(device)       \n    val_X, val_y = torch.tensor(val_X_npy.reshape(-1, 2, 200)).to(device), torch.tensor(val_y_npy).to(device)     \n    test_X = torch.tensor(test_df[features].values.astype(np.float32).reshape(-1, 2, 200)).to(device)\n    trn_dataset = TrainData(trn_X, trn_y)\n    val_dataset = ValidData(val_X, val_y)\n    test_dataset = TestData(test_X)\n    #trn_loader = torch.utils.data.DataLoader(dataset=trn_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    val_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=256) #batch_size=len(val_idx))\n    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=256)\n    filename_net = f\"net_{fold_}.pth\"\n    \n    net = Model().to(device)\n    optimizer = optim.Adam(net.parameters(), lr=0.00002)\n    \n    best_epoch = 0\n    min_auc = 0.5\n    for epoch in range(100):\n        if epoch - EARLY_STOPPING > best_epoch:\n            break\n            \n        # train dataset with shuffling\n        trn_dataset.shuffle()\n        trn_loader = torch.utils.data.DataLoader(dataset=trn_dataset, batch_size=BATCH_SIZE, shuffle=True)\n\n        # train\n        net = net.train()\n        oof_ = np.zeros((len(trn_idx), 2), dtype=np.float32)\n\n        for data, label, idx in trn_loader:\n            optimizer.zero_grad()\n            output = net(data)\n            loss = loss_func(output, label)\n            loss.backward()\n            oof_[idx.numpy()] = output.detach().cpu().numpy()\n            optimizer.step()\n            \n        # eval\n        net = net.eval()\n        with torch.no_grad():\n            # train data\n            loss = loss_func(torch.from_numpy(oof_), torch.from_numpy(trn_y_npy))\n            auc = roc_auc_score(trn_y_npy, oof_[:, 1] - oof_[:, 0])\n            print(f\"epoch {epoch}: train loss: {loss:.5f}, train auc: {auc:.5f}, \", end=\"\")\n\n            # valid data\n            output = np.zeros((len(val_idx), 2), dtype=np.float32)\n            for data, _, idx in val_loader:\n                output[idx.numpy()] = net(data).detach().cpu().numpy()\n            loss = loss_func(torch.from_numpy(output), torch.from_numpy(val_y_npy))\n            auc = roc_auc_score(val_y_npy, output[:, 1] - output[:, 0])\n            print(f\"valid loss: {loss:.5f}, valid auc: {auc:.5f}\")\n\n            if auc > min_auc:\n                torch.save(net.state_dict(), filename_net)\n                min_auc = auc\n                best_epoch = epoch\n\n    net.load_state_dict(torch.load(filename_net))\n    output = np.zeros((len(val_idx), 2), dtype=np.float32)\n    for data, _, idx in val_loader:\n        output[idx.numpy()] = net(data).detach().cpu().numpy()\n    val_auc = roc_auc_score(val_y_npy, output[:, 1] - output[:, 0])\n    print(f\"fold {fold_} auc: {val_auc:.5f}\")\n    oof[val_idx] = expit(output[:, 1] - output[:, 0])\n    fold_oof[fold_, val_idx] = oof[val_idx]\n    \n    output = np.zeros((len(test_dataset), 2), dtype=np.float32)\n    for data, _, idx in test_loader:\n        output[idx.numpy()] = net(data).detach().cpu().numpy()\n    fold_preds[fold_, :] = expit(output[:, 1] - output[:, 0])\n    predictions += fold_preds[fold_] / N_SPLITS\n    \n    break  # due to execution time limitation\n    \n\nnp.save(\"oof.npy\", oof)\nnp.save(\"fold_oof.npy\", fold_oof)\nnp.save(\"fold_preds.npy\", fold_preds)\nnp.save(\"predictions.npy\", predictions)\nprint(\"CV score: {:<8.5f}\".format(roc_auc_score(target, oof)))","execution_count":null,"outputs":[{"output_type":"stream","text":"fold n°0\nepoch 0: train loss: 0.26154, train auc: 0.79790, valid loss: 0.21507, valid auc: 0.88499\nepoch 1: train loss: 0.20963, train auc: 0.88863, valid loss: 0.20686, valid auc: 0.89543\nepoch 2: train loss: 0.20270, train auc: 0.89609, valid loss: 0.20075, valid auc: 0.89984\nepoch 3: train loss: 0.20035, train auc: 0.89963, valid loss: 0.19880, valid auc: 0.90284\nepoch 4: train loss: 0.19658, train auc: 0.90388, valid loss: 0.19816, valid auc: 0.90618\nepoch 5: train loss: 0.19400, train auc: 0.90658, valid loss: 0.19344, valid auc: 0.90826\nepoch 6: train loss: 0.19105, train auc: 0.91015, valid loss: 0.19023, valid auc: 0.91140\nepoch 7: train loss: 0.18709, train auc: 0.91454, valid loss: 0.18625, valid auc: 0.91423\nepoch 8: train loss: 0.18678, train auc: 0.91403, valid loss: 0.19635, valid auc: 0.91646\nepoch 9: train loss: 0.18392, train auc: 0.91659, valid loss: 0.18672, valid auc: 0.91767\nepoch 10: train loss: 0.18243, train auc: 0.91798, valid loss: 0.18104, valid auc: 0.91874\nepoch 11: train loss: 0.18019, train auc: 0.92019, valid loss: 0.18363, valid auc: 0.92010\nepoch 12: train loss: 0.17933, train auc: 0.92099, valid loss: 0.18366, valid auc: 0.92023\nepoch 13: train loss: 0.17884, train auc: 0.92160, valid loss: 0.18351, valid auc: 0.92073\nepoch 14: train loss: 0.17804, train auc: 0.92230, valid loss: 0.17858, valid auc: 0.92259\nepoch 15: train loss: 0.17719, train auc: 0.92256, valid loss: 0.17710, valid auc: 0.92239\nepoch 16: train loss: 0.17602, train auc: 0.92429, valid loss: 0.17741, valid auc: 0.92278\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df = pd.DataFrame({\"ID_code\":test_df[\"ID_code\"].values})\nsub_df[\"target\"] = predictions\nsub_df.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}