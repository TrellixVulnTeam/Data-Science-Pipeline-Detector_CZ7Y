{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import gc\nimport numpy as np\nimport os\nimport pandas as pd\nimport warnings\nfrom sklearn.exceptions import ConvergenceWarning\n\npd.set_option('display.max_columns', None)\nwarnings.filterwarnings(action=\"ignore\", category=ConvergenceWarning)\nwarnings.filterwarnings(action=\"ignore\", category=UserWarning)\nwarnings.filterwarnings(action=\"ignore\", category=FutureWarning)\n\n# Utils\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nimport seaborn as sns\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import RocCurveDisplay\nfrom sklearn.model_selection import train_test_split\nfrom statsmodels.graphics.gofplots import qqplot\n\n# Normalization\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\n\n# Autoencoder\nimport tensorflow as tf\nimport tensorflow.keras.layers as L\nimport tensorflow.keras.metrics as T\nimport tensorflow.keras.models as M\nimport tensorflow.keras.optimizers as O\nimport tensorflow.keras.regularizers as R\nimport tensorflow.keras.initializers as I\nimport tensorflow.keras.callbacks as C\nimport tensorflow.keras.losses as S\n\n# Model\nfrom imblearn.over_sampling import RandomOverSampler as ROS\nimport lightgbm as lgb\nfrom sklearn.model_selection import StratifiedKFold","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-31T01:17:15.71307Z","iopub.execute_input":"2022-05-31T01:17:15.713619Z","iopub.status.idle":"2022-05-31T01:17:26.394708Z","shell.execute_reply.started":"2022-05-31T01:17:15.713461Z","shell.execute_reply":"2022-05-31T01:17:26.393378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Utils","metadata":{}},{"cell_type":"code","source":"def plot_data_stats(X):\n    fig, ax = plt.subplots(3, 1, figsize=(12, 12))\n\n    bins = X.shape[1]\n    \n    sns.distplot(X.mean(axis=0), bins=bins, color='blue', kde=True, ax=ax[0])\n    sns.distplot(X.std(axis=0), bins=bins, color='red', kde=True, ax=ax[1])\n\n    outlier = (np.abs(stats.zscore(X)) > 3) * 1\n    outlier_col_count = np.sum(outlier, axis=0)\n    X_total = X.shape[0]\n    outlier_col_perc = outlier_col_count/X_total*100\n\n    pd.DataFrame(outlier_col_perc).plot.bar(color='purple', ax=ax[2], legend=None)\n    ax[2].axes.get_xaxis().set_ticklabels([])\n    ax[2].axhline(y=outlier_col_perc.mean(), color='orange', ls='--', lw=2.0)\n\n    ax[0].set_title('Mean')\n    ax[1].set_title('Standard deviation')\n    ax[2].set_title('Z-score')\n\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-31T01:17:26.39657Z","iopub.execute_input":"2022-05-31T01:17:26.396907Z","iopub.status.idle":"2022-05-31T01:17:26.409084Z","shell.execute_reply.started":"2022-05-31T01:17:26.39687Z","shell.execute_reply":"2022-05-31T01:17:26.40748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_eval_model(Xtrain, ytrain, Xvalid, yvalid, \n                     parameters, verbose=True):\n    model = lgb.LGBMClassifier(**parameters)\n    verbose_eval = 200 if verbose else 1000\n    model.fit(\n        Xtrain, ytrain, \n        eval_set=(Xvalid, yvalid), \n        eval_metric='auc', verbose=verbose_eval)\n    ypred = model.predict(Xvalid)\n    score = roc_auc_score(yvalid, ypred)\n    \n    if verbose:\n        print(f'\\n\\nScore = {score:.5f}', end='\\n\\n')\n        print(classification_report(yvalid, np.round(ypred)))\n\n        fig, ax = plt.subplots(1, 2, figsize=(15, 6))\n        lgb.plot_metric(model, ax=ax[0])\n        \n        ypred = model.predict(Xtrain)\n        fpr, tpr, _ = roc_curve(ytrain, ypred)\n        roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr).plot(ax=ax[1], color='blue')\n        \n        ypred = model.predict(Xvalid)\n        fpr, tpr, _ = roc_curve(yvalid, ypred)\n        roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr).plot(ax=ax[1], color='orange', linestyle=\"--\")\n        \n        plt.show()\n    return score","metadata":{"execution":{"iopub.status.busy":"2022-05-31T01:17:26.410876Z","iopub.execute_input":"2022-05-31T01:17:26.411336Z","iopub.status.idle":"2022-05-31T01:17:26.439137Z","shell.execute_reply.started":"2022-05-31T01:17:26.41124Z","shell.execute_reply":"2022-05-31T01:17:26.437973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"colors = plt.rcParams['axes.prop_cycle'].by_key()['color']","metadata":{"execution":{"iopub.status.busy":"2022-05-31T01:17:26.441622Z","iopub.execute_input":"2022-05-31T01:17:26.441877Z","iopub.status.idle":"2022-05-31T01:17:26.452217Z","shell.execute_reply.started":"2022-05-31T01:17:26.441845Z","shell.execute_reply":"2022-05-31T01:17:26.450948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_loss(history, label='', n=0):\n    # Use a log scale on y-axis to show the wide range of values.\n    plt.semilogy(history.epoch, history.history['loss'], color=colors[n], label='Train ' + label)\n    plt.semilogy(history.epoch, history.history['val_loss'], color=colors[n], label='Val ' + label, linestyle=\"--\")\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')","metadata":{"execution":{"iopub.status.busy":"2022-05-31T01:17:26.453517Z","iopub.execute_input":"2022-05-31T01:17:26.453762Z","iopub.status.idle":"2022-05-31T01:17:26.467594Z","shell.execute_reply.started":"2022-05-31T01:17:26.453731Z","shell.execute_reply":"2022-05-31T01:17:26.466449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def summarize_diagnostics(history):\n    fig, ax = plt.subplots(1, 1, figsize=(15, 8))\n    try:\n        # plot loss\n        plt.subplot(211)\n        plt.title('Cross Entropy Loss')\n        plt.plot(history.history['loss'], color='blue', label='train')\n        plt.plot(history.history['val_loss'], color='orange', label='test')\n        # plot accuracy\n        plt.subplot(212)\n        plt.title('Classification Accuracy')\n        plt.plot(history.history['accuracy'], color='blue', label='train')\n        plt.plot(history.history['val_accuracy'], color='orange', label='test')\n    except:\n        pass","metadata":{"execution":{"iopub.status.busy":"2022-05-31T01:17:26.469127Z","iopub.execute_input":"2022-05-31T01:17:26.469419Z","iopub.status.idle":"2022-05-31T01:17:26.487508Z","shell.execute_reply.started":"2022-05-31T01:17:26.469379Z","shell.execute_reply":"2022-05-31T01:17:26.486401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_metrics(history):\n    fig, ax = plt.subplots(1, 1, figsize=(15, 8))\n    metrics = ['loss', 'prc', 'precision', 'recall']\n    for n, metric in enumerate(metrics):\n        name = metric.replace(\"_\",\" \").capitalize()\n        plt.subplot(2,2,n+1)\n        plt.plot(history.epoch, history.history[metric], color=colors[n], label='Train')\n        plt.plot(history.epoch, history.history['val_'+metric], color=colors[n], linestyle=\"--\", label='Val')\n        plt.xlabel('Epoch')\n        plt.ylabel(name)\n        if metric == 'loss':\n            plt.ylim([0, plt.ylim()[1]])\n        elif metric == 'auc':\n            plt.ylim([0.8,1])\n        else:\n            plt.ylim([0,1])\n    plt.legend();","metadata":{"execution":{"iopub.status.busy":"2022-05-31T01:17:26.489012Z","iopub.execute_input":"2022-05-31T01:17:26.489325Z","iopub.status.idle":"2022-05-31T01:17:26.501958Z","shell.execute_reply.started":"2022-05-31T01:17:26.489199Z","shell.execute_reply":"2022-05-31T01:17:26.500492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ResidualBlock(tf.keras.Model):\n    def __init__(self, filters, dropout):\n        super(ResidualBlock, self).__init__(name='')\n        # Layer 0\n        self.dense_0 = L.Dense(units=filters, activation='relu')\n        self.batch_0 = L.BatchNormalization()\n        self.dropt_0 = L.Dropout(dropout)\n        # Layer 1\n        self.dense_1 = L.Dense(units=filters, activation='relu')\n        self.batch_1 = L.BatchNormalization()\n        self.dropt_1 = L.Dropout(dropout)\n\n    def call(self, input_tensor, training=False):\n        x_skip = input_tensor\n        # Layer 0\n        x = self.dense_0(input_tensor)\n        x = self.batch_0(x, training=training)\n        x = self.dropt_0(x, training=training)\n        # Layer 1\n        x = self.dense_1(x)\n        x = self.batch_1(x, training=training)\n        x = self.dropt_1(x, training=training)\n        # Add Residue\n        x = L.Add()([x, x_skip])     \n        x = L.Activation('relu')(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-05-31T01:17:26.503707Z","iopub.execute_input":"2022-05-31T01:17:26.504378Z","iopub.status.idle":"2022-05-31T01:17:26.515843Z","shell.execute_reply.started":"2022-05-31T01:17:26.504341Z","shell.execute_reply":"2022-05-31T01:17:26.515092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Stop","metadata":{}},{"cell_type":"code","source":"# True == False for quick saves\nassert True == True","metadata":{"execution":{"iopub.status.busy":"2022-05-31T01:17:26.517183Z","iopub.execute_input":"2022-05-31T01:17:26.518183Z","iopub.status.idle":"2022-05-31T01:17:26.535774Z","shell.execute_reply.started":"2022-05-31T01:17:26.518123Z","shell.execute_reply":"2022-05-31T01:17:26.534525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data loading","metadata":{}},{"cell_type":"code","source":"fnames = []\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        fnames.append(os.path.join(dirname, filename))\n\ntrain = pd\\\n    .read_csv([fname for fname in fnames if 'train' in fname][0])\\\n    .drop('ID_code', axis=1)\nX, y = train.drop(columns=['target']), train['target']\n\ntest = pd\\\n    .read_csv([fname for fname in fnames if 'test' in fname][0])\ntest_id = test.ID_code\ntest = test.drop('ID_code', axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T01:17:26.538241Z","iopub.execute_input":"2022-05-31T01:17:26.538694Z","iopub.status.idle":"2022-05-31T01:17:46.705739Z","shell.execute_reply.started":"2022-05-31T01:17:26.538661Z","shell.execute_reply":"2022-05-31T01:17:46.70454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Base estimator","metadata":{}},{"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n\nparams = {\n    'objective': 'binary',\n    'metric': 'auc',\n    'n_estimators': 400,\n    'early_stopping_round': 10,\n    'learning_rate': 0.1,\n}\n\ntrain_eval_model(X_train, y_train, X_valid, y_valid, params)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T01:19:22.660722Z","iopub.execute_input":"2022-05-31T01:19:22.661667Z","iopub.status.idle":"2022-05-31T01:20:25.348501Z","shell.execute_reply.started":"2022-05-31T01:19:22.661627Z","shell.execute_reply":"2022-05-31T01:20:25.347627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data analysis","metadata":{}},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2022-05-31T01:20:25.350553Z","iopub.execute_input":"2022-05-31T01:20:25.35181Z","iopub.status.idle":"2022-05-31T01:20:25.38654Z","shell.execute_reply.started":"2022-05-31T01:20:25.351754Z","shell.execute_reply":"2022-05-31T01:20:25.38534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Normally distributed","metadata":{}},{"cell_type":"code","source":"describe = train.describe()\ncolumns_ordered_by_scale = describe.transpose().sort_values(by=['max']).index\n\ncols = 8\nrows = train.shape[1]//cols\n\nfig, ax = plt.subplots(rows, cols, figsize=(4*cols, 3*rows))\n\nfor idx, col in enumerate([\n    column for column in columns_ordered_by_scale\n    if 'var_' in column\n]):\n    sns.kdeplot(\n        data=train.loc[train.target == 0][[col]],\n        legend=False, ax=ax[idx//cols, idx%cols],\n        palette=sns.color_palette('Blues', 1)\n    )\n    sns.kdeplot(\n        data=train.loc[train.target == 1][[col]],\n        legend=False, ax=ax[idx//cols, idx%cols],\n        palette=sns.color_palette('Greens', 1)\n    )\n    sns.kdeplot(\n        data=test[[col]],\n        legend=False, ax=ax[idx//cols, idx%cols],\n        palette=sns.color_palette('Oranges', 1)\n    )\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"describe = train.describe()\ncolumns_ordered_by_scale = describe.transpose().sort_values(by=['max']).index\n\ncols = 8\nrows = train.shape[1]//cols\n\nfig, ax = plt.subplots(rows, cols, figsize=(4*cols, 3*rows))\n\nfor idx, col in enumerate([\n    column for column in columns_ordered_by_scale\n    if 'var_' in column\n]):\n    qqplot(\n        train[col], line='s',\n        marker='.', markerfacecolor='b', markeredgecolor='b', alpha=0.3,\n        ax=ax[idx//cols, idx%cols])\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"describe = train.describe()\ncolumns_ordered_by_scale = describe.transpose().sort_values(by=['max']).index\n\ncols = 8\nrows = train.shape[1]//cols\n\nfig, ax = plt.subplots(rows, cols, figsize=(4*cols, 3*rows))\n\nfor idx, col in enumerate([\n    column for column in columns_ordered_by_scale\n    if 'var_' in column\n]):\n    sns.boxplot(\n        data=train[col], ax=ax[idx//cols, idx%cols],\n        palette=sns.color_palette('Blues', 1)\n    )\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Different scales","metadata":{}},{"cell_type":"code","source":"train.describe()","metadata":{"execution":{"iopub.status.busy":"2022-05-31T01:20:25.388158Z","iopub.execute_input":"2022-05-31T01:20:25.388584Z","iopub.status.idle":"2022-05-31T01:20:27.962993Z","shell.execute_reply.started":"2022-05-31T01:20:25.388534Z","shell.execute_reply":"2022-05-31T01:20:27.9619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"min_max_scl = MinMaxScaler()\n\nX_scl = min_max_scl.fit_transform(X, y)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T01:20:27.965172Z","iopub.execute_input":"2022-05-31T01:20:27.965539Z","iopub.status.idle":"2022-05-31T01:20:28.365302Z","shell.execute_reply.started":"2022-05-31T01:20:27.965499Z","shell.execute_reply":"2022-05-31T01:20:28.364239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Estimate scaled data","metadata":{}},{"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(\n    X_scl, y, test_size=0.2, random_state=42)\n\ntrain_eval_model(X_train, y_train, X_valid, y_valid, params)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T01:20:28.366711Z","iopub.execute_input":"2022-05-31T01:20:28.367049Z","iopub.status.idle":"2022-05-31T01:21:31.037334Z","shell.execute_reply.started":"2022-05-31T01:20:28.366925Z","shell.execute_reply":"2022-05-31T01:21:31.036344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### No null values","metadata":{}},{"cell_type":"code","source":"np.sum(train.isna().sum().values)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T01:21:31.039046Z","iopub.execute_input":"2022-05-31T01:21:31.039612Z","iopub.status.idle":"2022-05-31T01:21:31.154116Z","shell.execute_reply.started":"2022-05-31T01:21:31.039564Z","shell.execute_reply":"2022-05-31T01:21:31.153369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Outliers","metadata":{}},{"cell_type":"code","source":"plot_data_stats(X_scl)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T01:21:31.155376Z","iopub.execute_input":"2022-05-31T01:21:31.155762Z","iopub.status.idle":"2022-05-31T01:21:35.346706Z","shell.execute_reply.started":"2022-05-31T01:21:31.155712Z","shell.execute_reply":"2022-05-31T01:21:35.34587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Principal component analysis","metadata":{}},{"cell_type":"code","source":"pca = PCA(n_components=0.2)\nX_pca = pca.fit_transform(X_scl, y)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T01:21:35.348002Z","iopub.execute_input":"2022-05-31T01:21:35.348412Z","iopub.status.idle":"2022-05-31T01:21:41.956626Z","shell.execute_reply.started":"2022-05-31T01:21:35.348369Z","shell.execute_reply":"2022-05-31T01:21:41.955443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_data_stats(X_pca)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T01:21:41.957959Z","iopub.execute_input":"2022-05-31T01:21:41.958336Z","iopub.status.idle":"2022-05-31T01:21:42.995337Z","shell.execute_reply.started":"2022-05-31T01:21:41.958288Z","shell.execute_reply":"2022-05-31T01:21:42.994242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Estimate PCA data","metadata":{}},{"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(\n    X_pca, y, test_size=0.2, random_state=42)\n\ntrain_eval_model(X_train, y_train, X_valid, y_valid, params)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T01:21:42.999236Z","iopub.execute_input":"2022-05-31T01:21:42.999537Z","iopub.status.idle":"2022-05-31T01:21:50.070703Z","shell.execute_reply.started":"2022-05-31T01:21:42.999498Z","shell.execute_reply":"2022-05-31T01:21:50.069267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Independent features","metadata":{}},{"cell_type":"code","source":"correlation = train.corr()\nres = correlation[correlation.abs()>0.5].fillna(0).sum().reset_index()\nres.columns = ['feature', 'correlations']\nres[res.correlations > 1]","metadata":{"execution":{"iopub.status.busy":"2022-05-31T01:21:50.072611Z","iopub.execute_input":"2022-05-31T01:21:50.072978Z","iopub.status.idle":"2022-05-31T01:22:08.866131Z","shell.execute_reply.started":"2022-05-31T01:21:50.072917Z","shell.execute_reply":"2022-05-31T01:22:08.864931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize=(10,8))\nsns.heatmap(correlation);","metadata":{"execution":{"iopub.status.busy":"2022-05-31T01:22:08.867581Z","iopub.execute_input":"2022-05-31T01:22:08.867857Z","iopub.status.idle":"2022-05-31T01:22:09.941203Z","shell.execute_reply.started":"2022-05-31T01:22:08.867825Z","shell.execute_reply":"2022-05-31T01:22:09.939996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Imbalanced dataset","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize=(8, 6))\nsns.countplot(x=y);","metadata":{"execution":{"iopub.status.busy":"2022-05-31T01:22:09.942819Z","iopub.execute_input":"2022-05-31T01:22:09.943059Z","iopub.status.idle":"2022-05-31T01:22:10.100183Z","shell.execute_reply.started":"2022-05-31T01:22:09.943027Z","shell.execute_reply":"2022-05-31T01:22:10.099066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Estimate scaled and PCA data","metadata":{}},{"cell_type":"code","source":"params = {\n    'objective': 'binary',\n    'is_unbalance': True,\n    'metric': 'auc',\n    'n_estimators': 400,\n    'early_stopping_round': 10,\n    'learning_rate': 0.1,\n}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(\n    X_pca, y, test_size=0.2, random_state=42)\n\ntrain_eval_model(X_train, y_train, X_valid, y_valid, params)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(\n    X_scl, y, test_size=0.2, random_state=42)\n\ntrain_eval_model(X_train, y_train, X_valid, y_valid, params)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_scl_pca = np.concatenate((X_scl, X_pca), axis=1)\n\nX_train, X_valid, y_train, y_valid = train_test_split(\n    X_scl_pca, y, test_size=0.2, random_state=42)\n\ntrain_eval_model(X_train, y_train, X_valid, y_valid, params)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T01:22:10.102465Z","iopub.execute_input":"2022-05-31T01:22:10.102865Z","iopub.status.idle":"2022-05-31T01:23:03.101017Z","shell.execute_reply.started":"2022-05-31T01:22:10.102813Z","shell.execute_reply":"2022-05-31T01:23:03.099675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, the combination of the min max scaled and PCA datasets  and informing the model the dataset is not balanced improve results.","metadata":{}},{"cell_type":"code","source":"plot_data_stats(X_scl_pca)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T01:23:03.103134Z","iopub.execute_input":"2022-05-31T01:23:03.103541Z","iopub.status.idle":"2022-05-31T01:23:07.515932Z","shell.execute_reply.started":"2022-05-31T01:23:03.103481Z","shell.execute_reply":"2022-05-31T01:23:07.514874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del(X_scl)\ndel(X_pca)\n\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-05-31T01:23:07.517495Z","iopub.execute_input":"2022-05-31T01:23:07.517954Z","iopub.status.idle":"2022-05-31T01:23:07.793627Z","shell.execute_reply.started":"2022-05-31T01:23:07.517921Z","shell.execute_reply":"2022-05-31T01:23:07.792942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Autoencoder","metadata":{}},{"cell_type":"markdown","source":"We will use an autoencoder in order to remove unwanted characteristics from the dataset.","metadata":{}},{"cell_type":"code","source":"input_dim = X_scl_pca.shape[1] \n\nautoencoder = tf.keras.Sequential([\n    L.Dense(64, input_shape=(input_dim, )),\n    L.BatchNormalization(),\n    L.LeakyReLU(),\n    L.Dense(32),\n    L.BatchNormalization(),\n    L.LeakyReLU(),\n    L.Dense(16),\n    L.Dense(32),\n    L.BatchNormalization(),\n    L.LeakyReLU(),\n    L.Dense(64),\n    L.BatchNormalization(),\n    L.LeakyReLU(),    \n    L.Dense(input_dim, activation = 'linear')\n])\n\nautoencoder_model = tf.keras.Model(inputs=autoencoder.input, outputs=autoencoder.output)\nautoencoder_model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-05-31T01:23:48.670506Z","iopub.execute_input":"2022-05-31T01:23:48.671201Z","iopub.status.idle":"2022-05-31T01:23:48.830386Z","shell.execute_reply.started":"2022-05-31T01:23:48.67114Z","shell.execute_reply":"2022-05-31T01:23:48.829335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"autoencoder_model.compile(loss='mse', optimizer='adam')\n\nepochs = 30\nbatch_size = 32\nsample_perc = 0.25\nsample_size = int(X.shape[0]*sample_perc)\nrandom_indexes = np.random.choice(np.arange(X.shape[0]), sample_size)\n\nautoencoder_history = autoencoder_model.fit(\n    X_scl_pca[random_indexes],\n    X_scl_pca[random_indexes],\n    batch_size=batch_size, epochs=epochs,\n    steps_per_epoch=sample_size//batch_size,\n)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T01:23:49.783591Z","iopub.execute_input":"2022-05-31T01:23:49.783883Z","iopub.status.idle":"2022-05-31T01:27:14.021441Z","shell.execute_reply.started":"2022-05-31T01:23:49.78385Z","shell.execute_reply":"2022-05-31T01:27:14.020298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The summarize diagnostics was used to determine the right amount of epochs.","metadata":{}},{"cell_type":"code","source":"summarize_diagnostics(autoencoder_history)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T01:27:14.023801Z","iopub.execute_input":"2022-05-31T01:27:14.024053Z","iopub.status.idle":"2022-05-31T01:27:14.26912Z","shell.execute_reply.started":"2022-05-31T01:27:14.024022Z","shell.execute_reply":"2022-05-31T01:27:14.268361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_enc = autoencoder_model.predict(X_scl_pca)\n\nX_train, X_valid, y_train, y_valid = train_test_split(\n    X_enc, y, test_size=0.2, random_state=42)\n\ntrain_eval_model(X_train, y_train, X_valid, y_valid, params)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T01:27:14.270793Z","iopub.execute_input":"2022-05-31T01:27:14.271352Z","iopub.status.idle":"2022-05-31T01:27:37.724091Z","shell.execute_reply.started":"2022-05-31T01:27:14.271301Z","shell.execute_reply":"2022-05-31T01:27:37.723363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Autoencoded dataset alone does not improve results.","metadata":{}},{"cell_type":"code","source":"plot_data_stats(X_enc)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T01:27:37.726132Z","iopub.execute_input":"2022-05-31T01:27:37.726588Z","iopub.status.idle":"2022-05-31T01:27:41.631943Z","shell.execute_reply.started":"2022-05-31T01:27:37.726537Z","shell.execute_reply":"2022-05-31T01:27:41.631208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_scl_pca_enc = np.concatenate((X_scl_pca, X_enc), axis=1)\n\nX_train, X_valid, y_train, y_valid = train_test_split(\n    X_scl_pca_enc, y, test_size=0.2, random_state=42)\n\ntrain_eval_model(X_train, y_train, X_valid, y_valid, params)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T01:27:41.63362Z","iopub.execute_input":"2022-05-31T01:27:41.634146Z","iopub.status.idle":"2022-05-31T01:29:13.535871Z","shell.execute_reply.started":"2022-05-31T01:27:41.634096Z","shell.execute_reply":"2022-05-31T01:29:13.535144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Neither does the combination of min max, PCA and autoencoder by a small margin. At least using LightGBM Classifier.","metadata":{}},{"cell_type":"code","source":"plot_data_stats(X_scl_pca_enc)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T01:29:13.537105Z","iopub.execute_input":"2022-05-31T01:29:13.53746Z","iopub.status.idle":"2022-05-31T01:29:22.648494Z","shell.execute_reply.started":"2022-05-31T01:29:13.537429Z","shell.execute_reply":"2022-05-31T01:29:22.646633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del(X_scl_pca)\ndel(X_enc)\n\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-05-31T01:29:22.65123Z","iopub.execute_input":"2022-05-31T01:29:22.651872Z","iopub.status.idle":"2022-05-31T01:29:23.329194Z","shell.execute_reply.started":"2022-05-31T01:29:22.651644Z","shell.execute_reply":"2022-05-31T01:29:23.325964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"X_test_scl = min_max_scl.transform(test)\nX_test_pca = pca.transform(X_test_scl)\nX_test_scl_pca = np.concatenate((X_test_scl, X_test_pca), axis=1)\nX_test_enc = autoencoder_model.predict(X_test_scl_pca)\nX_test_scl_pca_enc = np.concatenate((X_test_scl_pca, X_test_enc), axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T01:29:23.335442Z","iopub.execute_input":"2022-05-31T01:29:23.33667Z","iopub.status.idle":"2022-05-31T01:29:35.447568Z","shell.execute_reply.started":"2022-05-31T01:29:23.336619Z","shell.execute_reply":"2022-05-31T01:29:35.446517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del(X_test_scl)\ndel(X_test_pca)\ndel(X_test_scl_pca)\ndel(X_test_enc)\n\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-05-31T01:29:35.449144Z","iopub.execute_input":"2022-05-31T01:29:35.449525Z","iopub.status.idle":"2022-05-31T01:29:35.689031Z","shell.execute_reply.started":"2022-05-31T01:29:35.449467Z","shell.execute_reply":"2022-05-31T01:29:35.6878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_resampled, y_resampled = ROS().fit_resample(X_scl_pca_enc, y)\nstd_scl = StandardScaler()\n\nX_train_scaled = std_scl.fit_transform(X_resampled)\nX_test_scaled = std_scl.transform(X_test_scl_pca_enc)\n\nprediction = np.zeros(test.shape[0])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"skf = StratifiedKFold(n_splits=10, shuffle=True)\n\nparams = {\n    'objective': 'binary',\n    'metric': 'auc',\n    'is_unbalance': True,\n    'learning_rate': 0.05,\n    'boosting': 'gbdt',\n    'force_col_wise': True,\n    'feature_fraction': 0.8,\n}\n\nfor train_index, test_index in skf.split(X_train_scaled, y_resampled):\n    X_train, X_test = X_train_scaled[train_index], X_train_scaled[test_index]\n    y_train, y_test = y_resampled[train_index],    y_resampled[test_index]\n    \n    train_data = lgb.Dataset(X_train, label=y_train)\n    test_data = lgb.Dataset(X_test, label=y_test)\n    \n    model = lgb.train(\n        params, \n        train_data, \n        valid_sets=test_data, \n        num_boost_round=200, \n        early_stopping_rounds=10,\n        verbose_eval=False\n    )\n    \n    y_pred = model.predict(X_test)\n    score = roc_auc_score(y_test, y_pred)\n    \n    print(f'Score = {score:.5f}')\n    \n    prediction += model.predict(X_test_scaled)/skf.n_splits","metadata":{"execution":{"iopub.status.busy":"2022-05-31T01:38:22.92708Z","iopub.execute_input":"2022-05-31T01:38:22.927577Z","iopub.status.idle":"2022-05-31T02:19:42.227673Z","shell.execute_reply.started":"2022-05-31T01:38:22.927542Z","shell.execute_reply":"2022-05-31T02:19:42.226819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"test['target'] = prediction\ntest['ID_code'] = test_id\n\ntest[['ID_code', 'target']].to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T02:28:53.114553Z","iopub.execute_input":"2022-05-31T02:28:53.115067Z","iopub.status.idle":"2022-05-31T02:28:53.835687Z","shell.execute_reply.started":"2022-05-31T02:28:53.115014Z","shell.execute_reply":"2022-05-31T02:28:53.834822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# References\n\nhttps://www.kaggle.com/code/fatemetardasti/santander-transaction-prediction-lgbm\n\nhttps://www.kaggle.com/code/alirezahanifi/santander-customer-pca-dae-using-lr-lgbm\n\nhttps://www.kaggle.com/code/ricardopennaleite/internship-work\n\nhttps://www.tensorflow.org/tutorials/structured_data/imbalanced_data\n\nhttps://www.tensorflow.org/tutorials/customization/custom_layers","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}