{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n%matplotlib inline\nfrom matplotlib import pyplot as plt\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os, gc\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.\n","execution_count":14,"outputs":[{"output_type":"stream","text":"['santander-customer-transaction-prediction', 'list-of-fake-samples-and-public-private-lb-split']\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Funny pattern in numbers that are not perfectly round\n\nLooking at the data, something called my attention. \nIt's probably really silly, but well, I'm looking for magic in desperate places (Ok, maybe not the smartest place, I know)    \n\nThe data should have round numbers with 4 decimal places. Nonetheless, some numbers haven't got perfecly rounded, they kept lots of decimal places.  \n\nThis kernel takes a quick look at how these numbers behave.\n\n--------------------------------------\n\n(Hidden code, loading data - Takes into accout real and fake test data as found here: https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/85125#latest-509548)    "},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Loading data\n\nnClasses = 200\nfeatures = ['var_'+ str(i) for i in range(nClasses)]\n\nif not ('trainFrame' in globals()):\n    trainFrame = pd.read_csv('../input/santander-customer-transaction-prediction/train.csv')\n    testFrame = pd.read_csv('../input/santander-customer-transaction-prediction/test.csv')\n    trueIndices = np.load('../input/list-of-fake-samples-and-public-private-lb-split/real_samples_indexes.npy')\n    \n    trainData = trainFrame[features].values\n    testData = testFrame[features].values\n    realData = testData[trueIndices]\n    totalData = np.concatenate([trainData, testData], axis=0)\n    realTotal = np.concatenate([trainData, realData], axis=0)\n    targets = trainFrame['target'].values\n    posSelector = targets.astype(bool)\n    negSelector = (1-targets).astype(bool)\n    posData = trainData[posSelector]\n    negData = trainData[negSelector]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Plotting the numbers\n\nLet's plot a few vars and a general graph for the entire data."},{"metadata":{"trusted":true},"cell_type":"code","source":"#base plotting and analysing functions\n\ndef strTrunc(x):\n    return str(x).split('.')[1]\nstrTrunc = np.vectorize(strTrunc)\n\ngetLen = np.vectorize(len)\n\ndef bigIsZero(x):\n    return ((x[3] == '0') or (x[3:10] == '9'*len(range(3,10))))\nbigIsZero = np.vectorize(bigIsZero)\n\n    \ndef plotWeirdDecimals(i, data, showNormalized = False):\n    truncatedData = strTrunc(data) #decimal part of the numbers as string\n    dataLen = getLen(truncatedData) #count of decimals    \n    luniq, lcount = np.unique(dataLen, return_counts=True)\n        \n    bigNums = truncatedData[dataLen > 10] #select only the long numbers\n    #print(bigNums)\n    print('var', i, \n          '\\n\\tlong numbers are around zero?:', bigIsZero(bigNums).all(),\n          '\\n\\tunique decimal lengths:', luniq, \n          '\\n\\tunique counts:', lcount)\n    del bigNums\n\n    #generate a histogram for each decimal count for big numbers\n    bigLens = luniq[luniq > 10]\n    lenRang = range(bigLens.min(), bigLens.max()+1)   \n\n    bigByLen = [data[dataLen == l] for l in lenRang]\n    hist, bins = np.histogram(data, bins=300)\n    hist = hist / hist.max()\n    \n    hists = [np.histogram(d, bins=bins) for d in bigByLen]\n    hists1 = [h[0]/h[0].max() for h in hists]\n    bins = (bins[:-1] + bins[1:])/2\n    \n    if showNormalized == False:\n        plt.figure(figsize=(20,5))\n        for h, lab in zip(\n            [hist] + hists1, \n            ['original data'] + [str(j) + ' decimals' for j in lenRang]\n        ):\n            plt.plot(bins, h, label=lab)\n            \n        plt.legend()\n        plt.suptitle('var_'+str(i))\n        plt.show()\n    \n    if showNormalized == True:\n        hists2 = [h/hist for h in hists1]\n        plt.figure(figsize=(20,5))\n        for h, lab in zip(\n            [hist] + hists2, \n            ['original data'] + [str(j) + ' decimals' for j in lenRang]\n        ):\n            plt.plot(bins, h, label=lab)\n            \n        plt.legend()\n        plt.suptitle('var_'+str(i))\n        plt.show()\n        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Vars 0 to 2"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(0,3):    \n    data = trainData[:,i]\n    plotWeirdDecimals(i, data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Remaining vars (same behavior)"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"for i in range(3,200):    \n    data = trainData[:,i]\n    plotWeirdDecimals(i, data)\n    \n#outputs hidden - click to show","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Total data \n\nIf you take a look at all those graphs, you will see that they all follow exactly the same pattern.   \n\n- Some sudden bumps near numbers like 4, 5, 8, 10, 16, 20.5, 32, etc.\n\nHere we plot them both with their original values and normalized."},{"metadata":{"trusted":true},"cell_type":"code","source":"def plotEntire(data, name, normed):\n    flatData = data.reshape((-1,))\n    if normed == True:\n        flatData = flatData[np.logical_and(flatData>-25, flatData < 34)]\n    plotWeirdDecimals(name, flatData, normed)\n    \nplotEntire(trainData, 'entire train data', False)\nplotEntire(trainData, 'entire train data normalized', True)\nplotEntire(realData, 'entire real test data normalized', True)\nplotEntire(posData, 'entire positive targets normalized', True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Some observations\n\n## All long numbers are around zero\n\nWe can see that all numbers that are long should result in 0 at the 4th decimal place. (Results for `long numbers around zero?` are `True` for all cases)   \nI don't know how machines work deep in their calculations, but I would expect these \"accidents(?)\" to happen with a lot of numbers, not only with the ones that end with 0 at the 4th place. (Selections from different machines? Hidden codes in decimals? Conspiracy theory?)    \n\n## But not all numbers ended in zero suffer from this\n\nWe can also see, by the counts, that there are numbers with less than 4 decimals, meaning not all roundings of zero got imperfect.   \nSo, again, what is special about some numbers that they don't round?    \n\n## A very consistent pattern for all vars\n\nThe graphs are very consistent for all vars, and for the entire data.    \nEspecially when you look at the normalized graphs, you see clear steady plateaus in the pattern.   \nThis really tends to tell me that it's just a result of an underlying machine architecture, considering underflows or something like that. But it's weird.\n\nIt should be natural to expect bumps in multiples of 10 (since this increases/decreases the length of the part before the decimal point).   \nBut other places? Well, considering 4, 8, 16 and 32, we could be seeing effects of binary systems?   \nWhat about 20.5? Magic number?  \n\nPS, the higher left end of the positive targets graph is very probably due to normalization with very few numbers. (Had to cut the borders of all graphs)    \n\n## If zeros are special, how is the distribution of other endings?"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = trainData.reshape((-1,))\n\ndef getLastDigit(x):\n    x = str(x).split('.')[1]\n    if len(x) == 4:\n        return x[-1] #for regular numbers\n    elif len(x) < 4: \n        return '0' #for perfect zeros\n    else:\n        return 'L' #for long numbers\ngetLastDigit = np.vectorize(getLastDigit)\n    \nlastDigits = getLastDigit(data)\nunique, counts = np.unique(lastDigits, return_counts=True)\n\nfor u,c in zip(unique,counts):\n    print('digit',u,\":\",c,'instances')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, the numbers are evenly distributed (you can check individual vars as well)."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}