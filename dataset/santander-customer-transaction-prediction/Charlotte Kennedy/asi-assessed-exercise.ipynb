{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-05-26T22:31:09.13932Z","iopub.execute_input":"2021-05-26T22:31:09.139727Z","iopub.status.idle":"2021-05-26T22:31:09.149414Z","shell.execute_reply.started":"2021-05-26T22:31:09.139692Z","shell.execute_reply":"2021-05-26T22:31:09.148116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## A","metadata":{}},{"cell_type":"markdown","source":"Due to space issues on my computer, I directly worked on kaggle, thus the path to retrieve the dataset.","metadata":{}},{"cell_type":"code","source":"data_df = pd.read_csv(\"/kaggle/input/santander-customer-transaction-prediction/train.csv\")\nX_df = data_df.iloc[:,2:].copy()\ny_df =  data_df.iloc[:,1].copy()","metadata":{"execution":{"iopub.status.busy":"2021-05-26T22:31:09.818479Z","iopub.execute_input":"2021-05-26T22:31:09.818838Z","iopub.status.idle":"2021-05-26T22:31:17.764661Z","shell.execute_reply.started":"2021-05-26T22:31:09.818806Z","shell.execute_reply":"2021-05-26T22:31:17.763611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train_df, y_test_df = train_test_split(\n    X_df, y_df, random_state=1, test_size=0.5)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T22:31:17.766018Z","iopub.execute_input":"2021-05-26T22:31:17.766297Z","iopub.status.idle":"2021-05-26T22:31:18.13518Z","shell.execute_reply.started":"2021-05-26T22:31:17.76627Z","shell.execute_reply":"2021-05-26T22:31:18.134002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## B","metadata":{}},{"cell_type":"code","source":"label_1 = y_df.sum()\nratio = label_1/len(y_df)\nprint('Ratio of label 1 : {}'.format(ratio))","metadata":{"execution":{"iopub.status.busy":"2021-05-26T22:31:18.137159Z","iopub.execute_input":"2021-05-26T22:31:18.137474Z","iopub.status.idle":"2021-05-26T22:31:18.142988Z","shell.execute_reply.started":"2021-05-26T22:31:18.137445Z","shell.execute_reply":"2021-05-26T22:31:18.142106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Only 10% of the dataset is composed of the label 1 --> The dataset is not equilibrated. Therefore a good model should provide an accuracy above 90% as a predicting that all outputs are 0 would ensure such an accuracy.","metadata":{}},{"cell_type":"code","source":"X_df.head(2)\n","metadata":{"execution":{"iopub.status.busy":"2021-05-26T22:31:18.144142Z","iopub.execute_input":"2021-05-26T22:31:18.144433Z","iopub.status.idle":"2021-05-26T22:31:18.186445Z","shell.execute_reply.started":"2021-05-26T22:31:18.144394Z","shell.execute_reply":"2021-05-26T22:31:18.185325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The input is composed in total of 200 000 single inputs (therefore the training and testing datasets each contain 100 000 inputs). Each input has 200 features. This high number of features justifies the use of a Bayesian approach compared to other machine learning meathods. As a matter of fact, due to the class independence assumption, the model doesn't have to take into account any possible correlation between the different features and therefore needs less input data to train.","metadata":{}},{"cell_type":"markdown","source":"# 1. Bayesian Linear Regression","metadata":{}},{"cell_type":"markdown","source":"## 1A","metadata":{}},{"cell_type":"code","source":"def compute_posterior(X, y, sigma2priorweights, sigma2noise):\n    Sigma_inverse =    1/sigma2noise*X.T@X + np.diag([sigma2priorweights]*len(X[0]))\n    posterior_mu = np.linalg.solve(Sigma_inverse, X.T@y)/sigma2noise\n    posterior_Sigma =  np.linalg.solve(Sigma_inverse, np.identity(len(Sigma_inverse)))\n    return posterior_mu, posterior_Sigma","metadata":{"execution":{"iopub.status.busy":"2021-05-26T22:31:18.188002Z","iopub.execute_input":"2021-05-26T22:31:18.188341Z","iopub.status.idle":"2021-05-26T22:31:18.201288Z","shell.execute_reply.started":"2021-05-26T22:31:18.188307Z","shell.execute_reply":"2021-05-26T22:31:18.200093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_predictive(Xnew, w_posterior_mu, w_posterior_Sigma, sigma2noise):\n    y_posterior_mu = Xnew@w_posterior_mu\n    #y_posterior_Sigma = sigma2noise + Xnew@w_posterior_Sigma@Xnew.T\n    return y_posterior_mu #,y_posterior_Sigma","metadata":{"execution":{"iopub.status.busy":"2021-05-26T22:31:18.202784Z","iopub.execute_input":"2021-05-26T22:31:18.203232Z","iopub.status.idle":"2021-05-26T22:31:18.215665Z","shell.execute_reply.started":"2021-05-26T22:31:18.20319Z","shell.execute_reply":"2021-05-26T22:31:18.21449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1B","metadata":{}},{"cell_type":"markdown","source":"We first need to transform the dataframes into numpy arrays in order to be able to compute the posterior.\nThe input will then be in the right format to apply linear regression as for each input point (represented on a row of the array), the features are in different columns.","metadata":{}},{"cell_type":"code","source":"X_train = X_train.to_numpy()\ny_train = y_train_df.to_numpy()","metadata":{"execution":{"iopub.status.busy":"2021-05-26T22:31:18.217573Z","iopub.execute_input":"2021-05-26T22:31:18.217994Z","iopub.status.idle":"2021-05-26T22:31:18.235093Z","shell.execute_reply.started":"2021-05-26T22:31:18.217947Z","shell.execute_reply":"2021-05-26T22:31:18.233861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1C","metadata":{}},{"cell_type":"code","source":"sigma2noise = 1\nsigma2priorweights = 1\nw_posterior_mu, w_posterior_Sigma = compute_posterior(X_train, y_train, sigma2priorweights, sigma2noise)\n","metadata":{"execution":{"iopub.status.busy":"2021-05-26T22:31:18.237652Z","iopub.execute_input":"2021-05-26T22:31:18.237988Z","iopub.status.idle":"2021-05-26T22:31:18.430158Z","shell.execute_reply.started":"2021-05-26T22:31:18.237945Z","shell.execute_reply":"2021-05-26T22:31:18.428809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(w_posterior_Sigma)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T22:31:18.432088Z","iopub.execute_input":"2021-05-26T22:31:18.432808Z","iopub.status.idle":"2021-05-26T22:31:18.440314Z","shell.execute_reply.started":"2021-05-26T22:31:18.432748Z","shell.execute_reply":"2021-05-26T22:31:18.439109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The variance of the posterior characterizes the dispersion of the parameter. The low values show that the uncertainty on the model parameter is small. The larger values on the diagonal compared to the values in the rest of the covariance matrix strenghtens the previous assumption of uncorrelated features.","metadata":{}},{"cell_type":"markdown","source":"# 1D","metadata":{}},{"cell_type":"markdown","source":"The predicted outputs are floats between 0 and 1. Therefore, in order to classify the predictions into two different classes, one has to find the threshold dividing the predictions. If the threshold value is 0.5, the predictions are therefore classified accordingly to their distance to the labels. Therefore a prediction of 0.3 is closer to 0 and will be classified as such.","metadata":{}},{"cell_type":"code","source":"threshold = 0.5\ny_posterior_mu = compute_predictive(X_test, w_posterior_mu, w_posterior_Sigma, sigma2noise) \ny_pred = np.array(y_posterior_mu > threshold).astype(int)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T22:31:21.963105Z","iopub.execute_input":"2021-05-26T22:31:21.963723Z","iopub.status.idle":"2021-05-26T22:31:21.980638Z","shell.execute_reply.started":"2021-05-26T22:31:21.963671Z","shell.execute_reply":"2021-05-26T22:31:21.979436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, accuracy_score\ny_test = y_test_df.to_numpy()\ncm_linear_regression = confusion_matrix(y_test, y_pred)\nprint(cm_linear_regression)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T22:31:22.176501Z","iopub.execute_input":"2021-05-26T22:31:22.176878Z","iopub.status.idle":"2021-05-26T22:31:22.317987Z","shell.execute_reply.started":"2021-05-26T22:31:22.176842Z","shell.execute_reply":"2021-05-26T22:31:22.317184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"--> From the confusion matrix, we can see that the majority of the errors come from data labelled as 1 but classified as 0. (threshold = 0.5)","metadata":{}},{"cell_type":"code","source":"accuracy_linear_regression = accuracy_score(y_test, y_pred)\nprint('Accuracy of linear regression : {}'.format(accuracy_linear_regression))\nerror_rate_linear_regression = 1 - accuracy_linear_regression","metadata":{"execution":{"iopub.status.busy":"2021-05-26T22:31:53.67448Z","iopub.execute_input":"2021-05-26T22:31:53.675006Z","iopub.status.idle":"2021-05-26T22:31:53.687825Z","shell.execute_reply.started":"2021-05-26T22:31:53.67497Z","shell.execute_reply":"2021-05-26T22:31:53.6866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"--> We obtain a 90% accuracy (which is the same accuracy than if all the data was predicted as 0). This accuracy remains around 90% with a threshold from 0.3 to 1 (at 1 all the labels are classified as 0 thus the 90% accuracy).","metadata":{}},{"cell_type":"markdown","source":"# Logistic Regression","metadata":{}},{"cell_type":"markdown","source":"## 2A : Markov Chain Monte Carlo","metadata":{}},{"cell_type":"code","source":"import torch\ndef set_seed(seed: int=0):\n    np.random.seed(seed)\n    torch.manual_seed(seed)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T22:32:01.109115Z","iopub.execute_input":"2021-05-26T22:32:01.109555Z","iopub.status.idle":"2021-05-26T22:32:01.118563Z","shell.execute_reply.started":"2021-05-26T22:32:01.109516Z","shell.execute_reply":"2021-05-26T22:32:01.116774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def logistic(z):\n    return 1/(1+np.exp(-z))","metadata":{"execution":{"iopub.status.busy":"2021-05-26T22:32:01.291611Z","iopub.execute_input":"2021-05-26T22:32:01.292146Z","iopub.status.idle":"2021-05-26T22:32:01.295952Z","shell.execute_reply.started":"2021-05-26T22:32:01.292103Z","shell.execute_reply":"2021-05-26T22:32:01.295242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BernoulliLikelihood():\n    def logdensity(self, y, p):\n        return y*np.log(p) + (1-y)*np.log(1-p)\n\nclass NormalPrior():\n    def __init__(self, sigma2x):\n        self.sigma2x = sigma2x\n        \n    def logdensity(self, x):\n        return -1/(2*self.sigma2x)*x.T@x","metadata":{"execution":{"iopub.status.busy":"2021-05-26T22:32:01.476175Z","iopub.execute_input":"2021-05-26T22:32:01.476743Z","iopub.status.idle":"2021-05-26T22:32:01.484599Z","shell.execute_reply.started":"2021-05-26T22:32:01.47669Z","shell.execute_reply":"2021-05-26T22:32:01.483442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MHSampler():\n    @property\n    def samples(self):\n        return self._samples\n    @samples.getter    \n    def samples(self):\n        return np.asarray(self._samples)\n    \n    def __init__(self, initial_sample, likelihood, prior):\n        self.likelihood = likelihood\n        self.prior = prior\n        self._samples = [initial_sample]\n        self.acceptance_rate = 0\n        \n        \n    def unnormalized_logposterior(self, w, X, y):\n        log_likelihood = self.likelihood.logdensity(y, logistic((w.T@X)))\n        log_prior = self.prior.logdensity(w)\n        return (log_likelihood + log_prior)[0][0]\n\n    def step(self, X, y, step_proposal):\n        w_prev = self._samples[-1]\n        w_proposal = np.random.randn(200, 1)*step_proposal + w_prev \n        \n        log_gw_prev = self.unnormalized_logposterior(w_prev, X, y)\n        log_gw_proposal = self.unnormalized_logposterior(w_proposal, X, y)\n        acceptance_ratio = np.exp(log_gw_proposal - log_gw_prev) ###TO COMPLETE\n        \n        if acceptance_ratio >=1:\n            self._samples.append(w_proposal)\n        else:\n            u = np.random.uniform(0,1)\n            if u<=acceptance_ratio:\n                self._samples.append(w_proposal)\n                self.acceptance_rate += 1\n            else:\n                self._samples.append(w_prev)\n        return min(acceptance_ratio, 1)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T22:32:01.640622Z","iopub.execute_input":"2021-05-26T22:32:01.642026Z","iopub.status.idle":"2021-05-26T22:32:01.658265Z","shell.execute_reply.started":"2021-05-26T22:32:01.641882Z","shell.execute_reply":"2021-05-26T22:32:01.657141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"set_seed()\n\nnumber_steps = 1000\nstep_size = 10**(-3)\nlikelihood = BernoulliLikelihood()\nprior = NormalPrior(1)\n\nstarting_point = np.random.randn(200, 1)\nsampler = MHSampler(starting_point, likelihood, prior)\nfor _ in range(number_steps):\n        sampler.step(X_train.T, y_train, step_size)\nsamples_MCMC = sampler.samples","metadata":{"execution":{"iopub.status.busy":"2021-05-26T22:32:02.209103Z","iopub.execute_input":"2021-05-26T22:32:02.209515Z","iopub.status.idle":"2021-05-26T22:32:43.876526Z","shell.execute_reply.started":"2021-05-26T22:32:02.209481Z","shell.execute_reply":"2021-05-26T22:32:43.874989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2B : Variational Inference","metadata":{}},{"cell_type":"markdown","source":"To use the variational inference algorithm we need to normalize the features of the inputs. As a matter of fact, if the features aren't noramlized, the gradients can either vanish or explode over cascaded layers of a neural network.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nX_scaler = StandardScaler()\nX_scaler.fit(X_df)\nX = X_scaler.transform(X_df)\n\nfrom sklearn.model_selection import train_test_split\nX_train_norm, X_test_norm, y_train_df_norm, y_test_df_norm = train_test_split(\n    X, y_df, random_state=1, test_size=0.5)\ny_train_norm = y_train_df_norm.to_numpy()","metadata":{"execution":{"iopub.status.busy":"2021-05-26T22:32:43.878508Z","iopub.execute_input":"2021-05-26T22:32:43.879168Z","iopub.status.idle":"2021-05-26T22:32:45.477819Z","shell.execute_reply.started":"2021-05-26T22:32:43.879117Z","shell.execute_reply":"2021-05-26T22:32:45.477005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport scipy as scipy\nimport scipy.spatial\nimport time \n\nimport matplotlib \nimport matplotlib.font_manager\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nimport pandas as pd\nimport torch\nimport torch.nn as nn\n\nwarnings.filterwarnings(\"ignore\")\ndef set_seed(seed: int=0):\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    \ndef args_as_tensors(*index):\n    \"\"\"A simple decorator to convert numpy arrays to torch tensors\"\"\"\n    def decorator(method):\n        def wrapper(*args, **kwargs):\n            converted_args = [torch.tensor(a).float() \n                              if i in index and type(a) is np.ndarray else a \n                              for i, a in enumerate(args)]\n            return method(*converted_args, **kwargs)\n        return wrapper  \n    return decorator","metadata":{"execution":{"iopub.status.busy":"2021-05-26T22:32:45.479714Z","iopub.execute_input":"2021-05-26T22:32:45.480318Z","iopub.status.idle":"2021-05-26T22:32:45.489872Z","shell.execute_reply.started":"2021-05-26T22:32:45.480273Z","shell.execute_reply":"2021-05-26T22:32:45.489066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"jitter = 1e-10\n\nclass Distribution(nn.Module):  \n    pass\n\nclass Bernoulli(Distribution):\n    @args_as_tensors(1, 2)\n    def logdensity(self, y, p):\n        return y*torch.log(p+jitter) + (1-y)*torch.log(1-p+jitter)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T22:32:45.491476Z","iopub.execute_input":"2021-05-26T22:32:45.492022Z","iopub.status.idle":"2021-05-26T22:32:45.511284Z","shell.execute_reply.started":"2021-05-26T22:32:45.491991Z","shell.execute_reply":"2021-05-26T22:32:45.510229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class NormalDiagonal(Distribution):\n    @property\n    def var(self):\n        return self.logvar.exp()\n    \n    def extra_repr(self):\n        return 'train=%s' % self.train\n    \n    def __init__(self, d, train=True):\n        super(NormalDiagonal, self).__init__()\n        self.train = train\n        self.d = d\n        self.mean = nn.Parameter(torch.zeros(d), requires_grad=train)\n        self.logvar = nn.Parameter(torch.zeros(d), requires_grad=train)\n    \n    def sample(self, n=1):\n        eps = torch.randn(n,self.d,requires_grad=self.train) \n        samples = self.mean + eps*(torch.sqrt((self.var)))\n        return samples","metadata":{"execution":{"iopub.status.busy":"2021-05-26T22:32:45.512473Z","iopub.execute_input":"2021-05-26T22:32:45.512883Z","iopub.status.idle":"2021-05-26T22:32:45.525244Z","shell.execute_reply.started":"2021-05-26T22:32:45.512853Z","shell.execute_reply":"2021-05-26T22:32:45.524476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from functools import total_ordering\n\n_KL_REGISTRY = {}  # Source of truth mapping a few general (type, type) pairs to functions.\n_KL_MEMOIZE = {}  # Memoized version mapping many specific (type, type) pairs to functions.\n\n@total_ordering\nclass _Match(object):\n    __slots__ = ['types']\n\n    def __init__(self, *types):\n        self.types = types\n\n    def __eq__(self, other):\n        return self.types == other.types\n\n    def __le__(self, other):\n        for x, y in zip(self.types, other.types):\n            if not issubclass(x, y):\n                return False\n            if x is not y:\n                break\n        return True\n\ndef _dispatch_kl(type_q, type_p):\n    matches = [(super_q, super_p) for super_q, super_p in _KL_REGISTRY\n               if issubclass(type_q, super_q) and issubclass(type_p, super_p)]\n    if not matches:\n        return NotImplemented\n    left_q, left_p = min(_Match(*m) for m in matches).types\n    right_p, right_q = min(_Match(*reversed(m)) for m in matches).types\n    left_fun = _KL_REGISTRY[left_q, left_p]\n    right_fun = _KL_REGISTRY[right_q, right_p]\n    if left_fun is not right_fun:\n        logger.warning('Ambiguous kl_divergence({}, {}). Please register_kl({}, {})'.format(\n            type_q.__name__, type_p.__name__, left_q.__name__, right_p.__name__))\n    return left_fun\n\n\ndef register_kl(type_q, type_p):\n    \"\"\"\n    Decorator to register a pairwise function with kl_divergence.\n    Usage:\n\n        @register_kl(Normal, Normal)\n        def kl_normal_normal(q, p):\n            # insert implementation here\n    \"\"\"\n    if not isinstance(type_q, type) and issubclass(type_q, BaseDistribution):\n        raise TypeError('Expected type_q to be a Distribution subclass but got {}'.format(type_q))\n    if not isinstance(type_p, type) and issubclass(type_p, BaseDistribution):\n        raise TypeError('Expected type_p to be a Distribution subclass but got {}'.format(type_p))\n    \n    def decorator(fun):\n        _KL_REGISTRY[type_q, type_p] = fun\n        _KL_MEMOIZE.clear()  # reset since lookup order may have changed\n        print('KL divergence between \\'%s\\' and \\'%s\\' registered.' % (type_q.__name__, type_p.__name__))\n        return fun\n    return decorator\n\n\ndef kl_divergence(q, p):\n    r\"\"\"Compute Kullback-Leibler divergence KL(p|q) between two distributions.\"\"\"\n    try:\n        fun = _KL_MEMOIZE[type(q), type(p)]\n    except KeyError:\n        fun = _dispatch_kl(type(q), type(p))\n        _KL_MEMOIZE[type(q), type(p)] = fun\n    if fun is NotImplemented:\n        raise NotImplementedError('KL divergence for pair %s - %s not registered' % (type(q).__name__,\n                                                                                     type(p).__name__))\n    return fun(q, p)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T22:32:45.526612Z","iopub.execute_input":"2021-05-26T22:32:45.527106Z","iopub.status.idle":"2021-05-26T22:32:45.566995Z","shell.execute_reply.started":"2021-05-26T22:32:45.527072Z","shell.execute_reply":"2021-05-26T22:32:45.565717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@register_kl(NormalDiagonal, NormalDiagonal)\ndef _normaldiagonal_normaldiagonal(q, p):\n    kl = (1/2) * torch.sum(p.logvar - q.logvar + (q.var + (q.mean - p.mean)**2)/p.var - 1)\n    return kl","metadata":{"execution":{"iopub.status.busy":"2021-05-26T22:32:45.568405Z","iopub.execute_input":"2021-05-26T22:32:45.568809Z","iopub.status.idle":"2021-05-26T22:32:45.588007Z","shell.execute_reply.started":"2021-05-26T22:32:45.568775Z","shell.execute_reply":"2021-05-26T22:32:45.587005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def logistic_tensor(z):\n    return 1/(1 + torch.exp(-z))\n\n\nclass LogisticRegression(nn.Module):\n    def __init__(self, input_dim):\n        super(LogisticRegression, self).__init__()\n        \n        self.prior_w = NormalDiagonal(input_dim)## *** TO COMPLETE *** ##\n        self.posterior_w = NormalDiagonal(input_dim) ## *** TO COMPLETE *** ##\n        \n    @args_as_tensors(1)\n    def predict_y(self, X, mc_samples=1):\n        w_samples = self.posterior_w.sample(mc_samples)\n        w_samples=torch.unsqueeze(w_samples, dim=2)\n        y_samples = logistic_tensor(X @ w_samples)\n        \n        return y_samples","metadata":{"execution":{"iopub.status.busy":"2021-05-26T22:32:45.590156Z","iopub.execute_input":"2021-05-26T22:32:45.590485Z","iopub.status.idle":"2021-05-26T22:32:45.604064Z","shell.execute_reply.started":"2021-05-26T22:32:45.590453Z","shell.execute_reply":"2021-05-26T22:32:45.602873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class VariationalObjective(nn.Module):    \n    def __init__(self, model, likelihood, N, mc_samples):\n        super(VariationalObjective, self).__init__()\n        self.N = N\n        self.model = model\n        self.likelihood = likelihood\n        self.mc_samples = mc_samples\n        \n    def expected_loglikelihood(self, Xbatch, ybatch):\n        ypred = model.predict_y(Xbatch, self.mc_samples)\n        logliks = self.likelihood.logdensity(ybatch, ypred)\n        return - (self.N/(self.mc_samples*Xbatch.shape[0]))*torch.sum(logliks)\n\n    \n    def kl(self):\n        return kl_divergence(self.model.posterior_w, self.model.prior_w) \n    \n    def compute_objective(self, Xbatch, ybatch):\n        return self.expected_loglikelihood(Xbatch, ybatch) + self.kl() ","metadata":{"execution":{"iopub.status.busy":"2021-05-26T22:32:45.605753Z","iopub.execute_input":"2021-05-26T22:32:45.606061Z","iopub.status.idle":"2021-05-26T22:32:45.619861Z","shell.execute_reply.started":"2021-05-26T22:32:45.606021Z","shell.execute_reply":"2021-05-26T22:32:45.618741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Dataset():\n    def __init__(self, X, y, minibatch_size):\n        self.X = X\n        self.y = y \n        self.minibatch_size = min(minibatch_size, len(self.X))\n        self._i = 0  \n    def next_batch(self):  \n        if len(self.X) <= self._i + self.minibatch_size:\n            shuffle = np.random.permutation(len(self.X))\n            self.X = self.X[shuffle]\n            self.y = self.y[shuffle]\n            Xbatch = self.X[self._i:]\n            ybatch = self.y[self._i:]\n            self._i = 0\n            return Xbatch, ybatch\n\n        Xbatch = self.X[self._i:self._i + self.minibatch_size]\n        ybatch = self.y[self._i:self._i + self.minibatch_size]\n        self._i += self.minibatch_size\n        return Xbatch, ybatch","metadata":{"execution":{"iopub.status.busy":"2021-05-26T22:32:45.621522Z","iopub.execute_input":"2021-05-26T22:32:45.621983Z","iopub.status.idle":"2021-05-26T22:32:45.639164Z","shell.execute_reply.started":"2021-05-26T22:32:45.62195Z","shell.execute_reply":"2021-05-26T22:32:45.638008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Summary:\n    @property\n    def data(self):\n        data = pd.DataFrame(self._data, columns=['step', self.name, 'time'])\n        data.time = data.time - data.time.iloc[0]\n        return data\n    \n    def __init__(self, name):\n        \"\"\"A simple class to store some values during optimization\"\"\"\n        self.name = str(name)\n        self._data = []\n    \n    def append(self, step, value):\n        self._data.append([step, float(value.detach().numpy()), time.time()])","metadata":{"execution":{"iopub.status.busy":"2021-05-26T22:32:45.640947Z","iopub.execute_input":"2021-05-26T22:32:45.6415Z","iopub.status.idle":"2021-05-26T22:32:45.653815Z","shell.execute_reply.started":"2021-05-26T22:32:45.641453Z","shell.execute_reply":"2021-05-26T22:32:45.652657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr = 10**(-6)\nbatch_size = 100\n\ndataset = Dataset(X_train_norm, y_train_norm, minibatch_size = batch_size)\nlikelihood = Bernoulli()\nmodel = LogisticRegression(len(X_train_norm[0]))\n\nnelbo = VariationalObjective(model, likelihood, len(X_train_norm[0]), mc_samples = 1000)\nnelbo_summary = Summary('nelbo')\nnll_summary = Summary('expected_loglik')\nkl_summary = Summary('kl')\nfrom IPython.display import clear_output\noptim = torch.optim.SGD(filter(lambda p: p.requires_grad,model.parameters()), lr = lr) #SGD\nnum_iterations = 1000\n\nfor step in range(num_iterations):\n    optim.zero_grad()\n    Xbatch, ybatch = dataset.next_batch()\n    loss = nelbo.compute_objective(Xbatch, ybatch)\n\n    nelbo_summary.append(step, loss)\n    nll_summary.append(step, loss - nelbo.kl())\n    kl_summary.append(step, nelbo.kl())\n\n    loss.backward()\n    optim.step()\n    clear_output()\n    print('Epoch [{}/{}], Loss: {:.4f}'.format(step, num_iterations, loss.item()))\n","metadata":{"execution":{"iopub.status.busy":"2021-05-26T22:32:45.655146Z","iopub.execute_input":"2021-05-26T22:32:45.655478Z","iopub.status.idle":"2021-05-26T22:34:21.932404Z","shell.execute_reply.started":"2021-05-26T22:32:45.655446Z","shell.execute_reply":"2021-05-26T22:34:21.931481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2C : Predictive distribution","metadata":{}},{"cell_type":"markdown","source":"#### Prediction for MCMC","metadata":{}},{"cell_type":"code","source":"def predict(x_new, w_samples):\n    p = 0\n    for w in w_samples:\n        p += logistic(w.T @ x_new)\n    return p / len(w_samples)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T22:34:21.933958Z","iopub.execute_input":"2021-05-26T22:34:21.934409Z","iopub.status.idle":"2021-05-26T22:34:21.94038Z","shell.execute_reply.started":"2021-05-26T22:34:21.934344Z","shell.execute_reply":"2021-05-26T22:34:21.939326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_mcmc = np.array(predict(X_test.T, samples_MCMC))","metadata":{"execution":{"iopub.status.busy":"2021-05-26T22:34:21.942061Z","iopub.execute_input":"2021-05-26T22:34:21.94249Z","iopub.status.idle":"2021-05-26T22:35:02.374408Z","shell.execute_reply.started":"2021-05-26T22:34:21.942445Z","shell.execute_reply":"2021-05-26T22:35:02.373011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"log_likelihood_mcmc = BernoulliLikelihood().logdensity(y_test, predictions_mcmc[0])\nlog_likelihood_mcmc_no_nan = [x for x in log_likelihood_mcmc if (~np.isnan(x) and ~np.isinf(x))]\nmean_log_likelihood_mcmc = np.mean(log_likelihood_mcmc_no_nan)\nprint('Mean loglikelihood for MCMC : {}'.format(mean_log_likelihood_mcmc))\n\ny_pred_mcmc = np.array(predictions_mcmc > 0.5).astype(int)\nacc_mcmc = accuracy_score(y_test, y_pred_mcmc[0])\nerror_rate_mcmc = 1 - acc_mcmc\nprint('Error rate for MCMC : {}'.format(error_rate_mcmc))\n\ncm_mcmc = confusion_matrix(y_test, y_pred_mcmc[0])\nprint(cm_mcmc)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T22:35:02.376289Z","iopub.execute_input":"2021-05-26T22:35:02.377037Z","iopub.status.idle":"2021-05-26T22:35:03.328314Z","shell.execute_reply.started":"2021-05-26T22:35:02.376973Z","shell.execute_reply":"2021-05-26T22:35:03.327466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Predictions for variational inference","metadata":{}},{"cell_type":"code","source":"predictions_vi = model.predict_y(X_test_norm).detach().numpy()","metadata":{"execution":{"iopub.status.busy":"2021-05-26T22:35:03.330035Z","iopub.execute_input":"2021-05-26T22:35:03.330555Z","iopub.status.idle":"2021-05-26T22:35:03.491219Z","shell.execute_reply.started":"2021-05-26T22:35:03.330509Z","shell.execute_reply":"2021-05-26T22:35:03.490256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"log_likelihood_vi = Bernoulli().logdensity(y_test, predictions_vi[0].T[0]).detach().numpy()\nlog_likelihood_vi_no_nan = [x for x in log_likelihood_vi if (~np.isnan(x) and ~np.isinf(x))]\nsum_l = np.sum(log_likelihood_vi_no_nan)\nprint(sum_l)\nmean_log_likelihood_vi = np.mean(log_likelihood_vi_no_nan)\nprint('Mean loglikelihood for VI : {}'.format(mean_log_likelihood_vi))\n\ny_pred_vi = np.array(predictions_vi > 0.5).astype(int)\nacc_vi = accuracy_score(y_test, y_pred_vi[0])\nerror_rate_vi = 1 - acc_vi\nprint('Error rate for VI : {}'.format(error_rate_vi))\n\ncm_vi = confusion_matrix(y_test, y_pred_vi[0])\nprint(cm_vi)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T22:46:23.084512Z","iopub.execute_input":"2021-05-26T22:46:23.08488Z","iopub.status.idle":"2021-05-26T22:46:24.027722Z","shell.execute_reply.started":"2021-05-26T22:46:23.08485Z","shell.execute_reply":"2021-05-26T22:46:24.026438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2D : Tuning of Metropolis Hastings","metadata":{"execution":{"iopub.status.busy":"2021-05-25T14:42:52.628638Z","iopub.execute_input":"2021-05-25T14:42:52.628978Z","iopub.status.idle":"2021-05-25T14:42:52.634758Z","shell.execute_reply.started":"2021-05-25T14:42:52.628948Z","shell.execute_reply":"2021-05-25T14:42:52.633722Z"}}},{"cell_type":"code","source":"set_seed()\n\nnumber_steps = 1000\nstep_size = [10**(-i) for i in range(1,3)]\naccuracy = []\nacceptance_rate = []\n\nfor s in step_size:\n    likelihood = BernoulliLikelihood()\n    prior = NormalPrior(1)\n\n    starting_point = np.random.randn(200, 1)\n    sampler = MHSampler(starting_point, likelihood, prior)\n    \n    for _ in range(number_steps):\n        sampler.step(X_train.T, y_train, s)\n    ps = predict(X_test.T, sampler.samples[round(0.1 * number_steps):]) #burn-in of 10% of the samples\n    y_pred = np.array(ps > 0.5).astype(int)\n    acc = accuracy_score(y_test, y_pred[0])\n    accuracy.append(acc)\n    acceptance_rate.append(sampler.acceptance_rate/number_steps)\n    print(s)\n    print(acc)\n    print(sampler.acceptance_rate/number_steps)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T22:35:04.428191Z","iopub.execute_input":"2021-05-26T22:35:04.428683Z","iopub.status.idle":"2021-05-26T22:37:59.079981Z","shell.execute_reply.started":"2021-05-26T22:35:04.428628Z","shell.execute_reply":"2021-05-26T22:37:59.078736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For 1000 samples\n\n<table>\n<tr>\n<th>Step size</th>\n<th>Accuracy</th>\n<th>Acceptance rate</th>\n</tr>\n    \n<tr>\n<td>0.0001</td>\n<td>0.88</td>\n<td>0.50</td>\n</tr>\n    \n<tr>\n<td>0.001</td>\n<td>0.88</td>\n<td>0.47</td>\n</tr>\n\n<tr>\n<td>0.01</td>\n<td>0.52</td>\n<td>0</td>\n</tr>\n\n<tr>\n<td>0.1</td>\n<td>0.70</td>\n<td>0.199</td>\n</tr>\n\n<tr>\n<td>0.2</td>\n<td>0.51</td>\n<td>0.167</td>\n</tr>\n    \n<tr>\n<td>0.3</td>\n<td>0.20</td>\n<td>0.092</td>\n</tr>\n    \n<tr>\n<td>0.4</td>\n<td>0.12</td>\n<td>0.001</td>\n</tr>\n</table>\n\nThe step proposal determines the variance:\n- a low variance (small step size) will lead to highly correlated samples\n- a high variance (larger step size) will lead to a lower acceptance rate as many samples will be rejected as they don't belong to the aread of high density in the posterior space. As shown in the results, the acceptance rate decreases with the augmentation of the step size.\n\nThe optimal acceptance rate for multi dimension distributions is believed to be equal to 0.234, which is coherent with the results obtained : the higher accuracy is achieved when the acceptance rate is close to this value.\n\nIn order to ensure that samples are representative of samples of the posterior over model\nparameters, one can use the burn-in method. For a step size of 0.1 and 1000 samples, the accuracy is now 71% (instead of 70%). This small difference is due to the fact that if there are enough samples, the first samples coming from a random point will not matter.","metadata":{}},{"cell_type":"markdown","source":"## 2E : Tuning of Variational Inference","metadata":{"execution":{"iopub.status.busy":"2021-05-25T12:37:19.774587Z","iopub.execute_input":"2021-05-25T12:37:19.775439Z","iopub.status.idle":"2021-05-25T12:37:19.780085Z","shell.execute_reply.started":"2021-05-25T12:37:19.775385Z","shell.execute_reply":"2021-05-25T12:37:19.778852Z"}}},{"cell_type":"code","source":"lr = [10**(-i) for i in range(3,7)]\nminibatch_size = [10**i for i in range(2,4)]\naccuracy = np.zeros((len(lr), len(minibatch_size)))\n\nfor l in range(len(lr)):\n    for s in range(len(minibatch_size)):\n        \n        dataset = Dataset(X_train_norm, y_train, minibatch_size = minibatch_size[s])\n\n        likelihood = Bernoulli()\n        model = LogisticRegression(len(X_train[0]))\n\n        nelbo = VariationalObjective(model, likelihood, len(X_train_norm[0]), mc_samples = 1000)\n        nelbo_summary = Summary('nelbo')\n        nll_summary = Summary('expected_loglik')\n        kl_summary = Summary('kl')\n        from IPython.display import clear_output\n        optim = torch.optim.SGD(filter(lambda p: p.requires_grad,model.parameters()), lr = lr[l]) #SGD\n        num_iterations = 1000\n\n        for step in range(num_iterations):\n            optim.zero_grad()\n            Xbatch, ybatch = dataset.next_batch()\n            loss = nelbo.compute_objective(Xbatch, ybatch)\n\n            nelbo_summary.append(step, loss)\n            nll_summary.append(step, loss - nelbo.kl())\n            kl_summary.append(step, nelbo.kl())\n\n            loss.backward()\n            optim.step()\n            clear_output()\n            print('Lr: {}, Batch: {}, Epoch [{}/{}], Loss: {:.4f}'.format(lr[l], minibatch_size[s], step, num_iterations, loss.item()))\n            \n        ypred = model.predict_y(X_test_norm)\n        y_pred = np.array(ypred > 0.5).astype(int)\n\n        accuracy[l][s] = accuracy_score(y_test, y_pred[0])\n","metadata":{"execution":{"iopub.status.busy":"2021-05-26T23:18:39.039058Z","iopub.execute_input":"2021-05-26T23:18:39.039475Z","iopub.status.idle":"2021-05-26T23:24:03.803225Z","shell.execute_reply.started":"2021-05-26T23:18:39.039432Z","shell.execute_reply":"2021-05-26T23:24:03.801072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(l, s) = np.unravel_index(accuracy.argmax(), accuracy.shape)\nprint(lr[l], minibatch_size[s])","metadata":{"execution":{"iopub.status.busy":"2021-05-25T19:00:50.896912Z","iopub.execute_input":"2021-05-25T19:00:50.89725Z","iopub.status.idle":"2021-05-25T19:00:50.909018Z","shell.execute_reply.started":"2021-05-25T19:00:50.897223Z","shell.execute_reply":"2021-05-25T19:00:50.908228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Bonus question : Laplace","metadata":{}},{"cell_type":"markdown","source":"There seems to be an issue when trying to evaluate the second derivative of log_g in w_hat. Due to a lack of time, I wasn't able to fix the issue.","metadata":{}},{"cell_type":"code","source":"from jax import hessian\nimport scipy\n\nclass Laplace():\n    @property\n    def samples(self):\n        return self._samples\n    @samples.getter    \n    def samples(self):\n        return np.asarray(self._samples)\n    \n    def __init__(self, number_samples, likelihood, prior):\n        self.likelihood = likelihood\n        self.prior = prior\n        self.mean = []\n        self.n_samples = number_samples\n        self._samples = []\n    \n    def log_g(self, w, X, y):\n        w = w.reshape(200,1)\n        log_likelihood = self.likelihood.logdensity(y, logistic((w.T@X)))\n        log_prior = self.prior.logdensity(w)\n        return (log_likelihood + log_prior)[0][0]\n\n    def w_hat(self, X, y):\n        max_w = scipy.optimize.fmin(lambda w: -self.log_g(w, X, y), np.random.randn(200, 1))\n        return max_w\n    \n    def inv_sigma(self, X, y):\n        return - hessian(lambda w: self.log_g(w, X, y))(self.mean)\n    \n    def approx(self, X, y):\n        self.mean = self.w_hat(X,y).reshape(200,1)\n        print('mean ok')\n        sigma_inv = self.inv_sigma(X, y)\n        print('inv ok')\n        sigma = np.linalg.solve(sigma_inv, np.identity(len(sigma_inv)))\n        print('sigma ok')\n        self._samples = np.random.multivariate_normal(self.mean, sigma, self.n_samples)\n        return self._samples","metadata":{"execution":{"iopub.status.busy":"2021-05-26T12:23:58.621919Z","iopub.execute_input":"2021-05-26T12:23:58.622299Z","iopub.status.idle":"2021-05-26T12:23:58.635355Z","shell.execute_reply.started":"2021-05-26T12:23:58.622267Z","shell.execute_reply":"2021-05-26T12:23:58.634403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"set_seed()\n\nlikelihood = BernoulliLikelihood()\nprior = NormalPrior(1)\nn_samples = 100\nsampler = Laplace(n_samples, likelihood, prior)\nsampler.approx(X_train.T, y_train)\nprint(sampler.samples)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T12:26:37.847492Z","iopub.execute_input":"2021-05-26T12:26:37.848099Z","iopub.status.idle":"2021-05-26T12:40:33.342978Z","shell.execute_reply.started":"2021-05-26T12:26:37.84805Z","shell.execute_reply":"2021-05-26T12:40:33.339689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}