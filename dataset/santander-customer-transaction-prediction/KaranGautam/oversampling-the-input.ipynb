{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Oversampling Attempt \nThis is another attempt at trying different techniques that might help in this competition. \n\nI have oversampled the positive cases by 20k and then merged the oversampled cases with the original dataset. (Technique - SMOTE)\n\nOne particular thing that is happening with oversampling the positive cases is the model is overfitting drastically. \n\nFor e.g. \n* Lightgbm on normal dataset (no FE) give 90% .\n* But when you oversample the dataset with postive cases (even small oversampling ) causes big overfitting. \n\nThe validation dataset is next to useless when oversampling the training cases, as it will never depict the test scenario. So I was not too reliant on validation dataset. \n\nBut with 20% over sampling. \n* AUC for training data - 100%\n* AUC for validation data - 96%\n* AUC for test data - 84%\n\nNow what intrigues me is that it is difficult to overfit lightgbm model for this dataset. But with adding little impurities the model starts to overfit immediately. \nMy hypothesis is that the oversampled cases (produced through SMOTE (I guess using nn approach)) are linked with the input in such a way that it helps classify the training data. But since we cannot create the synthetic data for the test cases it underperforms. \n\nThe code might be little assorted but I will try to put comments where ever necessary. I hope it might be helpful to some, who may make some sense out of it and probably help improve their LB\n"},{"metadata":{},"cell_type":"markdown","source":"### Kaggle wrote these lines for me :)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Loading the libraries "},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import os\nimport shutil\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn import metrics\nfrom datetime import datetime\npd.set_option('display.max_columns', 200)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport sklearn\nimport scipy\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import classification_report,accuracy_score\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom imblearn.combine import SMOTEENN,SMOTETomek\nfrom sklearn.svm import OneClassSVM\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 14, 8\nRANDOM_SEED = 42\nLABELS = [\"Nobuy\", \"Buy\"]\nimport plotly.plotly as py\nimport plotly.graph_objs as go\nimport plotly\nimport plotly.figure_factory as ff\nfrom plotly.offline import init_notebook_mode, iplot","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Reading the input"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Reading training data\")\ndata = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Extremely useful display function, that is borrowed from fast.ai modules, it helps analyse the data in full. Thanks to @jeremyhoward"},{"metadata":{"trusted":true},"cell_type":"code","source":"def display_all(df):\n    with pd.option_context(\"display.max_rows\", 1000, \"display.max_columns\", 1000): \n        display(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Removing the ID_code variable as I will not be using this in training and prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.drop(['ID_code'],axis=1)\ntest_ids = test.ID_code.values\ntest = test.drop(['ID_code'],axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### This is the Crux of the code. \n\nI have imported the SMOTE function to oversample my target variable for positive cases. \nI am taking 1/4 of non-positive cases , i.e. 40k and 20k postive cases. After oversampling I am expecting 40k nonpositive and 40k positive cases\n\nMeaning I have introduced 20k addition positive datapoints just in hope that I am able to classify the positive cases better. \n\nThe resampled cases are stored in X_resampled, y_resampled. "},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom imblearn.over_sampling import SMOTE\nsub_non_fraud = data.loc[data.loc[:, 'target'] == 0, :].sample(int(len(data.loc[:, 'target'])/4 ))\n#sub_non_fraud = data.loc[data.loc[:, 'target'] == 0, :]\ndata_resample = pd.concat([sub_non_fraud, data.loc[data.loc[:, 'target'] == 1, :]])\nX = data_resample.drop(['target'],axis=1)\ny = data_resample.loc[:, \"target\"]\nsm = SMOTETomek()\nX_resampled, y_resampled = sm.fit_sample(X, y)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Verifiying whether the oversampling has happened correctly by checking the size"},{"metadata":{"trusted":true},"cell_type":"code","source":"# size of X and y after SMOTE\nprint(\"Size of X\", X_resampled.shape)\nprint(\"Size of y\", y_resampled.shape)\nprint(\"Size of positive cases\", y_resampled[y_resampled == 1].shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Changing the resampled cases into pandas dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_resampled = pd.DataFrame(X_resampled)\ny_resampled = pd.DataFrame(y_resampled)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Validating duplicates in resampled cases\n\nNo duplicate cases are created "},{"metadata":{"trusted":true},"cell_type":"code","source":"duplicateRowsDF = X_resampled[X_resampled.duplicated()]\nduplicateRowsDF.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Merging with the original dataset. \nI would be merging my resampled dataset with the original dataset. creating 220k training + validation cases with 40k positive cases and 160k negative cases. \n\nTo perform merge I am changing the variable name of the resampled cases so that merging happens in axis=0 "},{"metadata":{"trusted":true},"cell_type":"code","source":"X_resampled = X_resampled.add_prefix('var_')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Merging with original dataset\n\nSome naive variable assignment in tem_var as I was doing lot of experimentation (negelect it)"},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_xvar = X_resampled\ntemp_yvar = y_resampled\nX_resampled1 = pd.concat([data.drop(['target'],axis=1), temp_xvar])\ny_resampled1 = pd.concat([data['target'],temp_yvar])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_resampled1.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Removing dupicates \n\nMy resmapled dataset had 80k cases. and original dataset had 200k cases. 60k cases are duplicate that needs to be removed. \n\nSome non pythonic code :) to perform this deletion "},{"metadata":{"trusted":true},"cell_type":"code","source":"data1 = X_resampled1\ndata1['target'] = y_resampled1\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data1.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data1.drop_duplicates(keep = 'first', inplace = True) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data1.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Resetting the index "},{"metadata":{"trusted":true},"cell_type":"code","source":"data1.reset_index(drop=True,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"data1.shape"},{"metadata":{"trusted":true},"cell_type":"code","source":"del X_resampled\ndel y_resampled","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_resampled = data1.drop(['target'],axis=1)\ny_resampled = data1['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = X_resampled.values.astype(float)\ny = np.array(y_resampled)\nX_test = test.values.astype(float)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ny = y.flatten()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training with kfold 5. \nThe below code has been taken from @abhishekthakur's framework for competition. \nI guess everyone will be familiar with the code, but its the output that is interesting"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport shutil\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn import metrics\nfrom sklearn.ensemble import RandomForestClassifier\nimport math\nimport gc\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler\nNFOLDS = 5\nRANDOM_STATE = 42","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\nparam = {\n    'bagging_freq': 5,\n    'bagging_fraction': 0.335,\n    'boost_from_average':'false',\n    'boost': 'gbdt',\n    'feature_fraction': 0.041,\n    'learning_rate': 0.1,\n    'max_depth': 7,\n    'metric':'auc',\n    'min_data_in_leaf': 80,\n    'min_sum_hessian_in_leaf': 10.0,\n    'num_leaves': 13,\n    'num_threads': 8,\n    'tree_learner': 'serial',\n    'objective': 'binary', \n    'verbosity': -1\n#    'device': 'gpu',\n#    'gpu_platform_id': 0,\n #   'gpu_device_id': 0\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clfs = []\nfolds = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=RANDOM_STATE)\noof_preds = np.zeros((len(X_resampled), 1))\noof_preds_lgb = np.zeros((len(X_resampled), 1))\ntest_preds = np.zeros((len(test), 1))\ntest_preds_lgb = np.zeros((len(test), 1))\n#del train, test\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val1_x= data.drop(['target'],axis=1).values.astype(float)\nval1_y=data['target']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Overfitting the training data. \n\nI am unable to understand why. \n\nI know it will overfit but so quickly and problem is that the validation set auc also keeps on improving. \n\nWith lower learning rate and high iterations the kernel will run for day. Not ideal for person reliant on kaggle kernels for compute power :)"},{"metadata":{"trusted":true},"cell_type":"code","source":"c = 0\nfor fold_, (trn_, val_) in enumerate(folds.split(y, y)):\n    print(\"Current Fold: {}\".format(fold_))\n    trn_x, trn_y = X[trn_, :], y[trn_]\n    val_x, val_y = X[val_, :], y[val_]\n#    X_tr, y_tr = augment(trn_x, trn_y)\n#    X_tr = pd.DataFrame(X_tr)\n    trn_data = lgb.Dataset(trn_x, label=trn_y)\n    val_data = lgb.Dataset(val_x, label=val_y)\n    val1_data = lgb.Dataset(val1_x,label=val1_y)\n    \n    clf = lgb.train(param, trn_data, 100000, valid_sets = [trn_data, val_data,val1_data], verbose_eval=500, early_stopping_rounds = 4000)\n #   validation_data=([val_x1,val_x2],val_y)\n #   validation_data.shape\n#    logger = Logger(patience=10, out_path='./', out_fn='cv_{}.h5'.format(c))\n#    model.fit(trn_x,trn_y,batch_size=512,epochs=500,verbose=1,callbacks=[logger],validation_data=(val_x,val_y))\n#    model.load_weights('cv_{}.h5'.format(c))\n    val_pred = clf.predict(val_x,num_iteration=clf.best_iteration)\n    test_fold_pred = clf.predict(X_test,num_iteration=clf.best_iteration)\n\n    print(\"AUC = {}\".format(metrics.roc_auc_score(val_y, val_pred)))\n    oof_preds_lgb[val_, :] = val_pred.reshape((-1, 1))\n    test_preds_lgb += test_fold_pred.reshape((-1, 1))\n    del trn_x, trn_y , val_x,val_y\n    gc.collect()\n    \ntest_preds_lgb /= NFOLDS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.mean(test_preds_lgb)","execution_count":33,"outputs":[{"output_type":"execute_result","execution_count":33,"data":{"text/plain":"0.07955757988566148"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_score = metrics.roc_auc_score(y, oof_preds_lgb.ravel())\nprint(\"Overall AUC = {}\".format(roc_score))\n\nprint(\"Saving OOF predictions\")\n#oof_preds = pd.DataFrame(np.column_stack((train_ids, oof_preds.ravel())), columns=['ID_code', 'target'])\n#oof_preds.to_csv('../kfolds/nn__{}.csv'.format( str(roc_score)), index=False)\n\nprint(\"Saving code to reproduce\")\n#shutil.copyfile('../model_source/nn__{}.py'.format( str(roc_score)))\n\n\n#abc =  test_preds_lgb + test_preds_sq_lgb + test_preds_c_lgb +  test_preds_log_lgb\n#abc = abc/4\nprint(\"Saving submission file\")\nsample = pd.read_csv('../input/sample_submission.csv')\nsample.target = test_preds_lgb.astype(float)\nsample.ID_code = test_ids\nsample.to_csv('submission__smote_lgb_{}.csv'.format(str(roc_score)), index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}