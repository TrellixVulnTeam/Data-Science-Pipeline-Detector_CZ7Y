{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Introdução**\n\n## **Objetivos:** \n### Criação de um modelo para predizer se um determinado cliente do Banco Santander irá fazer um transação ou não, independente do seu valor.\n\n## **Dados fornecidos**\n### São fornecidos dados numéricos \"anonimizados\", uma coluna \"target\" binária e uma string com a identificação \"mascarada\" do cliente.  \n### Estão disponíveis 3 arquivos:\n* Um com dados de treino\n* Um com dados de teste (sem a varíavel \"target\" disponível)\n* Um de exemplo de submissão com a predição dos dados de teste\n\n## **Autor:** \n### André Roberto Antunes Paes","metadata":{}},{"cell_type":"markdown","source":"# **Análise Exploratória dos Dados**","metadata":{}},{"cell_type":"code","source":"# importando bibliotecas\n\nimport pandas as pd\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2022-06-26T23:52:05.063085Z","iopub.execute_input":"2022-06-26T23:52:05.063444Z","iopub.status.idle":"2022-06-26T23:52:06.422941Z","shell.execute_reply.started":"2022-06-26T23:52:05.06335Z","shell.execute_reply":"2022-06-26T23:52:06.421796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Verificando os arquivos disponíveis --> código padrão Kaggle\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2022-06-26T23:52:06.42497Z","iopub.execute_input":"2022-06-26T23:52:06.425369Z","iopub.status.idle":"2022-06-26T23:52:06.436082Z","shell.execute_reply.started":"2022-06-26T23:52:06.425316Z","shell.execute_reply":"2022-06-26T23:52:06.435164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Obtendo os dados de treino\n\ndf = pd.read_csv('../input/santander-customer-transaction-prediction/train.csv')\n\ndf.sample(5)","metadata":{"execution":{"iopub.status.busy":"2022-06-26T23:52:06.441606Z","iopub.execute_input":"2022-06-26T23:52:06.442812Z","iopub.status.idle":"2022-06-26T23:52:17.366831Z","shell.execute_reply.started":"2022-06-26T23:52:06.442758Z","shell.execute_reply":"2022-06-26T23:52:17.365869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  ","metadata":{"execution":{"iopub.status.busy":"2022-06-26T23:52:17.368549Z","iopub.execute_input":"2022-06-26T23:52:17.368823Z","iopub.status.idle":"2022-06-26T23:52:17.372526Z","shell.execute_reply.started":"2022-06-26T23:52:17.368783Z","shell.execute_reply":"2022-06-26T23:52:17.371793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Verificando se todos os dados são numéricos\n\ndf.info()","metadata":{"execution":{"iopub.status.busy":"2022-06-26T23:53:36.34802Z","iopub.execute_input":"2022-06-26T23:53:36.348387Z","iopub.status.idle":"2022-06-26T23:53:36.384047Z","shell.execute_reply.started":"2022-06-26T23:53:36.348351Z","shell.execute_reply":"2022-06-26T23:53:36.382897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Confirmando que a coluna do tipo Object é a identificação do \"cliente\"\n\ncols = df.dtypes.to_frame(name = 'type')\ncols[cols['type'] == 'object']","metadata":{"execution":{"iopub.status.busy":"2022-06-26T23:53:40.217014Z","iopub.execute_input":"2022-06-26T23:53:40.21734Z","iopub.status.idle":"2022-06-26T23:53:40.23067Z","shell.execute_reply.started":"2022-06-26T23:53:40.217305Z","shell.execute_reply":"2022-06-26T23:53:40.22972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Eliminando a coluna de identificação \"mascarada\" do cliente, já que ela não pode influenciar o modelo\n\ndf.drop(columns='ID_code', inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-26T23:53:41.507801Z","iopub.execute_input":"2022-06-26T23:53:41.508842Z","iopub.status.idle":"2022-06-26T23:53:41.684067Z","shell.execute_reply.started":"2022-06-26T23:53:41.508801Z","shell.execute_reply":"2022-06-26T23:53:41.682946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Gerando uma estatística descritiva das variáveis\n\ndf.describe().T","metadata":{"execution":{"iopub.status.busy":"2022-06-26T23:53:43.74373Z","iopub.execute_input":"2022-06-26T23:53:43.744275Z","iopub.status.idle":"2022-06-26T23:53:46.209558Z","shell.execute_reply.started":"2022-06-26T23:53:43.744214Z","shell.execute_reply":"2022-06-26T23:53:46.208185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As variáveis têm ordens de grandeza distintas. Provavelmente, uma padronização será benéfica ao modelo","metadata":{}},{"cell_type":"code","source":"# Verificando se há campos nulos\n\ndf.isnull().values.any()","metadata":{"execution":{"iopub.status.busy":"2022-06-26T23:53:51.227349Z","iopub.execute_input":"2022-06-26T23:53:51.227655Z","iopub.status.idle":"2022-06-26T23:53:51.328288Z","shell.execute_reply.started":"2022-06-26T23:53:51.227622Z","shell.execute_reply":"2022-06-26T23:53:51.327165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Avalianda a correlação linear entre as variáveis e entre elas e variável target\n\ncorr = np.array(df.corr())\nnp.fill_diagonal(corr, np.nan)\n\nplt.subplots(figsize=(15, 10))\nsns.heatmap(corr, cmap='seismic')","metadata":{"execution":{"iopub.status.busy":"2022-06-26T23:53:53.147649Z","iopub.execute_input":"2022-06-26T23:53:53.148296Z","iopub.status.idle":"2022-06-26T23:54:16.899129Z","shell.execute_reply.started":"2022-06-26T23:53:53.148252Z","shell.execute_reply":"2022-06-26T23:54:16.898391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualmente, os pares têm baixa correlação linear. Varrendo a matriz para confirmar que não há alguma correlação forte\n\npairs =  np.tril_indices(corr.shape[0])\npositive_pairs = corr[pairs] > 0.5 \nprint('Existe correlação direta forte?',np.any(positive_pairs) == True)\n\ninverse_pairs = corr[pairs] < -0.5 \nprint('Existe correlação inversa forte?',np.any(inverse_pairs) == True)\n\n# Salvando quais são as variáveis que têm alguma correlação inversa com o target\n\ninv_cols = df.columns[corr[:,0] < 0]\ninv_cols","metadata":{"execution":{"iopub.status.busy":"2022-06-26T23:54:16.900696Z","iopub.execute_input":"2022-06-26T23:54:16.90154Z","iopub.status.idle":"2022-06-26T23:54:16.914861Z","shell.execute_reply.started":"2022-06-26T23:54:16.901492Z","shell.execute_reply":"2022-06-26T23:54:16.914174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Não há nenhuma correlação linear forte","metadata":{}},{"cell_type":"code","source":"# Verificando a distrbuição das variáveis\n\nfig, axs = plt.subplots(40,5,figsize=(25,200))\nfor i, col in enumerate(df.columns):\n    if col != 'target': \n        j = (i - 1) // 5\n        k = (i - 1) % 5\n        axs[j,k].boxplot(df[col])\n        axs[j,k].set_title(col)\n        \nplt.show()\n\n# Verificando a realçaõ média x desvio padrão\n\nmeans = df[:][1:].mean(axis=0)\nstds  = df[:][1:].std(axis=0)\n\nplt.subplots(figsize=(8, 8))\nsns.scatterplot(means,stds).set( xlabel = 'Média', ylabel = 'Desvio padrão')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-26T23:54:16.9161Z","iopub.execute_input":"2022-06-26T23:54:16.916566Z","iopub.status.idle":"2022-06-26T23:54:39.433801Z","shell.execute_reply.started":"2022-06-26T23:54:16.916532Z","shell.execute_reply":"2022-06-26T23:54:39.432614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Visualmente, as variáveis aparecem bem distribuídas, de ordem de grandeza distintas. Não há nenhuma constante","metadata":{}},{"cell_type":"code","source":"# Verificando o balanceamento das classes \n\ndf.target.value_counts().to_frame(name='Target').plot(kind='bar', figsize=(10,5))\ndf.target.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-06-26T23:54:39.436447Z","iopub.execute_input":"2022-06-26T23:54:39.436696Z","iopub.status.idle":"2022-06-26T23:54:39.667274Z","shell.execute_reply.started":"2022-06-26T23:54:39.436667Z","shell.execute_reply":"2022-06-26T23:54:39.665978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As classes estão desbalanceadas. Provavelmente, será preciso optar por \"undersampling\" ou um \"oversampling\"","metadata":{}},{"cell_type":"markdown","source":"# **Estratégia  de  validação**\n\nA estratégia de validação será feita reservando 20% da base de treino para teste. Como vimmos que as variáveis têm\nordens de grandeza distintas, vamos padronizar a escala para que a grandeza não enviese o modelo. Como as classes estão \ndesbalanceadas, será feito um balanceamento da base de treino.","metadata":{}},{"cell_type":"code","source":"# Importando bibliotecas\n\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import SelectKBest, chi2, f_classif, mutual_info_classif\nfrom sklearn import preprocessing\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.decomposition import PCA","metadata":{"execution":{"iopub.status.busy":"2022-06-26T23:54:39.668682Z","iopub.execute_input":"2022-06-26T23:54:39.668944Z","iopub.status.idle":"2022-06-26T23:54:40.178994Z","shell.execute_reply.started":"2022-06-26T23:54:39.668913Z","shell.execute_reply":"2022-06-26T23:54:40.177719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Embaralhando a base de treino, caso ela tenha vindo com alguma ordenação\n\ndf_train = shuffle(df, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2022-06-26T23:54:40.180613Z","iopub.execute_input":"2022-06-26T23:54:40.180868Z","iopub.status.idle":"2022-06-26T23:54:40.602087Z","shell.execute_reply.started":"2022-06-26T23:54:40.180835Z","shell.execute_reply":"2022-06-26T23:54:40.601079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Invertendo as correlações negativas\n\nfor col in inv_cols:\n    df_train[col] = df_train[col] * -1 ","metadata":{"execution":{"iopub.status.busy":"2022-06-26T23:54:40.603641Z","iopub.execute_input":"2022-06-26T23:54:40.603988Z","iopub.status.idle":"2022-06-26T23:54:40.699486Z","shell.execute_reply.started":"2022-06-26T23:54:40.603944Z","shell.execute_reply":"2022-06-26T23:54:40.69849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dividindo a base de treino e de teste\n\nY = np.array(df_train.target)\n\nX = np.array(df_train.drop(columns=['target']))\n\n# Código temporário com redução do tamanho da base de treino para teste de código\n#sample = np.random.default_rng().integers(low=0, high=len(X), size=1000)\n#X = X[sample]\n#Y = Y[sample]\n\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.2,random_state=42,shuffle=True)\n\nprint(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-26T23:54:40.700827Z","iopub.execute_input":"2022-06-26T23:54:40.701559Z","iopub.status.idle":"2022-06-26T23:54:41.461344Z","shell.execute_reply.started":"2022-06-26T23:54:40.701521Z","shell.execute_reply":"2022-06-26T23:54:41.46003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Padronizando as variáveis\n\n#standard = preprocessing.StandardScaler()\n# Testei primeiro padronizando as variáveis como planejado durante a EDA, mas obtive resultados ligeiramente melhores \n# normalizando-as\nstandard = preprocessing.MinMaxScaler()\n\nX_train = standard.fit_transform(X_train)\nX_test = standard.transform(X_test)\n\nprint(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)\npd.DataFrame(X_train).describe().T","metadata":{"execution":{"iopub.status.busy":"2022-06-26T23:54:41.462911Z","iopub.execute_input":"2022-06-26T23:54:41.463272Z","iopub.status.idle":"2022-06-26T23:54:46.943923Z","shell.execute_reply.started":"2022-06-26T23:54:41.463199Z","shell.execute_reply":"2022-06-26T23:54:46.942851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Rebalanceando as classes da base de treino para que o modelo não fique tendencioso\n\nundersample = RandomUnderSampler(sampling_strategy='majority',random_state=42)\nX_train, Y_train = undersample.fit_resample(X_train, Y_train)\n\n# Testei o balanceamento com oversampling, o tempo de processamento aumentou muito sem grandes ganhos na acurácia\n#oversample = SMOTE(sampling_strategy='minority',random_state=42)\n#X_train, Y_train = oversample.fit_resample(X_train, Y_train)\n\n# Tamanhos das bases geradas\n\nprint(X_train.shape, X_train.shape, X_test.shape, Y_train.shape, Y_train.shape, Y_test.shape)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-26T23:54:46.946724Z","iopub.execute_input":"2022-06-26T23:54:46.946965Z","iopub.status.idle":"2022-06-26T23:54:47.038156Z","shell.execute_reply.started":"2022-06-26T23:54:46.946937Z","shell.execute_reply":"2022-06-26T23:54:47.037325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Verificando o rebalanceamento\n\npd.DataFrame(Y_train).value_counts().to_frame(name='Target').plot(kind='bar', figsize=(10,5))","metadata":{"execution":{"iopub.status.busy":"2022-06-26T23:54:47.039294Z","iopub.execute_input":"2022-06-26T23:54:47.040159Z","iopub.status.idle":"2022-06-26T23:54:47.263819Z","shell.execute_reply.started":"2022-06-26T23:54:47.040105Z","shell.execute_reply":"2022-06-26T23:54:47.262922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reduzindo a dimensionalidade\n\n# Testei várias opções de PCA, mas nenhuma melhorou a acurácia\n#pca = PCA(n_components=40, svd_solver='full')\n#X_train = pca.fit_transform(X_train)\n#X_test  = pca.transform(X_test)\n\n#pca_standard = preprocessing.MinMaxScaler()\n#X_train = pca_standard.fit_transform(X_train)\n#X_test = pca_standard.transform(X_test)\n\n#print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)\n#sns.scatterplot(X_train[:,0], X_train[:,1], hue=Y_train, palette='Set1')\n#plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-06-26T23:54:47.26537Z","iopub.execute_input":"2022-06-26T23:54:47.266451Z","iopub.status.idle":"2022-06-26T23:54:47.272141Z","shell.execute_reply.started":"2022-06-26T23:54:47.266393Z","shell.execute_reply":"2022-06-26T23:54:47.270941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Após testar reduzir a dimensionalidade, testei opções com seleção de variáveis\n# Selecionando as melhores variáveis de acordo com o teste de variância ANOVA (\"f_classif\") entre as variáveis e o target\n\nSelect = SelectKBest(f_classif, k=180)\n\nX_train = Select.fit_transform(X_train,Y_train)\nX_test  = Select.transform(X_test)\n\nprint(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-26T23:54:47.27379Z","iopub.execute_input":"2022-06-26T23:54:47.274699Z","iopub.status.idle":"2022-06-26T23:54:47.455288Z","shell.execute_reply.started":"2022-06-26T23:54:47.274649Z","shell.execute_reply":"2022-06-26T23:54:47.454399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Treino do modelo**\n\nForam treinados e avaliados vários modelos com hiperparâmetros distintos para encontrar o ideal. Com a base de treino fornecida, um algoritmo \"burro\" que definisse 0 para toda a saída, teria, matematicamente, uma acurácia de ~90%, já que as classes estão bem desbalanceadas. ","metadata":{"execution":{"iopub.status.busy":"2022-04-15T22:59:21.048022Z","iopub.execute_input":"2022-04-15T22:59:21.048373Z","iopub.status.idle":"2022-04-15T22:59:21.055628Z","shell.execute_reply.started":"2022-04-15T22:59:21.048335Z","shell.execute_reply":"2022-04-15T22:59:21.054308Z"}}},{"cell_type":"code","source":"# Importando bibliotecas\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n\nimport random as rd\nrd.seed(42)","metadata":{"execution":{"iopub.status.busy":"2022-06-26T23:54:49.167886Z","iopub.execute_input":"2022-06-26T23:54:49.168537Z","iopub.status.idle":"2022-06-26T23:54:49.173462Z","shell.execute_reply.started":"2022-06-26T23:54:49.168495Z","shell.execute_reply":"2022-06-26T23:54:49.172573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Definido função para treinar e imprimir resultados de um modelo\n\ndef train_results(algorithm, model, parameters, X_train, Y_train, X_test, Y_test):\n    \n    model = GridSearchCV(model, parameters, scoring = 'roc_auc', cv=5, verbose=1, return_train_score=True, n_jobs=-1)\n\n    model.fit(X_train, Y_train)\n    print('Melhores hyperparâmetros para ' + algorithm + ':', model.best_params_)\n    \n    yhat       = model.predict(X_train)\n    yhat_proba = model.predict_proba(X_train)[:,1]\n    print('---------------------- Desempenho de treino ----------------------')\n    print_results(Y_train, yhat, yhat_proba)\n    \n    yhat       = model.predict(X_test)\n    yhat_proba = model.predict_proba(X_test)[:,1]\n    print('---------------------- Desempenho de teste ----------------------')\n    print_results(Y_test, yhat, yhat_proba)\n    \n    return model.best_estimator_\n\n\n# Função para imprimir resultados\n\ndef print_results(y_actual,y_predicted, y_probable):\n    \n    cf_matrix = confusion_matrix(y_actual,y_predicted)\n    sns.heatmap(cf_matrix/np.sum(cf_matrix), annot=True, fmt='.0%', cmap='seismic')\n    plt.show()\n    print(classification_report(y_actual,y_predicted))\n    print('**** AUC =', roc_auc_score(y_actual,y_probable))\n    ","metadata":{"execution":{"iopub.status.busy":"2022-06-26T23:54:55.993774Z","iopub.execute_input":"2022-06-26T23:54:55.9941Z","iopub.status.idle":"2022-06-26T23:54:56.005847Z","shell.execute_reply.started":"2022-06-26T23:54:55.994067Z","shell.execute_reply":"2022-06-26T23:54:56.004544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Treinando e encontrando os melhores hyperparâmetros para regressão logística\n\nfrom sklearn.linear_model import LogisticRegression\n\nparameters ={'penalty':['l2'],'dual':[False], 'tol':[0.0001],'C':[0.01],'fit_intercept':[True],'intercept_scaling':[1],'class_weight':['balanced'],\n             'random_state':[42],'solver':['newton-cg','lbfgs'],'max_iter':[5000],'multi_class':['auto'],'verbose':[0],'warm_start':[False],\n             'n_jobs':[-1],'l1_ratio':[None]}\n#'class_weight':[{0:.1,1:.9}]\n\nlogreg = LogisticRegression()\n\nlogreg = train_results('Regressão Logística',logreg,parameters,X_train,Y_train,X_test,Y_test)","metadata":{"execution":{"iopub.status.busy":"2022-06-26T23:54:59.627339Z","iopub.execute_input":"2022-06-26T23:54:59.627685Z","iopub.status.idle":"2022-06-26T23:55:14.651369Z","shell.execute_reply.started":"2022-06-26T23:54:59.627648Z","shell.execute_reply":"2022-06-26T23:55:14.65005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Treinando e encontrando os melhores hyperparâmetros para Support Vector Machine\n\n#from sklearn.svm import LinearSVC\nfrom sklearn.svm import SVC\n\n#parameters ={'penalty':['l2'],'loss':['squared_hinge'],'dual':[False],'tol':[0.000001],'C':[1.0],\n#             'fit_intercept':[True],'intercept_scaling':[1],'class_weight':[None], \n#             'verbose':[0],'random_state':[42],'max_iter':[10000]}\n\nparameters ={'kernel':['linear','rbf'],'C':[0.1],'degree':[3],'gamma':['scale'],'coef0':[0.0],\n             'shrinking':[False],'probability':[False],'tol':[0.001],'cache_size':[200], \n             'class_weight':[None],'verbose':[0],'max_iter':[5000],'break_ties':[False],\n             'random_state':[42],'probability':[True]}\n#'class_weight':[None,{0:.15,1:.85}\n             \n#supvec = LinearSVC()\nsupvec = SVC()\n\nsupvec = train_results('Linear SVM',supvec,parameters,X_train,Y_train,X_test,Y_test)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T03:50:26.195549Z","iopub.execute_input":"2022-06-27T03:50:26.195885Z","iopub.status.idle":"2022-06-27T03:50:36.124004Z","shell.execute_reply.started":"2022-06-27T03:50:26.195852Z","shell.execute_reply":"2022-06-27T03:50:36.121352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Treinando e encontrando os melhores hyperparâmetros para random forest\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nparameters ={'n_estimators':[100,200],'criterion':['gini','entropy'],'max_depth':[None],'min_samples_split':[1000],\n             'min_samples_leaf':[1000],'min_weight_fraction_leaf':[0.0],'max_features':['sqrt'],'max_leaf_nodes':[None],\n             'min_impurity_decrease':[0.0],'bootstrap':[True],'oob_score':[False],'n_jobs':[-1],'random_state':[42],\n             'verbose':[0],'warm_start':[False],'class_weight':[None],'ccp_alpha':[0.0],'max_samples':[None]}\n#'class_weight':['balanced',{0:.15,1:.85}]\n\nranfor = RandomForestClassifier()\n\nranfor = train_results('Random Forest', ranfor,parameters, X_train,Y_train,X_test,Y_test)","metadata":{"execution":{"iopub.status.busy":"2022-06-23T03:00:22.218033Z","iopub.execute_input":"2022-06-23T03:00:22.218626Z","iopub.status.idle":"2022-06-23T03:00:27.078435Z","shell.execute_reply.started":"2022-06-23T03:00:22.218567Z","shell.execute_reply":"2022-06-23T03:00:27.077583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Treinando e encontrando os melhores hyperparâmetros para um algoritmo composto\n\nfrom sklearn.ensemble import HistGradientBoostingClassifier\n\nparameters ={'loss':['auto'],'learning_rate':[0.05,0.1],'max_iter':[10000], 'max_leaf_nodes':[None], \n             'max_depth':[None],'min_samples_leaf':[1000],'l2_regularization':[0.1],'max_bins':[255], \n             'categorical_features':[None],'monotonic_cst':[None],'warm_start':[False],\n             'early_stopping':['auto'],'scoring':['roc_auc'],'validation_fraction':[0.1],'n_iter_no_change':[100],\n             'tol':[1e-07],'random_state':[42],'verbose':[0]}\n\ngraboos=HistGradientBoostingClassifier()\n\ngraboos = train_results('Gradient Boosting', graboos,parameters, X_train,Y_train,X_test,Y_test)","metadata":{"execution":{"iopub.status.busy":"2022-06-23T04:07:05.938337Z","iopub.execute_input":"2022-06-23T04:07:05.938633Z","iopub.status.idle":"2022-06-23T04:39:28.977637Z","shell.execute_reply.started":"2022-06-23T04:07:05.938595Z","shell.execute_reply":"2022-06-23T04:39:28.976276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  Como o balanceamento, a redução de dimensionalidade, a seleção de features e o ajuste de diferentes hiperparâmetros em \n# diferentes algoritmos geraram resultados (AUC ~0.80) aquém da meta (AUC 0.85), usando o PCA para, \n# ao invés de reduzir a dimensionalidade, adicionar novas variáveis\n\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.1,random_state=42,shuffle=True)\n\nX_train = standard.fit_transform(X_train)\nX_test = standard.transform(X_test)\n\nundersample = RandomUnderSampler(sampling_strategy='majority',random_state=42)\nX_train, Y_train = undersample.fit_resample(X_train, Y_train)\n\npca = PCA(n_components=20, svd_solver='full')\nX_train_new_cols = pca.fit_transform(X_train)\nX_test_new_cols  = pca.transform(X_test)\n\npca_standard = preprocessing.MinMaxScaler()\nX_train_new_cols = pca_standard.fit_transform(X_train_new_cols)\nX_test_new_cols  = pca_standard.transform(X_test_new_cols)\n\nnew_X_train = np.array(pd.concat([pd.DataFrame(X_train), pd.DataFrame(X_train_new_cols)], axis = 1))\nnew_X_test = np.array(pd.concat([pd.DataFrame(X_test), pd.DataFrame(X_test_new_cols)], axis = 1))\n\nprint(new_X_train.shape, new_X_test.shape, Y_train.shape, Y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T00:55:58.384617Z","iopub.execute_input":"2022-06-27T00:55:58.384874Z","iopub.status.idle":"2022-06-27T00:56:00.918728Z","shell.execute_reply.started":"2022-06-27T00:55:58.38484Z","shell.execute_reply":"2022-06-27T00:56:00.917341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reavaliando os hiperparâmetros com as novas variáveis\n\nfrom sklearn.ensemble import HistGradientBoostingClassifier\n\nparameters ={'loss':['auto'],'learning_rate':[0.04,0.05],'max_iter':[10000], 'max_leaf_nodes':[None], \n             'max_depth':[None],'min_samples_leaf':[1000],'l2_regularization':[0.1,0.5],'max_bins':[255], \n             'categorical_features':[None],'monotonic_cst':[None],'warm_start':[False],\n             'early_stopping':['auto'],'scoring':['roc_auc'],'validation_fraction':[0.1],'n_iter_no_change':[100],\n             'tol':[1e-07],'random_state':[42],'verbose':[0]}\n\ngraboos=HistGradientBoostingClassifier()\n\ngraboos = train_results('Gradient Boosting', graboos,parameters, new_X_train,Y_train,new_X_test,Y_test)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T00:56:00.920035Z","iopub.execute_input":"2022-06-27T00:56:00.920266Z","iopub.status.idle":"2022-06-27T02:30:23.98817Z","shell.execute_reply.started":"2022-06-27T00:56:00.920215Z","shell.execute_reply":"2022-06-27T02:30:23.98689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Criando um dataframe com os dados das taxas de positivo e falso positivo por \"threshold\" para tentar encontrar \n# um ponto \"ótimo\" onde a classe 1 seja considerada no modelo de melhor desempenho para a base de teste\n\nfrom sklearn.metrics import roc_curve\n\nprobabilites = graboos.predict_proba(new_X_test)[:,1]\n\nfalse_pos_rate, tru_pos_rate, thresholds = roc_curve(Y_test, probabilites)\ntru_pos_rate = 1 - tru_pos_rate\n\nthresholds = pd.DataFrame({'Threshold':thresholds,'Falso Positivo':false_pos_rate,'Verd. Positivo': tru_pos_rate})\nthresholds.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-27T03:13:51.97151Z","iopub.execute_input":"2022-06-27T03:13:51.971822Z","iopub.status.idle":"2022-06-27T03:13:54.371316Z","shell.execute_reply.started":"2022-06-27T03:13:51.971784Z","shell.execute_reply":"2022-06-27T03:13:54.370565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Gerando o gráfico para escolher um threshold a partir do qual consideraremos a classe 1\n\nthresholds.plot(x='Threshold',y=['Verd. Positivo','Falso Positivo'], figsize=(10,5))","metadata":{"execution":{"iopub.status.busy":"2022-06-27T03:13:54.374335Z","iopub.execute_input":"2022-06-27T03:13:54.374581Z","iopub.status.idle":"2022-06-27T03:13:54.649831Z","shell.execute_reply.started":"2022-06-27T03:13:54.374551Z","shell.execute_reply":"2022-06-27T03:13:54.648947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Procurando o threshold que fornece o melhor resultado\n\nbest_auc = 0.88\nbest_threshold = 0\n\nfor threshold in range(5000,10000,1):\n    #new_auc = roc_auc_score(Y_test, np.where(probabilites >= threshold / 10000, 1, 0))\n    new_probabilities = np.copy(probabilites)\n    chg_probabilites = np.where(new_probabilities >= threshold / 10000)\n    new_probabilities[chg_probabilites] = 1\n    yhat = new_probabilities\n    new_auc = roc_auc_score(Y_test,yhat)\n    if new_auc > best_auc:\n        #print(best_auc , new_auc)\n        best_auc = new_auc\n        best_threshold = threshold  \n\nbest_threshold = best_threshold / 10000\n\nprint('Melhor threshold:', best_threshold)\n\nprint('---------------------- Desempenho de teste com ajuste do threshold ----------------------')\nprint('**** AUC =', best_auc)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T03:48:59.282783Z","iopub.execute_input":"2022-06-27T03:48:59.283111Z","iopub.status.idle":"2022-06-27T03:49:33.696784Z","shell.execute_reply.started":"2022-06-27T03:48:59.28308Z","shell.execute_reply":"2022-06-27T03:49:33.695522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Como os resultados não melhoraram muito, treinando um modelo com o lightgbm","metadata":{}},{"cell_type":"code","source":"# Validação cruzada com o lightgbm\n\nimport lightgbm\n\nY = df.target\nX = df.drop(columns=['target'])\n\nX_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.2,random_state=42)\n\ntrain_data = lightgbm.Dataset(X_train, label=Y_train)\n\nparameters = {'objective': 'binary','metric': 'auc','is_unbalance': 'true','boosting': 'gbdt','num_leaves': 63,\n              'feature_fraction': 0.5,'bagging_fraction': 0.5,'bagging_freq': 20,'learning_rate': 0.01,\n              'verbose':0,'force_col_wise':[True]}\n\neval_hist = lightgbm.cv(parameters, train_data, num_boost_round=5000, nfold=5, stratified=True, \n                         shuffle=True, verbose_eval = 50)\n\neval_hist.keys()","metadata":{"execution":{"iopub.status.busy":"2022-06-23T03:31:53.689558Z","iopub.execute_input":"2022-06-23T03:31:53.68985Z","iopub.status.idle":"2022-06-23T03:51:13.261417Z","shell.execute_reply.started":"2022-06-23T03:31:53.689819Z","shell.execute_reply":"2022-06-23T03:51:13.260279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Treinando o modelo Lightgbm\n\nbest_round =  np.argmax(eval_hist['auc-mean'])\nprint('qtde de \"rounds\":', best_round )\n\nmodel_lgbm = lightgbm.train(parameters,train_data,num_boost_round=best_round)\n\nyhat = model_lgbm.predict(X_train)\nprint('---------------------- Desempenho de treino ----------------------')\nprint('**** AUC =', roc_auc_score(Y_train,yhat))\n\nyhat = model_lgbm.predict(X_test)\nprint('---------------------- Desempenho de teste ----------------------')\nprint('**** AUC =', roc_auc_score(Y_test,yhat))","metadata":{"execution":{"iopub.status.busy":"2022-06-23T03:51:13.263007Z","iopub.execute_input":"2022-06-23T03:51:13.264222Z","iopub.status.idle":"2022-06-23T03:54:59.518857Z","shell.execute_reply.started":"2022-06-23T03:51:13.264188Z","shell.execute_reply":"2022-06-23T03:54:59.517869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Submissão**\nExecução da predição com o modelo final nos dados de teste e geração de um arquivo de submissão no formato da competição.","metadata":{}},{"cell_type":"code","source":"# Retreinando o modelo com todos os dados\n\ntrain_data = lightgbm.Dataset(X, label=Y)\n\nmodel_lgbm = lightgbm.train(parameters,train_data,num_boost_round=best_round)\n\nyhat = model_lgbm.predict(X)\nprint('---------------------- Desempenho de treino final ----------------------')\nprint('**** AUC =', roc_auc_score(Y,yhat))","metadata":{"execution":{"iopub.status.busy":"2022-06-23T03:54:59.521139Z","iopub.execute_input":"2022-06-23T03:54:59.522415Z","iopub.status.idle":"2022-06-23T03:59:28.488091Z","shell.execute_reply.started":"2022-06-23T03:54:59.522374Z","shell.execute_reply":"2022-06-23T03:59:28.486865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# recuperando o arquivo com as variáveis para teste\n\ntest = pd.read_csv('../input/santander-customer-transaction-prediction/test.csv')\n\ntest.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-23T03:59:28.489438Z","iopub.execute_input":"2022-06-23T03:59:28.489682Z","iopub.status.idle":"2022-06-23T03:59:40.036868Z","shell.execute_reply.started":"2022-06-23T03:59:28.489651Z","shell.execute_reply":"2022-06-23T03:59:40.035876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Verificando se ele não tem nulos\n\ntest.isnull().values.any()","metadata":{"execution":{"iopub.status.busy":"2022-06-23T03:59:40.03909Z","iopub.execute_input":"2022-06-23T03:59:40.039526Z","iopub.status.idle":"2022-06-23T03:59:40.12652Z","shell.execute_reply.started":"2022-06-23T03:59:40.039476Z","shell.execute_reply":"2022-06-23T03:59:40.125594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dando uma olhada em como é o arquivo de exemplo de submissão\n\nexample = pd.read_csv('../input/santander-customer-transaction-prediction/sample_submission.csv')\n\nexample.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-23T03:59:40.128059Z","iopub.execute_input":"2022-06-23T03:59:40.128705Z","iopub.status.idle":"2022-06-23T03:59:40.315496Z","shell.execute_reply.started":"2022-06-23T03:59:40.128656Z","shell.execute_reply":"2022-06-23T03:59:40.314701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fazendo as predições\n\nX = test.drop(columns=['ID_code'])\n    \ntarget = model_lgbm.predict(X)","metadata":{"execution":{"iopub.status.busy":"2022-06-23T03:59:40.316752Z","iopub.execute_input":"2022-06-23T03:59:40.316973Z","iopub.status.idle":"2022-06-23T04:00:19.091904Z","shell.execute_reply.started":"2022-06-23T03:59:40.316945Z","shell.execute_reply":"2022-06-23T04:00:19.091195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target","metadata":{"execution":{"iopub.status.busy":"2022-06-23T04:00:19.093453Z","iopub.execute_input":"2022-06-23T04:00:19.093925Z","iopub.status.idle":"2022-06-23T04:00:19.102589Z","shell.execute_reply.started":"2022-06-23T04:00:19.093888Z","shell.execute_reply":"2022-06-23T04:00:19.101957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Criando o arquivo de submissão\n\nsubmission = test[['ID_code']]\n\nsubmission['target'] = pd.Series(target)\n\nsubmission.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Gravando arquivo de saída\n\nsubmission.to_csv('submission.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Referência: LGBM with parameters (https://www.kaggle.com/code/dimanjung/lgbm-with-parameters)","metadata":{}}]}