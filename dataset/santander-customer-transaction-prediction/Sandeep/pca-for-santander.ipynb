{"cells":[{"metadata":{"_uuid":"b998f041f8b158be11812ecb85b6712bf36e5dfa"},"cell_type":"markdown","source":"# PCA\n\n**Dimensionality reduction:** If we want to visualize 2 variables(2 dimensional) we can do it using any type of plot, 1 variable at x-axis and another in y-axis. But, what if want to visualize 200 variables (200 dimension)? we can use dimensionality reduction techniques like PCA,t-SNE, UMAP for the same. This techniques intelligently summarizes/group information related to multi dimension to the required low dimension. Unfortunately this techniques didn't help much in this competetion."},{"metadata":{"_uuid":"89a460171211a008cbed47f02f65e0c00100e10f"},"cell_type":"markdown","source":"# Please upvote, if you find this kernel interesting"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nsns.set_style('whitegrid')\nimport os\nimport time\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold,StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nimport sklearn","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"print(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9cfc6228ed99f045fbb9897f26dff791ad2952e3"},"cell_type":"code","source":"train_df = pd.read_csv('../input/train.csv')\ntest_df = pd.read_csv('../input/test.csv')\nprint('Rows: ',train_df.shape[0],'Columns: ',train_df.shape[1])\ntrain_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1ad39049762bc5be772c57953ed327a15ab3524a"},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3a541ecef4c30912e5ac6de1fa7fd2273b737689"},"cell_type":"code","source":"train_df['target'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"252c4d408cd4639d166a0e46a2cd93a67663d75d"},"cell_type":"code","source":"sns.countplot(train_df['target'])\nsns.set_style('whitegrid')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dd32e3f355b0ecc35655cab9b78383ade2fa3f5e"},"cell_type":"code","source":"X_test = test_df.drop('ID_code',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"77cb649b9787dc4cb1751922fbe0f6bb93494f6e"},"cell_type":"code","source":"X = train_df.drop(['ID_code','target'],axis=1)\ny = train_df['target']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d4ad507daa1086f48b7c7b66da734b8bc69ea6c0"},"cell_type":"markdown","source":"n_components=2 (Final output will be converted to 2 Dimension)"},{"metadata":{"trusted":true,"_uuid":"b083abda379a0ad2fa7fe29cb74c12adf9264c1a"},"cell_type":"code","source":"from sklearn.decomposition import PCA\npca = PCA(n_components=2)\nprincipalComponents = pca.fit_transform(X)\nprincipalDf = pd.DataFrame(data = principalComponents\n             , columns = ['principal component 1', 'principal component 2'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9a89aa4060400ed6f6fd335e4e7769590a4ac597"},"cell_type":"markdown","source":"\"explained_variance_ratio_\" gives you percentage of variance covered by the first 2 principal component"},{"metadata":{"trusted":true,"_uuid":"22a8d9a294cae098954571843c830d047174fa53"},"cell_type":"code","source":"pca.explained_variance_ratio_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e0564dd9f7feb1ae48fd60e544551a845527d29f"},"cell_type":"code","source":"finalDf = pd.concat([principalDf, train_df[['target']]], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"18b01ea620651cd856f725c5d06c04f10292c636"},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfig = plt.figure(figsize = (8,8))\nax = fig.add_subplot(1,1,1) \nax.set_xlabel('Principal Component 1', fontsize = 15)\nax.set_ylabel('Principal Component 2', fontsize = 15)\nax.set_title('2 component PCA', fontsize = 20)\ntargets = [1.0, 0.0]\ncolors = ['r', 'g']\nfor target, color in zip(targets,colors):\n    indicesToKeep = finalDf['target'] == target\n    ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']\n               , finalDf.loc[indicesToKeep, 'principal component 2']\n               , c = color\n               , s = 50)\nax.legend(targets)\nax.grid()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize = (8,8))\nax = fig.add_subplot(1,1,1) \nax.set_xlabel('Principal Component 1', fontsize = 15)\nax.set_ylabel('Principal Component 2', fontsize = 15)\nax.set_title('2 component PCA', fontsize = 20)\ntargets = [1.0]\ncolors = ['r']\nfor target, color in zip(targets,colors):\n    indicesToKeep = finalDf['target'] == target\n    ax.scatter(finalDf.loc[indicesToKeep, 'principal component 2']\n               , finalDf.loc[indicesToKeep, 'principal component 1']\n               , c = color\n               , s = 50)\nax.legend(targets)\nax.grid()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eb38c09fad282039e0f363138168c2b3444e7796"},"cell_type":"markdown","source":"Another way to use PCA. \nBy specifying PCA(.95) we mean that we need 95% of variance to be covered. So out might be multiple principal, instead just 2."},{"metadata":{"trusted":true,"_uuid":"f9fdb10338c4bb2bf53461ec68121588f61e0a41"},"cell_type":"code","source":"#pca = PCA(n_components=2)\npca = PCA(.95)\nP_X = pca.fit_transform(X)\nP_X_TEST = pca.transform(X_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0b737968512a599622a39207cff510bb3ff7aae2"},"cell_type":"markdown","source":"Shows number principal components required to cover 95% of variance"},{"metadata":{"trusted":true,"_uuid":"c444093136761d649f0ab8d8ddfc988f03b21662"},"cell_type":"code","source":"pca.n_components_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3bc2f879b97d56cf6f7ddeabc9c8d9358e9529a7"},"cell_type":"code","source":"pca.explained_variance_ratio_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ce2e3872488db6a10ef388fc0997c45ed078f700"},"cell_type":"markdown","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nP_X_T=scaler.fit_transform(P_X)\nP_X_TEST_T=scaler.transform(P_X_TEST)"},{"metadata":{"_uuid":"1ac024dea6cc5973bbb793a2a0bdac2929b83439"},"cell_type":"markdown","source":"Running a model by taking the output of PCA(111 principal components)."},{"metadata":{"trusted":true,"_uuid":"17e50fb8308ee2caf5f7ab3a8deff6660e9048f7"},"cell_type":"code","source":"from catboost import CatBoostClassifier,Pool\ntrain_pool = Pool(P_X,y)\nm = CatBoostClassifier(iterations=3000,eval_metric=\"AUC\", objective=\"Logloss\",learning_rate=0.003)\nm.fit(P_X,y,silent=True)\ny_pred1 = m.predict_proba(P_X_TEST)[:,0]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f48a329ef00183f087e8f8609bba8a4cec27cd83"},"cell_type":"code","source":"sub1 = pd.DataFrame({\"ID_code\": test_df.ID_code.values})\nsub1[\"target\"] = y_pred1\nsub1.to_csv(\"submission1.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7fa4297c56093f03cf4543c8cb4157ee90c26c1d"},"cell_type":"markdown","source":"# Please upvote, if you find this kernel interesting"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}