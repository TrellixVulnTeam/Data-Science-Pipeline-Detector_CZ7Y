{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"raw","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \nimport pandas as pd# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"},{"metadata":{},"cell_type":"markdown","source":"## Baseline"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom torch.utils.data import TensorDataset\nfrom torch.utils.data import random_split\n#from math import ceil\nfrom sklearn import metrics\nfrom tqdm import tqdm\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = pd.read_csv(\"../input/santander-customer-transaction-prediction/train.csv\")\ntest_data = pd.read_csv(\"../input/santander-customer-transaction-prediction/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_data(train,test):\n    #train/validation set\n    train_data = train\n    #train_data = pd.read_csv(\"../input/santander-customer-transaction-prediction/train.csv\")\n    y = train_data[\"target\"]\n    X = train_data.drop([\"ID_code\",\"target\"],axis=1)\n    y_tensor = torch.tensor(y.values, dtype=torch.float32)\n    X_tensor = torch.tensor(X.values, dtype=torch.float32)\n    ds = TensorDataset(X_tensor, y_tensor)\n    train_ds, val_ds = random_split(ds,[int(0.8*len(ds)), len(ds)-(int(0.8*len(ds)))])\n    \n    #test set\n    #test_data = pd.read_csv(\"../input/santander-customer-transaction-prediction/test.csv\")\n    test_data = test\n    test_ids = test_data[\"ID_code\"]\n    X_test = test_data.drop([\"ID_code\"],axis=1)\n    #y_tensor = torch.tensor(y.values,dtype=torch.float32)\n    X_tensor = torch.tensor(X_test.values,dtype=torch.float32)\n    y_tensor = torch.tensor(y.values,dtype=torch.float32)\n    test_ds = TensorDataset(X_tensor,y_tensor)\n    \n    return train_ds, val_ds, test_ds, test_ids\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# create a simple NN as Baseline"},{"metadata":{"trusted":true},"cell_type":"code","source":"class NN(nn.Module):\n    def __init__(self, input_size):\n        super(NN, self).__init__()\n        self.net = nn.Sequential(\n            nn.BatchNorm1d(input_size),\n            nn.Linear(input_size, 50),\n            nn.ReLU(inplace=True),\n            nn.Linear(50,1),\n        )\n    def forward(self,x):\n        return torch.sigmoid(self.net(x)).view(-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_predicitons(loader, model, device):\n    model.eval()\n    saved_preds=[]\n    true_labels=[]\n    \n    with torch.no_grad():\n        for x,y in loader:\n            x=x.to(device)\n            y=y.to(device)\n            scores = model(x)\n            saved_preds += scores.tolist()\n            true_labels += y.tolist()\n            \n    model.train()\n    return saved_preds, true_labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check if GPU is available\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nmodel = NN(input_size=200).to(DEVICE)\noptimizer = optim.Adam(model.parameters(), lr=2e-3, weight_decay=1e-4)\nloss_fn = nn.BCELoss() #if nn.BCEWithLogitsLoss is used then the sigmoid is not required in the model because is\n#included in the loss function\n\ntrain_ds, val_ds, test_ds, test_ids = get_data(train_data,test_data)\n\ntrain_loader = DataLoader(train_ds, batch_size = 1024, shuffle = True)\nval_loader = DataLoader(val_ds, batch_size = 1024)\ntest_loader = DataLoader(test_ds, batch_size = 1024)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EPOCHS = 20\n\n#model.train()\nfor epoch in range(EPOCHS):\n    probabilities, true = get_predicitons(val_loader, model, device=DEVICE)\n    print(f\"VALID ROC:{metrics.roc_auc_score(true, probabilities)}\")\n    #data, targets = next(iter(train_loader))\n    for batch_idx, (data, targets) in enumerate(train_loader):\n        data = data.to(DEVICE)\n        targets = targets.to(DEVICE)\n\n        #forward\n        scores = model(data)\n        #print(scores.shape)\n        loss = loss_fn(scores, targets)\n        #print(loss)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model improvment"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check if data is correlated\n\ntrain_data.corr().abs()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"features seems to be uncorrelated"},{"metadata":{"trusted":true},"cell_type":"code","source":"col_names = [f\"var_{i}\" for i in range(200)]\nfor col in tqdm(col_names):\n             count = test_data[col].value_counts()\n             uniques = count.index[count == 1]\n             #print(uniques)\n             test_data[col + \"_u\"] = test_data[col].isin(uniques)\n\ntest_data[\"has_unique\"] = test_data[[col + \"_u\" for col in col_names]].any(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"real_test = test_data.loc[test_data[\"has_unique\"], [\"ID_code\"]+col_names]\nfake_test = test_data.loc[~test_data[\"has_unique\"], [\"ID_code\"]+col_names]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_and_test = pd.concat([train_data, real_test],axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in tqdm(col_names):\n    count = train_and_test[col].value_counts().to_dict()\n    #print(count)\n    train_and_test[col+\"_unique\"] = train_and_test[col].apply(\n        lambda x: 1 if count[x]==1 else 0).values\n    fake_test[col+\"_unique\"] = fake_test[col].apply(\n        lambda x: 1 if count[x]==1 else 0).values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"real_test = train_and_test[train_and_test[\"ID_code\"].str.contains(\"test\")].copy()\nreal_test.drop([\"target\"], axis=1, inplace=True)\ntrain_data_2 = train_and_test[train_and_test[\"ID_code\"].str.contains(\"train\")].copy()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data_2 = pd.concat([real_test, fake_test], axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# new nn\n\nclass NN_new(nn.Module):\n    def __init__(self, input_size, hidden_dim):\n        super(NN_new, self).__init__()\n        self.bn = nn.BatchNorm1d(input_size)\n        self.fc1 = nn.Linear(2, hidden_dim)\n        self.fc2 = nn.Linear(input_size//2*hidden_dim, 1)\n        #self.net = nn.Sequential(\n        #    nn.BatchNorm1d(input_size),\n        #    nn.Linear(input_size, 50),\n        #    nn.ReLU(inplace=True),\n        #    nn.Linear(50,1),\n        #)\n    def forward(self,x):\n        BATCH_SIZE = x.shape[0]\n        x = self.bn(x)\n        orig_features = x[:,:200].unsqueeze(2) #(BATCH_SIZE, 200, 1)\n        new_features = x[:,200:].unsqueeze(2) #(BATCH_SIZE, 200, 1)\n        x = torch.cat([orig_features,new_features],dim=2) #(BATCH_SIZE, 200, 2)\n        #x = x.view(-1,1)\n        x=self.fc1(x) #(BATCH_SIZE, 200*hidden_dim)\n        x= F.relu(x).reshape(BATCH_SIZE,-1)\n        x=self.fc2(x)\n        return torch.sigmoid(x).view(-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check if GPU is available\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nmodel = NN_new(input_size=400, hidden_dim=16).to(DEVICE)\noptimizer = optim.Adam(model.parameters(), lr=2e-3, weight_decay=1e-4)\nloss_fn = nn.BCELoss() #if nn.BCEWithLogitsLoss is used then the sigmoid is not required in the model because is\n#included in the loss function\n\ntrain_ds, val_ds, test_ds, test_ids = get_data(train_data_2,test_data_2)\n\ntrain_loader = DataLoader(train_ds, batch_size = 1024, shuffle = True)\nval_loader = DataLoader(val_ds, batch_size = 1024)\ntest_loader = DataLoader(test_ds, batch_size = 1024)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data, targets = next(iter(test_loader))\ndata.size()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EPOCHS = 30\n\n#model.train()\nfor epoch in range(EPOCHS):\n    probabilities, true = get_predicitons(val_loader, model, device=DEVICE)\n    print(f\"VALID ROC:{metrics.roc_auc_score(true, probabilities):.4f}\")\n    #data, targets = next(iter(train_loader))\n    for batch_idx, (data, targets) in enumerate(train_loader):\n        data = data.to(DEVICE)\n        targets = targets.to(DEVICE)\n\n        #forward\n        scores = model(data)\n        #print(scores.shape)\n        loss = loss_fn(scores, targets)\n        #print(loss)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_submission(model, loader, test_ids, device):\n    all_preds=[]\n    model.eval()\n    with torch.no_grad():\n        for x,y in loader:\n            x = x.to(device)\n            y = y.to(device)\n            score =model(x)\n            prediction=score.float()\n            all_preds += prediction.tolist()\n            \n    model.train()\n    df= pd.DataFrame({\n        \"ID_code\" : test_ids.values,\n        \"target\" : np.array(all_preds)\n    })\n    \n    df.to_csv(\"submission.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_submission(model, test_loader, test_ids, DEVICE)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}