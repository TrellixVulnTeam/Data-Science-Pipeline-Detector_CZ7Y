{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Santander Customer Transaction Prediction - Decision Tree\n\nIn the Kaggle competition, the objective is to identify which customer will make a transaction in the future.\n\n**Link to the competition**: https://www.kaggle.com/c/santander-customer-transaction-prediction/  \n**Type of Problem**: Classification  \n**Metric for evalution**: AOC (Area Under Curve)\n\nThis Python 3 environment comes with many helpful analytics libraries installed\nIt is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import KFold\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn import metrics\n\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import plot_tree","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step1: Read the datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"input_dir = '/kaggle/input/santander-customer-transaction-prediction/'\n\ndf_train = pd.read_csv(input_dir + 'train.csv')\ndf_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We already know the profile of data based on the overview provide by Kaggle.  \nLet us confirm the event rate for the training data. Event rate is approx 10%"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.groupby('target').size()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Step2: Split the data into training and validation data\n20% of data would be kept for validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"var_columns = [c for c in df_train.columns if c not in ['ID_code','target']]\nX = df_train.loc[:,var_columns]\ny = df_train.loc[:,'target']\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\nX_train.shape, X_valid.shape, y_train.shape, y_valid.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step3: Simple decision tree\n\nLet us try to use a simple decision tree to predict the target variable.  \nAlso plot the tree to make sure it looks fine."},{"metadata":{"trusted":true},"cell_type":"code","source":"model_tree = DecisionTreeClassifier(max_leaf_nodes=8, class_weight='balanced')\nmodel_tree.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create the figure\nplt.figure(figsize=(20,10))\n\n#Create the tree plot\nplot_tree(model_tree,\n           feature_names = var_columns, #Feature names\n           class_names = [\"0\",\"1\"], #Class names\n           rounded = True,\n           filled = True)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us look at the training and validation performance"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred = model_tree.predict(X_train)\ny_valid_pred = model_tree.predict(X_valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"auc_train = metrics.roc_auc_score(y_train, y_train_pred)\nauc_valid = metrics.roc_auc_score(y_valid, y_valid_pred)\n\nprint(\"AUC Train = {}\\nAUC Valid = {}\".format(round(auc_train,4), round(auc_valid,4)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step4: Iterate over number of leaf nodes\nLet us iterate through the steps to find the appropriate level of tree depth (max leaf nodes)  \nFor that, we will write all steps as a function and call that function in loop"},{"metadata":{"trusted":true},"cell_type":"code","source":"def tree_training(max_leaf_nodes, X_train, y_train, X_valid, y_valid):\n    model_tree = DecisionTreeClassifier(max_leaf_nodes=max_leaf_nodes, class_weight='balanced')\n    model_tree.fit(X_train, y_train)\n    \n    y_train_pred = model_tree.predict(X_train)\n    y_valid_pred = model_tree.predict(X_valid)\n    \n    auc_train = metrics.roc_auc_score(y_train, y_train_pred)\n    auc_valid = metrics.roc_auc_score(y_valid, y_valid_pred)\n    \n    print(\"Nodes:{}, Train:{:.4f}, Valid:{:.4f}, Diff:{:.4f}\".format(max_leaf_nodes,\n                                                                     auc_train,\n                                                                     auc_valid,\n                                                                     auc_train-auc_valid))\n          \n\n# Run few iterations to find which max_tree_nodes works best\nfor i in range(2, 20):\n    tree_training(i, X_train, y_train, X_valid, y_valid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The performance on validation data peaks with less number of nodes. It appears that we don't need very high number of leaf nodes.  \n\nAt `6 leaf nodes`, we are getting the highest validation AUC. Performance of the model on train and validation is virtually the same.\n\n## Step5: k-fold cross validation\n\nLooking at the result, I felt the need to perform a k-fold cross validation. Let us try `5-fold cross validation`"},{"metadata":{"trusted":true},"cell_type":"code","source":"kfold = KFold(5, shuffle=True, random_state=1)\n\nfor idx_train, idx_valid in kfold.split(df_train):\n    X_train = df_train.loc[idx_train, var_columns]\n    y_train = df_train.loc[idx_train, 'target']\n    \n    X_valid = df_train.loc[idx_valid, var_columns]\n    y_valid = df_train.loc[idx_valid, 'target']\n    \n    # Try 10 leaf nodes, we saw lot of leaf nodes don't increase performance\n    print(\"Iteration Starts\")\n    for i in range(2, 16):\n        tree_training(i, X_train, y_train, X_valid, y_valid)\n    \n    print(\"Iteration Ends\\n-----------------------\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A better way to perform 5-fold cross validation is using the sklearn function `cross_val_score`  \n\nI will iterate over the number of nodes and take average AUC for each iteration"},{"metadata":{"trusted":true},"cell_type":"code","source":"# CV function requires a scorer of this form\ndef cv_roc_auc_scorer(model, X, y): return metrics.roc_auc_score(y, model.predict(X))\n\n# Loop through multiple values of max_leaf_nodes to find best parameter\nfor num_leaf_node in range(2,16):\n    model_tree = DecisionTreeClassifier(max_leaf_nodes=num_leaf_node, class_weight='balanced')\n    kfold_scores = cross_validate(model_tree,\n                                  X,\n                                  y,\n                                  cv=5,\n                                  scoring=cv_roc_auc_scorer,\n                                  return_train_score=True)\n\n    # Find average train and test score\n    train_auc_avg = np.mean(kfold_scores['train_score'])\n    test_auc_avg = np.mean(kfold_scores['test_score'])\n\n    print(\"Nodes:{}, Train:{:.4f}, Valid:{:.4f}, Diff:{:.4f}\".format(num_leaf_node,\n                                                                     train_auc_avg,\n                                                                     test_auc_avg,\n                                                                     train_auc_avg-test_auc_avg))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The best performance on validation set (with minimum number of trees) is for 8 nodes.  \n\n## Step6: Final Model using Trees\nThe find model has `8 leaf nodes`. Let us create that model with entire training data and look at the output."},{"metadata":{"trusted":true},"cell_type":"code","source":"model_tree = DecisionTreeClassifier(max_leaf_nodes=8, class_weight='balanced')\nmodel_tree.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Print the final tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\n\nplot_tree(model_tree,\n          feature_names=var_columns,\n          class_names = ['0','1'],\n          rounded=True,\n          filled=True)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us find the final AUC value on training data  \nAnd also plot the AUC curve"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model_tree.predict(X)\n\nfpr, tpr, threshold = metrics.roc_curve(y, y_pred)\nmetrics.auc(fpr, tpr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"zeros_probs = [0 for _ in range(len(y))]\nfpr_zeros, tpr_zeros, _ = metrics.roc_curve(y, zeros_probs)\n\n# Plot the roc curve for the model\nplt.plot(fpr_zeros, tpr_zeros, linestyle='--', label='No Model')\nplt.plot(fpr, tpr, marker='.', label='Model')\n\n# axis labels\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\n\n# Add legend\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step7: Find Predictions for Test Data and store as final excel"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = pd.read_csv(input_dir + 'test.csv')\ndf_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = df_test.loc[:, var_columns]\ny_test_pred  = model_tree.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sample_subm = pd.read_csv(input_dir + 'sample_submission.csv')\ndf_sample_subm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sample_subm['target'] = y_test_pred\ndf_sample_subm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output_dir = '/kaggle/working/'\ndf_sample_subm.to_csv(output_dir + '/01_tree_scores.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}