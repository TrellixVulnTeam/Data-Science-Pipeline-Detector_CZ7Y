{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Santander Customer Transaction Prediction Competition\n\n## Desafio vivencial - Trilha de formação de cientistas de dados da Petrobras\n\n\"*In this challenge, we invite Kagglers to help us identify which customers will make a specific transaction in the future, irrespective of the amount of money transacted. The data provided for this competition has the same structure as the real data we have available to solve this problem.*\"\n\nKaggle challende URL: https://www.kaggle.com/competitions/santander-customer-transaction-prediction/submit\n\nAs mentioned in the introductory competition text, the provided data *has the same structure* (distribution?) as the real data. This suggests that the data provided in synthetic and realistic, but not necessarily real.\n\nThis notebook is inspired by the following works:\n\n* https://www.kaggle.com/code/cdeotte/200-magical-models-santander-0-920\n* https://www.kaggle.com/code/whitebird/0-923-in-n-5-aug\n* https://deb-sahoo19.medium.com/santander-customer-transaction-prediction-9e0edc8f9baa\n* https://rstudio-pubs-static.s3.amazonaws.com/496841_bc9c66aa3ce14d55bcabdaddcf595412.html\n* https://www.kaggle.com/code/navarro380/santander-eda-keras-model-logistic-reg-lasso/notebook","metadata":{}},{"cell_type":"markdown","source":"## Environment configuration","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Importando bibliotecas necessárias para utilizar os algoritmos de Machine Learning.\n\nimport lightgbm\n\nimport xgboost as xgb\n\nfrom xgboost import XGBClassifier\n\nfrom sklearn import tree\n\nfrom sklearn.svm import SVC\n\nfrom sklearn.naive_bayes import GaussianNB\n\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n\nfrom imblearn.ensemble import EasyEnsembleClassifier, BalancedRandomForestClassifier\n\nfrom sklearn.model_selection import StratifiedKFold, KFold, cross_val_score, train_test_split\n\n# Importando classe, para carregar e salvar modelos preditivos em arquivos externos.\n\nimport pickle\n\n# Importando classe, para fazer a busca dos melhores parâmetros, a serem utilizados em cada um dos modelos treinados.\n\nfrom sklearn.model_selection import GridSearchCV\n\n# Importando classes, para calcular as métricas de avaliação dos modelos preditivos.\n\nfrom sklearn.metrics import accuracy_score, balanced_accuracy_score, average_precision_score, precision_score\nfrom sklearn.metrics import recall_score, f1_score, roc_auc_score, cohen_kappa_score\n\nfrom IPython.display import Image\n\nkaggle = True\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","tags":[],"execution":{"iopub.status.busy":"2022-06-01T08:48:13.25192Z","iopub.execute_input":"2022-06-01T08:48:13.252853Z","iopub.status.idle":"2022-06-01T08:48:13.261443Z","shell.execute_reply.started":"2022-06-01T08:48:13.252807Z","shell.execute_reply":"2022-06-01T08:48:13.260612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Competition evaluation metric\n\nSubmissions are evaluated on area under the ROC curve (`AUC`) between the predicted probability and the observed target. \n\nThis makes sense for this dataset, since, as we will see later, the distribution of the binary target variable in unbalanced.\n\nHere is a link about AUC:\n\nhttps://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5","metadata":{}},{"cell_type":"markdown","source":"# 1. Reading input dataset","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv(os.path.join('../input/santander-customer-transaction-prediction', 'train.csv'))\ndf_test = pd.read_csv(os.path.join('../input/santander-customer-transaction-prediction', 'test.csv'))","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-06-01T08:48:13.272129Z","iopub.execute_input":"2022-06-01T08:48:13.27285Z","iopub.status.idle":"2022-06-01T08:48:32.417662Z","shell.execute_reply.started":"2022-06-01T08:48:13.27281Z","shell.execute_reply":"2022-06-01T08:48:32.416527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Train dataset has {len(df_train.index)} entries.')\nprint(f'Test dataset has {len(df_test.index)} entries.')","metadata":{"execution":{"iopub.status.busy":"2022-06-01T08:48:32.41953Z","iopub.execute_input":"2022-06-01T08:48:32.420347Z","iopub.status.idle":"2022-06-01T08:48:32.425907Z","shell.execute_reply.started":"2022-06-01T08:48:32.420297Z","shell.execute_reply":"2022-06-01T08:48:32.424841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Comments\n\nIt is interesting to see that both train and test datasets have the same size (200k entries). \n\nThe `df_train` dataset will be used in the whole model training process to generate the final ML model, which will be used to make predictions on the `df_test` dataset. These predictions will be sent to Kaggle through a submission csv file.","metadata":{}},{"cell_type":"markdown","source":"# 2. Exploratory Data Analysis (EDA)\n\n## 2.1. Data description\n\nWe are provided with an anonymized dataset containing several numeric feature variables, the binary target column, and a string ID_code column.\n\nThe task is to predict the value of the binary **target** column in the test set.\n\n#### Comments\n\nUnfortunately, since the dataset is anonymized and the real column names are unknown, data analysis becomes harder and it is more difficult to work on the data and devise new (derived) features, which could help improving the model results.","metadata":{}},{"cell_type":"code","source":"df_train.describe()","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-06-01T08:48:32.427322Z","iopub.execute_input":"2022-06-01T08:48:32.427723Z","iopub.status.idle":"2022-06-01T08:48:34.717151Z","shell.execute_reply.started":"2022-06-01T08:48:32.427695Z","shell.execute_reply":"2022-06-01T08:48:34.716389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Comments\n\nComparing each column in the train dataset, Min, Max and Standard Deviation values are quite different. This probably indicates that the provided data is **not** standardized or normalized.","metadata":{}},{"cell_type":"markdown","source":"## 2.2. Checking data types and column names\n\nBased on the info command below, we can see that the dataset is formed by an ID column called `ID_code` (`string`), the `target` column (binary, `int64`) and 200 numeric columns (real-valued, `float64`).","metadata":{}},{"cell_type":"code","source":"df_train.info(verbose = True, show_counts = True)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-06-01T08:48:34.718663Z","iopub.execute_input":"2022-06-01T08:48:34.719346Z","iopub.status.idle":"2022-06-01T08:48:34.851193Z","shell.execute_reply.started":"2022-06-01T08:48:34.71931Z","shell.execute_reply":"2022-06-01T08:48:34.850006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Convert float64 columns to float32 to save space","metadata":{}},{"cell_type":"code","source":"for col in df_train.columns:\n    if col != 'ID_code' and col != 'target':\n        df_train[col] = df_train[col].astype(np.float32)\ndf_train.info()","metadata":{"execution":{"iopub.status.busy":"2022-06-01T08:48:34.852373Z","iopub.execute_input":"2022-06-01T08:48:34.852776Z","iopub.status.idle":"2022-06-01T08:48:44.678506Z","shell.execute_reply.started":"2022-06-01T08:48:34.852744Z","shell.execute_reply":"2022-06-01T08:48:44.677573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.3. Checking for null values\n\n#### Comments \n\nWe can see that there are no null (`NaN`) values in any of the columns, neither in the train set nor in the test set.","metadata":{}},{"cell_type":"code","source":"df_train.isna().sum().sum()","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-06-01T08:48:44.679725Z","iopub.execute_input":"2022-06-01T08:48:44.680363Z","iopub.status.idle":"2022-06-01T08:48:44.782694Z","shell.execute_reply.started":"2022-06-01T08:48:44.68033Z","shell.execute_reply":"2022-06-01T08:48:44.781651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.isna().sum().sum()","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-06-01T08:48:44.784291Z","iopub.execute_input":"2022-06-01T08:48:44.784738Z","iopub.status.idle":"2022-06-01T08:48:44.883627Z","shell.execute_reply.started":"2022-06-01T08:48:44.784696Z","shell.execute_reply":"2022-06-01T08:48:44.882608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.4. Proportion of target values\n\nLet's check if the distribution of 0 and 1 values of the target is imbalanced or not.","metadata":{}},{"cell_type":"code","source":"df_train_prop = df_train.groupby(by=['target']).count()\ndisplay(df_train_prop)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-06-01T08:48:44.884895Z","iopub.execute_input":"2022-06-01T08:48:44.885336Z","iopub.status.idle":"2022-06-01T08:48:45.178295Z","shell.execute_reply.started":"2022-06-01T08:48:44.885294Z","shell.execute_reply":"2022-06-01T08:48:45.177258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_prop.reset_index().plot.bar(x='target', y='ID_code')","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-06-01T08:48:45.179689Z","iopub.execute_input":"2022-06-01T08:48:45.180113Z","iopub.status.idle":"2022-06-01T08:48:45.401849Z","shell.execute_reply.started":"2022-06-01T08:48:45.180079Z","shell.execute_reply":"2022-06-01T08:48:45.400916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"zero_prop = np.round(100.0 * len(df_train[df_train['target'] == 0].index) / len(df_train.index), 2)\none_prop = np.round(100.0 - zero_prop, 2)\nprint(f'Proportion of class 0: {zero_prop} % ; Proportion of class 1: {one_prop} %')","metadata":{"execution":{"iopub.status.busy":"2022-06-01T08:48:45.40452Z","iopub.execute_input":"2022-06-01T08:48:45.404864Z","iopub.status.idle":"2022-06-01T08:48:45.490656Z","shell.execute_reply.started":"2022-06-01T08:48:45.404828Z","shell.execute_reply":"2022-06-01T08:48:45.489085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Comments\n\nAs we can see in the graph, the target (binary) class is imbalanced, with 10.05% of the records belonging to class 1 and 89.95% belonging to class 0. In other words, there are a lot less `committed customer transactions` than the whole possible amount. ","metadata":{}},{"cell_type":"markdown","source":"## 2.5. Correlation analysis\n\nInspired by: \n* https://www.kaggle.com/code/xuanzhihuang/santander-customer-transaction-prediction-lgbm\n* https://www.analyticsvidhya.com/blog/2020/10/feature-selection-techniques-in-machine-learning/\n\nCorrelation analysis can help us identify the degree of importance of each feature. It can also help us later during feature selection, when we want to find the best set of features to build the ML model.\n\n","metadata":{}},{"cell_type":"markdown","source":"### 2.5.0. Feature scaling\n\nMany of the feature selection algorithms we'll use later are sensitive to the magnitude of each feature. Therefore it is important to apply feature scaling on all X columns.\n\nThe use of the StandardScaler will not affect the values of correlation.\n\n#### a) Before scaling\n\nBefore applying StandardScaler, let's observe the distribution of values of the provided features, to see the different range differences.\nThese differences may affect the training of many algorithms, such as Neural Networks.","metadata":{}},{"cell_type":"code","source":"X = df_train.drop([\"ID_code\", \"target\"], axis = 1)\ny = df_train['target']","metadata":{"execution":{"iopub.status.busy":"2022-06-01T08:48:45.492263Z","iopub.execute_input":"2022-06-01T08:48:45.492704Z","iopub.status.idle":"2022-06-01T08:48:45.547961Z","shell.execute_reply.started":"2022-06-01T08:48:45.492662Z","shell.execute_reply":"2022-06-01T08:48:45.546953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X[X.columns[:100]].plot(kind='box', figsize=[15,4], title='Non standarized values')\nX[X.columns[100:]].plot(kind='box', figsize=[15,4], title='Non standarized values')","metadata":{"execution":{"iopub.status.busy":"2022-06-01T08:48:45.549234Z","iopub.execute_input":"2022-06-01T08:48:45.549625Z","iopub.status.idle":"2022-06-01T08:48:50.814593Z","shell.execute_reply.started":"2022-06-01T08:48:45.549595Z","shell.execute_reply":"2022-06-01T08:48:50.813832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**We can observe, in the blox plot value range, that there are no absurd values, such as -999999. So we don't need to investigate the replacement of these outlier values.**\n\n**The differences between featues can also be observed through a distribution plot where each curve represents the distribution of values of each feature**","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(20, 10))\nfor val in X:\n    sns.kdeplot(data=X[val], color='crimson', label=val, fill=False, ax=ax)\n# end for\n\nplt.title('Density of non-stadarized feature values')\nplt.xlabel('Features')\nplt.ylabel('Density')","metadata":{"execution":{"iopub.status.busy":"2022-06-01T08:48:50.816031Z","iopub.execute_input":"2022-06-01T08:48:50.816673Z","iopub.status.idle":"2022-06-01T08:51:53.582084Z","shell.execute_reply.started":"2022-06-01T08:48:50.816637Z","shell.execute_reply":"2022-06-01T08:51:53.581103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**It is also very interesting to obseve the different magnitudes among the maximum value observed for each feature**","metadata":{}},{"cell_type":"code","source":"val_max = pd.DataFrame(data=X.max(), columns=['max'])\nval_max['var'] = val_max.index\nval_max[['var', 'max']].sort_values(by=['max'])","metadata":{"execution":{"iopub.status.busy":"2022-06-01T08:51:53.583274Z","iopub.execute_input":"2022-06-01T08:51:53.583597Z","iopub.status.idle":"2022-06-01T08:51:53.685039Z","shell.execute_reply.started":"2022-06-01T08:51:53.583569Z","shell.execute_reply":"2022-06-01T08:51:53.68401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_max['max'].plot(kind='box', title='Max values distribution')","metadata":{"execution":{"iopub.status.busy":"2022-06-01T08:51:53.686611Z","iopub.execute_input":"2022-06-01T08:51:53.686921Z","iopub.status.idle":"2022-06-01T08:51:53.828793Z","shell.execute_reply.started":"2022-06-01T08:51:53.686892Z","shell.execute_reply":"2022-06-01T08:51:53.827619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### b) Applying StandardScaler to features\n\n**Note: before applying StandardScaler, it is important to split our dataset into train and test data, so that the scaling on train data does not interfere the scaling of test data and vice-versa. Let's do that now. We'll split the data in the proportion of 75% x 25%.**","metadata":{}},{"cell_type":"code","source":"# Randomly split train and test data\nfrom sklearn.model_selection import train_test_split\n\ntrain, test = train_test_split(df_train, test_size=0.25, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2022-06-01T08:51:53.830586Z","iopub.execute_input":"2022-06-01T08:51:53.83149Z","iopub.status.idle":"2022-06-01T08:51:54.071946Z","shell.execute_reply.started":"2022-06-01T08:51:53.831436Z","shell.execute_reply":"2022-06-01T08:51:54.071066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = train.drop([\"ID_code\", \"target\"], axis = 1)\nX_test = test.drop([\"ID_code\", \"target\"], axis = 1)\ny_train = train['target']\ny_test = test['target']","metadata":{"execution":{"iopub.status.busy":"2022-06-01T08:51:54.073365Z","iopub.execute_input":"2022-06-01T08:51:54.073978Z","iopub.status.idle":"2022-06-01T08:51:54.133828Z","shell.execute_reply.started":"2022-06-01T08:51:54.073933Z","shell.execute_reply":"2022-06-01T08:51:54.133078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train = pd.DataFrame(scaler.fit_transform(X_train), columns = X_train.columns)\nX_test = pd.DataFrame(scaler.fit_transform(X_test), columns = X_test.columns)\nX_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-01T08:51:54.135169Z","iopub.execute_input":"2022-06-01T08:51:54.135936Z","iopub.status.idle":"2022-06-01T08:51:54.720364Z","shell.execute_reply.started":"2022-06-01T08:51:54.135898Z","shell.execute_reply":"2022-06-01T08:51:54.719679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### c) After scaling\n\nNow we can observe similar data ranges on all features.","metadata":{}},{"cell_type":"code","source":"X[X.columns[:100]].plot(kind='box', figsize=[15,4], title='Non standarized values')\nX_train[X_train.columns[:100]].plot(kind='box', figsize=[15,4], title='Standarized values')\nX[X.columns[100:]].plot(kind='box', figsize=[15,4], title='Non standarized values')\nX_train[X_train.columns[100:]].plot(kind='box', figsize=[15,4], title='Standarized values')","metadata":{"execution":{"iopub.status.busy":"2022-06-01T08:51:54.721239Z","iopub.execute_input":"2022-06-01T08:51:54.721651Z","iopub.status.idle":"2022-06-01T08:52:04.675867Z","shell.execute_reply.started":"2022-06-01T08:51:54.721623Z","shell.execute_reply":"2022-06-01T08:52:04.674837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Once again, let's see the distribution plot where each curve represents the distribution of values of each feature**","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(20, 10))\nfor val in X:\n    sns.kdeplot(data=X_train[val], color='crimson', label=val, fill=False, ax=ax)\n# end for\n\nplt.title('Density of stadarized feature values')\nplt.xlabel('Features')\nplt.ylabel('Density')","metadata":{"execution":{"iopub.status.busy":"2022-06-01T08:52:04.67722Z","iopub.execute_input":"2022-06-01T08:52:04.6777Z","iopub.status.idle":"2022-06-01T08:54:22.542311Z","shell.execute_reply.started":"2022-06-01T08:52:04.677658Z","shell.execute_reply":"2022-06-01T08:54:22.541389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_max_std = pd.DataFrame(data=X_train.max(), columns=['max'])\nval_max_std['var'] = val_max_std.index\nval_max_std['max'].plot(kind='box', title='Max values distribution - standardized values')","metadata":{"execution":{"iopub.status.busy":"2022-06-01T08:54:22.543989Z","iopub.execute_input":"2022-06-01T08:54:22.54461Z","iopub.status.idle":"2022-06-01T08:54:22.773955Z","shell.execute_reply.started":"2022-06-01T08:54:22.544564Z","shell.execute_reply":"2022-06-01T08:54:22.773244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.5.1. Obtain the correlation coefficient between different features\n\nCorrelation is a measure of the linear relationship of 2 or more variables. \nThe logic behind using correlation for feature selection is that the good variables are highly correlated with the target. \n\n**Furthermore, variables should be correlated with the target but should be uncorrelated among themselves. The main idea is that, if two variables are highly correlated, we can predict one from the other. Therefore, if two features are correlated, the model only really needs one of them, as the second one does not add additional information.**","metadata":{}},{"cell_type":"markdown","source":"### 2.5.1.a. Pearson correlation coefficient\n\nLet's try the Pearson correlation coefficient first.","metadata":{}},{"cell_type":"code","source":"# Correlation matrix\ncor = df_train.drop([\"ID_code\"], axis = 1).corr()","metadata":{"execution":{"iopub.status.busy":"2022-06-01T08:54:22.774975Z","iopub.execute_input":"2022-06-01T08:54:22.775379Z","iopub.status.idle":"2022-06-01T08:54:44.714613Z","shell.execute_reply.started":"2022-06-01T08:54:22.775349Z","shell.execute_reply":"2022-06-01T08:54:44.713588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"correlations = cor.abs().unstack().sort_values(kind = \"quicksort\", ascending=False).reset_index()\ncorrelations = correlations[correlations['level_0'] != correlations['level_1']]\ncorrelations.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-06-01T08:54:44.715787Z","iopub.execute_input":"2022-06-01T08:54:44.716124Z","iopub.status.idle":"2022-06-01T08:54:44.745369Z","shell.execute_reply.started":"2022-06-01T08:54:44.716094Z","shell.execute_reply":"2022-06-01T08:54:44.744456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"correlations[(correlations['level_0'] != 'target') & (correlations['level_1'] != 'target')].head(4)","metadata":{"execution":{"iopub.status.busy":"2022-06-01T08:54:44.746621Z","iopub.execute_input":"2022-06-01T08:54:44.747402Z","iopub.status.idle":"2022-06-01T08:54:44.765965Z","shell.execute_reply.started":"2022-06-01T08:54:44.747368Z","shell.execute_reply":"2022-06-01T08:54:44.765301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Comments\n\nWe can see that the highest correlations found between different features are really small (max value found was `0.080917` between the target and `var_81`, and the highest correlation found between features was `0.009844`). \n\n**Therefore, there are no perfectly correlated features (redundant features, with absolute correlation value equal to 1) to be excluded.**\n\nLet's now check the correlations between target variable and features.","metadata":{}},{"cell_type":"markdown","source":"#### Correlation heatmap\n\nThe correlation heatmap below confirms what we found in the correlation list above: all correlations have near-zero values.\n\nIf we had found predictor variables / features correlated among themselves, we would be able to drop the variable which had a lower correlation coefficient value with the target variable. But that is not the case.","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# Plotting heatmap\nplt.figure(figsize=(50,30))\nsns.heatmap(cor, vmin = -1, vmax = +1, cmap = 'coolwarm')  # annot = True, ","metadata":{"execution":{"iopub.status.busy":"2022-06-01T08:54:44.766849Z","iopub.execute_input":"2022-06-01T08:54:44.767611Z","iopub.status.idle":"2022-06-01T08:54:49.022653Z","shell.execute_reply.started":"2022-06-01T08:54:44.767582Z","shell.execute_reply":"2022-06-01T08:54:49.021728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Let's try the other two types of correlation, besides perason\n\n* pearson : standard correlation coefficient\n\n* kendall : Kendall Tau correlation coefficient\n\n* spearman : Spearman rank correlation","metadata":{}},{"cell_type":"code","source":"cor_kendall = df_train.drop([\"ID_code\"], axis = 1).corr(method='kendall')\ncor_spearman = df_train.drop([\"ID_code\"], axis = 1).corr(method='spearman')","metadata":{"execution":{"iopub.status.busy":"2022-06-01T08:54:49.024218Z","iopub.execute_input":"2022-06-01T08:54:49.024529Z","iopub.status.idle":"2022-06-01T09:19:58.95041Z","shell.execute_reply.started":"2022-06-01T08:54:49.024501Z","shell.execute_reply":"2022-06-01T09:19:58.949388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"correlations_kendall = cor_kendall.abs().unstack().sort_values(kind = \"quicksort\", ascending=False).reset_index()\ncorrelations_kendall = correlations_kendall[correlations_kendall['level_0'] != correlations_kendall['level_1']]\ncorrelations_kendall.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-06-01T09:19:58.951742Z","iopub.execute_input":"2022-06-01T09:19:58.952035Z","iopub.status.idle":"2022-06-01T09:19:58.979882Z","shell.execute_reply.started":"2022-06-01T09:19:58.95201Z","shell.execute_reply":"2022-06-01T09:19:58.978766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"correlations_kendall[(correlations_kendall['level_0'] != 'target') & (correlations_kendall['level_1'] != 'target')].head(4)","metadata":{"execution":{"iopub.status.busy":"2022-06-01T09:19:58.984834Z","iopub.execute_input":"2022-06-01T09:19:58.985162Z","iopub.status.idle":"2022-06-01T09:19:59.00617Z","shell.execute_reply.started":"2022-06-01T09:19:58.985135Z","shell.execute_reply":"2022-06-01T09:19:59.00553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Conclusion: Kendall correlations remain low (less than `0.06`).","metadata":{}},{"cell_type":"code","source":"correlations_spearman = cor_spearman.abs().unstack().sort_values(kind = \"quicksort\", ascending=False).reset_index()\ncorrelations_spearman = correlations_spearman[correlations_spearman['level_0'] != correlations_spearman['level_1']]\ndisplay(correlations_spearman.head(10))\ncorrelations_spearman[(correlations_spearman['level_0'] != 'target') & (correlations_spearman['level_1'] != 'target')].head(4)","metadata":{"execution":{"iopub.status.busy":"2022-06-01T09:19:59.007238Z","iopub.execute_input":"2022-06-01T09:19:59.008055Z","iopub.status.idle":"2022-06-01T09:19:59.050381Z","shell.execute_reply.started":"2022-06-01T09:19:59.00802Z","shell.execute_reply":"2022-06-01T09:19:59.049362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Conclusion: Spearman correlations remain low (less than `0.07`).","metadata":{}},{"cell_type":"markdown","source":"### 2.5.2. Obtain the correlations between (categorical) target and (continuous) features\n\nThere are three big-picture methods to understand if a continuous and categorical are significantly correlated — point biserial correlation, logistic regression, and Kruskal Wallis H Test.\n\nReference: https://medium.com/@outside2SDs/an-overview-of-correlation-measures-between-categorical-and-continuous-variables-4c7f85610365","metadata":{}},{"cell_type":"markdown","source":"#### a) Point biserial Correlation\n\n* Similar to the Pearson coefficient, the point biserial correlation can range from -1 to +1.\n* The point biserial calculation assumes that the continuous variable is normally distributed and homoscedastic.\n* If the dichotomous variable is artificially binarized, i.e. there is likely continuous data underlying it, biserial correlation is a more apt measurement of similarity. There is a simple formula to calculate the biserial correlation from point biserial correlation, but nonetheless this is an important point to keep in mind.\n\n#### b) Logistic Regression\n\nThe idea behind using logistic regression to understand correlation between variables is actually quite straightforward and follows as such: If there is a relationship between the categorical and continuous variable, we should be able to construct an accurate predictor of the categorical variable from the continuous variable. If the resulting classifier has a high degree of fit, is accurate, sensitive, and specific we can conclude the two variables share a relationship and are indeed correlated.\n\nThere are a number of positive things about this approach. Logistic regression does not make many of the key assumptions of linear regression and other models that are based on least squares algorithms — particularly regarding linearity, normality, homoscedasticity, and measurement level.\n\n#### c) Kruskal-Wallis H Test (Or parametric forms such as t-test or ANOVA)\n\n(Estimate variance explained in continuous variable using the discrete variable)\n\nThe final family of methods to estimate association between a continuous and discrete variable rely on estimating the variance of the continuous variable, which can be explained through the categorical variable. \nThere are many ways to do this. A simple approach could be to group the continuous variable using the categorical variable, measure the variance in each group and comparing it to the overall variance of the continuous variable. If the variance after grouping falls down significantly, it means that the categorical variable can explain most of the variance of the continuous variable and so the two variables likely have a strong association. If the variables have no correlation, then the variance in the groups is expected to be similar to the original variance.","metadata":{}},{"cell_type":"markdown","source":"### Let's use the first option\n\n#### a.1) Point Biserial Correlation - First option of calculation\n\nIf a categorical variable only has two values (i.e. true/false), then we can convert it into a numeric datatype (0 and 1). Since it becomes a numeric variable, we can find out the correlation using the dataframe.corr() function. That is exactly our case!","metadata":{}},{"cell_type":"code","source":"variables = X.columns.values.tolist()\nvariables[0:5]","metadata":{"execution":{"iopub.status.busy":"2022-06-01T09:19:59.052985Z","iopub.execute_input":"2022-06-01T09:19:59.053337Z","iopub.status.idle":"2022-06-01T09:19:59.05974Z","shell.execute_reply.started":"2022-06-01T09:19:59.053307Z","shell.execute_reply":"2022-06-01T09:19:59.058754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.corrcoef(df_train['var_0'],y)","metadata":{"execution":{"iopub.status.busy":"2022-06-01T09:19:59.06142Z","iopub.execute_input":"2022-06-01T09:19:59.062095Z","iopub.status.idle":"2022-06-01T09:19:59.07494Z","shell.execute_reply.started":"2022-06-01T09:19:59.062052Z","shell.execute_reply":"2022-06-01T09:19:59.073921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr_feature_target = np.zeros(len(variables))\n# let's store the correlation between each feature df_train[var] and the target df_train['target'], accessing the element [0, 1] from the corr matrix\nfor i, var in enumerate(variables):\n    corr_feature_target[i] = np.corrcoef(df_train[var], y)[0, 1]","metadata":{"execution":{"iopub.status.busy":"2022-06-01T09:19:59.076065Z","iopub.execute_input":"2022-06-01T09:19:59.076772Z","iopub.status.idle":"2022-06-01T09:19:59.50558Z","shell.execute_reply.started":"2022-06-01T09:19:59.076729Z","shell.execute_reply":"2022-06-01T09:19:59.504601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr_feature_target = abs(pd.DataFrame(corr_feature_target))\ncorr_feature_target.columns = ['corr_feature_target']\ncorr_feature_target.sort_values(by = 'corr_feature_target')","metadata":{"execution":{"iopub.status.busy":"2022-06-01T09:19:59.506791Z","iopub.execute_input":"2022-06-01T09:19:59.507107Z","iopub.status.idle":"2022-06-01T09:19:59.520025Z","shell.execute_reply.started":"2022-06-01T09:19:59.50708Z","shell.execute_reply":"2022-06-01T09:19:59.519239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### a.2) Point Biserial Correlation - Second option of calculation\n\nSource: https://towardsdatascience.com/point-biserial-correlation-with-python-f7cd591bd3b1\n\nSciPy conveniently has a point biserial correlation function called pointbiserialr.\n\nThe point biserial correlation is used to measure the relationship between a binary variable, `x`, and a continuous variable, `y`.","metadata":{}},{"cell_type":"code","source":"from scipy.stats import pointbiserialr\n\npbc = dict()\n\nfor col in df_train.columns:\n    if col != 'target' and col != 'ID_code':\n        pbc_ = pointbiserialr(x=df_train['target'], y=df_train[col])\n        pbc[col] = pbc_\n# end for\ndf_pbc = pd.DataFrame.from_dict(pbc, orient='index', columns=['correlation', 'p-value'])\ndf_pbc['correlation_abs'] = np.abs(df_pbc['correlation'])\ndf_pbc.sort_values(by=['correlation_abs'], ascending=False, inplace=True)\ndf_pbc","metadata":{"execution":{"iopub.status.busy":"2022-06-01T09:19:59.521425Z","iopub.execute_input":"2022-06-01T09:19:59.522435Z","iopub.status.idle":"2022-06-01T09:20:00.318586Z","shell.execute_reply.started":"2022-06-01T09:19:59.522399Z","shell.execute_reply":"2022-06-01T09:20:00.317669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Comments\n\nBoth methods used to calculate the Point Biserial Correlation yielded the same results.\n\nThe correlations between target variable and features are also **small**. The largest correlation observed was `0.080917`.  ","metadata":{}},{"cell_type":"markdown","source":"**We will not use the other methods (b) and (c) to establish the correlation between the features and the target variable because Point Biserial (a) is the preferred method to do so when the categorical feature is binary.**","metadata":{}},{"cell_type":"markdown","source":"### 2.5.X. Detecting duplicate rows","metadata":{}},{"cell_type":"code","source":"# Checking for duplicate rows\nX.duplicated().sum()","metadata":{"execution":{"iopub.status.busy":"2022-06-01T09:20:00.320059Z","iopub.execute_input":"2022-06-01T09:20:00.320665Z","iopub.status.idle":"2022-06-01T09:20:03.662143Z","shell.execute_reply.started":"2022-06-01T09:20:00.320623Z","shell.execute_reply":"2022-06-01T09:20:03.661016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"No duplicate rows found.","metadata":{}},{"cell_type":"markdown","source":"## 2.6. Feature selection\n\nSource: https://www.analyticsvidhya.com/blog/2020/10/feature-selection-techniques-in-machine-learning/\n\nLet's select the most important features.\n\nUsing a feature selection metric, it is possible to choose a subset of the input variables most related to the output variable.\n\nClassification Feature Selection:\n(Numerical Input, Categorical Output)","metadata":{}},{"cell_type":"markdown","source":"### 2.6.1. Information gain\n\nInformation gain calculates the reduction in entropy from the transformation of a dataset. It can be used for feature selection by evaluating the Information gain of each variable in the context of the target variable.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import mutual_info_classif\nif kaggle:\n    df_information_gain = pd.read_pickle('../input/santandercustomertransactionfeatureimportance/df_information_gain.pkl')\nelif os.path.exists('df_information_gain.pkl.gz'):\n    df_information_gain = pd.read_pickle('df_information_gain.pkl.gz')\nelse:\n    importances = mutual_info_classif(X_train, y_train)\n    feature_importances = pd.Series(importances, X_train.columns)\n    feature_importances.sort_values(ascending=False, inplace=True)\n    df_information_gain = pd.DataFrame(data={'information_gain': feature_importances}).reset_index()\n    df_information_gain.to_pickle('df_information_gain.pkl.gz')","metadata":{"execution":{"iopub.status.busy":"2022-06-01T09:20:03.663615Z","iopub.execute_input":"2022-06-01T09:20:03.663938Z","iopub.status.idle":"2022-06-01T09:20:03.706029Z","shell.execute_reply.started":"2022-06-01T09:20:03.66391Z","shell.execute_reply":"2022-06-01T09:20:03.705289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_list = df_information_gain[df_information_gain['information_gain'] > 0]\nprint(len(feature_list))","metadata":{"execution":{"iopub.status.busy":"2022-06-01T09:20:03.707608Z","iopub.execute_input":"2022-06-01T09:20:03.708353Z","iopub.status.idle":"2022-06-01T09:20:03.715575Z","shell.execute_reply.started":"2022-06-01T09:20:03.708306Z","shell.execute_reply":"2022-06-01T09:20:03.714611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_information_gain","metadata":{"execution":{"iopub.status.busy":"2022-06-01T09:20:03.716555Z","iopub.execute_input":"2022-06-01T09:20:03.717555Z","iopub.status.idle":"2022-06-01T09:20:03.735521Z","shell.execute_reply.started":"2022-06-01T09:20:03.717506Z","shell.execute_reply":"2022-06-01T09:20:03.734245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.6.2. Fisher’s Score (disabled)\n\nFisher score is one of the most widely used supervised feature selection methods. The algorithm which we will use returns the ranks of the variables based on the fisher’s score in descending order. We can then select the variables as per the case.\n\n**We were unable to apply Fisher's score due to out of memory errors.**","metadata":{}},{"cell_type":"raw","source":"!pip install skfeature-chappers","metadata":{}},{"cell_type":"markdown","source":"from skfeature.function.similarity_based import fisher_score\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nif kaggle:\n    df_fisher_score = pd.read_pickle('../input/santandercustomertransactionfeatureimportance/df_fisher_score.pkl')\nelif os.path.exists('df_fisher_score.pkl.gz'):\n    df_fisher_score = pd.read_pickle('df_fisher_score.pkl.gz')\nelse:\n    # Calculating scores\n    # Calculating the scores based on X_train gives out of memory error, so let's calculate based on a smaller set\n    train1, test1 = train_test_split(df_train, test_size=0.1, random_state=42)\n    X_train1 = train1.drop([\"ID_code\", \"target\"], axis = 1)\n    y_train1 = train1['target']\n    from sklearn.preprocessing import StandardScaler\n    scaler = StandardScaler()\n    X_train1 = pd.DataFrame(scaler.fit_transform(X_train1), columns = X_train1.columns)\n\n    ranks = fisher_score.fisher_score(X_train1, y_train1)\n\n    # rank features in descending order according to score\n    fisher_feature_rank = fisher_score.feature_ranking(score)\n    df_fisher_score = pd.DataFrame(pd.Series(data=fisher_feature_rank, name='fisher_score')).reset_index()\n    df_fisher_score.to_pickle('df_fisher_score.pkl.gz')","metadata":{}},{"cell_type":"raw","source":"# Plotting the ranks\nfeature_importances = pd.Series(ranks, X_train.columns)\nfeature_importances.plot(kind='barh', color='teal', figsize=(50,300))\nplt.show()","metadata":{}},{"cell_type":"markdown","source":"### 2.6.3. Variance Threshold\n\nThe variance threshold is a simple baseline approach to feature selection. It removes all features which variance doesn’t meet some threshold. By default, it removes all zero-variance features, i.e., features that have the same value in all samples. We assume that features with a higher variance may contain more useful information, but note that we are not taking the relationship between feature variables or feature and target variables into account, which is one of the drawbacks of filter methods.\n\nVariance Threshold is a simple approach to eliminate features based on our expected variance within each feature. Although, there are some down-side with the Variance Threshold method. The Variance Threshold feature selection only sees the input features (X) without considering any information from the dependent variable (y). It is only useful for eliminating features for Unsupervised Modelling rather than Supervised Modelling.\n\n**Attention**: We need to transform all of these numerical features before we use the Variance Threshold Feature Selection as the variance is affected by the numerical scale. (See: https://towardsdatascience.com/5-feature-selection-method-from-scikit-learn-you-should-know-ed4d116e4172)","metadata":{}},{"cell_type":"markdown","source":"With all the features on the same scale, let’s try to select only the features we want using the Variance Threshold method.\n\n**The idea here is to eliminate Quasi-constant variables.** Quasi-constant variables are those that have the same value for the vast majority of observations in the data set. In general, these features provide little or no information for a machine learning model to be able to discriminate or predict a target variable. But there can be exceptions. So, we must be careful when removing this kind of feature.\n\n#### Let’s say to set a thrshold for the minimum variance value.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import VarianceThreshold","metadata":{"execution":{"iopub.status.busy":"2022-06-01T09:20:03.737168Z","iopub.execute_input":"2022-06-01T09:20:03.738322Z","iopub.status.idle":"2022-06-01T09:20:03.742924Z","shell.execute_reply.started":"2022-06-01T09:20:03.738273Z","shell.execute_reply":"2022-06-01T09:20:03.742225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if kaggle:\n    df_train_var_thres_bool = pd.read_pickle('../input/santandercustomertransactionfeatureimportance/df_train_var_thres_bool.pkl')\nelif os.path.exists('df_train_var_thres_bool.pkl.gz'):\n    df_train_var_thres_bool = pd.read_pickle('df_train_var_thres_bool.pkl.gz')\nelse:\n    # threshold = 0.1 indicates that features with a training set variance less than this threshold will be removed (they are quasi-constant features)\n    vt = VarianceThreshold(threshold = 0.01)  \n\n    # Training the model to find variables with low variance.\n    vt.fit(X_train) \n    # Checking the number of variables that are not quasi-constants\n    display(sum(vt.get_support()))\n    v_thrshold = VarianceThreshold(threshold=1)\n    v_thrshold.fit(X_train)\n    df_train_var_thres_bool = pd.DataFrame(pd.Series(data=v_thrshold.get_support(), name='train_var_thres_bool')).reset_index()\n    df_train_var_thres_bool.to_pickle('df_train_var_thres_bool.pkl.gz')\n    # get a list of features with variance == 1\n    df_train_var_thres_bool = X_train.loc[:, v_thrshold.get_support()]\n    df_train_var_thres_bool.columns","metadata":{"execution":{"iopub.status.busy":"2022-06-01T09:20:03.743971Z","iopub.execute_input":"2022-06-01T09:20:03.744314Z","iopub.status.idle":"2022-06-01T09:20:03.763001Z","shell.execute_reply.started":"2022-06-01T09:20:03.744285Z","shell.execute_reply":"2022-06-01T09:20:03.762209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_var_thres_bool","metadata":{"execution":{"iopub.status.busy":"2022-06-01T09:20:03.764109Z","iopub.execute_input":"2022-06-01T09:20:03.764568Z","iopub.status.idle":"2022-06-01T09:20:03.777889Z","shell.execute_reply.started":"2022-06-01T09:20:03.764537Z","shell.execute_reply":"2022-06-01T09:20:03.776615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.6.4. Mean Absolute Difference (MAD)\n\nThe mean absolute difference (MAD) computes the absolute difference from the mean value. The main difference between the variance and MAD measures is the absence of the square in the latter. The MAD, like the variance, is also a scale variant. This means that higher the MAD, higher the discriminatory power.","metadata":{}},{"cell_type":"code","source":"if kaggle:\n    df_mean_abs_diff = pd.read_pickle('../input/santandercustomertransactionfeatureimportance/df_mean_abs_diff.pkl')\nelif os.path.exists('df_mean_abs_diff.pkl.gz'):\n    df_mean_abs_diff = pd.read_pickle('df_mean_abs_diff.pkl.gz')\nelse:\n    # Calculate MAD\n    mean_abs_diff = np.sum(np.abs(X_train - np.mean(X_train, axis=0)), axis=0) / X_train.shape[0]\n    # Rank the features by MAD value, descending order\n    df_mean_abs_diff = pd.DataFrame(pd.Series(data=mean_abs_diff, name='mean_abs_diff')).reset_index()\n    df_mean_abs_diff.sort_values(by=['mean_abs_diff'], ascending=False, inplace=True)\n    df_mean_abs_diff.to_pickle('df_mean_abs_diff.pkl.gz')\n# end if","metadata":{"execution":{"iopub.status.busy":"2022-06-01T09:20:03.779228Z","iopub.execute_input":"2022-06-01T09:20:03.779787Z","iopub.status.idle":"2022-06-01T09:20:03.791024Z","shell.execute_reply.started":"2022-06-01T09:20:03.779755Z","shell.execute_reply":"2022-06-01T09:20:03.789915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_mean_abs_diff.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-06-01T09:20:03.7924Z","iopub.execute_input":"2022-06-01T09:20:03.792758Z","iopub.status.idle":"2022-06-01T09:20:03.803339Z","shell.execute_reply.started":"2022-06-01T09:20:03.79272Z","shell.execute_reply":"2022-06-01T09:20:03.802587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_mean_abs_diff.hist(column='mean_abs_diff', bins=30)","metadata":{"execution":{"iopub.status.busy":"2022-06-01T09:20:03.804662Z","iopub.execute_input":"2022-06-01T09:20:03.80512Z","iopub.status.idle":"2022-06-01T09:20:03.984326Z","shell.execute_reply.started":"2022-06-01T09:20:03.80509Z","shell.execute_reply":"2022-06-01T09:20:03.983333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.6.5. Dispersion ratio\n\nAnother measure of dispersion applies the arithmetic mean (AM) and the geometric mean (GM).\nHigher dispersion implies a higher value of Ri, thus a more relevant feature. Conversely, when all the feature samples have (roughly) the same value, Ri is close to 1, indicating a low relevance feature.","metadata":{}},{"cell_type":"code","source":"if kaggle:\n    df_disp_ratio = pd.read_pickle('../input/santandercustomertransactionfeatureimportance/df_disp_ratio.pkl')\nelif os.path.exists('df_disp_ratio.pkl.gz'):\n    df_disp_ratio = pd.read_pickle('df_disp_ratio.pkl.gz')\nelse:\n    X_1 = X_train + 1  # avoid 0 in the denominator\n    # Arithmetic mean\n    am = np.mean(X_1, axis=0)\n    # Geometric mean\n    gm = np.power(np.prod(X_1, axis=0), 1/X_1.shape[0])\n    # Ratio of Arithmetic and Geometric means\n    disp_ratio = am/gm\n    # Sorting disp_ratio in descending order\n    df_disp_ratio = pd.DataFrame(pd.Series(data=disp_ratio, name='disp_ratio')).reset_index()\n    df_disp_ratio.sort_values(by=['disp_ratio'], ascending=False, inplace=True)\n    df_disp_ratio.to_pickle('df_disp_ratio.pkl.gz')","metadata":{"execution":{"iopub.status.busy":"2022-06-01T09:20:03.985692Z","iopub.execute_input":"2022-06-01T09:20:03.986308Z","iopub.status.idle":"2022-06-01T09:20:03.999402Z","shell.execute_reply.started":"2022-06-01T09:20:03.986264Z","shell.execute_reply":"2022-06-01T09:20:03.998501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_disp_ratio","metadata":{"execution":{"iopub.status.busy":"2022-06-01T09:20:04.001194Z","iopub.execute_input":"2022-06-01T09:20:04.001647Z","iopub.status.idle":"2022-06-01T09:20:04.016228Z","shell.execute_reply.started":"2022-06-01T09:20:04.001602Z","shell.execute_reply":"2022-06-01T09:20:04.015111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.6.6. Forward Feature Selection\n\nThis is an iterative method wherein we start with the best performing variable against the target. Next, we select another variable that gives the best performance in combination with the first selected variable. This process continues until the preset criterion is achieved.\n\n**Important: we should choose the `scoring=roc_auc` option instead of the default `accuracy`, since the target class is imbalanced.**\n\nDocs: http://rasbt.github.io/mlxtend/user_guide/feature_selection/SequentialFeatureSelector/","metadata":{}},{"cell_type":"raw","source":"pip install mlxtend","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom mlxtend.feature_selection import SequentialFeatureSelector","metadata":{"execution":{"iopub.status.busy":"2022-06-01T09:20:04.017851Z","iopub.execute_input":"2022-06-01T09:20:04.020511Z","iopub.status.idle":"2022-06-01T09:20:04.124477Z","shell.execute_reply.started":"2022-06-01T09:20:04.020464Z","shell.execute_reply":"2022-06-01T09:20:04.12359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"(recorded output - I had to erase the actual output because of the elevated number of warnings)\n\n(...)\n\n[2022-05-19 00:01:47] Features: 97/100 -- score: 0.8453200660826633[Parallel(n_jobs=-1)]: Using backend LokyBackend with 48 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  60 out of 103 | elapsed:  2.7min remaining:  1.9min\n[Parallel(n_jobs=-1)]: Done 103 out of 103 | elapsed:  3.7min finished\n\n[2022-05-19 00:05:32] Features: 98/100 -- score: 0.8457850826911045[Parallel(n_jobs=-1)]: Using backend LokyBackend with 48 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  59 out of 102 | elapsed:  2.8min remaining:  2.0min\n[Parallel(n_jobs=-1)]: Done 102 out of 102 | elapsed:  3.8min finished\n\n[2022-05-19 00:09:20] Features: 99/100 -- score: 0.8462397145871037[Parallel(n_jobs=-1)]: Using backend LokyBackend with 48 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  57 out of 101 | elapsed:  2.8min remaining:  2.1min\n[Parallel(n_jobs=-1)]: Done 101 out of 101 | elapsed:  3.9min finished\n\n[2022-05-19 00:13:12] Features: 100/100 -- score: 0.8466816707787078\n\nSequentialFeatureSelector(cv=4,\n                          estimator=LogisticRegression(class_weight='balanced',\n                                                       max_iter=500, n_jobs=-1,\n                                                       random_state=42,\n                                                       solver='saga'),\n                          k_features=100, n_jobs=-1, scoring='roc_auc',\n                          verbose=2)","metadata":{}},{"cell_type":"code","source":"if kaggle:\n    df_bfs = pd.read_pickle('../input/santandercustomertransactionfeatureimportance/df_bfs.pkl')\n    df_backward_selection_stats = pd.read_pickle('../input/santandercustomertransactionfeatureimportance/df_backward_selection_stats.pkl')\nelif os.path.exists('df_bfs.pkl.gz'):\n    df_bfs = pd.read_pickle('df_bfs.pkl.gz')\n    df_backward_selection_stats = pd.read_pickle('df_backward_selection_stats')\nelse:\n    # Sequential Forward Selection\n    lr = LogisticRegression(class_weight='balanced', solver='saga', random_state=42, n_jobs=-1, max_iter=500)\n    # n_features_to_select = 3, cv =10, direction ='backward'\n    bfs = SequentialFeatureSelector(lr, k_features=100, forward=True, scoring='roc_auc', cv=4, n_jobs=-1, verbose=2)\n    bfs.fit(X_train, y_train)\n    bfs_features = list(bfs.k_feature_names_)\n    ###bfs_features = list(map(int, bfs_features))\n    display(len(bfs_features))\n    # X.columns[bfs.get_support()]\n    df_bfs = pd.DataFrame(pd.Series(data=bfs_features, name='bfs_features')).reset_index()\n    df_bfs.to_pickle('df_bfs.pkl.gz')\n    \n    print('\\nSequential Forward Selection (k=100):')\n    print(bfs.k_feature_idx_)\n    print('CV Score:')\n    print(bfs.k_score_)\n    df_backward_selection_stats = pd.DataFrame.from_dict(bfs.get_metric_dict()).T\n    display(df_backward_selection_stats)\n    \n    bfs_features = list(bfs.k_feature_names_)\n    display(len(bfs_features))\n    df_bfs = pd.DataFrame(pd.Series(data=bfs_features, name='bfs_features')).reset_index()\n    df_bfs.to_pickle('df_bfs.pkl.gz')\n    df_backward_selection_stats.to_pickle('df_backward_selection_stats.pkl.gz')","metadata":{"execution":{"iopub.status.busy":"2022-06-01T09:20:04.125845Z","iopub.execute_input":"2022-06-01T09:20:04.126439Z","iopub.status.idle":"2022-06-01T09:20:04.149042Z","shell.execute_reply.started":"2022-06-01T09:20:04.126393Z","shell.execute_reply":"2022-06-01T09:20:04.147869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_backward_selection_stats","metadata":{"execution":{"iopub.status.busy":"2022-06-01T09:20:04.150254Z","iopub.execute_input":"2022-06-01T09:20:04.150576Z","iopub.status.idle":"2022-06-01T09:20:04.178905Z","shell.execute_reply.started":"2022-06-01T09:20:04.150548Z","shell.execute_reply":"2022-06-01T09:20:04.178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.6.7. Backward feature selection","metadata":{}},{"cell_type":"code","source":"# Sequential Backward Selection\nif kaggle:\n    df_bfs = pd.read_pickle('../input/santandercustomertransactionfeatureimportance/df_sbs.pkl')\n    df_forward_selection_stats = pd.read_pickle('../input/santandercustomertransactionfeatureimportance/df_forward_selection_stats.pkl')\nelif os.path.exists('df_sbs.pkl.gz'):\n    df_bfs = pd.read_pickle('df_sbs.pkl.gz')\n    df_forward_selection_stats = pd.read_pickle('df_forward_selection_stats.pkl.gz')\nelse:\n    sbs = SequentialFeatureSelector(lr, k_features=100, forward=False, scoring='roc_auc', cv=4, n_jobs=-1, verbose=2)\n    sbs.fit(X_train, y_train)\n\n    print('\\nSequential Backward Selection (k=100):')\n    print(sbs.k_feature_idx_)\n    print('CV Score:')\n    print(sbs.k_score_)\n    \n    df_forward_selection_stats = pd.DataFrame.from_dict(sbs.get_metric_dict()).T\n    sbs_features = list(sbs.k_feature_names_)\n    display(len(sbs_features))\n    df_sbs = pd.DataFrame(pd.Series(data=sbs_features, name='sbs_features')).reset_index()\n    df_sbs.to_pickle('df_sbs.pkl.gz')\n    df_forward_selection_stats.to_pickle('df_forward_selection_stats.pkl.gz')","metadata":{"execution":{"iopub.status.busy":"2022-06-01T09:20:04.180376Z","iopub.execute_input":"2022-06-01T09:20:04.180891Z","iopub.status.idle":"2022-06-01T09:20:04.203024Z","shell.execute_reply.started":"2022-06-01T09:20:04.18086Z","shell.execute_reply":"2022-06-01T09:20:04.202262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"(recorded output - I had to erase the actual output because of the elevated number of warnings)\n\n\nSequential Backward Selection (k=100):\n\n(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199)\n\nCV Score:\n\n0.8592703741344669\n\n\nSTOPPING EARLY DUE TO KEYBOARD INTERRUPT...","metadata":{}},{"cell_type":"code","source":"display(df_forward_selection_stats)","metadata":{"execution":{"iopub.status.busy":"2022-06-01T09:20:04.204276Z","iopub.execute_input":"2022-06-01T09:20:04.204611Z","iopub.status.idle":"2022-06-01T09:20:04.232369Z","shell.execute_reply.started":"2022-06-01T09:20:04.204582Z","shell.execute_reply":"2022-06-01T09:20:04.231332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.6.8. Recursive Feature Elimination\n\nGiven an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute.\n\nThen, the least important features are pruned from the current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.\n\nBy default, the number of features selected for RFE is the median of the total features, and the step (the number of features eliminated each iteration) is one.","metadata":{}},{"cell_type":"code","source":"# Recursive feature selection\nif kaggle:\n    df_rfe = pd.read_pickle('../input/santandercustomertransactionfeatureimportance/df_rfe.pkl')\nelif os.path.exists('df_sbs.pkl.gz'):\n    df_rfe = pd.read_pickle('df_rfe.pkl.gz')\nelse:\n    from sklearn.feature_selection import RFE\n    rfe = RFE(lr, n_features_to_select=150, step=2, verbose=1)\n    rfe.fit(X_train, y_train)\n    display(X_train.columns[rfe.get_support()])\n    df_rfe = pd.DataFrame(pd.Series(data=rfe.get_support(), name='rfe_features')).reset_index()\n    df_rfe.to_pickle('df_rfe.pkl.gz')\n\n    # y_pred = rfe.predict(X)\n    # y_pred\n# end if\ndf_rfe","metadata":{"execution":{"iopub.status.busy":"2022-06-01T09:20:04.233744Z","iopub.execute_input":"2022-06-01T09:20:04.234683Z","iopub.status.idle":"2022-06-01T09:20:04.258013Z","shell.execute_reply.started":"2022-06-01T09:20:04.234635Z","shell.execute_reply":"2022-06-01T09:20:04.257247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.6.9. LASSO Regularization (L1)\n\nRegularization consists of adding a penalty to the different parameters of the machine learning model to reduce the freedom of the model, i.e. to avoid over-fitting. In linear model regularization, the penalty is applied over the coefficients that multiply each of the predictors. From the different types of regularization, Lasso or L1 has the property that is able to shrink some of the coefficients to zero. Therefore, that feature can be removed from the model.\n\nIn our case, since we are dealing with a binary classification problem, we will replace Linear Regression with Logistic Regression.","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n# Grid search cross validation\nfrom sklearn.model_selection import GridSearchCV\n\nif kaggle:\n    lr_df = pd.read_pickle('../input/santandercustomertransactionfeatureimportance/lr_df.pkl')\n    df_lasso_l1 = pd.read_pickle('../input/santandercustomertransactionfeatureimportance/df_lasso_l1.pkl')\nelif os.path.exists('lr_df.pkl.gz'):\n    lr_df = pd.read_pickle('lr_df.pkl.gz')\n    df_lasso_l1 = pd.read_pickle('df_lasso_l1.pkl.gz')\nelse:\n    logistic = LogisticRegression(solver='liblinear', random_state=42) # .fit(X_train, y_train)\n    parameters = [{'penalty': ['l1','l2']}, \n                  {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}]\n    logreg_cv = GridSearchCV(estimator = logistic,  \n                               param_grid = parameters,\n                               scoring = 'roc_auc',\n                               cv = 5,\n                               verbose=1)\n    logreg_cv.fit(X_train, y_train)\n    print(\"tuned hpyerparameters :(best parameters) \", logreg_cv.best_params_)\n    print(\"accuracy :\", logreg_cv.best_score_)\n    tuned_logistic = logreg_cv.best_estimator_\n    \n    from sklearn.feature_selection import SelectFromModel\n    sfm_selector = SelectFromModel(estimator=tuned_logistic, prefit=True)\n    X_selected = sfm_selector.transform(X_train)\n    # print(X_selected)  # X_train.columns[sfm_selector.get_support()])\n    \n    feature_idx = sfm_selector.get_support()\n    df_lasso_l1 = pd.DataFrame(pd.Series(data=sfm_selector.get_support(), name='sfm_features')).reset_index()\n    df_lasso_l1.to_pickle('df_lasso_l1.pkl.gz')\n    feature_name = X_train.columns[feature_idx]\n    print(len(feature_name))\n    display(feature_name)\n    \n    # Let's obtain the LR coefficients \n    print(tuned_logistic.coef_.shape)\n    print(tuned_logistic.coef_, tuned_logistic.intercept_)\n    \n    # Save LR coefficients to file (we can use them later to determine the most important features)\n    lr_weights = tuned_logistic.coef_.reshape(200,1)\n    # Create a DataFrame with the scores obtained by LR, for each feature\n    lr_df = pd.DataFrame(lr_weights, index=X_train.columns, columns=['score'])\n\n    # Sorting Dataframe by score\n    lr_df = lr_df.sort_values(by='score', ascending = False)\n    lr_df.to_pickle('lr_df.pkl.gz')","metadata":{"execution":{"iopub.status.busy":"2022-06-01T09:20:04.259345Z","iopub.execute_input":"2022-06-01T09:20:04.259761Z","iopub.status.idle":"2022-06-01T09:20:04.282788Z","shell.execute_reply.started":"2022-06-01T09:20:04.259728Z","shell.execute_reply":"2022-06-01T09:20:04.281483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr_df","metadata":{"execution":{"iopub.status.busy":"2022-06-01T09:20:04.284131Z","iopub.execute_input":"2022-06-01T09:20:04.28465Z","iopub.status.idle":"2022-06-01T09:20:04.296912Z","shell.execute_reply.started":"2022-06-01T09:20:04.284595Z","shell.execute_reply":"2022-06-01T09:20:04.296015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.6.10. Random Forest Importance\n\nRandom Forests is a kind of a Bagging Algorithm that aggregates a specified number of decision trees. The tree-based strategies used by random forests naturally rank by how well they improve the purity of the node, or in other words a decrease in the impurity (Gini impurity) over all trees. \n\nNodes with the greatest decrease in impurity happen at the start of the trees, while notes with the least decrease in impurity occur at the end of trees. Thus, **by pruning trees below a particular node, we can create a subset of the most important features**.","metadata":{}},{"cell_type":"code","source":"if kaggle:\n    rf_importance_df = pd.read_pickle('../input/santandercustomertransactionfeatureimportance/rf_importance_df.pkl')\nelif os.path.exists('rf_importance_df.pkl.gz'):\n    rf_importance_df = pd.read_pickle('rf_importance_df.pkl.gz')\nelse:\n    from sklearn.ensemble import RandomForestClassifier\n    # create the random forest model\n    rf_model = RandomForestClassifier(n_estimators = 200, random_state = 42, verbose=1, n_jobs=-1)\n\n    # fit the model to start training\n    rf_model.fit(X_train, y_train)\n\n    # get the importance of the resulting featurs\n    importances = rf_model.feature_importances_\n\n    # create a data frame for visualization\n    rf_importance_df = pd.DataFrame({'feature': X_train.columns, 'rf_importance': importances})\n\n    # sort in descending order\n    rf_importance_df.sort_values('rf_importance', ascending=False, inplace=True)\n    rf_importance_df.to_pickle('rf_importance_df.pkl.gz')","metadata":{"execution":{"iopub.status.busy":"2022-06-01T09:20:04.298641Z","iopub.execute_input":"2022-06-01T09:20:04.299219Z","iopub.status.idle":"2022-06-01T09:20:04.315497Z","shell.execute_reply.started":"2022-06-01T09:20:04.299159Z","shell.execute_reply":"2022-06-01T09:20:04.314489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_importance_df","metadata":{"execution":{"iopub.status.busy":"2022-06-01T09:20:04.316909Z","iopub.execute_input":"2022-06-01T09:20:04.317482Z","iopub.status.idle":"2022-06-01T09:20:04.332498Z","shell.execute_reply.started":"2022-06-01T09:20:04.317436Z","shell.execute_reply":"2022-06-01T09:20:04.33178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.6.11. ANOVA F measure via the f_classif() function","metadata":{}},{"cell_type":"code","source":"if kaggle:\n    df_anova = pd.read_pickle('../input/santandercustomertransactionfeatureimportance/df_anova.pkl')\n    df_anova_scores = pd.read_pickle('../input/santandercustomertransactionfeatureimportance/df_anova_scores.pkl')\nelif os.path.exists('df_anova.pkl.gz'):\n    df_anova = pd.read_pickle('df_anova.pkl.gz')\n    df_anova_scores = pd.read_pickle('df_anova_scores.pkl.gz')\nelse:\n    from sklearn.feature_selection import SelectKBest\n    from sklearn.feature_selection import f_classif\n    from sklearn.feature_selection import mutual_info_classif\n    \n    # define feature selection\n    # Feature selection is performed using ANOVA F measure via the f_classif() function.\n    skb = SelectKBest(score_func=f_classif, k=150)\n    bestFeatuesANOVA = skb.fit_transform(X_train, y_train)\n    bfAnova = X_train.columns[skb.get_support()]\n    \n    # Creating a DataFrame with the scores obtained for each feature\n    sc = pd.DataFrame(skb.scores_, index = X_train.columns, columns = ['score'])\n\n    # Capturing the scores of the best features\n    sc = sc[skb.get_support()]\n\n    # Sorting the Dataframe by score value\n    sc = sc.sort_values(by = 'score', ascending = False)\n    sc.to_pickle('df_anova_scores.pkl.gz')\n\n    df_anova = pd.DataFrame(pd.Series(data=skb.get_support(), name='anova_features')).reset_index()\n    df_anova.to_pickle('df_anova.pkl.gz')\n    print(bfAnova.shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-01T09:20:04.334061Z","iopub.execute_input":"2022-06-01T09:20:04.334794Z","iopub.status.idle":"2022-06-01T09:20:04.356662Z","shell.execute_reply.started":"2022-06-01T09:20:04.334751Z","shell.execute_reply":"2022-06-01T09:20:04.355467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_anova","metadata":{"execution":{"iopub.status.busy":"2022-06-01T09:20:04.35826Z","iopub.execute_input":"2022-06-01T09:20:04.358932Z","iopub.status.idle":"2022-06-01T09:20:04.370918Z","shell.execute_reply.started":"2022-06-01T09:20:04.358884Z","shell.execute_reply":"2022-06-01T09:20:04.370006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Comments\n\nWe will train the classification model with different subsets of features, as shown below. Moreover, we can improve the model with feature engineering (see next section).","metadata":{}},{"cell_type":"markdown","source":"## Summmary\n\nWe have different feature datasets to be tested during model training:\n\n* X_train and X_test ----> 200 features NORMALIZED\n\n* Information Gain ----> 152  features NORMALIZED\n\n* Variance Threshold ----> 101  features NORMALIZED\n\n* Mean-Abs Difference ---->  features NORMALIZED (importance rank)\n\n* Forward Feature Selection ----> 100  features NORMALIZED\n\n* Backward feature selection (sbs) ---> 196 features NORMALIZED\n\n* Recursive Feature Elimination (RFE) ----> features NORMALIZED (importance rank)\n\n* X_train_lasso_l1 and X_test_lasso_l1 ----> 90 features NORMALIZED\n","metadata":{}},{"cell_type":"markdown","source":"## 2.7. Eliminating constant and quasi-constant variables from the dataset\n\nReference: https://franklin390.github.io/Project_04_Santander_Customer_Satisfaction/\n\nOur dataset has a very large number of variables. And it is very likely that there are unnecessary features for our analysis within the dataset. Therefore, we will detect and eliminate variables that are constant and quasi-constant.\n\n### 2.7.1. Constant variables\n\nConstant variables are those that always have the same value for all observations in the data set. That is, the same value for all rows in the dataset. These features do not provide information for a machine learning model to be able to discriminate or predict a target variable. Therefore, we will eliminate them.","metadata":{}},{"cell_type":"code","source":"# Counting the number of unique values of each variable, in the df_train dataset\ndtNunique = X.nunique().sort_values()","metadata":{"execution":{"iopub.status.busy":"2022-06-01T09:20:04.372412Z","iopub.execute_input":"2022-06-01T09:20:04.373017Z","iopub.status.idle":"2022-06-01T09:20:05.583183Z","shell.execute_reply.started":"2022-06-01T09:20:04.372973Z","shell.execute_reply":"2022-06-01T09:20:05.582391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Capturing the variables that are constant, within the training dataset => such variables have only a single value => (nunique == 1)\nvarsWithOneValue = dtNunique[dtNunique == 1]\nvarsWithOneValue","metadata":{"execution":{"iopub.status.busy":"2022-06-01T09:20:05.584478Z","iopub.execute_input":"2022-06-01T09:20:05.585085Z","iopub.status.idle":"2022-06-01T09:20:05.593454Z","shell.execute_reply.started":"2022-06-01T09:20:05.585039Z","shell.execute_reply":"2022-06-01T09:20:05.592255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**No columns with unique values found.**","metadata":{}},{"cell_type":"markdown","source":"### 2.7.2. Quasi-constant variables\n\nQuasi-constant variables are those that have the same value for the vast majority of observations in the data set. In general, these features provide little or no information for a machine learning model to be able to discriminate or predict a target variable. But there can be exceptions. So, we must be careful when removing this kind of feature.","metadata":{}},{"cell_type":"code","source":"import pickle\n\nif kaggle:\n    with open('../input/santandercustomertransactionfeatureimportance/quasiConstantFeatures.pickle', 'rb') as handle:\n        quasiConstantFeatures = pickle.load(handle)\n    # end with\nelif os.path.exists('quasiConstantFeatures.pickle'):\n    with open('quasiConstantFeatures.pickle', 'rb') as handle:\n        quasiConstantFeatures = pickle.load(handle)\n    # end with\nelse:\n    # threshold = 0.1 indicates that the features with a training set variance lower than this threshold will be removed \n    # (they are quasi-constant features)\n    from sklearn.feature_selection import VarianceThreshold\n    quasiConstantFeatures = dict()\n    for threshold in [0.001, 0.01, 0.1]:\n        vt = VarianceThreshold(threshold=threshold)  \n\n        # Training the model to find variables with low variance\n        vt.fit(df_train.drop(labels = ['ID_code', 'target'], axis = 1))\n\n        # Checking the number of variables that are not quasi-constants\n        display(sum(vt.get_support()))\n        constantFeatures = df_train.drop(labels = ['ID_code', 'target'], axis = 1).columns[vt.get_support()]\n\n        # Capturing the name of variables, which are quasi-constants, in the training dataset\n        quasiConstantFeatures[threshold] = [f for f in df_train.drop(labels = ['ID_code', 'target'], axis = 1).columns if f not in constantFeatures ]\n        print(f'quasiConstantFeatures per threshold', quasiConstantFeatures)\n    # end for\n    import pickle\n    with open('quasiConstantFeatures.pickle', 'wb') as handle:\n        pickle.dump(quasiConstantFeatures, handle, protocol=pickle.HIGHEST_PROTOCOL)","metadata":{"execution":{"iopub.status.busy":"2022-06-01T09:20:05.594651Z","iopub.execute_input":"2022-06-01T09:20:05.594979Z","iopub.status.idle":"2022-06-01T09:20:05.608672Z","shell.execute_reply.started":"2022-06-01T09:20:05.594941Z","shell.execute_reply":"2022-06-01T09:20:05.607543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"quasiConstantFeatures","metadata":{"execution":{"iopub.status.busy":"2022-06-01T09:20:05.609802Z","iopub.execute_input":"2022-06-01T09:20:05.610203Z","iopub.status.idle":"2022-06-01T09:20:05.620712Z","shell.execute_reply.started":"2022-06-01T09:20:05.610146Z","shell.execute_reply":"2022-06-01T09:20:05.620019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Comment\n\nLater in the model training stage, we will drop some of these quasi-constant variables to check if the model performance improves.","metadata":{}},{"cell_type":"markdown","source":"## 2.8. Plots of the most important variables: feature values vs. target values\n\nWe will plot the 50 mots important variables (according to the random forest importance function) discriminating by the target variable to see if there is any difference.","metadata":{}},{"cell_type":"code","source":"# Read the dataframe with the Random Forest importance of each feature\nrf_importance_df.sort_values('rf_importance', ascending=False, inplace=True)\ntop_50_features = rf_importance_df.head(50)['feature'].to_list()\ntop_50_features[:10]","metadata":{"execution":{"iopub.status.busy":"2022-06-01T09:20:05.621796Z","iopub.execute_input":"2022-06-01T09:20:05.622627Z","iopub.status.idle":"2022-06-01T09:20:05.634742Z","shell.execute_reply.started":"2022-06-01T09:20:05.622593Z","shell.execute_reply":"2022-06-01T09:20:05.633815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axs = plt.subplots(ncols=3, nrows=17, figsize=(15, 80))\nfor k, var in enumerate(top_50_features):\n    i = k // 3\n    j = k % 3\n    sns.kdeplot(ax=axs[i,j], data=df_train, x=var, hue=\"target\", multiple=\"stack\", bw_adjust=0.5)","metadata":{"execution":{"iopub.status.busy":"2022-06-01T09:20:05.635836Z","iopub.execute_input":"2022-06-01T09:20:05.636466Z","iopub.status.idle":"2022-06-01T09:21:01.280161Z","shell.execute_reply.started":"2022-06-01T09:20:05.636422Z","shell.execute_reply":"2022-06-01T09:21:01.279482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Comments\n\nWe can observe that most of the features follow a distribution very close to the gaussian. Also, the value distribution of some features appear to be bimodal (e.g., `var_13`, `var_40`).","metadata":{}},{"cell_type":"markdown","source":"#### Let's take a look at the violin plots, once again comparing feature and target values","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(ncols=3, nrows=17, figsize=(15, 80))\nfor k, var in enumerate(top_50_features):\n    i = k // 3\n    j = k % 3\n    sns.violinplot(ax=axs[i,j], data=df_train, y=var, x=\"target\")","metadata":{"execution":{"iopub.status.busy":"2022-06-01T09:21:01.281353Z","iopub.execute_input":"2022-06-01T09:21:01.282203Z","iopub.status.idle":"2022-06-01T09:21:33.59962Z","shell.execute_reply.started":"2022-06-01T09:21:01.282147Z","shell.execute_reply":"2022-06-01T09:21:33.598506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Comments\n\nBoxplots look more or less the same. For each feature, regardless of the target value (0 or 1), the distribution looks the same.\n\nAlso there we don't see any extreme outliers in any feature values. **Perhaps the outliers were previously removed to anonymize the data ?**","metadata":{}},{"cell_type":"markdown","source":"# 3. Feature engineering\n\nInspired by:\n\n* https://www.kaggle.com/code/enesimek/for-beginners-santander-prediction-lgbm-model","metadata":{}},{"cell_type":"markdown","source":"## 3.0. Remove ID_code column","metadata":{}},{"cell_type":"code","source":"# Eliminando a variável ID, do dataset de teste\noriginalTestFeatures = df_test.drop('ID_code', axis = 1)\n\n# Capturando a variável ID, do dataset de teste\ntestID = df_test.ID_code","metadata":{"execution":{"iopub.status.busy":"2022-06-01T09:21:33.601086Z","iopub.execute_input":"2022-06-01T09:21:33.601411Z","iopub.status.idle":"2022-06-01T09:21:33.708289Z","shell.execute_reply.started":"2022-06-01T09:21:33.601383Z","shell.execute_reply":"2022-06-01T09:21:33.70712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.1. Create features based on the statistical measures of each dataset column\n\nSome features added:\n\na) maximum value of each row;\n\nb) sum of each row;\n\nc) standard deviation: as we have seen in the EDA part (`Section 2.8`), `target=0` likelihood distributions are more concentrated around their mean than `target=1`.\n\nd) skewness: Skewness refers to a distortion or asymmetry that deviates from the symmetrical bell curve (normal distribution), in a set of data.\n\ne) kurtosis: kurtosis is a measure of whether the data are heavy-tailed or light-tailed, relative to a normal distribution. Kurtosis identifies weather the tails of the distributions contains extreme values. There may be a chance that, if the behavior of customer is different, he will make a transaction.","metadata":{}},{"cell_type":"code","source":"%%time\nidx = features = X_train.columns.values\nfor df in [X_test, X_train]:    \n    df['sum'] = df[idx].sum(axis=1)  \n    df['min'] = df[idx].min(axis=1)\n    df['max'] = df[idx].max(axis=1)\n    df['mean'] = df[idx].mean(axis=1)\n    df['std'] = df[idx].std(axis=1)\n    df['skew'] = df[idx].skew(axis=1)\n    df['kurt'] = df[idx].kurtosis(axis=1)\n    df['med'] = df[idx].median(axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-06-01T09:21:33.709679Z","iopub.execute_input":"2022-06-01T09:21:33.710107Z","iopub.status.idle":"2022-06-01T09:21:41.611651Z","shell.execute_reply.started":"2022-06-01T09:21:33.710076Z","shell.execute_reply":"2022-06-01T09:21:41.610409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train[X_train.columns[200:]].head()","metadata":{"execution":{"iopub.status.busy":"2022-06-01T09:21:41.613651Z","iopub.execute_input":"2022-06-01T09:21:41.614133Z","iopub.status.idle":"2022-06-01T09:21:41.726569Z","shell.execute_reply.started":"2022-06-01T09:21:41.614087Z","shell.execute_reply":"2022-06-01T09:21:41.725492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.2. Create features from the rounding of the value of each existing feature (disabled)\n\nRound each value to 1 or 2 digits after the period.","metadata":{}},{"cell_type":"raw","source":"features = [c for c in df_train.columns if c not in ['ID_code', 'target']]\nfor feature in features:\n    df_train['r2_'+feature] = np.round(df_train[feature], 2)\n    df_test['r2_'+feature] = np.round(df_test[feature], 2)\n    df_train['r1_'+feature] = np.round(df_train[feature], 1)\n    df_test['r1_'+feature] = np.round(df_test[feature], 1)","metadata":{}},{"cell_type":"markdown","source":"## 3.3. Create features from the frequency of each value, on each column (disabled)\n\nTaken from Kernel: https://www.kaggle.com/code/cdeotte/200-magical-models-santander-0-920","metadata":{}},{"cell_type":"raw","source":"# FREQUENCY ENCODE\ndef encode_FE(df,col,test):\n    cv = df[col].value_counts()\n    nm = col+'_FE'\n    df[nm] = df[col].map(cv)\n    test[nm] = test[col].map(cv)\n    test[nm].fillna(0,inplace=True)\n    if cv.max()<=255:\n        df[nm] = df[nm].astype('uint8')\n        test[nm] = test[nm].astype('uint8')\n    else:\n        df[nm] = df[nm].astype('uint16')\n        test[nm] = test[nm].astype('uint16')        \n    return\n\ndf_test['target'] = -1\ncomb = pd.concat([df_train, df_test.loc[real_samples_indexes]], axis = 0, sort = True)\nfor i in range(200):\n    encode_FE(comb, 'var_'+str(i), df_test)\ndf_train = comb[:len(train)]; del comb\nprint('Added 200 new magic features!')","metadata":{}},{"cell_type":"markdown","source":"# 4. Let's try the Pycaret library for model selection and hyperparameter tuning (not in use)\n\nPyCaret is an open-source, low-code machine learning library in Python that automates machine learning workflows.\nPyCaret is essentially a Python wrapper around several machine learning libraries and frameworks such as scikit-learn, XGBoost, LightGBM, CatBoost, spaCy, Optuna, Hyperopt, Ray, and a few more.\n","metadata":{}},{"cell_type":"raw","source":"!pip install pycaret==2.3.10","metadata":{}},{"cell_type":"raw","source":"!conda install -c conda-forge pycaret","metadata":{}},{"cell_type":"markdown","source":"## 4.1. A note on the train and test datasets (from Section 2.5.0)\n\nTo effectively apply the final prediction on unseen data (with Pycaret `predict_model()` function), in `Section 2.5.0` we took a sample of 25% of the records from the original dataset, to be used for predictions at the end. \n\nHowever, this should not be confused with a train-test-split for cross-validation purposes, as this particular split is performed to simulate a real-life scenario. Another way to think about this is that these 25% transactions are not available at the time of training of machine learning models.","metadata":{}},{"cell_type":"code","source":"# data constains 75% of the original Kaggle training dataset, data_unseen contains the other 25%\ndata = pd.concat([X_train, y_train], axis=1)\ndata_unseen = pd.concat([X_test, y_test], axis=1)\n\n# print the revised shape\nprint('Data for Modeling: ' + str(data.shape))\nprint('Unseen Data For Predictions: ' + str(data_unseen.shape))","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-06-01T09:21:41.728119Z","iopub.execute_input":"2022-06-01T09:21:41.728633Z","iopub.status.idle":"2022-06-01T09:21:41.889068Z","shell.execute_reply.started":"2022-06-01T09:21:41.728586Z","shell.execute_reply":"2022-06-01T09:21:41.887724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.2. Pre-process the dataset\n\nLet's remove the `ID_code` column, which is not a useful feature.","metadata":{}},{"cell_type":"code","source":"data = data[[_ for _ in data.columns if _ != 'ID_code']]\ndata_unseen = data_unseen[[_ for _ in data_unseen.columns if _ != 'ID_code']]\n\nprint('Data for Modeling: ' + str(data.shape))\nprint('Unseen Data For Predictions: ' + str(data_unseen.shape))","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-06-01T09:21:41.890772Z","iopub.execute_input":"2022-06-01T09:21:41.891142Z","iopub.status.idle":"2022-06-01T09:21:42.031884Z","shell.execute_reply.started":"2022-06-01T09:21:41.89111Z","shell.execute_reply":"2022-06-01T09:21:42.030567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.4. Setting up Pycaret Environment  (not in use)\n\n### 4.4.1. Set up an experiment in PyCaret and get started with building classification models.\n\n\n\nThe setup function in PyCaret initializes the environment and creates the transformation pipeline for modeling and deployment. setup must be called before executing any other function in pycaret. It takes two mandatory parameters: a pandas dataframe and the name of the target column. All other parameters are optional can be used to customize the preprocessing pipeline.\n\nWhen setup is executed, PyCaret's inference algorithm will automatically infer the data types for all features based on certain properties. The data type should be inferred correctly but this is not always the case. To handle this, PyCaret displays a prompt, asking for data types confirmation, once you execute the setup. You can press enter if all data types are correct or type quit to exit the setup.\n\nEnsuring that the data types are correct is really important in PyCaret as it automatically performs multiple type-specific preprocessing tasks which are imperative for machine learning models.","metadata":{}},{"cell_type":"code","source":"feature_index_list = [_ for _ in range(X_train.shape[1])] # fs.get_support(indices=True)\n###feature_index_list = fs2.get_support(indices=True)\nfeature_index_list = list(feature_index_list)","metadata":{"execution":{"iopub.status.busy":"2022-06-01T09:21:42.033619Z","iopub.execute_input":"2022-06-01T09:21:42.034083Z","iopub.status.idle":"2022-06-01T09:21:42.039891Z","shell.execute_reply.started":"2022-06-01T09:21:42.034038Z","shell.execute_reply":"2022-06-01T09:21:42.038715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"selected_features = X_train.columns[feature_index_list]\nselected_features = [str(_) for _ in selected_features]","metadata":{"execution":{"iopub.status.busy":"2022-06-01T09:21:42.042367Z","iopub.execute_input":"2022-06-01T09:21:42.042658Z","iopub.status.idle":"2022-06-01T09:21:42.051476Z","shell.execute_reply.started":"2022-06-01T09:21:42.042632Z","shell.execute_reply":"2022-06-01T09:21:42.050414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Train-test split for model traning and Pycaret setup\n\nTo perform model training, we will use a typical 75% x 25% to split the dataset into train and test data (`train_size=0.75` parameter).\n\n### 4.2.2. Test Pycaret AutoML on the original dataset (including all features)  (not in use)\n\nLet's perform a first model training with Pycaret, considering the original / full set of features, including the engineered ones. We will use the obtained results as a baseline for more advanced models and future tests with a subset of the original features.","metadata":{}},{"cell_type":"markdown","source":"# init pycaret setup (not in use)\nfrom pycaret.classification import *\n###selected_features.append('target')\nused_feature_list = selected_features + ['target']\nused_feature_list = list(data.columns)\ns = setup(data = data[used_feature_list], target = 'target', session_id=123,\n                  n_jobs = -1, \n                  # use_gpu = True,\n                  train_size = 0.75, \n                  # fix_imbalance=True, \n                  remove_perfect_collinearity = False,\n                  #normalize = True, \n                  transformation = True, \n                  ###ignore_low_variance = True,\n                  #feature_selection = True, \n                  #feature_selection_threshold = 0.8, \n                  #feature_selection_method = 'boruta',\n                  #remove_multicollinearity = True, multicollinearity_threshold = 0.95,\n                  #pca = True, pca_method = 'linear', pca_components = 10\n         )","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-05-31T17:09:46.443919Z","iopub.execute_input":"2022-05-31T17:09:46.444363Z","iopub.status.idle":"2022-05-31T17:09:46.470561Z","shell.execute_reply.started":"2022-05-31T17:09:46.444329Z","shell.execute_reply":"2022-05-31T17:09:46.4695Z"}}},{"cell_type":"markdown","source":"transformed_x = get_config('X_test')\nfinal_features = [x for x in transformed_x.columns]\nfinal_features","metadata":{}},{"cell_type":"markdown","source":"## 4.2.3. Create baseline models and compare them  (not in use)\n\n### Create different models, perform stratified cross-validation and evaluate classification metrics\n\nComparing all models to evaluate performance is the recommended starting point for modeling once the setup is completed (unless you exactly know what kind of model you need, which is often not the case). This function trains all models in the model library and scores them using stratified cross-validation for metric evaluation. The output prints a scoring grid that shows average Accuracy, AUC, Recall, Precision, F1, Kappa, and MCC across the folds (10 by default) along with training times.\n\n#### Comments\n\nThe scoring grid printed below highlights the highest performing metric for comparison purposes only. The grid by default is sorted using Accuracy (highest to lowest), but we will use the `sort='AUC'` parameter, since the binary `target` value is highly unbalanced and the area under de ROC curve is a better metric in these cases.\n\nAlso, to speed-up training times, we will reduce the number of CV fold to 5.\n\n**Important**: As we will see in the results below, the metrics of interest (AUC, and very low Precision, Recall and F1) are really bad! This is because, even though the results are sorted according to the `AUC` metric, the model training was performed using the `Accuracy` metric, which is inadequate for our unbalanced case.","metadata":{}},{"cell_type":"markdown","source":"top5 = compare_models(n_select = 3, sort='AUC', turbo=True, fold=5)","metadata":{"tags":[]}},{"cell_type":"markdown","source":"## 4.3. Defining auxiliary functions for model training purposes\n\nInstead of using Pycaret AutoML, we will now define a couple of datasets, each one with a different type of normalization / standardization, and with a distinct subset of features.","metadata":{}},{"cell_type":"markdown","source":"### 4.3.1. Data transform function to apply different types of stanrdization and normalization","metadata":{}},{"cell_type":"markdown","source":"#### Applying different scales to train and test data","metadata":{}},{"cell_type":"code","source":"%%time\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, PowerTransformer, normalize\n\ntrain_dict = dict()\ntest_dict = dict()\ntrainFeatures = train.drop([\"ID_code\", \"target\"], axis = 1)\ntestFeatures = test.drop([\"ID_code\", \"target\"], axis = 1)\n\n# Criando um objeto da classe MinMaxScaler().\nscaler = MinMaxScaler()\n# Aplicando a escala nas Features e capturando o resultado obtido.\ntrain_dict['MM'] = scaler.fit_transform(trainFeatures)\ntest_dict['MM'] = scaler.fit_transform(testFeatures)\n# Criando um DataFrame com os resultados obtidos.\ntrain_dict['MM'] = pd.DataFrame(data = train_dict['MM'], columns = trainFeatures.columns)\ntest_dict['MM'] = pd.DataFrame(data = test_dict['MM'], columns = testFeatures.columns)\n\n# Criando um objeto da classe StandardScaler().\nscaler = StandardScaler()\n# Aplicando a escala nas Features e capturando o resultado obtido.\ntrain_dict['SS'] = scaler.fit_transform(trainFeatures)\ntest_dict['SS'] = scaler.fit_transform(testFeatures)\n# Criando um DataFrame com os resultados obtidos.\ntrain_dict['SS'] = pd.DataFrame(data = train_dict['SS'], columns = trainFeatures.columns)\ntest_dict['SS'] = pd.DataFrame(data = test_dict['SS'], columns = testFeatures.columns)\n\n# Criando um objeto da classe PowerTransformer().\nscaler = PowerTransformer(method = 'yeo-johnson', standardize = False)\n# Aplicando a escala nas Features e capturando o resultado obtido.\ntrain_dict['ND'] = scaler.fit_transform(train_dict['SS'])\ntest_dict['ND'] = scaler.fit_transform(test_dict['SS'])\n# Criando um DataFrame com os resultados obtidos.\ntrain_dict['ND'] = pd.DataFrame(data = train_dict['ND'], columns = trainFeatures.columns)\ntest_dict['ND'] = pd.DataFrame(data = test_dict['ND'], columns = trainFeatures.columns)\n\n# Normalizando cada feature para uma unidade uniforme (vetor unitário).\ntrain_dict['NM'] = normalize(trainFeatures, axis = 1)\ntest_dict['NM'] = normalize(testFeatures, axis = 1)\n# Criando um DataFrame com os resultados obtidos.\ntrain_dict['NM'] = pd.DataFrame(data = train_dict['NM'], columns = trainFeatures.columns)\ntest_dict['NM'] = pd.DataFrame(data = test_dict['NM'], columns = trainFeatures.columns)","metadata":{"execution":{"iopub.status.busy":"2022-06-01T09:21:42.053124Z","iopub.execute_input":"2022-06-01T09:21:42.053481Z","iopub.status.idle":"2022-06-01T09:23:18.849825Z","shell.execute_reply.started":"2022-06-01T09:21:42.053452Z","shell.execute_reply":"2022-06-01T09:23:18.848791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.3.2. Create a dict with different subsets of features to use during model training\n\nPreviously, in `Section 2.6. Feature Selection`, we applied several methods to select the most important model features, and thus reduce the number of features to be used in model training. We will now create a `dict()` with these feature lists, to help us traing different models.","metadata":{}},{"cell_type":"code","source":"# Criando uma lista com todos os resultados gerados pelas técnicas de Feature Selection utilizadas anteriormente\nfeaturelist_dict = dict()\ndf_dict_feature_score = dict()\n\n# Load all feature list from each method used\ndf_dict_feature_score['information_gain'] = df_information_gain.rename(columns={\"information_gain\": \"score\"}) # information_gain (descending), up to  152 features\ndf_dict_feature_score['mean_abs_diff'] = df_mean_abs_diff.rename(columns={\"mean_abs_diff\": \"score\"}) # mean_abs_diff (descending)\ndf_dict_feature_score['disp_ratio'] = df_disp_ratio.rename(columns={\"disp_ratio\": \"score\"}) # disp_ratio (descending)\ndf_dict_feature_score['lr'] = lr_df.reset_index() # 'score'\ndf_dict_feature_score['anova'] = df_anova_scores.reset_index() # anova: 'score', up to 150 features\ndf_dict_feature_score['rf'] = rf_importance_df.rename(columns={\"rf_importance\": \"score\", \"feature\": \"index\"}) # rf_importance\n\nnum_feature_list = [150]  # 20, 40, 80, 120\n\nfor fs_method_name, fs_df in df_dict_feature_score.items():\n    print(f'Process feature selection method: {fs_method_name}')\n    fs_df.sort_values(by=['score'], ascending=False, inplace=True)\n    for num_features in num_feature_list:\n        featurelist = fs_df.head(num_features)['index'].to_list()\n        featurelist_dict[fs_method_name + '_nf-' + str(num_features)] = featurelist\n    # end for\n# end for\nfeaturelist_dict.keys()","metadata":{"execution":{"iopub.status.busy":"2022-06-01T09:23:18.851045Z","iopub.execute_input":"2022-06-01T09:23:18.85173Z","iopub.status.idle":"2022-06-01T09:23:18.872996Z","shell.execute_reply.started":"2022-06-01T09:23:18.851695Z","shell.execute_reply":"2022-06-01T09:23:18.872159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# the forward and backward methods have a different data structure regarning feature scores\ndf_dict_feature_score['bfs'] = df_backward_selection_stats #.rename(columns={\"bfs_features\": \"score\"}) # bfs_features, up to  100 features\ndf_dict_feature_score['sbs'] = df_forward_selection_stats #.rename(columns={\"sbs_features\": \"score\"}) # sbs_features, up to 196 features\n\n# feature_idx \tcv_scores \tavg_score \tfeature_names\n#fs_method_name = 'bfs'\n#for row in df_dict_feature_score['bfs'].itertuples():  # 1 .. 100\n#    num_features = len(row.feature_names)\n#    if num_features in [100]:  # 40, 80\n#        featurelist = row.feature_names\n#        featurelist_dict[fs_method_name + '_nf-' + str(num_features)] = list(featurelist)\n#        print(num_features)\n#        print(featurelist)\n#    # end if\n# end for\nfs_method_name = 'sbs'\nfor row in df_dict_feature_score['sbs'].itertuples():  # 196 .. 200\n    num_features = len(row.feature_names)\n    if num_features in [196]:\n        featurelist = row.feature_names\n        featurelist_dict[fs_method_name + '_nf-' + str(num_features)] = list(featurelist)\n        print(num_features)\n        print(featurelist)\n    # end if\n# end for\n\n# the full feature set - with or without stats features ; with or without quasiConstantFeatures\n# Use the quasiConstantFeatures dict()\ndrop_cols = ['sum', 'min', 'max', 'mean', 'std', 'skew', 'kurt', 'med']\n# featurelist_dict['full_plus_stats' + '_nf-' + str(len(X_train.columns))] = X_train.columns.to_list()\nX_original = X_train.drop(columns=drop_cols)\nfeaturelist_dict['original' + '_nf-' + str(len(X_original.columns))] = X_original.columns.to_list()\nX_original2 = X_original.drop(columns=quasiConstantFeatures[0.1])\nfeaturelist_dict['original_without_quasi_constant' + '_nf-' + str(len(X_original2.columns))] = X_original2.columns.to_list()\n\n# the lasso_l1 and anova methods have generated a single list of features, with a specific number of variables\ndf_dict_feature_score['lasso_l1'] = df_lasso_l1 # sfm_features, 90 features\ndf_dict_feature_score['anova'] = df_anova # anova_features, 150 features\nfeaturelist_dict['lasso_l1' + '_nf-' + str(90)] = X.columns[df_dict_feature_score['lasso_l1']['sfm_features']].to_list()\nfeaturelist_dict['anova' + '_nf-' + str(150)] = X.columns[df_dict_feature_score['anova']['anova_features']].to_list()\n","metadata":{"execution":{"iopub.status.busy":"2022-06-01T09:23:18.874475Z","iopub.execute_input":"2022-06-01T09:23:18.875209Z","iopub.status.idle":"2022-06-01T09:23:19.106715Z","shell.execute_reply.started":"2022-06-01T09:23:18.875149Z","shell.execute_reply":"2022-06-01T09:23:19.105696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# obtain the dataset on the selected features\n# featurelist_dict['lasso_l1_nf-90']\nfeaturelist_dict.keys()","metadata":{"execution":{"iopub.status.busy":"2022-06-01T09:23:19.108434Z","iopub.execute_input":"2022-06-01T09:23:19.108892Z","iopub.status.idle":"2022-06-01T09:23:19.115586Z","shell.execute_reply.started":"2022-06-01T09:23:19.108846Z","shell.execute_reply":"2022-06-01T09:23:19.114611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.3.3. Boxplot function","metadata":{}},{"cell_type":"code","source":"# Definindo uma função para criar gráficos de Boxplot interativos com o plotly\ndef plotBoxplot(data, filename, title = 'Comparison of classification algorithms', yaxis = 'ROC', xaxis = 'Algorithm'):\n    \n    plt.figure(figsize=(15,10))\n    ax = sns.boxplot(x=\"model\", y=\"value\", data=data)\n    ax.set_title(title)\n    fig = ax.get_figure()\n    fig.savefig(f\"{filename}.png\")","metadata":{"execution":{"iopub.status.busy":"2022-06-01T09:23:19.117116Z","iopub.execute_input":"2022-06-01T09:23:19.118277Z","iopub.status.idle":"2022-06-01T09:23:19.132042Z","shell.execute_reply.started":"2022-06-01T09:23:19.118228Z","shell.execute_reply":"2022-06-01T09:23:19.131207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.3.4. Model training functions\n\nLet's define functions to automate model training, testing each combination of normalization method, feature set, and removal of quasiConstantFeatures. \n\nWe will test a couple of classifier algorithms using the `roc_auc` metric and `StratifiedKFold` with 5 folds:\n\n* Logistic Regression (LR)\n* Gaussian NB (NB)\n* BalancedRandomForest (BRF)\n* XGBoost\n* LightGBM (LGBM)","metadata":{}},{"cell_type":"code","source":"# Definindo uma função, para treinar diferentes algoritmos, para prever a variável Target.\ndef classifiersTraining(features, tTarget, model_label, printMeans = True, scoring = 'roc_auc', \n                        make_plots=True, num_folds = 5):\n    import time\n    import os.path\n    \n    if kaggle:\n        base_folder = '../input/santandercustomertransactionv1models'\n    else:\n        base_folder = '.'\n    # end if\n    results_filename = os.path.join(base_folder, f'test1_model_results-{model_label}.pkl')\n    means_filename = os.path.join(base_folder, f'test1_model_means-{model_label}.pkl')\n    if not kaggle:\n        results_filename += '.gz'\n        means_filename += '.gz'\n    # end if\n    if os.path.isfile(results_filename) and os.path.isfile(means_filename):\n        print(f'Results filename {results_filename} already exists. Skipping training.')\n        results = pd.read_pickle(results_filename)\n        means = pd.read_pickle(means_filename)\n    else:    \n        # Definindo os valores, para o número de folds e para o seed.\n        seed      = 100\n\n        # Criando uma lista para armazenar os modelos que serão utilizados.\n        models = []\n\n        # Adicionando os modelos a lista.\n        models.append(('LR'     , LogisticRegression(C=0.1, class_weight='balanced', n_jobs=-1)         ))\n        # models.append(('LDA'    , LinearDiscriminantAnalysis() ))\n\n        gnb = GaussianNB()\n        gnb_param = {'var_smoothing': np.logspace(0,-15, num=20)}\n        gnb_cv = GridSearchCV(gnb, gnb_param, cv=num_folds, scoring=scoring, verbose=0, n_jobs=-1, return_train_score=True)\n        models.append(('NB'     , gnb_cv                 ))\n\n        # models.append(('KNN'    , KNeighborsClassifier()       ))\n        # models.append(('CART'   , DecisionTreeClassifier()     ))\n\n        # rf_classifier = RandomForestClassifier(n_estimators=200, max_depth=10, class_weight='balanced_subsample', n_jobs=-1, oob_score=True, max_samples=0.4, verbose=1)\n        # models.append(('RF'     , rf_classifier     ))\n\n        brf_classifier = BalancedRandomForestClassifier(max_depth=15, n_estimators=200, oob_score=True, replacement=True, verbose=1, max_samples=0.4, n_jobs=-1)\n        models.append(('BRF'     , brf_classifier     ))\n\n        # easy=EasyEnsembleClassifier(n_estimators=50, sampling_strategy='majority', random_state=42, replacement=True, verbose=1, n_jobs=-1)\n        # models.append(('EEC'     , easy     ))\n\n        xgb_class = XGBClassifier(max_depth=4, n_estimators=600, eta=0.1, booster='gbtree', subsample=0.4, verbosity=1, scale_pos_weight=8,\n                      colsample_bytree=0.2, tree_method='gpu_hist', eval_metric='aucpr', use_label_encoder=False)\n        models.append(('XGBoost', xgb_class          ))\n\n        lgbm_param = {\n             'num_leaves': 18,\n             'max_bin': 63,\n             'min_data_in_leaf': 5,\n             'learning_rate': 0.010614430970330217,\n             'min_sum_hessian_in_leaf': 0.0093586657313989123,\n             'feature_fraction': 0.056701788569420042,\n              # 'lambda_l1': 0.060222413158420585,\n              # 'lambda_l2': 4.6580550589317573,\n             'min_gain_to_split': 0.29588543202055562,\n             'max_depth': 49,\n             'save_binary': True,\n             'seed': 1337,\n             'feature_fraction_seed': 1337,\n             'bagging_seed': 1337,\n             'drop_seed': 1337,\n             'data_random_seed': 1337,\n             'objective': 'binary',\n             'boosting_type': 'gbdt',\n             'verbose': 1,\n             'metric': 'auc',\n             'is_unbalance': True,\n             'boost_from_average': False\n        }\n        lgbm_class = lightgbm.LGBMClassifier(**lgbm_param)\n        models.append(('LGBM',    lgbm_class    ))\n\n        # Criando Listas, para armazenar os resultados e os nomes, de cada um dos algoritmos testados.\n        results = []\n        names   = []\n\n        # Criando um Dataframe, para armazenar a média e o desvio-padrão, de cada um dos algoritmos testados.\n        means   = pd.DataFrame(columns = ['mean', 'std'])\n\n        # Avaliando cada um dos modelos da lista de modelos.\n        for name, model in models:\n            start = time.time()\n\n            # IMPORTANTE! Instanciando um objeto da StratifiedKFold para criar os folds, pois ha desbalanceamento do target\n            kfold = StratifiedKFold(n_splits = num_folds , shuffle = True, random_state = seed)\n\n            # Treinando o modelo com Cross Validation.\n            cv_results = cross_val_score(model, features, tTarget, cv = kfold, scoring = scoring)\n\n            # Adicionando os resultados gerados a lista de resultados.\n            results.append(cv_results)\n\n            # Adicionando o nome do modelo avaliado a lista de nomes.\n            names.append(name)\n\n            stop = time.time()\n            time_spent = stop - start\n            print(f\"{name} model training time: {time_spent}s\")\n\n            # Adicionando a média e o desvio-padrão dos resultados gerados, pelo modelo analisado ao Dataframe de médias.\n            means = means.append(\n                pd.DataFrame (\n                    data    = [[cv_results.mean(), cv_results.std(), time_spent]], \n                    columns = ['mean', 'std', 'time_spent_s'], \n                    index   = [name]\n                )\n            )\n\n            # Imprime uma mensagem, contendo os resultados obtidos, ao fim do treinamento de cada um dos modelos.\n            if printMeans:\n                # Cria a mensagem a ser impressa.\n                msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n\n                # Imprime a mensagem.\n                print(msg)\n            # end if\n        # end for\n\n        # Cria um DataFrame com os resultados obtidos por cada um dos modelos avaliados.\n        results = pd.DataFrame(np.transpose(results), columns = names)\n\n        # Salva os resultados do treinamento em disco\n        results.to_pickle(results_filename)\n        print(f'Saved file {results_filename}.')\n        display(results)\n\n        means.to_pickle(means_filename)\n        print(means_filename)\n        display(means)\n    # end if\n    \n    if make_plots:\n        # Plotando os scores da metrica escolhida dos classificadores treinados, na forma de boxplots\n        data_to_plot = pd.melt(results.reset_index(), id_vars=['index'], var_name='model', value_vars=results.columns.to_list())\n        plotBoxplot(data = data_to_plot, filename = f'boxplot_{model_label}')\n    # end if\n    # Retorna o DataFrame, com os resultados e com as médias geradas.\n    return (results, means)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-01T09:23:19.133683Z","iopub.execute_input":"2022-06-01T09:23:19.134138Z","iopub.status.idle":"2022-06-01T09:23:19.158298Z","shell.execute_reply.started":"2022-06-01T09:23:19.134092Z","shell.execute_reply":"2022-06-01T09:23:19.157291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Definindo uma função para realizar a plotagem de Confusions Matrix\ndef plotConfusionMatrix(data, labels, figsize = (6, 6), fontScale = 1.2, title = 'Confusion Matrix', \n                        xlabel = 'Actual', ylabel = 'Predicted'):\n\n    # Definindo a área de plotagem e suas dimensões.\n    _, ax = plt.subplots(figsize = figsize)\n\n    # Definindo o tamanho da fonte utilizada no gráfico.\n    sns.set(font_scale = fontScale)\n\n    # Criando Heatmap para representar a Confusion Matrix.\n    ax = sns.heatmap (\n        data       = data,\n        annot      = True,\n        cmap       = 'Blues',\n        linewidths = 5,\n        cbar       = False,\n        #fmt        = 'd'\n    ) \n\n    # Definindo as labels e o título do gráfico. \n    ax.set_xlabel(xlabel)\n    ax.set_ylabel(ylabel) \n    ax.set_title(title)\n\n    # Definindo as ticklabels do gráfico.\n    ax.xaxis.set_ticklabels(labels)\n    ax.yaxis.set_ticklabels(labels);","metadata":{"execution":{"iopub.status.busy":"2022-06-01T09:23:19.159971Z","iopub.execute_input":"2022-06-01T09:23:19.160467Z","iopub.status.idle":"2022-06-01T09:23:19.17767Z","shell.execute_reply.started":"2022-06-01T09:23:19.160431Z","shell.execute_reply":"2022-06-01T09:23:19.17656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Definindo uma função para criar uma Confusion Matrix\ndef confusionMatrix(yTrue, yPred, labelPositive = 'Yes', labelNegative = 'No', classError = True):\n\n    # Convertendo arrays para o tipo de dado categórico.\n    labels = [labelNegative, labelPositive]\n    yTrue = pd.Categorical(values = yTrue, categories = labels)\n    yPred = pd.Categorical(values = yPred, categories = labels)\n\n    # Transformando arrays em Séries Temporais.\n    yPred = pd.Series(data = yPred, name = 'Predicted')\n    yTrue = pd.Series(data = yTrue, name = 'Actual')\n\n    # Criando a Confusion Matrix.\n    cm = pd.crosstab(index = yPred, columns = yTrue, dropna = False)\n\n    # Calculando os erros, das classes da Confusion Matrix.\n    if classError:\n        # Capturando cada um dos valores da Confusion Matrix.\n        truePositve, falsePositive, falseNegative, trueNegative = np.array(cm).ravel()\n\n        # Criando um DataFrame, contendo os erros das classes.\n        ce = pd.DataFrame (\n            data = [\n                falsePositive / (truePositve + falsePositive),\n                1 - trueNegative / (trueNegative + falseNegative)\n            ],\n            columns = ['classError'],\n            index   = labels\n        )\n\n        # Inserindo no DataFrame, as colunas da Confusion Matrix.\n        for c in range(cm.shape[1] - 1, -1, -1):\n            # Inserindo as colunas no DataFrame.\n            ce.insert(loc = 0, column = labels[c], value = cm[labels[c]])\n\n        # Atribuindo índices e colunas ao DataFrame.\n        ce.index   = pd.Series(ce.index, name = 'Predicted')\n        ce.columns = pd.Series(ce.columns, name = 'Actual')\n\n        # Retornando a Confusion Matrix, com o erro das classes.\n        return ce\n\n    # Retornando Confusion Matrix\n    return cm","metadata":{"execution":{"iopub.status.busy":"2022-06-01T09:23:19.178931Z","iopub.execute_input":"2022-06-01T09:23:19.179606Z","iopub.status.idle":"2022-06-01T09:23:19.195073Z","shell.execute_reply.started":"2022-06-01T09:23:19.17957Z","shell.execute_reply":"2022-06-01T09:23:19.193819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Definindo uma função, para realizar o cálculo do Intervalo de Confiança, da acurácia e do erro de modelos \n# preditivos de classificação.\ndef CI(score, nElements,  ci = .95):\n\n    # Define o Z-score, a ser utilizado para o Intervalo de Confiança selecionado.\n    z = {\n        .90  : 1.645,      # Para Intervalos com 90% de Confiança.\n        .95  : 1.96,       # Para Intervalos com 95% de Confiança.\n        .98  : 2.326,      # Para Intervalos com 98% de Confiança.\n        .99  : 2.576,      # Para Intervalos com 99% de Confiança.\n        .995 : 2.807,      # Para Intervalos com 99.5% de Confiança.\n        .999 : 3.291       # Para Intervalos com 99.9% de Confiança.\n    }\n\n    # Calculando o range de variação do Intervalo.\n    interval = z.get(ci) * np.sqrt( (score * (1 - score)) / nElements)\n\n    # Retornando o Intervalo de Confiança obtido.\n    return score - interval, score + interval","metadata":{"execution":{"iopub.status.busy":"2022-06-01T09:23:19.196933Z","iopub.execute_input":"2022-06-01T09:23:19.197422Z","iopub.status.idle":"2022-06-01T09:23:19.212859Z","shell.execute_reply.started":"2022-06-01T09:23:19.197375Z","shell.execute_reply":"2022-06-01T09:23:19.211872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Definindo uma função para calcular as métricas baseadas na Confusion Matrix\ndef getClassificationMetrics(yTrue, predProb, labelPositive = 'Yes', labelNegative = 'No'):\n\n    # Binarizando os scores obtidos nas previsões.\n    yPred = [labelPositive if v >= 0.5 else labelNegative for v in predProb]\n    \n    # Convertendo arrays para o tipo categórico.\n    labels = [labelPositive, labelNegative]\n\n    yTrue = pd.Categorical(values = yTrue, categories = labels)\n    yPred = pd.Categorical(values = yPred, categories = labels)\n\n    # Convertendo arrays para o tipo numérico. \n    yNTrue = [1 if v == labelPositive else 0 for v in yTrue]\n    yNPred = [1 if v == labelPositive else 0 for v in yPred]\n\n    # Transformando arrays em Séries Temporais.\n    yPred = pd.Series(data = yPred, name = 'Predicted')\n    yTrue = pd.Series(data = yTrue, name = 'Actual')\n\n    # Criando a Confusion Matrix.\n    cm = confusionMatrix(yTrue, yPred, labelPositive = labelPositive, labelNegative = labelNegative, classError = False)\n\n    # Capturando cada um dos valores da Confusion Matrix.\n    truePositve, falsePositive, falseNegative, trueNegative = np.array(cm).ravel()\n\n    # Calculando as métricas.\n    accuracy     = accuracy_score(yTrue, yPred)\n    accuracyCI   = CI(accuracy_score(yTrue, yPred), len(yTrue))\n    kappa        = cohen_kappa_score(yTrue, yPred)\n    sensitivity  = recall_score(yNTrue, yNPred)\n    specificity  = trueNegative /(trueNegative + falsePositive)\n    prevalence   = (truePositve + falseNegative) / len(yTrue)\n    ppv          = (sensitivity * prevalence) /((sensitivity * prevalence) + ((1 - specificity) * (1 - prevalence)))\n    npv          = (specificity * (1 - prevalence)) / (((1 - sensitivity) * prevalence) + ((specificity) * (1 - prevalence)))\n    precision    = precision_score(yNTrue, yNPred)\n    avgPrecision = average_precision_score(yNTrue, yNPred)\n    dRate        = truePositve / len(yTrue)\n    dPrevalence  = (truePositve + falsePositive) / len(yTrue)\n    f1           = f1_score(yNTrue, yNPred)\n    rocAuc       = roc_auc_score(yNTrue, predProb)\n    Aucpr        = average_precision_score(yNTrue, predProb)\n    error        = 1 - accuracy_score(yTrue, yPred)\n    errorCI      = CI(error, len(yTrue))\n    bAccuracy    = balanced_accuracy_score(yTrue, yPred)\n\n    # Criando um DataFrame, com o resultado das métricas calculadas.\n    metrics = pd.DataFrame([{\n        'Accuracy'            : accuracy,     # Determina a precisão geral prevista do modelo.\n        '95% CI for Accuracy' : accuracyCI,   # Determina um intervalo de confiança de 95% para a acurácia.\n        'Kappa'               : kappa,        # Determina o coeficiente de Kappa.\n        'Recall (Sensitivity)': sensitivity,  # Determina a proporção de registros positivos que foram classificados \n                                              # pelo algoritmo como positivos.\n        'Specificity'         : specificity,  # Determina a proporção de registros negativos que foram classificados \n                                              # pelo algoritmo como negativos.\n        'Pos Pred Value'      : ppv,          # Determina a porcentagem de positivos previstos que são realmente positivos.\n        'Neg Pred Value'      : npv,          # Determina a porcentagem de negativos previstos que são realmente negativos.\n        'Precision'           : precision,    # Determina a proporção de classificações positivas, que realmente são \n                                              # positivas.\n        'Avarage Precision'   : avgPrecision, # Determina a precisão como a média ponderada de precisões alcançadas em \n                                              # cada limite.\n        'Prevalence'          : prevalence,   # Determina a frequência com que a classe positiva realmente ocorre em nossa \n                                              # amostra.\n        'Detection Rate'      : dRate,        # Determina a proporção de classificações positivas feitas corretamente em \n                                              # relação a todas as previsões feitas.\n        'Detection Prevalence': dPrevalence,  # Determina o número de previsões positivas como uma proporção de todas\n                                              # as previsões.\n        'F1'                  : f1,           # Determina a média Harmônica entre a precision e o recall do modelo.\n        'ROC AUC'             : rocAuc,       # Determina a medida de separabilidade ROC. Ela indica o quanto o modelo \n                                              # é capaz de distinguir as classes.  \n        'AUC PR'              : Aucpr,        # Determina a medida AUC PR (Precision-Recall). Pode ser utilizada como alternativa\n                                              # a medida ROC, no caso de desbalanceamento das classes.\n        'Error'               : error,        # Determina o erro do modelo em relação a sua acurácia.\n        '95% CI for Error'    : errorCI,      # Determina um intervalo de confiança de 95% para o erro.\n        'Balanced Accuracy'   : bAccuracy,    # Determina a acurácia do modelo balanceada pelos tamanhos das classes.\n        'Positive Class'      : labelPositive # Define qual classe é a classe positiva.\n    }], index = ['Metrics']).transpose()\n\n    # Retornando o DataFrame, com as métricas obtidas.\n    return metrics\n\n# Definindo uma função para salvar um modelo preditivo já treinado\ndef saveModel(name, model, fold = './', ext = '.sav'):\n    # Definindo o diretório e o nome do arquivo que será utilizado para salvar o modelo\n    dir = fold + name + ext\n\n    # Salvando o modelo especificado\n    pickle.dump(model, open(dir, 'wb'))\n\n    # Imprimindo mensagem de sucesso.\n    print(\"Modelo salvo!\")\n\n# Definindo uma função para carregar um modelo preditivo já treinado\ndef loadModel(name, fold = './', ext = '.sav'):\n    # Definindo o diretório e o nome do arquivo que será utilizado para carregar o modelo\n    dir = fold + name + ext\n\n    # Imprimindo mensagem de sucesso\n    print(\"Modelo carregado!\")\n\n    # Carregando o modelo especificado\n    return pickle.load(open(dir, 'rb'))","metadata":{"execution":{"iopub.status.busy":"2022-06-01T09:23:19.219658Z","iopub.execute_input":"2022-06-01T09:23:19.220288Z","iopub.status.idle":"2022-06-01T09:23:19.239473Z","shell.execute_reply.started":"2022-06-01T09:23:19.220241Z","shell.execute_reply":"2022-06-01T09:23:19.23864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.4. Model training loop\n\nLet's now train several models, with the different combinations of normalization / standardization and feature subsets.\n\nWe will also generate boxplots depicting the score distribution of each model validation.\n","metadata":{}},{"cell_type":"code","source":"%%time\n# Treinando classificadores a partir do metodo de normalizacao e da técnica de Feature Selection utilizada\ntrain_results = dict()\nfor fs_name, feature_list in featurelist_dict.items():\n    # obtain the dataset on the selected features\n    # featurelist_dict['lasso_l1_nf-90']\n    for normalization_type, normalized_dataset in train_dict.items():\n        print(f'Training for {fs_name} x {normalization_type}...')\n        train_results[(fs_name,normalization_type)] = classifiersTraining (\n            features = normalized_dataset[feature_list], \n            tTarget  = y_train,\n            model_label = f'{fs_name}-{normalization_type}'\n        )\n    # end for\n# end for","metadata":{"execution":{"iopub.status.busy":"2022-06-01T09:23:19.240658Z","iopub.execute_input":"2022-06-01T09:23:19.241196Z","iopub.status.idle":"2022-06-01T09:23:33.554054Z","shell.execute_reply.started":"2022-06-01T09:23:19.241143Z","shell.execute_reply":"2022-06-01T09:23:33.553042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Let's check the best normalization / standardization method to be used\n\n* MM - MinMaxScaler\n* SS - StandardScaler\n* ND - PowerTransformer\n* NM - normalize\n","metadata":{}},{"cell_type":"code","source":"# Concatenate all model training result dataframes\nall_train_results = []\nfor key, df_list in train_results.items():\n    (dataset_name, norm_type) = key\n    df = df_list[1].reset_index()\n    df  = df.rename(columns={\"index\": \"model\"})\n    df['dataset_name'] = str(dataset_name)\n    df['norm_type'] = str(norm_type)\n    all_train_results.append(df)\n# end for\nall_train_results_df = pd.concat(all_train_results)\n# for each combination of dataset_name, show the best performing model and norm_type\nall_train_results_df.sort_values(by=['dataset_name', 'mean'], ascending=False, inplace=True)\nall_train_results_df.groupby(by=['dataset_name']).head(2)","metadata":{"execution":{"iopub.status.busy":"2022-06-01T09:23:33.555326Z","iopub.execute_input":"2022-06-01T09:23:33.555735Z","iopub.status.idle":"2022-06-01T09:23:33.648508Z","shell.execute_reply.started":"2022-06-01T09:23:33.555702Z","shell.execute_reply":"2022-06-01T09:23:33.647851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Comments\n\nThe best normalization / standardization type (with the highest `AUC score` on the majority of datasets) is **`MM - MinMaxScaler`**. \n\nThe top3, best performing models are:\n\n* Gaussian Naive Bayes (NB)\n* XGboost\n* LGBM\n\nFirst, we will generate different submissions to Kaggle, based on these best-performing models, and save the obtained scores. \n\nThen we will invest time in tuning the best models.","metadata":{}},{"cell_type":"markdown","source":"## 4.5. Re-train the best-performing models, based on the whole dataset, and submit to Kaggle","metadata":{}},{"cell_type":"code","source":"def full_model_retrain(fs_name, X_full_MM, y_full, num_folds = 10, scoring = 'roc_auc'):\n    if kaggle:\n        base_folder = '../input/santandercustomertransactionv1models/'\n    else:\n        base_folder = './'\n    # end if\n    if os.path.exists(os.path.join(base_folder, f'GNB-{fs_name}.sav')):\n        gnb_cv = loadModel(f'GNB-{fs_name}', fold=base_folder)\n        print('Skipping training, model already exists.')\n    else:\n        gnb = GaussianNB()\n        gnb_param = {'var_smoothing': np.logspace(0,-15, num=20)}\n        gnb_cv = GridSearchCV(gnb, gnb_param, cv=num_folds, scoring=scoring, verbose=0, n_jobs=-1, return_train_score=True)\n        gnb_cv.fit(X_full_MM, y_full)\n        saveModel(f'GNB-{fs_name}', gnb_cv)\n    # end if\n    if os.path.exists(os.path.join(base_folder, f'XGB-{fs_name}.sav')):\n        xgb_class = loadModel(f'XGB-{fs_name}', fold=base_folder)\n        print('Skipping training, model already exists.')\n    else:\n        xgb_class = XGBClassifier(max_depth=4, n_estimators=600, eta=0.1, booster='gbtree', subsample=0.4, verbosity=1, scale_pos_weight=8,\n                      colsample_bytree=0.2, tree_method='hist', eval_metric='aucpr', use_label_encoder=False)\n        xgb_class.fit(X_full_MM, y_full)\n        saveModel(f'XGB-{fs_name}', xgb_class, fold=base_folder)\n    # end if\n    if os.path.exists(os.path.join(base_folder, f'LGBM-{fs_name}.sav')):\n        lgbm_class = loadModel(f'LGBM-{fs_name}', fold=base_folder)\n        print('Skipping training, model already exists.')\n    else:\n        lgbm_param = {\n             'num_leaves': 18,\n             'max_bin': 63,\n             'min_data_in_leaf': 5,\n             'learning_rate': 0.010614430970330217,\n             'min_sum_hessian_in_leaf': 0.0093586657313989123,\n             'feature_fraction': 0.056701788569420042,\n             'min_gain_to_split': 0.29588543202055562,\n             'max_depth': 49,\n             'save_binary': True,\n             'seed': 1337,\n             'feature_fraction_seed': 1337,\n             'bagging_seed': 1337,\n             'drop_seed': 1337,\n             'data_random_seed': 1337,\n             'objective': 'binary',\n             'boosting_type': 'gbdt',\n             'verbose': 1,\n             'metric': 'auc',\n             'is_unbalance': True,\n             'boost_from_average': False\n        }\n        lgbm_class = lightgbm.LGBMClassifier(**lgbm_param)\n        lgbm_class.fit(X_full_MM, y_full)\n        saveModel(f'LGBM-{fs_name}', lgbm_class)\n    # end if","metadata":{"execution":{"iopub.status.busy":"2022-06-01T09:52:10.116726Z","iopub.execute_input":"2022-06-01T09:52:10.117105Z","iopub.status.idle":"2022-06-01T09:52:10.130722Z","shell.execute_reply.started":"2022-06-01T09:52:10.117076Z","shell.execute_reply":"2022-06-01T09:52:10.129594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.5.1. Retrain models based on full Kaggle train dataset and a predefined hyperparameter setting","metadata":{}},{"cell_type":"code","source":"# Treinando classificadores a partir do metodo de normalizacao e da técnica de Feature Selection utilizada\n# Criando um objeto da classe MinMaxScaler().\nscaler = MinMaxScaler()\n# Aplicando a escala nas Features e capturando o resultado obtido\nfull_features = pd.concat([train.drop([\"ID_code\", \"target\"], axis = 1), test.drop([\"ID_code\", \"target\"], axis = 1)])\nidx = features = full_features.columns.values\nfor df in [full_features]:\n    df['sum'] = df[idx].sum(axis=1)  \n    df['min'] = df[idx].min(axis=1)\n    df['max'] = df[idx].max(axis=1)\n    df['mean'] = df[idx].mean(axis=1)\n    df['std'] = df[idx].std(axis=1)\n    df['skew'] = df[idx].skew(axis=1)\n    df['kurt'] = df[idx].kurtosis(axis=1)\n    df['med'] = df[idx].median(axis=1)\nnormalized_originalTestFeatures = scaler.fit_transform(full_features)\n# Criando um DataFrame com os resultados obtidos.\nnormalized_originalTestFeatures_df = pd.DataFrame(data = normalized_originalTestFeatures, columns = full_features.columns)\ny_full = pd.concat([y_train, y_test])","metadata":{"execution":{"iopub.status.busy":"2022-06-01T10:38:20.945269Z","iopub.execute_input":"2022-06-01T10:38:20.946632Z","iopub.status.idle":"2022-06-01T10:38:29.40523Z","shell.execute_reply.started":"2022-06-01T10:38:20.946582Z","shell.execute_reply":"2022-06-01T10:38:29.404259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.5.2. Predict values for Kaggle test dataset\n\nFor each combination of model, normalization type and feature subset, we will generate a Kaggle CSV submission file and we will manually submit it to Kaggle, recording the public score obtained.","metadata":{}},{"cell_type":"code","source":"%%time\nimport os\n\ndf_val = df_test.drop(columns=['ID_code'])\nidx = features = df_val.columns.values\nfor df in [df_val]:\n    df['sum'] = df[idx].sum(axis=1)  \n    df['min'] = df[idx].min(axis=1)\n    df['max'] = df[idx].max(axis=1)\n    df['mean'] = df[idx].mean(axis=1)\n    df['std'] = df[idx].std(axis=1)\n    df['skew'] = df[idx].skew(axis=1)\n    df['kurt'] = df[idx].kurtosis(axis=1)\n    df['med'] = df[idx].median(axis=1)\n# end for\nnormalized_originalValFeatures = scaler.fit_transform(df_val)\n# Criando um DataFrame com os resultados obtidos.\nnormalized_originalValFeatures_df = pd.DataFrame(data = normalized_originalValFeatures, columns = df_val.columns)\n\nbase_folder = '.'\nif kaggle:\n    base_folder = '/kaggle/working'\n    base_folder = \"../input/santandercustomertransactionv1models\"\n# end if\nnum_folds = 10\nscoring = 'roc_auc'\nfor model_basename in ['XGB', 'GNB', 'LGBM']:\n    model_filename = os.path.join(base_folder, model_basename)\n    print(f'Processing model {model_basename}...')\n    if 'XGB' in model_basename:\n        model = XGBClassifier(max_depth=4, n_estimators=600, eta=0.1, booster='gbtree', subsample=0.4, verbosity=1, scale_pos_weight=8,\n                  colsample_bytree=0.2, tree_method='hist', eval_metric='aucpr', use_label_encoder=False)\n    elif 'GNB' in model_basename:\n        gnb = GaussianNB()\n        gnb_param = {'var_smoothing': np.logspace(0,-15, num=20)}\n        model = GridSearchCV(gnb, gnb_param, cv=num_folds, scoring=scoring, verbose=0, n_jobs=-1, return_train_score=True)\n    else:  # LGBM\n        lgbm_param = {\n             'num_leaves': 18,\n             'max_bin': 63,\n             'min_data_in_leaf': 5,\n             'learning_rate': 0.010614430970330217,\n             'min_sum_hessian_in_leaf': 0.0093586657313989123,\n             'feature_fraction': 0.056701788569420042,\n             'min_gain_to_split': 0.29588543202055562,\n             'max_depth': 49,\n             'save_binary': True,\n             'seed': 1337,\n             'feature_fraction_seed': 1337,\n             'bagging_seed': 1337,\n             'drop_seed': 1337,\n             'data_random_seed': 1337,\n             'objective': 'binary',\n             'boosting_type': 'gbdt',\n             'verbose': 1,\n             'metric': 'auc',\n             'is_unbalance': True,\n             'boost_from_average': False\n        }\n        model = lightgbm.LGBMClassifier(**lgbm_param)\n    # end if\n    for fs_name in featurelist_dict.keys():\n        feature_list = featurelist_dict[fs_name]\n        X_full_MM = normalized_originalTestFeatures_df[feature_list]\n        # fit the model on the training data\n        print(f'Training for dataset_name = {fs_name}')\n        model.fit(X_full_MM, y_full)\n\n        print(f'Prediction for dataset_name = {fs_name}')\n        df_val_subset = normalized_originalValFeatures_df[feature_list]\n        # Salvando as previsões feitas pelo classificador para o conjunto de teste em um arquivo CSV\n        pd.DataFrame({\n            'ID_code'    : testID,\n            'target': model.predict_proba(df_val_subset)[:, 1]\n        }).to_csv(f'pred-{model_basename}.csv', index = False)\n    # end for\n# end for","metadata":{"execution":{"iopub.status.busy":"2022-06-01T10:57:36.726067Z","iopub.execute_input":"2022-06-01T10:57:36.727425Z","iopub.status.idle":"2022-06-01T11:21:02.833564Z","shell.execute_reply.started":"2022-06-01T10:57:36.727358Z","shell.execute_reply":"2022-06-01T11:21:02.831998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"featurelist_dict.keys()","metadata":{"execution":{"iopub.status.busy":"2022-06-01T11:21:02.835653Z","iopub.execute_input":"2022-06-01T11:21:02.8365Z","iopub.status.idle":"2022-06-01T11:21:02.84623Z","shell.execute_reply.started":"2022-06-01T11:21:02.836458Z","shell.execute_reply":"2022-06-01T11:21:02.845265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.5.3. Evaluate Kaggle score values (typed into s csv file after manual submission)\n\nWe can see, in the Kaggle results below (v1), that the best-performing models/datasets are:\n\n* XGB and GNB models\n* original_nf-200, sbs_nf-196, anova_nf-150, and rf_nf-150\n\n**We will now try to tune the hyperparameters of each model, to generate a new set of classifiers and a new set of Kaggle predictions (v2).**","metadata":{}},{"cell_type":"code","source":"if not kaggle:\n    kaggle_v1_df = pd.read_csv('kaggle_model_scores_v1.csv')\nelse:\n    kaggle_v1_df = pd.read_csv('../input/santandercusttransactionmydata/kaggle_model_scores_v1.csv')\n# end if\nkaggle_v1_df.sort_values(by=['public_score'], ascending=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-01T11:26:09.890491Z","iopub.execute_input":"2022-06-01T11:26:09.893854Z","iopub.status.idle":"2022-06-01T11:26:09.941474Z","shell.execute_reply.started":"2022-06-01T11:26:09.893739Z","shell.execute_reply":"2022-06-01T11:26:09.940221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.6. Tune the hyperparameters of the best model(s) and re-submit to Kaggle\n\nAfter some preliminary tests (above), we observed that the best models and feature subsets are:\n\n* XGB and GNB models\n* original_nf-200, sbs_nf-196, anova_nf-150, and rf_nf-150\n\n**We will now try to tune the hyperparameters of each model, to generate a new set of classifiers and a new set of Kaggle predictions (v2).**\n","metadata":{}},{"cell_type":"markdown","source":"### 4.6.1. XGBoost Classifier\n\nWe will look for the best hyper-parameters to create a model with the Xgboost algorithm.","metadata":{}},{"cell_type":"markdown","source":"### XGBoost using Optuna\n\nOptuna is an automatic hyperparameter optimization software framework, particularly designed for machine learning. It features an imperative, define-by-run style user API.\nThe code written with Optuna enjoys high modularity, and the user of Optuna can dynamically construct the search spaces for the hyperparameters.\n\n**As basic concepts, We use the terms study and trial as follows**:\n\n* **Study** : optimization based on an objective function;\n* **Trial** : a single execution of the objective function.\n\n\nThe following optimization function uses XGBoostClassifier model, so it takes the following arguments:\n\n* the data\n* the target\n* number of trials (How many executions will be done)\n\nAnd returns:\n\n* objective value: AUC_PR (suits best than AUC when target data is heavily imbalanced)\n\nNotes:\n\n* We used some XGBoostRegressor hyperparameters from Xgboost official site;\n* Also we used `early_stopping_rounds` to avoid overfiting and to speedup the training process\n* We can use the GPU, or you can comment the first param argument (the training process will takes a lot of time by only using the cpu)\n\nReferences:\n* https://medium.com/optuna/using-optuna-to-optimize-xgboost-hyperparameters-63bfcdfd3407\n* https://www.kaggle.com/code/hamzaghanmi/xgboost-catboost-using-optuna?scriptVersionId=94510532\n* https://neptune.ai/blog/f1-score-accuracy-roc-auc-pr-auc","metadata":{}},{"cell_type":"code","source":"import optuna\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.model_selection import KFold, cross_validate, train_test_split, cross_val_score\n\ndef objective(trial, data, target, cv, scoring, use_cv):\n    \n    train_x, test_x, train_y, test_y = train_test_split(data, target, test_size=0.6, random_state=42)\n    dtrain = xgb.DMatrix(train_x, label=train_y)\n    dtest = xgb.DMatrix(test_x, label=test_y)\n    optuna_xgb_param = {\n        'tree_method':'gpu_hist',  # this parameter means using the GPU when training our model to speedup the training process\n        'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n        'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]),\n        'subsample': trial.suggest_categorical('subsample', [0.4,0.5,0.6,0.7,0.8,0.9,1.0]),\n        'eta': trial.suggest_float(\"eta\", 1e-8, 1.0, log=True),\n        #'eta': trial.suggest_categorical('learning_rate', [0.008,0.01,0.012,0.014,0.016,0.018, 0.02, 0.03]),\n        'n_estimators': [340, 600, 10000],\n        'max_depth': trial.suggest_categorical('max_depth', [4,5,7,9,11,13,15,17]),\n        'random_state': trial.suggest_categorical('random_state', [2020]),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 300),\n        'booster': 'gbtree',\n        'scale_pos_weight': 8,\n        'eval_metric': 'aucpr', \n        #'eval_metric': 'auc', \n        'use_label_encoder': False,\n        'nthread': 12, \n        \"objective\": \"binary:logistic\",\n        \"silent\": 1,\n    }\n    if use_cv:\n        # Perform CV\n        xgb_model = XGBClassifier(**optuna_xgb_param)\n        scores = cross_val_score(xgb_model, data, target, cv=cv, scoring=scoring, n_jobs=-1)\n        return scores.mean()\n    else:\n        xgb_model = xgb.XGBClassifier(**param)\n        # Add a callback for pruning.\n        pruning_callback = optuna.integration.XGBoostPruningCallback(trial, \"validation-aucpr\")\n        bst = xgb.train(param, dtrain, evals=[(dtest, \"validation\")], callbacks=[pruning_callback], early_stopping_rounds=10)\n        # Kaggle accepts probabilities as answer, so\n        # predict will return the probability of class == 1, how to set threshold (TODO)\n        preds = bst.predict(dtest, iteration_range=(0, bst.best_iteration + 1))\n        pred_labels = preds\n        ### pred_labels = np.rint(preds)\n        # model.fit(train_x,train_y,eval_set=[(test_x,test_y)],early_stopping_rounds=100,verbose=False)\n        # auc = roc_auc_score(test_y, pred_labels)\n        aucpr = average_precision_score(test_y, pred_labels)\n        return aucpr","metadata":{"execution":{"iopub.status.busy":"2022-06-01T11:26:29.24289Z","iopub.execute_input":"2022-06-01T11:26:29.243449Z","iopub.status.idle":"2022-06-01T11:26:31.350807Z","shell.execute_reply.started":"2022-06-01T11:26:29.24339Z","shell.execute_reply":"2022-06-01T11:26:31.349621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# treat all python warnings as lower-level \"ignore\" events\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2022-06-01T11:26:31.352608Z","iopub.execute_input":"2022-06-01T11:26:31.352997Z","iopub.status.idle":"2022-06-01T11:26:31.357873Z","shell.execute_reply.started":"2022-06-01T11:26:31.352963Z","shell.execute_reply":"2022-06-01T11:26:31.356973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optuna.logging.set_verbosity(optuna.logging.FATAL)","metadata":{"execution":{"iopub.status.busy":"2022-06-01T11:26:31.359138Z","iopub.execute_input":"2022-06-01T11:26:31.359673Z","iopub.status.idle":"2022-06-01T11:26:31.375836Z","shell.execute_reply.started":"2022-06-01T11:26:31.359626Z","shell.execute_reply":"2022-06-01T11:26:31.37421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's start optimizing the hyperparameters\n\nNote that the objective of our fuction is to **maximize the `AUC-PR`** that's why I set direction='maximize'.\n\n#### Note:\n\n* n_trials (number of executions) can be modified\n\n* we will test different values for the learning rate parameter `eta`, as well as other xgb parameters.","metadata":{}},{"cell_type":"code","source":"xgb_param = {\n        'tree_method': optuna.distributions.CategoricalDistribution(['gpu_hist']),  # this parameter means using the GPU when training our model to speedup the training process\n        'lambda': optuna.distributions.LogUniformDistribution(1e-3, 10.0),\n        'alpha': optuna.distributions.LogUniformDistribution(1e-3, 10.0),\n        'colsample_bytree': optuna.distributions.CategoricalDistribution([0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]),\n        'subsample': optuna.distributions.CategoricalDistribution([0.4,0.5,0.6,0.7,0.8,0.9,1.0]),\n        'eta': optuna.distributions.LogUniformDistribution(1e-8, 1.0),\n        #'eta': optuna.distributions.CategoricalDistribution([0.008,0.01,0.012,0.014,0.016,0.018, 0.02, 0.03]),\n        'n_estimators': optuna.distributions.CategoricalDistribution([340, 600, 10000]),\n        'max_depth': optuna.distributions.CategoricalDistribution([4,5,7,9,11,13,15,17]),\n        'random_state': optuna.distributions.CategoricalDistribution([2020]),\n        'min_child_weight': optuna.distributions.IntUniformDistribution(1, 300),\n        'booster': optuna.distributions.CategoricalDistribution(['gbtree']),\n        'scale_pos_weight': optuna.distributions.CategoricalDistribution([8]),\n        'eval_metric': optuna.distributions.CategoricalDistribution(['aucpr']), \n        #'eval_metric': 'auc', \n        'use_label_encoder': optuna.distributions.CategoricalDistribution([False]),\n        'nthread': optuna.distributions.CategoricalDistribution([12]), \n        \"objective\": optuna.distributions.CategoricalDistribution([\"binary:logistic\"]),\n    }","metadata":{"execution":{"iopub.status.busy":"2022-06-01T11:26:38.207311Z","iopub.execute_input":"2022-06-01T11:26:38.207795Z","iopub.status.idle":"2022-06-01T11:26:38.219429Z","shell.execute_reply.started":"2022-06-01T11:26:38.207758Z","shell.execute_reply":"2022-06-01T11:26:38.217902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"use_optimize_func = False\ntimeout = 1800 # 600 seconds == 10 min for each model","metadata":{"execution":{"iopub.status.busy":"2022-06-01T11:26:40.056847Z","iopub.execute_input":"2022-06-01T11:26:40.057293Z","iopub.status.idle":"2022-06-01T11:26:40.062408Z","shell.execute_reply.started":"2022-06-01T11:26:40.057259Z","shell.execute_reply":"2022-06-01T11:26:40.061626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nif not kaggle:\n    df_study_dict = dict()\n    study_dict = dict()\n    # IMPORTANTE! Instanciando um objeto da StratifiedKFold para criar os folds, pois ha desbalanceamento do target\n    kfold = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 42)\n\n    for fs_name, feature_list in featurelist_dict.items():\n        if fs_name in ['original_nf-200', 'sbs_nf-196']:  # , 'anova_nf-150', 'rf_nf-150']:\n            print(f'Tuning XGB model for {fs_name} x MM...')\n            data = normalized_originalTestFeatures_df[feature_list]\n            target = y_full\n            # obtain the dataset on the selected features\n            # train_dict['MM', fs_name]  # normalization_type, normalized_dataset\n            for normalization_type, normalized_dataset in train_dict.items():\n                model_label = f'{fs_name}-MM'\n                study = optuna.create_study(direction='maximize')\n                if use_optimize_func:\n                    # Wrap the Optuna objective inside a lambda and call objective inside it\n                    func = lambda trial: objective(trial, data, target, cv=kfold, scoring='average_precision', use_cv=True)\n                    study.optimize(func, n_trials=50)\n                else:\n                    xgb_model = XGBClassifier()\n                    optuna_search = optuna.integration.OptunaSearchCV(xgb_model, xgb_param, cv=kfold, n_jobs=-1, random_state=42, study=study, timeout=timeout)  #, enable_pruning=True)\n                    optuna_search.fit(data, target)\n                    y_pred = optuna_search.predict(data)\n                # end if\n                print('Number of finished trials:', len(study.trials))\n                print('Best trial:', study.best_trial.params)\n\n                study_dict[model_label] = study\n                df_study_dict[model_label] = study.trials_dataframe()\n            # end if\n        # end for\n    # end for\n# end if","metadata":{"execution":{"iopub.status.busy":"2022-06-01T11:26:42.634211Z","iopub.execute_input":"2022-06-01T11:26:42.635002Z","iopub.status.idle":"2022-06-01T11:26:42.644124Z","shell.execute_reply.started":"2022-06-01T11:26:42.634957Z","shell.execute_reply":"2022-06-01T11:26:42.643374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Optuna output removed due to excessive output","metadata":{"execution":{"iopub.status.busy":"2022-06-01T11:26:50.093986Z","iopub.execute_input":"2022-06-01T11:26:50.094521Z","iopub.status.idle":"2022-06-01T11:26:50.099372Z","shell.execute_reply.started":"2022-06-01T11:26:50.09448Z","shell.execute_reply":"2022-06-01T11:26:50.098146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not kaggle:\n    display(df_study_dict.keys())","metadata":{"execution":{"iopub.status.busy":"2022-06-01T11:26:50.289822Z","iopub.execute_input":"2022-06-01T11:26:50.290492Z","iopub.status.idle":"2022-06-01T11:26:50.295978Z","shell.execute_reply.started":"2022-06-01T11:26:50.290447Z","shell.execute_reply":"2022-06-01T11:26:50.294712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not kaggle:\n    display(df_study_dict['original_nf-200-MM'])","metadata":{"execution":{"iopub.status.busy":"2022-06-01T11:26:50.586798Z","iopub.execute_input":"2022-06-01T11:26:50.587534Z","iopub.status.idle":"2022-06-01T11:26:50.592936Z","shell.execute_reply.started":"2022-06-01T11:26:50.587475Z","shell.execute_reply":"2022-06-01T11:26:50.592125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's do some Quick Visualization for Hyperparameter Optimization Analysis\n\nOptuna provides various visualization features in optuna.visualization to analyze optimization results visually.","metadata":{}},{"cell_type":"code","source":"if not kaggle:\n    # plot_optimization_histor: shows the scores from all trials as well as the best score so far at each point.\n    optuna.visualization.plot_optimization_history(study_dict['original_nf-200-MM'])\nelse:\n    display(Image(\"../input/santandercustomeroptunaplots/Optuna_optimization_history_plot-original_nf-200-MM.PNG\"))","metadata":{"execution":{"iopub.status.busy":"2022-06-01T11:26:51.450869Z","iopub.execute_input":"2022-06-01T11:26:51.451583Z","iopub.status.idle":"2022-06-01T11:26:51.469429Z","shell.execute_reply.started":"2022-06-01T11:26:51.451515Z","shell.execute_reply":"2022-06-01T11:26:51.467937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not kaggle:\n    optuna.visualization.plot_optimization_history(study_dict['sbs_nf-196-MM'])\nelse:\n    display(Image(\"../input/santandercustomeroptunaplots/Optuna_optimization_history_plot-sbs_nf-196-MM.PNG\"))","metadata":{"execution":{"iopub.status.busy":"2022-06-01T11:26:52.861326Z","iopub.execute_input":"2022-06-01T11:26:52.861767Z","iopub.status.idle":"2022-06-01T11:26:52.87283Z","shell.execute_reply.started":"2022-06-01T11:26:52.86173Z","shell.execute_reply":"2022-06-01T11:26:52.871581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not kaggle:\n    '''plot_slice: shows the evolution of the search. You can see where in the hyperparameter space your search\n    went and which parts of the space were explored more.'''\n    optuna.visualization.plot_slice(study_dict['original_nf-200-MM'])\nelse:\n    display(Image(\"../input/santandercustomeroptunaplots/Optuna_slice_plot-original_nf-200-MM.PNG\"))","metadata":{"execution":{"iopub.status.busy":"2022-06-01T11:26:53.052351Z","iopub.execute_input":"2022-06-01T11:26:53.053151Z","iopub.status.idle":"2022-06-01T11:26:53.066467Z","shell.execute_reply.started":"2022-06-01T11:26:53.053114Z","shell.execute_reply":"2022-06-01T11:26:53.065572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not kaggle:\n    optuna.visualization.plot_slice(study_dict['sbs_nf-196-MM'])\nelse:\n    display(Image(\"../input/santandercustomeroptunaplots/Optuna_slice_plot-sbs_nf-196-MM.PNG\"))","metadata":{"execution":{"iopub.status.busy":"2022-06-01T11:26:53.251397Z","iopub.execute_input":"2022-06-01T11:26:53.251947Z","iopub.status.idle":"2022-06-01T11:26:53.264102Z","shell.execute_reply.started":"2022-06-01T11:26:53.251912Z","shell.execute_reply":"2022-06-01T11:26:53.263022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's create a XGBoostClassifier model with the best hyperparameters for each of the two datasets (`sbs_nf-196-MM` and `original_nf-200-MM`)","metadata":{}},{"cell_type":"code","source":"best_xgb_trial_params = dict()\n\nbest_xgb_trial_params['sbs_nf-196-MM'] = {'tree_method': 'hist', 'lambda': 5.852983163624425, 'alpha': 0.1550111155371834, \n                                          'colsample_bytree': 1.0, 'subsample': 0.9, 'eta': 0.025334831995211547, \n                                          'n_estimators': 10000, 'max_depth': 9, 'random_state': 2020, 'min_child_weight': 43, \n                                          'booster': 'gbtree', 'scale_pos_weight': 8, 'eval_metric': 'aucpr', \n                                          'use_label_encoder': False, 'nthread': 12, 'objective': 'binary:logistic'}\n\nbest_xgb_trial_params['original_nf-200-MM'] = {'tree_method': 'hist', 'lambda': 0.0010001992411276593, 'alpha': 0.6641490292943272, \n                           'colsample_bytree': 0.3, 'subsample': 0.7, 'eta': 0.016423404013017992, 'n_estimators': 340, \n                           'max_depth': 15, 'random_state': 2020, 'min_child_weight': 34, 'booster': 'gbtree', \n                           'scale_pos_weight': 8, 'eval_metric': 'aucpr', 'use_label_encoder': False, 'nthread': 12, \n                           'objective': 'binary:logistic'}","metadata":{"execution":{"iopub.status.busy":"2022-06-01T11:27:00.074381Z","iopub.execute_input":"2022-06-01T11:27:00.074841Z","iopub.status.idle":"2022-06-01T11:27:00.08328Z","shell.execute_reply.started":"2022-06-01T11:27:00.074802Z","shell.execute_reply":"2022-06-01T11:27:00.082336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" ## 5. Predict on test / hold-out Sample\n\nBefore finalizing the model, it is advisable to perform one final check by predicting the test/hold-out set (25%) and reviewing the evaluation metrics.\n\nAll of the evaluation metrics we have seen above (from Optuna results and graphs) are cross-validated results based on the training set (75%). Now, using our final trained model, we will predict the test / hold-out sample and evaluate the metrics to see if they are materially different than the CV results.","metadata":{}},{"cell_type":"code","source":"def holdout_model_retrain(fs_name, X_train, y_train, X_test, y_test):\n    xgb_class = XGBClassifier(**best_xgb_trial_params[fs_name])\n    # Fits the model on the training data, and evaluates the metrics based on the test/validation data\n    xgb_class.fit(X_train, y_train, eval_set=[(X_test, y_test)], early_stopping_rounds=10, verbose=False)\n    saveModel(f'holdout-XGB-{fs_name}', xgb_class)\n    return xgb_class","metadata":{"execution":{"iopub.status.busy":"2022-06-01T11:30:57.290171Z","iopub.execute_input":"2022-06-01T11:30:57.290616Z","iopub.status.idle":"2022-06-01T11:30:57.298128Z","shell.execute_reply.started":"2022-06-01T11:30:57.290583Z","shell.execute_reply":"2022-06-01T11:30:57.296576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nclassifierXGB = dict()\nfor fs_name in best_xgb_trial_params.keys():\n    print(f'****** Processing dataset {fs_name}...')\n    classifierXGB[fs_name] = holdout_model_retrain(fs_name, X_train, y_train, X_test, y_test)\n# end for","metadata":{"execution":{"iopub.status.busy":"2022-06-01T11:30:57.533961Z","iopub.execute_input":"2022-06-01T11:30:57.534858Z","iopub.status.idle":"2022-06-01T11:37:23.837394Z","shell.execute_reply.started":"2022-06-01T11:30:57.534743Z","shell.execute_reply":"2022-06-01T11:37:23.836165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculando a acurácia, ROC AUC e AUC-PR do modelo para os conjuntos de dados de treino e de validacao\nfor fs_name in best_xgb_trial_params.keys():\n    print(f'****** Processing dataset {fs_name}...')\n    \n    print('***** Train metrics: *****')\n    predTrain = classifierXGB[fs_name].predict(X_train)\n    predProbTrain = classifierXGB[fs_name].predict_proba(X_train)[:,1]\n    df_metrics_train = getClassificationMetrics(y_train, predProbTrain, labelPositive = 1, labelNegative = 0)\n    display(df_metrics_train)\n    cm = confusionMatrix(y_train, predTrain, labelPositive = 1, labelNegative = 0, classError = False)\n    plotConfusionMatrix(cm, labels=['0', '1'], title=f'{fs_name} - Train')\n    \n    # plt.figure(figsize=(10,10))\n    # sns.regplot(y_train, predTrain, fit_reg=True, scatter_kws={\"s\": 100})\n    \n    print('***** Holdout metrics: *****')\n    predValidation = classifierXGB[fs_name].predict(X_test)\n    predProbValidation = classifierXGB[fs_name].predict_proba(X_test)[:,1]\n    df_metrics_validation = getClassificationMetrics(y_test, predProbValidation, labelPositive = 1, labelNegative = 0)\n    display(df_metrics_validation)\n    cm = confusionMatrix(y_test, predValidation, labelPositive = 1, labelNegative = 0, classError = False)\n    plotConfusionMatrix(cm, labels=['0', '1'], title=f'{fs_name} - Validation')\n    \n    plt.figure(figsize=(10,10))\n    sns.regplot(y_test, predValidation, fit_reg=True, scatter_kws={\"s\": 100})\n# end for","metadata":{"execution":{"iopub.status.busy":"2022-06-01T12:15:42.887919Z","iopub.status.idle":"2022-06-01T12:15:42.888765Z","shell.execute_reply.started":"2022-06-01T12:15:42.888314Z","shell.execute_reply":"2022-06-01T12:15:42.888356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Comments\n\nWe conclude this hold-out analysis, finding out that the XGBoost model obtained a hold-out score of:\n\n**Dataset sbs-196-MM**\n\nF1      0.55\n\nROC AUC 0.88\n\nAUC PR  0.59\n\n**Dataset original-200-MM**\n\nF1      0.49\n\nROC AUC 0.85\n\nAUC PR  0.51\n\nRegarding the training metrics, both models achieved a ROC AUC metric of 0.99, which suggests a possible overfitting to the training set.","metadata":{}},{"cell_type":"markdown","source":"## 6. Finalize Model for Deployment\n\nModel finalization is the last step in the experiment. We started the machine learning workflow by normalizing the dataset, then generating feature importance scores and distinct datasets with different subsets of features.\n\nThe next step consisted in testing a couple of sklearn models over these different datasets, comparing all models using the `AUC` metric and shortlisting a few candidate models to perform hyperparameter tuning. This workflow led us to the best model for use in making predictions on new and unseen data : `xgboost`. \n\nIn this last step, we will fit the model onto the complete dataset *including the test/hold-out sample (25% in our case)*. The purpose of this step is to train the model on the complete dataset before it is deployed to predict the Kaggle validation (`df_test`) set.\n","metadata":{}},{"cell_type":"markdown","source":"**Assembling the whole dataset to perform a full final model train on all available data (`X_train` and `X_test`)**","metadata":{}},{"cell_type":"code","source":"# Treinando classificadores a partir do metodo de normalizacao e da técnica de Feature Selection utilizada\n# Criando um objeto da classe MinMaxScaler().\nscaler = MinMaxScaler()\n# Aplicando a escala nas Features e capturando o resultado obtido\nfull_features = pd.concat([train.drop([\"ID_code\", \"target\"], axis = 1), test.drop([\"ID_code\", \"target\"], axis = 1)])\nidx = features = full_features.columns.values\nfor df in [full_features]:\n    df['sum'] = df[idx].sum(axis=1)  \n    df['min'] = df[idx].min(axis=1)\n    df['max'] = df[idx].max(axis=1)\n    df['mean'] = df[idx].mean(axis=1)\n    df['std'] = df[idx].std(axis=1)\n    df['skew'] = df[idx].skew(axis=1)\n    df['kurt'] = df[idx].kurtosis(axis=1)\n    df['med'] = df[idx].median(axis=1)\nnormalized_originalTestFeatures = scaler.fit_transform(full_features)\n# Criando um DataFrame com os resultados obtidos.\nnormalized_originalTestFeatures_df = pd.DataFrame(data = normalized_originalTestFeatures, columns = full_features.columns)\ny_full = pd.concat([y_train, y_test])","metadata":{"execution":{"iopub.status.busy":"2022-06-01T11:38:40.328564Z","iopub.execute_input":"2022-06-01T11:38:40.328966Z","iopub.status.idle":"2022-06-01T11:38:48.95433Z","shell.execute_reply.started":"2022-06-01T11:38:40.328932Z","shell.execute_reply":"2022-06-01T11:38:48.9528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def finalize_model_retrain(fs_name, X_full_MM, y_full):\n    xgb_class = XGBClassifier(**best_xgb_trial_params[fs_name])\n    xgb_class.fit(X_full_MM, y_full)\n    saveModel(f'best-XGB-{fs_name}', xgb_class)\n    return xgb_class","metadata":{"execution":{"iopub.status.busy":"2022-06-01T11:38:48.956903Z","iopub.execute_input":"2022-06-01T11:38:48.957366Z","iopub.status.idle":"2022-06-01T11:38:48.962746Z","shell.execute_reply.started":"2022-06-01T11:38:48.957326Z","shell.execute_reply":"2022-06-01T11:38:48.961411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nfinal_classifier_XGB = dict()\n\nfor fs_name, feature_list in featurelist_dict.items():\n    if fs_name + '-MM' in best_xgb_trial_params.keys():\n        print(f'Full training for {fs_name} x MM...')\n        X_full_MM = normalized_originalTestFeatures_df[feature_list]\n        final_xgb_model = finalize_model_retrain(fs_name + '-MM', X_full_MM, y_full)\n        final_classifier_XGB[f'{fs_name} x MM'] = final_xgb_model\n    # end if\n# end for","metadata":{"execution":{"iopub.status.busy":"2022-06-01T11:38:48.963951Z","iopub.execute_input":"2022-06-01T11:38:48.964936Z","iopub.status.idle":"2022-06-01T12:15:31.633002Z","shell.execute_reply.started":"2022-06-01T11:38:48.964892Z","shell.execute_reply":"2022-06-01T12:15:31.631932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 7. Submission","metadata":{}},{"cell_type":"code","source":"# sample_submission format\n# ID_code,target\n# test_0,0\n# test_1,1\n# test_2,0\n# etc.","metadata":{"execution":{"iopub.status.busy":"2022-06-01T12:15:31.634982Z","iopub.execute_input":"2022-06-01T12:15:31.635388Z","iopub.status.idle":"2022-06-01T12:15:31.640397Z","shell.execute_reply.started":"2022-06-01T12:15:31.635353Z","shell.execute_reply":"2022-06-01T12:15:31.639353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_val = df_test.drop(columns=['ID_code'])\nidx = features = df_val.columns.values\nfor df in [df_val]:\n    df['sum'] = df[idx].sum(axis=1)  \n    df['min'] = df[idx].min(axis=1)\n    df['max'] = df[idx].max(axis=1)\n    df['mean'] = df[idx].mean(axis=1)\n    df['std'] = df[idx].std(axis=1)\n    df['skew'] = df[idx].skew(axis=1)\n    df['kurt'] = df[idx].kurtosis(axis=1)\n    df['med'] = df[idx].median(axis=1)\n# end for\nnormalized_originalValFeatures = scaler.fit_transform(df_val)\n# Criando um DataFrame com os resultados obtidos.\nnormalized_originalValFeatures_df = pd.DataFrame(data = normalized_originalValFeatures, columns = df_val.columns)","metadata":{"execution":{"iopub.status.busy":"2022-06-01T12:15:31.641668Z","iopub.execute_input":"2022-06-01T12:15:31.642016Z","iopub.status.idle":"2022-06-01T12:15:42.840801Z","shell.execute_reply.started":"2022-06-01T12:15:31.641984Z","shell.execute_reply":"2022-06-01T12:15:42.839558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Fazendo previsoes para o conjunto de teste, considerando diferentes subconjuntos de features e normalizacao MM\nfor fs_name, feature_list in featurelist_dict.items():\n    if fs_name + '-MM' in best_xgb_trial_params.keys():\n        print(f'Kaggle prediction for {fs_name} x MM...')\n        X_kaggle = normalized_originalValFeatures_df[feature_list]\n        final_model = final_classifier_XGB[f'{fs_name} x MM']\n\n        # Salvando as previsões feitas pelo classificador para o conjunto de teste em um arquivo CSV\n        pd.DataFrame({\n            'ID_code'    : testID,\n            'target': final_model.predict_proba(X_kaggle)[:, 1]\n        }).to_csv(f'final-pred-{fs_name}-MM.csv', index = False)\n        # Salvando a previsao que sera considerada na avaliacao do Kaggle\n        if fs_name == 'sbs_nf-196':\n            pd.DataFrame({\n            'ID_code'    : testID,\n            'target': final_model.predict_proba(X_kaggle)[:, 1]\n        }).to_csv('submission.csv', index = False)\n        # end if\n    # end if\n# end for","metadata":{"execution":{"iopub.status.busy":"2022-06-01T12:33:45.919799Z","iopub.execute_input":"2022-06-01T12:33:45.921204Z","iopub.status.idle":"2022-06-01T12:35:31.290243Z","shell.execute_reply.started":"2022-06-01T12:33:45.921138Z","shell.execute_reply":"2022-06-01T12:35:31.288721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We conclude this analysis, finding out that the XGBoost algorithm generated the model with the best score for ROC AUC metric. The score achieved for the test datasets for the ROC AUC, as assessed by Kaggle, was:\n\nDataset : Kaggle public score (ROC AUC for the test data)\n* Information gain_nf-150 : 0.87631\n* final-pred-mean_abs_diff_nf-150-MM : 0.83892\n* final-pred-disp_ratio_nf-150-MM : 0.85341\n* final-pred-lr_nf-150-MM : 0.80854\n* final-pred-anova_nf-150-MM : 0.88891\n* final-pred-rf_nf-150-MM : 0.88782\n* final-pred-sbs_nf-196-MM : 0.88936, then 0.89041\n* final-pred-original_nf-200-MM : 0.87310, then 0.89044\n* final-pred-original_without_quasi_constant_nf-190-MM : 0.88187\n* final-pred-lasso_l1_nf-90-MM : 0.86930","metadata":{}},{"cell_type":"markdown","source":"### The highest score obtained with the submissions above was 0.89044 with XGBoost and original_nf-200-MM dataset.","metadata":{}},{"cell_type":"markdown","source":"## 8. Observing the target values *vs.* feature values using PCA","metadata":{}},{"cell_type":"code","source":"y_train.sum() / y_train.count()","metadata":{"execution":{"iopub.status.busy":"2022-06-01T12:35:55.001756Z","iopub.execute_input":"2022-06-01T12:35:55.002217Z","iopub.status.idle":"2022-06-01T12:35:55.010287Z","shell.execute_reply.started":"2022-06-01T12:35:55.002165Z","shell.execute_reply":"2022-06-01T12:35:55.009523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test.sum() / y_test.count()","metadata":{"execution":{"iopub.status.busy":"2022-06-01T12:35:55.506484Z","iopub.execute_input":"2022-06-01T12:35:55.507096Z","iopub.status.idle":"2022-06-01T12:35:55.514234Z","shell.execute_reply.started":"2022-06-01T12:35:55.507043Z","shell.execute_reply":"2022-06-01T12:35:55.513405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\nfrom sklearn.pipeline import Pipeline\n\ndef plot_pca_values_with_target(X, y, title):\n    scaler = StandardScaler()\n    mm_scaler = MinMaxScaler()\n    rob_scaler = RobustScaler()\n    pca2D = PCA(n_components=2)\n    pipe = Pipeline([('mm_scaler', scaler), ('pca', pca2D)])\n    Xt = pipe.fit_transform(X)\n\n    plt.figure(figsize=(8,6))\n    plt.title(title)\n    df_pca = pd.concat([pd.DataFrame(data=Xt, columns=['PC1', 'PC2']), y.to_frame()], axis=1)\n    sns.scatterplot(data=df_pca, x=\"PC1\", y=\"PC2\", hue=\"target\", style='target')","metadata":{"execution":{"iopub.status.busy":"2022-06-01T12:35:57.745021Z","iopub.execute_input":"2022-06-01T12:35:57.745642Z","iopub.status.idle":"2022-06-01T12:35:57.754044Z","shell.execute_reply.started":"2022-06-01T12:35:57.745593Z","shell.execute_reply":"2022-06-01T12:35:57.753251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_pca_values_with_target(X_train, y_train, 'Real train data')","metadata":{"execution":{"iopub.status.busy":"2022-06-01T12:35:59.693094Z","iopub.execute_input":"2022-06-01T12:35:59.693535Z","iopub.status.idle":"2022-06-01T12:36:05.079053Z","shell.execute_reply.started":"2022-06-01T12:35:59.6935Z","shell.execute_reply":"2022-06-01T12:36:05.077894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for fs_name, feature_list in featurelist_dict.items():\n    if fs_name + '-MM' in best_xgb_trial_params.keys():\n        X_train_MM = train_dict['MM'][feature_list]\n        predTrain = pd.Series(data=final_classifier_XGB[fs_name + ' x MM'].predict(X_train_MM), name='target')\n        plot_pca_values_with_target(X_train, predTrain, f'Predicted train data - {fs_name}')\n    # end if\n# end for","metadata":{"execution":{"iopub.status.busy":"2022-06-01T12:36:05.081479Z","iopub.execute_input":"2022-06-01T12:36:05.081973Z","iopub.status.idle":"2022-06-01T12:36:56.467522Z","shell.execute_reply.started":"2022-06-01T12:36:05.081924Z","shell.execute_reply":"2022-06-01T12:36:56.466717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_pca_values_with_target(X_test, y_test, 'Validation data')","metadata":{"execution":{"iopub.status.busy":"2022-06-01T12:36:56.468932Z","iopub.execute_input":"2022-06-01T12:36:56.469711Z","iopub.status.idle":"2022-06-01T12:36:57.859042Z","shell.execute_reply.started":"2022-06-01T12:36:56.469671Z","shell.execute_reply":"2022-06-01T12:36:57.857918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for fs_name, feature_list in featurelist_dict.items():\n    if fs_name + '-MM' in best_xgb_trial_params.keys():\n        X_val_MM = normalized_originalValFeatures_df[feature_list]\n        predTrain = pd.Series(data=final_classifier_XGB[fs_name + ' x MM'].predict(X_val_MM), name='target')\n        plot_pca_values_with_target(X_train, predTrain, f'Predicted test data - {fs_name}')\n    # end if\n# end for","metadata":{"execution":{"iopub.status.busy":"2022-06-01T12:36:57.860474Z","iopub.execute_input":"2022-06-01T12:36:57.860814Z","iopub.status.idle":"2022-06-01T12:38:04.247116Z","shell.execute_reply.started":"2022-06-01T12:36:57.860784Z","shell.execute_reply":"2022-06-01T12:38:04.245966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Comment\n\nIt is difficult to establish a decision frontier based on the 2D PCA transformation of the dataset.","metadata":{}},{"cell_type":"markdown","source":"## 9. Exploring the rounding thrshold of the predicted target values\n\nLet's test Kaggle submissions with different thresholds when rounding the predicted `target` value, instead of submitting the `predict_proba` value to Kaggle.","metadata":{}},{"cell_type":"code","source":"if not kaggle:\n    # Fazendo previsoes para o conjunto de teste, considerando diferentes subconjuntos de features e normalizacao MM\n    for threshold in np.arange(0.1, 0.9, 0.1):\n        for fs_name, feature_list in featurelist_dict.items():\n            if fs_name + '-MM' in best_xgb_trial_params.keys():\n                print(f'Kaggle prediction for {fs_name} x MM and threshold = {threshold}...')\n                final_model = final_classifier_XGB[f'{fs_name} x MM']\n\n                df_val_subset = normalized_originalValFeatures_df[feature_list]\n                # Salvando as previsões feitas pelo classificador para o conjunto de teste em um arquivo CSV\n                rounder = lambda t: 1 if t >= threshold else 0\n                vround = np.vectorize(rounder)\n                predicted = vround(final_model.predict_proba(df_val_subset)[:, 1])\n                pd.DataFrame({\n                    'ID_code'    : testID,\n                    'target': predicted\n                }).to_csv(f'final-pred-{fs_name}-MM-thres-{threshold}.csv', index = False)\n            # end if\n        # end for\n    # end for","metadata":{"execution":{"iopub.status.busy":"2022-06-01T12:38:04.249474Z","iopub.execute_input":"2022-06-01T12:38:04.250442Z","iopub.status.idle":"2022-06-01T12:38:04.260532Z","shell.execute_reply.started":"2022-06-01T12:38:04.250389Z","shell.execute_reply":"2022-06-01T12:38:04.259005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if kaggle:\n    base_folder = '../input/santandercusttransactionmydata'\nelse:\n    base_folder = '.'\n# end if\ndf_kaggle_threshold = pd.read_csv(os.path.join(base_folder, 'kaggle_model_threshold_scores.csv'))\ndf_kaggle_threshold","metadata":{"execution":{"iopub.status.busy":"2022-06-01T12:38:04.262242Z","iopub.execute_input":"2022-06-01T12:38:04.262911Z","iopub.status.idle":"2022-06-01T12:38:04.304913Z","shell.execute_reply.started":"2022-06-01T12:38:04.26286Z","shell.execute_reply":"2022-06-01T12:38:04.303665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Comment\n\nAs we can see, the rounding of the `target` value, following a threshold between `0.1` and `0.8`, did not improve the scores. \n\nIt is better to stick with the `predict_proba` float output as predicted `target` values.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}