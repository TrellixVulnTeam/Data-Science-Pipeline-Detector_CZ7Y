{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Moodbit’s method to analyze word-choice within the job postings to alert any biases and determine promotions available\n"},{"metadata":{},"cell_type":"markdown","source":"Our submission aims to solve the challenge that the City of Los Angeles will be facing in the upcoming years: How to improve job bulletins so that as many applicants feel encouraged to apply as possible. We have created a method that will analyze word-choice within the job postings to alert any biases that may be present. \n\nResearch shows that men will apply for a job after only meeting 60% of the qualifications on the job description, while women are more hesitant and will only apply after meeting all 100% of the qualification. Therefore, it is clear that using too many masculine-coded words would create a deficit in applications submitted by women. With our method, identifying these word-choices that include a gender bias is fast and reliable. Words that are flagged as gender-coded can be altered with the provided recommendations of gender-neutral words*. The diversity of the applicant pool is improved as we are definite that the job posting’s content, tone, and requirements do not lean towards one gender. The quality of the applicant pool is improved as we will see a rise in qualified applicants applying that were previously discouraged from applying previously. \n\nLastly, our method made it easier to determine which promotions are available to employees in each job class by extracting the requirement of years, lower job class and required degree needed in one class to be able to move to the next. Our data makes it easy to notify an eligible employee once that time range passes. With our method, job-posting biases are eliminated, word-choice is improved to be more inclusive to the population, and employees are more likely to be aware of new career paths that they are eligible for.\n\n*Further scientific support for our method will be provided in the following sections \n\n"},{"metadata":{},"cell_type":"markdown","source":"Importing Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport csv as csv\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport re\nimport os, sys\nimport numpy as np\nfrom datetime import datetime\nfrom collections  import Counter\nfrom nltk import word_tokenize\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport calendar\nfrom wordcloud import WordCloud ,STOPWORDS\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom gensim.models import word2vec\nfrom sklearn.manifold import TSNE\nfrom nltk import pos_tag\nfrom nltk.help import upenn_tagset\nimport gensim\nimport matplotlib.colors as mcolors\nfrom nltk import jaccard_distance\nfrom nltk import ngrams\nplt.style.use('ggplot')\n\n\nimport spacy\nfrom spacy import displacy\nimport nltk\nimport xml.etree.cElementTree as ET\nfrom collections import OrderedDict\nimport json\nimport networkx as nx","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Goal 0 Convert a folder full of plain-text job postings into a single structured CSV file\nchecking all subdirectories"},{"metadata":{"trusted":true},"cell_type":"code","source":"bulletins=os.listdir(\"../input/data-science-for-good-city-of-los-angeles/cityofla/CityofLA/Job Bulletins/\")\nadditional=os.listdir(\"../input/data-science-for-good-city-of-los-angeles/cityofla/CityofLA/Additional data/\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"files=[dir for dir in os.walk('../input/data-science-for-good-city-of-los-angeles/CityofLA/CityofLA/')]\nfor file in files:\n    print(os.listdir(file[0]))\n    print(\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"csvfiles=[]\nfor file in additional:\n    if file.endswith('.csv'):\n        print(file)\n        csvfiles.append(\"../input/data-science-for-good-city-of-los-angeles/cityofla/CityofLA/Additional data/\"+file)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Reading the required csv files"},{"metadata":{"trusted":true},"cell_type":"code","source":"job_title=pd.read_csv(csvfiles[2])\nsample_job=pd.read_csv(csvfiles[0])\nkaggle_data=pd.read_csv(csvfiles[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"job_title.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Extracting the headings from job bulletins"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_headings(bulletin):       \n    \n    \"\"\"\"function to get the headings from text file\n        takes a single argument\n        1.takes single argument list of bulletin files\"\"\"\n    \n    with open(\"../input/data-science-for-good-city-of-los-angeles/cityofla/CityofLA/Job Bulletins/\"+bulletins[bulletin]) as f:    ##reading text files \n        data=f.read().replace('\\t','').split('\\n')\n        data=[head for head in data if head.isupper()]\n        return data\n        \ndef clean_text(bulletin):      \n    \n    \n    \"\"\"function to do basic data cleaning\n        takes a single argument\n        1.takes single argument list of bulletin files\"\"\"\n                                            \n    \n    with open(\"../input/data-science-for-good-city-of-los-angeles/cityofla/CityofLA/Job Bulletins/\"+bulletins[bulletin]) as f:\n        data=f.read().replace('\\t','').replace('\\n','')\n        return data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Headings"},{"metadata":{"trusted":true},"cell_type":"code","source":"get_headings(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Extracting Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"def to_dataframe(num,df):\n    \"\"\"\"function to extract features from job bulletin text files and convert to\n    pandas dataframe.\n    function take two arguments \n                        1.the number of files to be read\n                        2.dataframe object                                      \"\"\"\n    \n\n    \n    opendate=re.compile(r'(Open [D,d]ate:)(\\s+)(\\d+-\\d\\d-\\d\\d)')       #match open date\n    \n    salary=re.compile(r'\\$(\\d+,\\d+)((\\s(to|and)\\s)(\\$\\d+,\\d+))?')       #match salary\n    \n    requirements=re.compile(r'(REQUIREMENTS?/\\s?MINIMUM QUALIFICATIONS?)(.*)(PROCESS NOTE)')      #match requirements\n    \n    apply=re.compile(r'(WHERE TO APPLY?)(.*)(NOTE)')\n    \n    for no in range(0,num):\n        with open(\"../input/data-science-for-good-city-of-los-angeles/cityofla/CityofLA/Job Bulletins/\"+bulletins[no],encoding=\"ISO-8859-1\") as f:         #reading files \n                try:\n                    file=f.read().replace('\\t','')\n                    data=file.replace('\\n','')\n                    headings=[heading for heading in file.split('\\n') if heading.isupper()]             ##getting heading from job bulletin\n\n                    try:\n                        date=datetime.strptime(re.search(opendate,data).group(3),'%m-%d-%y')\n                    except Exception as e:\n                        date=np.nan\n                    try:\n                        req=re.search(requirements,data).group(2)\n                    except Exception as e:\n                        try: \n                            req=re.search('(.*)POST-Certified',re.findall(r'(REQUIREMENTS?)(.*)(POST-Certified)', \n                                                                  data)[0][1][:1200]).group(1)\n                        except Exception as e:\n                            req=re.search('(.*)NOTES?',re.findall(r'(REQUIREMENTS?)(.*)(NOTES?)',\n                                                              data)[0][1][:1200]).group(1)\n                    \n                    try:\n                        duties=re.search(r'(DUTIES)(.*)(REQ[A-Z])',data).group(2)\n                        \n                    except Exception as e:\n                        duties=np.nan\n                    \n                    app=re.search(apply,data).group(2)\n                    \n                    try:\n                        enddate=re.search(\n                                r'(JANUARY|FEBRUARY|MARCH|APRIL|MAY|JUNE|JULY|AUGUST|SEPTEMBER|OCTOBER|NOVEMBER|DECEMBER)\\s(\\d{1,2},\\s\\d{4})'\n                                ,data).group()\n                    except Exception as e:\n                        enddate=np.nan\n                        \n                    sal=re.search(salary,data)\n                    selection= [z[0] for z in re.findall('([A-Z][a-z]+)((\\s\\.\\s)+)',data)]     ##match selection criteria\n                    try:\n                        sal=re.search(salary,data)\n                        df=df.append({'FILE_NAME':bulletins[no],'Position':headings[0].lower(), 'salary_start':sal.group(1),'salary_end':sal.group(5),\n                                  \"opendate\":date,\"requirements\":req, 'duties':duties, 'apply':app,#'duties':duties, 'apply':app,\n                                  'deadline':enddate},ignore_index=True)  #,'selection':selection\n                    \n                    except Exception as e:\n                        sal=np.nan\n                        df=df.append({'FILE_NAME':bulletins[no],'Position':headings[0].lower(), 'salary_start':sal,'salary_end':sal,\n                                  \"opendate\":date, \"requirements\":req, 'duties':duties, 'apply':app,#\"requirements\":req,'duties':duties, 'apply':app,\n                                  'deadline':enddate, 'selection':selection},ignore_index=True)  #,'selection':selection\n                    \n                    #selection= [z[0] for z in re.findall('([A-Z][a-z]+)((\\s\\.\\s)+)',data)]     ##match selection criteria\n                    \n                    #df=df.append({'File Name':bulletins[no],'Position':headings[0].lower(), 'salary_start':sal.group(1),'salary_end':sal.group(5),\n                    #              \"opendate\":date,#\"requirements\":req,'duties':duties, 'apply':app,\n                    #              'deadline':enddate},ignore_index=True)  #,'selection':selection\n                    \n                    \n                    reg=re.compile(r'(One|Two|Three|Four|Five|Six|Seven|Eight|Nine|Ten|one|two|three|four)\\s(years?)\\s(of\\sfull(-|\\s)time)')\n                    df['EXPERIENCE_LENGTH']=df['requirements'].apply(lambda x :  re.search(reg,x).group(1) if re.search(reg,x) is not None  else np.nan)\n                    df['FULL_TIME_PART_TIME']=df['EXPERIENCE_LENGTH'].apply(lambda x:  'FULL_TIME' if x is not np.nan else np.nan )\n                    \n                    reg=re.compile(r'(One|Two|Three|Four|Five|Six|Seven|Eight|Nine|Ten|one|two|three|four)(\\s|-)(years?)\\s(college)')\n                    df['EDUCATION_YEARS']=df['requirements'].apply(lambda x :  re.search(reg,x).group(1) if re.search(reg,x) is not None  else np.nan)\n                    df['SCHOOL_TYPE']=df['EDUCATION_YEARS'].apply(lambda x : 'College or University' if x is not np.nan else np.nan)\n                    \n                #except Exception as e:\n                #    print('umatched sequence')\n                #    print(f)\n                except IOError:\n                    print('An error occured trying to read the file.')\n    \n                except ValueError:\n                    print('Non-numeric data found in the file.')\n\n                except ImportError:\n                    print (\"NO module found\")\n    \n                except EOFError:\n                    print('Why did you do an EOF on me?')\n                except KeyboardInterrupt:\n                    print('You cancelled the operation.')\n\n                except Exception as e:\n                    print(e)\n                    print(f)\n                \n                \n        \n           \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.DataFrame(columns=['FILE_NAME','Position','salary_start','salary_end', 'opendate', 'requirements', 'duties', 'apply', 'deadline']) #,'duties', 'apply'\ndf=to_dataframe(len(bulletins),df)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"checking missing value"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bulletin_dir = \"../input/data-science-for-good-city-of-los-angeles/cityofla/CityofLA/Job Bulletins/\"\nadditional_data_dir = \"../input/data-science-for-good-city-of-los-angeles/cityofla/CityofLA/Additional data/\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"headings = {}\nfor filename in os.listdir(bulletin_dir):\n    with open(bulletin_dir + \"/\" + filename, 'r', errors='ignore') as f:\n        for line in f.readlines():\n            line = line.replace(\"\\n\",\"\").replace(\"\\t\",\"\").replace(\":\",\"\").strip()\n            \n            if line.isupper():\n                if line not in headings.keys():\n                    headings[line] = 1\n                else:\n                    count = int(headings[line])\n                    headings[line] = count+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del headings['$103,606 TO $151,484'] #This is not a heading, it's an Annual Salary component\nheadingsFrame = []\nfor i,j in (sorted(headings.items(), key = lambda kv:(kv[1], kv[0]), reverse = True)):\n    headingsFrame.append([i,j])\nheadingsFrame = pd.DataFrame(headingsFrame)\nheadingsFrame.columns = [\"Heading\",\"Count\"]\n#headingsFrame.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Extracting all \"note\" parts for every job posting"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check for note components\nnoteHeadings = [k for k in headingsFrame['Heading'].values if 'note' in k.lower()]\nnote_list = []\nfor filename in os.listdir(bulletin_dir):\n    with open(bulletin_dir + \"/\" + filename, 'r', errors='ignore') as f:\n        readNext = 0\n        for line in f.readlines():\n            clean_line = line.replace(\"\\n\",\"\").replace(\"\\t\",\"\").replace(\":\",\"\").strip()  \n            if clean_line in noteHeadings:\n                readNext = 1\n            elif readNext == 1:\n                if clean_line in headingsFrame['Heading'].values:\n                    break\n                elif len(clean_line)<2:\n                    continue\n                else:\n                    note_list.append([filename, clean_line])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_note = pd.DataFrame(note_list)\ndf_note.columns = ['FILE_NAME','NOTE_TEXT']\ndf_note.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Merge all \"note\" into one for every job posting"},{"metadata":{"trusted":true},"cell_type":"code","source":"file_name = df_note['FILE_NAME'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"note_list = []\nfor title in file_name:\n    d = df_note[df_note['FILE_NAME']==title]\n    context = ' '.join(list(d['NOTE_TEXT']))\n    note_list.append([title, context])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_note = pd.DataFrame(note_list)\ndf_note.columns = ['FILE_NAME','NOTE_TEXT']\ndf_note.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Merge the new feature \"note\" into previous dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"result = pd.merge(df, df_note, how='left', left_on='FILE_NAME', right_on='FILE_NAME', sort=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Extracting all \"process\" parts for every job posting"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check for process components\nproHeadings = [k for k in headingsFrame['Heading'].values if 'process' in k.lower()]\npro_list = []\nfor filename in os.listdir(bulletin_dir):\n    with open(bulletin_dir + \"/\" + filename, 'r', errors='ignore') as f:\n        readNext = 0\n        for line in f.readlines():\n            clean_line = line.replace(\"\\n\",\"\").replace(\"\\t\",\"\").replace(\":\",\"\").strip()  \n            if clean_line in proHeadings:\n                readNext = 1\n            elif readNext == 1:\n                if clean_line in headingsFrame['Heading'].values:\n                    break\n                elif len(clean_line)<2:\n                    continue\n                else:\n                    pro_list.append([filename, clean_line])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_pro = pd.DataFrame(pro_list)\ndf_pro.columns = ['FILE_NAME','PROCESS_TEXT']\ndf_pro.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Merge all \"process\" into one for every job posting"},{"metadata":{"trusted":true},"cell_type":"code","source":"pro_list = []\nfor title in file_name:\n    df = df_pro[df_pro['FILE_NAME']==title]\n    context = ' '.join(list(df['PROCESS_TEXT']))\n    pro_list.append([title, context])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_pro = pd.DataFrame(pro_list)\ndf_pro.columns = ['FILE_NAME','PROCESS_TEXT']\ndf_pro.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Merge the new feature \"process\" into previous dataframe and get the final dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"result = pd.merge(result, df_pro, how='left', left_on='FILE_NAME', right_on='FILE_NAME', sort=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Goal 1 & 2 Identify language that can negatively bias the pool of applicants and improve the diversity and quality of the applicant pool"},{"metadata":{},"cell_type":"markdown","source":"We created our dictionary of replacement words with information provided by the University of North Carolina Chapel Hill’s Writing Center. The institution used research findings to create a list of non-gender nouns to replace those that were previously biased. See link here: \nhttps://writingcenter.unc.edu/tips-and-tools/gender-inclusive-language/. \nAlong with this list, it also provided us with tips and tools to expand further from this minimal list. Based on UNC’s suggestions, we used a thesaurus to find replacement words for other known biased words. An additional list of words was retrieved from a research study by Danielle Gaucher, Justin Friesen, and Aaron C. Kay “Evidence That Gendered Wording in Job Advertisements Exists and Sustains Gender Inequality.” Accessible here: http://gender-decoder.katmatfield.com/static/documents/Gaucher-Friesen-Kay-JPSP-Gendered-Wording-in-Job-ads.pdf\nWe used this list of commonly found gender-biased language in job postings and created synonyms for each word/phrase on it. We ensured that the synonyms we choose did not include any biases by putting it through the insightful gender decoder created by Kat Matfield. Accessible here: http://gender-decoder.katmatfield.com/. \n"},{"metadata":{},"cell_type":"markdown","source":"Read in biased words and replacement list"},{"metadata":{"trusted":true},"cell_type":"code","source":"word_list = pd.read_csv('../input/genderbiasdictionary/genderbiascatalog1.csv')\nword_list","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Get bias words list"},{"metadata":{"trusted":true},"cell_type":"code","source":"masculine_list = word_list['Biased Catalog']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Get replace words list"},{"metadata":{"trusted":true},"cell_type":"code","source":"replace_list = word_list['Replace']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Count frequencies for bias words"},{"metadata":{"trusted":true},"cell_type":"code","source":"master_dic = {}\n\nmasculine_list = [w.lower() for w in masculine_list]\ncolumns = ['requirements', 'NOTE_TEXT', 'duties', 'PROCESS_TEXT', 'apply']\n\nfor c in columns:\n    for sentence in result[c]:\n        for word in str(sentence).split():\n            if word.lower() not in master_dic and word.lower() in masculine_list:\n                master_dic[word.lower()] = 1\n            elif word.lower() in master_dic and word.lower() in masculine_list:\n                master_dic[word.lower()] += 1\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"master_dic","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"file_dic = {}\n\nfor name in result['FILE_NAME']:\n    for c in columns:\n        for sentence in result[result['FILE_NAME']==name][c]:\n            for word in str(sentence).split():\n                if name not in file_dic and word.lower() in masculine_list:\n                    file_dic[name] = [word.lower()]\n                elif name in file_dic and word.lower() in masculine_list and word.lower() not in file_dic[name]:\n                    file_dic[name].append(word.lower())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check bias words in each job posting file"},{"metadata":{"trusted":true},"cell_type":"code","source":"file_dic","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"file_df = pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in file_dic.items() ]))\nfile_df_T = file_df.transpose()\nfile_df_T.columns = ['bias_1', 'bias_2', 'bias_3', 'bias_4', 'bias_5', 'bias_6', 'bias_7']\nfile_df_T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Define function to get replacement for each bias word"},{"metadata":{"trusted":true},"cell_type":"code","source":"def ifef(col):\n    col = str(col)\n    if col in masculine_list:\n        i = masculine_list.index(col)\n        return  replace_list[i]\n    else:\n        return 'NaN'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Show bias words and its replacement for each job posting file"},{"metadata":{"trusted":true},"cell_type":"code","source":"file_df_T['replace_1'] = file_df_T['bias_1'].apply(ifef)\nfile_df_T['replace_2'] = file_df_T['bias_2'].apply(ifef)\nfile_df_T['replace_3'] = file_df_T['bias_3'].apply(ifef)\nfile_df_T['replace_4'] = file_df_T['bias_4'].apply(ifef)\nfile_df_T['replace_5'] = file_df_T['bias_5'].apply(ifef)\nfile_df_T['replace_6'] = file_df_T['bias_6'].apply(ifef)\nfile_df_T['replace_7'] = file_df_T['bias_7'].apply(ifef)\nfile_df_T = file_df_T[['bias_1', 'replace_1', 'bias_2', 'replace_2', 'bias_3', 'replace_3', 'bias_4', 'replace_4', 'bias_5', 'replace_5', 'bias_6', 'replace_6', 'bias_7', 'replace_7']]\nfile_df_T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Goal 3 Make it easier to determine which promotions are available to employees in each job class\nExtract Lower Job Class"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extract Lower Job Class\nreq_pos = result[['Position', 'requirements']].copy()\n# regular expression for extracting job class\njob_regex = r'(?:(?<=experience\\swith\\s)|(?<=experience\\s))(.+?)(?:\\.|\\;)'\n\nclass_dict2 = {}\n\nfor name in req_pos['Position']:\n    for sentence in req_pos[req_pos['Position']==name][\"requirements\"]:\n            job = re.findall(job_regex, sentence)\n            class_dict2[name] = job","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"class_dict2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Extract degree requirements"},{"metadata":{"trusted":true},"cell_type":"code","source":"degree_dict = {}\n#regex for extracting degree\ndegree_regex = r'(?<=degree\\s).+?\\.'\nfor name in req_pos['Position']:\n    for sentence in req_pos[req_pos['Position']==name][\"requirements\"]:\n            degree = re.findall(degree_regex, sentence)\n            degree_dict[name] = degree\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"degree_dict","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create Direct Graph"},{"metadata":{"trusted":true},"cell_type":"code","source":"class_df = pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in class_dict2.items() ]))\nclass_df = class_df.transpose()\nclass_df.reset_index(inplace=True)\nclass_df = pd.melt(class_df, id_vars=['index'], var_name='number', value_name = 'requirements')\n\nclass_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for name in class_df.loc[0:100,'index']:\n    group = class_df.groupby('index').get_group(name)\n    FG = nx.from_pandas_edgelist(group, source='requirements', target='index', edge_attr=True)\n    plt.figure()\n    nx.draw_networkx(FG, with_labels=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Recommendation Sections:\n\nDid the authors use the structured data to make an original insight?\nThe structured data was used to identify the number of masculine coded and feminine coded words used throughout all 683 job posts. The structured data was also extracted to obtain the required past experience for a job position in order to assist with the creation of an easy method for showing employees promotion opportunities available.\n\nDid the authors identify and communicate details about something that they discovered in the data?\nWe communicated that the data showed gender-coded words were being used frequently throughout all job class postings. This finding can be contributing to the problem: Biases existing in job postings preventing people from applying. We communicated the need for these words to be evenly distributed throughout a job positing so that all genders feel qualified to apply and not feel un-wanted due to the language choice. We also found that it was difficult to interpret the promotion possibilities from one job to the next with the current structured data. \n\nDid the authors make an actionable recommendation to the City of LA?\nYes. We recommended that the City of LA analyzes the words that they use prior to posting the job for applicants to read. They first should decode the biased words, then use replace them with the gender-neutral words that we have provided, and then can analyze the posting again with our technology to be sure all biases have been eliminated. We also recommend that the City of LA provides more understandable pathways for each employee to be aware of, as well as give notifications to employees when they have reached the year requirement for a promotional opportunity. This would allow more employees to move up to a more distinguished position as well as open up more lower-class jobs to future applicants. \n\nDid the authors make effective use of data visualizations to communicate their recommendations to the City of LA?\nWe showed the high frequency of gender-coded words in the original job postings provided by the City of LA in a bar graph. We were able to demonstrate the most commonly used words in job postings that should be replaced as well as the amount of biased words found in each individual job posting. We also provided at list of new recommendations for the biased words. \n\nDo the recommendations give helpful solutions to one or more of the following issues? Yes, all of them.\n(1) identify language that can bias the pool of applicants – Yes, with the gender-decoder.\n(2) improve the diversity and quality of the applicant pool; and/or – Yes, with the unbiased replacement words provided.\n(3) increase the discoverability of promotional pathways – Yes, with our ability to extract the requirements from job postings and create an on-time notification system for employees to be aware of when they reach that point. \n"},{"metadata":{},"cell_type":"markdown","source":"## Moodbit's Team\nStephanie Hinck, Chang Qu, Ting Cai, Alfredo Jaldin, Yichao Shen\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}